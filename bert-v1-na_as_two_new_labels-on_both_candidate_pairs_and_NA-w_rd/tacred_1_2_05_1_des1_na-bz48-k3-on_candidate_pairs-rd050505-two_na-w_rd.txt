#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 129.3779977CurrentTrain: epoch  0, batch     1 | loss: 91.5984037CurrentTrain: epoch  0, batch     2 | loss: 78.4685677CurrentTrain: epoch  0, batch     3 | loss: 87.9470699CurrentTrain: epoch  0, batch     4 | loss: 87.3746906CurrentTrain: epoch  0, batch     5 | loss: 87.4750140CurrentTrain: epoch  0, batch     6 | loss: 102.3074502CurrentTrain: epoch  0, batch     7 | loss: 100.8070974CurrentTrain: epoch  0, batch     8 | loss: 86.0856671CurrentTrain: epoch  0, batch     9 | loss: 86.5745613CurrentTrain: epoch  0, batch    10 | loss: 77.2056033CurrentTrain: epoch  0, batch    11 | loss: 101.0818543CurrentTrain: epoch  0, batch    12 | loss: 99.2936629CurrentTrain: epoch  0, batch    13 | loss: 193.4966849CurrentTrain: epoch  0, batch    14 | loss: 100.5358663CurrentTrain: epoch  0, batch    15 | loss: 86.5357287CurrentTrain: epoch  0, batch    16 | loss: 86.5922547CurrentTrain: epoch  0, batch    17 | loss: 118.2685568CurrentTrain: epoch  0, batch    18 | loss: 99.5527763CurrentTrain: epoch  0, batch    19 | loss: 86.2684664CurrentTrain: epoch  0, batch    20 | loss: 146.8876981CurrentTrain: epoch  0, batch    21 | loss: 145.7447596CurrentTrain: epoch  0, batch    22 | loss: 193.5459085CurrentTrain: epoch  0, batch    23 | loss: 146.0156473CurrentTrain: epoch  0, batch    24 | loss: 99.4403051CurrentTrain: epoch  0, batch    25 | loss: 191.7583025CurrentTrain: epoch  0, batch    26 | loss: 86.0583595CurrentTrain: epoch  0, batch    27 | loss: 117.7476414CurrentTrain: epoch  0, batch    28 | loss: 144.7363923CurrentTrain: epoch  0, batch    29 | loss: 98.7775981CurrentTrain: epoch  0, batch    30 | loss: 145.7283896CurrentTrain: epoch  0, batch    31 | loss: 145.4891134CurrentTrain: epoch  0, batch    32 | loss: 117.2245106CurrentTrain: epoch  0, batch    33 | loss: 85.0575331CurrentTrain: epoch  0, batch    34 | loss: 146.2439133CurrentTrain: epoch  0, batch    35 | loss: 85.1374565CurrentTrain: epoch  0, batch    36 | loss: 84.4994811CurrentTrain: epoch  0, batch    37 | loss: 98.4600357CurrentTrain: epoch  0, batch    38 | loss: 97.6534249CurrentTrain: epoch  0, batch    39 | loss: 84.3264313CurrentTrain: epoch  0, batch    40 | loss: 116.3412426CurrentTrain: epoch  0, batch    41 | loss: 85.2665874CurrentTrain: epoch  0, batch    42 | loss: 84.1772365CurrentTrain: epoch  0, batch    43 | loss: 73.6459045CurrentTrain: epoch  0, batch    44 | loss: 96.8330764CurrentTrain: epoch  0, batch    45 | loss: 82.6422292CurrentTrain: epoch  0, batch    46 | loss: 113.3105419CurrentTrain: epoch  0, batch    47 | loss: 82.5759435CurrentTrain: epoch  0, batch    48 | loss: 95.0950545CurrentTrain: epoch  0, batch    49 | loss: 96.4385799CurrentTrain: epoch  0, batch    50 | loss: 95.7613558CurrentTrain: epoch  0, batch    51 | loss: 95.9689165CurrentTrain: epoch  0, batch    52 | loss: 93.7404058CurrentTrain: epoch  0, batch    53 | loss: 140.5673197CurrentTrain: epoch  0, batch    54 | loss: 112.3894514CurrentTrain: epoch  0, batch    55 | loss: 81.1145746CurrentTrain: epoch  0, batch    56 | loss: 82.5769956CurrentTrain: epoch  0, batch    57 | loss: 95.1067518CurrentTrain: epoch  0, batch    58 | loss: 92.9794220CurrentTrain: epoch  0, batch    59 | loss: 93.3855039CurrentTrain: epoch  0, batch    60 | loss: 91.7881685CurrentTrain: epoch  0, batch    61 | loss: 93.4442895CurrentTrain: epoch  0, batch    62 | loss: 80.8889904CurrentTrain: epoch  0, batch    63 | loss: 109.0962369CurrentTrain: epoch  0, batch    64 | loss: 91.0609804CurrentTrain: epoch  0, batch    65 | loss: 69.2619713CurrentTrain: epoch  0, batch    66 | loss: 88.7463608CurrentTrain: epoch  0, batch    67 | loss: 91.3720067CurrentTrain: epoch  0, batch    68 | loss: 79.9831363CurrentTrain: epoch  0, batch    69 | loss: 134.1558908CurrentTrain: epoch  0, batch    70 | loss: 116.6649988CurrentTrain: epoch  0, batch    71 | loss: 80.3530814CurrentTrain: epoch  0, batch    72 | loss: 94.1687924CurrentTrain: epoch  0, batch    73 | loss: 108.5153686CurrentTrain: epoch  0, batch    74 | loss: 138.7680484CurrentTrain: epoch  0, batch    75 | loss: 91.1975231CurrentTrain: epoch  0, batch    76 | loss: 107.0415944CurrentTrain: epoch  0, batch    77 | loss: 90.0431280CurrentTrain: epoch  0, batch    78 | loss: 107.3685405CurrentTrain: epoch  0, batch    79 | loss: 108.6514916CurrentTrain: epoch  0, batch    80 | loss: 78.0611332CurrentTrain: epoch  0, batch    81 | loss: 88.9438764CurrentTrain: epoch  0, batch    82 | loss: 107.8899013CurrentTrain: epoch  0, batch    83 | loss: 90.7654007CurrentTrain: epoch  0, batch    84 | loss: 103.6394871CurrentTrain: epoch  0, batch    85 | loss: 78.0632128CurrentTrain: epoch  0, batch    86 | loss: 130.6699386CurrentTrain: epoch  0, batch    87 | loss: 79.5442844CurrentTrain: epoch  0, batch    88 | loss: 76.6021538CurrentTrain: epoch  0, batch    89 | loss: 63.7138832CurrentTrain: epoch  0, batch    90 | loss: 89.3713975CurrentTrain: epoch  0, batch    91 | loss: 78.5372742CurrentTrain: epoch  0, batch    92 | loss: 86.2694956CurrentTrain: epoch  0, batch    93 | loss: 131.9367238CurrentTrain: epoch  0, batch    94 | loss: 76.5289258CurrentTrain: epoch  0, batch    95 | loss: 90.0172079CurrentTrain: epoch  1, batch     0 | loss: 75.7334583CurrentTrain: epoch  1, batch     1 | loss: 87.8603465CurrentTrain: epoch  1, batch     2 | loss: 75.4806750CurrentTrain: epoch  1, batch     3 | loss: 76.1700353CurrentTrain: epoch  1, batch     4 | loss: 84.8799659CurrentTrain: epoch  1, batch     5 | loss: 74.2079743CurrentTrain: epoch  1, batch     6 | loss: 91.1653782CurrentTrain: epoch  1, batch     7 | loss: 130.5424907CurrentTrain: epoch  1, batch     8 | loss: 67.2072485CurrentTrain: epoch  1, batch     9 | loss: 100.9049744CurrentTrain: epoch  1, batch    10 | loss: 88.2626844CurrentTrain: epoch  1, batch    11 | loss: 74.3086341CurrentTrain: epoch  1, batch    12 | loss: 73.3068634CurrentTrain: epoch  1, batch    13 | loss: 105.7820062CurrentTrain: epoch  1, batch    14 | loss: 100.4708399CurrentTrain: epoch  1, batch    15 | loss: 89.9225365CurrentTrain: epoch  1, batch    16 | loss: 129.3594954CurrentTrain: epoch  1, batch    17 | loss: 73.8460646CurrentTrain: epoch  1, batch    18 | loss: 83.2520946CurrentTrain: epoch  1, batch    19 | loss: 104.7446506CurrentTrain: epoch  1, batch    20 | loss: 132.8402310CurrentTrain: epoch  1, batch    21 | loss: 76.7990191CurrentTrain: epoch  1, batch    22 | loss: 72.1790404CurrentTrain: epoch  1, batch    23 | loss: 86.3527258CurrentTrain: epoch  1, batch    24 | loss: 128.9051792CurrentTrain: epoch  1, batch    25 | loss: 87.2541002CurrentTrain: epoch  1, batch    26 | loss: 62.8041057CurrentTrain: epoch  1, batch    27 | loss: 72.8491461CurrentTrain: epoch  1, batch    28 | loss: 62.6959073CurrentTrain: epoch  1, batch    29 | loss: 63.0507182CurrentTrain: epoch  1, batch    30 | loss: 88.4213060CurrentTrain: epoch  1, batch    31 | loss: 86.9366244CurrentTrain: epoch  1, batch    32 | loss: 106.6905953CurrentTrain: epoch  1, batch    33 | loss: 90.9818667CurrentTrain: epoch  1, batch    34 | loss: 99.8674060CurrentTrain: epoch  1, batch    35 | loss: 130.4348697CurrentTrain: epoch  1, batch    36 | loss: 73.7946478CurrentTrain: epoch  1, batch    37 | loss: 85.4048748CurrentTrain: epoch  1, batch    38 | loss: 89.0409296CurrentTrain: epoch  1, batch    39 | loss: 62.1482374CurrentTrain: epoch  1, batch    40 | loss: 70.5168685CurrentTrain: epoch  1, batch    41 | loss: 70.3320482CurrentTrain: epoch  1, batch    42 | loss: 77.9592523CurrentTrain: epoch  1, batch    43 | loss: 85.7636851CurrentTrain: epoch  1, batch    44 | loss: 129.8355788CurrentTrain: epoch  1, batch    45 | loss: 85.3501724CurrentTrain: epoch  1, batch    46 | loss: 103.4741361CurrentTrain: epoch  1, batch    47 | loss: 68.5932133CurrentTrain: epoch  1, batch    48 | loss: 87.3599237CurrentTrain: epoch  1, batch    49 | loss: 86.4199604CurrentTrain: epoch  1, batch    50 | loss: 84.9202103CurrentTrain: epoch  1, batch    51 | loss: 107.3402149CurrentTrain: epoch  1, batch    52 | loss: 105.7644590CurrentTrain: epoch  1, batch    53 | loss: 131.4947264CurrentTrain: epoch  1, batch    54 | loss: 76.5189512CurrentTrain: epoch  1, batch    55 | loss: 85.5809567CurrentTrain: epoch  1, batch    56 | loss: 84.7016310CurrentTrain: epoch  1, batch    57 | loss: 102.6723704CurrentTrain: epoch  1, batch    58 | loss: 86.9372374CurrentTrain: epoch  1, batch    59 | loss: 71.0213076CurrentTrain: epoch  1, batch    60 | loss: 84.0442997CurrentTrain: epoch  1, batch    61 | loss: 127.3591582CurrentTrain: epoch  1, batch    62 | loss: 134.7460446CurrentTrain: epoch  1, batch    63 | loss: 70.4693826CurrentTrain: epoch  1, batch    64 | loss: 85.6859869CurrentTrain: epoch  1, batch    65 | loss: 103.7119819CurrentTrain: epoch  1, batch    66 | loss: 64.2510584CurrentTrain: epoch  1, batch    67 | loss: 130.2844988CurrentTrain: epoch  1, batch    68 | loss: 69.0605418CurrentTrain: epoch  1, batch    69 | loss: 77.4648942CurrentTrain: epoch  1, batch    70 | loss: 70.3078599CurrentTrain: epoch  1, batch    71 | loss: 83.0630914CurrentTrain: epoch  1, batch    72 | loss: 103.7524893CurrentTrain: epoch  1, batch    73 | loss: 96.4358518CurrentTrain: epoch  1, batch    74 | loss: 102.1332249CurrentTrain: epoch  1, batch    75 | loss: 81.7304456CurrentTrain: epoch  1, batch    76 | loss: 88.5545153CurrentTrain: epoch  1, batch    77 | loss: 87.6452253CurrentTrain: epoch  1, batch    78 | loss: 81.6722428CurrentTrain: epoch  1, batch    79 | loss: 86.8318256CurrentTrain: epoch  1, batch    80 | loss: 64.1168188CurrentTrain: epoch  1, batch    81 | loss: 99.0427295CurrentTrain: epoch  1, batch    82 | loss: 74.0740412CurrentTrain: epoch  1, batch    83 | loss: 174.2043579CurrentTrain: epoch  1, batch    84 | loss: 100.6826181CurrentTrain: epoch  1, batch    85 | loss: 67.3469480CurrentTrain: epoch  1, batch    86 | loss: 62.3838745CurrentTrain: epoch  1, batch    87 | loss: 72.4013773CurrentTrain: epoch  1, batch    88 | loss: 104.2596274CurrentTrain: epoch  1, batch    89 | loss: 82.3289197CurrentTrain: epoch  1, batch    90 | loss: 74.5629646CurrentTrain: epoch  1, batch    91 | loss: 83.9419097CurrentTrain: epoch  1, batch    92 | loss: 85.4519666CurrentTrain: epoch  1, batch    93 | loss: 129.9209775CurrentTrain: epoch  1, batch    94 | loss: 90.0653916CurrentTrain: epoch  1, batch    95 | loss: 67.3082944CurrentTrain: epoch  2, batch     0 | loss: 60.1679142CurrentTrain: epoch  2, batch     1 | loss: 99.0638438CurrentTrain: epoch  2, batch     2 | loss: 84.4356238CurrentTrain: epoch  2, batch     3 | loss: 87.4038076CurrentTrain: epoch  2, batch     4 | loss: 132.2321045CurrentTrain: epoch  2, batch     5 | loss: 69.8150225CurrentTrain: epoch  2, batch     6 | loss: 124.1704088CurrentTrain: epoch  2, batch     7 | loss: 79.3967875CurrentTrain: epoch  2, batch     8 | loss: 104.7654684CurrentTrain: epoch  2, batch     9 | loss: 59.0152359CurrentTrain: epoch  2, batch    10 | loss: 95.7012854CurrentTrain: epoch  2, batch    11 | loss: 58.9891036CurrentTrain: epoch  2, batch    12 | loss: 63.0809684CurrentTrain: epoch  2, batch    13 | loss: 97.9215961CurrentTrain: epoch  2, batch    14 | loss: 102.2600268CurrentTrain: epoch  2, batch    15 | loss: 104.9185794CurrentTrain: epoch  2, batch    16 | loss: 79.4377537CurrentTrain: epoch  2, batch    17 | loss: 83.9179743CurrentTrain: epoch  2, batch    18 | loss: 81.9349246CurrentTrain: epoch  2, batch    19 | loss: 97.2233382CurrentTrain: epoch  2, batch    20 | loss: 71.9528151CurrentTrain: epoch  2, batch    21 | loss: 99.7708350CurrentTrain: epoch  2, batch    22 | loss: 71.6366731CurrentTrain: epoch  2, batch    23 | loss: 79.9627159CurrentTrain: epoch  2, batch    24 | loss: 79.0709180CurrentTrain: epoch  2, batch    25 | loss: 79.9626810CurrentTrain: epoch  2, batch    26 | loss: 86.0332111CurrentTrain: epoch  2, batch    27 | loss: 75.7026863CurrentTrain: epoch  2, batch    28 | loss: 100.4568439CurrentTrain: epoch  2, batch    29 | loss: 96.9992182CurrentTrain: epoch  2, batch    30 | loss: 83.8489971CurrentTrain: epoch  2, batch    31 | loss: 82.2767681CurrentTrain: epoch  2, batch    32 | loss: 73.4227208CurrentTrain: epoch  2, batch    33 | loss: 104.5652283CurrentTrain: epoch  2, batch    34 | loss: 101.1849430CurrentTrain: epoch  2, batch    35 | loss: 98.5490851CurrentTrain: epoch  2, batch    36 | loss: 83.9483167CurrentTrain: epoch  2, batch    37 | loss: 98.6331893CurrentTrain: epoch  2, batch    38 | loss: 67.9486347CurrentTrain: epoch  2, batch    39 | loss: 98.6996909CurrentTrain: epoch  2, batch    40 | loss: 58.1820308CurrentTrain: epoch  2, batch    41 | loss: 102.8044333CurrentTrain: epoch  2, batch    42 | loss: 98.7327999CurrentTrain: epoch  2, batch    43 | loss: 71.4770834CurrentTrain: epoch  2, batch    44 | loss: 81.0635739CurrentTrain: epoch  2, batch    45 | loss: 70.0670971CurrentTrain: epoch  2, batch    46 | loss: 84.7981586CurrentTrain: epoch  2, batch    47 | loss: 73.5035163CurrentTrain: epoch  2, batch    48 | loss: 126.1084639CurrentTrain: epoch  2, batch    49 | loss: 127.4663264CurrentTrain: epoch  2, batch    50 | loss: 67.5060232CurrentTrain: epoch  2, batch    51 | loss: 130.8383680CurrentTrain: epoch  2, batch    52 | loss: 69.5640228CurrentTrain: epoch  2, batch    53 | loss: 85.4473623CurrentTrain: epoch  2, batch    54 | loss: 103.4948572CurrentTrain: epoch  2, batch    55 | loss: 82.6725060CurrentTrain: epoch  2, batch    56 | loss: 80.2730919CurrentTrain: epoch  2, batch    57 | loss: 66.7748167CurrentTrain: epoch  2, batch    58 | loss: 72.4562298CurrentTrain: epoch  2, batch    59 | loss: 70.0636438CurrentTrain: epoch  2, batch    60 | loss: 82.2658999CurrentTrain: epoch  2, batch    61 | loss: 65.3474328CurrentTrain: epoch  2, batch    62 | loss: 78.1665791CurrentTrain: epoch  2, batch    63 | loss: 67.7257311CurrentTrain: epoch  2, batch    64 | loss: 65.9086938CurrentTrain: epoch  2, batch    65 | loss: 79.9708646CurrentTrain: epoch  2, batch    66 | loss: 129.5046297CurrentTrain: epoch  2, batch    67 | loss: 101.1464426CurrentTrain: epoch  2, batch    68 | loss: 67.6353309CurrentTrain: epoch  2, batch    69 | loss: 66.7809904CurrentTrain: epoch  2, batch    70 | loss: 85.3331650CurrentTrain: epoch  2, batch    71 | loss: 69.0467200CurrentTrain: epoch  2, batch    72 | loss: 178.1390541CurrentTrain: epoch  2, batch    73 | loss: 81.1528500CurrentTrain: epoch  2, batch    74 | loss: 78.4483629CurrentTrain: epoch  2, batch    75 | loss: 79.3128353CurrentTrain: epoch  2, batch    76 | loss: 97.0801668CurrentTrain: epoch  2, batch    77 | loss: 59.3168728CurrentTrain: epoch  2, batch    78 | loss: 85.3802871CurrentTrain: epoch  2, batch    79 | loss: 79.6429643CurrentTrain: epoch  2, batch    80 | loss: 66.7826497CurrentTrain: epoch  2, batch    81 | loss: 81.9114097CurrentTrain: epoch  2, batch    82 | loss: 102.7535192CurrentTrain: epoch  2, batch    83 | loss: 103.0068522CurrentTrain: epoch  2, batch    84 | loss: 72.8907347CurrentTrain: epoch  2, batch    85 | loss: 130.5286590CurrentTrain: epoch  2, batch    86 | loss: 58.4526930CurrentTrain: epoch  2, batch    87 | loss: 59.9287444CurrentTrain: epoch  2, batch    88 | loss: 61.2565988CurrentTrain: epoch  2, batch    89 | loss: 97.2557953CurrentTrain: epoch  2, batch    90 | loss: 79.1518099CurrentTrain: epoch  2, batch    91 | loss: 70.3820403CurrentTrain: epoch  2, batch    92 | loss: 69.6384412CurrentTrain: epoch  2, batch    93 | loss: 84.2225548CurrentTrain: epoch  2, batch    94 | loss: 82.2737874CurrentTrain: epoch  2, batch    95 | loss: 68.0313108CurrentTrain: epoch  3, batch     0 | loss: 80.7984893CurrentTrain: epoch  3, batch     1 | loss: 97.4264451CurrentTrain: epoch  3, batch     2 | loss: 82.1104602CurrentTrain: epoch  3, batch     3 | loss: 83.9958774CurrentTrain: epoch  3, batch     4 | loss: 77.2454678CurrentTrain: epoch  3, batch     5 | loss: 94.4806123CurrentTrain: epoch  3, batch     6 | loss: 77.0182506CurrentTrain: epoch  3, batch     7 | loss: 58.0093894CurrentTrain: epoch  3, batch     8 | loss: 70.6256115CurrentTrain: epoch  3, batch     9 | loss: 119.2406188CurrentTrain: epoch  3, batch    10 | loss: 70.0634365CurrentTrain: epoch  3, batch    11 | loss: 76.1018194CurrentTrain: epoch  3, batch    12 | loss: 97.9815407CurrentTrain: epoch  3, batch    13 | loss: 99.2434190CurrentTrain: epoch  3, batch    14 | loss: 80.1869998CurrentTrain: epoch  3, batch    15 | loss: 98.9052206CurrentTrain: epoch  3, batch    16 | loss: 93.4623172CurrentTrain: epoch  3, batch    17 | loss: 65.7159670CurrentTrain: epoch  3, batch    18 | loss: 67.8769472CurrentTrain: epoch  3, batch    19 | loss: 69.7606074CurrentTrain: epoch  3, batch    20 | loss: 99.6806305CurrentTrain: epoch  3, batch    21 | loss: 76.5355298CurrentTrain: epoch  3, batch    22 | loss: 101.6251158CurrentTrain: epoch  3, batch    23 | loss: 82.5644827CurrentTrain: epoch  3, batch    24 | loss: 68.7082353CurrentTrain: epoch  3, batch    25 | loss: 82.9114938CurrentTrain: epoch  3, batch    26 | loss: 98.0790262CurrentTrain: epoch  3, batch    27 | loss: 102.7308373CurrentTrain: epoch  3, batch    28 | loss: 100.0265457CurrentTrain: epoch  3, batch    29 | loss: 68.3107230CurrentTrain: epoch  3, batch    30 | loss: 84.9670969CurrentTrain: epoch  3, batch    31 | loss: 77.4509462CurrentTrain: epoch  3, batch    32 | loss: 78.5615438CurrentTrain: epoch  3, batch    33 | loss: 124.8929147CurrentTrain: epoch  3, batch    34 | loss: 79.9369349CurrentTrain: epoch  3, batch    35 | loss: 120.2832071CurrentTrain: epoch  3, batch    36 | loss: 57.6196342CurrentTrain: epoch  3, batch    37 | loss: 124.8563171CurrentTrain: epoch  3, batch    38 | loss: 63.2587625CurrentTrain: epoch  3, batch    39 | loss: 82.2011061CurrentTrain: epoch  3, batch    40 | loss: 77.5891939CurrentTrain: epoch  3, batch    41 | loss: 77.1748860CurrentTrain: epoch  3, batch    42 | loss: 84.0975736CurrentTrain: epoch  3, batch    43 | loss: 67.0376362CurrentTrain: epoch  3, batch    44 | loss: 61.8306482CurrentTrain: epoch  3, batch    45 | loss: 68.4113070CurrentTrain: epoch  3, batch    46 | loss: 97.1903398CurrentTrain: epoch  3, batch    47 | loss: 80.7754134CurrentTrain: epoch  3, batch    48 | loss: 124.8609451CurrentTrain: epoch  3, batch    49 | loss: 58.5581092CurrentTrain: epoch  3, batch    50 | loss: 68.5712305CurrentTrain: epoch  3, batch    51 | loss: 71.1315693CurrentTrain: epoch  3, batch    52 | loss: 127.7535441CurrentTrain: epoch  3, batch    53 | loss: 70.5160917CurrentTrain: epoch  3, batch    54 | loss: 98.5066038CurrentTrain: epoch  3, batch    55 | loss: 74.3671159CurrentTrain: epoch  3, batch    56 | loss: 67.9379412CurrentTrain: epoch  3, batch    57 | loss: 70.4851583CurrentTrain: epoch  3, batch    58 | loss: 79.1852376CurrentTrain: epoch  3, batch    59 | loss: 119.3706589CurrentTrain: epoch  3, batch    60 | loss: 81.9891856CurrentTrain: epoch  3, batch    61 | loss: 66.5726036CurrentTrain: epoch  3, batch    62 | loss: 76.8475112CurrentTrain: epoch  3, batch    63 | loss: 98.5033198CurrentTrain: epoch  3, batch    64 | loss: 77.2747119CurrentTrain: epoch  3, batch    65 | loss: 98.8394057CurrentTrain: epoch  3, batch    66 | loss: 99.3803943CurrentTrain: epoch  3, batch    67 | loss: 66.6120053CurrentTrain: epoch  3, batch    68 | loss: 79.8319030CurrentTrain: epoch  3, batch    69 | loss: 64.8611363CurrentTrain: epoch  3, batch    70 | loss: 78.8034947CurrentTrain: epoch  3, batch    71 | loss: 55.8885608CurrentTrain: epoch  3, batch    72 | loss: 70.5935956CurrentTrain: epoch  3, batch    73 | loss: 55.1996476CurrentTrain: epoch  3, batch    74 | loss: 82.1680371CurrentTrain: epoch  3, batch    75 | loss: 101.2475581CurrentTrain: epoch  3, batch    76 | loss: 92.1917495CurrentTrain: epoch  3, batch    77 | loss: 79.7293181CurrentTrain: epoch  3, batch    78 | loss: 121.8011796CurrentTrain: epoch  3, batch    79 | loss: 75.7048259CurrentTrain: epoch  3, batch    80 | loss: 96.6821030CurrentTrain: epoch  3, batch    81 | loss: 59.8979595CurrentTrain: epoch  3, batch    82 | loss: 104.9727393CurrentTrain: epoch  3, batch    83 | loss: 76.0566182CurrentTrain: epoch  3, batch    84 | loss: 67.8576906CurrentTrain: epoch  3, batch    85 | loss: 101.5502492CurrentTrain: epoch  3, batch    86 | loss: 67.9787660CurrentTrain: epoch  3, batch    87 | loss: 79.5946142CurrentTrain: epoch  3, batch    88 | loss: 65.0308051CurrentTrain: epoch  3, batch    89 | loss: 65.9955158CurrentTrain: epoch  3, batch    90 | loss: 90.9878490CurrentTrain: epoch  3, batch    91 | loss: 95.1178532CurrentTrain: epoch  3, batch    92 | loss: 70.8200381CurrentTrain: epoch  3, batch    93 | loss: 79.2456723CurrentTrain: epoch  3, batch    94 | loss: 100.0487272CurrentTrain: epoch  3, batch    95 | loss: 50.1848276CurrentTrain: epoch  4, batch     0 | loss: 76.0740450CurrentTrain: epoch  4, batch     1 | loss: 97.7785451CurrentTrain: epoch  4, batch     2 | loss: 93.4109507CurrentTrain: epoch  4, batch     3 | loss: 70.6196869CurrentTrain: epoch  4, batch     4 | loss: 123.0806851CurrentTrain: epoch  4, batch     5 | loss: 90.3358698CurrentTrain: epoch  4, batch     6 | loss: 67.7886045CurrentTrain: epoch  4, batch     7 | loss: 63.1715507CurrentTrain: epoch  4, batch     8 | loss: 96.8745494CurrentTrain: epoch  4, batch     9 | loss: 122.4757701CurrentTrain: epoch  4, batch    10 | loss: 94.9306537CurrentTrain: epoch  4, batch    11 | loss: 65.4107038CurrentTrain: epoch  4, batch    12 | loss: 77.3312478CurrentTrain: epoch  4, batch    13 | loss: 67.0134051CurrentTrain: epoch  4, batch    14 | loss: 76.6479467CurrentTrain: epoch  4, batch    15 | loss: 97.4998354CurrentTrain: epoch  4, batch    16 | loss: 94.4715665CurrentTrain: epoch  4, batch    17 | loss: 119.0401134CurrentTrain: epoch  4, batch    18 | loss: 74.3380351CurrentTrain: epoch  4, batch    19 | loss: 77.3064455CurrentTrain: epoch  4, batch    20 | loss: 71.5330410CurrentTrain: epoch  4, batch    21 | loss: 64.9819648CurrentTrain: epoch  4, batch    22 | loss: 100.0530088CurrentTrain: epoch  4, batch    23 | loss: 69.8986594CurrentTrain: epoch  4, batch    24 | loss: 85.9956639CurrentTrain: epoch  4, batch    25 | loss: 100.3280279CurrentTrain: epoch  4, batch    26 | loss: 79.0209416CurrentTrain: epoch  4, batch    27 | loss: 58.6514056CurrentTrain: epoch  4, batch    28 | loss: 89.6106005CurrentTrain: epoch  4, batch    29 | loss: 79.6903257CurrentTrain: epoch  4, batch    30 | loss: 81.4611552CurrentTrain: epoch  4, batch    31 | loss: 66.9469317CurrentTrain: epoch  4, batch    32 | loss: 90.5430380CurrentTrain: epoch  4, batch    33 | loss: 54.9457463CurrentTrain: epoch  4, batch    34 | loss: 96.0795840CurrentTrain: epoch  4, batch    35 | loss: 77.6883012CurrentTrain: epoch  4, batch    36 | loss: 67.9526296CurrentTrain: epoch  4, batch    37 | loss: 59.9591823CurrentTrain: epoch  4, batch    38 | loss: 79.8438380CurrentTrain: epoch  4, batch    39 | loss: 68.9463949CurrentTrain: epoch  4, batch    40 | loss: 77.2180683CurrentTrain: epoch  4, batch    41 | loss: 65.5747097CurrentTrain: epoch  4, batch    42 | loss: 81.5152682CurrentTrain: epoch  4, batch    43 | loss: 57.6916661CurrentTrain: epoch  4, batch    44 | loss: 64.0380267CurrentTrain: epoch  4, batch    45 | loss: 80.4879551CurrentTrain: epoch  4, batch    46 | loss: 67.4012358CurrentTrain: epoch  4, batch    47 | loss: 93.3503088CurrentTrain: epoch  4, batch    48 | loss: 71.5245862CurrentTrain: epoch  4, batch    49 | loss: 77.9704380CurrentTrain: epoch  4, batch    50 | loss: 75.3048756CurrentTrain: epoch  4, batch    51 | loss: 67.0882561CurrentTrain: epoch  4, batch    52 | loss: 75.9444836CurrentTrain: epoch  4, batch    53 | loss: 119.4820917CurrentTrain: epoch  4, batch    54 | loss: 77.4148719CurrentTrain: epoch  4, batch    55 | loss: 98.1192955CurrentTrain: epoch  4, batch    56 | loss: 124.4366128CurrentTrain: epoch  4, batch    57 | loss: 68.5229553CurrentTrain: epoch  4, batch    58 | loss: 122.1105694CurrentTrain: epoch  4, batch    59 | loss: 96.0394455CurrentTrain: epoch  4, batch    60 | loss: 77.5287867CurrentTrain: epoch  4, batch    61 | loss: 55.9121848CurrentTrain: epoch  4, batch    62 | loss: 54.4135869CurrentTrain: epoch  4, batch    63 | loss: 63.8793360CurrentTrain: epoch  4, batch    64 | loss: 79.3148456CurrentTrain: epoch  4, batch    65 | loss: 117.8724967CurrentTrain: epoch  4, batch    66 | loss: 68.3176618CurrentTrain: epoch  4, batch    67 | loss: 67.3651045CurrentTrain: epoch  4, batch    68 | loss: 94.6584530CurrentTrain: epoch  4, batch    69 | loss: 67.0844494CurrentTrain: epoch  4, batch    70 | loss: 88.6067254CurrentTrain: epoch  4, batch    71 | loss: 98.6802687CurrentTrain: epoch  4, batch    72 | loss: 62.8765101CurrentTrain: epoch  4, batch    73 | loss: 69.8798035CurrentTrain: epoch  4, batch    74 | loss: 122.6673319CurrentTrain: epoch  4, batch    75 | loss: 78.2176058CurrentTrain: epoch  4, batch    76 | loss: 124.5770525CurrentTrain: epoch  4, batch    77 | loss: 94.6458123CurrentTrain: epoch  4, batch    78 | loss: 65.9084106CurrentTrain: epoch  4, batch    79 | loss: 91.9619715CurrentTrain: epoch  4, batch    80 | loss: 68.0823192CurrentTrain: epoch  4, batch    81 | loss: 122.6668338CurrentTrain: epoch  4, batch    82 | loss: 74.9405704CurrentTrain: epoch  4, batch    83 | loss: 67.2857270CurrentTrain: epoch  4, batch    84 | loss: 71.6538585CurrentTrain: epoch  4, batch    85 | loss: 80.3830722CurrentTrain: epoch  4, batch    86 | loss: 101.9594045CurrentTrain: epoch  4, batch    87 | loss: 116.4415239CurrentTrain: epoch  4, batch    88 | loss: 98.4428171CurrentTrain: epoch  4, batch    89 | loss: 66.3480367CurrentTrain: epoch  4, batch    90 | loss: 113.4592300CurrentTrain: epoch  4, batch    91 | loss: 89.8632036CurrentTrain: epoch  4, batch    92 | loss: 83.2527179CurrentTrain: epoch  4, batch    93 | loss: 75.2843111CurrentTrain: epoch  4, batch    94 | loss: 83.9476170CurrentTrain: epoch  4, batch    95 | loss: 55.3593726CurrentTrain: epoch  5, batch     0 | loss: 75.8710344CurrentTrain: epoch  5, batch     1 | loss: 78.4321113CurrentTrain: epoch  5, batch     2 | loss: 92.3553505CurrentTrain: epoch  5, batch     3 | loss: 80.2198267CurrentTrain: epoch  5, batch     4 | loss: 74.2353980CurrentTrain: epoch  5, batch     5 | loss: 64.2238326CurrentTrain: epoch  5, batch     6 | loss: 119.9434916CurrentTrain: epoch  5, batch     7 | loss: 74.8051584CurrentTrain: epoch  5, batch     8 | loss: 75.4513201CurrentTrain: epoch  5, batch     9 | loss: 71.7097799CurrentTrain: epoch  5, batch    10 | loss: 67.0699651CurrentTrain: epoch  5, batch    11 | loss: 94.9401335CurrentTrain: epoch  5, batch    12 | loss: 121.0641448CurrentTrain: epoch  5, batch    13 | loss: 60.8552237CurrentTrain: epoch  5, batch    14 | loss: 92.9502276CurrentTrain: epoch  5, batch    15 | loss: 60.3848187CurrentTrain: epoch  5, batch    16 | loss: 78.6444751CurrentTrain: epoch  5, batch    17 | loss: 63.4442269CurrentTrain: epoch  5, batch    18 | loss: 79.7079643CurrentTrain: epoch  5, batch    19 | loss: 93.6023683CurrentTrain: epoch  5, batch    20 | loss: 76.5012472CurrentTrain: epoch  5, batch    21 | loss: 61.4381788CurrentTrain: epoch  5, batch    22 | loss: 77.4745145CurrentTrain: epoch  5, batch    23 | loss: 89.8318324CurrentTrain: epoch  5, batch    24 | loss: 54.3187590CurrentTrain: epoch  5, batch    25 | loss: 76.9610688CurrentTrain: epoch  5, batch    26 | loss: 95.5350515CurrentTrain: epoch  5, batch    27 | loss: 91.6784633CurrentTrain: epoch  5, batch    28 | loss: 119.5515069CurrentTrain: epoch  5, batch    29 | loss: 255.0651527CurrentTrain: epoch  5, batch    30 | loss: 67.3722254CurrentTrain: epoch  5, batch    31 | loss: 156.6427268CurrentTrain: epoch  5, batch    32 | loss: 62.6343779CurrentTrain: epoch  5, batch    33 | loss: 70.6076314CurrentTrain: epoch  5, batch    34 | loss: 75.8778089CurrentTrain: epoch  5, batch    35 | loss: 58.1056057CurrentTrain: epoch  5, batch    36 | loss: 120.5779127CurrentTrain: epoch  5, batch    37 | loss: 76.7858900CurrentTrain: epoch  5, batch    38 | loss: 88.1449335CurrentTrain: epoch  5, batch    39 | loss: 93.8164298CurrentTrain: epoch  5, batch    40 | loss: 74.4205464CurrentTrain: epoch  5, batch    41 | loss: 95.5158919CurrentTrain: epoch  5, batch    42 | loss: 121.6134543CurrentTrain: epoch  5, batch    43 | loss: 66.0065734CurrentTrain: epoch  5, batch    44 | loss: 62.5928583CurrentTrain: epoch  5, batch    45 | loss: 77.6347062CurrentTrain: epoch  5, batch    46 | loss: 79.4711461CurrentTrain: epoch  5, batch    47 | loss: 64.2074225CurrentTrain: epoch  5, batch    48 | loss: 60.2970149CurrentTrain: epoch  5, batch    49 | loss: 68.0525059CurrentTrain: epoch  5, batch    50 | loss: 57.3522129CurrentTrain: epoch  5, batch    51 | loss: 78.1248743CurrentTrain: epoch  5, batch    52 | loss: 77.8130050CurrentTrain: epoch  5, batch    53 | loss: 70.4412603CurrentTrain: epoch  5, batch    54 | loss: 83.0940590CurrentTrain: epoch  5, batch    55 | loss: 95.4262731CurrentTrain: epoch  5, batch    56 | loss: 120.1553313CurrentTrain: epoch  5, batch    57 | loss: 79.4967071CurrentTrain: epoch  5, batch    58 | loss: 78.9170529CurrentTrain: epoch  5, batch    59 | loss: 120.0197697CurrentTrain: epoch  5, batch    60 | loss: 95.0098059CurrentTrain: epoch  5, batch    61 | loss: 74.4211942CurrentTrain: epoch  5, batch    62 | loss: 92.9918041CurrentTrain: epoch  5, batch    63 | loss: 80.1275190CurrentTrain: epoch  5, batch    64 | loss: 52.5420713CurrentTrain: epoch  5, batch    65 | loss: 72.1479538CurrentTrain: epoch  5, batch    66 | loss: 74.1775986CurrentTrain: epoch  5, batch    67 | loss: 75.9921039CurrentTrain: epoch  5, batch    68 | loss: 59.2421444CurrentTrain: epoch  5, batch    69 | loss: 74.4261475CurrentTrain: epoch  5, batch    70 | loss: 94.6255536CurrentTrain: epoch  5, batch    71 | loss: 94.3945754CurrentTrain: epoch  5, batch    72 | loss: 93.1693142CurrentTrain: epoch  5, batch    73 | loss: 78.7901300CurrentTrain: epoch  5, batch    74 | loss: 60.8703091CurrentTrain: epoch  5, batch    75 | loss: 63.8644561CurrentTrain: epoch  5, batch    76 | loss: 120.3516766CurrentTrain: epoch  5, batch    77 | loss: 79.2915524CurrentTrain: epoch  5, batch    78 | loss: 72.6975293CurrentTrain: epoch  5, batch    79 | loss: 77.6493520CurrentTrain: epoch  5, batch    80 | loss: 92.0374806CurrentTrain: epoch  5, batch    81 | loss: 92.0193404CurrentTrain: epoch  5, batch    82 | loss: 76.5803510CurrentTrain: epoch  5, batch    83 | loss: 96.3298360CurrentTrain: epoch  5, batch    84 | loss: 90.5302685CurrentTrain: epoch  5, batch    85 | loss: 80.9732108CurrentTrain: epoch  5, batch    86 | loss: 97.9518613CurrentTrain: epoch  5, batch    87 | loss: 60.9745507CurrentTrain: epoch  5, batch    88 | loss: 98.2501080CurrentTrain: epoch  5, batch    89 | loss: 73.8670630CurrentTrain: epoch  5, batch    90 | loss: 75.9315126CurrentTrain: epoch  5, batch    91 | loss: 80.8125380CurrentTrain: epoch  5, batch    92 | loss: 78.0427214CurrentTrain: epoch  5, batch    93 | loss: 64.7989216CurrentTrain: epoch  5, batch    94 | loss: 116.4818418CurrentTrain: epoch  5, batch    95 | loss: 58.1487479CurrentTrain: epoch  6, batch     0 | loss: 53.6192186CurrentTrain: epoch  6, batch     1 | loss: 73.9951714CurrentTrain: epoch  6, batch     2 | loss: 72.4551441CurrentTrain: epoch  6, batch     3 | loss: 92.5831428CurrentTrain: epoch  6, batch     4 | loss: 117.6291150CurrentTrain: epoch  6, batch     5 | loss: 161.2297270CurrentTrain: epoch  6, batch     6 | loss: 77.5180655CurrentTrain: epoch  6, batch     7 | loss: 77.2461640CurrentTrain: epoch  6, batch     8 | loss: 92.0054264CurrentTrain: epoch  6, batch     9 | loss: 77.6036988CurrentTrain: epoch  6, batch    10 | loss: 116.2876887CurrentTrain: epoch  6, batch    11 | loss: 75.6334732CurrentTrain: epoch  6, batch    12 | loss: 94.5892322CurrentTrain: epoch  6, batch    13 | loss: 91.4851350CurrentTrain: epoch  6, batch    14 | loss: 55.9926653CurrentTrain: epoch  6, batch    15 | loss: 72.6019079CurrentTrain: epoch  6, batch    16 | loss: 62.5912954CurrentTrain: epoch  6, batch    17 | loss: 77.1642741CurrentTrain: epoch  6, batch    18 | loss: 62.0613722CurrentTrain: epoch  6, batch    19 | loss: 62.2953655CurrentTrain: epoch  6, batch    20 | loss: 63.1768048CurrentTrain: epoch  6, batch    21 | loss: 76.8127661CurrentTrain: epoch  6, batch    22 | loss: 76.6304920CurrentTrain: epoch  6, batch    23 | loss: 90.0265475CurrentTrain: epoch  6, batch    24 | loss: 93.7004414CurrentTrain: epoch  6, batch    25 | loss: 63.0000082CurrentTrain: epoch  6, batch    26 | loss: 61.5477201CurrentTrain: epoch  6, batch    27 | loss: 65.7777496CurrentTrain: epoch  6, batch    28 | loss: 65.8891046CurrentTrain: epoch  6, batch    29 | loss: 86.0777449CurrentTrain: epoch  6, batch    30 | loss: 92.2191847CurrentTrain: epoch  6, batch    31 | loss: 54.3833825CurrentTrain: epoch  6, batch    32 | loss: 90.0657200CurrentTrain: epoch  6, batch    33 | loss: 77.4600148CurrentTrain: epoch  6, batch    34 | loss: 90.9363340CurrentTrain: epoch  6, batch    35 | loss: 60.7082691CurrentTrain: epoch  6, batch    36 | loss: 62.3153198CurrentTrain: epoch  6, batch    37 | loss: 68.0025560CurrentTrain: epoch  6, batch    38 | loss: 93.5838512CurrentTrain: epoch  6, batch    39 | loss: 71.7935041CurrentTrain: epoch  6, batch    40 | loss: 63.1662316CurrentTrain: epoch  6, batch    41 | loss: 90.4947248CurrentTrain: epoch  6, batch    42 | loss: 72.1735814CurrentTrain: epoch  6, batch    43 | loss: 53.4972151CurrentTrain: epoch  6, batch    44 | loss: 51.5080559CurrentTrain: epoch  6, batch    45 | loss: 75.6099302CurrentTrain: epoch  6, batch    46 | loss: 71.9933308CurrentTrain: epoch  6, batch    47 | loss: 122.3998864CurrentTrain: epoch  6, batch    48 | loss: 67.7472455CurrentTrain: epoch  6, batch    49 | loss: 60.3977053CurrentTrain: epoch  6, batch    50 | loss: 92.1436559CurrentTrain: epoch  6, batch    51 | loss: 92.5550611CurrentTrain: epoch  6, batch    52 | loss: 114.1730291CurrentTrain: epoch  6, batch    53 | loss: 93.3539387CurrentTrain: epoch  6, batch    54 | loss: 76.0641385CurrentTrain: epoch  6, batch    55 | loss: 76.4593856CurrentTrain: epoch  6, batch    56 | loss: 97.1395166CurrentTrain: epoch  6, batch    57 | loss: 59.8181275CurrentTrain: epoch  6, batch    58 | loss: 92.5473183CurrentTrain: epoch  6, batch    59 | loss: 54.0077285CurrentTrain: epoch  6, batch    60 | loss: 73.7012354CurrentTrain: epoch  6, batch    61 | loss: 76.5604695CurrentTrain: epoch  6, batch    62 | loss: 96.6311164CurrentTrain: epoch  6, batch    63 | loss: 76.1764505CurrentTrain: epoch  6, batch    64 | loss: 74.1589671CurrentTrain: epoch  6, batch    65 | loss: 94.6618368CurrentTrain: epoch  6, batch    66 | loss: 74.9996937CurrentTrain: epoch  6, batch    67 | loss: 74.9039975CurrentTrain: epoch  6, batch    68 | loss: 95.9684234CurrentTrain: epoch  6, batch    69 | loss: 73.7358644CurrentTrain: epoch  6, batch    70 | loss: 93.0090212CurrentTrain: epoch  6, batch    71 | loss: 51.2888007CurrentTrain: epoch  6, batch    72 | loss: 63.7869409CurrentTrain: epoch  6, batch    73 | loss: 59.6593931CurrentTrain: epoch  6, batch    74 | loss: 95.3545565CurrentTrain: epoch  6, batch    75 | loss: 93.0464912CurrentTrain: epoch  6, batch    76 | loss: 51.8581886CurrentTrain: epoch  6, batch    77 | loss: 54.2773333CurrentTrain: epoch  6, batch    78 | loss: 91.6490928CurrentTrain: epoch  6, batch    79 | loss: 89.4566867CurrentTrain: epoch  6, batch    80 | loss: 75.9280478CurrentTrain: epoch  6, batch    81 | loss: 54.5547568CurrentTrain: epoch  6, batch    82 | loss: 69.3339713CurrentTrain: epoch  6, batch    83 | loss: 88.1388520CurrentTrain: epoch  6, batch    84 | loss: 75.0502052CurrentTrain: epoch  6, batch    85 | loss: 55.1014779CurrentTrain: epoch  6, batch    86 | loss: 75.1859670CurrentTrain: epoch  6, batch    87 | loss: 92.1976049CurrentTrain: epoch  6, batch    88 | loss: 82.5625013CurrentTrain: epoch  6, batch    89 | loss: 95.8622651CurrentTrain: epoch  6, batch    90 | loss: 90.4402607CurrentTrain: epoch  6, batch    91 | loss: 56.3539921CurrentTrain: epoch  6, batch    92 | loss: 75.4531529CurrentTrain: epoch  6, batch    93 | loss: 75.0206032CurrentTrain: epoch  6, batch    94 | loss: 63.5265429CurrentTrain: epoch  6, batch    95 | loss: 93.7175068CurrentTrain: epoch  7, batch     0 | loss: 73.2885233CurrentTrain: epoch  7, batch     1 | loss: 91.8321600CurrentTrain: epoch  7, batch     2 | loss: 76.8256677CurrentTrain: epoch  7, batch     3 | loss: 73.3590868CurrentTrain: epoch  7, batch     4 | loss: 73.0652372CurrentTrain: epoch  7, batch     5 | loss: 89.3813977CurrentTrain: epoch  7, batch     6 | loss: 76.0102320CurrentTrain: epoch  7, batch     7 | loss: 78.1070167CurrentTrain: epoch  7, batch     8 | loss: 62.9894185CurrentTrain: epoch  7, batch     9 | loss: 59.6284150CurrentTrain: epoch  7, batch    10 | loss: 61.2403116CurrentTrain: epoch  7, batch    11 | loss: 62.7852017CurrentTrain: epoch  7, batch    12 | loss: 89.2557274CurrentTrain: epoch  7, batch    13 | loss: 76.3094541CurrentTrain: epoch  7, batch    14 | loss: 73.3698523CurrentTrain: epoch  7, batch    15 | loss: 61.9813782CurrentTrain: epoch  7, batch    16 | loss: 96.2807233CurrentTrain: epoch  7, batch    17 | loss: 70.0478932CurrentTrain: epoch  7, batch    18 | loss: 63.2330902CurrentTrain: epoch  7, batch    19 | loss: 59.9875411CurrentTrain: epoch  7, batch    20 | loss: 69.5984121CurrentTrain: epoch  7, batch    21 | loss: 73.0804169CurrentTrain: epoch  7, batch    22 | loss: 92.8382059CurrentTrain: epoch  7, batch    23 | loss: 89.8100225CurrentTrain: epoch  7, batch    24 | loss: 62.8855034CurrentTrain: epoch  7, batch    25 | loss: 51.8726351CurrentTrain: epoch  7, batch    26 | loss: 73.9262774CurrentTrain: epoch  7, batch    27 | loss: 90.3520096CurrentTrain: epoch  7, batch    28 | loss: 62.4918918CurrentTrain: epoch  7, batch    29 | loss: 93.8663890CurrentTrain: epoch  7, batch    30 | loss: 75.7863142CurrentTrain: epoch  7, batch    31 | loss: 77.0078167CurrentTrain: epoch  7, batch    32 | loss: 113.9386864CurrentTrain: epoch  7, batch    33 | loss: 75.9317997CurrentTrain: epoch  7, batch    34 | loss: 81.0851556CurrentTrain: epoch  7, batch    35 | loss: 90.2084169CurrentTrain: epoch  7, batch    36 | loss: 73.4695373CurrentTrain: epoch  7, batch    37 | loss: 77.1627325CurrentTrain: epoch  7, batch    38 | loss: 75.7895835CurrentTrain: epoch  7, batch    39 | loss: 94.9864811CurrentTrain: epoch  7, batch    40 | loss: 73.1332929CurrentTrain: epoch  7, batch    41 | loss: 59.1790437CurrentTrain: epoch  7, batch    42 | loss: 88.4867103CurrentTrain: epoch  7, batch    43 | loss: 60.5405490CurrentTrain: epoch  7, batch    44 | loss: 72.0427963CurrentTrain: epoch  7, batch    45 | loss: 89.8842252CurrentTrain: epoch  7, batch    46 | loss: 65.9910537CurrentTrain: epoch  7, batch    47 | loss: 71.3195629CurrentTrain: epoch  7, batch    48 | loss: 114.7192661CurrentTrain: epoch  7, batch    49 | loss: 52.5059343CurrentTrain: epoch  7, batch    50 | loss: 84.8444708CurrentTrain: epoch  7, batch    51 | loss: 71.8029170CurrentTrain: epoch  7, batch    52 | loss: 64.4936564CurrentTrain: epoch  7, batch    53 | loss: 90.1472733CurrentTrain: epoch  7, batch    54 | loss: 61.5733977CurrentTrain: epoch  7, batch    55 | loss: 92.4924715CurrentTrain: epoch  7, batch    56 | loss: 86.8758995CurrentTrain: epoch  7, batch    57 | loss: 76.2683015CurrentTrain: epoch  7, batch    58 | loss: 74.9082933CurrentTrain: epoch  7, batch    59 | loss: 116.9176007CurrentTrain: epoch  7, batch    60 | loss: 60.7669123CurrentTrain: epoch  7, batch    61 | loss: 62.5600201CurrentTrain: epoch  7, batch    62 | loss: 112.9638335CurrentTrain: epoch  7, batch    63 | loss: 74.8098947CurrentTrain: epoch  7, batch    64 | loss: 58.1473566CurrentTrain: epoch  7, batch    65 | loss: 69.2077826CurrentTrain: epoch  7, batch    66 | loss: 74.5434014CurrentTrain: epoch  7, batch    67 | loss: 73.0975681CurrentTrain: epoch  7, batch    68 | loss: 57.9535273CurrentTrain: epoch  7, batch    69 | loss: 74.6657537CurrentTrain: epoch  7, batch    70 | loss: 64.0428489CurrentTrain: epoch  7, batch    71 | loss: 90.2769682CurrentTrain: epoch  7, batch    72 | loss: 63.0917014CurrentTrain: epoch  7, batch    73 | loss: 72.0345224CurrentTrain: epoch  7, batch    74 | loss: 52.7838941CurrentTrain: epoch  7, batch    75 | loss: 60.5492775CurrentTrain: epoch  7, batch    76 | loss: 64.8102050CurrentTrain: epoch  7, batch    77 | loss: 92.8740843CurrentTrain: epoch  7, batch    78 | loss: 79.2333445CurrentTrain: epoch  7, batch    79 | loss: 115.9619282CurrentTrain: epoch  7, batch    80 | loss: 64.3173829CurrentTrain: epoch  7, batch    81 | loss: 62.3896442CurrentTrain: epoch  7, batch    82 | loss: 72.1171234CurrentTrain: epoch  7, batch    83 | loss: 60.0168936CurrentTrain: epoch  7, batch    84 | loss: 75.4365151CurrentTrain: epoch  7, batch    85 | loss: 77.5773562CurrentTrain: epoch  7, batch    86 | loss: 60.2288817CurrentTrain: epoch  7, batch    87 | loss: 92.8637574CurrentTrain: epoch  7, batch    88 | loss: 87.3884617CurrentTrain: epoch  7, batch    89 | loss: 119.2052607CurrentTrain: epoch  7, batch    90 | loss: 74.6284356CurrentTrain: epoch  7, batch    91 | loss: 74.0109404CurrentTrain: epoch  7, batch    92 | loss: 95.7034700CurrentTrain: epoch  7, batch    93 | loss: 53.2015258CurrentTrain: epoch  7, batch    94 | loss: 53.4208978CurrentTrain: epoch  7, batch    95 | loss: 77.9297994CurrentTrain: epoch  8, batch     0 | loss: 63.6627665CurrentTrain: epoch  8, batch     1 | loss: 59.0033594CurrentTrain: epoch  8, batch     2 | loss: 85.6646425CurrentTrain: epoch  8, batch     3 | loss: 118.0849681CurrentTrain: epoch  8, batch     4 | loss: 74.1210865CurrentTrain: epoch  8, batch     5 | loss: 64.0412077CurrentTrain: epoch  8, batch     6 | loss: 75.6267853CurrentTrain: epoch  8, batch     7 | loss: 70.0123534CurrentTrain: epoch  8, batch     8 | loss: 61.3199602CurrentTrain: epoch  8, batch     9 | loss: 87.8006321CurrentTrain: epoch  8, batch    10 | loss: 70.9143490CurrentTrain: epoch  8, batch    11 | loss: 72.6873019CurrentTrain: epoch  8, batch    12 | loss: 70.2594989CurrentTrain: epoch  8, batch    13 | loss: 84.3979207CurrentTrain: epoch  8, batch    14 | loss: 72.4657976CurrentTrain: epoch  8, batch    15 | loss: 71.3940800CurrentTrain: epoch  8, batch    16 | loss: 86.1295788CurrentTrain: epoch  8, batch    17 | loss: 73.4291642CurrentTrain: epoch  8, batch    18 | loss: 90.9663679CurrentTrain: epoch  8, batch    19 | loss: 59.6170635CurrentTrain: epoch  8, batch    20 | loss: 70.7060495CurrentTrain: epoch  8, batch    21 | loss: 69.9521224CurrentTrain: epoch  8, batch    22 | loss: 84.8407611CurrentTrain: epoch  8, batch    23 | loss: 61.8020813CurrentTrain: epoch  8, batch    24 | loss: 59.3050524CurrentTrain: epoch  8, batch    25 | loss: 86.2121197CurrentTrain: epoch  8, batch    26 | loss: 58.8529295CurrentTrain: epoch  8, batch    27 | loss: 72.2418437CurrentTrain: epoch  8, batch    28 | loss: 61.4119485CurrentTrain: epoch  8, batch    29 | loss: 64.1659713CurrentTrain: epoch  8, batch    30 | loss: 93.8563668CurrentTrain: epoch  8, batch    31 | loss: 49.6178880CurrentTrain: epoch  8, batch    32 | loss: 156.1939973CurrentTrain: epoch  8, batch    33 | loss: 89.3051297CurrentTrain: epoch  8, batch    34 | loss: 87.8024334CurrentTrain: epoch  8, batch    35 | loss: 60.7601160CurrentTrain: epoch  8, batch    36 | loss: 88.9246243CurrentTrain: epoch  8, batch    37 | loss: 62.6996911CurrentTrain: epoch  8, batch    38 | loss: 72.5850708CurrentTrain: epoch  8, batch    39 | loss: 72.8862653CurrentTrain: epoch  8, batch    40 | loss: 74.5964852CurrentTrain: epoch  8, batch    41 | loss: 92.2869517CurrentTrain: epoch  8, batch    42 | loss: 73.7180259CurrentTrain: epoch  8, batch    43 | loss: 95.4576346CurrentTrain: epoch  8, batch    44 | loss: 112.5783855CurrentTrain: epoch  8, batch    45 | loss: 76.2800691CurrentTrain: epoch  8, batch    46 | loss: 61.9402717CurrentTrain: epoch  8, batch    47 | loss: 75.7681764CurrentTrain: epoch  8, batch    48 | loss: 59.2978750CurrentTrain: epoch  8, batch    49 | loss: 68.6480619CurrentTrain: epoch  8, batch    50 | loss: 87.0742537CurrentTrain: epoch  8, batch    51 | loss: 57.2979170CurrentTrain: epoch  8, batch    52 | loss: 71.4311650CurrentTrain: epoch  8, batch    53 | loss: 78.0594589CurrentTrain: epoch  8, batch    54 | loss: 89.9318366CurrentTrain: epoch  8, batch    55 | loss: 86.8068325CurrentTrain: epoch  8, batch    56 | loss: 63.2977102CurrentTrain: epoch  8, batch    57 | loss: 60.5490316CurrentTrain: epoch  8, batch    58 | loss: 71.8766260CurrentTrain: epoch  8, batch    59 | loss: 51.7176742CurrentTrain: epoch  8, batch    60 | loss: 112.5210629CurrentTrain: epoch  8, batch    61 | loss: 87.4044343CurrentTrain: epoch  8, batch    62 | loss: 52.0287329CurrentTrain: epoch  8, batch    63 | loss: 72.8258581CurrentTrain: epoch  8, batch    64 | loss: 73.0652042CurrentTrain: epoch  8, batch    65 | loss: 74.1105403CurrentTrain: epoch  8, batch    66 | loss: 68.1666432CurrentTrain: epoch  8, batch    67 | loss: 85.0907051CurrentTrain: epoch  8, batch    68 | loss: 89.7075890CurrentTrain: epoch  8, batch    69 | loss: 75.9571344CurrentTrain: epoch  8, batch    70 | loss: 86.8542799CurrentTrain: epoch  8, batch    71 | loss: 113.1824294CurrentTrain: epoch  8, batch    72 | loss: 68.6005257CurrentTrain: epoch  8, batch    73 | loss: 73.0737896CurrentTrain: epoch  8, batch    74 | loss: 89.6431118CurrentTrain: epoch  8, batch    75 | loss: 71.4700080CurrentTrain: epoch  8, batch    76 | loss: 51.1196713CurrentTrain: epoch  8, batch    77 | loss: 113.2540871CurrentTrain: epoch  8, batch    78 | loss: 61.5637261CurrentTrain: epoch  8, batch    79 | loss: 65.0817069CurrentTrain: epoch  8, batch    80 | loss: 85.4136665CurrentTrain: epoch  8, batch    81 | loss: 71.8262393CurrentTrain: epoch  8, batch    82 | loss: 60.4811952CurrentTrain: epoch  8, batch    83 | loss: 59.9913625CurrentTrain: epoch  8, batch    84 | loss: 117.0207178CurrentTrain: epoch  8, batch    85 | loss: 109.6706595CurrentTrain: epoch  8, batch    86 | loss: 90.1675369CurrentTrain: epoch  8, batch    87 | loss: 72.9710279CurrentTrain: epoch  8, batch    88 | loss: 47.5320040CurrentTrain: epoch  8, batch    89 | loss: 70.2921695CurrentTrain: epoch  8, batch    90 | loss: 69.7702139CurrentTrain: epoch  8, batch    91 | loss: 76.5656343CurrentTrain: epoch  8, batch    92 | loss: 73.7206359CurrentTrain: epoch  8, batch    93 | loss: 70.9678539CurrentTrain: epoch  8, batch    94 | loss: 93.6567646CurrentTrain: epoch  8, batch    95 | loss: 92.8017733CurrentTrain: epoch  9, batch     0 | loss: 59.6007739CurrentTrain: epoch  9, batch     1 | loss: 108.0526711CurrentTrain: epoch  9, batch     2 | loss: 69.0928026CurrentTrain: epoch  9, batch     3 | loss: 60.4650453CurrentTrain: epoch  9, batch     4 | loss: 71.3687295CurrentTrain: epoch  9, batch     5 | loss: 49.1702676CurrentTrain: epoch  9, batch     6 | loss: 92.8460322CurrentTrain: epoch  9, batch     7 | loss: 68.4276420CurrentTrain: epoch  9, batch     8 | loss: 89.8320603CurrentTrain: epoch  9, batch     9 | loss: 59.6061496CurrentTrain: epoch  9, batch    10 | loss: 90.0013392CurrentTrain: epoch  9, batch    11 | loss: 60.6586281CurrentTrain: epoch  9, batch    12 | loss: 84.5663568CurrentTrain: epoch  9, batch    13 | loss: 91.3295115CurrentTrain: epoch  9, batch    14 | loss: 90.7494039CurrentTrain: epoch  9, batch    15 | loss: 70.3694120CurrentTrain: epoch  9, batch    16 | loss: 119.2684067CurrentTrain: epoch  9, batch    17 | loss: 70.6771013CurrentTrain: epoch  9, batch    18 | loss: 72.4843282CurrentTrain: epoch  9, batch    19 | loss: 72.2452638CurrentTrain: epoch  9, batch    20 | loss: 60.3206817CurrentTrain: epoch  9, batch    21 | loss: 87.4814849CurrentTrain: epoch  9, batch    22 | loss: 60.7696605CurrentTrain: epoch  9, batch    23 | loss: 61.0544618CurrentTrain: epoch  9, batch    24 | loss: 113.7304798CurrentTrain: epoch  9, batch    25 | loss: 86.3373273CurrentTrain: epoch  9, batch    26 | loss: 56.4370420CurrentTrain: epoch  9, batch    27 | loss: 109.7785065CurrentTrain: epoch  9, batch    28 | loss: 60.7896766CurrentTrain: epoch  9, batch    29 | loss: 59.9447292CurrentTrain: epoch  9, batch    30 | loss: 57.7314406CurrentTrain: epoch  9, batch    31 | loss: 75.5046968CurrentTrain: epoch  9, batch    32 | loss: 85.7727618CurrentTrain: epoch  9, batch    33 | loss: 63.4258984CurrentTrain: epoch  9, batch    34 | loss: 56.4867483CurrentTrain: epoch  9, batch    35 | loss: 110.5875298CurrentTrain: epoch  9, batch    36 | loss: 77.0122059CurrentTrain: epoch  9, batch    37 | loss: 73.2267211CurrentTrain: epoch  9, batch    38 | loss: 76.9514124CurrentTrain: epoch  9, batch    39 | loss: 60.2854888CurrentTrain: epoch  9, batch    40 | loss: 57.5838564CurrentTrain: epoch  9, batch    41 | loss: 72.7380500CurrentTrain: epoch  9, batch    42 | loss: 56.4783513CurrentTrain: epoch  9, batch    43 | loss: 58.5024461CurrentTrain: epoch  9, batch    44 | loss: 115.6958611CurrentTrain: epoch  9, batch    45 | loss: 62.4006638CurrentTrain: epoch  9, batch    46 | loss: 73.6664110CurrentTrain: epoch  9, batch    47 | loss: 91.0921577CurrentTrain: epoch  9, batch    48 | loss: 71.4039074CurrentTrain: epoch  9, batch    49 | loss: 60.4436240CurrentTrain: epoch  9, batch    50 | loss: 67.9177911CurrentTrain: epoch  9, batch    51 | loss: 70.8050956CurrentTrain: epoch  9, batch    52 | loss: 64.0786095CurrentTrain: epoch  9, batch    53 | loss: 73.7515007CurrentTrain: epoch  9, batch    54 | loss: 61.0723950CurrentTrain: epoch  9, batch    55 | loss: 57.6202962CurrentTrain: epoch  9, batch    56 | loss: 76.9599453CurrentTrain: epoch  9, batch    57 | loss: 48.2166897CurrentTrain: epoch  9, batch    58 | loss: 87.2704977CurrentTrain: epoch  9, batch    59 | loss: 87.6749256CurrentTrain: epoch  9, batch    60 | loss: 67.1870273CurrentTrain: epoch  9, batch    61 | loss: 50.2731825CurrentTrain: epoch  9, batch    62 | loss: 88.3577754CurrentTrain: epoch  9, batch    63 | loss: 59.4233665CurrentTrain: epoch  9, batch    64 | loss: 51.2716996CurrentTrain: epoch  9, batch    65 | loss: 85.2662229CurrentTrain: epoch  9, batch    66 | loss: 88.3326660CurrentTrain: epoch  9, batch    67 | loss: 69.3009601CurrentTrain: epoch  9, batch    68 | loss: 112.8827773CurrentTrain: epoch  9, batch    69 | loss: 91.2558803CurrentTrain: epoch  9, batch    70 | loss: 70.6244202CurrentTrain: epoch  9, batch    71 | loss: 89.5669235CurrentTrain: epoch  9, batch    72 | loss: 59.5065921CurrentTrain: epoch  9, batch    73 | loss: 88.7248054CurrentTrain: epoch  9, batch    74 | loss: 121.7458344CurrentTrain: epoch  9, batch    75 | loss: 93.8340945CurrentTrain: epoch  9, batch    76 | loss: 87.7963440CurrentTrain: epoch  9, batch    77 | loss: 59.2521929CurrentTrain: epoch  9, batch    78 | loss: 113.7892233CurrentTrain: epoch  9, batch    79 | loss: 72.3979653CurrentTrain: epoch  9, batch    80 | loss: 50.8808630CurrentTrain: epoch  9, batch    81 | loss: 51.2580102CurrentTrain: epoch  9, batch    82 | loss: 91.9346677CurrentTrain: epoch  9, batch    83 | loss: 113.4136622CurrentTrain: epoch  9, batch    84 | loss: 114.8948335CurrentTrain: epoch  9, batch    85 | loss: 61.3733676CurrentTrain: epoch  9, batch    86 | loss: 71.8690161CurrentTrain: epoch  9, batch    87 | loss: 61.0432323CurrentTrain: epoch  9, batch    88 | loss: 118.3344209CurrentTrain: epoch  9, batch    89 | loss: 53.0058046CurrentTrain: epoch  9, batch    90 | loss: 75.0842019CurrentTrain: epoch  9, batch    91 | loss: 73.0970519CurrentTrain: epoch  9, batch    92 | loss: 50.3978168CurrentTrain: epoch  9, batch    93 | loss: 62.4763382CurrentTrain: epoch  9, batch    94 | loss: 86.2304182CurrentTrain: epoch  9, batch    95 | loss: 61.2168242

F1 score per class: {32: 0.5806451612903226, 6: 0.8356807511737089, 19: 0.32653061224489793, 24: 0.7419354838709677, 26: 0.8770053475935828, 29: 0.7800829875518672}
Micro-average F1 score: 0.7457627118644068
Weighted-average F1 score: 0.7437269776315201
F1 score per class: {32: 0.6132075471698113, 6: 0.8181818181818182, 19: 0.21333333333333335, 24: 0.7329842931937173, 26: 0.9072164948453608, 29: 0.839622641509434}
Micro-average F1 score: 0.7427536231884058
Weighted-average F1 score: 0.725322048651853
F1 score per class: {32: 0.6124401913875598, 6: 0.8256880733944955, 19: 0.23880597014925373, 24: 0.7329842931937173, 26: 0.9119170984455959, 29: 0.8240740740740741}
Micro-average F1 score: 0.7477148080438757
Weighted-average F1 score: 0.7343595824440436

F1 score per class: {32: 0.5806451612903226, 6: 0.8356807511737089, 19: 0.32653061224489793, 24: 0.7419354838709677, 26: 0.8770053475935828, 29: 0.7800829875518672}
Micro-average F1 score: 0.7457627118644068
Weighted-average F1 score: 0.7437269776315201
F1 score per class: {32: 0.6132075471698113, 6: 0.8181818181818182, 19: 0.21333333333333335, 24: 0.7329842931937173, 26: 0.9072164948453608, 29: 0.839622641509434}
Micro-average F1 score: 0.7427536231884058
Weighted-average F1 score: 0.725322048651853
F1 score per class: {32: 0.6124401913875598, 6: 0.8256880733944955, 19: 0.23880597014925373, 24: 0.7329842931937173, 26: 0.9119170984455959, 29: 0.8240740740740741}
Micro-average F1 score: 0.7477148080438757
Weighted-average F1 score: 0.7343595824440436

F1 score per class: {32: 0.4268774703557312, 6: 0.777292576419214, 19: 0.17391304347826086, 24: 0.6666666666666666, 26: 0.82, 29: 0.5497076023391813}
Micro-average F1 score: 0.5986394557823129
Weighted-average F1 score: 0.5774101038997664
F1 score per class: {32: 0.429042904290429, 6: 0.7531380753138075, 19: 0.12030075187969924, 24: 0.660377358490566, 26: 0.822429906542056, 29: 0.6544117647058824}
Micro-average F1 score: 0.5972323379461034
Weighted-average F1 score: 0.5682977608560131
F1 score per class: {32: 0.4280936454849498, 6: 0.7659574468085106, 19: 0.13559322033898305, 24: 0.660377358490566, 26: 0.8262910798122066, 29: 0.6402877697841727}
Micro-average F1 score: 0.603690036900369
Weighted-average F1 score: 0.5776088315362696

F1 score per class: {32: 0.4268774703557312, 6: 0.777292576419214, 19: 0.17391304347826086, 24: 0.6666666666666666, 26: 0.82, 29: 0.5497076023391813}
Micro-average F1 score: 0.5986394557823129
Weighted-average F1 score: 0.5774101038997664
F1 score per class: {32: 0.429042904290429, 6: 0.7531380753138075, 19: 0.12030075187969924, 24: 0.660377358490566, 26: 0.822429906542056, 29: 0.6544117647058824}
Micro-average F1 score: 0.5972323379461034
Weighted-average F1 score: 0.5682977608560131
F1 score per class: {32: 0.4280936454849498, 6: 0.7659574468085106, 19: 0.13559322033898305, 24: 0.660377358490566, 26: 0.8262910798122066, 29: 0.6402877697841727}
Micro-average F1 score: 0.603690036900369
Weighted-average F1 score: 0.5776088315362696
cur_acc_wo_na:  ['0.7458']
his_acc_wo_na:  ['0.7458']
cur_acc des_wo_na:  ['0.7428']
his_acc des_wo_na:  ['0.7428']
cur_acc rrf_wo_na:  ['0.7477']
his_acc rrf_wo_na:  ['0.7477']
cur_acc_w_na:  ['0.5986']
his_acc_w_na:  ['0.5986']
cur_acc des_w_na:  ['0.5972']
his_acc des_w_na:  ['0.5972']
cur_acc rrf_w_na:  ['0.6037']
his_acc rrf_w_na:  ['0.6037']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 83.0427098CurrentTrain: epoch  0, batch     1 | loss: 144.0085354CurrentTrain: epoch  0, batch     2 | loss: 105.5134748CurrentTrain: epoch  0, batch     3 | loss: 113.0457558CurrentTrain: epoch  0, batch     4 | loss: 23.4908611CurrentTrain: epoch  1, batch     0 | loss: 104.0630454CurrentTrain: epoch  1, batch     1 | loss: 84.7876218CurrentTrain: epoch  1, batch     2 | loss: 104.9303632CurrentTrain: epoch  1, batch     3 | loss: 81.9714680CurrentTrain: epoch  1, batch     4 | loss: 25.1269367CurrentTrain: epoch  2, batch     0 | loss: 123.6338736CurrentTrain: epoch  2, batch     1 | loss: 70.8541890CurrentTrain: epoch  2, batch     2 | loss: 82.3968396CurrentTrain: epoch  2, batch     3 | loss: 81.0250964CurrentTrain: epoch  2, batch     4 | loss: 17.6626028CurrentTrain: epoch  3, batch     0 | loss: 81.5168667CurrentTrain: epoch  3, batch     1 | loss: 80.6482826CurrentTrain: epoch  3, batch     2 | loss: 80.0682234CurrentTrain: epoch  3, batch     3 | loss: 97.2364539CurrentTrain: epoch  3, batch     4 | loss: 23.3219368CurrentTrain: epoch  4, batch     0 | loss: 81.3696987CurrentTrain: epoch  4, batch     1 | loss: 95.3793181CurrentTrain: epoch  4, batch     2 | loss: 64.1863081CurrentTrain: epoch  4, batch     3 | loss: 93.4478657CurrentTrain: epoch  4, batch     4 | loss: 13.3837638CurrentTrain: epoch  5, batch     0 | loss: 75.3346559CurrentTrain: epoch  5, batch     1 | loss: 75.0222156CurrentTrain: epoch  5, batch     2 | loss: 74.4515778CurrentTrain: epoch  5, batch     3 | loss: 119.8985046CurrentTrain: epoch  5, batch     4 | loss: 15.6029094CurrentTrain: epoch  6, batch     0 | loss: 73.3891766CurrentTrain: epoch  6, batch     1 | loss: 93.2504912CurrentTrain: epoch  6, batch     2 | loss: 74.0395869CurrentTrain: epoch  6, batch     3 | loss: 89.8699502CurrentTrain: epoch  6, batch     4 | loss: 22.7360696CurrentTrain: epoch  7, batch     0 | loss: 111.6589333CurrentTrain: epoch  7, batch     1 | loss: 85.5466113CurrentTrain: epoch  7, batch     2 | loss: 91.6937855CurrentTrain: epoch  7, batch     3 | loss: 73.7855162CurrentTrain: epoch  7, batch     4 | loss: 13.8438159CurrentTrain: epoch  8, batch     0 | loss: 86.8874973CurrentTrain: epoch  8, batch     1 | loss: 60.4142030CurrentTrain: epoch  8, batch     2 | loss: 73.7582863CurrentTrain: epoch  8, batch     3 | loss: 112.1729282CurrentTrain: epoch  8, batch     4 | loss: 14.3096389CurrentTrain: epoch  9, batch     0 | loss: 72.9800138CurrentTrain: epoch  9, batch     1 | loss: 69.9819230CurrentTrain: epoch  9, batch     2 | loss: 70.4330581CurrentTrain: epoch  9, batch     3 | loss: 60.3602428CurrentTrain: epoch  9, batch     4 | loss: 37.0931087
MemoryTrain:  epoch  0, batch     0 | loss: 2.1233805MemoryTrain:  epoch  1, batch     0 | loss: 1.8642155MemoryTrain:  epoch  2, batch     0 | loss: 1.3984435MemoryTrain:  epoch  3, batch     0 | loss: 1.1706445MemoryTrain:  epoch  4, batch     0 | loss: 0.9281715MemoryTrain:  epoch  5, batch     0 | loss: 0.6692223MemoryTrain:  epoch  6, batch     0 | loss: 0.5119890MemoryTrain:  epoch  7, batch     0 | loss: 0.4182627MemoryTrain:  epoch  8, batch     0 | loss: 0.3378779MemoryTrain:  epoch  9, batch     0 | loss: 0.2657800

F1 score per class: {32: 0.5263157894736842, 2: 0.0, 6: 0.656934306569343, 39: 0.6666666666666666, 11: 0.0, 12: 0.0, 19: 0.19607843137254902, 24: 0.0, 28: 0.4444444444444444}
Micro-average F1 score: 0.5550660792951542
Weighted-average F1 score: 0.4893422237309165
F1 score per class: {32: 0.6666666666666666, 2: 0.0, 6: 0.5909090909090909, 39: 0.6411483253588517, 11: 0.0, 12: 0.0, 19: 0.2702702702702703, 24: 0.0, 28: 0.0, 29: 0.42857142857142855}
Micro-average F1 score: 0.5091649694501018
Weighted-average F1 score: 0.44015810772776714
F1 score per class: {32: 0.7272727272727273, 2: 0.0, 6: 0.5625, 39: 0.6291079812206573, 11: 0.0, 12: 0.0, 19: 0.24390243902439024, 24: 0.0, 28: 0.0, 29: 0.48}
Micro-average F1 score: 0.5115303983228512
Weighted-average F1 score: 0.4510563093809481

F1 score per class: {32: 0.47619047619047616, 2: 0.6415094339622641, 6: 0.625, 39: 0.38285714285714284, 11: 0.8115942028985508, 12: 0.34285714285714286, 19: 0.7582417582417582, 24: 0.10989010989010989, 26: 0.8864864864864865, 28: 0.7679324894514767, 29: 0.38095238095238093}
Micro-average F1 score: 0.6243323442136498
Weighted-average F1 score: 0.585936974198982
F1 score per class: {32: 0.5714285714285714, 2: 0.6220472440944882, 6: 0.5492957746478874, 39: 0.3743016759776536, 11: 0.7927927927927928, 12: 0.2702702702702703, 19: 0.7486631016042781, 24: 0.20833333333333334, 26: 0.8969072164948454, 28: 0.7045454545454546, 29: 0.35294117647058826}
Micro-average F1 score: 0.6116343490304709
Weighted-average F1 score: 0.5853189682912525
F1 score per class: {32: 0.6666666666666666, 2: 0.6192468619246861, 6: 0.5255474452554745, 39: 0.3582887700534759, 11: 0.7927927927927928, 12: 0.3333333333333333, 19: 0.7567567567567568, 24: 0.1724137931034483, 26: 0.8969072164948454, 28: 0.7072243346007605, 29: 0.3870967741935484}
Micro-average F1 score: 0.6116449971735444
Weighted-average F1 score: 0.5851080294298927

F1 score per class: {32: 0.35714285714285715, 2: 0.0, 6: 0.5806451612903226, 39: 0.5275590551181102, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.07874015748031496, 26: 0.0, 28: 0.34782608695652173}
Micro-average F1 score: 0.38650306748466257
Weighted-average F1 score: 0.32201743210882455
F1 score per class: {32: 0.41025641025641024, 2: 0.0, 6: 0.4785276073619632, 39: 0.4908424908424908, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.12658227848101267, 26: 0.0, 28: 0.0, 29: 0.26666666666666666}
Micro-average F1 score: 0.3429355281207133
Weighted-average F1 score: 0.29483729845962553
F1 score per class: {32: 0.5, 2: 0.0, 6: 0.4768211920529801, 39: 0.48201438848920863, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.10204081632653061, 26: 0.0, 28: 0.0, 29: 0.3333333333333333}
Micro-average F1 score: 0.3505747126436782
Weighted-average F1 score: 0.3015834445558448

F1 score per class: {32: 0.3225806451612903, 2: 0.4108761329305136, 6: 0.5232558139534884, 39: 0.21069182389937108, 11: 0.7671232876712328, 12: 0.25, 19: 0.6934673366834171, 24: 0.04830917874396135, 26: 0.8159203980099502, 28: 0.5565749235474006, 29: 0.21052631578947367}
Micro-average F1 score: 0.43669572436695725
Weighted-average F1 score: 0.3886070756222778
F1 score per class: {32: 0.34782608695652173, 2: 0.3700234192037471, 6: 0.43333333333333335, 39: 0.21303656597774245, 11: 0.7364016736401674, 12: 0.15748031496062992, 19: 0.6666666666666666, 24: 0.09615384615384616, 26: 0.8130841121495327, 28: 0.5054347826086957, 29: 0.18181818181818182}
Micro-average F1 score: 0.42298850574712643
Weighted-average F1 score: 0.38767868599862587
F1 score per class: {32: 0.4444444444444444, 2: 0.38046272493573263, 6: 0.42857142857142855, 39: 0.2027231467473525, 11: 0.7364016736401674, 12: 0.1794871794871795, 19: 0.6796116504854369, 24: 0.07194244604316546, 26: 0.8207547169811321, 28: 0.5152354570637119, 29: 0.21428571428571427}
Micro-average F1 score: 0.425147347740668
Weighted-average F1 score: 0.38714627682591063
cur_acc_wo_na:  ['0.7458', '0.5551']
his_acc_wo_na:  ['0.7458', '0.6243']
cur_acc des_wo_na:  ['0.7428', '0.5092']
his_acc des_wo_na:  ['0.7428', '0.6116']
cur_acc rrf_wo_na:  ['0.7477', '0.5115']
his_acc rrf_wo_na:  ['0.7477', '0.6116']
cur_acc_w_na:  ['0.5986', '0.3865']
his_acc_w_na:  ['0.5986', '0.4367']
cur_acc des_w_na:  ['0.5972', '0.3429']
his_acc des_w_na:  ['0.5972', '0.4230']
cur_acc rrf_w_na:  ['0.6037', '0.3506']
his_acc rrf_w_na:  ['0.6037', '0.4251']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 115.3614855CurrentTrain: epoch  0, batch     1 | loss: 74.3015146CurrentTrain: epoch  0, batch     2 | loss: 111.2641384CurrentTrain: epoch  0, batch     3 | loss: 8.7812234CurrentTrain: epoch  1, batch     0 | loss: 71.2416416CurrentTrain: epoch  1, batch     1 | loss: 69.1157245CurrentTrain: epoch  1, batch     2 | loss: 81.2313659CurrentTrain: epoch  1, batch     3 | loss: 12.5430673CurrentTrain: epoch  2, batch     0 | loss: 66.1552833CurrentTrain: epoch  2, batch     1 | loss: 79.1261312CurrentTrain: epoch  2, batch     2 | loss: 68.5766009CurrentTrain: epoch  2, batch     3 | loss: 11.3642834CurrentTrain: epoch  3, batch     0 | loss: 115.4840263CurrentTrain: epoch  3, batch     1 | loss: 75.8850414CurrentTrain: epoch  3, batch     2 | loss: 61.8914212CurrentTrain: epoch  3, batch     3 | loss: 15.4615619CurrentTrain: epoch  4, batch     0 | loss: 117.1554561CurrentTrain: epoch  4, batch     1 | loss: 59.7855042CurrentTrain: epoch  4, batch     2 | loss: 61.8312387CurrentTrain: epoch  4, batch     3 | loss: 5.1287248CurrentTrain: epoch  5, batch     0 | loss: 60.3230757CurrentTrain: epoch  5, batch     1 | loss: 86.2866871CurrentTrain: epoch  5, batch     2 | loss: 72.3093028CurrentTrain: epoch  5, batch     3 | loss: 9.4849474CurrentTrain: epoch  6, batch     0 | loss: 61.4113516CurrentTrain: epoch  6, batch     1 | loss: 86.1881353CurrentTrain: epoch  6, batch     2 | loss: 69.6729787CurrentTrain: epoch  6, batch     3 | loss: 16.5222443CurrentTrain: epoch  7, batch     0 | loss: 66.4279774CurrentTrain: epoch  7, batch     1 | loss: 85.7651254CurrentTrain: epoch  7, batch     2 | loss: 69.1198998CurrentTrain: epoch  7, batch     3 | loss: 9.0342348CurrentTrain: epoch  8, batch     0 | loss: 87.6230217CurrentTrain: epoch  8, batch     1 | loss: 69.5739536CurrentTrain: epoch  8, batch     2 | loss: 54.5781454CurrentTrain: epoch  8, batch     3 | loss: 7.5739263CurrentTrain: epoch  9, batch     0 | loss: 59.5857564CurrentTrain: epoch  9, batch     1 | loss: 85.2039152CurrentTrain: epoch  9, batch     2 | loss: 56.1245982CurrentTrain: epoch  9, batch     3 | loss: 8.4655865
MemoryTrain:  epoch  0, batch     0 | loss: 1.0597049MemoryTrain:  epoch  1, batch     0 | loss: 0.8895515MemoryTrain:  epoch  2, batch     0 | loss: 0.7669668MemoryTrain:  epoch  3, batch     0 | loss: 0.6367299MemoryTrain:  epoch  4, batch     0 | loss: 0.4910194MemoryTrain:  epoch  5, batch     0 | loss: 0.3804731MemoryTrain:  epoch  6, batch     0 | loss: 0.3136624MemoryTrain:  epoch  7, batch     0 | loss: 0.2467693MemoryTrain:  epoch  8, batch     0 | loss: 0.2249913MemoryTrain:  epoch  9, batch     0 | loss: 0.1830037

F1 score per class: {32: 0.0, 6: 0.75, 7: 0.8620689655172413, 40: 0.0, 9: 0.0, 12: 0.0, 19: 0.34782608695652173, 26: 0.0, 27: 0.0, 31: 0.2857142857142857}
Micro-average F1 score: 0.35125448028673834
Weighted-average F1 score: 0.29021755356088186
F1 score per class: {32: 0.0, 6: 0.5714285714285714, 7: 0.6944444444444444, 40: 0.0, 9: 0.0, 12: 0.0, 19: 0.0, 24: 0.47619047619047616, 26: 0.0, 27: 0.0, 28: 0.5, 29: 0.0, 31: 0.2956521739130435}
Micro-average F1 score: 0.3388704318936877
Weighted-average F1 score: 0.2876603078649114
F1 score per class: {32: 0.0, 6: 0.5714285714285714, 7: 0.7692307692307693, 40: 0.0, 9: 0.0, 12: 0.0, 19: 0.5454545454545454, 26: 0.0, 27: 0.0, 28: 0.3333333333333333, 29: 0.0, 31: 0.29310344827586204}
Micro-average F1 score: 0.35294117647058826
Weighted-average F1 score: 0.2940242680144158

F1 score per class: {32: 0.5263157894736842, 2: 0.4258064516129032, 6: 0.06060606060606061, 7: 0.847457627118644, 40: 0.6015037593984962, 11: 0.4131455399061033, 12: 0.6311475409836066, 39: 0.18181818181818182, 9: 0.7314285714285714, 19: 0.21621621621621623, 24: 0.11235955056179775, 26: 0.861878453038674, 27: 0.0, 28: 0.8088888888888889, 29: 0.34782608695652173, 31: 0.15454545454545454}
Micro-average F1 score: 0.5176223040504997
Weighted-average F1 score: 0.470860269770633
F1 score per class: {32: 0.5517241379310345, 2: 0.4772727272727273, 6: 0.0625, 7: 0.625, 40: 0.6187050359712231, 9: 0.35467980295566504, 12: 0.6199261992619927, 11: 0.23809523809523808, 39: 0.7303370786516854, 19: 0.3125, 24: 0.125, 26: 0.8677248677248677, 27: 0.19047619047619047, 28: 0.7711864406779662, 29: 0.3333333333333333, 31: 0.1642512077294686}
Micro-average F1 score: 0.5230146686899343
Weighted-average F1 score: 0.48876404305481835
F1 score per class: {32: 0.6666666666666666, 2: 0.47953216374269003, 6: 0.06060606060606061, 7: 0.7575757575757576, 40: 0.6231884057971014, 11: 0.36363636363636365, 12: 0.6363636363636364, 39: 0.2857142857142857, 9: 0.7344632768361582, 19: 0.3, 24: 0.125, 26: 0.8677248677248677, 27: 0.15384615384615385, 28: 0.7878787878787878, 29: 0.38461538461538464, 31: 0.1588785046728972}
Micro-average F1 score: 0.5348595213319459
Weighted-average F1 score: 0.49828254670278505

F1 score per class: {32: 0.0, 6: 0.5, 7: 0.78125, 40: 0.0, 9: 0.0, 12: 0.0, 11: 0.0, 19: 0.32, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.2446043165467626}
Micro-average F1 score: 0.28488372093023256
Weighted-average F1 score: 0.23649741945573977
F1 score per class: {32: 0.0, 6: 0.5, 7: 0.6097560975609756, 40: 0.0, 9: 0.0, 12: 0.0, 11: 0.0, 19: 0.0, 24: 0.43478260869565216, 26: 0.0, 27: 0.0, 28: 0.36363636363636365, 29: 0.0, 31: 0.2537313432835821}
Micro-average F1 score: 0.27945205479452057
Weighted-average F1 score: 0.23909626884388385
F1 score per class: {32: 0.0, 6: 0.4444444444444444, 7: 0.704225352112676, 40: 0.0, 9: 0.0, 12: 0.0, 11: 0.0, 19: 0.5, 26: 0.0, 27: 0.0, 28: 0.2222222222222222, 29: 0.0, 31: 0.2518518518518518}
Micro-average F1 score: 0.29394812680115273
Weighted-average F1 score: 0.24634140880891775

F1 score per class: {32: 0.3448275862068966, 2: 0.28820960698689957, 6: 0.030303030303030304, 7: 0.7692307692307693, 40: 0.5228758169934641, 39: 0.25071225071225073, 12: 0.5945945945945946, 11: 0.14814814814814814, 9: 0.6772486772486772, 19: 0.1702127659574468, 24: 0.060240963855421686, 26: 0.8, 27: 0.0, 28: 0.6127946127946128, 29: 0.19047619047619047, 31: 0.1223021582733813}
Micro-average F1 score: 0.38785967678360267
Weighted-average F1 score: 0.34165564453100894
F1 score per class: {32: 0.35555555555555557, 2: 0.3088235294117647, 6: 0.0392156862745098, 7: 0.5319148936170213, 40: 0.4942528735632184, 39: 0.22857142857142856, 12: 0.5714285714285714, 11: 0.16393442622950818, 9: 0.6701030927835051, 19: 0.21739130434782608, 24: 0.05917159763313609, 26: 0.7922705314009661, 27: 0.12121212121212122, 28: 0.5814696485623003, 29: 0.18518518518518517, 31: 0.13076923076923078}
Micro-average F1 score: 0.39270793771363466
Weighted-average F1 score: 0.3591670018564116
F1 score per class: {32: 0.4375, 2: 0.30943396226415093, 6: 0.03773584905660377, 7: 0.684931506849315, 40: 0.5119047619047619, 11: 0.22929936305732485, 12: 0.5936395759717314, 39: 0.21621621621621623, 9: 0.6770833333333334, 19: 0.2222222222222222, 24: 0.05952380952380952, 26: 0.7961165048543689, 27: 0.09090909090909091, 28: 0.5909090909090909, 29: 0.2, 31: 0.12639405204460966}
Micro-average F1 score: 0.40361209265802905
Weighted-average F1 score: 0.36706144723596273
cur_acc_wo_na:  ['0.7458', '0.5551', '0.3513']
his_acc_wo_na:  ['0.7458', '0.6243', '0.5176']
cur_acc des_wo_na:  ['0.7428', '0.5092', '0.3389']
his_acc des_wo_na:  ['0.7428', '0.6116', '0.5230']
cur_acc rrf_wo_na:  ['0.7477', '0.5115', '0.3529']
his_acc rrf_wo_na:  ['0.7477', '0.6116', '0.5349']
cur_acc_w_na:  ['0.5986', '0.3865', '0.2849']
his_acc_w_na:  ['0.5986', '0.4367', '0.3879']
cur_acc des_w_na:  ['0.5972', '0.3429', '0.2795']
his_acc des_w_na:  ['0.5972', '0.4230', '0.3927']
cur_acc rrf_w_na:  ['0.6037', '0.3506', '0.2939']
his_acc rrf_w_na:  ['0.6037', '0.4251', '0.4036']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 85.1155531CurrentTrain: epoch  0, batch     1 | loss: 83.9155945CurrentTrain: epoch  0, batch     2 | loss: 113.4208864CurrentTrain: epoch  0, batch     3 | loss: 123.6280435CurrentTrain: epoch  1, batch     0 | loss: 71.5649040CurrentTrain: epoch  1, batch     1 | loss: 104.1315378CurrentTrain: epoch  1, batch     2 | loss: 77.6203136CurrentTrain: epoch  1, batch     3 | loss: 71.6956245CurrentTrain: epoch  2, batch     0 | loss: 81.8167567CurrentTrain: epoch  2, batch     1 | loss: 68.4229516CurrentTrain: epoch  2, batch     2 | loss: 65.1920966CurrentTrain: epoch  2, batch     3 | loss: 84.1248750CurrentTrain: epoch  3, batch     0 | loss: 98.4984978CurrentTrain: epoch  3, batch     1 | loss: 75.8378083CurrentTrain: epoch  3, batch     2 | loss: 78.7185527CurrentTrain: epoch  3, batch     3 | loss: 53.4119795CurrentTrain: epoch  4, batch     0 | loss: 75.9830684CurrentTrain: epoch  4, batch     1 | loss: 90.2614936CurrentTrain: epoch  4, batch     2 | loss: 118.3556195CurrentTrain: epoch  4, batch     3 | loss: 47.2128637CurrentTrain: epoch  5, batch     0 | loss: 90.1170533CurrentTrain: epoch  5, batch     1 | loss: 70.2884299CurrentTrain: epoch  5, batch     2 | loss: 94.8051841CurrentTrain: epoch  5, batch     3 | loss: 41.5844039CurrentTrain: epoch  6, batch     0 | loss: 118.8432171CurrentTrain: epoch  6, batch     1 | loss: 82.5326165CurrentTrain: epoch  6, batch     2 | loss: 73.7127934CurrentTrain: epoch  6, batch     3 | loss: 58.0116111CurrentTrain: epoch  7, batch     0 | loss: 72.5531951CurrentTrain: epoch  7, batch     1 | loss: 57.5924378CurrentTrain: epoch  7, batch     2 | loss: 73.2999234CurrentTrain: epoch  7, batch     3 | loss: 47.3206641CurrentTrain: epoch  8, batch     0 | loss: 88.3142890CurrentTrain: epoch  8, batch     1 | loss: 60.9216645CurrentTrain: epoch  8, batch     2 | loss: 68.6509404CurrentTrain: epoch  8, batch     3 | loss: 57.9726872CurrentTrain: epoch  9, batch     0 | loss: 71.3397536CurrentTrain: epoch  9, batch     1 | loss: 85.2291322CurrentTrain: epoch  9, batch     2 | loss: 57.1788673CurrentTrain: epoch  9, batch     3 | loss: 73.9292470
MemoryTrain:  epoch  0, batch     0 | loss: 0.9246839MemoryTrain:  epoch  1, batch     0 | loss: 0.7994953MemoryTrain:  epoch  2, batch     0 | loss: 0.6792188MemoryTrain:  epoch  3, batch     0 | loss: 0.5853875MemoryTrain:  epoch  4, batch     0 | loss: 0.4525864MemoryTrain:  epoch  5, batch     0 | loss: 0.3547348MemoryTrain:  epoch  6, batch     0 | loss: 0.2774364MemoryTrain:  epoch  7, batch     0 | loss: 0.2155186MemoryTrain:  epoch  8, batch     0 | loss: 0.2029393MemoryTrain:  epoch  9, batch     0 | loss: 0.1716227

F1 score per class: {32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 1.0, 6: 0.0, 11: 0.6052631578947368, 12: 0.0, 7: 0.0, 40: 0.0, 15: 0.8210526315789474, 19: 0.47572815533980584, 25: 0.6521739130434783, 27: 0.0, 28: 0.0}
Micro-average F1 score: 0.5528455284552846
Weighted-average F1 score: 0.48745042162660357
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 15: 0.7, 19: 0.0, 24: 0.0, 25: 0.6904761904761905, 26: 0.0, 27: 0.0, 28: 0.0, 32: 0.0, 35: 0.7636363636363637, 37: 0.535031847133758, 38: 0.7017543859649122, 40: 0.0}
Micro-average F1 score: 0.546875
Weighted-average F1 score: 0.4707355302724776
F1 score per class: {32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.8181818181818182, 6: 0.0, 11: 0.6987951807228916, 12: 0.0, 7: 0.0, 40: 0.0, 15: 0.0, 19: 0.7818181818181819, 25: 0.525, 26: 0.6666666666666666, 27: 0.0, 28: 0.0}
Micro-average F1 score: 0.5696969696969697
Weighted-average F1 score: 0.5015014914326446

F1 score per class: {2: 0.6086956521739131, 6: 0.47674418604651164, 7: 0.057971014492753624, 9: 0.819672131147541, 11: 0.3177570093457944, 12: 0.33136094674556216, 15: 0.6451612903225806, 19: 0.6183206106870229, 24: 0.10526315789473684, 25: 0.6052631578947368, 26: 0.735632183908046, 27: 0.3018867924528302, 28: 0.17142857142857143, 29: 0.8633879781420765, 31: 0.2222222222222222, 32: 0.7692307692307693, 35: 0.5735294117647058, 37: 0.18525519848771266, 38: 0.4411764705882353, 39: 0.125, 40: 0.16748768472906403}
Micro-average F1 score: 0.4519049415314976
Weighted-average F1 score: 0.40780004592168817
F1 score per class: {2: 0.4444444444444444, 6: 0.47115384615384615, 7: 0.04819277108433735, 9: 0.5376344086021505, 11: 0.5142857142857142, 12: 0.33816425120772947, 15: 0.45161290322580644, 19: 0.5346534653465347, 24: 0.25, 25: 0.6904761904761905, 26: 0.73224043715847, 27: 0.34782608695652173, 28: 0.2, 29: 0.8900523560209425, 31: 0.13333333333333333, 32: 0.75, 35: 0.46408839779005523, 37: 0.23661971830985915, 38: 0.43010752688172044, 39: 0.0, 40: 0.17679558011049723}
Micro-average F1 score: 0.4698275862068966
Weighted-average F1 score: 0.4386009611748381
F1 score per class: {2: 0.5833333333333334, 6: 0.48756218905472637, 7: 0.05063291139240506, 9: 0.7142857142857143, 11: 0.5179856115107914, 12: 0.3384615384615385, 15: 0.4864864864864865, 19: 0.5797101449275363, 24: 0.18181818181818182, 25: 0.6987951807228916, 26: 0.7262569832402235, 27: 0.3157894736842105, 28: 0.1875, 29: 0.8842105263157894, 31: 0.15384615384615385, 32: 0.7758620689655172, 35: 0.5, 37: 0.22580645161290322, 38: 0.41379310344827586, 39: 0.0, 40: 0.16831683168316833}
Micro-average F1 score: 0.4776670358065707
Weighted-average F1 score: 0.4421655355543608

F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.8, 19: 0.0, 25: 0.5227272727272727, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6724137931034483, 37: 0.3726235741444867, 38: 0.5263157894736842, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.40236686390532544
Weighted-average F1 score: 0.34827541434941806
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 15: 0.5, 19: 0.0, 24: 0.0, 25: 0.5979381443298969, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6176470588235294, 37: 0.42424242424242425, 38: 0.5333333333333333, 40: 0.0}
Micro-average F1 score: 0.39270687237026647
Weighted-average F1 score: 0.33597526895891905
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.6206896551724138, 19: 0.0, 24: 0.0, 25: 0.6041666666666666, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6466165413533834, 37: 0.417910447761194, 38: 0.5373134328358209, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.41409691629955947
Weighted-average F1 score: 0.3568324795573166

F1 score per class: {2: 0.4117647058823529, 6: 0.3178294573643411, 7: 0.03125, 9: 0.746268656716418, 11: 0.3090909090909091, 12: 0.22580645161290322, 15: 0.425531914893617, 19: 0.5684210526315789, 24: 0.1, 25: 0.5168539325842697, 26: 0.6701570680628273, 27: 0.20512820512820512, 28: 0.09090909090909091, 29: 0.7707317073170732, 31: 0.14285714285714285, 32: 0.6071428571428571, 35: 0.37142857142857144, 37: 0.1104847801578354, 38: 0.3125, 39: 0.1111111111111111, 40: 0.12830188679245283}
Micro-average F1 score: 0.3271436373566357
Weighted-average F1 score: 0.2874760988607404
F1 score per class: {2: 0.2962962962962963, 6: 0.2916666666666667, 7: 0.02857142857142857, 9: 0.4065040650406504, 11: 0.4528301886792453, 12: 0.21212121212121213, 15: 0.27450980392156865, 19: 0.4682080924855491, 24: 0.14492753623188406, 25: 0.58, 26: 0.6504854368932039, 27: 0.2222222222222222, 28: 0.08571428571428572, 29: 0.7798165137614679, 31: 0.09523809523809523, 32: 0.5750798722044729, 35: 0.3, 37: 0.14165261382799327, 38: 0.26490066225165565, 39: 0.0, 40: 0.13278008298755187}
Micro-average F1 score: 0.3305534495830174
Weighted-average F1 score: 0.30206892957316356
F1 score per class: {2: 0.3783783783783784, 6: 0.30153846153846153, 7: 0.029197080291970802, 9: 0.6493506493506493, 11: 0.4645161290322581, 12: 0.2129032258064516, 15: 0.2727272727272727, 19: 0.5144694533762058, 24: 0.13793103448275862, 25: 0.58, 26: 0.6565656565656566, 27: 0.2, 28: 0.08450704225352113, 29: 0.7777777777777778, 31: 0.11764705882352941, 32: 0.6, 35: 0.3359375, 37: 0.13354531001589826, 38: 0.26865671641791045, 39: 0.0, 40: 0.1297709923664122}
Micro-average F1 score: 0.3399894902785076
Weighted-average F1 score: 0.30718550726344035
cur_acc_wo_na:  ['0.7458', '0.5551', '0.3513', '0.5528']
his_acc_wo_na:  ['0.7458', '0.6243', '0.5176', '0.4519']
cur_acc des_wo_na:  ['0.7428', '0.5092', '0.3389', '0.5469']
his_acc des_wo_na:  ['0.7428', '0.6116', '0.5230', '0.4698']
cur_acc rrf_wo_na:  ['0.7477', '0.5115', '0.3529', '0.5697']
his_acc rrf_wo_na:  ['0.7477', '0.6116', '0.5349', '0.4777']
cur_acc_w_na:  ['0.5986', '0.3865', '0.2849', '0.4024']
his_acc_w_na:  ['0.5986', '0.4367', '0.3879', '0.3271']
cur_acc des_w_na:  ['0.5972', '0.3429', '0.2795', '0.3927']
his_acc des_w_na:  ['0.5972', '0.4230', '0.3927', '0.3306']
cur_acc rrf_w_na:  ['0.6037', '0.3506', '0.2939', '0.4141']
his_acc rrf_w_na:  ['0.6037', '0.4251', '0.4036', '0.3400']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 143.7892376CurrentTrain: epoch  0, batch     1 | loss: 98.4338547CurrentTrain: epoch  0, batch     2 | loss: 101.3746959CurrentTrain: epoch  0, batch     3 | loss: 112.5319781CurrentTrain: epoch  0, batch     4 | loss: 72.9308263CurrentTrain: epoch  1, batch     0 | loss: 84.0780206CurrentTrain: epoch  1, batch     1 | loss: 105.9818868CurrentTrain: epoch  1, batch     2 | loss: 73.6585550CurrentTrain: epoch  1, batch     3 | loss: 100.5014338CurrentTrain: epoch  1, batch     4 | loss: 99.2692994CurrentTrain: epoch  2, batch     0 | loss: 82.2269265CurrentTrain: epoch  2, batch     1 | loss: 81.7883799CurrentTrain: epoch  2, batch     2 | loss: 70.3566811CurrentTrain: epoch  2, batch     3 | loss: 80.6870185CurrentTrain: epoch  2, batch     4 | loss: 95.1904480CurrentTrain: epoch  3, batch     0 | loss: 63.7837323CurrentTrain: epoch  3, batch     1 | loss: 95.6086854CurrentTrain: epoch  3, batch     2 | loss: 82.9180572CurrentTrain: epoch  3, batch     3 | loss: 98.6207076CurrentTrain: epoch  3, batch     4 | loss: 137.9952620CurrentTrain: epoch  4, batch     0 | loss: 75.3902983CurrentTrain: epoch  4, batch     1 | loss: 96.7450468CurrentTrain: epoch  4, batch     2 | loss: 76.6866649CurrentTrain: epoch  4, batch     3 | loss: 78.9481197CurrentTrain: epoch  4, batch     4 | loss: 65.6692814CurrentTrain: epoch  5, batch     0 | loss: 118.5614935CurrentTrain: epoch  5, batch     1 | loss: 61.9272282CurrentTrain: epoch  5, batch     2 | loss: 91.6591811CurrentTrain: epoch  5, batch     3 | loss: 92.2292782CurrentTrain: epoch  5, batch     4 | loss: 64.7080458CurrentTrain: epoch  6, batch     0 | loss: 90.3504467CurrentTrain: epoch  6, batch     1 | loss: 64.1338155CurrentTrain: epoch  6, batch     2 | loss: 91.5065995CurrentTrain: epoch  6, batch     3 | loss: 90.5972792CurrentTrain: epoch  6, batch     4 | loss: 33.8044152CurrentTrain: epoch  7, batch     0 | loss: 87.6603221CurrentTrain: epoch  7, batch     1 | loss: 73.8348417CurrentTrain: epoch  7, batch     2 | loss: 71.5250212CurrentTrain: epoch  7, batch     3 | loss: 88.0812590CurrentTrain: epoch  7, batch     4 | loss: 48.0916027CurrentTrain: epoch  8, batch     0 | loss: 89.1822740CurrentTrain: epoch  8, batch     1 | loss: 60.0805470CurrentTrain: epoch  8, batch     2 | loss: 88.1955597CurrentTrain: epoch  8, batch     3 | loss: 69.3155508CurrentTrain: epoch  8, batch     4 | loss: 48.5637999CurrentTrain: epoch  9, batch     0 | loss: 70.0824044CurrentTrain: epoch  9, batch     1 | loss: 58.3540190CurrentTrain: epoch  9, batch     2 | loss: 72.5983530CurrentTrain: epoch  9, batch     3 | loss: 111.4607269CurrentTrain: epoch  9, batch     4 | loss: 62.1741757
MemoryTrain:  epoch  0, batch     0 | loss: 1.2419077MemoryTrain:  epoch  1, batch     0 | loss: 1.0510427MemoryTrain:  epoch  2, batch     0 | loss: 0.9385381MemoryTrain:  epoch  3, batch     0 | loss: 0.7474169MemoryTrain:  epoch  4, batch     0 | loss: 0.6108412MemoryTrain:  epoch  5, batch     0 | loss: 0.5730099MemoryTrain:  epoch  6, batch     0 | loss: 0.4576139MemoryTrain:  epoch  7, batch     0 | loss: 0.3722377MemoryTrain:  epoch  8, batch     0 | loss: 0.2596118MemoryTrain:  epoch  9, batch     0 | loss: 0.2464408

F1 score per class: {1: 0.19161676646706588, 3: 0.7393939393939394, 6: 0.0, 7: 0.0, 14: 0.11267605633802817, 19: 0.0, 22: 0.5352112676056338, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.7413793103448276, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.37243947858473
Weighted-average F1 score: 0.3141664226028173
F1 score per class: {1: 0.2222222222222222, 3: 0.5355191256830601, 6: 0.0, 9: 0.0, 12: 0.0, 14: 0.06976744186046512, 15: 0.0, 19: 0.0, 22: 0.5597014925373134, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.7596899224806202, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.336480686695279
Weighted-average F1 score: 0.287385617433938
F1 score per class: {1: 0.23728813559322035, 3: 0.5263157894736842, 6: 0.0, 7: 0.0, 9: 0.0, 12: 0.0, 14: 0.07407407407407407, 19: 0.0, 22: 0.5652173913043478, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.7518796992481203, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3497835497835498
Weighted-average F1 score: 0.30533708388593866

F1 score per class: {1: 0.1523809523809524, 2: 0.6, 3: 0.5062240663900415, 6: 0.44919786096256686, 7: 0.05263157894736842, 9: 0.78125, 11: 0.22, 12: 0.32679738562091504, 14: 0.1095890410958904, 15: 0.6363636363636364, 19: 0.5527272727272727, 22: 0.4293785310734463, 24: 0.0, 25: 0.5070422535211268, 26: 0.7415730337078652, 27: 0.0, 28: 0.11267605633802817, 29: 0.8297872340425532, 31: 0.0, 32: 0.6517857142857143, 34: 0.303886925795053, 35: 0.14, 37: 0.09872611464968153, 38: 0.3902439024390244, 39: 0.125, 40: 0.2328042328042328}
Micro-average F1 score: 0.36768513723459345
Weighted-average F1 score: 0.3362845059069359
F1 score per class: {1: 0.1646090534979424, 2: 0.4827586206896552, 3: 0.3356164383561644, 6: 0.46296296296296297, 7: 0.06896551724137931, 9: 0.49019607843137253, 11: 0.1276595744680851, 12: 0.31088082901554404, 14: 0.05454545454545454, 15: 0.5714285714285714, 19: 0.5080385852090032, 22: 0.46439628482972134, 24: 0.08, 25: 0.6976744186046512, 26: 0.7195767195767195, 27: 0.0, 28: 0.1388888888888889, 29: 0.8229166666666666, 31: 0.08695652173913043, 32: 0.6007604562737643, 34: 0.29878048780487804, 35: 0.16883116883116883, 37: 0.09444444444444444, 38: 0.3418803418803419, 39: 0.0, 40: 0.2736842105263158}
Micro-average F1 score: 0.36706349206349204
Weighted-average F1 score: 0.3491168944519254
F1 score per class: {1: 0.18025751072961374, 2: 0.5833333333333334, 3: 0.32786885245901637, 6: 0.4784688995215311, 7: 0.06779661016949153, 9: 0.7246376811594203, 11: 0.1276595744680851, 12: 0.3128491620111732, 14: 0.06521739130434782, 15: 0.631578947368421, 19: 0.5302013422818792, 22: 0.4369747899159664, 24: 0.0, 25: 0.6829268292682927, 26: 0.7282608695652174, 27: 0.0, 28: 0.13157894736842105, 29: 0.8253968253968254, 31: 0.13333333333333333, 32: 0.6184738955823293, 34: 0.29411764705882354, 35: 0.17266187050359713, 37: 0.09026128266033254, 38: 0.34615384615384615, 39: 0.0, 40: 0.25}
Micro-average F1 score: 0.36708229426433914
Weighted-average F1 score: 0.34576530330654537

F1 score per class: {1: 0.11072664359861592, 2: 0.0, 3: 0.5951219512195122, 6: 0.0, 7: 0.0, 9: 0.0, 12: 0.0, 14: 0.10666666666666667, 19: 0.0, 22: 0.41530054644808745, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5149700598802395, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.24227740763173833
Weighted-average F1 score: 0.2063159682034447
F1 score per class: {1: 0.125, 2: 0.0, 3: 0.4066390041493776, 6: 0.0, 7: 0.0, 9: 0.0, 12: 0.0, 14: 0.05714285714285714, 15: 0.0, 19: 0.0, 22: 0.4411764705882353, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.547486033519553, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.21729490022172948
Weighted-average F1 score: 0.188489641077683
F1 score per class: {1: 0.13291139240506328, 2: 0.0, 3: 0.398406374501992, 6: 0.0, 7: 0.0, 9: 0.0, 12: 0.0, 14: 0.06382978723404255, 15: 0.0, 19: 0.0, 22: 0.43213296398891965, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5347593582887701, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.22494432071269488
Weighted-average F1 score: 0.19883005375893248

F1 score per class: {1: 0.08672086720867209, 2: 0.375, 3: 0.3609467455621302, 6: 0.27450980392156865, 7: 0.024539877300613498, 9: 0.704225352112676, 11: 0.21568627450980393, 12: 0.211864406779661, 14: 0.09876543209876543, 15: 0.4827586206896552, 19: 0.49673202614379086, 22: 0.316008316008316, 24: 0.0, 25: 0.45, 26: 0.676923076923077, 27: 0.0, 28: 0.06611570247933884, 29: 0.7222222222222222, 31: 0.0, 32: 0.5, 34: 0.1601489757914339, 35: 0.0880503144654088, 37: 0.055357142857142855, 38: 0.24427480916030533, 39: 0.125, 40: 0.1660377358490566}
Micro-average F1 score: 0.2484689413823272
Weighted-average F1 score: 0.22097043870828528
F1 score per class: {1: 0.09174311926605505, 2: 0.2916666666666667, 3: 0.2327790973871734, 6: 0.28169014084507044, 7: 0.04, 9: 0.3424657534246575, 11: 0.12371134020618557, 12: 0.20477815699658702, 14: 0.04054054054054054, 15: 0.4, 19: 0.4376731301939058, 22: 0.33783783783783783, 24: 0.07142857142857142, 25: 0.594059405940594, 26: 0.6384976525821596, 27: 0.0, 28: 0.06944444444444445, 29: 0.7022222222222222, 31: 0.0392156862745098, 32: 0.441340782122905, 34: 0.17594254937163376, 35: 0.10077519379844961, 37: 0.06525911708253358, 38: 0.20512820512820512, 39: 0.0, 40: 0.2139917695473251}
Micro-average F1 score: 0.2544266804194602
Weighted-average F1 score: 0.2377893398747461
F1 score per class: {1: 0.09905660377358491, 2: 0.35, 3: 0.22624434389140272, 6: 0.29411764705882354, 7: 0.037383177570093455, 9: 0.625, 11: 0.125, 12: 0.2007168458781362, 14: 0.05128205128205128, 15: 0.4444444444444444, 19: 0.46745562130177515, 22: 0.3151515151515151, 24: 0.0, 25: 0.5833333333333334, 26: 0.6600985221674877, 27: 0.0, 28: 0.06535947712418301, 29: 0.7058823529411765, 31: 0.05714285714285714, 32: 0.4583333333333333, 34: 0.16806722689075632, 35: 0.10256410256410256, 37: 0.05891472868217054, 38: 0.22641509433962265, 39: 0.0, 40: 0.1918819188191882}
Micro-average F1 score: 0.25366189901774944
Weighted-average F1 score: 0.23419591731473288
cur_acc_wo_na:  ['0.7458', '0.5551', '0.3513', '0.5528', '0.3724']
his_acc_wo_na:  ['0.7458', '0.6243', '0.5176', '0.4519', '0.3677']
cur_acc des_wo_na:  ['0.7428', '0.5092', '0.3389', '0.5469', '0.3365']
his_acc des_wo_na:  ['0.7428', '0.6116', '0.5230', '0.4698', '0.3671']
cur_acc rrf_wo_na:  ['0.7477', '0.5115', '0.3529', '0.5697', '0.3498']
his_acc rrf_wo_na:  ['0.7477', '0.6116', '0.5349', '0.4777', '0.3671']
cur_acc_w_na:  ['0.5986', '0.3865', '0.2849', '0.4024', '0.2423']
his_acc_w_na:  ['0.5986', '0.4367', '0.3879', '0.3271', '0.2485']
cur_acc des_w_na:  ['0.5972', '0.3429', '0.2795', '0.3927', '0.2173']
his_acc des_w_na:  ['0.5972', '0.4230', '0.3927', '0.3306', '0.2544']
cur_acc rrf_w_na:  ['0.6037', '0.3506', '0.2939', '0.4141', '0.2249']
his_acc rrf_w_na:  ['0.6037', '0.4251', '0.4036', '0.3400', '0.2537']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 86.0362311CurrentTrain: epoch  0, batch     1 | loss: 85.5085450CurrentTrain: epoch  0, batch     2 | loss: 139.7191247CurrentTrain: epoch  0, batch     3 | loss: 79.9572875CurrentTrain: epoch  1, batch     0 | loss: 127.1023367CurrentTrain: epoch  1, batch     1 | loss: 75.1899638CurrentTrain: epoch  1, batch     2 | loss: 83.0698109CurrentTrain: epoch  1, batch     3 | loss: 113.0934020CurrentTrain: epoch  2, batch     0 | loss: 104.2265898CurrentTrain: epoch  2, batch     1 | loss: 98.9814859CurrentTrain: epoch  2, batch     2 | loss: 69.2550282CurrentTrain: epoch  2, batch     3 | loss: 68.3820185CurrentTrain: epoch  3, batch     0 | loss: 79.8385250CurrentTrain: epoch  3, batch     1 | loss: 164.6515026CurrentTrain: epoch  3, batch     2 | loss: 66.1602605CurrentTrain: epoch  3, batch     3 | loss: 64.9080169CurrentTrain: epoch  4, batch     0 | loss: 93.4526863CurrentTrain: epoch  4, batch     1 | loss: 74.4631607CurrentTrain: epoch  4, batch     2 | loss: 79.6197898CurrentTrain: epoch  4, batch     3 | loss: 66.0849188CurrentTrain: epoch  5, batch     0 | loss: 72.8925697CurrentTrain: epoch  5, batch     1 | loss: 95.6562021CurrentTrain: epoch  5, batch     2 | loss: 76.1508942CurrentTrain: epoch  5, batch     3 | loss: 59.0003581CurrentTrain: epoch  6, batch     0 | loss: 115.3872062CurrentTrain: epoch  6, batch     1 | loss: 70.6009597CurrentTrain: epoch  6, batch     2 | loss: 76.7578954CurrentTrain: epoch  6, batch     3 | loss: 60.5141217CurrentTrain: epoch  7, batch     0 | loss: 62.4021341CurrentTrain: epoch  7, batch     1 | loss: 75.4728235CurrentTrain: epoch  7, batch     2 | loss: 72.8305950CurrentTrain: epoch  7, batch     3 | loss: 58.8508408CurrentTrain: epoch  8, batch     0 | loss: 61.3222818CurrentTrain: epoch  8, batch     1 | loss: 60.9215029CurrentTrain: epoch  8, batch     2 | loss: 72.0181810CurrentTrain: epoch  8, batch     3 | loss: 125.4945871CurrentTrain: epoch  9, batch     0 | loss: 59.4230367CurrentTrain: epoch  9, batch     1 | loss: 61.4383938CurrentTrain: epoch  9, batch     2 | loss: 72.5712592CurrentTrain: epoch  9, batch     3 | loss: 58.9947223
MemoryTrain:  epoch  0, batch     0 | loss: 1.1807124MemoryTrain:  epoch  1, batch     0 | loss: 1.0085011MemoryTrain:  epoch  2, batch     0 | loss: 0.8123674MemoryTrain:  epoch  3, batch     0 | loss: 0.6716096MemoryTrain:  epoch  4, batch     0 | loss: 0.6491930MemoryTrain:  epoch  5, batch     0 | loss: 0.4744412MemoryTrain:  epoch  6, batch     0 | loss: 0.4117376MemoryTrain:  epoch  7, batch     0 | loss: 0.3646130MemoryTrain:  epoch  8, batch     0 | loss: 0.3016876MemoryTrain:  epoch  9, batch     0 | loss: 0.2947216

F1 score per class: {0: 0.875, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7096774193548387, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.43478260869565216, 15: 0.0, 19: 0.0, 21: 0.20512820512820512, 22: 0.0, 23: 0.6987951807228916, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.47058823529411764
Weighted-average F1 score: 0.34345986780273957
F1 score per class: {0: 0.6796116504854369, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7337278106508875, 6: 0.0, 7: 0.0, 9: 0.0, 12: 0.0, 13: 0.3448275862068966, 14: 0.0, 19: 0.0, 21: 0.4838709677419355, 22: 0.0, 23: 0.735632183908046, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.445440956651719
Weighted-average F1 score: 0.3300283681341591
F1 score per class: {0: 0.7346938775510204, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7757575757575758, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.3448275862068966, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.3508771929824561, 22: 0.0, 23: 0.735632183908046, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.459375
Weighted-average F1 score: 0.3382488699944902

F1 score per class: {0: 0.5109489051094891, 1: 0.17490494296577946, 2: 0.375, 3: 0.4881889763779528, 4: 0.7096774193548387, 6: 0.463768115942029, 7: 0.08163265306122448, 9: 0.7692307692307693, 11: 0.2857142857142857, 12: 0.096, 13: 0.09090909090909091, 14: 0.030303030303030304, 15: 0.5454545454545454, 19: 0.558303886925795, 21: 0.06896551724137931, 22: 0.4479495268138801, 23: 0.651685393258427, 24: 0.0, 25: 0.4857142857142857, 26: 0.7103825136612022, 27: 0.0, 28: 0.10526315789473684, 29: 0.8020833333333334, 31: 0.13333333333333333, 32: 0.6209677419354839, 34: 0.23417721518987342, 35: 0.08888888888888889, 37: 0.21212121212121213, 38: 0.463768115942029, 39: 0.25, 40: 0.23529411764705882}
Micro-average F1 score: 0.40208488458674607
Weighted-average F1 score: 0.3869020316974905
F1 score per class: {0: 0.2777777777777778, 1: 0.18181818181818182, 2: 0.22950819672131148, 3: 0.3016393442622951, 4: 0.7209302325581395, 6: 0.44357976653696496, 7: 0.07547169811320754, 9: 0.352112676056338, 11: 0.19801980198019803, 12: 0.23204419889502761, 13: 0.08333333333333333, 14: 0.05263157894736842, 15: 0.6, 19: 0.47398843930635837, 21: 0.16483516483516483, 22: 0.4626334519572954, 23: 0.6530612244897959, 24: 0.0, 25: 0.7058823529411765, 26: 0.717391304347826, 27: 0.0, 28: 0.1111111111111111, 29: 0.81, 31: 0.058823529411764705, 32: 0.564935064935065, 34: 0.2735562310030395, 35: 0.16494845360824742, 37: 0.09836065573770492, 38: 0.38596491228070173, 39: 0.0, 40: 0.25287356321839083}
Micro-average F1 score: 0.36224700536968196
Weighted-average F1 score: 0.3405193406188839
F1 score per class: {0: 0.33488372093023255, 1: 0.17692307692307693, 2: 0.34146341463414637, 3: 0.3372093023255814, 4: 0.7757575757575758, 6: 0.45901639344262296, 7: 0.07407407407407407, 9: 0.684931506849315, 11: 0.18, 12: 0.2485207100591716, 13: 0.08620689655172414, 14: 0.044444444444444446, 15: 0.5454545454545454, 19: 0.5110410094637224, 21: 0.10101010101010101, 22: 0.4253968253968254, 23: 0.6464646464646465, 24: 0.0, 25: 0.6666666666666666, 26: 0.7213114754098361, 27: 0.0, 28: 0.10256410256410256, 29: 0.8102564102564103, 31: 0.08333333333333333, 32: 0.5567010309278351, 34: 0.25882352941176473, 35: 0.12857142857142856, 37: 0.09950248756218906, 38: 0.4329896907216495, 39: 0.0, 40: 0.23157894736842105}
Micro-average F1 score: 0.3711163488322263
Weighted-average F1 score: 0.34857071482538377

F1 score per class: {0: 0.8235294117647058, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.6875, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.2564102564102564, 15: 0.0, 19: 0.0, 21: 0.12903225806451613, 22: 0.0, 23: 0.6236559139784946, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3575418994413408
Weighted-average F1 score: 0.24634179944690596
F1 score per class: {0: 0.5982905982905983, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7045454545454546, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.20833333333333334, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.3333333333333333, 22: 0.0, 23: 0.6153846153846154, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.32426550598476606
Weighted-average F1 score: 0.23620976464324103
F1 score per class: {0: 0.6605504587155964, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7485380116959064, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.18867924528301888, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.23809523809523808, 22: 0.0, 23: 0.64, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.33676975945017185
Weighted-average F1 score: 0.23935908614175316

F1 score per class: {0: 0.358974358974359, 1: 0.0954356846473029, 2: 0.24, 3: 0.35327635327635326, 4: 0.6790123456790124, 6: 0.29003021148036257, 7: 0.057971014492753624, 9: 0.684931506849315, 11: 0.2727272727272727, 12: 0.0784313725490196, 13: 0.0425531914893617, 14: 0.028169014084507043, 15: 0.3870967741935484, 19: 0.5, 21: 0.043010752688172046, 22: 0.33890214797136037, 23: 0.5576923076923077, 24: 0.0, 25: 0.4533333333333333, 26: 0.6467661691542289, 27: 0.0, 28: 0.08333333333333333, 29: 0.6875, 31: 0.06666666666666667, 32: 0.4502923976608187, 34: 0.15513626834381553, 35: 0.06153846153846154, 37: 0.16470588235294117, 38: 0.32653061224489793, 39: 0.25, 40: 0.19900497512437812}
Micro-average F1 score: 0.2964860907759883
Weighted-average F1 score: 0.274544467792697
F1 score per class: {0: 0.1876675603217158, 1: 0.09829059829059829, 2: 0.15384615384615385, 3: 0.2039911308203991, 4: 0.6666666666666666, 6: 0.25333333333333335, 7: 0.047058823529411764, 9: 0.2617801047120419, 11: 0.18867924528301888, 12: 0.16342412451361868, 13: 0.038314176245210725, 14: 0.04285714285714286, 15: 0.42857142857142855, 19: 0.38954869358669836, 21: 0.10526315789473684, 22: 0.3542234332425068, 23: 0.5039370078740157, 24: 0.0, 25: 0.5217391304347826, 26: 0.6439024390243903, 27: 0.0, 28: 0.07547169811320754, 29: 0.6721991701244814, 31: 0.02857142857142857, 32: 0.3972602739726027, 34: 0.17341040462427745, 35: 0.09785932721712538, 37: 0.075, 38: 0.23783783783783785, 39: 0.0, 40: 0.19555555555555557}
Micro-average F1 score: 0.25161382871897864
Weighted-average F1 score: 0.23264728347026223
F1 score per class: {0: 0.23003194888178913, 1: 0.09523809523809523, 2: 0.22950819672131148, 3: 0.22393822393822393, 4: 0.735632183908046, 6: 0.26540284360189575, 7: 0.045454545454545456, 9: 0.5952380952380952, 11: 0.17307692307692307, 12: 0.1693548387096774, 13: 0.03745318352059925, 14: 0.037383177570093455, 15: 0.35294117647058826, 19: 0.432, 21: 0.06430868167202572, 22: 0.32524271844660196, 23: 0.5203252032520326, 24: 0.0, 25: 0.5346534653465347, 26: 0.6502463054187192, 27: 0.0, 28: 0.07142857142857142, 29: 0.6899563318777293, 31: 0.04081632653061224, 32: 0.39901477832512317, 34: 0.1647940074906367, 35: 0.07468879668049792, 37: 0.07874015748031496, 38: 0.2781456953642384, 39: 0.0, 40: 0.18032786885245902}
Micro-average F1 score: 0.26009911398107827
Weighted-average F1 score: 0.23843471218515896
cur_acc_wo_na:  ['0.7458', '0.5551', '0.3513', '0.5528', '0.3724', '0.4706']
his_acc_wo_na:  ['0.7458', '0.6243', '0.5176', '0.4519', '0.3677', '0.4021']
cur_acc des_wo_na:  ['0.7428', '0.5092', '0.3389', '0.5469', '0.3365', '0.4454']
his_acc des_wo_na:  ['0.7428', '0.6116', '0.5230', '0.4698', '0.3671', '0.3622']
cur_acc rrf_wo_na:  ['0.7477', '0.5115', '0.3529', '0.5697', '0.3498', '0.4594']
his_acc rrf_wo_na:  ['0.7477', '0.6116', '0.5349', '0.4777', '0.3671', '0.3711']
cur_acc_w_na:  ['0.5986', '0.3865', '0.2849', '0.4024', '0.2423', '0.3575']
his_acc_w_na:  ['0.5986', '0.4367', '0.3879', '0.3271', '0.2485', '0.2965']
cur_acc des_w_na:  ['0.5972', '0.3429', '0.2795', '0.3927', '0.2173', '0.3243']
his_acc des_w_na:  ['0.5972', '0.4230', '0.3927', '0.3306', '0.2544', '0.2516']
cur_acc rrf_w_na:  ['0.6037', '0.3506', '0.2939', '0.4141', '0.2249', '0.3368']
his_acc rrf_w_na:  ['0.6037', '0.4251', '0.4036', '0.3400', '0.2537', '0.2601']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 104.9117448CurrentTrain: epoch  0, batch     1 | loss: 94.6966870CurrentTrain: epoch  0, batch     2 | loss: 85.8163647CurrentTrain: epoch  0, batch     3 | loss: 82.6083902CurrentTrain: epoch  1, batch     0 | loss: 101.0280562CurrentTrain: epoch  1, batch     1 | loss: 88.3077320CurrentTrain: epoch  1, batch     2 | loss: 72.7954577CurrentTrain: epoch  1, batch     3 | loss: 45.9702950CurrentTrain: epoch  2, batch     0 | loss: 69.0431737CurrentTrain: epoch  2, batch     1 | loss: 77.4492765CurrentTrain: epoch  2, batch     2 | loss: 80.1630130CurrentTrain: epoch  2, batch     3 | loss: 55.3288204CurrentTrain: epoch  3, batch     0 | loss: 64.1683215CurrentTrain: epoch  3, batch     1 | loss: 77.6901024CurrentTrain: epoch  3, batch     2 | loss: 64.1523276CurrentTrain: epoch  3, batch     3 | loss: 55.3236742CurrentTrain: epoch  4, batch     0 | loss: 116.9954454CurrentTrain: epoch  4, batch     1 | loss: 75.8560797CurrentTrain: epoch  4, batch     2 | loss: 62.6118436CurrentTrain: epoch  4, batch     3 | loss: 43.3118797CurrentTrain: epoch  5, batch     0 | loss: 91.7829090CurrentTrain: epoch  5, batch     1 | loss: 65.0352045CurrentTrain: epoch  5, batch     2 | loss: 87.4514872CurrentTrain: epoch  5, batch     3 | loss: 42.4592035CurrentTrain: epoch  6, batch     0 | loss: 60.2787586CurrentTrain: epoch  6, batch     1 | loss: 92.0160755CurrentTrain: epoch  6, batch     2 | loss: 87.1949751CurrentTrain: epoch  6, batch     3 | loss: 50.8803944CurrentTrain: epoch  7, batch     0 | loss: 58.7644421CurrentTrain: epoch  7, batch     1 | loss: 69.4677562CurrentTrain: epoch  7, batch     2 | loss: 75.3963161CurrentTrain: epoch  7, batch     3 | loss: 41.8284808CurrentTrain: epoch  8, batch     0 | loss: 70.0901856CurrentTrain: epoch  8, batch     1 | loss: 70.9822449CurrentTrain: epoch  8, batch     2 | loss: 71.4840150CurrentTrain: epoch  8, batch     3 | loss: 51.7575859CurrentTrain: epoch  9, batch     0 | loss: 69.2659202CurrentTrain: epoch  9, batch     1 | loss: 68.2869732CurrentTrain: epoch  9, batch     2 | loss: 57.6133484CurrentTrain: epoch  9, batch     3 | loss: 65.7794707
MemoryTrain:  epoch  0, batch     0 | loss: 0.7231731MemoryTrain:  epoch  1, batch     0 | loss: 0.5985942MemoryTrain:  epoch  2, batch     0 | loss: 0.4598421MemoryTrain:  epoch  3, batch     0 | loss: 0.4115355MemoryTrain:  epoch  4, batch     0 | loss: 0.3479673MemoryTrain:  epoch  5, batch     0 | loss: 0.2988881MemoryTrain:  epoch  6, batch     0 | loss: 0.2704879MemoryTrain:  epoch  7, batch     0 | loss: 0.2033571MemoryTrain:  epoch  8, batch     0 | loss: 0.1889316MemoryTrain:  epoch  9, batch     0 | loss: 0.1739519

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 6: 0.0, 7: 0.0, 8: 0.6702702702702703, 12: 0.0, 13: 0.0, 19: 0.0, 20: 0.8521739130434782, 22: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8333333333333334, 32: 0.0, 33: 0.4444444444444444, 34: 0.0, 35: 0.0, 36: 0.4367816091954023, 37: 0.0, 40: 0.0}
Micro-average F1 score: 0.5559701492537313
Weighted-average F1 score: 0.4857078963103001
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.6125, 9: 0.0, 12: 0.0, 13: 0.0, 19: 0.0, 20: 0.7155963302752294, 21: 0.0, 22: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.85, 31: 0.0, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 35: 0.0, 36: 0.6727272727272727, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.506993006993007
Weighted-average F1 score: 0.40387570074666074
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.6134969325153374, 12: 0.0, 13: 0.0, 19: 0.0, 20: 0.7155963302752294, 22: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8421052631578947, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 35: 0.0, 36: 0.5263157894736842, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.47927927927927927
Weighted-average F1 score: 0.38103092690001794

F1 score per class: {0: 0.41975308641975306, 1: 0.17391304347826086, 2: 0.48, 3: 0.3956043956043956, 4: 0.8165680473372781, 6: 0.37575757575757573, 7: 0.0, 8: 0.2250453720508167, 9: 0.7246376811594203, 11: 0.06593406593406594, 12: 0.078125, 13: 0.08888888888888889, 14: 0.0, 15: 0.5714285714285714, 19: 0.5288135593220339, 20: 0.4537037037037037, 21: 0.10852713178294573, 22: 0.5425101214574899, 23: 0.6813186813186813, 24: 0.0, 25: 0.3225806451612903, 26: 0.6881720430107527, 27: 0.0, 28: 0.18181818181818182, 29: 0.8041237113402062, 30: 0.8108108108108109, 31: 0.15384615384615385, 32: 0.6015625, 33: 0.125, 34: 0.24242424242424243, 35: 0.16666666666666666, 36: 0.40860215053763443, 37: 0.25, 38: 0.1935483870967742, 39: 0.0, 40: 0.24489795918367346}
Micro-average F1 score: 0.38009438009438007
Weighted-average F1 score: 0.3687454956196187
F1 score per class: {0: 0.2821576763485477, 1: 0.1862348178137652, 2: 0.2692307692307692, 3: 0.3408360128617363, 4: 0.8314606741573034, 6: 0.49361702127659574, 7: 0.04081632653061224, 8: 0.2857142857142857, 9: 0.33557046979865773, 11: 0.14, 12: 0.10687022900763359, 13: 0.0784313725490196, 14: 0.07228915662650602, 15: 0.7058823529411765, 19: 0.44327176781002636, 20: 0.430939226519337, 21: 0.11612903225806452, 22: 0.4855072463768116, 23: 0.6868686868686869, 24: 0.0, 25: 0.39436619718309857, 26: 0.6989247311827957, 27: 0.0, 28: 0.13333333333333333, 29: 0.8115942028985508, 30: 0.5483870967741935, 31: 0.045454545454545456, 32: 0.5253731343283582, 33: 0.075, 34: 0.33962264150943394, 35: 0.12173913043478261, 36: 0.578125, 37: 0.16666666666666666, 38: 0.5, 39: 0.21052631578947367, 40: 0.15625}
Micro-average F1 score: 0.3740705433746425
Weighted-average F1 score: 0.36020257296132724
F1 score per class: {0: 0.3008849557522124, 1: 0.1776061776061776, 2: 0.35, 3: 0.37453183520599254, 4: 0.8439306358381503, 6: 0.44878048780487806, 7: 0.04081632653061224, 8: 0.2638522427440633, 9: 0.6756756756756757, 11: 0.08333333333333333, 12: 0.10144927536231885, 13: 0.08888888888888889, 14: 0.07894736842105263, 15: 0.631578947368421, 19: 0.48725212464589235, 20: 0.42162162162162165, 21: 0.11538461538461539, 22: 0.49146757679180886, 23: 0.7096774193548387, 24: 0.0, 25: 0.3283582089552239, 26: 0.6989247311827957, 27: 0.0, 28: 0.13333333333333333, 29: 0.8078817733990148, 30: 0.5614035087719298, 31: 0.0625, 32: 0.5530546623794212, 33: 0.07792207792207792, 34: 0.3231441048034934, 35: 0.15135135135135136, 36: 0.4807692307692308, 37: 0.16666666666666666, 38: 0.5106382978723404, 39: 0.2222222222222222, 40: 0.16541353383458646}
Micro-average F1 score: 0.3802845121218193
Weighted-average F1 score: 0.3683356796701543

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.46441947565543074, 12: 0.0, 13: 0.0, 14: 0.0, 19: 0.0, 20: 0.6163522012578616, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.7894736842105263, 31: 0.0, 32: 0.0, 33: 0.34782608695652173, 34: 0.0, 35: 0.0, 36: 0.36893203883495146, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.37626262626262624
Weighted-average F1 score: 0.3278492015763941
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 6: 0.0, 7: 0.0, 8: 0.4558139534883721, 9: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 19: 0.0, 20: 0.527027027027027, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.7906976744186046, 31: 0.0, 32: 0.0, 33: 0.25, 34: 0.0, 35: 0.0, 36: 0.5873015873015873, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3395784543325527
Weighted-average F1 score: 0.2697984746904079
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.4424778761061947, 12: 0.0, 13: 0.0, 14: 0.0, 19: 0.0, 20: 0.5234899328859061, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8, 31: 0.0, 32: 0.0, 33: 0.25, 34: 0.0, 35: 0.0, 36: 0.45454545454545453, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.32399512789281365
Weighted-average F1 score: 0.26091667060942253

F1 score per class: {0: 0.2869198312236287, 1: 0.09448818897637795, 2: 0.26666666666666666, 3: 0.2823529411764706, 4: 0.7796610169491526, 6: 0.23574144486692014, 7: 0.0, 8: 0.11957569913211186, 9: 0.6493506493506493, 11: 0.06593406593406594, 12: 0.06711409395973154, 13: 0.04040404040404041, 14: 0.0, 15: 0.4, 19: 0.48, 20: 0.2481012658227848, 21: 0.07106598984771574, 22: 0.42948717948717946, 23: 0.5688073394495413, 24: 0.0, 25: 0.3076923076923077, 26: 0.6213592233009708, 27: 0.0, 28: 0.15384615384615385, 29: 0.6753246753246753, 30: 0.6382978723404256, 31: 0.06896551724137931, 32: 0.4438040345821326, 33: 0.08421052631578947, 34: 0.16494845360824742, 35: 0.1095890410958904, 36: 0.2992125984251969, 37: 0.20606060606060606, 38: 0.17142857142857143, 39: 0.0, 40: 0.20930232558139536}
Micro-average F1 score: 0.26702833031946954
Weighted-average F1 score: 0.24703316173748902
F1 score per class: {0: 0.1883656509695291, 1: 0.10043668122270742, 2: 0.16666666666666666, 3: 0.2231578947368421, 4: 0.7956989247311828, 6: 0.28501228501228504, 7: 0.029850746268656716, 8: 0.15555555555555556, 9: 0.24509803921568626, 11: 0.13725490196078433, 12: 0.0880503144654088, 13: 0.03333333333333333, 14: 0.06382978723404255, 15: 0.5, 19: 0.37168141592920356, 20: 0.24605678233438485, 21: 0.07317073170731707, 22: 0.37640449438202245, 23: 0.544, 24: 0.0, 25: 0.3783783783783784, 26: 0.6220095693779905, 27: 0.0, 28: 0.08695652173913043, 29: 0.6640316205533597, 30: 0.3953488372093023, 31: 0.022727272727272728, 32: 0.3697478991596639, 33: 0.049586776859504134, 34: 0.22784810126582278, 35: 0.07088607594936709, 36: 0.4277456647398844, 37: 0.12716763005780346, 38: 0.3333333333333333, 39: 0.14285714285714285, 40: 0.12987012987012986}
Micro-average F1 score: 0.25829383886255924
Weighted-average F1 score: 0.24199208287857732
F1 score per class: {0: 0.19767441860465115, 1: 0.0968421052631579, 2: 0.2153846153846154, 3: 0.24330900243309003, 4: 0.8156424581005587, 6: 0.27218934911242604, 7: 0.028985507246376812, 8: 0.13986013986013987, 9: 0.5747126436781609, 11: 0.08247422680412371, 12: 0.08383233532934131, 13: 0.037383177570093455, 14: 0.07058823529411765, 15: 0.4444444444444444, 19: 0.42053789731051344, 20: 0.2422360248447205, 21: 0.07142857142857142, 22: 0.3923705722070845, 23: 0.5739130434782609, 24: 0.0, 25: 0.3142857142857143, 26: 0.625, 27: 0.0, 28: 0.10526315789473684, 29: 0.6666666666666666, 30: 0.3855421686746988, 31: 0.03125, 32: 0.39631336405529954, 33: 0.05084745762711865, 34: 0.2132564841498559, 35: 0.09003215434083602, 36: 0.3597122302158273, 37: 0.1348314606741573, 38: 0.34782608695652173, 39: 0.18181818181818182, 40: 0.1375}
Micro-average F1 score: 0.2649357900614182
Weighted-average F1 score: 0.24772677384832442
cur_acc_wo_na:  ['0.7458', '0.5551', '0.3513', '0.5528', '0.3724', '0.4706', '0.5560']
his_acc_wo_na:  ['0.7458', '0.6243', '0.5176', '0.4519', '0.3677', '0.4021', '0.3801']
cur_acc des_wo_na:  ['0.7428', '0.5092', '0.3389', '0.5469', '0.3365', '0.4454', '0.5070']
his_acc des_wo_na:  ['0.7428', '0.6116', '0.5230', '0.4698', '0.3671', '0.3622', '0.3741']
cur_acc rrf_wo_na:  ['0.7477', '0.5115', '0.3529', '0.5697', '0.3498', '0.4594', '0.4793']
his_acc rrf_wo_na:  ['0.7477', '0.6116', '0.5349', '0.4777', '0.3671', '0.3711', '0.3803']
cur_acc_w_na:  ['0.5986', '0.3865', '0.2849', '0.4024', '0.2423', '0.3575', '0.3763']
his_acc_w_na:  ['0.5986', '0.4367', '0.3879', '0.3271', '0.2485', '0.2965', '0.2670']
cur_acc des_w_na:  ['0.5972', '0.3429', '0.2795', '0.3927', '0.2173', '0.3243', '0.3396']
his_acc des_w_na:  ['0.5972', '0.4230', '0.3927', '0.3306', '0.2544', '0.2516', '0.2583']
cur_acc rrf_w_na:  ['0.6037', '0.3506', '0.2939', '0.4141', '0.2249', '0.3368', '0.3240']
his_acc rrf_w_na:  ['0.6037', '0.4251', '0.4036', '0.3400', '0.2537', '0.2601', '0.2649']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 122.4394234CurrentTrain: epoch  0, batch     1 | loss: 140.8493188CurrentTrain: epoch  0, batch     2 | loss: 111.8807089CurrentTrain: epoch  0, batch     3 | loss: 80.9271693CurrentTrain: epoch  0, batch     4 | loss: 60.9096110CurrentTrain: epoch  1, batch     0 | loss: 87.2157312CurrentTrain: epoch  1, batch     1 | loss: 88.3963364CurrentTrain: epoch  1, batch     2 | loss: 104.5995621CurrentTrain: epoch  1, batch     3 | loss: 124.6738089CurrentTrain: epoch  1, batch     4 | loss: 106.0352013CurrentTrain: epoch  2, batch     0 | loss: 105.9676075CurrentTrain: epoch  2, batch     1 | loss: 122.1925843CurrentTrain: epoch  2, batch     2 | loss: 96.7065741CurrentTrain: epoch  2, batch     3 | loss: 98.0711537CurrentTrain: epoch  2, batch     4 | loss: 50.2994512CurrentTrain: epoch  3, batch     0 | loss: 76.5210078CurrentTrain: epoch  3, batch     1 | loss: 97.9220835CurrentTrain: epoch  3, batch     2 | loss: 78.8202823CurrentTrain: epoch  3, batch     3 | loss: 95.7766818CurrentTrain: epoch  3, batch     4 | loss: 83.3215648CurrentTrain: epoch  4, batch     0 | loss: 77.5033500CurrentTrain: epoch  4, batch     1 | loss: 66.1353722CurrentTrain: epoch  4, batch     2 | loss: 97.3178508CurrentTrain: epoch  4, batch     3 | loss: 74.5585914CurrentTrain: epoch  4, batch     4 | loss: 155.7972127CurrentTrain: epoch  5, batch     0 | loss: 74.0811376CurrentTrain: epoch  5, batch     1 | loss: 76.7810281CurrentTrain: epoch  5, batch     2 | loss: 76.0060013CurrentTrain: epoch  5, batch     3 | loss: 94.1945888CurrentTrain: epoch  5, batch     4 | loss: 47.2012385CurrentTrain: epoch  6, batch     0 | loss: 73.7350899CurrentTrain: epoch  6, batch     1 | loss: 76.2400483CurrentTrain: epoch  6, batch     2 | loss: 63.0508319CurrentTrain: epoch  6, batch     3 | loss: 89.7095632CurrentTrain: epoch  6, batch     4 | loss: 72.9898125CurrentTrain: epoch  7, batch     0 | loss: 88.0456144CurrentTrain: epoch  7, batch     1 | loss: 71.8261405CurrentTrain: epoch  7, batch     2 | loss: 117.7932566CurrentTrain: epoch  7, batch     3 | loss: 60.9513033CurrentTrain: epoch  7, batch     4 | loss: 55.4497953CurrentTrain: epoch  8, batch     0 | loss: 88.6582417CurrentTrain: epoch  8, batch     1 | loss: 71.4914020CurrentTrain: epoch  8, batch     2 | loss: 87.9214741CurrentTrain: epoch  8, batch     3 | loss: 71.2341273CurrentTrain: epoch  8, batch     4 | loss: 46.2922686CurrentTrain: epoch  9, batch     0 | loss: 88.0204652CurrentTrain: epoch  9, batch     1 | loss: 84.8673103CurrentTrain: epoch  9, batch     2 | loss: 68.5647573CurrentTrain: epoch  9, batch     3 | loss: 88.9612298CurrentTrain: epoch  9, batch     4 | loss: 53.8058392
MemoryTrain:  epoch  0, batch     0 | loss: 0.9094421MemoryTrain:  epoch  1, batch     0 | loss: 0.7665749MemoryTrain:  epoch  2, batch     0 | loss: 0.5580203MemoryTrain:  epoch  3, batch     0 | loss: 0.4592366MemoryTrain:  epoch  4, batch     0 | loss: 0.3520326MemoryTrain:  epoch  5, batch     0 | loss: 0.3269533MemoryTrain:  epoch  6, batch     0 | loss: 0.2889302MemoryTrain:  epoch  7, batch     0 | loss: 0.2191510MemoryTrain:  epoch  8, batch     0 | loss: 0.1978954MemoryTrain:  epoch  9, batch     0 | loss: 0.1761083

F1 score per class: {0: 0.0, 2: 0.0, 3: 0.0, 5: 0.8792270531400966, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.4857142857142857, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.7169811320754716, 17: 0.6153846153846154, 18: 0.24324324324324326, 20: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 33: 0.0, 34: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.513911620294599
Weighted-average F1 score: 0.4222625988472719
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.7245283018867924, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.5675675675675675, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.684931506849315, 17: 0.7142857142857143, 18: 0.24, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.488
Weighted-average F1 score: 0.42391117388293903
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.7550200803212851, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.5753424657534246, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.7272727272727273, 17: 0.7142857142857143, 18: 0.29411764705882354, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.5077574047954866
Weighted-average F1 score: 0.433798242966742

F1 score per class: {0: 0.425, 1: 0.17582417582417584, 2: 0.48, 3: 0.44324324324324327, 4: 0.717948717948718, 5: 0.7947598253275109, 6: 0.275, 7: 0.0, 8: 0.18009478672985782, 9: 0.684931506849315, 10: 0.2956521739130435, 11: 0.08602150537634409, 12: 0.03636363636363636, 13: 0.08333333333333333, 14: 0.0, 15: 0.5217391304347826, 16: 0.6333333333333333, 17: 0.5, 18: 0.1276595744680851, 19: 0.5165562913907285, 20: 0.38247011952191234, 21: 0.12598425196850394, 22: 0.496, 23: 0.651685393258427, 24: 0.0, 25: 0.3225806451612903, 26: 0.6772486772486772, 27: 0.0, 28: 0.2, 29: 0.7875647668393783, 30: 0.7777777777777778, 31: 0.2222222222222222, 32: 0.6033057851239669, 33: 0.11538461538461539, 34: 0.12861736334405144, 35: 0.203125, 36: 0.1388888888888889, 37: 0.1836734693877551, 38: 0.1875, 39: 0.2, 40: 0.17834394904458598}
Micro-average F1 score: 0.3661591828868761
Weighted-average F1 score: 0.3567518746861729
F1 score per class: {0: 0.3225806451612903, 1: 0.1640625, 2: 0.2641509433962264, 3: 0.30036630036630035, 4: 0.8043478260869565, 5: 0.56973293768546, 6: 0.4602510460251046, 7: 0.03333333333333333, 8: 0.30414746543778803, 9: 0.25252525252525254, 10: 0.35294117647058826, 11: 0.34782608695652173, 12: 0.22666666666666666, 13: 0.10526315789473684, 14: 0.04938271604938271, 15: 0.6666666666666666, 16: 0.5154639175257731, 17: 0.3125, 18: 0.12605042016806722, 19: 0.42105263157894735, 20: 0.37037037037037035, 21: 0.15267175572519084, 22: 0.42424242424242425, 23: 0.6355140186915887, 24: 0.0, 25: 0.39436619718309857, 26: 0.694300518134715, 27: 0.0, 28: 0.14814814814814814, 29: 0.7777777777777778, 30: 0.5151515151515151, 31: 0.06451612903225806, 32: 0.5102639296187683, 33: 0.10204081632653061, 34: 0.2639593908629442, 35: 0.14754098360655737, 36: 0.4716981132075472, 37: 0.08053691275167785, 38: 0.3018867924528302, 39: 0.14285714285714285, 40: 0.1323529411764706}
Micro-average F1 score: 0.36299047311480703
Weighted-average F1 score: 0.34824752449384316
F1 score per class: {0: 0.32710280373831774, 1: 0.16541353383458646, 2: 0.3333333333333333, 3: 0.32061068702290074, 4: 0.8235294117647058, 5: 0.618421052631579, 6: 0.3961352657004831, 7: 0.0, 8: 0.277992277992278, 9: 0.6024096385542169, 10: 0.3605150214592275, 11: 0.1782178217821782, 12: 0.23129251700680273, 13: 0.10810810810810811, 14: 0.0273972602739726, 15: 0.631578947368421, 16: 0.5714285714285714, 17: 0.4, 18: 0.13636363636363635, 19: 0.4702702702702703, 20: 0.34893617021276596, 21: 0.1509433962264151, 22: 0.4605263157894737, 23: 0.6666666666666666, 24: 0.0, 25: 0.3333333333333333, 26: 0.6979166666666666, 27: 0.0, 28: 0.08333333333333333, 29: 0.8076923076923077, 30: 0.5714285714285714, 31: 0.13333333333333333, 32: 0.5478547854785478, 33: 0.11764705882352941, 34: 0.216, 35: 0.14814814814814814, 36: 0.37209302325581395, 37: 0.08633093525179857, 38: 0.27906976744186046, 39: 0.18181818181818182, 40: 0.12903225806451613}
Micro-average F1 score: 0.37121859511194666
Weighted-average F1 score: 0.35937969588320373

F1 score per class: {0: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.7398373983739838, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.422360248447205, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.48717948717948717, 17: 0.5714285714285714, 18: 0.1875, 20: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 33: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.3405639913232104
Weighted-average F1 score: 0.264836677336903
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.5565217391304348, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.49122807017543857, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.43478260869565216, 17: 0.5263157894736842, 18: 0.19480519480519481, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.31069609507640067
Weighted-average F1 score: 0.2619034440083013
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.5802469135802469, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.5029940119760479, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.46601941747572817, 17: 0.5263157894736842, 18: 0.22388059701492538, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.325497287522604
Weighted-average F1 score: 0.27069265147624494

F1 score per class: {0: 0.29694323144104806, 1: 0.0963855421686747, 2: 0.2553191489361702, 3: 0.311787072243346, 4: 0.6871165644171779, 5: 0.621160409556314, 6: 0.17670682730923695, 7: 0.0, 8: 0.0927960927960928, 9: 0.5952380952380952, 10: 0.2305084745762712, 11: 0.0851063829787234, 12: 0.034782608695652174, 13: 0.047058823529411764, 14: 0.0, 15: 0.4, 16: 0.4222222222222222, 17: 0.32, 18: 0.08530805687203792, 19: 0.45481049562682213, 20: 0.192, 21: 0.07881773399014778, 22: 0.3875, 23: 0.5686274509803921, 24: 0.0, 25: 0.3125, 26: 0.6037735849056604, 27: 0.0, 28: 0.18181818181818182, 29: 0.6468085106382979, 30: 0.6363636363636364, 31: 0.09090909090909091, 32: 0.44648318042813456, 33: 0.07792207792207792, 34: 0.08097165991902834, 35: 0.14130434782608695, 36: 0.12345679012345678, 37: 0.1487603305785124, 38: 0.16666666666666666, 39: 0.16, 40: 0.15300546448087432}
Micro-average F1 score: 0.2581171036543948
Weighted-average F1 score: 0.24014243926609113
F1 score per class: {0: 0.22082018927444794, 1: 0.08823529411764706, 2: 0.1590909090909091, 3: 0.19294117647058823, 4: 0.74, 5: 0.3582089552238806, 6: 0.2564102564102564, 7: 0.02197802197802198, 8: 0.1935483870967742, 9: 0.18248175182481752, 10: 0.24925816023738873, 11: 0.3305785123966942, 12: 0.16831683168316833, 13: 0.057971014492753624, 14: 0.04081632653061224, 15: 0.5217391304347826, 16: 0.30864197530864196, 17: 0.15873015873015872, 18: 0.08928571428571429, 19: 0.3380281690140845, 20: 0.19656019656019655, 21: 0.09615384615384616, 22: 0.314214463840399, 23: 0.48226950354609927, 24: 0.0, 25: 0.37333333333333335, 26: 0.6063348416289592, 27: 0.0, 28: 0.08, 29: 0.5915492957746479, 30: 0.3469387755102041, 31: 0.031746031746031744, 32: 0.35655737704918034, 33: 0.058823529411764705, 34: 0.14942528735632185, 35: 0.08955223880597014, 36: 0.31446540880503143, 37: 0.06091370558375635, 38: 0.1951219512195122, 39: 0.08888888888888889, 40: 0.09782608695652174}
Micro-average F1 score: 0.24544164210066602
Weighted-average F1 score: 0.23234076036914053
F1 score per class: {0: 0.21739130434782608, 1: 0.08888888888888889, 2: 0.2028985507246377, 3: 0.21374045801526717, 4: 0.7865168539325843, 5: 0.38923395445134573, 6: 0.22841225626740946, 7: 0.0, 8: 0.16941176470588235, 9: 0.49019607843137253, 10: 0.26006191950464397, 11: 0.17647058823529413, 12: 0.16831683168316833, 13: 0.05714285714285714, 14: 0.022988505747126436, 15: 0.48, 16: 0.34285714285714286, 17: 0.19230769230769232, 18: 0.09554140127388536, 19: 0.38752783964365256, 20: 0.18262806236080179, 21: 0.0975609756097561, 22: 0.35175879396984927, 23: 0.5151515151515151, 24: 0.0, 25: 0.3188405797101449, 26: 0.6118721461187214, 27: 0.0, 28: 0.047619047619047616, 29: 0.6387832699619772, 30: 0.3764705882352941, 31: 0.05405405405405406, 32: 0.39429928741092635, 33: 0.06493506493506493, 34: 0.1291866028708134, 35: 0.09061488673139159, 36: 0.2644628099173554, 37: 0.06779661016949153, 38: 0.21428571428571427, 39: 0.14814814814814814, 40: 0.0970873786407767}
Micro-average F1 score: 0.2538569424964937
Weighted-average F1 score: 0.24012985801262943
cur_acc_wo_na:  ['0.7458', '0.5551', '0.3513', '0.5528', '0.3724', '0.4706', '0.5560', '0.5139']
his_acc_wo_na:  ['0.7458', '0.6243', '0.5176', '0.4519', '0.3677', '0.4021', '0.3801', '0.3662']
cur_acc des_wo_na:  ['0.7428', '0.5092', '0.3389', '0.5469', '0.3365', '0.4454', '0.5070', '0.4880']
his_acc des_wo_na:  ['0.7428', '0.6116', '0.5230', '0.4698', '0.3671', '0.3622', '0.3741', '0.3630']
cur_acc rrf_wo_na:  ['0.7477', '0.5115', '0.3529', '0.5697', '0.3498', '0.4594', '0.4793', '0.5078']
his_acc rrf_wo_na:  ['0.7477', '0.6116', '0.5349', '0.4777', '0.3671', '0.3711', '0.3803', '0.3712']
cur_acc_w_na:  ['0.5986', '0.3865', '0.2849', '0.4024', '0.2423', '0.3575', '0.3763', '0.3406']
his_acc_w_na:  ['0.5986', '0.4367', '0.3879', '0.3271', '0.2485', '0.2965', '0.2670', '0.2581']
cur_acc des_w_na:  ['0.5972', '0.3429', '0.2795', '0.3927', '0.2173', '0.3243', '0.3396', '0.3107']
his_acc des_w_na:  ['0.5972', '0.4230', '0.3927', '0.3306', '0.2544', '0.2516', '0.2583', '0.2454']
cur_acc rrf_w_na:  ['0.6037', '0.3506', '0.2939', '0.4141', '0.2249', '0.3368', '0.3240', '0.3255']
his_acc rrf_w_na:  ['0.6037', '0.4251', '0.4036', '0.3400', '0.2537', '0.2601', '0.2649', '0.2539']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 131.3081318CurrentTrain: epoch  0, batch     1 | loss: 89.9767058CurrentTrain: epoch  0, batch     2 | loss: 78.6726511CurrentTrain: epoch  0, batch     3 | loss: 120.5099816CurrentTrain: epoch  0, batch     4 | loss: 77.6977887CurrentTrain: epoch  0, batch     5 | loss: 148.4032623CurrentTrain: epoch  0, batch     6 | loss: 88.0528676CurrentTrain: epoch  0, batch     7 | loss: 100.0316776CurrentTrain: epoch  0, batch     8 | loss: 86.2639516CurrentTrain: epoch  0, batch     9 | loss: 119.0347996CurrentTrain: epoch  0, batch    10 | loss: 90.1909250CurrentTrain: epoch  0, batch    11 | loss: 99.9218760CurrentTrain: epoch  0, batch    12 | loss: 86.2256681CurrentTrain: epoch  0, batch    13 | loss: 145.9632455CurrentTrain: epoch  0, batch    14 | loss: 99.9474790CurrentTrain: epoch  0, batch    15 | loss: 100.3445353CurrentTrain: epoch  0, batch    16 | loss: 99.0849473CurrentTrain: epoch  0, batch    17 | loss: 76.0368822CurrentTrain: epoch  0, batch    18 | loss: 86.8452081CurrentTrain: epoch  0, batch    19 | loss: 99.3505209CurrentTrain: epoch  0, batch    20 | loss: 118.9752942CurrentTrain: epoch  0, batch    21 | loss: 119.2444042CurrentTrain: epoch  0, batch    22 | loss: 99.1803504CurrentTrain: epoch  0, batch    23 | loss: 98.9088724CurrentTrain: epoch  0, batch    24 | loss: 86.3798750CurrentTrain: epoch  0, batch    25 | loss: 117.3425469CurrentTrain: epoch  0, batch    26 | loss: 99.0770607CurrentTrain: epoch  0, batch    27 | loss: 98.8475959CurrentTrain: epoch  0, batch    28 | loss: 85.7058884CurrentTrain: epoch  0, batch    29 | loss: 98.5464091CurrentTrain: epoch  0, batch    30 | loss: 98.3213830CurrentTrain: epoch  0, batch    31 | loss: 117.2504039CurrentTrain: epoch  0, batch    32 | loss: 83.7018302CurrentTrain: epoch  0, batch    33 | loss: 98.2122402CurrentTrain: epoch  0, batch    34 | loss: 117.1740770CurrentTrain: epoch  0, batch    35 | loss: 99.5736559CurrentTrain: epoch  0, batch    36 | loss: 98.6841360CurrentTrain: epoch  0, batch    37 | loss: 97.9256133CurrentTrain: epoch  0, batch    38 | loss: 84.5720298CurrentTrain: epoch  0, batch    39 | loss: 97.4005240CurrentTrain: epoch  0, batch    40 | loss: 144.8204370CurrentTrain: epoch  0, batch    41 | loss: 98.4863448CurrentTrain: epoch  0, batch    42 | loss: 96.5609291CurrentTrain: epoch  0, batch    43 | loss: 96.6140639CurrentTrain: epoch  0, batch    44 | loss: 114.2765304CurrentTrain: epoch  0, batch    45 | loss: 115.5915860CurrentTrain: epoch  0, batch    46 | loss: 141.2186715CurrentTrain: epoch  0, batch    47 | loss: 92.9675738CurrentTrain: epoch  0, batch    48 | loss: 82.3873014CurrentTrain: epoch  0, batch    49 | loss: 94.9755726CurrentTrain: epoch  0, batch    50 | loss: 72.4381301CurrentTrain: epoch  0, batch    51 | loss: 115.0055560CurrentTrain: epoch  0, batch    52 | loss: 81.5894557CurrentTrain: epoch  0, batch    53 | loss: 79.9592616CurrentTrain: epoch  0, batch    54 | loss: 72.8001518CurrentTrain: epoch  0, batch    55 | loss: 187.9663619CurrentTrain: epoch  0, batch    56 | loss: 140.2469508CurrentTrain: epoch  0, batch    57 | loss: 186.1654819CurrentTrain: epoch  0, batch    58 | loss: 110.5672861CurrentTrain: epoch  0, batch    59 | loss: 114.1356389CurrentTrain: epoch  0, batch    60 | loss: 135.1466968CurrentTrain: epoch  0, batch    61 | loss: 70.3708322CurrentTrain: epoch  0, batch    62 | loss: 185.5840265CurrentTrain: epoch  0, batch    63 | loss: 110.4298974CurrentTrain: epoch  0, batch    64 | loss: 79.3833453CurrentTrain: epoch  0, batch    65 | loss: 91.4731331CurrentTrain: epoch  0, batch    66 | loss: 131.2535391CurrentTrain: epoch  0, batch    67 | loss: 90.9300358CurrentTrain: epoch  0, batch    68 | loss: 80.6863562CurrentTrain: epoch  0, batch    69 | loss: 137.4538486CurrentTrain: epoch  0, batch    70 | loss: 91.8249126CurrentTrain: epoch  0, batch    71 | loss: 77.6867044CurrentTrain: epoch  0, batch    72 | loss: 92.6409848CurrentTrain: epoch  0, batch    73 | loss: 91.6008547CurrentTrain: epoch  0, batch    74 | loss: 111.2016044CurrentTrain: epoch  0, batch    75 | loss: 108.4972973CurrentTrain: epoch  0, batch    76 | loss: 135.5315062CurrentTrain: epoch  0, batch    77 | loss: 109.4710133CurrentTrain: epoch  0, batch    78 | loss: 106.7489461CurrentTrain: epoch  0, batch    79 | loss: 88.1865208CurrentTrain: epoch  0, batch    80 | loss: 79.5119983CurrentTrain: epoch  0, batch    81 | loss: 77.3077531CurrentTrain: epoch  0, batch    82 | loss: 88.4870177CurrentTrain: epoch  0, batch    83 | loss: 79.6049778CurrentTrain: epoch  0, batch    84 | loss: 76.7176349CurrentTrain: epoch  0, batch    85 | loss: 108.3354335CurrentTrain: epoch  0, batch    86 | loss: 76.5600593CurrentTrain: epoch  0, batch    87 | loss: 108.5084082CurrentTrain: epoch  0, batch    88 | loss: 73.3728720CurrentTrain: epoch  0, batch    89 | loss: 107.5866792CurrentTrain: epoch  0, batch    90 | loss: 87.4326977CurrentTrain: epoch  0, batch    91 | loss: 88.6200729CurrentTrain: epoch  0, batch    92 | loss: 85.9622244CurrentTrain: epoch  0, batch    93 | loss: 92.2923920CurrentTrain: epoch  0, batch    94 | loss: 132.4092932CurrentTrain: epoch  0, batch    95 | loss: 108.9018495CurrentTrain: epoch  1, batch     0 | loss: 87.1655043CurrentTrain: epoch  1, batch     1 | loss: 72.6204339CurrentTrain: epoch  1, batch     2 | loss: 109.1939192CurrentTrain: epoch  1, batch     3 | loss: 71.6365762CurrentTrain: epoch  1, batch     4 | loss: 103.8803950CurrentTrain: epoch  1, batch     5 | loss: 104.8984394CurrentTrain: epoch  1, batch     6 | loss: 127.2969704CurrentTrain: epoch  1, batch     7 | loss: 128.4369288CurrentTrain: epoch  1, batch     8 | loss: 137.9098506CurrentTrain: epoch  1, batch     9 | loss: 107.6515289CurrentTrain: epoch  1, batch    10 | loss: 88.4135460CurrentTrain: epoch  1, batch    11 | loss: 71.5734749CurrentTrain: epoch  1, batch    12 | loss: 107.3109702CurrentTrain: epoch  1, batch    13 | loss: 78.2211139CurrentTrain: epoch  1, batch    14 | loss: 85.4687777CurrentTrain: epoch  1, batch    15 | loss: 88.5092089CurrentTrain: epoch  1, batch    16 | loss: 103.3241094CurrentTrain: epoch  1, batch    17 | loss: 83.7224040CurrentTrain: epoch  1, batch    18 | loss: 84.6534266CurrentTrain: epoch  1, batch    19 | loss: 74.8982367CurrentTrain: epoch  1, batch    20 | loss: 74.5487556CurrentTrain: epoch  1, batch    21 | loss: 129.1441962CurrentTrain: epoch  1, batch    22 | loss: 69.5391396CurrentTrain: epoch  1, batch    23 | loss: 85.5703988CurrentTrain: epoch  1, batch    24 | loss: 71.0844009CurrentTrain: epoch  1, batch    25 | loss: 81.9422300CurrentTrain: epoch  1, batch    26 | loss: 84.2644467CurrentTrain: epoch  1, batch    27 | loss: 101.6192930CurrentTrain: epoch  1, batch    28 | loss: 131.2308173CurrentTrain: epoch  1, batch    29 | loss: 76.9481105CurrentTrain: epoch  1, batch    30 | loss: 83.7454659CurrentTrain: epoch  1, batch    31 | loss: 85.7114047CurrentTrain: epoch  1, batch    32 | loss: 73.8449441CurrentTrain: epoch  1, batch    33 | loss: 64.3580994CurrentTrain: epoch  1, batch    34 | loss: 129.7221507CurrentTrain: epoch  1, batch    35 | loss: 104.9101592CurrentTrain: epoch  1, batch    36 | loss: 73.0696937CurrentTrain: epoch  1, batch    37 | loss: 104.4962552CurrentTrain: epoch  1, batch    38 | loss: 136.2663608CurrentTrain: epoch  1, batch    39 | loss: 85.9891729CurrentTrain: epoch  1, batch    40 | loss: 82.0546546CurrentTrain: epoch  1, batch    41 | loss: 86.1818015CurrentTrain: epoch  1, batch    42 | loss: 103.7022663CurrentTrain: epoch  1, batch    43 | loss: 75.1174590CurrentTrain: epoch  1, batch    44 | loss: 101.6628077CurrentTrain: epoch  1, batch    45 | loss: 102.4261756CurrentTrain: epoch  1, batch    46 | loss: 175.0730237CurrentTrain: epoch  1, batch    47 | loss: 86.6140734CurrentTrain: epoch  1, batch    48 | loss: 88.4296998CurrentTrain: epoch  1, batch    49 | loss: 83.8038617CurrentTrain: epoch  1, batch    50 | loss: 101.0694686CurrentTrain: epoch  1, batch    51 | loss: 88.0718119CurrentTrain: epoch  1, batch    52 | loss: 128.9491969CurrentTrain: epoch  1, batch    53 | loss: 83.1850425CurrentTrain: epoch  1, batch    54 | loss: 82.4948786CurrentTrain: epoch  1, batch    55 | loss: 86.3186023CurrentTrain: epoch  1, batch    56 | loss: 72.4609092CurrentTrain: epoch  1, batch    57 | loss: 70.1751500CurrentTrain: epoch  1, batch    58 | loss: 104.0857633CurrentTrain: epoch  1, batch    59 | loss: 98.9330786CurrentTrain: epoch  1, batch    60 | loss: 70.3585238CurrentTrain: epoch  1, batch    61 | loss: 61.0706733CurrentTrain: epoch  1, batch    62 | loss: 104.8107299CurrentTrain: epoch  1, batch    63 | loss: 110.4278688CurrentTrain: epoch  1, batch    64 | loss: 73.7396661CurrentTrain: epoch  1, batch    65 | loss: 83.7209985CurrentTrain: epoch  1, batch    66 | loss: 101.3165089CurrentTrain: epoch  1, batch    67 | loss: 68.8421343CurrentTrain: epoch  1, batch    68 | loss: 84.8217319CurrentTrain: epoch  1, batch    69 | loss: 72.4117770CurrentTrain: epoch  1, batch    70 | loss: 104.4519969CurrentTrain: epoch  1, batch    71 | loss: 71.3647132CurrentTrain: epoch  1, batch    72 | loss: 101.5305131CurrentTrain: epoch  1, batch    73 | loss: 80.6662466CurrentTrain: epoch  1, batch    74 | loss: 82.1011335CurrentTrain: epoch  1, batch    75 | loss: 71.6077062CurrentTrain: epoch  1, batch    76 | loss: 82.6418891CurrentTrain: epoch  1, batch    77 | loss: 68.6089346CurrentTrain: epoch  1, batch    78 | loss: 85.8803897CurrentTrain: epoch  1, batch    79 | loss: 100.4522817CurrentTrain: epoch  1, batch    80 | loss: 82.4179742CurrentTrain: epoch  1, batch    81 | loss: 101.6628882CurrentTrain: epoch  1, batch    82 | loss: 75.6863756CurrentTrain: epoch  1, batch    83 | loss: 82.4873889CurrentTrain: epoch  1, batch    84 | loss: 72.4360460CurrentTrain: epoch  1, batch    85 | loss: 74.6382567CurrentTrain: epoch  1, batch    86 | loss: 84.5898485CurrentTrain: epoch  1, batch    87 | loss: 72.7840756CurrentTrain: epoch  1, batch    88 | loss: 96.9442332CurrentTrain: epoch  1, batch    89 | loss: 84.8939756CurrentTrain: epoch  1, batch    90 | loss: 74.4428424CurrentTrain: epoch  1, batch    91 | loss: 86.5650770CurrentTrain: epoch  1, batch    92 | loss: 85.2636317CurrentTrain: epoch  1, batch    93 | loss: 82.2969630CurrentTrain: epoch  1, batch    94 | loss: 86.0016638CurrentTrain: epoch  1, batch    95 | loss: 68.0156328CurrentTrain: epoch  2, batch     0 | loss: 61.8596705CurrentTrain: epoch  2, batch     1 | loss: 61.4201184CurrentTrain: epoch  2, batch     2 | loss: 78.5349651CurrentTrain: epoch  2, batch     3 | loss: 70.0440097CurrentTrain: epoch  2, batch     4 | loss: 58.2542620CurrentTrain: epoch  2, batch     5 | loss: 98.5808517CurrentTrain: epoch  2, batch     6 | loss: 72.0617158CurrentTrain: epoch  2, batch     7 | loss: 83.0804773CurrentTrain: epoch  2, batch     8 | loss: 127.3306665CurrentTrain: epoch  2, batch     9 | loss: 81.1277005CurrentTrain: epoch  2, batch    10 | loss: 78.0860499CurrentTrain: epoch  2, batch    11 | loss: 71.5362804CurrentTrain: epoch  2, batch    12 | loss: 60.9819008CurrentTrain: epoch  2, batch    13 | loss: 98.6272597CurrentTrain: epoch  2, batch    14 | loss: 80.1232876CurrentTrain: epoch  2, batch    15 | loss: 96.6329714CurrentTrain: epoch  2, batch    16 | loss: 59.6460952CurrentTrain: epoch  2, batch    17 | loss: 97.1715975CurrentTrain: epoch  2, batch    18 | loss: 122.5584436CurrentTrain: epoch  2, batch    19 | loss: 82.7230620CurrentTrain: epoch  2, batch    20 | loss: 82.7650218CurrentTrain: epoch  2, batch    21 | loss: 98.2173328CurrentTrain: epoch  2, batch    22 | loss: 88.1836992CurrentTrain: epoch  2, batch    23 | loss: 59.2779567CurrentTrain: epoch  2, batch    24 | loss: 59.1162130CurrentTrain: epoch  2, batch    25 | loss: 83.0023922CurrentTrain: epoch  2, batch    26 | loss: 65.7087628CurrentTrain: epoch  2, batch    27 | loss: 79.2417379CurrentTrain: epoch  2, batch    28 | loss: 72.9474560CurrentTrain: epoch  2, batch    29 | loss: 83.0440830CurrentTrain: epoch  2, batch    30 | loss: 81.6737288CurrentTrain: epoch  2, batch    31 | loss: 82.0501195CurrentTrain: epoch  2, batch    32 | loss: 104.6086200CurrentTrain: epoch  2, batch    33 | loss: 125.7638304CurrentTrain: epoch  2, batch    34 | loss: 83.8240774CurrentTrain: epoch  2, batch    35 | loss: 84.2208073CurrentTrain: epoch  2, batch    36 | loss: 120.1173702CurrentTrain: epoch  2, batch    37 | loss: 98.1501645CurrentTrain: epoch  2, batch    38 | loss: 81.3353959CurrentTrain: epoch  2, batch    39 | loss: 101.1542065CurrentTrain: epoch  2, batch    40 | loss: 82.5830997CurrentTrain: epoch  2, batch    41 | loss: 67.8177440CurrentTrain: epoch  2, batch    42 | loss: 60.7652778CurrentTrain: epoch  2, batch    43 | loss: 65.5776076CurrentTrain: epoch  2, batch    44 | loss: 123.4583791CurrentTrain: epoch  2, batch    45 | loss: 88.1204760CurrentTrain: epoch  2, batch    46 | loss: 83.6543848CurrentTrain: epoch  2, batch    47 | loss: 80.2138873CurrentTrain: epoch  2, batch    48 | loss: 123.6137079CurrentTrain: epoch  2, batch    49 | loss: 60.1108729CurrentTrain: epoch  2, batch    50 | loss: 101.9831134CurrentTrain: epoch  2, batch    51 | loss: 83.2266335CurrentTrain: epoch  2, batch    52 | loss: 100.6252913CurrentTrain: epoch  2, batch    53 | loss: 59.8539359CurrentTrain: epoch  2, batch    54 | loss: 103.2169316CurrentTrain: epoch  2, batch    55 | loss: 80.1489205CurrentTrain: epoch  2, batch    56 | loss: 99.0712542CurrentTrain: epoch  2, batch    57 | loss: 79.9132097CurrentTrain: epoch  2, batch    58 | loss: 78.8742284CurrentTrain: epoch  2, batch    59 | loss: 96.5816941CurrentTrain: epoch  2, batch    60 | loss: 96.2357428CurrentTrain: epoch  2, batch    61 | loss: 101.2307058CurrentTrain: epoch  2, batch    62 | loss: 69.1467097CurrentTrain: epoch  2, batch    63 | loss: 82.7080720CurrentTrain: epoch  2, batch    64 | loss: 59.1884074CurrentTrain: epoch  2, batch    65 | loss: 99.8767531CurrentTrain: epoch  2, batch    66 | loss: 67.6114446CurrentTrain: epoch  2, batch    67 | loss: 86.1169905CurrentTrain: epoch  2, batch    68 | loss: 68.3766390CurrentTrain: epoch  2, batch    69 | loss: 70.9187598CurrentTrain: epoch  2, batch    70 | loss: 127.9893391CurrentTrain: epoch  2, batch    71 | loss: 68.0040936CurrentTrain: epoch  2, batch    72 | loss: 78.0847813CurrentTrain: epoch  2, batch    73 | loss: 127.3865696CurrentTrain: epoch  2, batch    74 | loss: 79.4611870CurrentTrain: epoch  2, batch    75 | loss: 71.1602823CurrentTrain: epoch  2, batch    76 | loss: 98.9624519CurrentTrain: epoch  2, batch    77 | loss: 83.5578388CurrentTrain: epoch  2, batch    78 | loss: 98.4503835CurrentTrain: epoch  2, batch    79 | loss: 69.7583462CurrentTrain: epoch  2, batch    80 | loss: 126.4952347CurrentTrain: epoch  2, batch    81 | loss: 69.5947014CurrentTrain: epoch  2, batch    82 | loss: 87.7263786CurrentTrain: epoch  2, batch    83 | loss: 74.3172972CurrentTrain: epoch  2, batch    84 | loss: 67.9481690CurrentTrain: epoch  2, batch    85 | loss: 82.3989695CurrentTrain: epoch  2, batch    86 | loss: 131.9802452CurrentTrain: epoch  2, batch    87 | loss: 99.9596384CurrentTrain: epoch  2, batch    88 | loss: 80.2713159CurrentTrain: epoch  2, batch    89 | loss: 81.2540288CurrentTrain: epoch  2, batch    90 | loss: 68.8993731CurrentTrain: epoch  2, batch    91 | loss: 83.2242037CurrentTrain: epoch  2, batch    92 | loss: 98.9649609CurrentTrain: epoch  2, batch    93 | loss: 129.0124940CurrentTrain: epoch  2, batch    94 | loss: 71.3533306CurrentTrain: epoch  2, batch    95 | loss: 102.7686023CurrentTrain: epoch  3, batch     0 | loss: 132.0028348CurrentTrain: epoch  3, batch     1 | loss: 66.6322669CurrentTrain: epoch  3, batch     2 | loss: 59.4653011CurrentTrain: epoch  3, batch     3 | loss: 67.2384397CurrentTrain: epoch  3, batch     4 | loss: 99.3087954CurrentTrain: epoch  3, batch     5 | loss: 69.2100059CurrentTrain: epoch  3, batch     6 | loss: 67.7464905CurrentTrain: epoch  3, batch     7 | loss: 121.1925594CurrentTrain: epoch  3, batch     8 | loss: 76.0655572CurrentTrain: epoch  3, batch     9 | loss: 58.0169683CurrentTrain: epoch  3, batch    10 | loss: 82.3002773CurrentTrain: epoch  3, batch    11 | loss: 95.0633823CurrentTrain: epoch  3, batch    12 | loss: 68.0502616CurrentTrain: epoch  3, batch    13 | loss: 98.1309042CurrentTrain: epoch  3, batch    14 | loss: 67.9643061CurrentTrain: epoch  3, batch    15 | loss: 66.9933444CurrentTrain: epoch  3, batch    16 | loss: 83.4795635CurrentTrain: epoch  3, batch    17 | loss: 67.5803281CurrentTrain: epoch  3, batch    18 | loss: 78.2115337CurrentTrain: epoch  3, batch    19 | loss: 99.4925372CurrentTrain: epoch  3, batch    20 | loss: 79.2717702CurrentTrain: epoch  3, batch    21 | loss: 85.6105242CurrentTrain: epoch  3, batch    22 | loss: 75.2515479CurrentTrain: epoch  3, batch    23 | loss: 121.6856564CurrentTrain: epoch  3, batch    24 | loss: 63.3024656CurrentTrain: epoch  3, batch    25 | loss: 95.8739993CurrentTrain: epoch  3, batch    26 | loss: 97.0818428CurrentTrain: epoch  3, batch    27 | loss: 56.7064709CurrentTrain: epoch  3, batch    28 | loss: 79.4372081CurrentTrain: epoch  3, batch    29 | loss: 68.5215942CurrentTrain: epoch  3, batch    30 | loss: 123.3973404CurrentTrain: epoch  3, batch    31 | loss: 99.0843795CurrentTrain: epoch  3, batch    32 | loss: 127.9627159CurrentTrain: epoch  3, batch    33 | loss: 96.1136094CurrentTrain: epoch  3, batch    34 | loss: 97.5594170CurrentTrain: epoch  3, batch    35 | loss: 78.5638215CurrentTrain: epoch  3, batch    36 | loss: 94.8987797CurrentTrain: epoch  3, batch    37 | loss: 66.7254681CurrentTrain: epoch  3, batch    38 | loss: 70.6359664CurrentTrain: epoch  3, batch    39 | loss: 81.3546896CurrentTrain: epoch  3, batch    40 | loss: 55.1801287CurrentTrain: epoch  3, batch    41 | loss: 57.6641376CurrentTrain: epoch  3, batch    42 | loss: 127.9252502CurrentTrain: epoch  3, batch    43 | loss: 85.1933661CurrentTrain: epoch  3, batch    44 | loss: 94.8187924CurrentTrain: epoch  3, batch    45 | loss: 53.6953876CurrentTrain: epoch  3, batch    46 | loss: 121.3669129CurrentTrain: epoch  3, batch    47 | loss: 94.2360022CurrentTrain: epoch  3, batch    48 | loss: 78.1185897CurrentTrain: epoch  3, batch    49 | loss: 76.9314789CurrentTrain: epoch  3, batch    50 | loss: 92.8178899CurrentTrain: epoch  3, batch    51 | loss: 65.5748587CurrentTrain: epoch  3, batch    52 | loss: 98.6660057CurrentTrain: epoch  3, batch    53 | loss: 67.0161855CurrentTrain: epoch  3, batch    54 | loss: 68.7519472CurrentTrain: epoch  3, batch    55 | loss: 97.2510391CurrentTrain: epoch  3, batch    56 | loss: 96.2305754CurrentTrain: epoch  3, batch    57 | loss: 80.2741210CurrentTrain: epoch  3, batch    58 | loss: 66.6271756CurrentTrain: epoch  3, batch    59 | loss: 95.7898409CurrentTrain: epoch  3, batch    60 | loss: 81.2833707CurrentTrain: epoch  3, batch    61 | loss: 80.9083244CurrentTrain: epoch  3, batch    62 | loss: 76.1334141CurrentTrain: epoch  3, batch    63 | loss: 78.5867516CurrentTrain: epoch  3, batch    64 | loss: 77.6743995CurrentTrain: epoch  3, batch    65 | loss: 60.5864887CurrentTrain: epoch  3, batch    66 | loss: 83.0500808CurrentTrain: epoch  3, batch    67 | loss: 66.0360369CurrentTrain: epoch  3, batch    68 | loss: 96.2348797CurrentTrain: epoch  3, batch    69 | loss: 58.7225916CurrentTrain: epoch  3, batch    70 | loss: 82.3592277CurrentTrain: epoch  3, batch    71 | loss: 89.3460227CurrentTrain: epoch  3, batch    72 | loss: 67.3733978CurrentTrain: epoch  3, batch    73 | loss: 81.8259701CurrentTrain: epoch  3, batch    74 | loss: 66.0518957CurrentTrain: epoch  3, batch    75 | loss: 74.9451121CurrentTrain: epoch  3, batch    76 | loss: 83.0310356CurrentTrain: epoch  3, batch    77 | loss: 84.4750051CurrentTrain: epoch  3, batch    78 | loss: 98.8820518CurrentTrain: epoch  3, batch    79 | loss: 96.5513645CurrentTrain: epoch  3, batch    80 | loss: 77.9424275CurrentTrain: epoch  3, batch    81 | loss: 95.0966714CurrentTrain: epoch  3, batch    82 | loss: 69.9342966CurrentTrain: epoch  3, batch    83 | loss: 125.1993864CurrentTrain: epoch  3, batch    84 | loss: 76.6453172CurrentTrain: epoch  3, batch    85 | loss: 81.8661770CurrentTrain: epoch  3, batch    86 | loss: 78.7279925CurrentTrain: epoch  3, batch    87 | loss: 100.4046328CurrentTrain: epoch  3, batch    88 | loss: 162.2160482CurrentTrain: epoch  3, batch    89 | loss: 84.6489199CurrentTrain: epoch  3, batch    90 | loss: 83.3812060CurrentTrain: epoch  3, batch    91 | loss: 97.4371105CurrentTrain: epoch  3, batch    92 | loss: 96.1031391CurrentTrain: epoch  3, batch    93 | loss: 92.0960568CurrentTrain: epoch  3, batch    94 | loss: 81.7963555CurrentTrain: epoch  3, batch    95 | loss: 69.2780000CurrentTrain: epoch  4, batch     0 | loss: 73.9232512CurrentTrain: epoch  4, batch     1 | loss: 90.7739638CurrentTrain: epoch  4, batch     2 | loss: 76.3764879CurrentTrain: epoch  4, batch     3 | loss: 65.0281919CurrentTrain: epoch  4, batch     4 | loss: 125.0149381CurrentTrain: epoch  4, batch     5 | loss: 119.7199423CurrentTrain: epoch  4, batch     6 | loss: 76.5717296CurrentTrain: epoch  4, batch     7 | loss: 76.9540795CurrentTrain: epoch  4, batch     8 | loss: 64.0320630CurrentTrain: epoch  4, batch     9 | loss: 96.7085831CurrentTrain: epoch  4, batch    10 | loss: 94.9625525CurrentTrain: epoch  4, batch    11 | loss: 94.1452000CurrentTrain: epoch  4, batch    12 | loss: 69.2431446CurrentTrain: epoch  4, batch    13 | loss: 71.2186092CurrentTrain: epoch  4, batch    14 | loss: 77.9380804CurrentTrain: epoch  4, batch    15 | loss: 54.6623229CurrentTrain: epoch  4, batch    16 | loss: 123.8356232CurrentTrain: epoch  4, batch    17 | loss: 67.0096322CurrentTrain: epoch  4, batch    18 | loss: 119.3015053CurrentTrain: epoch  4, batch    19 | loss: 94.7400070CurrentTrain: epoch  4, batch    20 | loss: 80.9179167CurrentTrain: epoch  4, batch    21 | loss: 77.3272211CurrentTrain: epoch  4, batch    22 | loss: 78.6670927CurrentTrain: epoch  4, batch    23 | loss: 72.4461021CurrentTrain: epoch  4, batch    24 | loss: 79.5922934CurrentTrain: epoch  4, batch    25 | loss: 80.3618803CurrentTrain: epoch  4, batch    26 | loss: 66.0195269CurrentTrain: epoch  4, batch    27 | loss: 77.0028489CurrentTrain: epoch  4, batch    28 | loss: 70.2339881CurrentTrain: epoch  4, batch    29 | loss: 80.2035618CurrentTrain: epoch  4, batch    30 | loss: 96.4854304CurrentTrain: epoch  4, batch    31 | loss: 65.8068829CurrentTrain: epoch  4, batch    32 | loss: 76.2609266CurrentTrain: epoch  4, batch    33 | loss: 77.3751701CurrentTrain: epoch  4, batch    34 | loss: 94.8650312CurrentTrain: epoch  4, batch    35 | loss: 62.1983729CurrentTrain: epoch  4, batch    36 | loss: 79.1244813CurrentTrain: epoch  4, batch    37 | loss: 91.4244313CurrentTrain: epoch  4, batch    38 | loss: 64.3504205CurrentTrain: epoch  4, batch    39 | loss: 65.3360553CurrentTrain: epoch  4, batch    40 | loss: 80.7513646CurrentTrain: epoch  4, batch    41 | loss: 76.0114705CurrentTrain: epoch  4, batch    42 | loss: 74.6465216CurrentTrain: epoch  4, batch    43 | loss: 76.7426344CurrentTrain: epoch  4, batch    44 | loss: 93.1943605CurrentTrain: epoch  4, batch    45 | loss: 67.8480402CurrentTrain: epoch  4, batch    46 | loss: 60.7813351CurrentTrain: epoch  4, batch    47 | loss: 75.0303840CurrentTrain: epoch  4, batch    48 | loss: 53.7528037CurrentTrain: epoch  4, batch    49 | loss: 77.7122258CurrentTrain: epoch  4, batch    50 | loss: 63.7749540CurrentTrain: epoch  4, batch    51 | loss: 79.4975942CurrentTrain: epoch  4, batch    52 | loss: 78.5516027CurrentTrain: epoch  4, batch    53 | loss: 79.6785793CurrentTrain: epoch  4, batch    54 | loss: 94.9018321CurrentTrain: epoch  4, batch    55 | loss: 96.1471039CurrentTrain: epoch  4, batch    56 | loss: 117.7686126CurrentTrain: epoch  4, batch    57 | loss: 62.6935394CurrentTrain: epoch  4, batch    58 | loss: 75.7281784CurrentTrain: epoch  4, batch    59 | loss: 68.2154432CurrentTrain: epoch  4, batch    60 | loss: 58.7146845CurrentTrain: epoch  4, batch    61 | loss: 82.1211071CurrentTrain: epoch  4, batch    62 | loss: 79.8993829CurrentTrain: epoch  4, batch    63 | loss: 67.3873634CurrentTrain: epoch  4, batch    64 | loss: 83.1481362CurrentTrain: epoch  4, batch    65 | loss: 77.1282408CurrentTrain: epoch  4, batch    66 | loss: 94.3848615CurrentTrain: epoch  4, batch    67 | loss: 65.8957933CurrentTrain: epoch  4, batch    68 | loss: 76.4097774CurrentTrain: epoch  4, batch    69 | loss: 101.1132446CurrentTrain: epoch  4, batch    70 | loss: 76.1607753CurrentTrain: epoch  4, batch    71 | loss: 128.5719958CurrentTrain: epoch  4, batch    72 | loss: 120.7531770CurrentTrain: epoch  4, batch    73 | loss: 94.8466612CurrentTrain: epoch  4, batch    74 | loss: 80.3586044CurrentTrain: epoch  4, batch    75 | loss: 79.6272951CurrentTrain: epoch  4, batch    76 | loss: 91.1736759CurrentTrain: epoch  4, batch    77 | loss: 90.9497184CurrentTrain: epoch  4, batch    78 | loss: 124.8245057CurrentTrain: epoch  4, batch    79 | loss: 118.8260032CurrentTrain: epoch  4, batch    80 | loss: 78.7746402CurrentTrain: epoch  4, batch    81 | loss: 74.7012970CurrentTrain: epoch  4, batch    82 | loss: 60.3236680CurrentTrain: epoch  4, batch    83 | loss: 96.6467884CurrentTrain: epoch  4, batch    84 | loss: 84.6129122CurrentTrain: epoch  4, batch    85 | loss: 95.6060914CurrentTrain: epoch  4, batch    86 | loss: 96.9508925CurrentTrain: epoch  4, batch    87 | loss: 67.6977819CurrentTrain: epoch  4, batch    88 | loss: 91.8412491CurrentTrain: epoch  4, batch    89 | loss: 94.4603636CurrentTrain: epoch  4, batch    90 | loss: 115.2637996CurrentTrain: epoch  4, batch    91 | loss: 77.7495833CurrentTrain: epoch  4, batch    92 | loss: 113.8119510CurrentTrain: epoch  4, batch    93 | loss: 95.9728172CurrentTrain: epoch  4, batch    94 | loss: 57.1095287CurrentTrain: epoch  4, batch    95 | loss: 65.7950368CurrentTrain: epoch  5, batch     0 | loss: 56.0125076CurrentTrain: epoch  5, batch     1 | loss: 93.5813926CurrentTrain: epoch  5, batch     2 | loss: 78.1517210CurrentTrain: epoch  5, batch     3 | loss: 93.8759916CurrentTrain: epoch  5, batch     4 | loss: 95.3159831CurrentTrain: epoch  5, batch     5 | loss: 77.3001572CurrentTrain: epoch  5, batch     6 | loss: 116.8331899CurrentTrain: epoch  5, batch     7 | loss: 76.2231871CurrentTrain: epoch  5, batch     8 | loss: 64.8936801CurrentTrain: epoch  5, batch     9 | loss: 52.4052502CurrentTrain: epoch  5, batch    10 | loss: 76.9407410CurrentTrain: epoch  5, batch    11 | loss: 115.5273958CurrentTrain: epoch  5, batch    12 | loss: 63.0612691CurrentTrain: epoch  5, batch    13 | loss: 95.6084731CurrentTrain: epoch  5, batch    14 | loss: 64.9377804CurrentTrain: epoch  5, batch    15 | loss: 92.5529539CurrentTrain: epoch  5, batch    16 | loss: 75.8269060CurrentTrain: epoch  5, batch    17 | loss: 67.4130477CurrentTrain: epoch  5, batch    18 | loss: 75.6802912CurrentTrain: epoch  5, batch    19 | loss: 78.8170140CurrentTrain: epoch  5, batch    20 | loss: 91.4692217CurrentTrain: epoch  5, batch    21 | loss: 66.6253867CurrentTrain: epoch  5, batch    22 | loss: 77.4633162CurrentTrain: epoch  5, batch    23 | loss: 91.5425806CurrentTrain: epoch  5, batch    24 | loss: 71.5655252CurrentTrain: epoch  5, batch    25 | loss: 60.6107654CurrentTrain: epoch  5, batch    26 | loss: 66.4346953CurrentTrain: epoch  5, batch    27 | loss: 97.7717289CurrentTrain: epoch  5, batch    28 | loss: 98.1646866CurrentTrain: epoch  5, batch    29 | loss: 76.8051243CurrentTrain: epoch  5, batch    30 | loss: 93.0814448CurrentTrain: epoch  5, batch    31 | loss: 67.0732581CurrentTrain: epoch  5, batch    32 | loss: 75.5376096CurrentTrain: epoch  5, batch    33 | loss: 65.6217346CurrentTrain: epoch  5, batch    34 | loss: 77.4325653CurrentTrain: epoch  5, batch    35 | loss: 73.5764838CurrentTrain: epoch  5, batch    36 | loss: 66.4621779CurrentTrain: epoch  5, batch    37 | loss: 95.9767072CurrentTrain: epoch  5, batch    38 | loss: 81.0548124CurrentTrain: epoch  5, batch    39 | loss: 89.6650154CurrentTrain: epoch  5, batch    40 | loss: 64.3623234CurrentTrain: epoch  5, batch    41 | loss: 95.0286992CurrentTrain: epoch  5, batch    42 | loss: 76.5000970CurrentTrain: epoch  5, batch    43 | loss: 72.0155304CurrentTrain: epoch  5, batch    44 | loss: 62.3823737CurrentTrain: epoch  5, batch    45 | loss: 59.7511205CurrentTrain: epoch  5, batch    46 | loss: 93.0961065CurrentTrain: epoch  5, batch    47 | loss: 121.8250992CurrentTrain: epoch  5, batch    48 | loss: 62.7831998CurrentTrain: epoch  5, batch    49 | loss: 64.4416644CurrentTrain: epoch  5, batch    50 | loss: 56.2187503CurrentTrain: epoch  5, batch    51 | loss: 92.9863578CurrentTrain: epoch  5, batch    52 | loss: 116.7133302CurrentTrain: epoch  5, batch    53 | loss: 64.3577685CurrentTrain: epoch  5, batch    54 | loss: 95.9109178CurrentTrain: epoch  5, batch    55 | loss: 88.7510973CurrentTrain: epoch  5, batch    56 | loss: 63.0203945CurrentTrain: epoch  5, batch    57 | loss: 95.0364532CurrentTrain: epoch  5, batch    58 | loss: 61.3598643CurrentTrain: epoch  5, batch    59 | loss: 67.5003772CurrentTrain: epoch  5, batch    60 | loss: 89.9190043CurrentTrain: epoch  5, batch    61 | loss: 57.6238975CurrentTrain: epoch  5, batch    62 | loss: 65.8935093CurrentTrain: epoch  5, batch    63 | loss: 77.0618959CurrentTrain: epoch  5, batch    64 | loss: 117.4738540CurrentTrain: epoch  5, batch    65 | loss: 61.9821710CurrentTrain: epoch  5, batch    66 | loss: 122.3121865CurrentTrain: epoch  5, batch    67 | loss: 65.1703871CurrentTrain: epoch  5, batch    68 | loss: 89.6897443CurrentTrain: epoch  5, batch    69 | loss: 71.9607538CurrentTrain: epoch  5, batch    70 | loss: 116.0706192CurrentTrain: epoch  5, batch    71 | loss: 89.8277773CurrentTrain: epoch  5, batch    72 | loss: 115.7295001CurrentTrain: epoch  5, batch    73 | loss: 68.7804318CurrentTrain: epoch  5, batch    74 | loss: 74.8096839CurrentTrain: epoch  5, batch    75 | loss: 66.4561421CurrentTrain: epoch  5, batch    76 | loss: 74.6444351CurrentTrain: epoch  5, batch    77 | loss: 61.5606060CurrentTrain: epoch  5, batch    78 | loss: 66.1969685CurrentTrain: epoch  5, batch    79 | loss: 63.7053534CurrentTrain: epoch  5, batch    80 | loss: 97.1528992CurrentTrain: epoch  5, batch    81 | loss: 78.3323693CurrentTrain: epoch  5, batch    82 | loss: 89.0832601CurrentTrain: epoch  5, batch    83 | loss: 73.6472643CurrentTrain: epoch  5, batch    84 | loss: 92.8186541CurrentTrain: epoch  5, batch    85 | loss: 114.5199089CurrentTrain: epoch  5, batch    86 | loss: 85.6001158CurrentTrain: epoch  5, batch    87 | loss: 78.9126457CurrentTrain: epoch  5, batch    88 | loss: 74.4239779CurrentTrain: epoch  5, batch    89 | loss: 77.6439780CurrentTrain: epoch  5, batch    90 | loss: 116.6182688CurrentTrain: epoch  5, batch    91 | loss: 89.6918717CurrentTrain: epoch  5, batch    92 | loss: 74.9816581CurrentTrain: epoch  5, batch    93 | loss: 92.4739876CurrentTrain: epoch  5, batch    94 | loss: 78.8952716CurrentTrain: epoch  5, batch    95 | loss: 79.9739537CurrentTrain: epoch  6, batch     0 | loss: 61.9881232CurrentTrain: epoch  6, batch     1 | loss: 62.4328545CurrentTrain: epoch  6, batch     2 | loss: 73.4673871CurrentTrain: epoch  6, batch     3 | loss: 63.8399647CurrentTrain: epoch  6, batch     4 | loss: 85.9012242CurrentTrain: epoch  6, batch     5 | loss: 61.0833023CurrentTrain: epoch  6, batch     6 | loss: 76.0034535CurrentTrain: epoch  6, batch     7 | loss: 76.1703700CurrentTrain: epoch  6, batch     8 | loss: 66.3560217CurrentTrain: epoch  6, batch     9 | loss: 94.6429090CurrentTrain: epoch  6, batch    10 | loss: 49.3481353CurrentTrain: epoch  6, batch    11 | loss: 118.7721274CurrentTrain: epoch  6, batch    12 | loss: 94.5159504CurrentTrain: epoch  6, batch    13 | loss: 63.2844839CurrentTrain: epoch  6, batch    14 | loss: 90.2018748CurrentTrain: epoch  6, batch    15 | loss: 59.8501548CurrentTrain: epoch  6, batch    16 | loss: 74.6343678CurrentTrain: epoch  6, batch    17 | loss: 98.4765047CurrentTrain: epoch  6, batch    18 | loss: 77.7143356CurrentTrain: epoch  6, batch    19 | loss: 74.8942962CurrentTrain: epoch  6, batch    20 | loss: 70.8649545CurrentTrain: epoch  6, batch    21 | loss: 76.4968862CurrentTrain: epoch  6, batch    22 | loss: 71.1143893CurrentTrain: epoch  6, batch    23 | loss: 73.3186516CurrentTrain: epoch  6, batch    24 | loss: 89.8063473CurrentTrain: epoch  6, batch    25 | loss: 73.8741236CurrentTrain: epoch  6, batch    26 | loss: 73.2273833CurrentTrain: epoch  6, batch    27 | loss: 120.5643530CurrentTrain: epoch  6, batch    28 | loss: 60.2360745CurrentTrain: epoch  6, batch    29 | loss: 76.4656655CurrentTrain: epoch  6, batch    30 | loss: 70.3068130CurrentTrain: epoch  6, batch    31 | loss: 61.8763794CurrentTrain: epoch  6, batch    32 | loss: 116.1243614CurrentTrain: epoch  6, batch    33 | loss: 73.2789282CurrentTrain: epoch  6, batch    34 | loss: 75.7626810CurrentTrain: epoch  6, batch    35 | loss: 92.5199648CurrentTrain: epoch  6, batch    36 | loss: 64.0499533CurrentTrain: epoch  6, batch    37 | loss: 83.0410921CurrentTrain: epoch  6, batch    38 | loss: 51.8325597CurrentTrain: epoch  6, batch    39 | loss: 97.2950039CurrentTrain: epoch  6, batch    40 | loss: 61.0183614CurrentTrain: epoch  6, batch    41 | loss: 58.1743518CurrentTrain: epoch  6, batch    42 | loss: 77.3724779CurrentTrain: epoch  6, batch    43 | loss: 54.0538711CurrentTrain: epoch  6, batch    44 | loss: 61.8714057CurrentTrain: epoch  6, batch    45 | loss: 91.9704227CurrentTrain: epoch  6, batch    46 | loss: 62.0576161CurrentTrain: epoch  6, batch    47 | loss: 93.1020917CurrentTrain: epoch  6, batch    48 | loss: 94.0286386CurrentTrain: epoch  6, batch    49 | loss: 93.6086690CurrentTrain: epoch  6, batch    50 | loss: 65.1835432CurrentTrain: epoch  6, batch    51 | loss: 64.7430854CurrentTrain: epoch  6, batch    52 | loss: 62.3239891CurrentTrain: epoch  6, batch    53 | loss: 117.5741261CurrentTrain: epoch  6, batch    54 | loss: 63.9607845CurrentTrain: epoch  6, batch    55 | loss: 90.2903734CurrentTrain: epoch  6, batch    56 | loss: 76.8141352CurrentTrain: epoch  6, batch    57 | loss: 75.4615675CurrentTrain: epoch  6, batch    58 | loss: 82.0906373CurrentTrain: epoch  6, batch    59 | loss: 73.7317392CurrentTrain: epoch  6, batch    60 | loss: 76.8284187CurrentTrain: epoch  6, batch    61 | loss: 63.3346077CurrentTrain: epoch  6, batch    62 | loss: 73.9440727CurrentTrain: epoch  6, batch    63 | loss: 76.8977024CurrentTrain: epoch  6, batch    64 | loss: 88.5118207CurrentTrain: epoch  6, batch    65 | loss: 60.6324355CurrentTrain: epoch  6, batch    66 | loss: 96.6844128CurrentTrain: epoch  6, batch    67 | loss: 90.2289699CurrentTrain: epoch  6, batch    68 | loss: 89.4545123CurrentTrain: epoch  6, batch    69 | loss: 86.5960462CurrentTrain: epoch  6, batch    70 | loss: 72.3503017CurrentTrain: epoch  6, batch    71 | loss: 60.8247375CurrentTrain: epoch  6, batch    72 | loss: 117.7816476CurrentTrain: epoch  6, batch    73 | loss: 90.4155428CurrentTrain: epoch  6, batch    74 | loss: 77.3081222CurrentTrain: epoch  6, batch    75 | loss: 90.5359917CurrentTrain: epoch  6, batch    76 | loss: 116.6148326CurrentTrain: epoch  6, batch    77 | loss: 78.9112739CurrentTrain: epoch  6, batch    78 | loss: 74.8877747CurrentTrain: epoch  6, batch    79 | loss: 75.0236505CurrentTrain: epoch  6, batch    80 | loss: 60.8647035CurrentTrain: epoch  6, batch    81 | loss: 96.7513554CurrentTrain: epoch  6, batch    82 | loss: 62.0806308CurrentTrain: epoch  6, batch    83 | loss: 89.9321123CurrentTrain: epoch  6, batch    84 | loss: 91.3469549CurrentTrain: epoch  6, batch    85 | loss: 70.7302619CurrentTrain: epoch  6, batch    86 | loss: 74.8222377CurrentTrain: epoch  6, batch    87 | loss: 95.8660782CurrentTrain: epoch  6, batch    88 | loss: 74.4543017CurrentTrain: epoch  6, batch    89 | loss: 57.3108777CurrentTrain: epoch  6, batch    90 | loss: 114.9982042CurrentTrain: epoch  6, batch    91 | loss: 94.2333234CurrentTrain: epoch  6, batch    92 | loss: 77.4350083CurrentTrain: epoch  6, batch    93 | loss: 69.7878663CurrentTrain: epoch  6, batch    94 | loss: 74.5515153CurrentTrain: epoch  6, batch    95 | loss: 65.9724051CurrentTrain: epoch  7, batch     0 | loss: 62.0923817CurrentTrain: epoch  7, batch     1 | loss: 71.4288176CurrentTrain: epoch  7, batch     2 | loss: 59.6912048CurrentTrain: epoch  7, batch     3 | loss: 73.5940690CurrentTrain: epoch  7, batch     4 | loss: 74.7016544CurrentTrain: epoch  7, batch     5 | loss: 90.2154727CurrentTrain: epoch  7, batch     6 | loss: 112.0890986CurrentTrain: epoch  7, batch     7 | loss: 72.4228216CurrentTrain: epoch  7, batch     8 | loss: 58.0874886CurrentTrain: epoch  7, batch     9 | loss: 63.6793560CurrentTrain: epoch  7, batch    10 | loss: 53.1796594CurrentTrain: epoch  7, batch    11 | loss: 72.0785272CurrentTrain: epoch  7, batch    12 | loss: 91.2861626CurrentTrain: epoch  7, batch    13 | loss: 51.4859166CurrentTrain: epoch  7, batch    14 | loss: 61.8025362CurrentTrain: epoch  7, batch    15 | loss: 71.7865867CurrentTrain: epoch  7, batch    16 | loss: 72.3818832CurrentTrain: epoch  7, batch    17 | loss: 62.2615239CurrentTrain: epoch  7, batch    18 | loss: 115.5994892CurrentTrain: epoch  7, batch    19 | loss: 84.8359298CurrentTrain: epoch  7, batch    20 | loss: 65.8037317CurrentTrain: epoch  7, batch    21 | loss: 63.3295341CurrentTrain: epoch  7, batch    22 | loss: 71.9336843CurrentTrain: epoch  7, batch    23 | loss: 91.8118307CurrentTrain: epoch  7, batch    24 | loss: 72.3791117CurrentTrain: epoch  7, batch    25 | loss: 73.4914751CurrentTrain: epoch  7, batch    26 | loss: 91.4484033CurrentTrain: epoch  7, batch    27 | loss: 111.4545876CurrentTrain: epoch  7, batch    28 | loss: 75.7910208CurrentTrain: epoch  7, batch    29 | loss: 85.4457299CurrentTrain: epoch  7, batch    30 | loss: 50.3403485CurrentTrain: epoch  7, batch    31 | loss: 77.8331666CurrentTrain: epoch  7, batch    32 | loss: 92.8860355CurrentTrain: epoch  7, batch    33 | loss: 72.5647046CurrentTrain: epoch  7, batch    34 | loss: 71.5768285CurrentTrain: epoch  7, batch    35 | loss: 114.2151761CurrentTrain: epoch  7, batch    36 | loss: 73.4145381CurrentTrain: epoch  7, batch    37 | loss: 73.6177630CurrentTrain: epoch  7, batch    38 | loss: 86.9655004CurrentTrain: epoch  7, batch    39 | loss: 64.8055498CurrentTrain: epoch  7, batch    40 | loss: 74.9857137CurrentTrain: epoch  7, batch    41 | loss: 108.9379930CurrentTrain: epoch  7, batch    42 | loss: 63.6873321CurrentTrain: epoch  7, batch    43 | loss: 89.8038282CurrentTrain: epoch  7, batch    44 | loss: 88.5386059CurrentTrain: epoch  7, batch    45 | loss: 71.3035905CurrentTrain: epoch  7, batch    46 | loss: 91.5191339CurrentTrain: epoch  7, batch    47 | loss: 93.1807948CurrentTrain: epoch  7, batch    48 | loss: 88.1640235CurrentTrain: epoch  7, batch    49 | loss: 67.5825839CurrentTrain: epoch  7, batch    50 | loss: 52.9986042CurrentTrain: epoch  7, batch    51 | loss: 87.7275078CurrentTrain: epoch  7, batch    52 | loss: 72.9579633CurrentTrain: epoch  7, batch    53 | loss: 76.3357736CurrentTrain: epoch  7, batch    54 | loss: 58.3413111CurrentTrain: epoch  7, batch    55 | loss: 70.8940927CurrentTrain: epoch  7, batch    56 | loss: 71.9344357CurrentTrain: epoch  7, batch    57 | loss: 88.4797777CurrentTrain: epoch  7, batch    58 | loss: 89.2248831CurrentTrain: epoch  7, batch    59 | loss: 69.0861763CurrentTrain: epoch  7, batch    60 | loss: 71.4627442CurrentTrain: epoch  7, batch    61 | loss: 75.1146234CurrentTrain: epoch  7, batch    62 | loss: 73.6410748CurrentTrain: epoch  7, batch    63 | loss: 89.2031950CurrentTrain: epoch  7, batch    64 | loss: 64.3336901CurrentTrain: epoch  7, batch    65 | loss: 71.2210669CurrentTrain: epoch  7, batch    66 | loss: 91.5295530CurrentTrain: epoch  7, batch    67 | loss: 91.4221115CurrentTrain: epoch  7, batch    68 | loss: 72.8531167CurrentTrain: epoch  7, batch    69 | loss: 61.4179102CurrentTrain: epoch  7, batch    70 | loss: 59.9569868CurrentTrain: epoch  7, batch    71 | loss: 56.1259342CurrentTrain: epoch  7, batch    72 | loss: 91.5245622CurrentTrain: epoch  7, batch    73 | loss: 70.7196237CurrentTrain: epoch  7, batch    74 | loss: 77.4503032CurrentTrain: epoch  7, batch    75 | loss: 70.7171221CurrentTrain: epoch  7, batch    76 | loss: 121.4130069CurrentTrain: epoch  7, batch    77 | loss: 61.5750807CurrentTrain: epoch  7, batch    78 | loss: 113.2017556CurrentTrain: epoch  7, batch    79 | loss: 78.2348752CurrentTrain: epoch  7, batch    80 | loss: 121.6924162CurrentTrain: epoch  7, batch    81 | loss: 52.2296164CurrentTrain: epoch  7, batch    82 | loss: 61.7188003CurrentTrain: epoch  7, batch    83 | loss: 52.9954006CurrentTrain: epoch  7, batch    84 | loss: 61.5770841CurrentTrain: epoch  7, batch    85 | loss: 116.5452777CurrentTrain: epoch  7, batch    86 | loss: 56.8308223CurrentTrain: epoch  7, batch    87 | loss: 117.1854380CurrentTrain: epoch  7, batch    88 | loss: 73.7021793CurrentTrain: epoch  7, batch    89 | loss: 72.0630788CurrentTrain: epoch  7, batch    90 | loss: 119.2158664CurrentTrain: epoch  7, batch    91 | loss: 72.5790685CurrentTrain: epoch  7, batch    92 | loss: 51.0875772CurrentTrain: epoch  7, batch    93 | loss: 84.6384787CurrentTrain: epoch  7, batch    94 | loss: 73.0824017CurrentTrain: epoch  7, batch    95 | loss: 132.9610560CurrentTrain: epoch  8, batch     0 | loss: 70.4060583CurrentTrain: epoch  8, batch     1 | loss: 62.7265648CurrentTrain: epoch  8, batch     2 | loss: 53.8764228CurrentTrain: epoch  8, batch     3 | loss: 72.6381287CurrentTrain: epoch  8, batch     4 | loss: 59.8881617CurrentTrain: epoch  8, batch     5 | loss: 72.8185472CurrentTrain: epoch  8, batch     6 | loss: 56.7066205CurrentTrain: epoch  8, batch     7 | loss: 74.1477457CurrentTrain: epoch  8, batch     8 | loss: 71.1573473CurrentTrain: epoch  8, batch     9 | loss: 53.1824461CurrentTrain: epoch  8, batch    10 | loss: 72.6104698CurrentTrain: epoch  8, batch    11 | loss: 56.5186045CurrentTrain: epoch  8, batch    12 | loss: 74.5553899CurrentTrain: epoch  8, batch    13 | loss: 87.8969056CurrentTrain: epoch  8, batch    14 | loss: 90.6625376CurrentTrain: epoch  8, batch    15 | loss: 59.3596509CurrentTrain: epoch  8, batch    16 | loss: 114.0235427CurrentTrain: epoch  8, batch    17 | loss: 73.7031452CurrentTrain: epoch  8, batch    18 | loss: 76.1548045CurrentTrain: epoch  8, batch    19 | loss: 106.4156572CurrentTrain: epoch  8, batch    20 | loss: 71.9563552CurrentTrain: epoch  8, batch    21 | loss: 73.8310462CurrentTrain: epoch  8, batch    22 | loss: 69.4955182CurrentTrain: epoch  8, batch    23 | loss: 88.4220874CurrentTrain: epoch  8, batch    24 | loss: 71.1723962CurrentTrain: epoch  8, batch    25 | loss: 72.3972049CurrentTrain: epoch  8, batch    26 | loss: 90.3927603CurrentTrain: epoch  8, batch    27 | loss: 73.5831157CurrentTrain: epoch  8, batch    28 | loss: 89.5058056CurrentTrain: epoch  8, batch    29 | loss: 72.2915395CurrentTrain: epoch  8, batch    30 | loss: 71.3082360CurrentTrain: epoch  8, batch    31 | loss: 53.8436888CurrentTrain: epoch  8, batch    32 | loss: 58.9536177CurrentTrain: epoch  8, batch    33 | loss: 109.3636761CurrentTrain: epoch  8, batch    34 | loss: 112.3351599CurrentTrain: epoch  8, batch    35 | loss: 87.7100786CurrentTrain: epoch  8, batch    36 | loss: 86.7949398CurrentTrain: epoch  8, batch    37 | loss: 74.2094307CurrentTrain: epoch  8, batch    38 | loss: 74.6957455CurrentTrain: epoch  8, batch    39 | loss: 74.7598478CurrentTrain: epoch  8, batch    40 | loss: 73.0533280CurrentTrain: epoch  8, batch    41 | loss: 71.7038007CurrentTrain: epoch  8, batch    42 | loss: 60.6642712CurrentTrain: epoch  8, batch    43 | loss: 115.4988322CurrentTrain: epoch  8, batch    44 | loss: 118.7123827CurrentTrain: epoch  8, batch    45 | loss: 55.9131256CurrentTrain: epoch  8, batch    46 | loss: 53.3911904CurrentTrain: epoch  8, batch    47 | loss: 72.4608564CurrentTrain: epoch  8, batch    48 | loss: 76.8402687CurrentTrain: epoch  8, batch    49 | loss: 64.4112994CurrentTrain: epoch  8, batch    50 | loss: 49.4016198CurrentTrain: epoch  8, batch    51 | loss: 85.6394706CurrentTrain: epoch  8, batch    52 | loss: 114.0494219CurrentTrain: epoch  8, batch    53 | loss: 70.9122646CurrentTrain: epoch  8, batch    54 | loss: 67.7659764CurrentTrain: epoch  8, batch    55 | loss: 62.3739735CurrentTrain: epoch  8, batch    56 | loss: 61.3887153CurrentTrain: epoch  8, batch    57 | loss: 72.7447569CurrentTrain: epoch  8, batch    58 | loss: 58.0553187CurrentTrain: epoch  8, batch    59 | loss: 78.6206538CurrentTrain: epoch  8, batch    60 | loss: 90.4366097CurrentTrain: epoch  8, batch    61 | loss: 60.7717398CurrentTrain: epoch  8, batch    62 | loss: 114.3482829CurrentTrain: epoch  8, batch    63 | loss: 64.7345223CurrentTrain: epoch  8, batch    64 | loss: 92.2236166CurrentTrain: epoch  8, batch    65 | loss: 51.7385567CurrentTrain: epoch  8, batch    66 | loss: 87.4162582CurrentTrain: epoch  8, batch    67 | loss: 89.0180049CurrentTrain: epoch  8, batch    68 | loss: 87.5584027CurrentTrain: epoch  8, batch    69 | loss: 87.9659127CurrentTrain: epoch  8, batch    70 | loss: 57.8944811CurrentTrain: epoch  8, batch    71 | loss: 48.6710239CurrentTrain: epoch  8, batch    72 | loss: 83.2532575CurrentTrain: epoch  8, batch    73 | loss: 73.0317257CurrentTrain: epoch  8, batch    74 | loss: 77.4706540CurrentTrain: epoch  8, batch    75 | loss: 92.9169124CurrentTrain: epoch  8, batch    76 | loss: 75.1774143CurrentTrain: epoch  8, batch    77 | loss: 90.6468219CurrentTrain: epoch  8, batch    78 | loss: 65.8236589CurrentTrain: epoch  8, batch    79 | loss: 69.6011835CurrentTrain: epoch  8, batch    80 | loss: 70.3816809CurrentTrain: epoch  8, batch    81 | loss: 74.1538218CurrentTrain: epoch  8, batch    82 | loss: 60.3637270CurrentTrain: epoch  8, batch    83 | loss: 158.3582832CurrentTrain: epoch  8, batch    84 | loss: 61.7728834CurrentTrain: epoch  8, batch    85 | loss: 73.7556729CurrentTrain: epoch  8, batch    86 | loss: 61.0211702CurrentTrain: epoch  8, batch    87 | loss: 59.4270620CurrentTrain: epoch  8, batch    88 | loss: 76.0158401CurrentTrain: epoch  8, batch    89 | loss: 86.3666765CurrentTrain: epoch  8, batch    90 | loss: 57.1832325CurrentTrain: epoch  8, batch    91 | loss: 76.4513029CurrentTrain: epoch  8, batch    92 | loss: 62.5238650CurrentTrain: epoch  8, batch    93 | loss: 96.2324197CurrentTrain: epoch  8, batch    94 | loss: 59.1706798CurrentTrain: epoch  8, batch    95 | loss: 73.1341061CurrentTrain: epoch  9, batch     0 | loss: 85.1521262CurrentTrain: epoch  9, batch     1 | loss: 58.4660303CurrentTrain: epoch  9, batch     2 | loss: 86.8209221CurrentTrain: epoch  9, batch     3 | loss: 51.4045484CurrentTrain: epoch  9, batch     4 | loss: 87.5749038CurrentTrain: epoch  9, batch     5 | loss: 58.6754829CurrentTrain: epoch  9, batch     6 | loss: 57.0940275CurrentTrain: epoch  9, batch     7 | loss: 89.4831889CurrentTrain: epoch  9, batch     8 | loss: 86.4514535CurrentTrain: epoch  9, batch     9 | loss: 74.7047279CurrentTrain: epoch  9, batch    10 | loss: 90.4100190CurrentTrain: epoch  9, batch    11 | loss: 111.9474753CurrentTrain: epoch  9, batch    12 | loss: 87.6906322CurrentTrain: epoch  9, batch    13 | loss: 85.4471860CurrentTrain: epoch  9, batch    14 | loss: 86.6753308CurrentTrain: epoch  9, batch    15 | loss: 60.9681605CurrentTrain: epoch  9, batch    16 | loss: 72.2178663CurrentTrain: epoch  9, batch    17 | loss: 119.2490394CurrentTrain: epoch  9, batch    18 | loss: 75.5757126CurrentTrain: epoch  9, batch    19 | loss: 86.6622852CurrentTrain: epoch  9, batch    20 | loss: 82.9646026CurrentTrain: epoch  9, batch    21 | loss: 54.9130985CurrentTrain: epoch  9, batch    22 | loss: 69.2516129CurrentTrain: epoch  9, batch    23 | loss: 92.1117906CurrentTrain: epoch  9, batch    24 | loss: 91.8265468CurrentTrain: epoch  9, batch    25 | loss: 88.0247125CurrentTrain: epoch  9, batch    26 | loss: 70.9943133CurrentTrain: epoch  9, batch    27 | loss: 62.3576187CurrentTrain: epoch  9, batch    28 | loss: 59.8484515CurrentTrain: epoch  9, batch    29 | loss: 72.5409741CurrentTrain: epoch  9, batch    30 | loss: 55.0703905CurrentTrain: epoch  9, batch    31 | loss: 87.9045627CurrentTrain: epoch  9, batch    32 | loss: 60.0267519CurrentTrain: epoch  9, batch    33 | loss: 70.4945633CurrentTrain: epoch  9, batch    34 | loss: 108.0265516CurrentTrain: epoch  9, batch    35 | loss: 72.5612469CurrentTrain: epoch  9, batch    36 | loss: 58.0385459CurrentTrain: epoch  9, batch    37 | loss: 156.5273646CurrentTrain: epoch  9, batch    38 | loss: 75.0850147CurrentTrain: epoch  9, batch    39 | loss: 69.5442072CurrentTrain: epoch  9, batch    40 | loss: 95.9892005CurrentTrain: epoch  9, batch    41 | loss: 62.3193686CurrentTrain: epoch  9, batch    42 | loss: 56.9565612CurrentTrain: epoch  9, batch    43 | loss: 50.8507850CurrentTrain: epoch  9, batch    44 | loss: 67.9996040CurrentTrain: epoch  9, batch    45 | loss: 64.1975943CurrentTrain: epoch  9, batch    46 | loss: 109.7373432CurrentTrain: epoch  9, batch    47 | loss: 118.1245701CurrentTrain: epoch  9, batch    48 | loss: 74.9803063CurrentTrain: epoch  9, batch    49 | loss: 61.7691278CurrentTrain: epoch  9, batch    50 | loss: 70.4818526CurrentTrain: epoch  9, batch    51 | loss: 66.2482899CurrentTrain: epoch  9, batch    52 | loss: 65.5896370CurrentTrain: epoch  9, batch    53 | loss: 86.5266600CurrentTrain: epoch  9, batch    54 | loss: 59.0640343CurrentTrain: epoch  9, batch    55 | loss: 71.9493031CurrentTrain: epoch  9, batch    56 | loss: 60.1032715CurrentTrain: epoch  9, batch    57 | loss: 70.4769825CurrentTrain: epoch  9, batch    58 | loss: 58.5944095CurrentTrain: epoch  9, batch    59 | loss: 66.9509123CurrentTrain: epoch  9, batch    60 | loss: 86.5693741CurrentTrain: epoch  9, batch    61 | loss: 86.8743590CurrentTrain: epoch  9, batch    62 | loss: 66.2237526CurrentTrain: epoch  9, batch    63 | loss: 87.3393629CurrentTrain: epoch  9, batch    64 | loss: 151.1909327CurrentTrain: epoch  9, batch    65 | loss: 73.2355733CurrentTrain: epoch  9, batch    66 | loss: 89.8380785CurrentTrain: epoch  9, batch    67 | loss: 70.6588449CurrentTrain: epoch  9, batch    68 | loss: 70.4011564CurrentTrain: epoch  9, batch    69 | loss: 60.6224799CurrentTrain: epoch  9, batch    70 | loss: 68.2330993CurrentTrain: epoch  9, batch    71 | loss: 67.5791002CurrentTrain: epoch  9, batch    72 | loss: 59.6860299CurrentTrain: epoch  9, batch    73 | loss: 58.8760245CurrentTrain: epoch  9, batch    74 | loss: 71.6012108CurrentTrain: epoch  9, batch    75 | loss: 53.2554225CurrentTrain: epoch  9, batch    76 | loss: 70.7323326CurrentTrain: epoch  9, batch    77 | loss: 58.5236792CurrentTrain: epoch  9, batch    78 | loss: 92.5497263CurrentTrain: epoch  9, batch    79 | loss: 71.0310341CurrentTrain: epoch  9, batch    80 | loss: 112.6591433CurrentTrain: epoch  9, batch    81 | loss: 69.0638987CurrentTrain: epoch  9, batch    82 | loss: 87.8499473CurrentTrain: epoch  9, batch    83 | loss: 58.2934183CurrentTrain: epoch  9, batch    84 | loss: 91.8386704CurrentTrain: epoch  9, batch    85 | loss: 59.9778976CurrentTrain: epoch  9, batch    86 | loss: 67.1104719CurrentTrain: epoch  9, batch    87 | loss: 77.2746504CurrentTrain: epoch  9, batch    88 | loss: 62.1280154CurrentTrain: epoch  9, batch    89 | loss: 162.0101351CurrentTrain: epoch  9, batch    90 | loss: 61.8650685CurrentTrain: epoch  9, batch    91 | loss: 69.6302322CurrentTrain: epoch  9, batch    92 | loss: 59.5530235CurrentTrain: epoch  9, batch    93 | loss: 72.3148462CurrentTrain: epoch  9, batch    94 | loss: 58.4719637CurrentTrain: epoch  9, batch    95 | loss: 72.7048156

F1 score per class: {32: 0.46706586826347307, 6: 0.8110599078341014, 19: 0.3404255319148936, 24: 0.7613636363636364, 26: 0.95, 29: 0.7811158798283262}
Micro-average F1 score: 0.7461538461538462
Weighted-average F1 score: 0.752513048269307
F1 score per class: {32: 0.6448598130841121, 6: 0.8157894736842105, 19: 0.2127659574468085, 24: 0.7431693989071039, 26: 0.9447236180904522, 29: 0.7963800904977375}
Micro-average F1 score: 0.7410008779631255
Weighted-average F1 score: 0.717677895876966
F1 score per class: {32: 0.6509433962264151, 6: 0.8177777777777778, 19: 0.27692307692307694, 24: 0.7431693989071039, 26: 0.9447236180904522, 29: 0.7857142857142857}
Micro-average F1 score: 0.7581227436823105
Weighted-average F1 score: 0.7470890456674256

F1 score per class: {32: 0.46706586826347307, 6: 0.8110599078341014, 19: 0.3404255319148936, 24: 0.7613636363636364, 26: 0.95, 29: 0.7811158798283262}
Micro-average F1 score: 0.7461538461538462
Weighted-average F1 score: 0.752513048269307
F1 score per class: {32: 0.6448598130841121, 6: 0.8157894736842105, 19: 0.2127659574468085, 24: 0.7431693989071039, 26: 0.9447236180904522, 29: 0.7963800904977375}
Micro-average F1 score: 0.7410008779631255
Weighted-average F1 score: 0.717677895876966
F1 score per class: {32: 0.6509433962264151, 6: 0.8177777777777778, 19: 0.27692307692307694, 24: 0.7431693989071039, 26: 0.9447236180904522, 29: 0.7857142857142857}
Micro-average F1 score: 0.7581227436823105
Weighted-average F1 score: 0.7470890456674256

F1 score per class: {32: 0.37681159420289856, 6: 0.7521367521367521, 19: 0.18181818181818182, 24: 0.7052631578947368, 26: 0.867579908675799, 29: 0.5909090909090909}
Micro-average F1 score: 0.622792937399679
Weighted-average F1 score: 0.6091445361886607
F1 score per class: {32: 0.46621621621621623, 6: 0.7380952380952381, 19: 0.11976047904191617, 24: 0.6834170854271356, 26: 0.8545454545454545, 29: 0.6376811594202898}
Micro-average F1 score: 0.5985815602836879
Weighted-average F1 score: 0.5648529103215381
F1 score per class: {32: 0.47750865051903113, 6: 0.7449392712550608, 19: 0.14634146341463414, 24: 0.6834170854271356, 26: 0.8506787330316742, 29: 0.6175438596491228}
Micro-average F1 score: 0.6158357771260997
Weighted-average F1 score: 0.5909658317616243

F1 score per class: {32: 0.37681159420289856, 6: 0.7521367521367521, 19: 0.18181818181818182, 24: 0.7052631578947368, 26: 0.867579908675799, 29: 0.5909090909090909}
Micro-average F1 score: 0.622792937399679
Weighted-average F1 score: 0.6091445361886607
F1 score per class: {32: 0.46621621621621623, 6: 0.7380952380952381, 19: 0.11976047904191617, 24: 0.6834170854271356, 26: 0.8545454545454545, 29: 0.6376811594202898}
Micro-average F1 score: 0.5985815602836879
Weighted-average F1 score: 0.5648529103215381
F1 score per class: {32: 0.47750865051903113, 6: 0.7449392712550608, 19: 0.14634146341463414, 24: 0.6834170854271356, 26: 0.8506787330316742, 29: 0.6175438596491228}
Micro-average F1 score: 0.6158357771260997
Weighted-average F1 score: 0.5909658317616243
cur_acc_wo_na:  ['0.7462']
his_acc_wo_na:  ['0.7462']
cur_acc des_wo_na:  ['0.7410']
his_acc des_wo_na:  ['0.7410']
cur_acc rrf_wo_na:  ['0.7581']
his_acc rrf_wo_na:  ['0.7581']
cur_acc_w_na:  ['0.6228']
his_acc_w_na:  ['0.6228']
cur_acc des_w_na:  ['0.5986']
his_acc des_w_na:  ['0.5986']
cur_acc rrf_w_na:  ['0.6158']
his_acc rrf_w_na:  ['0.6158']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 78.9418664CurrentTrain: epoch  0, batch     1 | loss: 92.6194833CurrentTrain: epoch  0, batch     2 | loss: 96.4888321CurrentTrain: epoch  0, batch     3 | loss: 64.6695733CurrentTrain: epoch  1, batch     0 | loss: 131.6088438CurrentTrain: epoch  1, batch     1 | loss: 68.2178746CurrentTrain: epoch  1, batch     2 | loss: 104.8029304CurrentTrain: epoch  1, batch     3 | loss: 59.9088234CurrentTrain: epoch  2, batch     0 | loss: 99.2911498CurrentTrain: epoch  2, batch     1 | loss: 84.1203526CurrentTrain: epoch  2, batch     2 | loss: 79.6790127CurrentTrain: epoch  2, batch     3 | loss: 57.3316893CurrentTrain: epoch  3, batch     0 | loss: 96.1443438CurrentTrain: epoch  3, batch     1 | loss: 64.7611261CurrentTrain: epoch  3, batch     2 | loss: 95.1613912CurrentTrain: epoch  3, batch     3 | loss: 57.4438653CurrentTrain: epoch  4, batch     0 | loss: 90.6374067CurrentTrain: epoch  4, batch     1 | loss: 90.4704393CurrentTrain: epoch  4, batch     2 | loss: 64.1244738CurrentTrain: epoch  4, batch     3 | loss: 74.5930632CurrentTrain: epoch  5, batch     0 | loss: 118.1702419CurrentTrain: epoch  5, batch     1 | loss: 75.8198579CurrentTrain: epoch  5, batch     2 | loss: 64.3777439CurrentTrain: epoch  5, batch     3 | loss: 41.2795271CurrentTrain: epoch  6, batch     0 | loss: 64.2494715CurrentTrain: epoch  6, batch     1 | loss: 63.1675782CurrentTrain: epoch  6, batch     2 | loss: 61.1092694CurrentTrain: epoch  6, batch     3 | loss: 94.0048495CurrentTrain: epoch  7, batch     0 | loss: 111.0939721CurrentTrain: epoch  7, batch     1 | loss: 71.9333825CurrentTrain: epoch  7, batch     2 | loss: 71.7464945CurrentTrain: epoch  7, batch     3 | loss: 43.1833051CurrentTrain: epoch  8, batch     0 | loss: 85.3456193CurrentTrain: epoch  8, batch     1 | loss: 107.1641702CurrentTrain: epoch  8, batch     2 | loss: 87.9287359CurrentTrain: epoch  8, batch     3 | loss: 35.9734298CurrentTrain: epoch  9, batch     0 | loss: 59.0588570CurrentTrain: epoch  9, batch     1 | loss: 59.2693612CurrentTrain: epoch  9, batch     2 | loss: 90.5326566CurrentTrain: epoch  9, batch     3 | loss: 50.0160868
MemoryTrain:  epoch  0, batch     0 | loss: 1.2458532MemoryTrain:  epoch  1, batch     0 | loss: 1.0636776MemoryTrain:  epoch  2, batch     0 | loss: 0.8767557MemoryTrain:  epoch  3, batch     0 | loss: 0.5694567MemoryTrain:  epoch  4, batch     0 | loss: 0.5487583MemoryTrain:  epoch  5, batch     0 | loss: 0.3478761MemoryTrain:  epoch  6, batch     0 | loss: 0.3281808MemoryTrain:  epoch  7, batch     0 | loss: 0.2820764MemoryTrain:  epoch  8, batch     0 | loss: 0.2508593MemoryTrain:  epoch  9, batch     0 | loss: 0.1805780

F1 score per class: {32: 0.0, 33: 0.5179856115107914, 36: 0.0, 6: 0.5822784810126582, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.8717948717948718, 26: 0.0, 29: 0.4, 30: 0.734375}
Micro-average F1 score: 0.5650224215246636
Weighted-average F1 score: 0.5077547671108827
F1 score per class: {32: 0.0, 33: 0.6408839779005525, 36: 0.0, 6: 0.6666666666666666, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.5964912280701754, 26: 0.0, 29: 0.36363636363636365, 30: 0.6966292134831461}
Micro-average F1 score: 0.5657894736842105
Weighted-average F1 score: 0.5166512900977036
F1 score per class: {32: 0.0, 33: 0.6629213483146067, 36: 0.0, 6: 0.6436781609195402, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.6666666666666666, 26: 0.0, 29: 0.36363636363636365, 30: 0.7093023255813954}
Micro-average F1 score: 0.5827586206896552
Weighted-average F1 score: 0.5331350428132462

F1 score per class: {32: 0.5615763546798029, 33: 0.375, 36: 0.7932489451476793, 6: 0.5609756097560976, 8: 0.32653061224489793, 19: 0.7340425531914894, 20: 0.9082125603864735, 24: 0.8717948717948718, 26: 0.7711864406779662, 29: 0.25, 30: 0.6619718309859155}
Micro-average F1 score: 0.6741713570981863
Weighted-average F1 score: 0.6724986320414688
F1 score per class: {32: 0.6255144032921811, 33: 0.4496124031007752, 36: 0.7307692307692307, 6: 0.6138613861386139, 8: 0.22429906542056074, 19: 0.700507614213198, 20: 0.8796296296296297, 24: 0.32075471698113206, 26: 0.7346938775510204, 29: 0.21621621621621623, 30: 0.543859649122807}
Micro-average F1 score: 0.6096096096096096
Weighted-average F1 score: 0.5868026957014659
F1 score per class: {32: 0.591304347826087, 33: 0.4591439688715953, 36: 0.7224334600760456, 6: 0.6086956521739131, 8: 0.2962962962962963, 19: 0.711340206185567, 20: 0.8878504672897196, 24: 0.4722222222222222, 26: 0.7346938775510204, 29: 0.21052631578947367, 30: 0.583732057416268}
Micro-average F1 score: 0.6311345646437995
Weighted-average F1 score: 0.6160819167473405

F1 score per class: {32: 0.0, 33: 0.4161849710982659, 36: 0.0, 6: 0.4791666666666667, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.8095238095238095, 26: 0.0, 29: 0.35294117647058826, 30: 0.5251396648044693}
Micro-average F1 score: 0.41652892561983473
Weighted-average F1 score: 0.37029811267865415
F1 score per class: {32: 0.0, 33: 0.4496124031007752, 36: 0.0, 6: 0.512396694214876, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.5, 26: 0.0, 29: 0.23529411764705882, 30: 0.43661971830985913}
Micro-average F1 score: 0.37269772481040087
Weighted-average F1 score: 0.34538560283044156
F1 score per class: {32: 0.0, 33: 0.4573643410852713, 36: 0.0, 6: 0.49122807017543857, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.5862068965517241, 26: 0.0, 29: 0.21052631578947367, 30: 0.4552238805970149}
Micro-average F1 score: 0.38540478905359177
Weighted-average F1 score: 0.35747473058057183

F1 score per class: {32: 0.4028268551236749, 33: 0.26865671641791045, 36: 0.7121212121212122, 6: 0.41818181818181815, 8: 0.1702127659574468, 19: 0.6540284360189573, 20: 0.7932489451476793, 24: 0.8095238095238095, 26: 0.5796178343949044, 29: 0.17647058823529413, 30: 0.44976076555023925}
Micro-average F1 score: 0.5217812197483059
Weighted-average F1 score: 0.5082782766458601
F1 score per class: {32: 0.41192411924119243, 33: 0.2761904761904762, 36: 0.6209150326797386, 6: 0.4305555555555556, 8: 0.11940298507462686, 19: 0.6026200873362445, 20: 0.7251908396946565, 24: 0.2537313432835821, 26: 0.5487804878048781, 29: 0.14285714285714285, 30: 0.31876606683804626}
Micro-average F1 score: 0.42917547568710357
Weighted-average F1 score: 0.4077814152783291
F1 score per class: {32: 0.40718562874251496, 33: 0.27634660421545665, 36: 0.6168831168831169, 6: 0.4148148148148148, 8: 0.1509433962264151, 19: 0.6188340807174888, 20: 0.7364341085271318, 24: 0.39080459770114945, 26: 0.5405405405405406, 29: 0.125, 30: 0.3446327683615819}
Micro-average F1 score: 0.4459358687546607
Weighted-average F1 score: 0.4267796928697398
cur_acc_wo_na:  ['0.7462', '0.5650']
his_acc_wo_na:  ['0.7462', '0.6742']
cur_acc des_wo_na:  ['0.7410', '0.5658']
his_acc des_wo_na:  ['0.7410', '0.6096']
cur_acc rrf_wo_na:  ['0.7581', '0.5828']
his_acc rrf_wo_na:  ['0.7581', '0.6311']
cur_acc_w_na:  ['0.6228', '0.4165']
his_acc_w_na:  ['0.6228', '0.5218']
cur_acc des_w_na:  ['0.5986', '0.3727']
his_acc des_w_na:  ['0.5986', '0.4292']
cur_acc rrf_w_na:  ['0.6158', '0.3854']
his_acc rrf_w_na:  ['0.6158', '0.4459']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 100.9803851CurrentTrain: epoch  0, batch     1 | loss: 92.7209921CurrentTrain: epoch  0, batch     2 | loss: 86.1359019CurrentTrain: epoch  0, batch     3 | loss: 108.3387794CurrentTrain: epoch  0, batch     4 | loss: 19.3562183CurrentTrain: epoch  1, batch     0 | loss: 107.5061907CurrentTrain: epoch  1, batch     1 | loss: 66.8176381CurrentTrain: epoch  1, batch     2 | loss: 83.3606949CurrentTrain: epoch  1, batch     3 | loss: 79.2601064CurrentTrain: epoch  1, batch     4 | loss: 18.0983788CurrentTrain: epoch  2, batch     0 | loss: 81.0009822CurrentTrain: epoch  2, batch     1 | loss: 80.6400289CurrentTrain: epoch  2, batch     2 | loss: 75.9434516CurrentTrain: epoch  2, batch     3 | loss: 119.9702745CurrentTrain: epoch  2, batch     4 | loss: 41.8011572CurrentTrain: epoch  3, batch     0 | loss: 66.4291198CurrentTrain: epoch  3, batch     1 | loss: 66.1814962CurrentTrain: epoch  3, batch     2 | loss: 92.7163794CurrentTrain: epoch  3, batch     3 | loss: 93.5254291CurrentTrain: epoch  3, batch     4 | loss: 15.9489965CurrentTrain: epoch  4, batch     0 | loss: 79.8072057CurrentTrain: epoch  4, batch     1 | loss: 92.7378363CurrentTrain: epoch  4, batch     2 | loss: 73.7535376CurrentTrain: epoch  4, batch     3 | loss: 70.3025858CurrentTrain: epoch  4, batch     4 | loss: 21.6775851CurrentTrain: epoch  5, batch     0 | loss: 62.5340261CurrentTrain: epoch  5, batch     1 | loss: 89.9438606CurrentTrain: epoch  5, batch     2 | loss: 76.7174878CurrentTrain: epoch  5, batch     3 | loss: 64.0581781CurrentTrain: epoch  5, batch     4 | loss: 11.5276176CurrentTrain: epoch  6, batch     0 | loss: 87.6399016CurrentTrain: epoch  6, batch     1 | loss: 58.3779312CurrentTrain: epoch  6, batch     2 | loss: 72.9003191CurrentTrain: epoch  6, batch     3 | loss: 115.5868864CurrentTrain: epoch  6, batch     4 | loss: 22.6573577CurrentTrain: epoch  7, batch     0 | loss: 110.7355860CurrentTrain: epoch  7, batch     1 | loss: 113.9813892CurrentTrain: epoch  7, batch     2 | loss: 59.0252046CurrentTrain: epoch  7, batch     3 | loss: 59.5563791CurrentTrain: epoch  7, batch     4 | loss: 16.0195789CurrentTrain: epoch  8, batch     0 | loss: 69.2901286CurrentTrain: epoch  8, batch     1 | loss: 112.9661510CurrentTrain: epoch  8, batch     2 | loss: 57.3510945CurrentTrain: epoch  8, batch     3 | loss: 86.7148270CurrentTrain: epoch  8, batch     4 | loss: 21.4189657CurrentTrain: epoch  9, batch     0 | loss: 59.4204211CurrentTrain: epoch  9, batch     1 | loss: 109.8406480CurrentTrain: epoch  9, batch     2 | loss: 87.1587455CurrentTrain: epoch  9, batch     3 | loss: 69.9235465CurrentTrain: epoch  9, batch     4 | loss: 7.9572028
MemoryTrain:  epoch  0, batch     0 | loss: 1.1818297MemoryTrain:  epoch  1, batch     0 | loss: 1.0261940MemoryTrain:  epoch  2, batch     0 | loss: 0.8969218MemoryTrain:  epoch  3, batch     0 | loss: 0.6357439MemoryTrain:  epoch  4, batch     0 | loss: 0.4776537MemoryTrain:  epoch  5, batch     0 | loss: 0.4033775MemoryTrain:  epoch  6, batch     0 | loss: 0.3681806MemoryTrain:  epoch  7, batch     0 | loss: 0.3876358MemoryTrain:  epoch  8, batch     0 | loss: 0.2825806MemoryTrain:  epoch  9, batch     0 | loss: 0.2525219

F1 score per class: {32: 0.6086956521739131, 33: 0.0, 2: 0.0, 6: 0.6419753086419753, 39: 0.6270270270270271, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.5555555555555556, 20: 0.0, 24: 0.0, 28: 0.0, 29: 0.4444444444444444}
Micro-average F1 score: 0.5550660792951542
Weighted-average F1 score: 0.4964193302151982
F1 score per class: {32: 0.4375, 33: 0.0, 2: 0.0, 36: 0.6380368098159509, 6: 0.6432160804020101, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.3125, 19: 0.0, 20: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 30: 0.5714285714285714}
Micro-average F1 score: 0.4400656814449918
Weighted-average F1 score: 0.3412657761435777
F1 score per class: {32: 0.5, 33: 0.0, 2: 0.0, 6: 0.6463414634146342, 39: 0.6766169154228856, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.3225806451612903, 20: 0.0, 24: 0.0, 28: 0.0, 29: 0.5714285714285714}
Micro-average F1 score: 0.5027124773960217
Weighted-average F1 score: 0.4143102231015967

F1 score per class: {32: 0.56, 33: 0.5943396226415094, 2: 0.39766081871345027, 36: 0.4315352697095436, 6: 0.3841059602649007, 39: 0.8108108108108109, 8: 0.5476190476190477, 11: 0.2857142857142857, 12: 0.7403314917127072, 19: 0.2222222222222222, 20: 0.8571428571428571, 24: 0.8333333333333334, 26: 0.6920152091254753, 28: 0.3333333333333333, 29: 0.5208333333333334, 30: 0.36363636363636365}
Micro-average F1 score: 0.5797773654916512
Weighted-average F1 score: 0.5651867021860033
F1 score per class: {32: 0.3181818181818182, 33: 0.6083650190114068, 2: 0.3058103975535168, 36: 0.4727272727272727, 6: 0.36056338028169016, 39: 0.7335907335907336, 8: 0.5576923076923077, 11: 0.20869565217391303, 12: 0.7272727272727273, 19: 0.15873015873015872, 20: 0.8359788359788359, 24: 0.4186046511627907, 26: 0.6714801444043321, 28: 0.25, 29: 0.6724137931034483, 30: 0.3333333333333333}
Micro-average F1 score: 0.525328330206379
Weighted-average F1 score: 0.49625776333094435
F1 score per class: {32: 0.4, 33: 0.5907172995780591, 2: 0.34686346863468637, 36: 0.4649122807017544, 6: 0.384180790960452, 39: 0.749003984063745, 8: 0.5360824742268041, 11: 0.2647058823529412, 12: 0.7311827956989247, 19: 0.15625, 20: 0.8404255319148937, 24: 0.7555555555555555, 26: 0.6690647482014388, 28: 0.25, 29: 0.6422018348623854, 30: 0.375}
Micro-average F1 score: 0.5512768544791244
Weighted-average F1 score: 0.5278387105737234

F1 score per class: {32: 0.358974358974359, 33: 0.0, 2: 0.0, 36: 0.5024154589371981, 6: 0.5110132158590308, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.24390243902439024, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.38095238095238093}
Micro-average F1 score: 0.38591117917304746
Weighted-average F1 score: 0.3367990690122714
F1 score per class: {32: 0.28, 33: 0.0, 2: 0.0, 36: 0.5098039215686274, 6: 0.5039370078740157, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.16666666666666666, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.46153846153846156}
Micro-average F1 score: 0.29321663019693656
Weighted-average F1 score: 0.23371033427293156
F1 score per class: {32: 0.30434782608695654, 33: 0.0, 2: 0.0, 36: 0.5096153846153846, 6: 0.5354330708661418, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.1724137931034483, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.46153846153846156}
Micro-average F1 score: 0.33413461538461536
Weighted-average F1 score: 0.2757431644797444

F1 score per class: {32: 0.2916666666666667, 33: 0.39622641509433965, 2: 0.27530364372469635, 36: 0.32398753894080995, 6: 0.2188679245283019, 39: 0.7346938775510204, 8: 0.4, 11: 0.16470588235294117, 12: 0.6666666666666666, 19: 0.10869565217391304, 20: 0.7677725118483413, 24: 0.7692307692307693, 26: 0.5141242937853108, 28: 0.2608695652173913, 29: 0.3875968992248062, 30: 0.20512820512820512}
Micro-average F1 score: 0.41708375041708373
Weighted-average F1 score: 0.3919945398378305
F1 score per class: {32: 0.1794871794871795, 33: 0.3764705882352941, 2: 0.18018018018018017, 36: 0.35135135135135137, 6: 0.19937694704049844, 39: 0.6168831168831169, 8: 0.3790849673202614, 11: 0.11764705882352941, 12: 0.6415094339622641, 19: 0.08333333333333333, 20: 0.7707317073170732, 24: 0.34285714285714286, 26: 0.48947368421052634, 28: 0.2, 29: 0.45614035087719296, 30: 0.16}
Micro-average F1 score: 0.35362465269007326
Weighted-average F1 score: 0.3273527593691942
F1 score per class: {32: 0.22580645161290322, 33: 0.36939313984168864, 2: 0.21266968325791855, 36: 0.33974358974358976, 6: 0.20955315870570107, 39: 0.6416382252559727, 8: 0.36619718309859156, 11: 0.13432835820895522, 12: 0.6570048309178744, 19: 0.08130081300813008, 20: 0.7783251231527094, 24: 0.6666666666666666, 26: 0.48186528497409326, 28: 0.20689655172413793, 29: 0.45454545454545453, 30: 0.17142857142857143}
Micro-average F1 score: 0.37403740374037403
Weighted-average F1 score: 0.34740651867862454
cur_acc_wo_na:  ['0.7462', '0.5650', '0.5551']
his_acc_wo_na:  ['0.7462', '0.6742', '0.5798']
cur_acc des_wo_na:  ['0.7410', '0.5658', '0.4401']
his_acc des_wo_na:  ['0.7410', '0.6096', '0.5253']
cur_acc rrf_wo_na:  ['0.7581', '0.5828', '0.5027']
his_acc rrf_wo_na:  ['0.7581', '0.6311', '0.5513']
cur_acc_w_na:  ['0.6228', '0.4165', '0.3859']
his_acc_w_na:  ['0.6228', '0.5218', '0.4171']
cur_acc des_w_na:  ['0.5986', '0.3727', '0.2932']
his_acc des_w_na:  ['0.5986', '0.4292', '0.3536']
cur_acc rrf_w_na:  ['0.6158', '0.3854', '0.3341']
his_acc rrf_w_na:  ['0.6158', '0.4459', '0.3740']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 86.2535384CurrentTrain: epoch  0, batch     1 | loss: 113.5780504CurrentTrain: epoch  0, batch     2 | loss: 114.0631480CurrentTrain: epoch  0, batch     3 | loss: 139.6012680CurrentTrain: epoch  0, batch     4 | loss: 80.5003188CurrentTrain: epoch  1, batch     0 | loss: 80.1067950CurrentTrain: epoch  1, batch     1 | loss: 92.2578753CurrentTrain: epoch  1, batch     2 | loss: 98.8549953CurrentTrain: epoch  1, batch     3 | loss: 124.9100918CurrentTrain: epoch  1, batch     4 | loss: 62.0487164CurrentTrain: epoch  2, batch     0 | loss: 81.1572096CurrentTrain: epoch  2, batch     1 | loss: 100.1914718CurrentTrain: epoch  2, batch     2 | loss: 78.6469699CurrentTrain: epoch  2, batch     3 | loss: 99.0139250CurrentTrain: epoch  2, batch     4 | loss: 78.1277456CurrentTrain: epoch  3, batch     0 | loss: 67.5250007CurrentTrain: epoch  3, batch     1 | loss: 80.2101022CurrentTrain: epoch  3, batch     2 | loss: 96.2863395CurrentTrain: epoch  3, batch     3 | loss: 77.9531386CurrentTrain: epoch  3, batch     4 | loss: 50.5026351CurrentTrain: epoch  4, batch     0 | loss: 92.3844343CurrentTrain: epoch  4, batch     1 | loss: 92.1770005CurrentTrain: epoch  4, batch     2 | loss: 65.9023213CurrentTrain: epoch  4, batch     3 | loss: 78.4347992CurrentTrain: epoch  4, batch     4 | loss: 59.2254049CurrentTrain: epoch  5, batch     0 | loss: 119.4121532CurrentTrain: epoch  5, batch     1 | loss: 75.3347895CurrentTrain: epoch  5, batch     2 | loss: 75.5523944CurrentTrain: epoch  5, batch     3 | loss: 91.1256883CurrentTrain: epoch  5, batch     4 | loss: 47.2964844CurrentTrain: epoch  6, batch     0 | loss: 71.5366395CurrentTrain: epoch  6, batch     1 | loss: 91.9783101CurrentTrain: epoch  6, batch     2 | loss: 89.2040023CurrentTrain: epoch  6, batch     3 | loss: 71.8404199CurrentTrain: epoch  6, batch     4 | loss: 95.6203203CurrentTrain: epoch  7, batch     0 | loss: 59.4074840CurrentTrain: epoch  7, batch     1 | loss: 90.6186292CurrentTrain: epoch  7, batch     2 | loss: 86.8660527CurrentTrain: epoch  7, batch     3 | loss: 88.5887931CurrentTrain: epoch  7, batch     4 | loss: 99.4671098CurrentTrain: epoch  8, batch     0 | loss: 57.8174892CurrentTrain: epoch  8, batch     1 | loss: 88.6426576CurrentTrain: epoch  8, batch     2 | loss: 61.3370493CurrentTrain: epoch  8, batch     3 | loss: 115.8011544CurrentTrain: epoch  8, batch     4 | loss: 69.6734441CurrentTrain: epoch  9, batch     0 | loss: 70.5751241CurrentTrain: epoch  9, batch     1 | loss: 71.9787282CurrentTrain: epoch  9, batch     2 | loss: 72.7933853CurrentTrain: epoch  9, batch     3 | loss: 112.7728577CurrentTrain: epoch  9, batch     4 | loss: 43.2951425
MemoryTrain:  epoch  0, batch     0 | loss: 1.2173786MemoryTrain:  epoch  1, batch     0 | loss: 1.2838478MemoryTrain:  epoch  2, batch     0 | loss: 0.8900366MemoryTrain:  epoch  3, batch     0 | loss: 0.7312126MemoryTrain:  epoch  4, batch     0 | loss: 0.6038339MemoryTrain:  epoch  5, batch     0 | loss: 0.4956592MemoryTrain:  epoch  6, batch     0 | loss: 0.3937158MemoryTrain:  epoch  7, batch     0 | loss: 0.3448641MemoryTrain:  epoch  8, batch     0 | loss: 0.3214658MemoryTrain:  epoch  9, batch     0 | loss: 0.2207586

F1 score per class: {32: 0.0, 2: 0.9047619047619048, 5: 0.0, 6: 0.0, 39: 0.26666666666666666, 8: 0.0, 10: 0.0, 12: 0.7017543859649122, 11: 0.2, 16: 0.05, 17: 0.0, 18: 0.0, 20: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.510556621880998
Weighted-average F1 score: 0.5067531072579408
F1 score per class: {2: 0.0, 5: 0.7404580152671756, 6: 0.0, 8: 0.0, 10: 0.45588235294117646, 11: 0.0, 12: 0.0, 16: 0.7058823529411765, 17: 0.7142857142857143, 18: 0.49230769230769234, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.5110782865583456
Weighted-average F1 score: 0.4502382457530511
F1 score per class: {2: 0.0, 5: 0.7649402390438247, 6: 0.0, 8: 0.0, 10: 0.45588235294117646, 11: 0.0, 12: 0.0, 16: 0.7164179104477612, 17: 0.6153846153846154, 18: 0.2, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.49230769230769234
Weighted-average F1 score: 0.43713032606846397

F1 score per class: {2: 0.36363636363636365, 5: 0.8597285067873304, 6: 0.5116279069767442, 8: 0.17475728155339806, 10: 0.18604651162790697, 11: 0.23232323232323232, 12: 0.3745019920318725, 16: 0.6060606060606061, 17: 0.1111111111111111, 18: 0.04, 19: 0.827906976744186, 20: 0.5681818181818182, 24: 0.32558139534883723, 26: 0.7222222222222222, 28: 0.30303030303030304, 29: 0.8527918781725888, 30: 0.8648648648648649, 32: 0.6691729323308271, 33: 0.375, 36: 0.2077922077922078, 39: 0.16}
Micro-average F1 score: 0.5326915363016446
Weighted-average F1 score: 0.5553142784121372
F1 score per class: {2: 0.34285714285714286, 5: 0.627831715210356, 6: 0.5481481481481482, 8: 0.26490066225165565, 10: 0.33695652173913043, 11: 0.3915343915343915, 12: 0.3234323432343234, 16: 0.5581395348837209, 17: 0.37037037037037035, 18: 0.2962962962962963, 19: 0.7230769230769231, 20: 0.49523809523809526, 24: 0.2222222222222222, 26: 0.7, 28: 0.16666666666666666, 29: 0.8191489361702128, 30: 0.39080459770114945, 32: 0.621160409556314, 33: 0.2857142857142857, 36: 0.6190476190476191, 39: 0.12121212121212122}
Micro-average F1 score: 0.5058860960865416
Weighted-average F1 score: 0.4975186287683224
F1 score per class: {2: 0.4, 5: 0.6666666666666666, 6: 0.5384615384615384, 8: 0.2684563758389262, 10: 0.31794871794871793, 11: 0.36936936936936937, 12: 0.33865814696485624, 16: 0.6, 17: 0.3333333333333333, 18: 0.11494252873563218, 19: 0.7611336032388664, 20: 0.5473684210526316, 24: 0.2153846153846154, 26: 0.711340206185567, 28: 0.14492753623188406, 29: 0.8191489361702128, 30: 0.7391304347826086, 32: 0.615916955017301, 33: 0.3, 36: 0.4423076923076923, 39: 0.2}
Micro-average F1 score: 0.5092623779050185
Weighted-average F1 score: 0.5032116836911037

F1 score per class: {2: 0.0, 5: 0.7509881422924901, 6: 0.0, 8: 0.0, 10: 0.24060150375939848, 11: 0.0, 12: 0.0, 16: 0.49382716049382713, 17: 0.18181818181818182, 18: 0.04878048780487805, 19: 0.0, 20: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.35
Weighted-average F1 score: 0.30699375804721524
F1 score per class: {2: 0.0, 5: 0.5418994413407822, 6: 0.0, 8: 0.0, 10: 0.3803680981595092, 11: 0.0, 12: 0.0, 16: 0.45714285714285713, 17: 0.5555555555555556, 18: 0.32653061224489793, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.323062558356676
Weighted-average F1 score: 0.2804746260158792
F1 score per class: {2: 0.0, 5: 0.5630498533724341, 6: 0.0, 8: 0.0, 10: 0.3780487804878049, 11: 0.0, 12: 0.0, 16: 0.46601941747572817, 17: 0.47058823529411764, 18: 0.15151515151515152, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.3128054740957967
Weighted-average F1 score: 0.2705062909852699

F1 score per class: {2: 0.22857142857142856, 5: 0.6551724137931034, 6: 0.32448377581120946, 8: 0.1651376146788991, 10: 0.15763546798029557, 11: 0.17037037037037037, 12: 0.2012847965738758, 16: 0.40404040404040403, 17: 0.07692307692307693, 18: 0.031746031746031744, 19: 0.7478991596638656, 20: 0.3787878787878788, 24: 0.1891891891891892, 26: 0.6467661691542289, 28: 0.14925373134328357, 29: 0.7400881057268722, 30: 0.8205128205128205, 32: 0.5250737463126843, 33: 0.2857142857142857, 36: 0.16842105263157894, 39: 0.09090909090909091}
Micro-average F1 score: 0.3931320307874482
Weighted-average F1 score: 0.38780249910113773
F1 score per class: {2: 0.2, 5: 0.40165631469979296, 6: 0.3135593220338983, 8: 0.21052631578947367, 10: 0.23308270676691728, 11: 0.3148936170212766, 12: 0.17043478260869566, 16: 0.3356643356643357, 17: 0.2222222222222222, 18: 0.16, 19: 0.618421052631579, 20: 0.33766233766233766, 24: 0.12244897959183673, 26: 0.6060606060606061, 28: 0.09523809523809523, 29: 0.7475728155339806, 30: 0.3238095238095238, 32: 0.47643979057591623, 33: 0.2222222222222222, 36: 0.4105263157894737, 39: 0.07407407407407407}
Micro-average F1 score: 0.3439325113562622
Weighted-average F1 score: 0.3287350426332702
F1 score per class: {2: 0.25, 5: 0.42857142857142855, 6: 0.3197969543147208, 8: 0.21505376344086022, 10: 0.22302158273381295, 11: 0.2751677852348993, 12: 0.17785234899328858, 16: 0.3609022556390977, 17: 0.20512820512820512, 18: 0.0684931506849315, 19: 0.6738351254480287, 20: 0.3586206896551724, 24: 0.11290322580645161, 26: 0.6272727272727273, 28: 0.08403361344537816, 29: 0.7475728155339806, 30: 0.6538461538461539, 32: 0.46842105263157896, 33: 0.23076923076923078, 36: 0.3087248322147651, 39: 0.10909090909090909}
Micro-average F1 score: 0.3499190002314279
Weighted-average F1 score: 0.33387693015649955
cur_acc_wo_na:  ['0.7462', '0.5650', '0.5551', '0.5106']
his_acc_wo_na:  ['0.7462', '0.6742', '0.5798', '0.5327']
cur_acc des_wo_na:  ['0.7410', '0.5658', '0.4401', '0.5111']
his_acc des_wo_na:  ['0.7410', '0.6096', '0.5253', '0.5059']
cur_acc rrf_wo_na:  ['0.7581', '0.5828', '0.5027', '0.4923']
his_acc rrf_wo_na:  ['0.7581', '0.6311', '0.5513', '0.5093']
cur_acc_w_na:  ['0.6228', '0.4165', '0.3859', '0.3500']
his_acc_w_na:  ['0.6228', '0.5218', '0.4171', '0.3931']
cur_acc des_w_na:  ['0.5986', '0.3727', '0.2932', '0.3231']
his_acc des_w_na:  ['0.5986', '0.4292', '0.3536', '0.3439']
cur_acc rrf_w_na:  ['0.6158', '0.3854', '0.3341', '0.3128']
his_acc rrf_w_na:  ['0.6158', '0.4459', '0.3740', '0.3499']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 87.7604245CurrentTrain: epoch  0, batch     1 | loss: 97.6259484CurrentTrain: epoch  0, batch     2 | loss: 80.3405440CurrentTrain: epoch  0, batch     3 | loss: 139.8676324CurrentTrain: epoch  0, batch     4 | loss: 70.1396445CurrentTrain: epoch  1, batch     0 | loss: 79.6922783CurrentTrain: epoch  1, batch     1 | loss: 96.2287326CurrentTrain: epoch  1, batch     2 | loss: 170.2202301CurrentTrain: epoch  1, batch     3 | loss: 85.7940168CurrentTrain: epoch  1, batch     4 | loss: 56.7480754CurrentTrain: epoch  2, batch     0 | loss: 83.2621871CurrentTrain: epoch  2, batch     1 | loss: 83.8489391CurrentTrain: epoch  2, batch     2 | loss: 84.1584204CurrentTrain: epoch  2, batch     3 | loss: 96.4920435CurrentTrain: epoch  2, batch     4 | loss: 68.1297053CurrentTrain: epoch  3, batch     0 | loss: 64.7538454CurrentTrain: epoch  3, batch     1 | loss: 66.1956709CurrentTrain: epoch  3, batch     2 | loss: 83.4881739CurrentTrain: epoch  3, batch     3 | loss: 80.8287292CurrentTrain: epoch  3, batch     4 | loss: 134.7988904CurrentTrain: epoch  4, batch     0 | loss: 77.9404811CurrentTrain: epoch  4, batch     1 | loss: 77.7091121CurrentTrain: epoch  4, batch     2 | loss: 76.5089408CurrentTrain: epoch  4, batch     3 | loss: 76.7293459CurrentTrain: epoch  4, batch     4 | loss: 63.4165130CurrentTrain: epoch  5, batch     0 | loss: 76.9570320CurrentTrain: epoch  5, batch     1 | loss: 117.8435742CurrentTrain: epoch  5, batch     2 | loss: 89.2033279CurrentTrain: epoch  5, batch     3 | loss: 62.8867724CurrentTrain: epoch  5, batch     4 | loss: 43.1128721CurrentTrain: epoch  6, batch     0 | loss: 73.4054041CurrentTrain: epoch  6, batch     1 | loss: 78.0824031CurrentTrain: epoch  6, batch     2 | loss: 62.5475493CurrentTrain: epoch  6, batch     3 | loss: 73.7362716CurrentTrain: epoch  6, batch     4 | loss: 50.3420442CurrentTrain: epoch  7, batch     0 | loss: 71.2208666CurrentTrain: epoch  7, batch     1 | loss: 74.7168985CurrentTrain: epoch  7, batch     2 | loss: 74.0635149CurrentTrain: epoch  7, batch     3 | loss: 63.4180702CurrentTrain: epoch  7, batch     4 | loss: 62.5743825CurrentTrain: epoch  8, batch     0 | loss: 86.1735081CurrentTrain: epoch  8, batch     1 | loss: 113.4533353CurrentTrain: epoch  8, batch     2 | loss: 111.3193830CurrentTrain: epoch  8, batch     3 | loss: 59.7442133CurrentTrain: epoch  8, batch     4 | loss: 46.3826454CurrentTrain: epoch  9, batch     0 | loss: 60.9251071CurrentTrain: epoch  9, batch     1 | loss: 74.2235593CurrentTrain: epoch  9, batch     2 | loss: 70.9564234CurrentTrain: epoch  9, batch     3 | loss: 86.7133594CurrentTrain: epoch  9, batch     4 | loss: 62.1215398
MemoryTrain:  epoch  0, batch     0 | loss: 1.1488905MemoryTrain:  epoch  1, batch     0 | loss: 0.9478969MemoryTrain:  epoch  2, batch     0 | loss: 0.7248178MemoryTrain:  epoch  3, batch     0 | loss: 0.6985982MemoryTrain:  epoch  4, batch     0 | loss: 0.5213161MemoryTrain:  epoch  5, batch     0 | loss: 0.5205867MemoryTrain:  epoch  6, batch     0 | loss: 0.3926461MemoryTrain:  epoch  7, batch     0 | loss: 0.3315235MemoryTrain:  epoch  8, batch     0 | loss: 0.2968017MemoryTrain:  epoch  9, batch     0 | loss: 0.2200430

F1 score per class: {1: 0.20915032679738563, 3: 0.6732673267326733, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.08823529411764706, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.5126582278481012, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.6363636363636364, 36: 0.0}
Micro-average F1 score: 0.40384615384615385
Weighted-average F1 score: 0.37855531392931363
F1 score per class: {1: 0.22085889570552147, 3: 0.582995951417004, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.064, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.5114503816793893, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.6486486486486487, 36: 0.0}
Micro-average F1 score: 0.3492063492063492
Weighted-average F1 score: 0.31579370811375657
F1 score per class: {1: 0.2275449101796407, 3: 0.6192468619246861, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.07692307692307693, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.5192307692307693, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.6620689655172414, 36: 0.0}
Micro-average F1 score: 0.3882657463330457
Weighted-average F1 score: 0.36287215744916124

F1 score per class: {1: 0.1797752808988764, 2: 0.45454545454545453, 3: 0.48226950354609927, 5: 0.8468468468468469, 6: 0.543859649122807, 8: 0.32061068702290074, 10: 0.354978354978355, 11: 0.12871287128712872, 12: 0.33935018050541516, 14: 0.07894736842105263, 16: 0.5970149253731343, 17: 0.13333333333333333, 18: 0.038461538461538464, 19: 0.7068965517241379, 20: 0.5060240963855421, 22: 0.4080604534005038, 24: 0.1, 26: 0.7379679144385026, 28: 0.14925373134328357, 29: 0.8080808080808081, 30: 0.8947368421052632, 32: 0.5273311897106109, 33: 0.375, 34: 0.2616822429906542, 36: 0.3655913978494624, 39: 0.21428571428571427}
Micro-average F1 score: 0.45042778057372923
Weighted-average F1 score: 0.4421555667580004
F1 score per class: {1: 0.1875, 2: 0.4375, 3: 0.3902439024390244, 5: 0.697841726618705, 6: 0.5535055350553506, 8: 0.3592814371257485, 10: 0.35714285714285715, 11: 0.16352201257861634, 12: 0.31475409836065577, 14: 0.0427807486631016, 16: 0.5714285714285714, 17: 0.3076923076923077, 18: 0.06153846153846154, 19: 0.62, 20: 0.5287356321839081, 22: 0.432258064516129, 24: 0.10909090909090909, 26: 0.7035175879396985, 28: 0.14285714285714285, 29: 0.8, 30: 0.6666666666666666, 32: 0.5043988269794721, 33: 0.3333333333333333, 34: 0.20556745182012848, 36: 0.5985401459854015, 39: 0.24242424242424243}
Micro-average F1 score: 0.4203739522888459
Weighted-average F1 score: 0.40539534558435913
F1 score per class: {1: 0.19487179487179487, 2: 0.5, 3: 0.4054794520547945, 5: 0.7348484848484849, 6: 0.5182186234817814, 8: 0.39215686274509803, 10: 0.3453815261044177, 11: 0.17142857142857143, 12: 0.31210191082802546, 14: 0.056074766355140186, 16: 0.5822784810126582, 17: 0.25, 18: 0.06557377049180328, 19: 0.6308243727598566, 20: 0.5116279069767442, 22: 0.4122137404580153, 24: 0.14814814814814814, 26: 0.711340206185567, 28: 0.13333333333333333, 29: 0.8, 30: 0.8947368421052632, 32: 0.5014925373134328, 33: 0.35294117647058826, 34: 0.21768707482993196, 36: 0.55, 39: 0.24242424242424243}
Micro-average F1 score: 0.42857142857142855
Weighted-average F1 score: 0.4150850445055685

F1 score per class: {1: 0.11552346570397112, 2: 0.0, 3: 0.4981684981684982, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.08333333333333333, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.3960880195599022, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.47191011235955055, 36: 0.0}
Micro-average F1 score: 0.26448362720403024
Weighted-average F1 score: 0.24364387619018174
F1 score per class: {1: 0.125, 2: 0.0, 3: 0.384, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.056338028169014086, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.3941176470588235, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.47058823529411764, 36: 0.0}
Micro-average F1 score: 0.2268041237113402
Weighted-average F1 score: 0.20814994973764153
F1 score per class: {1: 0.13013698630136986, 2: 0.0, 3: 0.4111111111111111, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.06896551724137931, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.39803439803439805, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.4824120603015075, 36: 0.0}
Micro-average F1 score: 0.2515371716042482
Weighted-average F1 score: 0.23509743866019392

F1 score per class: {1: 0.09495548961424333, 2: 0.25, 3: 0.30357142857142855, 5: 0.5911949685534591, 6: 0.3229166666666667, 8: 0.25301204819277107, 10: 0.2546583850931677, 11: 0.09665427509293681, 12: 0.15640599001663893, 14: 0.06976744186046512, 16: 0.41237113402061853, 17: 0.09090909090909091, 18: 0.030303030303030304, 19: 0.6259541984732825, 20: 0.328125, 22: 0.2983425414364641, 24: 0.1, 26: 0.6540284360189573, 28: 0.07874015748031496, 29: 0.7079646017699115, 30: 0.85, 32: 0.3914081145584726, 33: 0.2608695652173913, 34: 0.15272727272727274, 36: 0.26153846153846155, 39: 0.1111111111111111}
Micro-average F1 score: 0.30395652912209203
Weighted-average F1 score: 0.28925400257878475
F1 score per class: {1: 0.1016949152542373, 2: 0.2413793103448276, 3: 0.22641509433962265, 5: 0.42637362637362636, 6: 0.29880478087649404, 8: 0.2553191489361702, 10: 0.24861878453038674, 11: 0.13471502590673576, 12: 0.15213946117274169, 14: 0.035555555555555556, 16: 0.3356643356643357, 17: 0.19047619047619047, 18: 0.043478260869565216, 19: 0.5166666666666667, 20: 0.3484848484848485, 22: 0.3145539906103286, 24: 0.07317073170731707, 26: 0.603448275862069, 28: 0.07462686567164178, 29: 0.7027027027027027, 30: 0.5454545454545454, 32: 0.3722943722943723, 33: 0.2222222222222222, 34: 0.11822660098522167, 36: 0.4079601990049751, 39: 0.1509433962264151}
Micro-average F1 score: 0.2740647330811265
Weighted-average F1 score: 0.26126345628980413
F1 score per class: {1: 0.10644257703081232, 2: 0.2857142857142857, 3: 0.23717948717948717, 5: 0.4597156398104265, 6: 0.2929061784897025, 8: 0.2857142857142857, 10: 0.23497267759562843, 11: 0.14150943396226415, 12: 0.14939024390243902, 14: 0.046153846153846156, 16: 0.368, 17: 0.14634146341463414, 18: 0.04819277108433735, 19: 0.55, 20: 0.3308270676691729, 22: 0.2977941176470588, 24: 0.12903225806451613, 26: 0.6188340807174888, 28: 0.06944444444444445, 29: 0.7027027027027027, 30: 0.8292682926829268, 32: 0.36923076923076925, 33: 0.2608695652173913, 34: 0.12260536398467432, 36: 0.3707865168539326, 39: 0.15384615384615385}
Micro-average F1 score: 0.28071709663314387
Weighted-average F1 score: 0.2666679551892998
cur_acc_wo_na:  ['0.7462', '0.5650', '0.5551', '0.5106', '0.4038']
his_acc_wo_na:  ['0.7462', '0.6742', '0.5798', '0.5327', '0.4504']
cur_acc des_wo_na:  ['0.7410', '0.5658', '0.4401', '0.5111', '0.3492']
his_acc des_wo_na:  ['0.7410', '0.6096', '0.5253', '0.5059', '0.4204']
cur_acc rrf_wo_na:  ['0.7581', '0.5828', '0.5027', '0.4923', '0.3883']
his_acc rrf_wo_na:  ['0.7581', '0.6311', '0.5513', '0.5093', '0.4286']
cur_acc_w_na:  ['0.6228', '0.4165', '0.3859', '0.3500', '0.2645']
his_acc_w_na:  ['0.6228', '0.5218', '0.4171', '0.3931', '0.3040']
cur_acc des_w_na:  ['0.5986', '0.3727', '0.2932', '0.3231', '0.2268']
his_acc des_w_na:  ['0.5986', '0.4292', '0.3536', '0.3439', '0.2741']
cur_acc rrf_w_na:  ['0.6158', '0.3854', '0.3341', '0.3128', '0.2515']
his_acc rrf_w_na:  ['0.6158', '0.4459', '0.3740', '0.3499', '0.2807']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 85.8960211CurrentTrain: epoch  0, batch     1 | loss: 98.1867299CurrentTrain: epoch  0, batch     2 | loss: 73.9441212CurrentTrain: epoch  0, batch     3 | loss: 17.0378508CurrentTrain: epoch  1, batch     0 | loss: 77.5377066CurrentTrain: epoch  1, batch     1 | loss: 70.7602532CurrentTrain: epoch  1, batch     2 | loss: 99.9542784CurrentTrain: epoch  1, batch     3 | loss: 15.3495328CurrentTrain: epoch  2, batch     0 | loss: 68.0243106CurrentTrain: epoch  2, batch     1 | loss: 64.2540733CurrentTrain: epoch  2, batch     2 | loss: 80.7739007CurrentTrain: epoch  2, batch     3 | loss: 9.8042927CurrentTrain: epoch  3, batch     0 | loss: 75.3127743CurrentTrain: epoch  3, batch     1 | loss: 62.3788384CurrentTrain: epoch  3, batch     2 | loss: 91.5068173CurrentTrain: epoch  3, batch     3 | loss: 19.0413356CurrentTrain: epoch  4, batch     0 | loss: 76.6549851CurrentTrain: epoch  4, batch     1 | loss: 61.5950613CurrentTrain: epoch  4, batch     2 | loss: 58.0927875CurrentTrain: epoch  4, batch     3 | loss: 6.2931571CurrentTrain: epoch  5, batch     0 | loss: 59.0877457CurrentTrain: epoch  5, batch     1 | loss: 73.1261946CurrentTrain: epoch  5, batch     2 | loss: 71.5603994CurrentTrain: epoch  5, batch     3 | loss: 18.6918293CurrentTrain: epoch  6, batch     0 | loss: 60.6280124CurrentTrain: epoch  6, batch     1 | loss: 61.5852049CurrentTrain: epoch  6, batch     2 | loss: 59.5677775CurrentTrain: epoch  6, batch     3 | loss: 4.2809816CurrentTrain: epoch  7, batch     0 | loss: 57.6556212CurrentTrain: epoch  7, batch     1 | loss: 59.4280269CurrentTrain: epoch  7, batch     2 | loss: 60.5499884CurrentTrain: epoch  7, batch     3 | loss: 17.6881029CurrentTrain: epoch  8, batch     0 | loss: 57.7656821CurrentTrain: epoch  8, batch     1 | loss: 56.7823254CurrentTrain: epoch  8, batch     2 | loss: 69.8344980CurrentTrain: epoch  8, batch     3 | loss: 20.1045077CurrentTrain: epoch  9, batch     0 | loss: 57.8429927CurrentTrain: epoch  9, batch     1 | loss: 57.2022578CurrentTrain: epoch  9, batch     2 | loss: 56.0846877CurrentTrain: epoch  9, batch     3 | loss: 14.0341869
MemoryTrain:  epoch  0, batch     0 | loss: 0.7683963MemoryTrain:  epoch  1, batch     0 | loss: 0.7057889MemoryTrain:  epoch  2, batch     0 | loss: 0.6034650MemoryTrain:  epoch  3, batch     0 | loss: 0.4400689MemoryTrain:  epoch  4, batch     0 | loss: 0.3426560MemoryTrain:  epoch  5, batch     0 | loss: 0.3126698MemoryTrain:  epoch  6, batch     0 | loss: 0.2522574MemoryTrain:  epoch  7, batch     0 | loss: 0.2122378MemoryTrain:  epoch  8, batch     0 | loss: 0.2065102MemoryTrain:  epoch  9, batch     0 | loss: 0.1675568

F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 3: 0.5714285714285714, 6: 0.819672131147541, 7: 0.0, 40: 0.0, 9: 0.0, 16: 0.0, 19: 0.0, 22: 0.2857142857142857, 26: 0.0, 27: 0.0, 31: 0.40298507462686567}
Micro-average F1 score: 0.36065573770491804
Weighted-average F1 score: 0.3029160632825426
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.6410256410256411, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 31: 0.18181818181818182, 32: 0.0, 34: 0.0, 40: 0.5526315789473685}
Micro-average F1 score: 0.40229885057471265
Weighted-average F1 score: 0.3527077211287738
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.6944444444444444, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 31: 0.2, 32: 0.0, 34: 0.0, 40: 0.5490196078431373}
Micro-average F1 score: 0.4093567251461988
Weighted-average F1 score: 0.3568156502367029

F1 score per class: {1: 0.14583333333333334, 2: 0.45454545454545453, 3: 0.5391304347826087, 5: 0.7901234567901234, 6: 0.36666666666666664, 7: 0.03636363636363636, 8: 0.16981132075471697, 9: 0.7692307692307693, 10: 0.2289156626506024, 11: 0.18, 12: 0.13664596273291926, 14: 0.08196721311475409, 16: 0.6101694915254238, 17: 0.0, 18: 0.03571428571428571, 19: 0.5793650793650794, 20: 0.5617977528089888, 22: 0.4357976653696498, 24: 0.1, 26: 0.6586826347305389, 27: 0.0, 28: 0.1388888888888889, 29: 0.776595744680851, 30: 0.8648648648648649, 31: 0.07407407407407407, 32: 0.5517241379310345, 33: 0.35294117647058826, 34: 0.3181818181818182, 36: 0.3132530120481928, 39: 0.18181818181818182, 40: 0.12949640287769784}
Micro-average F1 score: 0.3794428434197887
Weighted-average F1 score: 0.36342264709069233
F1 score per class: {1: 0.16113744075829384, 2: 0.29411764705882354, 3: 0.4982456140350877, 5: 0.6298701298701299, 6: 0.4433497536945813, 7: 0.05128205128205128, 8: 0.36220472440944884, 9: 0.45871559633027525, 10: 0.31137724550898205, 11: 0.1761006289308176, 12: 0.2131979695431472, 14: 0.060240963855421686, 16: 0.5057471264367817, 17: 0.3157894736842105, 18: 0.057971014492753624, 19: 0.5277777777777778, 20: 0.5681818181818182, 22: 0.4580152671755725, 24: 0.12, 26: 0.6951871657754011, 27: 0.0, 28: 0.11494252873563218, 29: 0.7717391304347826, 30: 0.6938775510204082, 31: 0.03636363636363636, 32: 0.5256410256410257, 33: 0.2857142857142857, 34: 0.2211764705882353, 36: 0.6290322580645161, 39: 0.14285714285714285, 40: 0.25149700598802394}
Micro-average F1 score: 0.38615741717661956
Weighted-average F1 score: 0.36798391018301924
F1 score per class: {1: 0.14285714285714285, 2: 0.38461538461538464, 3: 0.5, 5: 0.6576271186440678, 6: 0.43781094527363185, 7: 0.04938271604938271, 8: 0.31666666666666665, 9: 0.5681818181818182, 10: 0.30303030303030304, 11: 0.18518518518518517, 12: 0.20408163265306123, 14: 0.08484848484848485, 16: 0.6111111111111112, 17: 0.15384615384615385, 18: 0.0625, 19: 0.5757575757575758, 20: 0.5842696629213483, 22: 0.45652173913043476, 24: 0.08333333333333333, 26: 0.7103825136612022, 27: 0.0, 28: 0.11627906976744186, 29: 0.7717391304347826, 30: 0.8205128205128205, 31: 0.046511627906976744, 32: 0.54, 33: 0.2857142857142857, 34: 0.23232323232323232, 36: 0.5, 39: 0.14285714285714285, 40: 0.225201072386059}
Micro-average F1 score: 0.3897749617653485
Weighted-average F1 score: 0.3711414431231152

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.7575757575757576, 11: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.25, 32: 0.0, 34: 0.0, 40: 0.3673469387755102}
Micro-average F1 score: 0.3005464480874317
Weighted-average F1 score: 0.24589799844901888
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.5434782608695652, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.15384615384615385, 32: 0.0, 34: 0.0, 36: 0.0, 40: 0.48}
Micro-average F1 score: 0.311804008908686
Weighted-average F1 score: 0.26884994045539146
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.625, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.16666666666666666, 32: 0.0, 34: 0.0, 36: 0.0, 40: 0.4772727272727273}
Micro-average F1 score: 0.3218390804597701
Weighted-average F1 score: 0.2744703375544497

F1 score per class: {1: 0.07865168539325842, 2: 0.2631578947368421, 3: 0.3220779220779221, 5: 0.553314121037464, 6: 0.2323943661971831, 7: 0.022598870056497175, 8: 0.15, 9: 0.6666666666666666, 10: 0.20212765957446807, 11: 0.11764705882352941, 12: 0.09482758620689655, 14: 0.07092198581560284, 16: 0.42857142857142855, 17: 0.0, 18: 0.02857142857142857, 19: 0.5367647058823529, 20: 0.3472222222222222, 22: 0.32748538011695905, 24: 0.09523809523809523, 26: 0.6043956043956044, 27: 0.0, 28: 0.06666666666666667, 29: 0.6759259259259259, 30: 0.8205128205128205, 31: 0.0392156862745098, 32: 0.4155844155844156, 33: 0.2727272727272727, 34: 0.19090909090909092, 36: 0.2708333333333333, 39: 0.12121212121212122, 40: 0.10207939508506617}
Micro-average F1 score: 0.27316735822959887
Weighted-average F1 score: 0.256805941322543
F1 score per class: {1: 0.08994708994708994, 2: 0.16393442622950818, 3: 0.3008474576271186, 5: 0.4, 6: 0.2631578947368421, 7: 0.03076923076923077, 8: 0.27218934911242604, 9: 0.34965034965034963, 10: 0.23529411764705882, 11: 0.15217391304347827, 12: 0.12280701754385964, 14: 0.050505050505050504, 16: 0.3142857142857143, 17: 0.1875, 18: 0.03571428571428571, 19: 0.4676923076923077, 20: 0.3424657534246575, 22: 0.3314917127071823, 24: 0.09230769230769231, 26: 0.5963302752293578, 27: 0.0, 28: 0.05952380952380952, 29: 0.6794258373205742, 30: 0.5862068965517241, 31: 0.022222222222222223, 32: 0.3942307692307692, 33: 0.21052631578947367, 34: 0.12806539509536785, 36: 0.430939226519337, 39: 0.0851063829787234, 40: 0.18961625282167044}
Micro-average F1 score: 0.26422177302916544
Weighted-average F1 score: 0.2501248227097939
F1 score per class: {1: 0.08042895442359249, 2: 0.21739130434782608, 3: 0.302771855010661, 5: 0.43207126948775054, 6: 0.25958702064896755, 7: 0.029411764705882353, 8: 0.25333333333333335, 9: 0.46296296296296297, 10: 0.22935779816513763, 11: 0.15625, 12: 0.12012012012012012, 14: 0.06896551724137931, 16: 0.3793103448275862, 17: 0.08695652173913043, 18: 0.04, 19: 0.5259515570934256, 20: 0.3466666666666667, 22: 0.3324538258575198, 24: 0.06896551724137931, 26: 0.6310679611650486, 27: 0.0, 28: 0.05952380952380952, 29: 0.6794258373205742, 30: 0.7441860465116279, 31: 0.02702702702702703, 32: 0.405, 33: 0.2, 34: 0.13199426111908177, 36: 0.3472222222222222, 39: 0.08333333333333333, 40: 0.17073170731707318}
Micro-average F1 score: 0.26863424183104956
Weighted-average F1 score: 0.2526352153203921
cur_acc_wo_na:  ['0.7462', '0.5650', '0.5551', '0.5106', '0.4038', '0.3607']
his_acc_wo_na:  ['0.7462', '0.6742', '0.5798', '0.5327', '0.4504', '0.3794']
cur_acc des_wo_na:  ['0.7410', '0.5658', '0.4401', '0.5111', '0.3492', '0.4023']
his_acc des_wo_na:  ['0.7410', '0.6096', '0.5253', '0.5059', '0.4204', '0.3862']
cur_acc rrf_wo_na:  ['0.7581', '0.5828', '0.5027', '0.4923', '0.3883', '0.4094']
his_acc rrf_wo_na:  ['0.7581', '0.6311', '0.5513', '0.5093', '0.4286', '0.3898']
cur_acc_w_na:  ['0.6228', '0.4165', '0.3859', '0.3500', '0.2645', '0.3005']
his_acc_w_na:  ['0.6228', '0.5218', '0.4171', '0.3931', '0.3040', '0.2732']
cur_acc des_w_na:  ['0.5986', '0.3727', '0.2932', '0.3231', '0.2268', '0.3118']
his_acc des_w_na:  ['0.5986', '0.4292', '0.3536', '0.3439', '0.2741', '0.2642']
cur_acc rrf_w_na:  ['0.6158', '0.3854', '0.3341', '0.3128', '0.2515', '0.3218']
his_acc rrf_w_na:  ['0.6158', '0.4459', '0.3740', '0.3499', '0.2807', '0.2686']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 90.4399357CurrentTrain: epoch  0, batch     1 | loss: 82.3908756CurrentTrain: epoch  0, batch     2 | loss: 84.0791739CurrentTrain: epoch  0, batch     3 | loss: 69.7328216CurrentTrain: epoch  1, batch     0 | loss: 72.9980176CurrentTrain: epoch  1, batch     1 | loss: 84.1349863CurrentTrain: epoch  1, batch     2 | loss: 85.7006833CurrentTrain: epoch  1, batch     3 | loss: 65.5029866CurrentTrain: epoch  2, batch     0 | loss: 64.8208306CurrentTrain: epoch  2, batch     1 | loss: 65.7519767CurrentTrain: epoch  2, batch     2 | loss: 79.9065534CurrentTrain: epoch  2, batch     3 | loss: 81.8259818CurrentTrain: epoch  3, batch     0 | loss: 73.5167933CurrentTrain: epoch  3, batch     1 | loss: 61.6509088CurrentTrain: epoch  3, batch     2 | loss: 95.1520476CurrentTrain: epoch  3, batch     3 | loss: 43.8748137CurrentTrain: epoch  4, batch     0 | loss: 70.0700604CurrentTrain: epoch  4, batch     1 | loss: 72.0234100CurrentTrain: epoch  4, batch     2 | loss: 89.5766636CurrentTrain: epoch  4, batch     3 | loss: 53.7636095CurrentTrain: epoch  5, batch     0 | loss: 60.3534673CurrentTrain: epoch  5, batch     1 | loss: 90.9195465CurrentTrain: epoch  5, batch     2 | loss: 72.0716191CurrentTrain: epoch  5, batch     3 | loss: 79.8260847CurrentTrain: epoch  6, batch     0 | loss: 58.7973418CurrentTrain: epoch  6, batch     1 | loss: 91.7517455CurrentTrain: epoch  6, batch     2 | loss: 75.0775789CurrentTrain: epoch  6, batch     3 | loss: 40.7236096CurrentTrain: epoch  7, batch     0 | loss: 71.1422000CurrentTrain: epoch  7, batch     1 | loss: 69.5093828CurrentTrain: epoch  7, batch     2 | loss: 60.3599393CurrentTrain: epoch  7, batch     3 | loss: 58.8756173CurrentTrain: epoch  8, batch     0 | loss: 109.0432381CurrentTrain: epoch  8, batch     1 | loss: 70.4036630CurrentTrain: epoch  8, batch     2 | loss: 61.7838912CurrentTrain: epoch  8, batch     3 | loss: 59.8815210CurrentTrain: epoch  9, batch     0 | loss: 56.1790640CurrentTrain: epoch  9, batch     1 | loss: 67.1343905CurrentTrain: epoch  9, batch     2 | loss: 73.0378580CurrentTrain: epoch  9, batch     3 | loss: 47.3736289
MemoryTrain:  epoch  0, batch     0 | loss: 0.8158115MemoryTrain:  epoch  1, batch     0 | loss: 0.7615534MemoryTrain:  epoch  2, batch     0 | loss: 0.5837856MemoryTrain:  epoch  3, batch     0 | loss: 0.5167465MemoryTrain:  epoch  4, batch     0 | loss: 0.4397842MemoryTrain:  epoch  5, batch     0 | loss: 0.3769614MemoryTrain:  epoch  6, batch     0 | loss: 0.3242786MemoryTrain:  epoch  7, batch     0 | loss: 0.2888237MemoryTrain:  epoch  8, batch     0 | loss: 0.2537865MemoryTrain:  epoch  9, batch     0 | loss: 0.2068456

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.8181818181818182, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.375, 26: 0.0, 27: 0.0, 28: 0.0, 32: 0.0, 34: 0.0, 35: 0.8653846153846154, 37: 0.6153846153846154, 38: 0.55, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4976303317535545
Weighted-average F1 score: 0.3885090023099073
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 24: 0.0, 25: 0.55, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7878787878787878, 36: 0.0, 37: 0.4264705882352941, 38: 0.6086956521739131, 40: 0.0}
Micro-average F1 score: 0.39015817223198596
Weighted-average F1 score: 0.28239076520795636
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.7272727272727273, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.4657534246575342, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7766990291262136, 36: 0.0, 37: 0.4838709677419355, 38: 0.5714285714285714, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4045368620037807
Weighted-average F1 score: 0.29836642102173144

F1 score per class: {1: 0.1574074074074074, 2: 0.4, 3: 0.2777777777777778, 5: 0.7866108786610879, 6: 0.36363636363636365, 7: 0.04878048780487805, 8: 0.18181818181818182, 9: 0.8064516129032258, 10: 0.2634730538922156, 11: 0.13580246913580246, 12: 0.15217391304347827, 14: 0.04819277108433735, 15: 0.3103448275862069, 16: 0.6268656716417911, 17: 0.0, 18: 0.038461538461538464, 19: 0.5679012345679012, 20: 0.27692307692307694, 22: 0.42323651452282157, 24: 0.09523809523809523, 25: 0.375, 26: 0.6818181818181818, 27: 0.0, 28: 0.16129032258064516, 29: 0.7979274611398963, 30: 0.8421052631578947, 31: 0.09523809523809523, 32: 0.550185873605948, 33: 0.375, 34: 0.26153846153846155, 35: 0.3125, 36: 0.058823529411764705, 37: 0.18421052631578946, 38: 0.27848101265822783, 39: 0.0, 40: 0.21081081081081082}
Micro-average F1 score: 0.35589856670341785
Weighted-average F1 score: 0.3504087858840849
F1 score per class: {1: 0.16170212765957448, 2: 0.35294117647058826, 3: 0.42962962962962964, 5: 0.5969230769230769, 6: 0.42790697674418604, 7: 0.06349206349206349, 8: 0.3142857142857143, 9: 0.43103448275862066, 10: 0.3392857142857143, 11: 0.20408163265306123, 12: 0.1722488038277512, 14: 0.08333333333333333, 15: 0.34146341463414637, 16: 0.5142857142857142, 17: 0.3333333333333333, 18: 0.12121212121212122, 19: 0.4878048780487805, 20: 0.475, 22: 0.43724696356275305, 24: 0.11538461538461539, 25: 0.5301204819277109, 26: 0.6631578947368421, 27: 0.0, 28: 0.10989010989010989, 29: 0.7894736842105263, 30: 0.6415094339622641, 31: 0.034482758620689655, 32: 0.5512367491166078, 33: 0.35294117647058826, 34: 0.22602739726027396, 35: 0.3236514522821577, 36: 0.4864864864864865, 37: 0.09931506849315068, 38: 0.2857142857142857, 39: 0.0, 40: 0.2625}
Micro-average F1 score: 0.3469352318110783
Weighted-average F1 score: 0.3252133985065796
F1 score per class: {1: 0.18025751072961374, 2: 0.375, 3: 0.4827586206896552, 5: 0.6378737541528239, 6: 0.4131455399061033, 7: 0.0625, 8: 0.2900763358778626, 9: 0.47619047619047616, 10: 0.30612244897959184, 11: 0.18064516129032257, 12: 0.17272727272727273, 14: 0.1037037037037037, 15: 0.32, 16: 0.6, 17: 0.0, 18: 0.03508771929824561, 19: 0.5574912891986062, 20: 0.4675324675324675, 22: 0.46494464944649444, 24: 0.07407407407407407, 25: 0.4594594594594595, 26: 0.6810810810810811, 27: 0.0, 28: 0.13043478260869565, 29: 0.7853403141361257, 30: 0.7727272727272727, 31: 0.047619047619047616, 32: 0.5390070921985816, 33: 0.3333333333333333, 34: 0.2445414847161572, 35: 0.2888086642599278, 36: 0.23529411764705882, 37: 0.10752688172043011, 38: 0.25263157894736843, 39: 0.0, 40: 0.23497267759562843}
Micro-average F1 score: 0.3498168498168498
Weighted-average F1 score: 0.3314077513367701

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.6, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.36923076923076925, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7377049180327869, 37: 0.5283018867924528, 38: 0.5116279069767442, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3602058319039451
Weighted-average F1 score: 0.2647993575325442
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.5, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.0, 25: 0.5238095238095238, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6902654867256637, 36: 0.0, 37: 0.3625, 38: 0.4827586206896552, 40: 0.0}
Micro-average F1 score: 0.2717258261933905
Weighted-average F1 score: 0.19365442136296357
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.5517241379310345, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.4473684210526316, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6837606837606838, 36: 0.0, 37: 0.3973509933774834, 38: 0.4528301886792453, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.28194993412384717
Weighted-average F1 score: 0.20399193721951622

F1 score per class: {1: 0.08585858585858586, 2: 0.2127659574468085, 3: 0.20202020202020202, 5: 0.6064516129032258, 6: 0.21794871794871795, 7: 0.03278688524590164, 8: 0.15625, 9: 0.684931506849315, 10: 0.2328042328042328, 11: 0.10232558139534884, 12: 0.09180327868852459, 14: 0.0425531914893617, 15: 0.2, 16: 0.3925233644859813, 17: 0.0, 18: 0.03076923076923077, 19: 0.5207547169811321, 20: 0.18947368421052632, 22: 0.31097560975609756, 24: 0.09523809523809523, 25: 0.36923076923076925, 26: 0.6060606060606061, 27: 0.0, 28: 0.07751937984496124, 29: 0.6637931034482759, 30: 0.8, 31: 0.046511627906976744, 32: 0.42528735632183906, 33: 0.2727272727272727, 34: 0.15384615384615385, 35: 0.18404907975460122, 36: 0.05555555555555555, 37: 0.11359026369168357, 38: 0.1476510067114094, 39: 0.0, 40: 0.1455223880597015}
Micro-average F1 score: 0.2502325581395349
Weighted-average F1 score: 0.2362685391131042
F1 score per class: {1: 0.08816705336426914, 2: 0.19672131147540983, 3: 0.27884615384615385, 5: 0.4127659574468085, 6: 0.2569832402234637, 7: 0.039603960396039604, 8: 0.22110552763819097, 9: 0.3246753246753247, 10: 0.2389937106918239, 11: 0.1724137931034483, 12: 0.0962566844919786, 14: 0.06751054852320675, 15: 0.21875, 16: 0.3103448275862069, 17: 0.20689655172413793, 18: 0.06779661016949153, 19: 0.42105263157894735, 20: 0.2695035460992908, 22: 0.32432432432432434, 24: 0.08, 25: 0.4536082474226804, 26: 0.5575221238938053, 27: 0.0, 28: 0.056179775280898875, 29: 0.6493506493506493, 30: 0.53125, 31: 0.018691588785046728, 32: 0.41160949868073876, 33: 0.24, 34: 0.14012738853503184, 35: 0.20418848167539266, 36: 0.32142857142857145, 37: 0.06799531066822978, 38: 0.1590909090909091, 39: 0.0, 40: 0.17573221757322174}
Micro-average F1 score: 0.23539114043355325
Weighted-average F1 score: 0.2214094985936793
F1 score per class: {1: 0.09859154929577464, 2: 0.2222222222222222, 3: 0.3373493975903614, 5: 0.4507042253521127, 6: 0.24788732394366197, 7: 0.038834951456310676, 8: 0.2159090909090909, 9: 0.4065040650406504, 10: 0.24489795918367346, 11: 0.14893617021276595, 12: 0.09669211195928754, 14: 0.08536585365853659, 15: 0.20512820512820512, 16: 0.35294117647058826, 17: 0.0, 18: 0.022988505747126436, 19: 0.4984423676012461, 20: 0.26865671641791045, 22: 0.3351063829787234, 24: 0.0625, 25: 0.4146341463414634, 26: 0.5887850467289719, 27: 0.0, 28: 0.06557377049180328, 29: 0.6550218340611353, 30: 0.68, 31: 0.024096385542168676, 32: 0.40425531914893614, 33: 0.21428571428571427, 34: 0.14507772020725387, 35: 0.17738359201773837, 36: 0.1941747572815534, 37: 0.06857142857142857, 38: 0.13636363636363635, 39: 0.0, 40: 0.15837937384898712}
Micro-average F1 score: 0.2388097024256064
Weighted-average F1 score: 0.22344028683229347
cur_acc_wo_na:  ['0.7462', '0.5650', '0.5551', '0.5106', '0.4038', '0.3607', '0.4976']
his_acc_wo_na:  ['0.7462', '0.6742', '0.5798', '0.5327', '0.4504', '0.3794', '0.3559']
cur_acc des_wo_na:  ['0.7410', '0.5658', '0.4401', '0.5111', '0.3492', '0.4023', '0.3902']
his_acc des_wo_na:  ['0.7410', '0.6096', '0.5253', '0.5059', '0.4204', '0.3862', '0.3469']
cur_acc rrf_wo_na:  ['0.7581', '0.5828', '0.5027', '0.4923', '0.3883', '0.4094', '0.4045']
his_acc rrf_wo_na:  ['0.7581', '0.6311', '0.5513', '0.5093', '0.4286', '0.3898', '0.3498']
cur_acc_w_na:  ['0.6228', '0.4165', '0.3859', '0.3500', '0.2645', '0.3005', '0.3602']
his_acc_w_na:  ['0.6228', '0.5218', '0.4171', '0.3931', '0.3040', '0.2732', '0.2502']
cur_acc des_w_na:  ['0.5986', '0.3727', '0.2932', '0.3231', '0.2268', '0.3118', '0.2717']
his_acc des_w_na:  ['0.5986', '0.4292', '0.3536', '0.3439', '0.2741', '0.2642', '0.2354']
cur_acc rrf_w_na:  ['0.6158', '0.3854', '0.3341', '0.3128', '0.2515', '0.3218', '0.2819']
his_acc rrf_w_na:  ['0.6158', '0.4459', '0.3740', '0.3499', '0.2807', '0.2686', '0.2388']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 98.6933771CurrentTrain: epoch  0, batch     1 | loss: 149.2797266CurrentTrain: epoch  0, batch     2 | loss: 80.4816097CurrentTrain: epoch  0, batch     3 | loss: 78.7833370CurrentTrain: epoch  1, batch     0 | loss: 85.1515733CurrentTrain: epoch  1, batch     1 | loss: 78.8080658CurrentTrain: epoch  1, batch     2 | loss: 74.4219313CurrentTrain: epoch  1, batch     3 | loss: 69.1054841CurrentTrain: epoch  2, batch     0 | loss: 106.5652115CurrentTrain: epoch  2, batch     1 | loss: 72.1372642CurrentTrain: epoch  2, batch     2 | loss: 78.5634285CurrentTrain: epoch  2, batch     3 | loss: 80.8614535CurrentTrain: epoch  3, batch     0 | loss: 99.9288341CurrentTrain: epoch  3, batch     1 | loss: 83.9459340CurrentTrain: epoch  3, batch     2 | loss: 75.7696318CurrentTrain: epoch  3, batch     3 | loss: 63.0008991CurrentTrain: epoch  4, batch     0 | loss: 75.0103488CurrentTrain: epoch  4, batch     1 | loss: 92.7685366CurrentTrain: epoch  4, batch     2 | loss: 124.7723045CurrentTrain: epoch  4, batch     3 | loss: 62.5668925CurrentTrain: epoch  5, batch     0 | loss: 71.9123314CurrentTrain: epoch  5, batch     1 | loss: 76.2729169CurrentTrain: epoch  5, batch     2 | loss: 82.3030824CurrentTrain: epoch  5, batch     3 | loss: 71.3534447CurrentTrain: epoch  6, batch     0 | loss: 95.4242320CurrentTrain: epoch  6, batch     1 | loss: 63.0286629CurrentTrain: epoch  6, batch     2 | loss: 72.2030946CurrentTrain: epoch  6, batch     3 | loss: 60.1848408CurrentTrain: epoch  7, batch     0 | loss: 76.1278384CurrentTrain: epoch  7, batch     1 | loss: 61.9731323CurrentTrain: epoch  7, batch     2 | loss: 71.0248114CurrentTrain: epoch  7, batch     3 | loss: 74.5492817CurrentTrain: epoch  8, batch     0 | loss: 94.7544654CurrentTrain: epoch  8, batch     1 | loss: 71.4248832CurrentTrain: epoch  8, batch     2 | loss: 73.6175762CurrentTrain: epoch  8, batch     3 | loss: 55.6504387CurrentTrain: epoch  9, batch     0 | loss: 61.3438731CurrentTrain: epoch  9, batch     1 | loss: 68.4953096CurrentTrain: epoch  9, batch     2 | loss: 91.0966165CurrentTrain: epoch  9, batch     3 | loss: 71.0039327
MemoryTrain:  epoch  0, batch     0 | loss: 0.8473849MemoryTrain:  epoch  1, batch     0 | loss: 0.7197044MemoryTrain:  epoch  2, batch     0 | loss: 0.5514269MemoryTrain:  epoch  3, batch     0 | loss: 0.4169428MemoryTrain:  epoch  4, batch     0 | loss: 0.3579391MemoryTrain:  epoch  5, batch     0 | loss: 0.3142098MemoryTrain:  epoch  6, batch     0 | loss: 0.2816564MemoryTrain:  epoch  7, batch     0 | loss: 0.2660751MemoryTrain:  epoch  8, batch     0 | loss: 0.2339832MemoryTrain:  epoch  9, batch     0 | loss: 0.1839309

F1 score per class: {0: 0.8947368421052632, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8522727272727273, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.25, 15: 0.0, 19: 0.0, 21: 0.4090909090909091, 23: 0.8043478260869565, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.6181102362204725
Weighted-average F1 score: 0.4997614517601646
F1 score per class: {0: 0.7659574468085106, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7692307692307693, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.15384615384615385, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.43333333333333335, 22: 0.0, 23: 0.7294117647058823, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.47425897035881437
Weighted-average F1 score: 0.35689211520491115
F1 score per class: {0: 0.8089887640449438, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8361581920903954, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.16, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.5, 22: 0.0, 23: 0.7415730337078652, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.5231788079470199
Weighted-average F1 score: 0.3940043331693434

F1 score per class: {0: 0.4788732394366197, 1: 0.14583333333333334, 2: 0.3076923076923077, 3: 0.3380281690140845, 4: 0.8522727272727273, 5: 0.7916666666666666, 6: 0.3316062176165803, 7: 0.0, 8: 0.06593406593406594, 9: 0.8333333333333334, 10: 0.16058394160583941, 11: 0.16352201257861634, 12: 0.0, 13: 0.03007518796992481, 14: 0.0, 15: 0.4, 16: 0.6296296296296297, 17: 0.0, 18: 0.0392156862745098, 19: 0.5118483412322274, 20: 0.475, 21: 0.10344827586206896, 22: 0.2097902097902098, 23: 0.7047619047619048, 24: 0.09523809523809523, 25: 0.3880597014925373, 26: 0.6588235294117647, 27: 0.0, 28: 0.1, 29: 0.7772020725388601, 30: 0.8421052631578947, 31: 0.1111111111111111, 32: 0.5461254612546126, 33: 0.3157894736842105, 34: 0.2677165354330709, 35: 0.3524590163934426, 36: 0.029850746268656716, 37: 0.20512820512820512, 38: 0.3333333333333333, 39: 0.0, 40: 0.2894736842105263}
Micro-average F1 score: 0.3646546359676416
Weighted-average F1 score: 0.36055311239924676
F1 score per class: {0: 0.2696629213483146, 1: 0.1735159817351598, 2: 0.1518987341772152, 3: 0.4674329501915709, 4: 0.7486631016042781, 5: 0.632258064516129, 6: 0.39069767441860465, 7: 0.037037037037037035, 8: 0.3310344827586207, 9: 0.5208333333333334, 10: 0.26737967914438504, 11: 0.174496644295302, 12: 0.12290502793296089, 13: 0.02197802197802198, 14: 0.07547169811320754, 15: 0.41379310344827586, 16: 0.5393258426966292, 17: 0.14285714285714285, 18: 0.15151515151515152, 19: 0.5102040816326531, 20: 0.46511627906976744, 21: 0.11016949152542373, 22: 0.25316455696202533, 23: 0.6526315789473685, 24: 0.08333333333333333, 25: 0.5063291139240507, 26: 0.63, 27: 0.0, 28: 0.12307692307692308, 29: 0.7692307692307693, 30: 0.6666666666666666, 31: 0.07692307692307693, 32: 0.5276872964169381, 33: 0.3157894736842105, 34: 0.24242424242424243, 35: 0.340080971659919, 36: 0.375, 37: 0.1366120218579235, 38: 0.3008849557522124, 39: 0.0, 40: 0.32234432234432236}
Micro-average F1 score: 0.3514043109079033
Weighted-average F1 score: 0.3306585872098969
F1 score per class: {0: 0.3076923076923077, 1: 0.1643835616438356, 2: 0.1935483870967742, 3: 0.48868778280542985, 4: 0.8314606741573034, 5: 0.6644067796610169, 6: 0.3783783783783784, 7: 0.041666666666666664, 8: 0.2857142857142857, 9: 0.7246376811594203, 10: 0.23952095808383234, 11: 0.19875776397515527, 12: 0.11891891891891893, 13: 0.020833333333333332, 14: 0.04395604395604396, 15: 0.42424242424242425, 16: 0.64, 17: 0.0, 18: 0.15625, 19: 0.5791505791505791, 20: 0.4819277108433735, 21: 0.11403508771929824, 22: 0.267515923566879, 23: 0.66, 24: 0.09090909090909091, 25: 0.5128205128205128, 26: 0.6596858638743456, 27: 0.0, 28: 0.11267605633802817, 29: 0.7653061224489796, 30: 0.7555555555555555, 31: 0.08695652173913043, 32: 0.535593220338983, 33: 0.3, 34: 0.21212121212121213, 35: 0.33204633204633205, 36: 0.25316455696202533, 37: 0.135, 38: 0.2857142857142857, 39: 0.0, 40: 0.3010033444816054}
Micro-average F1 score: 0.3575209151442718
Weighted-average F1 score: 0.3342296262591224

F1 score per class: {0: 0.85, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.819672131147541, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.125, 15: 0.0, 19: 0.0, 20: 0.0, 21: 0.29508196721311475, 23: 0.7047619047619048, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.4707646176911544
Weighted-average F1 score: 0.3527660769078967
F1 score per class: {0: 0.6792452830188679, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7291666666666666, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.08695652173913043, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.30952380952380953, 22: 0.0, 23: 0.6326530612244898, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3329682365826944
Weighted-average F1 score: 0.23980742176067885
F1 score per class: {0: 0.72, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7956989247311828, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.08, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.34210526315789475, 22: 0.0, 23: 0.6470588235294118, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3665893271461717
Weighted-average F1 score: 0.26226336396122213

F1 score per class: {0: 0.3238095238095238, 1: 0.0784313725490196, 2: 0.19047619047619047, 3: 0.24489795918367346, 4: 0.8064516129032258, 5: 0.6148867313915858, 6: 0.2064516129032258, 7: 0.0, 8: 0.0625, 9: 0.7142857142857143, 10: 0.1527777777777778, 11: 0.12745098039215685, 12: 0.0, 13: 0.0149812734082397, 14: 0.0, 15: 0.24561403508771928, 16: 0.5074626865671642, 17: 0.0, 18: 0.02666666666666667, 19: 0.4778761061946903, 20: 0.2733812949640288, 21: 0.07228915662650602, 22: 0.16759776536312848, 23: 0.578125, 24: 0.09090909090909091, 25: 0.37142857142857144, 26: 0.5863874345549738, 27: 0.0, 28: 0.06315789473684211, 29: 0.6410256410256411, 30: 0.7804878048780488, 31: 0.05263157894736842, 32: 0.4065934065934066, 33: 0.21428571428571427, 34: 0.16831683168316833, 35: 0.21608040201005024, 36: 0.028169014084507043, 37: 0.1206896551724138, 38: 0.176, 39: 0.0, 40: 0.24512534818941503}
Micro-average F1 score: 0.2640036041447665
Weighted-average F1 score: 0.24846341886700604
F1 score per class: {0: 0.16901408450704225, 1: 0.09313725490196079, 2: 0.10256410256410256, 3: 0.29901960784313725, 4: 0.6451612903225806, 5: 0.4365256124721604, 6: 0.23076923076923078, 7: 0.023121387283236993, 8: 0.2553191489361702, 9: 0.42016806722689076, 10: 0.18867924528301888, 11: 0.15028901734104047, 12: 0.07942238267148015, 13: 0.0111731843575419, 14: 0.06504065040650407, 15: 0.26666666666666666, 16: 0.31788079470198677, 17: 0.08333333333333333, 18: 0.09090909090909091, 19: 0.436046511627907, 20: 0.2631578947368421, 21: 0.07471264367816093, 22: 0.19607843137254902, 23: 0.5391304347826087, 24: 0.06896551724137931, 25: 0.46511627906976744, 26: 0.5431034482758621, 27: 0.0, 28: 0.0625, 29: 0.6024096385542169, 30: 0.5666666666666667, 31: 0.03508771929824561, 32: 0.38571428571428573, 33: 0.2222222222222222, 34: 0.1471264367816092, 35: 0.208955223880597, 36: 0.2647058823529412, 37: 0.08375209380234507, 38: 0.16346153846153846, 39: 0.0, 40: 0.24175824175824176}
Micro-average F1 score: 0.23800044238000442
Weighted-average F1 score: 0.2215502953606079
F1 score per class: {0: 0.18997361477572558, 1: 0.0891089108910891, 2: 0.1276595744680851, 3: 0.34615384615384615, 4: 0.7474747474747475, 5: 0.460093896713615, 6: 0.22641509433962265, 7: 0.02531645569620253, 8: 0.25, 9: 0.6097560975609756, 10: 0.19138755980861244, 11: 0.15841584158415842, 12: 0.07560137457044673, 13: 0.010416666666666666, 14: 0.038461538461538464, 15: 0.27450980392156865, 16: 0.366412213740458, 17: 0.0, 18: 0.0970873786407767, 19: 0.5154639175257731, 20: 0.2631578947368421, 21: 0.07580174927113703, 22: 0.21, 23: 0.55, 24: 0.08, 25: 0.47058823529411764, 26: 0.5806451612903226, 27: 0.0, 28: 0.057971014492753624, 29: 0.6224066390041494, 30: 0.6666666666666666, 31: 0.043478260869565216, 32: 0.39012345679012345, 33: 0.1935483870967742, 34: 0.11830985915492957, 35: 0.2, 36: 0.21505376344086022, 37: 0.08626198083067092, 38: 0.14678899082568808, 39: 0.0, 40: 0.22556390977443608}
Micro-average F1 score: 0.24428371441903873
Weighted-average F1 score: 0.2236319499805924
cur_acc_wo_na:  ['0.7462', '0.5650', '0.5551', '0.5106', '0.4038', '0.3607', '0.4976', '0.6181']
his_acc_wo_na:  ['0.7462', '0.6742', '0.5798', '0.5327', '0.4504', '0.3794', '0.3559', '0.3647']
cur_acc des_wo_na:  ['0.7410', '0.5658', '0.4401', '0.5111', '0.3492', '0.4023', '0.3902', '0.4743']
his_acc des_wo_na:  ['0.7410', '0.6096', '0.5253', '0.5059', '0.4204', '0.3862', '0.3469', '0.3514']
cur_acc rrf_wo_na:  ['0.7581', '0.5828', '0.5027', '0.4923', '0.3883', '0.4094', '0.4045', '0.5232']
his_acc rrf_wo_na:  ['0.7581', '0.6311', '0.5513', '0.5093', '0.4286', '0.3898', '0.3498', '0.3575']
cur_acc_w_na:  ['0.6228', '0.4165', '0.3859', '0.3500', '0.2645', '0.3005', '0.3602', '0.4708']
his_acc_w_na:  ['0.6228', '0.5218', '0.4171', '0.3931', '0.3040', '0.2732', '0.2502', '0.2640']
cur_acc des_w_na:  ['0.5986', '0.3727', '0.2932', '0.3231', '0.2268', '0.3118', '0.2717', '0.3330']
his_acc des_w_na:  ['0.5986', '0.4292', '0.3536', '0.3439', '0.2741', '0.2642', '0.2354', '0.2380']
cur_acc rrf_w_na:  ['0.6158', '0.3854', '0.3341', '0.3128', '0.2515', '0.3218', '0.2819', '0.3666']
his_acc rrf_w_na:  ['0.6158', '0.4459', '0.3740', '0.3499', '0.2807', '0.2686', '0.2388', '0.2443']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 85.3957536CurrentTrain: epoch  0, batch     1 | loss: 80.2011234CurrentTrain: epoch  0, batch     2 | loss: 103.1623606CurrentTrain: epoch  0, batch     3 | loss: 102.6207441CurrentTrain: epoch  0, batch     4 | loss: 87.7413718CurrentTrain: epoch  0, batch     5 | loss: 119.5712056CurrentTrain: epoch  0, batch     6 | loss: 100.6911606CurrentTrain: epoch  0, batch     7 | loss: 119.4995470CurrentTrain: epoch  0, batch     8 | loss: 86.4668453CurrentTrain: epoch  0, batch     9 | loss: 119.0858506CurrentTrain: epoch  0, batch    10 | loss: 86.1655839CurrentTrain: epoch  0, batch    11 | loss: 86.7034329CurrentTrain: epoch  0, batch    12 | loss: 86.5938148CurrentTrain: epoch  0, batch    13 | loss: 118.8417328CurrentTrain: epoch  0, batch    14 | loss: 146.8673227CurrentTrain: epoch  0, batch    15 | loss: 118.0180233CurrentTrain: epoch  0, batch    16 | loss: 118.7045686CurrentTrain: epoch  0, batch    17 | loss: 99.6757522CurrentTrain: epoch  0, batch    18 | loss: 145.8036130CurrentTrain: epoch  0, batch    19 | loss: 100.1312946CurrentTrain: epoch  0, batch    20 | loss: 76.3398734CurrentTrain: epoch  0, batch    21 | loss: 88.1253349CurrentTrain: epoch  0, batch    22 | loss: 87.0132737CurrentTrain: epoch  0, batch    23 | loss: 117.7956522CurrentTrain: epoch  0, batch    24 | loss: 116.8940380CurrentTrain: epoch  0, batch    25 | loss: 117.6389080CurrentTrain: epoch  0, batch    26 | loss: 85.1464804CurrentTrain: epoch  0, batch    27 | loss: 192.9225300CurrentTrain: epoch  0, batch    28 | loss: 99.1575553CurrentTrain: epoch  0, batch    29 | loss: 99.3218600CurrentTrain: epoch  0, batch    30 | loss: 98.9866158CurrentTrain: epoch  0, batch    31 | loss: 85.3586981CurrentTrain: epoch  0, batch    32 | loss: 117.9864881CurrentTrain: epoch  0, batch    33 | loss: 84.8140701CurrentTrain: epoch  0, batch    34 | loss: 98.3726360CurrentTrain: epoch  0, batch    35 | loss: 98.5123040CurrentTrain: epoch  0, batch    36 | loss: 117.5213995CurrentTrain: epoch  0, batch    37 | loss: 96.9689734CurrentTrain: epoch  0, batch    38 | loss: 98.0913017CurrentTrain: epoch  0, batch    39 | loss: 115.6801180CurrentTrain: epoch  0, batch    40 | loss: 84.1557744CurrentTrain: epoch  0, batch    41 | loss: 97.5137339CurrentTrain: epoch  0, batch    42 | loss: 145.2451102CurrentTrain: epoch  0, batch    43 | loss: 116.3677901CurrentTrain: epoch  0, batch    44 | loss: 97.0006560CurrentTrain: epoch  0, batch    45 | loss: 96.7806047CurrentTrain: epoch  0, batch    46 | loss: 83.6591843CurrentTrain: epoch  0, batch    47 | loss: 95.9257302CurrentTrain: epoch  0, batch    48 | loss: 95.1220290CurrentTrain: epoch  0, batch    49 | loss: 94.5363288CurrentTrain: epoch  0, batch    50 | loss: 83.4712118CurrentTrain: epoch  0, batch    51 | loss: 114.1221864CurrentTrain: epoch  0, batch    52 | loss: 72.3076126CurrentTrain: epoch  0, batch    53 | loss: 114.5713698CurrentTrain: epoch  0, batch    54 | loss: 93.2271412CurrentTrain: epoch  0, batch    55 | loss: 111.0083911CurrentTrain: epoch  0, batch    56 | loss: 140.7895487CurrentTrain: epoch  0, batch    57 | loss: 69.2261345CurrentTrain: epoch  0, batch    58 | loss: 110.7135581CurrentTrain: epoch  0, batch    59 | loss: 78.7222476CurrentTrain: epoch  0, batch    60 | loss: 109.7569401CurrentTrain: epoch  0, batch    61 | loss: 93.5136389CurrentTrain: epoch  0, batch    62 | loss: 93.1053457CurrentTrain: epoch  0, batch    63 | loss: 68.5949576CurrentTrain: epoch  0, batch    64 | loss: 112.1993241CurrentTrain: epoch  0, batch    65 | loss: 89.8092925CurrentTrain: epoch  0, batch    66 | loss: 92.2386802CurrentTrain: epoch  0, batch    67 | loss: 133.2420621CurrentTrain: epoch  0, batch    68 | loss: 106.0026424CurrentTrain: epoch  0, batch    69 | loss: 78.2486005CurrentTrain: epoch  0, batch    70 | loss: 76.3864617CurrentTrain: epoch  0, batch    71 | loss: 78.9364129CurrentTrain: epoch  0, batch    72 | loss: 110.0521376CurrentTrain: epoch  0, batch    73 | loss: 92.5166850CurrentTrain: epoch  0, batch    74 | loss: 75.2967773CurrentTrain: epoch  0, batch    75 | loss: 91.7007790CurrentTrain: epoch  0, batch    76 | loss: 68.1047149CurrentTrain: epoch  0, batch    77 | loss: 91.2807643CurrentTrain: epoch  0, batch    78 | loss: 79.7050685CurrentTrain: epoch  0, batch    79 | loss: 110.9316470CurrentTrain: epoch  0, batch    80 | loss: 91.0086329CurrentTrain: epoch  0, batch    81 | loss: 91.4913472CurrentTrain: epoch  0, batch    82 | loss: 79.4113809CurrentTrain: epoch  0, batch    83 | loss: 87.1710800CurrentTrain: epoch  0, batch    84 | loss: 106.2913228CurrentTrain: epoch  0, batch    85 | loss: 89.9677453CurrentTrain: epoch  0, batch    86 | loss: 89.0003334CurrentTrain: epoch  0, batch    87 | loss: 107.5394155CurrentTrain: epoch  0, batch    88 | loss: 91.4894288CurrentTrain: epoch  0, batch    89 | loss: 107.3161346CurrentTrain: epoch  0, batch    90 | loss: 91.2331797CurrentTrain: epoch  0, batch    91 | loss: 76.6195817CurrentTrain: epoch  0, batch    92 | loss: 77.8556538CurrentTrain: epoch  0, batch    93 | loss: 90.6965537CurrentTrain: epoch  0, batch    94 | loss: 89.8566061CurrentTrain: epoch  0, batch    95 | loss: 111.4638226CurrentTrain: epoch  1, batch     0 | loss: 76.5374677CurrentTrain: epoch  1, batch     1 | loss: 74.8331292CurrentTrain: epoch  1, batch     2 | loss: 106.8646991CurrentTrain: epoch  1, batch     3 | loss: 76.8333287CurrentTrain: epoch  1, batch     4 | loss: 73.2420122CurrentTrain: epoch  1, batch     5 | loss: 103.4319828CurrentTrain: epoch  1, batch     6 | loss: 90.4669141CurrentTrain: epoch  1, batch     7 | loss: 71.7033023CurrentTrain: epoch  1, batch     8 | loss: 78.5109845CurrentTrain: epoch  1, batch     9 | loss: 88.7468935CurrentTrain: epoch  1, batch    10 | loss: 103.8492105CurrentTrain: epoch  1, batch    11 | loss: 108.9858369CurrentTrain: epoch  1, batch    12 | loss: 103.9191342CurrentTrain: epoch  1, batch    13 | loss: 105.8835317CurrentTrain: epoch  1, batch    14 | loss: 87.9941046CurrentTrain: epoch  1, batch    15 | loss: 138.9900250CurrentTrain: epoch  1, batch    16 | loss: 86.8821662CurrentTrain: epoch  1, batch    17 | loss: 99.3110704CurrentTrain: epoch  1, batch    18 | loss: 101.9538274CurrentTrain: epoch  1, batch    19 | loss: 85.7524602CurrentTrain: epoch  1, batch    20 | loss: 180.9735241CurrentTrain: epoch  1, batch    21 | loss: 107.6796586CurrentTrain: epoch  1, batch    22 | loss: 89.2189071CurrentTrain: epoch  1, batch    23 | loss: 99.0864290CurrentTrain: epoch  1, batch    24 | loss: 103.6387343CurrentTrain: epoch  1, batch    25 | loss: 85.8080235CurrentTrain: epoch  1, batch    26 | loss: 79.8823390CurrentTrain: epoch  1, batch    27 | loss: 64.1446353CurrentTrain: epoch  1, batch    28 | loss: 104.9973252CurrentTrain: epoch  1, batch    29 | loss: 71.2162522CurrentTrain: epoch  1, batch    30 | loss: 73.2581648CurrentTrain: epoch  1, batch    31 | loss: 86.3767311CurrentTrain: epoch  1, batch    32 | loss: 65.9419623CurrentTrain: epoch  1, batch    33 | loss: 73.1028928CurrentTrain: epoch  1, batch    34 | loss: 86.2105753CurrentTrain: epoch  1, batch    35 | loss: 90.2524559CurrentTrain: epoch  1, batch    36 | loss: 86.0404002CurrentTrain: epoch  1, batch    37 | loss: 74.1894856CurrentTrain: epoch  1, batch    38 | loss: 65.0781144CurrentTrain: epoch  1, batch    39 | loss: 71.4525952CurrentTrain: epoch  1, batch    40 | loss: 81.6193594CurrentTrain: epoch  1, batch    41 | loss: 105.7597316CurrentTrain: epoch  1, batch    42 | loss: 80.2310552CurrentTrain: epoch  1, batch    43 | loss: 102.5579078CurrentTrain: epoch  1, batch    44 | loss: 72.1617487CurrentTrain: epoch  1, batch    45 | loss: 103.9800880CurrentTrain: epoch  1, batch    46 | loss: 65.0052489CurrentTrain: epoch  1, batch    47 | loss: 73.3956333CurrentTrain: epoch  1, batch    48 | loss: 88.5943453CurrentTrain: epoch  1, batch    49 | loss: 102.2441398CurrentTrain: epoch  1, batch    50 | loss: 68.8703116CurrentTrain: epoch  1, batch    51 | loss: 71.6559348CurrentTrain: epoch  1, batch    52 | loss: 104.6172249CurrentTrain: epoch  1, batch    53 | loss: 100.3638938CurrentTrain: epoch  1, batch    54 | loss: 82.1443648CurrentTrain: epoch  1, batch    55 | loss: 131.5164845CurrentTrain: epoch  1, batch    56 | loss: 85.9513158CurrentTrain: epoch  1, batch    57 | loss: 104.9690903CurrentTrain: epoch  1, batch    58 | loss: 178.1814828CurrentTrain: epoch  1, batch    59 | loss: 87.0178705CurrentTrain: epoch  1, batch    60 | loss: 106.9014750CurrentTrain: epoch  1, batch    61 | loss: 61.0875342CurrentTrain: epoch  1, batch    62 | loss: 73.6258396CurrentTrain: epoch  1, batch    63 | loss: 98.8848914CurrentTrain: epoch  1, batch    64 | loss: 86.9105790CurrentTrain: epoch  1, batch    65 | loss: 98.1656945CurrentTrain: epoch  1, batch    66 | loss: 69.1544477CurrentTrain: epoch  1, batch    67 | loss: 79.4542240CurrentTrain: epoch  1, batch    68 | loss: 129.0373570CurrentTrain: epoch  1, batch    69 | loss: 87.8176172CurrentTrain: epoch  1, batch    70 | loss: 85.8756351CurrentTrain: epoch  1, batch    71 | loss: 93.4481143CurrentTrain: epoch  1, batch    72 | loss: 80.9173726CurrentTrain: epoch  1, batch    73 | loss: 72.1608157CurrentTrain: epoch  1, batch    74 | loss: 101.3660171CurrentTrain: epoch  1, batch    75 | loss: 83.7431570CurrentTrain: epoch  1, batch    76 | loss: 103.7301557CurrentTrain: epoch  1, batch    77 | loss: 96.2799885CurrentTrain: epoch  1, batch    78 | loss: 75.6152644CurrentTrain: epoch  1, batch    79 | loss: 87.2881334CurrentTrain: epoch  1, batch    80 | loss: 81.6080745CurrentTrain: epoch  1, batch    81 | loss: 103.8810410CurrentTrain: epoch  1, batch    82 | loss: 65.9051610CurrentTrain: epoch  1, batch    83 | loss: 102.1147409CurrentTrain: epoch  1, batch    84 | loss: 84.7644407CurrentTrain: epoch  1, batch    85 | loss: 131.9816002CurrentTrain: epoch  1, batch    86 | loss: 70.1414529CurrentTrain: epoch  1, batch    87 | loss: 59.3696088CurrentTrain: epoch  1, batch    88 | loss: 100.0552458CurrentTrain: epoch  1, batch    89 | loss: 131.1878414CurrentTrain: epoch  1, batch    90 | loss: 83.5877334CurrentTrain: epoch  1, batch    91 | loss: 84.6230562CurrentTrain: epoch  1, batch    92 | loss: 72.2740643CurrentTrain: epoch  1, batch    93 | loss: 72.2842267CurrentTrain: epoch  1, batch    94 | loss: 69.2866954CurrentTrain: epoch  1, batch    95 | loss: 70.9019954CurrentTrain: epoch  2, batch     0 | loss: 81.7232116CurrentTrain: epoch  2, batch     1 | loss: 84.8428727CurrentTrain: epoch  2, batch     2 | loss: 77.3922629CurrentTrain: epoch  2, batch     3 | loss: 59.3931602CurrentTrain: epoch  2, batch     4 | loss: 124.1971078CurrentTrain: epoch  2, batch     5 | loss: 68.7018070CurrentTrain: epoch  2, batch     6 | loss: 58.8574696CurrentTrain: epoch  2, batch     7 | loss: 97.8989430CurrentTrain: epoch  2, batch     8 | loss: 83.9575561CurrentTrain: epoch  2, batch     9 | loss: 100.8126067CurrentTrain: epoch  2, batch    10 | loss: 78.1771887CurrentTrain: epoch  2, batch    11 | loss: 102.9502855CurrentTrain: epoch  2, batch    12 | loss: 129.7568568CurrentTrain: epoch  2, batch    13 | loss: 130.3177716CurrentTrain: epoch  2, batch    14 | loss: 82.3388330CurrentTrain: epoch  2, batch    15 | loss: 97.4488790CurrentTrain: epoch  2, batch    16 | loss: 97.6315794CurrentTrain: epoch  2, batch    17 | loss: 70.7030435CurrentTrain: epoch  2, batch    18 | loss: 82.2044035CurrentTrain: epoch  2, batch    19 | loss: 99.5352551CurrentTrain: epoch  2, batch    20 | loss: 171.3274613CurrentTrain: epoch  2, batch    21 | loss: 95.4643415CurrentTrain: epoch  2, batch    22 | loss: 101.8320801CurrentTrain: epoch  2, batch    23 | loss: 132.9838126CurrentTrain: epoch  2, batch    24 | loss: 98.7069457CurrentTrain: epoch  2, batch    25 | loss: 62.3343612CurrentTrain: epoch  2, batch    26 | loss: 83.8620213CurrentTrain: epoch  2, batch    27 | loss: 68.5430347CurrentTrain: epoch  2, batch    28 | loss: 88.2169048CurrentTrain: epoch  2, batch    29 | loss: 77.6741391CurrentTrain: epoch  2, batch    30 | loss: 126.3466236CurrentTrain: epoch  2, batch    31 | loss: 76.5189284CurrentTrain: epoch  2, batch    32 | loss: 83.2910131CurrentTrain: epoch  2, batch    33 | loss: 70.0492356CurrentTrain: epoch  2, batch    34 | loss: 84.2800545CurrentTrain: epoch  2, batch    35 | loss: 71.0523408CurrentTrain: epoch  2, batch    36 | loss: 69.6154210CurrentTrain: epoch  2, batch    37 | loss: 82.7875153CurrentTrain: epoch  2, batch    38 | loss: 82.5011295CurrentTrain: epoch  2, batch    39 | loss: 130.6323072CurrentTrain: epoch  2, batch    40 | loss: 98.9951245CurrentTrain: epoch  2, batch    41 | loss: 102.2949560CurrentTrain: epoch  2, batch    42 | loss: 82.5025990CurrentTrain: epoch  2, batch    43 | loss: 66.5621651CurrentTrain: epoch  2, batch    44 | loss: 96.6280863CurrentTrain: epoch  2, batch    45 | loss: 124.7408425CurrentTrain: epoch  2, batch    46 | loss: 81.0016508CurrentTrain: epoch  2, batch    47 | loss: 77.3667526CurrentTrain: epoch  2, batch    48 | loss: 81.1539210CurrentTrain: epoch  2, batch    49 | loss: 168.6484675CurrentTrain: epoch  2, batch    50 | loss: 97.0925974CurrentTrain: epoch  2, batch    51 | loss: 98.7381418CurrentTrain: epoch  2, batch    52 | loss: 84.2620437CurrentTrain: epoch  2, batch    53 | loss: 82.5621751CurrentTrain: epoch  2, batch    54 | loss: 67.1943799CurrentTrain: epoch  2, batch    55 | loss: 81.9325077CurrentTrain: epoch  2, batch    56 | loss: 64.6078830CurrentTrain: epoch  2, batch    57 | loss: 130.0270302CurrentTrain: epoch  2, batch    58 | loss: 94.7140225CurrentTrain: epoch  2, batch    59 | loss: 72.4527168CurrentTrain: epoch  2, batch    60 | loss: 84.5445097CurrentTrain: epoch  2, batch    61 | loss: 79.2959780CurrentTrain: epoch  2, batch    62 | loss: 71.2503189CurrentTrain: epoch  2, batch    63 | loss: 80.5427163CurrentTrain: epoch  2, batch    64 | loss: 96.0939921CurrentTrain: epoch  2, batch    65 | loss: 81.4646901CurrentTrain: epoch  2, batch    66 | loss: 66.8781175CurrentTrain: epoch  2, batch    67 | loss: 78.4573866CurrentTrain: epoch  2, batch    68 | loss: 89.8223266CurrentTrain: epoch  2, batch    69 | loss: 105.0501802CurrentTrain: epoch  2, batch    70 | loss: 81.3229507CurrentTrain: epoch  2, batch    71 | loss: 128.5954879CurrentTrain: epoch  2, batch    72 | loss: 124.3774049CurrentTrain: epoch  2, batch    73 | loss: 79.7328966CurrentTrain: epoch  2, batch    74 | loss: 87.6501568CurrentTrain: epoch  2, batch    75 | loss: 86.9347378CurrentTrain: epoch  2, batch    76 | loss: 69.5370839CurrentTrain: epoch  2, batch    77 | loss: 85.3342253CurrentTrain: epoch  2, batch    78 | loss: 81.2184381CurrentTrain: epoch  2, batch    79 | loss: 84.4958358CurrentTrain: epoch  2, batch    80 | loss: 76.9716524CurrentTrain: epoch  2, batch    81 | loss: 66.9224387CurrentTrain: epoch  2, batch    82 | loss: 97.3523368CurrentTrain: epoch  2, batch    83 | loss: 70.7691273CurrentTrain: epoch  2, batch    84 | loss: 83.3113069CurrentTrain: epoch  2, batch    85 | loss: 69.6298042CurrentTrain: epoch  2, batch    86 | loss: 80.3271901CurrentTrain: epoch  2, batch    87 | loss: 79.8504199CurrentTrain: epoch  2, batch    88 | loss: 97.8785376CurrentTrain: epoch  2, batch    89 | loss: 54.5621742CurrentTrain: epoch  2, batch    90 | loss: 99.9671890CurrentTrain: epoch  2, batch    91 | loss: 66.1580032CurrentTrain: epoch  2, batch    92 | loss: 79.0943755CurrentTrain: epoch  2, batch    93 | loss: 77.4589897CurrentTrain: epoch  2, batch    94 | loss: 173.1318426CurrentTrain: epoch  2, batch    95 | loss: 58.0239354CurrentTrain: epoch  3, batch     0 | loss: 81.8924360CurrentTrain: epoch  3, batch     1 | loss: 68.2239753CurrentTrain: epoch  3, batch     2 | loss: 128.4016334CurrentTrain: epoch  3, batch     3 | loss: 56.0153956CurrentTrain: epoch  3, batch     4 | loss: 61.0586612CurrentTrain: epoch  3, batch     5 | loss: 100.3964388CurrentTrain: epoch  3, batch     6 | loss: 66.7046004CurrentTrain: epoch  3, batch     7 | loss: 94.8908969CurrentTrain: epoch  3, batch     8 | loss: 99.6761881CurrentTrain: epoch  3, batch     9 | loss: 96.2745319CurrentTrain: epoch  3, batch    10 | loss: 79.5497073CurrentTrain: epoch  3, batch    11 | loss: 81.1079770CurrentTrain: epoch  3, batch    12 | loss: 60.0185341CurrentTrain: epoch  3, batch    13 | loss: 125.9563561CurrentTrain: epoch  3, batch    14 | loss: 127.3585576CurrentTrain: epoch  3, batch    15 | loss: 67.3719923CurrentTrain: epoch  3, batch    16 | loss: 94.6492712CurrentTrain: epoch  3, batch    17 | loss: 100.7198742CurrentTrain: epoch  3, batch    18 | loss: 80.3739910CurrentTrain: epoch  3, batch    19 | loss: 94.6852180CurrentTrain: epoch  3, batch    20 | loss: 65.5258242CurrentTrain: epoch  3, batch    21 | loss: 92.2815710CurrentTrain: epoch  3, batch    22 | loss: 96.0788769CurrentTrain: epoch  3, batch    23 | loss: 98.2005940CurrentTrain: epoch  3, batch    24 | loss: 97.6460198CurrentTrain: epoch  3, batch    25 | loss: 84.3301091CurrentTrain: epoch  3, batch    26 | loss: 78.6860779CurrentTrain: epoch  3, batch    27 | loss: 83.5322205CurrentTrain: epoch  3, batch    28 | loss: 63.2951980CurrentTrain: epoch  3, batch    29 | loss: 99.1903288CurrentTrain: epoch  3, batch    30 | loss: 123.1260783CurrentTrain: epoch  3, batch    31 | loss: 57.6003388CurrentTrain: epoch  3, batch    32 | loss: 97.0631162CurrentTrain: epoch  3, batch    33 | loss: 75.3347201CurrentTrain: epoch  3, batch    34 | loss: 94.3215322CurrentTrain: epoch  3, batch    35 | loss: 72.9290721CurrentTrain: epoch  3, batch    36 | loss: 66.7098207CurrentTrain: epoch  3, batch    37 | loss: 67.1702081CurrentTrain: epoch  3, batch    38 | loss: 94.6752801CurrentTrain: epoch  3, batch    39 | loss: 55.2950596CurrentTrain: epoch  3, batch    40 | loss: 66.2252354CurrentTrain: epoch  3, batch    41 | loss: 82.1220253CurrentTrain: epoch  3, batch    42 | loss: 66.8388508CurrentTrain: epoch  3, batch    43 | loss: 100.7465184CurrentTrain: epoch  3, batch    44 | loss: 94.5436122CurrentTrain: epoch  3, batch    45 | loss: 99.6662544CurrentTrain: epoch  3, batch    46 | loss: 58.1632864CurrentTrain: epoch  3, batch    47 | loss: 59.7827788CurrentTrain: epoch  3, batch    48 | loss: 71.6539759CurrentTrain: epoch  3, batch    49 | loss: 65.8197345CurrentTrain: epoch  3, batch    50 | loss: 58.0404227CurrentTrain: epoch  3, batch    51 | loss: 68.3567479CurrentTrain: epoch  3, batch    52 | loss: 63.1776636CurrentTrain: epoch  3, batch    53 | loss: 78.8089686CurrentTrain: epoch  3, batch    54 | loss: 80.6492391CurrentTrain: epoch  3, batch    55 | loss: 57.0795468CurrentTrain: epoch  3, batch    56 | loss: 77.4251299CurrentTrain: epoch  3, batch    57 | loss: 92.1899411CurrentTrain: epoch  3, batch    58 | loss: 99.1425092CurrentTrain: epoch  3, batch    59 | loss: 83.9475344CurrentTrain: epoch  3, batch    60 | loss: 76.5149116CurrentTrain: epoch  3, batch    61 | loss: 64.4727835CurrentTrain: epoch  3, batch    62 | loss: 69.0271017CurrentTrain: epoch  3, batch    63 | loss: 132.5917566CurrentTrain: epoch  3, batch    64 | loss: 65.6161331CurrentTrain: epoch  3, batch    65 | loss: 121.0802176CurrentTrain: epoch  3, batch    66 | loss: 119.5671452CurrentTrain: epoch  3, batch    67 | loss: 80.7467690CurrentTrain: epoch  3, batch    68 | loss: 68.5404588CurrentTrain: epoch  3, batch    69 | loss: 81.4195780CurrentTrain: epoch  3, batch    70 | loss: 95.3965193CurrentTrain: epoch  3, batch    71 | loss: 95.5896992CurrentTrain: epoch  3, batch    72 | loss: 75.9118992CurrentTrain: epoch  3, batch    73 | loss: 66.2489568CurrentTrain: epoch  3, batch    74 | loss: 82.7411399CurrentTrain: epoch  3, batch    75 | loss: 96.0899495CurrentTrain: epoch  3, batch    76 | loss: 66.4799357CurrentTrain: epoch  3, batch    77 | loss: 64.5295643CurrentTrain: epoch  3, batch    78 | loss: 97.0978909CurrentTrain: epoch  3, batch    79 | loss: 80.6594789CurrentTrain: epoch  3, batch    80 | loss: 98.6840826CurrentTrain: epoch  3, batch    81 | loss: 67.5427825CurrentTrain: epoch  3, batch    82 | loss: 68.5771828CurrentTrain: epoch  3, batch    83 | loss: 72.3560830CurrentTrain: epoch  3, batch    84 | loss: 65.2817306CurrentTrain: epoch  3, batch    85 | loss: 98.4774606CurrentTrain: epoch  3, batch    86 | loss: 60.5559705CurrentTrain: epoch  3, batch    87 | loss: 83.2212829CurrentTrain: epoch  3, batch    88 | loss: 118.2037702CurrentTrain: epoch  3, batch    89 | loss: 80.9964040CurrentTrain: epoch  3, batch    90 | loss: 66.9100036CurrentTrain: epoch  3, batch    91 | loss: 66.6469730CurrentTrain: epoch  3, batch    92 | loss: 67.4013571CurrentTrain: epoch  3, batch    93 | loss: 65.8843667CurrentTrain: epoch  3, batch    94 | loss: 100.0418878CurrentTrain: epoch  3, batch    95 | loss: 62.2865689CurrentTrain: epoch  4, batch     0 | loss: 64.4381168CurrentTrain: epoch  4, batch     1 | loss: 94.8985518CurrentTrain: epoch  4, batch     2 | loss: 64.9452411CurrentTrain: epoch  4, batch     3 | loss: 81.0160172CurrentTrain: epoch  4, batch     4 | loss: 58.4511670CurrentTrain: epoch  4, batch     5 | loss: 64.4024834CurrentTrain: epoch  4, batch     6 | loss: 80.7044787CurrentTrain: epoch  4, batch     7 | loss: 76.3715196CurrentTrain: epoch  4, batch     8 | loss: 64.5487225CurrentTrain: epoch  4, batch     9 | loss: 72.2228878CurrentTrain: epoch  4, batch    10 | loss: 69.0669499CurrentTrain: epoch  4, batch    11 | loss: 72.8957566CurrentTrain: epoch  4, batch    12 | loss: 70.8878132CurrentTrain: epoch  4, batch    13 | loss: 119.1783601CurrentTrain: epoch  4, batch    14 | loss: 98.0789627CurrentTrain: epoch  4, batch    15 | loss: 80.3655934CurrentTrain: epoch  4, batch    16 | loss: 76.2079296CurrentTrain: epoch  4, batch    17 | loss: 78.8447060CurrentTrain: epoch  4, batch    18 | loss: 93.0429089CurrentTrain: epoch  4, batch    19 | loss: 118.3976168CurrentTrain: epoch  4, batch    20 | loss: 74.3810964CurrentTrain: epoch  4, batch    21 | loss: 115.9396372CurrentTrain: epoch  4, batch    22 | loss: 66.6385933CurrentTrain: epoch  4, batch    23 | loss: 80.0513868CurrentTrain: epoch  4, batch    24 | loss: 77.2065796CurrentTrain: epoch  4, batch    25 | loss: 63.3561030CurrentTrain: epoch  4, batch    26 | loss: 72.1188418CurrentTrain: epoch  4, batch    27 | loss: 79.5589487CurrentTrain: epoch  4, batch    28 | loss: 97.3846710CurrentTrain: epoch  4, batch    29 | loss: 64.5890810CurrentTrain: epoch  4, batch    30 | loss: 90.9771182CurrentTrain: epoch  4, batch    31 | loss: 67.8562207CurrentTrain: epoch  4, batch    32 | loss: 95.7472760CurrentTrain: epoch  4, batch    33 | loss: 58.2456079CurrentTrain: epoch  4, batch    34 | loss: 90.0762442CurrentTrain: epoch  4, batch    35 | loss: 85.9199765CurrentTrain: epoch  4, batch    36 | loss: 122.5515595CurrentTrain: epoch  4, batch    37 | loss: 99.6564619CurrentTrain: epoch  4, batch    38 | loss: 97.5435502CurrentTrain: epoch  4, batch    39 | loss: 93.3147727CurrentTrain: epoch  4, batch    40 | loss: 80.2749629CurrentTrain: epoch  4, batch    41 | loss: 77.4336991CurrentTrain: epoch  4, batch    42 | loss: 89.0434139CurrentTrain: epoch  4, batch    43 | loss: 97.3352767CurrentTrain: epoch  4, batch    44 | loss: 124.3885715CurrentTrain: epoch  4, batch    45 | loss: 77.8844648CurrentTrain: epoch  4, batch    46 | loss: 78.3630174CurrentTrain: epoch  4, batch    47 | loss: 98.8390746CurrentTrain: epoch  4, batch    48 | loss: 120.7418473CurrentTrain: epoch  4, batch    49 | loss: 77.2844652CurrentTrain: epoch  4, batch    50 | loss: 86.8119504CurrentTrain: epoch  4, batch    51 | loss: 120.5172291CurrentTrain: epoch  4, batch    52 | loss: 78.6119757CurrentTrain: epoch  4, batch    53 | loss: 65.9191616CurrentTrain: epoch  4, batch    54 | loss: 58.8004460CurrentTrain: epoch  4, batch    55 | loss: 97.1953818CurrentTrain: epoch  4, batch    56 | loss: 74.6177222CurrentTrain: epoch  4, batch    57 | loss: 71.2978868CurrentTrain: epoch  4, batch    58 | loss: 62.9010806CurrentTrain: epoch  4, batch    59 | loss: 94.6674948CurrentTrain: epoch  4, batch    60 | loss: 67.4305154CurrentTrain: epoch  4, batch    61 | loss: 78.8721007CurrentTrain: epoch  4, batch    62 | loss: 76.9953793CurrentTrain: epoch  4, batch    63 | loss: 76.3647076CurrentTrain: epoch  4, batch    64 | loss: 66.9118613CurrentTrain: epoch  4, batch    65 | loss: 64.6586058CurrentTrain: epoch  4, batch    66 | loss: 96.0370721CurrentTrain: epoch  4, batch    67 | loss: 77.3205102CurrentTrain: epoch  4, batch    68 | loss: 58.0396684CurrentTrain: epoch  4, batch    69 | loss: 120.8307609CurrentTrain: epoch  4, batch    70 | loss: 75.9613671CurrentTrain: epoch  4, batch    71 | loss: 118.4449967CurrentTrain: epoch  4, batch    72 | loss: 55.5098100CurrentTrain: epoch  4, batch    73 | loss: 120.6188429CurrentTrain: epoch  4, batch    74 | loss: 58.2706765CurrentTrain: epoch  4, batch    75 | loss: 68.4542932CurrentTrain: epoch  4, batch    76 | loss: 64.8426932CurrentTrain: epoch  4, batch    77 | loss: 98.9468917CurrentTrain: epoch  4, batch    78 | loss: 98.7655787CurrentTrain: epoch  4, batch    79 | loss: 78.3658511CurrentTrain: epoch  4, batch    80 | loss: 69.9001370CurrentTrain: epoch  4, batch    81 | loss: 97.5978515CurrentTrain: epoch  4, batch    82 | loss: 74.3978865CurrentTrain: epoch  4, batch    83 | loss: 93.8147649CurrentTrain: epoch  4, batch    84 | loss: 67.3765593CurrentTrain: epoch  4, batch    85 | loss: 94.4025192CurrentTrain: epoch  4, batch    86 | loss: 99.2837099CurrentTrain: epoch  4, batch    87 | loss: 95.1688402CurrentTrain: epoch  4, batch    88 | loss: 95.3161988CurrentTrain: epoch  4, batch    89 | loss: 94.7205021CurrentTrain: epoch  4, batch    90 | loss: 92.2481068CurrentTrain: epoch  4, batch    91 | loss: 79.2627536CurrentTrain: epoch  4, batch    92 | loss: 95.8921405CurrentTrain: epoch  4, batch    93 | loss: 64.1431847CurrentTrain: epoch  4, batch    94 | loss: 65.5499882CurrentTrain: epoch  4, batch    95 | loss: 62.4587634CurrentTrain: epoch  5, batch     0 | loss: 93.3925984CurrentTrain: epoch  5, batch     1 | loss: 63.4749543CurrentTrain: epoch  5, batch     2 | loss: 89.4049682CurrentTrain: epoch  5, batch     3 | loss: 112.4814064CurrentTrain: epoch  5, batch     4 | loss: 64.7230986CurrentTrain: epoch  5, batch     5 | loss: 88.3255976CurrentTrain: epoch  5, batch     6 | loss: 96.2450825CurrentTrain: epoch  5, batch     7 | loss: 77.6972745CurrentTrain: epoch  5, batch     8 | loss: 60.6612756CurrentTrain: epoch  5, batch     9 | loss: 54.3903090CurrentTrain: epoch  5, batch    10 | loss: 93.4136114CurrentTrain: epoch  5, batch    11 | loss: 92.7458505CurrentTrain: epoch  5, batch    12 | loss: 61.6070482CurrentTrain: epoch  5, batch    13 | loss: 88.6875089CurrentTrain: epoch  5, batch    14 | loss: 98.0340568CurrentTrain: epoch  5, batch    15 | loss: 64.4650377CurrentTrain: epoch  5, batch    16 | loss: 120.2571101CurrentTrain: epoch  5, batch    17 | loss: 73.2340501CurrentTrain: epoch  5, batch    18 | loss: 89.8443164CurrentTrain: epoch  5, batch    19 | loss: 97.0251756CurrentTrain: epoch  5, batch    20 | loss: 79.1966343CurrentTrain: epoch  5, batch    21 | loss: 72.9319103CurrentTrain: epoch  5, batch    22 | loss: 76.8739410CurrentTrain: epoch  5, batch    23 | loss: 74.1192207CurrentTrain: epoch  5, batch    24 | loss: 63.8537584CurrentTrain: epoch  5, batch    25 | loss: 92.1451950CurrentTrain: epoch  5, batch    26 | loss: 73.9840191CurrentTrain: epoch  5, batch    27 | loss: 62.2584282CurrentTrain: epoch  5, batch    28 | loss: 118.5681206CurrentTrain: epoch  5, batch    29 | loss: 65.9479789CurrentTrain: epoch  5, batch    30 | loss: 86.4472974CurrentTrain: epoch  5, batch    31 | loss: 75.3021593CurrentTrain: epoch  5, batch    32 | loss: 66.8625055CurrentTrain: epoch  5, batch    33 | loss: 85.4490259CurrentTrain: epoch  5, batch    34 | loss: 92.1648498CurrentTrain: epoch  5, batch    35 | loss: 53.5021626CurrentTrain: epoch  5, batch    36 | loss: 62.4135908CurrentTrain: epoch  5, batch    37 | loss: 118.2194367CurrentTrain: epoch  5, batch    38 | loss: 73.3530160CurrentTrain: epoch  5, batch    39 | loss: 59.4992034CurrentTrain: epoch  5, batch    40 | loss: 125.0093528CurrentTrain: epoch  5, batch    41 | loss: 75.9186600CurrentTrain: epoch  5, batch    42 | loss: 79.6171764CurrentTrain: epoch  5, batch    43 | loss: 61.3419758CurrentTrain: epoch  5, batch    44 | loss: 93.2362596CurrentTrain: epoch  5, batch    45 | loss: 74.0549039CurrentTrain: epoch  5, batch    46 | loss: 67.3602720CurrentTrain: epoch  5, batch    47 | loss: 64.4817128CurrentTrain: epoch  5, batch    48 | loss: 98.1924548CurrentTrain: epoch  5, batch    49 | loss: 51.4855012CurrentTrain: epoch  5, batch    50 | loss: 89.2569089CurrentTrain: epoch  5, batch    51 | loss: 67.7097304CurrentTrain: epoch  5, batch    52 | loss: 80.2125065CurrentTrain: epoch  5, batch    53 | loss: 80.0352913CurrentTrain: epoch  5, batch    54 | loss: 78.4912898CurrentTrain: epoch  5, batch    55 | loss: 115.3700292CurrentTrain: epoch  5, batch    56 | loss: 167.9395721CurrentTrain: epoch  5, batch    57 | loss: 64.5061380CurrentTrain: epoch  5, batch    58 | loss: 77.2054378CurrentTrain: epoch  5, batch    59 | loss: 78.7586605CurrentTrain: epoch  5, batch    60 | loss: 60.1773824CurrentTrain: epoch  5, batch    61 | loss: 92.2968789CurrentTrain: epoch  5, batch    62 | loss: 91.1181130CurrentTrain: epoch  5, batch    63 | loss: 63.9963193CurrentTrain: epoch  5, batch    64 | loss: 73.4268084CurrentTrain: epoch  5, batch    65 | loss: 161.5963677CurrentTrain: epoch  5, batch    66 | loss: 76.6013841CurrentTrain: epoch  5, batch    67 | loss: 95.9527356CurrentTrain: epoch  5, batch    68 | loss: 64.1437597CurrentTrain: epoch  5, batch    69 | loss: 75.5069696CurrentTrain: epoch  5, batch    70 | loss: 69.3724064CurrentTrain: epoch  5, batch    71 | loss: 160.7195561CurrentTrain: epoch  5, batch    72 | loss: 55.3587922CurrentTrain: epoch  5, batch    73 | loss: 90.5965840CurrentTrain: epoch  5, batch    74 | loss: 67.0484193CurrentTrain: epoch  5, batch    75 | loss: 66.7663165CurrentTrain: epoch  5, batch    76 | loss: 65.2309535CurrentTrain: epoch  5, batch    77 | loss: 62.9781850CurrentTrain: epoch  5, batch    78 | loss: 67.4246374CurrentTrain: epoch  5, batch    79 | loss: 55.0707964CurrentTrain: epoch  5, batch    80 | loss: 87.8387267CurrentTrain: epoch  5, batch    81 | loss: 66.4735261CurrentTrain: epoch  5, batch    82 | loss: 72.9646845CurrentTrain: epoch  5, batch    83 | loss: 66.2713010CurrentTrain: epoch  5, batch    84 | loss: 92.5264597CurrentTrain: epoch  5, batch    85 | loss: 96.3688373CurrentTrain: epoch  5, batch    86 | loss: 78.1473315CurrentTrain: epoch  5, batch    87 | loss: 80.7837007CurrentTrain: epoch  5, batch    88 | loss: 77.2539671CurrentTrain: epoch  5, batch    89 | loss: 73.7678597CurrentTrain: epoch  5, batch    90 | loss: 79.1000757CurrentTrain: epoch  5, batch    91 | loss: 96.8258941CurrentTrain: epoch  5, batch    92 | loss: 80.0737316CurrentTrain: epoch  5, batch    93 | loss: 66.4368207CurrentTrain: epoch  5, batch    94 | loss: 91.1166700CurrentTrain: epoch  5, batch    95 | loss: 64.0733548CurrentTrain: epoch  6, batch     0 | loss: 92.9353763CurrentTrain: epoch  6, batch     1 | loss: 69.8881003CurrentTrain: epoch  6, batch     2 | loss: 76.6717036CurrentTrain: epoch  6, batch     3 | loss: 71.4418317CurrentTrain: epoch  6, batch     4 | loss: 73.4931350CurrentTrain: epoch  6, batch     5 | loss: 90.6276258CurrentTrain: epoch  6, batch     6 | loss: 79.1520206CurrentTrain: epoch  6, batch     7 | loss: 74.9939420CurrentTrain: epoch  6, batch     8 | loss: 91.1736657CurrentTrain: epoch  6, batch     9 | loss: 111.0745568CurrentTrain: epoch  6, batch    10 | loss: 76.3510141CurrentTrain: epoch  6, batch    11 | loss: 94.2640493CurrentTrain: epoch  6, batch    12 | loss: 76.5274816CurrentTrain: epoch  6, batch    13 | loss: 76.2405908CurrentTrain: epoch  6, batch    14 | loss: 60.1129414CurrentTrain: epoch  6, batch    15 | loss: 116.8044234CurrentTrain: epoch  6, batch    16 | loss: 87.9899137CurrentTrain: epoch  6, batch    17 | loss: 108.5368527CurrentTrain: epoch  6, batch    18 | loss: 120.9265000CurrentTrain: epoch  6, batch    19 | loss: 64.5568388CurrentTrain: epoch  6, batch    20 | loss: 162.3665489CurrentTrain: epoch  6, batch    21 | loss: 78.1773749CurrentTrain: epoch  6, batch    22 | loss: 73.2458722CurrentTrain: epoch  6, batch    23 | loss: 93.4080163CurrentTrain: epoch  6, batch    24 | loss: 61.2231488CurrentTrain: epoch  6, batch    25 | loss: 93.6637354CurrentTrain: epoch  6, batch    26 | loss: 61.8898658CurrentTrain: epoch  6, batch    27 | loss: 59.6697629CurrentTrain: epoch  6, batch    28 | loss: 71.4949873CurrentTrain: epoch  6, batch    29 | loss: 117.1273462CurrentTrain: epoch  6, batch    30 | loss: 66.3335351CurrentTrain: epoch  6, batch    31 | loss: 63.4554917CurrentTrain: epoch  6, batch    32 | loss: 66.5715132CurrentTrain: epoch  6, batch    33 | loss: 118.9764738CurrentTrain: epoch  6, batch    34 | loss: 79.7239699CurrentTrain: epoch  6, batch    35 | loss: 61.8051500CurrentTrain: epoch  6, batch    36 | loss: 94.8606184CurrentTrain: epoch  6, batch    37 | loss: 113.0618989CurrentTrain: epoch  6, batch    38 | loss: 94.8097722CurrentTrain: epoch  6, batch    39 | loss: 64.1296832CurrentTrain: epoch  6, batch    40 | loss: 90.8662150CurrentTrain: epoch  6, batch    41 | loss: 92.6131913CurrentTrain: epoch  6, batch    42 | loss: 55.4169381CurrentTrain: epoch  6, batch    43 | loss: 72.4725057CurrentTrain: epoch  6, batch    44 | loss: 73.0803713CurrentTrain: epoch  6, batch    45 | loss: 51.8748849CurrentTrain: epoch  6, batch    46 | loss: 66.4531401CurrentTrain: epoch  6, batch    47 | loss: 72.0663890CurrentTrain: epoch  6, batch    48 | loss: 62.9097670CurrentTrain: epoch  6, batch    49 | loss: 115.9848869CurrentTrain: epoch  6, batch    50 | loss: 111.0582133CurrentTrain: epoch  6, batch    51 | loss: 56.7633377CurrentTrain: epoch  6, batch    52 | loss: 61.2674020CurrentTrain: epoch  6, batch    53 | loss: 63.0483935CurrentTrain: epoch  6, batch    54 | loss: 77.5489257CurrentTrain: epoch  6, batch    55 | loss: 112.4579868CurrentTrain: epoch  6, batch    56 | loss: 115.2640470CurrentTrain: epoch  6, batch    57 | loss: 76.1168756CurrentTrain: epoch  6, batch    58 | loss: 88.7907162CurrentTrain: epoch  6, batch    59 | loss: 91.7437009CurrentTrain: epoch  6, batch    60 | loss: 94.7141246CurrentTrain: epoch  6, batch    61 | loss: 63.0468704CurrentTrain: epoch  6, batch    62 | loss: 69.7766939CurrentTrain: epoch  6, batch    63 | loss: 75.3862031CurrentTrain: epoch  6, batch    64 | loss: 71.0359731CurrentTrain: epoch  6, batch    65 | loss: 96.5572312CurrentTrain: epoch  6, batch    66 | loss: 72.4186165CurrentTrain: epoch  6, batch    67 | loss: 88.1136921CurrentTrain: epoch  6, batch    68 | loss: 77.4417403CurrentTrain: epoch  6, batch    69 | loss: 88.6834579CurrentTrain: epoch  6, batch    70 | loss: 91.5524777CurrentTrain: epoch  6, batch    71 | loss: 88.5821968CurrentTrain: epoch  6, batch    72 | loss: 52.0263225CurrentTrain: epoch  6, batch    73 | loss: 77.7603777CurrentTrain: epoch  6, batch    74 | loss: 94.0546759CurrentTrain: epoch  6, batch    75 | loss: 67.2734362CurrentTrain: epoch  6, batch    76 | loss: 76.9014448CurrentTrain: epoch  6, batch    77 | loss: 97.3826349CurrentTrain: epoch  6, batch    78 | loss: 90.9197904CurrentTrain: epoch  6, batch    79 | loss: 89.6895400CurrentTrain: epoch  6, batch    80 | loss: 89.7611911CurrentTrain: epoch  6, batch    81 | loss: 67.8176031CurrentTrain: epoch  6, batch    82 | loss: 73.2490987CurrentTrain: epoch  6, batch    83 | loss: 61.4836351CurrentTrain: epoch  6, batch    84 | loss: 90.5455054CurrentTrain: epoch  6, batch    85 | loss: 70.4583831CurrentTrain: epoch  6, batch    86 | loss: 90.7664291CurrentTrain: epoch  6, batch    87 | loss: 90.8377811CurrentTrain: epoch  6, batch    88 | loss: 110.2469593CurrentTrain: epoch  6, batch    89 | loss: 68.4381832CurrentTrain: epoch  6, batch    90 | loss: 59.6516165CurrentTrain: epoch  6, batch    91 | loss: 113.8575807CurrentTrain: epoch  6, batch    92 | loss: 62.3096721CurrentTrain: epoch  6, batch    93 | loss: 60.8583226CurrentTrain: epoch  6, batch    94 | loss: 60.1609963CurrentTrain: epoch  6, batch    95 | loss: 101.9635540CurrentTrain: epoch  7, batch     0 | loss: 59.1493033CurrentTrain: epoch  7, batch     1 | loss: 62.9454202CurrentTrain: epoch  7, batch     2 | loss: 115.5599823CurrentTrain: epoch  7, batch     3 | loss: 88.9067434CurrentTrain: epoch  7, batch     4 | loss: 113.5993034CurrentTrain: epoch  7, batch     5 | loss: 73.8413937CurrentTrain: epoch  7, batch     6 | loss: 71.4159207CurrentTrain: epoch  7, batch     7 | loss: 86.9107683CurrentTrain: epoch  7, batch     8 | loss: 49.2449892CurrentTrain: epoch  7, batch     9 | loss: 59.5634533CurrentTrain: epoch  7, batch    10 | loss: 109.0628961CurrentTrain: epoch  7, batch    11 | loss: 117.6332439CurrentTrain: epoch  7, batch    12 | loss: 70.9426373CurrentTrain: epoch  7, batch    13 | loss: 75.4219627CurrentTrain: epoch  7, batch    14 | loss: 50.8585340CurrentTrain: epoch  7, batch    15 | loss: 91.7038295CurrentTrain: epoch  7, batch    16 | loss: 59.2652357CurrentTrain: epoch  7, batch    17 | loss: 97.0502937CurrentTrain: epoch  7, batch    18 | loss: 61.0018726CurrentTrain: epoch  7, batch    19 | loss: 87.7053393CurrentTrain: epoch  7, batch    20 | loss: 91.0955217CurrentTrain: epoch  7, batch    21 | loss: 77.4161363CurrentTrain: epoch  7, batch    22 | loss: 89.3784139CurrentTrain: epoch  7, batch    23 | loss: 78.0308109CurrentTrain: epoch  7, batch    24 | loss: 112.1023139CurrentTrain: epoch  7, batch    25 | loss: 68.9790462CurrentTrain: epoch  7, batch    26 | loss: 76.1364445CurrentTrain: epoch  7, batch    27 | loss: 53.0105363CurrentTrain: epoch  7, batch    28 | loss: 51.9629425CurrentTrain: epoch  7, batch    29 | loss: 58.5723145CurrentTrain: epoch  7, batch    30 | loss: 89.3297424CurrentTrain: epoch  7, batch    31 | loss: 62.5837895CurrentTrain: epoch  7, batch    32 | loss: 88.4082860CurrentTrain: epoch  7, batch    33 | loss: 88.8016505CurrentTrain: epoch  7, batch    34 | loss: 92.8948228CurrentTrain: epoch  7, batch    35 | loss: 74.2745161CurrentTrain: epoch  7, batch    36 | loss: 73.1491829CurrentTrain: epoch  7, batch    37 | loss: 58.5295993CurrentTrain: epoch  7, batch    38 | loss: 71.4756177CurrentTrain: epoch  7, batch    39 | loss: 70.0335622CurrentTrain: epoch  7, batch    40 | loss: 121.2882384CurrentTrain: epoch  7, batch    41 | loss: 74.0542192CurrentTrain: epoch  7, batch    42 | loss: 73.1261376CurrentTrain: epoch  7, batch    43 | loss: 116.4801170CurrentTrain: epoch  7, batch    44 | loss: 53.0429988CurrentTrain: epoch  7, batch    45 | loss: 96.0816931CurrentTrain: epoch  7, batch    46 | loss: 58.8443515CurrentTrain: epoch  7, batch    47 | loss: 74.1493202CurrentTrain: epoch  7, batch    48 | loss: 74.9808363CurrentTrain: epoch  7, batch    49 | loss: 115.0566106CurrentTrain: epoch  7, batch    50 | loss: 92.6037439CurrentTrain: epoch  7, batch    51 | loss: 96.6967551CurrentTrain: epoch  7, batch    52 | loss: 70.5807522CurrentTrain: epoch  7, batch    53 | loss: 113.2063849CurrentTrain: epoch  7, batch    54 | loss: 74.0056312CurrentTrain: epoch  7, batch    55 | loss: 75.1282461CurrentTrain: epoch  7, batch    56 | loss: 59.7894946CurrentTrain: epoch  7, batch    57 | loss: 115.5578465CurrentTrain: epoch  7, batch    58 | loss: 63.4696792CurrentTrain: epoch  7, batch    59 | loss: 74.9932131CurrentTrain: epoch  7, batch    60 | loss: 86.2640554CurrentTrain: epoch  7, batch    61 | loss: 73.7433510CurrentTrain: epoch  7, batch    62 | loss: 89.6628422CurrentTrain: epoch  7, batch    63 | loss: 75.5879821CurrentTrain: epoch  7, batch    64 | loss: 54.2832647CurrentTrain: epoch  7, batch    65 | loss: 70.2078181CurrentTrain: epoch  7, batch    66 | loss: 62.4413230CurrentTrain: epoch  7, batch    67 | loss: 75.6846094CurrentTrain: epoch  7, batch    68 | loss: 72.0675854CurrentTrain: epoch  7, batch    69 | loss: 92.8519500CurrentTrain: epoch  7, batch    70 | loss: 72.4176431CurrentTrain: epoch  7, batch    71 | loss: 51.2732457CurrentTrain: epoch  7, batch    72 | loss: 72.6185819CurrentTrain: epoch  7, batch    73 | loss: 71.9718931CurrentTrain: epoch  7, batch    74 | loss: 114.6944198CurrentTrain: epoch  7, batch    75 | loss: 90.1394655CurrentTrain: epoch  7, batch    76 | loss: 75.6720232CurrentTrain: epoch  7, batch    77 | loss: 53.9118071CurrentTrain: epoch  7, batch    78 | loss: 245.5932314CurrentTrain: epoch  7, batch    79 | loss: 76.1113236CurrentTrain: epoch  7, batch    80 | loss: 73.5331546CurrentTrain: epoch  7, batch    81 | loss: 57.2003810CurrentTrain: epoch  7, batch    82 | loss: 95.7329905CurrentTrain: epoch  7, batch    83 | loss: 74.4600880CurrentTrain: epoch  7, batch    84 | loss: 74.5807701CurrentTrain: epoch  7, batch    85 | loss: 73.0533378CurrentTrain: epoch  7, batch    86 | loss: 117.8193703CurrentTrain: epoch  7, batch    87 | loss: 90.4491737CurrentTrain: epoch  7, batch    88 | loss: 114.1292039CurrentTrain: epoch  7, batch    89 | loss: 73.3820678CurrentTrain: epoch  7, batch    90 | loss: 72.6513647CurrentTrain: epoch  7, batch    91 | loss: 115.7232516CurrentTrain: epoch  7, batch    92 | loss: 116.4121942CurrentTrain: epoch  7, batch    93 | loss: 58.9078916CurrentTrain: epoch  7, batch    94 | loss: 47.1197879CurrentTrain: epoch  7, batch    95 | loss: 94.7295473CurrentTrain: epoch  8, batch     0 | loss: 91.5055350CurrentTrain: epoch  8, batch     1 | loss: 89.2998356CurrentTrain: epoch  8, batch     2 | loss: 73.7398609CurrentTrain: epoch  8, batch     3 | loss: 59.9804006CurrentTrain: epoch  8, batch     4 | loss: 62.8149911CurrentTrain: epoch  8, batch     5 | loss: 86.7138606CurrentTrain: epoch  8, batch     6 | loss: 73.1645894CurrentTrain: epoch  8, batch     7 | loss: 118.4407863CurrentTrain: epoch  8, batch     8 | loss: 61.3470140CurrentTrain: epoch  8, batch     9 | loss: 59.1352280CurrentTrain: epoch  8, batch    10 | loss: 54.9529081CurrentTrain: epoch  8, batch    11 | loss: 59.3297318CurrentTrain: epoch  8, batch    12 | loss: 59.3468049CurrentTrain: epoch  8, batch    13 | loss: 159.3216960CurrentTrain: epoch  8, batch    14 | loss: 73.0376676CurrentTrain: epoch  8, batch    15 | loss: 73.6415380CurrentTrain: epoch  8, batch    16 | loss: 114.7116366CurrentTrain: epoch  8, batch    17 | loss: 91.7191034CurrentTrain: epoch  8, batch    18 | loss: 89.5866179CurrentTrain: epoch  8, batch    19 | loss: 60.8431715CurrentTrain: epoch  8, batch    20 | loss: 49.7288918CurrentTrain: epoch  8, batch    21 | loss: 57.7694552CurrentTrain: epoch  8, batch    22 | loss: 115.1254493CurrentTrain: epoch  8, batch    23 | loss: 69.0901972CurrentTrain: epoch  8, batch    24 | loss: 89.6514087CurrentTrain: epoch  8, batch    25 | loss: 60.0643498CurrentTrain: epoch  8, batch    26 | loss: 87.8238403CurrentTrain: epoch  8, batch    27 | loss: 112.2586043CurrentTrain: epoch  8, batch    28 | loss: 74.5910705CurrentTrain: epoch  8, batch    29 | loss: 83.5023323CurrentTrain: epoch  8, batch    30 | loss: 62.6996992CurrentTrain: epoch  8, batch    31 | loss: 113.4635497CurrentTrain: epoch  8, batch    32 | loss: 115.8573348CurrentTrain: epoch  8, batch    33 | loss: 86.8025583CurrentTrain: epoch  8, batch    34 | loss: 114.7710654CurrentTrain: epoch  8, batch    35 | loss: 110.3681633CurrentTrain: epoch  8, batch    36 | loss: 71.0869160CurrentTrain: epoch  8, batch    37 | loss: 87.3998043CurrentTrain: epoch  8, batch    38 | loss: 115.7385589CurrentTrain: epoch  8, batch    39 | loss: 72.3764831CurrentTrain: epoch  8, batch    40 | loss: 73.9039908CurrentTrain: epoch  8, batch    41 | loss: 58.2276840CurrentTrain: epoch  8, batch    42 | loss: 155.1809599CurrentTrain: epoch  8, batch    43 | loss: 87.7940911CurrentTrain: epoch  8, batch    44 | loss: 87.6131360CurrentTrain: epoch  8, batch    45 | loss: 73.5563453CurrentTrain: epoch  8, batch    46 | loss: 73.3703551CurrentTrain: epoch  8, batch    47 | loss: 115.2802966CurrentTrain: epoch  8, batch    48 | loss: 70.2643995CurrentTrain: epoch  8, batch    49 | loss: 62.6337500CurrentTrain: epoch  8, batch    50 | loss: 61.4613087CurrentTrain: epoch  8, batch    51 | loss: 61.4914654CurrentTrain: epoch  8, batch    52 | loss: 58.6048428CurrentTrain: epoch  8, batch    53 | loss: 95.0254896CurrentTrain: epoch  8, batch    54 | loss: 89.7779035CurrentTrain: epoch  8, batch    55 | loss: 72.1387112CurrentTrain: epoch  8, batch    56 | loss: 74.9873443CurrentTrain: epoch  8, batch    57 | loss: 84.2907956CurrentTrain: epoch  8, batch    58 | loss: 88.3135390CurrentTrain: epoch  8, batch    59 | loss: 112.9877653CurrentTrain: epoch  8, batch    60 | loss: 71.5986751CurrentTrain: epoch  8, batch    61 | loss: 94.6504644CurrentTrain: epoch  8, batch    62 | loss: 59.3303113CurrentTrain: epoch  8, batch    63 | loss: 62.1525168CurrentTrain: epoch  8, batch    64 | loss: 70.0423855CurrentTrain: epoch  8, batch    65 | loss: 88.5347034CurrentTrain: epoch  8, batch    66 | loss: 61.3975138CurrentTrain: epoch  8, batch    67 | loss: 72.8006229CurrentTrain: epoch  8, batch    68 | loss: 72.4180978CurrentTrain: epoch  8, batch    69 | loss: 63.3118312CurrentTrain: epoch  8, batch    70 | loss: 66.1444481CurrentTrain: epoch  8, batch    71 | loss: 59.8597916CurrentTrain: epoch  8, batch    72 | loss: 122.7028073CurrentTrain: epoch  8, batch    73 | loss: 61.9399277CurrentTrain: epoch  8, batch    74 | loss: 71.1907255CurrentTrain: epoch  8, batch    75 | loss: 92.6826982CurrentTrain: epoch  8, batch    76 | loss: 73.1165168CurrentTrain: epoch  8, batch    77 | loss: 63.6974539CurrentTrain: epoch  8, batch    78 | loss: 59.4261074CurrentTrain: epoch  8, batch    79 | loss: 50.2963125CurrentTrain: epoch  8, batch    80 | loss: 72.8917617CurrentTrain: epoch  8, batch    81 | loss: 59.1241151CurrentTrain: epoch  8, batch    82 | loss: 60.7885956CurrentTrain: epoch  8, batch    83 | loss: 60.2375113CurrentTrain: epoch  8, batch    84 | loss: 90.5812322CurrentTrain: epoch  8, batch    85 | loss: 70.5118447CurrentTrain: epoch  8, batch    86 | loss: 52.8195995CurrentTrain: epoch  8, batch    87 | loss: 71.0189912CurrentTrain: epoch  8, batch    88 | loss: 73.6922149CurrentTrain: epoch  8, batch    89 | loss: 61.4192725CurrentTrain: epoch  8, batch    90 | loss: 119.0125613CurrentTrain: epoch  8, batch    91 | loss: 59.7299234CurrentTrain: epoch  8, batch    92 | loss: 62.4887036CurrentTrain: epoch  8, batch    93 | loss: 91.6823697CurrentTrain: epoch  8, batch    94 | loss: 91.5897666CurrentTrain: epoch  8, batch    95 | loss: 76.1196917CurrentTrain: epoch  9, batch     0 | loss: 110.6809995CurrentTrain: epoch  9, batch     1 | loss: 73.8873381CurrentTrain: epoch  9, batch     2 | loss: 61.3621115CurrentTrain: epoch  9, batch     3 | loss: 112.4044042CurrentTrain: epoch  9, batch     4 | loss: 151.0731480CurrentTrain: epoch  9, batch     5 | loss: 89.7421805CurrentTrain: epoch  9, batch     6 | loss: 95.2063750CurrentTrain: epoch  9, batch     7 | loss: 58.4720318CurrentTrain: epoch  9, batch     8 | loss: 73.8235702CurrentTrain: epoch  9, batch     9 | loss: 73.9226017CurrentTrain: epoch  9, batch    10 | loss: 61.5087643CurrentTrain: epoch  9, batch    11 | loss: 72.4228382CurrentTrain: epoch  9, batch    12 | loss: 61.1732956CurrentTrain: epoch  9, batch    13 | loss: 72.7152706CurrentTrain: epoch  9, batch    14 | loss: 85.5620697CurrentTrain: epoch  9, batch    15 | loss: 51.1946233CurrentTrain: epoch  9, batch    16 | loss: 112.2295317CurrentTrain: epoch  9, batch    17 | loss: 112.8397048CurrentTrain: epoch  9, batch    18 | loss: 66.8820909CurrentTrain: epoch  9, batch    19 | loss: 90.8709241CurrentTrain: epoch  9, batch    20 | loss: 70.2597733CurrentTrain: epoch  9, batch    21 | loss: 87.8521578CurrentTrain: epoch  9, batch    22 | loss: 69.1981938CurrentTrain: epoch  9, batch    23 | loss: 83.5222689CurrentTrain: epoch  9, batch    24 | loss: 87.6696380CurrentTrain: epoch  9, batch    25 | loss: 90.4418520CurrentTrain: epoch  9, batch    26 | loss: 74.4430884CurrentTrain: epoch  9, batch    27 | loss: 157.5575407CurrentTrain: epoch  9, batch    28 | loss: 60.5590448CurrentTrain: epoch  9, batch    29 | loss: 87.8907532CurrentTrain: epoch  9, batch    30 | loss: 70.2228920CurrentTrain: epoch  9, batch    31 | loss: 66.9061384CurrentTrain: epoch  9, batch    32 | loss: 68.6747247CurrentTrain: epoch  9, batch    33 | loss: 69.5842338CurrentTrain: epoch  9, batch    34 | loss: 60.0391907CurrentTrain: epoch  9, batch    35 | loss: 59.9408303CurrentTrain: epoch  9, batch    36 | loss: 70.7521686CurrentTrain: epoch  9, batch    37 | loss: 60.1895406CurrentTrain: epoch  9, batch    38 | loss: 68.2247947CurrentTrain: epoch  9, batch    39 | loss: 90.2889955CurrentTrain: epoch  9, batch    40 | loss: 73.5150415CurrentTrain: epoch  9, batch    41 | loss: 68.7774629CurrentTrain: epoch  9, batch    42 | loss: 68.1353802CurrentTrain: epoch  9, batch    43 | loss: 61.5027839CurrentTrain: epoch  9, batch    44 | loss: 47.9284023CurrentTrain: epoch  9, batch    45 | loss: 60.7422962CurrentTrain: epoch  9, batch    46 | loss: 74.7506860CurrentTrain: epoch  9, batch    47 | loss: 53.4272466CurrentTrain: epoch  9, batch    48 | loss: 69.4769744CurrentTrain: epoch  9, batch    49 | loss: 68.6732708CurrentTrain: epoch  9, batch    50 | loss: 61.0152691CurrentTrain: epoch  9, batch    51 | loss: 64.6398311CurrentTrain: epoch  9, batch    52 | loss: 86.8294817CurrentTrain: epoch  9, batch    53 | loss: 88.6350706CurrentTrain: epoch  9, batch    54 | loss: 66.8277219CurrentTrain: epoch  9, batch    55 | loss: 70.7986207CurrentTrain: epoch  9, batch    56 | loss: 116.2753894CurrentTrain: epoch  9, batch    57 | loss: 69.4434510CurrentTrain: epoch  9, batch    58 | loss: 90.7212160CurrentTrain: epoch  9, batch    59 | loss: 69.7977388CurrentTrain: epoch  9, batch    60 | loss: 49.0632813CurrentTrain: epoch  9, batch    61 | loss: 71.5013397CurrentTrain: epoch  9, batch    62 | loss: 70.1052907CurrentTrain: epoch  9, batch    63 | loss: 89.6730280CurrentTrain: epoch  9, batch    64 | loss: 110.5816249CurrentTrain: epoch  9, batch    65 | loss: 115.0372023CurrentTrain: epoch  9, batch    66 | loss: 111.9854028CurrentTrain: epoch  9, batch    67 | loss: 74.0054891CurrentTrain: epoch  9, batch    68 | loss: 69.4195081CurrentTrain: epoch  9, batch    69 | loss: 57.2570664CurrentTrain: epoch  9, batch    70 | loss: 68.6645396CurrentTrain: epoch  9, batch    71 | loss: 61.3804945CurrentTrain: epoch  9, batch    72 | loss: 71.3711121CurrentTrain: epoch  9, batch    73 | loss: 107.1139716CurrentTrain: epoch  9, batch    74 | loss: 51.9285494CurrentTrain: epoch  9, batch    75 | loss: 58.7971415CurrentTrain: epoch  9, batch    76 | loss: 71.4759864CurrentTrain: epoch  9, batch    77 | loss: 58.6776794CurrentTrain: epoch  9, batch    78 | loss: 62.9762970CurrentTrain: epoch  9, batch    79 | loss: 51.6178030CurrentTrain: epoch  9, batch    80 | loss: 84.6539184CurrentTrain: epoch  9, batch    81 | loss: 89.6754198CurrentTrain: epoch  9, batch    82 | loss: 62.7627760CurrentTrain: epoch  9, batch    83 | loss: 62.0403817CurrentTrain: epoch  9, batch    84 | loss: 69.1949239CurrentTrain: epoch  9, batch    85 | loss: 76.0419285CurrentTrain: epoch  9, batch    86 | loss: 51.8959048CurrentTrain: epoch  9, batch    87 | loss: 114.0837594CurrentTrain: epoch  9, batch    88 | loss: 48.7028969CurrentTrain: epoch  9, batch    89 | loss: 52.2144396CurrentTrain: epoch  9, batch    90 | loss: 61.9988034CurrentTrain: epoch  9, batch    91 | loss: 107.6342611CurrentTrain: epoch  9, batch    92 | loss: 114.0569560CurrentTrain: epoch  9, batch    93 | loss: 56.3425047CurrentTrain: epoch  9, batch    94 | loss: 74.8547445CurrentTrain: epoch  9, batch    95 | loss: 100.8411658

F1 score per class: {32: 0.6467661691542289, 6: 0.8144796380090498, 19: 0.32653061224489793, 24: 0.7513812154696132, 26: 0.9183673469387755, 29: 0.8157894736842105}
Micro-average F1 score: 0.7695167286245354
Weighted-average F1 score: 0.7660287897025163
F1 score per class: {32: 0.6153846153846154, 6: 0.788135593220339, 19: 0.20253164556962025, 24: 0.7513812154696132, 26: 0.94, 29: 0.8317757009345794}
Micro-average F1 score: 0.7457013574660634
Weighted-average F1 score: 0.7285974750917072
F1 score per class: {32: 0.6153846153846154, 6: 0.801762114537445, 19: 0.22857142857142856, 24: 0.7513812154696132, 26: 0.94, 29: 0.8130841121495327}
Micro-average F1 score: 0.7506899724011039
Weighted-average F1 score: 0.7376529148182666

F1 score per class: {32: 0.6467661691542289, 6: 0.8144796380090498, 19: 0.32653061224489793, 24: 0.7513812154696132, 26: 0.9183673469387755, 29: 0.8157894736842105}
Micro-average F1 score: 0.7695167286245354
Weighted-average F1 score: 0.7660287897025163
F1 score per class: {32: 0.6153846153846154, 6: 0.788135593220339, 19: 0.20253164556962025, 24: 0.7513812154696132, 26: 0.94, 29: 0.8317757009345794}
Micro-average F1 score: 0.7457013574660634
Weighted-average F1 score: 0.7285974750917072
F1 score per class: {32: 0.6153846153846154, 6: 0.801762114537445, 19: 0.22857142857142856, 24: 0.7513812154696132, 26: 0.94, 29: 0.8130841121495327}
Micro-average F1 score: 0.7506899724011039
Weighted-average F1 score: 0.7376529148182666

F1 score per class: {32: 0.4659498207885305, 6: 0.759493670886076, 19: 0.17391304347826086, 24: 0.6868686868686869, 26: 0.8411214953271028, 29: 0.5961538461538461}
Micro-average F1 score: 0.6216216216216216
Weighted-average F1 score: 0.601856035890623
F1 score per class: {32: 0.43795620437956206, 6: 0.7209302325581395, 19: 0.11940298507462686, 24: 0.6766169154228856, 26: 0.8663594470046083, 29: 0.6472727272727272}
Micro-average F1 score: 0.6063281824871228
Weighted-average F1 score: 0.5789978520866402
F1 score per class: {32: 0.43795620437956206, 6: 0.7338709677419355, 19: 0.1322314049586777, 24: 0.6766169154228856, 26: 0.8663594470046083, 29: 0.6258992805755396}
Micro-average F1 score: 0.6094100074682599
Weighted-average F1 score: 0.584104206088052

F1 score per class: {32: 0.4659498207885305, 6: 0.759493670886076, 19: 0.17391304347826086, 24: 0.6868686868686869, 26: 0.8411214953271028, 29: 0.5961538461538461}
Micro-average F1 score: 0.6216216216216216
Weighted-average F1 score: 0.601856035890623
F1 score per class: {32: 0.43795620437956206, 6: 0.7209302325581395, 19: 0.11940298507462686, 24: 0.6766169154228856, 26: 0.8663594470046083, 29: 0.6472727272727272}
Micro-average F1 score: 0.6063281824871228
Weighted-average F1 score: 0.5789978520866402
F1 score per class: {32: 0.43795620437956206, 6: 0.7338709677419355, 19: 0.1322314049586777, 24: 0.6766169154228856, 26: 0.8663594470046083, 29: 0.6258992805755396}
Micro-average F1 score: 0.6094100074682599
Weighted-average F1 score: 0.584104206088052
cur_acc_wo_na:  ['0.7695']
his_acc_wo_na:  ['0.7695']
cur_acc des_wo_na:  ['0.7457']
his_acc des_wo_na:  ['0.7457']
cur_acc rrf_wo_na:  ['0.7507']
his_acc rrf_wo_na:  ['0.7507']
cur_acc_w_na:  ['0.6216']
his_acc_w_na:  ['0.6216']
cur_acc des_w_na:  ['0.6063']
his_acc des_w_na:  ['0.6063']
cur_acc rrf_w_na:  ['0.6094']
his_acc rrf_w_na:  ['0.6094']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 90.4703880CurrentTrain: epoch  0, batch     1 | loss: 118.9944572CurrentTrain: epoch  0, batch     2 | loss: 115.9897553CurrentTrain: epoch  0, batch     3 | loss: 116.3037976CurrentTrain: epoch  0, batch     4 | loss: 56.9868403CurrentTrain: epoch  1, batch     0 | loss: 85.7697829CurrentTrain: epoch  1, batch     1 | loss: 102.3750569CurrentTrain: epoch  1, batch     2 | loss: 105.0143161CurrentTrain: epoch  1, batch     3 | loss: 81.7789443CurrentTrain: epoch  1, batch     4 | loss: 83.5315552CurrentTrain: epoch  2, batch     0 | loss: 80.9122928CurrentTrain: epoch  2, batch     1 | loss: 98.9907542CurrentTrain: epoch  2, batch     2 | loss: 100.1465247CurrentTrain: epoch  2, batch     3 | loss: 78.7669510CurrentTrain: epoch  2, batch     4 | loss: 77.8189487CurrentTrain: epoch  3, batch     0 | loss: 252.4040452CurrentTrain: epoch  3, batch     1 | loss: 77.9358559CurrentTrain: epoch  3, batch     2 | loss: 79.3737395CurrentTrain: epoch  3, batch     3 | loss: 97.1285637CurrentTrain: epoch  3, batch     4 | loss: 50.8392140CurrentTrain: epoch  4, batch     0 | loss: 69.1836045CurrentTrain: epoch  4, batch     1 | loss: 90.4106384CurrentTrain: epoch  4, batch     2 | loss: 119.1541569CurrentTrain: epoch  4, batch     3 | loss: 79.0038034CurrentTrain: epoch  4, batch     4 | loss: 75.7672083CurrentTrain: epoch  5, batch     0 | loss: 96.0270623CurrentTrain: epoch  5, batch     1 | loss: 77.5981608CurrentTrain: epoch  5, batch     2 | loss: 114.2315567CurrentTrain: epoch  5, batch     3 | loss: 75.6545941CurrentTrain: epoch  5, batch     4 | loss: 71.5905532CurrentTrain: epoch  6, batch     0 | loss: 73.7316755CurrentTrain: epoch  6, batch     1 | loss: 115.9025658CurrentTrain: epoch  6, batch     2 | loss: 74.5547089CurrentTrain: epoch  6, batch     3 | loss: 116.1605852CurrentTrain: epoch  6, batch     4 | loss: 59.8264005CurrentTrain: epoch  7, batch     0 | loss: 62.6624888CurrentTrain: epoch  7, batch     1 | loss: 91.4556169CurrentTrain: epoch  7, batch     2 | loss: 60.2183477CurrentTrain: epoch  7, batch     3 | loss: 76.0734764CurrentTrain: epoch  7, batch     4 | loss: 147.6151700CurrentTrain: epoch  8, batch     0 | loss: 61.3187943CurrentTrain: epoch  8, batch     1 | loss: 114.1415810CurrentTrain: epoch  8, batch     2 | loss: 63.4049263CurrentTrain: epoch  8, batch     3 | loss: 73.1098094CurrentTrain: epoch  8, batch     4 | loss: 67.9838535CurrentTrain: epoch  9, batch     0 | loss: 58.4941112CurrentTrain: epoch  9, batch     1 | loss: 73.4265430CurrentTrain: epoch  9, batch     2 | loss: 71.8442646CurrentTrain: epoch  9, batch     3 | loss: 89.4109825CurrentTrain: epoch  9, batch     4 | loss: 69.7813506
MemoryTrain:  epoch  0, batch     0 | loss: 1.9285411MemoryTrain:  epoch  1, batch     0 | loss: 1.6195669MemoryTrain:  epoch  2, batch     0 | loss: 1.3537883MemoryTrain:  epoch  3, batch     0 | loss: 0.9490520MemoryTrain:  epoch  4, batch     0 | loss: 0.8325501MemoryTrain:  epoch  5, batch     0 | loss: 0.6592856MemoryTrain:  epoch  6, batch     0 | loss: 0.4688740MemoryTrain:  epoch  7, batch     0 | loss: 0.3830440MemoryTrain:  epoch  8, batch     0 | loss: 0.3134563MemoryTrain:  epoch  9, batch     0 | loss: 0.2178702

F1 score per class: {32: 0.8921568627450981, 5: 0.0, 6: 0.14814814814814814, 10: 0.6428571428571429, 16: 0.0, 17: 0.18604651162790697, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5249457700650759
Weighted-average F1 score: 0.6043246686201543
F1 score per class: {32: 0.7449392712550608, 5: 0.0, 6: 0.5454545454545454, 10: 0.6129032258064516, 16: 0.0, 17: 0.43636363636363634, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5536912751677853
Weighted-average F1 score: 0.5200877094043478
F1 score per class: {32: 0.7829787234042553, 5: 0.0, 6: 0.5, 10: 0.6, 16: 0.0, 17: 0.4230769230769231, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5631768953068592
Weighted-average F1 score: 0.5397420198047781

F1 score per class: {32: 0.8878048780487805, 5: 0.6044444444444445, 6: 0.14678899082568808, 10: 0.6, 16: 0.0, 17: 0.17777777777777778, 18: 0.7932489451476793, 19: 0.3225806451612903, 24: 0.7403314917127072, 26: 0.8923076923076924, 29: 0.7666666666666667}
Micro-average F1 score: 0.686624203821656
Weighted-average F1 score: 0.7273923449664343
F1 score per class: {32: 0.7159533073929961, 5: 0.6016949152542372, 6: 0.4692737430167598, 10: 0.5507246376811594, 16: 0.0, 17: 0.4067796610169492, 18: 0.6886446886446886, 19: 0.24444444444444444, 24: 0.7351351351351352, 26: 0.8866995073891626, 29: 0.7286821705426356}
Micro-average F1 score: 0.650575973669775
Weighted-average F1 score: 0.6486093942413766
F1 score per class: {32: 0.757201646090535, 5: 0.6016949152542372, 6: 0.4375, 10: 0.5373134328358209, 16: 0.0, 17: 0.39285714285714285, 18: 0.7153846153846154, 19: 0.2558139534883721, 24: 0.7391304347826086, 26: 0.8844221105527639, 29: 0.746031746031746}
Micro-average F1 score: 0.6613545816733067
Weighted-average F1 score: 0.6631398595605086

F1 score per class: {32: 0.7679324894514767, 5: 0.0, 6: 0.14285714285714285, 10: 0.391304347826087, 16: 0.0, 17: 0.1509433962264151, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.40878378378378377
Weighted-average F1 score: 0.42277531858853823
F1 score per class: {32: 0.5644171779141104, 5: 0.0, 6: 0.4329896907216495, 10: 0.3838383838383838, 16: 0.0, 17: 0.32432432432432434, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.3923900118906064
Weighted-average F1 score: 0.365918870126146
F1 score per class: {32: 0.6032786885245902, 5: 0.0, 6: 0.40229885057471265, 10: 0.3711340206185567, 16: 0.0, 17: 0.3142857142857143, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.4036222509702458
Weighted-average F1 score: 0.37963713087110984

F1 score per class: {32: 0.7551867219917012, 5: 0.3965014577259475, 6: 0.13333333333333333, 10: 0.34615384615384615, 16: 0.0, 17: 0.13793103448275862, 18: 0.706766917293233, 19: 0.16806722689075632, 24: 0.6600985221674877, 26: 0.8055555555555556, 29: 0.5768025078369906}
Micro-average F1 score: 0.536318407960199
Weighted-average F1 score: 0.5395248558100856
F1 score per class: {32: 0.519774011299435, 5: 0.3837837837837838, 6: 0.34710743801652894, 10: 0.3333333333333333, 16: 0.0, 17: 0.2857142857142857, 18: 0.6045016077170418, 19: 0.14012738853503184, 24: 0.6415094339622641, 26: 0.7377049180327869, 29: 0.5497076023391813}
Micro-average F1 score: 0.48427929767251937
Weighted-average F1 score: 0.47401655523857317
F1 score per class: {32: 0.5661538461538461, 5: 0.38274932614555257, 6: 0.3286384976525822, 10: 0.3185840707964602, 16: 0.0, 17: 0.275, 18: 0.6241610738255033, 19: 0.14965986394557823, 24: 0.6476190476190476, 26: 0.7787610619469026, 29: 0.5595238095238095}
Micro-average F1 score: 0.496793501496366
Weighted-average F1 score: 0.4859630641024993
cur_acc_wo_na:  ['0.7695', '0.5249']
his_acc_wo_na:  ['0.7695', '0.6866']
cur_acc des_wo_na:  ['0.7457', '0.5537']
his_acc des_wo_na:  ['0.7457', '0.6506']
cur_acc rrf_wo_na:  ['0.7507', '0.5632']
his_acc rrf_wo_na:  ['0.7507', '0.6614']
cur_acc_w_na:  ['0.6216', '0.4088']
his_acc_w_na:  ['0.6216', '0.5363']
cur_acc des_w_na:  ['0.6063', '0.3924']
his_acc des_w_na:  ['0.6063', '0.4843']
cur_acc rrf_w_na:  ['0.6094', '0.4036']
his_acc rrf_w_na:  ['0.6094', '0.4968']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 77.3817915CurrentTrain: epoch  0, batch     1 | loss: 94.2315596CurrentTrain: epoch  0, batch     2 | loss: 109.1992961CurrentTrain: epoch  0, batch     3 | loss: 74.1284295CurrentTrain: epoch  0, batch     4 | loss: 36.3280525CurrentTrain: epoch  1, batch     0 | loss: 88.7005365CurrentTrain: epoch  1, batch     1 | loss: 73.7367874CurrentTrain: epoch  1, batch     2 | loss: 88.1847943CurrentTrain: epoch  1, batch     3 | loss: 80.1294252CurrentTrain: epoch  1, batch     4 | loss: 34.7162793CurrentTrain: epoch  2, batch     0 | loss: 94.7281944CurrentTrain: epoch  2, batch     1 | loss: 82.4157700CurrentTrain: epoch  2, batch     2 | loss: 95.0514788CurrentTrain: epoch  2, batch     3 | loss: 63.5780295CurrentTrain: epoch  2, batch     4 | loss: 22.0596375CurrentTrain: epoch  3, batch     0 | loss: 79.4294821CurrentTrain: epoch  3, batch     1 | loss: 68.4818856CurrentTrain: epoch  3, batch     2 | loss: 93.1588330CurrentTrain: epoch  3, batch     3 | loss: 76.5533061CurrentTrain: epoch  3, batch     4 | loss: 12.8369033CurrentTrain: epoch  4, batch     0 | loss: 93.4998702CurrentTrain: epoch  4, batch     1 | loss: 74.6359963CurrentTrain: epoch  4, batch     2 | loss: 61.5945297CurrentTrain: epoch  4, batch     3 | loss: 75.4588969CurrentTrain: epoch  4, batch     4 | loss: 36.6071913CurrentTrain: epoch  5, batch     0 | loss: 73.2499209CurrentTrain: epoch  5, batch     1 | loss: 88.0200292CurrentTrain: epoch  5, batch     2 | loss: 87.9677390CurrentTrain: epoch  5, batch     3 | loss: 73.4563758CurrentTrain: epoch  5, batch     4 | loss: 37.9323718CurrentTrain: epoch  6, batch     0 | loss: 62.3140244CurrentTrain: epoch  6, batch     1 | loss: 74.3406640CurrentTrain: epoch  6, batch     2 | loss: 71.5499619CurrentTrain: epoch  6, batch     3 | loss: 91.3735213CurrentTrain: epoch  6, batch     4 | loss: 14.2267287CurrentTrain: epoch  7, batch     0 | loss: 89.1845800CurrentTrain: epoch  7, batch     1 | loss: 88.8673490CurrentTrain: epoch  7, batch     2 | loss: 66.7484023CurrentTrain: epoch  7, batch     3 | loss: 112.5765414CurrentTrain: epoch  7, batch     4 | loss: 21.1999792CurrentTrain: epoch  8, batch     0 | loss: 89.0342370CurrentTrain: epoch  8, batch     1 | loss: 84.2406075CurrentTrain: epoch  8, batch     2 | loss: 58.9672087CurrentTrain: epoch  8, batch     3 | loss: 60.0590967CurrentTrain: epoch  8, batch     4 | loss: 21.8288845CurrentTrain: epoch  9, batch     0 | loss: 70.3339681CurrentTrain: epoch  9, batch     1 | loss: 68.6375053CurrentTrain: epoch  9, batch     2 | loss: 68.8546470CurrentTrain: epoch  9, batch     3 | loss: 71.4781624CurrentTrain: epoch  9, batch     4 | loss: 23.4318569
MemoryTrain:  epoch  0, batch     0 | loss: 1.1668834MemoryTrain:  epoch  1, batch     0 | loss: 1.0681541MemoryTrain:  epoch  2, batch     0 | loss: 0.8035594MemoryTrain:  epoch  3, batch     0 | loss: 0.6191050MemoryTrain:  epoch  4, batch     0 | loss: 0.6032895MemoryTrain:  epoch  5, batch     0 | loss: 0.4246390MemoryTrain:  epoch  6, batch     0 | loss: 0.3927750MemoryTrain:  epoch  7, batch     0 | loss: 0.3012120MemoryTrain:  epoch  8, batch     0 | loss: 0.2301666MemoryTrain:  epoch  9, batch     0 | loss: 0.1984062

F1 score per class: {32: 0.6, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.6303030303030303, 11: 0.6705882352941176, 12: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.24242424242424243, 24: 0.0, 28: 0.0, 29: 0.14285714285714285}
Micro-average F1 score: 0.5020746887966805
Weighted-average F1 score: 0.41876469910466774
F1 score per class: {32: 0.56, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.6103896103896104, 11: 0.6428571428571429, 12: 0.0, 10: 0.0, 18: 0.0, 19: 0.23255813953488372, 24: 0.0, 28: 0.0, 29: 0.29411764705882354}
Micro-average F1 score: 0.4568345323741007
Weighted-average F1 score: 0.37086613597892404
F1 score per class: {32: 0.6086956521739131, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.6025641025641025, 11: 0.648936170212766, 12: 0.0, 10: 0.0, 18: 0.0, 19: 0.22727272727272727, 24: 0.0, 28: 0.0, 29: 0.3125}
Micro-average F1 score: 0.46904315196998125
Weighted-average F1 score: 0.3834691059405584

F1 score per class: {32: 0.5217391304347826, 2: 0.8053097345132744, 5: 0.5023255813953489, 6: 0.09174311926605505, 39: 0.4094488188976378, 11: 0.4351145038167939, 12: 0.5818181818181818, 10: 0.0, 16: 0.0, 17: 0.753968253968254, 18: 0.3055555555555556, 19: 0.7301587301587301, 24: 0.13333333333333333, 26: 0.8571428571428571, 28: 0.6943396226415094, 29: 0.08163265306122448}
Micro-average F1 score: 0.5584872471416007
Weighted-average F1 score: 0.5686744981829844
F1 score per class: {32: 0.4, 2: 0.6445182724252492, 5: 0.5503875968992248, 6: 0.422360248447205, 39: 0.47715736040609136, 11: 0.37058823529411766, 12: 0.6071428571428571, 10: 0.0, 16: 0.1935483870967742, 17: 0.7131782945736435, 18: 0.2222222222222222, 19: 0.7319587628865979, 24: 0.13333333333333333, 26: 0.8645833333333334, 28: 0.6438356164383562, 29: 0.2222222222222222}
Micro-average F1 score: 0.5473684210526316
Weighted-average F1 score: 0.5352550015720664
F1 score per class: {32: 0.5384615384615384, 2: 0.6834532374100719, 5: 0.5569620253164557, 6: 0.35294117647058826, 39: 0.44761904761904764, 11: 0.3935483870967742, 12: 0.6071428571428571, 10: 0.0, 16: 0.0, 17: 0.7335907335907336, 18: 0.22727272727272727, 19: 0.7263157894736842, 24: 0.11904761904761904, 26: 0.8645833333333334, 28: 0.6460481099656358, 29: 0.19607843137254902}
Micro-average F1 score: 0.5494327390599676
Weighted-average F1 score: 0.5429666095538495

F1 score per class: {32: 0.3870967741935484, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.49056603773584906, 11: 0.5327102803738317, 12: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.13333333333333333, 26: 0.0, 28: 0.0, 29: 0.08888888888888889}
Micro-average F1 score: 0.34770114942528735
Weighted-average F1 score: 0.29300666365410705
F1 score per class: {32: 0.358974358974359, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.5053763440860215, 11: 0.5, 12: 0.0, 10: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.13157894736842105, 26: 0.0, 28: 0.0, 29: 0.20408163265306123}
Micro-average F1 score: 0.31013431013431014
Weighted-average F1 score: 0.2539772300892844
F1 score per class: {32: 0.4117647058823529, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.4845360824742268, 11: 0.5126050420168067, 12: 0.0, 10: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.12987012987012986, 26: 0.0, 28: 0.0, 29: 0.2127659574468085}
Micro-average F1 score: 0.32341526520051744
Weighted-average F1 score: 0.2678280753509563

F1 score per class: {32: 0.3333333333333333, 2: 0.5759493670886076, 5: 0.2983425414364641, 6: 0.0847457627118644, 39: 0.26, 11: 0.21631878557874762, 12: 0.367816091954023, 10: 0.0, 16: 0.0, 17: 0.6810035842293907, 18: 0.16666666666666666, 19: 0.6448598130841121, 24: 0.07207207207207207, 26: 0.7641509433962265, 28: 0.5054945054945055, 29: 0.047619047619047616}
Micro-average F1 score: 0.3842662632375189
Weighted-average F1 score: 0.3716211796230996
F1 score per class: {32: 0.23333333333333334, 2: 0.40670859538784065, 5: 0.31767337807606266, 6: 0.31336405529953915, 39: 0.32752613240418116, 11: 0.1926605504587156, 12: 0.41975308641975306, 10: 0.0, 16: 0.1188118811881188, 17: 0.6323024054982818, 18: 0.1282051282051282, 19: 0.6339285714285714, 24: 0.07194244604316546, 26: 0.7757009345794392, 28: 0.47715736040609136, 29: 0.1282051282051282}
Micro-average F1 score: 0.36591086786551996
Weighted-average F1 score: 0.3457724431188086
F1 score per class: {32: 0.3333333333333333, 2: 0.4408352668213457, 5: 0.3308270676691729, 6: 0.2857142857142857, 39: 0.29283489096573206, 11: 0.20165289256198346, 12: 0.38636363636363635, 10: 0.0, 16: 0.0, 17: 0.6506849315068494, 18: 0.13245033112582782, 19: 0.6359447004608295, 24: 0.06451612903225806, 26: 0.7757009345794392, 28: 0.47715736040609136, 29: 0.10989010989010989}
Micro-average F1 score: 0.3709986320109439
Weighted-average F1 score: 0.3525167221805913
cur_acc_wo_na:  ['0.7695', '0.5249', '0.5021']
his_acc_wo_na:  ['0.7695', '0.6866', '0.5585']
cur_acc des_wo_na:  ['0.7457', '0.5537', '0.4568']
his_acc des_wo_na:  ['0.7457', '0.6506', '0.5474']
cur_acc rrf_wo_na:  ['0.7507', '0.5632', '0.4690']
his_acc rrf_wo_na:  ['0.7507', '0.6614', '0.5494']
cur_acc_w_na:  ['0.6216', '0.4088', '0.3477']
his_acc_w_na:  ['0.6216', '0.5363', '0.3843']
cur_acc des_w_na:  ['0.6063', '0.3924', '0.3101']
his_acc des_w_na:  ['0.6063', '0.4843', '0.3659']
cur_acc rrf_w_na:  ['0.6094', '0.4036', '0.3234']
his_acc rrf_w_na:  ['0.6094', '0.4968', '0.3710']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 110.6899140CurrentTrain: epoch  0, batch     1 | loss: 123.6038427CurrentTrain: epoch  0, batch     2 | loss: 95.1652799CurrentTrain: epoch  0, batch     3 | loss: 99.8413051CurrentTrain: epoch  1, batch     0 | loss: 87.7409791CurrentTrain: epoch  1, batch     1 | loss: 78.5382273CurrentTrain: epoch  1, batch     2 | loss: 101.2097320CurrentTrain: epoch  1, batch     3 | loss: 78.6682815CurrentTrain: epoch  2, batch     0 | loss: 101.8943407CurrentTrain: epoch  2, batch     1 | loss: 81.1449176CurrentTrain: epoch  2, batch     2 | loss: 87.7654870CurrentTrain: epoch  2, batch     3 | loss: 81.5199488CurrentTrain: epoch  3, batch     0 | loss: 99.7811498CurrentTrain: epoch  3, batch     1 | loss: 95.2913753CurrentTrain: epoch  3, batch     2 | loss: 83.1939132CurrentTrain: epoch  3, batch     3 | loss: 72.5735698CurrentTrain: epoch  4, batch     0 | loss: 96.0025146CurrentTrain: epoch  4, batch     1 | loss: 81.0485379CurrentTrain: epoch  4, batch     2 | loss: 78.7055916CurrentTrain: epoch  4, batch     3 | loss: 63.9126357CurrentTrain: epoch  5, batch     0 | loss: 76.7567520CurrentTrain: epoch  5, batch     1 | loss: 115.1191225CurrentTrain: epoch  5, batch     2 | loss: 93.6134239CurrentTrain: epoch  5, batch     3 | loss: 55.0619966CurrentTrain: epoch  6, batch     0 | loss: 60.8929388CurrentTrain: epoch  6, batch     1 | loss: 79.2999762CurrentTrain: epoch  6, batch     2 | loss: 73.6782859CurrentTrain: epoch  6, batch     3 | loss: 132.7345610CurrentTrain: epoch  7, batch     0 | loss: 92.7794713CurrentTrain: epoch  7, batch     1 | loss: 90.4435376CurrentTrain: epoch  7, batch     2 | loss: 73.5422922CurrentTrain: epoch  7, batch     3 | loss: 73.0248927CurrentTrain: epoch  8, batch     0 | loss: 63.2420095CurrentTrain: epoch  8, batch     1 | loss: 62.4289454CurrentTrain: epoch  8, batch     2 | loss: 75.3304735CurrentTrain: epoch  8, batch     3 | loss: 74.0430008CurrentTrain: epoch  9, batch     0 | loss: 60.5332491CurrentTrain: epoch  9, batch     1 | loss: 58.5056316CurrentTrain: epoch  9, batch     2 | loss: 157.0578764CurrentTrain: epoch  9, batch     3 | loss: 94.8811632
MemoryTrain:  epoch  0, batch     0 | loss: 1.1600815MemoryTrain:  epoch  1, batch     0 | loss: 0.9838927MemoryTrain:  epoch  2, batch     0 | loss: 0.8762440MemoryTrain:  epoch  3, batch     0 | loss: 0.7158316MemoryTrain:  epoch  4, batch     0 | loss: 0.5810256MemoryTrain:  epoch  5, batch     0 | loss: 0.5460716MemoryTrain:  epoch  6, batch     0 | loss: 0.4281379MemoryTrain:  epoch  7, batch     0 | loss: 0.3939458MemoryTrain:  epoch  8, batch     0 | loss: 0.3246321MemoryTrain:  epoch  9, batch     0 | loss: 0.3469405

F1 score per class: {0: 0.8823529411764706, 2: 0.0, 4: 0.8165680473372781, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.36363636363636365, 17: 0.0, 19: 0.0, 21: 0.5633802816901409, 23: 0.7058823529411765, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.593320235756385
Weighted-average F1 score: 0.46794704257415676
F1 score per class: {0: 0.8095238095238095, 2: 0.0, 4: 0.8089887640449438, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.4, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.45977011494252873, 23: 0.6904761904761905, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.5432525951557093
Weighted-average F1 score: 0.43140639141766857
F1 score per class: {0: 0.868421052631579, 2: 0.0, 4: 0.8208092485549133, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.4, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.4444444444444444, 23: 0.6904761904761905, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.5575539568345323
Weighted-average F1 score: 0.4391587424612545

F1 score per class: {0: 0.6976744186046512, 2: 0.42424242424242425, 4: 0.8117647058823529, 5: 0.7948717948717948, 6: 0.5391304347826087, 10: 0.20869565217391303, 11: 0.38, 12: 0.32460732984293195, 13: 0.06557377049180328, 16: 0.5217391304347826, 17: 0.0, 18: 0.0, 19: 0.7322834645669292, 21: 0.3389830508474576, 23: 0.6451612903225806, 24: 0.3225806451612903, 26: 0.708994708994709, 28: 0.375, 29: 0.8497409326424871, 32: 0.636986301369863, 39: 0.21052631578947367}
Micro-average F1 score: 0.5612762871646121
Weighted-average F1 score: 0.5610855174818337
F1 score per class: {0: 0.5483870967741935, 2: 0.2916666666666667, 4: 0.8, 5: 0.6736842105263158, 6: 0.5151515151515151, 10: 0.40718562874251496, 11: 0.4474885844748858, 12: 0.35454545454545455, 13: 0.1111111111111111, 16: 0.576271186440678, 17: 0.0, 18: 0.19047619047619047, 19: 0.7045454545454546, 21: 0.21978021978021978, 23: 0.651685393258427, 24: 0.30303030303030304, 26: 0.6834170854271356, 28: 0.2127659574468085, 29: 0.845360824742268, 32: 0.6460481099656358, 39: 0.23076923076923078}
Micro-average F1 score: 0.5461157024793388
Weighted-average F1 score: 0.5323608821335483
F1 score per class: {0: 0.6111111111111112, 2: 0.3783783783783784, 4: 0.8160919540229885, 5: 0.7011070110701108, 6: 0.5245901639344263, 10: 0.3972602739726027, 11: 0.398406374501992, 12: 0.34285714285714286, 13: 0.09302325581395349, 16: 0.6181818181818182, 17: 0.0, 18: 0.16666666666666666, 19: 0.6943396226415094, 21: 0.21621621621621623, 23: 0.6444444444444445, 24: 0.30303030303030304, 26: 0.694300518134715, 28: 0.2127659574468085, 29: 0.845360824742268, 32: 0.6460481099656358, 39: 0.21052631578947367}
Micro-average F1 score: 0.547610979329041
Weighted-average F1 score: 0.533443125596748

F1 score per class: {0: 0.8333333333333334, 2: 0.0, 4: 0.7840909090909091, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.23529411764705882, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.425531914893617, 23: 0.594059405940594, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.43081312410841655
Weighted-average F1 score: 0.3128249973564042
F1 score per class: {0: 0.7311827956989247, 2: 0.0, 4: 0.7741935483870968, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.3333333333333333, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3448275862068966, 23: 0.6105263157894737, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.38957816377171217
Weighted-average F1 score: 0.28750140450383005
F1 score per class: {0: 0.7951807228915663, 2: 0.0, 4: 0.7845303867403315, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.3333333333333333, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.33613445378151263, 23: 0.6041666666666666, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.40207522697795073
Weighted-average F1 score: 0.29336226227178225

F1 score per class: {0: 0.5825242718446602, 2: 0.27450980392156865, 4: 0.7666666666666667, 5: 0.5535714285714286, 6: 0.31794871794871793, 10: 0.1889763779527559, 11: 0.2446351931330472, 12: 0.18235294117647058, 13: 0.03508771929824561, 16: 0.3582089552238806, 17: 0.0, 18: 0.0, 19: 0.636986301369863, 21: 0.21164021164021163, 23: 0.47244094488188976, 24: 0.18518518518518517, 26: 0.6320754716981132, 28: 0.21428571428571427, 29: 0.7522935779816514, 32: 0.43155452436194897, 39: 0.10810810810810811}
Micro-average F1 score: 0.39661798616448884
Weighted-average F1 score: 0.37826108807241504
F1 score per class: {0: 0.4146341463414634, 2: 0.17721518987341772, 4: 0.7236180904522613, 5: 0.40083507306889354, 6: 0.2937365010799136, 10: 0.29955947136563876, 11: 0.3081761006289308, 12: 0.1875, 13: 0.06666666666666667, 16: 0.35789473684210527, 17: 0.0, 18: 0.11538461538461539, 19: 0.6, 21: 0.14035087719298245, 23: 0.5370370370370371, 24: 0.1694915254237288, 26: 0.5991189427312775, 28: 0.10526315789473684, 29: 0.7420814479638009, 32: 0.44976076555023925, 39: 0.125}
Micro-average F1 score: 0.37173717371737175
Weighted-average F1 score: 0.3512142936585554
F1 score per class: {0: 0.48175182481751827, 2: 0.2222222222222222, 4: 0.7634408602150538, 5: 0.4377880184331797, 6: 0.29698375870069604, 10: 0.3118279569892473, 11: 0.2801120448179272, 12: 0.183206106870229, 13: 0.05405405405405406, 16: 0.38202247191011235, 17: 0.0, 18: 0.0975609756097561, 19: 0.5935483870967742, 21: 0.1388888888888889, 23: 0.5225225225225225, 24: 0.1694915254237288, 26: 0.6090909090909091, 28: 0.1111111111111111, 29: 0.7420814479638009, 32: 0.44339622641509435, 39: 0.11320754716981132}
Micro-average F1 score: 0.3773937412424101
Weighted-average F1 score: 0.35541472571798416
cur_acc_wo_na:  ['0.7695', '0.5249', '0.5021', '0.5933']
his_acc_wo_na:  ['0.7695', '0.6866', '0.5585', '0.5613']
cur_acc des_wo_na:  ['0.7457', '0.5537', '0.4568', '0.5433']
his_acc des_wo_na:  ['0.7457', '0.6506', '0.5474', '0.5461']
cur_acc rrf_wo_na:  ['0.7507', '0.5632', '0.4690', '0.5576']
his_acc rrf_wo_na:  ['0.7507', '0.6614', '0.5494', '0.5476']
cur_acc_w_na:  ['0.6216', '0.4088', '0.3477', '0.4308']
his_acc_w_na:  ['0.6216', '0.5363', '0.3843', '0.3966']
cur_acc des_w_na:  ['0.6063', '0.3924', '0.3101', '0.3896']
his_acc des_w_na:  ['0.6063', '0.4843', '0.3659', '0.3717']
cur_acc rrf_w_na:  ['0.6094', '0.4036', '0.3234', '0.4021']
his_acc rrf_w_na:  ['0.6094', '0.4968', '0.3710', '0.3774']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 74.6493907CurrentTrain: epoch  0, batch     1 | loss: 112.1254002CurrentTrain: epoch  0, batch     2 | loss: 89.3458248CurrentTrain: epoch  0, batch     3 | loss: 75.8772281CurrentTrain: epoch  1, batch     0 | loss: 87.7980362CurrentTrain: epoch  1, batch     1 | loss: 69.9285513CurrentTrain: epoch  1, batch     2 | loss: 126.7507689CurrentTrain: epoch  1, batch     3 | loss: 52.0632564CurrentTrain: epoch  2, batch     0 | loss: 93.9679000CurrentTrain: epoch  2, batch     1 | loss: 73.9594356CurrentTrain: epoch  2, batch     2 | loss: 67.7647904CurrentTrain: epoch  2, batch     3 | loss: 68.5578207CurrentTrain: epoch  3, batch     0 | loss: 79.0676211CurrentTrain: epoch  3, batch     1 | loss: 73.5730343CurrentTrain: epoch  3, batch     2 | loss: 77.3433156CurrentTrain: epoch  3, batch     3 | loss: 80.2479789CurrentTrain: epoch  4, batch     0 | loss: 73.9837158CurrentTrain: epoch  4, batch     1 | loss: 81.2962075CurrentTrain: epoch  4, batch     2 | loss: 62.1598349CurrentTrain: epoch  4, batch     3 | loss: 48.2180427CurrentTrain: epoch  5, batch     0 | loss: 74.1743227CurrentTrain: epoch  5, batch     1 | loss: 69.0635002CurrentTrain: epoch  5, batch     2 | loss: 73.5787685CurrentTrain: epoch  5, batch     3 | loss: 65.4842715CurrentTrain: epoch  6, batch     0 | loss: 67.9509763CurrentTrain: epoch  6, batch     1 | loss: 62.7902058CurrentTrain: epoch  6, batch     2 | loss: 93.1869934CurrentTrain: epoch  6, batch     3 | loss: 49.4827371CurrentTrain: epoch  7, batch     0 | loss: 70.3852790CurrentTrain: epoch  7, batch     1 | loss: 70.9091402CurrentTrain: epoch  7, batch     2 | loss: 86.8810368CurrentTrain: epoch  7, batch     3 | loss: 57.8772754CurrentTrain: epoch  8, batch     0 | loss: 89.1678264CurrentTrain: epoch  8, batch     1 | loss: 59.7948001CurrentTrain: epoch  8, batch     2 | loss: 69.8699441CurrentTrain: epoch  8, batch     3 | loss: 58.3343929CurrentTrain: epoch  9, batch     0 | loss: 70.5586870CurrentTrain: epoch  9, batch     1 | loss: 69.8997349CurrentTrain: epoch  9, batch     2 | loss: 72.2410016CurrentTrain: epoch  9, batch     3 | loss: 42.9164193
MemoryTrain:  epoch  0, batch     0 | loss: 1.0159944MemoryTrain:  epoch  1, batch     0 | loss: 0.9385667MemoryTrain:  epoch  2, batch     0 | loss: 0.7743165MemoryTrain:  epoch  3, batch     0 | loss: 0.6368730MemoryTrain:  epoch  4, batch     0 | loss: 0.5087729MemoryTrain:  epoch  5, batch     0 | loss: 0.4299865MemoryTrain:  epoch  6, batch     0 | loss: 0.3962059MemoryTrain:  epoch  7, batch     0 | loss: 0.2846222MemoryTrain:  epoch  8, batch     0 | loss: 0.2614942MemoryTrain:  epoch  9, batch     0 | loss: 0.2128431

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.7, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5945945945945946, 29: 0.0, 32: 0.0, 35: 0.3835616438356164, 37: 0.5321100917431193, 38: 0.6046511627906976, 39: 0.0}
Micro-average F1 score: 0.37861915367483295
Weighted-average F1 score: 0.25670402896265304
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6666666666666666, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.6419753086419753, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.780952380952381, 37: 0.5523809523809524, 38: 0.7868852459016393, 39: 0.0}
Micro-average F1 score: 0.4790874524714829
Weighted-average F1 score: 0.35983508254546503
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.631578947368421, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.6419753086419753, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.651685393258427, 37: 0.5660377358490566, 38: 0.7636363636363637, 39: 0.0}
Micro-average F1 score: 0.4471057884231537
Weighted-average F1 score: 0.3190715236266391

F1 score per class: {0: 0.6987951807228916, 2: 0.3333333333333333, 4: 0.8095238095238095, 5: 0.7372549019607844, 6: 0.4734982332155477, 10: 0.176, 11: 0.2033898305084746, 12: 0.3225806451612903, 13: 0.041237113402061855, 15: 0.2413793103448276, 16: 0.5614035087719298, 17: 0.0, 18: 0.0, 19: 0.6838235294117647, 21: 0.2376237623762376, 23: 0.6, 24: 0.3076923076923077, 25: 0.5945945945945946, 26: 0.6732673267326733, 28: 0.4, 29: 0.85, 32: 0.6084142394822006, 35: 0.2616822429906542, 37: 0.1336405529953917, 38: 0.2857142857142857, 39: 0.18181818181818182}
Micro-average F1 score: 0.46118980169971674
Weighted-average F1 score: 0.4395107450082722
F1 score per class: {0: 0.5932203389830508, 2: 0.1794871794871795, 4: 0.7888888888888889, 5: 0.5791044776119403, 6: 0.4662576687116564, 10: 0.3333333333333333, 11: 0.24489795918367346, 12: 0.34959349593495936, 13: 0.047619047619047616, 15: 0.2926829268292683, 16: 0.6197183098591549, 17: 0.0, 18: 0.14736842105263157, 19: 0.6920152091254753, 21: 0.15763546798029557, 23: 0.5517241379310345, 24: 0.3181818181818182, 25: 0.6341463414634146, 26: 0.6448598130841121, 28: 0.16216216216216217, 29: 0.8309178743961353, 32: 0.6739130434782609, 35: 0.41624365482233505, 37: 0.2109090909090909, 38: 0.2857142857142857, 39: 0.2702702702702703}
Micro-average F1 score: 0.46115906288532676
Weighted-average F1 score: 0.4369714945407896
F1 score per class: {0: 0.6407766990291263, 2: 0.3111111111111111, 4: 0.8409090909090909, 5: 0.6188925081433225, 6: 0.4779874213836478, 10: 0.3431952662721893, 11: 0.22448979591836735, 12: 0.36444444444444446, 13: 0.04081632653061224, 15: 0.26666666666666666, 16: 0.6461538461538462, 17: 0.0, 18: 0.045454545454545456, 19: 0.6840148698884758, 21: 0.17204301075268819, 23: 0.5581395348837209, 24: 0.3181818181818182, 25: 0.6341463414634146, 26: 0.6602870813397129, 28: 0.2222222222222222, 29: 0.83, 32: 0.6138613861386139, 35: 0.4027777777777778, 37: 0.17751479289940827, 38: 0.302158273381295, 39: 0.21621621621621623}
Micro-average F1 score: 0.46790890269151136
Weighted-average F1 score: 0.4443264225189515

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.5384615384615384, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5365853658536586, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.3181818181818182, 37: 0.43609022556390975, 38: 0.5098039215686274, 39: 0.0}
Micro-average F1 score: 0.27287319422150885
Weighted-average F1 score: 0.18968187091461014
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.5217391304347826, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5473684210526316, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.5942028985507246, 37: 0.4715447154471545, 38: 0.5714285714285714, 39: 0.0}
Micro-average F1 score: 0.3251612903225806
Weighted-average F1 score: 0.2503874913728208
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.46153846153846156, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5531914893617021, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.5178571428571429, 37: 0.48, 38: 0.56, 39: 0.0}
Micro-average F1 score: 0.3076923076923077
Weighted-average F1 score: 0.22718774886545084

F1 score per class: {0: 0.5979381443298969, 2: 0.2222222222222222, 4: 0.7597765363128491, 5: 0.5645645645645646, 6: 0.2774327122153209, 10: 0.1527777777777778, 11: 0.15319148936170213, 12: 0.1671309192200557, 13: 0.022598870056497175, 15: 0.14583333333333334, 16: 0.3764705882352941, 17: 0.0, 18: 0.0, 19: 0.5602409638554217, 21: 0.1643835616438356, 23: 0.4948453608247423, 24: 0.1518987341772152, 25: 0.5238095238095238, 26: 0.5787234042553191, 28: 0.25, 29: 0.6967213114754098, 32: 0.428246013667426, 35: 0.1590909090909091, 37: 0.07880434782608696, 38: 0.15950920245398773, 39: 0.11538461538461539}
Micro-average F1 score: 0.31909055272442177
Weighted-average F1 score: 0.2943618150465023
F1 score per class: {0: 0.45161290322580644, 2: 0.112, 4: 0.7135678391959799, 5: 0.388, 6: 0.26161790017211706, 10: 0.23754789272030652, 11: 0.200836820083682, 12: 0.17954070981210857, 13: 0.02631578947368421, 15: 0.19047619047619047, 16: 0.37606837606837606, 17: 0.0, 18: 0.10687022900763359, 19: 0.5814696485623003, 21: 0.1038961038961039, 23: 0.43636363636363634, 24: 0.15555555555555556, 25: 0.5048543689320388, 26: 0.5454545454545454, 28: 0.08695652173913043, 29: 0.6590038314176245, 32: 0.5027027027027027, 35: 0.24404761904761904, 37: 0.11983471074380166, 38: 0.16161616161616163, 39: 0.16393442622950818}
Micro-average F1 score: 0.3082248228119334
Weighted-average F1 score: 0.2871812625955703
F1 score per class: {0: 0.4925373134328358, 2: 0.1917808219178082, 4: 0.7789473684210526, 5: 0.4288939051918736, 6: 0.27486437613019893, 10: 0.27230046948356806, 11: 0.171875, 12: 0.18141592920353983, 13: 0.022857142857142857, 15: 0.17391304347826086, 16: 0.3853211009174312, 17: 0.0, 18: 0.037037037037037035, 19: 0.5696594427244582, 21: 0.11307420494699646, 23: 0.42857142857142855, 24: 0.15730337078651685, 25: 0.5098039215686274, 26: 0.5679012345679012, 28: 0.10714285714285714, 29: 0.6561264822134387, 32: 0.45036319612590797, 35: 0.23868312757201646, 37: 0.10600706713780919, 38: 0.15671641791044777, 39: 0.125}
Micro-average F1 score: 0.3146536721197355
Weighted-average F1 score: 0.29257340998783693
cur_acc_wo_na:  ['0.7695', '0.5249', '0.5021', '0.5933', '0.3786']
his_acc_wo_na:  ['0.7695', '0.6866', '0.5585', '0.5613', '0.4612']
cur_acc des_wo_na:  ['0.7457', '0.5537', '0.4568', '0.5433', '0.4791']
his_acc des_wo_na:  ['0.7457', '0.6506', '0.5474', '0.5461', '0.4612']
cur_acc rrf_wo_na:  ['0.7507', '0.5632', '0.4690', '0.5576', '0.4471']
his_acc rrf_wo_na:  ['0.7507', '0.6614', '0.5494', '0.5476', '0.4679']
cur_acc_w_na:  ['0.6216', '0.4088', '0.3477', '0.4308', '0.2729']
his_acc_w_na:  ['0.6216', '0.5363', '0.3843', '0.3966', '0.3191']
cur_acc des_w_na:  ['0.6063', '0.3924', '0.3101', '0.3896', '0.3252']
his_acc des_w_na:  ['0.6063', '0.4843', '0.3659', '0.3717', '0.3082']
cur_acc rrf_w_na:  ['0.6094', '0.4036', '0.3234', '0.4021', '0.3077']
his_acc rrf_w_na:  ['0.6094', '0.4968', '0.3710', '0.3774', '0.3147']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 97.3753157CurrentTrain: epoch  0, batch     1 | loss: 97.1455529CurrentTrain: epoch  0, batch     2 | loss: 75.0671454CurrentTrain: epoch  0, batch     3 | loss: 56.9026510CurrentTrain: epoch  1, batch     0 | loss: 70.3790891CurrentTrain: epoch  1, batch     1 | loss: 71.5888883CurrentTrain: epoch  1, batch     2 | loss: 82.4494021CurrentTrain: epoch  1, batch     3 | loss: 51.6753755CurrentTrain: epoch  2, batch     0 | loss: 66.9762427CurrentTrain: epoch  2, batch     1 | loss: 98.0542110CurrentTrain: epoch  2, batch     2 | loss: 81.6268975CurrentTrain: epoch  2, batch     3 | loss: 46.7249179CurrentTrain: epoch  3, batch     0 | loss: 65.5642120CurrentTrain: epoch  3, batch     1 | loss: 94.5941848CurrentTrain: epoch  3, batch     2 | loss: 76.3146206CurrentTrain: epoch  3, batch     3 | loss: 44.5881761CurrentTrain: epoch  4, batch     0 | loss: 63.3571624CurrentTrain: epoch  4, batch     1 | loss: 76.1324084CurrentTrain: epoch  4, batch     2 | loss: 63.4598075CurrentTrain: epoch  4, batch     3 | loss: 98.3171180CurrentTrain: epoch  5, batch     0 | loss: 85.2699276CurrentTrain: epoch  5, batch     1 | loss: 74.0695946CurrentTrain: epoch  5, batch     2 | loss: 92.2796799CurrentTrain: epoch  5, batch     3 | loss: 44.4577836CurrentTrain: epoch  6, batch     0 | loss: 73.1187686CurrentTrain: epoch  6, batch     1 | loss: 86.9087375CurrentTrain: epoch  6, batch     2 | loss: 62.0985956CurrentTrain: epoch  6, batch     3 | loss: 48.8696238CurrentTrain: epoch  7, batch     0 | loss: 71.8920895CurrentTrain: epoch  7, batch     1 | loss: 74.4819004CurrentTrain: epoch  7, batch     2 | loss: 56.3348127CurrentTrain: epoch  7, batch     3 | loss: 54.2207765CurrentTrain: epoch  8, batch     0 | loss: 85.2549507CurrentTrain: epoch  8, batch     1 | loss: 84.0927191CurrentTrain: epoch  8, batch     2 | loss: 82.1893420CurrentTrain: epoch  8, batch     3 | loss: 42.3164497CurrentTrain: epoch  9, batch     0 | loss: 68.9821841CurrentTrain: epoch  9, batch     1 | loss: 88.8598081CurrentTrain: epoch  9, batch     2 | loss: 85.0328473CurrentTrain: epoch  9, batch     3 | loss: 31.7729548
MemoryTrain:  epoch  0, batch     0 | loss: 0.7332886MemoryTrain:  epoch  1, batch     0 | loss: 0.7167748MemoryTrain:  epoch  2, batch     0 | loss: 0.6401945MemoryTrain:  epoch  3, batch     0 | loss: 0.4431742MemoryTrain:  epoch  4, batch     0 | loss: 0.3889033MemoryTrain:  epoch  5, batch     0 | loss: 0.3097299MemoryTrain:  epoch  6, batch     0 | loss: 0.2751817MemoryTrain:  epoch  7, batch     0 | loss: 0.2264930MemoryTrain:  epoch  8, batch     0 | loss: 0.2122709MemoryTrain:  epoch  9, batch     0 | loss: 0.1889345

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.6466165413533834, 11: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 20: 0.8363636363636363, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8823529411764706, 32: 0.0, 33: 0.4, 35: 0.0, 36: 0.5490196078431373, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.5162523900573613
Weighted-average F1 score: 0.39404254721486154
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.6792452830188679, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.8, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8571428571428571, 32: 0.0, 33: 0.35294117647058826, 35: 0.0, 36: 0.7310344827586207, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.5214723926380368
Weighted-average F1 score: 0.4117928479489596
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.6625, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.7924528301886793, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9230769230769231, 32: 0.0, 33: 0.4, 35: 0.0, 36: 0.656, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.5122349102773246
Weighted-average F1 score: 0.39871881055327174

F1 score per class: {0: 0.7619047619047619, 2: 0.3684210526315789, 4: 0.8117647058823529, 5: 0.7450980392156863, 6: 0.49469964664310956, 8: 0.38738738738738737, 10: 0.1694915254237288, 11: 0.2033898305084746, 12: 0.28, 13: 0.07142857142857142, 15: 0.27906976744186046, 16: 0.5172413793103449, 17: 0.0, 18: 0.0, 19: 0.7489711934156379, 20: 0.3665338645418327, 21: 0.10126582278481013, 23: 0.5822784810126582, 24: 0.24561403508771928, 25: 0.38235294117647056, 26: 0.6600985221674877, 28: 0.3333333333333333, 29: 0.8324873096446701, 30: 0.8823529411764706, 32: 0.5830721003134797, 33: 0.25, 35: 0.29906542056074764, 36: 0.28865979381443296, 37: 0.15447154471544716, 38: 0.24561403508771928, 39: 0.07142857142857142}
Micro-average F1 score: 0.4638646847770374
Weighted-average F1 score: 0.46032649095419614
F1 score per class: {0: 0.56, 2: 0.19444444444444445, 4: 0.8241758241758241, 5: 0.5025641025641026, 6: 0.5111821086261981, 8: 0.2975206611570248, 10: 0.3181818181818182, 11: 0.28125, 12: 0.2916666666666667, 13: 0.08695652173913043, 15: 0.26666666666666666, 16: 0.5070422535211268, 17: 0.0, 18: 0.22580645161290322, 19: 0.6692015209125475, 20: 0.4263959390862944, 21: 0.14906832298136646, 23: 0.5434782608695652, 24: 0.2857142857142857, 25: 0.5517241379310345, 26: 0.6261682242990654, 28: 0.15384615384615385, 29: 0.8151658767772512, 30: 0.5373134328358209, 32: 0.6388888888888888, 33: 0.15384615384615385, 35: 0.39215686274509803, 36: 0.343042071197411, 37: 0.15819209039548024, 38: 0.29333333333333333, 39: 0.13114754098360656}
Micro-average F1 score: 0.43743641912512715
Weighted-average F1 score: 0.4215253985447167
F1 score per class: {0: 0.6666666666666666, 2: 0.28, 4: 0.8409090909090909, 5: 0.6312292358803987, 6: 0.5142857142857142, 8: 0.30994152046783624, 10: 0.2804878048780488, 11: 0.23923444976076555, 12: 0.3111111111111111, 13: 0.0784313725490196, 15: 0.24489795918367346, 16: 0.5373134328358209, 17: 0.0, 18: 0.0851063829787234, 19: 0.6716417910447762, 20: 0.4019138755980861, 21: 0.15873015873015872, 23: 0.5647058823529412, 24: 0.29333333333333333, 25: 0.5569620253164557, 26: 0.638095238095238, 28: 0.16, 29: 0.8252427184466019, 30: 0.8, 32: 0.6052631578947368, 33: 0.1875, 35: 0.37267080745341613, 36: 0.31297709923664124, 37: 0.12903225806451613, 38: 0.3137254901960784, 39: 0.046511627906976744}
Micro-average F1 score: 0.44858583643937733
Weighted-average F1 score: 0.43367734081915515

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5733333333333334, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.5168539325842697, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8333333333333334, 32: 0.0, 33: 0.4, 35: 0.0, 36: 0.4028776978417266, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.33877038895859474
Weighted-average F1 score: 0.2629445693455162
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5346534653465347, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5957446808510638, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.75, 32: 0.0, 33: 0.2857142857142857, 35: 0.0, 36: 0.5380710659898477, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.3281853281853282
Weighted-average F1 score: 0.26287864477364736
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5353535353535354, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5675675675675675, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8372093023255814, 32: 0.0, 33: 0.4, 35: 0.0, 36: 0.4685714285714286, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.3257261410788382
Weighted-average F1 score: 0.25751026460820553

F1 score per class: {0: 0.5925925925925926, 2: 0.23333333333333334, 4: 0.770949720670391, 5: 0.5919003115264797, 6: 0.2845528455284553, 8: 0.24089635854341737, 10: 0.15037593984962405, 11: 0.15450643776824036, 12: 0.15555555555555556, 13: 0.042105263157894736, 15: 0.18461538461538463, 16: 0.3614457831325301, 17: 0.0, 18: 0.0, 19: 0.6453900709219859, 20: 0.16546762589928057, 21: 0.08247422680412371, 23: 0.5348837209302325, 24: 0.12280701754385964, 25: 0.3611111111111111, 26: 0.5606694560669456, 28: 0.19047619047619047, 29: 0.6748971193415638, 30: 0.8333333333333334, 32: 0.40789473684210525, 33: 0.1935483870967742, 35: 0.19161676646706588, 36: 0.19112627986348124, 37: 0.09669211195928754, 38: 0.14432989690721648, 39: 0.044444444444444446}
Micro-average F1 score: 0.318549806406195
Weighted-average F1 score: 0.30017140420369587
F1 score per class: {0: 0.4093567251461988, 2: 0.1308411214953271, 4: 0.7731958762886598, 5: 0.3141025641025641, 6: 0.29250457038391225, 8: 0.16314199395770393, 10: 0.23628691983122363, 11: 0.225, 12: 0.16129032258064516, 13: 0.05263157894736842, 15: 0.17142857142857143, 16: 0.32142857142857145, 17: 0.0, 18: 0.16091954022988506, 19: 0.56957928802589, 20: 0.21761658031088082, 21: 0.100418410041841, 23: 0.43478260869565216, 24: 0.16176470588235295, 25: 0.5106382978723404, 26: 0.5234375, 28: 0.0784313725490196, 29: 0.6564885496183206, 30: 0.42857142857142855, 32: 0.46582278481012657, 33: 0.09090909090909091, 35: 0.2247191011235955, 36: 0.23043478260869565, 37: 0.08383233532934131, 38: 0.1732283464566929, 39: 0.06557377049180328}
Micro-average F1 score: 0.2864756828780813
Weighted-average F1 score: 0.26914632172411895
F1 score per class: {0: 0.4857142857142857, 2: 0.18421052631578946, 4: 0.7914438502673797, 5: 0.43478260869565216, 6: 0.29508196721311475, 8: 0.1737704918032787, 10: 0.2100456621004566, 11: 0.18587360594795538, 12: 0.16706443914081145, 13: 0.04597701149425287, 15: 0.16216216216216217, 16: 0.3333333333333333, 17: 0.0, 18: 0.05970149253731343, 19: 0.5642633228840125, 20: 0.19811320754716982, 21: 0.11049723756906077, 23: 0.46601941747572817, 24: 0.15384615384615385, 25: 0.5176470588235295, 26: 0.5317460317460317, 28: 0.08163265306122448, 29: 0.6719367588932806, 30: 0.6666666666666666, 32: 0.42990654205607476, 33: 0.11320754716981132, 35: 0.24, 36: 0.20918367346938777, 37: 0.07329842931937172, 38: 0.17486338797814208, 39: 0.024691358024691357}
Micro-average F1 score: 0.29669373549883993
Weighted-average F1 score: 0.2783954498784338
cur_acc_wo_na:  ['0.7695', '0.5249', '0.5021', '0.5933', '0.3786', '0.5163']
his_acc_wo_na:  ['0.7695', '0.6866', '0.5585', '0.5613', '0.4612', '0.4639']
cur_acc des_wo_na:  ['0.7457', '0.5537', '0.4568', '0.5433', '0.4791', '0.5215']
his_acc des_wo_na:  ['0.7457', '0.6506', '0.5474', '0.5461', '0.4612', '0.4374']
cur_acc rrf_wo_na:  ['0.7507', '0.5632', '0.4690', '0.5576', '0.4471', '0.5122']
his_acc rrf_wo_na:  ['0.7507', '0.6614', '0.5494', '0.5476', '0.4679', '0.4486']
cur_acc_w_na:  ['0.6216', '0.4088', '0.3477', '0.4308', '0.2729', '0.3388']
his_acc_w_na:  ['0.6216', '0.5363', '0.3843', '0.3966', '0.3191', '0.3185']
cur_acc des_w_na:  ['0.6063', '0.3924', '0.3101', '0.3896', '0.3252', '0.3282']
his_acc des_w_na:  ['0.6063', '0.4843', '0.3659', '0.3717', '0.3082', '0.2865']
cur_acc rrf_w_na:  ['0.6094', '0.4036', '0.3234', '0.4021', '0.3077', '0.3257']
his_acc rrf_w_na:  ['0.6094', '0.4968', '0.3710', '0.3774', '0.3147', '0.2967']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 101.5578478CurrentTrain: epoch  0, batch     1 | loss: 82.9330524CurrentTrain: epoch  0, batch     2 | loss: 91.6916025CurrentTrain: epoch  0, batch     3 | loss: 11.9466230CurrentTrain: epoch  1, batch     0 | loss: 69.1902989CurrentTrain: epoch  1, batch     1 | loss: 73.8195841CurrentTrain: epoch  1, batch     2 | loss: 107.6252057CurrentTrain: epoch  1, batch     3 | loss: 23.0378850CurrentTrain: epoch  2, batch     0 | loss: 68.5242398CurrentTrain: epoch  2, batch     1 | loss: 77.6287763CurrentTrain: epoch  2, batch     2 | loss: 68.1904024CurrentTrain: epoch  2, batch     3 | loss: 17.7557861CurrentTrain: epoch  3, batch     0 | loss: 76.9950859CurrentTrain: epoch  3, batch     1 | loss: 75.5065565CurrentTrain: epoch  3, batch     2 | loss: 91.5046799CurrentTrain: epoch  3, batch     3 | loss: 21.0008449CurrentTrain: epoch  4, batch     0 | loss: 72.1232876CurrentTrain: epoch  4, batch     1 | loss: 66.9691055CurrentTrain: epoch  4, batch     2 | loss: 62.0447873CurrentTrain: epoch  4, batch     3 | loss: 4.5024658CurrentTrain: epoch  5, batch     0 | loss: 62.3640014CurrentTrain: epoch  5, batch     1 | loss: 70.4443830CurrentTrain: epoch  5, batch     2 | loss: 71.9287922CurrentTrain: epoch  5, batch     3 | loss: 16.2439668CurrentTrain: epoch  6, batch     0 | loss: 62.3350765CurrentTrain: epoch  6, batch     1 | loss: 58.0282749CurrentTrain: epoch  6, batch     2 | loss: 74.4656306CurrentTrain: epoch  6, batch     3 | loss: 3.4266168CurrentTrain: epoch  7, batch     0 | loss: 70.2974927CurrentTrain: epoch  7, batch     1 | loss: 73.2725425CurrentTrain: epoch  7, batch     2 | loss: 57.5112945CurrentTrain: epoch  7, batch     3 | loss: 14.4773294CurrentTrain: epoch  8, batch     0 | loss: 55.7671636CurrentTrain: epoch  8, batch     1 | loss: 85.4120449CurrentTrain: epoch  8, batch     2 | loss: 70.7832433CurrentTrain: epoch  8, batch     3 | loss: 4.6021631CurrentTrain: epoch  9, batch     0 | loss: 57.1126179CurrentTrain: epoch  9, batch     1 | loss: 59.5379255CurrentTrain: epoch  9, batch     2 | loss: 57.1093175CurrentTrain: epoch  9, batch     3 | loss: 5.1054766
MemoryTrain:  epoch  0, batch     0 | loss: 0.8243991MemoryTrain:  epoch  1, batch     0 | loss: 0.6241031MemoryTrain:  epoch  2, batch     0 | loss: 0.5187205MemoryTrain:  epoch  3, batch     0 | loss: 0.4661242MemoryTrain:  epoch  4, batch     0 | loss: 0.3550091MemoryTrain:  epoch  5, batch     0 | loss: 0.3148639MemoryTrain:  epoch  6, batch     0 | loss: 0.2689285MemoryTrain:  epoch  7, batch     0 | loss: 0.2188181MemoryTrain:  epoch  8, batch     0 | loss: 0.1990147MemoryTrain:  epoch  9, batch     0 | loss: 0.1636898

F1 score per class: {32: 0.0, 0: 0.0, 35: 0.75, 37: 0.8620689655172413, 6: 0.0, 7: 0.0, 40: 0.0, 9: 0.0, 13: 0.0, 16: 0.0, 19: 0.5333333333333333, 21: 0.6666666666666666, 24: 0.0, 26: 0.0, 27: 0.0, 31: 0.14516129032258066}
Micro-average F1 score: 0.2939297124600639
Weighted-average F1 score: 0.2396913190349225
F1 score per class: {0: 0.0, 6: 0.0, 7: 0.6666666666666666, 9: 0.684931506849315, 10: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.5185185185185185, 29: 0.0, 31: 0.6666666666666666, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.39705882352941174}
Micro-average F1 score: 0.3652173913043478
Weighted-average F1 score: 0.3020968546082701
F1 score per class: {0: 0.0, 6: 0.0, 7: 0.75, 9: 0.819672131147541, 10: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.5185185185185185, 31: 0.6666666666666666, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.38848920863309355}
Micro-average F1 score: 0.3795180722891566
Weighted-average F1 score: 0.30480359783963074

F1 score per class: {0: 0.6176470588235294, 2: 0.3783783783783784, 4: 0.7577639751552795, 5: 0.7307692307692307, 6: 0.34523809523809523, 7: 0.04, 8: 0.1981981981981982, 9: 0.8620689655172413, 10: 0.05714285714285714, 11: 0.22485207100591717, 12: 0.23529411764705882, 13: 0.06557377049180328, 15: 0.35294117647058826, 16: 0.5, 17: 0.0, 18: 0.0, 19: 0.5827338129496403, 20: 0.3424124513618677, 21: 0.06060606060606061, 23: 0.4864864864864865, 24: 0.08, 25: 0.3880597014925373, 26: 0.67, 27: 0.27586206896551724, 28: 0.23529411764705882, 29: 0.8383838383838383, 30: 0.8484848484848485, 31: 0.6666666666666666, 32: 0.6304347826086957, 33: 0.2, 35: 0.2702702702702703, 36: 0.4424778761061947, 37: 0.18983050847457628, 38: 0.21739130434782608, 39: 0.0, 40: 0.06206896551724138}
Micro-average F1 score: 0.4012770137524558
Weighted-average F1 score: 0.38857066603846857
F1 score per class: {0: 0.5486725663716814, 2: 0.14893617021276595, 4: 0.7425149700598802, 5: 0.4743276283618582, 6: 0.34782608695652173, 7: 0.04285714285714286, 8: 0.3487179487179487, 9: 0.5747126436781609, 10: 0.24806201550387597, 11: 0.2898550724637681, 12: 0.22857142857142856, 13: 0.05, 15: 0.26666666666666666, 16: 0.5384615384615384, 17: 0.0, 18: 0.13432835820895522, 19: 0.5531914893617021, 20: 0.41, 21: 0.18604651162790697, 23: 0.5057471264367817, 24: 0.24, 25: 0.5185185185185185, 26: 0.6210045662100456, 27: 0.27450980392156865, 28: 0.13793103448275862, 29: 0.8275862068965517, 30: 0.8717948717948718, 31: 0.25, 32: 0.6615384615384615, 33: 0.1276595744680851, 35: 0.496551724137931, 36: 0.5434782608695652, 37: 0.16260162601626016, 38: 0.3516483516483517, 39: 0.09523809523809523, 40: 0.18815331010452963}
Micro-average F1 score: 0.404727939678011
Weighted-average F1 score: 0.3836064133846309
F1 score per class: {0: 0.6585365853658537, 2: 0.23728813559322035, 4: 0.7757575757575758, 5: 0.5555555555555556, 6: 0.3315508021390374, 7: 0.041379310344827586, 8: 0.38461538461538464, 9: 0.7936507936507936, 10: 0.2204724409448819, 11: 0.2336448598130841, 12: 0.22085889570552147, 13: 0.05194805194805195, 15: 0.2608695652173913, 16: 0.56, 17: 0.0, 18: 0.10714285714285714, 19: 0.5842696629213483, 20: 0.3778801843317972, 21: 0.17475728155339806, 23: 0.5, 24: 0.2127659574468085, 25: 0.5384615384615384, 26: 0.6538461538461539, 27: 0.2545454545454545, 28: 0.13333333333333333, 29: 0.835820895522388, 30: 0.8421052631578947, 31: 0.4, 32: 0.6468401486988847, 33: 0.13333333333333333, 35: 0.40336134453781514, 36: 0.43956043956043955, 37: 0.1360544217687075, 38: 0.3188405797101449, 39: 0.0, 40: 0.17088607594936708}
Micro-average F1 score: 0.4059040590405904
Weighted-average F1 score: 0.38285838682398415

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.5454545454545454, 9: 0.819672131147541, 10: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.47058823529411764, 31: 0.6666666666666666, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.11464968152866242}
Micro-average F1 score: 0.2340966921119593
Weighted-average F1 score: 0.18893940188811936
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.5454545454545454, 8: 0.0, 9: 0.6024096385542169, 10: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.4117647058823529, 29: 0.0, 31: 0.5, 32: 0.0, 33: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.3176470588235294}
Micro-average F1 score: 0.27692307692307694
Weighted-average F1 score: 0.23359863781186818
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.5454545454545454, 9: 0.7692307692307693, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.4117647058823529, 31: 0.5, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.31213872832369943}
Micro-average F1 score: 0.2896551724137931
Weighted-average F1 score: 0.2358079519322278

F1 score per class: {0: 0.5060240963855421, 2: 0.22580645161290322, 4: 0.7218934911242604, 5: 0.5352112676056338, 6: 0.22568093385214008, 7: 0.02127659574468085, 8: 0.16541353383458646, 9: 0.819672131147541, 10: 0.05454545454545454, 11: 0.18181818181818182, 12: 0.13090909090909092, 13: 0.038834951456310676, 15: 0.2553191489361702, 16: 0.35555555555555557, 17: 0.0, 18: 0.0, 19: 0.5346534653465347, 20: 0.14965986394557823, 21: 0.05333333333333334, 23: 0.4444444444444444, 24: 0.045454545454545456, 25: 0.36619718309859156, 26: 0.5775862068965517, 27: 0.21333333333333335, 28: 0.13333333333333333, 29: 0.6887966804979253, 30: 0.8, 31: 0.3333333333333333, 32: 0.46900269541778977, 33: 0.14285714285714285, 35: 0.21739130434782608, 36: 0.2824858757062147, 37: 0.11618257261410789, 38: 0.15625, 39: 0.0, 40: 0.04411764705882353}
Micro-average F1 score: 0.2838283828382838
Weighted-average F1 score: 0.2612737984110927
F1 score per class: {0: 0.4460431654676259, 2: 0.10294117647058823, 4: 0.6966292134831461, 5: 0.2952815829528158, 6: 0.2191780821917808, 7: 0.022900763358778626, 8: 0.24727272727272728, 9: 0.47619047619047616, 10: 0.2077922077922078, 11: 0.242914979757085, 12: 0.13745704467353953, 13: 0.028985507246376812, 15: 0.1935483870967742, 16: 0.34710743801652894, 17: 0.0, 18: 0.0962566844919786, 19: 0.5032258064516129, 20: 0.19523809523809524, 21: 0.13043478260869565, 23: 0.4, 24: 0.13636363636363635, 25: 0.4883720930232558, 26: 0.5230769230769231, 27: 0.18181818181818182, 28: 0.07547169811320754, 29: 0.6693227091633466, 30: 0.7906976744186046, 31: 0.125, 32: 0.5014577259475219, 33: 0.07228915662650602, 35: 0.3412322274881517, 36: 0.3472222222222222, 37: 0.09501187648456057, 38: 0.1871345029239766, 39: 0.058823529411764705, 40: 0.13267813267813267}
Micro-average F1 score: 0.27912860154602953
Weighted-average F1 score: 0.2589756180589506
F1 score per class: {0: 0.5242718446601942, 2: 0.16470588235294117, 4: 0.7314285714285714, 5: 0.3538175046554935, 6: 0.21232876712328766, 7: 0.02181818181818182, 8: 0.28708133971291866, 9: 0.746268656716418, 10: 0.18791946308724833, 11: 0.19305019305019305, 12: 0.12631578947368421, 13: 0.030303030303030304, 15: 0.19047619047619047, 16: 0.358974358974359, 17: 0.0, 18: 0.06666666666666667, 19: 0.5360824742268041, 20: 0.1786492374727669, 21: 0.1276595744680851, 23: 0.40404040404040403, 24: 0.1111111111111111, 25: 0.5060240963855421, 26: 0.5596707818930041, 27: 0.16666666666666666, 28: 0.07272727272727272, 29: 0.680161943319838, 30: 0.7804878048780488, 31: 0.18181818181818182, 32: 0.48739495798319327, 33: 0.08695652173913043, 35: 0.2823529411764706, 36: 0.2857142857142857, 37: 0.08281573498964803, 38: 0.16923076923076924, 39: 0.0, 40: 0.12162162162162163}
Micro-average F1 score: 0.28107620622275664
Weighted-average F1 score: 0.25759559291404455
cur_acc_wo_na:  ['0.7695', '0.5249', '0.5021', '0.5933', '0.3786', '0.5163', '0.2939']
his_acc_wo_na:  ['0.7695', '0.6866', '0.5585', '0.5613', '0.4612', '0.4639', '0.4013']
cur_acc des_wo_na:  ['0.7457', '0.5537', '0.4568', '0.5433', '0.4791', '0.5215', '0.3652']
his_acc des_wo_na:  ['0.7457', '0.6506', '0.5474', '0.5461', '0.4612', '0.4374', '0.4047']
cur_acc rrf_wo_na:  ['0.7507', '0.5632', '0.4690', '0.5576', '0.4471', '0.5122', '0.3795']
his_acc rrf_wo_na:  ['0.7507', '0.6614', '0.5494', '0.5476', '0.4679', '0.4486', '0.4059']
cur_acc_w_na:  ['0.6216', '0.4088', '0.3477', '0.4308', '0.2729', '0.3388', '0.2341']
his_acc_w_na:  ['0.6216', '0.5363', '0.3843', '0.3966', '0.3191', '0.3185', '0.2838']
cur_acc des_w_na:  ['0.6063', '0.3924', '0.3101', '0.3896', '0.3252', '0.3282', '0.2769']
his_acc des_w_na:  ['0.6063', '0.4843', '0.3659', '0.3717', '0.3082', '0.2865', '0.2791']
cur_acc rrf_w_na:  ['0.6094', '0.4036', '0.3234', '0.4021', '0.3077', '0.3257', '0.2897']
his_acc rrf_w_na:  ['0.6094', '0.4968', '0.3710', '0.3774', '0.3147', '0.2967', '0.2811']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 95.8387225CurrentTrain: epoch  0, batch     1 | loss: 98.0058900CurrentTrain: epoch  0, batch     2 | loss: 85.7206315CurrentTrain: epoch  0, batch     3 | loss: 111.4417896CurrentTrain: epoch  0, batch     4 | loss: 60.8013174CurrentTrain: epoch  1, batch     0 | loss: 89.8828870CurrentTrain: epoch  1, batch     1 | loss: 82.9113595CurrentTrain: epoch  1, batch     2 | loss: 72.1803801CurrentTrain: epoch  1, batch     3 | loss: 128.4185569CurrentTrain: epoch  1, batch     4 | loss: 79.8442940CurrentTrain: epoch  2, batch     0 | loss: 84.6586891CurrentTrain: epoch  2, batch     1 | loss: 98.9417173CurrentTrain: epoch  2, batch     2 | loss: 70.0011221CurrentTrain: epoch  2, batch     3 | loss: 94.5870366CurrentTrain: epoch  2, batch     4 | loss: 46.9856507CurrentTrain: epoch  3, batch     0 | loss: 101.4620750CurrentTrain: epoch  3, batch     1 | loss: 80.1375989CurrentTrain: epoch  3, batch     2 | loss: 67.7594156CurrentTrain: epoch  3, batch     3 | loss: 64.3936377CurrentTrain: epoch  3, batch     4 | loss: 65.2168001CurrentTrain: epoch  4, batch     0 | loss: 64.7814745CurrentTrain: epoch  4, batch     1 | loss: 92.5673478CurrentTrain: epoch  4, batch     2 | loss: 97.8346226CurrentTrain: epoch  4, batch     3 | loss: 117.4501428CurrentTrain: epoch  4, batch     4 | loss: 36.0234740CurrentTrain: epoch  5, batch     0 | loss: 75.0960002CurrentTrain: epoch  5, batch     1 | loss: 93.6532725CurrentTrain: epoch  5, batch     2 | loss: 75.0036137CurrentTrain: epoch  5, batch     3 | loss: 75.9726767CurrentTrain: epoch  5, batch     4 | loss: 50.7839940CurrentTrain: epoch  6, batch     0 | loss: 76.4729048CurrentTrain: epoch  6, batch     1 | loss: 63.8619122CurrentTrain: epoch  6, batch     2 | loss: 64.3049477CurrentTrain: epoch  6, batch     3 | loss: 73.3891192CurrentTrain: epoch  6, batch     4 | loss: 87.0870869CurrentTrain: epoch  7, batch     0 | loss: 71.2464844CurrentTrain: epoch  7, batch     1 | loss: 74.2460537CurrentTrain: epoch  7, batch     2 | loss: 86.5366465CurrentTrain: epoch  7, batch     3 | loss: 86.1264939CurrentTrain: epoch  7, batch     4 | loss: 85.0953047CurrentTrain: epoch  8, batch     0 | loss: 90.1394099CurrentTrain: epoch  8, batch     1 | loss: 87.6428390CurrentTrain: epoch  8, batch     2 | loss: 112.2192822CurrentTrain: epoch  8, batch     3 | loss: 83.3691468CurrentTrain: epoch  8, batch     4 | loss: 39.1511939CurrentTrain: epoch  9, batch     0 | loss: 87.1133294CurrentTrain: epoch  9, batch     1 | loss: 71.2021434CurrentTrain: epoch  9, batch     2 | loss: 60.5150818CurrentTrain: epoch  9, batch     3 | loss: 87.5676918CurrentTrain: epoch  9, batch     4 | loss: 60.6081192
MemoryTrain:  epoch  0, batch     0 | loss: 1.0649252MemoryTrain:  epoch  1, batch     0 | loss: 0.9180748MemoryTrain:  epoch  2, batch     0 | loss: 0.7440843MemoryTrain:  epoch  3, batch     0 | loss: 0.5585967MemoryTrain:  epoch  4, batch     0 | loss: 0.4786678MemoryTrain:  epoch  5, batch     0 | loss: 0.4012639MemoryTrain:  epoch  6, batch     0 | loss: 0.3344149MemoryTrain:  epoch  7, batch     0 | loss: 0.3087046MemoryTrain:  epoch  8, batch     0 | loss: 0.2552259MemoryTrain:  epoch  9, batch     0 | loss: 0.2155342

F1 score per class: {1: 0.2289156626506024, 3: 0.7065868263473054, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.04819277108433735, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.5789473684210527, 23: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.6391752577319587, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3733862959285005
Weighted-average F1 score: 0.3192022037022629
F1 score per class: {0: 0.0, 1: 0.2345679012345679, 2: 0.0, 3: 0.6857142857142857, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.05063291139240506, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5521885521885522, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.6713286713286714, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3549201009251472
Weighted-average F1 score: 0.3075286858969815
F1 score per class: {0: 0.0, 1: 0.19753086419753085, 3: 0.6777777777777778, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.045454545454545456, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.5514950166112956, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.6666666666666666, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3542757417102967
Weighted-average F1 score: 0.3077167049209873

F1 score per class: {0: 0.5263157894736842, 1: 0.17272727272727273, 2: 0.42424242424242425, 3: 0.44866920152091255, 4: 0.7530864197530864, 5: 0.7863247863247863, 6: 0.34444444444444444, 7: 0.030534351145038167, 8: 0.3448275862068966, 9: 0.8888888888888888, 10: 0.12844036697247707, 11: 0.18543046357615894, 12: 0.26582278481012656, 13: 0.056338028169014086, 14: 0.027777777777777776, 15: 0.46153846153846156, 16: 0.5483870967741935, 17: 0.0, 18: 0.0, 19: 0.5486111111111112, 20: 0.40404040404040403, 21: 0.0, 22: 0.45427728613569324, 23: 0.5569620253164557, 24: 0.0, 25: 0.3125, 26: 0.6504854368932039, 27: 0.16, 28: 0.26666666666666666, 29: 0.8118811881188119, 30: 0.8484848484848485, 31: 0.4, 32: 0.5109034267912772, 33: 0.20689655172413793, 34: 0.16756756756756758, 35: 0.0, 36: 0.48333333333333334, 37: 0.13095238095238096, 38: 0.17857142857142858, 39: 0.0, 40: 0.12631578947368421}
Micro-average F1 score: 0.3767163024560046
Weighted-average F1 score: 0.3653943223994636
F1 score per class: {0: 0.5084745762711864, 1: 0.168141592920354, 2: 0.175, 3: 0.3973509933774834, 4: 0.7329192546583851, 5: 0.5159574468085106, 6: 0.328042328042328, 7: 0.04, 8: 0.32, 9: 0.43103448275862066, 10: 0.10619469026548672, 11: 0.17054263565891473, 12: 0.26666666666666666, 13: 0.07407407407407407, 14: 0.032, 15: 0.3, 16: 0.5079365079365079, 17: 0.0, 18: 0.0, 19: 0.4873417721518987, 20: 0.430939226519337, 21: 0.03636363636363636, 22: 0.4304461942257218, 23: 0.5617977528089888, 24: 0.11764705882352941, 25: 0.5063291139240507, 26: 0.6188340807174888, 27: 0.15384615384615385, 28: 0.06896551724137931, 29: 0.8115942028985508, 30: 0.8292682926829268, 31: 0.125, 32: 0.5, 33: 0.21052631578947367, 34: 0.15311004784688995, 35: 0.18487394957983194, 36: 0.5176470588235295, 37: 0.04918032786885246, 38: 0.21505376344086022, 39: 0.11764705882352941, 40: 0.19933554817275748}
Micro-average F1 score: 0.34945547725816783
Weighted-average F1 score: 0.33784382891891773
F1 score per class: {0: 0.6, 1: 0.14349775784753363, 2: 0.2978723404255319, 3: 0.38125, 4: 0.7607361963190185, 5: 0.6713286713286714, 6: 0.3315508021390374, 7: 0.040268456375838924, 8: 0.3253012048192771, 9: 0.8333333333333334, 10: 0.11965811965811966, 11: 0.15384615384615385, 12: 0.26595744680851063, 13: 0.08333333333333333, 14: 0.02631578947368421, 15: 0.3076923076923077, 16: 0.5084745762711864, 17: 0.0, 18: 0.0, 19: 0.5150501672240803, 20: 0.4148936170212766, 21: 0.06060606060606061, 22: 0.41604010025062654, 23: 0.5714285714285714, 24: 0.08333333333333333, 25: 0.5135135135135135, 26: 0.6301369863013698, 27: 0.15384615384615385, 28: 0.07017543859649122, 29: 0.8195121951219512, 30: 0.8888888888888888, 31: 0.2222222222222222, 32: 0.49707602339181284, 33: 0.16666666666666666, 34: 0.15384615384615385, 35: 0.10204081632653061, 36: 0.5135135135135135, 37: 0.10112359550561797, 38: 0.1728395061728395, 39: 0.1111111111111111, 40: 0.182370820668693}
Micro-average F1 score: 0.35794107687097865
Weighted-average F1 score: 0.341757568313052

F1 score per class: {0: 0.0, 1: 0.14232209737827714, 2: 0.0, 3: 0.5645933014354066, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.043010752688172046, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.4425287356321839, 23: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5344827586206896, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.25217974513749164
Weighted-average F1 score: 0.2152657535223253
F1 score per class: {0: 0.0, 1: 0.14559386973180077, 2: 0.0, 3: 0.5405405405405406, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.046511627906976744, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.42597402597402595, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.49230769230769234, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.23289183222958057
Weighted-average F1 score: 0.20334343009048209
F1 score per class: {0: 0.0, 1: 0.12030075187969924, 2: 0.0, 3: 0.49795918367346936, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.04081632653061224, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.4223918575063613, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5030674846625767, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2330654420206659
Weighted-average F1 score: 0.2054579015039978

F1 score per class: {0: 0.410958904109589, 1: 0.10555555555555556, 2: 0.24561403508771928, 3: 0.2964824120603015, 4: 0.7134502923976608, 5: 0.6153846153846154, 6: 0.21830985915492956, 7: 0.015151515151515152, 8: 0.2777777777777778, 9: 0.8727272727272727, 10: 0.1206896551724138, 11: 0.15555555555555556, 12: 0.13636363636363635, 13: 0.030534351145038167, 14: 0.022598870056497175, 15: 0.2926829268292683, 16: 0.3953488372093023, 17: 0.0, 18: 0.0, 19: 0.4891640866873065, 20: 0.18604651162790697, 21: 0.0, 22: 0.3117408906882591, 23: 0.4782608695652174, 24: 0.0, 25: 0.30303030303030304, 26: 0.5583333333333333, 27: 0.08695652173913043, 28: 0.14814814814814814, 29: 0.6533864541832669, 30: 0.8, 31: 0.2857142857142857, 32: 0.35964912280701755, 33: 0.14634146341463414, 34: 0.11524163568773234, 35: 0.0, 36: 0.29591836734693877, 37: 0.09649122807017543, 38: 0.10869565217391304, 39: 0.0, 40: 0.0855106888361045}
Micro-average F1 score: 0.2648538409245411
Weighted-average F1 score: 0.24845947102172314
F1 score per class: {0: 0.3821656050955414, 1: 0.09947643979057591, 2: 0.11570247933884298, 3: 0.25862068965517243, 4: 0.6941176470588235, 5: 0.32550335570469796, 6: 0.2052980132450331, 7: 0.020618556701030927, 8: 0.21333333333333335, 9: 0.33557046979865773, 10: 0.0975609756097561, 11: 0.16541353383458646, 12: 0.14016172506738545, 13: 0.037037037037037035, 14: 0.026490066225165563, 15: 0.20689655172413793, 16: 0.34782608695652173, 17: 0.0, 18: 0.0, 19: 0.4375, 20: 0.21666666666666667, 21: 0.03389830508474576, 22: 0.2954954954954955, 23: 0.4065040650406504, 24: 0.07547169811320754, 25: 0.47619047619047616, 26: 0.5130111524163569, 27: 0.058823529411764705, 28: 0.04040404040404041, 29: 0.6387832699619772, 30: 0.7083333333333334, 31: 0.045454545454545456, 32: 0.35365853658536583, 33: 0.1095890410958904, 34: 0.09421000981354269, 35: 0.12087912087912088, 36: 0.29931972789115646, 37: 0.041379310344827586, 38: 0.11904761904761904, 39: 0.06896551724137931, 40: 0.14184397163120568}
Micro-average F1 score: 0.23566259855275948
Weighted-average F1 score: 0.22345270547170198
F1 score per class: {0: 0.4444444444444444, 1: 0.08398950131233596, 2: 0.17721518987341772, 3: 0.23921568627450981, 4: 0.7251461988304093, 5: 0.4549763033175355, 6: 0.2108843537414966, 7: 0.020202020202020204, 8: 0.2317596566523605, 9: 0.7692307692307693, 10: 0.1076923076923077, 11: 0.14473684210526316, 12: 0.14084507042253522, 13: 0.038834951456310676, 14: 0.021621621621621623, 15: 0.21818181818181817, 16: 0.3448275862068966, 17: 0.0, 18: 0.0, 19: 0.4652567975830816, 20: 0.20103092783505155, 21: 0.06060606060606061, 22: 0.2842465753424658, 23: 0.46153846153846156, 24: 0.05555555555555555, 25: 0.4810126582278481, 26: 0.5369649805447471, 27: 0.06896551724137931, 28: 0.04395604395604396, 29: 0.6486486486486487, 30: 0.8205128205128205, 31: 0.07142857142857142, 32: 0.3512396694214876, 33: 0.09375, 34: 0.0944700460829493, 35: 0.07633587786259542, 36: 0.2846441947565543, 37: 0.07659574468085106, 38: 0.10071942446043165, 39: 0.06896551724137931, 40: 0.13129102844638948}
Micro-average F1 score: 0.24388555606829718
Weighted-average F1 score: 0.22773470652117653
cur_acc_wo_na:  ['0.7695', '0.5249', '0.5021', '0.5933', '0.3786', '0.5163', '0.2939', '0.3734']
his_acc_wo_na:  ['0.7695', '0.6866', '0.5585', '0.5613', '0.4612', '0.4639', '0.4013', '0.3767']
cur_acc des_wo_na:  ['0.7457', '0.5537', '0.4568', '0.5433', '0.4791', '0.5215', '0.3652', '0.3549']
his_acc des_wo_na:  ['0.7457', '0.6506', '0.5474', '0.5461', '0.4612', '0.4374', '0.4047', '0.3495']
cur_acc rrf_wo_na:  ['0.7507', '0.5632', '0.4690', '0.5576', '0.4471', '0.5122', '0.3795', '0.3543']
his_acc rrf_wo_na:  ['0.7507', '0.6614', '0.5494', '0.5476', '0.4679', '0.4486', '0.4059', '0.3579']
cur_acc_w_na:  ['0.6216', '0.4088', '0.3477', '0.4308', '0.2729', '0.3388', '0.2341', '0.2522']
his_acc_w_na:  ['0.6216', '0.5363', '0.3843', '0.3966', '0.3191', '0.3185', '0.2838', '0.2649']
cur_acc des_w_na:  ['0.6063', '0.3924', '0.3101', '0.3896', '0.3252', '0.3282', '0.2769', '0.2329']
his_acc des_w_na:  ['0.6063', '0.4843', '0.3659', '0.3717', '0.3082', '0.2865', '0.2791', '0.2357']
cur_acc rrf_w_na:  ['0.6094', '0.4036', '0.3234', '0.4021', '0.3077', '0.3257', '0.2897', '0.2331']
his_acc rrf_w_na:  ['0.6094', '0.4968', '0.3710', '0.3774', '0.3147', '0.2967', '0.2811', '0.2439']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 112.0480891CurrentTrain: epoch  0, batch     1 | loss: 200.7677598CurrentTrain: epoch  0, batch     2 | loss: 121.3165952CurrentTrain: epoch  0, batch     3 | loss: 102.0691042CurrentTrain: epoch  0, batch     4 | loss: 87.2663795CurrentTrain: epoch  0, batch     5 | loss: 89.3945195CurrentTrain: epoch  0, batch     6 | loss: 77.8921934CurrentTrain: epoch  0, batch     7 | loss: 119.5186976CurrentTrain: epoch  0, batch     8 | loss: 118.2010342CurrentTrain: epoch  0, batch     9 | loss: 119.1347646CurrentTrain: epoch  0, batch    10 | loss: 147.4691101CurrentTrain: epoch  0, batch    11 | loss: 118.8037557CurrentTrain: epoch  0, batch    12 | loss: 100.2970340CurrentTrain: epoch  0, batch    13 | loss: 146.9315063CurrentTrain: epoch  0, batch    14 | loss: 118.0255894CurrentTrain: epoch  0, batch    15 | loss: 87.0737050CurrentTrain: epoch  0, batch    16 | loss: 99.9167953CurrentTrain: epoch  0, batch    17 | loss: 86.5077197CurrentTrain: epoch  0, batch    18 | loss: 100.4491773CurrentTrain: epoch  0, batch    19 | loss: 118.1444482CurrentTrain: epoch  0, batch    20 | loss: 86.3318068CurrentTrain: epoch  0, batch    21 | loss: 145.3550469CurrentTrain: epoch  0, batch    22 | loss: 75.8467129CurrentTrain: epoch  0, batch    23 | loss: 99.1490936CurrentTrain: epoch  0, batch    24 | loss: 117.6625111CurrentTrain: epoch  0, batch    25 | loss: 98.9273379CurrentTrain: epoch  0, batch    26 | loss: 86.2894774CurrentTrain: epoch  0, batch    27 | loss: 85.8096880CurrentTrain: epoch  0, batch    28 | loss: 117.4038784CurrentTrain: epoch  0, batch    29 | loss: 98.4177707CurrentTrain: epoch  0, batch    30 | loss: 117.7969891CurrentTrain: epoch  0, batch    31 | loss: 75.5669330CurrentTrain: epoch  0, batch    32 | loss: 116.5088933CurrentTrain: epoch  0, batch    33 | loss: 117.7530292CurrentTrain: epoch  0, batch    34 | loss: 84.4224646CurrentTrain: epoch  0, batch    35 | loss: 144.3712378CurrentTrain: epoch  0, batch    36 | loss: 98.3562718CurrentTrain: epoch  0, batch    37 | loss: 97.9267832CurrentTrain: epoch  0, batch    38 | loss: 84.6770758CurrentTrain: epoch  0, batch    39 | loss: 97.0952600CurrentTrain: epoch  0, batch    40 | loss: 97.1665025CurrentTrain: epoch  0, batch    41 | loss: 96.5596842CurrentTrain: epoch  0, batch    42 | loss: 114.4506415CurrentTrain: epoch  0, batch    43 | loss: 143.5554191CurrentTrain: epoch  0, batch    44 | loss: 97.1826895CurrentTrain: epoch  0, batch    45 | loss: 115.9003940CurrentTrain: epoch  0, batch    46 | loss: 113.8928796CurrentTrain: epoch  0, batch    47 | loss: 93.3619595CurrentTrain: epoch  0, batch    48 | loss: 115.9316006CurrentTrain: epoch  0, batch    49 | loss: 94.4584666CurrentTrain: epoch  0, batch    50 | loss: 80.9146305CurrentTrain: epoch  0, batch    51 | loss: 95.5717017CurrentTrain: epoch  0, batch    52 | loss: 72.0249421CurrentTrain: epoch  0, batch    53 | loss: 83.7371062CurrentTrain: epoch  0, batch    54 | loss: 138.6179079CurrentTrain: epoch  0, batch    55 | loss: 82.7138342CurrentTrain: epoch  0, batch    56 | loss: 92.9898393CurrentTrain: epoch  0, batch    57 | loss: 113.2756818CurrentTrain: epoch  0, batch    58 | loss: 184.1640874CurrentTrain: epoch  0, batch    59 | loss: 140.0874917CurrentTrain: epoch  0, batch    60 | loss: 92.0614215CurrentTrain: epoch  0, batch    61 | loss: 91.0383293CurrentTrain: epoch  0, batch    62 | loss: 183.5515610CurrentTrain: epoch  0, batch    63 | loss: 110.2255505CurrentTrain: epoch  0, batch    64 | loss: 80.0869716CurrentTrain: epoch  0, batch    65 | loss: 78.4936331CurrentTrain: epoch  0, batch    66 | loss: 108.5299903CurrentTrain: epoch  0, batch    67 | loss: 108.8725795CurrentTrain: epoch  0, batch    68 | loss: 92.1438255CurrentTrain: epoch  0, batch    69 | loss: 135.8565912CurrentTrain: epoch  0, batch    70 | loss: 88.1973041CurrentTrain: epoch  0, batch    71 | loss: 68.3908058CurrentTrain: epoch  0, batch    72 | loss: 112.0558520CurrentTrain: epoch  0, batch    73 | loss: 91.2222641CurrentTrain: epoch  0, batch    74 | loss: 92.7081563CurrentTrain: epoch  0, batch    75 | loss: 93.4953878CurrentTrain: epoch  0, batch    76 | loss: 77.4409361CurrentTrain: epoch  0, batch    77 | loss: 75.3415811CurrentTrain: epoch  0, batch    78 | loss: 69.2351251CurrentTrain: epoch  0, batch    79 | loss: 92.8639406CurrentTrain: epoch  0, batch    80 | loss: 77.5185233CurrentTrain: epoch  0, batch    81 | loss: 76.9002734CurrentTrain: epoch  0, batch    82 | loss: 77.0443524CurrentTrain: epoch  0, batch    83 | loss: 65.5938764CurrentTrain: epoch  0, batch    84 | loss: 91.6940005CurrentTrain: epoch  0, batch    85 | loss: 104.7684631CurrentTrain: epoch  0, batch    86 | loss: 78.7528322CurrentTrain: epoch  0, batch    87 | loss: 91.2597992CurrentTrain: epoch  0, batch    88 | loss: 88.7665996CurrentTrain: epoch  0, batch    89 | loss: 131.2552954CurrentTrain: epoch  0, batch    90 | loss: 137.7935781CurrentTrain: epoch  0, batch    91 | loss: 106.3908945CurrentTrain: epoch  0, batch    92 | loss: 74.5285016CurrentTrain: epoch  0, batch    93 | loss: 77.5968356CurrentTrain: epoch  0, batch    94 | loss: 82.7677048CurrentTrain: epoch  0, batch    95 | loss: 92.0269641CurrentTrain: epoch  1, batch     0 | loss: 64.8267326CurrentTrain: epoch  1, batch     1 | loss: 76.1650523CurrentTrain: epoch  1, batch     2 | loss: 64.3899021CurrentTrain: epoch  1, batch     3 | loss: 103.4830550CurrentTrain: epoch  1, batch     4 | loss: 72.0585518CurrentTrain: epoch  1, batch     5 | loss: 85.3467770CurrentTrain: epoch  1, batch     6 | loss: 104.2305031CurrentTrain: epoch  1, batch     7 | loss: 83.8490814CurrentTrain: epoch  1, batch     8 | loss: 63.0370134CurrentTrain: epoch  1, batch     9 | loss: 75.1641085CurrentTrain: epoch  1, batch    10 | loss: 85.3227828CurrentTrain: epoch  1, batch    11 | loss: 136.1088305CurrentTrain: epoch  1, batch    12 | loss: 76.2821014CurrentTrain: epoch  1, batch    13 | loss: 105.1722362CurrentTrain: epoch  1, batch    14 | loss: 73.6068596CurrentTrain: epoch  1, batch    15 | loss: 88.4075209CurrentTrain: epoch  1, batch    16 | loss: 84.8499338CurrentTrain: epoch  1, batch    17 | loss: 84.9030812CurrentTrain: epoch  1, batch    18 | loss: 87.3916282CurrentTrain: epoch  1, batch    19 | loss: 106.0206900CurrentTrain: epoch  1, batch    20 | loss: 66.7872462CurrentTrain: epoch  1, batch    21 | loss: 73.2186320CurrentTrain: epoch  1, batch    22 | loss: 137.6577211CurrentTrain: epoch  1, batch    23 | loss: 131.0418570CurrentTrain: epoch  1, batch    24 | loss: 72.8815161CurrentTrain: epoch  1, batch    25 | loss: 76.1429985CurrentTrain: epoch  1, batch    26 | loss: 104.1154618CurrentTrain: epoch  1, batch    27 | loss: 71.3335791CurrentTrain: epoch  1, batch    28 | loss: 74.1924532CurrentTrain: epoch  1, batch    29 | loss: 129.1449105CurrentTrain: epoch  1, batch    30 | loss: 74.2411265CurrentTrain: epoch  1, batch    31 | loss: 73.4622503CurrentTrain: epoch  1, batch    32 | loss: 88.4966014CurrentTrain: epoch  1, batch    33 | loss: 70.6555896CurrentTrain: epoch  1, batch    34 | loss: 86.6815126CurrentTrain: epoch  1, batch    35 | loss: 129.3448132CurrentTrain: epoch  1, batch    36 | loss: 132.5284764CurrentTrain: epoch  1, batch    37 | loss: 183.7029482CurrentTrain: epoch  1, batch    38 | loss: 86.8723117CurrentTrain: epoch  1, batch    39 | loss: 74.2429072CurrentTrain: epoch  1, batch    40 | loss: 86.9696916CurrentTrain: epoch  1, batch    41 | loss: 75.8631916CurrentTrain: epoch  1, batch    42 | loss: 84.6674722CurrentTrain: epoch  1, batch    43 | loss: 65.6931428CurrentTrain: epoch  1, batch    44 | loss: 131.6543935CurrentTrain: epoch  1, batch    45 | loss: 102.7843228CurrentTrain: epoch  1, batch    46 | loss: 130.8310929CurrentTrain: epoch  1, batch    47 | loss: 72.6699842CurrentTrain: epoch  1, batch    48 | loss: 69.8975827CurrentTrain: epoch  1, batch    49 | loss: 103.5933480CurrentTrain: epoch  1, batch    50 | loss: 69.7125442CurrentTrain: epoch  1, batch    51 | loss: 74.4028385CurrentTrain: epoch  1, batch    52 | loss: 130.8247214CurrentTrain: epoch  1, batch    53 | loss: 86.4005087CurrentTrain: epoch  1, batch    54 | loss: 88.6463413CurrentTrain: epoch  1, batch    55 | loss: 129.1134785CurrentTrain: epoch  1, batch    56 | loss: 98.6560184CurrentTrain: epoch  1, batch    57 | loss: 64.0433077CurrentTrain: epoch  1, batch    58 | loss: 103.0157109CurrentTrain: epoch  1, batch    59 | loss: 80.2738601CurrentTrain: epoch  1, batch    60 | loss: 62.4997086CurrentTrain: epoch  1, batch    61 | loss: 65.6698517CurrentTrain: epoch  1, batch    62 | loss: 86.9117192CurrentTrain: epoch  1, batch    63 | loss: 72.3116771CurrentTrain: epoch  1, batch    64 | loss: 73.1946571CurrentTrain: epoch  1, batch    65 | loss: 87.9408782CurrentTrain: epoch  1, batch    66 | loss: 63.7497379CurrentTrain: epoch  1, batch    67 | loss: 103.5911276CurrentTrain: epoch  1, batch    68 | loss: 59.1909537CurrentTrain: epoch  1, batch    69 | loss: 105.8104079CurrentTrain: epoch  1, batch    70 | loss: 133.2555642CurrentTrain: epoch  1, batch    71 | loss: 67.9725740CurrentTrain: epoch  1, batch    72 | loss: 136.7375362CurrentTrain: epoch  1, batch    73 | loss: 133.5790777CurrentTrain: epoch  1, batch    74 | loss: 64.6598519CurrentTrain: epoch  1, batch    75 | loss: 64.0583869CurrentTrain: epoch  1, batch    76 | loss: 73.3003449CurrentTrain: epoch  1, batch    77 | loss: 76.1894401CurrentTrain: epoch  1, batch    78 | loss: 103.1227563CurrentTrain: epoch  1, batch    79 | loss: 85.5338855CurrentTrain: epoch  1, batch    80 | loss: 67.1073795CurrentTrain: epoch  1, batch    81 | loss: 127.5458106CurrentTrain: epoch  1, batch    82 | loss: 72.5450574CurrentTrain: epoch  1, batch    83 | loss: 99.4662093CurrentTrain: epoch  1, batch    84 | loss: 67.8227843CurrentTrain: epoch  1, batch    85 | loss: 84.2715251CurrentTrain: epoch  1, batch    86 | loss: 86.4923727CurrentTrain: epoch  1, batch    87 | loss: 84.1479086CurrentTrain: epoch  1, batch    88 | loss: 95.5072511CurrentTrain: epoch  1, batch    89 | loss: 83.3453076CurrentTrain: epoch  1, batch    90 | loss: 101.4615372CurrentTrain: epoch  1, batch    91 | loss: 87.4573013CurrentTrain: epoch  1, batch    92 | loss: 80.9098909CurrentTrain: epoch  1, batch    93 | loss: 100.4211775CurrentTrain: epoch  1, batch    94 | loss: 80.6524262CurrentTrain: epoch  1, batch    95 | loss: 59.5058441CurrentTrain: epoch  2, batch     0 | loss: 96.0335942CurrentTrain: epoch  2, batch     1 | loss: 71.3293836CurrentTrain: epoch  2, batch     2 | loss: 67.0486247CurrentTrain: epoch  2, batch     3 | loss: 58.1796215CurrentTrain: epoch  2, batch     4 | loss: 93.7515560CurrentTrain: epoch  2, batch     5 | loss: 102.9118350CurrentTrain: epoch  2, batch     6 | loss: 81.4429568CurrentTrain: epoch  2, batch     7 | loss: 69.2996823CurrentTrain: epoch  2, batch     8 | loss: 68.7122253CurrentTrain: epoch  2, batch     9 | loss: 98.6392849CurrentTrain: epoch  2, batch    10 | loss: 70.7040193CurrentTrain: epoch  2, batch    11 | loss: 85.8033997CurrentTrain: epoch  2, batch    12 | loss: 67.1681062CurrentTrain: epoch  2, batch    13 | loss: 70.6952364CurrentTrain: epoch  2, batch    14 | loss: 101.5809257CurrentTrain: epoch  2, batch    15 | loss: 62.1487141CurrentTrain: epoch  2, batch    16 | loss: 63.5108044CurrentTrain: epoch  2, batch    17 | loss: 169.5891458CurrentTrain: epoch  2, batch    18 | loss: 78.7277607CurrentTrain: epoch  2, batch    19 | loss: 104.5230230CurrentTrain: epoch  2, batch    20 | loss: 83.1624192CurrentTrain: epoch  2, batch    21 | loss: 91.8781825CurrentTrain: epoch  2, batch    22 | loss: 65.2958802CurrentTrain: epoch  2, batch    23 | loss: 69.6399156CurrentTrain: epoch  2, batch    24 | loss: 67.9100585CurrentTrain: epoch  2, batch    25 | loss: 72.5448239CurrentTrain: epoch  2, batch    26 | loss: 98.8162561CurrentTrain: epoch  2, batch    27 | loss: 60.2839914CurrentTrain: epoch  2, batch    28 | loss: 101.6974077CurrentTrain: epoch  2, batch    29 | loss: 67.3887888CurrentTrain: epoch  2, batch    30 | loss: 95.3032516CurrentTrain: epoch  2, batch    31 | loss: 103.0085160CurrentTrain: epoch  2, batch    32 | loss: 77.6393420CurrentTrain: epoch  2, batch    33 | loss: 71.6777564CurrentTrain: epoch  2, batch    34 | loss: 83.8958476CurrentTrain: epoch  2, batch    35 | loss: 85.6198513CurrentTrain: epoch  2, batch    36 | loss: 80.0399793CurrentTrain: epoch  2, batch    37 | loss: 127.4608143CurrentTrain: epoch  2, batch    38 | loss: 69.0471633CurrentTrain: epoch  2, batch    39 | loss: 97.1899283CurrentTrain: epoch  2, batch    40 | loss: 81.4534324CurrentTrain: epoch  2, batch    41 | loss: 72.6500749CurrentTrain: epoch  2, batch    42 | loss: 69.7581159CurrentTrain: epoch  2, batch    43 | loss: 67.0578491CurrentTrain: epoch  2, batch    44 | loss: 106.2258515CurrentTrain: epoch  2, batch    45 | loss: 80.5650903CurrentTrain: epoch  2, batch    46 | loss: 104.1732773CurrentTrain: epoch  2, batch    47 | loss: 103.3701044CurrentTrain: epoch  2, batch    48 | loss: 99.1243981CurrentTrain: epoch  2, batch    49 | loss: 100.5546471CurrentTrain: epoch  2, batch    50 | loss: 73.1190607CurrentTrain: epoch  2, batch    51 | loss: 72.7076199CurrentTrain: epoch  2, batch    52 | loss: 82.9629962CurrentTrain: epoch  2, batch    53 | loss: 79.2825228CurrentTrain: epoch  2, batch    54 | loss: 101.8924678CurrentTrain: epoch  2, batch    55 | loss: 82.3146093CurrentTrain: epoch  2, batch    56 | loss: 128.6966418CurrentTrain: epoch  2, batch    57 | loss: 99.1434954CurrentTrain: epoch  2, batch    58 | loss: 83.5822744CurrentTrain: epoch  2, batch    59 | loss: 83.5834660CurrentTrain: epoch  2, batch    60 | loss: 94.5686622CurrentTrain: epoch  2, batch    61 | loss: 117.4187533CurrentTrain: epoch  2, batch    62 | loss: 82.0301842CurrentTrain: epoch  2, batch    63 | loss: 82.0441484CurrentTrain: epoch  2, batch    64 | loss: 83.9690954CurrentTrain: epoch  2, batch    65 | loss: 82.8509962CurrentTrain: epoch  2, batch    66 | loss: 101.2009496CurrentTrain: epoch  2, batch    67 | loss: 98.3334637CurrentTrain: epoch  2, batch    68 | loss: 129.0249120CurrentTrain: epoch  2, batch    69 | loss: 102.6769959CurrentTrain: epoch  2, batch    70 | loss: 60.6356244CurrentTrain: epoch  2, batch    71 | loss: 94.2288778CurrentTrain: epoch  2, batch    72 | loss: 169.0380557CurrentTrain: epoch  2, batch    73 | loss: 83.2995425CurrentTrain: epoch  2, batch    74 | loss: 79.4135221CurrentTrain: epoch  2, batch    75 | loss: 67.9528374CurrentTrain: epoch  2, batch    76 | loss: 95.8021447CurrentTrain: epoch  2, batch    77 | loss: 78.5329715CurrentTrain: epoch  2, batch    78 | loss: 70.4016142CurrentTrain: epoch  2, batch    79 | loss: 101.0039474CurrentTrain: epoch  2, batch    80 | loss: 171.6274536CurrentTrain: epoch  2, batch    81 | loss: 81.4496192CurrentTrain: epoch  2, batch    82 | loss: 83.4055650CurrentTrain: epoch  2, batch    83 | loss: 56.2277585CurrentTrain: epoch  2, batch    84 | loss: 82.7464076CurrentTrain: epoch  2, batch    85 | loss: 57.4544404CurrentTrain: epoch  2, batch    86 | loss: 77.4555652CurrentTrain: epoch  2, batch    87 | loss: 95.4871717CurrentTrain: epoch  2, batch    88 | loss: 117.9265630CurrentTrain: epoch  2, batch    89 | loss: 129.5975029CurrentTrain: epoch  2, batch    90 | loss: 122.9169377CurrentTrain: epoch  2, batch    91 | loss: 102.4200558CurrentTrain: epoch  2, batch    92 | loss: 104.9800622CurrentTrain: epoch  2, batch    93 | loss: 107.4664434CurrentTrain: epoch  2, batch    94 | loss: 69.4281486CurrentTrain: epoch  2, batch    95 | loss: 68.4016731CurrentTrain: epoch  3, batch     0 | loss: 57.3121305CurrentTrain: epoch  3, batch     1 | loss: 59.8439305CurrentTrain: epoch  3, batch     2 | loss: 67.3541501CurrentTrain: epoch  3, batch     3 | loss: 82.1234178CurrentTrain: epoch  3, batch     4 | loss: 65.6904663CurrentTrain: epoch  3, batch     5 | loss: 79.4456221CurrentTrain: epoch  3, batch     6 | loss: 102.2658357CurrentTrain: epoch  3, batch     7 | loss: 78.0665615CurrentTrain: epoch  3, batch     8 | loss: 57.1652854CurrentTrain: epoch  3, batch     9 | loss: 102.3450261CurrentTrain: epoch  3, batch    10 | loss: 80.2021398CurrentTrain: epoch  3, batch    11 | loss: 92.6560853CurrentTrain: epoch  3, batch    12 | loss: 82.9490984CurrentTrain: epoch  3, batch    13 | loss: 102.4198655CurrentTrain: epoch  3, batch    14 | loss: 82.2518821CurrentTrain: epoch  3, batch    15 | loss: 100.4331548CurrentTrain: epoch  3, batch    16 | loss: 97.9395134CurrentTrain: epoch  3, batch    17 | loss: 68.6717557CurrentTrain: epoch  3, batch    18 | loss: 79.2462040CurrentTrain: epoch  3, batch    19 | loss: 79.6585257CurrentTrain: epoch  3, batch    20 | loss: 68.9965703CurrentTrain: epoch  3, batch    21 | loss: 66.0334063CurrentTrain: epoch  3, batch    22 | loss: 95.9826975CurrentTrain: epoch  3, batch    23 | loss: 168.2776457CurrentTrain: epoch  3, batch    24 | loss: 77.3457460CurrentTrain: epoch  3, batch    25 | loss: 99.2855123CurrentTrain: epoch  3, batch    26 | loss: 94.0749316CurrentTrain: epoch  3, batch    27 | loss: 123.4772671CurrentTrain: epoch  3, batch    28 | loss: 63.3077852CurrentTrain: epoch  3, batch    29 | loss: 97.4441978CurrentTrain: epoch  3, batch    30 | loss: 81.5852807CurrentTrain: epoch  3, batch    31 | loss: 64.3281819CurrentTrain: epoch  3, batch    32 | loss: 96.4071223CurrentTrain: epoch  3, batch    33 | loss: 80.4735169CurrentTrain: epoch  3, batch    34 | loss: 68.9414511CurrentTrain: epoch  3, batch    35 | loss: 59.2550562CurrentTrain: epoch  3, batch    36 | loss: 83.1153694CurrentTrain: epoch  3, batch    37 | loss: 63.9021327CurrentTrain: epoch  3, batch    38 | loss: 99.1638347CurrentTrain: epoch  3, batch    39 | loss: 78.8320876CurrentTrain: epoch  3, batch    40 | loss: 94.8248929CurrentTrain: epoch  3, batch    41 | loss: 77.3166114CurrentTrain: epoch  3, batch    42 | loss: 79.7552347CurrentTrain: epoch  3, batch    43 | loss: 79.8188001CurrentTrain: epoch  3, batch    44 | loss: 67.8684603CurrentTrain: epoch  3, batch    45 | loss: 78.8198381CurrentTrain: epoch  3, batch    46 | loss: 64.5466973CurrentTrain: epoch  3, batch    47 | loss: 74.1553080CurrentTrain: epoch  3, batch    48 | loss: 71.2037722CurrentTrain: epoch  3, batch    49 | loss: 101.1883389CurrentTrain: epoch  3, batch    50 | loss: 79.0814130CurrentTrain: epoch  3, batch    51 | loss: 79.8418049CurrentTrain: epoch  3, batch    52 | loss: 97.0578163CurrentTrain: epoch  3, batch    53 | loss: 58.2202795CurrentTrain: epoch  3, batch    54 | loss: 63.9665812CurrentTrain: epoch  3, batch    55 | loss: 77.3398285CurrentTrain: epoch  3, batch    56 | loss: 68.2560926CurrentTrain: epoch  3, batch    57 | loss: 98.2918512CurrentTrain: epoch  3, batch    58 | loss: 161.5822976CurrentTrain: epoch  3, batch    59 | loss: 77.4223232CurrentTrain: epoch  3, batch    60 | loss: 96.4651571CurrentTrain: epoch  3, batch    61 | loss: 93.2835835CurrentTrain: epoch  3, batch    62 | loss: 125.8808764CurrentTrain: epoch  3, batch    63 | loss: 80.8659537CurrentTrain: epoch  3, batch    64 | loss: 63.9436708CurrentTrain: epoch  3, batch    65 | loss: 68.9897141CurrentTrain: epoch  3, batch    66 | loss: 63.9990869CurrentTrain: epoch  3, batch    67 | loss: 124.8494080CurrentTrain: epoch  3, batch    68 | loss: 96.3598815CurrentTrain: epoch  3, batch    69 | loss: 103.1732470CurrentTrain: epoch  3, batch    70 | loss: 64.9087959CurrentTrain: epoch  3, batch    71 | loss: 69.4626440CurrentTrain: epoch  3, batch    72 | loss: 78.0261231CurrentTrain: epoch  3, batch    73 | loss: 96.7674976CurrentTrain: epoch  3, batch    74 | loss: 57.8965648CurrentTrain: epoch  3, batch    75 | loss: 68.1240370CurrentTrain: epoch  3, batch    76 | loss: 119.4627802CurrentTrain: epoch  3, batch    77 | loss: 66.0463035CurrentTrain: epoch  3, batch    78 | loss: 75.3764936CurrentTrain: epoch  3, batch    79 | loss: 77.9295387CurrentTrain: epoch  3, batch    80 | loss: 83.7647295CurrentTrain: epoch  3, batch    81 | loss: 69.8501075CurrentTrain: epoch  3, batch    82 | loss: 53.6585673CurrentTrain: epoch  3, batch    83 | loss: 80.2798917CurrentTrain: epoch  3, batch    84 | loss: 100.8447832CurrentTrain: epoch  3, batch    85 | loss: 75.9588037CurrentTrain: epoch  3, batch    86 | loss: 80.1344451CurrentTrain: epoch  3, batch    87 | loss: 84.1680842CurrentTrain: epoch  3, batch    88 | loss: 81.8093034CurrentTrain: epoch  3, batch    89 | loss: 125.2150570CurrentTrain: epoch  3, batch    90 | loss: 60.4071790CurrentTrain: epoch  3, batch    91 | loss: 98.9313323CurrentTrain: epoch  3, batch    92 | loss: 69.5733303CurrentTrain: epoch  3, batch    93 | loss: 63.9268139CurrentTrain: epoch  3, batch    94 | loss: 68.1567238CurrentTrain: epoch  3, batch    95 | loss: 71.0110737CurrentTrain: epoch  4, batch     0 | loss: 80.3391890CurrentTrain: epoch  4, batch     1 | loss: 82.2240491CurrentTrain: epoch  4, batch     2 | loss: 58.8144141CurrentTrain: epoch  4, batch     3 | loss: 125.1268342CurrentTrain: epoch  4, batch     4 | loss: 77.8029025CurrentTrain: epoch  4, batch     5 | loss: 78.1046093CurrentTrain: epoch  4, batch     6 | loss: 68.2264450CurrentTrain: epoch  4, batch     7 | loss: 64.2635807CurrentTrain: epoch  4, batch     8 | loss: 80.8519838CurrentTrain: epoch  4, batch     9 | loss: 61.2083212CurrentTrain: epoch  4, batch    10 | loss: 79.6981022CurrentTrain: epoch  4, batch    11 | loss: 77.7583135CurrentTrain: epoch  4, batch    12 | loss: 63.5202738CurrentTrain: epoch  4, batch    13 | loss: 115.6083545CurrentTrain: epoch  4, batch    14 | loss: 77.8591268CurrentTrain: epoch  4, batch    15 | loss: 70.9487740CurrentTrain: epoch  4, batch    16 | loss: 95.4674779CurrentTrain: epoch  4, batch    17 | loss: 80.5458241CurrentTrain: epoch  4, batch    18 | loss: 56.0337465CurrentTrain: epoch  4, batch    19 | loss: 65.5681868CurrentTrain: epoch  4, batch    20 | loss: 62.7609798CurrentTrain: epoch  4, batch    21 | loss: 53.1190734CurrentTrain: epoch  4, batch    22 | loss: 77.0935583CurrentTrain: epoch  4, batch    23 | loss: 86.4244514CurrentTrain: epoch  4, batch    24 | loss: 124.8416720CurrentTrain: epoch  4, batch    25 | loss: 65.6355129CurrentTrain: epoch  4, batch    26 | loss: 88.9578831CurrentTrain: epoch  4, batch    27 | loss: 75.0257801CurrentTrain: epoch  4, batch    28 | loss: 78.7309485CurrentTrain: epoch  4, batch    29 | loss: 120.2725813CurrentTrain: epoch  4, batch    30 | loss: 77.0183827CurrentTrain: epoch  4, batch    31 | loss: 95.5989293CurrentTrain: epoch  4, batch    32 | loss: 87.7507337CurrentTrain: epoch  4, batch    33 | loss: 95.1867750CurrentTrain: epoch  4, batch    34 | loss: 98.1907977CurrentTrain: epoch  4, batch    35 | loss: 76.6525488CurrentTrain: epoch  4, batch    36 | loss: 79.2645396CurrentTrain: epoch  4, batch    37 | loss: 76.0827236CurrentTrain: epoch  4, batch    38 | loss: 121.8225336CurrentTrain: epoch  4, batch    39 | loss: 82.7242307CurrentTrain: epoch  4, batch    40 | loss: 96.9578488CurrentTrain: epoch  4, batch    41 | loss: 80.8916784CurrentTrain: epoch  4, batch    42 | loss: 72.9221741CurrentTrain: epoch  4, batch    43 | loss: 121.9504799CurrentTrain: epoch  4, batch    44 | loss: 78.7042809CurrentTrain: epoch  4, batch    45 | loss: 93.6144504CurrentTrain: epoch  4, batch    46 | loss: 88.0426691CurrentTrain: epoch  4, batch    47 | loss: 66.5597962CurrentTrain: epoch  4, batch    48 | loss: 75.2680366CurrentTrain: epoch  4, batch    49 | loss: 77.6912044CurrentTrain: epoch  4, batch    50 | loss: 80.1338474CurrentTrain: epoch  4, batch    51 | loss: 96.0556427CurrentTrain: epoch  4, batch    52 | loss: 71.5748477CurrentTrain: epoch  4, batch    53 | loss: 66.1760255CurrentTrain: epoch  4, batch    54 | loss: 96.0216258CurrentTrain: epoch  4, batch    55 | loss: 75.0867450CurrentTrain: epoch  4, batch    56 | loss: 76.8093921CurrentTrain: epoch  4, batch    57 | loss: 80.5380596CurrentTrain: epoch  4, batch    58 | loss: 60.1829966CurrentTrain: epoch  4, batch    59 | loss: 75.9840584CurrentTrain: epoch  4, batch    60 | loss: 102.1629626CurrentTrain: epoch  4, batch    61 | loss: 67.5895964CurrentTrain: epoch  4, batch    62 | loss: 77.9332430CurrentTrain: epoch  4, batch    63 | loss: 78.6811942CurrentTrain: epoch  4, batch    64 | loss: 90.3378465CurrentTrain: epoch  4, batch    65 | loss: 64.9923147CurrentTrain: epoch  4, batch    66 | loss: 78.6528969CurrentTrain: epoch  4, batch    67 | loss: 73.6391306CurrentTrain: epoch  4, batch    68 | loss: 80.1029143CurrentTrain: epoch  4, batch    69 | loss: 85.1172992CurrentTrain: epoch  4, batch    70 | loss: 65.4101976CurrentTrain: epoch  4, batch    71 | loss: 91.0665161CurrentTrain: epoch  4, batch    72 | loss: 75.7256853CurrentTrain: epoch  4, batch    73 | loss: 67.3148163CurrentTrain: epoch  4, batch    74 | loss: 80.9654695CurrentTrain: epoch  4, batch    75 | loss: 63.0519494CurrentTrain: epoch  4, batch    76 | loss: 98.9001776CurrentTrain: epoch  4, batch    77 | loss: 66.5134755CurrentTrain: epoch  4, batch    78 | loss: 119.5777804CurrentTrain: epoch  4, batch    79 | loss: 67.5097706CurrentTrain: epoch  4, batch    80 | loss: 79.3071337CurrentTrain: epoch  4, batch    81 | loss: 71.8082905CurrentTrain: epoch  4, batch    82 | loss: 62.7478427CurrentTrain: epoch  4, batch    83 | loss: 77.5000478CurrentTrain: epoch  4, batch    84 | loss: 92.9053913CurrentTrain: epoch  4, batch    85 | loss: 63.6699579CurrentTrain: epoch  4, batch    86 | loss: 161.9987564CurrentTrain: epoch  4, batch    87 | loss: 97.3453094CurrentTrain: epoch  4, batch    88 | loss: 75.5560170CurrentTrain: epoch  4, batch    89 | loss: 65.7600775CurrentTrain: epoch  4, batch    90 | loss: 79.5209181CurrentTrain: epoch  4, batch    91 | loss: 67.8226640CurrentTrain: epoch  4, batch    92 | loss: 92.8551798CurrentTrain: epoch  4, batch    93 | loss: 74.0023922CurrentTrain: epoch  4, batch    94 | loss: 79.9621801CurrentTrain: epoch  4, batch    95 | loss: 78.7158986CurrentTrain: epoch  5, batch     0 | loss: 61.4855777CurrentTrain: epoch  5, batch     1 | loss: 74.1938747CurrentTrain: epoch  5, batch     2 | loss: 65.5367604CurrentTrain: epoch  5, batch     3 | loss: 76.0835025CurrentTrain: epoch  5, batch     4 | loss: 92.9199998CurrentTrain: epoch  5, batch     5 | loss: 156.7073551CurrentTrain: epoch  5, batch     6 | loss: 123.0776426CurrentTrain: epoch  5, batch     7 | loss: 90.7778282CurrentTrain: epoch  5, batch     8 | loss: 77.8114945CurrentTrain: epoch  5, batch     9 | loss: 160.5655717CurrentTrain: epoch  5, batch    10 | loss: 88.2582099CurrentTrain: epoch  5, batch    11 | loss: 60.9042580CurrentTrain: epoch  5, batch    12 | loss: 93.4818269CurrentTrain: epoch  5, batch    13 | loss: 73.9561991CurrentTrain: epoch  5, batch    14 | loss: 79.5020546CurrentTrain: epoch  5, batch    15 | loss: 74.3478456CurrentTrain: epoch  5, batch    16 | loss: 81.4443804CurrentTrain: epoch  5, batch    17 | loss: 79.2263149CurrentTrain: epoch  5, batch    18 | loss: 75.0771236CurrentTrain: epoch  5, batch    19 | loss: 71.0563132CurrentTrain: epoch  5, batch    20 | loss: 78.8480080CurrentTrain: epoch  5, batch    21 | loss: 93.7132900CurrentTrain: epoch  5, batch    22 | loss: 89.3374572CurrentTrain: epoch  5, batch    23 | loss: 76.1102220CurrentTrain: epoch  5, batch    24 | loss: 96.1965014CurrentTrain: epoch  5, batch    25 | loss: 53.8643213CurrentTrain: epoch  5, batch    26 | loss: 67.0897222CurrentTrain: epoch  5, batch    27 | loss: 78.2933590CurrentTrain: epoch  5, batch    28 | loss: 60.1433041CurrentTrain: epoch  5, batch    29 | loss: 61.6569697CurrentTrain: epoch  5, batch    30 | loss: 52.3774092CurrentTrain: epoch  5, batch    31 | loss: 61.6699941CurrentTrain: epoch  5, batch    32 | loss: 68.2470471CurrentTrain: epoch  5, batch    33 | loss: 76.8133503CurrentTrain: epoch  5, batch    34 | loss: 72.9947971CurrentTrain: epoch  5, batch    35 | loss: 74.6980472CurrentTrain: epoch  5, batch    36 | loss: 87.4347206CurrentTrain: epoch  5, batch    37 | loss: 91.4728065CurrentTrain: epoch  5, batch    38 | loss: 77.9924527CurrentTrain: epoch  5, batch    39 | loss: 63.0886385CurrentTrain: epoch  5, batch    40 | loss: 54.2254676CurrentTrain: epoch  5, batch    41 | loss: 114.7724713CurrentTrain: epoch  5, batch    42 | loss: 83.3166681CurrentTrain: epoch  5, batch    43 | loss: 74.8089135CurrentTrain: epoch  5, batch    44 | loss: 89.3889225CurrentTrain: epoch  5, batch    45 | loss: 92.7443568CurrentTrain: epoch  5, batch    46 | loss: 77.7288769CurrentTrain: epoch  5, batch    47 | loss: 51.3353942CurrentTrain: epoch  5, batch    48 | loss: 77.2927843CurrentTrain: epoch  5, batch    49 | loss: 90.4992687CurrentTrain: epoch  5, batch    50 | loss: 79.0787434CurrentTrain: epoch  5, batch    51 | loss: 80.1037603CurrentTrain: epoch  5, batch    52 | loss: 91.5650751CurrentTrain: epoch  5, batch    53 | loss: 119.7732150CurrentTrain: epoch  5, batch    54 | loss: 78.9043983CurrentTrain: epoch  5, batch    55 | loss: 77.6008889CurrentTrain: epoch  5, batch    56 | loss: 76.4520565CurrentTrain: epoch  5, batch    57 | loss: 63.4109261CurrentTrain: epoch  5, batch    58 | loss: 92.5181314CurrentTrain: epoch  5, batch    59 | loss: 121.2517047CurrentTrain: epoch  5, batch    60 | loss: 75.7504980CurrentTrain: epoch  5, batch    61 | loss: 83.1034451CurrentTrain: epoch  5, batch    62 | loss: 92.6245751CurrentTrain: epoch  5, batch    63 | loss: 66.9590456CurrentTrain: epoch  5, batch    64 | loss: 77.9111916CurrentTrain: epoch  5, batch    65 | loss: 73.8214302CurrentTrain: epoch  5, batch    66 | loss: 66.0611701CurrentTrain: epoch  5, batch    67 | loss: 78.8241835CurrentTrain: epoch  5, batch    68 | loss: 66.5684812CurrentTrain: epoch  5, batch    69 | loss: 73.7176291CurrentTrain: epoch  5, batch    70 | loss: 93.2307523CurrentTrain: epoch  5, batch    71 | loss: 77.3157212CurrentTrain: epoch  5, batch    72 | loss: 63.7366318CurrentTrain: epoch  5, batch    73 | loss: 91.8603987CurrentTrain: epoch  5, batch    74 | loss: 72.5226782CurrentTrain: epoch  5, batch    75 | loss: 115.4766807CurrentTrain: epoch  5, batch    76 | loss: 56.6594192CurrentTrain: epoch  5, batch    77 | loss: 115.4785103CurrentTrain: epoch  5, batch    78 | loss: 91.0458828CurrentTrain: epoch  5, batch    79 | loss: 77.1620029CurrentTrain: epoch  5, batch    80 | loss: 73.5587560CurrentTrain: epoch  5, batch    81 | loss: 80.9145872CurrentTrain: epoch  5, batch    82 | loss: 64.9924607CurrentTrain: epoch  5, batch    83 | loss: 92.2622702CurrentTrain: epoch  5, batch    84 | loss: 76.3443463CurrentTrain: epoch  5, batch    85 | loss: 63.5333601CurrentTrain: epoch  5, batch    86 | loss: 77.8277305CurrentTrain: epoch  5, batch    87 | loss: 91.9022078CurrentTrain: epoch  5, batch    88 | loss: 123.4380446CurrentTrain: epoch  5, batch    89 | loss: 61.3147516CurrentTrain: epoch  5, batch    90 | loss: 75.5982425CurrentTrain: epoch  5, batch    91 | loss: 63.3824277CurrentTrain: epoch  5, batch    92 | loss: 92.7729991CurrentTrain: epoch  5, batch    93 | loss: 53.7415697CurrentTrain: epoch  5, batch    94 | loss: 66.2791527CurrentTrain: epoch  5, batch    95 | loss: 78.9447385CurrentTrain: epoch  6, batch     0 | loss: 72.2330584CurrentTrain: epoch  6, batch     1 | loss: 75.7126143CurrentTrain: epoch  6, batch     2 | loss: 90.3881314CurrentTrain: epoch  6, batch     3 | loss: 63.0428532CurrentTrain: epoch  6, batch     4 | loss: 53.5333042CurrentTrain: epoch  6, batch     5 | loss: 71.7753195CurrentTrain: epoch  6, batch     6 | loss: 74.8786270CurrentTrain: epoch  6, batch     7 | loss: 73.5256234CurrentTrain: epoch  6, batch     8 | loss: 77.5284718CurrentTrain: epoch  6, batch     9 | loss: 91.6516190CurrentTrain: epoch  6, batch    10 | loss: 63.4574827CurrentTrain: epoch  6, batch    11 | loss: 55.1692553CurrentTrain: epoch  6, batch    12 | loss: 112.4643250CurrentTrain: epoch  6, batch    13 | loss: 75.4300607CurrentTrain: epoch  6, batch    14 | loss: 88.3272066CurrentTrain: epoch  6, batch    15 | loss: 64.7246559CurrentTrain: epoch  6, batch    16 | loss: 51.3025785CurrentTrain: epoch  6, batch    17 | loss: 73.8023650CurrentTrain: epoch  6, batch    18 | loss: 61.4398206CurrentTrain: epoch  6, batch    19 | loss: 90.5615021CurrentTrain: epoch  6, batch    20 | loss: 57.9259325CurrentTrain: epoch  6, batch    21 | loss: 71.5425293CurrentTrain: epoch  6, batch    22 | loss: 62.7983365CurrentTrain: epoch  6, batch    23 | loss: 53.2957167CurrentTrain: epoch  6, batch    24 | loss: 78.6116732CurrentTrain: epoch  6, batch    25 | loss: 53.9986437CurrentTrain: epoch  6, batch    26 | loss: 51.9578482CurrentTrain: epoch  6, batch    27 | loss: 75.7938648CurrentTrain: epoch  6, batch    28 | loss: 61.8557779CurrentTrain: epoch  6, batch    29 | loss: 88.9065743CurrentTrain: epoch  6, batch    30 | loss: 75.6153091CurrentTrain: epoch  6, batch    31 | loss: 62.2716825CurrentTrain: epoch  6, batch    32 | loss: 119.4675011CurrentTrain: epoch  6, batch    33 | loss: 70.6309402CurrentTrain: epoch  6, batch    34 | loss: 94.3564061CurrentTrain: epoch  6, batch    35 | loss: 52.5175353CurrentTrain: epoch  6, batch    36 | loss: 76.7198862CurrentTrain: epoch  6, batch    37 | loss: 57.4700186CurrentTrain: epoch  6, batch    38 | loss: 74.6435548CurrentTrain: epoch  6, batch    39 | loss: 91.5551549CurrentTrain: epoch  6, batch    40 | loss: 76.4470522CurrentTrain: epoch  6, batch    41 | loss: 74.2777355CurrentTrain: epoch  6, batch    42 | loss: 97.0844372CurrentTrain: epoch  6, batch    43 | loss: 74.9918730CurrentTrain: epoch  6, batch    44 | loss: 92.9394511CurrentTrain: epoch  6, batch    45 | loss: 117.3620171CurrentTrain: epoch  6, batch    46 | loss: 76.4171254CurrentTrain: epoch  6, batch    47 | loss: 161.9238335CurrentTrain: epoch  6, batch    48 | loss: 161.2123707CurrentTrain: epoch  6, batch    49 | loss: 165.1113403CurrentTrain: epoch  6, batch    50 | loss: 55.3866842CurrentTrain: epoch  6, batch    51 | loss: 77.3542040CurrentTrain: epoch  6, batch    52 | loss: 87.9221355CurrentTrain: epoch  6, batch    53 | loss: 91.3980786CurrentTrain: epoch  6, batch    54 | loss: 63.4282561CurrentTrain: epoch  6, batch    55 | loss: 53.7463935CurrentTrain: epoch  6, batch    56 | loss: 71.4377225CurrentTrain: epoch  6, batch    57 | loss: 77.9808052CurrentTrain: epoch  6, batch    58 | loss: 59.6888390CurrentTrain: epoch  6, batch    59 | loss: 93.8214963CurrentTrain: epoch  6, batch    60 | loss: 64.1532817CurrentTrain: epoch  6, batch    61 | loss: 74.2302747CurrentTrain: epoch  6, batch    62 | loss: 91.2901497CurrentTrain: epoch  6, batch    63 | loss: 53.4551061CurrentTrain: epoch  6, batch    64 | loss: 76.4721355CurrentTrain: epoch  6, batch    65 | loss: 53.4498715CurrentTrain: epoch  6, batch    66 | loss: 65.1179921CurrentTrain: epoch  6, batch    67 | loss: 64.3097241CurrentTrain: epoch  6, batch    68 | loss: 61.4282494CurrentTrain: epoch  6, batch    69 | loss: 58.6384639CurrentTrain: epoch  6, batch    70 | loss: 74.2024047CurrentTrain: epoch  6, batch    71 | loss: 74.9434685CurrentTrain: epoch  6, batch    72 | loss: 95.5370560CurrentTrain: epoch  6, batch    73 | loss: 51.2949715CurrentTrain: epoch  6, batch    74 | loss: 120.4545656CurrentTrain: epoch  6, batch    75 | loss: 62.7343330CurrentTrain: epoch  6, batch    76 | loss: 79.2492780CurrentTrain: epoch  6, batch    77 | loss: 94.6881429CurrentTrain: epoch  6, batch    78 | loss: 77.0631330CurrentTrain: epoch  6, batch    79 | loss: 73.0234322CurrentTrain: epoch  6, batch    80 | loss: 61.7906126CurrentTrain: epoch  6, batch    81 | loss: 96.9887771CurrentTrain: epoch  6, batch    82 | loss: 71.4482679CurrentTrain: epoch  6, batch    83 | loss: 170.5077604CurrentTrain: epoch  6, batch    84 | loss: 74.7237564CurrentTrain: epoch  6, batch    85 | loss: 75.5207109CurrentTrain: epoch  6, batch    86 | loss: 116.0808902CurrentTrain: epoch  6, batch    87 | loss: 74.1389322CurrentTrain: epoch  6, batch    88 | loss: 51.6042648CurrentTrain: epoch  6, batch    89 | loss: 64.9329810CurrentTrain: epoch  6, batch    90 | loss: 72.5328283CurrentTrain: epoch  6, batch    91 | loss: 91.4678425CurrentTrain: epoch  6, batch    92 | loss: 74.1203210CurrentTrain: epoch  6, batch    93 | loss: 94.7604022CurrentTrain: epoch  6, batch    94 | loss: 51.5228200CurrentTrain: epoch  6, batch    95 | loss: 76.8169462CurrentTrain: epoch  7, batch     0 | loss: 61.3472576CurrentTrain: epoch  7, batch     1 | loss: 52.0485390CurrentTrain: epoch  7, batch     2 | loss: 90.0694723CurrentTrain: epoch  7, batch     3 | loss: 87.9867280CurrentTrain: epoch  7, batch     4 | loss: 73.8688550CurrentTrain: epoch  7, batch     5 | loss: 71.3853422CurrentTrain: epoch  7, batch     6 | loss: 67.8025720CurrentTrain: epoch  7, batch     7 | loss: 89.3826815CurrentTrain: epoch  7, batch     8 | loss: 115.5332048CurrentTrain: epoch  7, batch     9 | loss: 74.1961700CurrentTrain: epoch  7, batch    10 | loss: 71.5754053CurrentTrain: epoch  7, batch    11 | loss: 90.4635852CurrentTrain: epoch  7, batch    12 | loss: 53.1611948CurrentTrain: epoch  7, batch    13 | loss: 73.9762921CurrentTrain: epoch  7, batch    14 | loss: 59.7332458CurrentTrain: epoch  7, batch    15 | loss: 75.0541828CurrentTrain: epoch  7, batch    16 | loss: 71.8111771CurrentTrain: epoch  7, batch    17 | loss: 93.2183989CurrentTrain: epoch  7, batch    18 | loss: 71.1773807CurrentTrain: epoch  7, batch    19 | loss: 85.8645730CurrentTrain: epoch  7, batch    20 | loss: 92.2456753CurrentTrain: epoch  7, batch    21 | loss: 116.7285559CurrentTrain: epoch  7, batch    22 | loss: 89.5230065CurrentTrain: epoch  7, batch    23 | loss: 63.9972818CurrentTrain: epoch  7, batch    24 | loss: 52.4951977CurrentTrain: epoch  7, batch    25 | loss: 79.1640742CurrentTrain: epoch  7, batch    26 | loss: 88.5866204CurrentTrain: epoch  7, batch    27 | loss: 91.9085707CurrentTrain: epoch  7, batch    28 | loss: 63.2571999CurrentTrain: epoch  7, batch    29 | loss: 92.5917180CurrentTrain: epoch  7, batch    30 | loss: 61.9421637CurrentTrain: epoch  7, batch    31 | loss: 52.5375960CurrentTrain: epoch  7, batch    32 | loss: 117.4004743CurrentTrain: epoch  7, batch    33 | loss: 59.9592074CurrentTrain: epoch  7, batch    34 | loss: 59.8066803CurrentTrain: epoch  7, batch    35 | loss: 117.0860664CurrentTrain: epoch  7, batch    36 | loss: 92.5330851CurrentTrain: epoch  7, batch    37 | loss: 56.1352517CurrentTrain: epoch  7, batch    38 | loss: 49.4693418CurrentTrain: epoch  7, batch    39 | loss: 88.2424839CurrentTrain: epoch  7, batch    40 | loss: 76.2993623CurrentTrain: epoch  7, batch    41 | loss: 62.3891461CurrentTrain: epoch  7, batch    42 | loss: 85.5323546CurrentTrain: epoch  7, batch    43 | loss: 93.2965073CurrentTrain: epoch  7, batch    44 | loss: 92.3707761CurrentTrain: epoch  7, batch    45 | loss: 60.2751512CurrentTrain: epoch  7, batch    46 | loss: 63.0163611CurrentTrain: epoch  7, batch    47 | loss: 90.1479643CurrentTrain: epoch  7, batch    48 | loss: 58.3363777CurrentTrain: epoch  7, batch    49 | loss: 63.0015462CurrentTrain: epoch  7, batch    50 | loss: 91.3858182CurrentTrain: epoch  7, batch    51 | loss: 94.0036125CurrentTrain: epoch  7, batch    52 | loss: 61.3073585CurrentTrain: epoch  7, batch    53 | loss: 116.3758270CurrentTrain: epoch  7, batch    54 | loss: 53.7517289CurrentTrain: epoch  7, batch    55 | loss: 51.7160836CurrentTrain: epoch  7, batch    56 | loss: 54.4647599CurrentTrain: epoch  7, batch    57 | loss: 76.2938831CurrentTrain: epoch  7, batch    58 | loss: 58.8931808CurrentTrain: epoch  7, batch    59 | loss: 75.4092429CurrentTrain: epoch  7, batch    60 | loss: 74.8398560CurrentTrain: epoch  7, batch    61 | loss: 71.4157125CurrentTrain: epoch  7, batch    62 | loss: 55.3254159CurrentTrain: epoch  7, batch    63 | loss: 72.3400639CurrentTrain: epoch  7, batch    64 | loss: 89.5459005CurrentTrain: epoch  7, batch    65 | loss: 90.2195783CurrentTrain: epoch  7, batch    66 | loss: 71.0832760CurrentTrain: epoch  7, batch    67 | loss: 85.7516891CurrentTrain: epoch  7, batch    68 | loss: 93.5225434CurrentTrain: epoch  7, batch    69 | loss: 91.7996061CurrentTrain: epoch  7, batch    70 | loss: 60.3096740CurrentTrain: epoch  7, batch    71 | loss: 92.6825387CurrentTrain: epoch  7, batch    72 | loss: 72.7608237CurrentTrain: epoch  7, batch    73 | loss: 73.0756129CurrentTrain: epoch  7, batch    74 | loss: 86.7960316CurrentTrain: epoch  7, batch    75 | loss: 64.8087523CurrentTrain: epoch  7, batch    76 | loss: 72.8632825CurrentTrain: epoch  7, batch    77 | loss: 90.1631073CurrentTrain: epoch  7, batch    78 | loss: 89.9653396CurrentTrain: epoch  7, batch    79 | loss: 78.0359944CurrentTrain: epoch  7, batch    80 | loss: 59.6806246CurrentTrain: epoch  7, batch    81 | loss: 77.0772072CurrentTrain: epoch  7, batch    82 | loss: 94.8544651CurrentTrain: epoch  7, batch    83 | loss: 60.6954491CurrentTrain: epoch  7, batch    84 | loss: 61.5780818CurrentTrain: epoch  7, batch    85 | loss: 91.4021649CurrentTrain: epoch  7, batch    86 | loss: 72.6774991CurrentTrain: epoch  7, batch    87 | loss: 91.1253718CurrentTrain: epoch  7, batch    88 | loss: 92.3458030CurrentTrain: epoch  7, batch    89 | loss: 87.3170020CurrentTrain: epoch  7, batch    90 | loss: 76.0469377CurrentTrain: epoch  7, batch    91 | loss: 92.6654458CurrentTrain: epoch  7, batch    92 | loss: 71.0762136CurrentTrain: epoch  7, batch    93 | loss: 71.9941594CurrentTrain: epoch  7, batch    94 | loss: 86.6176357CurrentTrain: epoch  7, batch    95 | loss: 58.0700505CurrentTrain: epoch  8, batch     0 | loss: 73.2137723CurrentTrain: epoch  8, batch     1 | loss: 62.4886279CurrentTrain: epoch  8, batch     2 | loss: 63.3424511CurrentTrain: epoch  8, batch     3 | loss: 114.2984361CurrentTrain: epoch  8, batch     4 | loss: 70.5616207CurrentTrain: epoch  8, batch     5 | loss: 70.8540772CurrentTrain: epoch  8, batch     6 | loss: 57.3974967CurrentTrain: epoch  8, batch     7 | loss: 53.0924165CurrentTrain: epoch  8, batch     8 | loss: 53.6463552CurrentTrain: epoch  8, batch     9 | loss: 64.2022572CurrentTrain: epoch  8, batch    10 | loss: 92.9469913CurrentTrain: epoch  8, batch    11 | loss: 61.3019893CurrentTrain: epoch  8, batch    12 | loss: 74.8882410CurrentTrain: epoch  8, batch    13 | loss: 74.6021472CurrentTrain: epoch  8, batch    14 | loss: 73.1716283CurrentTrain: epoch  8, batch    15 | loss: 111.5844904CurrentTrain: epoch  8, batch    16 | loss: 113.6174401CurrentTrain: epoch  8, batch    17 | loss: 56.0791217CurrentTrain: epoch  8, batch    18 | loss: 59.3319232CurrentTrain: epoch  8, batch    19 | loss: 50.4815481CurrentTrain: epoch  8, batch    20 | loss: 59.5952911CurrentTrain: epoch  8, batch    21 | loss: 86.6305179CurrentTrain: epoch  8, batch    22 | loss: 72.7718816CurrentTrain: epoch  8, batch    23 | loss: 88.1747107CurrentTrain: epoch  8, batch    24 | loss: 119.2005577CurrentTrain: epoch  8, batch    25 | loss: 58.7446352CurrentTrain: epoch  8, batch    26 | loss: 69.7882158CurrentTrain: epoch  8, batch    27 | loss: 48.6968489CurrentTrain: epoch  8, batch    28 | loss: 70.1955454CurrentTrain: epoch  8, batch    29 | loss: 59.7844351CurrentTrain: epoch  8, batch    30 | loss: 116.0338177CurrentTrain: epoch  8, batch    31 | loss: 86.4190329CurrentTrain: epoch  8, batch    32 | loss: 69.0395952CurrentTrain: epoch  8, batch    33 | loss: 74.3291133CurrentTrain: epoch  8, batch    34 | loss: 73.2270716CurrentTrain: epoch  8, batch    35 | loss: 58.0781134CurrentTrain: epoch  8, batch    36 | loss: 70.4953249CurrentTrain: epoch  8, batch    37 | loss: 62.9287672CurrentTrain: epoch  8, batch    38 | loss: 67.0990249CurrentTrain: epoch  8, batch    39 | loss: 94.6612316CurrentTrain: epoch  8, batch    40 | loss: 61.7975793CurrentTrain: epoch  8, batch    41 | loss: 111.3314517CurrentTrain: epoch  8, batch    42 | loss: 75.2169486CurrentTrain: epoch  8, batch    43 | loss: 53.0845076CurrentTrain: epoch  8, batch    44 | loss: 51.3406548CurrentTrain: epoch  8, batch    45 | loss: 76.5916905CurrentTrain: epoch  8, batch    46 | loss: 115.3031990CurrentTrain: epoch  8, batch    47 | loss: 92.3222294CurrentTrain: epoch  8, batch    48 | loss: 49.1666669CurrentTrain: epoch  8, batch    49 | loss: 60.9931413CurrentTrain: epoch  8, batch    50 | loss: 55.3865220CurrentTrain: epoch  8, batch    51 | loss: 61.0857687CurrentTrain: epoch  8, batch    52 | loss: 70.4417785CurrentTrain: epoch  8, batch    53 | loss: 58.9705912CurrentTrain: epoch  8, batch    54 | loss: 85.5176941CurrentTrain: epoch  8, batch    55 | loss: 245.5549569CurrentTrain: epoch  8, batch    56 | loss: 70.9746126CurrentTrain: epoch  8, batch    57 | loss: 108.4216258CurrentTrain: epoch  8, batch    58 | loss: 71.1774645CurrentTrain: epoch  8, batch    59 | loss: 60.3457997CurrentTrain: epoch  8, batch    60 | loss: 75.4640318CurrentTrain: epoch  8, batch    61 | loss: 60.0537353CurrentTrain: epoch  8, batch    62 | loss: 72.1012220CurrentTrain: epoch  8, batch    63 | loss: 90.0954796CurrentTrain: epoch  8, batch    64 | loss: 71.6339778CurrentTrain: epoch  8, batch    65 | loss: 112.6817073CurrentTrain: epoch  8, batch    66 | loss: 78.4195325CurrentTrain: epoch  8, batch    67 | loss: 70.6070903CurrentTrain: epoch  8, batch    68 | loss: 114.6755801CurrentTrain: epoch  8, batch    69 | loss: 55.7229581CurrentTrain: epoch  8, batch    70 | loss: 116.5161978CurrentTrain: epoch  8, batch    71 | loss: 71.8020924CurrentTrain: epoch  8, batch    72 | loss: 57.5398749CurrentTrain: epoch  8, batch    73 | loss: 85.2138058CurrentTrain: epoch  8, batch    74 | loss: 74.2799282CurrentTrain: epoch  8, batch    75 | loss: 88.9778648CurrentTrain: epoch  8, batch    76 | loss: 59.9093025CurrentTrain: epoch  8, batch    77 | loss: 89.5130570CurrentTrain: epoch  8, batch    78 | loss: 155.1221220CurrentTrain: epoch  8, batch    79 | loss: 68.5690187CurrentTrain: epoch  8, batch    80 | loss: 114.1408820CurrentTrain: epoch  8, batch    81 | loss: 67.5741391CurrentTrain: epoch  8, batch    82 | loss: 60.5208945CurrentTrain: epoch  8, batch    83 | loss: 68.7606160CurrentTrain: epoch  8, batch    84 | loss: 89.4919736CurrentTrain: epoch  8, batch    85 | loss: 73.5709369CurrentTrain: epoch  8, batch    86 | loss: 114.6194742CurrentTrain: epoch  8, batch    87 | loss: 71.4109798CurrentTrain: epoch  8, batch    88 | loss: 64.9127325CurrentTrain: epoch  8, batch    89 | loss: 77.1274003CurrentTrain: epoch  8, batch    90 | loss: 90.3155470CurrentTrain: epoch  8, batch    91 | loss: 74.8844891CurrentTrain: epoch  8, batch    92 | loss: 72.1408687CurrentTrain: epoch  8, batch    93 | loss: 89.1039987CurrentTrain: epoch  8, batch    94 | loss: 74.8085167CurrentTrain: epoch  8, batch    95 | loss: 48.4596842CurrentTrain: epoch  9, batch     0 | loss: 72.5032318CurrentTrain: epoch  9, batch     1 | loss: 74.2597809CurrentTrain: epoch  9, batch     2 | loss: 89.5449754CurrentTrain: epoch  9, batch     3 | loss: 113.9340799CurrentTrain: epoch  9, batch     4 | loss: 60.3035486CurrentTrain: epoch  9, batch     5 | loss: 57.6585956CurrentTrain: epoch  9, batch     6 | loss: 59.9189258CurrentTrain: epoch  9, batch     7 | loss: 51.4317765CurrentTrain: epoch  9, batch     8 | loss: 58.1735109CurrentTrain: epoch  9, batch     9 | loss: 86.5175923CurrentTrain: epoch  9, batch    10 | loss: 85.9111136CurrentTrain: epoch  9, batch    11 | loss: 51.5869702CurrentTrain: epoch  9, batch    12 | loss: 90.3126417CurrentTrain: epoch  9, batch    13 | loss: 55.9130532CurrentTrain: epoch  9, batch    14 | loss: 70.4553660CurrentTrain: epoch  9, batch    15 | loss: 155.8885470CurrentTrain: epoch  9, batch    16 | loss: 63.5362792CurrentTrain: epoch  9, batch    17 | loss: 69.3295152CurrentTrain: epoch  9, batch    18 | loss: 57.6823313CurrentTrain: epoch  9, batch    19 | loss: 152.6396106CurrentTrain: epoch  9, batch    20 | loss: 61.9975823CurrentTrain: epoch  9, batch    21 | loss: 85.1046446CurrentTrain: epoch  9, batch    22 | loss: 53.7343285CurrentTrain: epoch  9, batch    23 | loss: 84.5461500CurrentTrain: epoch  9, batch    24 | loss: 70.9275407CurrentTrain: epoch  9, batch    25 | loss: 74.7624656CurrentTrain: epoch  9, batch    26 | loss: 60.1446460CurrentTrain: epoch  9, batch    27 | loss: 115.0873246CurrentTrain: epoch  9, batch    28 | loss: 69.0830443CurrentTrain: epoch  9, batch    29 | loss: 86.2483057CurrentTrain: epoch  9, batch    30 | loss: 72.2295429CurrentTrain: epoch  9, batch    31 | loss: 56.7607743CurrentTrain: epoch  9, batch    32 | loss: 92.8892246CurrentTrain: epoch  9, batch    33 | loss: 93.1113128CurrentTrain: epoch  9, batch    34 | loss: 66.6439653CurrentTrain: epoch  9, batch    35 | loss: 115.3798537CurrentTrain: epoch  9, batch    36 | loss: 57.8986239CurrentTrain: epoch  9, batch    37 | loss: 76.0032409CurrentTrain: epoch  9, batch    38 | loss: 65.0156740CurrentTrain: epoch  9, batch    39 | loss: 71.0034755CurrentTrain: epoch  9, batch    40 | loss: 83.4364187CurrentTrain: epoch  9, batch    41 | loss: 55.2170778CurrentTrain: epoch  9, batch    42 | loss: 83.1614340CurrentTrain: epoch  9, batch    43 | loss: 86.4718201CurrentTrain: epoch  9, batch    44 | loss: 114.4803082CurrentTrain: epoch  9, batch    45 | loss: 69.8526557CurrentTrain: epoch  9, batch    46 | loss: 75.1970996CurrentTrain: epoch  9, batch    47 | loss: 93.9574093CurrentTrain: epoch  9, batch    48 | loss: 118.2862710CurrentTrain: epoch  9, batch    49 | loss: 69.3939949CurrentTrain: epoch  9, batch    50 | loss: 74.3398751CurrentTrain: epoch  9, batch    51 | loss: 53.2062317CurrentTrain: epoch  9, batch    52 | loss: 59.5106854CurrentTrain: epoch  9, batch    53 | loss: 62.4494053CurrentTrain: epoch  9, batch    54 | loss: 92.8367674CurrentTrain: epoch  9, batch    55 | loss: 56.2327859CurrentTrain: epoch  9, batch    56 | loss: 89.7390832CurrentTrain: epoch  9, batch    57 | loss: 57.8969584CurrentTrain: epoch  9, batch    58 | loss: 64.4588052CurrentTrain: epoch  9, batch    59 | loss: 83.4269783CurrentTrain: epoch  9, batch    60 | loss: 57.0210259CurrentTrain: epoch  9, batch    61 | loss: 68.3578243CurrentTrain: epoch  9, batch    62 | loss: 70.3693220CurrentTrain: epoch  9, batch    63 | loss: 70.5611135CurrentTrain: epoch  9, batch    64 | loss: 59.1050060CurrentTrain: epoch  9, batch    65 | loss: 51.7295435CurrentTrain: epoch  9, batch    66 | loss: 86.0563410CurrentTrain: epoch  9, batch    67 | loss: 72.0770492CurrentTrain: epoch  9, batch    68 | loss: 72.7419483CurrentTrain: epoch  9, batch    69 | loss: 71.9669860CurrentTrain: epoch  9, batch    70 | loss: 59.7954270CurrentTrain: epoch  9, batch    71 | loss: 117.4087890CurrentTrain: epoch  9, batch    72 | loss: 114.5534038CurrentTrain: epoch  9, batch    73 | loss: 86.6562274CurrentTrain: epoch  9, batch    74 | loss: 72.1360275CurrentTrain: epoch  9, batch    75 | loss: 82.7027520CurrentTrain: epoch  9, batch    76 | loss: 70.5884795CurrentTrain: epoch  9, batch    77 | loss: 52.7495351CurrentTrain: epoch  9, batch    78 | loss: 68.9075515CurrentTrain: epoch  9, batch    79 | loss: 63.7138782CurrentTrain: epoch  9, batch    80 | loss: 70.4273950CurrentTrain: epoch  9, batch    81 | loss: 59.8545869CurrentTrain: epoch  9, batch    82 | loss: 64.5144489CurrentTrain: epoch  9, batch    83 | loss: 70.6893381CurrentTrain: epoch  9, batch    84 | loss: 89.1624864CurrentTrain: epoch  9, batch    85 | loss: 96.3327411CurrentTrain: epoch  9, batch    86 | loss: 60.2750004CurrentTrain: epoch  9, batch    87 | loss: 58.0913615CurrentTrain: epoch  9, batch    88 | loss: 86.3605039CurrentTrain: epoch  9, batch    89 | loss: 73.3508743CurrentTrain: epoch  9, batch    90 | loss: 72.7990378CurrentTrain: epoch  9, batch    91 | loss: 60.8864833CurrentTrain: epoch  9, batch    92 | loss: 69.9318760CurrentTrain: epoch  9, batch    93 | loss: 71.5894798CurrentTrain: epoch  9, batch    94 | loss: 74.9538438CurrentTrain: epoch  9, batch    95 | loss: 74.3270773

F1 score per class: {32: 0.59375, 6: 0.8130841121495327, 19: 0.36363636363636365, 24: 0.75, 26: 0.9393939393939394, 29: 0.7796610169491526}
Micro-average F1 score: 0.7643192488262911
Weighted-average F1 score: 0.7675788639950328
F1 score per class: {32: 0.6413502109704642, 6: 0.7848101265822784, 19: 0.25925925925925924, 24: 0.7311827956989247, 26: 0.9463414634146341, 29: 0.8202764976958525}
Micro-average F1 score: 0.7570422535211268
Weighted-average F1 score: 0.749089359056825
F1 score per class: {32: 0.6437768240343348, 6: 0.8, 19: 0.3111111111111111, 24: 0.7311827956989247, 26: 0.9458128078817734, 29: 0.8108108108108109}
Micro-average F1 score: 0.7649687220732797
Weighted-average F1 score: 0.7608014578326384

F1 score per class: {32: 0.59375, 6: 0.8130841121495327, 19: 0.36363636363636365, 24: 0.75, 26: 0.9393939393939394, 29: 0.7796610169491526}
Micro-average F1 score: 0.7643192488262911
Weighted-average F1 score: 0.7675788639950328
F1 score per class: {32: 0.6413502109704642, 6: 0.7848101265822784, 19: 0.25925925925925924, 24: 0.7311827956989247, 26: 0.9463414634146341, 29: 0.8202764976958525}
Micro-average F1 score: 0.7570422535211268
Weighted-average F1 score: 0.749089359056825
F1 score per class: {32: 0.6437768240343348, 6: 0.8, 19: 0.3111111111111111, 24: 0.7311827956989247, 26: 0.9458128078817734, 29: 0.8108108108108109}
Micro-average F1 score: 0.7649687220732797
Weighted-average F1 score: 0.7608014578326384

F1 score per class: {32: 0.42379182156133827, 6: 0.7631578947368421, 19: 0.23529411764705882, 24: 0.6857142857142857, 26: 0.8691588785046729, 29: 0.5542168674698795}
Micro-average F1 score: 0.6242331288343558
Weighted-average F1 score: 0.6108915153736019
F1 score per class: {32: 0.428169014084507, 6: 0.7045454545454546, 19: 0.13725490196078433, 24: 0.6732673267326733, 26: 0.8622222222222222, 29: 0.644927536231884}
Micro-average F1 score: 0.6039325842696629
Weighted-average F1 score: 0.5808126450143839
F1 score per class: {32: 0.4297994269340974, 6: 0.7215686274509804, 19: 0.15555555555555556, 24: 0.6732673267326733, 26: 0.8648648648648649, 29: 0.6382978723404256}
Micro-average F1 score: 0.6114285714285714
Weighted-average F1 score: 0.5900455648159817

F1 score per class: {32: 0.42379182156133827, 6: 0.7631578947368421, 19: 0.23529411764705882, 24: 0.6857142857142857, 26: 0.8691588785046729, 29: 0.5542168674698795}
Micro-average F1 score: 0.6242331288343558
Weighted-average F1 score: 0.6108915153736019
F1 score per class: {32: 0.428169014084507, 6: 0.7045454545454546, 19: 0.13725490196078433, 24: 0.6732673267326733, 26: 0.8622222222222222, 29: 0.644927536231884}
Micro-average F1 score: 0.6039325842696629
Weighted-average F1 score: 0.5808126450143839
F1 score per class: {32: 0.4297994269340974, 6: 0.7215686274509804, 19: 0.15555555555555556, 24: 0.6732673267326733, 26: 0.8648648648648649, 29: 0.6382978723404256}
Micro-average F1 score: 0.6114285714285714
Weighted-average F1 score: 0.5900455648159817
cur_acc_wo_na:  ['0.7643']
his_acc_wo_na:  ['0.7643']
cur_acc des_wo_na:  ['0.7570']
his_acc des_wo_na:  ['0.7570']
cur_acc rrf_wo_na:  ['0.7650']
his_acc rrf_wo_na:  ['0.7650']
cur_acc_w_na:  ['0.6242']
his_acc_w_na:  ['0.6242']
cur_acc des_w_na:  ['0.6039']
his_acc des_w_na:  ['0.6039']
cur_acc rrf_w_na:  ['0.6114']
his_acc rrf_w_na:  ['0.6114']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 80.3012737CurrentTrain: epoch  0, batch     1 | loss: 140.6503802CurrentTrain: epoch  0, batch     2 | loss: 72.9662933CurrentTrain: epoch  0, batch     3 | loss: 10.8415946CurrentTrain: epoch  1, batch     0 | loss: 80.9471870CurrentTrain: epoch  1, batch     1 | loss: 80.8408265CurrentTrain: epoch  1, batch     2 | loss: 80.1941644CurrentTrain: epoch  1, batch     3 | loss: 11.9604046CurrentTrain: epoch  2, batch     0 | loss: 67.1638575CurrentTrain: epoch  2, batch     1 | loss: 75.2430713CurrentTrain: epoch  2, batch     2 | loss: 66.4198361CurrentTrain: epoch  2, batch     3 | loss: 8.6542010CurrentTrain: epoch  3, batch     0 | loss: 74.4811692CurrentTrain: epoch  3, batch     1 | loss: 79.0548071CurrentTrain: epoch  3, batch     2 | loss: 62.0459844CurrentTrain: epoch  3, batch     3 | loss: 17.4567427CurrentTrain: epoch  4, batch     0 | loss: 73.7088720CurrentTrain: epoch  4, batch     1 | loss: 73.2041230CurrentTrain: epoch  4, batch     2 | loss: 71.9696165CurrentTrain: epoch  4, batch     3 | loss: 15.1855299CurrentTrain: epoch  5, batch     0 | loss: 71.6992967CurrentTrain: epoch  5, batch     1 | loss: 61.0676330CurrentTrain: epoch  5, batch     2 | loss: 61.7818928CurrentTrain: epoch  5, batch     3 | loss: 9.1095370CurrentTrain: epoch  6, batch     0 | loss: 61.2622456CurrentTrain: epoch  6, batch     1 | loss: 58.2933504CurrentTrain: epoch  6, batch     2 | loss: 60.1317671CurrentTrain: epoch  6, batch     3 | loss: 15.3861367CurrentTrain: epoch  7, batch     0 | loss: 59.8854087CurrentTrain: epoch  7, batch     1 | loss: 70.4230175CurrentTrain: epoch  7, batch     2 | loss: 83.7569744CurrentTrain: epoch  7, batch     3 | loss: 16.9477413CurrentTrain: epoch  8, batch     0 | loss: 55.9179557CurrentTrain: epoch  8, batch     1 | loss: 110.0471545CurrentTrain: epoch  8, batch     2 | loss: 58.8030645CurrentTrain: epoch  8, batch     3 | loss: 8.2900000CurrentTrain: epoch  9, batch     0 | loss: 57.2860046CurrentTrain: epoch  9, batch     1 | loss: 56.8756237CurrentTrain: epoch  9, batch     2 | loss: 69.5803846CurrentTrain: epoch  9, batch     3 | loss: 4.8007484
MemoryTrain:  epoch  0, batch     0 | loss: 2.3379346MemoryTrain:  epoch  1, batch     0 | loss: 1.7709828MemoryTrain:  epoch  2, batch     0 | loss: 1.5595697MemoryTrain:  epoch  3, batch     0 | loss: 1.3070604MemoryTrain:  epoch  4, batch     0 | loss: 1.1293725MemoryTrain:  epoch  5, batch     0 | loss: 0.8616470MemoryTrain:  epoch  6, batch     0 | loss: 0.7179845MemoryTrain:  epoch  7, batch     0 | loss: 0.6896868MemoryTrain:  epoch  8, batch     0 | loss: 0.5401941MemoryTrain:  epoch  9, batch     0 | loss: 0.4003215

F1 score per class: {32: 0.0, 6: 0.6666666666666666, 7: 0.78125, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.7142857142857143, 26: 0.0, 27: 0.5, 29: 0.0, 31: 0.12698412698412698}
Micro-average F1 score: 0.30718954248366015
Weighted-average F1 score: 0.26788297784391535
F1 score per class: {32: 0.0, 6: 0.5454545454545454, 7: 0.6578947368421053, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.64, 26: 0.0, 27: 0.6666666666666666, 29: 0.0, 31: 0.18867924528301888}
Micro-average F1 score: 0.3137254901960784
Weighted-average F1 score: 0.27960386511590585
F1 score per class: {32: 0.0, 6: 0.6, 7: 0.7352941176470589, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.6923076923076923, 26: 0.0, 27: 0.6666666666666666, 29: 0.0, 31: 0.18518518518518517}
Micro-average F1 score: 0.3277591973244147
Weighted-average F1 score: 0.2862509568391921

F1 score per class: {32: 0.40993788819875776, 6: 0.04918032786885246, 7: 0.78125, 40: 0.6209386281588448, 9: 0.27586206896551724, 19: 0.6863905325443787, 24: 0.46511627906976744, 26: 0.9320388349514563, 27: 0.3333333333333333, 29: 0.8155339805825242, 31: 0.04804804804804805}
Micro-average F1 score: 0.504950495049505
Weighted-average F1 score: 0.4410687490994302
F1 score per class: {32: 0.4172661870503597, 6: 0.05084745762711865, 7: 0.5494505494505495, 40: 0.5686274509803921, 9: 0.20408163265306123, 19: 0.7395833333333334, 24: 0.3404255319148936, 26: 0.926829268292683, 27: 0.3333333333333333, 29: 0.8173076923076923, 31: 0.09950248756218906}
Micro-average F1 score: 0.5357142857142857
Weighted-average F1 score: 0.49462266891683737
F1 score per class: {32: 0.4084507042253521, 6: 0.05217391304347826, 7: 0.7246376811594203, 40: 0.58, 9: 0.2, 19: 0.7301587301587301, 24: 0.3829787234042553, 26: 0.926829268292683, 27: 0.4444444444444444, 29: 0.821256038647343, 31: 0.08968609865470852}
Micro-average F1 score: 0.5407503234152652
Weighted-average F1 score: 0.4974428988955825

F1 score per class: {32: 0.0, 6: 0.5, 7: 0.7246376811594203, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.6666666666666666, 26: 0.0, 27: 0.5, 29: 0.0, 31: 0.1111111111111111}
Micro-average F1 score: 0.27089337175792505
Weighted-average F1 score: 0.2379278887022869
F1 score per class: {32: 0.0, 6: 0.42857142857142855, 7: 0.5747126436781609, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.5925925925925926, 26: 0.0, 27: 0.4444444444444444, 29: 0.0, 31: 0.1694915254237288}
Micro-average F1 score: 0.2696629213483146
Weighted-average F1 score: 0.2429611479343162
F1 score per class: {32: 0.0, 6: 0.5, 7: 0.6756756756756757, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.6428571428571429, 26: 0.0, 27: 0.5, 29: 0.0, 31: 0.16260162601626016}
Micro-average F1 score: 0.28654970760233917
Weighted-average F1 score: 0.2522735921323855

F1 score per class: {32: 0.2796610169491525, 6: 0.030456852791878174, 7: 0.7246376811594203, 40: 0.581081081081081, 9: 0.18181818181818182, 19: 0.6373626373626373, 24: 0.3448275862068966, 26: 0.8311688311688312, 27: 0.2222222222222222, 29: 0.6536964980544747, 31: 0.036036036036036036}
Micro-average F1 score: 0.40336134453781514
Weighted-average F1 score: 0.3508696971789836
F1 score per class: {32: 0.31693989071038253, 6: 0.02926829268292683, 7: 0.46296296296296297, 40: 0.5209580838323353, 9: 0.11494252873563218, 19: 0.6729857819905213, 24: 0.2711864406779661, 26: 0.8333333333333334, 27: 0.2, 29: 0.6367041198501873, 31: 0.0784313725490196}
Micro-average F1 score: 0.4292284108329075
Weighted-average F1 score: 0.3912428512818748
F1 score per class: {32: 0.30851063829787234, 6: 0.029556650246305417, 7: 0.6666666666666666, 40: 0.5370370370370371, 9: 0.1111111111111111, 19: 0.6798029556650246, 24: 0.3103448275862069, 26: 0.8296943231441049, 27: 0.2857142857142857, 29: 0.6463878326996197, 31: 0.06968641114982578}
Micro-average F1 score: 0.4363256784968685
Weighted-average F1 score: 0.39488396997105285
cur_acc_wo_na:  ['0.7643', '0.3072']
his_acc_wo_na:  ['0.7643', '0.5050']
cur_acc des_wo_na:  ['0.7570', '0.3137']
his_acc des_wo_na:  ['0.7570', '0.5357']
cur_acc rrf_wo_na:  ['0.7650', '0.3278']
his_acc rrf_wo_na:  ['0.7650', '0.5408']
cur_acc_w_na:  ['0.6242', '0.2709']
his_acc_w_na:  ['0.6242', '0.4034']
cur_acc des_w_na:  ['0.6039', '0.2697']
his_acc des_w_na:  ['0.6039', '0.4292']
cur_acc rrf_w_na:  ['0.6114', '0.2865']
his_acc rrf_w_na:  ['0.6114', '0.4363']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 102.0458949CurrentTrain: epoch  0, batch     1 | loss: 102.2821356CurrentTrain: epoch  0, batch     2 | loss: 89.4704146CurrentTrain: epoch  0, batch     3 | loss: 90.5540842CurrentTrain: epoch  1, batch     0 | loss: 81.0406351CurrentTrain: epoch  1, batch     1 | loss: 100.3645251CurrentTrain: epoch  1, batch     2 | loss: 77.8130298CurrentTrain: epoch  1, batch     3 | loss: 135.9981670CurrentTrain: epoch  2, batch     0 | loss: 89.2591143CurrentTrain: epoch  2, batch     1 | loss: 122.0425633CurrentTrain: epoch  2, batch     2 | loss: 83.3804199CurrentTrain: epoch  2, batch     3 | loss: 64.7534942CurrentTrain: epoch  3, batch     0 | loss: 68.9550138CurrentTrain: epoch  3, batch     1 | loss: 126.2712510CurrentTrain: epoch  3, batch     2 | loss: 69.7010042CurrentTrain: epoch  3, batch     3 | loss: 53.9401464CurrentTrain: epoch  4, batch     0 | loss: 79.1157302CurrentTrain: epoch  4, batch     1 | loss: 81.0186741CurrentTrain: epoch  4, batch     2 | loss: 66.4559890CurrentTrain: epoch  4, batch     3 | loss: 80.8996841CurrentTrain: epoch  5, batch     0 | loss: 77.0759595CurrentTrain: epoch  5, batch     1 | loss: 72.6969002CurrentTrain: epoch  5, batch     2 | loss: 78.3276340CurrentTrain: epoch  5, batch     3 | loss: 74.1939010CurrentTrain: epoch  6, batch     0 | loss: 76.9798225CurrentTrain: epoch  6, batch     1 | loss: 89.5272198CurrentTrain: epoch  6, batch     2 | loss: 90.2035380CurrentTrain: epoch  6, batch     3 | loss: 60.8215441CurrentTrain: epoch  7, batch     0 | loss: 63.3806692CurrentTrain: epoch  7, batch     1 | loss: 92.0779181CurrentTrain: epoch  7, batch     2 | loss: 78.5521607CurrentTrain: epoch  7, batch     3 | loss: 56.7255401CurrentTrain: epoch  8, batch     0 | loss: 60.2139610CurrentTrain: epoch  8, batch     1 | loss: 88.4067809CurrentTrain: epoch  8, batch     2 | loss: 94.1075625CurrentTrain: epoch  8, batch     3 | loss: 72.7502255CurrentTrain: epoch  9, batch     0 | loss: 70.1399046CurrentTrain: epoch  9, batch     1 | loss: 112.0880900CurrentTrain: epoch  9, batch     2 | loss: 88.0322597CurrentTrain: epoch  9, batch     3 | loss: 50.3531998
MemoryTrain:  epoch  0, batch     0 | loss: 1.6114762MemoryTrain:  epoch  1, batch     0 | loss: 1.3791548MemoryTrain:  epoch  2, batch     0 | loss: 1.0873763MemoryTrain:  epoch  3, batch     0 | loss: 0.9013738MemoryTrain:  epoch  4, batch     0 | loss: 0.7145163MemoryTrain:  epoch  5, batch     0 | loss: 0.5863470MemoryTrain:  epoch  6, batch     0 | loss: 0.4342518MemoryTrain:  epoch  7, batch     0 | loss: 0.3885284MemoryTrain:  epoch  8, batch     0 | loss: 0.3444833MemoryTrain:  epoch  9, batch     0 | loss: 0.2775490

F1 score per class: {0: 0.8089887640449438, 32: 0.8888888888888888, 4: 0.0, 6: 0.0, 7: 0.19230769230769232, 40: 0.0, 13: 0.46, 19: 0.717948717948718, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0}
Micro-average F1 score: 0.5920826161790017
Weighted-average F1 score: 0.4881257579953462
F1 score per class: {0: 0.7368421052631579, 32: 0.9333333333333333, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.15384615384615385, 9: 0.0, 13: 0.4946236559139785, 19: 0.6153846153846154, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5818181818181818
Weighted-average F1 score: 0.48409835778256827
F1 score per class: {0: 0.7368421052631579, 32: 0.9424083769633508, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.15, 9: 0.0, 13: 0.48936170212765956, 19: 0.6, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5823627287853578
Weighted-average F1 score: 0.4839259623175083

F1 score per class: {32: 0.48322147651006714, 0: 0.8888888888888888, 4: 0.44692737430167595, 6: 0.07058823529411765, 7: 0.7936507936507936, 40: 0.08130081300813008, 9: 0.6377952755905512, 13: 0.20087336244541484, 19: 0.6746987951807228, 21: 0.0, 23: 0.7103825136612022, 24: 0.3684210526315789, 26: 0.8857142857142857, 27: 0.6666666666666666, 29: 0.7892376681614349, 31: 0.13934426229508196}
Micro-average F1 score: 0.5225066195939982
Weighted-average F1 score: 0.46343402008343737
F1 score per class: {32: 0.45751633986928103, 0: 0.9333333333333333, 4: 0.48205128205128206, 6: 0.07142857142857142, 7: 0.5747126436781609, 40: 0.075, 9: 0.5705128205128205, 13: 0.26744186046511625, 19: 0.5714285714285714, 21: 0.16, 23: 0.6666666666666666, 24: 0.30434782608695654, 26: 0.8715596330275229, 27: 0.2857142857142857, 29: 0.7372881355932204, 31: 0.10679611650485436}
Micro-average F1 score: 0.5298184961106309
Weighted-average F1 score: 0.4907237414152711
F1 score per class: {32: 0.44871794871794873, 0: 0.9424083769633508, 4: 0.48704663212435234, 6: 0.07317073170731707, 7: 0.746268656716418, 40: 0.06896551724137931, 9: 0.6027397260273972, 13: 0.24468085106382978, 19: 0.5581395348837209, 21: 0.08695652173913043, 23: 0.6865671641791045, 24: 0.34146341463414637, 26: 0.8767123287671232, 27: 0.18181818181818182, 29: 0.75, 31: 0.09777777777777778}
Micro-average F1 score: 0.5318221447253705
Weighted-average F1 score: 0.4867558357942758

F1 score per class: {0: 0.75, 32: 0.8465608465608465, 4: 0.0, 6: 0.0, 7: 0.09174311926605505, 40: 0.0, 13: 0.3333333333333333, 19: 0.6292134831460674, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.442159383033419
Weighted-average F1 score: 0.3427040012659257
F1 score per class: {0: 0.660377358490566, 32: 0.875, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.07228915662650602, 9: 0.0, 13: 0.368, 19: 0.5333333333333333, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.42718446601941745
Weighted-average F1 score: 0.3364733862351819
F1 score per class: {0: 0.6666666666666666, 32: 0.8910891089108911, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.06818181818181818, 9: 0.0, 13: 0.36220472440944884, 19: 0.5161290322580645, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.43103448275862066
Weighted-average F1 score: 0.3385086188826236

F1 score per class: {32: 0.36, 0: 0.8421052631578947, 4: 0.28776978417266186, 6: 0.047244094488188976, 7: 0.7352941176470589, 40: 0.03663003663003663, 9: 0.5912408759124088, 13: 0.1270718232044199, 19: 0.5833333333333334, 21: 0.0, 23: 0.6532663316582915, 24: 0.21875, 26: 0.7782426778242678, 27: 0.4, 29: 0.5751633986928104, 31: 0.11888111888111888}
Micro-average F1 score: 0.39611910337905654
Weighted-average F1 score: 0.3440714037781151
F1 score per class: {32: 0.33175355450236965, 0: 0.8666666666666667, 4: 0.30618892508143325, 6: 0.04411764705882353, 7: 0.4807692307692308, 40: 0.031914893617021274, 9: 0.5056818181818182, 13: 0.16606498194945848, 19: 0.4948453608247423, 21: 0.11764705882352941, 23: 0.5974025974025974, 24: 0.2028985507246377, 26: 0.7450980392156863, 27: 0.19047619047619047, 29: 0.5072886297376094, 31: 0.07971014492753623}
Micro-average F1 score: 0.3940855030536805
Weighted-average F1 score: 0.3581451842841909
F1 score per class: {32: 0.32558139534883723, 0: 0.8866995073891626, 4: 0.31333333333333335, 6: 0.04580152671755725, 7: 0.684931506849315, 40: 0.029556650246305417, 9: 0.546583850931677, 13: 0.15181518151815182, 19: 0.4752475247524752, 21: 0.06666666666666667, 23: 0.6301369863013698, 24: 0.21875, 26: 0.75, 27: 0.125, 29: 0.5288753799392097, 31: 0.0738255033557047}
Micro-average F1 score: 0.39830231798889976
Weighted-average F1 score: 0.3563314265043416
cur_acc_wo_na:  ['0.7643', '0.3072', '0.5921']
his_acc_wo_na:  ['0.7643', '0.5050', '0.5225']
cur_acc des_wo_na:  ['0.7570', '0.3137', '0.5818']
his_acc des_wo_na:  ['0.7570', '0.5357', '0.5298']
cur_acc rrf_wo_na:  ['0.7650', '0.3278', '0.5824']
his_acc rrf_wo_na:  ['0.7650', '0.5408', '0.5318']
cur_acc_w_na:  ['0.6242', '0.2709', '0.4422']
his_acc_w_na:  ['0.6242', '0.4034', '0.3961']
cur_acc des_w_na:  ['0.6039', '0.2697', '0.4272']
his_acc des_w_na:  ['0.6039', '0.4292', '0.3941']
cur_acc rrf_w_na:  ['0.6114', '0.2865', '0.4310']
his_acc rrf_w_na:  ['0.6114', '0.4363', '0.3983']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 110.4406500CurrentTrain: epoch  0, batch     1 | loss: 111.0725110CurrentTrain: epoch  0, batch     2 | loss: 142.5704391CurrentTrain: epoch  0, batch     3 | loss: 133.7528766CurrentTrain: epoch  0, batch     4 | loss: 50.8816243CurrentTrain: epoch  1, batch     0 | loss: 110.0737278CurrentTrain: epoch  1, batch     1 | loss: 127.6346129CurrentTrain: epoch  1, batch     2 | loss: 87.9744958CurrentTrain: epoch  1, batch     3 | loss: 83.9113807CurrentTrain: epoch  1, batch     4 | loss: 77.2737068CurrentTrain: epoch  2, batch     0 | loss: 128.1932424CurrentTrain: epoch  2, batch     1 | loss: 80.8030604CurrentTrain: epoch  2, batch     2 | loss: 81.2120327CurrentTrain: epoch  2, batch     3 | loss: 80.9650610CurrentTrain: epoch  2, batch     4 | loss: 59.1907382CurrentTrain: epoch  3, batch     0 | loss: 78.5142753CurrentTrain: epoch  3, batch     1 | loss: 76.0268388CurrentTrain: epoch  3, batch     2 | loss: 126.3780732CurrentTrain: epoch  3, batch     3 | loss: 76.3951276CurrentTrain: epoch  3, batch     4 | loss: 109.3642651CurrentTrain: epoch  4, batch     0 | loss: 121.1228796CurrentTrain: epoch  4, batch     1 | loss: 75.8654660CurrentTrain: epoch  4, batch     2 | loss: 93.1929145CurrentTrain: epoch  4, batch     3 | loss: 121.3880827CurrentTrain: epoch  4, batch     4 | loss: 56.0520624CurrentTrain: epoch  5, batch     0 | loss: 95.9314306CurrentTrain: epoch  5, batch     1 | loss: 90.0845987CurrentTrain: epoch  5, batch     2 | loss: 89.8183762CurrentTrain: epoch  5, batch     3 | loss: 92.2854843CurrentTrain: epoch  5, batch     4 | loss: 97.5781199CurrentTrain: epoch  6, batch     0 | loss: 73.7330080CurrentTrain: epoch  6, batch     1 | loss: 93.4273755CurrentTrain: epoch  6, batch     2 | loss: 115.5119748CurrentTrain: epoch  6, batch     3 | loss: 73.2792845CurrentTrain: epoch  6, batch     4 | loss: 56.5091652CurrentTrain: epoch  7, batch     0 | loss: 76.9946442CurrentTrain: epoch  7, batch     1 | loss: 61.0404678CurrentTrain: epoch  7, batch     2 | loss: 75.6713069CurrentTrain: epoch  7, batch     3 | loss: 75.3199069CurrentTrain: epoch  7, batch     4 | loss: 54.6689795CurrentTrain: epoch  8, batch     0 | loss: 113.3567650CurrentTrain: epoch  8, batch     1 | loss: 84.0955445CurrentTrain: epoch  8, batch     2 | loss: 73.3225089CurrentTrain: epoch  8, batch     3 | loss: 59.6051030CurrentTrain: epoch  8, batch     4 | loss: 100.4699929CurrentTrain: epoch  9, batch     0 | loss: 87.5359645CurrentTrain: epoch  9, batch     1 | loss: 154.7200086CurrentTrain: epoch  9, batch     2 | loss: 68.3125036CurrentTrain: epoch  9, batch     3 | loss: 85.5820809CurrentTrain: epoch  9, batch     4 | loss: 43.9836688
MemoryTrain:  epoch  0, batch     0 | loss: 1.1689194MemoryTrain:  epoch  1, batch     0 | loss: 0.9229843MemoryTrain:  epoch  2, batch     0 | loss: 0.6643458MemoryTrain:  epoch  3, batch     0 | loss: 0.5904572MemoryTrain:  epoch  4, batch     0 | loss: 0.4804309MemoryTrain:  epoch  5, batch     0 | loss: 0.4150867MemoryTrain:  epoch  6, batch     0 | loss: 0.3152394MemoryTrain:  epoch  7, batch     0 | loss: 0.2668673MemoryTrain:  epoch  8, batch     0 | loss: 0.2680002MemoryTrain:  epoch  9, batch     0 | loss: 0.1852077

F1 score per class: {0: 0.0, 32: 0.8613861386138614, 5: 0.0, 6: 0.0, 7: 0.32, 40: 0.0, 10: 0.7017543859649122, 13: 0.2, 16: 0.32653061224489793, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 29: 0.0}
Micro-average F1 score: 0.5386138613861386
Weighted-average F1 score: 0.5193324100725122
F1 score per class: {0: 0.0, 5: 0.6550522648083623, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.3492063492063492, 13: 0.0, 16: 0.6451612903225806, 17: 0.6153846153846154, 18: 0.46938775510204084, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4880239520958084
Weighted-average F1 score: 0.4676364576826897
F1 score per class: {0: 0.0, 32: 0.7258687258687259, 5: 0.0, 6: 0.0, 7: 0.0, 40: 0.3464566929133858, 10: 0.0, 9: 0.6451612903225806, 13: 0.2, 16: 0.4782608695652174, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 29: 0.0}
Micro-average F1 score: 0.5096153846153846
Weighted-average F1 score: 0.4940142736729839

F1 score per class: {0: 0.6181818181818182, 4: 0.6928104575163399, 5: 0.8571428571428571, 6: 0.4787234042553192, 7: 0.05714285714285714, 9: 0.7246376811594203, 10: 0.2857142857142857, 13: 0.12903225806451613, 16: 0.625, 17: 0.15384615384615385, 18: 0.3076923076923077, 19: 0.6332046332046332, 21: 0.26900584795321636, 23: 0.7578947368421053, 24: 0.0, 26: 0.6583850931677019, 27: 0.27692307692307694, 29: 0.8682926829268293, 31: 0.6666666666666666, 32: 0.7850467289719626, 40: 0.11406844106463879}
Micro-average F1 score: 0.5410286611700039
Weighted-average F1 score: 0.508283545551804
F1 score per class: {0: 0.5333333333333333, 4: 0.9090909090909091, 5: 0.5930599369085173, 6: 0.45, 7: 0.043478260869565216, 9: 0.5154639175257731, 10: 0.32592592592592595, 13: 0.16666666666666666, 16: 0.5555555555555556, 17: 0.2962962962962963, 18: 0.39316239316239315, 19: 0.5358255451713395, 21: 0.22826086956521738, 23: 0.6588235294117647, 24: 0.08695652173913043, 26: 0.6632124352331606, 27: 0.27692307692307694, 29: 0.8333333333333334, 31: 0.18181818181818182, 32: 0.7391304347826086, 40: 0.1187214611872146}
Micro-average F1 score: 0.5199724517906336
Weighted-average F1 score: 0.4984982997150045
F1 score per class: {0: 0.5255474452554745, 4: 0.8950276243093923, 5: 0.6836363636363636, 6: 0.46, 7: 0.03571428571428571, 9: 0.6756756756756757, 10: 0.3188405797101449, 13: 0.16, 16: 0.5633802816901409, 17: 0.1, 18: 0.44, 19: 0.5833333333333334, 21: 0.2198952879581152, 23: 0.6888888888888889, 24: 0.0, 26: 0.6629213483146067, 27: 0.2903225806451613, 29: 0.8425925925925926, 31: 0.25, 32: 0.7589285714285714, 40: 0.10196078431372549}
Micro-average F1 score: 0.5295373665480427
Weighted-average F1 score: 0.5023763954577908

F1 score per class: {0: 0.0, 4: 0.0, 5: 0.75, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.2777777777777778, 13: 0.0, 16: 0.45454545454545453, 17: 0.14285714285714285, 18: 0.2, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.40117994100294985
Weighted-average F1 score: 0.3654232445277221
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.4723618090452261, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.3013698630136986, 13: 0.0, 16: 0.43010752688172044, 17: 0.36363636363636365, 18: 0.2822085889570552, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.3316378433367243
Weighted-average F1 score: 0.31401578772774763
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.5222222222222223, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.2972972972972973, 13: 0.0, 16: 0.43956043956043955, 17: 0.1111111111111111, 18: 0.2838709677419355, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.34415584415584416
Weighted-average F1 score: 0.3261697670598507

F1 score per class: {0: 0.46258503401360546, 4: 0.6666666666666666, 5: 0.7467811158798283, 6: 0.31802120141342755, 7: 0.038834951456310676, 9: 0.6578947368421053, 10: 0.23529411764705882, 13: 0.08, 16: 0.37735849056603776, 17: 0.08, 18: 0.18823529411764706, 19: 0.5857142857142857, 21: 0.17424242424242425, 23: 0.6857142857142857, 24: 0.0, 26: 0.6057142857142858, 27: 0.16822429906542055, 29: 0.7235772357723578, 31: 0.5, 32: 0.6176470588235294, 40: 0.09404388714733543}
Micro-average F1 score: 0.42702200185931205
Weighted-average F1 score: 0.39400946720814023
F1 score per class: {0: 0.4, 4: 0.85, 5: 0.3868312757201646, 6: 0.32142857142857145, 7: 0.02564102564102564, 9: 0.4098360655737705, 10: 0.2603550295857988, 13: 0.11764705882352941, 16: 0.34782608695652173, 17: 0.1509433962264151, 18: 0.22885572139303484, 19: 0.46866485013623976, 21: 0.14334470989761092, 23: 0.5833333333333334, 24: 0.06666666666666667, 26: 0.6009389671361502, 27: 0.16666666666666666, 29: 0.6545454545454545, 31: 0.1111111111111111, 32: 0.5666666666666667, 40: 0.0896551724137931}
Micro-average F1 score: 0.38638689866939613
Weighted-average F1 score: 0.36369712668994947
F1 score per class: {0: 0.4, 4: 0.8393782383419689, 5: 0.44028103044496486, 6: 0.3262411347517731, 7: 0.022727272727272728, 9: 0.6097560975609756, 10: 0.2543352601156069, 13: 0.10526315789473684, 16: 0.35398230088495575, 17: 0.05263157894736842, 18: 0.25142857142857145, 19: 0.5153374233128835, 21: 0.13815789473684212, 23: 0.5904761904761905, 24: 0.0, 26: 0.6113989637305699, 27: 0.16981132075471697, 29: 0.6715867158671587, 31: 0.16666666666666666, 32: 0.5923344947735192, 40: 0.0783132530120482}
Micro-average F1 score: 0.3968
Weighted-average F1 score: 0.3704498838288867
cur_acc_wo_na:  ['0.7643', '0.3072', '0.5921', '0.5386']
his_acc_wo_na:  ['0.7643', '0.5050', '0.5225', '0.5410']
cur_acc des_wo_na:  ['0.7570', '0.3137', '0.5818', '0.4880']
his_acc des_wo_na:  ['0.7570', '0.5357', '0.5298', '0.5200']
cur_acc rrf_wo_na:  ['0.7650', '0.3278', '0.5824', '0.5096']
his_acc rrf_wo_na:  ['0.7650', '0.5408', '0.5318', '0.5295']
cur_acc_w_na:  ['0.6242', '0.2709', '0.4422', '0.4012']
his_acc_w_na:  ['0.6242', '0.4034', '0.3961', '0.4270']
cur_acc des_w_na:  ['0.6039', '0.2697', '0.4272', '0.3316']
his_acc des_w_na:  ['0.6039', '0.4292', '0.3941', '0.3864']
cur_acc rrf_w_na:  ['0.6114', '0.2865', '0.4310', '0.3442']
his_acc rrf_w_na:  ['0.6114', '0.4363', '0.3983', '0.3968']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 76.0952672CurrentTrain: epoch  0, batch     1 | loss: 95.6495727CurrentTrain: epoch  0, batch     2 | loss: 92.5079475CurrentTrain: epoch  0, batch     3 | loss: 58.4740121CurrentTrain: epoch  1, batch     0 | loss: 80.2337012CurrentTrain: epoch  1, batch     1 | loss: 87.0791614CurrentTrain: epoch  1, batch     2 | loss: 70.9533042CurrentTrain: epoch  1, batch     3 | loss: 47.4357445CurrentTrain: epoch  2, batch     0 | loss: 68.7387247CurrentTrain: epoch  2, batch     1 | loss: 65.1682548CurrentTrain: epoch  2, batch     2 | loss: 92.9311643CurrentTrain: epoch  2, batch     3 | loss: 113.6913966CurrentTrain: epoch  3, batch     0 | loss: 63.3372355CurrentTrain: epoch  3, batch     1 | loss: 73.7918320CurrentTrain: epoch  3, batch     2 | loss: 65.9310862CurrentTrain: epoch  3, batch     3 | loss: 84.6202085CurrentTrain: epoch  4, batch     0 | loss: 64.6852414CurrentTrain: epoch  4, batch     1 | loss: 76.8110668CurrentTrain: epoch  4, batch     2 | loss: 64.1979131CurrentTrain: epoch  4, batch     3 | loss: 40.2947614CurrentTrain: epoch  5, batch     0 | loss: 61.6477519CurrentTrain: epoch  5, batch     1 | loss: 74.6191298CurrentTrain: epoch  5, batch     2 | loss: 88.2752347CurrentTrain: epoch  5, batch     3 | loss: 49.9055902CurrentTrain: epoch  6, batch     0 | loss: 73.4044244CurrentTrain: epoch  6, batch     1 | loss: 72.5180094CurrentTrain: epoch  6, batch     2 | loss: 60.6181675CurrentTrain: epoch  6, batch     3 | loss: 60.2667432CurrentTrain: epoch  7, batch     0 | loss: 72.2685631CurrentTrain: epoch  7, batch     1 | loss: 88.6719596CurrentTrain: epoch  7, batch     2 | loss: 84.5303215CurrentTrain: epoch  7, batch     3 | loss: 46.0707824CurrentTrain: epoch  8, batch     0 | loss: 57.6199913CurrentTrain: epoch  8, batch     1 | loss: 72.8346305CurrentTrain: epoch  8, batch     2 | loss: 58.7080915CurrentTrain: epoch  8, batch     3 | loss: 58.9073902CurrentTrain: epoch  9, batch     0 | loss: 88.7710761CurrentTrain: epoch  9, batch     1 | loss: 57.8325974CurrentTrain: epoch  9, batch     2 | loss: 60.2885589CurrentTrain: epoch  9, batch     3 | loss: 53.8613122
MemoryTrain:  epoch  0, batch     0 | loss: 0.7982917MemoryTrain:  epoch  1, batch     0 | loss: 0.6611130MemoryTrain:  epoch  2, batch     0 | loss: 0.5240101MemoryTrain:  epoch  3, batch     0 | loss: 0.4566869MemoryTrain:  epoch  4, batch     0 | loss: 0.3355185MemoryTrain:  epoch  5, batch     0 | loss: 0.2715845MemoryTrain:  epoch  6, batch     0 | loss: 0.2168285MemoryTrain:  epoch  7, batch     0 | loss: 0.1865630MemoryTrain:  epoch  8, batch     0 | loss: 0.1703245MemoryTrain:  epoch  9, batch     0 | loss: 0.1684137

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.56, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4411764705882353, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.6938775510204082, 37: 0.5827814569536424, 38: 0.25, 40: 0.0}
Micro-average F1 score: 0.4306418219461698
Weighted-average F1 score: 0.3620715638657887
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.0, 15: 0.6, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5277777777777778, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.7241379310344828, 37: 0.562962962962963, 38: 0.47619047619047616, 40: 0.0}
Micro-average F1 score: 0.42513863216266173
Weighted-average F1 score: 0.33092179400495836
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.6, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4857142857142857, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.6964285714285714, 37: 0.5531914893617021, 38: 0.3888888888888889, 40: 0.0}
Micro-average F1 score: 0.41779497098646035
Weighted-average F1 score: 0.33273614477661073

F1 score per class: {0: 0.6938775510204082, 4: 0.7951807228915663, 5: 0.7591836734693878, 6: 0.46956521739130436, 7: 0.06818181818181818, 9: 0.704225352112676, 10: 0.43781094527363185, 13: 0.08695652173913043, 15: 0.20588235294117646, 16: 0.5945945945945946, 17: 0.16666666666666666, 18: 0.0425531914893617, 19: 0.5907473309608541, 21: 0.19310344827586207, 23: 0.7058823529411765, 24: 0.0, 25: 0.4411764705882353, 26: 0.6989247311827957, 27: 0.24242424242424243, 29: 0.8240740740740741, 31: 0.6666666666666666, 32: 0.7555555555555555, 35: 0.4358974358974359, 37: 0.17221135029354206, 38: 0.22857142857142856, 40: 0.15315315315315314}
Micro-average F1 score: 0.47207409486387875
Weighted-average F1 score: 0.4320497126778951
F1 score per class: {0: 0.5669291338582677, 4: 0.8924731182795699, 5: 0.5630498533724341, 6: 0.45, 7: 0.08695652173913043, 9: 0.44642857142857145, 10: 0.44221105527638194, 13: 0.09302325581395349, 15: 0.42857142857142855, 16: 0.5048543689320388, 17: 0.0, 18: 0.32098765432098764, 19: 0.47645429362880887, 21: 0.17777777777777778, 23: 0.5681818181818182, 24: 0.1875, 25: 0.5205479452054794, 26: 0.616822429906542, 27: 0.22535211267605634, 29: 0.8053097345132744, 31: 0.3333333333333333, 32: 0.71900826446281, 35: 0.38009049773755654, 37: 0.21714285714285714, 38: 0.35714285714285715, 40: 0.16455696202531644}
Micro-average F1 score: 0.46464646464646464
Weighted-average F1 score: 0.4395013280869259
F1 score per class: {0: 0.5811965811965812, 4: 0.88268156424581, 5: 0.6274509803921569, 6: 0.45569620253164556, 7: 0.08571428571428572, 9: 0.6493506493506493, 10: 0.43902439024390244, 13: 0.08888888888888889, 15: 0.375, 16: 0.6097560975609756, 17: 0.0, 18: 0.3076923076923077, 19: 0.5180722891566265, 21: 0.19130434782608696, 23: 0.6024096385542169, 24: 0.0, 25: 0.4857142857142857, 26: 0.6534653465346535, 27: 0.22535211267605634, 29: 0.8071748878923767, 31: 0.6666666666666666, 32: 0.725, 35: 0.37320574162679426, 37: 0.19117647058823528, 38: 0.3333333333333333, 40: 0.15384615384615385}
Micro-average F1 score: 0.4718196457326892
Weighted-average F1 score: 0.4423834105237287

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.3888888888888889, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4166666666666667, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.576271186440678, 37: 0.4292682926829268, 38: 0.25, 40: 0.0}
Micro-average F1 score: 0.3076923076923077
Weighted-average F1 score: 0.2532600344736721
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.0, 15: 0.48, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.4578313253012048, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.5793103448275863, 37: 0.4634146341463415, 38: 0.425531914893617, 40: 0.0}
Micro-average F1 score: 0.2926208651399491
Weighted-average F1 score: 0.22586012279030546
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.48, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.41975308641975306, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.5652173913043478, 37: 0.4508670520231214, 38: 0.35, 40: 0.0}
Micro-average F1 score: 0.29071332436069985
Weighted-average F1 score: 0.2282190579857266

F1 score per class: {0: 0.5354330708661418, 4: 0.7630057803468208, 5: 0.5886075949367089, 6: 0.3008356545961003, 7: 0.0392156862745098, 9: 0.6493506493506493, 10: 0.3076923076923077, 13: 0.04819277108433735, 15: 0.09655172413793103, 16: 0.3697478991596639, 17: 0.09090909090909091, 18: 0.03636363636363636, 19: 0.5015105740181269, 21: 0.1308411214953271, 23: 0.6185567010309279, 24: 0.0, 25: 0.410958904109589, 26: 0.625, 27: 0.1391304347826087, 29: 0.5993265993265994, 31: 0.5, 32: 0.5782312925170068, 35: 0.2764227642276423, 37: 0.09596510359869138, 38: 0.16666666666666666, 40: 0.11447811447811448}
Micro-average F1 score: 0.331493890421758
Weighted-average F1 score: 0.2976269164917164
F1 score per class: {0: 0.4186046511627907, 4: 0.8383838383838383, 5: 0.3404255319148936, 6: 0.2903225806451613, 7: 0.05, 9: 0.3246753246753247, 10: 0.3013698630136986, 13: 0.05405405405405406, 15: 0.3076923076923077, 16: 0.2736842105263158, 17: 0.0, 18: 0.208, 19: 0.38826185101580135, 21: 0.11204481792717087, 23: 0.45045045045045046, 24: 0.12244897959183673, 25: 0.42696629213483145, 26: 0.5258964143426295, 27: 0.12403100775193798, 29: 0.5759493670886076, 31: 0.2222222222222222, 32: 0.5132743362831859, 35: 0.23661971830985915, 37: 0.1330998248686515, 38: 0.24390243902439024, 40: 0.11016949152542373}
Micro-average F1 score: 0.3174097664543524
Weighted-average F1 score: 0.29787088729945016
F1 score per class: {0: 0.4473684210526316, 4: 0.8315789473684211, 5: 0.4025157232704403, 6: 0.2918918918918919, 7: 0.04878048780487805, 9: 0.5617977528089888, 10: 0.3050847457627119, 13: 0.05194805194805195, 15: 0.24, 16: 0.3401360544217687, 17: 0.0, 18: 0.1941747572815534, 19: 0.4321608040201005, 21: 0.11733333333333333, 23: 0.5102040816326531, 24: 0.0, 25: 0.39080459770114945, 26: 0.5739130434782609, 27: 0.125, 29: 0.5590062111801242, 31: 0.4, 32: 0.525679758308157, 35: 0.23636363636363636, 37: 0.11694152923538231, 38: 0.2222222222222222, 40: 0.10569105691056911}
Micro-average F1 score: 0.32634119175793574
Weighted-average F1 score: 0.30347986900390245
cur_acc_wo_na:  ['0.7643', '0.3072', '0.5921', '0.5386', '0.4306']
his_acc_wo_na:  ['0.7643', '0.5050', '0.5225', '0.5410', '0.4721']
cur_acc des_wo_na:  ['0.7570', '0.3137', '0.5818', '0.4880', '0.4251']
his_acc des_wo_na:  ['0.7570', '0.5357', '0.5298', '0.5200', '0.4646']
cur_acc rrf_wo_na:  ['0.7650', '0.3278', '0.5824', '0.5096', '0.4178']
his_acc rrf_wo_na:  ['0.7650', '0.5408', '0.5318', '0.5295', '0.4718']
cur_acc_w_na:  ['0.6242', '0.2709', '0.4422', '0.4012', '0.3077']
his_acc_w_na:  ['0.6242', '0.4034', '0.3961', '0.4270', '0.3315']
cur_acc des_w_na:  ['0.6039', '0.2697', '0.4272', '0.3316', '0.2926']
his_acc des_w_na:  ['0.6039', '0.4292', '0.3941', '0.3864', '0.3174']
cur_acc rrf_w_na:  ['0.6114', '0.2865', '0.4310', '0.3442', '0.2907']
his_acc rrf_w_na:  ['0.6114', '0.4363', '0.3983', '0.3968', '0.3263']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 142.8793278CurrentTrain: epoch  0, batch     1 | loss: 78.0981006CurrentTrain: epoch  0, batch     2 | loss: 91.5539761CurrentTrain: epoch  0, batch     3 | loss: 107.6342538CurrentTrain: epoch  0, batch     4 | loss: 20.0983152CurrentTrain: epoch  1, batch     0 | loss: 128.6519630CurrentTrain: epoch  1, batch     1 | loss: 95.6326501CurrentTrain: epoch  1, batch     2 | loss: 75.6493714CurrentTrain: epoch  1, batch     3 | loss: 81.5598939CurrentTrain: epoch  1, batch     4 | loss: 25.5799284CurrentTrain: epoch  2, batch     0 | loss: 98.3331231CurrentTrain: epoch  2, batch     1 | loss: 78.1882885CurrentTrain: epoch  2, batch     2 | loss: 120.3026655CurrentTrain: epoch  2, batch     3 | loss: 67.7397390CurrentTrain: epoch  2, batch     4 | loss: 17.0035800CurrentTrain: epoch  3, batch     0 | loss: 76.2932970CurrentTrain: epoch  3, batch     1 | loss: 77.2911027CurrentTrain: epoch  3, batch     2 | loss: 93.8636458CurrentTrain: epoch  3, batch     3 | loss: 64.0557401CurrentTrain: epoch  3, batch     4 | loss: 34.6420968CurrentTrain: epoch  4, batch     0 | loss: 64.7903545CurrentTrain: epoch  4, batch     1 | loss: 90.6521773CurrentTrain: epoch  4, batch     2 | loss: 64.1209055CurrentTrain: epoch  4, batch     3 | loss: 90.9976454CurrentTrain: epoch  4, batch     4 | loss: 24.2412824CurrentTrain: epoch  5, batch     0 | loss: 78.0181234CurrentTrain: epoch  5, batch     1 | loss: 89.3953257CurrentTrain: epoch  5, batch     2 | loss: 71.8441731CurrentTrain: epoch  5, batch     3 | loss: 59.6645302CurrentTrain: epoch  5, batch     4 | loss: 37.4644303CurrentTrain: epoch  6, batch     0 | loss: 89.0369400CurrentTrain: epoch  6, batch     1 | loss: 89.2717374CurrentTrain: epoch  6, batch     2 | loss: 71.8066647CurrentTrain: epoch  6, batch     3 | loss: 71.9369003CurrentTrain: epoch  6, batch     4 | loss: 20.7862862CurrentTrain: epoch  7, batch     0 | loss: 61.9962883CurrentTrain: epoch  7, batch     1 | loss: 73.9247864CurrentTrain: epoch  7, batch     2 | loss: 71.4758037CurrentTrain: epoch  7, batch     3 | loss: 86.1578641CurrentTrain: epoch  7, batch     4 | loss: 20.6463694CurrentTrain: epoch  8, batch     0 | loss: 71.7522059CurrentTrain: epoch  8, batch     1 | loss: 72.2212304CurrentTrain: epoch  8, batch     2 | loss: 70.6743794CurrentTrain: epoch  8, batch     3 | loss: 58.7382906CurrentTrain: epoch  8, batch     4 | loss: 34.8553633CurrentTrain: epoch  9, batch     0 | loss: 68.7797807CurrentTrain: epoch  9, batch     1 | loss: 71.2839716CurrentTrain: epoch  9, batch     2 | loss: 85.4723681CurrentTrain: epoch  9, batch     3 | loss: 69.9552443CurrentTrain: epoch  9, batch     4 | loss: 34.9841262
MemoryTrain:  epoch  0, batch     0 | loss: 0.9254217MemoryTrain:  epoch  1, batch     0 | loss: 0.7352549MemoryTrain:  epoch  2, batch     0 | loss: 0.6235264MemoryTrain:  epoch  3, batch     0 | loss: 0.4954765MemoryTrain:  epoch  4, batch     0 | loss: 0.4540179MemoryTrain:  epoch  5, batch     0 | loss: 0.3687935MemoryTrain:  epoch  6, batch     0 | loss: 0.2959092MemoryTrain:  epoch  7, batch     0 | loss: 0.2607932MemoryTrain:  epoch  8, batch     0 | loss: 0.2294328MemoryTrain:  epoch  9, batch     0 | loss: 0.2282586

F1 score per class: {0: 0.0, 2: 0.5833333333333334, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.48366013071895425, 12: 0.6060606060606061, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 27: 0.0, 28: 0.35294117647058826, 29: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.2, 40: 0.0}
Micro-average F1 score: 0.3688969258589512
Weighted-average F1 score: 0.2693362828709782
F1 score per class: {0: 0.0, 2: 0.4, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.45517241379310347, 12: 0.5398773006134969, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.3333333333333333, 29: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.36363636363636365, 40: 0.0}
Micro-average F1 score: 0.32525951557093424
Weighted-average F1 score: 0.2300822333350458
F1 score per class: {0: 0.0, 2: 0.45161290322580644, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.4520547945205479, 12: 0.5555555555555556, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 27: 0.0, 28: 0.34285714285714286, 29: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.2857142857142857, 40: 0.0}
Micro-average F1 score: 0.3443223443223443
Weighted-average F1 score: 0.25032492126833894

F1 score per class: {0: 0.7157894736842105, 2: 0.2222222222222222, 4: 0.6111111111111112, 5: 0.7829787234042553, 6: 0.4049586776859504, 7: 0.028169014084507043, 9: 0.7246376811594203, 10: 0.3443708609271523, 11: 0.20218579234972678, 12: 0.36496350364963503, 13: 0.056338028169014086, 15: 0.20588235294117646, 16: 0.5833333333333334, 17: 0.0, 18: 0.0, 19: 0.6274509803921569, 21: 0.2597402597402597, 23: 0.7692307692307693, 24: 0.0, 25: 0.44776119402985076, 26: 0.7126436781609196, 27: 0.25396825396825395, 28: 0.1276595744680851, 29: 0.79, 31: 0.0, 32: 0.7053941908713693, 35: 0.39669421487603307, 37: 0.09448818897637795, 38: 0.18518518518518517, 39: 0.07547169811320754, 40: 0.20909090909090908}
Micro-average F1 score: 0.42222773505333666
Weighted-average F1 score: 0.3883775358070633
F1 score per class: {0: 0.4675324675324675, 2: 0.11475409836065574, 4: 0.6666666666666666, 5: 0.565597667638484, 6: 0.4122137404580153, 7: 0.028985507246376812, 9: 0.45454545454545453, 10: 0.3855421686746988, 11: 0.2323943661971831, 12: 0.29931972789115646, 13: 0.08163265306122448, 15: 0.4444444444444444, 16: 0.5494505494505495, 17: 0.0, 18: 0.0, 19: 0.5431309904153354, 21: 0.24691358024691357, 23: 0.5957446808510638, 24: 0.08333333333333333, 25: 0.5333333333333333, 26: 0.6600985221674877, 27: 0.2711864406779661, 28: 0.13953488372093023, 29: 0.81, 31: 0.4, 32: 0.6323024054982818, 35: 0.3783783783783784, 37: 0.21428571428571427, 38: 0.29850746268656714, 39: 0.14285714285714285, 40: 0.20382165605095542}
Micro-average F1 score: 0.4192726430391476
Weighted-average F1 score: 0.40221361728469107
F1 score per class: {0: 0.6086956521739131, 2: 0.14583333333333334, 4: 0.6577181208053692, 5: 0.6486486486486487, 6: 0.4092664092664093, 7: 0.03076923076923077, 9: 0.684931506849315, 10: 0.3780487804878049, 11: 0.2222222222222222, 12: 0.30716723549488056, 13: 0.08163265306122448, 15: 0.36363636363636365, 16: 0.625, 17: 0.0, 18: 0.0, 19: 0.5874125874125874, 21: 0.2787878787878788, 23: 0.5977011494252874, 24: 0.09523809523809523, 25: 0.4931506849315068, 26: 0.6804123711340206, 27: 0.26229508196721313, 28: 0.13186813186813187, 29: 0.8040201005025126, 31: 0.4, 32: 0.6742424242424242, 35: 0.42857142857142855, 37: 0.1935483870967742, 38: 0.25, 39: 0.10909090909090909, 40: 0.1797752808988764}
Micro-average F1 score: 0.43561442236938924
Weighted-average F1 score: 0.41659969420836085

F1 score per class: {0: 0.0, 2: 0.358974358974359, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.387434554973822, 12: 0.47619047619047616, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 28: 0.1518987341772152, 29: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.11764705882352941, 40: 0.0}
Micro-average F1 score: 0.24228028503562946
Weighted-average F1 score: 0.18650673132510384
F1 score per class: {0: 0.0, 2: 0.23728813559322035, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.3707865168539326, 12: 0.4607329842931937, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.15384615384615385, 29: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.20512820512820512, 40: 0.0}
Micro-average F1 score: 0.21315192743764172
Weighted-average F1 score: 0.155443937804874
F1 score per class: {0: 0.0, 2: 0.28, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.3687150837988827, 12: 0.46875, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 28: 0.15584415584415584, 29: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.15, 40: 0.0}
Micro-average F1 score: 0.2251497005988024
Weighted-average F1 score: 0.1669014924982875

F1 score per class: {0: 0.5573770491803278, 2: 0.15217391304347827, 4: 0.5827814569536424, 5: 0.6323024054982818, 6: 0.23058823529411765, 7: 0.015151515151515152, 9: 0.6578947368421053, 10: 0.27807486631016043, 11: 0.14095238095238094, 12: 0.1669449081803005, 13: 0.03636363636363636, 15: 0.10687022900763359, 16: 0.32558139534883723, 17: 0.0, 18: 0.0, 19: 0.5818181818181818, 21: 0.1724137931034483, 23: 0.6862745098039216, 24: 0.0, 25: 0.42857142857142855, 26: 0.6391752577319587, 27: 0.14035087719298245, 28: 0.05741626794258373, 29: 0.6422764227642277, 31: 0.0, 32: 0.5295950155763239, 35: 0.27586206896551724, 37: 0.07228915662650602, 38: 0.14705882352941177, 39: 0.046511627906976744, 40: 0.17424242424242425}
Micro-average F1 score: 0.29625761531766753
Weighted-average F1 score: 0.2653933932255904
F1 score per class: {0: 0.35121951219512193, 2: 0.07368421052631578, 4: 0.6329113924050633, 5: 0.3533697632058288, 6: 0.24161073825503357, 7: 0.016129032258064516, 9: 0.352112676056338, 10: 0.2831858407079646, 11: 0.165, 12: 0.16236162361623616, 13: 0.0547945205479452, 15: 0.3, 16: 0.29411764705882354, 17: 0.0, 18: 0.0, 19: 0.4899135446685879, 21: 0.1532567049808429, 23: 0.4628099173553719, 24: 0.05555555555555555, 25: 0.4444444444444444, 26: 0.5826086956521739, 27: 0.14953271028037382, 28: 0.06349206349206349, 29: 0.6428571428571429, 31: 0.2857142857142857, 32: 0.4520884520884521, 35: 0.2545454545454545, 37: 0.15584415584415584, 38: 0.2247191011235955, 39: 0.0761904761904762, 40: 0.16080402010050251}
Micro-average F1 score: 0.2878498727735369
Weighted-average F1 score: 0.2704925224184551
F1 score per class: {0: 0.445859872611465, 2: 0.09523809523809523, 4: 0.6282051282051282, 5: 0.43340857787810383, 6: 0.24036281179138322, 7: 0.016260162601626018, 9: 0.6024096385542169, 10: 0.2897196261682243, 11: 0.15789473684210525, 12: 0.16100178890876565, 13: 0.0547945205479452, 15: 0.22641509433962265, 16: 0.3401360544217687, 17: 0.0, 18: 0.0, 19: 0.5401929260450161, 21: 0.1678832116788321, 23: 0.5098039215686274, 24: 0.08333333333333333, 25: 0.42857142857142855, 26: 0.6055045871559633, 27: 0.14545454545454545, 28: 0.06, 29: 0.6374501992031872, 31: 0.2857142857142857, 32: 0.48501362397820164, 35: 0.29770992366412213, 37: 0.13636363636363635, 38: 0.1927710843373494, 39: 0.058823529411764705, 40: 0.14545454545454545}
Micro-average F1 score: 0.3018354860639021
Weighted-average F1 score: 0.2810906255788501
cur_acc_wo_na:  ['0.7643', '0.3072', '0.5921', '0.5386', '0.4306', '0.3689']
his_acc_wo_na:  ['0.7643', '0.5050', '0.5225', '0.5410', '0.4721', '0.4222']
cur_acc des_wo_na:  ['0.7570', '0.3137', '0.5818', '0.4880', '0.4251', '0.3253']
his_acc des_wo_na:  ['0.7570', '0.5357', '0.5298', '0.5200', '0.4646', '0.4193']
cur_acc rrf_wo_na:  ['0.7650', '0.3278', '0.5824', '0.5096', '0.4178', '0.3443']
his_acc rrf_wo_na:  ['0.7650', '0.5408', '0.5318', '0.5295', '0.4718', '0.4356']
cur_acc_w_na:  ['0.6242', '0.2709', '0.4422', '0.4012', '0.3077', '0.2423']
his_acc_w_na:  ['0.6242', '0.4034', '0.3961', '0.4270', '0.3315', '0.2963']
cur_acc des_w_na:  ['0.6039', '0.2697', '0.4272', '0.3316', '0.2926', '0.2132']
his_acc des_w_na:  ['0.6039', '0.4292', '0.3941', '0.3864', '0.3174', '0.2878']
cur_acc rrf_w_na:  ['0.6114', '0.2865', '0.4310', '0.3442', '0.2907', '0.2251']
his_acc rrf_w_na:  ['0.6114', '0.4363', '0.3983', '0.3968', '0.3263', '0.3018']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 91.3135048CurrentTrain: epoch  0, batch     1 | loss: 95.6723221CurrentTrain: epoch  0, batch     2 | loss: 188.5698502CurrentTrain: epoch  0, batch     3 | loss: 86.2829525CurrentTrain: epoch  0, batch     4 | loss: 78.8823031CurrentTrain: epoch  1, batch     0 | loss: 72.1042070CurrentTrain: epoch  1, batch     1 | loss: 94.6786652CurrentTrain: epoch  1, batch     2 | loss: 99.3859785CurrentTrain: epoch  1, batch     3 | loss: 102.1022377CurrentTrain: epoch  1, batch     4 | loss: 48.9937782CurrentTrain: epoch  2, batch     0 | loss: 102.3208672CurrentTrain: epoch  2, batch     1 | loss: 70.9337737CurrentTrain: epoch  2, batch     2 | loss: 67.8536903CurrentTrain: epoch  2, batch     3 | loss: 83.8956739CurrentTrain: epoch  2, batch     4 | loss: 55.3869648CurrentTrain: epoch  3, batch     0 | loss: 98.3698424CurrentTrain: epoch  3, batch     1 | loss: 80.7640808CurrentTrain: epoch  3, batch     2 | loss: 92.8257277CurrentTrain: epoch  3, batch     3 | loss: 94.7795391CurrentTrain: epoch  3, batch     4 | loss: 93.1541047CurrentTrain: epoch  4, batch     0 | loss: 91.7760654CurrentTrain: epoch  4, batch     1 | loss: 116.5754589CurrentTrain: epoch  4, batch     2 | loss: 77.2019487CurrentTrain: epoch  4, batch     3 | loss: 69.1571456CurrentTrain: epoch  4, batch     4 | loss: 50.0614506CurrentTrain: epoch  5, batch     0 | loss: 76.7467135CurrentTrain: epoch  5, batch     1 | loss: 73.3884562CurrentTrain: epoch  5, batch     2 | loss: 77.5396474CurrentTrain: epoch  5, batch     3 | loss: 115.4698250CurrentTrain: epoch  5, batch     4 | loss: 50.5004100CurrentTrain: epoch  6, batch     0 | loss: 74.8383207CurrentTrain: epoch  6, batch     1 | loss: 90.4866942CurrentTrain: epoch  6, batch     2 | loss: 154.4953341CurrentTrain: epoch  6, batch     3 | loss: 63.0339850CurrentTrain: epoch  6, batch     4 | loss: 46.1204452CurrentTrain: epoch  7, batch     0 | loss: 70.7956924CurrentTrain: epoch  7, batch     1 | loss: 89.8018802CurrentTrain: epoch  7, batch     2 | loss: 91.1909309CurrentTrain: epoch  7, batch     3 | loss: 74.5674981CurrentTrain: epoch  7, batch     4 | loss: 48.1239252CurrentTrain: epoch  8, batch     0 | loss: 87.6764380CurrentTrain: epoch  8, batch     1 | loss: 63.1879203CurrentTrain: epoch  8, batch     2 | loss: 86.9018280CurrentTrain: epoch  8, batch     3 | loss: 86.7713334CurrentTrain: epoch  8, batch     4 | loss: 61.2740508CurrentTrain: epoch  9, batch     0 | loss: 72.2646327CurrentTrain: epoch  9, batch     1 | loss: 73.2651641CurrentTrain: epoch  9, batch     2 | loss: 88.6685801CurrentTrain: epoch  9, batch     3 | loss: 70.7384378CurrentTrain: epoch  9, batch     4 | loss: 38.3717186
MemoryTrain:  epoch  0, batch     0 | loss: 1.1235619MemoryTrain:  epoch  1, batch     0 | loss: 0.9356778MemoryTrain:  epoch  2, batch     0 | loss: 0.7252188MemoryTrain:  epoch  3, batch     0 | loss: 0.5790102MemoryTrain:  epoch  4, batch     0 | loss: 0.4956680MemoryTrain:  epoch  5, batch     0 | loss: 0.4093136MemoryTrain:  epoch  6, batch     0 | loss: 0.3422645MemoryTrain:  epoch  7, batch     0 | loss: 0.2756029MemoryTrain:  epoch  8, batch     0 | loss: 0.2311801MemoryTrain:  epoch  9, batch     0 | loss: 0.2022976

F1 score per class: {0: 0.0, 1: 0.27631578947368424, 2: 0.0, 3: 0.6473988439306358, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.08955223880597014, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.48135593220338985, 23: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.6542056074766355, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3522727272727273
Weighted-average F1 score: 0.30446880732583687
F1 score per class: {0: 0.0, 1: 0.26519337016574585, 2: 0.0, 3: 0.5833333333333334, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.1518987341772152, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5201465201465202, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.6722689075630253, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.32242225859247137
Weighted-average F1 score: 0.26859043791782494
F1 score per class: {0: 0.0, 1: 0.29347826086956524, 2: 0.0, 3: 0.5671641791044776, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.1111111111111111, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5, 23: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.7142857142857143, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.34295415959252973
Weighted-average F1 score: 0.29749088701864523

F1 score per class: {0: 0.7567567567567568, 1: 0.20095693779904306, 2: 0.2857142857142857, 3: 0.3660130718954248, 4: 0.6111111111111112, 5: 0.8303571428571429, 6: 0.379746835443038, 7: 0.03225806451612903, 9: 0.7575757575757576, 10: 0.32894736842105265, 11: 0.17894736842105263, 12: 0.3250883392226148, 13: 0.11594202898550725, 14: 0.07894736842105263, 15: 0.3333333333333333, 16: 0.6415094339622641, 17: 0.0, 18: 0.0, 19: 0.5346534653465347, 21: 0.14545454545454545, 22: 0.3736842105263158, 23: 0.7391304347826086, 24: 0.0, 25: 0.45714285714285713, 26: 0.6961325966850829, 27: 0.125, 28: 0.09090909090909091, 29: 0.7843137254901961, 31: 0.0, 32: 0.7128712871287128, 34: 0.2147239263803681, 35: 0.18691588785046728, 37: 0.23841059602649006, 38: 0.21621621621621623, 39: 0.046511627906976744, 40: 0.15450643776824036}
Micro-average F1 score: 0.3891825945506303
Weighted-average F1 score: 0.36504570697671657
F1 score per class: {0: 0.45454545454545453, 1: 0.183206106870229, 2: 0.11864406779661017, 3: 0.35782747603833864, 4: 0.6206896551724138, 5: 0.5680473372781065, 6: 0.40329218106995884, 7: 0.034482758620689655, 9: 0.390625, 10: 0.4, 11: 0.15037593984962405, 12: 0.2917933130699088, 13: 0.07407407407407407, 14: 0.10434782608695652, 15: 0.48, 16: 0.6666666666666666, 17: 0.0, 18: 0.036036036036036036, 19: 0.4748603351955307, 21: 0.08247422680412371, 22: 0.4068767908309456, 23: 0.6530612244897959, 24: 0.08, 25: 0.5135135135135135, 26: 0.6699029126213593, 27: 0.0, 28: 0.1518987341772152, 29: 0.7761194029850746, 31: 0.15384615384615385, 32: 0.5804195804195804, 34: 0.21052631578947367, 35: 0.16129032258064516, 37: 0.14084507042253522, 38: 0.29213483146067415, 39: 0.16666666666666666, 40: 0.19883040935672514}
Micro-average F1 score: 0.36363636363636365
Weighted-average F1 score: 0.34788134863869996
F1 score per class: {0: 0.6296296296296297, 1: 0.20300751879699247, 2: 0.17721518987341772, 3: 0.33827893175074186, 4: 0.6013986013986014, 5: 0.6690140845070423, 6: 0.3902439024390244, 7: 0.034482758620689655, 9: 0.7142857142857143, 10: 0.3468208092485549, 11: 0.1568627450980392, 12: 0.2926829268292683, 13: 0.07142857142857142, 14: 0.08333333333333333, 15: 0.4444444444444444, 16: 0.7213114754098361, 17: 0.0, 18: 0.025974025974025976, 19: 0.4843304843304843, 21: 0.1111111111111111, 22: 0.37948717948717947, 23: 0.6521739130434783, 24: 0.0, 25: 0.4788732394366197, 26: 0.6700507614213198, 27: 0.0, 28: 0.1348314606741573, 29: 0.7738693467336684, 31: 0.0, 32: 0.6557377049180327, 34: 0.215633423180593, 35: 0.18421052631578946, 37: 0.17964071856287425, 38: 0.2716049382716049, 39: 0.17777777777777778, 40: 0.16842105263157894}
Micro-average F1 score: 0.3747896016457827
Weighted-average F1 score: 0.3580154592142108

F1 score per class: {0: 0.0, 1: 0.16091954022988506, 2: 0.0, 3: 0.4890829694323144, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.08571428571428572, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.3594936708860759, 23: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5147058823529411, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2300556586270872
Weighted-average F1 score: 0.2003471431743565
F1 score per class: {0: 0.0, 1: 0.15141955835962145, 2: 0.0, 3: 0.4357976653696498, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.1348314606741573, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.3869209809264305, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.20979765708200213
Weighted-average F1 score: 0.18048891349669355
F1 score per class: {0: 0.0, 1: 0.16718266253869968, 2: 0.0, 3: 0.41155234657039713, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.10256410256410256, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.3681592039800995, 23: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5405405405405406, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.22582448295136948
Weighted-average F1 score: 0.20054774961466845

F1 score per class: {0: 0.5656565656565656, 1: 0.1111111111111111, 2: 0.19444444444444445, 3: 0.23045267489711935, 4: 0.5789473684210527, 5: 0.6690647482014388, 6: 0.2222222222222222, 7: 0.017543859649122806, 9: 0.6756756756756757, 10: 0.26455026455026454, 11: 0.11683848797250859, 12: 0.14790996784565916, 13: 0.06722689075630252, 14: 0.06741573033707865, 15: 0.21428571428571427, 16: 0.38636363636363635, 17: 0.0, 18: 0.0, 19: 0.47928994082840237, 21: 0.12698412698412698, 22: 0.24912280701754386, 23: 0.6476190476190476, 24: 0.0, 25: 0.43243243243243246, 26: 0.6206896551724138, 27: 0.07142857142857142, 28: 0.04395604395604396, 29: 0.6299212598425197, 31: 0.0, 32: 0.5294117647058824, 34: 0.14198782961460446, 35: 0.13333333333333333, 37: 0.1782178217821782, 38: 0.15841584158415842, 39: 0.028169014084507043, 40: 0.12}
Micro-average F1 score: 0.26716917922948075
Weighted-average F1 score: 0.24384624123745324
F1 score per class: {0: 0.33175355450236965, 1: 0.10256410256410256, 2: 0.08536585365853659, 3: 0.23045267489711935, 4: 0.5960264900662252, 5: 0.34408602150537637, 6: 0.23444976076555024, 7: 0.018691588785046728, 9: 0.26595744680851063, 10: 0.2823529411764706, 11: 0.0975609756097561, 12: 0.14746543778801843, 13: 0.04597701149425287, 14: 0.08450704225352113, 15: 0.34285714285714286, 16: 0.384, 17: 0.0, 18: 0.027777777777777776, 19: 0.4146341463414634, 21: 0.06956521739130435, 22: 0.26591760299625467, 23: 0.49230769230769234, 24: 0.07142857142857142, 25: 0.4222222222222222, 26: 0.5702479338842975, 27: 0.0, 28: 0.07407407407407407, 29: 0.6290322580645161, 31: 0.08333333333333333, 32: 0.4119106699751861, 34: 0.1353637901861252, 35: 0.10714285714285714, 37: 0.10526315789473684, 38: 0.19696969696969696, 39: 0.09523809523809523, 40: 0.14847161572052403}
Micro-average F1 score: 0.24439918533604887
Weighted-average F1 score: 0.23101456329736889
F1 score per class: {0: 0.4444444444444444, 1: 0.11368421052631579, 2: 0.1206896551724138, 3: 0.2122905027932961, 4: 0.5733333333333334, 5: 0.4328018223234624, 6: 0.22695035460992907, 7: 0.018518518518518517, 9: 0.6172839506172839, 10: 0.2564102564102564, 11: 0.10300429184549356, 12: 0.1458966565349544, 13: 0.04395604395604396, 14: 0.06779661016949153, 15: 0.26666666666666666, 16: 0.4444444444444444, 17: 0.0, 18: 0.018691588785046728, 19: 0.43256997455470736, 21: 0.09615384615384616, 22: 0.24873949579831933, 23: 0.5555555555555556, 24: 0.0, 25: 0.4, 26: 0.5945945945945946, 27: 0.0, 28: 0.06779661016949153, 29: 0.6234817813765182, 31: 0.0, 32: 0.47337278106508873, 34: 0.1415929203539823, 35: 0.13023255813953488, 37: 0.13513513513513514, 38: 0.18487394957983194, 39: 0.10526315789473684, 40: 0.12955465587044535}
Micro-average F1 score: 0.2549942740806718
Weighted-average F1 score: 0.23935849555402428
cur_acc_wo_na:  ['0.7643', '0.3072', '0.5921', '0.5386', '0.4306', '0.3689', '0.3523']
his_acc_wo_na:  ['0.7643', '0.5050', '0.5225', '0.5410', '0.4721', '0.4222', '0.3892']
cur_acc des_wo_na:  ['0.7570', '0.3137', '0.5818', '0.4880', '0.4251', '0.3253', '0.3224']
his_acc des_wo_na:  ['0.7570', '0.5357', '0.5298', '0.5200', '0.4646', '0.4193', '0.3636']
cur_acc rrf_wo_na:  ['0.7650', '0.3278', '0.5824', '0.5096', '0.4178', '0.3443', '0.3430']
his_acc rrf_wo_na:  ['0.7650', '0.5408', '0.5318', '0.5295', '0.4718', '0.4356', '0.3748']
cur_acc_w_na:  ['0.6242', '0.2709', '0.4422', '0.4012', '0.3077', '0.2423', '0.2301']
his_acc_w_na:  ['0.6242', '0.4034', '0.3961', '0.4270', '0.3315', '0.2963', '0.2672']
cur_acc des_w_na:  ['0.6039', '0.2697', '0.4272', '0.3316', '0.2926', '0.2132', '0.2098']
his_acc des_w_na:  ['0.6039', '0.4292', '0.3941', '0.3864', '0.3174', '0.2878', '0.2444']
cur_acc rrf_w_na:  ['0.6114', '0.2865', '0.4310', '0.3442', '0.2907', '0.2251', '0.2258']
his_acc rrf_w_na:  ['0.6114', '0.4363', '0.3983', '0.3968', '0.3263', '0.3018', '0.2550']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 91.1778694CurrentTrain: epoch  0, batch     1 | loss: 79.4902072CurrentTrain: epoch  0, batch     2 | loss: 80.0402511CurrentTrain: epoch  0, batch     3 | loss: 58.2230871CurrentTrain: epoch  1, batch     0 | loss: 81.6781014CurrentTrain: epoch  1, batch     1 | loss: 80.2751066CurrentTrain: epoch  1, batch     2 | loss: 101.8853327CurrentTrain: epoch  1, batch     3 | loss: 51.8298575CurrentTrain: epoch  2, batch     0 | loss: 82.0854632CurrentTrain: epoch  2, batch     1 | loss: 76.7133995CurrentTrain: epoch  2, batch     2 | loss: 96.4733562CurrentTrain: epoch  2, batch     3 | loss: 45.8075256CurrentTrain: epoch  3, batch     0 | loss: 62.4385538CurrentTrain: epoch  3, batch     1 | loss: 65.8714393CurrentTrain: epoch  3, batch     2 | loss: 123.2979189CurrentTrain: epoch  3, batch     3 | loss: 44.8503463CurrentTrain: epoch  4, batch     0 | loss: 78.7963261CurrentTrain: epoch  4, batch     1 | loss: 72.5181733CurrentTrain: epoch  4, batch     2 | loss: 61.2338603CurrentTrain: epoch  4, batch     3 | loss: 54.4339287CurrentTrain: epoch  5, batch     0 | loss: 70.7150283CurrentTrain: epoch  5, batch     1 | loss: 74.3869433CurrentTrain: epoch  5, batch     2 | loss: 57.3321801CurrentTrain: epoch  5, batch     3 | loss: 58.1890057CurrentTrain: epoch  6, batch     0 | loss: 59.1849757CurrentTrain: epoch  6, batch     1 | loss: 87.4255379CurrentTrain: epoch  6, batch     2 | loss: 61.4824519CurrentTrain: epoch  6, batch     3 | loss: 42.2503899CurrentTrain: epoch  7, batch     0 | loss: 56.7332474CurrentTrain: epoch  7, batch     1 | loss: 86.4203049CurrentTrain: epoch  7, batch     2 | loss: 87.4740157CurrentTrain: epoch  7, batch     3 | loss: 53.3649618CurrentTrain: epoch  8, batch     0 | loss: 70.4854025CurrentTrain: epoch  8, batch     1 | loss: 86.7502141CurrentTrain: epoch  8, batch     2 | loss: 55.6863604CurrentTrain: epoch  8, batch     3 | loss: 66.6578958CurrentTrain: epoch  9, batch     0 | loss: 70.3757572CurrentTrain: epoch  9, batch     1 | loss: 65.2039554CurrentTrain: epoch  9, batch     2 | loss: 67.0299321CurrentTrain: epoch  9, batch     3 | loss: 92.6909261
MemoryTrain:  epoch  0, batch     0 | loss: 0.6927847MemoryTrain:  epoch  1, batch     0 | loss: 0.5621522MemoryTrain:  epoch  2, batch     0 | loss: 0.4779145MemoryTrain:  epoch  3, batch     0 | loss: 0.3516133MemoryTrain:  epoch  4, batch     0 | loss: 0.3194537MemoryTrain:  epoch  5, batch     0 | loss: 0.2545991MemoryTrain:  epoch  6, batch     0 | loss: 0.2404762MemoryTrain:  epoch  7, batch     0 | loss: 0.1993064MemoryTrain:  epoch  8, batch     0 | loss: 0.1812152MemoryTrain:  epoch  9, batch     0 | loss: 0.1517952

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5333333333333333, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.7628865979381443, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8648648648648649, 32: 0.0, 33: 0.42857142857142855, 34: 0.0, 35: 0.0, 36: 0.4791666666666667, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4220532319391635
Weighted-average F1 score: 0.2875674764902443
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.608, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.7850467289719626, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8, 31: 0.0, 32: 0.0, 33: 0.47058823529411764, 34: 0.0, 35: 0.0, 36: 0.631578947368421, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4437194127243067
Weighted-average F1 score: 0.3096827107878611
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.6031746031746031, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.7735849056603774, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8421052631578947, 32: 0.0, 33: 0.375, 34: 0.0, 35: 0.0, 36: 0.5346534653465347, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4288164665523156
Weighted-average F1 score: 0.2953327941031974

F1 score per class: {0: 0.6181818181818182, 1: 0.18018018018018017, 2: 0.358974358974359, 3: 0.368, 4: 0.7577639751552795, 5: 0.7300380228136882, 6: 0.38022813688212925, 7: 0.029850746268656716, 8: 0.3076923076923077, 9: 0.6578947368421053, 10: 0.24489795918367346, 11: 0.16666666666666666, 12: 0.3076923076923077, 13: 0.10256410256410256, 14: 0.06818181818181818, 15: 0.23529411764705882, 16: 0.6071428571428571, 17: 0.0, 18: 0.03333333333333333, 19: 0.515527950310559, 20: 0.5362318840579711, 21: 0.1839080459770115, 22: 0.4217687074829932, 23: 0.6741573033707865, 24: 0.0, 25: 0.4857142857142857, 26: 0.6702127659574468, 27: 0.0, 28: 0.25, 29: 0.7793427230046949, 30: 0.8648648648648649, 31: 0.18181818181818182, 32: 0.6550218340611353, 33: 0.13953488372093023, 34: 0.2717391304347826, 35: 0.19047619047619047, 36: 0.3108108108108108, 37: 0.17699115044247787, 38: 0.2, 39: 0.045454545454545456, 40: 0.17391304347826086}
Micro-average F1 score: 0.39538346984363365
Weighted-average F1 score: 0.38288359770858066
F1 score per class: {0: 0.35051546391752575, 1: 0.174496644295302, 2: 0.09859154929577464, 3: 0.3056768558951965, 4: 0.6754966887417219, 5: 0.4771084337349398, 6: 0.4072398190045249, 7: 0.028985507246376812, 8: 0.31275720164609055, 9: 0.33557046979865773, 10: 0.3076923076923077, 11: 0.15492957746478872, 12: 0.25675675675675674, 13: 0.0851063829787234, 14: 0.10752688172043011, 15: 0.375, 16: 0.6329113924050633, 17: 0.0, 18: 0.03773584905660377, 19: 0.4176904176904177, 20: 0.49122807017543857, 21: 0.2268041237113402, 22: 0.40828402366863903, 23: 0.6464646464646465, 24: 0.07692307692307693, 25: 0.5194805194805194, 26: 0.6476190476190476, 27: 0.09090909090909091, 28: 0.2631578947368421, 29: 0.772093023255814, 30: 0.5, 31: 0.07142857142857142, 32: 0.5498281786941581, 33: 0.14814814814814814, 34: 0.26755852842809363, 35: 0.17959183673469387, 36: 0.4067796610169492, 37: 0.1258741258741259, 38: 0.2909090909090909, 39: 0.18181818181818182, 40: 0.12738853503184713}
Micro-average F1 score: 0.35403530895334173
Weighted-average F1 score: 0.34197781314431497
F1 score per class: {0: 0.4121212121212121, 1: 0.16770186335403728, 2: 0.16091954022988506, 3: 0.33076923076923076, 4: 0.75, 5: 0.5875370919881305, 6: 0.40350877192982454, 7: 0.028169014084507043, 8: 0.3261802575107296, 9: 0.6024096385542169, 10: 0.30952380952380953, 11: 0.1437125748502994, 12: 0.2542955326460481, 13: 0.08695652173913043, 14: 0.09803921568627451, 15: 0.3157894736842105, 16: 0.676056338028169, 17: 0.0, 18: 0.05263157894736842, 19: 0.4386422976501306, 20: 0.47674418604651164, 21: 0.18181818181818182, 22: 0.41194029850746267, 23: 0.6206896551724138, 24: 0.0, 25: 0.4864864864864865, 26: 0.6502463054187192, 27: 0.09523809523809523, 28: 0.2857142857142857, 29: 0.7741935483870968, 30: 0.6274509803921569, 31: 0.125, 32: 0.5917602996254682, 33: 0.125, 34: 0.29118773946360155, 35: 0.16494845360824742, 36: 0.32335329341317365, 37: 0.15789473684210525, 38: 0.35294117647058826, 39: 0.14634146341463414, 40: 0.1452513966480447}
Micro-average F1 score: 0.3719380103316114
Weighted-average F1 score: 0.3601170272511501

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.45390070921985815, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5873015873015873, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8205128205128205, 31: 0.0, 32: 0.0, 33: 0.3, 34: 0.0, 35: 0.0, 36: 0.3898305084745763, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2824427480916031
Weighted-average F1 score: 0.1962183778195318
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5467625899280576, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5714285714285714, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.7441860465116279, 31: 0.0, 32: 0.0, 33: 0.32, 34: 0.0, 35: 0.0, 36: 0.5034965034965035, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.28422152560083597
Weighted-average F1 score: 0.20105342044654867
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5390070921985816, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5578231292517006, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.7804878048780488, 31: 0.0, 32: 0.0, 33: 0.2727272727272727, 34: 0.0, 35: 0.0, 36: 0.4122137404580153, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.27808676307007785
Weighted-average F1 score: 0.19638113980450764

F1 score per class: {0: 0.4533333333333333, 1: 0.10118043844856661, 2: 0.21212121212121213, 3: 0.22549019607843138, 4: 0.7176470588235294, 5: 0.5549132947976878, 6: 0.2127659574468085, 7: 0.016, 8: 0.18991097922848665, 9: 0.5882352941176471, 10: 0.21951219512195122, 11: 0.1125, 12: 0.14257425742574256, 13: 0.057971014492753624, 14: 0.05714285714285714, 15: 0.13186813186813187, 16: 0.3695652173913043, 17: 0.0, 18: 0.024096385542168676, 19: 0.4585635359116022, 20: 0.29365079365079366, 21: 0.12903225806451613, 22: 0.3155216284987277, 23: 0.594059405940594, 24: 0.0, 25: 0.4594594594594595, 26: 0.5887850467289719, 27: 0.0, 28: 0.14285714285714285, 29: 0.6036363636363636, 30: 0.7619047619047619, 31: 0.1111111111111111, 32: 0.487012987012987, 33: 0.09090909090909091, 34: 0.1736111111111111, 35: 0.13333333333333333, 36: 0.2081447963800905, 37: 0.14285714285714285, 38: 0.14285714285714285, 39: 0.03076923076923077, 40: 0.13134328358208955}
Micro-average F1 score: 0.27209838585703305
Weighted-average F1 score: 0.2557995389046645
F1 score per class: {0: 0.2527881040892193, 1: 0.09829867674858223, 2: 0.07106598984771574, 3: 0.1876675603217158, 4: 0.6415094339622641, 5: 0.2636484687083888, 6: 0.23809523809523808, 7: 0.015748031496062992, 8: 0.19387755102040816, 9: 0.25125628140703515, 10: 0.23636363636363636, 11: 0.11578947368421053, 12: 0.12837837837837837, 13: 0.05405405405405406, 14: 0.08771929824561403, 15: 0.3157894736842105, 16: 0.36764705882352944, 17: 0.0, 18: 0.028368794326241134, 19: 0.35490605427974947, 20: 0.2616822429906542, 21: 0.18181818181818182, 22: 0.2839506172839506, 23: 0.463768115942029, 24: 0.06451612903225806, 25: 0.4819277108433735, 26: 0.53125, 27: 0.058823529411764705, 28: 0.12987012987012986, 29: 0.5949820788530465, 30: 0.35555555555555557, 31: 0.03636363636363636, 32: 0.3970223325062035, 33: 0.0851063829787234, 34: 0.1652892561983471, 35: 0.11796246648793565, 36: 0.2608695652173913, 37: 0.09473684210526316, 38: 0.1839080459770115, 39: 0.09090909090909091, 40: 0.09852216748768473}
Micro-average F1 score: 0.2352325094260578
Weighted-average F1 score: 0.22452837989657753
F1 score per class: {0: 0.29955947136563876, 1: 0.09473684210526316, 2: 0.11382113821138211, 3: 0.2042755344418052, 4: 0.7100591715976331, 5: 0.3567567567567568, 6: 0.23, 7: 0.015151515151515152, 8: 0.20596205962059622, 9: 0.5050505050505051, 10: 0.24528301886792453, 11: 0.10909090909090909, 12: 0.1260647359454855, 13: 0.05333333333333334, 14: 0.08196721311475409, 15: 0.24, 16: 0.39669421487603307, 17: 0.0, 18: 0.03669724770642202, 19: 0.3835616438356164, 20: 0.2433234421364985, 21: 0.136986301369863, 22: 0.2917547568710359, 23: 0.5046728971962616, 24: 0.0, 25: 0.45, 26: 0.5454545454545454, 27: 0.06666666666666667, 28: 0.14492753623188406, 29: 0.6021505376344086, 30: 0.4383561643835616, 31: 0.07142857142857142, 32: 0.4305177111716621, 33: 0.07317073170731707, 34: 0.17194570135746606, 35: 0.11552346570397112, 36: 0.20149253731343283, 37: 0.12060301507537688, 38: 0.20930232558139536, 39: 0.08450704225352113, 40: 0.11255411255411256}
Micro-average F1 score: 0.25
Weighted-average F1 score: 0.2380144654548744
cur_acc_wo_na:  ['0.7643', '0.3072', '0.5921', '0.5386', '0.4306', '0.3689', '0.3523', '0.4221']
his_acc_wo_na:  ['0.7643', '0.5050', '0.5225', '0.5410', '0.4721', '0.4222', '0.3892', '0.3954']
cur_acc des_wo_na:  ['0.7570', '0.3137', '0.5818', '0.4880', '0.4251', '0.3253', '0.3224', '0.4437']
his_acc des_wo_na:  ['0.7570', '0.5357', '0.5298', '0.5200', '0.4646', '0.4193', '0.3636', '0.3540']
cur_acc rrf_wo_na:  ['0.7650', '0.3278', '0.5824', '0.5096', '0.4178', '0.3443', '0.3430', '0.4288']
his_acc rrf_wo_na:  ['0.7650', '0.5408', '0.5318', '0.5295', '0.4718', '0.4356', '0.3748', '0.3719']
cur_acc_w_na:  ['0.6242', '0.2709', '0.4422', '0.4012', '0.3077', '0.2423', '0.2301', '0.2824']
his_acc_w_na:  ['0.6242', '0.4034', '0.3961', '0.4270', '0.3315', '0.2963', '0.2672', '0.2721']
cur_acc des_w_na:  ['0.6039', '0.2697', '0.4272', '0.3316', '0.2926', '0.2132', '0.2098', '0.2842']
his_acc des_w_na:  ['0.6039', '0.4292', '0.3941', '0.3864', '0.3174', '0.2878', '0.2444', '0.2352']
cur_acc rrf_w_na:  ['0.6114', '0.2865', '0.4310', '0.3442', '0.2907', '0.2251', '0.2258', '0.2781']
his_acc rrf_w_na:  ['0.6114', '0.4363', '0.3983', '0.3968', '0.3263', '0.3018', '0.2550', '0.2500']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 131.1540377CurrentTrain: epoch  0, batch     1 | loss: 81.7521342CurrentTrain: epoch  0, batch     2 | loss: 123.4473249CurrentTrain: epoch  0, batch     3 | loss: 102.0241308CurrentTrain: epoch  0, batch     4 | loss: 101.3912940CurrentTrain: epoch  0, batch     5 | loss: 87.3394263CurrentTrain: epoch  0, batch     6 | loss: 118.8629065CurrentTrain: epoch  0, batch     7 | loss: 99.8933556CurrentTrain: epoch  0, batch     8 | loss: 146.8575182CurrentTrain: epoch  0, batch     9 | loss: 100.2398050CurrentTrain: epoch  0, batch    10 | loss: 118.0654607CurrentTrain: epoch  0, batch    11 | loss: 118.7875254CurrentTrain: epoch  0, batch    12 | loss: 100.3811781CurrentTrain: epoch  0, batch    13 | loss: 119.2752549CurrentTrain: epoch  0, batch    14 | loss: 99.7980720CurrentTrain: epoch  0, batch    15 | loss: 192.5667647CurrentTrain: epoch  0, batch    16 | loss: 119.0962276CurrentTrain: epoch  0, batch    17 | loss: 118.5672934CurrentTrain: epoch  0, batch    18 | loss: 117.7933383CurrentTrain: epoch  0, batch    19 | loss: 99.6114873CurrentTrain: epoch  0, batch    20 | loss: 86.5389397CurrentTrain: epoch  0, batch    21 | loss: 87.6979338CurrentTrain: epoch  0, batch    22 | loss: 99.0517976CurrentTrain: epoch  0, batch    23 | loss: 98.5022523CurrentTrain: epoch  0, batch    24 | loss: 85.6660142CurrentTrain: epoch  0, batch    25 | loss: 116.7119281CurrentTrain: epoch  0, batch    26 | loss: 98.3233811CurrentTrain: epoch  0, batch    27 | loss: 76.0373235CurrentTrain: epoch  0, batch    28 | loss: 117.3785594CurrentTrain: epoch  0, batch    29 | loss: 85.5303906CurrentTrain: epoch  0, batch    30 | loss: 117.0282942CurrentTrain: epoch  0, batch    31 | loss: 84.9382230CurrentTrain: epoch  0, batch    32 | loss: 75.1690447CurrentTrain: epoch  0, batch    33 | loss: 191.2542723CurrentTrain: epoch  0, batch    34 | loss: 83.8498609CurrentTrain: epoch  0, batch    35 | loss: 85.1626672CurrentTrain: epoch  0, batch    36 | loss: 117.2929743CurrentTrain: epoch  0, batch    37 | loss: 74.6634313CurrentTrain: epoch  0, batch    38 | loss: 96.4367289CurrentTrain: epoch  0, batch    39 | loss: 96.8601590CurrentTrain: epoch  0, batch    40 | loss: 97.0919267CurrentTrain: epoch  0, batch    41 | loss: 73.3276951CurrentTrain: epoch  0, batch    42 | loss: 188.3682105CurrentTrain: epoch  0, batch    43 | loss: 73.9542173CurrentTrain: epoch  0, batch    44 | loss: 113.9495334CurrentTrain: epoch  0, batch    45 | loss: 82.6446613CurrentTrain: epoch  0, batch    46 | loss: 96.6292293CurrentTrain: epoch  0, batch    47 | loss: 113.0693973CurrentTrain: epoch  0, batch    48 | loss: 82.4703553CurrentTrain: epoch  0, batch    49 | loss: 94.3153719CurrentTrain: epoch  0, batch    50 | loss: 94.2091027CurrentTrain: epoch  0, batch    51 | loss: 113.9633695CurrentTrain: epoch  0, batch    52 | loss: 81.5095363CurrentTrain: epoch  0, batch    53 | loss: 83.1254775CurrentTrain: epoch  0, batch    54 | loss: 95.7418543CurrentTrain: epoch  0, batch    55 | loss: 80.3072707CurrentTrain: epoch  0, batch    56 | loss: 80.1573765CurrentTrain: epoch  0, batch    57 | loss: 108.4685119CurrentTrain: epoch  0, batch    58 | loss: 109.6968845CurrentTrain: epoch  0, batch    59 | loss: 137.4214341CurrentTrain: epoch  0, batch    60 | loss: 90.7150223CurrentTrain: epoch  0, batch    61 | loss: 90.8508643CurrentTrain: epoch  0, batch    62 | loss: 90.9788210CurrentTrain: epoch  0, batch    63 | loss: 76.2717873CurrentTrain: epoch  0, batch    64 | loss: 93.1248522CurrentTrain: epoch  0, batch    65 | loss: 89.0983986CurrentTrain: epoch  0, batch    66 | loss: 77.4468307CurrentTrain: epoch  0, batch    67 | loss: 69.1259421CurrentTrain: epoch  0, batch    68 | loss: 75.2961720CurrentTrain: epoch  0, batch    69 | loss: 93.9729391CurrentTrain: epoch  0, batch    70 | loss: 110.1580180CurrentTrain: epoch  0, batch    71 | loss: 106.6333989CurrentTrain: epoch  0, batch    72 | loss: 73.3126244CurrentTrain: epoch  0, batch    73 | loss: 104.1000643CurrentTrain: epoch  0, batch    74 | loss: 72.3980596CurrentTrain: epoch  0, batch    75 | loss: 91.7448483CurrentTrain: epoch  0, batch    76 | loss: 136.8178129CurrentTrain: epoch  0, batch    77 | loss: 180.5603913CurrentTrain: epoch  0, batch    78 | loss: 77.7168488CurrentTrain: epoch  0, batch    79 | loss: 77.0711834CurrentTrain: epoch  0, batch    80 | loss: 133.9609176CurrentTrain: epoch  0, batch    81 | loss: 78.1045210CurrentTrain: epoch  0, batch    82 | loss: 136.9123036CurrentTrain: epoch  0, batch    83 | loss: 181.7741233CurrentTrain: epoch  0, batch    84 | loss: 79.9613158CurrentTrain: epoch  0, batch    85 | loss: 66.4182264CurrentTrain: epoch  0, batch    86 | loss: 87.0344939CurrentTrain: epoch  0, batch    87 | loss: 64.8189603CurrentTrain: epoch  0, batch    88 | loss: 109.6626028CurrentTrain: epoch  0, batch    89 | loss: 76.1518377CurrentTrain: epoch  0, batch    90 | loss: 87.8125899CurrentTrain: epoch  0, batch    91 | loss: 107.8239744CurrentTrain: epoch  0, batch    92 | loss: 91.4816792CurrentTrain: epoch  0, batch    93 | loss: 109.7284264CurrentTrain: epoch  0, batch    94 | loss: 83.9241118CurrentTrain: epoch  0, batch    95 | loss: 74.3175322CurrentTrain: epoch  1, batch     0 | loss: 79.8891555CurrentTrain: epoch  1, batch     1 | loss: 88.6488024CurrentTrain: epoch  1, batch     2 | loss: 65.4415086CurrentTrain: epoch  1, batch     3 | loss: 133.3884322CurrentTrain: epoch  1, batch     4 | loss: 106.1535304CurrentTrain: epoch  1, batch     5 | loss: 87.9854382CurrentTrain: epoch  1, batch     6 | loss: 132.7724467CurrentTrain: epoch  1, batch     7 | loss: 72.3656861CurrentTrain: epoch  1, batch     8 | loss: 86.2222260CurrentTrain: epoch  1, batch     9 | loss: 87.7284511CurrentTrain: epoch  1, batch    10 | loss: 85.4556164CurrentTrain: epoch  1, batch    11 | loss: 86.3327757CurrentTrain: epoch  1, batch    12 | loss: 178.0355088CurrentTrain: epoch  1, batch    13 | loss: 75.4817482CurrentTrain: epoch  1, batch    14 | loss: 70.8821949CurrentTrain: epoch  1, batch    15 | loss: 88.0146062CurrentTrain: epoch  1, batch    16 | loss: 63.8556903CurrentTrain: epoch  1, batch    17 | loss: 75.7389320CurrentTrain: epoch  1, batch    18 | loss: 59.9168978CurrentTrain: epoch  1, batch    19 | loss: 105.8737365CurrentTrain: epoch  1, batch    20 | loss: 81.9452377CurrentTrain: epoch  1, batch    21 | loss: 87.5194099CurrentTrain: epoch  1, batch    22 | loss: 71.0057776CurrentTrain: epoch  1, batch    23 | loss: 105.3464088CurrentTrain: epoch  1, batch    24 | loss: 89.9499904CurrentTrain: epoch  1, batch    25 | loss: 99.3400074CurrentTrain: epoch  1, batch    26 | loss: 97.1816689CurrentTrain: epoch  1, batch    27 | loss: 85.2802330CurrentTrain: epoch  1, batch    28 | loss: 86.7336297CurrentTrain: epoch  1, batch    29 | loss: 85.1319519CurrentTrain: epoch  1, batch    30 | loss: 84.2514495CurrentTrain: epoch  1, batch    31 | loss: 101.7113741CurrentTrain: epoch  1, batch    32 | loss: 72.5873056CurrentTrain: epoch  1, batch    33 | loss: 101.1843599CurrentTrain: epoch  1, batch    34 | loss: 85.2199709CurrentTrain: epoch  1, batch    35 | loss: 83.4145407CurrentTrain: epoch  1, batch    36 | loss: 73.8702759CurrentTrain: epoch  1, batch    37 | loss: 82.4771245CurrentTrain: epoch  1, batch    38 | loss: 63.7704792CurrentTrain: epoch  1, batch    39 | loss: 104.0929854CurrentTrain: epoch  1, batch    40 | loss: 102.1517098CurrentTrain: epoch  1, batch    41 | loss: 102.1088694CurrentTrain: epoch  1, batch    42 | loss: 85.9223700CurrentTrain: epoch  1, batch    43 | loss: 84.3150352CurrentTrain: epoch  1, batch    44 | loss: 86.2159730CurrentTrain: epoch  1, batch    45 | loss: 127.2738697CurrentTrain: epoch  1, batch    46 | loss: 79.1667280CurrentTrain: epoch  1, batch    47 | loss: 100.2563205CurrentTrain: epoch  1, batch    48 | loss: 102.4623235CurrentTrain: epoch  1, batch    49 | loss: 103.6640271CurrentTrain: epoch  1, batch    50 | loss: 75.6029522CurrentTrain: epoch  1, batch    51 | loss: 105.8525614CurrentTrain: epoch  1, batch    52 | loss: 82.3015236CurrentTrain: epoch  1, batch    53 | loss: 87.5542688CurrentTrain: epoch  1, batch    54 | loss: 133.3887662CurrentTrain: epoch  1, batch    55 | loss: 80.4831396CurrentTrain: epoch  1, batch    56 | loss: 67.2567222CurrentTrain: epoch  1, batch    57 | loss: 102.7896872CurrentTrain: epoch  1, batch    58 | loss: 84.7382202CurrentTrain: epoch  1, batch    59 | loss: 81.4698691CurrentTrain: epoch  1, batch    60 | loss: 68.1754330CurrentTrain: epoch  1, batch    61 | loss: 86.1118175CurrentTrain: epoch  1, batch    62 | loss: 99.3533767CurrentTrain: epoch  1, batch    63 | loss: 83.1145356CurrentTrain: epoch  1, batch    64 | loss: 82.7444388CurrentTrain: epoch  1, batch    65 | loss: 74.1320920CurrentTrain: epoch  1, batch    66 | loss: 84.9565404CurrentTrain: epoch  1, batch    67 | loss: 72.1067672CurrentTrain: epoch  1, batch    68 | loss: 71.9168181CurrentTrain: epoch  1, batch    69 | loss: 103.8868268CurrentTrain: epoch  1, batch    70 | loss: 100.2062194CurrentTrain: epoch  1, batch    71 | loss: 71.2429178CurrentTrain: epoch  1, batch    72 | loss: 175.2157735CurrentTrain: epoch  1, batch    73 | loss: 63.5451224CurrentTrain: epoch  1, batch    74 | loss: 71.5464549CurrentTrain: epoch  1, batch    75 | loss: 101.2207245CurrentTrain: epoch  1, batch    76 | loss: 102.8125290CurrentTrain: epoch  1, batch    77 | loss: 99.7341020CurrentTrain: epoch  1, batch    78 | loss: 98.1038437CurrentTrain: epoch  1, batch    79 | loss: 69.9715040CurrentTrain: epoch  1, batch    80 | loss: 87.9827811CurrentTrain: epoch  1, batch    81 | loss: 60.7311421CurrentTrain: epoch  1, batch    82 | loss: 74.3599299CurrentTrain: epoch  1, batch    83 | loss: 70.8466572CurrentTrain: epoch  1, batch    84 | loss: 80.1739074CurrentTrain: epoch  1, batch    85 | loss: 100.0872393CurrentTrain: epoch  1, batch    86 | loss: 89.7593187CurrentTrain: epoch  1, batch    87 | loss: 85.2564581CurrentTrain: epoch  1, batch    88 | loss: 103.4814317CurrentTrain: epoch  1, batch    89 | loss: 175.8422393CurrentTrain: epoch  1, batch    90 | loss: 82.1953645CurrentTrain: epoch  1, batch    91 | loss: 132.7935147CurrentTrain: epoch  1, batch    92 | loss: 104.1843654CurrentTrain: epoch  1, batch    93 | loss: 69.0586744CurrentTrain: epoch  1, batch    94 | loss: 98.6677636CurrentTrain: epoch  1, batch    95 | loss: 72.0286597CurrentTrain: epoch  2, batch     0 | loss: 98.9712133CurrentTrain: epoch  2, batch     1 | loss: 125.5951498CurrentTrain: epoch  2, batch     2 | loss: 84.3356991CurrentTrain: epoch  2, batch     3 | loss: 106.5963752CurrentTrain: epoch  2, batch     4 | loss: 82.8440727CurrentTrain: epoch  2, batch     5 | loss: 86.1897033CurrentTrain: epoch  2, batch     6 | loss: 81.0825537CurrentTrain: epoch  2, batch     7 | loss: 82.2632207CurrentTrain: epoch  2, batch     8 | loss: 82.6244517CurrentTrain: epoch  2, batch     9 | loss: 65.0366607CurrentTrain: epoch  2, batch    10 | loss: 98.5835329CurrentTrain: epoch  2, batch    11 | loss: 63.1182967CurrentTrain: epoch  2, batch    12 | loss: 127.9603818CurrentTrain: epoch  2, batch    13 | loss: 68.6308965CurrentTrain: epoch  2, batch    14 | loss: 68.8854103CurrentTrain: epoch  2, batch    15 | loss: 68.4371525CurrentTrain: epoch  2, batch    16 | loss: 101.3592851CurrentTrain: epoch  2, batch    17 | loss: 83.1791539CurrentTrain: epoch  2, batch    18 | loss: 101.5738244CurrentTrain: epoch  2, batch    19 | loss: 80.7263086CurrentTrain: epoch  2, batch    20 | loss: 99.9014470CurrentTrain: epoch  2, batch    21 | loss: 82.0736611CurrentTrain: epoch  2, batch    22 | loss: 130.7919951CurrentTrain: epoch  2, batch    23 | loss: 96.4771513CurrentTrain: epoch  2, batch    24 | loss: 83.8154300CurrentTrain: epoch  2, batch    25 | loss: 98.5344201CurrentTrain: epoch  2, batch    26 | loss: 100.5416932CurrentTrain: epoch  2, batch    27 | loss: 84.2926386CurrentTrain: epoch  2, batch    28 | loss: 86.9156615CurrentTrain: epoch  2, batch    29 | loss: 103.0823293CurrentTrain: epoch  2, batch    30 | loss: 69.6076064CurrentTrain: epoch  2, batch    31 | loss: 73.7076457CurrentTrain: epoch  2, batch    32 | loss: 82.4760162CurrentTrain: epoch  2, batch    33 | loss: 71.3075741CurrentTrain: epoch  2, batch    34 | loss: 83.6908085CurrentTrain: epoch  2, batch    35 | loss: 100.4868072CurrentTrain: epoch  2, batch    36 | loss: 79.7557832CurrentTrain: epoch  2, batch    37 | loss: 67.8110581CurrentTrain: epoch  2, batch    38 | loss: 60.4478623CurrentTrain: epoch  2, batch    39 | loss: 67.1454567CurrentTrain: epoch  2, batch    40 | loss: 80.2363911CurrentTrain: epoch  2, batch    41 | loss: 82.1727005CurrentTrain: epoch  2, batch    42 | loss: 80.8848247CurrentTrain: epoch  2, batch    43 | loss: 82.9433091CurrentTrain: epoch  2, batch    44 | loss: 85.8245781CurrentTrain: epoch  2, batch    45 | loss: 79.8264423CurrentTrain: epoch  2, batch    46 | loss: 79.7406564CurrentTrain: epoch  2, batch    47 | loss: 123.5592156CurrentTrain: epoch  2, batch    48 | loss: 124.1968406CurrentTrain: epoch  2, batch    49 | loss: 65.1819279CurrentTrain: epoch  2, batch    50 | loss: 60.7489738CurrentTrain: epoch  2, batch    51 | loss: 68.1629625CurrentTrain: epoch  2, batch    52 | loss: 71.4261601CurrentTrain: epoch  2, batch    53 | loss: 66.3217720CurrentTrain: epoch  2, batch    54 | loss: 77.9601306CurrentTrain: epoch  2, batch    55 | loss: 97.5878018CurrentTrain: epoch  2, batch    56 | loss: 67.7401076CurrentTrain: epoch  2, batch    57 | loss: 97.4288553CurrentTrain: epoch  2, batch    58 | loss: 126.6034071CurrentTrain: epoch  2, batch    59 | loss: 68.5034646CurrentTrain: epoch  2, batch    60 | loss: 82.1803603CurrentTrain: epoch  2, batch    61 | loss: 84.7752024CurrentTrain: epoch  2, batch    62 | loss: 67.6708536CurrentTrain: epoch  2, batch    63 | loss: 71.2786274CurrentTrain: epoch  2, batch    64 | loss: 74.0742323CurrentTrain: epoch  2, batch    65 | loss: 56.6973394CurrentTrain: epoch  2, batch    66 | loss: 96.2426838CurrentTrain: epoch  2, batch    67 | loss: 68.1779552CurrentTrain: epoch  2, batch    68 | loss: 82.2768565CurrentTrain: epoch  2, batch    69 | loss: 81.0786465CurrentTrain: epoch  2, batch    70 | loss: 86.9303188CurrentTrain: epoch  2, batch    71 | loss: 121.0827348CurrentTrain: epoch  2, batch    72 | loss: 56.4841066CurrentTrain: epoch  2, batch    73 | loss: 98.2221474CurrentTrain: epoch  2, batch    74 | loss: 58.7699360CurrentTrain: epoch  2, batch    75 | loss: 81.7509840CurrentTrain: epoch  2, batch    76 | loss: 65.3785265CurrentTrain: epoch  2, batch    77 | loss: 61.0180704CurrentTrain: epoch  2, batch    78 | loss: 126.6993206CurrentTrain: epoch  2, batch    79 | loss: 100.5287649CurrentTrain: epoch  2, batch    80 | loss: 59.5673429CurrentTrain: epoch  2, batch    81 | loss: 73.8422366CurrentTrain: epoch  2, batch    82 | loss: 59.7616342CurrentTrain: epoch  2, batch    83 | loss: 80.4994307CurrentTrain: epoch  2, batch    84 | loss: 98.7477968CurrentTrain: epoch  2, batch    85 | loss: 66.6638088CurrentTrain: epoch  2, batch    86 | loss: 62.1310343CurrentTrain: epoch  2, batch    87 | loss: 79.2415267CurrentTrain: epoch  2, batch    88 | loss: 130.9097930CurrentTrain: epoch  2, batch    89 | loss: 71.2600540CurrentTrain: epoch  2, batch    90 | loss: 100.5504959CurrentTrain: epoch  2, batch    91 | loss: 103.1887567CurrentTrain: epoch  2, batch    92 | loss: 79.5826823CurrentTrain: epoch  2, batch    93 | loss: 83.7187936CurrentTrain: epoch  2, batch    94 | loss: 100.6140699CurrentTrain: epoch  2, batch    95 | loss: 69.4126236CurrentTrain: epoch  3, batch     0 | loss: 97.0153307CurrentTrain: epoch  3, batch     1 | loss: 77.2428230CurrentTrain: epoch  3, batch     2 | loss: 79.6424399CurrentTrain: epoch  3, batch     3 | loss: 69.6943726CurrentTrain: epoch  3, batch     4 | loss: 68.1998124CurrentTrain: epoch  3, batch     5 | loss: 77.3400562CurrentTrain: epoch  3, batch     6 | loss: 80.3845440CurrentTrain: epoch  3, batch     7 | loss: 118.9172910CurrentTrain: epoch  3, batch     8 | loss: 58.0428761CurrentTrain: epoch  3, batch     9 | loss: 101.0464816CurrentTrain: epoch  3, batch    10 | loss: 74.1517014CurrentTrain: epoch  3, batch    11 | loss: 66.4740258CurrentTrain: epoch  3, batch    12 | loss: 119.3556929CurrentTrain: epoch  3, batch    13 | loss: 67.6577599CurrentTrain: epoch  3, batch    14 | loss: 166.6723535CurrentTrain: epoch  3, batch    15 | loss: 124.6305380CurrentTrain: epoch  3, batch    16 | loss: 93.8834041CurrentTrain: epoch  3, batch    17 | loss: 80.6972563CurrentTrain: epoch  3, batch    18 | loss: 64.9757782CurrentTrain: epoch  3, batch    19 | loss: 67.8174956CurrentTrain: epoch  3, batch    20 | loss: 121.2715425CurrentTrain: epoch  3, batch    21 | loss: 65.2165778CurrentTrain: epoch  3, batch    22 | loss: 65.6292332CurrentTrain: epoch  3, batch    23 | loss: 68.8787953CurrentTrain: epoch  3, batch    24 | loss: 78.8070802CurrentTrain: epoch  3, batch    25 | loss: 83.2816083CurrentTrain: epoch  3, batch    26 | loss: 61.1729061CurrentTrain: epoch  3, batch    27 | loss: 62.5474122CurrentTrain: epoch  3, batch    28 | loss: 80.8280497CurrentTrain: epoch  3, batch    29 | loss: 63.9227506CurrentTrain: epoch  3, batch    30 | loss: 69.3234220CurrentTrain: epoch  3, batch    31 | loss: 79.5189025CurrentTrain: epoch  3, batch    32 | loss: 80.4872788CurrentTrain: epoch  3, batch    33 | loss: 77.3487595CurrentTrain: epoch  3, batch    34 | loss: 78.8207087CurrentTrain: epoch  3, batch    35 | loss: 77.9195349CurrentTrain: epoch  3, batch    36 | loss: 75.6813714CurrentTrain: epoch  3, batch    37 | loss: 96.9999933CurrentTrain: epoch  3, batch    38 | loss: 105.5217078CurrentTrain: epoch  3, batch    39 | loss: 81.6626774CurrentTrain: epoch  3, batch    40 | loss: 58.4200563CurrentTrain: epoch  3, batch    41 | loss: 98.5757291CurrentTrain: epoch  3, batch    42 | loss: 84.8792668CurrentTrain: epoch  3, batch    43 | loss: 68.5434729CurrentTrain: epoch  3, batch    44 | loss: 80.3989045CurrentTrain: epoch  3, batch    45 | loss: 81.5075704CurrentTrain: epoch  3, batch    46 | loss: 68.5719510CurrentTrain: epoch  3, batch    47 | loss: 76.5806591CurrentTrain: epoch  3, batch    48 | loss: 64.5041378CurrentTrain: epoch  3, batch    49 | loss: 70.7894625CurrentTrain: epoch  3, batch    50 | loss: 65.9126536CurrentTrain: epoch  3, batch    51 | loss: 75.1024790CurrentTrain: epoch  3, batch    52 | loss: 82.5361447CurrentTrain: epoch  3, batch    53 | loss: 77.9961071CurrentTrain: epoch  3, batch    54 | loss: 67.9507867CurrentTrain: epoch  3, batch    55 | loss: 126.8865273CurrentTrain: epoch  3, batch    56 | loss: 76.8473655CurrentTrain: epoch  3, batch    57 | loss: 92.0402365CurrentTrain: epoch  3, batch    58 | loss: 56.7913782CurrentTrain: epoch  3, batch    59 | loss: 96.3368883CurrentTrain: epoch  3, batch    60 | loss: 79.2299864CurrentTrain: epoch  3, batch    61 | loss: 79.5879025CurrentTrain: epoch  3, batch    62 | loss: 174.6088633CurrentTrain: epoch  3, batch    63 | loss: 80.9289507CurrentTrain: epoch  3, batch    64 | loss: 77.5744427CurrentTrain: epoch  3, batch    65 | loss: 101.4372915CurrentTrain: epoch  3, batch    66 | loss: 74.0011015CurrentTrain: epoch  3, batch    67 | loss: 63.9241207CurrentTrain: epoch  3, batch    68 | loss: 67.2001706CurrentTrain: epoch  3, batch    69 | loss: 68.2515860CurrentTrain: epoch  3, batch    70 | loss: 54.6342406CurrentTrain: epoch  3, batch    71 | loss: 75.5165105CurrentTrain: epoch  3, batch    72 | loss: 73.6498340CurrentTrain: epoch  3, batch    73 | loss: 81.5381430CurrentTrain: epoch  3, batch    74 | loss: 102.1060263CurrentTrain: epoch  3, batch    75 | loss: 78.2588129CurrentTrain: epoch  3, batch    76 | loss: 82.1361299CurrentTrain: epoch  3, batch    77 | loss: 82.0504937CurrentTrain: epoch  3, batch    78 | loss: 68.3617753CurrentTrain: epoch  3, batch    79 | loss: 97.5099080CurrentTrain: epoch  3, batch    80 | loss: 67.7954291CurrentTrain: epoch  3, batch    81 | loss: 94.8842652CurrentTrain: epoch  3, batch    82 | loss: 95.2427864CurrentTrain: epoch  3, batch    83 | loss: 60.9654413CurrentTrain: epoch  3, batch    84 | loss: 83.7626951CurrentTrain: epoch  3, batch    85 | loss: 120.3370215CurrentTrain: epoch  3, batch    86 | loss: 68.2219367CurrentTrain: epoch  3, batch    87 | loss: 95.3824419CurrentTrain: epoch  3, batch    88 | loss: 98.8447445CurrentTrain: epoch  3, batch    89 | loss: 74.5434965CurrentTrain: epoch  3, batch    90 | loss: 77.2769473CurrentTrain: epoch  3, batch    91 | loss: 79.6109489CurrentTrain: epoch  3, batch    92 | loss: 97.3933726CurrentTrain: epoch  3, batch    93 | loss: 71.1696348CurrentTrain: epoch  3, batch    94 | loss: 64.6581755CurrentTrain: epoch  3, batch    95 | loss: 71.1641739CurrentTrain: epoch  4, batch     0 | loss: 77.3078262CurrentTrain: epoch  4, batch     1 | loss: 57.3978760CurrentTrain: epoch  4, batch     2 | loss: 62.3246805CurrentTrain: epoch  4, batch     3 | loss: 157.3055510CurrentTrain: epoch  4, batch     4 | loss: 91.8952255CurrentTrain: epoch  4, batch     5 | loss: 71.2195335CurrentTrain: epoch  4, batch     6 | loss: 67.5361776CurrentTrain: epoch  4, batch     7 | loss: 76.2188327CurrentTrain: epoch  4, batch     8 | loss: 91.0413938CurrentTrain: epoch  4, batch     9 | loss: 67.3263728CurrentTrain: epoch  4, batch    10 | loss: 95.4069950CurrentTrain: epoch  4, batch    11 | loss: 65.4037149CurrentTrain: epoch  4, batch    12 | loss: 77.4968750CurrentTrain: epoch  4, batch    13 | loss: 64.7417751CurrentTrain: epoch  4, batch    14 | loss: 94.9260547CurrentTrain: epoch  4, batch    15 | loss: 100.4827172CurrentTrain: epoch  4, batch    16 | loss: 124.5017244CurrentTrain: epoch  4, batch    17 | loss: 119.2115037CurrentTrain: epoch  4, batch    18 | loss: 78.8318624CurrentTrain: epoch  4, batch    19 | loss: 64.2180451CurrentTrain: epoch  4, batch    20 | loss: 63.8784816CurrentTrain: epoch  4, batch    21 | loss: 119.1696516CurrentTrain: epoch  4, batch    22 | loss: 97.0144296CurrentTrain: epoch  4, batch    23 | loss: 74.6667916CurrentTrain: epoch  4, batch    24 | loss: 93.9963017CurrentTrain: epoch  4, batch    25 | loss: 62.0309149CurrentTrain: epoch  4, batch    26 | loss: 82.1538195CurrentTrain: epoch  4, batch    27 | loss: 93.8658792CurrentTrain: epoch  4, batch    28 | loss: 116.7681950CurrentTrain: epoch  4, batch    29 | loss: 74.4301663CurrentTrain: epoch  4, batch    30 | loss: 79.5566110CurrentTrain: epoch  4, batch    31 | loss: 63.5395120CurrentTrain: epoch  4, batch    32 | loss: 80.6367623CurrentTrain: epoch  4, batch    33 | loss: 56.8327386CurrentTrain: epoch  4, batch    34 | loss: 94.8405586CurrentTrain: epoch  4, batch    35 | loss: 80.3102049CurrentTrain: epoch  4, batch    36 | loss: 100.2120755CurrentTrain: epoch  4, batch    37 | loss: 70.7942824CurrentTrain: epoch  4, batch    38 | loss: 78.1519952CurrentTrain: epoch  4, batch    39 | loss: 63.2994317CurrentTrain: epoch  4, batch    40 | loss: 67.6588254CurrentTrain: epoch  4, batch    41 | loss: 79.0274104CurrentTrain: epoch  4, batch    42 | loss: 56.8765628CurrentTrain: epoch  4, batch    43 | loss: 97.6734644CurrentTrain: epoch  4, batch    44 | loss: 74.6490322CurrentTrain: epoch  4, batch    45 | loss: 91.3448930CurrentTrain: epoch  4, batch    46 | loss: 81.7252138CurrentTrain: epoch  4, batch    47 | loss: 118.1444114CurrentTrain: epoch  4, batch    48 | loss: 92.5848292CurrentTrain: epoch  4, batch    49 | loss: 63.2324353CurrentTrain: epoch  4, batch    50 | loss: 80.5804589CurrentTrain: epoch  4, batch    51 | loss: 121.2019125CurrentTrain: epoch  4, batch    52 | loss: 70.1479287CurrentTrain: epoch  4, batch    53 | loss: 79.3788632CurrentTrain: epoch  4, batch    54 | loss: 94.7852467CurrentTrain: epoch  4, batch    55 | loss: 91.0882562CurrentTrain: epoch  4, batch    56 | loss: 74.5325792CurrentTrain: epoch  4, batch    57 | loss: 82.7998419CurrentTrain: epoch  4, batch    58 | loss: 79.1058112CurrentTrain: epoch  4, batch    59 | loss: 68.2627129CurrentTrain: epoch  4, batch    60 | loss: 76.3032247CurrentTrain: epoch  4, batch    61 | loss: 59.0992752CurrentTrain: epoch  4, batch    62 | loss: 61.5247162CurrentTrain: epoch  4, batch    63 | loss: 79.2002679CurrentTrain: epoch  4, batch    64 | loss: 95.3257594CurrentTrain: epoch  4, batch    65 | loss: 62.8862410CurrentTrain: epoch  4, batch    66 | loss: 94.8561109CurrentTrain: epoch  4, batch    67 | loss: 63.5092410CurrentTrain: epoch  4, batch    68 | loss: 79.5347035CurrentTrain: epoch  4, batch    69 | loss: 71.9999180CurrentTrain: epoch  4, batch    70 | loss: 57.4319352CurrentTrain: epoch  4, batch    71 | loss: 98.2855068CurrentTrain: epoch  4, batch    72 | loss: 164.5614708CurrentTrain: epoch  4, batch    73 | loss: 65.1161309CurrentTrain: epoch  4, batch    74 | loss: 95.2460824CurrentTrain: epoch  4, batch    75 | loss: 69.2362247CurrentTrain: epoch  4, batch    76 | loss: 55.0016072CurrentTrain: epoch  4, batch    77 | loss: 78.7145395CurrentTrain: epoch  4, batch    78 | loss: 59.1676155CurrentTrain: epoch  4, batch    79 | loss: 78.0153944CurrentTrain: epoch  4, batch    80 | loss: 120.1009421CurrentTrain: epoch  4, batch    81 | loss: 76.6367351CurrentTrain: epoch  4, batch    82 | loss: 66.9956697CurrentTrain: epoch  4, batch    83 | loss: 120.0451971CurrentTrain: epoch  4, batch    84 | loss: 73.4385549CurrentTrain: epoch  4, batch    85 | loss: 96.6775372CurrentTrain: epoch  4, batch    86 | loss: 96.4386708CurrentTrain: epoch  4, batch    87 | loss: 77.1114987CurrentTrain: epoch  4, batch    88 | loss: 76.9576394CurrentTrain: epoch  4, batch    89 | loss: 66.4658595CurrentTrain: epoch  4, batch    90 | loss: 67.8036853CurrentTrain: epoch  4, batch    91 | loss: 129.5613076CurrentTrain: epoch  4, batch    92 | loss: 66.0399688CurrentTrain: epoch  4, batch    93 | loss: 64.7281972CurrentTrain: epoch  4, batch    94 | loss: 95.4133543CurrentTrain: epoch  4, batch    95 | loss: 42.9122963CurrentTrain: epoch  5, batch     0 | loss: 73.1366074CurrentTrain: epoch  5, batch     1 | loss: 59.3037575CurrentTrain: epoch  5, batch     2 | loss: 62.4756868CurrentTrain: epoch  5, batch     3 | loss: 116.4895339CurrentTrain: epoch  5, batch     4 | loss: 87.5572614CurrentTrain: epoch  5, batch     5 | loss: 87.4195752CurrentTrain: epoch  5, batch     6 | loss: 93.3158597CurrentTrain: epoch  5, batch     7 | loss: 72.8105722CurrentTrain: epoch  5, batch     8 | loss: 52.1592624CurrentTrain: epoch  5, batch     9 | loss: 96.6859119CurrentTrain: epoch  5, batch    10 | loss: 119.8380231CurrentTrain: epoch  5, batch    11 | loss: 73.4988271CurrentTrain: epoch  5, batch    12 | loss: 91.3702901CurrentTrain: epoch  5, batch    13 | loss: 67.1030172CurrentTrain: epoch  5, batch    14 | loss: 74.1268870CurrentTrain: epoch  5, batch    15 | loss: 74.5584560CurrentTrain: epoch  5, batch    16 | loss: 74.0389009CurrentTrain: epoch  5, batch    17 | loss: 75.4877464CurrentTrain: epoch  5, batch    18 | loss: 81.2107934CurrentTrain: epoch  5, batch    19 | loss: 115.0326876CurrentTrain: epoch  5, batch    20 | loss: 73.5963681CurrentTrain: epoch  5, batch    21 | loss: 92.4821400CurrentTrain: epoch  5, batch    22 | loss: 73.1414475CurrentTrain: epoch  5, batch    23 | loss: 85.6301129CurrentTrain: epoch  5, batch    24 | loss: 94.9199905CurrentTrain: epoch  5, batch    25 | loss: 75.7055892CurrentTrain: epoch  5, batch    26 | loss: 63.0225068CurrentTrain: epoch  5, batch    27 | loss: 88.8496906CurrentTrain: epoch  5, batch    28 | loss: 95.0790951CurrentTrain: epoch  5, batch    29 | loss: 64.8075050CurrentTrain: epoch  5, batch    30 | loss: 99.9501335CurrentTrain: epoch  5, batch    31 | loss: 76.3686966CurrentTrain: epoch  5, batch    32 | loss: 64.6806778CurrentTrain: epoch  5, batch    33 | loss: 78.7590567CurrentTrain: epoch  5, batch    34 | loss: 73.7484905CurrentTrain: epoch  5, batch    35 | loss: 73.8752755CurrentTrain: epoch  5, batch    36 | loss: 64.2137057CurrentTrain: epoch  5, batch    37 | loss: 66.0358172CurrentTrain: epoch  5, batch    38 | loss: 95.0031346CurrentTrain: epoch  5, batch    39 | loss: 53.6172436CurrentTrain: epoch  5, batch    40 | loss: 76.0657419CurrentTrain: epoch  5, batch    41 | loss: 53.5307127CurrentTrain: epoch  5, batch    42 | loss: 64.8772143CurrentTrain: epoch  5, batch    43 | loss: 51.1053896CurrentTrain: epoch  5, batch    44 | loss: 54.1678885CurrentTrain: epoch  5, batch    45 | loss: 79.3825814CurrentTrain: epoch  5, batch    46 | loss: 93.9429307CurrentTrain: epoch  5, batch    47 | loss: 94.1012049CurrentTrain: epoch  5, batch    48 | loss: 117.0328067CurrentTrain: epoch  5, batch    49 | loss: 92.7013200CurrentTrain: epoch  5, batch    50 | loss: 75.7767898CurrentTrain: epoch  5, batch    51 | loss: 74.9639518CurrentTrain: epoch  5, batch    52 | loss: 117.5561870CurrentTrain: epoch  5, batch    53 | loss: 82.4767077CurrentTrain: epoch  5, batch    54 | loss: 61.2261463CurrentTrain: epoch  5, batch    55 | loss: 94.9672605CurrentTrain: epoch  5, batch    56 | loss: 56.0957058CurrentTrain: epoch  5, batch    57 | loss: 90.5746344CurrentTrain: epoch  5, batch    58 | loss: 119.2296044CurrentTrain: epoch  5, batch    59 | loss: 88.1848822CurrentTrain: epoch  5, batch    60 | loss: 61.8280387CurrentTrain: epoch  5, batch    61 | loss: 66.7793394CurrentTrain: epoch  5, batch    62 | loss: 79.5021714CurrentTrain: epoch  5, batch    63 | loss: 60.4394196CurrentTrain: epoch  5, batch    64 | loss: 82.3853442CurrentTrain: epoch  5, batch    65 | loss: 73.2258510CurrentTrain: epoch  5, batch    66 | loss: 76.9777477CurrentTrain: epoch  5, batch    67 | loss: 80.4040409CurrentTrain: epoch  5, batch    68 | loss: 93.3543718CurrentTrain: epoch  5, batch    69 | loss: 61.0382307CurrentTrain: epoch  5, batch    70 | loss: 73.8496800CurrentTrain: epoch  5, batch    71 | loss: 118.6809952CurrentTrain: epoch  5, batch    72 | loss: 53.5438778CurrentTrain: epoch  5, batch    73 | loss: 56.0915535CurrentTrain: epoch  5, batch    74 | loss: 68.5800470CurrentTrain: epoch  5, batch    75 | loss: 59.3397368CurrentTrain: epoch  5, batch    76 | loss: 67.0488715CurrentTrain: epoch  5, batch    77 | loss: 67.5773341CurrentTrain: epoch  5, batch    78 | loss: 54.9802328CurrentTrain: epoch  5, batch    79 | loss: 63.8106709CurrentTrain: epoch  5, batch    80 | loss: 73.1305611CurrentTrain: epoch  5, batch    81 | loss: 91.0434526CurrentTrain: epoch  5, batch    82 | loss: 71.4275452CurrentTrain: epoch  5, batch    83 | loss: 115.6213168CurrentTrain: epoch  5, batch    84 | loss: 51.7133655CurrentTrain: epoch  5, batch    85 | loss: 93.0950281CurrentTrain: epoch  5, batch    86 | loss: 81.2556261CurrentTrain: epoch  5, batch    87 | loss: 119.8994900CurrentTrain: epoch  5, batch    88 | loss: 63.6480804CurrentTrain: epoch  5, batch    89 | loss: 94.8622068CurrentTrain: epoch  5, batch    90 | loss: 122.0533899CurrentTrain: epoch  5, batch    91 | loss: 93.6852692CurrentTrain: epoch  5, batch    92 | loss: 74.5722769CurrentTrain: epoch  5, batch    93 | loss: 68.0197771CurrentTrain: epoch  5, batch    94 | loss: 52.9840643CurrentTrain: epoch  5, batch    95 | loss: 74.8675162CurrentTrain: epoch  6, batch     0 | loss: 74.3728575CurrentTrain: epoch  6, batch     1 | loss: 75.4290058CurrentTrain: epoch  6, batch     2 | loss: 73.3548933CurrentTrain: epoch  6, batch     3 | loss: 72.2896999CurrentTrain: epoch  6, batch     4 | loss: 94.6310466CurrentTrain: epoch  6, batch     5 | loss: 73.5496136CurrentTrain: epoch  6, batch     6 | loss: 69.5604265CurrentTrain: epoch  6, batch     7 | loss: 75.8220758CurrentTrain: epoch  6, batch     8 | loss: 59.8964015CurrentTrain: epoch  6, batch     9 | loss: 90.0183275CurrentTrain: epoch  6, batch    10 | loss: 60.4231850CurrentTrain: epoch  6, batch    11 | loss: 61.6116982CurrentTrain: epoch  6, batch    12 | loss: 62.4517974CurrentTrain: epoch  6, batch    13 | loss: 92.1207377CurrentTrain: epoch  6, batch    14 | loss: 64.7220329CurrentTrain: epoch  6, batch    15 | loss: 61.7239927CurrentTrain: epoch  6, batch    16 | loss: 83.0172246CurrentTrain: epoch  6, batch    17 | loss: 90.7776429CurrentTrain: epoch  6, batch    18 | loss: 62.0996777CurrentTrain: epoch  6, batch    19 | loss: 116.0765109CurrentTrain: epoch  6, batch    20 | loss: 63.6567442CurrentTrain: epoch  6, batch    21 | loss: 51.0485966CurrentTrain: epoch  6, batch    22 | loss: 62.0438820CurrentTrain: epoch  6, batch    23 | loss: 65.7799594CurrentTrain: epoch  6, batch    24 | loss: 70.3128597CurrentTrain: epoch  6, batch    25 | loss: 89.6579361CurrentTrain: epoch  6, batch    26 | loss: 86.4406175CurrentTrain: epoch  6, batch    27 | loss: 71.3492435CurrentTrain: epoch  6, batch    28 | loss: 90.0813402CurrentTrain: epoch  6, batch    29 | loss: 74.3609592CurrentTrain: epoch  6, batch    30 | loss: 75.8291848CurrentTrain: epoch  6, batch    31 | loss: 94.4094730CurrentTrain: epoch  6, batch    32 | loss: 93.2749129CurrentTrain: epoch  6, batch    33 | loss: 77.2302118CurrentTrain: epoch  6, batch    34 | loss: 90.6020555CurrentTrain: epoch  6, batch    35 | loss: 78.0657244CurrentTrain: epoch  6, batch    36 | loss: 110.2630726CurrentTrain: epoch  6, batch    37 | loss: 93.4524898CurrentTrain: epoch  6, batch    38 | loss: 88.8405346CurrentTrain: epoch  6, batch    39 | loss: 55.3272675CurrentTrain: epoch  6, batch    40 | loss: 71.9906123CurrentTrain: epoch  6, batch    41 | loss: 74.2560163CurrentTrain: epoch  6, batch    42 | loss: 71.2887527CurrentTrain: epoch  6, batch    43 | loss: 113.0758294CurrentTrain: epoch  6, batch    44 | loss: 118.3487473CurrentTrain: epoch  6, batch    45 | loss: 91.2341091CurrentTrain: epoch  6, batch    46 | loss: 85.7366777CurrentTrain: epoch  6, batch    47 | loss: 60.6604298CurrentTrain: epoch  6, batch    48 | loss: 94.7355402CurrentTrain: epoch  6, batch    49 | loss: 91.3575237CurrentTrain: epoch  6, batch    50 | loss: 61.1982914CurrentTrain: epoch  6, batch    51 | loss: 78.4346346CurrentTrain: epoch  6, batch    52 | loss: 63.3766247CurrentTrain: epoch  6, batch    53 | loss: 78.4413539CurrentTrain: epoch  6, batch    54 | loss: 94.5557750CurrentTrain: epoch  6, batch    55 | loss: 76.5018731CurrentTrain: epoch  6, batch    56 | loss: 89.4192173CurrentTrain: epoch  6, batch    57 | loss: 89.7274907CurrentTrain: epoch  6, batch    58 | loss: 90.8590451CurrentTrain: epoch  6, batch    59 | loss: 60.2735821CurrentTrain: epoch  6, batch    60 | loss: 74.3917777CurrentTrain: epoch  6, batch    61 | loss: 118.6297322CurrentTrain: epoch  6, batch    62 | loss: 76.9193502CurrentTrain: epoch  6, batch    63 | loss: 61.2086580CurrentTrain: epoch  6, batch    64 | loss: 53.1982986CurrentTrain: epoch  6, batch    65 | loss: 62.9847705CurrentTrain: epoch  6, batch    66 | loss: 93.3714776CurrentTrain: epoch  6, batch    67 | loss: 75.2707542CurrentTrain: epoch  6, batch    68 | loss: 71.0918083CurrentTrain: epoch  6, batch    69 | loss: 54.8353266CurrentTrain: epoch  6, batch    70 | loss: 75.4454552CurrentTrain: epoch  6, batch    71 | loss: 92.0068420CurrentTrain: epoch  6, batch    72 | loss: 91.9318628CurrentTrain: epoch  6, batch    73 | loss: 93.7652818CurrentTrain: epoch  6, batch    74 | loss: 65.2277092CurrentTrain: epoch  6, batch    75 | loss: 67.0084026CurrentTrain: epoch  6, batch    76 | loss: 55.8258221CurrentTrain: epoch  6, batch    77 | loss: 92.3890785CurrentTrain: epoch  6, batch    78 | loss: 72.3258609CurrentTrain: epoch  6, batch    79 | loss: 54.8803303CurrentTrain: epoch  6, batch    80 | loss: 77.4221260CurrentTrain: epoch  6, batch    81 | loss: 93.3546510CurrentTrain: epoch  6, batch    82 | loss: 67.2312283CurrentTrain: epoch  6, batch    83 | loss: 90.7193328CurrentTrain: epoch  6, batch    84 | loss: 76.6008186CurrentTrain: epoch  6, batch    85 | loss: 58.3129850CurrentTrain: epoch  6, batch    86 | loss: 79.6419404CurrentTrain: epoch  6, batch    87 | loss: 87.3475460CurrentTrain: epoch  6, batch    88 | loss: 88.7548080CurrentTrain: epoch  6, batch    89 | loss: 96.2314499CurrentTrain: epoch  6, batch    90 | loss: 94.9543097CurrentTrain: epoch  6, batch    91 | loss: 96.1057027CurrentTrain: epoch  6, batch    92 | loss: 73.4108885CurrentTrain: epoch  6, batch    93 | loss: 72.8645985CurrentTrain: epoch  6, batch    94 | loss: 75.7890045CurrentTrain: epoch  6, batch    95 | loss: 66.1628860CurrentTrain: epoch  7, batch     0 | loss: 57.8455016CurrentTrain: epoch  7, batch     1 | loss: 114.8610243CurrentTrain: epoch  7, batch     2 | loss: 90.3753410CurrentTrain: epoch  7, batch     3 | loss: 63.4274452CurrentTrain: epoch  7, batch     4 | loss: 73.3924474CurrentTrain: epoch  7, batch     5 | loss: 91.8069262CurrentTrain: epoch  7, batch     6 | loss: 69.1894626CurrentTrain: epoch  7, batch     7 | loss: 50.1660939CurrentTrain: epoch  7, batch     8 | loss: 62.7655747CurrentTrain: epoch  7, batch     9 | loss: 63.7041345CurrentTrain: epoch  7, batch    10 | loss: 95.7013395CurrentTrain: epoch  7, batch    11 | loss: 72.9200100CurrentTrain: epoch  7, batch    12 | loss: 89.0526468CurrentTrain: epoch  7, batch    13 | loss: 71.7525894CurrentTrain: epoch  7, batch    14 | loss: 87.2968543CurrentTrain: epoch  7, batch    15 | loss: 63.6981485CurrentTrain: epoch  7, batch    16 | loss: 85.2639231CurrentTrain: epoch  7, batch    17 | loss: 86.4412770CurrentTrain: epoch  7, batch    18 | loss: 61.0817439CurrentTrain: epoch  7, batch    19 | loss: 61.3048320CurrentTrain: epoch  7, batch    20 | loss: 60.2101045CurrentTrain: epoch  7, batch    21 | loss: 116.0428425CurrentTrain: epoch  7, batch    22 | loss: 59.9104637CurrentTrain: epoch  7, batch    23 | loss: 63.0411784CurrentTrain: epoch  7, batch    24 | loss: 60.5145198CurrentTrain: epoch  7, batch    25 | loss: 64.5272884CurrentTrain: epoch  7, batch    26 | loss: 92.5319218CurrentTrain: epoch  7, batch    27 | loss: 89.7997833CurrentTrain: epoch  7, batch    28 | loss: 72.2113302CurrentTrain: epoch  7, batch    29 | loss: 91.8919672CurrentTrain: epoch  7, batch    30 | loss: 74.2666881CurrentTrain: epoch  7, batch    31 | loss: 62.4143703CurrentTrain: epoch  7, batch    32 | loss: 70.9329095CurrentTrain: epoch  7, batch    33 | loss: 89.3814555CurrentTrain: epoch  7, batch    34 | loss: 72.5637545CurrentTrain: epoch  7, batch    35 | loss: 74.5250622CurrentTrain: epoch  7, batch    36 | loss: 67.2099524CurrentTrain: epoch  7, batch    37 | loss: 112.6378887CurrentTrain: epoch  7, batch    38 | loss: 75.0442077CurrentTrain: epoch  7, batch    39 | loss: 60.1019250CurrentTrain: epoch  7, batch    40 | loss: 59.0862060CurrentTrain: epoch  7, batch    41 | loss: 76.9432108CurrentTrain: epoch  7, batch    42 | loss: 113.8449216CurrentTrain: epoch  7, batch    43 | loss: 72.0753504CurrentTrain: epoch  7, batch    44 | loss: 61.0758817CurrentTrain: epoch  7, batch    45 | loss: 92.1873071CurrentTrain: epoch  7, batch    46 | loss: 77.1797840CurrentTrain: epoch  7, batch    47 | loss: 59.8575362CurrentTrain: epoch  7, batch    48 | loss: 87.6865262CurrentTrain: epoch  7, batch    49 | loss: 91.9737657CurrentTrain: epoch  7, batch    50 | loss: 63.5431635CurrentTrain: epoch  7, batch    51 | loss: 86.5783946CurrentTrain: epoch  7, batch    52 | loss: 109.3054324CurrentTrain: epoch  7, batch    53 | loss: 59.0895287CurrentTrain: epoch  7, batch    54 | loss: 62.7407388CurrentTrain: epoch  7, batch    55 | loss: 60.6471527CurrentTrain: epoch  7, batch    56 | loss: 57.1075771CurrentTrain: epoch  7, batch    57 | loss: 115.2019012CurrentTrain: epoch  7, batch    58 | loss: 56.7557093CurrentTrain: epoch  7, batch    59 | loss: 61.2217689CurrentTrain: epoch  7, batch    60 | loss: 71.1736403CurrentTrain: epoch  7, batch    61 | loss: 93.4320195CurrentTrain: epoch  7, batch    62 | loss: 63.9698159CurrentTrain: epoch  7, batch    63 | loss: 119.1609550CurrentTrain: epoch  7, batch    64 | loss: 60.7056310CurrentTrain: epoch  7, batch    65 | loss: 78.3653649CurrentTrain: epoch  7, batch    66 | loss: 118.4327738CurrentTrain: epoch  7, batch    67 | loss: 75.8261043CurrentTrain: epoch  7, batch    68 | loss: 111.8513885CurrentTrain: epoch  7, batch    69 | loss: 94.4868219CurrentTrain: epoch  7, batch    70 | loss: 87.7662472CurrentTrain: epoch  7, batch    71 | loss: 77.1171064CurrentTrain: epoch  7, batch    72 | loss: 72.5468568CurrentTrain: epoch  7, batch    73 | loss: 63.6401524CurrentTrain: epoch  7, batch    74 | loss: 112.0188794CurrentTrain: epoch  7, batch    75 | loss: 89.8562287CurrentTrain: epoch  7, batch    76 | loss: 71.9257911CurrentTrain: epoch  7, batch    77 | loss: 50.4568433CurrentTrain: epoch  7, batch    78 | loss: 67.0308223CurrentTrain: epoch  7, batch    79 | loss: 74.9040371CurrentTrain: epoch  7, batch    80 | loss: 62.3966900CurrentTrain: epoch  7, batch    81 | loss: 93.0426825CurrentTrain: epoch  7, batch    82 | loss: 66.1915307CurrentTrain: epoch  7, batch    83 | loss: 88.2841571CurrentTrain: epoch  7, batch    84 | loss: 75.5708531CurrentTrain: epoch  7, batch    85 | loss: 113.8293247CurrentTrain: epoch  7, batch    86 | loss: 72.6623118CurrentTrain: epoch  7, batch    87 | loss: 53.2660963CurrentTrain: epoch  7, batch    88 | loss: 56.6069653CurrentTrain: epoch  7, batch    89 | loss: 63.7483493CurrentTrain: epoch  7, batch    90 | loss: 72.6446647CurrentTrain: epoch  7, batch    91 | loss: 92.1066296CurrentTrain: epoch  7, batch    92 | loss: 60.1060312CurrentTrain: epoch  7, batch    93 | loss: 110.9784027CurrentTrain: epoch  7, batch    94 | loss: 69.6791348CurrentTrain: epoch  7, batch    95 | loss: 72.6260299CurrentTrain: epoch  8, batch     0 | loss: 63.5417727CurrentTrain: epoch  8, batch     1 | loss: 115.5543927CurrentTrain: epoch  8, batch     2 | loss: 59.1543534CurrentTrain: epoch  8, batch     3 | loss: 49.6728363CurrentTrain: epoch  8, batch     4 | loss: 89.6453052CurrentTrain: epoch  8, batch     5 | loss: 87.7565710CurrentTrain: epoch  8, batch     6 | loss: 85.9394043CurrentTrain: epoch  8, batch     7 | loss: 61.0304823CurrentTrain: epoch  8, batch     8 | loss: 72.2406285CurrentTrain: epoch  8, batch     9 | loss: 50.6956699CurrentTrain: epoch  8, batch    10 | loss: 69.7654310CurrentTrain: epoch  8, batch    11 | loss: 71.8457708CurrentTrain: epoch  8, batch    12 | loss: 73.4781846CurrentTrain: epoch  8, batch    13 | loss: 49.7993296CurrentTrain: epoch  8, batch    14 | loss: 112.5324856CurrentTrain: epoch  8, batch    15 | loss: 62.7240579CurrentTrain: epoch  8, batch    16 | loss: 91.6331540CurrentTrain: epoch  8, batch    17 | loss: 84.7266826CurrentTrain: epoch  8, batch    18 | loss: 60.6286157CurrentTrain: epoch  8, batch    19 | loss: 86.7612395CurrentTrain: epoch  8, batch    20 | loss: 73.1453204CurrentTrain: epoch  8, batch    21 | loss: 89.8243702CurrentTrain: epoch  8, batch    22 | loss: 154.1155523CurrentTrain: epoch  8, batch    23 | loss: 71.4682296CurrentTrain: epoch  8, batch    24 | loss: 75.4466601CurrentTrain: epoch  8, batch    25 | loss: 109.2604022CurrentTrain: epoch  8, batch    26 | loss: 72.9532244CurrentTrain: epoch  8, batch    27 | loss: 77.8459425CurrentTrain: epoch  8, batch    28 | loss: 74.5953300CurrentTrain: epoch  8, batch    29 | loss: 61.0898758CurrentTrain: epoch  8, batch    30 | loss: 71.1683277CurrentTrain: epoch  8, batch    31 | loss: 112.9974808CurrentTrain: epoch  8, batch    32 | loss: 69.6419131CurrentTrain: epoch  8, batch    33 | loss: 68.2562757CurrentTrain: epoch  8, batch    34 | loss: 58.0789395CurrentTrain: epoch  8, batch    35 | loss: 70.2202746CurrentTrain: epoch  8, batch    36 | loss: 69.8036976CurrentTrain: epoch  8, batch    37 | loss: 116.4417476CurrentTrain: epoch  8, batch    38 | loss: 91.8147385CurrentTrain: epoch  8, batch    39 | loss: 114.2177489CurrentTrain: epoch  8, batch    40 | loss: 51.0632772CurrentTrain: epoch  8, batch    41 | loss: 84.0226087CurrentTrain: epoch  8, batch    42 | loss: 91.0918321CurrentTrain: epoch  8, batch    43 | loss: 68.3268683CurrentTrain: epoch  8, batch    44 | loss: 52.9181105CurrentTrain: epoch  8, batch    45 | loss: 53.2937275CurrentTrain: epoch  8, batch    46 | loss: 76.2267330CurrentTrain: epoch  8, batch    47 | loss: 68.4692799CurrentTrain: epoch  8, batch    48 | loss: 76.4923700CurrentTrain: epoch  8, batch    49 | loss: 70.5510004CurrentTrain: epoch  8, batch    50 | loss: 73.7224277CurrentTrain: epoch  8, batch    51 | loss: 71.6254003CurrentTrain: epoch  8, batch    52 | loss: 68.8537310CurrentTrain: epoch  8, batch    53 | loss: 69.3680098CurrentTrain: epoch  8, batch    54 | loss: 113.4229358CurrentTrain: epoch  8, batch    55 | loss: 93.1568202CurrentTrain: epoch  8, batch    56 | loss: 53.7752902CurrentTrain: epoch  8, batch    57 | loss: 89.0086065CurrentTrain: epoch  8, batch    58 | loss: 69.8711769CurrentTrain: epoch  8, batch    59 | loss: 114.1148043CurrentTrain: epoch  8, batch    60 | loss: 61.1837342CurrentTrain: epoch  8, batch    61 | loss: 68.1210847CurrentTrain: epoch  8, batch    62 | loss: 156.1494072CurrentTrain: epoch  8, batch    63 | loss: 58.1767741CurrentTrain: epoch  8, batch    64 | loss: 72.2863221CurrentTrain: epoch  8, batch    65 | loss: 63.1765735CurrentTrain: epoch  8, batch    66 | loss: 68.9795854CurrentTrain: epoch  8, batch    67 | loss: 87.1552658CurrentTrain: epoch  8, batch    68 | loss: 93.3773663CurrentTrain: epoch  8, batch    69 | loss: 58.8820337CurrentTrain: epoch  8, batch    70 | loss: 62.4120296CurrentTrain: epoch  8, batch    71 | loss: 70.4736877CurrentTrain: epoch  8, batch    72 | loss: 71.9818406CurrentTrain: epoch  8, batch    73 | loss: 111.9874718CurrentTrain: epoch  8, batch    74 | loss: 71.7310956CurrentTrain: epoch  8, batch    75 | loss: 58.8458276CurrentTrain: epoch  8, batch    76 | loss: 90.0327876CurrentTrain: epoch  8, batch    77 | loss: 59.1923155CurrentTrain: epoch  8, batch    78 | loss: 87.0799405CurrentTrain: epoch  8, batch    79 | loss: 58.0853469CurrentTrain: epoch  8, batch    80 | loss: 88.5033017CurrentTrain: epoch  8, batch    81 | loss: 73.4150668CurrentTrain: epoch  8, batch    82 | loss: 57.3226944CurrentTrain: epoch  8, batch    83 | loss: 61.1108546CurrentTrain: epoch  8, batch    84 | loss: 71.3563527CurrentTrain: epoch  8, batch    85 | loss: 74.6764321CurrentTrain: epoch  8, batch    86 | loss: 87.6181776CurrentTrain: epoch  8, batch    87 | loss: 51.0368130CurrentTrain: epoch  8, batch    88 | loss: 150.5189238CurrentTrain: epoch  8, batch    89 | loss: 57.7702716CurrentTrain: epoch  8, batch    90 | loss: 92.1385507CurrentTrain: epoch  8, batch    91 | loss: 59.7016436CurrentTrain: epoch  8, batch    92 | loss: 57.4726201CurrentTrain: epoch  8, batch    93 | loss: 63.4945054CurrentTrain: epoch  8, batch    94 | loss: 56.7967874CurrentTrain: epoch  8, batch    95 | loss: 76.8520826CurrentTrain: epoch  9, batch     0 | loss: 69.7539883CurrentTrain: epoch  9, batch     1 | loss: 57.3627105CurrentTrain: epoch  9, batch     2 | loss: 111.8917910CurrentTrain: epoch  9, batch     3 | loss: 73.3439358CurrentTrain: epoch  9, batch     4 | loss: 66.8738060CurrentTrain: epoch  9, batch     5 | loss: 75.3928997CurrentTrain: epoch  9, batch     6 | loss: 60.1359932CurrentTrain: epoch  9, batch     7 | loss: 61.0909052CurrentTrain: epoch  9, batch     8 | loss: 70.4057499CurrentTrain: epoch  9, batch     9 | loss: 73.7454031CurrentTrain: epoch  9, batch    10 | loss: 70.3314529CurrentTrain: epoch  9, batch    11 | loss: 70.1743540CurrentTrain: epoch  9, batch    12 | loss: 86.7339355CurrentTrain: epoch  9, batch    13 | loss: 71.3063734CurrentTrain: epoch  9, batch    14 | loss: 71.8891280CurrentTrain: epoch  9, batch    15 | loss: 57.3682781CurrentTrain: epoch  9, batch    16 | loss: 71.6259043CurrentTrain: epoch  9, batch    17 | loss: 69.4439373CurrentTrain: epoch  9, batch    18 | loss: 113.9992977CurrentTrain: epoch  9, batch    19 | loss: 57.0753035CurrentTrain: epoch  9, batch    20 | loss: 88.0180477CurrentTrain: epoch  9, batch    21 | loss: 108.6189902CurrentTrain: epoch  9, batch    22 | loss: 85.2406271CurrentTrain: epoch  9, batch    23 | loss: 71.9598216CurrentTrain: epoch  9, batch    24 | loss: 87.1541537CurrentTrain: epoch  9, batch    25 | loss: 57.6885070CurrentTrain: epoch  9, batch    26 | loss: 62.7619998CurrentTrain: epoch  9, batch    27 | loss: 68.6540831CurrentTrain: epoch  9, batch    28 | loss: 108.6755214CurrentTrain: epoch  9, batch    29 | loss: 50.3070251CurrentTrain: epoch  9, batch    30 | loss: 72.8042213CurrentTrain: epoch  9, batch    31 | loss: 90.0794941CurrentTrain: epoch  9, batch    32 | loss: 67.0406509CurrentTrain: epoch  9, batch    33 | loss: 70.4019444CurrentTrain: epoch  9, batch    34 | loss: 113.9206662CurrentTrain: epoch  9, batch    35 | loss: 62.5989497CurrentTrain: epoch  9, batch    36 | loss: 67.4820349CurrentTrain: epoch  9, batch    37 | loss: 90.1365847CurrentTrain: epoch  9, batch    38 | loss: 85.1038516CurrentTrain: epoch  9, batch    39 | loss: 73.4633657CurrentTrain: epoch  9, batch    40 | loss: 50.2238810CurrentTrain: epoch  9, batch    41 | loss: 89.3250341CurrentTrain: epoch  9, batch    42 | loss: 60.8686331CurrentTrain: epoch  9, batch    43 | loss: 68.5960469CurrentTrain: epoch  9, batch    44 | loss: 59.0138350CurrentTrain: epoch  9, batch    45 | loss: 88.5053326CurrentTrain: epoch  9, batch    46 | loss: 73.1713513CurrentTrain: epoch  9, batch    47 | loss: 88.4145323CurrentTrain: epoch  9, batch    48 | loss: 75.1518485CurrentTrain: epoch  9, batch    49 | loss: 73.5108548CurrentTrain: epoch  9, batch    50 | loss: 56.2250671CurrentTrain: epoch  9, batch    51 | loss: 63.1442938CurrentTrain: epoch  9, batch    52 | loss: 92.4364526CurrentTrain: epoch  9, batch    53 | loss: 74.4177521CurrentTrain: epoch  9, batch    54 | loss: 89.0762445CurrentTrain: epoch  9, batch    55 | loss: 71.4310249CurrentTrain: epoch  9, batch    56 | loss: 70.3927275CurrentTrain: epoch  9, batch    57 | loss: 52.6962515CurrentTrain: epoch  9, batch    58 | loss: 56.6580010CurrentTrain: epoch  9, batch    59 | loss: 114.9368347CurrentTrain: epoch  9, batch    60 | loss: 89.9036676CurrentTrain: epoch  9, batch    61 | loss: 74.0351914CurrentTrain: epoch  9, batch    62 | loss: 60.9850420CurrentTrain: epoch  9, batch    63 | loss: 109.6363479CurrentTrain: epoch  9, batch    64 | loss: 68.8655993CurrentTrain: epoch  9, batch    65 | loss: 73.9691628CurrentTrain: epoch  9, batch    66 | loss: 58.0629411CurrentTrain: epoch  9, batch    67 | loss: 84.8095876CurrentTrain: epoch  9, batch    68 | loss: 66.2122710CurrentTrain: epoch  9, batch    69 | loss: 60.5408062CurrentTrain: epoch  9, batch    70 | loss: 59.1785681CurrentTrain: epoch  9, batch    71 | loss: 61.9314820CurrentTrain: epoch  9, batch    72 | loss: 86.5910342CurrentTrain: epoch  9, batch    73 | loss: 67.9999921CurrentTrain: epoch  9, batch    74 | loss: 71.3267376CurrentTrain: epoch  9, batch    75 | loss: 61.2886165CurrentTrain: epoch  9, batch    76 | loss: 89.8246739CurrentTrain: epoch  9, batch    77 | loss: 72.3721150CurrentTrain: epoch  9, batch    78 | loss: 50.4876227CurrentTrain: epoch  9, batch    79 | loss: 114.5755364CurrentTrain: epoch  9, batch    80 | loss: 71.2253649CurrentTrain: epoch  9, batch    81 | loss: 71.1603445CurrentTrain: epoch  9, batch    82 | loss: 88.2243289CurrentTrain: epoch  9, batch    83 | loss: 59.8888868CurrentTrain: epoch  9, batch    84 | loss: 70.8639138CurrentTrain: epoch  9, batch    85 | loss: 52.1078640CurrentTrain: epoch  9, batch    86 | loss: 70.6787126CurrentTrain: epoch  9, batch    87 | loss: 87.1642160CurrentTrain: epoch  9, batch    88 | loss: 70.3134368CurrentTrain: epoch  9, batch    89 | loss: 68.9618033CurrentTrain: epoch  9, batch    90 | loss: 60.5857454CurrentTrain: epoch  9, batch    91 | loss: 85.1264254CurrentTrain: epoch  9, batch    92 | loss: 69.6340602CurrentTrain: epoch  9, batch    93 | loss: 63.2208231CurrentTrain: epoch  9, batch    94 | loss: 154.2578661CurrentTrain: epoch  9, batch    95 | loss: 61.6379910

F1 score per class: {32: 0.6238532110091743, 6: 0.8018433179723502, 19: 0.3157894736842105, 24: 0.7403314917127072, 26: 0.9393939393939394, 29: 0.8202764976958525}
Micro-average F1 score: 0.7670720299345183
Weighted-average F1 score: 0.7654191355631249
F1 score per class: {32: 0.6530612244897959, 6: 0.7964601769911505, 19: 0.26229508196721313, 24: 0.7351351351351352, 26: 0.9393939393939394, 29: 0.8148148148148148}
Micro-average F1 score: 0.7550839964633068
Weighted-average F1 score: 0.7428909615695051
F1 score per class: {32: 0.6583333333333333, 6: 0.8, 19: 0.2962962962962963, 24: 0.7391304347826086, 26: 0.9393939393939394, 29: 0.8202764976958525}
Micro-average F1 score: 0.7638640429338104
Weighted-average F1 score: 0.7549220769101556

F1 score per class: {32: 0.6238532110091743, 6: 0.8018433179723502, 19: 0.3157894736842105, 24: 0.7403314917127072, 26: 0.9393939393939394, 29: 0.8202764976958525}
Micro-average F1 score: 0.7670720299345183
Weighted-average F1 score: 0.7654191355631249
F1 score per class: {32: 0.6530612244897959, 6: 0.7964601769911505, 19: 0.26229508196721313, 24: 0.7351351351351352, 26: 0.9393939393939394, 29: 0.8148148148148148}
Micro-average F1 score: 0.7550839964633068
Weighted-average F1 score: 0.7428909615695051
F1 score per class: {32: 0.6583333333333333, 6: 0.8, 19: 0.2962962962962963, 24: 0.7391304347826086, 26: 0.9393939393939394, 29: 0.8202764976958525}
Micro-average F1 score: 0.7638640429338104
Weighted-average F1 score: 0.7549220769101556

F1 score per class: {32: 0.43174603174603177, 6: 0.7467811158798283, 19: 0.18461538461538463, 24: 0.6802030456852792, 26: 0.8571428571428571, 29: 0.6334519572953736}
Micro-average F1 score: 0.6269113149847095
Weighted-average F1 score: 0.6098030662050457
F1 score per class: {32: 0.4519774011299435, 6: 0.7407407407407407, 19: 0.14814814814814814, 24: 0.6601941747572816, 26: 0.8493150684931506, 29: 0.6353790613718412}
Micro-average F1 score: 0.6069651741293532
Weighted-average F1 score: 0.5821965002070163
F1 score per class: {32: 0.45272206303724927, 6: 0.743801652892562, 19: 0.16842105263157894, 24: 0.6732673267326733, 26: 0.8493150684931506, 29: 0.6402877697841727}
Micro-average F1 score: 0.6166064981949458
Weighted-average F1 score: 0.5940357934932534

F1 score per class: {32: 0.43174603174603177, 6: 0.7467811158798283, 19: 0.18461538461538463, 24: 0.6802030456852792, 26: 0.8571428571428571, 29: 0.6334519572953736}
Micro-average F1 score: 0.6269113149847095
Weighted-average F1 score: 0.6098030662050457
F1 score per class: {32: 0.4519774011299435, 6: 0.7407407407407407, 19: 0.14814814814814814, 24: 0.6601941747572816, 26: 0.8493150684931506, 29: 0.6353790613718412}
Micro-average F1 score: 0.6069651741293532
Weighted-average F1 score: 0.5821965002070163
F1 score per class: {32: 0.45272206303724927, 6: 0.743801652892562, 19: 0.16842105263157894, 24: 0.6732673267326733, 26: 0.8493150684931506, 29: 0.6402877697841727}
Micro-average F1 score: 0.6166064981949458
Weighted-average F1 score: 0.5940357934932534
cur_acc_wo_na:  ['0.7671']
his_acc_wo_na:  ['0.7671']
cur_acc des_wo_na:  ['0.7551']
his_acc des_wo_na:  ['0.7551']
cur_acc rrf_wo_na:  ['0.7639']
his_acc rrf_wo_na:  ['0.7639']
cur_acc_w_na:  ['0.6269']
his_acc_w_na:  ['0.6269']
cur_acc des_w_na:  ['0.6070']
his_acc des_w_na:  ['0.6070']
cur_acc rrf_w_na:  ['0.6166']
his_acc rrf_w_na:  ['0.6166']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 90.9999599CurrentTrain: epoch  0, batch     1 | loss: 85.6574477CurrentTrain: epoch  0, batch     2 | loss: 131.9910980CurrentTrain: epoch  0, batch     3 | loss: 52.2117352CurrentTrain: epoch  1, batch     0 | loss: 70.1965698CurrentTrain: epoch  1, batch     1 | loss: 84.2238057CurrentTrain: epoch  1, batch     2 | loss: 97.1514813CurrentTrain: epoch  1, batch     3 | loss: 69.0914593CurrentTrain: epoch  2, batch     0 | loss: 100.1956252CurrentTrain: epoch  2, batch     1 | loss: 77.3753919CurrentTrain: epoch  2, batch     2 | loss: 78.1902352CurrentTrain: epoch  2, batch     3 | loss: 80.1398544CurrentTrain: epoch  3, batch     0 | loss: 75.4160281CurrentTrain: epoch  3, batch     1 | loss: 66.5477103CurrentTrain: epoch  3, batch     2 | loss: 93.5019420CurrentTrain: epoch  3, batch     3 | loss: 77.9799384CurrentTrain: epoch  4, batch     0 | loss: 89.3212221CurrentTrain: epoch  4, batch     1 | loss: 65.1633543CurrentTrain: epoch  4, batch     2 | loss: 63.4716320CurrentTrain: epoch  4, batch     3 | loss: 53.0789085CurrentTrain: epoch  5, batch     0 | loss: 74.7615180CurrentTrain: epoch  5, batch     1 | loss: 65.7229660CurrentTrain: epoch  5, batch     2 | loss: 59.5458683CurrentTrain: epoch  5, batch     3 | loss: 49.6807189CurrentTrain: epoch  6, batch     0 | loss: 72.6124852CurrentTrain: epoch  6, batch     1 | loss: 69.9920222CurrentTrain: epoch  6, batch     2 | loss: 60.2210292CurrentTrain: epoch  6, batch     3 | loss: 81.0873687CurrentTrain: epoch  7, batch     0 | loss: 89.3053978CurrentTrain: epoch  7, batch     1 | loss: 60.1177948CurrentTrain: epoch  7, batch     2 | loss: 71.1233599CurrentTrain: epoch  7, batch     3 | loss: 74.5316515CurrentTrain: epoch  8, batch     0 | loss: 60.1952248CurrentTrain: epoch  8, batch     1 | loss: 71.0968334CurrentTrain: epoch  8, batch     2 | loss: 72.1829029CurrentTrain: epoch  8, batch     3 | loss: 57.4889919CurrentTrain: epoch  9, batch     0 | loss: 59.1549379CurrentTrain: epoch  9, batch     1 | loss: 69.6174959CurrentTrain: epoch  9, batch     2 | loss: 72.7111792CurrentTrain: epoch  9, batch     3 | loss: 39.7928423
MemoryTrain:  epoch  0, batch     0 | loss: 2.4646091MemoryTrain:  epoch  1, batch     0 | loss: 2.2127637MemoryTrain:  epoch  2, batch     0 | loss: 1.7941991MemoryTrain:  epoch  3, batch     0 | loss: 1.2985107MemoryTrain:  epoch  4, batch     0 | loss: 1.1106146MemoryTrain:  epoch  5, batch     0 | loss: 0.9040546MemoryTrain:  epoch  6, batch     0 | loss: 0.7798486MemoryTrain:  epoch  7, batch     0 | loss: 0.5730286MemoryTrain:  epoch  8, batch     0 | loss: 0.4801846MemoryTrain:  epoch  9, batch     0 | loss: 0.4308988

F1 score per class: {32: 0.0, 35: 0.8421052631578947, 37: 0.0, 38: 0.0, 6: 0.6329113924050633, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.7956989247311828, 26: 0.5833333333333334, 29: 0.5}
Micro-average F1 score: 0.5897435897435898
Weighted-average F1 score: 0.5242693640321028
F1 score per class: {32: 0.0, 35: 0.6666666666666666, 37: 0.0, 38: 0.0, 6: 0.7818181818181819, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.7476635514018691, 26: 0.543046357615894, 29: 0.6666666666666666}
Micro-average F1 score: 0.593320235756385
Weighted-average F1 score: 0.5366834371223125
F1 score per class: {32: 0.0, 35: 0.7368421052631579, 37: 0.0, 38: 0.0, 6: 0.782608695652174, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.84, 26: 0.5394736842105263, 29: 0.7636363636363637}
Micro-average F1 score: 0.6215644820295984
Weighted-average F1 score: 0.5490650277169324

F1 score per class: {32: 0.603448275862069, 35: 0.6666666666666666, 37: 0.7767857142857143, 6: 0.3333333333333333, 38: 0.6329113924050633, 15: 0.7444444444444445, 19: 0.91, 24: 0.7782805429864253, 25: 0.6379310344827587, 26: 0.41420118343195267, 29: 0.4878048780487805}
Micro-average F1 score: 0.6845549738219895
Weighted-average F1 score: 0.6774795461316453
F1 score per class: {32: 0.6076923076923076, 35: 0.5714285714285714, 37: 0.7350427350427351, 6: 0.25287356321839083, 38: 0.7818181818181819, 15: 0.6796116504854369, 19: 0.9223300970873787, 24: 0.7793427230046949, 25: 0.46511627906976744, 26: 0.36444444444444446, 29: 0.4158415841584158}
Micro-average F1 score: 0.6267029972752044
Weighted-average F1 score: 0.5998623047547689
F1 score per class: {32: 0.599250936329588, 35: 0.6086956521739131, 37: 0.7288135593220338, 6: 0.30303030303030304, 38: 0.782608695652174, 15: 0.71875, 19: 0.9223300970873787, 24: 0.7727272727272727, 25: 0.5675675675675675, 26: 0.36936936936936937, 29: 0.6176470588235294}
Micro-average F1 score: 0.6574712643678161
Weighted-average F1 score: 0.635821188587667

F1 score per class: {32: 0.0, 35: 0.6956521739130435, 37: 0.0, 38: 0.0, 6: 0.5617977528089888, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6434782608695652, 26: 0.48951048951048953, 29: 0.39215686274509803}
Micro-average F1 score: 0.46464646464646464
Weighted-average F1 score: 0.4104135451801436
F1 score per class: {32: 0.0, 35: 0.48, 37: 0.0, 38: 0.0, 6: 0.6564885496183206, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.5925925925925926, 26: 0.4581005586592179, 29: 0.44680851063829785}
Micro-average F1 score: 0.4460856720827179
Weighted-average F1 score: 0.4051338264289909
F1 score per class: {32: 0.0, 35: 0.6086956521739131, 37: 0.0, 38: 0.0, 6: 0.6792452830188679, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6666666666666666, 26: 0.44565217391304346, 29: 0.5384615384615384}
Micro-average F1 score: 0.47419354838709676
Weighted-average F1 score: 0.4219003227913462

F1 score per class: {32: 0.41420118343195267, 35: 0.48484848484848486, 37: 0.7073170731707317, 6: 0.19718309859154928, 38: 0.5617977528089888, 15: 0.67, 19: 0.7844827586206896, 24: 0.6209386281588448, 25: 0.4327485380116959, 26: 0.27450980392156865, 29: 0.3389830508474576}
Micro-average F1 score: 0.5306950786402841
Weighted-average F1 score: 0.5117112482387849
F1 score per class: {32: 0.37889688249400477, 35: 0.36363636363636365, 37: 0.671875, 6: 0.12790697674418605, 38: 0.6515151515151515, 15: 0.5907172995780591, 19: 0.7661290322580645, 24: 0.6459143968871596, 25: 0.3041825095057034, 26: 0.24773413897280966, 29: 0.22105263157894736}
Micro-average F1 score: 0.45347003154574134
Weighted-average F1 score: 0.42281916993635354
F1 score per class: {32: 0.3764705882352941, 35: 0.4117647058823529, 37: 0.6590038314176245, 6: 0.145985401459854, 38: 0.6728971962616822, 15: 0.641860465116279, 19: 0.7851239669421488, 24: 0.6343283582089553, 25: 0.36363636363636365, 26: 0.23976608187134502, 29: 0.328125}
Micro-average F1 score: 0.4786610878661088
Weighted-average F1 score: 0.44911210117266676
cur_acc_wo_na:  ['0.7671', '0.5897']
his_acc_wo_na:  ['0.7671', '0.6846']
cur_acc des_wo_na:  ['0.7551', '0.5933']
his_acc des_wo_na:  ['0.7551', '0.6267']
cur_acc rrf_wo_na:  ['0.7639', '0.6216']
his_acc rrf_wo_na:  ['0.7639', '0.6575']
cur_acc_w_na:  ['0.6269', '0.4646']
his_acc_w_na:  ['0.6269', '0.5307']
cur_acc des_w_na:  ['0.6070', '0.4461']
his_acc des_w_na:  ['0.6070', '0.4535']
cur_acc rrf_w_na:  ['0.6166', '0.4742']
his_acc rrf_w_na:  ['0.6166', '0.4787']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 121.6998204CurrentTrain: epoch  0, batch     1 | loss: 87.0168690CurrentTrain: epoch  0, batch     2 | loss: 78.6202423CurrentTrain: epoch  0, batch     3 | loss: 49.5550542CurrentTrain: epoch  1, batch     0 | loss: 78.9717949CurrentTrain: epoch  1, batch     1 | loss: 70.6206309CurrentTrain: epoch  1, batch     2 | loss: 81.0887058CurrentTrain: epoch  1, batch     3 | loss: 78.8625384CurrentTrain: epoch  2, batch     0 | loss: 77.8597752CurrentTrain: epoch  2, batch     1 | loss: 80.6723418CurrentTrain: epoch  2, batch     2 | loss: 81.3364893CurrentTrain: epoch  2, batch     3 | loss: 47.5519952CurrentTrain: epoch  3, batch     0 | loss: 97.1265395CurrentTrain: epoch  3, batch     1 | loss: 79.2890259CurrentTrain: epoch  3, batch     2 | loss: 77.6794024CurrentTrain: epoch  3, batch     3 | loss: 53.8416077CurrentTrain: epoch  4, batch     0 | loss: 75.3681227CurrentTrain: epoch  4, batch     1 | loss: 96.1752356CurrentTrain: epoch  4, batch     2 | loss: 76.2773113CurrentTrain: epoch  4, batch     3 | loss: 51.8539250CurrentTrain: epoch  5, batch     0 | loss: 60.6256376CurrentTrain: epoch  5, batch     1 | loss: 75.4458003CurrentTrain: epoch  5, batch     2 | loss: 71.3918654CurrentTrain: epoch  5, batch     3 | loss: 46.9670758CurrentTrain: epoch  6, batch     0 | loss: 72.9023614CurrentTrain: epoch  6, batch     1 | loss: 62.5476823CurrentTrain: epoch  6, batch     2 | loss: 73.2080363CurrentTrain: epoch  6, batch     3 | loss: 52.8840085CurrentTrain: epoch  7, batch     0 | loss: 75.3809849CurrentTrain: epoch  7, batch     1 | loss: 68.2564059CurrentTrain: epoch  7, batch     2 | loss: 61.0296000CurrentTrain: epoch  7, batch     3 | loss: 65.2052248CurrentTrain: epoch  8, batch     0 | loss: 85.5025521CurrentTrain: epoch  8, batch     1 | loss: 86.8462506CurrentTrain: epoch  8, batch     2 | loss: 68.9390066CurrentTrain: epoch  8, batch     3 | loss: 50.4023113CurrentTrain: epoch  9, batch     0 | loss: 71.6911316CurrentTrain: epoch  9, batch     1 | loss: 88.6933760CurrentTrain: epoch  9, batch     2 | loss: 55.4325653CurrentTrain: epoch  9, batch     3 | loss: 42.0866541
MemoryTrain:  epoch  0, batch     0 | loss: 1.3001452MemoryTrain:  epoch  1, batch     0 | loss: 1.1607551MemoryTrain:  epoch  2, batch     0 | loss: 0.9460417MemoryTrain:  epoch  3, batch     0 | loss: 0.7231267MemoryTrain:  epoch  4, batch     0 | loss: 0.5700423MemoryTrain:  epoch  5, batch     0 | loss: 0.5906270MemoryTrain:  epoch  6, batch     0 | loss: 0.4133872MemoryTrain:  epoch  7, batch     0 | loss: 0.3314098MemoryTrain:  epoch  8, batch     0 | loss: 0.2991606MemoryTrain:  epoch  9, batch     0 | loss: 0.2295363

F1 score per class: {32: 0.0, 33: 0.5751633986928104, 35: 0.0, 36: 0.7708333333333334, 37: 0.0, 6: 0.0, 8: 0.8333333333333334, 15: 0.0, 20: 0.42857142857142855, 26: 0.0, 29: 0.6324786324786325, 30: 0.0}
Micro-average F1 score: 0.5631469979296067
Weighted-average F1 score: 0.4821252218836252
F1 score per class: {32: 0.0, 33: 0.6459627329192547, 35: 0.0, 36: 0.0, 37: 0.7368421052631579, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.0, 19: 0.75, 20: 0.0, 24: 0.375, 25: 0.0, 26: 0.704225352112676, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5766423357664233
Weighted-average F1 score: 0.501683506971272
F1 score per class: {32: 0.0, 33: 0.6583850931677019, 35: 0.0, 36: 0.0, 37: 0.7755102040816326, 6: 0.0, 8: 0.0, 15: 0.0, 19: 0.8780487804878049, 20: 0.0, 24: 0.4, 26: 0.0, 29: 0.64, 30: 0.0}
Micro-average F1 score: 0.5823754789272031
Weighted-average F1 score: 0.4998980398753941

F1 score per class: {32: 0.6044444444444445, 33: 0.43349753694581283, 35: 0.5833333333333334, 36: 0.7980769230769231, 37: 0.5138888888888888, 6: 0.15384615384615385, 38: 0.31746031746031744, 8: 0.7165775401069518, 15: 0.9054726368159204, 19: 0.8333333333333334, 20: 0.772093023255814, 24: 0.375, 25: 0.6141732283464567, 26: 0.5692307692307692, 29: 0.5081967213114754, 30: 0.13333333333333333}
Micro-average F1 score: 0.6326009197751661
Weighted-average F1 score: 0.6427847137702363
F1 score per class: {32: 0.616, 33: 0.48148148148148145, 35: 0.46153846153846156, 36: 0.7467811158798283, 37: 0.5035971223021583, 6: 0.2903225806451613, 38: 0.379746835443038, 8: 0.6504854368932039, 15: 0.9004739336492891, 19: 0.48, 20: 0.7685589519650655, 24: 0.24, 25: 0.56, 26: 0.4830917874396135, 29: 0.3018867924528302, 30: 0.425531914893617}
Micro-average F1 score: 0.5926581158779302
Weighted-average F1 score: 0.5902026132049609
F1 score per class: {32: 0.6104417670682731, 33: 0.4774774774774775, 35: 0.5, 36: 0.7426160337552743, 37: 0.5241379310344828, 6: 0.3137254901960784, 38: 0.34782608695652173, 8: 0.6802030456852792, 15: 0.9090909090909091, 19: 0.7659574468085106, 20: 0.7719298245614035, 24: 0.3, 25: 0.56, 26: 0.47337278106508873, 29: 0.2711864406779661, 30: 0.3333333333333333}
Micro-average F1 score: 0.6043298019345924
Weighted-average F1 score: 0.6073696870380106

F1 score per class: {32: 0.0, 33: 0.4467005076142132, 35: 0.0, 36: 0.0, 37: 0.5648854961832062, 6: 0.0, 38: 0.0, 8: 0.7894736842105263, 15: 0.0, 19: 0.42857142857142855, 20: 0.0, 26: 0.47435897435897434, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.40296296296296297
Weighted-average F1 score: 0.3475032974490487
F1 score per class: {32: 0.0, 33: 0.5123152709359606, 35: 0.0, 36: 0.0, 37: 0.5555555555555556, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.0, 19: 0.6666666666666666, 20: 0.0, 24: 0.375, 25: 0.0, 26: 0.5102040816326531, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.40152477763659467
Weighted-average F1 score: 0.34898799820101695
F1 score per class: {32: 0.0, 33: 0.5120772946859904, 35: 0.0, 36: 0.0, 37: 0.5714285714285714, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.8181818181818182, 19: 0.0, 20: 0.4, 24: 0.0, 26: 0.4624277456647399, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.4069611780455154
Weighted-average F1 score: 0.351160679290629

F1 score per class: {32: 0.39766081871345027, 33: 0.30662020905923343, 35: 0.4117647058823529, 36: 0.7345132743362832, 37: 0.30578512396694213, 6: 0.10526315789473684, 38: 0.3076923076923077, 8: 0.6291079812206573, 15: 0.7878787878787878, 19: 0.7894736842105263, 20: 0.6102941176470589, 24: 0.3, 25: 0.40414507772020725, 26: 0.4111111111111111, 29: 0.36904761904761907, 30: 0.1111111111111111}
Micro-average F1 score: 0.47891682785299805
Weighted-average F1 score: 0.46935460791461925
F1 score per class: {32: 0.39285714285714285, 33: 0.3365695792880259, 35: 0.35294117647058826, 36: 0.6615969581749049, 37: 0.30837004405286345, 6: 0.14634146341463414, 38: 0.35294117647058826, 8: 0.5654008438818565, 15: 0.7569721115537849, 19: 0.3829787234042553, 20: 0.6089965397923875, 24: 0.17142857142857143, 25: 0.3605150214592275, 26: 0.3472222222222222, 29: 0.22535211267605634, 30: 0.2702702702702703}
Micro-average F1 score: 0.435630689206762
Weighted-average F1 score: 0.4240042735985674
F1 score per class: {32: 0.39378238341968913, 33: 0.3302180685358255, 35: 0.35294117647058826, 36: 0.6616541353383458, 37: 0.3206751054852321, 6: 0.16842105263157894, 38: 0.3333333333333333, 8: 0.5982142857142857, 15: 0.76, 19: 0.6428571428571429, 20: 0.6132404181184669, 24: 0.24, 25: 0.3620689655172414, 26: 0.3463203463203463, 29: 0.20253164556962025, 30: 0.21428571428571427}
Micro-average F1 score: 0.4477815699658703
Weighted-average F1 score: 0.43789070197016366
cur_acc_wo_na:  ['0.7671', '0.5897', '0.5631']
his_acc_wo_na:  ['0.7671', '0.6846', '0.6326']
cur_acc des_wo_na:  ['0.7551', '0.5933', '0.5766']
his_acc des_wo_na:  ['0.7551', '0.6267', '0.5927']
cur_acc rrf_wo_na:  ['0.7639', '0.6216', '0.5824']
his_acc rrf_wo_na:  ['0.7639', '0.6575', '0.6043']
cur_acc_w_na:  ['0.6269', '0.4646', '0.4030']
his_acc_w_na:  ['0.6269', '0.5307', '0.4789']
cur_acc des_w_na:  ['0.6070', '0.4461', '0.4015']
his_acc des_w_na:  ['0.6070', '0.4535', '0.4356']
cur_acc rrf_w_na:  ['0.6166', '0.4742', '0.4070']
his_acc rrf_w_na:  ['0.6166', '0.4787', '0.4478']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 99.5887289CurrentTrain: epoch  0, batch     1 | loss: 136.5163252CurrentTrain: epoch  0, batch     2 | loss: 90.6748759CurrentTrain: epoch  0, batch     3 | loss: 81.6006765CurrentTrain: epoch  0, batch     4 | loss: 63.9186398CurrentTrain: epoch  1, batch     0 | loss: 71.0270779CurrentTrain: epoch  1, batch     1 | loss: 104.2065481CurrentTrain: epoch  1, batch     2 | loss: 86.5011007CurrentTrain: epoch  1, batch     3 | loss: 70.0882601CurrentTrain: epoch  1, batch     4 | loss: 104.2009301CurrentTrain: epoch  2, batch     0 | loss: 100.5393488CurrentTrain: epoch  2, batch     1 | loss: 71.5198464CurrentTrain: epoch  2, batch     2 | loss: 78.9025401CurrentTrain: epoch  2, batch     3 | loss: 81.3353987CurrentTrain: epoch  2, batch     4 | loss: 44.0125710CurrentTrain: epoch  3, batch     0 | loss: 122.2830494CurrentTrain: epoch  3, batch     1 | loss: 92.9089176CurrentTrain: epoch  3, batch     2 | loss: 76.0523556CurrentTrain: epoch  3, batch     3 | loss: 80.6131044CurrentTrain: epoch  3, batch     4 | loss: 52.9102569CurrentTrain: epoch  4, batch     0 | loss: 81.8139910CurrentTrain: epoch  4, batch     1 | loss: 76.9679503CurrentTrain: epoch  4, batch     2 | loss: 72.1380882CurrentTrain: epoch  4, batch     3 | loss: 91.2650399CurrentTrain: epoch  4, batch     4 | loss: 51.9026986CurrentTrain: epoch  5, batch     0 | loss: 60.7418420CurrentTrain: epoch  5, batch     1 | loss: 74.6335199CurrentTrain: epoch  5, batch     2 | loss: 114.6883266CurrentTrain: epoch  5, batch     3 | loss: 95.9026184CurrentTrain: epoch  5, batch     4 | loss: 62.7239721CurrentTrain: epoch  6, batch     0 | loss: 66.8011348CurrentTrain: epoch  6, batch     1 | loss: 74.5947972CurrentTrain: epoch  6, batch     2 | loss: 89.6798323CurrentTrain: epoch  6, batch     3 | loss: 72.9246118CurrentTrain: epoch  6, batch     4 | loss: 64.8365031CurrentTrain: epoch  7, batch     0 | loss: 72.5147500CurrentTrain: epoch  7, batch     1 | loss: 88.6747087CurrentTrain: epoch  7, batch     2 | loss: 74.0322365CurrentTrain: epoch  7, batch     3 | loss: 90.3864455CurrentTrain: epoch  7, batch     4 | loss: 34.6968675CurrentTrain: epoch  8, batch     0 | loss: 75.1304785CurrentTrain: epoch  8, batch     1 | loss: 60.3161974CurrentTrain: epoch  8, batch     2 | loss: 61.2289772CurrentTrain: epoch  8, batch     3 | loss: 88.1589815CurrentTrain: epoch  8, batch     4 | loss: 62.6161220CurrentTrain: epoch  9, batch     0 | loss: 113.0550480CurrentTrain: epoch  9, batch     1 | loss: 85.4388389CurrentTrain: epoch  9, batch     2 | loss: 88.7571757CurrentTrain: epoch  9, batch     3 | loss: 59.8369402CurrentTrain: epoch  9, batch     4 | loss: 46.4664639
MemoryTrain:  epoch  0, batch     0 | loss: 1.5037835MemoryTrain:  epoch  1, batch     0 | loss: 1.1410096MemoryTrain:  epoch  2, batch     0 | loss: 1.1079791MemoryTrain:  epoch  3, batch     0 | loss: 0.9105760MemoryTrain:  epoch  4, batch     0 | loss: 0.7007168MemoryTrain:  epoch  5, batch     0 | loss: 0.5630221MemoryTrain:  epoch  6, batch     0 | loss: 0.5150284MemoryTrain:  epoch  7, batch     0 | loss: 0.3938691MemoryTrain:  epoch  8, batch     0 | loss: 0.3534887MemoryTrain:  epoch  9, batch     0 | loss: 0.2965318

F1 score per class: {32: 0.20930232558139536, 1: 0.6242038216560509, 34: 0.0, 3: 0.0, 35: 0.13186813186813187, 37: 0.0, 6: 0.0, 33: 0.5569620253164557, 8: 0.0, 14: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.631578947368421, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.36610878661087864
Weighted-average F1 score: 0.3115776761596893
F1 score per class: {1: 0.2391304347826087, 3: 0.5433526011560693, 6: 0.0, 8: 0.0, 14: 0.09090909090909091, 19: 0.0, 20: 0.0, 22: 0.6090534979423868, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.6434782608695652, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.334841628959276
Weighted-average F1 score: 0.27951486849913265
F1 score per class: {1: 0.2391304347826087, 3: 0.5517241379310345, 6: 0.0, 8: 0.0, 14: 0.11428571428571428, 19: 0.0, 20: 0.0, 22: 0.5826771653543307, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.639344262295082, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.35097493036211697
Weighted-average F1 score: 0.3020701857109585

F1 score per class: {1: 0.16589861751152074, 3: 0.41702127659574467, 6: 0.5892116182572614, 8: 0.4585365853658537, 14: 0.11428571428571428, 15: 0.56, 19: 0.6577777777777778, 20: 0.4528301886792453, 22: 0.4943820224719101, 24: 0.07692307692307693, 25: 0.3225806451612903, 26: 0.7083333333333334, 29: 0.8761904761904762, 30: 0.8484848484848485, 32: 0.6111111111111112, 33: 0.35294117647058826, 34: 0.28125, 35: 0.11290322580645161, 36: 0.5048543689320388, 37: 0.2631578947368421, 38: 0.13333333333333333}
Micro-average F1 score: 0.46636185499673416
Weighted-average F1 score: 0.4557895367251831
F1 score per class: {1: 0.1896551724137931, 3: 0.3574144486692015, 6: 0.5692883895131086, 8: 0.3967611336032389, 14: 0.07194244604316546, 15: 0.5454545454545454, 19: 0.6064981949458483, 20: 0.4657534246575342, 22: 0.5285714285714286, 24: 0.08695652173913043, 25: 0.4, 26: 0.6296296296296297, 29: 0.8558558558558559, 30: 0.7058823529411765, 32: 0.592, 33: 0.17647058823529413, 34: 0.28793774319066145, 35: 0.16666666666666666, 36: 0.5164835164835165, 37: 0.1925925925925926, 38: 0.4444444444444444}
Micro-average F1 score: 0.4481586402266289
Weighted-average F1 score: 0.43755783521572067
F1 score per class: {1: 0.1888412017167382, 3: 0.3595505617977528, 6: 0.5725190839694656, 8: 0.40707964601769914, 14: 0.08759124087591241, 15: 0.6, 19: 0.6199261992619927, 20: 0.45751633986928103, 22: 0.5034013605442177, 24: 0.0625, 25: 0.36363636363636365, 26: 0.6634146341463415, 29: 0.8837209302325582, 30: 0.8717948717948718, 32: 0.5916666666666667, 33: 0.2222222222222222, 34: 0.2635135135135135, 35: 0.16176470588235295, 36: 0.5323741007194245, 37: 0.18333333333333332, 38: 0.45}
Micro-average F1 score: 0.4505558806319485
Weighted-average F1 score: 0.4372284193655901

F1 score per class: {1: 0.11076923076923077, 3: 0.4666666666666667, 6: 0.0, 8: 0.0, 14: 0.1111111111111111, 19: 0.0, 20: 0.0, 22: 0.45993031358885017, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.43902439024390244, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.2468265162200282
Weighted-average F1 score: 0.21379648880771968
F1 score per class: {1: 0.13253012048192772, 3: 0.4017094017094017, 6: 0.0, 8: 0.0, 14: 0.06711409395973154, 19: 0.0, 20: 0.0, 22: 0.5, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.48366013071895425, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.22437841115827775
Weighted-average F1 score: 0.19178110470411824
F1 score per class: {1: 0.13213213213213212, 3: 0.4050632911392405, 6: 0.0, 8: 0.0, 14: 0.0851063829787234, 19: 0.0, 20: 0.0, 22: 0.4728434504792332, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.46987951807228917, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.2366938008766437
Weighted-average F1 score: 0.20778842266841177

F1 score per class: {1: 0.08314087759815242, 3: 0.27450980392156865, 6: 0.37270341207349084, 8: 0.33935018050541516, 14: 0.0851063829787234, 15: 0.3783783783783784, 19: 0.5584905660377358, 20: 0.25, 22: 0.3916913946587537, 24: 0.06666666666666667, 25: 0.3125, 26: 0.6238532110091743, 29: 0.7272727272727273, 30: 0.8, 32: 0.49624060150375937, 33: 0.2857142857142857, 34: 0.17733990147783252, 35: 0.07179487179487179, 36: 0.3443708609271523, 37: 0.21428571428571427, 38: 0.1}
Micro-average F1 score: 0.32941176470588235
Weighted-average F1 score: 0.311362706178545
F1 score per class: {1: 0.10068649885583524, 3: 0.23618090452261306, 6: 0.3510392609699769, 8: 0.2684931506849315, 14: 0.05025125628140704, 15: 0.3870967741935484, 19: 0.509090909090909, 20: 0.2943722943722944, 22: 0.4054794520547945, 24: 0.06349206349206349, 25: 0.375, 26: 0.5354330708661418, 29: 0.6810035842293907, 30: 0.5901639344262295, 32: 0.4539877300613497, 33: 0.125, 34: 0.1791767554479419, 35: 0.10084033613445378, 36: 0.3298245614035088, 37: 0.15204678362573099, 38: 0.2631578947368421}
Micro-average F1 score: 0.3112335235097383
Weighted-average F1 score: 0.2990714418752005
F1 score per class: {1: 0.09954751131221719, 3: 0.23703703703703705, 6: 0.35714285714285715, 8: 0.2787878787878788, 14: 0.06153846153846154, 15: 0.4, 19: 0.5185185185185185, 20: 0.2892561983471074, 22: 0.38341968911917096, 24: 0.05263157894736842, 25: 0.35294117647058826, 26: 0.5811965811965812, 29: 0.7169811320754716, 30: 0.7906976744186046, 32: 0.46405228758169936, 33: 0.16216216216216217, 34: 0.16666666666666666, 35: 0.09954751131221719, 36: 0.3474178403755869, 37: 0.14666666666666667, 38: 0.2727272727272727}
Micro-average F1 score: 0.3153798894122466
Weighted-average F1 score: 0.30036708867795164
cur_acc_wo_na:  ['0.7671', '0.5897', '0.5631', '0.3661']
his_acc_wo_na:  ['0.7671', '0.6846', '0.6326', '0.4664']
cur_acc des_wo_na:  ['0.7551', '0.5933', '0.5766', '0.3348']
his_acc des_wo_na:  ['0.7551', '0.6267', '0.5927', '0.4482']
cur_acc rrf_wo_na:  ['0.7639', '0.6216', '0.5824', '0.3510']
his_acc rrf_wo_na:  ['0.7639', '0.6575', '0.6043', '0.4506']
cur_acc_w_na:  ['0.6269', '0.4646', '0.4030', '0.2468']
his_acc_w_na:  ['0.6269', '0.5307', '0.4789', '0.3294']
cur_acc des_w_na:  ['0.6070', '0.4461', '0.4015', '0.2244']
his_acc des_w_na:  ['0.6070', '0.4535', '0.4356', '0.3112']
cur_acc rrf_w_na:  ['0.6166', '0.4742', '0.4070', '0.2367']
his_acc rrf_w_na:  ['0.6166', '0.4787', '0.4478', '0.3154']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 149.8683227CurrentTrain: epoch  0, batch     1 | loss: 95.4074560CurrentTrain: epoch  0, batch     2 | loss: 86.5264785CurrentTrain: epoch  0, batch     3 | loss: 89.3883340CurrentTrain: epoch  0, batch     4 | loss: 114.9699577CurrentTrain: epoch  1, batch     0 | loss: 88.3782839CurrentTrain: epoch  1, batch     1 | loss: 104.4137803CurrentTrain: epoch  1, batch     2 | loss: 93.3828579CurrentTrain: epoch  1, batch     3 | loss: 96.7088138CurrentTrain: epoch  1, batch     4 | loss: 61.3396262CurrentTrain: epoch  2, batch     0 | loss: 85.6846925CurrentTrain: epoch  2, batch     1 | loss: 65.1090163CurrentTrain: epoch  2, batch     2 | loss: 100.6029655CurrentTrain: epoch  2, batch     3 | loss: 97.2583076CurrentTrain: epoch  2, batch     4 | loss: 109.6122168CurrentTrain: epoch  3, batch     0 | loss: 67.2759137CurrentTrain: epoch  3, batch     1 | loss: 91.2538043CurrentTrain: epoch  3, batch     2 | loss: 120.2060060CurrentTrain: epoch  3, batch     3 | loss: 82.4904571CurrentTrain: epoch  3, batch     4 | loss: 76.0407657CurrentTrain: epoch  4, batch     0 | loss: 76.2374304CurrentTrain: epoch  4, batch     1 | loss: 90.5687458CurrentTrain: epoch  4, batch     2 | loss: 116.4263064CurrentTrain: epoch  4, batch     3 | loss: 79.6706576CurrentTrain: epoch  4, batch     4 | loss: 48.9644270CurrentTrain: epoch  5, batch     0 | loss: 118.2742897CurrentTrain: epoch  5, batch     1 | loss: 78.0045334CurrentTrain: epoch  5, batch     2 | loss: 63.6987198CurrentTrain: epoch  5, batch     3 | loss: 92.6286948CurrentTrain: epoch  5, batch     4 | loss: 45.3134756CurrentTrain: epoch  6, batch     0 | loss: 76.9986037CurrentTrain: epoch  6, batch     1 | loss: 72.6730825CurrentTrain: epoch  6, batch     2 | loss: 76.9721352CurrentTrain: epoch  6, batch     3 | loss: 72.6947017CurrentTrain: epoch  6, batch     4 | loss: 59.9236139CurrentTrain: epoch  7, batch     0 | loss: 115.6474283CurrentTrain: epoch  7, batch     1 | loss: 91.9836789CurrentTrain: epoch  7, batch     2 | loss: 70.3920975CurrentTrain: epoch  7, batch     3 | loss: 73.7199284CurrentTrain: epoch  7, batch     4 | loss: 56.6467784CurrentTrain: epoch  8, batch     0 | loss: 88.2921167CurrentTrain: epoch  8, batch     1 | loss: 63.0291668CurrentTrain: epoch  8, batch     2 | loss: 114.3521044CurrentTrain: epoch  8, batch     3 | loss: 73.7948488CurrentTrain: epoch  8, batch     4 | loss: 36.1355090CurrentTrain: epoch  9, batch     0 | loss: 60.1131703CurrentTrain: epoch  9, batch     1 | loss: 87.6914721CurrentTrain: epoch  9, batch     2 | loss: 68.6054010CurrentTrain: epoch  9, batch     3 | loss: 88.9074084CurrentTrain: epoch  9, batch     4 | loss: 100.3375068
MemoryTrain:  epoch  0, batch     0 | loss: 1.1811659MemoryTrain:  epoch  1, batch     0 | loss: 0.9700828MemoryTrain:  epoch  2, batch     0 | loss: 0.6751270MemoryTrain:  epoch  3, batch     0 | loss: 0.5888117MemoryTrain:  epoch  4, batch     0 | loss: 0.4988877MemoryTrain:  epoch  5, batch     0 | loss: 0.4340705MemoryTrain:  epoch  6, batch     0 | loss: 0.3354875MemoryTrain:  epoch  7, batch     0 | loss: 0.2638455MemoryTrain:  epoch  8, batch     0 | loss: 0.2237475MemoryTrain:  epoch  9, batch     0 | loss: 0.2017155

F1 score per class: {3: 0.0, 5: 0.9393939393939394, 6: 0.0, 8: 0.0, 10: 0.5100671140939598, 14: 0.0, 15: 0.0, 16: 0.7741935483870968, 17: 0.0, 18: 0.18604651162790697, 20: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.535303776683087
Weighted-average F1 score: 0.45328638819629025
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6903914590747331, 6: 0.0, 8: 0.0, 10: 0.5548387096774193, 14: 0.0, 15: 0.0, 16: 0.6842105263157895, 17: 0.0, 18: 0.16417910447761194, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.42961165048543687
Weighted-average F1 score: 0.3702117526818237
F1 score per class: {3: 0.0, 5: 0.8311688311688312, 6: 0.0, 8: 0.0, 10: 0.535031847133758, 14: 0.0, 15: 0.0, 16: 0.7575757575757576, 17: 0.0, 18: 0.16541353383458646, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.46900269541778977
Weighted-average F1 score: 0.39170034053774394

F1 score per class: {1: 0.16822429906542055, 3: 0.44919786096256686, 5: 0.8773584905660378, 6: 0.5042016806722689, 8: 0.39436619718309857, 10: 0.3333333333333333, 14: 0.0970873786407767, 15: 0.5217391304347826, 16: 0.7272727272727273, 17: 0.0, 18: 0.09580838323353294, 19: 0.6475409836065574, 20: 0.4585987261146497, 22: 0.5038167938931297, 24: 0.0, 25: 0.31746031746031744, 26: 0.6938775510204082, 29: 0.8516746411483254, 30: 0.8484848484848485, 32: 0.6058091286307054, 33: 0.375, 34: 0.24369747899159663, 35: 0.1320754716981132, 36: 0.11428571428571428, 37: 0.21176470588235294, 38: 0.0}
Micro-average F1 score: 0.45360246705915336
Weighted-average F1 score: 0.4512275542888414
F1 score per class: {1: 0.17857142857142858, 3: 0.3252032520325203, 5: 0.5359116022099447, 6: 0.49382716049382713, 8: 0.3917525773195876, 10: 0.3467741935483871, 14: 0.1037037037037037, 15: 0.46153846153846156, 16: 0.4444444444444444, 17: 0.0, 18: 0.1, 19: 0.5620915032679739, 20: 0.5263157894736842, 22: 0.49838187702265374, 24: 0.10256410256410256, 25: 0.38961038961038963, 26: 0.6222222222222222, 29: 0.8193832599118943, 30: 0.6545454545454545, 32: 0.5653710247349824, 33: 0.2222222222222222, 34: 0.2923076923076923, 35: 0.18579234972677597, 36: 0.4666666666666667, 37: 0.28846153846153844, 38: 0.40816326530612246}
Micro-average F1 score: 0.42350332594235035
Weighted-average F1 score: 0.4150878096995016
F1 score per class: {1: 0.1735159817351598, 3: 0.328, 5: 0.6981818181818182, 6: 0.5241379310344828, 8: 0.4134078212290503, 10: 0.328125, 14: 0.10071942446043165, 15: 0.48, 16: 0.5681818181818182, 17: 0.0, 18: 0.09691629955947137, 19: 0.589041095890411, 20: 0.46258503401360546, 22: 0.5032679738562091, 24: 0.06060606060606061, 25: 0.38235294117647056, 26: 0.6698564593301436, 29: 0.8378378378378378, 30: 0.7804878048780488, 32: 0.5693430656934306, 33: 0.35294117647058826, 34: 0.2708333333333333, 35: 0.15384615384615385, 36: 0.3409090909090909, 37: 0.2857142857142857, 38: 0.0}
Micro-average F1 score: 0.43052930056710775
Weighted-average F1 score: 0.42180956204691733

F1 score per class: {3: 0.0, 5: 0.8051948051948052, 6: 0.0, 8: 0.0, 10: 0.4470588235294118, 14: 0.0, 15: 0.0, 16: 0.5217391304347826, 17: 0.0, 18: 0.14545454545454545, 20: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3738532110091743
Weighted-average F1 score: 0.3015782634777125
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.5173333333333333, 6: 0.0, 8: 0.0, 10: 0.46994535519125685, 14: 0.0, 15: 0.0, 16: 0.42276422764227645, 17: 0.0, 18: 0.13414634146341464, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.27917981072555204
Weighted-average F1 score: 0.23954212669770797
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6274509803921569, 6: 0.0, 8: 0.0, 10: 0.44919786096256686, 14: 0.0, 15: 0.0, 16: 0.45454545454545453, 17: 0.0, 18: 0.13496932515337423, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3028720626631854
Weighted-average F1 score: 0.25395748819872754

F1 score per class: {1: 0.08759124087591241, 3: 0.32941176470588235, 5: 0.6940298507462687, 6: 0.3116883116883117, 8: 0.3010752688172043, 10: 0.24918032786885247, 14: 0.08130081300813008, 15: 0.34285714285714286, 16: 0.4485981308411215, 17: 0.0, 18: 0.06639004149377593, 19: 0.5602836879432624, 20: 0.23300970873786409, 22: 0.3905325443786982, 24: 0.0, 25: 0.3076923076923077, 26: 0.5887445887445888, 29: 0.6872586872586872, 30: 0.8, 32: 0.48344370860927155, 33: 0.3333333333333333, 34: 0.1475826972010178, 35: 0.08860759493670886, 36: 0.10126582278481013, 37: 0.1935483870967742, 38: 0.0}
Micro-average F1 score: 0.32627545876184716
Weighted-average F1 score: 0.31370669078055097
F1 score per class: {1: 0.09280742459396751, 3: 0.215633423180593, 5: 0.3527272727272727, 6: 0.28933092224231466, 8: 0.2724014336917563, 10: 0.23822714681440443, 14: 0.08, 15: 0.34285714285714286, 16: 0.25120772946859904, 17: 0.0, 18: 0.07308970099667775, 19: 0.4444444444444444, 20: 0.29535864978902954, 22: 0.3632075471698113, 24: 0.07547169811320754, 25: 0.37037037037037035, 26: 0.5072463768115942, 29: 0.5923566878980892, 30: 0.4864864864864865, 32: 0.43478260869565216, 33: 0.13043478260869565, 34: 0.16170212765957448, 35: 0.11147540983606558, 36: 0.2931937172774869, 37: 0.24390243902439024, 38: 0.22988505747126436}
Micro-average F1 score: 0.28338278931750743
Weighted-average F1 score: 0.2752007106127301
F1 score per class: {1: 0.08857808857808858, 3: 0.21635883905013192, 5: 0.45823389021479716, 6: 0.3095723014256619, 8: 0.2890625, 10: 0.224, 14: 0.07608695652173914, 15: 0.32432432432432434, 16: 0.29411764705882354, 17: 0.0, 18: 0.06918238993710692, 19: 0.4712328767123288, 20: 0.25660377358490566, 22: 0.3746958637469586, 24: 0.05405405405405406, 25: 0.37142857142857144, 26: 0.5622489959839357, 29: 0.6241610738255033, 30: 0.6956521739130435, 32: 0.44192634560906513, 33: 0.21428571428571427, 34: 0.15145631067961166, 35: 0.09411764705882353, 36: 0.2542372881355932, 37: 0.23728813559322035, 38: 0.0}
Micro-average F1 score: 0.2910543130990415
Weighted-average F1 score: 0.28077821078647286
cur_acc_wo_na:  ['0.7671', '0.5897', '0.5631', '0.3661', '0.5353']
his_acc_wo_na:  ['0.7671', '0.6846', '0.6326', '0.4664', '0.4536']
cur_acc des_wo_na:  ['0.7551', '0.5933', '0.5766', '0.3348', '0.4296']
his_acc des_wo_na:  ['0.7551', '0.6267', '0.5927', '0.4482', '0.4235']
cur_acc rrf_wo_na:  ['0.7639', '0.6216', '0.5824', '0.3510', '0.4690']
his_acc rrf_wo_na:  ['0.7639', '0.6575', '0.6043', '0.4506', '0.4305']
cur_acc_w_na:  ['0.6269', '0.4646', '0.4030', '0.2468', '0.3739']
his_acc_w_na:  ['0.6269', '0.5307', '0.4789', '0.3294', '0.3263']
cur_acc des_w_na:  ['0.6070', '0.4461', '0.4015', '0.2244', '0.2792']
his_acc des_w_na:  ['0.6070', '0.4535', '0.4356', '0.3112', '0.2834']
cur_acc rrf_w_na:  ['0.6166', '0.4742', '0.4070', '0.2367', '0.3029']
his_acc rrf_w_na:  ['0.6166', '0.4787', '0.4478', '0.3154', '0.2911']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 105.8111452CurrentTrain: epoch  0, batch     1 | loss: 139.5180910CurrentTrain: epoch  0, batch     2 | loss: 112.6846603CurrentTrain: epoch  0, batch     3 | loss: 94.5018736CurrentTrain: epoch  1, batch     0 | loss: 142.2949667CurrentTrain: epoch  1, batch     1 | loss: 76.8090098CurrentTrain: epoch  1, batch     2 | loss: 98.9229154CurrentTrain: epoch  1, batch     3 | loss: 69.9214081CurrentTrain: epoch  2, batch     0 | loss: 102.4172993CurrentTrain: epoch  2, batch     1 | loss: 72.3627529CurrentTrain: epoch  2, batch     2 | loss: 96.0858436CurrentTrain: epoch  2, batch     3 | loss: 57.1137430CurrentTrain: epoch  3, batch     0 | loss: 69.0842780CurrentTrain: epoch  3, batch     1 | loss: 81.2813337CurrentTrain: epoch  3, batch     2 | loss: 67.9607239CurrentTrain: epoch  3, batch     3 | loss: 66.8654517CurrentTrain: epoch  4, batch     0 | loss: 79.2546737CurrentTrain: epoch  4, batch     1 | loss: 81.4398015CurrentTrain: epoch  4, batch     2 | loss: 80.6035845CurrentTrain: epoch  4, batch     3 | loss: 58.4463165CurrentTrain: epoch  5, batch     0 | loss: 92.0550581CurrentTrain: epoch  5, batch     1 | loss: 82.6687824CurrentTrain: epoch  5, batch     2 | loss: 115.9499568CurrentTrain: epoch  5, batch     3 | loss: 58.5011097CurrentTrain: epoch  6, batch     0 | loss: 80.1015931CurrentTrain: epoch  6, batch     1 | loss: 94.7097445CurrentTrain: epoch  6, batch     2 | loss: 91.8323862CurrentTrain: epoch  6, batch     3 | loss: 47.7347294CurrentTrain: epoch  7, batch     0 | loss: 61.3589546CurrentTrain: epoch  7, batch     1 | loss: 88.3342802CurrentTrain: epoch  7, batch     2 | loss: 91.8498011CurrentTrain: epoch  7, batch     3 | loss: 79.2535904CurrentTrain: epoch  8, batch     0 | loss: 70.8508963CurrentTrain: epoch  8, batch     1 | loss: 71.7679524CurrentTrain: epoch  8, batch     2 | loss: 66.3922677CurrentTrain: epoch  8, batch     3 | loss: 92.4136136CurrentTrain: epoch  9, batch     0 | loss: 60.8210909CurrentTrain: epoch  9, batch     1 | loss: 61.9427068CurrentTrain: epoch  9, batch     2 | loss: 88.1318555CurrentTrain: epoch  9, batch     3 | loss: 72.5457701
MemoryTrain:  epoch  0, batch     0 | loss: 1.0409108MemoryTrain:  epoch  1, batch     0 | loss: 0.8251924MemoryTrain:  epoch  2, batch     0 | loss: 0.6710040MemoryTrain:  epoch  3, batch     0 | loss: 0.5490123MemoryTrain:  epoch  4, batch     0 | loss: 0.4382807MemoryTrain:  epoch  5, batch     0 | loss: 0.3943154MemoryTrain:  epoch  6, batch     0 | loss: 0.3160837MemoryTrain:  epoch  7, batch     0 | loss: 0.2369811MemoryTrain:  epoch  8, batch     0 | loss: 0.2100134MemoryTrain:  epoch  9, batch     0 | loss: 0.2025913

F1 score per class: {0: 0.8717948717948718, 1: 0.0, 3: 0.0, 4: 0.9130434782608695, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.3076923076923077, 15: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.4827586206896552, 22: 0.0, 23: 0.6172839506172839, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.6103646833013435
Weighted-average F1 score: 0.49476052311843716
F1 score per class: {0: 0.7216494845360825, 1: 0.0, 3: 0.0, 4: 0.8717948717948718, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.36363636363636365, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.5277777777777778, 22: 0.0, 23: 0.6024096385542169, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5195618153364632
Weighted-average F1 score: 0.40961082565951473
F1 score per class: {0: 0.7816091954022989, 1: 0.0, 3: 0.0, 4: 0.9090909090909091, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.36363636363636365, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.5066666666666667, 22: 0.0, 23: 0.5952380952380952, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.5472636815920398
Weighted-average F1 score: 0.4299486420365145

F1 score per class: {0: 0.6296296296296297, 1: 0.18181818181818182, 3: 0.455026455026455, 4: 0.9130434782608695, 5: 0.8518518518518519, 6: 0.4827586206896552, 8: 0.3120567375886525, 10: 0.26262626262626265, 13: 0.04081632653061224, 14: 0.08333333333333333, 15: 0.5454545454545454, 16: 0.7241379310344828, 17: 0.0, 18: 0.10989010989010989, 19: 0.6637931034482759, 20: 0.4195121951219512, 21: 0.15217391304347827, 22: 0.5652173913043478, 23: 0.5, 24: 0.0, 25: 0.31746031746031744, 26: 0.66, 29: 0.8301886792452831, 30: 0.8484848484848485, 32: 0.5714285714285714, 33: 0.4, 34: 0.23622047244094488, 35: 0.11764705882352941, 36: 0.057971014492753624, 37: 0.4, 38: 0.12903225806451613}
Micro-average F1 score: 0.45854607342572334
Weighted-average F1 score: 0.4462467566152522
F1 score per class: {0: 0.3153153153153153, 1: 0.18357487922705315, 3: 0.3025830258302583, 4: 0.8717948717948718, 5: 0.519893899204244, 6: 0.49201277955271566, 8: 0.423963133640553, 10: 0.33497536945812806, 13: 0.06349206349206349, 14: 0.0625, 15: 0.6, 16: 0.5227272727272727, 17: 0.125, 18: 0.13253012048192772, 19: 0.5782312925170068, 20: 0.47297297297297297, 21: 0.17117117117117117, 22: 0.48905109489051096, 23: 0.5050505050505051, 24: 0.0, 25: 0.35443037974683544, 26: 0.6111111111111112, 29: 0.7679324894514767, 30: 0.5964912280701754, 32: 0.5144694533762058, 33: 0.2222222222222222, 34: 0.27102803738317754, 35: 0.17543859649122806, 36: 0.49122807017543857, 37: 0.28846153846153844, 38: 0.37037037037037035}
Micro-average F1 score: 0.4239450441609421
Weighted-average F1 score: 0.41152682508047095
F1 score per class: {0: 0.4121212121212121, 1: 0.1926605504587156, 3: 0.2968197879858657, 4: 0.9090909090909091, 5: 0.617363344051447, 6: 0.5205479452054794, 8: 0.39378238341968913, 10: 0.33636363636363636, 13: 0.056338028169014086, 14: 0.08163265306122448, 15: 0.5714285714285714, 16: 0.5897435897435898, 17: 0.13333333333333333, 18: 0.12080536912751678, 19: 0.5857142857142857, 20: 0.46987951807228917, 21: 0.1532258064516129, 22: 0.5142857142857142, 23: 0.4807692307692308, 24: 0.0, 25: 0.39436619718309857, 26: 0.6346153846153846, 29: 0.7863247863247863, 30: 0.7391304347826086, 32: 0.5133333333333333, 33: 0.35294117647058826, 34: 0.25, 35: 0.1388888888888889, 36: 0.38636363636363635, 37: 0.2916666666666667, 38: 0.2222222222222222}
Micro-average F1 score: 0.43157460840890355
Weighted-average F1 score: 0.4167964537488951

F1 score per class: {0: 0.8095238095238095, 1: 0.0, 3: 0.0, 4: 0.8704663212435233, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.16666666666666666, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3333333333333333, 22: 0.0, 23: 0.5434782608695652, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.4435146443514644
Weighted-average F1 score: 0.33056507443808947
F1 score per class: {0: 0.5982905982905983, 1: 0.0, 3: 0.0, 4: 0.8292682926829268, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.25, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3958333333333333, 22: 0.0, 23: 0.5376344086021505, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.36685082872928176
Weighted-average F1 score: 0.27466681468222004
F1 score per class: {0: 0.6732673267326733, 1: 0.0, 3: 0.0, 4: 0.8717948717948718, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.25, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.36893203883495146, 22: 0.0, 23: 0.5050505050505051, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.3873239436619718
Weighted-average F1 score: 0.2871603715033966

F1 score per class: {0: 0.4503311258278146, 1: 0.09523809523809523, 3: 0.31970260223048325, 4: 0.8571428571428571, 5: 0.6594982078853047, 6: 0.29439252336448596, 8: 0.2222222222222222, 10: 0.19696969696969696, 13: 0.021505376344086023, 14: 0.075, 15: 0.375, 16: 0.4827586206896552, 17: 0.0, 18: 0.07936507936507936, 19: 0.5877862595419847, 20: 0.19953596287703015, 21: 0.09395973154362416, 22: 0.5024154589371981, 23: 0.3816793893129771, 24: 0.0, 25: 0.30303030303030304, 26: 0.5739130434782609, 29: 0.6692015209125475, 30: 0.7567567567567568, 32: 0.42729970326409494, 33: 0.35294117647058826, 34: 0.14457831325301204, 35: 0.07865168539325842, 36: 0.056338028169014086, 37: 0.36538461538461536, 38: 0.1111111111111111}
Micro-average F1 score: 0.32562154696132595
Weighted-average F1 score: 0.30271857733126817
F1 score per class: {0: 0.2, 1: 0.09336609336609336, 3: 0.18764302059496568, 4: 0.8056872037914692, 5: 0.3408695652173913, 6: 0.2857142857142857, 8: 0.26900584795321636, 10: 0.22972972972972974, 13: 0.034482758620689655, 14: 0.05172413793103448, 15: 0.4444444444444444, 16: 0.32167832167832167, 17: 0.05714285714285714, 18: 0.09361702127659574, 19: 0.46070460704607047, 20: 0.24054982817869416, 21: 0.10734463276836158, 22: 0.4200626959247649, 23: 0.423728813559322, 24: 0.0, 25: 0.3333333333333333, 26: 0.5196850393700787, 29: 0.56, 30: 0.425, 32: 0.3827751196172249, 33: 0.12244897959183673, 34: 0.15591397849462366, 35: 0.10204081632653061, 36: 0.34782608695652173, 37: 0.25210084033613445, 38: 0.22727272727272727}
Micro-average F1 score: 0.28624436787702096
Weighted-average F1 score: 0.27238617097103685
F1 score per class: {0: 0.2546816479400749, 1: 0.10023866348448687, 3: 0.1838074398249453, 4: 0.8585858585858586, 5: 0.4247787610619469, 6: 0.304, 8: 0.2558922558922559, 10: 0.22910216718266255, 13: 0.031746031746031744, 14: 0.06896551724137931, 15: 0.42857142857142855, 16: 0.3511450381679389, 17: 0.06896551724137931, 18: 0.0821917808219178, 19: 0.47262247838616717, 20: 0.24375, 21: 0.0945273631840796, 22: 0.4548736462093863, 23: 0.37037037037037035, 24: 0.0, 25: 0.3783783783783784, 26: 0.55, 29: 0.5768025078369906, 30: 0.5862068965517241, 32: 0.39086294416243655, 33: 0.24, 34: 0.14903846153846154, 35: 0.08196721311475409, 36: 0.3063063063063063, 37: 0.25, 38: 0.15384615384615385}
Micro-average F1 score: 0.2945976364659538
Weighted-average F1 score: 0.27757704480912887
cur_acc_wo_na:  ['0.7671', '0.5897', '0.5631', '0.3661', '0.5353', '0.6104']
his_acc_wo_na:  ['0.7671', '0.6846', '0.6326', '0.4664', '0.4536', '0.4585']
cur_acc des_wo_na:  ['0.7551', '0.5933', '0.5766', '0.3348', '0.4296', '0.5196']
his_acc des_wo_na:  ['0.7551', '0.6267', '0.5927', '0.4482', '0.4235', '0.4239']
cur_acc rrf_wo_na:  ['0.7639', '0.6216', '0.5824', '0.3510', '0.4690', '0.5473']
his_acc rrf_wo_na:  ['0.7639', '0.6575', '0.6043', '0.4506', '0.4305', '0.4316']
cur_acc_w_na:  ['0.6269', '0.4646', '0.4030', '0.2468', '0.3739', '0.4435']
his_acc_w_na:  ['0.6269', '0.5307', '0.4789', '0.3294', '0.3263', '0.3256']
cur_acc des_w_na:  ['0.6070', '0.4461', '0.4015', '0.2244', '0.2792', '0.3669']
his_acc des_w_na:  ['0.6070', '0.4535', '0.4356', '0.3112', '0.2834', '0.2862']
cur_acc rrf_w_na:  ['0.6166', '0.4742', '0.4070', '0.2367', '0.3029', '0.3873']
his_acc rrf_w_na:  ['0.6166', '0.4787', '0.4478', '0.3154', '0.2911', '0.2946']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 86.5236433CurrentTrain: epoch  0, batch     1 | loss: 116.5458287CurrentTrain: epoch  0, batch     2 | loss: 105.3152824CurrentTrain: epoch  0, batch     3 | loss: 110.6349618CurrentTrain: epoch  0, batch     4 | loss: 27.7126337CurrentTrain: epoch  1, batch     0 | loss: 89.0463506CurrentTrain: epoch  1, batch     1 | loss: 123.2447604CurrentTrain: epoch  1, batch     2 | loss: 70.4602783CurrentTrain: epoch  1, batch     3 | loss: 173.2203712CurrentTrain: epoch  1, batch     4 | loss: 28.1504521CurrentTrain: epoch  2, batch     0 | loss: 103.0725005CurrentTrain: epoch  2, batch     1 | loss: 95.6391141CurrentTrain: epoch  2, batch     2 | loss: 91.3772191CurrentTrain: epoch  2, batch     3 | loss: 80.0084277CurrentTrain: epoch  2, batch     4 | loss: 13.7257213CurrentTrain: epoch  3, batch     0 | loss: 79.4928696CurrentTrain: epoch  3, batch     1 | loss: 78.3265399CurrentTrain: epoch  3, batch     2 | loss: 75.6818456CurrentTrain: epoch  3, batch     3 | loss: 76.0498520CurrentTrain: epoch  3, batch     4 | loss: 24.4744650CurrentTrain: epoch  4, batch     0 | loss: 64.0758357CurrentTrain: epoch  4, batch     1 | loss: 61.1194093CurrentTrain: epoch  4, batch     2 | loss: 79.8344308CurrentTrain: epoch  4, batch     3 | loss: 77.3000201CurrentTrain: epoch  4, batch     4 | loss: 39.3109896CurrentTrain: epoch  5, batch     0 | loss: 62.9200282CurrentTrain: epoch  5, batch     1 | loss: 114.9957259CurrentTrain: epoch  5, batch     2 | loss: 76.1415973CurrentTrain: epoch  5, batch     3 | loss: 73.0447467CurrentTrain: epoch  5, batch     4 | loss: 23.7918372CurrentTrain: epoch  6, batch     0 | loss: 73.5245983CurrentTrain: epoch  6, batch     1 | loss: 61.8218514CurrentTrain: epoch  6, batch     2 | loss: 70.6837033CurrentTrain: epoch  6, batch     3 | loss: 91.0486782CurrentTrain: epoch  6, batch     4 | loss: 36.2982668CurrentTrain: epoch  7, batch     0 | loss: 71.8958214CurrentTrain: epoch  7, batch     1 | loss: 62.3169350CurrentTrain: epoch  7, batch     2 | loss: 72.3908883CurrentTrain: epoch  7, batch     3 | loss: 71.4634123CurrentTrain: epoch  7, batch     4 | loss: 11.0643155CurrentTrain: epoch  8, batch     0 | loss: 59.7549230CurrentTrain: epoch  8, batch     1 | loss: 86.6240444CurrentTrain: epoch  8, batch     2 | loss: 73.5572547CurrentTrain: epoch  8, batch     3 | loss: 59.0715374CurrentTrain: epoch  8, batch     4 | loss: 21.5228921CurrentTrain: epoch  9, batch     0 | loss: 72.3249629CurrentTrain: epoch  9, batch     1 | loss: 84.4200431CurrentTrain: epoch  9, batch     2 | loss: 72.2775293CurrentTrain: epoch  9, batch     3 | loss: 69.9145389CurrentTrain: epoch  9, batch     4 | loss: 21.5385941
MemoryTrain:  epoch  0, batch     0 | loss: 0.8485678MemoryTrain:  epoch  1, batch     0 | loss: 0.7338472MemoryTrain:  epoch  2, batch     0 | loss: 0.6033085MemoryTrain:  epoch  3, batch     0 | loss: 0.5498618MemoryTrain:  epoch  4, batch     0 | loss: 0.4222825MemoryTrain:  epoch  5, batch     0 | loss: 0.3877980MemoryTrain:  epoch  6, batch     0 | loss: 0.3217849MemoryTrain:  epoch  7, batch     0 | loss: 0.2863216MemoryTrain:  epoch  8, batch     0 | loss: 0.2462276MemoryTrain:  epoch  9, batch     0 | loss: 0.2093212

F1 score per class: {0: 0.0, 2: 0.5185185185185185, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.4098360655737705, 12: 0.6881720430107527, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 28: 0.32558139534883723, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 39: 0.3333333333333333}
Micro-average F1 score: 0.41472868217054265
Weighted-average F1 score: 0.3272028196374037
F1 score per class: {0: 0.0, 2: 0.4375, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.45588235294117646, 12: 0.6802030456852792, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 28: 0.22857142857142856, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.5}
Micro-average F1 score: 0.36220472440944884
Weighted-average F1 score: 0.2622499445771085
F1 score per class: {0: 0.0, 2: 0.5185185185185185, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.4411764705882353, 12: 0.6938775510204082, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 28: 0.27906976744186046, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.43478260869565216}
Micro-average F1 score: 0.3918918918918919
Weighted-average F1 score: 0.2970577323277582

F1 score per class: {0: 0.7209302325581395, 1: 0.19047619047619047, 2: 0.25925925925925924, 3: 0.3717948717948718, 4: 0.8505747126436781, 5: 0.8679245283018868, 6: 0.48727272727272725, 8: 0.375, 10: 0.3087248322147651, 11: 0.19157088122605365, 12: 0.3069544364508393, 13: 0.0625, 14: 0.11764705882352941, 15: 0.5714285714285714, 16: 0.65625, 17: 0.0, 18: 0.08955223880597014, 19: 0.656, 20: 0.41, 21: 0.20408163265306123, 22: 0.5380116959064327, 23: 0.5957446808510638, 24: 0.0, 25: 0.3225806451612903, 26: 0.6666666666666666, 28: 0.10071942446043165, 29: 0.784688995215311, 30: 0.8484848484848485, 32: 0.5757575757575758, 33: 0.35294117647058826, 34: 0.24719101123595505, 35: 0.18, 36: 0.11428571428571428, 37: 0.2696629213483146, 38: 0.125, 39: 0.13333333333333333}
Micro-average F1 score: 0.42999787550456764
Weighted-average F1 score: 0.41585645649452874
F1 score per class: {0: 0.3302752293577982, 1: 0.1890547263681592, 2: 0.1076923076923077, 3: 0.3735408560311284, 4: 0.8673469387755102, 5: 0.5355191256830601, 6: 0.4553314121037464, 8: 0.3709090909090909, 10: 0.2840909090909091, 11: 0.20945945945945946, 12: 0.2797494780793319, 13: 0.05555555555555555, 14: 0.06315789473684211, 15: 0.6, 16: 0.5675675675675675, 17: 0.0, 18: 0.08, 19: 0.5426829268292683, 20: 0.46616541353383456, 21: 0.1371841155234657, 22: 0.4979253112033195, 23: 0.5714285714285714, 24: 0.0, 25: 0.3783783783783784, 26: 0.6226415094339622, 28: 0.09302325581395349, 29: 0.7853881278538812, 30: 0.6206896551724138, 32: 0.5, 33: 0.23076923076923078, 34: 0.23684210526315788, 35: 0.23668639053254437, 36: 0.3673469387755102, 37: 0.2608695652173913, 38: 0.25, 39: 0.23076923076923078}
Micro-average F1 score: 0.3875994694960212
Weighted-average F1 score: 0.37372719143195093
F1 score per class: {0: 0.5, 1: 0.20095693779904306, 2: 0.14432989690721648, 3: 0.3574144486692015, 4: 0.8924731182795699, 5: 0.6508474576271186, 6: 0.47619047619047616, 8: 0.4, 10: 0.29714285714285715, 11: 0.1941747572815534, 12: 0.28874734607218683, 13: 0.05405405405405406, 14: 0.11428571428571428, 15: 0.5714285714285714, 16: 0.5833333333333334, 17: 0.0, 18: 0.08602150537634409, 19: 0.5913621262458472, 20: 0.4563758389261745, 21: 0.14559386973180077, 22: 0.5370370370370371, 23: 0.5490196078431373, 24: 0.0, 25: 0.3582089552238806, 26: 0.6470588235294118, 28: 0.08633093525179857, 29: 0.8, 30: 0.7111111111111111, 32: 0.5014925373134328, 33: 0.3157894736842105, 34: 0.24242424242424243, 35: 0.19548872180451127, 36: 0.136986301369863, 37: 0.2641509433962264, 38: 0.13953488372093023, 39: 0.14084507042253522}
Micro-average F1 score: 0.3980081806864663
Weighted-average F1 score: 0.38263877306853866

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.2978723404255319, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.3246753246753247, 12: 0.5140562248995983, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 28: 0.16091954022988506, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.21621621621621623}
Micro-average F1 score: 0.2584541062801932
Weighted-average F1 score: 0.20858409610166775
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.25925925925925924, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.3563218390804598, 12: 0.5234375, 13: 0.0, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.12698412698412698, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.375}
Micro-average F1 score: 0.224609375
Weighted-average F1 score: 0.1710567972816313
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.2916666666666667, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.3488372093023256, 12: 0.5210727969348659, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.13186813186813187, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.23809523809523808}
Micro-average F1 score: 0.24293193717277486
Weighted-average F1 score: 0.19318174932014995

F1 score per class: {0: 0.5299145299145299, 1: 0.0975609756097561, 2: 0.1308411214953271, 3: 0.27884615384615385, 4: 0.8131868131868132, 5: 0.6789667896678967, 6: 0.27459016393442626, 8: 0.3, 10: 0.24731182795698925, 11: 0.12254901960784313, 12: 0.11873840445269017, 13: 0.02857142857142857, 14: 0.10416666666666667, 15: 0.41379310344827586, 16: 0.4117647058823529, 17: 0.0, 18: 0.056074766355140186, 19: 0.5795053003533569, 20: 0.19294117647058823, 21: 0.1271186440677966, 22: 0.4946236559139785, 23: 0.4827586206896552, 24: 0.0, 25: 0.3076923076923077, 26: 0.5777777777777777, 28: 0.051094890510948905, 29: 0.6482213438735178, 30: 0.7567567567567568, 32: 0.428169014084507, 33: 0.3, 34: 0.16793893129770993, 35: 0.13636363636363635, 36: 0.10810810810810811, 37: 0.2376237623762376, 38: 0.09302325581395349, 39: 0.07407407407407407}
Micro-average F1 score: 0.2811111111111111
Weighted-average F1 score: 0.25372410303267934
F1 score per class: {0: 0.20869565217391303, 1: 0.09429280397022333, 2: 0.06481481481481481, 3: 0.25196850393700787, 4: 0.7981220657276995, 5: 0.3408695652173913, 6: 0.25039619651347067, 8: 0.23665893271461716, 10: 0.2145922746781116, 11: 0.13537117903930132, 12: 0.1213768115942029, 13: 0.03125, 14: 0.05660377358490566, 15: 0.4444444444444444, 16: 0.3559322033898305, 17: 0.0, 18: 0.05063291139240506, 19: 0.4320388349514563, 20: 0.25203252032520324, 21: 0.08775981524249422, 22: 0.42704626334519574, 23: 0.4628099173553719, 24: 0.0, 25: 0.35443037974683544, 26: 0.5301204819277109, 28: 0.04790419161676647, 29: 0.6035087719298246, 30: 0.45, 32: 0.35802469135802467, 33: 0.16666666666666666, 34: 0.1422924901185771, 35: 0.13745704467353953, 36: 0.28346456692913385, 37: 0.22058823529411764, 38: 0.18461538461538463, 39: 0.11009174311926606}
Micro-average F1 score: 0.24962630792227206
Weighted-average F1 score: 0.23440626413937307
F1 score per class: {0: 0.3422459893048128, 1: 0.1016949152542373, 2: 0.08383233532934131, 3: 0.2416452442159383, 4: 0.8469387755102041, 5: 0.4528301886792453, 6: 0.2697841726618705, 8: 0.2755905511811024, 10: 0.22608695652173913, 11: 0.12658227848101267, 12: 0.12121212121212122, 13: 0.031746031746031744, 14: 0.0975609756097561, 15: 0.4, 16: 0.375, 17: 0.0, 18: 0.053691275167785234, 19: 0.48633879781420764, 20: 0.24727272727272728, 21: 0.09156626506024096, 22: 0.47540983606557374, 23: 0.42105263157894735, 24: 0.0, 25: 0.34285714285714286, 26: 0.5617021276595745, 28: 0.04332129963898917, 29: 0.6187050359712231, 30: 0.5245901639344263, 32: 0.375, 33: 0.25, 34: 0.14925373134328357, 35: 0.12149532710280374, 36: 0.11904761904761904, 37: 0.2222222222222222, 38: 0.10714285714285714, 39: 0.06896551724137931}
Micro-average F1 score: 0.25896783152048136
Weighted-average F1 score: 0.24020977335533997
cur_acc_wo_na:  ['0.7671', '0.5897', '0.5631', '0.3661', '0.5353', '0.6104', '0.4147']
his_acc_wo_na:  ['0.7671', '0.6846', '0.6326', '0.4664', '0.4536', '0.4585', '0.4300']
cur_acc des_wo_na:  ['0.7551', '0.5933', '0.5766', '0.3348', '0.4296', '0.5196', '0.3622']
his_acc des_wo_na:  ['0.7551', '0.6267', '0.5927', '0.4482', '0.4235', '0.4239', '0.3876']
cur_acc rrf_wo_na:  ['0.7639', '0.6216', '0.5824', '0.3510', '0.4690', '0.5473', '0.3919']
his_acc rrf_wo_na:  ['0.7639', '0.6575', '0.6043', '0.4506', '0.4305', '0.4316', '0.3980']
cur_acc_w_na:  ['0.6269', '0.4646', '0.4030', '0.2468', '0.3739', '0.4435', '0.2585']
his_acc_w_na:  ['0.6269', '0.5307', '0.4789', '0.3294', '0.3263', '0.3256', '0.2811']
cur_acc des_w_na:  ['0.6070', '0.4461', '0.4015', '0.2244', '0.2792', '0.3669', '0.2246']
his_acc des_w_na:  ['0.6070', '0.4535', '0.4356', '0.3112', '0.2834', '0.2862', '0.2496']
cur_acc rrf_w_na:  ['0.6166', '0.4742', '0.4070', '0.2367', '0.3029', '0.3873', '0.2429']
his_acc rrf_w_na:  ['0.6166', '0.4787', '0.4478', '0.3154', '0.2911', '0.2946', '0.2590']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 80.6213505CurrentTrain: epoch  0, batch     1 | loss: 93.2240493CurrentTrain: epoch  0, batch     2 | loss: 82.7689363CurrentTrain: epoch  0, batch     3 | loss: 24.1325581CurrentTrain: epoch  1, batch     0 | loss: 102.2737863CurrentTrain: epoch  1, batch     1 | loss: 73.0266564CurrentTrain: epoch  1, batch     2 | loss: 85.7644935CurrentTrain: epoch  1, batch     3 | loss: 13.5023342CurrentTrain: epoch  2, batch     0 | loss: 74.4490034CurrentTrain: epoch  2, batch     1 | loss: 67.4087437CurrentTrain: epoch  2, batch     2 | loss: 102.0259605CurrentTrain: epoch  2, batch     3 | loss: 5.4646882CurrentTrain: epoch  3, batch     0 | loss: 59.7138587CurrentTrain: epoch  3, batch     1 | loss: 90.4399360CurrentTrain: epoch  3, batch     2 | loss: 82.6426773CurrentTrain: epoch  3, batch     3 | loss: 15.7055965CurrentTrain: epoch  4, batch     0 | loss: 72.9303997CurrentTrain: epoch  4, batch     1 | loss: 64.2013847CurrentTrain: epoch  4, batch     2 | loss: 71.3066493CurrentTrain: epoch  4, batch     3 | loss: 14.0129648CurrentTrain: epoch  5, batch     0 | loss: 68.2573273CurrentTrain: epoch  5, batch     1 | loss: 72.6990148CurrentTrain: epoch  5, batch     2 | loss: 63.9506807CurrentTrain: epoch  5, batch     3 | loss: 7.4146881CurrentTrain: epoch  6, batch     0 | loss: 60.4168830CurrentTrain: epoch  6, batch     1 | loss: 73.9252683CurrentTrain: epoch  6, batch     2 | loss: 68.8231894CurrentTrain: epoch  6, batch     3 | loss: 11.2985500CurrentTrain: epoch  7, batch     0 | loss: 60.0507670CurrentTrain: epoch  7, batch     1 | loss: 57.4692063CurrentTrain: epoch  7, batch     2 | loss: 70.8856296CurrentTrain: epoch  7, batch     3 | loss: 7.8933850CurrentTrain: epoch  8, batch     0 | loss: 71.0596472CurrentTrain: epoch  8, batch     1 | loss: 58.2700245CurrentTrain: epoch  8, batch     2 | loss: 67.5749328CurrentTrain: epoch  8, batch     3 | loss: 3.5370769CurrentTrain: epoch  9, batch     0 | loss: 57.5767268CurrentTrain: epoch  9, batch     1 | loss: 68.2464875CurrentTrain: epoch  9, batch     2 | loss: 69.4066297CurrentTrain: epoch  9, batch     3 | loss: 7.6976521
MemoryTrain:  epoch  0, batch     0 | loss: 0.7807481MemoryTrain:  epoch  1, batch     0 | loss: 0.6542696MemoryTrain:  epoch  2, batch     0 | loss: 0.4995402MemoryTrain:  epoch  3, batch     0 | loss: 0.4208286MemoryTrain:  epoch  4, batch     0 | loss: 0.3576158MemoryTrain:  epoch  5, batch     0 | loss: 0.2872595MemoryTrain:  epoch  6, batch     0 | loss: 0.2582162MemoryTrain:  epoch  7, batch     0 | loss: 0.2151578MemoryTrain:  epoch  8, batch     0 | loss: 0.1746287MemoryTrain:  epoch  9, batch     0 | loss: 0.2147334

F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.8771929824561403, 10: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 31: 0.3333333333333333, 34: 0.0, 40: 0.33070866141732286}
Micro-average F1 score: 0.3072100313479624
Weighted-average F1 score: 0.2399072388095262
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.6578947368421053, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.2608695652173913, 30: 0.0, 31: 0.3333333333333333, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.4}
Micro-average F1 score: 0.3085399449035813
Weighted-average F1 score: 0.24409487136734803
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 9: 0.8064516129032258, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.24, 31: 0.3333333333333333, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.37593984962406013}
Micro-average F1 score: 0.3236994219653179
Weighted-average F1 score: 0.25355404640633844

F1 score per class: {0: 0.5038167938931297, 1: 0.125, 2: 0.34146341463414637, 3: 0.4, 4: 0.8372093023255814, 5: 0.8230088495575221, 6: 0.3314285714285714, 7: 0.03305785123966942, 8: 0.2975206611570248, 9: 0.847457627118644, 10: 0.2894736842105263, 11: 0.16236162361623616, 12: 0.32189973614775724, 13: 0.03333333333333333, 14: 0.047058823529411764, 15: 0.5714285714285714, 16: 0.6440677966101694, 17: 0.0, 18: 0.05063291139240506, 19: 0.5467625899280576, 20: 0.4174757281553398, 21: 0.18803418803418803, 22: 0.5888888888888889, 23: 0.5681818181818182, 24: 0.0, 25: 0.3492063492063492, 26: 0.6424870466321243, 27: 0.0, 28: 0.13186813186813187, 29: 0.7676767676767676, 30: 0.8484848484848485, 31: 0.10526315789473684, 32: 0.5714285714285714, 33: 0.375, 34: 0.22784810126582278, 35: 0.14, 36: 0.058823529411764705, 37: 0.30927835051546393, 38: 0.12121212121212122, 39: 0.1276595744680851, 40: 0.12612612612612611}
Micro-average F1 score: 0.38158667972575905
Weighted-average F1 score: 0.36029594844435625
F1 score per class: {0: 0.24742268041237114, 1: 0.14545454545454545, 2: 0.13861386138613863, 3: 0.4188034188034188, 4: 0.8172043010752689, 5: 0.5243243243243243, 6: 0.40298507462686567, 7: 0.039603960396039604, 8: 0.3392857142857143, 9: 0.4032258064516129, 10: 0.25, 11: 0.199203187250996, 12: 0.27461139896373055, 13: 0.030303030303030304, 14: 0.07317073170731707, 15: 0.5714285714285714, 16: 0.5853658536585366, 17: 0.0, 18: 0.1415929203539823, 19: 0.44692737430167595, 20: 0.4666666666666667, 21: 0.15819209039548024, 22: 0.5622119815668203, 23: 0.5416666666666666, 24: 0.0, 25: 0.3835616438356164, 26: 0.6432160804020101, 27: 0.08108108108108109, 28: 0.1388888888888889, 29: 0.7586206896551724, 30: 0.7555555555555555, 31: 0.07692307692307693, 32: 0.4909090909090909, 33: 0.2857142857142857, 34: 0.25274725274725274, 35: 0.2251655629139073, 36: 0.3673469387755102, 37: 0.31666666666666665, 38: 0.22641509433962265, 39: 0.2127659574468085, 40: 0.1607717041800643}
Micro-average F1 score: 0.3610223642172524
Weighted-average F1 score: 0.34283555582819236
F1 score per class: {0: 0.32, 1: 0.14545454545454545, 2: 0.18421052631578946, 3: 0.4083333333333333, 4: 0.847457627118644, 5: 0.6442953020134228, 6: 0.40707964601769914, 7: 0.03361344537815126, 8: 0.31137724550898205, 9: 0.7575757575757576, 10: 0.24050632911392406, 11: 0.1865671641791045, 12: 0.28346456692913385, 13: 0.029850746268656716, 14: 0.10416666666666667, 15: 0.5454545454545454, 16: 0.6956521739130435, 17: 0.0, 18: 0.13725490196078433, 19: 0.5110410094637224, 20: 0.4367816091954023, 21: 0.15217391304347827, 22: 0.6030150753768844, 23: 0.5306122448979592, 24: 0.0, 25: 0.4117647058823529, 26: 0.6461538461538462, 27: 0.07894736842105263, 28: 0.10416666666666667, 29: 0.7638190954773869, 30: 0.7567567567567568, 31: 0.08, 32: 0.5342960288808665, 33: 0.375, 34: 0.23655913978494625, 35: 0.176, 36: 0.10810810810810811, 37: 0.34545454545454546, 38: 0.13636363636363635, 39: 0.17777777777777778, 40: 0.14204545454545456}
Micro-average F1 score: 0.3677099367196853
Weighted-average F1 score: 0.3457714893485495

F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4444444444444444, 9: 0.8333333333333334, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.25, 32: 0.0, 34: 0.0, 35: 0.0, 40: 0.29577464788732394}
Micro-average F1 score: 0.23557692307692307
Weighted-average F1 score: 0.17714713594295725
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4, 9: 0.5555555555555556, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.23076923076923078, 28: 0.0, 30: 0.0, 31: 0.2222222222222222, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.33783783783783783}
Micro-average F1 score: 0.22672064777327935
Weighted-average F1 score: 0.18194246878457407
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4, 9: 0.7352941176470589, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.2222222222222222, 28: 0.0, 31: 0.25, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.32051282051282054}
Micro-average F1 score: 0.24295010845986983
Weighted-average F1 score: 0.18933311889745183

F1 score per class: {0: 0.3707865168539326, 1: 0.06870229007633588, 2: 0.18181818181818182, 3: 0.2857142857142857, 4: 0.8, 5: 0.62, 6: 0.21014492753623187, 7: 0.017316017316017316, 8: 0.2465753424657534, 9: 0.7692307692307693, 10: 0.22797927461139897, 11: 0.102803738317757, 12: 0.13510520487264674, 13: 0.01694915254237288, 14: 0.042105263157894736, 15: 0.46153846153846156, 16: 0.4, 17: 0.0, 18: 0.031746031746031744, 19: 0.5, 20: 0.2052505966587112, 21: 0.1164021164021164, 22: 0.527363184079602, 23: 0.44642857142857145, 24: 0.0, 25: 0.3333333333333333, 26: 0.5662100456621004, 27: 0.0, 28: 0.07058823529411765, 29: 0.6333333333333333, 30: 0.7777777777777778, 31: 0.05405405405405406, 32: 0.4489795918367347, 33: 0.3333333333333333, 34: 0.1506276150627615, 35: 0.11382113821138211, 36: 0.05714285714285714, 37: 0.2727272727272727, 38: 0.10256410256410256, 39: 0.07058823529411765, 40: 0.09170305676855896}
Micro-average F1 score: 0.2602886157135222
Weighted-average F1 score: 0.23372721053641526
F1 score per class: {0: 0.16179775280898875, 1: 0.07741935483870968, 2: 0.08333333333333333, 3: 0.2808022922636103, 4: 0.7676767676767676, 5: 0.3050314465408805, 6: 0.23126338329764454, 7: 0.0213903743315508, 8: 0.22418879056047197, 9: 0.30120481927710846, 10: 0.19138755980861244, 11: 0.13157894736842105, 12: 0.12470588235294118, 13: 0.017241379310344827, 14: 0.06896551724137931, 15: 0.4444444444444444, 16: 0.35555555555555557, 17: 0.0, 18: 0.08465608465608465, 19: 0.38095238095238093, 20: 0.24734982332155478, 21: 0.09929078014184398, 22: 0.48031496062992124, 23: 0.43333333333333335, 24: 0.0, 25: 0.358974358974359, 26: 0.5565217391304348, 27: 0.058823529411764705, 28: 0.07042253521126761, 29: 0.5968992248062015, 30: 0.5862068965517241, 31: 0.03333333333333333, 32: 0.3568281938325991, 33: 0.1875, 34: 0.15753424657534246, 35: 0.13877551020408163, 36: 0.288, 37: 0.2620689655172414, 38: 0.16, 39: 0.10638297872340426, 40: 0.11627906976744186}
Micro-average F1 score: 0.23824583596879612
Weighted-average F1 score: 0.22175139666993984
F1 score per class: {0: 0.21238938053097345, 1: 0.07667731629392971, 2: 0.1111111111111111, 3: 0.2737430167597765, 4: 0.8108108108108109, 5: 0.39751552795031053, 6: 0.2440318302387268, 7: 0.01694915254237288, 8: 0.22413793103448276, 9: 0.6410256410256411, 10: 0.18357487922705315, 11: 0.12437810945273632, 12: 0.12661195779601406, 13: 0.016666666666666666, 14: 0.09523809523809523, 15: 0.41379310344827586, 16: 0.41739130434782606, 17: 0.0, 18: 0.07954545454545454, 19: 0.4462809917355372, 20: 0.24126984126984127, 21: 0.09491525423728814, 22: 0.5286343612334802, 23: 0.4126984126984127, 24: 0.0, 25: 0.39436619718309857, 26: 0.5701357466063348, 27: 0.0594059405940594, 28: 0.05405405405405406, 29: 0.6031746031746031, 30: 0.6666666666666666, 31: 0.03636363636363636, 32: 0.3978494623655914, 33: 0.2608695652173913, 34: 0.1506849315068493, 35: 0.11224489795918367, 36: 0.0975609756097561, 37: 0.2923076923076923, 38: 0.1016949152542373, 39: 0.08421052631578947, 40: 0.10416666666666667}
Micro-average F1 score: 0.24560201050948138
Weighted-average F1 score: 0.2248534642273037
cur_acc_wo_na:  ['0.7671', '0.5897', '0.5631', '0.3661', '0.5353', '0.6104', '0.4147', '0.3072']
his_acc_wo_na:  ['0.7671', '0.6846', '0.6326', '0.4664', '0.4536', '0.4585', '0.4300', '0.3816']
cur_acc des_wo_na:  ['0.7551', '0.5933', '0.5766', '0.3348', '0.4296', '0.5196', '0.3622', '0.3085']
his_acc des_wo_na:  ['0.7551', '0.6267', '0.5927', '0.4482', '0.4235', '0.4239', '0.3876', '0.3610']
cur_acc rrf_wo_na:  ['0.7639', '0.6216', '0.5824', '0.3510', '0.4690', '0.5473', '0.3919', '0.3237']
his_acc rrf_wo_na:  ['0.7639', '0.6575', '0.6043', '0.4506', '0.4305', '0.4316', '0.3980', '0.3677']
cur_acc_w_na:  ['0.6269', '0.4646', '0.4030', '0.2468', '0.3739', '0.4435', '0.2585', '0.2356']
his_acc_w_na:  ['0.6269', '0.5307', '0.4789', '0.3294', '0.3263', '0.3256', '0.2811', '0.2603']
cur_acc des_w_na:  ['0.6070', '0.4461', '0.4015', '0.2244', '0.2792', '0.3669', '0.2246', '0.2267']
his_acc des_w_na:  ['0.6070', '0.4535', '0.4356', '0.3112', '0.2834', '0.2862', '0.2496', '0.2382']
cur_acc rrf_w_na:  ['0.6166', '0.4742', '0.4070', '0.2367', '0.3029', '0.3873', '0.2429', '0.2430']
his_acc rrf_w_na:  ['0.6166', '0.4787', '0.4478', '0.3154', '0.2911', '0.2946', '0.2590', '0.2456']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 109.1709042CurrentTrain: epoch  0, batch     1 | loss: 80.5884601CurrentTrain: epoch  0, batch     2 | loss: 120.8308465CurrentTrain: epoch  0, batch     3 | loss: 87.7631964CurrentTrain: epoch  0, batch     4 | loss: 88.3001547CurrentTrain: epoch  0, batch     5 | loss: 119.3873297CurrentTrain: epoch  0, batch     6 | loss: 101.8341592CurrentTrain: epoch  0, batch     7 | loss: 99.9595446CurrentTrain: epoch  0, batch     8 | loss: 86.9773634CurrentTrain: epoch  0, batch     9 | loss: 100.0965765CurrentTrain: epoch  0, batch    10 | loss: 100.7215400CurrentTrain: epoch  0, batch    11 | loss: 118.5187933CurrentTrain: epoch  0, batch    12 | loss: 85.9210607CurrentTrain: epoch  0, batch    13 | loss: 85.8841548CurrentTrain: epoch  0, batch    14 | loss: 99.5358465CurrentTrain: epoch  0, batch    15 | loss: 86.6034674CurrentTrain: epoch  0, batch    16 | loss: 99.1272155CurrentTrain: epoch  0, batch    17 | loss: 100.0477529CurrentTrain: epoch  0, batch    18 | loss: 99.0612558CurrentTrain: epoch  0, batch    19 | loss: 86.4155510CurrentTrain: epoch  0, batch    20 | loss: 145.7000918CurrentTrain: epoch  0, batch    21 | loss: 118.1740386CurrentTrain: epoch  0, batch    22 | loss: 117.8486095CurrentTrain: epoch  0, batch    23 | loss: 117.8901877CurrentTrain: epoch  0, batch    24 | loss: 75.8985828CurrentTrain: epoch  0, batch    25 | loss: 99.3124866CurrentTrain: epoch  0, batch    26 | loss: 191.9442065CurrentTrain: epoch  0, batch    27 | loss: 98.6135683CurrentTrain: epoch  0, batch    28 | loss: 191.3414419CurrentTrain: epoch  0, batch    29 | loss: 146.0942747CurrentTrain: epoch  0, batch    30 | loss: 117.4812599CurrentTrain: epoch  0, batch    31 | loss: 84.6236549CurrentTrain: epoch  0, batch    32 | loss: 116.2965332CurrentTrain: epoch  0, batch    33 | loss: 117.5240846CurrentTrain: epoch  0, batch    34 | loss: 117.0241546CurrentTrain: epoch  0, batch    35 | loss: 145.7290695CurrentTrain: epoch  0, batch    36 | loss: 96.3368369CurrentTrain: epoch  0, batch    37 | loss: 96.5709933CurrentTrain: epoch  0, batch    38 | loss: 82.4756628CurrentTrain: epoch  0, batch    39 | loss: 98.8320480CurrentTrain: epoch  0, batch    40 | loss: 84.9355389CurrentTrain: epoch  0, batch    41 | loss: 84.1475398CurrentTrain: epoch  0, batch    42 | loss: 83.6349687CurrentTrain: epoch  0, batch    43 | loss: 114.7140044CurrentTrain: epoch  0, batch    44 | loss: 95.9233842CurrentTrain: epoch  0, batch    45 | loss: 95.4976978CurrentTrain: epoch  0, batch    46 | loss: 93.6948477CurrentTrain: epoch  0, batch    47 | loss: 92.8249687CurrentTrain: epoch  0, batch    48 | loss: 81.1700151CurrentTrain: epoch  0, batch    49 | loss: 115.6441032CurrentTrain: epoch  0, batch    50 | loss: 94.7364202CurrentTrain: epoch  0, batch    51 | loss: 81.6421631CurrentTrain: epoch  0, batch    52 | loss: 80.4102616CurrentTrain: epoch  0, batch    53 | loss: 95.8439636CurrentTrain: epoch  0, batch    54 | loss: 93.4994154CurrentTrain: epoch  0, batch    55 | loss: 78.4753501CurrentTrain: epoch  0, batch    56 | loss: 110.0516766CurrentTrain: epoch  0, batch    57 | loss: 136.6153584CurrentTrain: epoch  0, batch    58 | loss: 91.8629268CurrentTrain: epoch  0, batch    59 | loss: 93.9001453CurrentTrain: epoch  0, batch    60 | loss: 138.1322565CurrentTrain: epoch  0, batch    61 | loss: 90.5268953CurrentTrain: epoch  0, batch    62 | loss: 93.5577533CurrentTrain: epoch  0, batch    63 | loss: 91.4158594CurrentTrain: epoch  0, batch    64 | loss: 78.1143230CurrentTrain: epoch  0, batch    65 | loss: 78.4707664CurrentTrain: epoch  0, batch    66 | loss: 89.7370873CurrentTrain: epoch  0, batch    67 | loss: 76.8065615CurrentTrain: epoch  0, batch    68 | loss: 89.9017507CurrentTrain: epoch  0, batch    69 | loss: 74.7563966CurrentTrain: epoch  0, batch    70 | loss: 108.9373114CurrentTrain: epoch  0, batch    71 | loss: 78.8523997CurrentTrain: epoch  0, batch    72 | loss: 86.7915944CurrentTrain: epoch  0, batch    73 | loss: 89.1361257CurrentTrain: epoch  0, batch    74 | loss: 76.8786258CurrentTrain: epoch  0, batch    75 | loss: 74.6449341CurrentTrain: epoch  0, batch    76 | loss: 93.0356872CurrentTrain: epoch  0, batch    77 | loss: 89.0474507CurrentTrain: epoch  0, batch    78 | loss: 134.8211276CurrentTrain: epoch  0, batch    79 | loss: 75.3119639CurrentTrain: epoch  0, batch    80 | loss: 77.5649394CurrentTrain: epoch  0, batch    81 | loss: 108.2414529CurrentTrain: epoch  0, batch    82 | loss: 107.8497280CurrentTrain: epoch  0, batch    83 | loss: 102.8438794CurrentTrain: epoch  0, batch    84 | loss: 91.0680788CurrentTrain: epoch  0, batch    85 | loss: 123.7781531CurrentTrain: epoch  0, batch    86 | loss: 92.6434300CurrentTrain: epoch  0, batch    87 | loss: 88.7947402CurrentTrain: epoch  0, batch    88 | loss: 87.4807512CurrentTrain: epoch  0, batch    89 | loss: 107.9396814CurrentTrain: epoch  0, batch    90 | loss: 76.5430806CurrentTrain: epoch  0, batch    91 | loss: 104.5192774CurrentTrain: epoch  0, batch    92 | loss: 87.7033147CurrentTrain: epoch  0, batch    93 | loss: 87.5468084CurrentTrain: epoch  0, batch    94 | loss: 108.7658563CurrentTrain: epoch  0, batch    95 | loss: 83.8597439CurrentTrain: epoch  1, batch     0 | loss: 84.8902619CurrentTrain: epoch  1, batch     1 | loss: 88.3733139CurrentTrain: epoch  1, batch     2 | loss: 75.0816490CurrentTrain: epoch  1, batch     3 | loss: 106.7126838CurrentTrain: epoch  1, batch     4 | loss: 87.7189801CurrentTrain: epoch  1, batch     5 | loss: 106.9172499CurrentTrain: epoch  1, batch     6 | loss: 64.9603260CurrentTrain: epoch  1, batch     7 | loss: 84.4057124CurrentTrain: epoch  1, batch     8 | loss: 84.8282801CurrentTrain: epoch  1, batch     9 | loss: 88.1771609CurrentTrain: epoch  1, batch    10 | loss: 85.8099311CurrentTrain: epoch  1, batch    11 | loss: 87.3697858CurrentTrain: epoch  1, batch    12 | loss: 84.5057225CurrentTrain: epoch  1, batch    13 | loss: 86.7944478CurrentTrain: epoch  1, batch    14 | loss: 103.9854295CurrentTrain: epoch  1, batch    15 | loss: 73.6600137CurrentTrain: epoch  1, batch    16 | loss: 63.9070677CurrentTrain: epoch  1, batch    17 | loss: 66.0763843CurrentTrain: epoch  1, batch    18 | loss: 100.0375392CurrentTrain: epoch  1, batch    19 | loss: 86.2873315CurrentTrain: epoch  1, batch    20 | loss: 90.0341359CurrentTrain: epoch  1, batch    21 | loss: 75.2940487CurrentTrain: epoch  1, batch    22 | loss: 75.6719250CurrentTrain: epoch  1, batch    23 | loss: 83.7287940CurrentTrain: epoch  1, batch    24 | loss: 103.7850098CurrentTrain: epoch  1, batch    25 | loss: 85.7039899CurrentTrain: epoch  1, batch    26 | loss: 73.0863612CurrentTrain: epoch  1, batch    27 | loss: 86.3089996CurrentTrain: epoch  1, batch    28 | loss: 100.7391810CurrentTrain: epoch  1, batch    29 | loss: 109.1319638CurrentTrain: epoch  1, batch    30 | loss: 87.4477243CurrentTrain: epoch  1, batch    31 | loss: 73.0556738CurrentTrain: epoch  1, batch    32 | loss: 104.8985449CurrentTrain: epoch  1, batch    33 | loss: 132.4523151CurrentTrain: epoch  1, batch    34 | loss: 82.9102278CurrentTrain: epoch  1, batch    35 | loss: 85.3968938CurrentTrain: epoch  1, batch    36 | loss: 71.8695016CurrentTrain: epoch  1, batch    37 | loss: 60.5874286CurrentTrain: epoch  1, batch    38 | loss: 84.8307193CurrentTrain: epoch  1, batch    39 | loss: 99.7023234CurrentTrain: epoch  1, batch    40 | loss: 81.2095403CurrentTrain: epoch  1, batch    41 | loss: 69.7236106CurrentTrain: epoch  1, batch    42 | loss: 85.0812784CurrentTrain: epoch  1, batch    43 | loss: 69.7311791CurrentTrain: epoch  1, batch    44 | loss: 79.4449146CurrentTrain: epoch  1, batch    45 | loss: 83.0582035CurrentTrain: epoch  1, batch    46 | loss: 101.8554584CurrentTrain: epoch  1, batch    47 | loss: 128.0634932CurrentTrain: epoch  1, batch    48 | loss: 82.8077067CurrentTrain: epoch  1, batch    49 | loss: 135.4014577CurrentTrain: epoch  1, batch    50 | loss: 105.2651793CurrentTrain: epoch  1, batch    51 | loss: 71.1617297CurrentTrain: epoch  1, batch    52 | loss: 128.3924360CurrentTrain: epoch  1, batch    53 | loss: 62.3146207CurrentTrain: epoch  1, batch    54 | loss: 86.6812287CurrentTrain: epoch  1, batch    55 | loss: 86.1660914CurrentTrain: epoch  1, batch    56 | loss: 175.0816721CurrentTrain: epoch  1, batch    57 | loss: 60.3838425CurrentTrain: epoch  1, batch    58 | loss: 169.4976049CurrentTrain: epoch  1, batch    59 | loss: 104.7427820CurrentTrain: epoch  1, batch    60 | loss: 68.3764798CurrentTrain: epoch  1, batch    61 | loss: 84.7693259CurrentTrain: epoch  1, batch    62 | loss: 84.3063021CurrentTrain: epoch  1, batch    63 | loss: 71.1411098CurrentTrain: epoch  1, batch    64 | loss: 97.1885612CurrentTrain: epoch  1, batch    65 | loss: 84.3559324CurrentTrain: epoch  1, batch    66 | loss: 86.1048166CurrentTrain: epoch  1, batch    67 | loss: 85.4621040CurrentTrain: epoch  1, batch    68 | loss: 102.1448115CurrentTrain: epoch  1, batch    69 | loss: 85.5842365CurrentTrain: epoch  1, batch    70 | loss: 85.5059157CurrentTrain: epoch  1, batch    71 | loss: 70.4023009CurrentTrain: epoch  1, batch    72 | loss: 116.3069338CurrentTrain: epoch  1, batch    73 | loss: 58.5485015CurrentTrain: epoch  1, batch    74 | loss: 98.0298153CurrentTrain: epoch  1, batch    75 | loss: 73.1172056CurrentTrain: epoch  1, batch    76 | loss: 135.8955914CurrentTrain: epoch  1, batch    77 | loss: 84.4165466CurrentTrain: epoch  1, batch    78 | loss: 84.3681446CurrentTrain: epoch  1, batch    79 | loss: 87.4634990CurrentTrain: epoch  1, batch    80 | loss: 88.7284664CurrentTrain: epoch  1, batch    81 | loss: 89.6897041CurrentTrain: epoch  1, batch    82 | loss: 102.0498221CurrentTrain: epoch  1, batch    83 | loss: 90.5491754CurrentTrain: epoch  1, batch    84 | loss: 100.4712911CurrentTrain: epoch  1, batch    85 | loss: 102.6636055CurrentTrain: epoch  1, batch    86 | loss: 80.7825674CurrentTrain: epoch  1, batch    87 | loss: 80.1170684CurrentTrain: epoch  1, batch    88 | loss: 63.5540406CurrentTrain: epoch  1, batch    89 | loss: 103.9320936CurrentTrain: epoch  1, batch    90 | loss: 70.8894486CurrentTrain: epoch  1, batch    91 | loss: 127.7611679CurrentTrain: epoch  1, batch    92 | loss: 71.6327726CurrentTrain: epoch  1, batch    93 | loss: 101.4793664CurrentTrain: epoch  1, batch    94 | loss: 84.6083894CurrentTrain: epoch  1, batch    95 | loss: 142.2041494CurrentTrain: epoch  2, batch     0 | loss: 71.8137401CurrentTrain: epoch  2, batch     1 | loss: 82.0236254CurrentTrain: epoch  2, batch     2 | loss: 79.7695240CurrentTrain: epoch  2, batch     3 | loss: 99.8728045CurrentTrain: epoch  2, batch     4 | loss: 85.5959975CurrentTrain: epoch  2, batch     5 | loss: 63.4588290CurrentTrain: epoch  2, batch     6 | loss: 61.0456563CurrentTrain: epoch  2, batch     7 | loss: 68.7186458CurrentTrain: epoch  2, batch     8 | loss: 100.2021389CurrentTrain: epoch  2, batch     9 | loss: 128.5771433CurrentTrain: epoch  2, batch    10 | loss: 70.2408226CurrentTrain: epoch  2, batch    11 | loss: 172.0603783CurrentTrain: epoch  2, batch    12 | loss: 102.2360579CurrentTrain: epoch  2, batch    13 | loss: 64.3751812CurrentTrain: epoch  2, batch    14 | loss: 123.7563186CurrentTrain: epoch  2, batch    15 | loss: 79.1741111CurrentTrain: epoch  2, batch    16 | loss: 80.4189979CurrentTrain: epoch  2, batch    17 | loss: 78.1185946CurrentTrain: epoch  2, batch    18 | loss: 68.6950999CurrentTrain: epoch  2, batch    19 | loss: 96.6454407CurrentTrain: epoch  2, batch    20 | loss: 69.9571085CurrentTrain: epoch  2, batch    21 | loss: 101.2205477CurrentTrain: epoch  2, batch    22 | loss: 60.9238198CurrentTrain: epoch  2, batch    23 | loss: 69.5104271CurrentTrain: epoch  2, batch    24 | loss: 69.2679150CurrentTrain: epoch  2, batch    25 | loss: 124.9497018CurrentTrain: epoch  2, batch    26 | loss: 105.4051094CurrentTrain: epoch  2, batch    27 | loss: 127.5224952CurrentTrain: epoch  2, batch    28 | loss: 83.0039993CurrentTrain: epoch  2, batch    29 | loss: 101.8796934CurrentTrain: epoch  2, batch    30 | loss: 86.1080052CurrentTrain: epoch  2, batch    31 | loss: 98.0114648CurrentTrain: epoch  2, batch    32 | loss: 81.1716728CurrentTrain: epoch  2, batch    33 | loss: 66.6624011CurrentTrain: epoch  2, batch    34 | loss: 84.3625662CurrentTrain: epoch  2, batch    35 | loss: 84.7987724CurrentTrain: epoch  2, batch    36 | loss: 65.9871785CurrentTrain: epoch  2, batch    37 | loss: 98.7822414CurrentTrain: epoch  2, batch    38 | loss: 79.4425711CurrentTrain: epoch  2, batch    39 | loss: 71.5237582CurrentTrain: epoch  2, batch    40 | loss: 85.8850700CurrentTrain: epoch  2, batch    41 | loss: 70.5296327CurrentTrain: epoch  2, batch    42 | loss: 98.1224237CurrentTrain: epoch  2, batch    43 | loss: 122.0601860CurrentTrain: epoch  2, batch    44 | loss: 69.1956537CurrentTrain: epoch  2, batch    45 | loss: 80.0059750CurrentTrain: epoch  2, batch    46 | loss: 60.5618352CurrentTrain: epoch  2, batch    47 | loss: 96.2942508CurrentTrain: epoch  2, batch    48 | loss: 81.6152546CurrentTrain: epoch  2, batch    49 | loss: 82.8515179CurrentTrain: epoch  2, batch    50 | loss: 82.3182386CurrentTrain: epoch  2, batch    51 | loss: 100.2657120CurrentTrain: epoch  2, batch    52 | loss: 70.9671545CurrentTrain: epoch  2, batch    53 | loss: 122.6526690CurrentTrain: epoch  2, batch    54 | loss: 57.0842496CurrentTrain: epoch  2, batch    55 | loss: 66.3720591CurrentTrain: epoch  2, batch    56 | loss: 95.0134297CurrentTrain: epoch  2, batch    57 | loss: 82.7842054CurrentTrain: epoch  2, batch    58 | loss: 102.7522164CurrentTrain: epoch  2, batch    59 | loss: 71.3070777CurrentTrain: epoch  2, batch    60 | loss: 125.3740518CurrentTrain: epoch  2, batch    61 | loss: 78.2873457CurrentTrain: epoch  2, batch    62 | loss: 77.2550803CurrentTrain: epoch  2, batch    63 | loss: 67.5277915CurrentTrain: epoch  2, batch    64 | loss: 68.2867447CurrentTrain: epoch  2, batch    65 | loss: 79.5880904CurrentTrain: epoch  2, batch    66 | loss: 68.6405396CurrentTrain: epoch  2, batch    67 | loss: 77.3097275CurrentTrain: epoch  2, batch    68 | loss: 81.9728570CurrentTrain: epoch  2, batch    69 | loss: 98.2300417CurrentTrain: epoch  2, batch    70 | loss: 99.9132619CurrentTrain: epoch  2, batch    71 | loss: 62.6352367CurrentTrain: epoch  2, batch    72 | loss: 67.9670710CurrentTrain: epoch  2, batch    73 | loss: 80.2779662CurrentTrain: epoch  2, batch    74 | loss: 81.3373917CurrentTrain: epoch  2, batch    75 | loss: 69.7428799CurrentTrain: epoch  2, batch    76 | loss: 84.3271490CurrentTrain: epoch  2, batch    77 | loss: 119.8526986CurrentTrain: epoch  2, batch    78 | loss: 96.9631574CurrentTrain: epoch  2, batch    79 | loss: 97.8450436CurrentTrain: epoch  2, batch    80 | loss: 60.4753476CurrentTrain: epoch  2, batch    81 | loss: 84.2259029CurrentTrain: epoch  2, batch    82 | loss: 63.6042095CurrentTrain: epoch  2, batch    83 | loss: 79.5547014CurrentTrain: epoch  2, batch    84 | loss: 72.5902744CurrentTrain: epoch  2, batch    85 | loss: 91.5005058CurrentTrain: epoch  2, batch    86 | loss: 98.0066385CurrentTrain: epoch  2, batch    87 | loss: 98.5154092CurrentTrain: epoch  2, batch    88 | loss: 129.5473004CurrentTrain: epoch  2, batch    89 | loss: 69.7095236CurrentTrain: epoch  2, batch    90 | loss: 84.2768770CurrentTrain: epoch  2, batch    91 | loss: 78.5341086CurrentTrain: epoch  2, batch    92 | loss: 81.0127094CurrentTrain: epoch  2, batch    93 | loss: 82.7879751CurrentTrain: epoch  2, batch    94 | loss: 68.5700277CurrentTrain: epoch  2, batch    95 | loss: 81.9853360CurrentTrain: epoch  3, batch     0 | loss: 78.4777362CurrentTrain: epoch  3, batch     1 | loss: 80.6562531CurrentTrain: epoch  3, batch     2 | loss: 93.1176020CurrentTrain: epoch  3, batch     3 | loss: 82.3679006CurrentTrain: epoch  3, batch     4 | loss: 101.7293073CurrentTrain: epoch  3, batch     5 | loss: 57.6671747CurrentTrain: epoch  3, batch     6 | loss: 67.2137428CurrentTrain: epoch  3, batch     7 | loss: 68.4953755CurrentTrain: epoch  3, batch     8 | loss: 117.1888194CurrentTrain: epoch  3, batch     9 | loss: 94.4586287CurrentTrain: epoch  3, batch    10 | loss: 100.9639062CurrentTrain: epoch  3, batch    11 | loss: 97.5660188CurrentTrain: epoch  3, batch    12 | loss: 68.4947040CurrentTrain: epoch  3, batch    13 | loss: 95.0507238CurrentTrain: epoch  3, batch    14 | loss: 97.0117446CurrentTrain: epoch  3, batch    15 | loss: 63.7991062CurrentTrain: epoch  3, batch    16 | loss: 74.3489896CurrentTrain: epoch  3, batch    17 | loss: 76.7071152CurrentTrain: epoch  3, batch    18 | loss: 94.5453483CurrentTrain: epoch  3, batch    19 | loss: 97.9897124CurrentTrain: epoch  3, batch    20 | loss: 68.4547125CurrentTrain: epoch  3, batch    21 | loss: 62.2249996CurrentTrain: epoch  3, batch    22 | loss: 62.4950440CurrentTrain: epoch  3, batch    23 | loss: 117.8167094CurrentTrain: epoch  3, batch    24 | loss: 83.2181108CurrentTrain: epoch  3, batch    25 | loss: 68.5223230CurrentTrain: epoch  3, batch    26 | loss: 96.4961695CurrentTrain: epoch  3, batch    27 | loss: 98.9050930CurrentTrain: epoch  3, batch    28 | loss: 57.1373038CurrentTrain: epoch  3, batch    29 | loss: 65.5723248CurrentTrain: epoch  3, batch    30 | loss: 80.2173675CurrentTrain: epoch  3, batch    31 | loss: 92.4195953CurrentTrain: epoch  3, batch    32 | loss: 95.7651308CurrentTrain: epoch  3, batch    33 | loss: 65.9086916CurrentTrain: epoch  3, batch    34 | loss: 79.3854269CurrentTrain: epoch  3, batch    35 | loss: 52.8113669CurrentTrain: epoch  3, batch    36 | loss: 67.3478728CurrentTrain: epoch  3, batch    37 | loss: 130.6860690CurrentTrain: epoch  3, batch    38 | loss: 76.8176233CurrentTrain: epoch  3, batch    39 | loss: 70.7114815CurrentTrain: epoch  3, batch    40 | loss: 82.2443989CurrentTrain: epoch  3, batch    41 | loss: 81.6843939CurrentTrain: epoch  3, batch    42 | loss: 59.9882346CurrentTrain: epoch  3, batch    43 | loss: 82.8999674CurrentTrain: epoch  3, batch    44 | loss: 92.7320054CurrentTrain: epoch  3, batch    45 | loss: 98.2519761CurrentTrain: epoch  3, batch    46 | loss: 92.9337302CurrentTrain: epoch  3, batch    47 | loss: 57.5034587CurrentTrain: epoch  3, batch    48 | loss: 68.5552889CurrentTrain: epoch  3, batch    49 | loss: 68.7528056CurrentTrain: epoch  3, batch    50 | loss: 63.9567140CurrentTrain: epoch  3, batch    51 | loss: 92.8165784CurrentTrain: epoch  3, batch    52 | loss: 65.8017630CurrentTrain: epoch  3, batch    53 | loss: 81.3189640CurrentTrain: epoch  3, batch    54 | loss: 76.5047830CurrentTrain: epoch  3, batch    55 | loss: 97.4066555CurrentTrain: epoch  3, batch    56 | loss: 78.2692061CurrentTrain: epoch  3, batch    57 | loss: 124.5824933CurrentTrain: epoch  3, batch    58 | loss: 121.9612950CurrentTrain: epoch  3, batch    59 | loss: 121.9164440CurrentTrain: epoch  3, batch    60 | loss: 77.7388624CurrentTrain: epoch  3, batch    61 | loss: 76.7247470CurrentTrain: epoch  3, batch    62 | loss: 96.8792192CurrentTrain: epoch  3, batch    63 | loss: 125.8039856CurrentTrain: epoch  3, batch    64 | loss: 57.2363991CurrentTrain: epoch  3, batch    65 | loss: 69.5465698CurrentTrain: epoch  3, batch    66 | loss: 120.5247112CurrentTrain: epoch  3, batch    67 | loss: 83.2976591CurrentTrain: epoch  3, batch    68 | loss: 54.2412938CurrentTrain: epoch  3, batch    69 | loss: 96.9930506CurrentTrain: epoch  3, batch    70 | loss: 81.7577471CurrentTrain: epoch  3, batch    71 | loss: 65.5955208CurrentTrain: epoch  3, batch    72 | loss: 67.6639332CurrentTrain: epoch  3, batch    73 | loss: 78.4054070CurrentTrain: epoch  3, batch    74 | loss: 83.3412046CurrentTrain: epoch  3, batch    75 | loss: 67.3422609CurrentTrain: epoch  3, batch    76 | loss: 69.7826502CurrentTrain: epoch  3, batch    77 | loss: 67.4400072CurrentTrain: epoch  3, batch    78 | loss: 66.3055243CurrentTrain: epoch  3, batch    79 | loss: 104.1538736CurrentTrain: epoch  3, batch    80 | loss: 84.1628592CurrentTrain: epoch  3, batch    81 | loss: 74.3377801CurrentTrain: epoch  3, batch    82 | loss: 99.3268646CurrentTrain: epoch  3, batch    83 | loss: 95.8691953CurrentTrain: epoch  3, batch    84 | loss: 96.2552445CurrentTrain: epoch  3, batch    85 | loss: 97.5213935CurrentTrain: epoch  3, batch    86 | loss: 94.2479418CurrentTrain: epoch  3, batch    87 | loss: 97.9750029CurrentTrain: epoch  3, batch    88 | loss: 93.7343114CurrentTrain: epoch  3, batch    89 | loss: 56.7591126CurrentTrain: epoch  3, batch    90 | loss: 129.7323680CurrentTrain: epoch  3, batch    91 | loss: 95.2230705CurrentTrain: epoch  3, batch    92 | loss: 84.0019199CurrentTrain: epoch  3, batch    93 | loss: 79.4394165CurrentTrain: epoch  3, batch    94 | loss: 82.7047241CurrentTrain: epoch  3, batch    95 | loss: 66.2036674CurrentTrain: epoch  4, batch     0 | loss: 76.5017049CurrentTrain: epoch  4, batch     1 | loss: 65.5647371CurrentTrain: epoch  4, batch     2 | loss: 63.6033171CurrentTrain: epoch  4, batch     3 | loss: 118.6586637CurrentTrain: epoch  4, batch     4 | loss: 58.6100631CurrentTrain: epoch  4, batch     5 | loss: 68.0195409CurrentTrain: epoch  4, batch     6 | loss: 77.3809882CurrentTrain: epoch  4, batch     7 | loss: 80.2174428CurrentTrain: epoch  4, batch     8 | loss: 116.6008095CurrentTrain: epoch  4, batch     9 | loss: 65.0015241CurrentTrain: epoch  4, batch    10 | loss: 160.9800136CurrentTrain: epoch  4, batch    11 | loss: 120.6458833CurrentTrain: epoch  4, batch    12 | loss: 92.7414469CurrentTrain: epoch  4, batch    13 | loss: 96.9284638CurrentTrain: epoch  4, batch    14 | loss: 78.9739672CurrentTrain: epoch  4, batch    15 | loss: 60.5202062CurrentTrain: epoch  4, batch    16 | loss: 93.9487841CurrentTrain: epoch  4, batch    17 | loss: 65.7728157CurrentTrain: epoch  4, batch    18 | loss: 79.0367501CurrentTrain: epoch  4, batch    19 | loss: 62.5923700CurrentTrain: epoch  4, batch    20 | loss: 79.3242170CurrentTrain: epoch  4, batch    21 | loss: 75.7357514CurrentTrain: epoch  4, batch    22 | loss: 66.1630659CurrentTrain: epoch  4, batch    23 | loss: 94.4567147CurrentTrain: epoch  4, batch    24 | loss: 76.5326755CurrentTrain: epoch  4, batch    25 | loss: 95.5876912CurrentTrain: epoch  4, batch    26 | loss: 97.1243342CurrentTrain: epoch  4, batch    27 | loss: 63.4177420CurrentTrain: epoch  4, batch    28 | loss: 73.3380561CurrentTrain: epoch  4, batch    29 | loss: 91.7532019CurrentTrain: epoch  4, batch    30 | loss: 79.5570027CurrentTrain: epoch  4, batch    31 | loss: 75.6464839CurrentTrain: epoch  4, batch    32 | loss: 76.0209050CurrentTrain: epoch  4, batch    33 | loss: 93.2063094CurrentTrain: epoch  4, batch    34 | loss: 64.1894481CurrentTrain: epoch  4, batch    35 | loss: 122.3465642CurrentTrain: epoch  4, batch    36 | loss: 115.9434107CurrentTrain: epoch  4, batch    37 | loss: 99.7056049CurrentTrain: epoch  4, batch    38 | loss: 78.8187620CurrentTrain: epoch  4, batch    39 | loss: 98.8147367CurrentTrain: epoch  4, batch    40 | loss: 67.6963874CurrentTrain: epoch  4, batch    41 | loss: 64.1846594CurrentTrain: epoch  4, batch    42 | loss: 66.5967264CurrentTrain: epoch  4, batch    43 | loss: 79.9025856CurrentTrain: epoch  4, batch    44 | loss: 79.5386607CurrentTrain: epoch  4, batch    45 | loss: 54.6057316CurrentTrain: epoch  4, batch    46 | loss: 118.5939622CurrentTrain: epoch  4, batch    47 | loss: 76.9704813CurrentTrain: epoch  4, batch    48 | loss: 92.1099745CurrentTrain: epoch  4, batch    49 | loss: 77.4360991CurrentTrain: epoch  4, batch    50 | loss: 79.2475000CurrentTrain: epoch  4, batch    51 | loss: 96.1040301CurrentTrain: epoch  4, batch    52 | loss: 62.6643695CurrentTrain: epoch  4, batch    53 | loss: 79.7915183CurrentTrain: epoch  4, batch    54 | loss: 53.7199137CurrentTrain: epoch  4, batch    55 | loss: 61.5853080CurrentTrain: epoch  4, batch    56 | loss: 120.7976612CurrentTrain: epoch  4, batch    57 | loss: 88.6126764CurrentTrain: epoch  4, batch    58 | loss: 53.9717691CurrentTrain: epoch  4, batch    59 | loss: 70.0002900CurrentTrain: epoch  4, batch    60 | loss: 97.0951203CurrentTrain: epoch  4, batch    61 | loss: 65.2854546CurrentTrain: epoch  4, batch    62 | loss: 80.6556520CurrentTrain: epoch  4, batch    63 | loss: 91.5569902CurrentTrain: epoch  4, batch    64 | loss: 96.0333218CurrentTrain: epoch  4, batch    65 | loss: 91.6735765CurrentTrain: epoch  4, batch    66 | loss: 77.1748517CurrentTrain: epoch  4, batch    67 | loss: 74.3398972CurrentTrain: epoch  4, batch    68 | loss: 81.7164211CurrentTrain: epoch  4, batch    69 | loss: 83.7355808CurrentTrain: epoch  4, batch    70 | loss: 77.1612519CurrentTrain: epoch  4, batch    71 | loss: 66.4389047CurrentTrain: epoch  4, batch    72 | loss: 79.0302512CurrentTrain: epoch  4, batch    73 | loss: 65.2717822CurrentTrain: epoch  4, batch    74 | loss: 79.8588311CurrentTrain: epoch  4, batch    75 | loss: 57.4741087CurrentTrain: epoch  4, batch    76 | loss: 77.5529836CurrentTrain: epoch  4, batch    77 | loss: 97.2930617CurrentTrain: epoch  4, batch    78 | loss: 65.5178011CurrentTrain: epoch  4, batch    79 | loss: 92.5282997CurrentTrain: epoch  4, batch    80 | loss: 79.0172583CurrentTrain: epoch  4, batch    81 | loss: 83.2234142CurrentTrain: epoch  4, batch    82 | loss: 94.8409570CurrentTrain: epoch  4, batch    83 | loss: 76.2257778CurrentTrain: epoch  4, batch    84 | loss: 76.2268879CurrentTrain: epoch  4, batch    85 | loss: 66.3398138CurrentTrain: epoch  4, batch    86 | loss: 156.0555829CurrentTrain: epoch  4, batch    87 | loss: 69.2069550CurrentTrain: epoch  4, batch    88 | loss: 125.0689523CurrentTrain: epoch  4, batch    89 | loss: 67.5225642CurrentTrain: epoch  4, batch    90 | loss: 72.3009824CurrentTrain: epoch  4, batch    91 | loss: 66.7566693CurrentTrain: epoch  4, batch    92 | loss: 74.6079271CurrentTrain: epoch  4, batch    93 | loss: 126.0992901CurrentTrain: epoch  4, batch    94 | loss: 64.9263526CurrentTrain: epoch  4, batch    95 | loss: 65.2811625CurrentTrain: epoch  5, batch     0 | loss: 97.9469121CurrentTrain: epoch  5, batch     1 | loss: 56.4701152CurrentTrain: epoch  5, batch     2 | loss: 62.4802846CurrentTrain: epoch  5, batch     3 | loss: 77.2333352CurrentTrain: epoch  5, batch     4 | loss: 64.1378746CurrentTrain: epoch  5, batch     5 | loss: 96.9291064CurrentTrain: epoch  5, batch     6 | loss: 76.7881869CurrentTrain: epoch  5, batch     7 | loss: 75.5045981CurrentTrain: epoch  5, batch     8 | loss: 55.1461169CurrentTrain: epoch  5, batch     9 | loss: 89.5300294CurrentTrain: epoch  5, batch    10 | loss: 78.0643262CurrentTrain: epoch  5, batch    11 | loss: 93.8982273CurrentTrain: epoch  5, batch    12 | loss: 91.1673206CurrentTrain: epoch  5, batch    13 | loss: 77.5602657CurrentTrain: epoch  5, batch    14 | loss: 63.9691288CurrentTrain: epoch  5, batch    15 | loss: 96.3384036CurrentTrain: epoch  5, batch    16 | loss: 76.1248009CurrentTrain: epoch  5, batch    17 | loss: 74.5580769CurrentTrain: epoch  5, batch    18 | loss: 154.7153045CurrentTrain: epoch  5, batch    19 | loss: 74.7282120CurrentTrain: epoch  5, batch    20 | loss: 77.4383434CurrentTrain: epoch  5, batch    21 | loss: 89.7654028CurrentTrain: epoch  5, batch    22 | loss: 76.9788921CurrentTrain: epoch  5, batch    23 | loss: 72.0119134CurrentTrain: epoch  5, batch    24 | loss: 54.4065162CurrentTrain: epoch  5, batch    25 | loss: 92.6802902CurrentTrain: epoch  5, batch    26 | loss: 80.5516602CurrentTrain: epoch  5, batch    27 | loss: 56.1801546CurrentTrain: epoch  5, batch    28 | loss: 61.0226423CurrentTrain: epoch  5, batch    29 | loss: 92.3922456CurrentTrain: epoch  5, batch    30 | loss: 72.6134528CurrentTrain: epoch  5, batch    31 | loss: 61.7022820CurrentTrain: epoch  5, batch    32 | loss: 88.3637750CurrentTrain: epoch  5, batch    33 | loss: 78.3183431CurrentTrain: epoch  5, batch    34 | loss: 65.0466081CurrentTrain: epoch  5, batch    35 | loss: 58.1084464CurrentTrain: epoch  5, batch    36 | loss: 63.0659190CurrentTrain: epoch  5, batch    37 | loss: 76.6076359CurrentTrain: epoch  5, batch    38 | loss: 59.1932235CurrentTrain: epoch  5, batch    39 | loss: 62.5984245CurrentTrain: epoch  5, batch    40 | loss: 64.2855792CurrentTrain: epoch  5, batch    41 | loss: 74.8422773CurrentTrain: epoch  5, batch    42 | loss: 94.5879222CurrentTrain: epoch  5, batch    43 | loss: 98.3446528CurrentTrain: epoch  5, batch    44 | loss: 112.7851457CurrentTrain: epoch  5, batch    45 | loss: 66.8761262CurrentTrain: epoch  5, batch    46 | loss: 91.8154145CurrentTrain: epoch  5, batch    47 | loss: 55.7952627CurrentTrain: epoch  5, batch    48 | loss: 78.7304867CurrentTrain: epoch  5, batch    49 | loss: 93.9731724CurrentTrain: epoch  5, batch    50 | loss: 75.5646338CurrentTrain: epoch  5, batch    51 | loss: 64.3655317CurrentTrain: epoch  5, batch    52 | loss: 94.8758787CurrentTrain: epoch  5, batch    53 | loss: 59.0949808CurrentTrain: epoch  5, batch    54 | loss: 118.8223168CurrentTrain: epoch  5, batch    55 | loss: 91.8894059CurrentTrain: epoch  5, batch    56 | loss: 66.6737121CurrentTrain: epoch  5, batch    57 | loss: 78.1452288CurrentTrain: epoch  5, batch    58 | loss: 92.5872119CurrentTrain: epoch  5, batch    59 | loss: 66.6692279CurrentTrain: epoch  5, batch    60 | loss: 79.3420686CurrentTrain: epoch  5, batch    61 | loss: 74.2468387CurrentTrain: epoch  5, batch    62 | loss: 64.8745717CurrentTrain: epoch  5, batch    63 | loss: 91.5174736CurrentTrain: epoch  5, batch    64 | loss: 75.0881340CurrentTrain: epoch  5, batch    65 | loss: 77.2760042CurrentTrain: epoch  5, batch    66 | loss: 60.8990627CurrentTrain: epoch  5, batch    67 | loss: 87.3065648CurrentTrain: epoch  5, batch    68 | loss: 91.3229683CurrentTrain: epoch  5, batch    69 | loss: 58.6262333CurrentTrain: epoch  5, batch    70 | loss: 92.8678436CurrentTrain: epoch  5, batch    71 | loss: 56.3221957CurrentTrain: epoch  5, batch    72 | loss: 80.4487344CurrentTrain: epoch  5, batch    73 | loss: 52.7692861CurrentTrain: epoch  5, batch    74 | loss: 76.4678084CurrentTrain: epoch  5, batch    75 | loss: 60.5920850CurrentTrain: epoch  5, batch    76 | loss: 118.0200930CurrentTrain: epoch  5, batch    77 | loss: 76.5076283CurrentTrain: epoch  5, batch    78 | loss: 98.8675159CurrentTrain: epoch  5, batch    79 | loss: 75.9368019CurrentTrain: epoch  5, batch    80 | loss: 95.0302906CurrentTrain: epoch  5, batch    81 | loss: 93.7929660CurrentTrain: epoch  5, batch    82 | loss: 79.5106906CurrentTrain: epoch  5, batch    83 | loss: 162.7061079CurrentTrain: epoch  5, batch    84 | loss: 55.4205805CurrentTrain: epoch  5, batch    85 | loss: 61.8515504CurrentTrain: epoch  5, batch    86 | loss: 166.8152635CurrentTrain: epoch  5, batch    87 | loss: 81.5950072CurrentTrain: epoch  5, batch    88 | loss: 114.1166690CurrentTrain: epoch  5, batch    89 | loss: 63.0608114CurrentTrain: epoch  5, batch    90 | loss: 67.4513444CurrentTrain: epoch  5, batch    91 | loss: 123.3949292CurrentTrain: epoch  5, batch    92 | loss: 62.3779442CurrentTrain: epoch  5, batch    93 | loss: 93.8566703CurrentTrain: epoch  5, batch    94 | loss: 89.8043960CurrentTrain: epoch  5, batch    95 | loss: 105.3025565CurrentTrain: epoch  6, batch     0 | loss: 65.9448555CurrentTrain: epoch  6, batch     1 | loss: 92.3904141CurrentTrain: epoch  6, batch     2 | loss: 73.2060453CurrentTrain: epoch  6, batch     3 | loss: 65.1726162CurrentTrain: epoch  6, batch     4 | loss: 76.7584894CurrentTrain: epoch  6, batch     5 | loss: 88.2306522CurrentTrain: epoch  6, batch     6 | loss: 91.8137531CurrentTrain: epoch  6, batch     7 | loss: 115.0482103CurrentTrain: epoch  6, batch     8 | loss: 60.2148273CurrentTrain: epoch  6, batch     9 | loss: 49.5821040CurrentTrain: epoch  6, batch    10 | loss: 87.4492870CurrentTrain: epoch  6, batch    11 | loss: 75.2198557CurrentTrain: epoch  6, batch    12 | loss: 89.5234294CurrentTrain: epoch  6, batch    13 | loss: 89.6402903CurrentTrain: epoch  6, batch    14 | loss: 54.8194642CurrentTrain: epoch  6, batch    15 | loss: 74.0398509CurrentTrain: epoch  6, batch    16 | loss: 52.6654459CurrentTrain: epoch  6, batch    17 | loss: 64.5902211CurrentTrain: epoch  6, batch    18 | loss: 89.7534648CurrentTrain: epoch  6, batch    19 | loss: 73.9048264CurrentTrain: epoch  6, batch    20 | loss: 53.5169064CurrentTrain: epoch  6, batch    21 | loss: 71.2326067CurrentTrain: epoch  6, batch    22 | loss: 76.4908232CurrentTrain: epoch  6, batch    23 | loss: 90.5537912CurrentTrain: epoch  6, batch    24 | loss: 91.2803887CurrentTrain: epoch  6, batch    25 | loss: 89.5708453CurrentTrain: epoch  6, batch    26 | loss: 72.0554494CurrentTrain: epoch  6, batch    27 | loss: 89.1602523CurrentTrain: epoch  6, batch    28 | loss: 75.5934413CurrentTrain: epoch  6, batch    29 | loss: 65.2177349CurrentTrain: epoch  6, batch    30 | loss: 91.3461545CurrentTrain: epoch  6, batch    31 | loss: 67.1632657CurrentTrain: epoch  6, batch    32 | loss: 54.1374756CurrentTrain: epoch  6, batch    33 | loss: 59.7794092CurrentTrain: epoch  6, batch    34 | loss: 90.5694476CurrentTrain: epoch  6, batch    35 | loss: 60.0905816CurrentTrain: epoch  6, batch    36 | loss: 78.9383100CurrentTrain: epoch  6, batch    37 | loss: 55.7641424CurrentTrain: epoch  6, batch    38 | loss: 90.1153390CurrentTrain: epoch  6, batch    39 | loss: 76.6635915CurrentTrain: epoch  6, batch    40 | loss: 74.9469388CurrentTrain: epoch  6, batch    41 | loss: 124.5812764CurrentTrain: epoch  6, batch    42 | loss: 114.5006669CurrentTrain: epoch  6, batch    43 | loss: 57.4947265CurrentTrain: epoch  6, batch    44 | loss: 78.2208423CurrentTrain: epoch  6, batch    45 | loss: 91.7274314CurrentTrain: epoch  6, batch    46 | loss: 76.4787304CurrentTrain: epoch  6, batch    47 | loss: 94.3364016CurrentTrain: epoch  6, batch    48 | loss: 76.3891840CurrentTrain: epoch  6, batch    49 | loss: 89.3225825CurrentTrain: epoch  6, batch    50 | loss: 115.2757519CurrentTrain: epoch  6, batch    51 | loss: 73.1684856CurrentTrain: epoch  6, batch    52 | loss: 99.2366652CurrentTrain: epoch  6, batch    53 | loss: 90.8371931CurrentTrain: epoch  6, batch    54 | loss: 92.6568085CurrentTrain: epoch  6, batch    55 | loss: 91.5293201CurrentTrain: epoch  6, batch    56 | loss: 94.5596096CurrentTrain: epoch  6, batch    57 | loss: 156.9739003CurrentTrain: epoch  6, batch    58 | loss: 75.2233659CurrentTrain: epoch  6, batch    59 | loss: 66.5891755CurrentTrain: epoch  6, batch    60 | loss: 114.7274484CurrentTrain: epoch  6, batch    61 | loss: 74.2220856CurrentTrain: epoch  6, batch    62 | loss: 91.4497558CurrentTrain: epoch  6, batch    63 | loss: 62.7396177CurrentTrain: epoch  6, batch    64 | loss: 73.3489981CurrentTrain: epoch  6, batch    65 | loss: 64.8426809CurrentTrain: epoch  6, batch    66 | loss: 62.4893677CurrentTrain: epoch  6, batch    67 | loss: 65.1902069CurrentTrain: epoch  6, batch    68 | loss: 77.2500966CurrentTrain: epoch  6, batch    69 | loss: 71.1009449CurrentTrain: epoch  6, batch    70 | loss: 61.2037146CurrentTrain: epoch  6, batch    71 | loss: 58.3902102CurrentTrain: epoch  6, batch    72 | loss: 100.9489061CurrentTrain: epoch  6, batch    73 | loss: 94.5787684CurrentTrain: epoch  6, batch    74 | loss: 56.1424154CurrentTrain: epoch  6, batch    75 | loss: 91.2762369CurrentTrain: epoch  6, batch    76 | loss: 74.0068865CurrentTrain: epoch  6, batch    77 | loss: 73.2168098CurrentTrain: epoch  6, batch    78 | loss: 72.4487682CurrentTrain: epoch  6, batch    79 | loss: 65.7954469CurrentTrain: epoch  6, batch    80 | loss: 75.0890709CurrentTrain: epoch  6, batch    81 | loss: 91.3934515CurrentTrain: epoch  6, batch    82 | loss: 92.1307160CurrentTrain: epoch  6, batch    83 | loss: 50.1418460CurrentTrain: epoch  6, batch    84 | loss: 62.3869802CurrentTrain: epoch  6, batch    85 | loss: 89.2961439CurrentTrain: epoch  6, batch    86 | loss: 60.9834338CurrentTrain: epoch  6, batch    87 | loss: 61.4781432CurrentTrain: epoch  6, batch    88 | loss: 75.4481307CurrentTrain: epoch  6, batch    89 | loss: 71.2201295CurrentTrain: epoch  6, batch    90 | loss: 88.4805434CurrentTrain: epoch  6, batch    91 | loss: 74.7038955CurrentTrain: epoch  6, batch    92 | loss: 62.6049297CurrentTrain: epoch  6, batch    93 | loss: 91.5862198CurrentTrain: epoch  6, batch    94 | loss: 91.9570590CurrentTrain: epoch  6, batch    95 | loss: 76.4640177CurrentTrain: epoch  7, batch     0 | loss: 72.6578779CurrentTrain: epoch  7, batch     1 | loss: 63.6098546CurrentTrain: epoch  7, batch     2 | loss: 113.2213296CurrentTrain: epoch  7, batch     3 | loss: 75.8139778CurrentTrain: epoch  7, batch     4 | loss: 75.6000868CurrentTrain: epoch  7, batch     5 | loss: 70.9136029CurrentTrain: epoch  7, batch     6 | loss: 71.5293439CurrentTrain: epoch  7, batch     7 | loss: 61.0125142CurrentTrain: epoch  7, batch     8 | loss: 62.6258888CurrentTrain: epoch  7, batch     9 | loss: 72.6693897CurrentTrain: epoch  7, batch    10 | loss: 115.2789333CurrentTrain: epoch  7, batch    11 | loss: 77.5794300CurrentTrain: epoch  7, batch    12 | loss: 90.7197199CurrentTrain: epoch  7, batch    13 | loss: 91.0649630CurrentTrain: epoch  7, batch    14 | loss: 52.0186503CurrentTrain: epoch  7, batch    15 | loss: 62.8530637CurrentTrain: epoch  7, batch    16 | loss: 115.3240857CurrentTrain: epoch  7, batch    17 | loss: 76.3077812CurrentTrain: epoch  7, batch    18 | loss: 89.2600935CurrentTrain: epoch  7, batch    19 | loss: 70.3294969CurrentTrain: epoch  7, batch    20 | loss: 88.0306237CurrentTrain: epoch  7, batch    21 | loss: 71.6958026CurrentTrain: epoch  7, batch    22 | loss: 73.2334406CurrentTrain: epoch  7, batch    23 | loss: 75.4184331CurrentTrain: epoch  7, batch    24 | loss: 74.5906777CurrentTrain: epoch  7, batch    25 | loss: 59.9470783CurrentTrain: epoch  7, batch    26 | loss: 71.8910316CurrentTrain: epoch  7, batch    27 | loss: 73.3117334CurrentTrain: epoch  7, batch    28 | loss: 72.0112134CurrentTrain: epoch  7, batch    29 | loss: 54.8351520CurrentTrain: epoch  7, batch    30 | loss: 72.7049871CurrentTrain: epoch  7, batch    31 | loss: 113.9021714CurrentTrain: epoch  7, batch    32 | loss: 75.4793675CurrentTrain: epoch  7, batch    33 | loss: 92.2443591CurrentTrain: epoch  7, batch    34 | loss: 82.8202107CurrentTrain: epoch  7, batch    35 | loss: 87.4126823CurrentTrain: epoch  7, batch    36 | loss: 60.0232819CurrentTrain: epoch  7, batch    37 | loss: 72.3295617CurrentTrain: epoch  7, batch    38 | loss: 86.8148895CurrentTrain: epoch  7, batch    39 | loss: 75.9762613CurrentTrain: epoch  7, batch    40 | loss: 111.6017218CurrentTrain: epoch  7, batch    41 | loss: 91.6015383CurrentTrain: epoch  7, batch    42 | loss: 92.1265576CurrentTrain: epoch  7, batch    43 | loss: 74.1049175CurrentTrain: epoch  7, batch    44 | loss: 65.2655297CurrentTrain: epoch  7, batch    45 | loss: 69.1397209CurrentTrain: epoch  7, batch    46 | loss: 52.3463746CurrentTrain: epoch  7, batch    47 | loss: 63.5603329CurrentTrain: epoch  7, batch    48 | loss: 62.0083092CurrentTrain: epoch  7, batch    49 | loss: 63.4950368CurrentTrain: epoch  7, batch    50 | loss: 73.3614320CurrentTrain: epoch  7, batch    51 | loss: 89.0755048CurrentTrain: epoch  7, batch    52 | loss: 60.5052389CurrentTrain: epoch  7, batch    53 | loss: 71.1547819CurrentTrain: epoch  7, batch    54 | loss: 92.6335073CurrentTrain: epoch  7, batch    55 | loss: 61.6081562CurrentTrain: epoch  7, batch    56 | loss: 62.7192546CurrentTrain: epoch  7, batch    57 | loss: 59.9356639CurrentTrain: epoch  7, batch    58 | loss: 50.7106652CurrentTrain: epoch  7, batch    59 | loss: 119.8689798CurrentTrain: epoch  7, batch    60 | loss: 58.3920801CurrentTrain: epoch  7, batch    61 | loss: 66.8223207CurrentTrain: epoch  7, batch    62 | loss: 72.3410790CurrentTrain: epoch  7, batch    63 | loss: 113.7226072CurrentTrain: epoch  7, batch    64 | loss: 118.9406992CurrentTrain: epoch  7, batch    65 | loss: 91.0926088CurrentTrain: epoch  7, batch    66 | loss: 92.4209567CurrentTrain: epoch  7, batch    67 | loss: 59.1881461CurrentTrain: epoch  7, batch    68 | loss: 61.0190185CurrentTrain: epoch  7, batch    69 | loss: 91.7452895CurrentTrain: epoch  7, batch    70 | loss: 93.3277138CurrentTrain: epoch  7, batch    71 | loss: 72.6303111CurrentTrain: epoch  7, batch    72 | loss: 55.3829812CurrentTrain: epoch  7, batch    73 | loss: 72.7684013CurrentTrain: epoch  7, batch    74 | loss: 121.5839308CurrentTrain: epoch  7, batch    75 | loss: 88.6885242CurrentTrain: epoch  7, batch    76 | loss: 86.9377838CurrentTrain: epoch  7, batch    77 | loss: 121.1218236CurrentTrain: epoch  7, batch    78 | loss: 61.1160627CurrentTrain: epoch  7, batch    79 | loss: 113.6590703CurrentTrain: epoch  7, batch    80 | loss: 75.1908249CurrentTrain: epoch  7, batch    81 | loss: 61.4635537CurrentTrain: epoch  7, batch    82 | loss: 87.2860229CurrentTrain: epoch  7, batch    83 | loss: 62.3427016CurrentTrain: epoch  7, batch    84 | loss: 73.3680148CurrentTrain: epoch  7, batch    85 | loss: 63.8848823CurrentTrain: epoch  7, batch    86 | loss: 64.7424486CurrentTrain: epoch  7, batch    87 | loss: 93.9516349CurrentTrain: epoch  7, batch    88 | loss: 93.5968953CurrentTrain: epoch  7, batch    89 | loss: 70.2572530CurrentTrain: epoch  7, batch    90 | loss: 86.9200291CurrentTrain: epoch  7, batch    91 | loss: 71.8190185CurrentTrain: epoch  7, batch    92 | loss: 58.1321259CurrentTrain: epoch  7, batch    93 | loss: 120.7451638CurrentTrain: epoch  7, batch    94 | loss: 62.3264144CurrentTrain: epoch  7, batch    95 | loss: 74.5942263CurrentTrain: epoch  8, batch     0 | loss: 47.9022081CurrentTrain: epoch  8, batch     1 | loss: 118.2503910CurrentTrain: epoch  8, batch     2 | loss: 73.8816764CurrentTrain: epoch  8, batch     3 | loss: 84.7832348CurrentTrain: epoch  8, batch     4 | loss: 72.6550711CurrentTrain: epoch  8, batch     5 | loss: 60.4477933CurrentTrain: epoch  8, batch     6 | loss: 92.0616410CurrentTrain: epoch  8, batch     7 | loss: 72.3668693CurrentTrain: epoch  8, batch     8 | loss: 71.4309737CurrentTrain: epoch  8, batch     9 | loss: 109.2564699CurrentTrain: epoch  8, batch    10 | loss: 88.8566765CurrentTrain: epoch  8, batch    11 | loss: 91.0625990CurrentTrain: epoch  8, batch    12 | loss: 48.8192683CurrentTrain: epoch  8, batch    13 | loss: 52.9157900CurrentTrain: epoch  8, batch    14 | loss: 54.7945109CurrentTrain: epoch  8, batch    15 | loss: 72.4809570CurrentTrain: epoch  8, batch    16 | loss: 71.9499240CurrentTrain: epoch  8, batch    17 | loss: 56.4475700CurrentTrain: epoch  8, batch    18 | loss: 53.6445984CurrentTrain: epoch  8, batch    19 | loss: 61.9563447CurrentTrain: epoch  8, batch    20 | loss: 58.7066911CurrentTrain: epoch  8, batch    21 | loss: 73.4043987CurrentTrain: epoch  8, batch    22 | loss: 71.2624377CurrentTrain: epoch  8, batch    23 | loss: 51.3447103CurrentTrain: epoch  8, batch    24 | loss: 86.1230739CurrentTrain: epoch  8, batch    25 | loss: 91.6132097CurrentTrain: epoch  8, batch    26 | loss: 74.1508371CurrentTrain: epoch  8, batch    27 | loss: 85.3994019CurrentTrain: epoch  8, batch    28 | loss: 90.4790354CurrentTrain: epoch  8, batch    29 | loss: 89.0870894CurrentTrain: epoch  8, batch    30 | loss: 51.4944983CurrentTrain: epoch  8, batch    31 | loss: 61.7888136CurrentTrain: epoch  8, batch    32 | loss: 58.4391474CurrentTrain: epoch  8, batch    33 | loss: 62.8774376CurrentTrain: epoch  8, batch    34 | loss: 60.6528825CurrentTrain: epoch  8, batch    35 | loss: 63.5574709CurrentTrain: epoch  8, batch    36 | loss: 61.5184180CurrentTrain: epoch  8, batch    37 | loss: 53.4540329CurrentTrain: epoch  8, batch    38 | loss: 61.2822077CurrentTrain: epoch  8, batch    39 | loss: 86.4401324CurrentTrain: epoch  8, batch    40 | loss: 77.0675415CurrentTrain: epoch  8, batch    41 | loss: 72.5562493CurrentTrain: epoch  8, batch    42 | loss: 54.2119417CurrentTrain: epoch  8, batch    43 | loss: 61.1895717CurrentTrain: epoch  8, batch    44 | loss: 70.0799658CurrentTrain: epoch  8, batch    45 | loss: 62.5206627CurrentTrain: epoch  8, batch    46 | loss: 59.1795425CurrentTrain: epoch  8, batch    47 | loss: 84.4612014CurrentTrain: epoch  8, batch    48 | loss: 60.0782891CurrentTrain: epoch  8, batch    49 | loss: 88.0211970CurrentTrain: epoch  8, batch    50 | loss: 72.4200084CurrentTrain: epoch  8, batch    51 | loss: 63.1772766CurrentTrain: epoch  8, batch    52 | loss: 63.7768619CurrentTrain: epoch  8, batch    53 | loss: 66.9866547CurrentTrain: epoch  8, batch    54 | loss: 88.3190881CurrentTrain: epoch  8, batch    55 | loss: 72.5623659CurrentTrain: epoch  8, batch    56 | loss: 112.3484519CurrentTrain: epoch  8, batch    57 | loss: 114.3945360CurrentTrain: epoch  8, batch    58 | loss: 69.9364012CurrentTrain: epoch  8, batch    59 | loss: 59.4509635CurrentTrain: epoch  8, batch    60 | loss: 93.9859731CurrentTrain: epoch  8, batch    61 | loss: 90.0530011CurrentTrain: epoch  8, batch    62 | loss: 75.5574483CurrentTrain: epoch  8, batch    63 | loss: 87.8454584CurrentTrain: epoch  8, batch    64 | loss: 92.0333060CurrentTrain: epoch  8, batch    65 | loss: 71.1506306CurrentTrain: epoch  8, batch    66 | loss: 62.3997159CurrentTrain: epoch  8, batch    67 | loss: 72.9982986CurrentTrain: epoch  8, batch    68 | loss: 61.9265013CurrentTrain: epoch  8, batch    69 | loss: 72.0988639CurrentTrain: epoch  8, batch    70 | loss: 89.0698514CurrentTrain: epoch  8, batch    71 | loss: 62.7462010CurrentTrain: epoch  8, batch    72 | loss: 61.1570376CurrentTrain: epoch  8, batch    73 | loss: 119.1895014CurrentTrain: epoch  8, batch    74 | loss: 113.2085119CurrentTrain: epoch  8, batch    75 | loss: 87.3319388CurrentTrain: epoch  8, batch    76 | loss: 90.5555236CurrentTrain: epoch  8, batch    77 | loss: 111.7879986CurrentTrain: epoch  8, batch    78 | loss: 86.5562430CurrentTrain: epoch  8, batch    79 | loss: 66.4988153CurrentTrain: epoch  8, batch    80 | loss: 56.8182579CurrentTrain: epoch  8, batch    81 | loss: 91.7715615CurrentTrain: epoch  8, batch    82 | loss: 117.3848357CurrentTrain: epoch  8, batch    83 | loss: 87.6588482CurrentTrain: epoch  8, batch    84 | loss: 74.6182241CurrentTrain: epoch  8, batch    85 | loss: 115.7371643CurrentTrain: epoch  8, batch    86 | loss: 71.8499847CurrentTrain: epoch  8, batch    87 | loss: 61.3984897CurrentTrain: epoch  8, batch    88 | loss: 116.3988958CurrentTrain: epoch  8, batch    89 | loss: 64.0407264CurrentTrain: epoch  8, batch    90 | loss: 73.3479703CurrentTrain: epoch  8, batch    91 | loss: 60.7308299CurrentTrain: epoch  8, batch    92 | loss: 71.8315280CurrentTrain: epoch  8, batch    93 | loss: 59.2063173CurrentTrain: epoch  8, batch    94 | loss: 73.2990923CurrentTrain: epoch  8, batch    95 | loss: 75.1447789CurrentTrain: epoch  9, batch     0 | loss: 74.7926677CurrentTrain: epoch  9, batch     1 | loss: 61.6542379CurrentTrain: epoch  9, batch     2 | loss: 59.4342931CurrentTrain: epoch  9, batch     3 | loss: 74.6663158CurrentTrain: epoch  9, batch     4 | loss: 72.5254159CurrentTrain: epoch  9, batch     5 | loss: 86.6237657CurrentTrain: epoch  9, batch     6 | loss: 69.3711659CurrentTrain: epoch  9, batch     7 | loss: 51.1975958CurrentTrain: epoch  9, batch     8 | loss: 70.1068057CurrentTrain: epoch  9, batch     9 | loss: 91.0427357CurrentTrain: epoch  9, batch    10 | loss: 59.7813330CurrentTrain: epoch  9, batch    11 | loss: 50.4744033CurrentTrain: epoch  9, batch    12 | loss: 73.4609587CurrentTrain: epoch  9, batch    13 | loss: 84.8228058CurrentTrain: epoch  9, batch    14 | loss: 58.1833589CurrentTrain: epoch  9, batch    15 | loss: 86.0270482CurrentTrain: epoch  9, batch    16 | loss: 74.0623476CurrentTrain: epoch  9, batch    17 | loss: 69.1508601CurrentTrain: epoch  9, batch    18 | loss: 73.0950415CurrentTrain: epoch  9, batch    19 | loss: 70.2065510CurrentTrain: epoch  9, batch    20 | loss: 84.4190463CurrentTrain: epoch  9, batch    21 | loss: 60.3218812CurrentTrain: epoch  9, batch    22 | loss: 66.5177467CurrentTrain: epoch  9, batch    23 | loss: 73.2469154CurrentTrain: epoch  9, batch    24 | loss: 60.5994306CurrentTrain: epoch  9, batch    25 | loss: 91.8071622CurrentTrain: epoch  9, batch    26 | loss: 59.2953848CurrentTrain: epoch  9, batch    27 | loss: 68.7330175CurrentTrain: epoch  9, batch    28 | loss: 69.2863072CurrentTrain: epoch  9, batch    29 | loss: 68.9717165CurrentTrain: epoch  9, batch    30 | loss: 69.9119825CurrentTrain: epoch  9, batch    31 | loss: 66.3839540CurrentTrain: epoch  9, batch    32 | loss: 61.6749943CurrentTrain: epoch  9, batch    33 | loss: 69.3472614CurrentTrain: epoch  9, batch    34 | loss: 69.3207735CurrentTrain: epoch  9, batch    35 | loss: 72.7190607CurrentTrain: epoch  9, batch    36 | loss: 90.7365076CurrentTrain: epoch  9, batch    37 | loss: 158.1306903CurrentTrain: epoch  9, batch    38 | loss: 86.1549222CurrentTrain: epoch  9, batch    39 | loss: 72.3332696CurrentTrain: epoch  9, batch    40 | loss: 60.5157729CurrentTrain: epoch  9, batch    41 | loss: 72.5472824CurrentTrain: epoch  9, batch    42 | loss: 59.7891182CurrentTrain: epoch  9, batch    43 | loss: 71.4419334CurrentTrain: epoch  9, batch    44 | loss: 58.1991852CurrentTrain: epoch  9, batch    45 | loss: 87.1736638CurrentTrain: epoch  9, batch    46 | loss: 72.3518046CurrentTrain: epoch  9, batch    47 | loss: 59.5553282CurrentTrain: epoch  9, batch    48 | loss: 71.2479615CurrentTrain: epoch  9, batch    49 | loss: 63.3421665CurrentTrain: epoch  9, batch    50 | loss: 60.6957832CurrentTrain: epoch  9, batch    51 | loss: 71.9971096CurrentTrain: epoch  9, batch    52 | loss: 61.2408770CurrentTrain: epoch  9, batch    53 | loss: 113.6969781CurrentTrain: epoch  9, batch    54 | loss: 60.7636001CurrentTrain: epoch  9, batch    55 | loss: 112.2427354CurrentTrain: epoch  9, batch    56 | loss: 88.4258211CurrentTrain: epoch  9, batch    57 | loss: 69.1502733CurrentTrain: epoch  9, batch    58 | loss: 87.2165973CurrentTrain: epoch  9, batch    59 | loss: 83.2212607CurrentTrain: epoch  9, batch    60 | loss: 72.9456446CurrentTrain: epoch  9, batch    61 | loss: 79.6853041CurrentTrain: epoch  9, batch    62 | loss: 59.0060361CurrentTrain: epoch  9, batch    63 | loss: 68.3575783CurrentTrain: epoch  9, batch    64 | loss: 117.0971918CurrentTrain: epoch  9, batch    65 | loss: 86.7689461CurrentTrain: epoch  9, batch    66 | loss: 75.1926658CurrentTrain: epoch  9, batch    67 | loss: 86.5366099CurrentTrain: epoch  9, batch    68 | loss: 116.2035406CurrentTrain: epoch  9, batch    69 | loss: 86.2251829CurrentTrain: epoch  9, batch    70 | loss: 69.9801115CurrentTrain: epoch  9, batch    71 | loss: 64.0432931CurrentTrain: epoch  9, batch    72 | loss: 49.0904671CurrentTrain: epoch  9, batch    73 | loss: 60.3304398CurrentTrain: epoch  9, batch    74 | loss: 68.4507491CurrentTrain: epoch  9, batch    75 | loss: 75.8228181CurrentTrain: epoch  9, batch    76 | loss: 59.8025409CurrentTrain: epoch  9, batch    77 | loss: 109.5607603CurrentTrain: epoch  9, batch    78 | loss: 84.8448746CurrentTrain: epoch  9, batch    79 | loss: 50.4426154CurrentTrain: epoch  9, batch    80 | loss: 52.4305114CurrentTrain: epoch  9, batch    81 | loss: 53.2587300CurrentTrain: epoch  9, batch    82 | loss: 62.0433179CurrentTrain: epoch  9, batch    83 | loss: 60.0572243CurrentTrain: epoch  9, batch    84 | loss: 77.7452109CurrentTrain: epoch  9, batch    85 | loss: 64.9573645CurrentTrain: epoch  9, batch    86 | loss: 50.5323442CurrentTrain: epoch  9, batch    87 | loss: 70.8528352CurrentTrain: epoch  9, batch    88 | loss: 84.8296275CurrentTrain: epoch  9, batch    89 | loss: 60.1827131CurrentTrain: epoch  9, batch    90 | loss: 69.1038604CurrentTrain: epoch  9, batch    91 | loss: 62.0811226CurrentTrain: epoch  9, batch    92 | loss: 118.0149951CurrentTrain: epoch  9, batch    93 | loss: 74.2108388CurrentTrain: epoch  9, batch    94 | loss: 68.5997891CurrentTrain: epoch  9, batch    95 | loss: 94.2979868

F1 score per class: {32: 0.5906735751295337, 6: 0.8217821782178217, 19: 0.36363636363636365, 24: 0.7513812154696132, 26: 0.9278350515463918, 29: 0.8078817733990148}
Micro-average F1 score: 0.7673956262425448
Weighted-average F1 score: 0.7694655042588278
F1 score per class: {32: 0.6608695652173913, 6: 0.8070175438596491, 19: 0.2608695652173913, 24: 0.7391304347826086, 26: 0.9333333333333333, 29: 0.835820895522388}
Micro-average F1 score: 0.7588075880758808
Weighted-average F1 score: 0.7431021571384548
F1 score per class: {32: 0.672566371681416, 6: 0.8105726872246696, 19: 0.3157894736842105, 24: 0.7391304347826086, 26: 0.9333333333333333, 29: 0.8316831683168316}
Micro-average F1 score: 0.76993583868011
Weighted-average F1 score: 0.7601870679577631

F1 score per class: {32: 0.5906735751295337, 6: 0.8217821782178217, 19: 0.36363636363636365, 24: 0.7513812154696132, 26: 0.9278350515463918, 29: 0.8078817733990148}
Micro-average F1 score: 0.7673956262425448
Weighted-average F1 score: 0.7694655042588278
F1 score per class: {32: 0.6608695652173913, 6: 0.8070175438596491, 19: 0.2608695652173913, 24: 0.7391304347826086, 26: 0.9333333333333333, 29: 0.835820895522388}
Micro-average F1 score: 0.7588075880758808
Weighted-average F1 score: 0.7431021571384548
F1 score per class: {32: 0.672566371681416, 6: 0.8105726872246696, 19: 0.3157894736842105, 24: 0.7391304347826086, 26: 0.9333333333333333, 29: 0.8316831683168316}
Micro-average F1 score: 0.76993583868011
Weighted-average F1 score: 0.7601870679577631

F1 score per class: {32: 0.43018867924528303, 6: 0.7757009345794392, 19: 0.22641509433962265, 24: 0.6938775510204082, 26: 0.8490566037735849, 29: 0.6431372549019608}
Micro-average F1 score: 0.6460251046025105
Weighted-average F1 score: 0.6332768477840757
F1 score per class: {32: 0.45645645645645644, 6: 0.7419354838709677, 19: 0.15, 24: 0.6732673267326733, 26: 0.8504672897196262, 29: 0.6640316205533597}
Micro-average F1 score: 0.6131386861313869
Weighted-average F1 score: 0.5853100730829714
F1 score per class: {32: 0.4634146341463415, 6: 0.7510204081632653, 19: 0.17475728155339806, 24: 0.6732673267326733, 26: 0.8504672897196262, 29: 0.6588235294117647}
Micro-average F1 score: 0.623608017817372
Weighted-average F1 score: 0.5997045236606414

F1 score per class: {32: 0.43018867924528303, 6: 0.7757009345794392, 19: 0.22641509433962265, 24: 0.6938775510204082, 26: 0.8490566037735849, 29: 0.6431372549019608}
Micro-average F1 score: 0.6460251046025105
Weighted-average F1 score: 0.6332768477840757
F1 score per class: {32: 0.45645645645645644, 6: 0.7419354838709677, 19: 0.15, 24: 0.6732673267326733, 26: 0.8504672897196262, 29: 0.6640316205533597}
Micro-average F1 score: 0.6131386861313869
Weighted-average F1 score: 0.5853100730829714
F1 score per class: {32: 0.4634146341463415, 6: 0.7510204081632653, 19: 0.17475728155339806, 24: 0.6732673267326733, 26: 0.8504672897196262, 29: 0.6588235294117647}
Micro-average F1 score: 0.623608017817372
Weighted-average F1 score: 0.5997045236606414
cur_acc_wo_na:  ['0.7674']
his_acc_wo_na:  ['0.7674']
cur_acc des_wo_na:  ['0.7588']
his_acc des_wo_na:  ['0.7588']
cur_acc rrf_wo_na:  ['0.7699']
his_acc rrf_wo_na:  ['0.7699']
cur_acc_w_na:  ['0.6460']
his_acc_w_na:  ['0.6460']
cur_acc des_w_na:  ['0.6131']
his_acc des_w_na:  ['0.6131']
cur_acc rrf_w_na:  ['0.6236']
his_acc rrf_w_na:  ['0.6236']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 79.0004013CurrentTrain: epoch  0, batch     1 | loss: 151.7464239CurrentTrain: epoch  0, batch     2 | loss: 113.5979914CurrentTrain: epoch  0, batch     3 | loss: 83.8793852CurrentTrain: epoch  0, batch     4 | loss: 117.7359673CurrentTrain: epoch  1, batch     0 | loss: 85.7378981CurrentTrain: epoch  1, batch     1 | loss: 102.4318632CurrentTrain: epoch  1, batch     2 | loss: 73.1119456CurrentTrain: epoch  1, batch     3 | loss: 104.1293935CurrentTrain: epoch  1, batch     4 | loss: 57.6527613CurrentTrain: epoch  2, batch     0 | loss: 78.5384470CurrentTrain: epoch  2, batch     1 | loss: 121.2257849CurrentTrain: epoch  2, batch     2 | loss: 104.8691285CurrentTrain: epoch  2, batch     3 | loss: 84.7326844CurrentTrain: epoch  2, batch     4 | loss: 62.2634076CurrentTrain: epoch  3, batch     0 | loss: 99.6817132CurrentTrain: epoch  3, batch     1 | loss: 80.5094643CurrentTrain: epoch  3, batch     2 | loss: 97.3615530CurrentTrain: epoch  3, batch     3 | loss: 94.7551456CurrentTrain: epoch  3, batch     4 | loss: 77.2565802CurrentTrain: epoch  4, batch     0 | loss: 96.4616321CurrentTrain: epoch  4, batch     1 | loss: 66.7864986CurrentTrain: epoch  4, batch     2 | loss: 120.8973647CurrentTrain: epoch  4, batch     3 | loss: 65.7342258CurrentTrain: epoch  4, batch     4 | loss: 75.3603051CurrentTrain: epoch  5, batch     0 | loss: 64.3923753CurrentTrain: epoch  5, batch     1 | loss: 95.9732306CurrentTrain: epoch  5, batch     2 | loss: 95.7389516CurrentTrain: epoch  5, batch     3 | loss: 116.8848019CurrentTrain: epoch  5, batch     4 | loss: 56.2964563CurrentTrain: epoch  6, batch     0 | loss: 75.8288389CurrentTrain: epoch  6, batch     1 | loss: 63.5407007CurrentTrain: epoch  6, batch     2 | loss: 114.8430549CurrentTrain: epoch  6, batch     3 | loss: 75.2494196CurrentTrain: epoch  6, batch     4 | loss: 102.1180407CurrentTrain: epoch  7, batch     0 | loss: 91.8587656CurrentTrain: epoch  7, batch     1 | loss: 88.8474216CurrentTrain: epoch  7, batch     2 | loss: 116.6682289CurrentTrain: epoch  7, batch     3 | loss: 60.0100573CurrentTrain: epoch  7, batch     4 | loss: 69.3064158CurrentTrain: epoch  8, batch     0 | loss: 63.1629560CurrentTrain: epoch  8, batch     1 | loss: 89.6021793CurrentTrain: epoch  8, batch     2 | loss: 74.1483858CurrentTrain: epoch  8, batch     3 | loss: 74.4075774CurrentTrain: epoch  8, batch     4 | loss: 53.1359686CurrentTrain: epoch  9, batch     0 | loss: 74.1246270CurrentTrain: epoch  9, batch     1 | loss: 87.4695638CurrentTrain: epoch  9, batch     2 | loss: 70.0425912CurrentTrain: epoch  9, batch     3 | loss: 73.7205911CurrentTrain: epoch  9, batch     4 | loss: 54.9156296
MemoryTrain:  epoch  0, batch     0 | loss: 1.8817962MemoryTrain:  epoch  1, batch     0 | loss: 1.6646353MemoryTrain:  epoch  2, batch     0 | loss: 1.2975030MemoryTrain:  epoch  3, batch     0 | loss: 1.0637337MemoryTrain:  epoch  4, batch     0 | loss: 0.8365305MemoryTrain:  epoch  5, batch     0 | loss: 0.6988402MemoryTrain:  epoch  6, batch     0 | loss: 0.6831384MemoryTrain:  epoch  7, batch     0 | loss: 0.3996234MemoryTrain:  epoch  8, batch     0 | loss: 0.4186106MemoryTrain:  epoch  9, batch     0 | loss: 0.2665641

F1 score per class: {32: 0.8544600938967136, 5: 0.0, 6: 0.2222222222222222, 10: 0.6, 16: 0.0, 17: 0.2564102564102564, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5139442231075697
Weighted-average F1 score: 0.5435601278802076
F1 score per class: {32: 0.7279693486590039, 5: 0.0, 6: 0.4647887323943662, 10: 0.6197183098591549, 16: 0.0, 17: 0.4235294117647059, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5436893203883495
Weighted-average F1 score: 0.5335147788409188
F1 score per class: {32: 0.7863247863247863, 5: 0.0, 6: 0.4788732394366197, 10: 0.6567164179104478, 16: 0.0, 17: 0.43373493975903615, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5724137931034483
Weighted-average F1 score: 0.558324053011387

F1 score per class: {32: 0.8544600938967136, 5: 0.5472636815920398, 6: 0.21487603305785125, 10: 0.5882352941176471, 16: 0.0, 17: 0.23809523809523808, 18: 0.8, 19: 0.3783783783783784, 24: 0.7486033519553073, 26: 0.8944723618090452, 29: 0.7981220657276995}
Micro-average F1 score: 0.6641221374045801
Weighted-average F1 score: 0.6747465909412214
F1 score per class: {32: 0.6959706959706959, 5: 0.6204081632653061, 6: 0.4258064516129032, 10: 0.5238095238095238, 16: 0.0, 17: 0.3564356435643564, 18: 0.7419354838709677, 19: 0.3728813559322034, 24: 0.7340425531914894, 26: 0.8952380952380953, 29: 0.7924528301886793}
Micro-average F1 score: 0.660366870483602
Weighted-average F1 score: 0.6585077649086962
F1 score per class: {32: 0.7666666666666667, 5: 0.611353711790393, 6: 0.43312101910828027, 10: 0.5641025641025641, 16: 0.0, 17: 0.3711340206185567, 18: 0.773109243697479, 19: 0.43478260869565216, 24: 0.7431693989071039, 26: 0.9082125603864735, 29: 0.7870370370370371}
Micro-average F1 score: 0.6790481717933836
Weighted-average F1 score: 0.6773432869732738

F1 score per class: {32: 0.7398373983739838, 5: 0.0, 6: 0.21487603305785125, 10: 0.379746835443038, 16: 0.0, 17: 0.19047619047619047, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.3993808049535604
Weighted-average F1 score: 0.3894287371510981
F1 score per class: {32: 0.5705705705705706, 5: 0.0, 6: 0.3815028901734104, 10: 0.39285714285714285, 16: 0.0, 17: 0.27906976744186046, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.38095238095238093
Weighted-average F1 score: 0.36086032713620264
F1 score per class: {32: 0.6112956810631229, 5: 0.0, 6: 0.3930635838150289, 10: 0.4074074074074074, 16: 0.0, 17: 0.2857142857142857, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.3980815347721823
Weighted-average F1 score: 0.37414054250633333

F1 score per class: {32: 0.7309236947791165, 5: 0.35947712418300654, 6: 0.19696969696969696, 10: 0.35714285714285715, 16: 0.0, 17: 0.16806722689075632, 18: 0.728744939271255, 19: 0.22950819672131148, 24: 0.6907216494845361, 26: 0.777292576419214, 29: 0.625}
Micro-average F1 score: 0.5286075949367088
Weighted-average F1 score: 0.5182244803040561
F1 score per class: {32: 0.5149051490514905, 5: 0.39790575916230364, 6: 0.3251231527093596, 10: 0.3142857142857143, 16: 0.0, 17: 0.22929936305732485, 18: 0.6618705035971223, 19: 0.1896551724137931, 24: 0.641860465116279, 26: 0.7203065134099617, 29: 0.6245353159851301}
Micro-average F1 score: 0.4876847290640394
Weighted-average F1 score: 0.4738263539169882
F1 score per class: {32: 0.5609756097560976, 5: 0.3878116343490305, 6: 0.3300970873786408, 10: 0.3308270676691729, 16: 0.0, 17: 0.24, 18: 0.6865671641791045, 19: 0.21978021978021978, 24: 0.6666666666666666, 26: 0.7430830039525692, 29: 0.6227106227106227}
Micro-average F1 score: 0.5025773195876289
Weighted-average F1 score: 0.48725421454668044
cur_acc_wo_na:  ['0.7674', '0.5139']
his_acc_wo_na:  ['0.7674', '0.6641']
cur_acc des_wo_na:  ['0.7588', '0.5437']
his_acc des_wo_na:  ['0.7588', '0.6604']
cur_acc rrf_wo_na:  ['0.7699', '0.5724']
his_acc rrf_wo_na:  ['0.7699', '0.6790']
cur_acc_w_na:  ['0.6460', '0.3994']
his_acc_w_na:  ['0.6460', '0.5286']
cur_acc des_w_na:  ['0.6131', '0.3810']
his_acc des_w_na:  ['0.6131', '0.4877']
cur_acc rrf_w_na:  ['0.6236', '0.3981']
his_acc rrf_w_na:  ['0.6236', '0.5026']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 97.5802584CurrentTrain: epoch  0, batch     1 | loss: 77.5913511CurrentTrain: epoch  0, batch     2 | loss: 77.8750188CurrentTrain: epoch  0, batch     3 | loss: 10.9302239CurrentTrain: epoch  1, batch     0 | loss: 74.7365038CurrentTrain: epoch  1, batch     1 | loss: 73.6289865CurrentTrain: epoch  1, batch     2 | loss: 68.6280427CurrentTrain: epoch  1, batch     3 | loss: 17.6180812CurrentTrain: epoch  2, batch     0 | loss: 63.5895994CurrentTrain: epoch  2, batch     1 | loss: 79.3662999CurrentTrain: epoch  2, batch     2 | loss: 92.5982856CurrentTrain: epoch  2, batch     3 | loss: 18.5664538CurrentTrain: epoch  3, batch     0 | loss: 63.1680861CurrentTrain: epoch  3, batch     1 | loss: 66.8688028CurrentTrain: epoch  3, batch     2 | loss: 75.2580895CurrentTrain: epoch  3, batch     3 | loss: 9.2953533CurrentTrain: epoch  4, batch     0 | loss: 61.2157038CurrentTrain: epoch  4, batch     1 | loss: 62.3570554CurrentTrain: epoch  4, batch     2 | loss: 64.2003581CurrentTrain: epoch  4, batch     3 | loss: 17.4354250CurrentTrain: epoch  5, batch     0 | loss: 60.3821721CurrentTrain: epoch  5, batch     1 | loss: 59.9704116CurrentTrain: epoch  5, batch     2 | loss: 63.4978052CurrentTrain: epoch  5, batch     3 | loss: 15.3803331CurrentTrain: epoch  6, batch     0 | loss: 61.6622443CurrentTrain: epoch  6, batch     1 | loss: 57.3857164CurrentTrain: epoch  6, batch     2 | loss: 61.5805486CurrentTrain: epoch  6, batch     3 | loss: 17.7487469CurrentTrain: epoch  7, batch     0 | loss: 56.4452298CurrentTrain: epoch  7, batch     1 | loss: 67.7081543CurrentTrain: epoch  7, batch     2 | loss: 88.7102552CurrentTrain: epoch  7, batch     3 | loss: 41.9114016CurrentTrain: epoch  8, batch     0 | loss: 58.0870225CurrentTrain: epoch  8, batch     1 | loss: 58.8105077CurrentTrain: epoch  8, batch     2 | loss: 57.8198211CurrentTrain: epoch  8, batch     3 | loss: 14.7268798CurrentTrain: epoch  9, batch     0 | loss: 71.1388898CurrentTrain: epoch  9, batch     1 | loss: 66.0477160CurrentTrain: epoch  9, batch     2 | loss: 66.4927594CurrentTrain: epoch  9, batch     3 | loss: 8.1906057
MemoryTrain:  epoch  0, batch     0 | loss: 1.1821690MemoryTrain:  epoch  1, batch     0 | loss: 0.9344351MemoryTrain:  epoch  2, batch     0 | loss: 0.7971002MemoryTrain:  epoch  3, batch     0 | loss: 0.6289558MemoryTrain:  epoch  4, batch     0 | loss: 0.4590907MemoryTrain:  epoch  5, batch     0 | loss: 0.3894717MemoryTrain:  epoch  6, batch     0 | loss: 0.3109671MemoryTrain:  epoch  7, batch     0 | loss: 0.2644654MemoryTrain:  epoch  8, batch     0 | loss: 0.2863192MemoryTrain:  epoch  9, batch     0 | loss: 0.1932207

F1 score per class: {32: 0.0, 6: 0.5714285714285714, 7: 0.8771929824561403, 40: 0.0, 9: 0.0, 16: 0.0, 17: 0.0, 19: 0.6086956521739131, 26: 0.0, 27: 0.5, 29: 0.0, 31: 0.19696969696969696}
Micro-average F1 score: 0.32989690721649484
Weighted-average F1 score: 0.2713494730327527
F1 score per class: {32: 0.0, 6: 0.3333333333333333, 7: 0.6493506493506493, 40: 0.0, 9: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 24: 0.6363636363636364, 26: 0.0, 27: 0.4, 29: 0.0, 31: 0.2698412698412698}
Micro-average F1 score: 0.3269230769230769
Weighted-average F1 score: 0.28297112539536784
F1 score per class: {32: 0.0, 6: 0.3333333333333333, 7: 0.8064516129032258, 40: 0.0, 9: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 24: 0.6363636363636364, 26: 0.0, 27: 0.5, 29: 0.0, 31: 0.26356589147286824}
Micro-average F1 score: 0.3434343434343434
Weighted-average F1 score: 0.28508406286867294

F1 score per class: {32: 0.7054263565891473, 5: 0.3448275862068966, 6: 0.044444444444444446, 7: 0.8771929824561403, 40: 0.0761904761904762, 10: 0.5862068965517241, 9: 0.0, 16: 0.18, 17: 0.6070038910505836, 18: 0.25, 19: 0.7314285714285714, 24: 0.3111111111111111, 26: 0.8944723618090452, 27: 0.2857142857142857, 29: 0.8393782383419689, 31: 0.08996539792387544}
Micro-average F1 score: 0.5027160493827161
Weighted-average F1 score: 0.4795150269480761
F1 score per class: {32: 0.5818181818181818, 5: 0.4897959183673469, 6: 0.03333333333333333, 7: 0.5617977528089888, 40: 0.40540540540540543, 10: 0.475, 9: 0.0, 16: 0.2545454545454545, 17: 0.6, 18: 0.2777777777777778, 19: 0.7391304347826086, 24: 0.3783783783783784, 26: 0.8910891089108911, 27: 0.2, 29: 0.8229665071770335, 31: 0.13991769547325103}
Micro-average F1 score: 0.529891304347826
Weighted-average F1 score: 0.5066957939114393
F1 score per class: {32: 0.6462585034013606, 5: 0.4946236559139785, 6: 0.03076923076923077, 7: 0.8064516129032258, 40: 0.4057971014492754, 10: 0.5142857142857142, 9: 0.0, 16: 0.26666666666666666, 17: 0.6046511627906976, 18: 0.25, 19: 0.7362637362637363, 24: 0.35, 26: 0.8910891089108911, 27: 0.25, 29: 0.819047619047619, 31: 0.1328125}
Micro-average F1 score: 0.5433145009416196
Weighted-average F1 score: 0.5174579235507204

F1 score per class: {32: 0.0, 5: 0.0, 6: 0.4444444444444444, 7: 0.8064516129032258, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 19: 0.5, 26: 0.0, 27: 0.4, 29: 0.0, 31: 0.16352201257861634}
Micro-average F1 score: 0.27988338192419826
Weighted-average F1 score: 0.23751327068536243
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.2857142857142857, 7: 0.5747126436781609, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 24: 0.5384615384615384, 26: 0.0, 27: 0.2857142857142857, 29: 0.0, 31: 0.2328767123287671}
Micro-average F1 score: 0.26842105263157895
Weighted-average F1 score: 0.23391674746620922
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.2857142857142857, 7: 0.746268656716418, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 24: 0.5, 26: 0.0, 27: 0.3333333333333333, 29: 0.0, 31: 0.22666666666666666}
Micro-average F1 score: 0.2857142857142857
Weighted-average F1 score: 0.24110470574815518

F1 score per class: {32: 0.4776902887139108, 5: 0.25510204081632654, 6: 0.02666666666666667, 7: 0.7936507936507936, 40: 0.07476635514018691, 10: 0.3148148148148148, 9: 0.0, 16: 0.1323529411764706, 17: 0.5693430656934306, 18: 0.21428571428571427, 19: 0.6701570680628273, 24: 0.2, 26: 0.7447698744769874, 27: 0.2222222222222222, 29: 0.6952789699570815, 31: 0.06683804627249357}
Micro-average F1 score: 0.38692512352717595
Weighted-average F1 score: 0.36142362276818346
F1 score per class: {32: 0.3817097415506958, 5: 0.3287671232876712, 6: 0.018518518518518517, 7: 0.4672897196261682, 40: 0.32608695652173914, 10: 0.2714285714285714, 9: 0.0, 16: 0.18064516129032257, 17: 0.5591397849462365, 18: 0.17543859649122806, 19: 0.6634146341463415, 24: 0.2641509433962264, 26: 0.7317073170731707, 27: 0.125, 29: 0.671875, 31: 0.10967741935483871}
Micro-average F1 score: 0.3968792401628223
Weighted-average F1 score: 0.373706455547461
F1 score per class: {32: 0.4212860310421286, 5: 0.3333333333333333, 6: 0.017543859649122806, 7: 0.7246376811594203, 40: 0.34146341463414637, 10: 0.27906976744186046, 9: 0.0, 16: 0.18791946308724833, 17: 0.5652173913043478, 18: 0.17391304347826086, 19: 0.67, 24: 0.23333333333333334, 26: 0.7407407407407407, 27: 0.16666666666666666, 29: 0.6640926640926641, 31: 0.10303030303030303}
Micro-average F1 score: 0.4096556620518282
Weighted-average F1 score: 0.3837768822755649
cur_acc_wo_na:  ['0.7674', '0.5139', '0.3299']
his_acc_wo_na:  ['0.7674', '0.6641', '0.5027']
cur_acc des_wo_na:  ['0.7588', '0.5437', '0.3269']
his_acc des_wo_na:  ['0.7588', '0.6604', '0.5299']
cur_acc rrf_wo_na:  ['0.7699', '0.5724', '0.3434']
his_acc rrf_wo_na:  ['0.7699', '0.6790', '0.5433']
cur_acc_w_na:  ['0.6460', '0.3994', '0.2799']
his_acc_w_na:  ['0.6460', '0.5286', '0.3869']
cur_acc des_w_na:  ['0.6131', '0.3810', '0.2684']
his_acc des_w_na:  ['0.6131', '0.4877', '0.3969']
cur_acc rrf_w_na:  ['0.6236', '0.3981', '0.2857']
his_acc rrf_w_na:  ['0.6236', '0.5026', '0.4097']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 92.6438243CurrentTrain: epoch  0, batch     1 | loss: 90.9024603CurrentTrain: epoch  0, batch     2 | loss: 115.2421409CurrentTrain: epoch  0, batch     3 | loss: 92.5443857CurrentTrain: epoch  1, batch     0 | loss: 73.6351934CurrentTrain: epoch  1, batch     1 | loss: 68.8058286CurrentTrain: epoch  1, batch     2 | loss: 90.6230564CurrentTrain: epoch  1, batch     3 | loss: 89.3115266CurrentTrain: epoch  2, batch     0 | loss: 81.2598377CurrentTrain: epoch  2, batch     1 | loss: 68.1521891CurrentTrain: epoch  2, batch     2 | loss: 104.3756356CurrentTrain: epoch  2, batch     3 | loss: 66.6780725CurrentTrain: epoch  3, batch     0 | loss: 94.9667807CurrentTrain: epoch  3, batch     1 | loss: 82.5788378CurrentTrain: epoch  3, batch     2 | loss: 78.8273949CurrentTrain: epoch  3, batch     3 | loss: 75.3456216CurrentTrain: epoch  4, batch     0 | loss: 91.9989336CurrentTrain: epoch  4, batch     1 | loss: 93.7578333CurrentTrain: epoch  4, batch     2 | loss: 81.9265679CurrentTrain: epoch  4, batch     3 | loss: 49.8664984CurrentTrain: epoch  5, batch     0 | loss: 92.4220490CurrentTrain: epoch  5, batch     1 | loss: 73.8529698CurrentTrain: epoch  5, batch     2 | loss: 97.2479726CurrentTrain: epoch  5, batch     3 | loss: 59.7373599CurrentTrain: epoch  6, batch     0 | loss: 74.1802515CurrentTrain: epoch  6, batch     1 | loss: 73.7790706CurrentTrain: epoch  6, batch     2 | loss: 76.9417220CurrentTrain: epoch  6, batch     3 | loss: 71.9094475CurrentTrain: epoch  7, batch     0 | loss: 72.3617553CurrentTrain: epoch  7, batch     1 | loss: 90.0617870CurrentTrain: epoch  7, batch     2 | loss: 73.0421172CurrentTrain: epoch  7, batch     3 | loss: 62.0861712CurrentTrain: epoch  8, batch     0 | loss: 75.9369866CurrentTrain: epoch  8, batch     1 | loss: 71.3212199CurrentTrain: epoch  8, batch     2 | loss: 70.7022359CurrentTrain: epoch  8, batch     3 | loss: 49.4864026CurrentTrain: epoch  9, batch     0 | loss: 68.7907200CurrentTrain: epoch  9, batch     1 | loss: 60.1029720CurrentTrain: epoch  9, batch     2 | loss: 90.7185610CurrentTrain: epoch  9, batch     3 | loss: 71.3587393
MemoryTrain:  epoch  0, batch     0 | loss: 0.9557272MemoryTrain:  epoch  1, batch     0 | loss: 0.8261871MemoryTrain:  epoch  2, batch     0 | loss: 0.6670716MemoryTrain:  epoch  3, batch     0 | loss: 0.5291615MemoryTrain:  epoch  4, batch     0 | loss: 0.4357308MemoryTrain:  epoch  5, batch     0 | loss: 0.3627194MemoryTrain:  epoch  6, batch     0 | loss: 0.2822350MemoryTrain:  epoch  7, batch     0 | loss: 0.2473380MemoryTrain:  epoch  8, batch     0 | loss: 0.2165785MemoryTrain:  epoch  9, batch     0 | loss: 0.1856429

F1 score per class: {0: 0.9166666666666666, 4: 0.9690721649484536, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 13: 0.3333333333333333, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.44155844155844154, 23: 0.8666666666666667, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.6726296958855098
Weighted-average F1 score: 0.5577811361362133
F1 score per class: {0: 0.8181818181818182, 4: 0.9514563106796117, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.25806451612903225, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.4810126582278481, 23: 0.7294117647058823, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.6256239600665557
Weighted-average F1 score: 0.5251904822373402
F1 score per class: {0: 0.8607594936708861, 4: 0.9746192893401016, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.3125, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.47619047619047616, 23: 0.7294117647058823, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.6294416243654822
Weighted-average F1 score: 0.5177445070002313

F1 score per class: {0: 0.7857142857142857, 4: 0.9690721649484536, 5: 0.6691176470588235, 6: 0.4431818181818182, 7: 0.046511627906976744, 9: 0.8064516129032258, 10: 0.05504587155963303, 13: 0.062111801242236024, 16: 0.6129032258064516, 17: 0.0, 18: 0.17204301075268819, 19: 0.6470588235294118, 21: 0.3008849557522124, 23: 0.7222222222222222, 24: 0.17391304347826086, 26: 0.6927374301675978, 27: 0.29850746268656714, 29: 0.8584905660377359, 31: 0.16666666666666666, 32: 0.7142857142857143, 40: 0.13496932515337423}
Micro-average F1 score: 0.5120056497175142
Weighted-average F1 score: 0.475335600518967
F1 score per class: {0: 0.6153846153846154, 4: 0.937799043062201, 5: 0.46601941747572817, 6: 0.4157303370786517, 7: 0.0594059405940594, 9: 0.5952380952380952, 10: 0.25806451612903225, 13: 0.05063291139240506, 16: 0.4, 17: 0.0, 18: 0.2777777777777778, 19: 0.572463768115942, 21: 0.3275862068965517, 23: 0.6595744680851063, 24: 0.16666666666666666, 26: 0.6842105263157895, 27: 0.358974358974359, 29: 0.8186046511627907, 31: 0.10526315789473684, 32: 0.7410714285714286, 40: 0.14222222222222222}
Micro-average F1 score: 0.4896073903002309
Weighted-average F1 score: 0.4597092808294019
F1 score per class: {0: 0.6732673267326733, 4: 0.9696969696969697, 5: 0.5026178010471204, 6: 0.4088397790055249, 7: 0.0425531914893617, 9: 0.684931506849315, 10: 0.24390243902439024, 13: 0.06289308176100629, 16: 0.5526315789473685, 17: 0.0, 18: 0.26785714285714285, 19: 0.5962264150943396, 21: 0.3125, 23: 0.6526315789473685, 24: 0.16666666666666666, 26: 0.6914893617021277, 27: 0.34146341463414637, 29: 0.8294930875576036, 31: 0.125, 32: 0.7155172413793104, 40: 0.13178294573643412}
Micro-average F1 score: 0.4978165938864629
Weighted-average F1 score: 0.46483192988146854

F1 score per class: {0: 0.88, 4: 0.9170731707317074, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.13513513513513514, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.34, 23: 0.6782608695652174, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.47414880201765447
Weighted-average F1 score: 0.3641164858317424
F1 score per class: {0: 0.7578947368421053, 4: 0.8949771689497716, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.11428571428571428, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3619047619047619, 23: 0.6019417475728155, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4502994011976048
Weighted-average F1 score: 0.35508689352108364
F1 score per class: {0: 0.8292682926829268, 4: 0.9230769230769231, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.13513513513513514, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.35714285714285715, 23: 0.5904761904761905, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4542124542124542
Weighted-average F1 score: 0.3497661029441639

F1 score per class: {0: 0.6111111111111112, 4: 0.9038461538461539, 5: 0.3947939262472885, 6: 0.29770992366412213, 7: 0.026845637583892617, 9: 0.7142857142857143, 10: 0.05042016806722689, 13: 0.030211480362537766, 16: 0.33043478260869563, 17: 0.0, 18: 0.128, 19: 0.5968992248062015, 21: 0.2073170731707317, 23: 0.4936708860759494, 24: 0.14285714285714285, 26: 0.6326530612244898, 27: 0.14925373134328357, 29: 0.6618181818181819, 31: 0.10526315789473684, 32: 0.5151515151515151, 40: 0.0912863070539419}
Micro-average F1 score: 0.3584672435105068
Weighted-average F1 score: 0.32498965806607416
F1 score per class: {0: 0.4675324675324675, 4: 0.8711111111111111, 5: 0.27705627705627706, 6: 0.27205882352941174, 7: 0.03529411764705882, 9: 0.5, 10: 0.22535211267605634, 13: 0.025, 16: 0.23333333333333334, 17: 0.0, 18: 0.18633540372670807, 19: 0.5031847133757962, 21: 0.2111111111111111, 23: 0.5040650406504065, 24: 0.125, 26: 0.6132075471698113, 27: 0.22580645161290322, 29: 0.6219081272084805, 31: 0.058823529411764705, 32: 0.578397212543554, 40: 0.10884353741496598}
Micro-average F1 score: 0.3468910705937354
Weighted-average F1 score: 0.317567565993366
F1 score per class: {0: 0.5230769230769231, 4: 0.9056603773584906, 5: 0.3018867924528302, 6: 0.27205882352941174, 7: 0.024691358024691357, 9: 0.6172839506172839, 10: 0.21897810218978103, 13: 0.030303030303030304, 16: 0.3, 17: 0.0, 18: 0.18181818181818182, 19: 0.541095890410959, 21: 0.20512820512820512, 23: 0.4881889763779528, 24: 0.125, 26: 0.625, 27: 0.2028985507246377, 29: 0.631578947368421, 31: 0.07692307692307693, 32: 0.5627118644067797, 40: 0.09855072463768116}
Micro-average F1 score: 0.3544606553456111
Weighted-average F1 score: 0.32219148832366823
cur_acc_wo_na:  ['0.7674', '0.5139', '0.3299', '0.6726']
his_acc_wo_na:  ['0.7674', '0.6641', '0.5027', '0.5120']
cur_acc des_wo_na:  ['0.7588', '0.5437', '0.3269', '0.6256']
his_acc des_wo_na:  ['0.7588', '0.6604', '0.5299', '0.4896']
cur_acc rrf_wo_na:  ['0.7699', '0.5724', '0.3434', '0.6294']
his_acc rrf_wo_na:  ['0.7699', '0.6790', '0.5433', '0.4978']
cur_acc_w_na:  ['0.6460', '0.3994', '0.2799', '0.4741']
his_acc_w_na:  ['0.6460', '0.5286', '0.3869', '0.3585']
cur_acc des_w_na:  ['0.6131', '0.3810', '0.2684', '0.4503']
his_acc des_w_na:  ['0.6131', '0.4877', '0.3969', '0.3469']
cur_acc rrf_w_na:  ['0.6236', '0.3981', '0.2857', '0.4542']
his_acc rrf_w_na:  ['0.6236', '0.5026', '0.4097', '0.3545']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 95.6627098CurrentTrain: epoch  0, batch     1 | loss: 82.7770749CurrentTrain: epoch  0, batch     2 | loss: 87.6397594CurrentTrain: epoch  0, batch     3 | loss: 52.7532977CurrentTrain: epoch  1, batch     0 | loss: 73.8516954CurrentTrain: epoch  1, batch     1 | loss: 126.4897901CurrentTrain: epoch  1, batch     2 | loss: 68.8057436CurrentTrain: epoch  1, batch     3 | loss: 53.0460472CurrentTrain: epoch  2, batch     0 | loss: 118.2175762CurrentTrain: epoch  2, batch     1 | loss: 97.7025631CurrentTrain: epoch  2, batch     2 | loss: 68.0401954CurrentTrain: epoch  2, batch     3 | loss: 44.9996855CurrentTrain: epoch  3, batch     0 | loss: 74.9895253CurrentTrain: epoch  3, batch     1 | loss: 67.2347434CurrentTrain: epoch  3, batch     2 | loss: 76.6079292CurrentTrain: epoch  3, batch     3 | loss: 96.2585442CurrentTrain: epoch  4, batch     0 | loss: 73.5142333CurrentTrain: epoch  4, batch     1 | loss: 76.0907326CurrentTrain: epoch  4, batch     2 | loss: 73.9244248CurrentTrain: epoch  4, batch     3 | loss: 95.3271950CurrentTrain: epoch  5, batch     0 | loss: 77.5919764CurrentTrain: epoch  5, batch     1 | loss: 75.8679157CurrentTrain: epoch  5, batch     2 | loss: 71.9357469CurrentTrain: epoch  5, batch     3 | loss: 34.7470048CurrentTrain: epoch  6, batch     0 | loss: 73.9983522CurrentTrain: epoch  6, batch     1 | loss: 68.4694811CurrentTrain: epoch  6, batch     2 | loss: 75.7994271CurrentTrain: epoch  6, batch     3 | loss: 43.6170178CurrentTrain: epoch  7, batch     0 | loss: 88.4140996CurrentTrain: epoch  7, batch     1 | loss: 60.2876540CurrentTrain: epoch  7, batch     2 | loss: 57.7108567CurrentTrain: epoch  7, batch     3 | loss: 44.7099269CurrentTrain: epoch  8, batch     0 | loss: 111.2694412CurrentTrain: epoch  8, batch     1 | loss: 58.2117486CurrentTrain: epoch  8, batch     2 | loss: 86.3339513CurrentTrain: epoch  8, batch     3 | loss: 40.8321133CurrentTrain: epoch  9, batch     0 | loss: 88.5957584CurrentTrain: epoch  9, batch     1 | loss: 69.5472154CurrentTrain: epoch  9, batch     2 | loss: 57.1445755CurrentTrain: epoch  9, batch     3 | loss: 33.8162754
MemoryTrain:  epoch  0, batch     0 | loss: 0.7342045MemoryTrain:  epoch  1, batch     0 | loss: 0.6425339MemoryTrain:  epoch  2, batch     0 | loss: 0.4868810MemoryTrain:  epoch  3, batch     0 | loss: 0.4488474MemoryTrain:  epoch  4, batch     0 | loss: 0.3506520MemoryTrain:  epoch  5, batch     0 | loss: 0.2990946MemoryTrain:  epoch  6, batch     0 | loss: 0.2421475MemoryTrain:  epoch  7, batch     0 | loss: 0.1840753MemoryTrain:  epoch  8, batch     0 | loss: 0.1660835MemoryTrain:  epoch  9, batch     0 | loss: 0.1623080

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.496, 9: 0.0, 13: 0.0, 16: 0.0, 20: 0.8333333333333334, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8333333333333334, 32: 0.0, 33: 0.35294117647058826, 36: 0.45977011494252873, 40: 0.0}
Micro-average F1 score: 0.5193621867881549
Weighted-average F1 score: 0.4459782987465543
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5405405405405406, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 20: 0.7567567567567568, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.75, 31: 0.0, 32: 0.0, 33: 0.3125, 36: 0.7441860465116279, 40: 0.0}
Micro-average F1 score: 0.5050505050505051
Weighted-average F1 score: 0.40922196220822016
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5416666666666666, 9: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 20: 0.773109243697479, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.7894736842105263, 31: 0.0, 32: 0.0, 33: 0.3333333333333333, 36: 0.7241379310344828, 40: 0.0}
Micro-average F1 score: 0.5394495412844037
Weighted-average F1 score: 0.451966021418451

F1 score per class: {0: 0.75, 4: 0.9130434782608695, 5: 0.7072243346007605, 6: 0.4074074074074074, 7: 0.05405405405405406, 8: 0.31313131313131315, 9: 0.8064516129032258, 10: 0.0392156862745098, 13: 0.10256410256410256, 16: 0.6037735849056604, 17: 0.0, 18: 0.09302325581395349, 19: 0.6220472440944882, 20: 0.6474820143884892, 21: 0.25, 23: 0.5783132530120482, 24: 0.16666666666666666, 26: 0.6528497409326425, 27: 0.2894736842105263, 29: 0.8571428571428571, 30: 0.8333333333333334, 31: 0.15384615384615385, 32: 0.7241379310344828, 33: 0.16666666666666666, 36: 0.43010752688172044, 40: 0.11764705882352941}
Micro-average F1 score: 0.521594684385382
Weighted-average F1 score: 0.5149934764499497
F1 score per class: {0: 0.5426356589147286, 4: 0.941747572815534, 5: 0.39918533604887985, 6: 0.40476190476190477, 7: 0.04395604395604396, 8: 0.2898550724637681, 9: 0.5681818181818182, 10: 0.20512820512820512, 13: 0.09523809523809523, 16: 0.4, 17: 0.0, 18: 0.4578313253012048, 19: 0.5629139072847682, 20: 0.6222222222222222, 21: 0.3140495867768595, 23: 0.5977011494252874, 24: 0.2, 26: 0.6538461538461539, 27: 0.34615384615384615, 29: 0.7911111111111111, 30: 0.5084745762711864, 31: 0.0625, 32: 0.711864406779661, 33: 0.09900990099009901, 36: 0.5853658536585366, 40: 0.08}
Micro-average F1 score: 0.46959280803807507
Weighted-average F1 score: 0.4484123563984925
F1 score per class: {0: 0.6732673267326733, 4: 0.9743589743589743, 5: 0.56, 6: 0.4235294117647059, 7: 0.04819277108433735, 8: 0.291044776119403, 9: 0.7352941176470589, 10: 0.09345794392523364, 13: 0.08888888888888889, 16: 0.5588235294117647, 17: 0.0, 18: 0.3783783783783784, 19: 0.5882352941176471, 20: 0.6133333333333333, 21: 0.2222222222222222, 23: 0.5977011494252874, 24: 0.14285714285714285, 26: 0.67, 27: 0.3103448275862069, 29: 0.8054298642533937, 30: 0.6521739130434783, 31: 0.09090909090909091, 32: 0.7148936170212766, 33: 0.10869565217391304, 36: 0.5874125874125874, 40: 0.07905138339920949}
Micro-average F1 score: 0.4968299711815562
Weighted-average F1 score: 0.47813167052306615

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.41333333333333333, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.6081081081081081, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.7894736842105263, 31: 0.0, 32: 0.0, 33: 0.3333333333333333, 36: 0.40816326530612246, 40: 0.0}
Micro-average F1 score: 0.37623762376237624
Weighted-average F1 score: 0.30953791996077706
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.40816326530612246, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.5384615384615384, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.6666666666666666, 31: 0.0, 32: 0.0, 33: 0.24390243902439024, 36: 0.5783132530120482, 40: 0.0}
Micro-average F1 score: 0.33783783783783783
Weighted-average F1 score: 0.2805838420678095
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.40625, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.5542168674698795, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.75, 31: 0.0, 32: 0.0, 33: 0.2702702702702703, 36: 0.5637583892617449, 40: 0.0}
Micro-average F1 score: 0.36658354114713215
Weighted-average F1 score: 0.3095184106696644

F1 score per class: {0: 0.6111111111111112, 4: 0.865979381443299, 5: 0.49206349206349204, 6: 0.30275229357798167, 7: 0.031007751937984496, 8: 0.20945945945945946, 9: 0.7246376811594203, 10: 0.038461538461538464, 13: 0.05063291139240506, 16: 0.37209302325581395, 17: 0.0, 18: 0.08, 19: 0.5703971119133574, 20: 0.3345724907063197, 21: 0.18, 23: 0.46601941747572817, 24: 0.13793103448275862, 26: 0.5887850467289719, 27: 0.16923076923076924, 29: 0.6818181818181818, 30: 0.7894736842105263, 31: 0.09090909090909091, 32: 0.5333333333333333, 33: 0.12, 36: 0.28776978417266186, 40: 0.0896358543417367}
Micro-average F1 score: 0.3898683883784455
Weighted-average F1 score: 0.3740140979166206
F1 score per class: {0: 0.4046242774566474, 4: 0.8738738738738738, 5: 0.23444976076555024, 6: 0.2677165354330709, 7: 0.024539877300613498, 8: 0.1789709172259508, 9: 0.4807692307692308, 10: 0.18604651162790697, 13: 0.05333333333333334, 16: 0.23728813559322035, 17: 0.0, 18: 0.2923076923076923, 19: 0.49707602339181284, 20: 0.3514644351464435, 21: 0.21468926553672316, 23: 0.4406779661016949, 24: 0.14634146341463414, 26: 0.576271186440678, 27: 0.21951219512195122, 29: 0.5836065573770491, 30: 0.38461538461538464, 31: 0.034482758620689655, 32: 0.5526315789473685, 33: 0.06711409395973154, 36: 0.3650190114068441, 40: 0.06}
Micro-average F1 score: 0.3280384189139269
Weighted-average F1 score: 0.308365320910235
F1 score per class: {0: 0.5112781954887218, 4: 0.9090909090909091, 5: 0.3373493975903614, 6: 0.2868525896414343, 7: 0.026845637583892617, 8: 0.18013856812933027, 9: 0.6410256410256411, 10: 0.08771929824561403, 13: 0.05, 16: 0.3089430894308943, 17: 0.0, 18: 0.26666666666666666, 19: 0.53125, 20: 0.3262411347517731, 21: 0.14634146341463414, 23: 0.4406779661016949, 24: 0.10526315789473684, 26: 0.5982142857142857, 27: 0.1875, 29: 0.5933333333333334, 30: 0.5555555555555556, 31: 0.05128205128205128, 32: 0.5508196721311476, 33: 0.07352941176470588, 36: 0.3559322033898305, 40: 0.05952380952380952}
Micro-average F1 score: 0.35076297049847405
Weighted-average F1 score: 0.33145075489857706
cur_acc_wo_na:  ['0.7674', '0.5139', '0.3299', '0.6726', '0.5194']
his_acc_wo_na:  ['0.7674', '0.6641', '0.5027', '0.5120', '0.5216']
cur_acc des_wo_na:  ['0.7588', '0.5437', '0.3269', '0.6256', '0.5051']
his_acc des_wo_na:  ['0.7588', '0.6604', '0.5299', '0.4896', '0.4696']
cur_acc rrf_wo_na:  ['0.7699', '0.5724', '0.3434', '0.6294', '0.5394']
his_acc rrf_wo_na:  ['0.7699', '0.6790', '0.5433', '0.4978', '0.4968']
cur_acc_w_na:  ['0.6460', '0.3994', '0.2799', '0.4741', '0.3762']
his_acc_w_na:  ['0.6460', '0.5286', '0.3869', '0.3585', '0.3899']
cur_acc des_w_na:  ['0.6131', '0.3810', '0.2684', '0.4503', '0.3378']
his_acc des_w_na:  ['0.6131', '0.4877', '0.3969', '0.3469', '0.3280']
cur_acc rrf_w_na:  ['0.6236', '0.3981', '0.2857', '0.4542', '0.3666']
his_acc rrf_w_na:  ['0.6236', '0.5026', '0.4097', '0.3545', '0.3508']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 107.9380000CurrentTrain: epoch  0, batch     1 | loss: 97.8699775CurrentTrain: epoch  0, batch     2 | loss: 109.8898496CurrentTrain: epoch  0, batch     3 | loss: 84.3438615CurrentTrain: epoch  0, batch     4 | loss: 28.7148475CurrentTrain: epoch  1, batch     0 | loss: 88.4476281CurrentTrain: epoch  1, batch     1 | loss: 125.2698464CurrentTrain: epoch  1, batch     2 | loss: 83.2540216CurrentTrain: epoch  1, batch     3 | loss: 97.4553266CurrentTrain: epoch  1, batch     4 | loss: 18.2138509CurrentTrain: epoch  2, batch     0 | loss: 67.3695273CurrentTrain: epoch  2, batch     1 | loss: 80.4864008CurrentTrain: epoch  2, batch     2 | loss: 74.4198442CurrentTrain: epoch  2, batch     3 | loss: 162.0530524CurrentTrain: epoch  2, batch     4 | loss: 21.9033109CurrentTrain: epoch  3, batch     0 | loss: 90.6111364CurrentTrain: epoch  3, batch     1 | loss: 65.6641216CurrentTrain: epoch  3, batch     2 | loss: 76.5087006CurrentTrain: epoch  3, batch     3 | loss: 95.5212158CurrentTrain: epoch  3, batch     4 | loss: 20.9025649CurrentTrain: epoch  4, batch     0 | loss: 72.7004027CurrentTrain: epoch  4, batch     1 | loss: 90.9279074CurrentTrain: epoch  4, batch     2 | loss: 73.3035798CurrentTrain: epoch  4, batch     3 | loss: 75.5261808CurrentTrain: epoch  4, batch     4 | loss: 34.8296608CurrentTrain: epoch  5, batch     0 | loss: 90.3474050CurrentTrain: epoch  5, batch     1 | loss: 60.5281410CurrentTrain: epoch  5, batch     2 | loss: 87.5435004CurrentTrain: epoch  5, batch     3 | loss: 74.6089604CurrentTrain: epoch  5, batch     4 | loss: 23.3100312CurrentTrain: epoch  6, batch     0 | loss: 72.6802963CurrentTrain: epoch  6, batch     1 | loss: 60.4017349CurrentTrain: epoch  6, batch     2 | loss: 74.1887915CurrentTrain: epoch  6, batch     3 | loss: 63.6857329CurrentTrain: epoch  6, batch     4 | loss: 40.3421606CurrentTrain: epoch  7, batch     0 | loss: 85.1736224CurrentTrain: epoch  7, batch     1 | loss: 69.4448875CurrentTrain: epoch  7, batch     2 | loss: 114.8258517CurrentTrain: epoch  7, batch     3 | loss: 71.5715294CurrentTrain: epoch  7, batch     4 | loss: 14.1724945CurrentTrain: epoch  8, batch     0 | loss: 85.5988143CurrentTrain: epoch  8, batch     1 | loss: 69.5910067CurrentTrain: epoch  8, batch     2 | loss: 72.3925382CurrentTrain: epoch  8, batch     3 | loss: 87.0947425CurrentTrain: epoch  8, batch     4 | loss: 7.6759363CurrentTrain: epoch  9, batch     0 | loss: 87.4816685CurrentTrain: epoch  9, batch     1 | loss: 55.8004268CurrentTrain: epoch  9, batch     2 | loss: 109.9957727CurrentTrain: epoch  9, batch     3 | loss: 67.9135478CurrentTrain: epoch  9, batch     4 | loss: 20.5430033
MemoryTrain:  epoch  0, batch     0 | loss: 0.7633558MemoryTrain:  epoch  1, batch     0 | loss: 0.7275088MemoryTrain:  epoch  2, batch     0 | loss: 0.6485372MemoryTrain:  epoch  3, batch     0 | loss: 0.4656064MemoryTrain:  epoch  4, batch     0 | loss: 0.4456688MemoryTrain:  epoch  5, batch     0 | loss: 0.3144281MemoryTrain:  epoch  6, batch     0 | loss: 0.3520100MemoryTrain:  epoch  7, batch     0 | loss: 0.2918533MemoryTrain:  epoch  8, batch     0 | loss: 0.2384522MemoryTrain:  epoch  9, batch     0 | loss: 0.1936664

F1 score per class: {0: 0.0, 2: 0.7, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.6375, 12: 0.6820809248554913, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 28: 0.2857142857142857, 29: 0.0, 32: 0.0, 33: 0.0, 39: 0.2857142857142857, 40: 0.0}
Micro-average F1 score: 0.5252100840336135
Weighted-average F1 score: 0.4401019191779634
F1 score per class: {0: 0.0, 2: 0.4, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.5070422535211268, 12: 0.6338797814207651, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 27: 0.0, 28: 0.2631578947368421, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.5454545454545454, 40: 0.0}
Micro-average F1 score: 0.38620689655172413
Weighted-average F1 score: 0.28435090513519656
F1 score per class: {0: 0.0, 2: 0.5185185185185185, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.52, 12: 0.6521739130434783, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 27: 0.0, 28: 0.2564102564102564, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.5217391304347826, 40: 0.0}
Micro-average F1 score: 0.42857142857142855
Weighted-average F1 score: 0.33333428792543274

F1 score per class: {0: 0.7586206896551724, 2: 0.4117647058823529, 4: 0.8255813953488372, 5: 0.774468085106383, 6: 0.4069767441860465, 7: 0.03076923076923077, 8: 0.3577981651376147, 9: 0.78125, 10: 0.05825242718446602, 11: 0.2991202346041056, 12: 0.3881578947368421, 13: 0.06451612903225806, 16: 0.5901639344262295, 17: 0.0, 18: 0.05128205128205128, 19: 0.6297872340425532, 20: 0.6206896551724138, 21: 0.3333333333333333, 23: 0.6590909090909091, 24: 0.09523809523809523, 26: 0.6455026455026455, 27: 0.28125, 28: 0.08849557522123894, 29: 0.7918781725888325, 30: 0.8333333333333334, 31: 0.16666666666666666, 32: 0.7295081967213115, 33: 0.3, 36: 0.2857142857142857, 39: 0.13953488372093023, 40: 0.1926605504587156}
Micro-average F1 score: 0.47941495124593714
Weighted-average F1 score: 0.4651058641222422
F1 score per class: {0: 0.5343511450381679, 2: 0.15217391304347827, 4: 0.8736842105263158, 5: 0.4801980198019802, 6: 0.45045045045045046, 7: 0.07228915662650602, 8: 0.2996941896024465, 9: 0.5555555555555556, 10: 0.23140495867768596, 11: 0.277992277992278, 12: 0.2878411910669975, 13: 0.0, 16: 0.42592592592592593, 17: 0.0, 18: 0.23880597014925373, 19: 0.5674740484429066, 20: 0.5714285714285714, 21: 0.26666666666666666, 23: 0.5742574257425742, 24: 0.13793103448275862, 26: 0.66, 27: 0.30434782608695654, 28: 0.10309278350515463, 29: 0.7794871794871795, 30: 0.5357142857142857, 31: 0.06060606060606061, 32: 0.7007874015748031, 33: 0.15789473684210525, 36: 0.45871559633027525, 39: 0.21052631578947367, 40: 0.11055276381909548}
Micro-average F1 score: 0.4255224544241885
Weighted-average F1 score: 0.40356640930006876
F1 score per class: {0: 0.6601941747572816, 2: 0.21875, 4: 0.9021739130434783, 5: 0.6213592233009708, 6: 0.44, 7: 0.07792207792207792, 8: 0.29375, 9: 0.7142857142857143, 10: 0.18181818181818182, 11: 0.2653061224489796, 12: 0.31007751937984496, 13: 0.0, 16: 0.47619047619047616, 17: 0.0, 18: 0.13043478260869565, 19: 0.587360594795539, 20: 0.5714285714285714, 21: 0.26785714285714285, 23: 0.5977011494252874, 24: 0.08, 26: 0.6428571428571429, 27: 0.32142857142857145, 28: 0.1, 29: 0.7794871794871795, 30: 0.7142857142857143, 31: 0.08333333333333333, 32: 0.7007874015748031, 33: 0.20689655172413793, 36: 0.3146067415730337, 39: 0.19672131147540983, 40: 0.125}
Micro-average F1 score: 0.4417324718832257
Weighted-average F1 score: 0.4214129490890698

F1 score per class: {0: 0.0, 2: 0.42424242424242425, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.5, 12: 0.5363636363636364, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 28: 0.125, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.23076923076923078, 40: 0.0}
Micro-average F1 score: 0.343878954607978
Weighted-average F1 score: 0.28408402621001044
F1 score per class: {0: 0.0, 2: 0.2857142857142857, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.4114285714285714, 12: 0.5, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.1282051282051282, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.42857142857142855, 40: 0.0}
Micro-average F1 score: 0.24534501642935377
Weighted-average F1 score: 0.18472221342538342
F1 score per class: {0: 0.0, 2: 0.358974358974359, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.4126984126984127, 12: 0.5128205128205128, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.1282051282051282, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.4, 40: 0.0}
Micro-average F1 score: 0.2727272727272727
Weighted-average F1 score: 0.2136840033553649

F1 score per class: {0: 0.5945945945945946, 2: 0.23333333333333334, 4: 0.7717391304347826, 5: 0.6319444444444444, 6: 0.2692307692307692, 7: 0.016129032258064516, 8: 0.21606648199445982, 9: 0.7246376811594203, 10: 0.057692307692307696, 11: 0.20238095238095238, 12: 0.1838006230529595, 13: 0.037037037037037035, 16: 0.33962264150943394, 17: 0.0, 18: 0.04878048780487805, 19: 0.592, 20: 0.32432432432432434, 21: 0.22764227642276422, 23: 0.5420560747663551, 24: 0.08333333333333333, 26: 0.580952380952381, 27: 0.1651376146788991, 28: 0.044444444444444446, 29: 0.6582278481012658, 30: 0.75, 31: 0.1, 32: 0.5250737463126843, 33: 0.21428571428571427, 36: 0.25, 39: 0.0759493670886076, 40: 0.15789473684210525}
Micro-average F1 score: 0.33497350492051475
Weighted-average F1 score: 0.3111889688427997
F1 score per class: {0: 0.39325842696629215, 2: 0.10687022900763359, 4: 0.8217821782178217, 5: 0.2966360856269113, 6: 0.2680965147453083, 7: 0.039735099337748346, 8: 0.17314487632508835, 9: 0.4132231404958678, 10: 0.2014388489208633, 11: 0.19889502762430938, 12: 0.15025906735751296, 13: 0.0, 16: 0.24598930481283424, 17: 0.0, 18: 0.14285714285714285, 19: 0.5256410256410257, 20: 0.3285024154589372, 21: 0.17647058823529413, 23: 0.40559440559440557, 24: 0.10256410256410256, 26: 0.5739130434782609, 27: 0.18421052631578946, 28: 0.05434782608695652, 29: 0.6229508196721312, 30: 0.38461538461538464, 31: 0.0392156862745098, 32: 0.5114942528735632, 33: 0.1111111111111111, 36: 0.2976190476190476, 39: 0.11214953271028037, 40: 0.08270676691729323}
Micro-average F1 score: 0.28431372549019607
Weighted-average F1 score: 0.2645978596839887
F1 score per class: {0: 0.49635036496350365, 2: 0.15217391304347827, 4: 0.8469387755102041, 5: 0.4, 6: 0.2674772036474164, 7: 0.04195804195804196, 8: 0.1649122807017544, 9: 0.6329113924050633, 10: 0.1694915254237288, 11: 0.18615751789976134, 12: 0.15645371577574968, 13: 0.0, 16: 0.28368794326241137, 17: 0.0, 18: 0.08571428571428572, 19: 0.5543859649122806, 20: 0.30638297872340425, 21: 0.1744186046511628, 23: 0.4369747899159664, 24: 0.05714285714285714, 26: 0.5727272727272728, 27: 0.1978021978021978, 28: 0.05405405405405406, 29: 0.6333333333333333, 30: 0.5555555555555556, 31: 0.05405405405405406, 32: 0.5085714285714286, 33: 0.14285714285714285, 36: 0.2204724409448819, 39: 0.10810810810810811, 40: 0.09688581314878893}
Micro-average F1 score: 0.29740615434187206
Weighted-average F1 score: 0.2756168313666786
cur_acc_wo_na:  ['0.7674', '0.5139', '0.3299', '0.6726', '0.5194', '0.5252']
his_acc_wo_na:  ['0.7674', '0.6641', '0.5027', '0.5120', '0.5216', '0.4794']
cur_acc des_wo_na:  ['0.7588', '0.5437', '0.3269', '0.6256', '0.5051', '0.3862']
his_acc des_wo_na:  ['0.7588', '0.6604', '0.5299', '0.4896', '0.4696', '0.4255']
cur_acc rrf_wo_na:  ['0.7699', '0.5724', '0.3434', '0.6294', '0.5394', '0.4286']
his_acc rrf_wo_na:  ['0.7699', '0.6790', '0.5433', '0.4978', '0.4968', '0.4417']
cur_acc_w_na:  ['0.6460', '0.3994', '0.2799', '0.4741', '0.3762', '0.3439']
his_acc_w_na:  ['0.6460', '0.5286', '0.3869', '0.3585', '0.3899', '0.3350']
cur_acc des_w_na:  ['0.6131', '0.3810', '0.2684', '0.4503', '0.3378', '0.2453']
his_acc des_w_na:  ['0.6131', '0.4877', '0.3969', '0.3469', '0.3280', '0.2843']
cur_acc rrf_w_na:  ['0.6236', '0.3981', '0.2857', '0.4542', '0.3666', '0.2727']
his_acc rrf_w_na:  ['0.6236', '0.5026', '0.4097', '0.3545', '0.3508', '0.2974']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 121.7583057CurrentTrain: epoch  0, batch     1 | loss: 101.6663166CurrentTrain: epoch  0, batch     2 | loss: 98.3539876CurrentTrain: epoch  0, batch     3 | loss: 88.8540317CurrentTrain: epoch  0, batch     4 | loss: 73.2684208CurrentTrain: epoch  1, batch     0 | loss: 69.8533729CurrentTrain: epoch  1, batch     1 | loss: 86.1633763CurrentTrain: epoch  1, batch     2 | loss: 85.5459339CurrentTrain: epoch  1, batch     3 | loss: 130.2229153CurrentTrain: epoch  1, batch     4 | loss: 46.7268010CurrentTrain: epoch  2, batch     0 | loss: 99.1916881CurrentTrain: epoch  2, batch     1 | loss: 97.1890256CurrentTrain: epoch  2, batch     2 | loss: 84.4739919CurrentTrain: epoch  2, batch     3 | loss: 67.4680287CurrentTrain: epoch  2, batch     4 | loss: 94.3599565CurrentTrain: epoch  3, batch     0 | loss: 94.1848701CurrentTrain: epoch  3, batch     1 | loss: 97.8646278CurrentTrain: epoch  3, batch     2 | loss: 78.6902401CurrentTrain: epoch  3, batch     3 | loss: 65.8050835CurrentTrain: epoch  3, batch     4 | loss: 51.2019511CurrentTrain: epoch  4, batch     0 | loss: 77.0633582CurrentTrain: epoch  4, batch     1 | loss: 74.0244590CurrentTrain: epoch  4, batch     2 | loss: 119.5647419CurrentTrain: epoch  4, batch     3 | loss: 79.1153243CurrentTrain: epoch  4, batch     4 | loss: 63.1922109CurrentTrain: epoch  5, batch     0 | loss: 92.8624672CurrentTrain: epoch  5, batch     1 | loss: 72.6082046CurrentTrain: epoch  5, batch     2 | loss: 93.5205957CurrentTrain: epoch  5, batch     3 | loss: 62.1994104CurrentTrain: epoch  5, batch     4 | loss: 51.8730684CurrentTrain: epoch  6, batch     0 | loss: 153.3815744CurrentTrain: epoch  6, batch     1 | loss: 63.5736870CurrentTrain: epoch  6, batch     2 | loss: 111.8798681CurrentTrain: epoch  6, batch     3 | loss: 60.9985034CurrentTrain: epoch  6, batch     4 | loss: 65.0527390CurrentTrain: epoch  7, batch     0 | loss: 89.4610127CurrentTrain: epoch  7, batch     1 | loss: 59.4375253CurrentTrain: epoch  7, batch     2 | loss: 88.2430086CurrentTrain: epoch  7, batch     3 | loss: 73.4556776CurrentTrain: epoch  7, batch     4 | loss: 64.0450367CurrentTrain: epoch  8, batch     0 | loss: 61.0798537CurrentTrain: epoch  8, batch     1 | loss: 70.2949005CurrentTrain: epoch  8, batch     2 | loss: 60.5044861CurrentTrain: epoch  8, batch     3 | loss: 150.0479736CurrentTrain: epoch  8, batch     4 | loss: 63.0017669CurrentTrain: epoch  9, batch     0 | loss: 70.2007976CurrentTrain: epoch  9, batch     1 | loss: 70.7259453CurrentTrain: epoch  9, batch     2 | loss: 112.8195031CurrentTrain: epoch  9, batch     3 | loss: 71.2250883CurrentTrain: epoch  9, batch     4 | loss: 60.4822116
MemoryTrain:  epoch  0, batch     0 | loss: 1.0608135MemoryTrain:  epoch  1, batch     0 | loss: 0.9269360MemoryTrain:  epoch  2, batch     0 | loss: 0.7840973MemoryTrain:  epoch  3, batch     0 | loss: 0.6471194MemoryTrain:  epoch  4, batch     0 | loss: 0.5152342MemoryTrain:  epoch  5, batch     0 | loss: 0.4245419MemoryTrain:  epoch  6, batch     0 | loss: 0.4261203MemoryTrain:  epoch  7, batch     0 | loss: 0.4042834MemoryTrain:  epoch  8, batch     0 | loss: 0.2886860MemoryTrain:  epoch  9, batch     0 | loss: 0.2562146

F1 score per class: {0: 0.0, 1: 0.2619047619047619, 2: 0.0, 3: 0.6378378378378379, 6: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.14084507042253522, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5855855855855856, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.717948717948718, 40: 0.0}
Micro-average F1 score: 0.39068825910931176
Weighted-average F1 score: 0.333946287382252
F1 score per class: {0: 0.0, 1: 0.24043715846994534, 3: 0.5758754863813229, 5: 0.0, 6: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.14736842105263157, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5182186234817814, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.7101449275362319, 36: 0.0, 40: 0.0}
Micro-average F1 score: 0.3500810372771475
Weighted-average F1 score: 0.3077242420607406
F1 score per class: {0: 0.0, 1: 0.25, 2: 0.0, 3: 0.5691699604743083, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.16666666666666666, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5375494071146245, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.6956521739130435, 36: 0.0, 40: 0.0}
Micro-average F1 score: 0.3694915254237288
Weighted-average F1 score: 0.3297239183167187

F1 score per class: {0: 0.7837837837837838, 1: 0.19469026548672566, 2: 0.4117647058823529, 3: 0.4125874125874126, 4: 0.7904191616766467, 5: 0.7896995708154506, 6: 0.35555555555555557, 7: 0.02857142857142857, 8: 0.33497536945812806, 9: 0.7540983606557377, 10: 0.09433962264150944, 11: 0.10572687224669604, 12: 0.35684647302904565, 13: 0.046511627906976744, 14: 0.12345679012345678, 16: 0.5660377358490566, 17: 0.0, 18: 0.05128205128205128, 19: 0.5620437956204379, 20: 0.5370370370370371, 21: 0.057971014492753624, 22: 0.4659498207885305, 23: 0.55, 24: 0.0, 26: 0.6494845360824743, 27: 0.0, 28: 0.05405405405405406, 29: 0.7609756097560976, 30: 0.8571428571428571, 31: 0.3333333333333333, 32: 0.6083650190114068, 33: 0.21052631578947367, 34: 0.22459893048128343, 36: 0.23076923076923078, 39: 0.125, 40: 0.1762114537444934}
Micro-average F1 score: 0.4
Weighted-average F1 score: 0.38236326957630495
F1 score per class: {0: 0.5038167938931297, 1: 0.16988416988416988, 2: 0.16091954022988506, 3: 0.34579439252336447, 4: 0.7976190476190477, 5: 0.5277777777777778, 6: 0.46788990825688076, 7: 0.030303030303030304, 8: 0.27647058823529413, 9: 0.5617977528089888, 10: 0.1951219512195122, 11: 0.10526315789473684, 12: 0.29362880886426596, 13: 0.034482758620689655, 14: 0.11382113821138211, 16: 0.5238095238095238, 17: 0.0, 18: 0.041666666666666664, 19: 0.5211726384364821, 20: 0.46956521739130436, 21: 0.1553398058252427, 22: 0.40764331210191085, 23: 0.5523809523809524, 24: 0.09302325581395349, 26: 0.6600985221674877, 27: 0.0, 28: 0.0847457627118644, 29: 0.7422680412371134, 30: 0.7111111111111111, 31: 0.07142857142857142, 32: 0.5704697986577181, 33: 0.10344827586206896, 34: 0.2076271186440678, 36: 0.39655172413793105, 39: 0.2857142857142857, 40: 0.17647058823529413}
Micro-average F1 score: 0.36470588235294116
Weighted-average F1 score: 0.34696960031367535
F1 score per class: {0: 0.673469387755102, 1: 0.17037037037037037, 2: 0.21212121212121213, 3: 0.3469879518072289, 4: 0.8165680473372781, 5: 0.6270627062706271, 6: 0.43781094527363185, 7: 0.029850746268656716, 8: 0.2987012987012987, 9: 0.7575757575757576, 10: 0.17543859649122806, 11: 0.09852216748768473, 12: 0.31268436578171094, 13: 0.03773584905660377, 14: 0.1346153846153846, 16: 0.5714285714285714, 17: 0.0, 18: 0.047619047619047616, 19: 0.5273972602739726, 20: 0.4909090909090909, 21: 0.08247422680412371, 22: 0.4171779141104294, 23: 0.5531914893617021, 24: 0.08, 26: 0.66, 27: 0.0, 28: 0.07246376811594203, 29: 0.7384615384615385, 30: 0.7894736842105263, 31: 0.08695652173913043, 32: 0.5850340136054422, 33: 0.1875, 34: 0.20125786163522014, 36: 0.3218390804597701, 39: 0.22857142857142856, 40: 0.1693548387096774}
Micro-average F1 score: 0.37506678539626004
Weighted-average F1 score: 0.3551598577205461

F1 score per class: {0: 0.0, 1: 0.14426229508196722, 2: 0.0, 3: 0.44696969696969696, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.12987012987012986, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4744525547445255, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5562913907284768, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2511385816525699
Weighted-average F1 score: 0.21516558948778178
F1 score per class: {0: 0.0, 1: 0.13333333333333333, 2: 0.0, 3: 0.3854166666666667, 5: 0.0, 6: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.11864406779661017, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.42524916943521596, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.532608695652174, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.22268041237113403
Weighted-average F1 score: 0.19746702520907058
F1 score per class: {0: 0.0, 1: 0.1377245508982036, 2: 0.0, 3: 0.3923705722070845, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.1346153846153846, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.43729903536977494, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5245901639344263, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.23466092572658773
Weighted-average F1 score: 0.2091334761980022

F1 score per class: {0: 0.6105263157894737, 1: 0.10328638497652583, 2: 0.22950819672131148, 3: 0.2318271119842829, 4: 0.7415730337078652, 5: 0.5841269841269842, 6: 0.24150943396226415, 7: 0.014598540145985401, 8: 0.2054380664652568, 9: 0.696969696969697, 10: 0.08849557522123894, 11: 0.08888888888888889, 12: 0.15693430656934307, 13: 0.02197802197802198, 14: 0.1111111111111111, 16: 0.38961038961038963, 17: 0.0, 18: 0.046511627906976744, 19: 0.5310344827586206, 20: 0.3005181347150259, 21: 0.047619047619047616, 22: 0.3448275862068966, 23: 0.44, 24: 0.0, 26: 0.5806451612903226, 27: 0.0, 28: 0.027932960893854747, 29: 0.6141732283464567, 30: 0.8108108108108109, 31: 0.16666666666666666, 32: 0.44321329639889195, 33: 0.15384615384615385, 34: 0.12982998454404945, 36: 0.1836734693877551, 39: 0.05970149253731343, 40: 0.13157894736842105}
Micro-average F1 score: 0.2705981941309255
Weighted-average F1 score: 0.24946532241187216
F1 score per class: {0: 0.38596491228070173, 1: 0.09224318658280922, 2: 0.11475409836065574, 3: 0.20498614958448755, 4: 0.7570621468926554, 5: 0.31932773109243695, 6: 0.2809917355371901, 7: 0.015873015873015872, 8: 0.15614617940199335, 9: 0.44642857142857145, 10: 0.17391304347826086, 11: 0.09049773755656108, 12: 0.14058355437665782, 13: 0.018518518518518517, 14: 0.0915032679738562, 16: 0.32592592592592595, 17: 0.0, 18: 0.034482758620689655, 19: 0.47904191616766467, 20: 0.26732673267326734, 21: 0.1095890410958904, 22: 0.3121951219512195, 23: 0.38926174496644295, 24: 0.06779661016949153, 26: 0.5726495726495726, 27: 0.0, 28: 0.045454545454545456, 29: 0.6206896551724138, 30: 0.5333333333333333, 31: 0.03773584905660377, 32: 0.41362530413625304, 33: 0.07058823529411765, 34: 0.11936662606577345, 36: 0.25274725274725274, 39: 0.13513513513513514, 40: 0.12883435582822086}
Micro-average F1 score: 0.23919753086419754
Weighted-average F1 score: 0.2230592667167297
F1 score per class: {0: 0.4925373134328358, 1: 0.09349593495934959, 2: 0.14285714285714285, 3: 0.20396600566572237, 4: 0.7752808988764045, 5: 0.4033970276008493, 6: 0.2691131498470948, 7: 0.016260162601626018, 8: 0.17228464419475656, 9: 0.6944444444444444, 10: 0.15873015873015872, 11: 0.08163265306122448, 12: 0.1444141689373297, 13: 0.019230769230769232, 14: 0.10526315789473684, 16: 0.3673469387755102, 17: 0.0, 18: 0.0425531914893617, 19: 0.4935897435897436, 20: 0.25961538461538464, 21: 0.06722689075630252, 22: 0.30357142857142855, 23: 0.40310077519379844, 24: 0.0625, 26: 0.584070796460177, 27: 0.0, 28: 0.03937007874015748, 29: 0.6101694915254238, 30: 0.6521739130434783, 31: 0.047619047619047616, 32: 0.428927680798005, 33: 0.13043478260869565, 34: 0.11510791366906475, 36: 0.21875, 39: 0.10810810810810811, 40: 0.1253731343283582}
Micro-average F1 score: 0.24689331770222744
Weighted-average F1 score: 0.22829692490860587
cur_acc_wo_na:  ['0.7674', '0.5139', '0.3299', '0.6726', '0.5194', '0.5252', '0.3907']
his_acc_wo_na:  ['0.7674', '0.6641', '0.5027', '0.5120', '0.5216', '0.4794', '0.4000']
cur_acc des_wo_na:  ['0.7588', '0.5437', '0.3269', '0.6256', '0.5051', '0.3862', '0.3501']
his_acc des_wo_na:  ['0.7588', '0.6604', '0.5299', '0.4896', '0.4696', '0.4255', '0.3647']
cur_acc rrf_wo_na:  ['0.7699', '0.5724', '0.3434', '0.6294', '0.5394', '0.4286', '0.3695']
his_acc rrf_wo_na:  ['0.7699', '0.6790', '0.5433', '0.4978', '0.4968', '0.4417', '0.3751']
cur_acc_w_na:  ['0.6460', '0.3994', '0.2799', '0.4741', '0.3762', '0.3439', '0.2511']
his_acc_w_na:  ['0.6460', '0.5286', '0.3869', '0.3585', '0.3899', '0.3350', '0.2706']
cur_acc des_w_na:  ['0.6131', '0.3810', '0.2684', '0.4503', '0.3378', '0.2453', '0.2227']
his_acc des_w_na:  ['0.6131', '0.4877', '0.3969', '0.3469', '0.3280', '0.2843', '0.2392']
cur_acc rrf_w_na:  ['0.6236', '0.3981', '0.2857', '0.4542', '0.3666', '0.2727', '0.2347']
his_acc rrf_w_na:  ['0.6236', '0.5026', '0.4097', '0.3545', '0.3508', '0.2974', '0.2469']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 103.2824683CurrentTrain: epoch  0, batch     1 | loss: 116.4406463CurrentTrain: epoch  0, batch     2 | loss: 114.6479891CurrentTrain: epoch  0, batch     3 | loss: 52.1600079CurrentTrain: epoch  1, batch     0 | loss: 79.4716436CurrentTrain: epoch  1, batch     1 | loss: 104.6352055CurrentTrain: epoch  1, batch     2 | loss: 79.1497743CurrentTrain: epoch  1, batch     3 | loss: 73.9769601CurrentTrain: epoch  2, batch     0 | loss: 72.0256458CurrentTrain: epoch  2, batch     1 | loss: 81.0168756CurrentTrain: epoch  2, batch     2 | loss: 97.8244410CurrentTrain: epoch  2, batch     3 | loss: 50.6237819CurrentTrain: epoch  3, batch     0 | loss: 120.9930606CurrentTrain: epoch  3, batch     1 | loss: 62.4933228CurrentTrain: epoch  3, batch     2 | loss: 75.7461419CurrentTrain: epoch  3, batch     3 | loss: 64.7539973CurrentTrain: epoch  4, batch     0 | loss: 96.8402267CurrentTrain: epoch  4, batch     1 | loss: 115.0686847CurrentTrain: epoch  4, batch     2 | loss: 59.7162108CurrentTrain: epoch  4, batch     3 | loss: 42.4584484CurrentTrain: epoch  5, batch     0 | loss: 78.8073376CurrentTrain: epoch  5, batch     1 | loss: 70.1729019CurrentTrain: epoch  5, batch     2 | loss: 88.8970370CurrentTrain: epoch  5, batch     3 | loss: 60.3825561CurrentTrain: epoch  6, batch     0 | loss: 59.7384667CurrentTrain: epoch  6, batch     1 | loss: 63.4063892CurrentTrain: epoch  6, batch     2 | loss: 69.3722986CurrentTrain: epoch  6, batch     3 | loss: 62.9181915CurrentTrain: epoch  7, batch     0 | loss: 71.7799058CurrentTrain: epoch  7, batch     1 | loss: 91.6225200CurrentTrain: epoch  7, batch     2 | loss: 69.9305116CurrentTrain: epoch  7, batch     3 | loss: 39.7077749CurrentTrain: epoch  8, batch     0 | loss: 68.7169251CurrentTrain: epoch  8, batch     1 | loss: 69.1652726CurrentTrain: epoch  8, batch     2 | loss: 84.9608956CurrentTrain: epoch  8, batch     3 | loss: 75.4572209CurrentTrain: epoch  9, batch     0 | loss: 69.9949820CurrentTrain: epoch  9, batch     1 | loss: 68.3966613CurrentTrain: epoch  9, batch     2 | loss: 89.2373476CurrentTrain: epoch  9, batch     3 | loss: 44.4704616
MemoryTrain:  epoch  0, batch     0 | loss: 0.9009744MemoryTrain:  epoch  1, batch     0 | loss: 0.8109110MemoryTrain:  epoch  2, batch     0 | loss: 0.6261858MemoryTrain:  epoch  3, batch     0 | loss: 0.5365579MemoryTrain:  epoch  4, batch     0 | loss: 0.5123278MemoryTrain:  epoch  5, batch     0 | loss: 0.4265784MemoryTrain:  epoch  6, batch     0 | loss: 0.2872128MemoryTrain:  epoch  7, batch     0 | loss: 0.2866345MemoryTrain:  epoch  8, batch     0 | loss: 0.2373957MemoryTrain:  epoch  9, batch     0 | loss: 0.2177268

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6956521739130435, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.45714285714285713, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7111111111111111, 36: 0.0, 37: 0.43636363636363634, 38: 0.5714285714285714, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3709677419354839
Weighted-average F1 score: 0.2536392028506988
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.6, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5647058823529412, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.8163265306122449, 36: 0.0, 37: 0.25, 38: 0.625, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.31869918699186994
Weighted-average F1 score: 0.20894357743097242
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.6, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5365853658536586, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.8041237113402062, 36: 0.0, 37: 0.2803738317757009, 38: 0.6382978723404256, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3368055555555556
Weighted-average F1 score: 0.2250466169738274

F1 score per class: {0: 0.7294117647058823, 1: 0.1746031746031746, 2: 0.358974358974359, 3: 0.20588235294117646, 4: 0.8323699421965318, 5: 0.6863468634686347, 6: 0.3553299492385787, 7: 0.02631578947368421, 8: 0.35294117647058826, 9: 0.7301587301587301, 10: 0.14414414414414414, 11: 0.10404624277456648, 12: 0.30278884462151395, 13: 0.03125, 14: 0.12658227848101267, 15: 0.17204301075268819, 16: 0.6037735849056604, 17: 0.0, 18: 0.045454545454545456, 19: 0.5310344827586206, 20: 0.4132231404958678, 21: 0.05263157894736842, 22: 0.44, 23: 0.5432098765432098, 24: 0.0, 25: 0.4507042253521127, 26: 0.6464646464646465, 27: 0.0, 28: 0.055248618784530384, 29: 0.7403846153846154, 30: 0.8333333333333334, 31: 0.13333333333333333, 32: 0.5588235294117647, 33: 0.1935483870967742, 34: 0.2413793103448276, 35: 0.23703703703703705, 36: 0.10810810810810811, 37: 0.09856262833675565, 38: 0.21238938053097345, 39: 0.10526315789473684, 40: 0.14173228346456693}
Micro-average F1 score: 0.3402672078471165
Weighted-average F1 score: 0.316542185122098
F1 score per class: {0: 0.4230769230769231, 1: 0.1568627450980392, 2: 0.14414414414414414, 3: 0.3125, 4: 0.7790697674418605, 5: 0.47002398081534774, 6: 0.4018264840182648, 7: 0.021505376344086023, 8: 0.3028169014084507, 9: 0.47619047619047616, 10: 0.16923076923076924, 11: 0.09467455621301775, 12: 0.2932551319648094, 13: 0.022988505747126436, 14: 0.125, 15: 0.2727272727272727, 16: 0.5454545454545454, 17: 0.0, 18: 0.038461538461538464, 19: 0.5015873015873016, 20: 0.35185185185185186, 21: 0.17142857142857143, 22: 0.39759036144578314, 23: 0.5360824742268041, 24: 0.058823529411764705, 25: 0.5333333333333333, 26: 0.6473429951690821, 27: 0.0, 28: 0.062111801242236024, 29: 0.7411167512690355, 30: 0.6808510638297872, 31: 0.07692307692307693, 32: 0.5625, 33: 0.10714285714285714, 34: 0.21782178217821782, 35: 0.3225806451612903, 36: 0.45454545454545453, 37: 0.05405405405405406, 38: 0.24193548387096775, 39: 0.1935483870967742, 40: 0.1752988047808765}
Micro-average F1 score: 0.32743619489559167
Weighted-average F1 score: 0.30630363610686384
F1 score per class: {0: 0.6285714285714286, 1: 0.16091954022988506, 2: 0.17721518987341772, 3: 0.3058103975535168, 4: 0.7904191616766467, 5: 0.5597667638483965, 6: 0.3886255924170616, 7: 0.022727272727272728, 8: 0.3173431734317343, 9: 0.7246376811594203, 10: 0.13675213675213677, 11: 0.0972972972972973, 12: 0.2831858407079646, 13: 0.023255813953488372, 14: 0.1414141414141414, 15: 0.2033898305084746, 16: 0.59375, 17: 0.0, 18: 0.04, 19: 0.5016077170418006, 20: 0.37168141592920356, 21: 0.08247422680412371, 22: 0.38285714285714284, 23: 0.5333333333333333, 24: 0.07407407407407407, 25: 0.5057471264367817, 26: 0.6600985221674877, 27: 0.0, 28: 0.05813953488372093, 29: 0.7373737373737373, 30: 0.7894736842105263, 31: 0.08695652173913043, 32: 0.5818181818181818, 33: 0.125, 34: 0.22564102564102564, 35: 0.308300395256917, 36: 0.1951219512195122, 37: 0.06172839506172839, 38: 0.22900763358778625, 39: 0.12121212121212122, 40: 0.16793893129770993}
Micro-average F1 score: 0.32904884318766064
Weighted-average F1 score: 0.30655182849374446

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.4444444444444444, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.4266666666666667, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6095238095238096, 36: 0.0, 37: 0.366412213740458, 38: 0.43636363636363634, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.24764468371467024
Weighted-average F1 score: 0.16988977852502826
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.4444444444444444, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5106382978723404, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7142857142857143, 36: 0.0, 37: 0.2222222222222222, 38: 0.5454545454545454, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2153846153846154
Weighted-average F1 score: 0.1409982116101013
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.4444444444444444, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.4943820224719101, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6964285714285714, 36: 0.0, 37: 0.24, 38: 0.5660377358490566, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.22877358490566038
Weighted-average F1 score: 0.15159316327725736

F1 score per class: {0: 0.543859649122807, 1: 0.09205020920502092, 2: 0.208955223880597, 3: 0.12416851441241686, 4: 0.7783783783783784, 5: 0.48186528497409326, 6: 0.23026315789473684, 7: 0.012422360248447204, 8: 0.21606648199445982, 9: 0.6666666666666666, 10: 0.13559322033898305, 11: 0.09523809523809523, 12: 0.13194444444444445, 13: 0.01680672268907563, 14: 0.10989010989010989, 15: 0.08602150537634409, 16: 0.4155844155844156, 17: 0.0, 18: 0.0392156862745098, 19: 0.4827586206896552, 20: 0.22522522522522523, 21: 0.044444444444444446, 22: 0.3384615384615385, 23: 0.4536082474226804, 24: 0.0, 25: 0.41025641025641024, 26: 0.5688888888888889, 27: 0.0, 28: 0.02824858757062147, 29: 0.5789473684210527, 30: 0.7317073170731707, 31: 0.07142857142857142, 32: 0.428169014084507, 33: 0.14285714285714285, 34: 0.13592233009708737, 35: 0.13445378151260504, 36: 0.09523809523809523, 37: 0.06241872561768531, 38: 0.11538461538461539, 39: 0.06557377049180328, 40: 0.1111111111111111}
Micro-average F1 score: 0.2266531485862341
Weighted-average F1 score: 0.20535606295722447
F1 score per class: {0: 0.30985915492957744, 1: 0.0823045267489712, 2: 0.09876543209876543, 3: 0.18587360594795538, 4: 0.7403314917127072, 5: 0.2792022792022792, 6: 0.23978201634877383, 7: 0.01092896174863388, 8: 0.17373737373737375, 9: 0.3546099290780142, 10: 0.14473684210526316, 11: 0.0855614973262032, 12: 0.1282051282051282, 13: 0.014285714285714285, 14: 0.10144927536231885, 15: 0.19672131147540983, 16: 0.3356643356643357, 17: 0.0, 18: 0.03076923076923077, 19: 0.441340782122905, 20: 0.21348314606741572, 21: 0.11842105263157894, 22: 0.29596412556053814, 23: 0.36879432624113473, 24: 0.0425531914893617, 25: 0.4752475247524752, 26: 0.5560165975103735, 27: 0.0, 28: 0.033444816053511704, 29: 0.6186440677966102, 30: 0.5, 31: 0.0425531914893617, 32: 0.413265306122449, 33: 0.0594059405940594, 34: 0.12772133526850507, 35: 0.2, 36: 0.26737967914438504, 37: 0.03672316384180791, 38: 0.14423076923076922, 39: 0.10714285714285714, 40: 0.12290502793296089}
Micro-average F1 score: 0.21326029467321497
Weighted-average F1 score: 0.197393377403726
F1 score per class: {0: 0.4489795918367347, 1: 0.08349900596421471, 2: 0.11666666666666667, 3: 0.18587360594795538, 4: 0.75, 5: 0.3404255319148936, 6: 0.23563218390804597, 7: 0.011560693641618497, 8: 0.18181818181818182, 9: 0.625, 10: 0.12030075187969924, 11: 0.08571428571428572, 12: 0.1213653603034134, 13: 0.014388489208633094, 14: 0.11290322580645161, 15: 0.13333333333333333, 16: 0.36538461538461536, 17: 0.0, 18: 0.03389830508474576, 19: 0.4495677233429395, 20: 0.21761658031088082, 21: 0.057971014492753624, 22: 0.2815126050420168, 23: 0.39344262295081966, 24: 0.06060606060606061, 25: 0.4583333333333333, 26: 0.5726495726495726, 27: 0.0, 28: 0.031446540880503145, 29: 0.6134453781512605, 30: 0.6382978723404256, 31: 0.05128205128205128, 32: 0.425531914893617, 33: 0.08, 34: 0.12960235640648013, 35: 0.18932038834951456, 36: 0.1415929203539823, 37: 0.04120879120879121, 38: 0.13274336283185842, 39: 0.07272727272727272, 40: 0.12021857923497267}
Micro-average F1 score: 0.2152750296794618
Weighted-average F1 score: 0.1975135047074073
cur_acc_wo_na:  ['0.7674', '0.5139', '0.3299', '0.6726', '0.5194', '0.5252', '0.3907', '0.3710']
his_acc_wo_na:  ['0.7674', '0.6641', '0.5027', '0.5120', '0.5216', '0.4794', '0.4000', '0.3403']
cur_acc des_wo_na:  ['0.7588', '0.5437', '0.3269', '0.6256', '0.5051', '0.3862', '0.3501', '0.3187']
his_acc des_wo_na:  ['0.7588', '0.6604', '0.5299', '0.4896', '0.4696', '0.4255', '0.3647', '0.3274']
cur_acc rrf_wo_na:  ['0.7699', '0.5724', '0.3434', '0.6294', '0.5394', '0.4286', '0.3695', '0.3368']
his_acc rrf_wo_na:  ['0.7699', '0.6790', '0.5433', '0.4978', '0.4968', '0.4417', '0.3751', '0.3290']
cur_acc_w_na:  ['0.6460', '0.3994', '0.2799', '0.4741', '0.3762', '0.3439', '0.2511', '0.2476']
his_acc_w_na:  ['0.6460', '0.5286', '0.3869', '0.3585', '0.3899', '0.3350', '0.2706', '0.2267']
cur_acc des_w_na:  ['0.6131', '0.3810', '0.2684', '0.4503', '0.3378', '0.2453', '0.2227', '0.2154']
his_acc des_w_na:  ['0.6131', '0.4877', '0.3969', '0.3469', '0.3280', '0.2843', '0.2392', '0.2133']
cur_acc rrf_w_na:  ['0.6236', '0.3981', '0.2857', '0.4542', '0.3666', '0.2727', '0.2347', '0.2288']
his_acc rrf_w_na:  ['0.6236', '0.5026', '0.4097', '0.3545', '0.3508', '0.2974', '0.2469', '0.2153']
----------END
his_acc mean_wo_na:  [0.76   0.6398 0.5523 0.5109 0.4544 0.4343 0.3927 0.3708]
his_acc des mean_wo_na:  [0.7501 0.6158 0.5413 0.4966 0.4344 0.4091 0.3736 0.3511]
his_acc rrf mean_wo_na:  [0.7592 0.6302 0.5525 0.5021 0.4438 0.4197 0.3806 0.3592]
his_acc mean_w_na:  [0.6234 0.4929 0.4085 0.372  0.3199 0.3075 0.27   0.2577]
his_acc des mean_w_na:  [0.6044 0.4511 0.3898 0.3485 0.2943 0.2768 0.251  0.2343]
his_acc rrf mean_w_na:  [0.6134 0.4642 0.4007 0.3557 0.3029 0.2865 0.2576 0.2422]
