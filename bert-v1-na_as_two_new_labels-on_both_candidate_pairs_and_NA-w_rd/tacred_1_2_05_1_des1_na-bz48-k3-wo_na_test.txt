#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 99.0717339
CurrentTrain: epoch  0, batch     1 | loss: 68.3246431
CurrentTrain: epoch  0, batch     2 | loss: 60.3929436
CurrentTrain: epoch  0, batch     3 | loss: 66.6118940
CurrentTrain: epoch  0, batch     4 | loss: 68.3647400
CurrentTrain: epoch  0, batch     5 | loss: 66.8946286
CurrentTrain: epoch  0, batch     6 | loss: 78.4852241
CurrentTrain: epoch  0, batch     7 | loss: 77.6272132
CurrentTrain: epoch  0, batch     8 | loss: 65.1001775
CurrentTrain: epoch  0, batch     9 | loss: 66.5996897
CurrentTrain: epoch  0, batch    10 | loss: 56.9361311
CurrentTrain: epoch  0, batch    11 | loss: 77.5070448
CurrentTrain: epoch  0, batch    12 | loss: 78.4222475
CurrentTrain: epoch  0, batch    13 | loss: 189.0484587
CurrentTrain: epoch  0, batch    14 | loss: 77.7160093
CurrentTrain: epoch  0, batch    15 | loss: 64.9459832
CurrentTrain: epoch  0, batch    16 | loss: 64.9088054
CurrentTrain: epoch  0, batch    17 | loss: 94.0387614
CurrentTrain: epoch  0, batch    18 | loss: 77.4481291
CurrentTrain: epoch  0, batch    19 | loss: 65.1917801
CurrentTrain: epoch  0, batch    20 | loss: 126.0952841
CurrentTrain: epoch  0, batch    21 | loss: 127.1528922
CurrentTrain: epoch  0, batch    22 | loss: 187.6291775
CurrentTrain: epoch  0, batch    23 | loss: 126.5281543
CurrentTrain: epoch  0, batch    24 | loss: 76.8004156
CurrentTrain: epoch  0, batch    25 | loss: 188.7662329
CurrentTrain: epoch  0, batch    26 | loss: 64.5336170
CurrentTrain: epoch  0, batch    27 | loss: 97.5983148
CurrentTrain: epoch  0, batch    28 | loss: 126.3324073
CurrentTrain: epoch  0, batch    29 | loss: 77.2288157
CurrentTrain: epoch  0, batch    30 | loss: 125.7219041
CurrentTrain: epoch  0, batch    31 | loss: 125.8927488
CurrentTrain: epoch  0, batch    32 | loss: 94.8769166
CurrentTrain: epoch  0, batch    33 | loss: 64.4749408
CurrentTrain: epoch  0, batch    34 | loss: 126.7004462
CurrentTrain: epoch  0, batch    35 | loss: 64.3845999
CurrentTrain: epoch  0, batch    36 | loss: 63.8105785
CurrentTrain: epoch  0, batch    37 | loss: 77.0620044
CurrentTrain: epoch  0, batch    38 | loss: 76.0290141
CurrentTrain: epoch  0, batch    39 | loss: 63.7510002
CurrentTrain: epoch  0, batch    40 | loss: 93.9211060
CurrentTrain: epoch  0, batch    41 | loss: 64.4754545
CurrentTrain: epoch  0, batch    42 | loss: 64.3790766
CurrentTrain: epoch  0, batch    43 | loss: 56.2905030
CurrentTrain: epoch  0, batch    44 | loss: 76.5568232
CurrentTrain: epoch  0, batch    45 | loss: 64.2227827
CurrentTrain: epoch  0, batch    46 | loss: 94.2749491
CurrentTrain: epoch  0, batch    47 | loss: 64.7728257
CurrentTrain: epoch  0, batch    48 | loss: 76.3033766
CurrentTrain: epoch  0, batch    49 | loss: 76.4822318
CurrentTrain: epoch  0, batch    50 | loss: 75.9524157
CurrentTrain: epoch  0, batch    51 | loss: 76.3414176
CurrentTrain: epoch  0, batch    52 | loss: 76.1653165
CurrentTrain: epoch  0, batch    53 | loss: 125.4117957
CurrentTrain: epoch  0, batch    54 | loss: 94.7404126
CurrentTrain: epoch  0, batch    55 | loss: 63.6122928
CurrentTrain: epoch  0, batch    56 | loss: 64.0233506
CurrentTrain: epoch  0, batch    57 | loss: 75.9528338
CurrentTrain: epoch  0, batch    58 | loss: 76.3963200
CurrentTrain: epoch  0, batch    59 | loss: 75.2072835
CurrentTrain: epoch  0, batch    60 | loss: 75.5007516
CurrentTrain: epoch  0, batch    61 | loss: 75.7821294
CurrentTrain: epoch  0, batch    62 | loss: 62.9361601
CurrentTrain: epoch  0, batch    63 | loss: 94.4887790
CurrentTrain: epoch  0, batch    64 | loss: 76.7753936
CurrentTrain: epoch  0, batch    65 | loss: 55.7058319
CurrentTrain: epoch  0, batch    66 | loss: 74.4570540
CurrentTrain: epoch  0, batch    67 | loss: 75.9638666
CurrentTrain: epoch  0, batch    68 | loss: 63.1779019
CurrentTrain: epoch  0, batch    69 | loss: 125.2016352
CurrentTrain: epoch  0, batch    70 | loss: 94.6588768
CurrentTrain: epoch  0, batch    71 | loss: 63.5497674
CurrentTrain: epoch  0, batch    72 | loss: 75.9080570
CurrentTrain: epoch  0, batch    73 | loss: 92.5703476
CurrentTrain: epoch  0, batch    74 | loss: 125.1402452
CurrentTrain: epoch  0, batch    75 | loss: 75.9308202
CurrentTrain: epoch  0, batch    76 | loss: 91.0465729
CurrentTrain: epoch  0, batch    77 | loss: 74.2127374
CurrentTrain: epoch  0, batch    78 | loss: 92.2035862
CurrentTrain: epoch  0, batch    79 | loss: 92.4732564
CurrentTrain: epoch  0, batch    80 | loss: 61.0365240
CurrentTrain: epoch  0, batch    81 | loss: 73.7346678
CurrentTrain: epoch  0, batch    82 | loss: 93.9142809
CurrentTrain: epoch  0, batch    83 | loss: 75.5147144
CurrentTrain: epoch  0, batch    84 | loss: 89.9728495
CurrentTrain: epoch  0, batch    85 | loss: 63.3118555
CurrentTrain: epoch  0, batch    86 | loss: 122.3811490
CurrentTrain: epoch  0, batch    87 | loss: 62.1899614
CurrentTrain: epoch  0, batch    88 | loss: 62.3969128
CurrentTrain: epoch  0, batch    89 | loss: 54.1166502
CurrentTrain: epoch  0, batch    90 | loss: 73.5797262
CurrentTrain: epoch  0, batch    91 | loss: 62.5559627
CurrentTrain: epoch  0, batch    92 | loss: 73.3436749
CurrentTrain: epoch  0, batch    93 | loss: 120.3566015
CurrentTrain: epoch  0, batch    94 | loss: 62.7520737
CurrentTrain: epoch  0, batch    95 | loss: 77.9444012
CurrentTrain: epoch  1, batch     0 | loss: 60.6085949
CurrentTrain: epoch  1, batch     1 | loss: 74.1182280
CurrentTrain: epoch  1, batch     2 | loss: 62.9817913
CurrentTrain: epoch  1, batch     3 | loss: 60.9403573
CurrentTrain: epoch  1, batch     4 | loss: 70.6212517
CurrentTrain: epoch  1, batch     5 | loss: 57.0725123
CurrentTrain: epoch  1, batch     6 | loss: 75.4064244
CurrentTrain: epoch  1, batch     7 | loss: 120.5444956
CurrentTrain: epoch  1, batch     8 | loss: 51.6317314
CurrentTrain: epoch  1, batch     9 | loss: 90.7609994
CurrentTrain: epoch  1, batch    10 | loss: 72.1517449
CurrentTrain: epoch  1, batch    11 | loss: 59.7808496
CurrentTrain: epoch  1, batch    12 | loss: 58.2920550
CurrentTrain: epoch  1, batch    13 | loss: 91.6090990
CurrentTrain: epoch  1, batch    14 | loss: 87.0204285
CurrentTrain: epoch  1, batch    15 | loss: 73.7465399
CurrentTrain: epoch  1, batch    16 | loss: 118.6273847
CurrentTrain: epoch  1, batch    17 | loss: 58.3796485
CurrentTrain: epoch  1, batch    18 | loss: 67.9504642
CurrentTrain: epoch  1, batch    19 | loss: 89.5399388
CurrentTrain: epoch  1, batch    20 | loss: 123.5050438
CurrentTrain: epoch  1, batch    21 | loss: 58.9292464
CurrentTrain: epoch  1, batch    22 | loss: 58.9138349
CurrentTrain: epoch  1, batch    23 | loss: 72.2397404
CurrentTrain: epoch  1, batch    24 | loss: 122.4573415
CurrentTrain: epoch  1, batch    25 | loss: 72.9968481
CurrentTrain: epoch  1, batch    26 | loss: 50.8678595
CurrentTrain: epoch  1, batch    27 | loss: 59.8642532
CurrentTrain: epoch  1, batch    28 | loss: 50.8893924
CurrentTrain: epoch  1, batch    29 | loss: 51.8987983
CurrentTrain: epoch  1, batch    30 | loss: 71.0878238
CurrentTrain: epoch  1, batch    31 | loss: 73.1530718
CurrentTrain: epoch  1, batch    32 | loss: 92.4319705
CurrentTrain: epoch  1, batch    33 | loss: 71.3523258
CurrentTrain: epoch  1, batch    34 | loss: 82.1718610
CurrentTrain: epoch  1, batch    35 | loss: 123.7680147
CurrentTrain: epoch  1, batch    36 | loss: 58.9572062
CurrentTrain: epoch  1, batch    37 | loss: 69.5220183
CurrentTrain: epoch  1, batch    38 | loss: 69.2196251
CurrentTrain: epoch  1, batch    39 | loss: 49.4583564
CurrentTrain: epoch  1, batch    40 | loss: 57.1381410
CurrentTrain: epoch  1, batch    41 | loss: 58.9419098
CurrentTrain: epoch  1, batch    42 | loss: 59.7964636
CurrentTrain: epoch  1, batch    43 | loss: 71.4821518
CurrentTrain: epoch  1, batch    44 | loss: 124.2379321
CurrentTrain: epoch  1, batch    45 | loss: 71.6895405
CurrentTrain: epoch  1, batch    46 | loss: 88.3006517
CurrentTrain: epoch  1, batch    47 | loss: 57.1863142
CurrentTrain: epoch  1, batch    48 | loss: 69.9447049
CurrentTrain: epoch  1, batch    49 | loss: 71.1404754
CurrentTrain: epoch  1, batch    50 | loss: 71.0278182
CurrentTrain: epoch  1, batch    51 | loss: 92.4311155
CurrentTrain: epoch  1, batch    52 | loss: 94.0573151
CurrentTrain: epoch  1, batch    53 | loss: 121.5341958
CurrentTrain: epoch  1, batch    54 | loss: 61.1518037
CurrentTrain: epoch  1, batch    55 | loss: 70.4165386
CurrentTrain: epoch  1, batch    56 | loss: 69.4655149
CurrentTrain: epoch  1, batch    57 | loss: 87.8286952
CurrentTrain: epoch  1, batch    58 | loss: 71.8915611
CurrentTrain: epoch  1, batch    59 | loss: 58.4786665
CurrentTrain: epoch  1, batch    60 | loss: 69.7513193
CurrentTrain: epoch  1, batch    61 | loss: 119.2385813
CurrentTrain: epoch  1, batch    62 | loss: 120.1940583
CurrentTrain: epoch  1, batch    63 | loss: 57.8432755
CurrentTrain: epoch  1, batch    64 | loss: 70.9992671
CurrentTrain: epoch  1, batch    65 | loss: 88.6526368
CurrentTrain: epoch  1, batch    66 | loss: 49.3668035
CurrentTrain: epoch  1, batch    67 | loss: 119.2215815
CurrentTrain: epoch  1, batch    68 | loss: 54.5134397
CurrentTrain: epoch  1, batch    69 | loss: 60.8286780
CurrentTrain: epoch  1, batch    70 | loss: 57.6588724
CurrentTrain: epoch  1, batch    71 | loss: 73.3092572
CurrentTrain: epoch  1, batch    72 | loss: 89.0278288
CurrentTrain: epoch  1, batch    73 | loss: 87.4387432
CurrentTrain: epoch  1, batch    74 | loss: 87.6979476
CurrentTrain: epoch  1, batch    75 | loss: 68.2408530
CurrentTrain: epoch  1, batch    76 | loss: 71.1347640
CurrentTrain: epoch  1, batch    77 | loss: 73.8378633
CurrentTrain: epoch  1, batch    78 | loss: 71.0645687
CurrentTrain: epoch  1, batch    79 | loss: 70.1291449
CurrentTrain: epoch  1, batch    80 | loss: 47.8011943
CurrentTrain: epoch  1, batch    81 | loss: 87.2829294
CurrentTrain: epoch  1, batch    82 | loss: 56.7925313
CurrentTrain: epoch  1, batch    83 | loss: 186.6166692
CurrentTrain: epoch  1, batch    84 | loss: 89.6674943
CurrentTrain: epoch  1, batch    85 | loss: 57.8122076
CurrentTrain: epoch  1, batch    86 | loss: 47.2682924
CurrentTrain: epoch  1, batch    87 | loss: 59.5640132
CurrentTrain: epoch  1, batch    88 | loss: 89.1172481
CurrentTrain: epoch  1, batch    89 | loss: 73.5067488
CurrentTrain: epoch  1, batch    90 | loss: 58.5993265
CurrentTrain: epoch  1, batch    91 | loss: 70.2104749
CurrentTrain: epoch  1, batch    92 | loss: 67.8102399
CurrentTrain: epoch  1, batch    93 | loss: 121.6532883
CurrentTrain: epoch  1, batch    94 | loss: 73.3584983
CurrentTrain: epoch  1, batch    95 | loss: 57.0777778
CurrentTrain: epoch  2, batch     0 | loss: 47.6499845
CurrentTrain: epoch  2, batch     1 | loss: 91.3912510
CurrentTrain: epoch  2, batch     2 | loss: 68.4412635
CurrentTrain: epoch  2, batch     3 | loss: 70.6235947
CurrentTrain: epoch  2, batch     4 | loss: 117.8835029
CurrentTrain: epoch  2, batch     5 | loss: 55.9866090
CurrentTrain: epoch  2, batch     6 | loss: 121.5455932
CurrentTrain: epoch  2, batch     7 | loss: 68.4528017
CurrentTrain: epoch  2, batch     8 | loss: 86.0712572
CurrentTrain: epoch  2, batch     9 | loss: 46.3866328
CurrentTrain: epoch  2, batch    10 | loss: 84.1937001
CurrentTrain: epoch  2, batch    11 | loss: 45.2949405
CurrentTrain: epoch  2, batch    12 | loss: 48.4362682
CurrentTrain: epoch  2, batch    13 | loss: 87.8565417
CurrentTrain: epoch  2, batch    14 | loss: 89.4270902
CurrentTrain: epoch  2, batch    15 | loss: 90.6178946
CurrentTrain: epoch  2, batch    16 | loss: 67.0320108
CurrentTrain: epoch  2, batch    17 | loss: 68.2007284
CurrentTrain: epoch  2, batch    18 | loss: 68.4375315
CurrentTrain: epoch  2, batch    19 | loss: 87.1142342
CurrentTrain: epoch  2, batch    20 | loss: 54.9122109
CurrentTrain: epoch  2, batch    21 | loss: 84.8237995
CurrentTrain: epoch  2, batch    22 | loss: 56.4823966
CurrentTrain: epoch  2, batch    23 | loss: 68.3941837
CurrentTrain: epoch  2, batch    24 | loss: 65.8865449
CurrentTrain: epoch  2, batch    25 | loss: 66.5335383
CurrentTrain: epoch  2, batch    26 | loss: 73.1383254
CurrentTrain: epoch  2, batch    27 | loss: 68.7894133
CurrentTrain: epoch  2, batch    28 | loss: 86.3944566
CurrentTrain: epoch  2, batch    29 | loss: 87.7587180
CurrentTrain: epoch  2, batch    30 | loss: 71.4355790
CurrentTrain: epoch  2, batch    31 | loss: 65.9994365
CurrentTrain: epoch  2, batch    32 | loss: 58.0370853
CurrentTrain: epoch  2, batch    33 | loss: 96.3634099
CurrentTrain: epoch  2, batch    34 | loss: 87.4897484
CurrentTrain: epoch  2, batch    35 | loss: 88.7428935
CurrentTrain: epoch  2, batch    36 | loss: 68.9517810
CurrentTrain: epoch  2, batch    37 | loss: 84.2905928
CurrentTrain: epoch  2, batch    38 | loss: 56.5480184
CurrentTrain: epoch  2, batch    39 | loss: 86.0870466
CurrentTrain: epoch  2, batch    40 | loss: 44.6098166
CurrentTrain: epoch  2, batch    41 | loss: 87.6160891
CurrentTrain: epoch  2, batch    42 | loss: 84.7539535
CurrentTrain: epoch  2, batch    43 | loss: 56.5507275
CurrentTrain: epoch  2, batch    44 | loss: 68.3816928
CurrentTrain: epoch  2, batch    45 | loss: 57.8059797
CurrentTrain: epoch  2, batch    46 | loss: 68.3352783
CurrentTrain: epoch  2, batch    47 | loss: 56.9501980
CurrentTrain: epoch  2, batch    48 | loss: 118.9625530
CurrentTrain: epoch  2, batch    49 | loss: 120.3266827
CurrentTrain: epoch  2, batch    50 | loss: 55.6054650
CurrentTrain: epoch  2, batch    51 | loss: 120.4062697
CurrentTrain: epoch  2, batch    52 | loss: 56.8554752
CurrentTrain: epoch  2, batch    53 | loss: 69.3135657
CurrentTrain: epoch  2, batch    54 | loss: 88.2410826
CurrentTrain: epoch  2, batch    55 | loss: 68.2451787
CurrentTrain: epoch  2, batch    56 | loss: 66.4625025
CurrentTrain: epoch  2, batch    57 | loss: 54.6444471
CurrentTrain: epoch  2, batch    58 | loss: 57.2495472
CurrentTrain: epoch  2, batch    59 | loss: 56.7177393
CurrentTrain: epoch  2, batch    60 | loss: 67.5390789
CurrentTrain: epoch  2, batch    61 | loss: 53.5299837
CurrentTrain: epoch  2, batch    62 | loss: 67.1342746
CurrentTrain: epoch  2, batch    63 | loss: 56.1415878
CurrentTrain: epoch  2, batch    64 | loss: 54.3702301
CurrentTrain: epoch  2, batch    65 | loss: 68.3782128
CurrentTrain: epoch  2, batch    66 | loss: 118.8963823
CurrentTrain: epoch  2, batch    67 | loss: 91.5614778
CurrentTrain: epoch  2, batch    68 | loss: 57.9854235
CurrentTrain: epoch  2, batch    69 | loss: 55.2063941
CurrentTrain: epoch  2, batch    70 | loss: 68.7345001
CurrentTrain: epoch  2, batch    71 | loss: 55.3021781
CurrentTrain: epoch  2, batch    72 | loss: 181.9403751
CurrentTrain: epoch  2, batch    73 | loss: 72.4071640
CurrentTrain: epoch  2, batch    74 | loss: 68.3508191
CurrentTrain: epoch  2, batch    75 | loss: 70.5971244
CurrentTrain: epoch  2, batch    76 | loss: 84.9015786
CurrentTrain: epoch  2, batch    77 | loss: 47.6768139
CurrentTrain: epoch  2, batch    78 | loss: 68.5282250
CurrentTrain: epoch  2, batch    79 | loss: 68.8289900
CurrentTrain: epoch  2, batch    80 | loss: 55.4856078
CurrentTrain: epoch  2, batch    81 | loss: 68.3196844
CurrentTrain: epoch  2, batch    82 | loss: 88.3445182
CurrentTrain: epoch  2, batch    83 | loss: 89.9389832
CurrentTrain: epoch  2, batch    84 | loss: 57.1707645
CurrentTrain: epoch  2, batch    85 | loss: 117.0922009
CurrentTrain: epoch  2, batch    86 | loss: 47.1579626
CurrentTrain: epoch  2, batch    87 | loss: 47.5556565
CurrentTrain: epoch  2, batch    88 | loss: 49.1028266
CurrentTrain: epoch  2, batch    89 | loss: 83.7029468
CurrentTrain: epoch  2, batch    90 | loss: 69.1200564
CurrentTrain: epoch  2, batch    91 | loss: 58.5928701
CurrentTrain: epoch  2, batch    92 | loss: 58.2231378
CurrentTrain: epoch  2, batch    93 | loss: 72.9600350
CurrentTrain: epoch  2, batch    94 | loss: 66.9226015
CurrentTrain: epoch  2, batch    95 | loss: 55.1497785
CurrentTrain: epoch  3, batch     0 | loss: 69.3767250
CurrentTrain: epoch  3, batch     1 | loss: 86.4252921
CurrentTrain: epoch  3, batch     2 | loss: 70.9276027
CurrentTrain: epoch  3, batch     3 | loss: 68.3362585
CurrentTrain: epoch  3, batch     4 | loss: 67.3928791
CurrentTrain: epoch  3, batch     5 | loss: 84.6240836
CurrentTrain: epoch  3, batch     6 | loss: 64.9414230
CurrentTrain: epoch  3, batch     7 | loss: 47.4306290
CurrentTrain: epoch  3, batch     8 | loss: 55.3297692
CurrentTrain: epoch  3, batch     9 | loss: 116.3698759
CurrentTrain: epoch  3, batch    10 | loss: 55.8993740
CurrentTrain: epoch  3, batch    11 | loss: 63.8387826
CurrentTrain: epoch  3, batch    12 | loss: 84.3196990
CurrentTrain: epoch  3, batch    13 | loss: 84.6213154
CurrentTrain: epoch  3, batch    14 | loss: 69.0482950
CurrentTrain: epoch  3, batch    15 | loss: 87.3097451
CurrentTrain: epoch  3, batch    16 | loss: 85.6409863
CurrentTrain: epoch  3, batch    17 | loss: 53.1003196
CurrentTrain: epoch  3, batch    18 | loss: 56.6232579
CurrentTrain: epoch  3, batch    19 | loss: 56.2307879
CurrentTrain: epoch  3, batch    20 | loss: 86.6302774
CurrentTrain: epoch  3, batch    21 | loss: 67.2116752
CurrentTrain: epoch  3, batch    22 | loss: 85.4299820
CurrentTrain: epoch  3, batch    23 | loss: 67.2256775
CurrentTrain: epoch  3, batch    24 | loss: 56.0430732
CurrentTrain: epoch  3, batch    25 | loss: 69.4389063
CurrentTrain: epoch  3, batch    26 | loss: 86.2039645
CurrentTrain: epoch  3, batch    27 | loss: 91.7241202
CurrentTrain: epoch  3, batch    28 | loss: 83.9644305
CurrentTrain: epoch  3, batch    29 | loss: 55.6766720
CurrentTrain: epoch  3, batch    30 | loss: 70.7107876
CurrentTrain: epoch  3, batch    31 | loss: 63.7034002
CurrentTrain: epoch  3, batch    32 | loss: 66.1950083
CurrentTrain: epoch  3, batch    33 | loss: 115.8916624
CurrentTrain: epoch  3, batch    34 | loss: 68.0200608
CurrentTrain: epoch  3, batch    35 | loss: 116.0737340
CurrentTrain: epoch  3, batch    36 | loss: 43.8423872
CurrentTrain: epoch  3, batch    37 | loss: 118.1280241
CurrentTrain: epoch  3, batch    38 | loss: 50.1090966
CurrentTrain: epoch  3, batch    39 | loss: 66.6123010
CurrentTrain: epoch  3, batch    40 | loss: 67.5283313
CurrentTrain: epoch  3, batch    41 | loss: 65.2928807
CurrentTrain: epoch  3, batch    42 | loss: 66.6704224
CurrentTrain: epoch  3, batch    43 | loss: 52.6286473
CurrentTrain: epoch  3, batch    44 | loss: 50.0438127
CurrentTrain: epoch  3, batch    45 | loss: 56.5877960
CurrentTrain: epoch  3, batch    46 | loss: 87.5279448
CurrentTrain: epoch  3, batch    47 | loss: 70.2654516
CurrentTrain: epoch  3, batch    48 | loss: 119.2085481
CurrentTrain: epoch  3, batch    49 | loss: 48.4600526
CurrentTrain: epoch  3, batch    50 | loss: 55.3240344
CurrentTrain: epoch  3, batch    51 | loss: 56.1274586
CurrentTrain: epoch  3, batch    52 | loss: 122.2891352
CurrentTrain: epoch  3, batch    53 | loss: 57.7248736
CurrentTrain: epoch  3, batch    54 | loss: 84.4522418
CurrentTrain: epoch  3, batch    55 | loss: 65.6570069
CurrentTrain: epoch  3, batch    56 | loss: 58.2691558
CurrentTrain: epoch  3, batch    57 | loss: 56.0031373
CurrentTrain: epoch  3, batch    58 | loss: 71.4566969
CurrentTrain: epoch  3, batch    59 | loss: 113.8094059
CurrentTrain: epoch  3, batch    60 | loss: 68.5329040
CurrentTrain: epoch  3, batch    61 | loss: 55.1658388
CurrentTrain: epoch  3, batch    62 | loss: 66.8915759
CurrentTrain: epoch  3, batch    63 | loss: 83.3430008
CurrentTrain: epoch  3, batch    64 | loss: 64.7205595
CurrentTrain: epoch  3, batch    65 | loss: 87.0780708
CurrentTrain: epoch  3, batch    66 | loss: 88.7176435
CurrentTrain: epoch  3, batch    67 | loss: 55.0461234
CurrentTrain: epoch  3, batch    68 | loss: 67.7717572
CurrentTrain: epoch  3, batch    69 | loss: 54.2791560
CurrentTrain: epoch  3, batch    70 | loss: 69.3463483
CurrentTrain: epoch  3, batch    71 | loss: 44.4676419
CurrentTrain: epoch  3, batch    72 | loss: 55.6038988
CurrentTrain: epoch  3, batch    73 | loss: 42.9732797
CurrentTrain: epoch  3, batch    74 | loss: 69.2377130
CurrentTrain: epoch  3, batch    75 | loss: 88.2978885
CurrentTrain: epoch  3, batch    76 | loss: 81.6281837
CurrentTrain: epoch  3, batch    77 | loss: 68.3465423
CurrentTrain: epoch  3, batch    78 | loss: 113.4137492
CurrentTrain: epoch  3, batch    79 | loss: 64.9567918
CurrentTrain: epoch  3, batch    80 | loss: 85.7737395
CurrentTrain: epoch  3, batch    81 | loss: 43.4359706
CurrentTrain: epoch  3, batch    82 | loss: 93.3055235
CurrentTrain: epoch  3, batch    83 | loss: 67.1178191
CurrentTrain: epoch  3, batch    84 | loss: 55.0310179
CurrentTrain: epoch  3, batch    85 | loss: 86.8257490
CurrentTrain: epoch  3, batch    86 | loss: 55.6135053
CurrentTrain: epoch  3, batch    87 | loss: 68.2662294
CurrentTrain: epoch  3, batch    88 | loss: 52.7767659
CurrentTrain: epoch  3, batch    89 | loss: 53.9853590
CurrentTrain: epoch  3, batch    90 | loss: 84.2259441
CurrentTrain: epoch  3, batch    91 | loss: 85.8730845
CurrentTrain: epoch  3, batch    92 | loss: 56.8215129
CurrentTrain: epoch  3, batch    93 | loss: 67.5406998
CurrentTrain: epoch  3, batch    94 | loss: 88.8910523
CurrentTrain: epoch  3, batch    95 | loss: 42.6188665
CurrentTrain: epoch  4, batch     0 | loss: 66.4263704
CurrentTrain: epoch  4, batch     1 | loss: 86.7932369
CurrentTrain: epoch  4, batch     2 | loss: 82.3738327
CurrentTrain: epoch  4, batch     3 | loss: 62.5186491
CurrentTrain: epoch  4, batch     4 | loss: 120.0414410
CurrentTrain: epoch  4, batch     5 | loss: 82.3014557
CurrentTrain: epoch  4, batch     6 | loss: 53.9083132
CurrentTrain: epoch  4, batch     7 | loss: 49.8212222
CurrentTrain: epoch  4, batch     8 | loss: 87.3739976
CurrentTrain: epoch  4, batch     9 | loss: 119.4775512
CurrentTrain: epoch  4, batch    10 | loss: 87.1275459
CurrentTrain: epoch  4, batch    11 | loss: 54.9337052
CurrentTrain: epoch  4, batch    12 | loss: 66.4964969
CurrentTrain: epoch  4, batch    13 | loss: 53.5931353
CurrentTrain: epoch  4, batch    14 | loss: 64.2319867
CurrentTrain: epoch  4, batch    15 | loss: 84.9008532
CurrentTrain: epoch  4, batch    16 | loss: 82.0153020
CurrentTrain: epoch  4, batch    17 | loss: 117.8624895
CurrentTrain: epoch  4, batch    18 | loss: 63.8814448
CurrentTrain: epoch  4, batch    19 | loss: 65.2084872
CurrentTrain: epoch  4, batch    20 | loss: 64.9578154
CurrentTrain: epoch  4, batch    21 | loss: 56.0938370
CurrentTrain: epoch  4, batch    22 | loss: 86.2777789
CurrentTrain: epoch  4, batch    23 | loss: 62.0991842
CurrentTrain: epoch  4, batch    24 | loss: 82.0526033
CurrentTrain: epoch  4, batch    25 | loss: 83.4797792
CurrentTrain: epoch  4, batch    26 | loss: 66.6190853
CurrentTrain: epoch  4, batch    27 | loss: 46.5482721
CurrentTrain: epoch  4, batch    28 | loss: 84.4521460
CurrentTrain: epoch  4, batch    29 | loss: 65.0367918
CurrentTrain: epoch  4, batch    30 | loss: 68.0576346
CurrentTrain: epoch  4, batch    31 | loss: 56.8640783
CurrentTrain: epoch  4, batch    32 | loss: 80.7782992
CurrentTrain: epoch  4, batch    33 | loss: 43.1692329
CurrentTrain: epoch  4, batch    34 | loss: 84.1972328
CurrentTrain: epoch  4, batch    35 | loss: 68.0142180
CurrentTrain: epoch  4, batch    36 | loss: 53.5634766
CurrentTrain: epoch  4, batch    37 | loss: 45.7447464
CurrentTrain: epoch  4, batch    38 | loss: 64.6970455
CurrentTrain: epoch  4, batch    39 | loss: 55.9004997
CurrentTrain: epoch  4, batch    40 | loss: 66.8045242
CurrentTrain: epoch  4, batch    41 | loss: 56.2656139
CurrentTrain: epoch  4, batch    42 | loss: 68.1734868
CurrentTrain: epoch  4, batch    43 | loss: 43.7196841
CurrentTrain: epoch  4, batch    44 | loss: 51.9958525
CurrentTrain: epoch  4, batch    45 | loss: 70.4332744
CurrentTrain: epoch  4, batch    46 | loss: 58.8834311
CurrentTrain: epoch  4, batch    47 | loss: 84.1104580
CurrentTrain: epoch  4, batch    48 | loss: 55.9372523
CurrentTrain: epoch  4, batch    49 | loss: 71.2692090
CurrentTrain: epoch  4, batch    50 | loss: 66.4559009
CurrentTrain: epoch  4, batch    51 | loss: 57.7525291
CurrentTrain: epoch  4, batch    52 | loss: 67.5694721
CurrentTrain: epoch  4, batch    53 | loss: 113.7947107
CurrentTrain: epoch  4, batch    54 | loss: 66.3796185
CurrentTrain: epoch  4, batch    55 | loss: 85.2808214
CurrentTrain: epoch  4, batch    56 | loss: 119.0862020
CurrentTrain: epoch  4, batch    57 | loss: 54.1315419
CurrentTrain: epoch  4, batch    58 | loss: 113.8951257
CurrentTrain: epoch  4, batch    59 | loss: 86.1538140
CurrentTrain: epoch  4, batch    60 | loss: 68.7032703
CurrentTrain: epoch  4, batch    61 | loss: 45.3344649
CurrentTrain: epoch  4, batch    62 | loss: 46.6226014
CurrentTrain: epoch  4, batch    63 | loss: 53.9778643
CurrentTrain: epoch  4, batch    64 | loss: 67.1445592
CurrentTrain: epoch  4, batch    65 | loss: 116.1665336
CurrentTrain: epoch  4, batch    66 | loss: 57.4994131
CurrentTrain: epoch  4, batch    67 | loss: 52.7426023
CurrentTrain: epoch  4, batch    68 | loss: 86.0771357
CurrentTrain: epoch  4, batch    69 | loss: 52.7393138
CurrentTrain: epoch  4, batch    70 | loss: 78.6170591
CurrentTrain: epoch  4, batch    71 | loss: 87.2229416
CurrentTrain: epoch  4, batch    72 | loss: 49.2805324
CurrentTrain: epoch  4, batch    73 | loss: 55.6545617
CurrentTrain: epoch  4, batch    74 | loss: 118.4015353
CurrentTrain: epoch  4, batch    75 | loss: 65.1391807
CurrentTrain: epoch  4, batch    76 | loss: 121.9684853
CurrentTrain: epoch  4, batch    77 | loss: 81.9815369
CurrentTrain: epoch  4, batch    78 | loss: 54.2645475
CurrentTrain: epoch  4, batch    79 | loss: 82.9241168
CurrentTrain: epoch  4, batch    80 | loss: 53.6604913
CurrentTrain: epoch  4, batch    81 | loss: 120.2245181
CurrentTrain: epoch  4, batch    82 | loss: 63.2203347
CurrentTrain: epoch  4, batch    83 | loss: 52.7988813
CurrentTrain: epoch  4, batch    84 | loss: 61.1182623
CurrentTrain: epoch  4, batch    85 | loss: 65.2594984
CurrentTrain: epoch  4, batch    86 | loss: 87.7918780
CurrentTrain: epoch  4, batch    87 | loss: 115.2492365
CurrentTrain: epoch  4, batch    88 | loss: 84.9054841
CurrentTrain: epoch  4, batch    89 | loss: 54.9804399
CurrentTrain: epoch  4, batch    90 | loss: 111.9847303
CurrentTrain: epoch  4, batch    91 | loss: 80.3692801
CurrentTrain: epoch  4, batch    92 | loss: 66.8192438
CurrentTrain: epoch  4, batch    93 | loss: 62.8071465
CurrentTrain: epoch  4, batch    94 | loss: 71.1637060
CurrentTrain: epoch  4, batch    95 | loss: 45.5346917
CurrentTrain: epoch  5, batch     0 | loss: 64.1273534
CurrentTrain: epoch  5, batch     1 | loss: 62.7392664
CurrentTrain: epoch  5, batch     2 | loss: 83.1211071
CurrentTrain: epoch  5, batch     3 | loss: 65.6435647
CurrentTrain: epoch  5, batch     4 | loss: 62.9806343
CurrentTrain: epoch  5, batch     5 | loss: 53.1091567
CurrentTrain: epoch  5, batch     6 | loss: 120.0359038
CurrentTrain: epoch  5, batch     7 | loss: 63.2133316
CurrentTrain: epoch  5, batch     8 | loss: 64.1300979
CurrentTrain: epoch  5, batch     9 | loss: 64.0002581
CurrentTrain: epoch  5, batch    10 | loss: 55.3977462
CurrentTrain: epoch  5, batch    11 | loss: 84.6823100
CurrentTrain: epoch  5, batch    12 | loss: 117.6587750
CurrentTrain: epoch  5, batch    13 | loss: 52.7723083
CurrentTrain: epoch  5, batch    14 | loss: 82.8580143
CurrentTrain: epoch  5, batch    15 | loss: 52.1769089
CurrentTrain: epoch  5, batch    16 | loss: 65.1371283
CurrentTrain: epoch  5, batch    17 | loss: 51.3380385
CurrentTrain: epoch  5, batch    18 | loss: 67.3368023
CurrentTrain: epoch  5, batch    19 | loss: 82.7970962
CurrentTrain: epoch  5, batch    20 | loss: 64.4395122
CurrentTrain: epoch  5, batch    21 | loss: 52.4168882
CurrentTrain: epoch  5, batch    22 | loss: 66.4662475
CurrentTrain: epoch  5, batch    23 | loss: 82.3662846
CurrentTrain: epoch  5, batch    24 | loss: 44.7748402
CurrentTrain: epoch  5, batch    25 | loss: 66.2994218
CurrentTrain: epoch  5, batch    26 | loss: 85.9291379
CurrentTrain: epoch  5, batch    27 | loss: 81.6566631
CurrentTrain: epoch  5, batch    28 | loss: 118.7228105
CurrentTrain: epoch  5, batch    29 | loss: 372.1657760
CurrentTrain: epoch  5, batch    30 | loss: 55.9915252
CurrentTrain: epoch  5, batch    31 | loss: 177.6028271
CurrentTrain: epoch  5, batch    32 | loss: 52.6913466
CurrentTrain: epoch  5, batch    33 | loss: 59.1418680
CurrentTrain: epoch  5, batch    34 | loss: 65.4699942
CurrentTrain: epoch  5, batch    35 | loss: 54.2468174
CurrentTrain: epoch  5, batch    36 | loss: 117.5352763
CurrentTrain: epoch  5, batch    37 | loss: 66.7024690
CurrentTrain: epoch  5, batch    38 | loss: 79.5861922
CurrentTrain: epoch  5, batch    39 | loss: 86.1314797
CurrentTrain: epoch  5, batch    40 | loss: 63.6048987
CurrentTrain: epoch  5, batch    41 | loss: 86.7395711
CurrentTrain: epoch  5, batch    42 | loss: 120.2538465
CurrentTrain: epoch  5, batch    43 | loss: 54.2410471
CurrentTrain: epoch  5, batch    44 | loss: 52.4322691
CurrentTrain: epoch  5, batch    45 | loss: 65.9873076
CurrentTrain: epoch  5, batch    46 | loss: 68.0390784
CurrentTrain: epoch  5, batch    47 | loss: 51.6659754
CurrentTrain: epoch  5, batch    48 | loss: 50.2734376
CurrentTrain: epoch  5, batch    49 | loss: 54.4772885
CurrentTrain: epoch  5, batch    50 | loss: 45.5616808
CurrentTrain: epoch  5, batch    51 | loss: 63.3990928
CurrentTrain: epoch  5, batch    52 | loss: 68.5237456
CurrentTrain: epoch  5, batch    53 | loss: 61.4924329
CurrentTrain: epoch  5, batch    54 | loss: 62.8833619
CurrentTrain: epoch  5, batch    55 | loss: 85.8532923
CurrentTrain: epoch  5, batch    56 | loss: 117.9539994
CurrentTrain: epoch  5, batch    57 | loss: 69.6317542
CurrentTrain: epoch  5, batch    58 | loss: 66.2152403
CurrentTrain: epoch  5, batch    59 | loss: 115.7450324
CurrentTrain: epoch  5, batch    60 | loss: 85.5688122
CurrentTrain: epoch  5, batch    61 | loss: 67.4467245
CurrentTrain: epoch  5, batch    62 | loss: 88.5661461
CurrentTrain: epoch  5, batch    63 | loss: 67.5376790
CurrentTrain: epoch  5, batch    64 | loss: 44.4521489
CurrentTrain: epoch  5, batch    65 | loss: 64.4871142
CurrentTrain: epoch  5, batch    66 | loss: 64.0473027
CurrentTrain: epoch  5, batch    67 | loss: 65.6331627
CurrentTrain: epoch  5, batch    68 | loss: 51.3362434
CurrentTrain: epoch  5, batch    69 | loss: 65.6350627
CurrentTrain: epoch  5, batch    70 | loss: 85.5021583
CurrentTrain: epoch  5, batch    71 | loss: 86.3623756
CurrentTrain: epoch  5, batch    72 | loss: 84.1448765
CurrentTrain: epoch  5, batch    73 | loss: 62.8382991
CurrentTrain: epoch  5, batch    74 | loss: 50.3684491
CurrentTrain: epoch  5, batch    75 | loss: 52.0937045
CurrentTrain: epoch  5, batch    76 | loss: 115.3604823
CurrentTrain: epoch  5, batch    77 | loss: 66.9103461
CurrentTrain: epoch  5, batch    78 | loss: 62.7950750
CurrentTrain: epoch  5, batch    79 | loss: 67.2941345
CurrentTrain: epoch  5, batch    80 | loss: 83.8036419
CurrentTrain: epoch  5, batch    81 | loss: 85.0164692
CurrentTrain: epoch  5, batch    82 | loss: 66.0204069
CurrentTrain: epoch  5, batch    83 | loss: 84.3273341
CurrentTrain: epoch  5, batch    84 | loss: 83.2665867
CurrentTrain: epoch  5, batch    85 | loss: 65.0405959
CurrentTrain: epoch  5, batch    86 | loss: 84.6854152
CurrentTrain: epoch  5, batch    87 | loss: 54.7875985
CurrentTrain: epoch  5, batch    88 | loss: 83.9975976
CurrentTrain: epoch  5, batch    89 | loss: 63.8196149
CurrentTrain: epoch  5, batch    90 | loss: 65.1119418
CurrentTrain: epoch  5, batch    91 | loss: 69.2540160
CurrentTrain: epoch  5, batch    92 | loss: 65.2069850
CurrentTrain: epoch  5, batch    93 | loss: 51.9216071
CurrentTrain: epoch  5, batch    94 | loss: 116.4643243
CurrentTrain: epoch  5, batch    95 | loss: 45.2025571
CurrentTrain: epoch  6, batch     0 | loss: 43.3165049
CurrentTrain: epoch  6, batch     1 | loss: 65.4218823
CurrentTrain: epoch  6, batch     2 | loss: 62.4092708
CurrentTrain: epoch  6, batch     3 | loss: 85.8757364
CurrentTrain: epoch  6, batch     4 | loss: 118.0145524
CurrentTrain: epoch  6, batch     5 | loss: 181.7738400
CurrentTrain: epoch  6, batch     6 | loss: 66.4734025
CurrentTrain: epoch  6, batch     7 | loss: 66.0456006
CurrentTrain: epoch  6, batch     8 | loss: 84.4075802
CurrentTrain: epoch  6, batch     9 | loss: 68.9387549
CurrentTrain: epoch  6, batch    10 | loss: 112.1952475
CurrentTrain: epoch  6, batch    11 | loss: 66.6607100
CurrentTrain: epoch  6, batch    12 | loss: 85.8621713
CurrentTrain: epoch  6, batch    13 | loss: 86.6042698
CurrentTrain: epoch  6, batch    14 | loss: 43.5156612
CurrentTrain: epoch  6, batch    15 | loss: 61.9021565
CurrentTrain: epoch  6, batch    16 | loss: 51.2198859
CurrentTrain: epoch  6, batch    17 | loss: 65.5426567
CurrentTrain: epoch  6, batch    18 | loss: 52.1622187
CurrentTrain: epoch  6, batch    19 | loss: 51.3123182
CurrentTrain: epoch  6, batch    20 | loss: 51.0081564
CurrentTrain: epoch  6, batch    21 | loss: 66.8027116
CurrentTrain: epoch  6, batch    22 | loss: 67.5985578
CurrentTrain: epoch  6, batch    23 | loss: 85.0425387
CurrentTrain: epoch  6, batch    24 | loss: 84.4957920
CurrentTrain: epoch  6, batch    25 | loss: 50.7749386
CurrentTrain: epoch  6, batch    26 | loss: 52.6455832
CurrentTrain: epoch  6, batch    27 | loss: 50.6757349
CurrentTrain: epoch  6, batch    28 | loss: 55.8258869
CurrentTrain: epoch  6, batch    29 | loss: 81.0887840
CurrentTrain: epoch  6, batch    30 | loss: 84.6837016
CurrentTrain: epoch  6, batch    31 | loss: 42.4229466
CurrentTrain: epoch  6, batch    32 | loss: 81.9871031
CurrentTrain: epoch  6, batch    33 | loss: 67.5785554
CurrentTrain: epoch  6, batch    34 | loss: 84.9460078
CurrentTrain: epoch  6, batch    35 | loss: 52.9371244
CurrentTrain: epoch  6, batch    36 | loss: 52.5187951
CurrentTrain: epoch  6, batch    37 | loss: 55.3971887
CurrentTrain: epoch  6, batch    38 | loss: 83.0366235
CurrentTrain: epoch  6, batch    39 | loss: 63.7707277
CurrentTrain: epoch  6, batch    40 | loss: 50.9477669
CurrentTrain: epoch  6, batch    41 | loss: 81.0207779
CurrentTrain: epoch  6, batch    42 | loss: 63.7300633
CurrentTrain: epoch  6, batch    43 | loss: 44.6645691
CurrentTrain: epoch  6, batch    44 | loss: 41.0301585
CurrentTrain: epoch  6, batch    45 | loss: 66.4639406
CurrentTrain: epoch  6, batch    46 | loss: 64.1069639
CurrentTrain: epoch  6, batch    47 | loss: 117.6652156
CurrentTrain: epoch  6, batch    48 | loss: 53.2484186
CurrentTrain: epoch  6, batch    49 | loss: 52.0535467
CurrentTrain: epoch  6, batch    50 | loss: 86.8170173
CurrentTrain: epoch  6, batch    51 | loss: 82.4126813
CurrentTrain: epoch  6, batch    52 | loss: 117.8943871
CurrentTrain: epoch  6, batch    53 | loss: 88.6863104
CurrentTrain: epoch  6, batch    54 | loss: 65.5882201
CurrentTrain: epoch  6, batch    55 | loss: 65.3864022
CurrentTrain: epoch  6, batch    56 | loss: 83.4204002
CurrentTrain: epoch  6, batch    57 | loss: 49.5422480
CurrentTrain: epoch  6, batch    58 | loss: 86.7050162
CurrentTrain: epoch  6, batch    59 | loss: 43.9294143
CurrentTrain: epoch  6, batch    60 | loss: 63.2206728
CurrentTrain: epoch  6, batch    61 | loss: 67.5551338
CurrentTrain: epoch  6, batch    62 | loss: 81.4528219
CurrentTrain: epoch  6, batch    63 | loss: 64.3475238
CurrentTrain: epoch  6, batch    64 | loss: 63.3242262
CurrentTrain: epoch  6, batch    65 | loss: 85.0801442
CurrentTrain: epoch  6, batch    66 | loss: 70.3955423
CurrentTrain: epoch  6, batch    67 | loss: 66.2607555
CurrentTrain: epoch  6, batch    68 | loss: 83.6331907
CurrentTrain: epoch  6, batch    69 | loss: 63.8591521
CurrentTrain: epoch  6, batch    70 | loss: 87.4769608
CurrentTrain: epoch  6, batch    71 | loss: 43.4374800
CurrentTrain: epoch  6, batch    72 | loss: 55.4719013
CurrentTrain: epoch  6, batch    73 | loss: 51.7840044
CurrentTrain: epoch  6, batch    74 | loss: 85.8444832
CurrentTrain: epoch  6, batch    75 | loss: 83.0078445
CurrentTrain: epoch  6, batch    76 | loss: 42.1278956
CurrentTrain: epoch  6, batch    77 | loss: 43.6694981
CurrentTrain: epoch  6, batch    78 | loss: 84.7638575
CurrentTrain: epoch  6, batch    79 | loss: 84.3392905
CurrentTrain: epoch  6, batch    80 | loss: 64.2991236
CurrentTrain: epoch  6, batch    81 | loss: 44.7674039
CurrentTrain: epoch  6, batch    82 | loss: 55.3643722
CurrentTrain: epoch  6, batch    83 | loss: 79.5915014
CurrentTrain: epoch  6, batch    84 | loss: 66.4087769
CurrentTrain: epoch  6, batch    85 | loss: 43.8805809
CurrentTrain: epoch  6, batch    86 | loss: 62.7417738
CurrentTrain: epoch  6, batch    87 | loss: 83.6142035
CurrentTrain: epoch  6, batch    88 | loss: 67.2301210
CurrentTrain: epoch  6, batch    89 | loss: 86.4921479
CurrentTrain: epoch  6, batch    90 | loss: 83.1438772
CurrentTrain: epoch  6, batch    91 | loss: 43.2410775
CurrentTrain: epoch  6, batch    92 | loss: 66.3934810
CurrentTrain: epoch  6, batch    93 | loss: 64.7680358
CurrentTrain: epoch  6, batch    94 | loss: 55.8705460
CurrentTrain: epoch  6, batch    95 | loss: 93.2604857
CurrentTrain: epoch  7, batch     0 | loss: 65.4474955
CurrentTrain: epoch  7, batch     1 | loss: 82.5100674
CurrentTrain: epoch  7, batch     2 | loss: 66.0867389
CurrentTrain: epoch  7, batch     3 | loss: 64.9065168
CurrentTrain: epoch  7, batch     4 | loss: 67.0663271
CurrentTrain: epoch  7, batch     5 | loss: 82.9536115
CurrentTrain: epoch  7, batch     6 | loss: 66.1152052
CurrentTrain: epoch  7, batch     7 | loss: 63.8239137
CurrentTrain: epoch  7, batch     8 | loss: 51.0792611
CurrentTrain: epoch  7, batch     9 | loss: 51.0948059
CurrentTrain: epoch  7, batch    10 | loss: 52.6542537
CurrentTrain: epoch  7, batch    11 | loss: 52.3893219
CurrentTrain: epoch  7, batch    12 | loss: 84.7082831
CurrentTrain: epoch  7, batch    13 | loss: 65.5916286
CurrentTrain: epoch  7, batch    14 | loss: 67.4801231
CurrentTrain: epoch  7, batch    15 | loss: 54.1340495
CurrentTrain: epoch  7, batch    16 | loss: 85.2932935
CurrentTrain: epoch  7, batch    17 | loss: 59.2158784
CurrentTrain: epoch  7, batch    18 | loss: 52.6326989
CurrentTrain: epoch  7, batch    19 | loss: 50.3691048
CurrentTrain: epoch  7, batch    20 | loss: 61.7849402
CurrentTrain: epoch  7, batch    21 | loss: 62.0622771
CurrentTrain: epoch  7, batch    22 | loss: 84.0806452
CurrentTrain: epoch  7, batch    23 | loss: 82.7962441
CurrentTrain: epoch  7, batch    24 | loss: 52.9561087
CurrentTrain: epoch  7, batch    25 | loss: 42.8531392
CurrentTrain: epoch  7, batch    26 | loss: 65.3831754
CurrentTrain: epoch  7, batch    27 | loss: 85.7902311
CurrentTrain: epoch  7, batch    28 | loss: 51.9637489
CurrentTrain: epoch  7, batch    29 | loss: 85.9642714
CurrentTrain: epoch  7, batch    30 | loss: 63.4322493
CurrentTrain: epoch  7, batch    31 | loss: 66.9188242
CurrentTrain: epoch  7, batch    32 | loss: 111.0806081
CurrentTrain: epoch  7, batch    33 | loss: 65.6513696
CurrentTrain: epoch  7, batch    34 | loss: 66.8403968
CurrentTrain: epoch  7, batch    35 | loss: 86.1527916
CurrentTrain: epoch  7, batch    36 | loss: 70.2597775
CurrentTrain: epoch  7, batch    37 | loss: 67.6383678
CurrentTrain: epoch  7, batch    38 | loss: 65.6342671
CurrentTrain: epoch  7, batch    39 | loss: 84.5035602
CurrentTrain: epoch  7, batch    40 | loss: 66.3675694
CurrentTrain: epoch  7, batch    41 | loss: 49.2608983
CurrentTrain: epoch  7, batch    42 | loss: 81.7426047
CurrentTrain: epoch  7, batch    43 | loss: 51.8475466
CurrentTrain: epoch  7, batch    44 | loss: 62.4508905
CurrentTrain: epoch  7, batch    45 | loss: 84.1854496
CurrentTrain: epoch  7, batch    46 | loss: 53.1585352
CurrentTrain: epoch  7, batch    47 | loss: 62.0931012
CurrentTrain: epoch  7, batch    48 | loss: 112.0538585
CurrentTrain: epoch  7, batch    49 | loss: 44.9392880
CurrentTrain: epoch  7, batch    50 | loss: 81.0057821
CurrentTrain: epoch  7, batch    51 | loss: 61.5804416
CurrentTrain: epoch  7, batch    52 | loss: 51.4241975
CurrentTrain: epoch  7, batch    53 | loss: 83.9309801
CurrentTrain: epoch  7, batch    54 | loss: 51.7938896
CurrentTrain: epoch  7, batch    55 | loss: 85.9082491
CurrentTrain: epoch  7, batch    56 | loss: 82.4909668
CurrentTrain: epoch  7, batch    57 | loss: 65.6573775
CurrentTrain: epoch  7, batch    58 | loss: 63.6716268
CurrentTrain: epoch  7, batch    59 | loss: 117.8996875
CurrentTrain: epoch  7, batch    60 | loss: 51.9620292
CurrentTrain: epoch  7, batch    61 | loss: 53.1463737
CurrentTrain: epoch  7, batch    62 | loss: 113.1829890
CurrentTrain: epoch  7, batch    63 | loss: 66.1619135
CurrentTrain: epoch  7, batch    64 | loss: 48.2035330
CurrentTrain: epoch  7, batch    65 | loss: 61.3531765
CurrentTrain: epoch  7, batch    66 | loss: 64.4701758
CurrentTrain: epoch  7, batch    67 | loss: 66.7589740
CurrentTrain: epoch  7, batch    68 | loss: 50.7901636
CurrentTrain: epoch  7, batch    69 | loss: 65.0326461
CurrentTrain: epoch  7, batch    70 | loss: 53.3959288
CurrentTrain: epoch  7, batch    71 | loss: 85.8082532
CurrentTrain: epoch  7, batch    72 | loss: 49.9124721
CurrentTrain: epoch  7, batch    73 | loss: 64.4213193
CurrentTrain: epoch  7, batch    74 | loss: 41.5286436
CurrentTrain: epoch  7, batch    75 | loss: 49.7678939
CurrentTrain: epoch  7, batch    76 | loss: 53.2453522
CurrentTrain: epoch  7, batch    77 | loss: 84.1742735
CurrentTrain: epoch  7, batch    78 | loss: 67.9310674
CurrentTrain: epoch  7, batch    79 | loss: 117.4845954
CurrentTrain: epoch  7, batch    80 | loss: 53.6218333
CurrentTrain: epoch  7, batch    81 | loss: 51.0544594
CurrentTrain: epoch  7, batch    82 | loss: 64.6062501
CurrentTrain: epoch  7, batch    83 | loss: 50.9399490
CurrentTrain: epoch  7, batch    84 | loss: 62.9703652
CurrentTrain: epoch  7, batch    85 | loss: 65.6461490
CurrentTrain: epoch  7, batch    86 | loss: 55.2271235
CurrentTrain: epoch  7, batch    87 | loss: 83.0893632
CurrentTrain: epoch  7, batch    88 | loss: 83.5832788
CurrentTrain: epoch  7, batch    89 | loss: 119.4215429
CurrentTrain: epoch  7, batch    90 | loss: 66.8435203
CurrentTrain: epoch  7, batch    91 | loss: 63.0836111
CurrentTrain: epoch  7, batch    92 | loss: 82.8896304
CurrentTrain: epoch  7, batch    93 | loss: 44.3297311
CurrentTrain: epoch  7, batch    94 | loss: 48.5309846
CurrentTrain: epoch  7, batch    95 | loss: 70.4591461
CurrentTrain: epoch  8, batch     0 | loss: 53.0205747
CurrentTrain: epoch  8, batch     1 | loss: 51.4343545
CurrentTrain: epoch  8, batch     2 | loss: 84.0966576
CurrentTrain: epoch  8, batch     3 | loss: 117.5141566
CurrentTrain: epoch  8, batch     4 | loss: 67.4997237
CurrentTrain: epoch  8, batch     5 | loss: 56.3269785
CurrentTrain: epoch  8, batch     6 | loss: 67.2101185
CurrentTrain: epoch  8, batch     7 | loss: 63.2876640
CurrentTrain: epoch  8, batch     8 | loss: 53.6625973
CurrentTrain: epoch  8, batch     9 | loss: 84.0417671
CurrentTrain: epoch  8, batch    10 | loss: 60.2854837
CurrentTrain: epoch  8, batch    11 | loss: 63.5235462
CurrentTrain: epoch  8, batch    12 | loss: 64.4171214
CurrentTrain: epoch  8, batch    13 | loss: 80.5964003
CurrentTrain: epoch  8, batch    14 | loss: 63.6475331
CurrentTrain: epoch  8, batch    15 | loss: 65.8423871
CurrentTrain: epoch  8, batch    16 | loss: 82.7259256
CurrentTrain: epoch  8, batch    17 | loss: 64.1184875
CurrentTrain: epoch  8, batch    18 | loss: 85.0877167
CurrentTrain: epoch  8, batch    19 | loss: 52.2321938
CurrentTrain: epoch  8, batch    20 | loss: 62.4130736
CurrentTrain: epoch  8, batch    21 | loss: 65.4290214
CurrentTrain: epoch  8, batch    22 | loss: 80.0821946
CurrentTrain: epoch  8, batch    23 | loss: 53.0906402
CurrentTrain: epoch  8, batch    24 | loss: 52.3520557
CurrentTrain: epoch  8, batch    25 | loss: 82.6676644
CurrentTrain: epoch  8, batch    26 | loss: 49.2668157
CurrentTrain: epoch  8, batch    27 | loss: 63.4299224
CurrentTrain: epoch  8, batch    28 | loss: 51.3479552
CurrentTrain: epoch  8, batch    29 | loss: 51.9817610
CurrentTrain: epoch  8, batch    30 | loss: 86.0991491
CurrentTrain: epoch  8, batch    31 | loss: 41.9052870
CurrentTrain: epoch  8, batch    32 | loss: 177.6000173
CurrentTrain: epoch  8, batch    33 | loss: 84.0311212
CurrentTrain: epoch  8, batch    34 | loss: 83.6219818
CurrentTrain: epoch  8, batch    35 | loss: 50.7083597
CurrentTrain: epoch  8, batch    36 | loss: 80.8025545
CurrentTrain: epoch  8, batch    37 | loss: 53.1637076
CurrentTrain: epoch  8, batch    38 | loss: 63.4628672
CurrentTrain: epoch  8, batch    39 | loss: 66.8635547
CurrentTrain: epoch  8, batch    40 | loss: 65.5233995
CurrentTrain: epoch  8, batch    41 | loss: 85.9394731
CurrentTrain: epoch  8, batch    42 | loss: 65.4503126
CurrentTrain: epoch  8, batch    43 | loss: 85.7222332
CurrentTrain: epoch  8, batch    44 | loss: 116.4912181
CurrentTrain: epoch  8, batch    45 | loss: 65.6459406
CurrentTrain: epoch  8, batch    46 | loss: 52.2997617
CurrentTrain: epoch  8, batch    47 | loss: 65.4357899
CurrentTrain: epoch  8, batch    48 | loss: 50.7666063
CurrentTrain: epoch  8, batch    49 | loss: 63.2518998
CurrentTrain: epoch  8, batch    50 | loss: 82.4102550
CurrentTrain: epoch  8, batch    51 | loss: 52.3234034
CurrentTrain: epoch  8, batch    52 | loss: 65.5303362
CurrentTrain: epoch  8, batch    53 | loss: 66.8724647
CurrentTrain: epoch  8, batch    54 | loss: 82.5595187
CurrentTrain: epoch  8, batch    55 | loss: 80.9594154
CurrentTrain: epoch  8, batch    56 | loss: 52.1319630
CurrentTrain: epoch  8, batch    57 | loss: 51.9617905
CurrentTrain: epoch  8, batch    58 | loss: 65.6870335
CurrentTrain: epoch  8, batch    59 | loss: 42.2287915
CurrentTrain: epoch  8, batch    60 | loss: 116.2728070
CurrentTrain: epoch  8, batch    61 | loss: 84.5905584
CurrentTrain: epoch  8, batch    62 | loss: 42.4873718
CurrentTrain: epoch  8, batch    63 | loss: 69.4223024
CurrentTrain: epoch  8, batch    64 | loss: 65.2664312
CurrentTrain: epoch  8, batch    65 | loss: 63.4311855
CurrentTrain: epoch  8, batch    66 | loss: 60.8501969
CurrentTrain: epoch  8, batch    67 | loss: 84.3322162
CurrentTrain: epoch  8, batch    68 | loss: 79.6517341
CurrentTrain: epoch  8, batch    69 | loss: 65.6908754
CurrentTrain: epoch  8, batch    70 | loss: 82.5226249
CurrentTrain: epoch  8, batch    71 | loss: 112.4417867
CurrentTrain: epoch  8, batch    72 | loss: 66.3526932
CurrentTrain: epoch  8, batch    73 | loss: 63.4317869
CurrentTrain: epoch  8, batch    74 | loss: 84.0925409
CurrentTrain: epoch  8, batch    75 | loss: 64.3450299
CurrentTrain: epoch  8, batch    76 | loss: 43.5001713
CurrentTrain: epoch  8, batch    77 | loss: 113.3131102
CurrentTrain: epoch  8, batch    78 | loss: 49.9848292
CurrentTrain: epoch  8, batch    79 | loss: 53.6969413
CurrentTrain: epoch  8, batch    80 | loss: 81.4662054
CurrentTrain: epoch  8, batch    81 | loss: 65.9660039
CurrentTrain: epoch  8, batch    82 | loss: 54.6765867
CurrentTrain: epoch  8, batch    83 | loss: 52.1404462
CurrentTrain: epoch  8, batch    84 | loss: 115.2491299
CurrentTrain: epoch  8, batch    85 | loss: 111.1113710
CurrentTrain: epoch  8, batch    86 | loss: 84.9947537
CurrentTrain: epoch  8, batch    87 | loss: 64.4599495
CurrentTrain: epoch  8, batch    88 | loss: 39.0133969
CurrentTrain: epoch  8, batch    89 | loss: 63.0627849
CurrentTrain: epoch  8, batch    90 | loss: 63.0668090
CurrentTrain: epoch  8, batch    91 | loss: 67.1037929
CurrentTrain: epoch  8, batch    92 | loss: 64.2460227
CurrentTrain: epoch  8, batch    93 | loss: 64.3557300
CurrentTrain: epoch  8, batch    94 | loss: 82.5280553
CurrentTrain: epoch  8, batch    95 | loss: 96.2553813
CurrentTrain: epoch  9, batch     0 | loss: 53.0549027
CurrentTrain: epoch  9, batch     1 | loss: 112.1699007
CurrentTrain: epoch  9, batch     2 | loss: 62.3138418
CurrentTrain: epoch  9, batch     3 | loss: 52.6597700
CurrentTrain: epoch  9, batch     4 | loss: 64.8252617
CurrentTrain: epoch  9, batch     5 | loss: 41.0719616
CurrentTrain: epoch  9, batch     6 | loss: 82.5480317
CurrentTrain: epoch  9, batch     7 | loss: 63.2989425
CurrentTrain: epoch  9, batch     8 | loss: 82.5582695
CurrentTrain: epoch  9, batch     9 | loss: 51.8997919
CurrentTrain: epoch  9, batch    10 | loss: 86.8905406
CurrentTrain: epoch  9, batch    11 | loss: 52.9772084
CurrentTrain: epoch  9, batch    12 | loss: 82.6830968
CurrentTrain: epoch  9, batch    13 | loss: 84.0734658
CurrentTrain: epoch  9, batch    14 | loss: 82.6984430
CurrentTrain: epoch  9, batch    15 | loss: 62.6578667
CurrentTrain: epoch  9, batch    16 | loss: 117.5065552
CurrentTrain: epoch  9, batch    17 | loss: 67.2400277
CurrentTrain: epoch  9, batch    18 | loss: 63.5042112
CurrentTrain: epoch  9, batch    19 | loss: 66.8192335
CurrentTrain: epoch  9, batch    20 | loss: 50.1787063
CurrentTrain: epoch  9, batch    21 | loss: 84.4534146
CurrentTrain: epoch  9, batch    22 | loss: 52.9995183
CurrentTrain: epoch  9, batch    23 | loss: 52.7220807
CurrentTrain: epoch  9, batch    24 | loss: 115.1819418
CurrentTrain: epoch  9, batch    25 | loss: 82.3560804
CurrentTrain: epoch  9, batch    26 | loss: 48.4465201
CurrentTrain: epoch  9, batch    27 | loss: 111.0215465
CurrentTrain: epoch  9, batch    28 | loss: 51.4293678
CurrentTrain: epoch  9, batch    29 | loss: 53.2923426
CurrentTrain: epoch  9, batch    30 | loss: 51.1891720
CurrentTrain: epoch  9, batch    31 | loss: 64.0679078
CurrentTrain: epoch  9, batch    32 | loss: 85.1431108
CurrentTrain: epoch  9, batch    33 | loss: 55.8655809
CurrentTrain: epoch  9, batch    34 | loss: 45.6489475
CurrentTrain: epoch  9, batch    35 | loss: 113.2419021
CurrentTrain: epoch  9, batch    36 | loss: 63.2362856
CurrentTrain: epoch  9, batch    37 | loss: 69.5583464
CurrentTrain: epoch  9, batch    38 | loss: 64.0418526
CurrentTrain: epoch  9, batch    39 | loss: 53.2856844
CurrentTrain: epoch  9, batch    40 | loss: 49.4522058
CurrentTrain: epoch  9, batch    41 | loss: 66.8578012
CurrentTrain: epoch  9, batch    42 | loss: 49.8231740
CurrentTrain: epoch  9, batch    43 | loss: 50.2713818
CurrentTrain: epoch  9, batch    44 | loss: 113.3668288
CurrentTrain: epoch  9, batch    45 | loss: 54.6403214
CurrentTrain: epoch  9, batch    46 | loss: 65.4668369
CurrentTrain: epoch  9, batch    47 | loss: 84.2320551
CurrentTrain: epoch  9, batch    48 | loss: 63.3856565
CurrentTrain: epoch  9, batch    49 | loss: 53.5061605
CurrentTrain: epoch  9, batch    50 | loss: 63.0007363
CurrentTrain: epoch  9, batch    51 | loss: 66.8744591
CurrentTrain: epoch  9, batch    52 | loss: 54.5401639
CurrentTrain: epoch  9, batch    53 | loss: 65.5125843
CurrentTrain: epoch  9, batch    54 | loss: 52.3565696
CurrentTrain: epoch  9, batch    55 | loss: 52.0538342
CurrentTrain: epoch  9, batch    56 | loss: 65.4378532
CurrentTrain: epoch  9, batch    57 | loss: 40.6911324
CurrentTrain: epoch  9, batch    58 | loss: 79.7891353
CurrentTrain: epoch  9, batch    59 | loss: 84.1251662
CurrentTrain: epoch  9, batch    60 | loss: 60.9954594
CurrentTrain: epoch  9, batch    61 | loss: 44.0559665
CurrentTrain: epoch  9, batch    62 | loss: 82.8280704
CurrentTrain: epoch  9, batch    63 | loss: 49.4109188
CurrentTrain: epoch  9, batch    64 | loss: 43.4763923
CurrentTrain: epoch  9, batch    65 | loss: 84.1654260
CurrentTrain: epoch  9, batch    66 | loss: 83.9429958
CurrentTrain: epoch  9, batch    67 | loss: 64.0955886
CurrentTrain: epoch  9, batch    68 | loss: 117.7067649
CurrentTrain: epoch  9, batch    69 | loss: 84.3994736
CurrentTrain: epoch  9, batch    70 | loss: 64.4300771
CurrentTrain: epoch  9, batch    71 | loss: 82.6824471
CurrentTrain: epoch  9, batch    72 | loss: 51.5088272
CurrentTrain: epoch  9, batch    73 | loss: 85.1261927
CurrentTrain: epoch  9, batch    74 | loss: 117.4867899
CurrentTrain: epoch  9, batch    75 | loss: 90.7174775
CurrentTrain: epoch  9, batch    76 | loss: 82.3194337
CurrentTrain: epoch  9, batch    77 | loss: 54.3400857
CurrentTrain: epoch  9, batch    78 | loss: 112.0084284
CurrentTrain: epoch  9, batch    79 | loss: 63.8702723
CurrentTrain: epoch  9, batch    80 | loss: 43.8395463
CurrentTrain: epoch  9, batch    81 | loss: 41.5686134
CurrentTrain: epoch  9, batch    82 | loss: 85.8407141
CurrentTrain: epoch  9, batch    83 | loss: 118.2924168
CurrentTrain: epoch  9, batch    84 | loss: 117.8193893
CurrentTrain: epoch  9, batch    85 | loss: 52.0069290
CurrentTrain: epoch  9, batch    86 | loss: 62.2953058
CurrentTrain: epoch  9, batch    87 | loss: 52.8082748
CurrentTrain: epoch  9, batch    88 | loss: 117.4836955
CurrentTrain: epoch  9, batch    89 | loss: 43.8048640
CurrentTrain: epoch  9, batch    90 | loss: 64.2853058
CurrentTrain: epoch  9, batch    91 | loss: 63.3876155
CurrentTrain: epoch  9, batch    92 | loss: 40.6664630
CurrentTrain: epoch  9, batch    93 | loss: 53.1426025
CurrentTrain: epoch  9, batch    94 | loss: 79.8405295
CurrentTrain: epoch  9, batch    95 | loss: 54.3061566

F1 score per class: {32: 0.6590909090909091, 6: 0.8888888888888888, 19: 0.2727272727272727, 24: 0.7700534759358288, 26: 0.918918918918919, 29: 0.9072164948453608}
Micro-average F1 score: 0.8177966101694916
Weighted-average F1 score: 0.8278733340913578
F1 score per class: {32: 0.7010309278350515, 6: 0.9010989010989011, 19: 0.42424242424242425, 24: 0.7593582887700535, 26: 0.9637305699481865, 29: 0.8888888888888888}
Micro-average F1 score: 0.8282208588957055
Weighted-average F1 score: 0.8281584319772354
F1 score per class: {32: 0.7046632124352331, 6: 0.9010989010989011, 19: 0.4375, 24: 0.7593582887700535, 26: 0.9637305699481865, 29: 0.8947368421052632}
Micro-average F1 score: 0.8311156601842374
Weighted-average F1 score: 0.8317346809678351

F1 score per class: {32: 0.6590909090909091, 6: 0.8888888888888888, 19: 0.2727272727272727, 24: 0.7700534759358288, 26: 0.918918918918919, 29: 0.9072164948453608}
Micro-average F1 score: 0.8177966101694916
Weighted-average F1 score: 0.8278733340913578
F1 score per class: {32: 0.7010309278350515, 6: 0.9010989010989011, 19: 0.42424242424242425, 24: 0.7593582887700535, 26: 0.9637305699481865, 29: 0.8888888888888888}
Micro-average F1 score: 0.8282208588957055
Weighted-average F1 score: 0.8281584319772354
F1 score per class: {32: 0.7046632124352331, 6: 0.9010989010989011, 19: 0.4375, 24: 0.7593582887700535, 26: 0.9637305699481865, 29: 0.8947368421052632}
Micro-average F1 score: 0.8311156601842374
Weighted-average F1 score: 0.8317346809678351
cur_acc:  ['0.8178']
his_acc:  ['0.8178']
cur_acc des:  ['0.8282']
his_acc des:  ['0.8282']
cur_acc rrf:  ['0.8311']
his_acc rrf:  ['0.8311']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 78.0894436
CurrentTrain: epoch  0, batch     1 | loss: 79.3187084
CurrentTrain: epoch  0, batch     2 | loss: 97.3719013
CurrentTrain: epoch  0, batch     3 | loss: 75.5264090
CurrentTrain: epoch  0, batch     4 | loss: 18.9388129
CurrentTrain: epoch  1, batch     0 | loss: 94.0452178
CurrentTrain: epoch  1, batch     1 | loss: 74.7312157
CurrentTrain: epoch  1, batch     2 | loss: 93.4591062
CurrentTrain: epoch  1, batch     3 | loss: 70.6443475
CurrentTrain: epoch  1, batch     4 | loss: 20.9489441
CurrentTrain: epoch  2, batch     0 | loss: 59.3485413
CurrentTrain: epoch  2, batch     1 | loss: 124.1178918
CurrentTrain: epoch  2, batch     2 | loss: 91.8621753
CurrentTrain: epoch  2, batch     3 | loss: 89.4739855
CurrentTrain: epoch  2, batch     4 | loss: 18.8115488
CurrentTrain: epoch  3, batch     0 | loss: 92.0459661
CurrentTrain: epoch  3, batch     1 | loss: 69.7543443
CurrentTrain: epoch  3, batch     2 | loss: 90.4442509
CurrentTrain: epoch  3, batch     3 | loss: 68.8633039
CurrentTrain: epoch  3, batch     4 | loss: 29.9562498
CurrentTrain: epoch  4, batch     0 | loss: 115.3788399
CurrentTrain: epoch  4, batch     1 | loss: 60.7265308
CurrentTrain: epoch  4, batch     2 | loss: 70.1153655
CurrentTrain: epoch  4, batch     3 | loss: 69.2860902
CurrentTrain: epoch  4, batch     4 | loss: 17.0040880
CurrentTrain: epoch  5, batch     0 | loss: 66.5996619
CurrentTrain: epoch  5, batch     1 | loss: 68.5865758
CurrentTrain: epoch  5, batch     2 | loss: 68.1903855
CurrentTrain: epoch  5, batch     3 | loss: 90.0791456
CurrentTrain: epoch  5, batch     4 | loss: 28.4265875
CurrentTrain: epoch  6, batch     0 | loss: 68.9073611
CurrentTrain: epoch  6, batch     1 | loss: 87.7362495
CurrentTrain: epoch  6, batch     2 | loss: 54.2671220
CurrentTrain: epoch  6, batch     3 | loss: 85.4980123
CurrentTrain: epoch  6, batch     4 | loss: 9.9964383
CurrentTrain: epoch  7, batch     0 | loss: 66.7076121
CurrentTrain: epoch  7, batch     1 | loss: 66.0427933
CurrentTrain: epoch  7, batch     2 | loss: 64.8788959
CurrentTrain: epoch  7, batch     3 | loss: 119.5705675
CurrentTrain: epoch  7, batch     4 | loss: 14.9832802
CurrentTrain: epoch  8, batch     0 | loss: 64.6819393
CurrentTrain: epoch  8, batch     1 | loss: 84.7351763
CurrentTrain: epoch  8, batch     2 | loss: 66.9125359
CurrentTrain: epoch  8, batch     3 | loss: 82.2499748
CurrentTrain: epoch  8, batch     4 | loss: 26.6986678
CurrentTrain: epoch  9, batch     0 | loss: 113.2539336
CurrentTrain: epoch  9, batch     1 | loss: 79.3610547
CurrentTrain: epoch  9, batch     2 | loss: 85.0887029
CurrentTrain: epoch  9, batch     3 | loss: 66.2272850
CurrentTrain: epoch  9, batch     4 | loss: 14.6728518
MemoryTrain:  epoch  0, batch     0 | loss: 0.7112266
MemoryTrain:  epoch  1, batch     0 | loss: 0.6042655
MemoryTrain:  epoch  2, batch     0 | loss: 0.4355610
MemoryTrain:  epoch  3, batch     0 | loss: 0.3732355
MemoryTrain:  epoch  4, batch     0 | loss: 0.3096034
MemoryTrain:  epoch  5, batch     0 | loss: 0.3575588
MemoryTrain:  epoch  6, batch     0 | loss: 0.1975090
MemoryTrain:  epoch  7, batch     0 | loss: 0.1660620
MemoryTrain:  epoch  8, batch     0 | loss: 0.1202773
MemoryTrain:  epoch  9, batch     0 | loss: 0.0998597

F1 score per class: {2: 0.8, 39: 0.27450980392156865, 11: 0.6111111111111112, 12: 0.0, 19: 0.4, 28: 0.25}
Micro-average F1 score: 0.4709897610921502
Weighted-average F1 score: 0.5247571100512278
F1 score per class: {2: 0.9411764705882353, 6: 0.0, 39: 0.6716417910447762, 11: 0.6842105263157895, 12: 0.0, 19: 0.47058823529411764, 28: 0.0, 29: 0.4}
Micro-average F1 score: 0.6457142857142857
Weighted-average F1 score: 0.615566226250639
F1 score per class: {2: 0.9411764705882353, 39: 0.6515151515151515, 11: 0.7096774193548387, 12: 0.0, 19: 0.5263157894736842, 28: 0.5}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6513008541092961

F1 score per class: {32: 0.8, 2: 0.6927374301675978, 6: 0.27184466019417475, 39: 0.5751633986928104, 11: 0.8839779005524862, 12: 0.34782608695652173, 19: 0.7675675675675676, 24: 0.2608695652173913, 26: 0.9361702127659575, 28: 0.9119170984455959, 29: 0.25}
Micro-average F1 score: 0.7339158061953932
Weighted-average F1 score: 0.7744760210665796
F1 score per class: {32: 0.9411764705882353, 2: 0.7135678391959799, 6: 0.6474820143884892, 39: 0.6666666666666666, 11: 0.8969072164948454, 12: 0.4444444444444444, 19: 0.7692307692307693, 24: 0.2857142857142857, 26: 0.9484536082474226, 28: 0.9175257731958762, 29: 0.36363636363636365}
Micro-average F1 score: 0.7810650887573964
Weighted-average F1 score: 0.7868929818190886
F1 score per class: {32: 0.8888888888888888, 2: 0.6947368421052632, 6: 0.6277372262773723, 39: 0.6748466257668712, 11: 0.8888888888888888, 12: 0.38461538461538464, 19: 0.7624309392265194, 24: 0.2631578947368421, 26: 0.9528795811518325, 28: 0.9175257731958762, 29: 0.5}
Micro-average F1 score: 0.7720861172976986
Weighted-average F1 score: 0.7731146212673679
cur_acc:  ['0.8178', '0.4710']
his_acc:  ['0.8178', '0.7339']
cur_acc des:  ['0.8282', '0.6457']
his_acc des:  ['0.8282', '0.7811']
cur_acc rrf:  ['0.8311', '0.6667']
his_acc rrf:  ['0.8311', '0.7721']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 69.1534182
CurrentTrain: epoch  0, batch     1 | loss: 71.4399760
CurrentTrain: epoch  0, batch     2 | loss: 66.5100891
CurrentTrain: epoch  0, batch     3 | loss: 15.7791591
CurrentTrain: epoch  1, batch     0 | loss: 75.2348552
CurrentTrain: epoch  1, batch     1 | loss: 61.4474012
CurrentTrain: epoch  1, batch     2 | loss: 71.9490883
CurrentTrain: epoch  1, batch     3 | loss: 11.0602895
CurrentTrain: epoch  2, batch     0 | loss: 89.1710662
CurrentTrain: epoch  2, batch     1 | loss: 89.8069098
CurrentTrain: epoch  2, batch     2 | loss: 52.5111760
CurrentTrain: epoch  2, batch     3 | loss: 27.6673249
CurrentTrain: epoch  3, batch     0 | loss: 68.6361214
CurrentTrain: epoch  3, batch     1 | loss: 56.0729521
CurrentTrain: epoch  3, batch     2 | loss: 53.0887555
CurrentTrain: epoch  3, batch     3 | loss: 27.4712930
CurrentTrain: epoch  4, batch     0 | loss: 86.1525566
CurrentTrain: epoch  4, batch     1 | loss: 51.8934823
CurrentTrain: epoch  4, batch     2 | loss: 86.5394856
CurrentTrain: epoch  4, batch     3 | loss: 5.6668438
CurrentTrain: epoch  5, batch     0 | loss: 53.2622669
CurrentTrain: epoch  5, batch     1 | loss: 53.1966056
CurrentTrain: epoch  5, batch     2 | loss: 65.3638190
CurrentTrain: epoch  5, batch     3 | loss: 11.0806009
CurrentTrain: epoch  6, batch     0 | loss: 50.9667878
CurrentTrain: epoch  6, batch     1 | loss: 66.0139749
CurrentTrain: epoch  6, batch     2 | loss: 54.2693373
CurrentTrain: epoch  6, batch     3 | loss: 11.7222234
CurrentTrain: epoch  7, batch     0 | loss: 114.6702409
CurrentTrain: epoch  7, batch     1 | loss: 64.9034873
CurrentTrain: epoch  7, batch     2 | loss: 48.1274363
CurrentTrain: epoch  7, batch     3 | loss: 27.4609795
CurrentTrain: epoch  8, batch     0 | loss: 114.5338021
CurrentTrain: epoch  8, batch     1 | loss: 48.7115416
CurrentTrain: epoch  8, batch     2 | loss: 51.9698066
CurrentTrain: epoch  8, batch     3 | loss: 5.8435600
CurrentTrain: epoch  9, batch     0 | loss: 49.9502327
CurrentTrain: epoch  9, batch     1 | loss: 80.2889574
CurrentTrain: epoch  9, batch     2 | loss: 63.7946312
CurrentTrain: epoch  9, batch     3 | loss: 11.1694252
MemoryTrain:  epoch  0, batch     0 | loss: 0.4638079
MemoryTrain:  epoch  1, batch     0 | loss: 0.4731598
MemoryTrain:  epoch  2, batch     0 | loss: 0.3951376
MemoryTrain:  epoch  3, batch     0 | loss: 0.2884390
MemoryTrain:  epoch  4, batch     0 | loss: 0.2751446
MemoryTrain:  epoch  5, batch     0 | loss: 0.1876635
MemoryTrain:  epoch  6, batch     0 | loss: 0.1473003
MemoryTrain:  epoch  7, batch     0 | loss: 0.1279621
MemoryTrain:  epoch  8, batch     0 | loss: 0.1128174
MemoryTrain:  epoch  9, batch     0 | loss: 0.0863598

F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.96, 9: 0.0, 19: 0.0, 26: 0.4444444444444444, 27: 0.0, 31: 0.3764705882352941}
Micro-average F1 score: 0.44019138755980863
Weighted-average F1 score: 0.34531662489557224
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6666666666666666, 27: 1.0, 31: 0.5319148936170213}
Micro-average F1 score: 0.5545454545454546
Weighted-average F1 score: 0.4416583172811477
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6666666666666666, 27: 1.0, 31: 0.5319148936170213}
Micro-average F1 score: 0.5545454545454546
Weighted-average F1 score: 0.4416583172811477

F1 score per class: {32: 0.7142857142857143, 2: 0.27419354838709675, 6: 0.08163265306122448, 7: 0.96, 40: 0.14736842105263157, 39: 0.18018018018018017, 11: 0.6995515695067265, 12: 0.10526315789473684, 9: 0.7613636363636364, 19: 0.3076923076923077, 24: 0.42857142857142855, 26: 0.907103825136612, 27: 0.0, 28: 0.8135593220338984, 29: 0.13333333333333333, 31: 0.3404255319148936}
Micro-average F1 score: 0.5685131195335277
Weighted-average F1 score: 0.6474102207008056
F1 score per class: {32: 0.8, 2: 0.45390070921985815, 6: 0.07142857142857142, 7: 0.9803921568627451, 40: 0.6714285714285714, 39: 0.6496815286624203, 11: 0.6846846846846847, 12: 0.3076923076923077, 9: 0.7640449438202247, 19: 0.4827586206896552, 24: 0.3076923076923077, 26: 0.9417989417989417, 27: 0.8, 28: 0.8795811518324608, 29: 0.25, 31: 0.43103448275862066}
Micro-average F1 score: 0.6726572528883183
Weighted-average F1 score: 0.6645142063370589
F1 score per class: {32: 0.8, 2: 0.4647887323943662, 6: 0.07272727272727272, 7: 0.9803921568627451, 40: 0.5669291338582677, 39: 0.6419753086419753, 11: 0.6909090909090909, 12: 0.24, 9: 0.7570621468926554, 19: 0.4827586206896552, 24: 0.2962962962962963, 26: 0.93048128342246, 27: 1.0, 28: 0.8795811518324608, 29: 0.25, 31: 0.423728813559322}
Micro-average F1 score: 0.6610608020698577
Weighted-average F1 score: 0.6551033677809124
cur_acc:  ['0.8178', '0.4710', '0.4402']
his_acc:  ['0.8178', '0.7339', '0.5685']
cur_acc des:  ['0.8282', '0.6457', '0.5545']
his_acc des:  ['0.8282', '0.7811', '0.6727']
cur_acc rrf:  ['0.8311', '0.6667', '0.5545']
his_acc rrf:  ['0.8311', '0.7721', '0.6611']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 68.9655147
CurrentTrain: epoch  0, batch     1 | loss: 77.8489920
CurrentTrain: epoch  0, batch     2 | loss: 71.3835609
CurrentTrain: epoch  0, batch     3 | loss: 55.2647546
CurrentTrain: epoch  1, batch     0 | loss: 63.8150375
CurrentTrain: epoch  1, batch     1 | loss: 131.3906193
CurrentTrain: epoch  1, batch     2 | loss: 59.7595051
CurrentTrain: epoch  1, batch     3 | loss: 51.8292311
CurrentTrain: epoch  2, batch     0 | loss: 74.6075187
CurrentTrain: epoch  2, batch     1 | loss: 68.8478783
CurrentTrain: epoch  2, batch     2 | loss: 75.1009589
CurrentTrain: epoch  2, batch     3 | loss: 47.8599613
CurrentTrain: epoch  3, batch     0 | loss: 52.5986809
CurrentTrain: epoch  3, batch     1 | loss: 115.8534752
CurrentTrain: epoch  3, batch     2 | loss: 71.7904067
CurrentTrain: epoch  3, batch     3 | loss: 81.8921613
CurrentTrain: epoch  4, batch     0 | loss: 71.5275414
CurrentTrain: epoch  4, batch     1 | loss: 54.9225857
CurrentTrain: epoch  4, batch     2 | loss: 58.5312527
CurrentTrain: epoch  4, batch     3 | loss: 45.8450912
CurrentTrain: epoch  5, batch     0 | loss: 57.1865574
CurrentTrain: epoch  5, batch     1 | loss: 115.8239905
CurrentTrain: epoch  5, batch     2 | loss: 56.3703542
CurrentTrain: epoch  5, batch     3 | loss: 56.5320136
CurrentTrain: epoch  6, batch     0 | loss: 53.9980661
CurrentTrain: epoch  6, batch     1 | loss: 65.0629950
CurrentTrain: epoch  6, batch     2 | loss: 87.4035865
CurrentTrain: epoch  6, batch     3 | loss: 122.9072594
CurrentTrain: epoch  7, batch     0 | loss: 53.4580695
CurrentTrain: epoch  7, batch     1 | loss: 84.8136825
CurrentTrain: epoch  7, batch     2 | loss: 65.3939534
CurrentTrain: epoch  7, batch     3 | loss: 59.6780449
CurrentTrain: epoch  8, batch     0 | loss: 64.9680091
CurrentTrain: epoch  8, batch     1 | loss: 53.5028252
CurrentTrain: epoch  8, batch     2 | loss: 54.9443498
CurrentTrain: epoch  8, batch     3 | loss: 78.1124580
CurrentTrain: epoch  9, batch     0 | loss: 82.2540998
CurrentTrain: epoch  9, batch     1 | loss: 64.9136044
CurrentTrain: epoch  9, batch     2 | loss: 66.4857330
CurrentTrain: epoch  9, batch     3 | loss: 43.4613710
MemoryTrain:  epoch  0, batch     0 | loss: 0.4901590
MemoryTrain:  epoch  1, batch     0 | loss: 0.4425256
MemoryTrain:  epoch  2, batch     0 | loss: 0.3071383
MemoryTrain:  epoch  3, batch     0 | loss: 0.2497218
MemoryTrain:  epoch  4, batch     0 | loss: 0.1796045
MemoryTrain:  epoch  5, batch     0 | loss: 0.1341210
MemoryTrain:  epoch  6, batch     0 | loss: 0.1262806
MemoryTrain:  epoch  7, batch     0 | loss: 0.0907157
MemoryTrain:  epoch  8, batch     0 | loss: 0.0753475
MemoryTrain:  epoch  9, batch     0 | loss: 0.0698033

F1 score per class: {32: 0.0, 35: 0.9473684210526315, 37: 0.42424242424242425, 38: 0.0, 11: 0.0, 15: 0.38095238095238093, 25: 0.75, 27: 0.8461538461538461}
Micro-average F1 score: 0.6138613861386139
Weighted-average F1 score: 0.6504395157645931
F1 score per class: {32: 0.0, 35: 0.75, 37: 0.0, 38: 0.8444444444444444, 11: 0.0, 15: 0.0, 24: 0.0, 25: 0.0, 26: 0.9484536082474226, 27: 0.8571428571428571, 28: 0.9230769230769231}
Micro-average F1 score: 0.8617886178861789
Weighted-average F1 score: 0.8389466047234826
F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.7619047619047619, 38: 0.0, 11: 0.0, 15: 0.0, 25: 0.0, 26: 0.813953488372093, 27: 0.8679245283018868, 28: 0.9056603773584906}
Micro-average F1 score: 0.8044692737430168
Weighted-average F1 score: 0.7719749560011315

F1 score per class: {2: 0.7142857142857143, 6: 0.38235294117647056, 7: 0.08, 9: 0.96, 11: 0.08602150537634409, 12: 0.14414414414414414, 15: 0.9473684210526315, 19: 0.6069651741293532, 24: 0.10526315789473684, 25: 0.42424242424242425, 26: 0.7692307692307693, 27: 0.2857142857142857, 28: 0.6153846153846154, 29: 0.918918918918919, 31: 0.0, 32: 0.7845303867403315, 35: 0.38095238095238093, 37: 0.43636363636363634, 38: 0.6875, 39: 0.0, 40: 0.3711340206185567}
Micro-average F1 score: 0.5430690245293782
Weighted-average F1 score: 0.598949229531041
F1 score per class: {2: 0.8, 6: 0.5569620253164557, 7: 0.07142857142857142, 9: 0.9803921568627451, 11: 0.39316239316239315, 12: 0.6956521739130435, 15: 0.7058823529411765, 19: 0.6542056074766355, 24: 0.3076923076923077, 25: 0.8444444444444444, 26: 0.7624309392265194, 27: 0.4666666666666667, 28: 0.45454545454545453, 29: 0.9583333333333334, 31: 0.8, 32: 0.9045226130653267, 35: 0.9019607843137255, 37: 0.5487804878048781, 38: 0.7741935483870968, 39: 0.25, 40: 0.3793103448275862}
Micro-average F1 score: 0.6800401203610833
Weighted-average F1 score: 0.6766825081524225
F1 score per class: {2: 0.8, 6: 0.5095541401273885, 7: 0.07547169811320754, 9: 0.9803921568627451, 11: 0.20202020202020202, 12: 0.5974025974025974, 15: 0.7368421052631579, 19: 0.6540284360189573, 24: 0.25, 25: 0.7619047619047619, 26: 0.7624309392265194, 27: 0.45161290322580644, 28: 0.35294117647058826, 29: 0.9417989417989417, 31: 1.0, 32: 0.88, 35: 0.7777777777777778, 37: 0.4972972972972973, 38: 0.7058823529411765, 39: 0.25, 40: 0.37037037037037035}
Micro-average F1 score: 0.6390593047034765
Weighted-average F1 score: 0.6465672575203123
cur_acc:  ['0.8178', '0.4710', '0.4402', '0.6139']
his_acc:  ['0.8178', '0.7339', '0.5685', '0.5431']
cur_acc des:  ['0.8282', '0.6457', '0.5545', '0.8618']
his_acc des:  ['0.8282', '0.7811', '0.6727', '0.6800']
cur_acc rrf:  ['0.8311', '0.6667', '0.5545', '0.8045']
his_acc rrf:  ['0.8311', '0.7721', '0.6611', '0.6391']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 109.6442557
CurrentTrain: epoch  0, batch     1 | loss: 110.7233381
CurrentTrain: epoch  0, batch     2 | loss: 78.3528871
CurrentTrain: epoch  0, batch     3 | loss: 76.4443991
CurrentTrain: epoch  0, batch     4 | loss: 46.1905313
CurrentTrain: epoch  1, batch     0 | loss: 68.3265684
CurrentTrain: epoch  1, batch     1 | loss: 76.2878124
CurrentTrain: epoch  1, batch     2 | loss: 68.7237864
CurrentTrain: epoch  1, batch     3 | loss: 125.6539411
CurrentTrain: epoch  1, batch     4 | loss: 69.2171760
CurrentTrain: epoch  2, batch     0 | loss: 119.1205387
CurrentTrain: epoch  2, batch     1 | loss: 69.6158928
CurrentTrain: epoch  2, batch     2 | loss: 122.8543895
CurrentTrain: epoch  2, batch     3 | loss: 59.6893653
CurrentTrain: epoch  2, batch     4 | loss: 51.2601400
CurrentTrain: epoch  3, batch     0 | loss: 71.2020246
CurrentTrain: epoch  3, batch     1 | loss: 70.2083089
CurrentTrain: epoch  3, batch     2 | loss: 86.3807837
CurrentTrain: epoch  3, batch     3 | loss: 69.0948636
CurrentTrain: epoch  3, batch     4 | loss: 52.3575513
CurrentTrain: epoch  4, batch     0 | loss: 66.2847197
CurrentTrain: epoch  4, batch     1 | loss: 120.8967280
CurrentTrain: epoch  4, batch     2 | loss: 71.0081784
CurrentTrain: epoch  4, batch     3 | loss: 55.6294262
CurrentTrain: epoch  4, batch     4 | loss: 214.1040732
CurrentTrain: epoch  5, batch     0 | loss: 84.0404170
CurrentTrain: epoch  5, batch     1 | loss: 65.6628144
CurrentTrain: epoch  5, batch     2 | loss: 86.9998873
CurrentTrain: epoch  5, batch     3 | loss: 67.2599374
CurrentTrain: epoch  5, batch     4 | loss: 106.2347855
CurrentTrain: epoch  6, batch     0 | loss: 116.5827186
CurrentTrain: epoch  6, batch     1 | loss: 69.0413475
CurrentTrain: epoch  6, batch     2 | loss: 85.7009046
CurrentTrain: epoch  6, batch     3 | loss: 83.3061242
CurrentTrain: epoch  6, batch     4 | loss: 62.5762257
CurrentTrain: epoch  7, batch     0 | loss: 52.0554089
CurrentTrain: epoch  7, batch     1 | loss: 66.6918726
CurrentTrain: epoch  7, batch     2 | loss: 116.8570058
CurrentTrain: epoch  7, batch     3 | loss: 86.4795343
CurrentTrain: epoch  7, batch     4 | loss: 103.0897627
CurrentTrain: epoch  8, batch     0 | loss: 113.5727140
CurrentTrain: epoch  8, batch     1 | loss: 65.7052306
CurrentTrain: epoch  8, batch     2 | loss: 66.3138860
CurrentTrain: epoch  8, batch     3 | loss: 82.3825593
CurrentTrain: epoch  8, batch     4 | loss: 65.1769033
CurrentTrain: epoch  9, batch     0 | loss: 65.0008774
CurrentTrain: epoch  9, batch     1 | loss: 83.2366041
CurrentTrain: epoch  9, batch     2 | loss: 52.9614916
CurrentTrain: epoch  9, batch     3 | loss: 84.9613931
CurrentTrain: epoch  9, batch     4 | loss: 103.3602812
MemoryTrain:  epoch  0, batch     0 | loss: 0.6295176
MemoryTrain:  epoch  1, batch     0 | loss: 0.4691549
MemoryTrain:  epoch  2, batch     0 | loss: 0.3892174
MemoryTrain:  epoch  3, batch     0 | loss: 0.3133568
MemoryTrain:  epoch  4, batch     0 | loss: 0.2527692
MemoryTrain:  epoch  5, batch     0 | loss: 0.2379694
MemoryTrain:  epoch  6, batch     0 | loss: 0.1735610
MemoryTrain:  epoch  7, batch     0 | loss: 0.1427150
MemoryTrain:  epoch  8, batch     0 | loss: 0.1180002
MemoryTrain:  epoch  9, batch     0 | loss: 0.1032911

F1 score per class: {32: 0.2857142857142857, 1: 0.2736842105263158, 34: 0.0, 3: 0.13793103448275862, 35: 0.5695364238410596, 37: 0.0, 38: 0.0, 11: 0.0, 14: 0.3076923076923077, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.3016949152542373
Weighted-average F1 score: 0.24517418105384461
F1 score per class: {32: 0.36507936507936506, 1: 0.5357142857142857, 34: 0.0, 3: 0.14678899082568808, 35: 0.5827814569536424, 37: 0.0, 38: 0.0, 11: 0.0, 14: 0.6904761904761905, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.4042232277526395
Weighted-average F1 score: 0.320789634078716
F1 score per class: {32: 0.36507936507936506, 1: 0.5225225225225225, 34: 0.0, 3: 0.14545454545454545, 35: 0.5733333333333334, 37: 0.0, 38: 0.0, 11: 0.0, 14: 0.6075949367088608, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.38484848484848483
Weighted-average F1 score: 0.30069534517084734

F1 score per class: {1: 0.2537313432835821, 2: 0.7142857142857143, 3: 0.25742574257425743, 6: 0.3230769230769231, 7: 0.044444444444444446, 9: 0.9803921568627451, 11: 0.0, 12: 0.18181818181818182, 14: 0.1276595744680851, 15: 0.8888888888888888, 19: 0.25, 22: 0.5512820512820513, 24: 0.08, 25: 0.47058823529411764, 26: 0.7675675675675676, 27: 0.0, 28: 0.5454545454545454, 29: 0.9010989010989011, 31: 0.0, 32: 0.4626865671641791, 34: 0.20833333333333334, 35: 0.14285714285714285, 37: 0.41830065359477125, 38: 0.6666666666666666, 39: 0.0, 40: 0.2619047619047619}
Micro-average F1 score: 0.4111466423024212
Weighted-average F1 score: 0.4589881821897517
F1 score per class: {1: 0.323943661971831, 2: 0.8, 3: 0.49586776859504134, 6: 0.5100671140939598, 7: 0.07017543859649122, 9: 0.9803921568627451, 11: 0.24299065420560748, 12: 0.7407407407407407, 14: 0.14285714285714285, 15: 0.75, 19: 0.3382352941176471, 22: 0.5605095541401274, 24: 0.07407407407407407, 25: 0.8478260869565217, 26: 0.7567567567567568, 27: 0.0, 28: 0.3333333333333333, 29: 0.9528795811518325, 31: 0.0, 32: 0.7727272727272727, 34: 0.36024844720496896, 35: 0.5507246376811594, 37: 0.46153846153846156, 38: 0.7213114754098361, 39: 0.13333333333333333, 40: 0.578125}
Micro-average F1 score: 0.5512670565302145
Weighted-average F1 score: 0.5470568576638982
F1 score per class: {1: 0.32167832167832167, 2: 0.8, 3: 0.464, 6: 0.4647887323943662, 7: 0.07272727272727272, 9: 0.9803921568627451, 11: 0.1414141414141414, 12: 0.5531914893617021, 14: 0.13675213675213677, 15: 0.75, 19: 0.32116788321167883, 22: 0.5512820512820513, 24: 0.07407407407407407, 25: 0.7816091954022989, 26: 0.7567567567567568, 27: 0.0, 28: 0.4, 29: 0.93048128342246, 31: 0.0, 32: 0.6451612903225806, 34: 0.3287671232876712, 35: 0.49612403100775193, 37: 0.42857142857142855, 38: 0.6875, 39: 0.13333333333333333, 40: 0.4864864864864865}
Micro-average F1 score: 0.5038167938931297
Weighted-average F1 score: 0.5023443633339544
cur_acc:  ['0.8178', '0.4710', '0.4402', '0.6139', '0.3017']
his_acc:  ['0.8178', '0.7339', '0.5685', '0.5431', '0.4111']
cur_acc des:  ['0.8282', '0.6457', '0.5545', '0.8618', '0.4042']
his_acc des:  ['0.8282', '0.7811', '0.6727', '0.6800', '0.5513']
cur_acc rrf:  ['0.8311', '0.6667', '0.5545', '0.8045', '0.3848']
his_acc rrf:  ['0.8311', '0.7721', '0.6611', '0.6391', '0.5038']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 100.0724503
CurrentTrain: epoch  0, batch     1 | loss: 82.0281406
CurrentTrain: epoch  0, batch     2 | loss: 75.6140794
CurrentTrain: epoch  0, batch     3 | loss: 108.7531628
CurrentTrain: epoch  1, batch     0 | loss: 102.9817660
CurrentTrain: epoch  1, batch     1 | loss: 69.7133130
CurrentTrain: epoch  1, batch     2 | loss: 58.6194108
CurrentTrain: epoch  1, batch     3 | loss: 50.6019426
CurrentTrain: epoch  2, batch     0 | loss: 68.4629612
CurrentTrain: epoch  2, batch     1 | loss: 60.0302848
CurrentTrain: epoch  2, batch     2 | loss: 74.2250868
CurrentTrain: epoch  2, batch     3 | loss: 71.1258701
CurrentTrain: epoch  3, batch     0 | loss: 74.6821571
CurrentTrain: epoch  3, batch     1 | loss: 54.1022302
CurrentTrain: epoch  3, batch     2 | loss: 72.5851463
CurrentTrain: epoch  3, batch     3 | loss: 53.2702703
CurrentTrain: epoch  4, batch     0 | loss: 85.3232522
CurrentTrain: epoch  4, batch     1 | loss: 70.3626653
CurrentTrain: epoch  4, batch     2 | loss: 68.4461824
CurrentTrain: epoch  4, batch     3 | loss: 53.6436926
CurrentTrain: epoch  5, batch     0 | loss: 86.8835080
CurrentTrain: epoch  5, batch     1 | loss: 83.0340601
CurrentTrain: epoch  5, batch     2 | loss: 63.6330183
CurrentTrain: epoch  5, batch     3 | loss: 149.4338598
CurrentTrain: epoch  6, batch     0 | loss: 84.7443437
CurrentTrain: epoch  6, batch     1 | loss: 54.5759007
CurrentTrain: epoch  6, batch     2 | loss: 115.7546802
CurrentTrain: epoch  6, batch     3 | loss: 67.5752350
CurrentTrain: epoch  7, batch     0 | loss: 64.9170448
CurrentTrain: epoch  7, batch     1 | loss: 63.0095636
CurrentTrain: epoch  7, batch     2 | loss: 87.7742755
CurrentTrain: epoch  7, batch     3 | loss: 53.3136428
CurrentTrain: epoch  8, batch     0 | loss: 67.3858625
CurrentTrain: epoch  8, batch     1 | loss: 67.3054553
CurrentTrain: epoch  8, batch     2 | loss: 52.1389559
CurrentTrain: epoch  8, batch     3 | loss: 66.6617404
CurrentTrain: epoch  9, batch     0 | loss: 52.9454483
CurrentTrain: epoch  9, batch     1 | loss: 66.4417446
CurrentTrain: epoch  9, batch     2 | loss: 65.1317546
CurrentTrain: epoch  9, batch     3 | loss: 66.4520090
MemoryTrain:  epoch  0, batch     0 | loss: 0.5553893
MemoryTrain:  epoch  1, batch     0 | loss: 0.4373607
MemoryTrain:  epoch  2, batch     0 | loss: 0.3361742
MemoryTrain:  epoch  3, batch     0 | loss: 0.2421401
MemoryTrain:  epoch  4, batch     0 | loss: 0.1867985
MemoryTrain:  epoch  5, batch     0 | loss: 0.1565414
MemoryTrain:  epoch  6, batch     0 | loss: 0.1184413
MemoryTrain:  epoch  7, batch     0 | loss: 0.1045793
MemoryTrain:  epoch  8, batch     0 | loss: 0.1051747
MemoryTrain:  epoch  9, batch     0 | loss: 0.0856032

F1 score per class: {0: 0.9722222222222222, 32: 0.0, 34: 0.0, 2: 0.918918918918919, 4: 0.5714285714285714, 1: 0.0, 37: 0.0, 13: 0.5581395348837209, 14: 0.0, 15: 0.8809523809523809, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8221153846153846
Weighted-average F1 score: 0.7784268139927846
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.0, 34: 0.93048128342246, 4: 0.0, 1: 0.5714285714285714, 11: 0.0, 13: 0.0, 14: 0.6521739130434783, 15: 0.8674698795180723, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.8205128205128205
Weighted-average F1 score: 0.7589143659042221
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.0, 34: 0.9361702127659575, 4: 0.0, 1: 0.75, 38: 0.0, 11: 0.0, 13: 0.6521739130434783, 14: 0.8536585365853658, 15: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.8251748251748252
Weighted-average F1 score: 0.7645956244718312

F1 score per class: {0: 0.958904109589041, 1: 0.32, 2: 0.6363636363636364, 3: 0.7448275862068966, 4: 0.918918918918919, 6: 0.42028985507246375, 7: 0.09302325581395349, 9: 0.9803921568627451, 11: 0.02197802197802198, 12: 0.4094488188976378, 13: 0.2, 14: 0.09523809523809523, 15: 0.5, 19: 0.5714285714285714, 21: 0.32, 22: 0.6040268456375839, 23: 0.8409090909090909, 24: 0.08695652173913043, 25: 0.7, 26: 0.7431693989071039, 27: 0.0, 28: 0.0, 29: 0.9081081081081082, 31: 0.6666666666666666, 32: 0.4927536231884058, 34: 0.14736842105263157, 35: 0.208955223880597, 37: 0.47398843930635837, 38: 0.6521739130434783, 39: 0.0, 40: 0.3695652173913043}
Micro-average F1 score: 0.5411426518145885
Weighted-average F1 score: 0.5682320125491277
F1 score per class: {0: 0.972972972972973, 1: 0.32894736842105265, 2: 0.6086956521739131, 3: 0.7866666666666666, 4: 0.93048128342246, 6: 0.5298013245033113, 7: 0.08163265306122448, 9: 0.9803921568627451, 11: 0.27586206896551724, 12: 0.7361963190184049, 13: 0.21052631578947367, 14: 0.08888888888888889, 15: 0.6, 19: 0.6, 21: 0.30612244897959184, 22: 0.5673758865248227, 23: 0.8181818181818182, 24: 0.0, 25: 0.851063829787234, 26: 0.7340425531914894, 27: 0.0, 28: 0.26666666666666666, 29: 0.9533678756476683, 31: 0.6666666666666666, 32: 0.7441860465116279, 34: 0.2727272727272727, 35: 0.6902654867256637, 37: 0.45112781954887216, 38: 0.7857142857142857, 39: 0.13333333333333333, 40: 0.559322033898305}
Micro-average F1 score: 0.6101472995090016
Weighted-average F1 score: 0.6016806903557594
F1 score per class: {0: 0.972972972972973, 1: 0.32894736842105265, 2: 0.5833333333333334, 3: 0.7283950617283951, 4: 0.9361702127659575, 6: 0.5584415584415584, 7: 0.08888888888888889, 9: 0.9803921568627451, 11: 0.2545454545454545, 12: 0.6792452830188679, 13: 0.2727272727272727, 14: 0.0851063829787234, 15: 0.5454545454545454, 19: 0.6113989637305699, 21: 0.30927835051546393, 22: 0.5571428571428572, 23: 0.8045977011494253, 24: 0.0, 25: 0.8421052631578947, 26: 0.7379679144385026, 27: 0.0, 28: 0.2, 29: 0.9368421052631579, 31: 0.8, 32: 0.7325581395348837, 34: 0.24113475177304963, 35: 0.46808510638297873, 37: 0.43243243243243246, 38: 0.75, 39: 0.13333333333333333, 40: 0.5137614678899083}
Micro-average F1 score: 0.5926170072511536
Weighted-average F1 score: 0.5862335873868798
cur_acc:  ['0.8178', '0.4710', '0.4402', '0.6139', '0.3017', '0.8221']
his_acc:  ['0.8178', '0.7339', '0.5685', '0.5431', '0.4111', '0.5411']
cur_acc des:  ['0.8282', '0.6457', '0.5545', '0.8618', '0.4042', '0.8205']
his_acc des:  ['0.8282', '0.7811', '0.6727', '0.6800', '0.5513', '0.6101']
cur_acc rrf:  ['0.8311', '0.6667', '0.5545', '0.8045', '0.3848', '0.8252']
his_acc rrf:  ['0.8311', '0.7721', '0.6611', '0.6391', '0.5038', '0.5926']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 77.0199755
CurrentTrain: epoch  0, batch     1 | loss: 78.6844423
CurrentTrain: epoch  0, batch     2 | loss: 74.9371270
CurrentTrain: epoch  0, batch     3 | loss: 62.6900984
CurrentTrain: epoch  1, batch     0 | loss: 73.2259094
CurrentTrain: epoch  1, batch     1 | loss: 62.3355575
CurrentTrain: epoch  1, batch     2 | loss: 56.8400882
CurrentTrain: epoch  1, batch     3 | loss: 54.8233952
CurrentTrain: epoch  2, batch     0 | loss: 90.1633829
CurrentTrain: epoch  2, batch     1 | loss: 64.6622298
CurrentTrain: epoch  2, batch     2 | loss: 70.3499809
CurrentTrain: epoch  2, batch     3 | loss: 33.9686559
CurrentTrain: epoch  3, batch     0 | loss: 67.0783003
CurrentTrain: epoch  3, batch     1 | loss: 69.7232820
CurrentTrain: epoch  3, batch     2 | loss: 66.7874019
CurrentTrain: epoch  3, batch     3 | loss: 38.9615495
CurrentTrain: epoch  4, batch     0 | loss: 53.3316776
CurrentTrain: epoch  4, batch     1 | loss: 82.1453610
CurrentTrain: epoch  4, batch     2 | loss: 65.4675572
CurrentTrain: epoch  4, batch     3 | loss: 72.4682366
CurrentTrain: epoch  5, batch     0 | loss: 51.7295462
CurrentTrain: epoch  5, batch     1 | loss: 85.6703211
CurrentTrain: epoch  5, batch     2 | loss: 114.3799391
CurrentTrain: epoch  5, batch     3 | loss: 48.3943381
CurrentTrain: epoch  6, batch     0 | loss: 52.3885909
CurrentTrain: epoch  6, batch     1 | loss: 84.0106644
CurrentTrain: epoch  6, batch     2 | loss: 83.7806581
CurrentTrain: epoch  6, batch     3 | loss: 36.8706431
CurrentTrain: epoch  7, batch     0 | loss: 49.0921496
CurrentTrain: epoch  7, batch     1 | loss: 63.9999796
CurrentTrain: epoch  7, batch     2 | loss: 64.9010125
CurrentTrain: epoch  7, batch     3 | loss: 232.1516542
CurrentTrain: epoch  8, batch     0 | loss: 53.7920437
CurrentTrain: epoch  8, batch     1 | loss: 114.0009058
CurrentTrain: epoch  8, batch     2 | loss: 51.2038870
CurrentTrain: epoch  8, batch     3 | loss: 37.3631554
CurrentTrain: epoch  9, batch     0 | loss: 49.6159569
CurrentTrain: epoch  9, batch     1 | loss: 64.9023020
CurrentTrain: epoch  9, batch     2 | loss: 108.4577082
CurrentTrain: epoch  9, batch     3 | loss: 108.0905464
MemoryTrain:  epoch  0, batch     0 | loss: 0.2133813
MemoryTrain:  epoch  1, batch     0 | loss: 0.2094327
MemoryTrain:  epoch  2, batch     0 | loss: 0.1622009
MemoryTrain:  epoch  3, batch     0 | loss: 0.1268955
MemoryTrain:  epoch  4, batch     0 | loss: 0.1023240
MemoryTrain:  epoch  5, batch     0 | loss: 0.0983931
MemoryTrain:  epoch  6, batch     0 | loss: 0.0829568
MemoryTrain:  epoch  7, batch     0 | loss: 0.0728986
MemoryTrain:  epoch  8, batch     0 | loss: 0.0654755
MemoryTrain:  epoch  9, batch     0 | loss: 0.0613342

F1 score per class: {33: 0.0, 34: 0.4117647058823529, 35: 0.0, 36: 0.0, 37: 0.851063829787234, 38: 0.0, 7: 0.0, 8: 0.972972972972973, 12: 0.42857142857142855, 13: 0.0, 20: 0.0, 26: 0.4827586206896552, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5819209039548022
Weighted-average F1 score: 0.5823314703148726
F1 score per class: {7: 0.0, 8: 0.7142857142857143, 11: 0.0, 13: 0.0, 20: 0.92, 21: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9142857142857143, 33: 0.42857142857142855, 34: 0.0, 35: 0.0, 36: 0.8035714285714286, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.7451923076923077
Weighted-average F1 score: 0.6883320463320463
F1 score per class: {33: 0.0, 34: 0.7142857142857143, 35: 0.0, 36: 0.0, 37: 0.92, 38: 0.0, 7: 0.0, 8: 0.0, 12: 0.972972972972973, 13: 0.4, 20: 0.0, 25: 0.0, 26: 0.7476635514018691, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7325301204819277
Weighted-average F1 score: 0.6751661753469962

F1 score per class: {0: 0.9444444444444444, 1: 0.29931972789115646, 2: 0.6666666666666666, 3: 0.6141732283464567, 4: 0.9473684210526315, 6: 0.4316546762589928, 7: 0.05405405405405406, 8: 0.3853211009174312, 9: 0.9803921568627451, 11: 0.0, 12: 0.3387096774193548, 13: 0.16666666666666666, 14: 0.05194805194805195, 15: 0.7777777777777778, 19: 0.45714285714285713, 20: 0.8247422680412371, 21: 0.23880597014925373, 22: 0.6068965517241379, 23: 0.7341772151898734, 24: 0.09090909090909091, 25: 0.5753424657534246, 26: 0.7263157894736842, 27: 0.0, 28: 0.0, 29: 0.9032258064516129, 30: 0.9473684210526315, 31: 0.6666666666666666, 32: 0.44776119402985076, 33: 0.35294117647058826, 34: 0.12903225806451613, 35: 0.16, 36: 0.46153846153846156, 37: 0.4897959183673469, 38: 0.4878048780487805, 39: 0.0, 40: 0.24096385542168675}
Micro-average F1 score: 0.5176151761517616
Weighted-average F1 score: 0.5627911557773253
F1 score per class: {0: 0.972972972972973, 1: 0.3150684931506849, 2: 0.5833333333333334, 3: 0.7222222222222222, 4: 0.9361702127659575, 6: 0.5234899328859061, 7: 0.09090909090909091, 8: 0.6428571428571429, 9: 0.9803921568627451, 11: 0.06315789473684211, 12: 0.7160493827160493, 13: 0.18181818181818182, 14: 0.0975609756097561, 15: 0.6, 19: 0.5955056179775281, 20: 0.9108910891089109, 21: 0.3106796116504854, 22: 0.6164383561643836, 23: 0.7764705882352941, 24: 0.08333333333333333, 25: 0.8210526315789474, 26: 0.7195767195767195, 27: 0.0, 28: 0.26666666666666666, 29: 0.9381443298969072, 30: 0.6037735849056604, 31: 0.5714285714285714, 32: 0.7294117647058823, 33: 0.13043478260869565, 34: 0.23529411764705882, 35: 0.5454545454545454, 36: 0.6122448979591837, 37: 0.3157894736842105, 38: 0.6666666666666666, 39: 0.125, 40: 0.5272727272727272}
Micro-average F1 score: 0.5983510011778563
Weighted-average F1 score: 0.6010176655583394
F1 score per class: {0: 0.972972972972973, 1: 0.3221476510067114, 2: 0.5217391304347826, 3: 0.7123287671232876, 4: 0.9361702127659575, 6: 0.5234899328859061, 7: 0.09090909090909091, 8: 0.6338028169014085, 9: 0.9803921568627451, 11: 0.021739130434782608, 12: 0.7073170731707317, 13: 0.26666666666666666, 14: 0.09302325581395349, 15: 0.5714285714285714, 19: 0.5792349726775956, 20: 0.9019607843137255, 21: 0.3125, 22: 0.6068965517241379, 23: 0.7857142857142857, 24: 0.08333333333333333, 25: 0.7956989247311828, 26: 0.723404255319149, 27: 0.0, 28: 0.2222222222222222, 29: 0.9270833333333334, 30: 0.6923076923076923, 31: 0.5714285714285714, 32: 0.7368421052631579, 33: 0.125, 34: 0.20754716981132076, 35: 0.40404040404040403, 36: 0.5925925925925926, 37: 0.34710743801652894, 38: 0.6666666666666666, 39: 0.0, 40: 0.45544554455445546}
Micro-average F1 score: 0.5869114598756292
Weighted-average F1 score: 0.5913917175043621
cur_acc:  ['0.8178', '0.4710', '0.4402', '0.6139', '0.3017', '0.8221', '0.5819']
his_acc:  ['0.8178', '0.7339', '0.5685', '0.5431', '0.4111', '0.5411', '0.5176']
cur_acc des:  ['0.8282', '0.6457', '0.5545', '0.8618', '0.4042', '0.8205', '0.7452']
his_acc des:  ['0.8282', '0.7811', '0.6727', '0.6800', '0.5513', '0.6101', '0.5984']
cur_acc rrf:  ['0.8311', '0.6667', '0.5545', '0.8045', '0.3848', '0.8252', '0.7325']
his_acc rrf:  ['0.8311', '0.7721', '0.6611', '0.6391', '0.5038', '0.5926', '0.5869']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 83.7022467
CurrentTrain: epoch  0, batch     1 | loss: 96.4618761
CurrentTrain: epoch  0, batch     2 | loss: 98.0412234
CurrentTrain: epoch  0, batch     3 | loss: 92.7226077
CurrentTrain: epoch  0, batch     4 | loss: 38.6579340
CurrentTrain: epoch  1, batch     0 | loss: 70.8627250
CurrentTrain: epoch  1, batch     1 | loss: 77.6822652
CurrentTrain: epoch  1, batch     2 | loss: 73.8544956
CurrentTrain: epoch  1, batch     3 | loss: 121.3121877
CurrentTrain: epoch  1, batch     4 | loss: 75.2537237
CurrentTrain: epoch  2, batch     0 | loss: 122.0447594
CurrentTrain: epoch  2, batch     1 | loss: 65.8042408
CurrentTrain: epoch  2, batch     2 | loss: 85.9444916
CurrentTrain: epoch  2, batch     3 | loss: 118.2379125
CurrentTrain: epoch  2, batch     4 | loss: 58.9751484
CurrentTrain: epoch  3, batch     0 | loss: 184.8777163
CurrentTrain: epoch  3, batch     1 | loss: 81.4107430
CurrentTrain: epoch  3, batch     2 | loss: 56.3260648
CurrentTrain: epoch  3, batch     3 | loss: 59.3200524
CurrentTrain: epoch  3, batch     4 | loss: 112.2296530
CurrentTrain: epoch  4, batch     0 | loss: 86.3121802
CurrentTrain: epoch  4, batch     1 | loss: 86.2576503
CurrentTrain: epoch  4, batch     2 | loss: 66.2768765
CurrentTrain: epoch  4, batch     3 | loss: 69.0839445
CurrentTrain: epoch  4, batch     4 | loss: 53.9692602
CurrentTrain: epoch  5, batch     0 | loss: 83.7964049
CurrentTrain: epoch  5, batch     1 | loss: 65.8182659
CurrentTrain: epoch  5, batch     2 | loss: 64.6888528
CurrentTrain: epoch  5, batch     3 | loss: 119.0270364
CurrentTrain: epoch  5, batch     4 | loss: 55.1528215
CurrentTrain: epoch  6, batch     0 | loss: 86.3740331
CurrentTrain: epoch  6, batch     1 | loss: 65.8489065
CurrentTrain: epoch  6, batch     2 | loss: 64.7292299
CurrentTrain: epoch  6, batch     3 | loss: 54.0939036
CurrentTrain: epoch  6, batch     4 | loss: 54.8278642
CurrentTrain: epoch  7, batch     0 | loss: 115.8117883
CurrentTrain: epoch  7, batch     1 | loss: 64.9024212
CurrentTrain: epoch  7, batch     2 | loss: 86.6940859
CurrentTrain: epoch  7, batch     3 | loss: 53.0305025
CurrentTrain: epoch  7, batch     4 | loss: 40.8901394
CurrentTrain: epoch  8, batch     0 | loss: 62.7154291
CurrentTrain: epoch  8, batch     1 | loss: 86.5582450
CurrentTrain: epoch  8, batch     2 | loss: 83.7534305
CurrentTrain: epoch  8, batch     3 | loss: 85.9948766
CurrentTrain: epoch  8, batch     4 | loss: 39.7258773
CurrentTrain: epoch  9, batch     0 | loss: 82.8414701
CurrentTrain: epoch  9, batch     1 | loss: 80.2540923
CurrentTrain: epoch  9, batch     2 | loss: 118.0731226
CurrentTrain: epoch  9, batch     3 | loss: 64.8052465
CurrentTrain: epoch  9, batch     4 | loss: 40.6973199
MemoryTrain:  epoch  0, batch     0 | loss: 0.1784712
MemoryTrain:  epoch  1, batch     0 | loss: 0.2184405
MemoryTrain:  epoch  2, batch     0 | loss: 0.1599930
MemoryTrain:  epoch  3, batch     0 | loss: 0.1155461
MemoryTrain:  epoch  4, batch     0 | loss: 0.1044360
MemoryTrain:  epoch  5, batch     0 | loss: 0.0901819
MemoryTrain:  epoch  6, batch     0 | loss: 0.0854286
MemoryTrain:  epoch  7, batch     0 | loss: 0.0742944
MemoryTrain:  epoch  8, batch     0 | loss: 0.0690039
MemoryTrain:  epoch  9, batch     0 | loss: 0.0605335

F1 score per class: {34: 0.9795918367346939, 5: 0.0, 38: 0.0, 7: 0.0, 8: 0.23008849557522124, 10: 0.0, 6: 0.9090909090909091, 13: 0.5, 16: 0.7666666666666667, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.6808510638297872
Weighted-average F1 score: 0.7159410970598686
F1 score per class: {34: 1.0, 5: 0.0, 38: 0.0, 6: 0.0, 8: 0.4496124031007752, 10: 0.0, 7: 0.9655172413793104, 13: 0.8, 16: 0.9117647058823529, 17: 0.0, 18: 0.0, 28: 0.0}
Micro-average F1 score: 0.7252336448598131
Weighted-average F1 score: 0.6684862869918768
F1 score per class: {34: 1.0, 5: 0.0, 38: 0.0, 6: 0.0, 8: 0.3870967741935484, 10: 0.0, 7: 0.9655172413793104, 13: 0.8, 16: 0.9117647058823529, 17: 0.0, 18: 0.0, 28: 0.0}
Micro-average F1 score: 0.7078651685393258
Weighted-average F1 score: 0.6565484931070497

F1 score per class: {0: 0.8985507246376812, 1: 0.30666666666666664, 2: 0.625, 3: 0.6821705426356589, 4: 0.918918918918919, 5: 0.8930232558139535, 6: 0.2809917355371901, 7: 0.0, 8: 0.22641509433962265, 9: 0.9803921568627451, 10: 0.23008849557522124, 11: 0.0, 12: 0.24347826086956523, 13: 0.06451612903225806, 14: 0.05333333333333334, 15: 0.75, 16: 0.8333333333333334, 17: 0.2222222222222222, 18: 0.3129251700680272, 19: 0.5888324873096447, 20: 0.7872340425531915, 21: 0.29508196721311475, 22: 0.6164383561643836, 23: 0.75, 24: 0.08333333333333333, 25: 0.6666666666666666, 26: 0.7431693989071039, 27: 0.0, 28: 0.0, 29: 0.8791208791208791, 30: 0.918918918918919, 31: 0.5, 32: 0.5211267605633803, 33: 0.35294117647058826, 34: 0.07228915662650602, 35: 0.13559322033898305, 36: 0.1891891891891892, 37: 0.2465753424657534, 38: 0.35, 39: 0.0, 40: 0.3448275862068966}
Micro-average F1 score: 0.5118978525827046
Weighted-average F1 score: 0.5693384094105819
F1 score per class: {0: 0.9444444444444444, 1: 0.3087248322147651, 2: 0.48, 3: 0.7972972972972973, 4: 0.9130434782608695, 5: 0.8583690987124464, 6: 0.5100671140939598, 7: 0.09523809523809523, 8: 0.5477707006369427, 9: 0.9803921568627451, 10: 0.43609022556390975, 11: 0.022222222222222223, 12: 0.6496815286624203, 13: 0.05555555555555555, 14: 0.10256410256410256, 15: 0.631578947368421, 16: 0.8615384615384616, 17: 0.42857142857142855, 18: 0.2683982683982684, 19: 0.673469387755102, 20: 0.8823529411764706, 21: 0.30927835051546393, 22: 0.6490066225165563, 23: 0.8235294117647058, 24: 0.08695652173913043, 25: 0.735632183908046, 26: 0.7351351351351352, 27: 0.0, 28: 0.13333333333333333, 29: 0.9278350515463918, 30: 0.8, 31: 0.5714285714285714, 32: 0.7624309392265194, 33: 0.2857142857142857, 34: 0.07142857142857142, 35: 0.6355140186915887, 36: 0.6095238095238096, 37: 0.2619047619047619, 38: 0.4528301886792453, 39: 0.10526315789473684, 40: 0.5370370370370371}
Micro-average F1 score: 0.5908977866202437
Weighted-average F1 score: 0.5908990783675834
F1 score per class: {0: 0.9444444444444444, 1: 0.31788079470198677, 2: 0.48, 3: 0.7870967741935484, 4: 0.9130434782608695, 5: 0.8583690987124464, 6: 0.4827586206896552, 7: 0.1, 8: 0.5534591194968553, 9: 0.9803921568627451, 10: 0.38095238095238093, 11: 0.0, 12: 0.6369426751592356, 13: 0.0425531914893617, 14: 0.0975609756097561, 15: 0.631578947368421, 16: 0.8615384615384616, 17: 0.41379310344827586, 18: 0.25833333333333336, 19: 0.6633165829145728, 20: 0.8653846153846154, 21: 0.3157894736842105, 22: 0.6533333333333333, 23: 0.8235294117647058, 24: 0.08695652173913043, 25: 0.7058823529411765, 26: 0.7351351351351352, 27: 0.0, 28: 0.13333333333333333, 29: 0.9270833333333334, 30: 0.8181818181818182, 31: 0.5, 32: 0.7624309392265194, 33: 0.2962962962962963, 34: 0.07142857142857142, 35: 0.4090909090909091, 36: 0.48936170212765956, 37: 0.2619047619047619, 38: 0.4528301886792453, 39: 0.0, 40: 0.5333333333333333}
Micro-average F1 score: 0.5755683237571821
Weighted-average F1 score: 0.5774721309227875
cur_acc:  ['0.8178', '0.4710', '0.4402', '0.6139', '0.3017', '0.8221', '0.5819', '0.6809']
his_acc:  ['0.8178', '0.7339', '0.5685', '0.5431', '0.4111', '0.5411', '0.5176', '0.5119']
cur_acc des:  ['0.8282', '0.6457', '0.5545', '0.8618', '0.4042', '0.8205', '0.7452', '0.7252']
his_acc des:  ['0.8282', '0.7811', '0.6727', '0.6800', '0.5513', '0.6101', '0.5984', '0.5909']
cur_acc rrf:  ['0.8311', '0.6667', '0.5545', '0.8045', '0.3848', '0.8252', '0.7325', '0.7079']
his_acc rrf:  ['0.8311', '0.7721', '0.6611', '0.6391', '0.5038', '0.5926', '0.5869', '0.5756']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 68.8886432
CurrentTrain: epoch  0, batch     1 | loss: 78.5997972
CurrentTrain: epoch  0, batch     2 | loss: 58.2360220
CurrentTrain: epoch  0, batch     3 | loss: 79.6117612
CurrentTrain: epoch  0, batch     4 | loss: 79.4597458
CurrentTrain: epoch  0, batch     5 | loss: 78.3312879
CurrentTrain: epoch  0, batch     6 | loss: 77.6352442
CurrentTrain: epoch  0, batch     7 | loss: 77.8872253
CurrentTrain: epoch  0, batch     8 | loss: 78.4803654
CurrentTrain: epoch  0, batch     9 | loss: 77.3485109
CurrentTrain: epoch  0, batch    10 | loss: 96.9143520
CurrentTrain: epoch  0, batch    11 | loss: 127.1186922
CurrentTrain: epoch  0, batch    12 | loss: 64.8752463
CurrentTrain: epoch  0, batch    13 | loss: 76.8117812
CurrentTrain: epoch  0, batch    14 | loss: 80.0051574
CurrentTrain: epoch  0, batch    15 | loss: 95.4292163
CurrentTrain: epoch  0, batch    16 | loss: 77.3211255
CurrentTrain: epoch  0, batch    17 | loss: 77.6769518
CurrentTrain: epoch  0, batch    18 | loss: 55.4478118
CurrentTrain: epoch  0, batch    19 | loss: 95.5207923
CurrentTrain: epoch  0, batch    20 | loss: 95.7692078
CurrentTrain: epoch  0, batch    21 | loss: 96.2361323
CurrentTrain: epoch  0, batch    22 | loss: 64.8721493
CurrentTrain: epoch  0, batch    23 | loss: 65.1241151
CurrentTrain: epoch  0, batch    24 | loss: 55.6601587
CurrentTrain: epoch  0, batch    25 | loss: 76.1334228
CurrentTrain: epoch  0, batch    26 | loss: 64.4789813
CurrentTrain: epoch  0, batch    27 | loss: 55.7481568
CurrentTrain: epoch  0, batch    28 | loss: 76.8251763
CurrentTrain: epoch  0, batch    29 | loss: 76.6608278
CurrentTrain: epoch  0, batch    30 | loss: 125.9957001
CurrentTrain: epoch  0, batch    31 | loss: 77.1364976
CurrentTrain: epoch  0, batch    32 | loss: 125.4798639
CurrentTrain: epoch  0, batch    33 | loss: 76.4593007
CurrentTrain: epoch  0, batch    34 | loss: 94.7898628
CurrentTrain: epoch  0, batch    35 | loss: 94.7047934
CurrentTrain: epoch  0, batch    36 | loss: 75.9970554
CurrentTrain: epoch  0, batch    37 | loss: 77.1999566
CurrentTrain: epoch  0, batch    38 | loss: 76.3481572
CurrentTrain: epoch  0, batch    39 | loss: 124.8728577
CurrentTrain: epoch  0, batch    40 | loss: 76.1178964
CurrentTrain: epoch  0, batch    41 | loss: 75.6741850
CurrentTrain: epoch  0, batch    42 | loss: 64.0639403
CurrentTrain: epoch  0, batch    43 | loss: 77.4834005
CurrentTrain: epoch  0, batch    44 | loss: 75.4732642
CurrentTrain: epoch  0, batch    45 | loss: 76.2884784
CurrentTrain: epoch  0, batch    46 | loss: 76.5928270
CurrentTrain: epoch  0, batch    47 | loss: 75.7959785
CurrentTrain: epoch  0, batch    48 | loss: 125.0841338
CurrentTrain: epoch  0, batch    49 | loss: 94.1369523
CurrentTrain: epoch  0, batch    50 | loss: 63.2227367
CurrentTrain: epoch  0, batch    51 | loss: 63.7359783
CurrentTrain: epoch  0, batch    52 | loss: 76.9385322
CurrentTrain: epoch  0, batch    53 | loss: 62.4551045
CurrentTrain: epoch  0, batch    54 | loss: 64.0155874
CurrentTrain: epoch  0, batch    55 | loss: 94.5309181
CurrentTrain: epoch  0, batch    56 | loss: 76.3562167
CurrentTrain: epoch  0, batch    57 | loss: 62.6817198
CurrentTrain: epoch  0, batch    58 | loss: 93.3578534
CurrentTrain: epoch  0, batch    59 | loss: 92.3377410
CurrentTrain: epoch  0, batch    60 | loss: 74.6570533
CurrentTrain: epoch  0, batch    61 | loss: 76.2961566
CurrentTrain: epoch  0, batch    62 | loss: 53.2153089
CurrentTrain: epoch  0, batch    63 | loss: 61.6768232
CurrentTrain: epoch  0, batch    64 | loss: 126.1254308
CurrentTrain: epoch  0, batch    65 | loss: 64.2043027
CurrentTrain: epoch  0, batch    66 | loss: 72.3062875
CurrentTrain: epoch  0, batch    67 | loss: 53.5993413
CurrentTrain: epoch  0, batch    68 | loss: 75.9821238
CurrentTrain: epoch  0, batch    69 | loss: 124.8123985
CurrentTrain: epoch  0, batch    70 | loss: 62.3593549
CurrentTrain: epoch  0, batch    71 | loss: 73.2636832
CurrentTrain: epoch  0, batch    72 | loss: 125.7206266
CurrentTrain: epoch  0, batch    73 | loss: 62.4154854
CurrentTrain: epoch  0, batch    74 | loss: 124.6065433
CurrentTrain: epoch  0, batch    75 | loss: 62.1692248
CurrentTrain: epoch  0, batch    76 | loss: 72.9662686
CurrentTrain: epoch  0, batch    77 | loss: 72.7656356
CurrentTrain: epoch  0, batch    78 | loss: 184.3049064
CurrentTrain: epoch  0, batch    79 | loss: 75.0792538
CurrentTrain: epoch  0, batch    80 | loss: 92.9195814
CurrentTrain: epoch  0, batch    81 | loss: 71.3454889
CurrentTrain: epoch  0, batch    82 | loss: 62.1975508
CurrentTrain: epoch  0, batch    83 | loss: 123.7167532
CurrentTrain: epoch  0, batch    84 | loss: 93.3090111
CurrentTrain: epoch  0, batch    85 | loss: 75.1190386
CurrentTrain: epoch  0, batch    86 | loss: 94.1382732
CurrentTrain: epoch  0, batch    87 | loss: 93.9330948
CurrentTrain: epoch  0, batch    88 | loss: 73.3791280
CurrentTrain: epoch  0, batch    89 | loss: 93.0160975
CurrentTrain: epoch  0, batch    90 | loss: 89.6681315
CurrentTrain: epoch  0, batch    91 | loss: 90.8195571
CurrentTrain: epoch  0, batch    92 | loss: 72.1510171
CurrentTrain: epoch  0, batch    93 | loss: 60.2538331
CurrentTrain: epoch  0, batch    94 | loss: 73.8574469
CurrentTrain: epoch  0, batch    95 | loss: 50.8508342
CurrentTrain: epoch  1, batch     0 | loss: 59.3302208
CurrentTrain: epoch  1, batch     1 | loss: 120.1016579
CurrentTrain: epoch  1, batch     2 | loss: 70.3085095
CurrentTrain: epoch  1, batch     3 | loss: 72.9567376
CurrentTrain: epoch  1, batch     4 | loss: 60.6756973
CurrentTrain: epoch  1, batch     5 | loss: 71.0456491
CurrentTrain: epoch  1, batch     6 | loss: 90.6547815
CurrentTrain: epoch  1, batch     7 | loss: 121.0449632
CurrentTrain: epoch  1, batch     8 | loss: 88.9741661
CurrentTrain: epoch  1, batch     9 | loss: 70.9101520
CurrentTrain: epoch  1, batch    10 | loss: 51.6841912
CurrentTrain: epoch  1, batch    11 | loss: 72.8536068
CurrentTrain: epoch  1, batch    12 | loss: 121.0152271
CurrentTrain: epoch  1, batch    13 | loss: 121.8085876
CurrentTrain: epoch  1, batch    14 | loss: 59.8056607
CurrentTrain: epoch  1, batch    15 | loss: 49.0169172
CurrentTrain: epoch  1, batch    16 | loss: 70.8879560
CurrentTrain: epoch  1, batch    17 | loss: 73.7170862
CurrentTrain: epoch  1, batch    18 | loss: 92.5129059
CurrentTrain: epoch  1, batch    19 | loss: 119.4850767
CurrentTrain: epoch  1, batch    20 | loss: 69.1287518
CurrentTrain: epoch  1, batch    21 | loss: 70.9195505
CurrentTrain: epoch  1, batch    22 | loss: 60.5214507
CurrentTrain: epoch  1, batch    23 | loss: 57.8819332
CurrentTrain: epoch  1, batch    24 | loss: 89.0431851
CurrentTrain: epoch  1, batch    25 | loss: 91.4795073
CurrentTrain: epoch  1, batch    26 | loss: 88.2353453
CurrentTrain: epoch  1, batch    27 | loss: 50.5430541
CurrentTrain: epoch  1, batch    28 | loss: 87.3423832
CurrentTrain: epoch  1, batch    29 | loss: 73.4411601
CurrentTrain: epoch  1, batch    30 | loss: 69.7910030
CurrentTrain: epoch  1, batch    31 | loss: 74.4925935
CurrentTrain: epoch  1, batch    32 | loss: 89.3992231
CurrentTrain: epoch  1, batch    33 | loss: 50.5195453
CurrentTrain: epoch  1, batch    34 | loss: 88.6076026
CurrentTrain: epoch  1, batch    35 | loss: 59.7211100
CurrentTrain: epoch  1, batch    36 | loss: 72.0277795
CurrentTrain: epoch  1, batch    37 | loss: 70.2795267
CurrentTrain: epoch  1, batch    38 | loss: 50.8133467
CurrentTrain: epoch  1, batch    39 | loss: 90.3976153
CurrentTrain: epoch  1, batch    40 | loss: 57.6371582
CurrentTrain: epoch  1, batch    41 | loss: 75.2486535
CurrentTrain: epoch  1, batch    42 | loss: 48.6638258
CurrentTrain: epoch  1, batch    43 | loss: 72.6428206
CurrentTrain: epoch  1, batch    44 | loss: 67.2075063
CurrentTrain: epoch  1, batch    45 | loss: 58.1203358
CurrentTrain: epoch  1, batch    46 | loss: 54.2649698
CurrentTrain: epoch  1, batch    47 | loss: 57.4033330
CurrentTrain: epoch  1, batch    48 | loss: 66.5338979
CurrentTrain: epoch  1, batch    49 | loss: 57.8204010
CurrentTrain: epoch  1, batch    50 | loss: 183.4077428
CurrentTrain: epoch  1, batch    51 | loss: 85.7633715
CurrentTrain: epoch  1, batch    52 | loss: 88.9983713
CurrentTrain: epoch  1, batch    53 | loss: 58.1917744
CurrentTrain: epoch  1, batch    54 | loss: 57.3315191
CurrentTrain: epoch  1, batch    55 | loss: 56.8897912
CurrentTrain: epoch  1, batch    56 | loss: 89.6521869
CurrentTrain: epoch  1, batch    57 | loss: 119.7477434
CurrentTrain: epoch  1, batch    58 | loss: 123.4409196
CurrentTrain: epoch  1, batch    59 | loss: 117.5553087
CurrentTrain: epoch  1, batch    60 | loss: 72.4726495
CurrentTrain: epoch  1, batch    61 | loss: 59.8082262
CurrentTrain: epoch  1, batch    62 | loss: 66.2565396
CurrentTrain: epoch  1, batch    63 | loss: 74.9211938
CurrentTrain: epoch  1, batch    64 | loss: 70.6228535
CurrentTrain: epoch  1, batch    65 | loss: 68.0865021
CurrentTrain: epoch  1, batch    66 | loss: 56.3484118
CurrentTrain: epoch  1, batch    67 | loss: 58.3525104
CurrentTrain: epoch  1, batch    68 | loss: 66.1591163
CurrentTrain: epoch  1, batch    69 | loss: 122.6854958
CurrentTrain: epoch  1, batch    70 | loss: 80.4048481
CurrentTrain: epoch  1, batch    71 | loss: 68.6306531
CurrentTrain: epoch  1, batch    72 | loss: 59.2353432
CurrentTrain: epoch  1, batch    73 | loss: 92.0789514
CurrentTrain: epoch  1, batch    74 | loss: 70.4976721
CurrentTrain: epoch  1, batch    75 | loss: 54.4409853
CurrentTrain: epoch  1, batch    76 | loss: 57.2205367
CurrentTrain: epoch  1, batch    77 | loss: 89.6692154
CurrentTrain: epoch  1, batch    78 | loss: 92.7289121
CurrentTrain: epoch  1, batch    79 | loss: 90.5027488
CurrentTrain: epoch  1, batch    80 | loss: 48.1267411
CurrentTrain: epoch  1, batch    81 | loss: 57.2798266
CurrentTrain: epoch  1, batch    82 | loss: 85.9223929
CurrentTrain: epoch  1, batch    83 | loss: 73.4449663
CurrentTrain: epoch  1, batch    84 | loss: 119.6613067
CurrentTrain: epoch  1, batch    85 | loss: 60.4295506
CurrentTrain: epoch  1, batch    86 | loss: 58.5119341
CurrentTrain: epoch  1, batch    87 | loss: 86.2021994
CurrentTrain: epoch  1, batch    88 | loss: 91.9465458
CurrentTrain: epoch  1, batch    89 | loss: 52.3527940
CurrentTrain: epoch  1, batch    90 | loss: 122.2321275
CurrentTrain: epoch  1, batch    91 | loss: 57.6794263
CurrentTrain: epoch  1, batch    92 | loss: 53.6088021
CurrentTrain: epoch  1, batch    93 | loss: 69.6471443
CurrentTrain: epoch  1, batch    94 | loss: 68.4068029
CurrentTrain: epoch  1, batch    95 | loss: 58.9866724
CurrentTrain: epoch  2, batch     0 | loss: 83.8166507
CurrentTrain: epoch  2, batch     1 | loss: 69.2532309
CurrentTrain: epoch  2, batch     2 | loss: 57.6616085
CurrentTrain: epoch  2, batch     3 | loss: 90.6402437
CurrentTrain: epoch  2, batch     4 | loss: 87.2445596
CurrentTrain: epoch  2, batch     5 | loss: 65.5994419
CurrentTrain: epoch  2, batch     6 | loss: 86.4613538
CurrentTrain: epoch  2, batch     7 | loss: 46.2661591
CurrentTrain: epoch  2, batch     8 | loss: 85.4392198
CurrentTrain: epoch  2, batch     9 | loss: 65.1657671
CurrentTrain: epoch  2, batch    10 | loss: 86.5546282
CurrentTrain: epoch  2, batch    11 | loss: 69.1832140
CurrentTrain: epoch  2, batch    12 | loss: 59.2770210
CurrentTrain: epoch  2, batch    13 | loss: 70.4880836
CurrentTrain: epoch  2, batch    14 | loss: 187.8646872
CurrentTrain: epoch  2, batch    15 | loss: 65.1395209
CurrentTrain: epoch  2, batch    16 | loss: 70.2317850
CurrentTrain: epoch  2, batch    17 | loss: 68.9782553
CurrentTrain: epoch  2, batch    18 | loss: 87.9397779
CurrentTrain: epoch  2, batch    19 | loss: 46.8945239
CurrentTrain: epoch  2, batch    20 | loss: 57.2062397
CurrentTrain: epoch  2, batch    21 | loss: 70.5921741
CurrentTrain: epoch  2, batch    22 | loss: 57.1201509
CurrentTrain: epoch  2, batch    23 | loss: 117.5208144
CurrentTrain: epoch  2, batch    24 | loss: 116.1792733
CurrentTrain: epoch  2, batch    25 | loss: 117.9148730
CurrentTrain: epoch  2, batch    26 | loss: 83.2301011
CurrentTrain: epoch  2, batch    27 | loss: 84.1554488
CurrentTrain: epoch  2, batch    28 | loss: 85.3413613
CurrentTrain: epoch  2, batch    29 | loss: 71.4228374
CurrentTrain: epoch  2, batch    30 | loss: 57.6167764
CurrentTrain: epoch  2, batch    31 | loss: 117.3344546
CurrentTrain: epoch  2, batch    32 | loss: 58.8050195
CurrentTrain: epoch  2, batch    33 | loss: 68.8336130
CurrentTrain: epoch  2, batch    34 | loss: 59.5949912
CurrentTrain: epoch  2, batch    35 | loss: 47.2961126
CurrentTrain: epoch  2, batch    36 | loss: 86.0222602
CurrentTrain: epoch  2, batch    37 | loss: 56.5579446
CurrentTrain: epoch  2, batch    38 | loss: 72.4505403
CurrentTrain: epoch  2, batch    39 | loss: 89.1080583
CurrentTrain: epoch  2, batch    40 | loss: 67.6507682
CurrentTrain: epoch  2, batch    41 | loss: 65.9177412
CurrentTrain: epoch  2, batch    42 | loss: 114.6412409
CurrentTrain: epoch  2, batch    43 | loss: 68.0724994
CurrentTrain: epoch  2, batch    44 | loss: 70.8555613
CurrentTrain: epoch  2, batch    45 | loss: 122.0249470
CurrentTrain: epoch  2, batch    46 | loss: 65.0598060
CurrentTrain: epoch  2, batch    47 | loss: 47.1667578
CurrentTrain: epoch  2, batch    48 | loss: 66.2603055
CurrentTrain: epoch  2, batch    49 | loss: 89.2822169
CurrentTrain: epoch  2, batch    50 | loss: 69.1354315
CurrentTrain: epoch  2, batch    51 | loss: 56.0983065
CurrentTrain: epoch  2, batch    52 | loss: 68.9017300
CurrentTrain: epoch  2, batch    53 | loss: 68.8113719
CurrentTrain: epoch  2, batch    54 | loss: 88.1771585
CurrentTrain: epoch  2, batch    55 | loss: 72.8831345
CurrentTrain: epoch  2, batch    56 | loss: 57.3381554
CurrentTrain: epoch  2, batch    57 | loss: 68.6129051
CurrentTrain: epoch  2, batch    58 | loss: 57.6252396
CurrentTrain: epoch  2, batch    59 | loss: 57.6573902
CurrentTrain: epoch  2, batch    60 | loss: 66.2675533
CurrentTrain: epoch  2, batch    61 | loss: 57.3509677
CurrentTrain: epoch  2, batch    62 | loss: 45.2038711
CurrentTrain: epoch  2, batch    63 | loss: 46.3745945
CurrentTrain: epoch  2, batch    64 | loss: 69.4418220
CurrentTrain: epoch  2, batch    65 | loss: 69.0812364
CurrentTrain: epoch  2, batch    66 | loss: 54.8511025
CurrentTrain: epoch  2, batch    67 | loss: 85.1944136
CurrentTrain: epoch  2, batch    68 | loss: 116.5798944
CurrentTrain: epoch  2, batch    69 | loss: 66.6517124
CurrentTrain: epoch  2, batch    70 | loss: 57.7083577
CurrentTrain: epoch  2, batch    71 | loss: 66.0293131
CurrentTrain: epoch  2, batch    72 | loss: 56.6770030
CurrentTrain: epoch  2, batch    73 | loss: 66.5777420
CurrentTrain: epoch  2, batch    74 | loss: 66.3152618
CurrentTrain: epoch  2, batch    75 | loss: 87.5700649
CurrentTrain: epoch  2, batch    76 | loss: 53.6700298
CurrentTrain: epoch  2, batch    77 | loss: 79.2873670
CurrentTrain: epoch  2, batch    78 | loss: 53.9047641
CurrentTrain: epoch  2, batch    79 | loss: 55.4174869
CurrentTrain: epoch  2, batch    80 | loss: 85.5214484
CurrentTrain: epoch  2, batch    81 | loss: 92.2790652
CurrentTrain: epoch  2, batch    82 | loss: 45.9803169
CurrentTrain: epoch  2, batch    83 | loss: 52.9469244
CurrentTrain: epoch  2, batch    84 | loss: 65.9278075
CurrentTrain: epoch  2, batch    85 | loss: 67.2747872
CurrentTrain: epoch  2, batch    86 | loss: 88.0063771
CurrentTrain: epoch  2, batch    87 | loss: 65.4520690
CurrentTrain: epoch  2, batch    88 | loss: 91.1728129
CurrentTrain: epoch  2, batch    89 | loss: 123.2536869
CurrentTrain: epoch  2, batch    90 | loss: 70.3449880
CurrentTrain: epoch  2, batch    91 | loss: 119.4008909
CurrentTrain: epoch  2, batch    92 | loss: 119.1956427
CurrentTrain: epoch  2, batch    93 | loss: 65.8610974
CurrentTrain: epoch  2, batch    94 | loss: 68.3750459
CurrentTrain: epoch  2, batch    95 | loss: 58.4057326
CurrentTrain: epoch  3, batch     0 | loss: 67.4287583
CurrentTrain: epoch  3, batch     1 | loss: 55.4845628
CurrentTrain: epoch  3, batch     2 | loss: 89.1619546
CurrentTrain: epoch  3, batch     3 | loss: 64.9994062
CurrentTrain: epoch  3, batch     4 | loss: 55.2105031
CurrentTrain: epoch  3, batch     5 | loss: 67.9890934
CurrentTrain: epoch  3, batch     6 | loss: 85.8332502
CurrentTrain: epoch  3, batch     7 | loss: 64.7103299
CurrentTrain: epoch  3, batch     8 | loss: 61.7075398
CurrentTrain: epoch  3, batch     9 | loss: 66.4931437
CurrentTrain: epoch  3, batch    10 | loss: 64.1748911
CurrentTrain: epoch  3, batch    11 | loss: 86.1739195
CurrentTrain: epoch  3, batch    12 | loss: 66.6884264
CurrentTrain: epoch  3, batch    13 | loss: 69.4916624
CurrentTrain: epoch  3, batch    14 | loss: 84.7816244
CurrentTrain: epoch  3, batch    15 | loss: 53.1665334
CurrentTrain: epoch  3, batch    16 | loss: 69.5241173
CurrentTrain: epoch  3, batch    17 | loss: 66.0200166
CurrentTrain: epoch  3, batch    18 | loss: 68.4181382
CurrentTrain: epoch  3, batch    19 | loss: 87.4031485
CurrentTrain: epoch  3, batch    20 | loss: 53.6003603
CurrentTrain: epoch  3, batch    21 | loss: 115.4444621
CurrentTrain: epoch  3, batch    22 | loss: 55.2850790
CurrentTrain: epoch  3, batch    23 | loss: 52.9778268
CurrentTrain: epoch  3, batch    24 | loss: 73.2283907
CurrentTrain: epoch  3, batch    25 | loss: 43.3319449
CurrentTrain: epoch  3, batch    26 | loss: 85.1006708
CurrentTrain: epoch  3, batch    27 | loss: 121.3326109
CurrentTrain: epoch  3, batch    28 | loss: 66.6990229
CurrentTrain: epoch  3, batch    29 | loss: 116.5809837
CurrentTrain: epoch  3, batch    30 | loss: 69.2146783
CurrentTrain: epoch  3, batch    31 | loss: 67.7106491
CurrentTrain: epoch  3, batch    32 | loss: 69.3247142
CurrentTrain: epoch  3, batch    33 | loss: 48.3437738
CurrentTrain: epoch  3, batch    34 | loss: 68.5609279
CurrentTrain: epoch  3, batch    35 | loss: 89.7875950
CurrentTrain: epoch  3, batch    36 | loss: 54.8988465
CurrentTrain: epoch  3, batch    37 | loss: 88.6301486
CurrentTrain: epoch  3, batch    38 | loss: 71.3469021
CurrentTrain: epoch  3, batch    39 | loss: 84.3802488
CurrentTrain: epoch  3, batch    40 | loss: 66.3699238
CurrentTrain: epoch  3, batch    41 | loss: 86.4960053
CurrentTrain: epoch  3, batch    42 | loss: 86.6076258
CurrentTrain: epoch  3, batch    43 | loss: 88.9768398
CurrentTrain: epoch  3, batch    44 | loss: 55.5837105
CurrentTrain: epoch  3, batch    45 | loss: 55.2453090
CurrentTrain: epoch  3, batch    46 | loss: 117.4147316
CurrentTrain: epoch  3, batch    47 | loss: 68.2051327
CurrentTrain: epoch  3, batch    48 | loss: 67.7927807
CurrentTrain: epoch  3, batch    49 | loss: 55.7190522
CurrentTrain: epoch  3, batch    50 | loss: 90.4773489
CurrentTrain: epoch  3, batch    51 | loss: 45.1492315
CurrentTrain: epoch  3, batch    52 | loss: 87.2731054
CurrentTrain: epoch  3, batch    53 | loss: 84.6531734
CurrentTrain: epoch  3, batch    54 | loss: 84.5880634
CurrentTrain: epoch  3, batch    55 | loss: 53.8913979
CurrentTrain: epoch  3, batch    56 | loss: 114.6114087
CurrentTrain: epoch  3, batch    57 | loss: 66.1652372
CurrentTrain: epoch  3, batch    58 | loss: 51.5206742
CurrentTrain: epoch  3, batch    59 | loss: 66.6845220
CurrentTrain: epoch  3, batch    60 | loss: 55.4317583
CurrentTrain: epoch  3, batch    61 | loss: 66.3871684
CurrentTrain: epoch  3, batch    62 | loss: 85.7208437
CurrentTrain: epoch  3, batch    63 | loss: 85.2125208
CurrentTrain: epoch  3, batch    64 | loss: 86.0911328
CurrentTrain: epoch  3, batch    65 | loss: 65.1423983
CurrentTrain: epoch  3, batch    66 | loss: 54.4522683
CurrentTrain: epoch  3, batch    67 | loss: 52.5250778
CurrentTrain: epoch  3, batch    68 | loss: 85.1140511
CurrentTrain: epoch  3, batch    69 | loss: 87.1788773
CurrentTrain: epoch  3, batch    70 | loss: 84.3148283
CurrentTrain: epoch  3, batch    71 | loss: 81.0129407
CurrentTrain: epoch  3, batch    72 | loss: 54.0225499
CurrentTrain: epoch  3, batch    73 | loss: 85.9469424
CurrentTrain: epoch  3, batch    74 | loss: 68.3479327
CurrentTrain: epoch  3, batch    75 | loss: 53.7596391
CurrentTrain: epoch  3, batch    76 | loss: 54.2748758
CurrentTrain: epoch  3, batch    77 | loss: 86.7397356
CurrentTrain: epoch  3, batch    78 | loss: 84.0976079
CurrentTrain: epoch  3, batch    79 | loss: 84.8014679
CurrentTrain: epoch  3, batch    80 | loss: 48.7848299
CurrentTrain: epoch  3, batch    81 | loss: 64.4955975
CurrentTrain: epoch  3, batch    82 | loss: 87.0120082
CurrentTrain: epoch  3, batch    83 | loss: 52.8266716
CurrentTrain: epoch  3, batch    84 | loss: 85.3587924
CurrentTrain: epoch  3, batch    85 | loss: 48.0991893
CurrentTrain: epoch  3, batch    86 | loss: 86.7362641
CurrentTrain: epoch  3, batch    87 | loss: 44.8487452
CurrentTrain: epoch  3, batch    88 | loss: 54.9051311
CurrentTrain: epoch  3, batch    89 | loss: 69.5936247
CurrentTrain: epoch  3, batch    90 | loss: 66.3862277
CurrentTrain: epoch  3, batch    91 | loss: 82.5642216
CurrentTrain: epoch  3, batch    92 | loss: 84.9400111
CurrentTrain: epoch  3, batch    93 | loss: 55.6071697
CurrentTrain: epoch  3, batch    94 | loss: 48.6871000
CurrentTrain: epoch  3, batch    95 | loss: 52.9529426
CurrentTrain: epoch  4, batch     0 | loss: 116.3816116
CurrentTrain: epoch  4, batch     1 | loss: 68.2382558
CurrentTrain: epoch  4, batch     2 | loss: 52.8053599
CurrentTrain: epoch  4, batch     3 | loss: 57.2716525
CurrentTrain: epoch  4, batch     4 | loss: 65.4121026
CurrentTrain: epoch  4, batch     5 | loss: 85.9322634
CurrentTrain: epoch  4, batch     6 | loss: 66.3039187
CurrentTrain: epoch  4, batch     7 | loss: 50.3230076
CurrentTrain: epoch  4, batch     8 | loss: 86.6605112
CurrentTrain: epoch  4, batch     9 | loss: 43.6883307
CurrentTrain: epoch  4, batch    10 | loss: 83.9645061
CurrentTrain: epoch  4, batch    11 | loss: 85.6024204
CurrentTrain: epoch  4, batch    12 | loss: 46.8507615
CurrentTrain: epoch  4, batch    13 | loss: 55.0217761
CurrentTrain: epoch  4, batch    14 | loss: 68.0725845
CurrentTrain: epoch  4, batch    15 | loss: 53.2582872
CurrentTrain: epoch  4, batch    16 | loss: 56.2302177
CurrentTrain: epoch  4, batch    17 | loss: 64.1647419
CurrentTrain: epoch  4, batch    18 | loss: 62.4013617
CurrentTrain: epoch  4, batch    19 | loss: 84.8644038
CurrentTrain: epoch  4, batch    20 | loss: 67.2394848
CurrentTrain: epoch  4, batch    21 | loss: 87.6215008
CurrentTrain: epoch  4, batch    22 | loss: 67.6812043
CurrentTrain: epoch  4, batch    23 | loss: 181.8916394
CurrentTrain: epoch  4, batch    24 | loss: 181.9772327
CurrentTrain: epoch  4, batch    25 | loss: 45.8935935
CurrentTrain: epoch  4, batch    26 | loss: 83.6498946
CurrentTrain: epoch  4, batch    27 | loss: 44.2071632
CurrentTrain: epoch  4, batch    28 | loss: 65.9728229
CurrentTrain: epoch  4, batch    29 | loss: 86.6305602
CurrentTrain: epoch  4, batch    30 | loss: 47.5320368
CurrentTrain: epoch  4, batch    31 | loss: 54.8349566
CurrentTrain: epoch  4, batch    32 | loss: 48.1366710
CurrentTrain: epoch  4, batch    33 | loss: 85.3594149
CurrentTrain: epoch  4, batch    34 | loss: 56.9335489
CurrentTrain: epoch  4, batch    35 | loss: 45.3786460
CurrentTrain: epoch  4, batch    36 | loss: 67.0266993
CurrentTrain: epoch  4, batch    37 | loss: 66.7441750
CurrentTrain: epoch  4, batch    38 | loss: 81.0371000
CurrentTrain: epoch  4, batch    39 | loss: 66.9162650
CurrentTrain: epoch  4, batch    40 | loss: 84.5611562
CurrentTrain: epoch  4, batch    41 | loss: 66.2410884
CurrentTrain: epoch  4, batch    42 | loss: 84.5510195
CurrentTrain: epoch  4, batch    43 | loss: 42.3573665
CurrentTrain: epoch  4, batch    44 | loss: 57.3653742
CurrentTrain: epoch  4, batch    45 | loss: 84.7511175
CurrentTrain: epoch  4, batch    46 | loss: 52.6888429
CurrentTrain: epoch  4, batch    47 | loss: 52.7689264
CurrentTrain: epoch  4, batch    48 | loss: 67.6712758
CurrentTrain: epoch  4, batch    49 | loss: 119.0072020
CurrentTrain: epoch  4, batch    50 | loss: 56.8363552
CurrentTrain: epoch  4, batch    51 | loss: 65.3544098
CurrentTrain: epoch  4, batch    52 | loss: 67.6686318
CurrentTrain: epoch  4, batch    53 | loss: 119.3190571
CurrentTrain: epoch  4, batch    54 | loss: 63.4628240
CurrentTrain: epoch  4, batch    55 | loss: 86.2984884
CurrentTrain: epoch  4, batch    56 | loss: 63.6699525
CurrentTrain: epoch  4, batch    57 | loss: 117.8752928
CurrentTrain: epoch  4, batch    58 | loss: 68.6024043
CurrentTrain: epoch  4, batch    59 | loss: 68.1337138
CurrentTrain: epoch  4, batch    60 | loss: 119.9199510
CurrentTrain: epoch  4, batch    61 | loss: 54.3319844
CurrentTrain: epoch  4, batch    62 | loss: 54.7387526
CurrentTrain: epoch  4, batch    63 | loss: 54.3831995
CurrentTrain: epoch  4, batch    64 | loss: 51.4741270
CurrentTrain: epoch  4, batch    65 | loss: 81.8190846
CurrentTrain: epoch  4, batch    66 | loss: 112.7727939
CurrentTrain: epoch  4, batch    67 | loss: 114.3686823
CurrentTrain: epoch  4, batch    68 | loss: 50.3733076
CurrentTrain: epoch  4, batch    69 | loss: 66.8669514
CurrentTrain: epoch  4, batch    70 | loss: 117.1590464
CurrentTrain: epoch  4, batch    71 | loss: 64.9428106
CurrentTrain: epoch  4, batch    72 | loss: 54.8280386
CurrentTrain: epoch  4, batch    73 | loss: 55.2563945
CurrentTrain: epoch  4, batch    74 | loss: 65.7892944
CurrentTrain: epoch  4, batch    75 | loss: 54.7266840
CurrentTrain: epoch  4, batch    76 | loss: 79.1662768
CurrentTrain: epoch  4, batch    77 | loss: 89.9660787
CurrentTrain: epoch  4, batch    78 | loss: 64.7413751
CurrentTrain: epoch  4, batch    79 | loss: 116.4263015
CurrentTrain: epoch  4, batch    80 | loss: 67.4728813
CurrentTrain: epoch  4, batch    81 | loss: 87.8247444
CurrentTrain: epoch  4, batch    82 | loss: 83.3010837
CurrentTrain: epoch  4, batch    83 | loss: 60.9242032
CurrentTrain: epoch  4, batch    84 | loss: 181.8314085
CurrentTrain: epoch  4, batch    85 | loss: 66.3654705
CurrentTrain: epoch  4, batch    86 | loss: 69.0423103
CurrentTrain: epoch  4, batch    87 | loss: 52.0357693
CurrentTrain: epoch  4, batch    88 | loss: 54.3279108
CurrentTrain: epoch  4, batch    89 | loss: 46.6358805
CurrentTrain: epoch  4, batch    90 | loss: 61.5550855
CurrentTrain: epoch  4, batch    91 | loss: 51.9314523
CurrentTrain: epoch  4, batch    92 | loss: 54.1112285
CurrentTrain: epoch  4, batch    93 | loss: 52.2210617
CurrentTrain: epoch  4, batch    94 | loss: 53.9879371
CurrentTrain: epoch  4, batch    95 | loss: 52.5065323
CurrentTrain: epoch  5, batch     0 | loss: 52.7394152
CurrentTrain: epoch  5, batch     1 | loss: 85.4009167
CurrentTrain: epoch  5, batch     2 | loss: 66.4551663
CurrentTrain: epoch  5, batch     3 | loss: 65.2312369
CurrentTrain: epoch  5, batch     4 | loss: 83.7874488
CurrentTrain: epoch  5, batch     5 | loss: 54.8935528
CurrentTrain: epoch  5, batch     6 | loss: 54.0124937
CurrentTrain: epoch  5, batch     7 | loss: 64.9729912
CurrentTrain: epoch  5, batch     8 | loss: 43.0811039
CurrentTrain: epoch  5, batch     9 | loss: 64.9827792
CurrentTrain: epoch  5, batch    10 | loss: 116.1535441
CurrentTrain: epoch  5, batch    11 | loss: 65.9819048
CurrentTrain: epoch  5, batch    12 | loss: 114.1328288
CurrentTrain: epoch  5, batch    13 | loss: 66.4436935
CurrentTrain: epoch  5, batch    14 | loss: 86.9081364
CurrentTrain: epoch  5, batch    15 | loss: 88.1720808
CurrentTrain: epoch  5, batch    16 | loss: 66.5685886
CurrentTrain: epoch  5, batch    17 | loss: 65.7438246
CurrentTrain: epoch  5, batch    18 | loss: 49.8130566
CurrentTrain: epoch  5, batch    19 | loss: 80.5377022
CurrentTrain: epoch  5, batch    20 | loss: 54.9495585
CurrentTrain: epoch  5, batch    21 | loss: 69.6601342
CurrentTrain: epoch  5, batch    22 | loss: 115.6609292
CurrentTrain: epoch  5, batch    23 | loss: 51.1600436
CurrentTrain: epoch  5, batch    24 | loss: 52.6533436
CurrentTrain: epoch  5, batch    25 | loss: 45.7971504
CurrentTrain: epoch  5, batch    26 | loss: 65.7082873
CurrentTrain: epoch  5, batch    27 | loss: 66.6050644
CurrentTrain: epoch  5, batch    28 | loss: 65.2202051
CurrentTrain: epoch  5, batch    29 | loss: 79.7774760
CurrentTrain: epoch  5, batch    30 | loss: 50.1786723
CurrentTrain: epoch  5, batch    31 | loss: 67.5039268
CurrentTrain: epoch  5, batch    32 | loss: 51.9287846
CurrentTrain: epoch  5, batch    33 | loss: 118.3082483
CurrentTrain: epoch  5, batch    34 | loss: 45.7744536
CurrentTrain: epoch  5, batch    35 | loss: 86.2279121
CurrentTrain: epoch  5, batch    36 | loss: 83.3376288
CurrentTrain: epoch  5, batch    37 | loss: 85.2580821
CurrentTrain: epoch  5, batch    38 | loss: 53.5434434
CurrentTrain: epoch  5, batch    39 | loss: 84.4068881
CurrentTrain: epoch  5, batch    40 | loss: 115.6235334
CurrentTrain: epoch  5, batch    41 | loss: 53.6686470
CurrentTrain: epoch  5, batch    42 | loss: 64.3293973
CurrentTrain: epoch  5, batch    43 | loss: 51.4689405
CurrentTrain: epoch  5, batch    44 | loss: 115.6938258
CurrentTrain: epoch  5, batch    45 | loss: 62.3736858
CurrentTrain: epoch  5, batch    46 | loss: 51.5614382
CurrentTrain: epoch  5, batch    47 | loss: 61.3511225
CurrentTrain: epoch  5, batch    48 | loss: 52.2567402
CurrentTrain: epoch  5, batch    49 | loss: 54.3351431
CurrentTrain: epoch  5, batch    50 | loss: 65.4422071
CurrentTrain: epoch  5, batch    51 | loss: 54.3661940
CurrentTrain: epoch  5, batch    52 | loss: 79.7440459
CurrentTrain: epoch  5, batch    53 | loss: 174.8231283
CurrentTrain: epoch  5, batch    54 | loss: 114.5586014
CurrentTrain: epoch  5, batch    55 | loss: 63.4714866
CurrentTrain: epoch  5, batch    56 | loss: 118.1070546
CurrentTrain: epoch  5, batch    57 | loss: 86.2398156
CurrentTrain: epoch  5, batch    58 | loss: 54.6959589
CurrentTrain: epoch  5, batch    59 | loss: 185.9842502
CurrentTrain: epoch  5, batch    60 | loss: 82.4630511
CurrentTrain: epoch  5, batch    61 | loss: 113.9910606
CurrentTrain: epoch  5, batch    62 | loss: 67.9806737
CurrentTrain: epoch  5, batch    63 | loss: 64.5735833
CurrentTrain: epoch  5, batch    64 | loss: 67.3903878
CurrentTrain: epoch  5, batch    65 | loss: 64.4431708
CurrentTrain: epoch  5, batch    66 | loss: 85.9218412
CurrentTrain: epoch  5, batch    67 | loss: 64.2385518
CurrentTrain: epoch  5, batch    68 | loss: 67.7981473
CurrentTrain: epoch  5, batch    69 | loss: 64.2772005
CurrentTrain: epoch  5, batch    70 | loss: 86.5905838
CurrentTrain: epoch  5, batch    71 | loss: 177.9569281
CurrentTrain: epoch  5, batch    72 | loss: 61.8818871
CurrentTrain: epoch  5, batch    73 | loss: 83.5998297
CurrentTrain: epoch  5, batch    74 | loss: 65.3224440
CurrentTrain: epoch  5, batch    75 | loss: 55.1116539
CurrentTrain: epoch  5, batch    76 | loss: 65.7156871
CurrentTrain: epoch  5, batch    77 | loss: 50.8482813
CurrentTrain: epoch  5, batch    78 | loss: 92.4460438
CurrentTrain: epoch  5, batch    79 | loss: 50.5784028
CurrentTrain: epoch  5, batch    80 | loss: 111.6430651
CurrentTrain: epoch  5, batch    81 | loss: 64.5622483
CurrentTrain: epoch  5, batch    82 | loss: 81.7241867
CurrentTrain: epoch  5, batch    83 | loss: 116.6362452
CurrentTrain: epoch  5, batch    84 | loss: 63.7527449
CurrentTrain: epoch  5, batch    85 | loss: 84.2872309
CurrentTrain: epoch  5, batch    86 | loss: 87.3591313
CurrentTrain: epoch  5, batch    87 | loss: 86.2885263
CurrentTrain: epoch  5, batch    88 | loss: 61.2791708
CurrentTrain: epoch  5, batch    89 | loss: 84.1041380
CurrentTrain: epoch  5, batch    90 | loss: 82.7038440
CurrentTrain: epoch  5, batch    91 | loss: 114.3778386
CurrentTrain: epoch  5, batch    92 | loss: 62.9914102
CurrentTrain: epoch  5, batch    93 | loss: 82.4560307
CurrentTrain: epoch  5, batch    94 | loss: 76.3569972
CurrentTrain: epoch  5, batch    95 | loss: 50.9583089
CurrentTrain: epoch  6, batch     0 | loss: 83.1138342
CurrentTrain: epoch  6, batch     1 | loss: 79.2725585
CurrentTrain: epoch  6, batch     2 | loss: 115.2975632
CurrentTrain: epoch  6, batch     3 | loss: 65.6673967
CurrentTrain: epoch  6, batch     4 | loss: 63.4969900
CurrentTrain: epoch  6, batch     5 | loss: 65.7637005
CurrentTrain: epoch  6, batch     6 | loss: 115.4623262
CurrentTrain: epoch  6, batch     7 | loss: 113.7586076
CurrentTrain: epoch  6, batch     8 | loss: 52.0929269
CurrentTrain: epoch  6, batch     9 | loss: 49.7423446
CurrentTrain: epoch  6, batch    10 | loss: 66.9030427
CurrentTrain: epoch  6, batch    11 | loss: 65.7461841
CurrentTrain: epoch  6, batch    12 | loss: 64.3620487
CurrentTrain: epoch  6, batch    13 | loss: 66.2707284
CurrentTrain: epoch  6, batch    14 | loss: 86.0141584
CurrentTrain: epoch  6, batch    15 | loss: 42.5205828
CurrentTrain: epoch  6, batch    16 | loss: 67.4459134
CurrentTrain: epoch  6, batch    17 | loss: 84.7669687
CurrentTrain: epoch  6, batch    18 | loss: 62.4218164
CurrentTrain: epoch  6, batch    19 | loss: 82.8446653
CurrentTrain: epoch  6, batch    20 | loss: 81.6868673
CurrentTrain: epoch  6, batch    21 | loss: 63.3395780
CurrentTrain: epoch  6, batch    22 | loss: 59.9593426
CurrentTrain: epoch  6, batch    23 | loss: 83.8685196
CurrentTrain: epoch  6, batch    24 | loss: 63.6696888
CurrentTrain: epoch  6, batch    25 | loss: 53.2798652
CurrentTrain: epoch  6, batch    26 | loss: 82.7766079
CurrentTrain: epoch  6, batch    27 | loss: 63.8462452
CurrentTrain: epoch  6, batch    28 | loss: 67.4392899
CurrentTrain: epoch  6, batch    29 | loss: 86.5410672
CurrentTrain: epoch  6, batch    30 | loss: 43.8491195
CurrentTrain: epoch  6, batch    31 | loss: 41.2991071
CurrentTrain: epoch  6, batch    32 | loss: 54.2120460
CurrentTrain: epoch  6, batch    33 | loss: 53.2260169
CurrentTrain: epoch  6, batch    34 | loss: 115.3547934
CurrentTrain: epoch  6, batch    35 | loss: 79.7352208
CurrentTrain: epoch  6, batch    36 | loss: 86.0074108
CurrentTrain: epoch  6, batch    37 | loss: 66.1061571
CurrentTrain: epoch  6, batch    38 | loss: 113.4911160
CurrentTrain: epoch  6, batch    39 | loss: 87.1745495
CurrentTrain: epoch  6, batch    40 | loss: 52.1138077
CurrentTrain: epoch  6, batch    41 | loss: 65.0291011
CurrentTrain: epoch  6, batch    42 | loss: 67.0843990
CurrentTrain: epoch  6, batch    43 | loss: 86.3515150
CurrentTrain: epoch  6, batch    44 | loss: 88.1364419
CurrentTrain: epoch  6, batch    45 | loss: 65.8332928
CurrentTrain: epoch  6, batch    46 | loss: 177.9589587
CurrentTrain: epoch  6, batch    47 | loss: 68.0849111
CurrentTrain: epoch  6, batch    48 | loss: 116.2872392
CurrentTrain: epoch  6, batch    49 | loss: 63.0301878
CurrentTrain: epoch  6, batch    50 | loss: 117.9468886
CurrentTrain: epoch  6, batch    51 | loss: 62.7091494
CurrentTrain: epoch  6, batch    52 | loss: 65.8525713
CurrentTrain: epoch  6, batch    53 | loss: 84.0298449
CurrentTrain: epoch  6, batch    54 | loss: 49.7366943
CurrentTrain: epoch  6, batch    55 | loss: 85.6553242
CurrentTrain: epoch  6, batch    56 | loss: 65.8067136
CurrentTrain: epoch  6, batch    57 | loss: 55.5150944
CurrentTrain: epoch  6, batch    58 | loss: 89.4606920
CurrentTrain: epoch  6, batch    59 | loss: 83.1023534
CurrentTrain: epoch  6, batch    60 | loss: 50.4348237
CurrentTrain: epoch  6, batch    61 | loss: 43.6161965
CurrentTrain: epoch  6, batch    62 | loss: 44.1093819
CurrentTrain: epoch  6, batch    63 | loss: 57.0336454
CurrentTrain: epoch  6, batch    64 | loss: 66.2110290
CurrentTrain: epoch  6, batch    65 | loss: 63.8029371
CurrentTrain: epoch  6, batch    66 | loss: 52.0418219
CurrentTrain: epoch  6, batch    67 | loss: 84.1716195
CurrentTrain: epoch  6, batch    68 | loss: 65.1663596
CurrentTrain: epoch  6, batch    69 | loss: 61.2206778
CurrentTrain: epoch  6, batch    70 | loss: 44.1115422
CurrentTrain: epoch  6, batch    71 | loss: 46.4226485
CurrentTrain: epoch  6, batch    72 | loss: 54.3215316
CurrentTrain: epoch  6, batch    73 | loss: 53.1629561
CurrentTrain: epoch  6, batch    74 | loss: 53.4704673
CurrentTrain: epoch  6, batch    75 | loss: 52.1810227
CurrentTrain: epoch  6, batch    76 | loss: 53.2106408
CurrentTrain: epoch  6, batch    77 | loss: 64.7480840
CurrentTrain: epoch  6, batch    78 | loss: 85.2565864
CurrentTrain: epoch  6, batch    79 | loss: 63.0777226
CurrentTrain: epoch  6, batch    80 | loss: 117.5747631
CurrentTrain: epoch  6, batch    81 | loss: 46.1470740
CurrentTrain: epoch  6, batch    82 | loss: 65.4882558
CurrentTrain: epoch  6, batch    83 | loss: 83.5180453
CurrentTrain: epoch  6, batch    84 | loss: 65.1782420
CurrentTrain: epoch  6, batch    85 | loss: 85.6692344
CurrentTrain: epoch  6, batch    86 | loss: 85.6262040
CurrentTrain: epoch  6, batch    87 | loss: 54.7980365
CurrentTrain: epoch  6, batch    88 | loss: 61.3087444
CurrentTrain: epoch  6, batch    89 | loss: 69.9682502
CurrentTrain: epoch  6, batch    90 | loss: 52.3244074
CurrentTrain: epoch  6, batch    91 | loss: 81.2274617
CurrentTrain: epoch  6, batch    92 | loss: 62.6706964
CurrentTrain: epoch  6, batch    93 | loss: 49.4153934
CurrentTrain: epoch  6, batch    94 | loss: 82.9098432
CurrentTrain: epoch  6, batch    95 | loss: 54.9614776
CurrentTrain: epoch  7, batch     0 | loss: 64.3958784
CurrentTrain: epoch  7, batch     1 | loss: 85.6285355
CurrentTrain: epoch  7, batch     2 | loss: 45.3653776
CurrentTrain: epoch  7, batch     3 | loss: 52.5244480
CurrentTrain: epoch  7, batch     4 | loss: 41.6766831
CurrentTrain: epoch  7, batch     5 | loss: 69.3984889
CurrentTrain: epoch  7, batch     6 | loss: 82.7718222
CurrentTrain: epoch  7, batch     7 | loss: 43.7415490
CurrentTrain: epoch  7, batch     8 | loss: 52.2482177
CurrentTrain: epoch  7, batch     9 | loss: 52.4283553
CurrentTrain: epoch  7, batch    10 | loss: 66.0491486
CurrentTrain: epoch  7, batch    11 | loss: 50.7443104
CurrentTrain: epoch  7, batch    12 | loss: 44.4178620
CurrentTrain: epoch  7, batch    13 | loss: 84.2192435
CurrentTrain: epoch  7, batch    14 | loss: 64.3053441
CurrentTrain: epoch  7, batch    15 | loss: 46.5642877
CurrentTrain: epoch  7, batch    16 | loss: 67.4716833
CurrentTrain: epoch  7, batch    17 | loss: 64.9945053
CurrentTrain: epoch  7, batch    18 | loss: 65.6248328
CurrentTrain: epoch  7, batch    19 | loss: 84.3275326
CurrentTrain: epoch  7, batch    20 | loss: 51.6767115
CurrentTrain: epoch  7, batch    21 | loss: 115.7930188
CurrentTrain: epoch  7, batch    22 | loss: 73.0031504
CurrentTrain: epoch  7, batch    23 | loss: 120.4867876
CurrentTrain: epoch  7, batch    24 | loss: 64.9685123
CurrentTrain: epoch  7, batch    25 | loss: 115.4027128
CurrentTrain: epoch  7, batch    26 | loss: 87.9858088
CurrentTrain: epoch  7, batch    27 | loss: 80.9993525
CurrentTrain: epoch  7, batch    28 | loss: 52.8227147
CurrentTrain: epoch  7, batch    29 | loss: 52.2643053
CurrentTrain: epoch  7, batch    30 | loss: 41.6825582
CurrentTrain: epoch  7, batch    31 | loss: 65.9935216
CurrentTrain: epoch  7, batch    32 | loss: 65.9286695
CurrentTrain: epoch  7, batch    33 | loss: 54.5626089
CurrentTrain: epoch  7, batch    34 | loss: 67.0053779
CurrentTrain: epoch  7, batch    35 | loss: 84.3829021
CurrentTrain: epoch  7, batch    36 | loss: 53.3991490
CurrentTrain: epoch  7, batch    37 | loss: 67.7679040
CurrentTrain: epoch  7, batch    38 | loss: 53.7682896
CurrentTrain: epoch  7, batch    39 | loss: 61.8703163
CurrentTrain: epoch  7, batch    40 | loss: 51.9858579
CurrentTrain: epoch  7, batch    41 | loss: 82.0033177
CurrentTrain: epoch  7, batch    42 | loss: 80.8932078
CurrentTrain: epoch  7, batch    43 | loss: 66.6721119
CurrentTrain: epoch  7, batch    44 | loss: 51.1502743
CurrentTrain: epoch  7, batch    45 | loss: 43.4718479
CurrentTrain: epoch  7, batch    46 | loss: 54.0534655
CurrentTrain: epoch  7, batch    47 | loss: 62.5819445
CurrentTrain: epoch  7, batch    48 | loss: 82.8811970
CurrentTrain: epoch  7, batch    49 | loss: 64.2970006
CurrentTrain: epoch  7, batch    50 | loss: 67.3861816
CurrentTrain: epoch  7, batch    51 | loss: 65.5957465
CurrentTrain: epoch  7, batch    52 | loss: 68.4469905
CurrentTrain: epoch  7, batch    53 | loss: 64.3949566
CurrentTrain: epoch  7, batch    54 | loss: 64.4572759
CurrentTrain: epoch  7, batch    55 | loss: 64.7068207
CurrentTrain: epoch  7, batch    56 | loss: 78.7205104
CurrentTrain: epoch  7, batch    57 | loss: 69.1850779
CurrentTrain: epoch  7, batch    58 | loss: 113.4561825
CurrentTrain: epoch  7, batch    59 | loss: 53.1732958
CurrentTrain: epoch  7, batch    60 | loss: 87.0104779
CurrentTrain: epoch  7, batch    61 | loss: 50.1056955
CurrentTrain: epoch  7, batch    62 | loss: 65.6865524
CurrentTrain: epoch  7, batch    63 | loss: 85.4804807
CurrentTrain: epoch  7, batch    64 | loss: 84.4089430
CurrentTrain: epoch  7, batch    65 | loss: 53.3707993
CurrentTrain: epoch  7, batch    66 | loss: 52.6629492
CurrentTrain: epoch  7, batch    67 | loss: 43.4833931
CurrentTrain: epoch  7, batch    68 | loss: 115.2978331
CurrentTrain: epoch  7, batch    69 | loss: 63.4538616
CurrentTrain: epoch  7, batch    70 | loss: 84.4942783
CurrentTrain: epoch  7, batch    71 | loss: 60.7123838
CurrentTrain: epoch  7, batch    72 | loss: 86.1969276
CurrentTrain: epoch  7, batch    73 | loss: 84.7308737
CurrentTrain: epoch  7, batch    74 | loss: 85.7488444
CurrentTrain: epoch  7, batch    75 | loss: 116.0565250
CurrentTrain: epoch  7, batch    76 | loss: 61.5816235
CurrentTrain: epoch  7, batch    77 | loss: 115.6842897
CurrentTrain: epoch  7, batch    78 | loss: 65.4063639
CurrentTrain: epoch  7, batch    79 | loss: 53.4852598
CurrentTrain: epoch  7, batch    80 | loss: 61.5988530
CurrentTrain: epoch  7, batch    81 | loss: 65.3784282
CurrentTrain: epoch  7, batch    82 | loss: 84.6046983
CurrentTrain: epoch  7, batch    83 | loss: 66.4822498
CurrentTrain: epoch  7, batch    84 | loss: 68.3176576
CurrentTrain: epoch  7, batch    85 | loss: 66.8584466
CurrentTrain: epoch  7, batch    86 | loss: 49.7681352
CurrentTrain: epoch  7, batch    87 | loss: 84.1289101
CurrentTrain: epoch  7, batch    88 | loss: 51.8375547
CurrentTrain: epoch  7, batch    89 | loss: 53.0897868
CurrentTrain: epoch  7, batch    90 | loss: 66.8616395
CurrentTrain: epoch  7, batch    91 | loss: 113.5925977
CurrentTrain: epoch  7, batch    92 | loss: 60.4690539
CurrentTrain: epoch  7, batch    93 | loss: 45.0670100
CurrentTrain: epoch  7, batch    94 | loss: 43.7381822
CurrentTrain: epoch  7, batch    95 | loss: 55.7446136
CurrentTrain: epoch  8, batch     0 | loss: 82.3809815
CurrentTrain: epoch  8, batch     1 | loss: 64.7168834
CurrentTrain: epoch  8, batch     2 | loss: 52.1291936
CurrentTrain: epoch  8, batch     3 | loss: 51.3951183
CurrentTrain: epoch  8, batch     4 | loss: 59.9853903
CurrentTrain: epoch  8, batch     5 | loss: 81.2444660
CurrentTrain: epoch  8, batch     6 | loss: 40.7171174
CurrentTrain: epoch  8, batch     7 | loss: 84.2292935
CurrentTrain: epoch  8, batch     8 | loss: 42.4708618
CurrentTrain: epoch  8, batch     9 | loss: 44.7392054
CurrentTrain: epoch  8, batch    10 | loss: 118.4052305
CurrentTrain: epoch  8, batch    11 | loss: 53.3734483
CurrentTrain: epoch  8, batch    12 | loss: 62.9858323
CurrentTrain: epoch  8, batch    13 | loss: 81.3566950
CurrentTrain: epoch  8, batch    14 | loss: 82.6516816
CurrentTrain: epoch  8, batch    15 | loss: 117.6105063
CurrentTrain: epoch  8, batch    16 | loss: 61.0620197
CurrentTrain: epoch  8, batch    17 | loss: 82.0214255
CurrentTrain: epoch  8, batch    18 | loss: 64.5580386
CurrentTrain: epoch  8, batch    19 | loss: 51.5912503
CurrentTrain: epoch  8, batch    20 | loss: 52.1407825
CurrentTrain: epoch  8, batch    21 | loss: 78.4086477
CurrentTrain: epoch  8, batch    22 | loss: 78.2923344
CurrentTrain: epoch  8, batch    23 | loss: 42.2729110
CurrentTrain: epoch  8, batch    24 | loss: 178.0097609
CurrentTrain: epoch  8, batch    25 | loss: 117.5786168
CurrentTrain: epoch  8, batch    26 | loss: 63.4839783
CurrentTrain: epoch  8, batch    27 | loss: 65.6872195
CurrentTrain: epoch  8, batch    28 | loss: 66.9247777
CurrentTrain: epoch  8, batch    29 | loss: 65.4316699
CurrentTrain: epoch  8, batch    30 | loss: 82.6187967
CurrentTrain: epoch  8, batch    31 | loss: 84.0481462
CurrentTrain: epoch  8, batch    32 | loss: 84.1812392
CurrentTrain: epoch  8, batch    33 | loss: 50.7229146
CurrentTrain: epoch  8, batch    34 | loss: 52.8430000
CurrentTrain: epoch  8, batch    35 | loss: 65.4991947
CurrentTrain: epoch  8, batch    36 | loss: 43.6140322
CurrentTrain: epoch  8, batch    37 | loss: 69.6215673
CurrentTrain: epoch  8, batch    38 | loss: 181.5573587
CurrentTrain: epoch  8, batch    39 | loss: 60.4067887
CurrentTrain: epoch  8, batch    40 | loss: 48.3260795
CurrentTrain: epoch  8, batch    41 | loss: 66.5889526
CurrentTrain: epoch  8, batch    42 | loss: 64.1155743
CurrentTrain: epoch  8, batch    43 | loss: 82.5943685
CurrentTrain: epoch  8, batch    44 | loss: 65.4348001
CurrentTrain: epoch  8, batch    45 | loss: 64.3812770
CurrentTrain: epoch  8, batch    46 | loss: 85.7530279
CurrentTrain: epoch  8, batch    47 | loss: 84.0512417
CurrentTrain: epoch  8, batch    48 | loss: 67.3305792
CurrentTrain: epoch  8, batch    49 | loss: 63.0160137
CurrentTrain: epoch  8, batch    50 | loss: 64.3576529
CurrentTrain: epoch  8, batch    51 | loss: 63.4590930
CurrentTrain: epoch  8, batch    52 | loss: 66.1126966
CurrentTrain: epoch  8, batch    53 | loss: 61.6852040
CurrentTrain: epoch  8, batch    54 | loss: 51.5542682
CurrentTrain: epoch  8, batch    55 | loss: 85.8511116
CurrentTrain: epoch  8, batch    56 | loss: 63.9904020
CurrentTrain: epoch  8, batch    57 | loss: 82.5466823
CurrentTrain: epoch  8, batch    58 | loss: 63.8248496
CurrentTrain: epoch  8, batch    59 | loss: 115.1885290
CurrentTrain: epoch  8, batch    60 | loss: 63.8602996
CurrentTrain: epoch  8, batch    61 | loss: 41.7897256
CurrentTrain: epoch  8, batch    62 | loss: 66.8345783
CurrentTrain: epoch  8, batch    63 | loss: 65.4569810
CurrentTrain: epoch  8, batch    64 | loss: 52.4473319
CurrentTrain: epoch  8, batch    65 | loss: 53.3752404
CurrentTrain: epoch  8, batch    66 | loss: 64.4084418
CurrentTrain: epoch  8, batch    67 | loss: 66.9911656
CurrentTrain: epoch  8, batch    68 | loss: 81.1379575
CurrentTrain: epoch  8, batch    69 | loss: 50.5349822
CurrentTrain: epoch  8, batch    70 | loss: 66.9628229
CurrentTrain: epoch  8, batch    71 | loss: 62.9020423
CurrentTrain: epoch  8, batch    72 | loss: 51.1131417
CurrentTrain: epoch  8, batch    73 | loss: 82.8734596
CurrentTrain: epoch  8, batch    74 | loss: 54.7415877
CurrentTrain: epoch  8, batch    75 | loss: 45.8297674
CurrentTrain: epoch  8, batch    76 | loss: 113.2138392
CurrentTrain: epoch  8, batch    77 | loss: 43.9565748
CurrentTrain: epoch  8, batch    78 | loss: 52.1383228
CurrentTrain: epoch  8, batch    79 | loss: 93.7793592
CurrentTrain: epoch  8, batch    80 | loss: 66.6868286
CurrentTrain: epoch  8, batch    81 | loss: 113.1087293
CurrentTrain: epoch  8, batch    82 | loss: 120.2148946
CurrentTrain: epoch  8, batch    83 | loss: 49.4998632
CurrentTrain: epoch  8, batch    84 | loss: 52.0873030
CurrentTrain: epoch  8, batch    85 | loss: 85.3640656
CurrentTrain: epoch  8, batch    86 | loss: 64.4743220
CurrentTrain: epoch  8, batch    87 | loss: 81.4948876
CurrentTrain: epoch  8, batch    88 | loss: 63.7192594
CurrentTrain: epoch  8, batch    89 | loss: 62.7521492
CurrentTrain: epoch  8, batch    90 | loss: 68.4576093
CurrentTrain: epoch  8, batch    91 | loss: 48.1305263
CurrentTrain: epoch  8, batch    92 | loss: 49.0660151
CurrentTrain: epoch  8, batch    93 | loss: 115.2540126
CurrentTrain: epoch  8, batch    94 | loss: 65.2949758
CurrentTrain: epoch  8, batch    95 | loss: 55.5740518
CurrentTrain: epoch  9, batch     0 | loss: 51.4226424
CurrentTrain: epoch  9, batch     1 | loss: 81.3051921
CurrentTrain: epoch  9, batch     2 | loss: 51.8290324
CurrentTrain: epoch  9, batch     3 | loss: 84.1348563
CurrentTrain: epoch  9, batch     4 | loss: 48.3927937
CurrentTrain: epoch  9, batch     5 | loss: 82.4738871
CurrentTrain: epoch  9, batch     6 | loss: 65.6250790
CurrentTrain: epoch  9, batch     7 | loss: 117.5595790
CurrentTrain: epoch  9, batch     8 | loss: 83.0409174
CurrentTrain: epoch  9, batch     9 | loss: 63.2519332
CurrentTrain: epoch  9, batch    10 | loss: 82.3557125
CurrentTrain: epoch  9, batch    11 | loss: 51.4387556
CurrentTrain: epoch  9, batch    12 | loss: 64.2665285
CurrentTrain: epoch  9, batch    13 | loss: 84.6713819
CurrentTrain: epoch  9, batch    14 | loss: 46.2348932
CurrentTrain: epoch  9, batch    15 | loss: 44.2166667
CurrentTrain: epoch  9, batch    16 | loss: 116.0850635
CurrentTrain: epoch  9, batch    17 | loss: 84.1101411
CurrentTrain: epoch  9, batch    18 | loss: 66.8684496
CurrentTrain: epoch  9, batch    19 | loss: 85.8268221
CurrentTrain: epoch  9, batch    20 | loss: 65.3925977
CurrentTrain: epoch  9, batch    21 | loss: 61.0006249
CurrentTrain: epoch  9, batch    22 | loss: 65.6197718
CurrentTrain: epoch  9, batch    23 | loss: 63.3850213
CurrentTrain: epoch  9, batch    24 | loss: 63.0625542
CurrentTrain: epoch  9, batch    25 | loss: 65.9451068
CurrentTrain: epoch  9, batch    26 | loss: 64.0837621
CurrentTrain: epoch  9, batch    27 | loss: 52.5846273
CurrentTrain: epoch  9, batch    28 | loss: 79.6316371
CurrentTrain: epoch  9, batch    29 | loss: 82.5927334
CurrentTrain: epoch  9, batch    30 | loss: 79.6044226
CurrentTrain: epoch  9, batch    31 | loss: 66.8609493
CurrentTrain: epoch  9, batch    32 | loss: 85.7439651
CurrentTrain: epoch  9, batch    33 | loss: 62.4197890
CurrentTrain: epoch  9, batch    34 | loss: 52.4093361
CurrentTrain: epoch  9, batch    35 | loss: 117.4644947
CurrentTrain: epoch  9, batch    36 | loss: 52.3483670
CurrentTrain: epoch  9, batch    37 | loss: 64.4372429
CurrentTrain: epoch  9, batch    38 | loss: 54.0891584
CurrentTrain: epoch  9, batch    39 | loss: 64.3326009
CurrentTrain: epoch  9, batch    40 | loss: 47.2071174
CurrentTrain: epoch  9, batch    41 | loss: 49.2099520
CurrentTrain: epoch  9, batch    42 | loss: 81.8793358
CurrentTrain: epoch  9, batch    43 | loss: 84.0032554
CurrentTrain: epoch  9, batch    44 | loss: 48.9210633
CurrentTrain: epoch  9, batch    45 | loss: 82.6479133
CurrentTrain: epoch  9, batch    46 | loss: 64.3817653
CurrentTrain: epoch  9, batch    47 | loss: 59.6919612
CurrentTrain: epoch  9, batch    48 | loss: 84.7686045
CurrentTrain: epoch  9, batch    49 | loss: 44.1983855
CurrentTrain: epoch  9, batch    50 | loss: 81.1182513
CurrentTrain: epoch  9, batch    51 | loss: 110.2372090
CurrentTrain: epoch  9, batch    52 | loss: 87.5582942
CurrentTrain: epoch  9, batch    53 | loss: 62.8509832
CurrentTrain: epoch  9, batch    54 | loss: 64.2500314
CurrentTrain: epoch  9, batch    55 | loss: 61.9477938
CurrentTrain: epoch  9, batch    56 | loss: 81.4165046
CurrentTrain: epoch  9, batch    57 | loss: 61.9805477
CurrentTrain: epoch  9, batch    58 | loss: 111.2403187
CurrentTrain: epoch  9, batch    59 | loss: 64.0027030
CurrentTrain: epoch  9, batch    60 | loss: 65.1940428
CurrentTrain: epoch  9, batch    61 | loss: 115.1854288
CurrentTrain: epoch  9, batch    62 | loss: 64.8265662
CurrentTrain: epoch  9, batch    63 | loss: 61.9161278
CurrentTrain: epoch  9, batch    64 | loss: 52.6999232
CurrentTrain: epoch  9, batch    65 | loss: 62.8282521
CurrentTrain: epoch  9, batch    66 | loss: 42.3751903
CurrentTrain: epoch  9, batch    67 | loss: 53.4271444
CurrentTrain: epoch  9, batch    68 | loss: 64.8983663
CurrentTrain: epoch  9, batch    69 | loss: 81.2955213
CurrentTrain: epoch  9, batch    70 | loss: 82.3843391
CurrentTrain: epoch  9, batch    71 | loss: 79.7718307
CurrentTrain: epoch  9, batch    72 | loss: 63.1637186
CurrentTrain: epoch  9, batch    73 | loss: 62.7835545
CurrentTrain: epoch  9, batch    74 | loss: 66.8879074
CurrentTrain: epoch  9, batch    75 | loss: 52.8485432
CurrentTrain: epoch  9, batch    76 | loss: 52.9349874
CurrentTrain: epoch  9, batch    77 | loss: 79.8264722
CurrentTrain: epoch  9, batch    78 | loss: 62.3172613
CurrentTrain: epoch  9, batch    79 | loss: 54.1328658
CurrentTrain: epoch  9, batch    80 | loss: 66.9765458
CurrentTrain: epoch  9, batch    81 | loss: 77.0930286
CurrentTrain: epoch  9, batch    82 | loss: 84.0457740
CurrentTrain: epoch  9, batch    83 | loss: 83.9223121
CurrentTrain: epoch  9, batch    84 | loss: 48.7465945
CurrentTrain: epoch  9, batch    85 | loss: 115.1796350
CurrentTrain: epoch  9, batch    86 | loss: 79.5468177
CurrentTrain: epoch  9, batch    87 | loss: 51.2345155
CurrentTrain: epoch  9, batch    88 | loss: 82.7580012
CurrentTrain: epoch  9, batch    89 | loss: 85.7500026
CurrentTrain: epoch  9, batch    90 | loss: 64.1966752
CurrentTrain: epoch  9, batch    91 | loss: 84.0154652
CurrentTrain: epoch  9, batch    92 | loss: 54.0180856
CurrentTrain: epoch  9, batch    93 | loss: 64.0816230
CurrentTrain: epoch  9, batch    94 | loss: 67.0081275
CurrentTrain: epoch  9, batch    95 | loss: 65.5244335

F1 score per class: {32: 0.6352941176470588, 6: 0.8764044943820225, 19: 0.4444444444444444, 24: 0.7868852459016393, 26: 0.9130434782608695, 29: 0.882051282051282}
Micro-average F1 score: 0.8110992529348986
Weighted-average F1 score: 0.8177377999420351
F1 score per class: {32: 0.7537688442211056, 6: 0.9010989010989011, 19: 0.46511627906976744, 24: 0.7734806629834254, 26: 0.9743589743589743, 29: 0.8359788359788359}
Micro-average F1 score: 0.8311425682507584
Weighted-average F1 score: 0.827514405759278
F1 score per class: {32: 0.7537688442211056, 6: 0.9010989010989011, 19: 0.46511627906976744, 24: 0.7734806629834254, 26: 0.9743589743589743, 29: 0.8359788359788359}
Micro-average F1 score: 0.8311425682507584
Weighted-average F1 score: 0.827514405759278

F1 score per class: {32: 0.6352941176470588, 6: 0.8764044943820225, 19: 0.4444444444444444, 24: 0.7868852459016393, 26: 0.9130434782608695, 29: 0.882051282051282}
Micro-average F1 score: 0.8110992529348986
Weighted-average F1 score: 0.8177377999420351
F1 score per class: {32: 0.7537688442211056, 6: 0.9010989010989011, 19: 0.46511627906976744, 24: 0.7734806629834254, 26: 0.9743589743589743, 29: 0.8359788359788359}
Micro-average F1 score: 0.8311425682507584
Weighted-average F1 score: 0.827514405759278
F1 score per class: {32: 0.7537688442211056, 6: 0.9010989010989011, 19: 0.46511627906976744, 24: 0.7734806629834254, 26: 0.9743589743589743, 29: 0.8359788359788359}
Micro-average F1 score: 0.8311425682507584
Weighted-average F1 score: 0.827514405759278
cur_acc:  ['0.8111']
his_acc:  ['0.8111']
cur_acc des:  ['0.8311']
his_acc des:  ['0.8311']
cur_acc rrf:  ['0.8311']
his_acc rrf:  ['0.8311']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 84.3407431
CurrentTrain: epoch  0, batch     1 | loss: 69.3834780
CurrentTrain: epoch  0, batch     2 | loss: 80.3936570
CurrentTrain: epoch  0, batch     3 | loss: 47.2468858
CurrentTrain: epoch  1, batch     0 | loss: 95.9362568
CurrentTrain: epoch  1, batch     1 | loss: 72.0382187
CurrentTrain: epoch  1, batch     2 | loss: 75.6106155
CurrentTrain: epoch  1, batch     3 | loss: 57.4209962
CurrentTrain: epoch  2, batch     0 | loss: 122.9055700
CurrentTrain: epoch  2, batch     1 | loss: 72.9737120
CurrentTrain: epoch  2, batch     2 | loss: 70.6860347
CurrentTrain: epoch  2, batch     3 | loss: 36.0829922
CurrentTrain: epoch  3, batch     0 | loss: 58.7375413
CurrentTrain: epoch  3, batch     1 | loss: 71.5760777
CurrentTrain: epoch  3, batch     2 | loss: 70.3491284
CurrentTrain: epoch  3, batch     3 | loss: 42.4685291
CurrentTrain: epoch  4, batch     0 | loss: 71.2333964
CurrentTrain: epoch  4, batch     1 | loss: 68.7691935
CurrentTrain: epoch  4, batch     2 | loss: 55.2884998
CurrentTrain: epoch  4, batch     3 | loss: 42.3664436
CurrentTrain: epoch  5, batch     0 | loss: 120.6900066
CurrentTrain: epoch  5, batch     1 | loss: 55.0108190
CurrentTrain: epoch  5, batch     2 | loss: 52.1942731
CurrentTrain: epoch  5, batch     3 | loss: 72.1366526
CurrentTrain: epoch  6, batch     0 | loss: 67.6170870
CurrentTrain: epoch  6, batch     1 | loss: 66.0427177
CurrentTrain: epoch  6, batch     2 | loss: 52.4187941
CurrentTrain: epoch  6, batch     3 | loss: 71.9621633
CurrentTrain: epoch  7, batch     0 | loss: 65.6304253
CurrentTrain: epoch  7, batch     1 | loss: 65.3904179
CurrentTrain: epoch  7, batch     2 | loss: 62.6549820
CurrentTrain: epoch  7, batch     3 | loss: 52.5998633
CurrentTrain: epoch  8, batch     0 | loss: 53.9691028
CurrentTrain: epoch  8, batch     1 | loss: 49.9526040
CurrentTrain: epoch  8, batch     2 | loss: 86.1387939
CurrentTrain: epoch  8, batch     3 | loss: 38.4301225
CurrentTrain: epoch  9, batch     0 | loss: 66.2995282
CurrentTrain: epoch  9, batch     1 | loss: 54.4149338
CurrentTrain: epoch  9, batch     2 | loss: 67.2151675
CurrentTrain: epoch  9, batch     3 | loss: 27.8395665
MemoryTrain:  epoch  0, batch     0 | loss: 0.3436658
MemoryTrain:  epoch  1, batch     0 | loss: 0.3133407
MemoryTrain:  epoch  2, batch     0 | loss: 0.1963894
MemoryTrain:  epoch  3, batch     0 | loss: 0.3977895
MemoryTrain:  epoch  4, batch     0 | loss: 0.1391410
MemoryTrain:  epoch  5, batch     0 | loss: 0.1133604
MemoryTrain:  epoch  6, batch     0 | loss: 0.1123764
MemoryTrain:  epoch  7, batch     0 | loss: 0.0765637
MemoryTrain:  epoch  8, batch     0 | loss: 0.0755850
MemoryTrain:  epoch  9, batch     0 | loss: 0.0844176

F1 score per class: {33: 0.4716981132075472, 36: 0.6829268292682927, 8: 0.0, 20: 0.0, 26: 0.8823529411764706, 29: 0.4, 30: 0.2857142857142857}
Micro-average F1 score: 0.5189873417721519
Weighted-average F1 score: 0.5752065330612363
F1 score per class: {33: 0.0, 36: 0.8260869565217391, 6: 0.9215686274509803, 8: 0.0, 20: 0.0, 26: 0.9444444444444444, 29: 0.5, 30: 0.8448275862068966}
Micro-average F1 score: 0.8405797101449275
Weighted-average F1 score: 0.8312523798103091
F1 score per class: {33: 0.0, 36: 0.8428571428571429, 6: 0.9306930693069307, 8: 0.0, 20: 0.0, 26: 0.9444444444444444, 29: 0.5, 30: 0.8448275862068966}
Micro-average F1 score: 0.8523002421307506
Weighted-average F1 score: 0.8476268162192173

F1 score per class: {32: 0.3787878787878788, 33: 0.32894736842105265, 36: 0.8439306358381503, 6: 0.6829268292682927, 8: 0.4166666666666667, 19: 0.7700534759358288, 20: 0.8901098901098901, 24: 0.8823529411764706, 26: 0.9052631578947369, 29: 0.24, 30: 0.2857142857142857}
Micro-average F1 score: 0.6740858505564388
Weighted-average F1 score: 0.7070743846390228
F1 score per class: {32: 0.6256983240223464, 33: 0.6440677966101694, 36: 0.8888888888888888, 6: 0.9215686274509803, 8: 0.5142857142857142, 19: 0.7777777777777778, 20: 0.9148936170212766, 24: 0.85, 26: 0.8756756756756757, 29: 0.3076923076923077, 30: 0.8376068376068376}
Micro-average F1 score: 0.7892122072391767
Weighted-average F1 score: 0.7843917647421473
F1 score per class: {32: 0.5165562913907285, 33: 0.592964824120603, 36: 0.8888888888888888, 6: 0.9306930693069307, 8: 0.5294117647058824, 19: 0.7741935483870968, 20: 0.9090909090909091, 24: 0.85, 26: 0.8817204301075269, 29: 0.2962962962962963, 30: 0.8448275862068966}
Micro-average F1 score: 0.7718550106609808
Weighted-average F1 score: 0.7689515577561119
cur_acc:  ['0.8111', '0.5190']
his_acc:  ['0.8111', '0.6741']
cur_acc des:  ['0.8311', '0.8406']
his_acc des:  ['0.8311', '0.7892']
cur_acc rrf:  ['0.8311', '0.8523']
his_acc rrf:  ['0.8311', '0.7719']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 81.2483377
CurrentTrain: epoch  0, batch     1 | loss: 77.7906345
CurrentTrain: epoch  0, batch     2 | loss: 77.2235764
CurrentTrain: epoch  0, batch     3 | loss: 96.9156908
CurrentTrain: epoch  0, batch     4 | loss: 29.2428461
CurrentTrain: epoch  1, batch     0 | loss: 62.1671098
CurrentTrain: epoch  1, batch     1 | loss: 181.5822107
CurrentTrain: epoch  1, batch     2 | loss: 95.9185612
CurrentTrain: epoch  1, batch     3 | loss: 59.8791287
CurrentTrain: epoch  1, batch     4 | loss: 20.9158327
CurrentTrain: epoch  2, batch     0 | loss: 74.3713792
CurrentTrain: epoch  2, batch     1 | loss: 65.7775894
CurrentTrain: epoch  2, batch     2 | loss: 122.5498048
CurrentTrain: epoch  2, batch     3 | loss: 74.0071000
CurrentTrain: epoch  2, batch     4 | loss: 28.0498779
CurrentTrain: epoch  3, batch     0 | loss: 68.3697820
CurrentTrain: epoch  3, batch     1 | loss: 178.4959363
CurrentTrain: epoch  3, batch     2 | loss: 87.9938215
CurrentTrain: epoch  3, batch     3 | loss: 71.1450025
CurrentTrain: epoch  3, batch     4 | loss: 17.7376288
CurrentTrain: epoch  4, batch     0 | loss: 89.5288819
CurrentTrain: epoch  4, batch     1 | loss: 69.0953506
CurrentTrain: epoch  4, batch     2 | loss: 67.4590831
CurrentTrain: epoch  4, batch     3 | loss: 68.7453314
CurrentTrain: epoch  4, batch     4 | loss: 26.8554635
CurrentTrain: epoch  5, batch     0 | loss: 67.9121107
CurrentTrain: epoch  5, batch     1 | loss: 184.2540734
CurrentTrain: epoch  5, batch     2 | loss: 55.3630763
CurrentTrain: epoch  5, batch     3 | loss: 67.5868637
CurrentTrain: epoch  5, batch     4 | loss: 16.0732080
CurrentTrain: epoch  6, batch     0 | loss: 87.8124048
CurrentTrain: epoch  6, batch     1 | loss: 68.2332058
CurrentTrain: epoch  6, batch     2 | loss: 67.9094649
CurrentTrain: epoch  6, batch     3 | loss: 54.0544080
CurrentTrain: epoch  6, batch     4 | loss: 17.1095982
CurrentTrain: epoch  7, batch     0 | loss: 53.7635361
CurrentTrain: epoch  7, batch     1 | loss: 65.3234282
CurrentTrain: epoch  7, batch     2 | loss: 54.6065056
CurrentTrain: epoch  7, batch     3 | loss: 86.2876152
CurrentTrain: epoch  7, batch     4 | loss: 59.8719137
CurrentTrain: epoch  8, batch     0 | loss: 67.6155141
CurrentTrain: epoch  8, batch     1 | loss: 83.4271768
CurrentTrain: epoch  8, batch     2 | loss: 85.5273132
CurrentTrain: epoch  8, batch     3 | loss: 64.3787901
CurrentTrain: epoch  8, batch     4 | loss: 15.2772675
CurrentTrain: epoch  9, batch     0 | loss: 114.9550916
CurrentTrain: epoch  9, batch     1 | loss: 83.7923521
CurrentTrain: epoch  9, batch     2 | loss: 65.4017944
CurrentTrain: epoch  9, batch     3 | loss: 62.3701183
CurrentTrain: epoch  9, batch     4 | loss: 26.6977984
MemoryTrain:  epoch  0, batch     0 | loss: 0.7112498
MemoryTrain:  epoch  1, batch     0 | loss: 0.6275982
MemoryTrain:  epoch  2, batch     0 | loss: 0.4707690
MemoryTrain:  epoch  3, batch     0 | loss: 0.3655281
MemoryTrain:  epoch  4, batch     0 | loss: 0.2097442
MemoryTrain:  epoch  5, batch     0 | loss: 0.1775779
MemoryTrain:  epoch  6, batch     0 | loss: 0.1478108
MemoryTrain:  epoch  7, batch     0 | loss: 0.1109420
MemoryTrain:  epoch  8, batch     0 | loss: 0.1102493
MemoryTrain:  epoch  9, batch     0 | loss: 0.0920213

F1 score per class: {2: 0.875, 36: 0.0, 39: 0.4, 8: 0.2905982905982906, 11: 0.0, 12: 0.6153846153846154, 19: 0.0, 28: 0.13333333333333333}
Micro-average F1 score: 0.36298932384341637
Weighted-average F1 score: 0.3721843121036669
F1 score per class: {33: 0.9411764705882353, 2: 0.0, 36: 0.7153284671532847, 39: 0.7804878048780488, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.6666666666666666, 24: 0.0, 26: 0.0, 28: 0.35294117647058826}
Micro-average F1 score: 0.6808510638297872
Weighted-average F1 score: 0.6131032889394293
F1 score per class: {33: 0.9411764705882353, 2: 0.0, 36: 0.7153284671532847, 39: 0.7878787878787878, 8: 0.0, 11: 0.0, 12: 0.6153846153846154, 19: 0.0, 24: 0.0, 28: 0.13333333333333333}
Micro-average F1 score: 0.679144385026738
Weighted-average F1 score: 0.625825059855448

F1 score per class: {32: 0.875, 33: 0.5135135135135135, 2: 0.24, 36: 0.3826086956521739, 6: 0.272, 39: 0.7218934911242604, 8: 0.65, 11: 0.34782608695652173, 12: 0.7613636363636364, 19: 0.1568627450980392, 20: 0.8715083798882681, 24: 0.9142857142857143, 26: 0.8958333333333334, 28: 0.3157894736842105, 29: 0.28205128205128205, 30: 0.13333333333333333}
Micro-average F1 score: 0.5956607495069034
Weighted-average F1 score: 0.6464037319535255
F1 score per class: {32: 0.9411764705882353, 33: 0.6285714285714286, 2: 0.5549132947976878, 36: 0.6901408450704225, 6: 0.7071823204419889, 39: 0.7624309392265194, 8: 0.8260869565217391, 11: 0.5, 12: 0.770949720670391, 19: 0.38095238095238093, 20: 0.8972972972972973, 24: 0.8780487804878049, 26: 0.8911917098445595, 28: 0.3157894736842105, 29: 0.58, 30: 0.2857142857142857}
Micro-average F1 score: 0.723744292237443
Weighted-average F1 score: 0.7257573637437371
F1 score per class: {32: 0.9411764705882353, 33: 0.6181818181818182, 2: 0.5227272727272727, 36: 0.6901408450704225, 6: 0.7027027027027027, 39: 0.7624309392265194, 8: 0.8260869565217391, 11: 0.5, 12: 0.768361581920904, 19: 0.2857142857142857, 20: 0.8839779005524862, 24: 0.8780487804878049, 26: 0.8808290155440415, 28: 0.3, 29: 0.58, 30: 0.1111111111111111}
Micro-average F1 score: 0.7116704805491991
Weighted-average F1 score: 0.71166002586159
cur_acc:  ['0.8111', '0.5190', '0.3630']
his_acc:  ['0.8111', '0.6741', '0.5957']
cur_acc des:  ['0.8311', '0.8406', '0.6809']
his_acc des:  ['0.8311', '0.7892', '0.7237']
cur_acc rrf:  ['0.8311', '0.8523', '0.6791']
his_acc rrf:  ['0.8311', '0.7719', '0.7117']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 66.3089923
CurrentTrain: epoch  0, batch     1 | loss: 135.2148521
CurrentTrain: epoch  0, batch     2 | loss: 95.1153431
CurrentTrain: epoch  0, batch     3 | loss: 131.5878255
CurrentTrain: epoch  0, batch     4 | loss: 63.9469117
CurrentTrain: epoch  1, batch     0 | loss: 97.7274697
CurrentTrain: epoch  1, batch     1 | loss: 69.4585688
CurrentTrain: epoch  1, batch     2 | loss: 121.8167258
CurrentTrain: epoch  1, batch     3 | loss: 71.2735467
CurrentTrain: epoch  1, batch     4 | loss: 57.4088306
CurrentTrain: epoch  2, batch     0 | loss: 87.7993430
CurrentTrain: epoch  2, batch     1 | loss: 86.5652611
CurrentTrain: epoch  2, batch     2 | loss: 58.8833880
CurrentTrain: epoch  2, batch     3 | loss: 75.0526180
CurrentTrain: epoch  2, batch     4 | loss: 56.8006069
CurrentTrain: epoch  3, batch     0 | loss: 90.0483248
CurrentTrain: epoch  3, batch     1 | loss: 68.9733282
CurrentTrain: epoch  3, batch     2 | loss: 90.7290925
CurrentTrain: epoch  3, batch     3 | loss: 86.7365718
CurrentTrain: epoch  3, batch     4 | loss: 42.6395825
CurrentTrain: epoch  4, batch     0 | loss: 69.5328096
CurrentTrain: epoch  4, batch     1 | loss: 69.6686771
CurrentTrain: epoch  4, batch     2 | loss: 117.8853669
CurrentTrain: epoch  4, batch     3 | loss: 62.5912034
CurrentTrain: epoch  4, batch     4 | loss: 55.2354216
CurrentTrain: epoch  5, batch     0 | loss: 88.7517386
CurrentTrain: epoch  5, batch     1 | loss: 66.3435052
CurrentTrain: epoch  5, batch     2 | loss: 55.3119748
CurrentTrain: epoch  5, batch     3 | loss: 84.4686141
CurrentTrain: epoch  5, batch     4 | loss: 55.2140200
CurrentTrain: epoch  6, batch     0 | loss: 85.7233284
CurrentTrain: epoch  6, batch     1 | loss: 116.5070090
CurrentTrain: epoch  6, batch     2 | loss: 84.2701862
CurrentTrain: epoch  6, batch     3 | loss: 52.2149813
CurrentTrain: epoch  6, batch     4 | loss: 73.2487997
CurrentTrain: epoch  7, batch     0 | loss: 85.1312161
CurrentTrain: epoch  7, batch     1 | loss: 84.7090992
CurrentTrain: epoch  7, batch     2 | loss: 83.4367554
CurrentTrain: epoch  7, batch     3 | loss: 82.3412909
CurrentTrain: epoch  7, batch     4 | loss: 52.1322670
CurrentTrain: epoch  8, batch     0 | loss: 52.3891391
CurrentTrain: epoch  8, batch     1 | loss: 115.6927697
CurrentTrain: epoch  8, batch     2 | loss: 54.0043246
CurrentTrain: epoch  8, batch     3 | loss: 63.8588634
CurrentTrain: epoch  8, batch     4 | loss: 116.6175793
CurrentTrain: epoch  9, batch     0 | loss: 66.1338365
CurrentTrain: epoch  9, batch     1 | loss: 84.6316273
CurrentTrain: epoch  9, batch     2 | loss: 61.6538523
CurrentTrain: epoch  9, batch     3 | loss: 113.4911552
CurrentTrain: epoch  9, batch     4 | loss: 54.1117621
MemoryTrain:  epoch  0, batch     0 | loss: 0.5810154
MemoryTrain:  epoch  1, batch     0 | loss: 0.4721457
MemoryTrain:  epoch  2, batch     0 | loss: 0.3223267
MemoryTrain:  epoch  3, batch     0 | loss: 0.2851803
MemoryTrain:  epoch  4, batch     0 | loss: 0.2392980
MemoryTrain:  epoch  5, batch     0 | loss: 0.2010573
MemoryTrain:  epoch  6, batch     0 | loss: 0.1347754
MemoryTrain:  epoch  7, batch     0 | loss: 0.1316127
MemoryTrain:  epoch  8, batch     0 | loss: 0.1046928
MemoryTrain:  epoch  9, batch     0 | loss: 0.0868179

F1 score per class: {5: 0.9637305699481865, 6: 0.0, 8: 0.0, 10: 0.31932773109243695, 11: 0.0, 12: 0.0, 16: 0.7843137254901961, 17: 0.7142857142857143, 18: 0.27906976744186046, 28: 0.0}
Micro-average F1 score: 0.6485260770975056
Weighted-average F1 score: 0.7115794261049182
F1 score per class: {5: 0.9950248756218906, 6: 0.0, 39: 0.0, 8: 0.6258503401360545, 10: 0.0, 11: 0.0, 12: 0.7755102040816326, 16: 0.8, 17: 0.8064516129032258, 18: 0.0, 20: 0.0, 28: 0.0}
Micro-average F1 score: 0.7701375245579568
Weighted-average F1 score: 0.7279333159842798
F1 score per class: {36: 0.9950248756218906, 5: 0.0, 6: 0.0, 39: 0.6442953020134228, 8: 0.0, 10: 0.0, 11: 0.76, 12: 0.8, 16: 0.7017543859649122, 17: 0.0, 18: 0.0, 20: 0.0, 28: 0.0}
Micro-average F1 score: 0.7658730158730159
Weighted-average F1 score: 0.7285222366481006

F1 score per class: {2: 0.7142857142857143, 5: 0.9489795918367347, 6: 0.4657534246575342, 8: 0.15555555555555556, 10: 0.2695035460992908, 11: 0.32432432432432434, 12: 0.5, 16: 0.7692307692307693, 17: 0.25, 18: 0.2033898305084746, 19: 0.7261904761904762, 20: 0.5405405405405406, 24: 0.34782608695652173, 26: 0.7640449438202247, 28: 0.23529411764705882, 29: 0.8457142857142858, 30: 0.9444444444444444, 32: 0.9025641025641026, 33: 0.3157894736842105, 36: 0.08695652173913043, 39: 0.125}
Micro-average F1 score: 0.5915492957746479
Weighted-average F1 score: 0.6546556899886425
F1 score per class: {2: 0.8, 5: 0.9174311926605505, 6: 0.6839378238341969, 8: 0.48175182481751827, 10: 0.5644171779141104, 11: 0.546875, 12: 0.6368715083798883, 16: 0.7755102040816326, 17: 0.36363636363636365, 18: 0.46296296296296297, 19: 0.7771428571428571, 20: 0.6896551724137931, 24: 0.5142857142857142, 26: 0.7752808988764045, 28: 0.35714285714285715, 29: 0.8651685393258427, 30: 0.8181818181818182, 32: 0.8795811518324608, 33: 0.2857142857142857, 36: 0.345679012345679, 39: 0.18181818181818182}
Micro-average F1 score: 0.6822801590808661
Weighted-average F1 score: 0.6894976232533196
F1 score per class: {2: 0.8, 5: 0.9049773755656109, 6: 0.5748502994011976, 8: 0.416, 10: 0.5614035087719298, 11: 0.5538461538461539, 12: 0.6256983240223464, 16: 0.7307692307692307, 17: 0.26666666666666666, 18: 0.40404040404040403, 19: 0.7909604519774012, 20: 0.6896551724137931, 24: 0.48484848484848486, 26: 0.7752808988764045, 28: 0.2857142857142857, 29: 0.8651685393258427, 30: 0.8571428571428571, 32: 0.875, 33: 0.2608695652173913, 36: 0.34146341463414637, 39: 0.2}
Micro-average F1 score: 0.6619280319857841
Weighted-average F1 score: 0.6663483571028821
cur_acc:  ['0.8111', '0.5190', '0.3630', '0.6485']
his_acc:  ['0.8111', '0.6741', '0.5957', '0.5915']
cur_acc des:  ['0.8311', '0.8406', '0.6809', '0.7701']
his_acc des:  ['0.8311', '0.7892', '0.7237', '0.6823']
cur_acc rrf:  ['0.8311', '0.8523', '0.6791', '0.7659']
his_acc rrf:  ['0.8311', '0.7719', '0.7117', '0.6619']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 102.3925418
CurrentTrain: epoch  0, batch     1 | loss: 83.0997346
CurrentTrain: epoch  0, batch     2 | loss: 89.0736782
CurrentTrain: epoch  0, batch     3 | loss: 91.1956604
CurrentTrain: epoch  0, batch     4 | loss: 108.4061350
CurrentTrain: epoch  1, batch     0 | loss: 73.8076145
CurrentTrain: epoch  1, batch     1 | loss: 127.2034732
CurrentTrain: epoch  1, batch     2 | loss: 77.1316717
CurrentTrain: epoch  1, batch     3 | loss: 59.6964159
CurrentTrain: epoch  1, batch     4 | loss: 68.2578550
CurrentTrain: epoch  2, batch     0 | loss: 186.6736669
CurrentTrain: epoch  2, batch     1 | loss: 69.1847745
CurrentTrain: epoch  2, batch     2 | loss: 58.0253751
CurrentTrain: epoch  2, batch     3 | loss: 70.6050310
CurrentTrain: epoch  2, batch     4 | loss: 67.5394596
CurrentTrain: epoch  3, batch     0 | loss: 89.4305858
CurrentTrain: epoch  3, batch     1 | loss: 67.2530922
CurrentTrain: epoch  3, batch     2 | loss: 88.1603918
CurrentTrain: epoch  3, batch     3 | loss: 87.0240455
CurrentTrain: epoch  3, batch     4 | loss: 99.5147596
CurrentTrain: epoch  4, batch     0 | loss: 66.9190670
CurrentTrain: epoch  4, batch     1 | loss: 56.2528149
CurrentTrain: epoch  4, batch     2 | loss: 56.4856813
CurrentTrain: epoch  4, batch     3 | loss: 89.1444192
CurrentTrain: epoch  4, batch     4 | loss: 47.2562035
CurrentTrain: epoch  5, batch     0 | loss: 119.7626232
CurrentTrain: epoch  5, batch     1 | loss: 53.9582920
CurrentTrain: epoch  5, batch     2 | loss: 85.0147712
CurrentTrain: epoch  5, batch     3 | loss: 68.5260235
CurrentTrain: epoch  5, batch     4 | loss: 45.0618080
CurrentTrain: epoch  6, batch     0 | loss: 53.4783835
CurrentTrain: epoch  6, batch     1 | loss: 85.3900825
CurrentTrain: epoch  6, batch     2 | loss: 67.0926152
CurrentTrain: epoch  6, batch     3 | loss: 118.3571266
CurrentTrain: epoch  6, batch     4 | loss: 36.3646394
CurrentTrain: epoch  7, batch     0 | loss: 66.8217983
CurrentTrain: epoch  7, batch     1 | loss: 53.8325262
CurrentTrain: epoch  7, batch     2 | loss: 65.8362507
CurrentTrain: epoch  7, batch     3 | loss: 67.3187884
CurrentTrain: epoch  7, batch     4 | loss: 64.1365699
CurrentTrain: epoch  8, batch     0 | loss: 52.2731518
CurrentTrain: epoch  8, batch     1 | loss: 83.5682855
CurrentTrain: epoch  8, batch     2 | loss: 65.4760415
CurrentTrain: epoch  8, batch     3 | loss: 84.9240609
CurrentTrain: epoch  8, batch     4 | loss: 104.8596119
CurrentTrain: epoch  9, batch     0 | loss: 85.9670917
CurrentTrain: epoch  9, batch     1 | loss: 64.1396944
CurrentTrain: epoch  9, batch     2 | loss: 116.2181659
CurrentTrain: epoch  9, batch     3 | loss: 81.6638628
CurrentTrain: epoch  9, batch     4 | loss: 28.8328714
MemoryTrain:  epoch  0, batch     0 | loss: 0.5044820
MemoryTrain:  epoch  1, batch     0 | loss: 0.4257974
MemoryTrain:  epoch  2, batch     0 | loss: 0.3375579
MemoryTrain:  epoch  3, batch     0 | loss: 0.3362068
MemoryTrain:  epoch  4, batch     0 | loss: 0.2682493
MemoryTrain:  epoch  5, batch     0 | loss: 0.2471015
MemoryTrain:  epoch  6, batch     0 | loss: 0.1602061
MemoryTrain:  epoch  7, batch     0 | loss: 0.1188758
MemoryTrain:  epoch  8, batch     0 | loss: 0.1660647
MemoryTrain:  epoch  9, batch     0 | loss: 0.1294672

F1 score per class: {32: 0.35294117647058826, 1: 0.7761194029850746, 34: 0.0, 3: 0.0, 10: 0.0547945205479452, 11: 0.0, 14: 0.7103825136612022, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.28125}
Micro-average F1 score: 0.5076660988074957
Weighted-average F1 score: 0.5283792440591195
F1 score per class: {32: 0.36363636363636365, 1: 0.9620253164556962, 34: 0.0, 3: 0.0, 10: 0.12345679012345678, 11: 0.0, 14: 0.7064676616915423, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.6746987951807228}
Micro-average F1 score: 0.6015037593984962
Weighted-average F1 score: 0.5891441091890826
F1 score per class: {32: 0.36363636363636365, 1: 0.9487179487179487, 34: 0.0, 3: 0.0, 10: 0.12345679012345678, 11: 0.0, 14: 0.7064676616915423, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.5897435897435898}
Micro-average F1 score: 0.5857359635811836
Weighted-average F1 score: 0.5742662772545007

F1 score per class: {1: 0.3387096774193548, 2: 0.7142857142857143, 3: 0.7761194029850746, 5: 0.9644670050761421, 6: 0.5575757575757576, 8: 0.1956521739130435, 10: 0.445859872611465, 11: 0.37398373983739835, 12: 0.3816793893129771, 14: 0.05128205128205128, 16: 0.7843137254901961, 17: 0.3, 18: 0.1568627450980392, 19: 0.49645390070921985, 20: 0.3582089552238806, 22: 0.6878306878306878, 24: 0.07407407407407407, 26: 0.7802197802197802, 28: 0.2702702702702703, 29: 0.8457142857142858, 30: 0.9444444444444444, 32: 0.8617021276595744, 33: 0.2727272727272727, 34: 0.1875, 36: 0.2631578947368421, 39: 0.1111111111111111}
Micro-average F1 score: 0.558857583944423
Weighted-average F1 score: 0.6014792980214414
F1 score per class: {1: 0.3389830508474576, 2: 0.6666666666666666, 3: 0.9325153374233128, 5: 0.91324200913242, 6: 0.6703296703296703, 8: 0.5142857142857142, 10: 0.6881720430107527, 11: 0.48854961832061067, 12: 0.7065868263473054, 14: 0.11904761904761904, 16: 0.9122807017543859, 17: 0.3157894736842105, 18: 0.40476190476190477, 19: 0.6538461538461539, 20: 0.6746987951807228, 22: 0.6761904761904762, 24: 0.046511627906976744, 26: 0.7734806629834254, 28: 0.29411764705882354, 29: 0.8913043478260869, 30: 0.95, 32: 0.8404255319148937, 33: 0.2857142857142857, 34: 0.38620689655172413, 36: 0.4235294117647059, 39: 0.16666666666666666}
Micro-average F1 score: 0.6474337470647433
Weighted-average F1 score: 0.6468429993630069
F1 score per class: {1: 0.33613445378151263, 2: 0.7058823529411765, 3: 0.9192546583850931, 5: 0.91324200913242, 6: 0.6514285714285715, 8: 0.5039370078740157, 10: 0.6834170854271356, 11: 0.48484848484848486, 12: 0.6744186046511628, 14: 0.11764705882352941, 16: 0.9122807017543859, 17: 0.34285714285714286, 18: 0.41975308641975306, 19: 0.6538461538461539, 20: 0.6588235294117647, 22: 0.6761904761904762, 24: 0.047619047619047616, 26: 0.7802197802197802, 28: 0.25, 29: 0.8715083798882681, 30: 0.95, 32: 0.8342245989304813, 33: 0.2727272727272727, 34: 0.3333333333333333, 36: 0.4418604651162791, 39: 0.10526315789473684}
Micro-average F1 score: 0.6401349072512648
Weighted-average F1 score: 0.640356017216118
cur_acc:  ['0.8111', '0.5190', '0.3630', '0.6485', '0.5077']
his_acc:  ['0.8111', '0.6741', '0.5957', '0.5915', '0.5589']
cur_acc des:  ['0.8311', '0.8406', '0.6809', '0.7701', '0.6015']
his_acc des:  ['0.8311', '0.7892', '0.7237', '0.6823', '0.6474']
cur_acc rrf:  ['0.8311', '0.8523', '0.6791', '0.7659', '0.5857']
his_acc rrf:  ['0.8311', '0.7719', '0.7117', '0.6619', '0.6401']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 88.9829823
CurrentTrain: epoch  0, batch     1 | loss: 70.5638292
CurrentTrain: epoch  0, batch     2 | loss: 79.4733237
CurrentTrain: epoch  0, batch     3 | loss: 11.0457325
CurrentTrain: epoch  1, batch     0 | loss: 66.9821431
CurrentTrain: epoch  1, batch     1 | loss: 62.1481070
CurrentTrain: epoch  1, batch     2 | loss: 62.5265035
CurrentTrain: epoch  1, batch     3 | loss: 27.7799585
CurrentTrain: epoch  2, batch     0 | loss: 92.4890577
CurrentTrain: epoch  2, batch     1 | loss: 57.9280121
CurrentTrain: epoch  2, batch     2 | loss: 56.7269990
CurrentTrain: epoch  2, batch     3 | loss: 6.4933578
CurrentTrain: epoch  3, batch     0 | loss: 57.1379719
CurrentTrain: epoch  3, batch     1 | loss: 68.6170604
CurrentTrain: epoch  3, batch     2 | loss: 69.4878908
CurrentTrain: epoch  3, batch     3 | loss: 3.5816433
CurrentTrain: epoch  4, batch     0 | loss: 68.7277445
CurrentTrain: epoch  4, batch     1 | loss: 53.3199562
CurrentTrain: epoch  4, batch     2 | loss: 65.5090374
CurrentTrain: epoch  4, batch     3 | loss: 27.4740965
CurrentTrain: epoch  5, batch     0 | loss: 82.1914074
CurrentTrain: epoch  5, batch     1 | loss: 52.9393915
CurrentTrain: epoch  5, batch     2 | loss: 65.8072251
CurrentTrain: epoch  5, batch     3 | loss: 11.7174950
CurrentTrain: epoch  6, batch     0 | loss: 49.9694597
CurrentTrain: epoch  6, batch     1 | loss: 54.7868537
CurrentTrain: epoch  6, batch     2 | loss: 114.4357986
CurrentTrain: epoch  6, batch     3 | loss: 5.9925901
CurrentTrain: epoch  7, batch     0 | loss: 61.5153679
CurrentTrain: epoch  7, batch     1 | loss: 67.7643094
CurrentTrain: epoch  7, batch     2 | loss: 61.5917447
CurrentTrain: epoch  7, batch     3 | loss: 27.5429547
CurrentTrain: epoch  8, batch     0 | loss: 52.9550171
CurrentTrain: epoch  8, batch     1 | loss: 63.1117170
CurrentTrain: epoch  8, batch     2 | loss: 52.0628718
CurrentTrain: epoch  8, batch     3 | loss: 11.7419128
CurrentTrain: epoch  9, batch     0 | loss: 65.5092189
CurrentTrain: epoch  9, batch     1 | loss: 50.5229051
CurrentTrain: epoch  9, batch     2 | loss: 52.0380324
CurrentTrain: epoch  9, batch     3 | loss: 9.9047078
MemoryTrain:  epoch  0, batch     0 | loss: 0.4166866
MemoryTrain:  epoch  1, batch     0 | loss: 0.3200264
MemoryTrain:  epoch  2, batch     0 | loss: 0.2463523
MemoryTrain:  epoch  3, batch     0 | loss: 0.2114631
MemoryTrain:  epoch  4, batch     0 | loss: 0.1736989
MemoryTrain:  epoch  5, batch     0 | loss: 0.1462368
MemoryTrain:  epoch  6, batch     0 | loss: 0.1387451
MemoryTrain:  epoch  7, batch     0 | loss: 0.0997973
MemoryTrain:  epoch  8, batch     0 | loss: 0.0992204
MemoryTrain:  epoch  9, batch     0 | loss: 0.0886149

F1 score per class: {1: 0.0, 34: 0.5714285714285714, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 17: 0.0, 19: 0.0, 22: 0.25, 26: 0.0, 27: 0.0, 31: 0.5}
Micro-average F1 score: 0.49523809523809526
Weighted-average F1 score: 0.40002771942110177
F1 score per class: {1: 0.0, 34: 0.0, 3: 0.0, 6: 0.5714285714285714, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 17: 0.0, 19: 0.0, 22: 0.3157894736842105, 26: 1.0, 27: 0.0, 31: 0.8403361344537815}
Micro-average F1 score: 0.7288888888888889
Weighted-average F1 score: 0.6522931600330981
F1 score per class: {1: 0.0, 34: 0.0, 3: 0.0, 6: 0.5714285714285714, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 17: 0.0, 19: 0.0, 22: 0.3157894736842105, 26: 1.0, 27: 0.0, 31: 0.8403361344537815}
Micro-average F1 score: 0.7321428571428571
Weighted-average F1 score: 0.6582230978515808

F1 score per class: {1: 0.32142857142857145, 2: 0.7142857142857143, 3: 0.6774193548387096, 5: 0.9552238805970149, 6: 0.12844036697247707, 7: 0.0547945205479452, 8: 0.13793103448275862, 9: 0.9615384615384616, 10: 0.22764227642276422, 11: 0.4251968503937008, 12: 0.2809917355371901, 14: 0.10526315789473684, 16: 0.8235294117647058, 17: 0.0, 18: 0.27586206896551724, 19: 0.582010582010582, 20: 0.5135135135135135, 22: 0.6506024096385542, 24: 0.0625, 26: 0.7624309392265194, 27: 0.1111111111111111, 28: 0.2702702702702703, 29: 0.8323699421965318, 30: 0.9142857142857143, 31: 0.0, 32: 0.8111111111111111, 33: 0.2857142857142857, 34: 0.13636363636363635, 36: 0.11428571428571428, 39: 0.11764705882352941, 40: 0.42201834862385323}
Micro-average F1 score: 0.5019834114677245
Weighted-average F1 score: 0.5402353267840153
F1 score per class: {1: 0.35051546391752575, 2: 0.7058823529411765, 3: 0.9182389937106918, 5: 0.8928571428571429, 6: 0.36507936507936506, 7: 0.043010752688172046, 8: 0.41739130434782606, 9: 0.9803921568627451, 10: 0.45517241379310347, 11: 0.535031847133758, 12: 0.5906040268456376, 14: 0.06666666666666667, 16: 0.9152542372881356, 17: 0.375, 18: 0.36363636363636365, 19: 0.6820809248554913, 20: 0.7272727272727273, 22: 0.6593406593406593, 24: 0.05555555555555555, 26: 0.7624309392265194, 27: 0.09523809523809523, 28: 0.17543859649122806, 29: 0.8791208791208791, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.8478260869565217, 33: 0.2727272727272727, 34: 0.40601503759398494, 36: 0.4418604651162791, 39: 0.15384615384615385, 40: 0.6493506493506493}
Micro-average F1 score: 0.5927786499215071
Weighted-average F1 score: 0.5787889649551239
F1 score per class: {1: 0.36, 2: 0.75, 3: 0.868421052631579, 5: 0.8968609865470852, 6: 0.3114754098360656, 7: 0.04597701149425287, 8: 0.3238095238095238, 9: 0.9803921568627451, 10: 0.4266666666666667, 11: 0.5625, 12: 0.5306122448979592, 14: 0.0851063829787234, 16: 0.9152542372881356, 17: 0.375, 18: 0.39473684210526316, 19: 0.6820809248554913, 20: 0.7333333333333333, 22: 0.6440677966101694, 24: 0.05555555555555555, 26: 0.7624309392265194, 27: 0.09836065573770492, 28: 0.16666666666666666, 29: 0.8587570621468926, 30: 0.972972972972973, 31: 1.0, 32: 0.8432432432432433, 33: 0.2727272727272727, 34: 0.2807017543859649, 36: 0.45977011494252873, 39: 0.1, 40: 0.6578947368421053}
Micro-average F1 score: 0.580089342693044
Weighted-average F1 score: 0.570689485811863
cur_acc:  ['0.8111', '0.5190', '0.3630', '0.6485', '0.5077', '0.4952']
his_acc:  ['0.8111', '0.6741', '0.5957', '0.5915', '0.5589', '0.5020']
cur_acc des:  ['0.8311', '0.8406', '0.6809', '0.7701', '0.6015', '0.7289']
his_acc des:  ['0.8311', '0.7892', '0.7237', '0.6823', '0.6474', '0.5928']
cur_acc rrf:  ['0.8311', '0.8523', '0.6791', '0.7659', '0.5857', '0.7321']
his_acc rrf:  ['0.8311', '0.7719', '0.7117', '0.6619', '0.6401', '0.5801']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 78.3451543
CurrentTrain: epoch  0, batch     1 | loss: 78.7754056
CurrentTrain: epoch  0, batch     2 | loss: 65.8927834
CurrentTrain: epoch  0, batch     3 | loss: 127.6666214
CurrentTrain: epoch  1, batch     0 | loss: 60.8968333
CurrentTrain: epoch  1, batch     1 | loss: 72.2646348
CurrentTrain: epoch  1, batch     2 | loss: 60.7266412
CurrentTrain: epoch  1, batch     3 | loss: 128.5415501
CurrentTrain: epoch  2, batch     0 | loss: 71.4716387
CurrentTrain: epoch  2, batch     1 | loss: 69.9239629
CurrentTrain: epoch  2, batch     2 | loss: 70.6298797
CurrentTrain: epoch  2, batch     3 | loss: 45.2566367
CurrentTrain: epoch  3, batch     0 | loss: 111.4981024
CurrentTrain: epoch  3, batch     1 | loss: 54.8236189
CurrentTrain: epoch  3, batch     2 | loss: 88.1705204
CurrentTrain: epoch  3, batch     3 | loss: 60.5660788
CurrentTrain: epoch  4, batch     0 | loss: 56.1759834
CurrentTrain: epoch  4, batch     1 | loss: 69.1808619
CurrentTrain: epoch  4, batch     2 | loss: 64.0135006
CurrentTrain: epoch  4, batch     3 | loss: 83.0228901
CurrentTrain: epoch  5, batch     0 | loss: 85.5389894
CurrentTrain: epoch  5, batch     1 | loss: 65.1524440
CurrentTrain: epoch  5, batch     2 | loss: 85.5121765
CurrentTrain: epoch  5, batch     3 | loss: 42.9964019
CurrentTrain: epoch  6, batch     0 | loss: 86.3788177
CurrentTrain: epoch  6, batch     1 | loss: 50.9389122
CurrentTrain: epoch  6, batch     2 | loss: 82.1035135
CurrentTrain: epoch  6, batch     3 | loss: 44.8771890
CurrentTrain: epoch  7, batch     0 | loss: 62.6316620
CurrentTrain: epoch  7, batch     1 | loss: 85.4188316
CurrentTrain: epoch  7, batch     2 | loss: 54.6122887
CurrentTrain: epoch  7, batch     3 | loss: 45.3318956
CurrentTrain: epoch  8, batch     0 | loss: 66.2614986
CurrentTrain: epoch  8, batch     1 | loss: 51.6484695
CurrentTrain: epoch  8, batch     2 | loss: 64.4067108
CurrentTrain: epoch  8, batch     3 | loss: 78.6056878
CurrentTrain: epoch  9, batch     0 | loss: 62.5550061
CurrentTrain: epoch  9, batch     1 | loss: 66.2276726
CurrentTrain: epoch  9, batch     2 | loss: 83.5114619
CurrentTrain: epoch  9, batch     3 | loss: 55.2926472
MemoryTrain:  epoch  0, batch     0 | loss: 0.4212886
MemoryTrain:  epoch  1, batch     0 | loss: 0.3876990
MemoryTrain:  epoch  2, batch     0 | loss: 0.3033219
MemoryTrain:  epoch  3, batch     0 | loss: 0.2184731
MemoryTrain:  epoch  4, batch     0 | loss: 0.1835119
MemoryTrain:  epoch  5, batch     0 | loss: 0.1518788
MemoryTrain:  epoch  6, batch     0 | loss: 0.1188501
MemoryTrain:  epoch  7, batch     0 | loss: 0.1046310
MemoryTrain:  epoch  8, batch     0 | loss: 0.0821233
MemoryTrain:  epoch  9, batch     0 | loss: 0.0737644

F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 5: 0.0, 37: 0.8888888888888888, 38: 0.0, 8: 0.5142857142857142, 40: 0.0, 11: 0.0, 10: 0.0, 14: 0.813953488372093, 15: 0.0, 18: 0.23529411764705882, 25: 0.8571428571428571, 28: 0.0}
Micro-average F1 score: 0.5232558139534884
Weighted-average F1 score: 0.4527201328842915
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 15: 0.75, 18: 0.0, 22: 0.0, 25: 0.6666666666666666, 26: 0.0, 28: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.9148936170212766, 36: 0.0, 37: 0.5, 38: 0.8163265306122449}
Micro-average F1 score: 0.5867346938775511
Weighted-average F1 score: 0.46239065972830035
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 15: 0.75, 18: 0.0, 22: 0.0, 25: 0.6666666666666666, 26: 0.0, 28: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.9263157894736842, 36: 0.0, 37: 0.5, 38: 0.84}
Micro-average F1 score: 0.5969387755102041
Weighted-average F1 score: 0.47681271240929546

F1 score per class: {1: 0.3106796116504854, 2: 0.8, 3: 0.4485981308411215, 5: 0.8495575221238938, 6: 0.18181818181818182, 7: 0.05263157894736842, 8: 0.32323232323232326, 9: 0.9803921568627451, 10: 0.21052631578947367, 11: 0.43537414965986393, 12: 0.2222222222222222, 14: 0.02564102564102564, 15: 0.8888888888888888, 16: 0.8727272727272727, 17: 0.0, 18: 0.3582089552238806, 19: 0.59375, 20: 0.2857142857142857, 22: 0.6588235294117647, 24: 0.07142857142857142, 25: 0.5142857142857142, 26: 0.7666666666666667, 27: 0.10909090909090909, 28: 0.3125, 29: 0.8587570621468926, 30: 0.9444444444444444, 31: 0.6666666666666666, 32: 0.8235294117647058, 33: 0.2857142857142857, 34: 0.0625, 35: 0.5185185185185185, 36: 0.1643835616438356, 37: 0.20512820512820512, 38: 0.3387096774193548, 39: 0.13333333333333333, 40: 0.4}
Micro-average F1 score: 0.484529702970297
Weighted-average F1 score: 0.5189716020934133
F1 score per class: {1: 0.3333333333333333, 2: 0.8235294117647058, 3: 0.8129032258064516, 5: 0.8264462809917356, 6: 0.4507042253521127, 7: 0.056338028169014086, 8: 0.589041095890411, 9: 0.9615384615384616, 10: 0.5, 11: 0.5798816568047337, 12: 0.5921052631578947, 14: 0.04597701149425287, 15: 0.6666666666666666, 16: 0.9152542372881356, 17: 0.16666666666666666, 18: 0.35135135135135137, 19: 0.6666666666666666, 20: 0.5526315789473685, 22: 0.7120418848167539, 24: 0.05714285714285714, 25: 0.6666666666666666, 26: 0.7567567567567568, 27: 0.11940298507462686, 28: 0.2553191489361702, 29: 0.8913043478260869, 30: 0.972972972972973, 31: 0.8, 32: 0.8315789473684211, 33: 0.3, 34: 0.14814814814814814, 35: 0.7543859649122807, 36: 0.34146341463414637, 37: 0.28776978417266186, 38: 0.425531914893617, 39: 0.0, 40: 0.6351351351351351}
Micro-average F1 score: 0.5805921052631579
Weighted-average F1 score: 0.5709950081268611
F1 score per class: {1: 0.3225806451612903, 2: 0.8235294117647058, 3: 0.7808219178082192, 5: 0.8333333333333334, 6: 0.37593984962406013, 7: 0.05194805194805195, 8: 0.46774193548387094, 9: 0.9803921568627451, 10: 0.4714285714285714, 11: 0.5568181818181818, 12: 0.5562913907284768, 14: 0.045454545454545456, 15: 0.7058823529411765, 16: 0.9310344827586207, 17: 0.0, 18: 0.37333333333333335, 19: 0.6555555555555556, 20: 0.5526315789473685, 22: 0.6984126984126984, 24: 0.0625, 25: 0.6666666666666666, 26: 0.7582417582417582, 27: 0.11764705882352941, 28: 0.2127659574468085, 29: 0.8901098901098901, 30: 0.972972972972973, 31: 1.0, 32: 0.8333333333333334, 33: 0.2857142857142857, 34: 0.13740458015267176, 35: 0.7333333333333333, 36: 0.38095238095238093, 37: 0.3007518796992481, 38: 0.38181818181818183, 39: 0.0, 40: 0.625}
Micro-average F1 score: 0.5648812810601878
Weighted-average F1 score: 0.5552403833641638
cur_acc:  ['0.8111', '0.5190', '0.3630', '0.6485', '0.5077', '0.4952', '0.5233']
his_acc:  ['0.8111', '0.6741', '0.5957', '0.5915', '0.5589', '0.5020', '0.4845']
cur_acc des:  ['0.8311', '0.8406', '0.6809', '0.7701', '0.6015', '0.7289', '0.5867']
his_acc des:  ['0.8311', '0.7892', '0.7237', '0.6823', '0.6474', '0.5928', '0.5806']
cur_acc rrf:  ['0.8311', '0.8523', '0.6791', '0.7659', '0.5857', '0.7321', '0.5969']
his_acc rrf:  ['0.8311', '0.7719', '0.7117', '0.6619', '0.6401', '0.5801', '0.5649']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 86.6540839
CurrentTrain: epoch  0, batch     1 | loss: 79.8596306
CurrentTrain: epoch  0, batch     2 | loss: 126.0356775
CurrentTrain: epoch  0, batch     3 | loss: 58.6187845
CurrentTrain: epoch  1, batch     0 | loss: 78.8912747
CurrentTrain: epoch  1, batch     1 | loss: 64.4184395
CurrentTrain: epoch  1, batch     2 | loss: 58.3850374
CurrentTrain: epoch  1, batch     3 | loss: 70.0619866
CurrentTrain: epoch  2, batch     0 | loss: 87.9949924
CurrentTrain: epoch  2, batch     1 | loss: 76.9635014
CurrentTrain: epoch  2, batch     2 | loss: 53.7359436
CurrentTrain: epoch  2, batch     3 | loss: 70.6983379
CurrentTrain: epoch  3, batch     0 | loss: 70.2797711
CurrentTrain: epoch  3, batch     1 | loss: 56.4956334
CurrentTrain: epoch  3, batch     2 | loss: 85.0749315
CurrentTrain: epoch  3, batch     3 | loss: 46.5775708
CurrentTrain: epoch  4, batch     0 | loss: 113.7067077
CurrentTrain: epoch  4, batch     1 | loss: 59.4984809
CurrentTrain: epoch  4, batch     2 | loss: 84.4074283
CurrentTrain: epoch  4, batch     3 | loss: 53.2581944
CurrentTrain: epoch  5, batch     0 | loss: 122.3662588
CurrentTrain: epoch  5, batch     1 | loss: 52.5993314
CurrentTrain: epoch  5, batch     2 | loss: 81.6839635
CurrentTrain: epoch  5, batch     3 | loss: 51.6737949
CurrentTrain: epoch  6, batch     0 | loss: 118.7173442
CurrentTrain: epoch  6, batch     1 | loss: 83.7163048
CurrentTrain: epoch  6, batch     2 | loss: 51.0297477
CurrentTrain: epoch  6, batch     3 | loss: 50.8759972
CurrentTrain: epoch  7, batch     0 | loss: 64.9002460
CurrentTrain: epoch  7, batch     1 | loss: 83.2091292
CurrentTrain: epoch  7, batch     2 | loss: 65.4574917
CurrentTrain: epoch  7, batch     3 | loss: 52.2766290
CurrentTrain: epoch  8, batch     0 | loss: 87.6920489
CurrentTrain: epoch  8, batch     1 | loss: 65.2324504
CurrentTrain: epoch  8, batch     2 | loss: 48.9458644
CurrentTrain: epoch  8, batch     3 | loss: 53.2614316
CurrentTrain: epoch  9, batch     0 | loss: 62.1960809
CurrentTrain: epoch  9, batch     1 | loss: 63.6938646
CurrentTrain: epoch  9, batch     2 | loss: 112.5286210
CurrentTrain: epoch  9, batch     3 | loss: 93.6621627
MemoryTrain:  epoch  0, batch     0 | loss: 0.2489076
MemoryTrain:  epoch  1, batch     0 | loss: 0.2343048
MemoryTrain:  epoch  2, batch     0 | loss: 0.1894435
MemoryTrain:  epoch  3, batch     0 | loss: 0.1506803
MemoryTrain:  epoch  4, batch     0 | loss: 0.1132023
MemoryTrain:  epoch  5, batch     0 | loss: 0.0889322
MemoryTrain:  epoch  6, batch     0 | loss: 0.0778356
MemoryTrain:  epoch  7, batch     0 | loss: 0.0714663
MemoryTrain:  epoch  8, batch     0 | loss: 0.0679237
MemoryTrain:  epoch  9, batch     0 | loss: 0.0629556

F1 score per class: {0: 0.9428571428571428, 32: 0.0, 2: 0.9743589743589743, 4: 0.5714285714285714, 38: 0.0, 13: 0.0, 14: 0.12121212121212122, 15: 0.0, 21: 0.6944444444444444, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.7753086419753087
Weighted-average F1 score: 0.7698636798636799
F1 score per class: {0: 0.9577464788732394, 1: 0.0, 2: 0.0, 4: 0.9637305699481865, 5: 0.0, 11: 0.0, 13: 0.75, 15: 0.0, 21: 0.6521739130434783, 23: 0.7631578947368421, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.8130841121495327
Weighted-average F1 score: 0.7517043794706348
F1 score per class: {0: 0.9577464788732394, 1: 0.0, 2: 0.0, 4: 0.9637305699481865, 5: 0.0, 11: 0.0, 13: 0.75, 15: 0.0, 21: 0.5581395348837209, 22: 0.0, 23: 0.7631578947368421, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.8009367681498829
Weighted-average F1 score: 0.7404327374533554

F1 score per class: {0: 0.9295774647887324, 1: 0.32989690721649484, 2: 0.7368421052631579, 3: 0.5565217391304348, 4: 0.9743589743589743, 5: 0.8695652173913043, 6: 0.18181818181818182, 7: 0.029850746268656716, 8: 0.2708333333333333, 9: 0.9803921568627451, 10: 0.16363636363636364, 11: 0.47297297297297297, 12: 0.2833333333333333, 13: 0.05555555555555555, 14: 0.05405405405405406, 15: 0.7058823529411765, 16: 0.8571428571428571, 17: 0.0, 18: 0.37333333333333335, 19: 0.6213592233009708, 20: 0.45714285714285713, 21: 0.09302325581395349, 22: 0.6626506024096386, 23: 0.6578947368421053, 24: 0.07692307692307693, 25: 0.5555555555555556, 26: 0.7046632124352331, 27: 0.09230769230769231, 28: 0.35294117647058826, 29: 0.8666666666666667, 30: 0.972972972972973, 31: 0.8, 32: 0.7868852459016393, 33: 0.2857142857142857, 34: 0.13592233009708737, 35: 0.423728813559322, 36: 0.21333333333333335, 37: 0.18666666666666668, 38: 0.391304347826087, 39: 0.13333333333333333, 40: 0.375}
Micro-average F1 score: 0.5201525054466231
Weighted-average F1 score: 0.5478774270010577
F1 score per class: {0: 0.9444444444444444, 1: 0.2682926829268293, 2: 0.6363636363636364, 3: 0.8727272727272727, 4: 0.9637305699481865, 5: 0.851063829787234, 6: 0.3968253968253968, 7: 0.04395604395604396, 8: 0.6330935251798561, 9: 0.9803921568627451, 10: 0.352, 11: 0.54421768707483, 12: 0.5695364238410596, 13: 0.09090909090909091, 14: 0.0759493670886076, 15: 0.6666666666666666, 16: 0.9, 17: 0.0, 18: 0.4, 19: 0.6798029556650246, 20: 0.735632183908046, 21: 0.2631578947368421, 22: 0.5637583892617449, 23: 0.7341772151898734, 24: 0.08, 25: 0.7058823529411765, 26: 0.700507614213198, 27: 0.16, 28: 0.3125, 29: 0.9148936170212766, 30: 0.9743589743589743, 31: 0.4, 32: 0.8235294117647058, 33: 0.35294117647058826, 34: 0.3333333333333333, 35: 0.7927927927927928, 36: 0.6138613861386139, 37: 0.32727272727272727, 38: 0.475, 39: 0.13333333333333333, 40: 0.5785123966942148}
Micro-average F1 score: 0.6022504892367906
Weighted-average F1 score: 0.5880728678962983
F1 score per class: {0: 0.9444444444444444, 1: 0.30952380952380953, 2: 0.6086956521739131, 3: 0.8513513513513513, 4: 0.9637305699481865, 5: 0.851063829787234, 6: 0.35772357723577236, 7: 0.044444444444444446, 8: 0.5203252032520326, 9: 0.9803921568627451, 10: 0.3464566929133858, 11: 0.5590062111801242, 12: 0.5714285714285714, 13: 0.08333333333333333, 14: 0.075, 15: 0.6666666666666666, 16: 0.9, 17: 0.0, 18: 0.3695652173913043, 19: 0.6796116504854369, 20: 0.7058823529411765, 21: 0.25806451612903225, 22: 0.6086956521739131, 23: 0.7341772151898734, 24: 0.07407407407407407, 25: 0.7058823529411765, 26: 0.7040816326530612, 27: 0.15584415584415584, 28: 0.4, 29: 0.8972972972972973, 30: 0.9743589743589743, 31: 0.4444444444444444, 32: 0.8333333333333334, 33: 0.3157894736842105, 34: 0.26356589147286824, 35: 0.7433628318584071, 36: 0.5319148936170213, 37: 0.3090909090909091, 38: 0.43956043956043955, 39: 0.13333333333333333, 40: 0.5714285714285714}
Micro-average F1 score: 0.5907193714706604
Weighted-average F1 score: 0.5768790750882028
cur_acc:  ['0.8111', '0.5190', '0.3630', '0.6485', '0.5077', '0.4952', '0.5233', '0.7753']
his_acc:  ['0.8111', '0.6741', '0.5957', '0.5915', '0.5589', '0.5020', '0.4845', '0.5202']
cur_acc des:  ['0.8311', '0.8406', '0.6809', '0.7701', '0.6015', '0.7289', '0.5867', '0.8131']
his_acc des:  ['0.8311', '0.7892', '0.7237', '0.6823', '0.6474', '0.5928', '0.5806', '0.6023']
cur_acc rrf:  ['0.8311', '0.8523', '0.6791', '0.7659', '0.5857', '0.7321', '0.5969', '0.8009']
his_acc rrf:  ['0.8311', '0.7719', '0.7117', '0.6619', '0.6401', '0.5801', '0.5649', '0.5907']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 82.7810369
CurrentTrain: epoch  0, batch     1 | loss: 64.7824421
CurrentTrain: epoch  0, batch     2 | loss: 98.3692192
CurrentTrain: epoch  0, batch     3 | loss: 97.7604496
CurrentTrain: epoch  0, batch     4 | loss: 128.3510619
CurrentTrain: epoch  0, batch     5 | loss: 96.6114241
CurrentTrain: epoch  0, batch     6 | loss: 96.0973677
CurrentTrain: epoch  0, batch     7 | loss: 96.5526078
CurrentTrain: epoch  0, batch     8 | loss: 96.5360078
CurrentTrain: epoch  0, batch     9 | loss: 77.8777789
CurrentTrain: epoch  0, batch    10 | loss: 125.3588401
CurrentTrain: epoch  0, batch    11 | loss: 65.9938008
CurrentTrain: epoch  0, batch    12 | loss: 64.9073359
CurrentTrain: epoch  0, batch    13 | loss: 56.7604953
CurrentTrain: epoch  0, batch    14 | loss: 78.0944083
CurrentTrain: epoch  0, batch    15 | loss: 77.5625328
CurrentTrain: epoch  0, batch    16 | loss: 77.5650098
CurrentTrain: epoch  0, batch    17 | loss: 126.2216764
CurrentTrain: epoch  0, batch    18 | loss: 187.1813181
CurrentTrain: epoch  0, batch    19 | loss: 76.9406072
CurrentTrain: epoch  0, batch    20 | loss: 126.2602688
CurrentTrain: epoch  0, batch    21 | loss: 65.8038273
CurrentTrain: epoch  0, batch    22 | loss: 76.8768733
CurrentTrain: epoch  0, batch    23 | loss: 55.8183335
CurrentTrain: epoch  0, batch    24 | loss: 95.3413890
CurrentTrain: epoch  0, batch    25 | loss: 76.8970132
CurrentTrain: epoch  0, batch    26 | loss: 65.1512652
CurrentTrain: epoch  0, batch    27 | loss: 125.9716467
CurrentTrain: epoch  0, batch    28 | loss: 76.5618914
CurrentTrain: epoch  0, batch    29 | loss: 75.9872303
CurrentTrain: epoch  0, batch    30 | loss: 76.9583600
CurrentTrain: epoch  0, batch    31 | loss: 55.6746322
CurrentTrain: epoch  0, batch    32 | loss: 76.6420661
CurrentTrain: epoch  0, batch    33 | loss: 125.0896083
CurrentTrain: epoch  0, batch    34 | loss: 125.2695875
CurrentTrain: epoch  0, batch    35 | loss: 55.4450636
CurrentTrain: epoch  0, batch    36 | loss: 63.8187583
CurrentTrain: epoch  0, batch    37 | loss: 76.5052456
CurrentTrain: epoch  0, batch    38 | loss: 75.6604860
CurrentTrain: epoch  0, batch    39 | loss: 93.1171363
CurrentTrain: epoch  0, batch    40 | loss: 56.1478628
CurrentTrain: epoch  0, batch    41 | loss: 95.9227216
CurrentTrain: epoch  0, batch    42 | loss: 74.3696253
CurrentTrain: epoch  0, batch    43 | loss: 63.8752928
CurrentTrain: epoch  0, batch    44 | loss: 63.4154240
CurrentTrain: epoch  0, batch    45 | loss: 63.8778841
CurrentTrain: epoch  0, batch    46 | loss: 75.7374105
CurrentTrain: epoch  0, batch    47 | loss: 126.5963483
CurrentTrain: epoch  0, batch    48 | loss: 62.9943345
CurrentTrain: epoch  0, batch    49 | loss: 76.2948035
CurrentTrain: epoch  0, batch    50 | loss: 61.3013222
CurrentTrain: epoch  0, batch    51 | loss: 61.2969898
CurrentTrain: epoch  0, batch    52 | loss: 94.4163573
CurrentTrain: epoch  0, batch    53 | loss: 94.4922702
CurrentTrain: epoch  0, batch    54 | loss: 75.5081491
CurrentTrain: epoch  0, batch    55 | loss: 75.6714488
CurrentTrain: epoch  0, batch    56 | loss: 124.9380598
CurrentTrain: epoch  0, batch    57 | loss: 74.8844547
CurrentTrain: epoch  0, batch    58 | loss: 93.8585846
CurrentTrain: epoch  0, batch    59 | loss: 75.3137975
CurrentTrain: epoch  0, batch    60 | loss: 75.9187852
CurrentTrain: epoch  0, batch    61 | loss: 93.5925329
CurrentTrain: epoch  0, batch    62 | loss: 73.2638390
CurrentTrain: epoch  0, batch    63 | loss: 75.8555461
CurrentTrain: epoch  0, batch    64 | loss: 53.9646214
CurrentTrain: epoch  0, batch    65 | loss: 92.6610895
CurrentTrain: epoch  0, batch    66 | loss: 73.3379038
CurrentTrain: epoch  0, batch    67 | loss: 74.8924543
CurrentTrain: epoch  0, batch    68 | loss: 63.2987882
CurrentTrain: epoch  0, batch    69 | loss: 72.0803351
CurrentTrain: epoch  0, batch    70 | loss: 62.0440749
CurrentTrain: epoch  0, batch    71 | loss: 90.0968403
CurrentTrain: epoch  0, batch    72 | loss: 71.1315656
CurrentTrain: epoch  0, batch    73 | loss: 71.3873276
CurrentTrain: epoch  0, batch    74 | loss: 61.3864785
CurrentTrain: epoch  0, batch    75 | loss: 90.2948143
CurrentTrain: epoch  0, batch    76 | loss: 72.7146734
CurrentTrain: epoch  0, batch    77 | loss: 90.8042478
CurrentTrain: epoch  0, batch    78 | loss: 93.0486749
CurrentTrain: epoch  0, batch    79 | loss: 184.0288807
CurrentTrain: epoch  0, batch    80 | loss: 60.8365495
CurrentTrain: epoch  0, batch    81 | loss: 59.6826351
CurrentTrain: epoch  0, batch    82 | loss: 60.8944926
CurrentTrain: epoch  0, batch    83 | loss: 74.7164617
CurrentTrain: epoch  0, batch    84 | loss: 61.8862205
CurrentTrain: epoch  0, batch    85 | loss: 69.7366051
CurrentTrain: epoch  0, batch    86 | loss: 91.0137178
CurrentTrain: epoch  0, batch    87 | loss: 90.5355835
CurrentTrain: epoch  0, batch    88 | loss: 72.2406296
CurrentTrain: epoch  0, batch    89 | loss: 69.3543932
CurrentTrain: epoch  0, batch    90 | loss: 66.7287511
CurrentTrain: epoch  0, batch    91 | loss: 71.3413904
CurrentTrain: epoch  0, batch    92 | loss: 71.2725046
CurrentTrain: epoch  0, batch    93 | loss: 94.8117761
CurrentTrain: epoch  0, batch    94 | loss: 60.4208507
CurrentTrain: epoch  0, batch    95 | loss: 78.0545252
CurrentTrain: epoch  1, batch     0 | loss: 120.4654018
CurrentTrain: epoch  1, batch     1 | loss: 59.4766000
CurrentTrain: epoch  1, batch     2 | loss: 70.8208497
CurrentTrain: epoch  1, batch     3 | loss: 183.5174407
CurrentTrain: epoch  1, batch     4 | loss: 67.1295123
CurrentTrain: epoch  1, batch     5 | loss: 90.3928068
CurrentTrain: epoch  1, batch     6 | loss: 89.9589994
CurrentTrain: epoch  1, batch     7 | loss: 72.8859842
CurrentTrain: epoch  1, batch     8 | loss: 58.7710503
CurrentTrain: epoch  1, batch     9 | loss: 70.2226804
CurrentTrain: epoch  1, batch    10 | loss: 70.5580458
CurrentTrain: epoch  1, batch    11 | loss: 73.9293888
CurrentTrain: epoch  1, batch    12 | loss: 88.1284612
CurrentTrain: epoch  1, batch    13 | loss: 66.7569769
CurrentTrain: epoch  1, batch    14 | loss: 59.2391126
CurrentTrain: epoch  1, batch    15 | loss: 59.5104269
CurrentTrain: epoch  1, batch    16 | loss: 91.0353955
CurrentTrain: epoch  1, batch    17 | loss: 69.2213932
CurrentTrain: epoch  1, batch    18 | loss: 88.8435560
CurrentTrain: epoch  1, batch    19 | loss: 57.4885005
CurrentTrain: epoch  1, batch    20 | loss: 70.7484062
CurrentTrain: epoch  1, batch    21 | loss: 67.0338677
CurrentTrain: epoch  1, batch    22 | loss: 67.7991656
CurrentTrain: epoch  1, batch    23 | loss: 118.8992835
CurrentTrain: epoch  1, batch    24 | loss: 73.4356482
CurrentTrain: epoch  1, batch    25 | loss: 59.9469329
CurrentTrain: epoch  1, batch    26 | loss: 72.2144112
CurrentTrain: epoch  1, batch    27 | loss: 58.4527648
CurrentTrain: epoch  1, batch    28 | loss: 119.6003104
CurrentTrain: epoch  1, batch    29 | loss: 88.8185474
CurrentTrain: epoch  1, batch    30 | loss: 69.2103635
CurrentTrain: epoch  1, batch    31 | loss: 69.6198264
CurrentTrain: epoch  1, batch    32 | loss: 89.0218135
CurrentTrain: epoch  1, batch    33 | loss: 68.1870830
CurrentTrain: epoch  1, batch    34 | loss: 87.1111304
CurrentTrain: epoch  1, batch    35 | loss: 122.2426514
CurrentTrain: epoch  1, batch    36 | loss: 87.9731815
CurrentTrain: epoch  1, batch    37 | loss: 56.5497654
CurrentTrain: epoch  1, batch    38 | loss: 123.7158142
CurrentTrain: epoch  1, batch    39 | loss: 89.6740790
CurrentTrain: epoch  1, batch    40 | loss: 118.0326605
CurrentTrain: epoch  1, batch    41 | loss: 119.2113974
CurrentTrain: epoch  1, batch    42 | loss: 48.8854933
CurrentTrain: epoch  1, batch    43 | loss: 58.0195034
CurrentTrain: epoch  1, batch    44 | loss: 84.6964359
CurrentTrain: epoch  1, batch    45 | loss: 85.4962932
CurrentTrain: epoch  1, batch    46 | loss: 53.4089745
CurrentTrain: epoch  1, batch    47 | loss: 58.2845904
CurrentTrain: epoch  1, batch    48 | loss: 72.5968312
CurrentTrain: epoch  1, batch    49 | loss: 57.0714958
CurrentTrain: epoch  1, batch    50 | loss: 47.0545376
CurrentTrain: epoch  1, batch    51 | loss: 57.0657173
CurrentTrain: epoch  1, batch    52 | loss: 90.7632944
CurrentTrain: epoch  1, batch    53 | loss: 72.7108677
CurrentTrain: epoch  1, batch    54 | loss: 49.9378841
CurrentTrain: epoch  1, batch    55 | loss: 70.8434165
CurrentTrain: epoch  1, batch    56 | loss: 58.9994613
CurrentTrain: epoch  1, batch    57 | loss: 120.5022949
CurrentTrain: epoch  1, batch    58 | loss: 119.9211701
CurrentTrain: epoch  1, batch    59 | loss: 56.9021200
CurrentTrain: epoch  1, batch    60 | loss: 47.9324429
CurrentTrain: epoch  1, batch    61 | loss: 70.4240986
CurrentTrain: epoch  1, batch    62 | loss: 57.9041599
CurrentTrain: epoch  1, batch    63 | loss: 56.9764335
CurrentTrain: epoch  1, batch    64 | loss: 66.4846225
CurrentTrain: epoch  1, batch    65 | loss: 120.2913205
CurrentTrain: epoch  1, batch    66 | loss: 69.7205253
CurrentTrain: epoch  1, batch    67 | loss: 117.5507970
CurrentTrain: epoch  1, batch    68 | loss: 87.4304504
CurrentTrain: epoch  1, batch    69 | loss: 88.8733954
CurrentTrain: epoch  1, batch    70 | loss: 86.8456193
CurrentTrain: epoch  1, batch    71 | loss: 71.1252245
CurrentTrain: epoch  1, batch    72 | loss: 68.8355890
CurrentTrain: epoch  1, batch    73 | loss: 56.6171435
CurrentTrain: epoch  1, batch    74 | loss: 120.0572128
CurrentTrain: epoch  1, batch    75 | loss: 71.8945318
CurrentTrain: epoch  1, batch    76 | loss: 58.9410363
CurrentTrain: epoch  1, batch    77 | loss: 59.5475017
CurrentTrain: epoch  1, batch    78 | loss: 87.3965469
CurrentTrain: epoch  1, batch    79 | loss: 85.5252986
CurrentTrain: epoch  1, batch    80 | loss: 86.5792984
CurrentTrain: epoch  1, batch    81 | loss: 59.8133559
CurrentTrain: epoch  1, batch    82 | loss: 56.4488895
CurrentTrain: epoch  1, batch    83 | loss: 59.4414032
CurrentTrain: epoch  1, batch    84 | loss: 67.4463841
CurrentTrain: epoch  1, batch    85 | loss: 86.3178762
CurrentTrain: epoch  1, batch    86 | loss: 89.2826916
CurrentTrain: epoch  1, batch    87 | loss: 92.0515815
CurrentTrain: epoch  1, batch    88 | loss: 57.7111529
CurrentTrain: epoch  1, batch    89 | loss: 59.5680717
CurrentTrain: epoch  1, batch    90 | loss: 68.8399084
CurrentTrain: epoch  1, batch    91 | loss: 58.1801659
CurrentTrain: epoch  1, batch    92 | loss: 55.9069970
CurrentTrain: epoch  1, batch    93 | loss: 69.4755459
CurrentTrain: epoch  1, batch    94 | loss: 70.8737536
CurrentTrain: epoch  1, batch    95 | loss: 55.8960549
CurrentTrain: epoch  2, batch     0 | loss: 66.1175448
CurrentTrain: epoch  2, batch     1 | loss: 59.1800141
CurrentTrain: epoch  2, batch     2 | loss: 65.3579805
CurrentTrain: epoch  2, batch     3 | loss: 114.8712264
CurrentTrain: epoch  2, batch     4 | loss: 87.2026874
CurrentTrain: epoch  2, batch     5 | loss: 55.7751938
CurrentTrain: epoch  2, batch     6 | loss: 88.2547657
CurrentTrain: epoch  2, batch     7 | loss: 86.9902882
CurrentTrain: epoch  2, batch     8 | loss: 64.0932416
CurrentTrain: epoch  2, batch     9 | loss: 85.1790557
CurrentTrain: epoch  2, batch    10 | loss: 87.0013324
CurrentTrain: epoch  2, batch    11 | loss: 57.5876482
CurrentTrain: epoch  2, batch    12 | loss: 56.6322893
CurrentTrain: epoch  2, batch    13 | loss: 69.9469034
CurrentTrain: epoch  2, batch    14 | loss: 67.5878350
CurrentTrain: epoch  2, batch    15 | loss: 86.6599950
CurrentTrain: epoch  2, batch    16 | loss: 85.1877362
CurrentTrain: epoch  2, batch    17 | loss: 66.6435117
CurrentTrain: epoch  2, batch    18 | loss: 71.2707181
CurrentTrain: epoch  2, batch    19 | loss: 66.2575956
CurrentTrain: epoch  2, batch    20 | loss: 84.0007454
CurrentTrain: epoch  2, batch    21 | loss: 85.8597072
CurrentTrain: epoch  2, batch    22 | loss: 69.4665195
CurrentTrain: epoch  2, batch    23 | loss: 121.3793539
CurrentTrain: epoch  2, batch    24 | loss: 72.1438595
CurrentTrain: epoch  2, batch    25 | loss: 119.8047551
CurrentTrain: epoch  2, batch    26 | loss: 85.8924230
CurrentTrain: epoch  2, batch    27 | loss: 86.1425330
CurrentTrain: epoch  2, batch    28 | loss: 118.5069513
CurrentTrain: epoch  2, batch    29 | loss: 56.4146138
CurrentTrain: epoch  2, batch    30 | loss: 117.2215639
CurrentTrain: epoch  2, batch    31 | loss: 55.2774624
CurrentTrain: epoch  2, batch    32 | loss: 86.9084313
CurrentTrain: epoch  2, batch    33 | loss: 46.7903163
CurrentTrain: epoch  2, batch    34 | loss: 66.7920544
CurrentTrain: epoch  2, batch    35 | loss: 57.7648115
CurrentTrain: epoch  2, batch    36 | loss: 43.3033554
CurrentTrain: epoch  2, batch    37 | loss: 89.2289042
CurrentTrain: epoch  2, batch    38 | loss: 66.4426841
CurrentTrain: epoch  2, batch    39 | loss: 54.1012995
CurrentTrain: epoch  2, batch    40 | loss: 86.3696695
CurrentTrain: epoch  2, batch    41 | loss: 58.0777647
CurrentTrain: epoch  2, batch    42 | loss: 119.9354197
CurrentTrain: epoch  2, batch    43 | loss: 84.7215860
CurrentTrain: epoch  2, batch    44 | loss: 71.6886998
CurrentTrain: epoch  2, batch    45 | loss: 85.6193969
CurrentTrain: epoch  2, batch    46 | loss: 117.1406450
CurrentTrain: epoch  2, batch    47 | loss: 115.0942576
CurrentTrain: epoch  2, batch    48 | loss: 67.8638360
CurrentTrain: epoch  2, batch    49 | loss: 65.6052802
CurrentTrain: epoch  2, batch    50 | loss: 56.7825047
CurrentTrain: epoch  2, batch    51 | loss: 114.3265712
CurrentTrain: epoch  2, batch    52 | loss: 120.6531167
CurrentTrain: epoch  2, batch    53 | loss: 52.2996891
CurrentTrain: epoch  2, batch    54 | loss: 112.6005844
CurrentTrain: epoch  2, batch    55 | loss: 52.9266190
CurrentTrain: epoch  2, batch    56 | loss: 46.9648472
CurrentTrain: epoch  2, batch    57 | loss: 68.6324781
CurrentTrain: epoch  2, batch    58 | loss: 64.6517904
CurrentTrain: epoch  2, batch    59 | loss: 54.9859373
CurrentTrain: epoch  2, batch    60 | loss: 69.3301111
CurrentTrain: epoch  2, batch    61 | loss: 86.5160188
CurrentTrain: epoch  2, batch    62 | loss: 70.6922700
CurrentTrain: epoch  2, batch    63 | loss: 69.7168942
CurrentTrain: epoch  2, batch    64 | loss: 83.0790339
CurrentTrain: epoch  2, batch    65 | loss: 67.7334172
CurrentTrain: epoch  2, batch    66 | loss: 87.0781532
CurrentTrain: epoch  2, batch    67 | loss: 86.3853860
CurrentTrain: epoch  2, batch    68 | loss: 89.9477927
CurrentTrain: epoch  2, batch    69 | loss: 57.0155286
CurrentTrain: epoch  2, batch    70 | loss: 113.5450569
CurrentTrain: epoch  2, batch    71 | loss: 66.4479806
CurrentTrain: epoch  2, batch    72 | loss: 69.2292858
CurrentTrain: epoch  2, batch    73 | loss: 52.5233199
CurrentTrain: epoch  2, batch    74 | loss: 67.7312397
CurrentTrain: epoch  2, batch    75 | loss: 53.2429772
CurrentTrain: epoch  2, batch    76 | loss: 47.2435951
CurrentTrain: epoch  2, batch    77 | loss: 69.5345012
CurrentTrain: epoch  2, batch    78 | loss: 56.8671535
CurrentTrain: epoch  2, batch    79 | loss: 70.9928196
CurrentTrain: epoch  2, batch    80 | loss: 118.3389193
CurrentTrain: epoch  2, batch    81 | loss: 52.9522914
CurrentTrain: epoch  2, batch    82 | loss: 87.5928861
CurrentTrain: epoch  2, batch    83 | loss: 58.0086691
CurrentTrain: epoch  2, batch    84 | loss: 67.7249713
CurrentTrain: epoch  2, batch    85 | loss: 54.4530657
CurrentTrain: epoch  2, batch    86 | loss: 57.2612561
CurrentTrain: epoch  2, batch    87 | loss: 60.2134180
CurrentTrain: epoch  2, batch    88 | loss: 91.0234039
CurrentTrain: epoch  2, batch    89 | loss: 53.4906645
CurrentTrain: epoch  2, batch    90 | loss: 53.8850795
CurrentTrain: epoch  2, batch    91 | loss: 70.3347236
CurrentTrain: epoch  2, batch    92 | loss: 70.7719700
CurrentTrain: epoch  2, batch    93 | loss: 55.1944546
CurrentTrain: epoch  2, batch    94 | loss: 60.9322774
CurrentTrain: epoch  2, batch    95 | loss: 59.2425319
CurrentTrain: epoch  3, batch     0 | loss: 117.9593712
CurrentTrain: epoch  3, batch     1 | loss: 68.9264124
CurrentTrain: epoch  3, batch     2 | loss: 86.8163541
CurrentTrain: epoch  3, batch     3 | loss: 55.3367765
CurrentTrain: epoch  3, batch     4 | loss: 88.2729675
CurrentTrain: epoch  3, batch     5 | loss: 68.6638316
CurrentTrain: epoch  3, batch     6 | loss: 56.9944941
CurrentTrain: epoch  3, batch     7 | loss: 86.2462976
CurrentTrain: epoch  3, batch     8 | loss: 54.8905435
CurrentTrain: epoch  3, batch     9 | loss: 55.3103723
CurrentTrain: epoch  3, batch    10 | loss: 50.7855850
CurrentTrain: epoch  3, batch    11 | loss: 56.7707006
CurrentTrain: epoch  3, batch    12 | loss: 85.3369883
CurrentTrain: epoch  3, batch    13 | loss: 80.4750074
CurrentTrain: epoch  3, batch    14 | loss: 54.8299171
CurrentTrain: epoch  3, batch    15 | loss: 86.0539500
CurrentTrain: epoch  3, batch    16 | loss: 85.5809107
CurrentTrain: epoch  3, batch    17 | loss: 117.0124131
CurrentTrain: epoch  3, batch    18 | loss: 61.8509470
CurrentTrain: epoch  3, batch    19 | loss: 120.3925471
CurrentTrain: epoch  3, batch    20 | loss: 114.7118249
CurrentTrain: epoch  3, batch    21 | loss: 66.5692516
CurrentTrain: epoch  3, batch    22 | loss: 57.5649688
CurrentTrain: epoch  3, batch    23 | loss: 55.6844146
CurrentTrain: epoch  3, batch    24 | loss: 85.0829935
CurrentTrain: epoch  3, batch    25 | loss: 85.8129391
CurrentTrain: epoch  3, batch    26 | loss: 53.2963858
CurrentTrain: epoch  3, batch    27 | loss: 118.7622864
CurrentTrain: epoch  3, batch    28 | loss: 67.1520078
CurrentTrain: epoch  3, batch    29 | loss: 54.0236937
CurrentTrain: epoch  3, batch    30 | loss: 65.7082731
CurrentTrain: epoch  3, batch    31 | loss: 85.0006787
CurrentTrain: epoch  3, batch    32 | loss: 65.9909271
CurrentTrain: epoch  3, batch    33 | loss: 44.1023595
CurrentTrain: epoch  3, batch    34 | loss: 121.0459731
CurrentTrain: epoch  3, batch    35 | loss: 50.8594204
CurrentTrain: epoch  3, batch    36 | loss: 69.3788967
CurrentTrain: epoch  3, batch    37 | loss: 68.8571476
CurrentTrain: epoch  3, batch    38 | loss: 46.2368268
CurrentTrain: epoch  3, batch    39 | loss: 70.2096379
CurrentTrain: epoch  3, batch    40 | loss: 66.6808194
CurrentTrain: epoch  3, batch    41 | loss: 83.6963331
CurrentTrain: epoch  3, batch    42 | loss: 68.1540996
CurrentTrain: epoch  3, batch    43 | loss: 116.5897555
CurrentTrain: epoch  3, batch    44 | loss: 53.2873308
CurrentTrain: epoch  3, batch    45 | loss: 53.5236206
CurrentTrain: epoch  3, batch    46 | loss: 52.3407409
CurrentTrain: epoch  3, batch    47 | loss: 66.4415865
CurrentTrain: epoch  3, batch    48 | loss: 65.3503677
CurrentTrain: epoch  3, batch    49 | loss: 81.7254816
CurrentTrain: epoch  3, batch    50 | loss: 55.5710735
CurrentTrain: epoch  3, batch    51 | loss: 65.0703450
CurrentTrain: epoch  3, batch    52 | loss: 70.5828792
CurrentTrain: epoch  3, batch    53 | loss: 118.5714496
CurrentTrain: epoch  3, batch    54 | loss: 52.4595767
CurrentTrain: epoch  3, batch    55 | loss: 53.2249916
CurrentTrain: epoch  3, batch    56 | loss: 87.7457580
CurrentTrain: epoch  3, batch    57 | loss: 45.9459439
CurrentTrain: epoch  3, batch    58 | loss: 66.6943530
CurrentTrain: epoch  3, batch    59 | loss: 64.9528109
CurrentTrain: epoch  3, batch    60 | loss: 53.6319155
CurrentTrain: epoch  3, batch    61 | loss: 86.2614408
CurrentTrain: epoch  3, batch    62 | loss: 54.3179166
CurrentTrain: epoch  3, batch    63 | loss: 58.0122549
CurrentTrain: epoch  3, batch    64 | loss: 85.8046402
CurrentTrain: epoch  3, batch    65 | loss: 70.2491003
CurrentTrain: epoch  3, batch    66 | loss: 86.4346582
CurrentTrain: epoch  3, batch    67 | loss: 64.0125169
CurrentTrain: epoch  3, batch    68 | loss: 68.3588605
CurrentTrain: epoch  3, batch    69 | loss: 67.4512331
CurrentTrain: epoch  3, batch    70 | loss: 43.4643153
CurrentTrain: epoch  3, batch    71 | loss: 53.6193353
CurrentTrain: epoch  3, batch    72 | loss: 65.4773130
CurrentTrain: epoch  3, batch    73 | loss: 68.7913036
CurrentTrain: epoch  3, batch    74 | loss: 87.8547518
CurrentTrain: epoch  3, batch    75 | loss: 176.4707408
CurrentTrain: epoch  3, batch    76 | loss: 91.3645988
CurrentTrain: epoch  3, batch    77 | loss: 85.4655771
CurrentTrain: epoch  3, batch    78 | loss: 84.3667407
CurrentTrain: epoch  3, batch    79 | loss: 66.7607096
CurrentTrain: epoch  3, batch    80 | loss: 51.4496851
CurrentTrain: epoch  3, batch    81 | loss: 56.3143521
CurrentTrain: epoch  3, batch    82 | loss: 65.5543459
CurrentTrain: epoch  3, batch    83 | loss: 116.5622061
CurrentTrain: epoch  3, batch    84 | loss: 182.8281231
CurrentTrain: epoch  3, batch    85 | loss: 68.7222126
CurrentTrain: epoch  3, batch    86 | loss: 88.3524980
CurrentTrain: epoch  3, batch    87 | loss: 83.4044919
CurrentTrain: epoch  3, batch    88 | loss: 85.6212437
CurrentTrain: epoch  3, batch    89 | loss: 64.9834287
CurrentTrain: epoch  3, batch    90 | loss: 87.5161071
CurrentTrain: epoch  3, batch    91 | loss: 54.5984413
CurrentTrain: epoch  3, batch    92 | loss: 117.7096923
CurrentTrain: epoch  3, batch    93 | loss: 119.1057650
CurrentTrain: epoch  3, batch    94 | loss: 46.9826756
CurrentTrain: epoch  3, batch    95 | loss: 68.7033789
CurrentTrain: epoch  4, batch     0 | loss: 55.3399546
CurrentTrain: epoch  4, batch     1 | loss: 89.6110572
CurrentTrain: epoch  4, batch     2 | loss: 53.2084401
CurrentTrain: epoch  4, batch     3 | loss: 66.8983841
CurrentTrain: epoch  4, batch     4 | loss: 45.2179192
CurrentTrain: epoch  4, batch     5 | loss: 68.1182312
CurrentTrain: epoch  4, batch     6 | loss: 116.1819599
CurrentTrain: epoch  4, batch     7 | loss: 68.7002955
CurrentTrain: epoch  4, batch     8 | loss: 65.9267847
CurrentTrain: epoch  4, batch     9 | loss: 62.9058338
CurrentTrain: epoch  4, batch    10 | loss: 113.6021628
CurrentTrain: epoch  4, batch    11 | loss: 68.9520980
CurrentTrain: epoch  4, batch    12 | loss: 116.1350406
CurrentTrain: epoch  4, batch    13 | loss: 68.6760150
CurrentTrain: epoch  4, batch    14 | loss: 58.5750399
CurrentTrain: epoch  4, batch    15 | loss: 113.2906637
CurrentTrain: epoch  4, batch    16 | loss: 372.5011404
CurrentTrain: epoch  4, batch    17 | loss: 64.7631561
CurrentTrain: epoch  4, batch    18 | loss: 68.8000496
CurrentTrain: epoch  4, batch    19 | loss: 50.7387919
CurrentTrain: epoch  4, batch    20 | loss: 45.2208971
CurrentTrain: epoch  4, batch    21 | loss: 82.4766142
CurrentTrain: epoch  4, batch    22 | loss: 50.5351353
CurrentTrain: epoch  4, batch    23 | loss: 81.5468869
CurrentTrain: epoch  4, batch    24 | loss: 82.3309032
CurrentTrain: epoch  4, batch    25 | loss: 67.0133249
CurrentTrain: epoch  4, batch    26 | loss: 69.5108349
CurrentTrain: epoch  4, batch    27 | loss: 64.4598403
CurrentTrain: epoch  4, batch    28 | loss: 65.3385592
CurrentTrain: epoch  4, batch    29 | loss: 55.5086447
CurrentTrain: epoch  4, batch    30 | loss: 66.9362914
CurrentTrain: epoch  4, batch    31 | loss: 67.4363938
CurrentTrain: epoch  4, batch    32 | loss: 45.9589847
CurrentTrain: epoch  4, batch    33 | loss: 178.9244747
CurrentTrain: epoch  4, batch    34 | loss: 55.8219528
CurrentTrain: epoch  4, batch    35 | loss: 85.6583710
CurrentTrain: epoch  4, batch    36 | loss: 51.6148552
CurrentTrain: epoch  4, batch    37 | loss: 118.4008190
CurrentTrain: epoch  4, batch    38 | loss: 82.3889986
CurrentTrain: epoch  4, batch    39 | loss: 54.4834008
CurrentTrain: epoch  4, batch    40 | loss: 183.0108311
CurrentTrain: epoch  4, batch    41 | loss: 65.5245388
CurrentTrain: epoch  4, batch    42 | loss: 63.5475905
CurrentTrain: epoch  4, batch    43 | loss: 53.7108997
CurrentTrain: epoch  4, batch    44 | loss: 61.9702479
CurrentTrain: epoch  4, batch    45 | loss: 112.0841558
CurrentTrain: epoch  4, batch    46 | loss: 51.9230042
CurrentTrain: epoch  4, batch    47 | loss: 113.5184961
CurrentTrain: epoch  4, batch    48 | loss: 52.2603466
CurrentTrain: epoch  4, batch    49 | loss: 67.1199898
CurrentTrain: epoch  4, batch    50 | loss: 67.9745578
CurrentTrain: epoch  4, batch    51 | loss: 66.8137639
CurrentTrain: epoch  4, batch    52 | loss: 64.4699913
CurrentTrain: epoch  4, batch    53 | loss: 53.0500854
CurrentTrain: epoch  4, batch    54 | loss: 117.2289473
CurrentTrain: epoch  4, batch    55 | loss: 82.4935873
CurrentTrain: epoch  4, batch    56 | loss: 82.3667871
CurrentTrain: epoch  4, batch    57 | loss: 90.9053698
CurrentTrain: epoch  4, batch    58 | loss: 65.1268135
CurrentTrain: epoch  4, batch    59 | loss: 66.1082655
CurrentTrain: epoch  4, batch    60 | loss: 54.9442453
CurrentTrain: epoch  4, batch    61 | loss: 68.1177018
CurrentTrain: epoch  4, batch    62 | loss: 64.7414809
CurrentTrain: epoch  4, batch    63 | loss: 87.3234146
CurrentTrain: epoch  4, batch    64 | loss: 83.8185557
CurrentTrain: epoch  4, batch    65 | loss: 86.8102303
CurrentTrain: epoch  4, batch    66 | loss: 67.2388069
CurrentTrain: epoch  4, batch    67 | loss: 51.4788067
CurrentTrain: epoch  4, batch    68 | loss: 66.0183381
CurrentTrain: epoch  4, batch    69 | loss: 86.2304227
CurrentTrain: epoch  4, batch    70 | loss: 50.3655233
CurrentTrain: epoch  4, batch    71 | loss: 68.2435697
CurrentTrain: epoch  4, batch    72 | loss: 51.5137067
CurrentTrain: epoch  4, batch    73 | loss: 69.1549314
CurrentTrain: epoch  4, batch    74 | loss: 68.9164307
CurrentTrain: epoch  4, batch    75 | loss: 88.4001694
CurrentTrain: epoch  4, batch    76 | loss: 67.0634506
CurrentTrain: epoch  4, batch    77 | loss: 66.6339318
CurrentTrain: epoch  4, batch    78 | loss: 65.7159334
CurrentTrain: epoch  4, batch    79 | loss: 64.3325558
CurrentTrain: epoch  4, batch    80 | loss: 118.2551161
CurrentTrain: epoch  4, batch    81 | loss: 63.8659310
CurrentTrain: epoch  4, batch    82 | loss: 55.9690623
CurrentTrain: epoch  4, batch    83 | loss: 54.0372087
CurrentTrain: epoch  4, batch    84 | loss: 87.1130944
CurrentTrain: epoch  4, batch    85 | loss: 83.3320236
CurrentTrain: epoch  4, batch    86 | loss: 51.8895796
CurrentTrain: epoch  4, batch    87 | loss: 67.3822195
CurrentTrain: epoch  4, batch    88 | loss: 86.5648809
CurrentTrain: epoch  4, batch    89 | loss: 67.7493347
CurrentTrain: epoch  4, batch    90 | loss: 51.5607210
CurrentTrain: epoch  4, batch    91 | loss: 65.6234617
CurrentTrain: epoch  4, batch    92 | loss: 67.3568948
CurrentTrain: epoch  4, batch    93 | loss: 55.2871824
CurrentTrain: epoch  4, batch    94 | loss: 85.8474174
CurrentTrain: epoch  4, batch    95 | loss: 72.6219193
CurrentTrain: epoch  5, batch     0 | loss: 65.0537103
CurrentTrain: epoch  5, batch     1 | loss: 64.0273314
CurrentTrain: epoch  5, batch     2 | loss: 64.0482448
CurrentTrain: epoch  5, batch     3 | loss: 53.6825567
CurrentTrain: epoch  5, batch     4 | loss: 67.2353279
CurrentTrain: epoch  5, batch     5 | loss: 67.9355443
CurrentTrain: epoch  5, batch     6 | loss: 54.3725959
CurrentTrain: epoch  5, batch     7 | loss: 46.8347802
CurrentTrain: epoch  5, batch     8 | loss: 80.6214374
CurrentTrain: epoch  5, batch     9 | loss: 63.6309703
CurrentTrain: epoch  5, batch    10 | loss: 66.4150882
CurrentTrain: epoch  5, batch    11 | loss: 53.9361966
CurrentTrain: epoch  5, batch    12 | loss: 81.9684709
CurrentTrain: epoch  5, batch    13 | loss: 52.7821129
CurrentTrain: epoch  5, batch    14 | loss: 51.2228167
CurrentTrain: epoch  5, batch    15 | loss: 83.3620637
CurrentTrain: epoch  5, batch    16 | loss: 50.9034420
CurrentTrain: epoch  5, batch    17 | loss: 85.4988331
CurrentTrain: epoch  5, batch    18 | loss: 63.8327945
CurrentTrain: epoch  5, batch    19 | loss: 65.5725097
CurrentTrain: epoch  5, batch    20 | loss: 45.1648160
CurrentTrain: epoch  5, batch    21 | loss: 51.6988211
CurrentTrain: epoch  5, batch    22 | loss: 83.2889050
CurrentTrain: epoch  5, batch    23 | loss: 67.5943902
CurrentTrain: epoch  5, batch    24 | loss: 55.0008179
CurrentTrain: epoch  5, batch    25 | loss: 54.3901059
CurrentTrain: epoch  5, batch    26 | loss: 116.1065776
CurrentTrain: epoch  5, batch    27 | loss: 80.9121466
CurrentTrain: epoch  5, batch    28 | loss: 64.5869562
CurrentTrain: epoch  5, batch    29 | loss: 71.6813545
CurrentTrain: epoch  5, batch    30 | loss: 54.3843803
CurrentTrain: epoch  5, batch    31 | loss: 80.5257114
CurrentTrain: epoch  5, batch    32 | loss: 62.9292858
CurrentTrain: epoch  5, batch    33 | loss: 66.7039716
CurrentTrain: epoch  5, batch    34 | loss: 68.2210944
CurrentTrain: epoch  5, batch    35 | loss: 86.2309018
CurrentTrain: epoch  5, batch    36 | loss: 68.0730783
CurrentTrain: epoch  5, batch    37 | loss: 85.6051952
CurrentTrain: epoch  5, batch    38 | loss: 66.0851796
CurrentTrain: epoch  5, batch    39 | loss: 63.5908696
CurrentTrain: epoch  5, batch    40 | loss: 66.7098249
CurrentTrain: epoch  5, batch    41 | loss: 64.6290080
CurrentTrain: epoch  5, batch    42 | loss: 63.2380656
CurrentTrain: epoch  5, batch    43 | loss: 51.4556962
CurrentTrain: epoch  5, batch    44 | loss: 117.8289135
CurrentTrain: epoch  5, batch    45 | loss: 65.2934064
CurrentTrain: epoch  5, batch    46 | loss: 66.1375483
CurrentTrain: epoch  5, batch    47 | loss: 67.5779930
CurrentTrain: epoch  5, batch    48 | loss: 50.1208881
CurrentTrain: epoch  5, batch    49 | loss: 51.1655007
CurrentTrain: epoch  5, batch    50 | loss: 118.4261878
CurrentTrain: epoch  5, batch    51 | loss: 54.0015604
CurrentTrain: epoch  5, batch    52 | loss: 65.5499045
CurrentTrain: epoch  5, batch    53 | loss: 44.4926421
CurrentTrain: epoch  5, batch    54 | loss: 65.7013081
CurrentTrain: epoch  5, batch    55 | loss: 119.3638621
CurrentTrain: epoch  5, batch    56 | loss: 84.3190925
CurrentTrain: epoch  5, batch    57 | loss: 52.8348131
CurrentTrain: epoch  5, batch    58 | loss: 86.9362807
CurrentTrain: epoch  5, batch    59 | loss: 65.2630289
CurrentTrain: epoch  5, batch    60 | loss: 115.3600376
CurrentTrain: epoch  5, batch    61 | loss: 67.1908457
CurrentTrain: epoch  5, batch    62 | loss: 67.2386341
CurrentTrain: epoch  5, batch    63 | loss: 83.6666136
CurrentTrain: epoch  5, batch    64 | loss: 116.7131541
CurrentTrain: epoch  5, batch    65 | loss: 54.5806025
CurrentTrain: epoch  5, batch    66 | loss: 82.4638754
CurrentTrain: epoch  5, batch    67 | loss: 52.6339809
CurrentTrain: epoch  5, batch    68 | loss: 86.1856751
CurrentTrain: epoch  5, batch    69 | loss: 61.1466768
CurrentTrain: epoch  5, batch    70 | loss: 85.0261559
CurrentTrain: epoch  5, batch    71 | loss: 116.6514830
CurrentTrain: epoch  5, batch    72 | loss: 68.3457258
CurrentTrain: epoch  5, batch    73 | loss: 64.7766889
CurrentTrain: epoch  5, batch    74 | loss: 63.9599113
CurrentTrain: epoch  5, batch    75 | loss: 115.4848538
CurrentTrain: epoch  5, batch    76 | loss: 42.7694458
CurrentTrain: epoch  5, batch    77 | loss: 52.1450683
CurrentTrain: epoch  5, batch    78 | loss: 53.7725189
CurrentTrain: epoch  5, batch    79 | loss: 64.7967585
CurrentTrain: epoch  5, batch    80 | loss: 83.0694050
CurrentTrain: epoch  5, batch    81 | loss: 52.1284976
CurrentTrain: epoch  5, batch    82 | loss: 113.6760879
CurrentTrain: epoch  5, batch    83 | loss: 44.6910393
CurrentTrain: epoch  5, batch    84 | loss: 63.3186759
CurrentTrain: epoch  5, batch    85 | loss: 88.4320893
CurrentTrain: epoch  5, batch    86 | loss: 42.1521719
CurrentTrain: epoch  5, batch    87 | loss: 87.8543407
CurrentTrain: epoch  5, batch    88 | loss: 54.9870984
CurrentTrain: epoch  5, batch    89 | loss: 112.3386587
CurrentTrain: epoch  5, batch    90 | loss: 118.2711826
CurrentTrain: epoch  5, batch    91 | loss: 54.1585626
CurrentTrain: epoch  5, batch    92 | loss: 55.0127133
CurrentTrain: epoch  5, batch    93 | loss: 66.5371370
CurrentTrain: epoch  5, batch    94 | loss: 83.4204684
CurrentTrain: epoch  5, batch    95 | loss: 69.4945450
CurrentTrain: epoch  6, batch     0 | loss: 71.9067433
CurrentTrain: epoch  6, batch     1 | loss: 48.9092522
CurrentTrain: epoch  6, batch     2 | loss: 60.5319504
CurrentTrain: epoch  6, batch     3 | loss: 63.5016151
CurrentTrain: epoch  6, batch     4 | loss: 109.8890309
CurrentTrain: epoch  6, batch     5 | loss: 69.0318858
CurrentTrain: epoch  6, batch     6 | loss: 68.6906051
CurrentTrain: epoch  6, batch     7 | loss: 54.5316006
CurrentTrain: epoch  6, batch     8 | loss: 47.1792768
CurrentTrain: epoch  6, batch     9 | loss: 54.3100970
CurrentTrain: epoch  6, batch    10 | loss: 49.3665628
CurrentTrain: epoch  6, batch    11 | loss: 82.7883844
CurrentTrain: epoch  6, batch    12 | loss: 64.7178185
CurrentTrain: epoch  6, batch    13 | loss: 63.6944884
CurrentTrain: epoch  6, batch    14 | loss: 43.0911778
CurrentTrain: epoch  6, batch    15 | loss: 82.9275337
CurrentTrain: epoch  6, batch    16 | loss: 50.9165928
CurrentTrain: epoch  6, batch    17 | loss: 54.8970548
CurrentTrain: epoch  6, batch    18 | loss: 50.2992030
CurrentTrain: epoch  6, batch    19 | loss: 82.4792201
CurrentTrain: epoch  6, batch    20 | loss: 119.9467152
CurrentTrain: epoch  6, batch    21 | loss: 115.8930708
CurrentTrain: epoch  6, batch    22 | loss: 61.8080703
CurrentTrain: epoch  6, batch    23 | loss: 82.9263586
CurrentTrain: epoch  6, batch    24 | loss: 85.8173482
CurrentTrain: epoch  6, batch    25 | loss: 117.6914420
CurrentTrain: epoch  6, batch    26 | loss: 82.9126258
CurrentTrain: epoch  6, batch    27 | loss: 82.8023521
CurrentTrain: epoch  6, batch    28 | loss: 83.4702963
CurrentTrain: epoch  6, batch    29 | loss: 65.0207936
CurrentTrain: epoch  6, batch    30 | loss: 66.0748820
CurrentTrain: epoch  6, batch    31 | loss: 63.4637889
CurrentTrain: epoch  6, batch    32 | loss: 52.5660791
CurrentTrain: epoch  6, batch    33 | loss: 52.6700137
CurrentTrain: epoch  6, batch    34 | loss: 52.2740841
CurrentTrain: epoch  6, batch    35 | loss: 45.1610027
CurrentTrain: epoch  6, batch    36 | loss: 82.4510988
CurrentTrain: epoch  6, batch    37 | loss: 84.7091216
CurrentTrain: epoch  6, batch    38 | loss: 67.1151985
CurrentTrain: epoch  6, batch    39 | loss: 65.5812463
CurrentTrain: epoch  6, batch    40 | loss: 113.8020341
CurrentTrain: epoch  6, batch    41 | loss: 85.2486977
CurrentTrain: epoch  6, batch    42 | loss: 65.0711399
CurrentTrain: epoch  6, batch    43 | loss: 67.2008655
CurrentTrain: epoch  6, batch    44 | loss: 63.1025496
CurrentTrain: epoch  6, batch    45 | loss: 81.8106878
CurrentTrain: epoch  6, batch    46 | loss: 52.8799615
CurrentTrain: epoch  6, batch    47 | loss: 84.9338786
CurrentTrain: epoch  6, batch    48 | loss: 84.2716315
CurrentTrain: epoch  6, batch    49 | loss: 43.2063330
CurrentTrain: epoch  6, batch    50 | loss: 76.0740765
CurrentTrain: epoch  6, batch    51 | loss: 62.0342210
CurrentTrain: epoch  6, batch    52 | loss: 51.9547103
CurrentTrain: epoch  6, batch    53 | loss: 54.0505172
CurrentTrain: epoch  6, batch    54 | loss: 82.6372553
CurrentTrain: epoch  6, batch    55 | loss: 86.9520253
CurrentTrain: epoch  6, batch    56 | loss: 83.4090552
CurrentTrain: epoch  6, batch    57 | loss: 63.3836272
CurrentTrain: epoch  6, batch    58 | loss: 79.3754475
CurrentTrain: epoch  6, batch    59 | loss: 53.0305251
CurrentTrain: epoch  6, batch    60 | loss: 83.3080899
CurrentTrain: epoch  6, batch    61 | loss: 85.5993246
CurrentTrain: epoch  6, batch    62 | loss: 113.1947336
CurrentTrain: epoch  6, batch    63 | loss: 82.8475069
CurrentTrain: epoch  6, batch    64 | loss: 84.3555235
CurrentTrain: epoch  6, batch    65 | loss: 113.3011581
CurrentTrain: epoch  6, batch    66 | loss: 81.1844332
CurrentTrain: epoch  6, batch    67 | loss: 45.4394124
CurrentTrain: epoch  6, batch    68 | loss: 68.1502030
CurrentTrain: epoch  6, batch    69 | loss: 50.3135847
CurrentTrain: epoch  6, batch    70 | loss: 70.4088613
CurrentTrain: epoch  6, batch    71 | loss: 56.0883947
CurrentTrain: epoch  6, batch    72 | loss: 56.9809864
CurrentTrain: epoch  6, batch    73 | loss: 43.5478919
CurrentTrain: epoch  6, batch    74 | loss: 66.3922622
CurrentTrain: epoch  6, batch    75 | loss: 68.5193167
CurrentTrain: epoch  6, batch    76 | loss: 79.3896159
CurrentTrain: epoch  6, batch    77 | loss: 44.6385556
CurrentTrain: epoch  6, batch    78 | loss: 90.7767507
CurrentTrain: epoch  6, batch    79 | loss: 87.9018151
CurrentTrain: epoch  6, batch    80 | loss: 41.6185708
CurrentTrain: epoch  6, batch    81 | loss: 62.9661874
CurrentTrain: epoch  6, batch    82 | loss: 51.1626964
CurrentTrain: epoch  6, batch    83 | loss: 69.6649918
CurrentTrain: epoch  6, batch    84 | loss: 116.4091649
CurrentTrain: epoch  6, batch    85 | loss: 66.3614425
CurrentTrain: epoch  6, batch    86 | loss: 117.6763492
CurrentTrain: epoch  6, batch    87 | loss: 45.7460435
CurrentTrain: epoch  6, batch    88 | loss: 66.0384484
CurrentTrain: epoch  6, batch    89 | loss: 114.2507964
CurrentTrain: epoch  6, batch    90 | loss: 63.6052758
CurrentTrain: epoch  6, batch    91 | loss: 118.7427028
CurrentTrain: epoch  6, batch    92 | loss: 63.6866465
CurrentTrain: epoch  6, batch    93 | loss: 65.3749522
CurrentTrain: epoch  6, batch    94 | loss: 53.5178786
CurrentTrain: epoch  6, batch    95 | loss: 54.3632122
CurrentTrain: epoch  7, batch     0 | loss: 115.5244135
CurrentTrain: epoch  7, batch     1 | loss: 43.8680130
CurrentTrain: epoch  7, batch     2 | loss: 66.3821855
CurrentTrain: epoch  7, batch     3 | loss: 82.4198538
CurrentTrain: epoch  7, batch     4 | loss: 85.0441089
CurrentTrain: epoch  7, batch     5 | loss: 62.9770833
CurrentTrain: epoch  7, batch     6 | loss: 52.3298870
CurrentTrain: epoch  7, batch     7 | loss: 84.1132938
CurrentTrain: epoch  7, batch     8 | loss: 67.3668282
CurrentTrain: epoch  7, batch     9 | loss: 62.7283897
CurrentTrain: epoch  7, batch    10 | loss: 81.3835255
CurrentTrain: epoch  7, batch    11 | loss: 113.6366179
CurrentTrain: epoch  7, batch    12 | loss: 51.7652583
CurrentTrain: epoch  7, batch    13 | loss: 50.3913210
CurrentTrain: epoch  7, batch    14 | loss: 43.5182905
CurrentTrain: epoch  7, batch    15 | loss: 49.3559823
CurrentTrain: epoch  7, batch    16 | loss: 84.4983770
CurrentTrain: epoch  7, batch    17 | loss: 62.5004917
CurrentTrain: epoch  7, batch    18 | loss: 61.9734012
CurrentTrain: epoch  7, batch    19 | loss: 63.5050855
CurrentTrain: epoch  7, batch    20 | loss: 113.5275006
CurrentTrain: epoch  7, batch    21 | loss: 117.6540714
CurrentTrain: epoch  7, batch    22 | loss: 65.1273274
CurrentTrain: epoch  7, batch    23 | loss: 82.0328331
CurrentTrain: epoch  7, batch    24 | loss: 55.3880570
CurrentTrain: epoch  7, batch    25 | loss: 82.7428000
CurrentTrain: epoch  7, batch    26 | loss: 68.9488948
CurrentTrain: epoch  7, batch    27 | loss: 52.7737538
CurrentTrain: epoch  7, batch    28 | loss: 66.6306043
CurrentTrain: epoch  7, batch    29 | loss: 86.3285400
CurrentTrain: epoch  7, batch    30 | loss: 66.0656207
CurrentTrain: epoch  7, batch    31 | loss: 111.7633684
CurrentTrain: epoch  7, batch    32 | loss: 82.7716500
CurrentTrain: epoch  7, batch    33 | loss: 42.7781891
CurrentTrain: epoch  7, batch    34 | loss: 42.8720993
CurrentTrain: epoch  7, batch    35 | loss: 61.0947289
CurrentTrain: epoch  7, batch    36 | loss: 115.3823968
CurrentTrain: epoch  7, batch    37 | loss: 67.7277931
CurrentTrain: epoch  7, batch    38 | loss: 65.5082188
CurrentTrain: epoch  7, batch    39 | loss: 67.4705130
CurrentTrain: epoch  7, batch    40 | loss: 65.0978735
CurrentTrain: epoch  7, batch    41 | loss: 51.9679724
CurrentTrain: epoch  7, batch    42 | loss: 61.9369863
CurrentTrain: epoch  7, batch    43 | loss: 51.5123021
CurrentTrain: epoch  7, batch    44 | loss: 84.6944897
CurrentTrain: epoch  7, batch    45 | loss: 117.5958702
CurrentTrain: epoch  7, batch    46 | loss: 83.4768030
CurrentTrain: epoch  7, batch    47 | loss: 63.4052810
CurrentTrain: epoch  7, batch    48 | loss: 64.3972415
CurrentTrain: epoch  7, batch    49 | loss: 48.1050890
CurrentTrain: epoch  7, batch    50 | loss: 66.1617026
CurrentTrain: epoch  7, batch    51 | loss: 66.1986435
CurrentTrain: epoch  7, batch    52 | loss: 62.5682687
CurrentTrain: epoch  7, batch    53 | loss: 110.8685719
CurrentTrain: epoch  7, batch    54 | loss: 65.6793269
CurrentTrain: epoch  7, batch    55 | loss: 86.5588602
CurrentTrain: epoch  7, batch    56 | loss: 64.5531769
CurrentTrain: epoch  7, batch    57 | loss: 63.2423558
CurrentTrain: epoch  7, batch    58 | loss: 49.2837821
CurrentTrain: epoch  7, batch    59 | loss: 65.3720762
CurrentTrain: epoch  7, batch    60 | loss: 84.7648875
CurrentTrain: epoch  7, batch    61 | loss: 113.7061150
CurrentTrain: epoch  7, batch    62 | loss: 51.0376188
CurrentTrain: epoch  7, batch    63 | loss: 41.5856985
CurrentTrain: epoch  7, batch    64 | loss: 118.2837390
CurrentTrain: epoch  7, batch    65 | loss: 51.0515479
CurrentTrain: epoch  7, batch    66 | loss: 85.2710347
CurrentTrain: epoch  7, batch    67 | loss: 62.2565168
CurrentTrain: epoch  7, batch    68 | loss: 78.3171812
CurrentTrain: epoch  7, batch    69 | loss: 66.0631261
CurrentTrain: epoch  7, batch    70 | loss: 50.6329684
CurrentTrain: epoch  7, batch    71 | loss: 44.3553279
CurrentTrain: epoch  7, batch    72 | loss: 86.2227771
CurrentTrain: epoch  7, batch    73 | loss: 61.7920890
CurrentTrain: epoch  7, batch    74 | loss: 66.9651072
CurrentTrain: epoch  7, batch    75 | loss: 65.6341050
CurrentTrain: epoch  7, batch    76 | loss: 84.2556754
CurrentTrain: epoch  7, batch    77 | loss: 88.5153670
CurrentTrain: epoch  7, batch    78 | loss: 66.9326108
CurrentTrain: epoch  7, batch    79 | loss: 65.0625912
CurrentTrain: epoch  7, batch    80 | loss: 78.8175135
CurrentTrain: epoch  7, batch    81 | loss: 62.8325996
CurrentTrain: epoch  7, batch    82 | loss: 70.2882652
CurrentTrain: epoch  7, batch    83 | loss: 86.7139753
CurrentTrain: epoch  7, batch    84 | loss: 64.5185649
CurrentTrain: epoch  7, batch    85 | loss: 115.3336944
CurrentTrain: epoch  7, batch    86 | loss: 83.4529511
CurrentTrain: epoch  7, batch    87 | loss: 53.3861251
CurrentTrain: epoch  7, batch    88 | loss: 63.4322289
CurrentTrain: epoch  7, batch    89 | loss: 84.9234855
CurrentTrain: epoch  7, batch    90 | loss: 64.8483509
CurrentTrain: epoch  7, batch    91 | loss: 67.1569076
CurrentTrain: epoch  7, batch    92 | loss: 42.6820107
CurrentTrain: epoch  7, batch    93 | loss: 65.1805385
CurrentTrain: epoch  7, batch    94 | loss: 50.9540313
CurrentTrain: epoch  7, batch    95 | loss: 69.2529319
CurrentTrain: epoch  8, batch     0 | loss: 50.1417717
CurrentTrain: epoch  8, batch     1 | loss: 65.2669060
CurrentTrain: epoch  8, batch     2 | loss: 49.5574542
CurrentTrain: epoch  8, batch     3 | loss: 115.6844893
CurrentTrain: epoch  8, batch     4 | loss: 64.9350224
CurrentTrain: epoch  8, batch     5 | loss: 84.2615794
CurrentTrain: epoch  8, batch     6 | loss: 44.8669994
CurrentTrain: epoch  8, batch     7 | loss: 65.3740466
CurrentTrain: epoch  8, batch     8 | loss: 65.5111949
CurrentTrain: epoch  8, batch     9 | loss: 53.2705123
CurrentTrain: epoch  8, batch    10 | loss: 50.7232957
CurrentTrain: epoch  8, batch    11 | loss: 50.9457985
CurrentTrain: epoch  8, batch    12 | loss: 53.0093624
CurrentTrain: epoch  8, batch    13 | loss: 82.7435155
CurrentTrain: epoch  8, batch    14 | loss: 81.5332015
CurrentTrain: epoch  8, batch    15 | loss: 84.3988807
CurrentTrain: epoch  8, batch    16 | loss: 67.4509795
CurrentTrain: epoch  8, batch    17 | loss: 61.6401121
CurrentTrain: epoch  8, batch    18 | loss: 84.1066002
CurrentTrain: epoch  8, batch    19 | loss: 82.9780384
CurrentTrain: epoch  8, batch    20 | loss: 56.9996788
CurrentTrain: epoch  8, batch    21 | loss: 63.3761423
CurrentTrain: epoch  8, batch    22 | loss: 64.3048357
CurrentTrain: epoch  8, batch    23 | loss: 85.9489677
CurrentTrain: epoch  8, batch    24 | loss: 82.7318594
CurrentTrain: epoch  8, batch    25 | loss: 42.1648035
CurrentTrain: epoch  8, batch    26 | loss: 65.4358454
CurrentTrain: epoch  8, batch    27 | loss: 64.3981629
CurrentTrain: epoch  8, batch    28 | loss: 54.2512018
CurrentTrain: epoch  8, batch    29 | loss: 49.3190061
CurrentTrain: epoch  8, batch    30 | loss: 65.6902436
CurrentTrain: epoch  8, batch    31 | loss: 42.8863032
CurrentTrain: epoch  8, batch    32 | loss: 49.9530358
CurrentTrain: epoch  8, batch    33 | loss: 82.2227887
CurrentTrain: epoch  8, batch    34 | loss: 84.1499587
CurrentTrain: epoch  8, batch    35 | loss: 51.6593173
CurrentTrain: epoch  8, batch    36 | loss: 65.6663866
CurrentTrain: epoch  8, batch    37 | loss: 42.9449788
CurrentTrain: epoch  8, batch    38 | loss: 116.1925677
CurrentTrain: epoch  8, batch    39 | loss: 59.6871696
CurrentTrain: epoch  8, batch    40 | loss: 52.9769913
CurrentTrain: epoch  8, batch    41 | loss: 64.3182923
CurrentTrain: epoch  8, batch    42 | loss: 64.6024108
CurrentTrain: epoch  8, batch    43 | loss: 66.4148431
CurrentTrain: epoch  8, batch    44 | loss: 66.1104962
CurrentTrain: epoch  8, batch    45 | loss: 50.9957473
CurrentTrain: epoch  8, batch    46 | loss: 64.7485382
CurrentTrain: epoch  8, batch    47 | loss: 115.7093623
CurrentTrain: epoch  8, batch    48 | loss: 62.4949436
CurrentTrain: epoch  8, batch    49 | loss: 118.4665259
CurrentTrain: epoch  8, batch    50 | loss: 43.8592829
CurrentTrain: epoch  8, batch    51 | loss: 117.4917299
CurrentTrain: epoch  8, batch    52 | loss: 49.2601478
CurrentTrain: epoch  8, batch    53 | loss: 82.7950599
CurrentTrain: epoch  8, batch    54 | loss: 54.0328937
CurrentTrain: epoch  8, batch    55 | loss: 46.7224618
CurrentTrain: epoch  8, batch    56 | loss: 65.3926022
CurrentTrain: epoch  8, batch    57 | loss: 53.3663470
CurrentTrain: epoch  8, batch    58 | loss: 49.4222602
CurrentTrain: epoch  8, batch    59 | loss: 53.0993700
CurrentTrain: epoch  8, batch    60 | loss: 64.7098352
CurrentTrain: epoch  8, batch    61 | loss: 117.5513398
CurrentTrain: epoch  8, batch    62 | loss: 85.8045404
CurrentTrain: epoch  8, batch    63 | loss: 54.1193053
CurrentTrain: epoch  8, batch    64 | loss: 84.2792441
CurrentTrain: epoch  8, batch    65 | loss: 83.2246243
CurrentTrain: epoch  8, batch    66 | loss: 83.4904566
CurrentTrain: epoch  8, batch    67 | loss: 116.9134236
CurrentTrain: epoch  8, batch    68 | loss: 45.1222758
CurrentTrain: epoch  8, batch    69 | loss: 85.8574729
CurrentTrain: epoch  8, batch    70 | loss: 81.7440533
CurrentTrain: epoch  8, batch    71 | loss: 62.9476333
CurrentTrain: epoch  8, batch    72 | loss: 53.1043916
CurrentTrain: epoch  8, batch    73 | loss: 50.0216626
CurrentTrain: epoch  8, batch    74 | loss: 65.4195185
CurrentTrain: epoch  8, batch    75 | loss: 69.6996413
CurrentTrain: epoch  8, batch    76 | loss: 43.9295798
CurrentTrain: epoch  8, batch    77 | loss: 67.0562075
CurrentTrain: epoch  8, batch    78 | loss: 82.5393650
CurrentTrain: epoch  8, batch    79 | loss: 65.0551999
CurrentTrain: epoch  8, batch    80 | loss: 86.6445561
CurrentTrain: epoch  8, batch    81 | loss: 54.0820738
CurrentTrain: epoch  8, batch    82 | loss: 66.9725497
CurrentTrain: epoch  8, batch    83 | loss: 67.3651334
CurrentTrain: epoch  8, batch    84 | loss: 44.4003558
CurrentTrain: epoch  8, batch    85 | loss: 60.7160152
CurrentTrain: epoch  8, batch    86 | loss: 56.6458727
CurrentTrain: epoch  8, batch    87 | loss: 64.2469988
CurrentTrain: epoch  8, batch    88 | loss: 64.9075778
CurrentTrain: epoch  8, batch    89 | loss: 62.0947770
CurrentTrain: epoch  8, batch    90 | loss: 69.3295339
CurrentTrain: epoch  8, batch    91 | loss: 115.9431448
CurrentTrain: epoch  8, batch    92 | loss: 42.5641945
CurrentTrain: epoch  8, batch    93 | loss: 49.9127529
CurrentTrain: epoch  8, batch    94 | loss: 83.5190128
CurrentTrain: epoch  8, batch    95 | loss: 71.6246577
CurrentTrain: epoch  9, batch     0 | loss: 67.5120694
CurrentTrain: epoch  9, batch     1 | loss: 117.4794208
CurrentTrain: epoch  9, batch     2 | loss: 63.1309620
CurrentTrain: epoch  9, batch     3 | loss: 51.9387855
CurrentTrain: epoch  9, batch     4 | loss: 67.4904810
CurrentTrain: epoch  9, batch     5 | loss: 84.2175999
CurrentTrain: epoch  9, batch     6 | loss: 65.0510218
CurrentTrain: epoch  9, batch     7 | loss: 81.4552777
CurrentTrain: epoch  9, batch     8 | loss: 82.3933053
CurrentTrain: epoch  9, batch     9 | loss: 85.7515179
CurrentTrain: epoch  9, batch    10 | loss: 174.6715272
CurrentTrain: epoch  9, batch    11 | loss: 85.3808676
CurrentTrain: epoch  9, batch    12 | loss: 51.2358920
CurrentTrain: epoch  9, batch    13 | loss: 113.2911279
CurrentTrain: epoch  9, batch    14 | loss: 63.3103032
CurrentTrain: epoch  9, batch    15 | loss: 52.0115432
CurrentTrain: epoch  9, batch    16 | loss: 53.4196009
CurrentTrain: epoch  9, batch    17 | loss: 51.2003247
CurrentTrain: epoch  9, batch    18 | loss: 51.0514223
CurrentTrain: epoch  9, batch    19 | loss: 80.9247048
CurrentTrain: epoch  9, batch    20 | loss: 84.3267651
CurrentTrain: epoch  9, batch    21 | loss: 64.4748389
CurrentTrain: epoch  9, batch    22 | loss: 65.4948572
CurrentTrain: epoch  9, batch    23 | loss: 50.6220971
CurrentTrain: epoch  9, batch    24 | loss: 59.9575699
CurrentTrain: epoch  9, batch    25 | loss: 82.4239074
CurrentTrain: epoch  9, batch    26 | loss: 44.5581237
CurrentTrain: epoch  9, batch    27 | loss: 61.7595024
CurrentTrain: epoch  9, batch    28 | loss: 60.6657388
CurrentTrain: epoch  9, batch    29 | loss: 117.9171495
CurrentTrain: epoch  9, batch    30 | loss: 63.2287391
CurrentTrain: epoch  9, batch    31 | loss: 81.0368335
CurrentTrain: epoch  9, batch    32 | loss: 81.2600207
CurrentTrain: epoch  9, batch    33 | loss: 82.7123074
CurrentTrain: epoch  9, batch    34 | loss: 50.8291174
CurrentTrain: epoch  9, batch    35 | loss: 81.5608327
CurrentTrain: epoch  9, batch    36 | loss: 64.7519410
CurrentTrain: epoch  9, batch    37 | loss: 82.9582785
CurrentTrain: epoch  9, batch    38 | loss: 82.4450326
CurrentTrain: epoch  9, batch    39 | loss: 82.8035042
CurrentTrain: epoch  9, batch    40 | loss: 61.9517069
CurrentTrain: epoch  9, batch    41 | loss: 54.8212774
CurrentTrain: epoch  9, batch    42 | loss: 115.6112699
CurrentTrain: epoch  9, batch    43 | loss: 51.9556453
CurrentTrain: epoch  9, batch    44 | loss: 83.0652073
CurrentTrain: epoch  9, batch    45 | loss: 67.5533122
CurrentTrain: epoch  9, batch    46 | loss: 66.1979414
CurrentTrain: epoch  9, batch    47 | loss: 50.9548598
CurrentTrain: epoch  9, batch    48 | loss: 40.2912704
CurrentTrain: epoch  9, batch    49 | loss: 49.8865763
CurrentTrain: epoch  9, batch    50 | loss: 86.1834858
CurrentTrain: epoch  9, batch    51 | loss: 64.2609080
CurrentTrain: epoch  9, batch    52 | loss: 63.1245562
CurrentTrain: epoch  9, batch    53 | loss: 87.3172324
CurrentTrain: epoch  9, batch    54 | loss: 42.0692875
CurrentTrain: epoch  9, batch    55 | loss: 62.9594189
CurrentTrain: epoch  9, batch    56 | loss: 82.6185421
CurrentTrain: epoch  9, batch    57 | loss: 84.1417227
CurrentTrain: epoch  9, batch    58 | loss: 117.9202781
CurrentTrain: epoch  9, batch    59 | loss: 64.0649153
CurrentTrain: epoch  9, batch    60 | loss: 65.7669564
CurrentTrain: epoch  9, batch    61 | loss: 84.0480087
CurrentTrain: epoch  9, batch    62 | loss: 117.5263777
CurrentTrain: epoch  9, batch    63 | loss: 115.2364100
CurrentTrain: epoch  9, batch    64 | loss: 52.9934067
CurrentTrain: epoch  9, batch    65 | loss: 82.3699805
CurrentTrain: epoch  9, batch    66 | loss: 49.5251573
CurrentTrain: epoch  9, batch    67 | loss: 170.0848149
CurrentTrain: epoch  9, batch    68 | loss: 64.3343374
CurrentTrain: epoch  9, batch    69 | loss: 50.8905754
CurrentTrain: epoch  9, batch    70 | loss: 65.3885167
CurrentTrain: epoch  9, batch    71 | loss: 54.9405662
CurrentTrain: epoch  9, batch    72 | loss: 82.5097082
CurrentTrain: epoch  9, batch    73 | loss: 84.6136964
CurrentTrain: epoch  9, batch    74 | loss: 51.3651243
CurrentTrain: epoch  9, batch    75 | loss: 65.5193857
CurrentTrain: epoch  9, batch    76 | loss: 49.4426339
CurrentTrain: epoch  9, batch    77 | loss: 85.7964476
CurrentTrain: epoch  9, batch    78 | loss: 66.4631431
CurrentTrain: epoch  9, batch    79 | loss: 50.9980344
CurrentTrain: epoch  9, batch    80 | loss: 67.1072973
CurrentTrain: epoch  9, batch    81 | loss: 42.4279920
CurrentTrain: epoch  9, batch    82 | loss: 66.0167389
CurrentTrain: epoch  9, batch    83 | loss: 53.8222081
CurrentTrain: epoch  9, batch    84 | loss: 82.6008274
CurrentTrain: epoch  9, batch    85 | loss: 62.3515445
CurrentTrain: epoch  9, batch    86 | loss: 42.5368610
CurrentTrain: epoch  9, batch    87 | loss: 60.1809959
CurrentTrain: epoch  9, batch    88 | loss: 49.8895214
CurrentTrain: epoch  9, batch    89 | loss: 50.1685093
CurrentTrain: epoch  9, batch    90 | loss: 51.8319484
CurrentTrain: epoch  9, batch    91 | loss: 64.2607416
CurrentTrain: epoch  9, batch    92 | loss: 51.7178283
CurrentTrain: epoch  9, batch    93 | loss: 65.4186560
CurrentTrain: epoch  9, batch    94 | loss: 62.5535279
CurrentTrain: epoch  9, batch    95 | loss: 68.7445183

F1 score per class: {32: 0.6153846153846154, 6: 0.8888888888888888, 19: 0.3333333333333333, 24: 0.7675675675675676, 26: 0.9361702127659575, 29: 0.896551724137931}
Micro-average F1 score: 0.8134878819810326
Weighted-average F1 score: 0.8249273761612621
F1 score per class: {32: 0.7384615384615385, 6: 0.918918918918919, 19: 0.5161290322580645, 24: 0.770949720670391, 26: 0.9795918367346939, 29: 0.8855721393034826}
Micro-average F1 score: 0.8490374873353597
Weighted-average F1 score: 0.8515144179327342
F1 score per class: {32: 0.7253886010362695, 6: 0.918918918918919, 19: 0.5161290322580645, 24: 0.770949720670391, 26: 0.9795918367346939, 29: 0.8855721393034826}
Micro-average F1 score: 0.8467005076142132
Weighted-average F1 score: 0.8493951943318613

F1 score per class: {32: 0.6153846153846154, 6: 0.8888888888888888, 19: 0.3333333333333333, 24: 0.7675675675675676, 26: 0.9361702127659575, 29: 0.896551724137931}
Micro-average F1 score: 0.8134878819810326
Weighted-average F1 score: 0.8249273761612621
F1 score per class: {32: 0.7384615384615385, 6: 0.918918918918919, 19: 0.5161290322580645, 24: 0.770949720670391, 26: 0.9795918367346939, 29: 0.8855721393034826}
Micro-average F1 score: 0.8490374873353597
Weighted-average F1 score: 0.8515144179327342
F1 score per class: {32: 0.7253886010362695, 6: 0.918918918918919, 19: 0.5161290322580645, 24: 0.770949720670391, 26: 0.9795918367346939, 29: 0.8855721393034826}
Micro-average F1 score: 0.8467005076142132
Weighted-average F1 score: 0.8493951943318613
cur_acc:  ['0.8135']
his_acc:  ['0.8135']
cur_acc des:  ['0.8490']
his_acc des:  ['0.8490']
cur_acc rrf:  ['0.8467']
his_acc rrf:  ['0.8467']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 92.6206275
CurrentTrain: epoch  0, batch     1 | loss: 91.9241752
CurrentTrain: epoch  0, batch     2 | loss: 81.0050276
CurrentTrain: epoch  0, batch     3 | loss: 97.6078615
CurrentTrain: epoch  0, batch     4 | loss: 62.1844796
CurrentTrain: epoch  1, batch     0 | loss: 72.0719222
CurrentTrain: epoch  1, batch     1 | loss: 62.7162949
CurrentTrain: epoch  1, batch     2 | loss: 186.5246390
CurrentTrain: epoch  1, batch     3 | loss: 74.2538522
CurrentTrain: epoch  1, batch     4 | loss: 47.1656957
CurrentTrain: epoch  2, batch     0 | loss: 72.9865886
CurrentTrain: epoch  2, batch     1 | loss: 90.2664107
CurrentTrain: epoch  2, batch     2 | loss: 71.8497766
CurrentTrain: epoch  2, batch     3 | loss: 88.3526214
CurrentTrain: epoch  2, batch     4 | loss: 118.0437490
CurrentTrain: epoch  3, batch     0 | loss: 58.4774919
CurrentTrain: epoch  3, batch     1 | loss: 89.2552571
CurrentTrain: epoch  3, batch     2 | loss: 89.8433124
CurrentTrain: epoch  3, batch     3 | loss: 70.3558682
CurrentTrain: epoch  3, batch     4 | loss: 55.3258030
CurrentTrain: epoch  4, batch     0 | loss: 54.9760565
CurrentTrain: epoch  4, batch     1 | loss: 71.3398236
CurrentTrain: epoch  4, batch     2 | loss: 182.2382484
CurrentTrain: epoch  4, batch     3 | loss: 120.5094515
CurrentTrain: epoch  4, batch     4 | loss: 39.9220479
CurrentTrain: epoch  5, batch     0 | loss: 56.3898585
CurrentTrain: epoch  5, batch     1 | loss: 86.7493576
CurrentTrain: epoch  5, batch     2 | loss: 120.1526362
CurrentTrain: epoch  5, batch     3 | loss: 55.0890955
CurrentTrain: epoch  5, batch     4 | loss: 52.6062672
CurrentTrain: epoch  6, batch     0 | loss: 118.3580823
CurrentTrain: epoch  6, batch     1 | loss: 69.6132255
CurrentTrain: epoch  6, batch     2 | loss: 66.4079411
CurrentTrain: epoch  6, batch     3 | loss: 63.2047120
CurrentTrain: epoch  6, batch     4 | loss: 44.5371989
CurrentTrain: epoch  7, batch     0 | loss: 82.1084549
CurrentTrain: epoch  7, batch     1 | loss: 65.9451033
CurrentTrain: epoch  7, batch     2 | loss: 66.5353958
CurrentTrain: epoch  7, batch     3 | loss: 86.6010218
CurrentTrain: epoch  7, batch     4 | loss: 109.9979020
CurrentTrain: epoch  8, batch     0 | loss: 69.8788126
CurrentTrain: epoch  8, batch     1 | loss: 116.3064795
CurrentTrain: epoch  8, batch     2 | loss: 84.8650393
CurrentTrain: epoch  8, batch     3 | loss: 52.8217594
CurrentTrain: epoch  8, batch     4 | loss: 39.4457963
CurrentTrain: epoch  9, batch     0 | loss: 183.2001769
CurrentTrain: epoch  9, batch     1 | loss: 67.3149104
CurrentTrain: epoch  9, batch     2 | loss: 50.8316703
CurrentTrain: epoch  9, batch     3 | loss: 52.1683505
CurrentTrain: epoch  9, batch     4 | loss: 42.2613707
MemoryTrain:  epoch  0, batch     0 | loss: 0.3878284
MemoryTrain:  epoch  1, batch     0 | loss: 0.4394687
MemoryTrain:  epoch  2, batch     0 | loss: 0.2706592
MemoryTrain:  epoch  3, batch     0 | loss: 0.2161615
MemoryTrain:  epoch  4, batch     0 | loss: 0.1729743
MemoryTrain:  epoch  5, batch     0 | loss: 0.1192229
MemoryTrain:  epoch  6, batch     0 | loss: 0.1030366
MemoryTrain:  epoch  7, batch     0 | loss: 0.0743619
MemoryTrain:  epoch  8, batch     0 | loss: 0.0598813
MemoryTrain:  epoch  9, batch     0 | loss: 0.0471909

F1 score per class: {5: 0.9690721649484536, 6: 0.0, 10: 0.5714285714285714, 16: 0.8, 17: 0.0, 18: 0.5769230769230769}
Micro-average F1 score: 0.7444933920704846
Weighted-average F1 score: 0.7786728793041778
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 10: 0.7577639751552795, 16: 0.8679245283018868, 17: 0.0, 18: 0.8615384615384616}
Micro-average F1 score: 0.8473895582329317
Weighted-average F1 score: 0.8505020546004793
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 10: 0.7577639751552795, 16: 0.8679245283018868, 17: 0.0, 18: 0.8615384615384616}
Micro-average F1 score: 0.8473895582329317
Weighted-average F1 score: 0.8505020546004793

F1 score per class: {32: 0.9690721649484536, 5: 0.525, 6: 0.5594405594405595, 10: 0.8, 16: 0.0, 17: 0.5769230769230769, 18: 0.8764044943820225, 19: 0.34782608695652173, 24: 0.7586206896551724, 26: 0.9637305699481865, 29: 0.898989898989899}
Micro-average F1 score: 0.7695590327169275
Weighted-average F1 score: 0.776580292242156
F1 score per class: {32: 0.9850746268656716, 5: 0.6187845303867403, 6: 0.7393939393939394, 10: 0.8679245283018868, 16: 0.0, 17: 0.8484848484848485, 18: 0.907103825136612, 19: 0.6206896551724138, 24: 0.7657142857142857, 26: 0.9746192893401016, 29: 0.914572864321608}
Micro-average F1 score: 0.8244788164088769
Weighted-average F1 score: 0.8179936144992853
F1 score per class: {32: 0.9850746268656716, 5: 0.5780346820809249, 6: 0.7305389221556886, 10: 0.8679245283018868, 16: 0.0, 17: 0.8484848484848485, 18: 0.907103825136612, 19: 0.46153846153846156, 24: 0.7657142857142857, 26: 0.9746192893401016, 29: 0.9191919191919192}
Micro-average F1 score: 0.8145650708024275
Weighted-average F1 score: 0.8073046116121789
cur_acc:  ['0.8135', '0.7445']
his_acc:  ['0.8135', '0.7696']
cur_acc des:  ['0.8490', '0.8474']
his_acc des:  ['0.8490', '0.8245']
cur_acc rrf:  ['0.8467', '0.8474']
his_acc rrf:  ['0.8467', '0.8146']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 79.4963965
CurrentTrain: epoch  0, batch     1 | loss: 99.6577959
CurrentTrain: epoch  0, batch     2 | loss: 66.3826215
CurrentTrain: epoch  0, batch     3 | loss: 78.9024894
CurrentTrain: epoch  0, batch     4 | loss: 60.1766800
CurrentTrain: epoch  1, batch     0 | loss: 92.8475748
CurrentTrain: epoch  1, batch     1 | loss: 63.6968197
CurrentTrain: epoch  1, batch     2 | loss: 59.4643435
CurrentTrain: epoch  1, batch     3 | loss: 94.9130649
CurrentTrain: epoch  1, batch     4 | loss: 59.9800625
CurrentTrain: epoch  2, batch     0 | loss: 72.3665000
CurrentTrain: epoch  2, batch     1 | loss: 73.3539253
CurrentTrain: epoch  2, batch     2 | loss: 70.8969049
CurrentTrain: epoch  2, batch     3 | loss: 123.4024374
CurrentTrain: epoch  2, batch     4 | loss: 30.3880894
CurrentTrain: epoch  3, batch     0 | loss: 90.9162788
CurrentTrain: epoch  3, batch     1 | loss: 58.9707814
CurrentTrain: epoch  3, batch     2 | loss: 70.8025177
CurrentTrain: epoch  3, batch     3 | loss: 86.0686840
CurrentTrain: epoch  3, batch     4 | loss: 28.4841393
CurrentTrain: epoch  4, batch     0 | loss: 89.4922403
CurrentTrain: epoch  4, batch     1 | loss: 70.0331233
CurrentTrain: epoch  4, batch     2 | loss: 68.0813884
CurrentTrain: epoch  4, batch     3 | loss: 87.6555006
CurrentTrain: epoch  4, batch     4 | loss: 17.0483596
CurrentTrain: epoch  5, batch     0 | loss: 67.9217285
CurrentTrain: epoch  5, batch     1 | loss: 68.3228049
CurrentTrain: epoch  5, batch     2 | loss: 67.8954677
CurrentTrain: epoch  5, batch     3 | loss: 68.5727276
CurrentTrain: epoch  5, batch     4 | loss: 28.3374346
CurrentTrain: epoch  6, batch     0 | loss: 69.0503591
CurrentTrain: epoch  6, batch     1 | loss: 85.8753858
CurrentTrain: epoch  6, batch     2 | loss: 55.6773141
CurrentTrain: epoch  6, batch     3 | loss: 54.6978433
CurrentTrain: epoch  6, batch     4 | loss: 26.0813438
CurrentTrain: epoch  7, batch     0 | loss: 67.2917949
CurrentTrain: epoch  7, batch     1 | loss: 82.8807048
CurrentTrain: epoch  7, batch     2 | loss: 83.8071410
CurrentTrain: epoch  7, batch     3 | loss: 66.1083128
CurrentTrain: epoch  7, batch     4 | loss: 59.9343933
CurrentTrain: epoch  8, batch     0 | loss: 68.9136062
CurrentTrain: epoch  8, batch     1 | loss: 52.6908066
CurrentTrain: epoch  8, batch     2 | loss: 85.7417734
CurrentTrain: epoch  8, batch     3 | loss: 65.0158494
CurrentTrain: epoch  8, batch     4 | loss: 15.6050491
CurrentTrain: epoch  9, batch     0 | loss: 85.4929708
CurrentTrain: epoch  9, batch     1 | loss: 85.9520096
CurrentTrain: epoch  9, batch     2 | loss: 52.8324318
CurrentTrain: epoch  9, batch     3 | loss: 51.3232036
CurrentTrain: epoch  9, batch     4 | loss: 26.7735766
MemoryTrain:  epoch  0, batch     0 | loss: 0.6412884
MemoryTrain:  epoch  1, batch     0 | loss: 0.4747784
MemoryTrain:  epoch  2, batch     0 | loss: 0.3607402
MemoryTrain:  epoch  3, batch     0 | loss: 0.2366711
MemoryTrain:  epoch  4, batch     0 | loss: 0.2131651
MemoryTrain:  epoch  5, batch     0 | loss: 0.1737894
MemoryTrain:  epoch  6, batch     0 | loss: 0.1306437
MemoryTrain:  epoch  7, batch     0 | loss: 0.0943899
MemoryTrain:  epoch  8, batch     0 | loss: 0.0776901
MemoryTrain:  epoch  9, batch     0 | loss: 0.0766882

F1 score per class: {2: 0.9411764705882353, 39: 0.0, 10: 0.3238095238095238, 11: 0.48484848484848486, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.5454545454545454, 28: 0.25}
Micro-average F1 score: 0.4290657439446367
Weighted-average F1 score: 0.43836698315689915
F1 score per class: {2: 0.8888888888888888, 6: 0.0, 39: 0.0, 10: 0.592, 11: 0.7375, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.6666666666666666, 28: 0.5263157894736842}
Micro-average F1 score: 0.6195652173913043
Weighted-average F1 score: 0.5466593665371483
F1 score per class: {2: 0.8888888888888888, 39: 0.0, 10: 0.6666666666666666, 11: 0.7607361963190185, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.5882352941176471, 28: 0.25}
Micro-average F1 score: 0.657608695652174
Weighted-average F1 score: 0.6110995392516129

F1 score per class: {32: 0.9411764705882353, 2: 0.9743589743589743, 5: 0.5930232558139535, 6: 0.21238938053097345, 39: 0.2982456140350877, 10: 0.46715328467153283, 11: 0.7719298245614035, 12: 0.0, 16: 0.23255813953488372, 17: 0.8439306358381503, 18: 0.46153846153846156, 19: 0.7586206896551724, 24: 0.4, 26: 0.9417989417989417, 28: 0.9045226130653267, 29: 0.25}
Micro-average F1 score: 0.6838323353293413
Weighted-average F1 score: 0.7472355889634559
F1 score per class: {32: 0.8888888888888888, 2: 0.9900990099009901, 5: 0.676923076923077, 6: 0.42748091603053434, 39: 0.5401459854014599, 10: 0.686046511627907, 11: 0.8275862068965517, 12: 0.0, 16: 0.5833333333333334, 17: 0.8864864864864865, 18: 0.4864864864864865, 19: 0.7586206896551724, 24: 0.3125, 26: 0.9361702127659575, 28: 0.9035532994923858, 29: 0.5}
Micro-average F1 score: 0.745928338762215
Weighted-average F1 score: 0.756648319299679
F1 score per class: {32: 0.8888888888888888, 2: 0.9950248756218906, 5: 0.656084656084656, 6: 0.42748091603053434, 39: 0.5789473684210527, 10: 0.7005649717514124, 11: 0.8275862068965517, 12: 0.0, 16: 0.45614035087719296, 17: 0.8804347826086957, 18: 0.4375, 19: 0.7586206896551724, 24: 0.2777777777777778, 26: 0.9361702127659575, 28: 0.8979591836734694, 29: 0.23529411764705882}
Micro-average F1 score: 0.7389645776566758
Weighted-average F1 score: 0.7502706614369037
cur_acc:  ['0.8135', '0.7445', '0.4291']
his_acc:  ['0.8135', '0.7696', '0.6838']
cur_acc des:  ['0.8490', '0.8474', '0.6196']
his_acc des:  ['0.8490', '0.8245', '0.7459']
cur_acc rrf:  ['0.8467', '0.8474', '0.6576']
his_acc rrf:  ['0.8467', '0.8146', '0.7390']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 101.4595763
CurrentTrain: epoch  0, batch     1 | loss: 80.4894191
CurrentTrain: epoch  0, batch     2 | loss: 70.7265288
CurrentTrain: epoch  0, batch     3 | loss: 92.7687588
CurrentTrain: epoch  1, batch     0 | loss: 78.0636163
CurrentTrain: epoch  1, batch     1 | loss: 95.8580209
CurrentTrain: epoch  1, batch     2 | loss: 68.6016632
CurrentTrain: epoch  1, batch     3 | loss: 77.7580314
CurrentTrain: epoch  2, batch     0 | loss: 185.7417296
CurrentTrain: epoch  2, batch     1 | loss: 74.1879328
CurrentTrain: epoch  2, batch     2 | loss: 59.0910510
CurrentTrain: epoch  2, batch     3 | loss: 73.4270658
CurrentTrain: epoch  3, batch     0 | loss: 89.1942880
CurrentTrain: epoch  3, batch     1 | loss: 84.2728647
CurrentTrain: epoch  3, batch     2 | loss: 57.1906439
CurrentTrain: epoch  3, batch     3 | loss: 78.2942987
CurrentTrain: epoch  4, batch     0 | loss: 62.3780626
CurrentTrain: epoch  4, batch     1 | loss: 69.2601692
CurrentTrain: epoch  4, batch     2 | loss: 56.9129675
CurrentTrain: epoch  4, batch     3 | loss: 67.7527579
CurrentTrain: epoch  5, batch     0 | loss: 68.4339348
CurrentTrain: epoch  5, batch     1 | loss: 56.0453160
CurrentTrain: epoch  5, batch     2 | loss: 65.7910045
CurrentTrain: epoch  5, batch     3 | loss: 73.8556601
CurrentTrain: epoch  6, batch     0 | loss: 87.9023764
CurrentTrain: epoch  6, batch     1 | loss: 58.4037308
CurrentTrain: epoch  6, batch     2 | loss: 63.7891947
CurrentTrain: epoch  6, batch     3 | loss: 53.8078638
CurrentTrain: epoch  7, batch     0 | loss: 70.5788503
CurrentTrain: epoch  7, batch     1 | loss: 84.8540314
CurrentTrain: epoch  7, batch     2 | loss: 86.2963043
CurrentTrain: epoch  7, batch     3 | loss: 49.7023123
CurrentTrain: epoch  8, batch     0 | loss: 86.1734125
CurrentTrain: epoch  8, batch     1 | loss: 53.6716327
CurrentTrain: epoch  8, batch     2 | loss: 85.0081789
CurrentTrain: epoch  8, batch     3 | loss: 44.8334349
CurrentTrain: epoch  9, batch     0 | loss: 65.4039100
CurrentTrain: epoch  9, batch     1 | loss: 55.7694820
CurrentTrain: epoch  9, batch     2 | loss: 82.1437507
CurrentTrain: epoch  9, batch     3 | loss: 54.0917171
MemoryTrain:  epoch  0, batch     0 | loss: 0.4380203
MemoryTrain:  epoch  1, batch     0 | loss: 0.4245997
MemoryTrain:  epoch  2, batch     0 | loss: 0.2809081
MemoryTrain:  epoch  3, batch     0 | loss: 0.2085543
MemoryTrain:  epoch  4, batch     0 | loss: 0.1765777
MemoryTrain:  epoch  5, batch     0 | loss: 0.1414119
MemoryTrain:  epoch  6, batch     0 | loss: 0.1210877
MemoryTrain:  epoch  7, batch     0 | loss: 0.0953348
MemoryTrain:  epoch  8, batch     0 | loss: 0.0761327
MemoryTrain:  epoch  9, batch     0 | loss: 0.0668867

F1 score per class: {0: 0.9428571428571428, 32: 0.0, 2: 0.9130434782608695, 4: 0.0, 12: 0.5714285714285714, 13: 0.45, 21: 0.810126582278481, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7960199004975125
Weighted-average F1 score: 0.763334316834985
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.9473684210526315, 4: 0.0, 11: 0.0, 12: 0.3333333333333333, 13: 0.8076923076923077, 21: 0.8674698795180723, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.8618266978922716
Weighted-average F1 score: 0.8178444186827426
F1 score per class: {0: 0.9722222222222222, 32: 0.0, 2: 0.9417989417989417, 4: 0.0, 11: 0.0, 12: 0.5714285714285714, 13: 0.8076923076923077, 21: 0.8674698795180723, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8591549295774648
Weighted-average F1 score: 0.8115591983734457

F1 score per class: {0: 0.9295774647887324, 2: 0.5833333333333334, 4: 0.9130434782608695, 5: 0.9795918367346939, 6: 0.5977011494252874, 10: 0.336, 11: 0.21153846153846154, 12: 0.4233576642335766, 13: 0.08695652173913043, 16: 0.7931034482758621, 17: 0.0, 18: 0.2727272727272727, 19: 0.8409090909090909, 21: 0.29508196721311475, 23: 0.8, 24: 0.10526315789473684, 26: 0.7252747252747253, 28: 0.2222222222222222, 29: 0.9479166666666666, 32: 0.9128205128205128, 39: 0.25}
Micro-average F1 score: 0.6887104393008975
Weighted-average F1 score: 0.7283256628999093
F1 score per class: {0: 0.972972972972973, 2: 0.56, 4: 0.9473684210526315, 5: 0.9950248756218906, 6: 0.6990291262135923, 10: 0.6625, 11: 0.5109489051094891, 12: 0.7411764705882353, 13: 0.047619047619047616, 16: 0.8571428571428571, 17: 0.0, 18: 0.5352112676056338, 19: 0.8770053475935828, 21: 0.5, 23: 0.8470588235294118, 24: 0.18181818181818182, 26: 0.73224043715847, 28: 0.47058823529411764, 29: 0.9484536082474226, 32: 0.9128205128205128, 39: 0.75}
Micro-average F1 score: 0.7702127659574468
Weighted-average F1 score: 0.7643959003634552
F1 score per class: {0: 0.958904109589041, 2: 0.5384615384615384, 4: 0.9417989417989417, 5: 1.0, 6: 0.6733668341708543, 10: 0.5733333333333334, 11: 0.5655172413793104, 12: 0.7325581395348837, 13: 0.07407407407407407, 16: 0.8571428571428571, 17: 0.0, 18: 0.53125, 19: 0.8695652173913043, 21: 0.4827586206896552, 23: 0.8571428571428571, 24: 0.09523809523809523, 26: 0.73224043715847, 28: 0.4, 29: 0.9533678756476683, 32: 0.9128205128205128, 39: 0.25}
Micro-average F1 score: 0.7544910179640718
Weighted-average F1 score: 0.7471468998606697
cur_acc:  ['0.8135', '0.7445', '0.4291', '0.7960']
his_acc:  ['0.8135', '0.7696', '0.6838', '0.6887']
cur_acc des:  ['0.8490', '0.8474', '0.6196', '0.8618']
his_acc des:  ['0.8490', '0.8245', '0.7459', '0.7702']
cur_acc rrf:  ['0.8467', '0.8474', '0.6576', '0.8592']
his_acc rrf:  ['0.8467', '0.8146', '0.7390', '0.7545']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 78.7527885
CurrentTrain: epoch  0, batch     1 | loss: 82.3423455
CurrentTrain: epoch  0, batch     2 | loss: 75.0210394
CurrentTrain: epoch  0, batch     3 | loss: 90.1049536
CurrentTrain: epoch  1, batch     0 | loss: 71.2977831
CurrentTrain: epoch  1, batch     1 | loss: 77.5113680
CurrentTrain: epoch  1, batch     2 | loss: 60.2424309
CurrentTrain: epoch  1, batch     3 | loss: 39.1627451
CurrentTrain: epoch  2, batch     0 | loss: 63.6663260
CurrentTrain: epoch  2, batch     1 | loss: 72.5640640
CurrentTrain: epoch  2, batch     2 | loss: 68.6085745
CurrentTrain: epoch  2, batch     3 | loss: 82.8723956
CurrentTrain: epoch  3, batch     0 | loss: 56.8436555
CurrentTrain: epoch  3, batch     1 | loss: 56.0388606
CurrentTrain: epoch  3, batch     2 | loss: 87.7253220
CurrentTrain: epoch  3, batch     3 | loss: 56.0366885
CurrentTrain: epoch  4, batch     0 | loss: 51.7506989
CurrentTrain: epoch  4, batch     1 | loss: 63.9625498
CurrentTrain: epoch  4, batch     2 | loss: 86.2469146
CurrentTrain: epoch  4, batch     3 | loss: 83.7136919
CurrentTrain: epoch  5, batch     0 | loss: 83.0731531
CurrentTrain: epoch  5, batch     1 | loss: 53.7265438
CurrentTrain: epoch  5, batch     2 | loss: 84.8350294
CurrentTrain: epoch  5, batch     3 | loss: 43.5033182
CurrentTrain: epoch  6, batch     0 | loss: 82.5094989
CurrentTrain: epoch  6, batch     1 | loss: 51.9718207
CurrentTrain: epoch  6, batch     2 | loss: 84.7774565
CurrentTrain: epoch  6, batch     3 | loss: 56.8729512
CurrentTrain: epoch  7, batch     0 | loss: 68.4901726
CurrentTrain: epoch  7, batch     1 | loss: 53.1108314
CurrentTrain: epoch  7, batch     2 | loss: 62.0565994
CurrentTrain: epoch  7, batch     3 | loss: 57.0652108
CurrentTrain: epoch  8, batch     0 | loss: 52.9087485
CurrentTrain: epoch  8, batch     1 | loss: 63.1948755
CurrentTrain: epoch  8, batch     2 | loss: 81.8433150
CurrentTrain: epoch  8, batch     3 | loss: 78.1276377
CurrentTrain: epoch  9, batch     0 | loss: 61.1645913
CurrentTrain: epoch  9, batch     1 | loss: 51.5783003
CurrentTrain: epoch  9, batch     2 | loss: 85.5262068
CurrentTrain: epoch  9, batch     3 | loss: 58.1064587
MemoryTrain:  epoch  0, batch     0 | loss: 0.3527764
MemoryTrain:  epoch  1, batch     0 | loss: 0.3129069
MemoryTrain:  epoch  2, batch     0 | loss: 0.2205106
MemoryTrain:  epoch  3, batch     0 | loss: 0.1851684
MemoryTrain:  epoch  4, batch     0 | loss: 0.1520140
MemoryTrain:  epoch  5, batch     0 | loss: 0.1203253
MemoryTrain:  epoch  6, batch     0 | loss: 0.0915215
MemoryTrain:  epoch  7, batch     0 | loss: 0.0784458
MemoryTrain:  epoch  8, batch     0 | loss: 0.0701145
MemoryTrain:  epoch  9, batch     0 | loss: 0.0624942

F1 score per class: {35: 0.0, 5: 0.0, 37: 0.0, 38: 0.8888888888888888, 10: 0.0, 13: 0.0, 15: 0.42424242424242425, 18: 0.7619047619047619, 23: 0.651685393258427, 25: 0.7272727272727273}
Micro-average F1 score: 0.5963855421686747
Weighted-average F1 score: 0.5246458185818337
F1 score per class: {35: 0.0, 5: 0.0, 37: 0.0, 38: 0.0, 10: 0.75, 11: 0.0, 13: 0.0, 15: 0.0, 18: 0.0, 21: 0.5945945945945946, 23: 0.9484536082474226, 24: 0.7096774193548387, 25: 0.88}
Micro-average F1 score: 0.6916890080428955
Weighted-average F1 score: 0.6045889645300709
F1 score per class: {35: 0.0, 5: 0.0, 37: 0.0, 38: 0.0, 10: 0.8235294117647058, 11: 0.0, 13: 0.0, 15: 0.0, 18: 0.5945945945945946, 21: 0.9484536082474226, 23: 0.7096774193548387, 25: 0.9230769230769231}
Micro-average F1 score: 0.7096774193548387
Weighted-average F1 score: 0.6318587949436919

F1 score per class: {0: 0.9295774647887324, 2: 0.5, 4: 0.8700564971751412, 5: 0.8687782805429864, 6: 0.5679012345679012, 10: 0.3089430894308943, 11: 0.044444444444444446, 12: 0.36923076923076925, 13: 0.1111111111111111, 15: 0.5925925925925926, 16: 0.8333333333333334, 17: 0.08333333333333333, 18: 0.19047619047619047, 19: 0.6533333333333333, 21: 0.32558139534883723, 23: 0.6842105263157895, 24: 0.10526315789473684, 25: 0.42424242424242425, 26: 0.7362637362637363, 28: 0.5454545454545454, 29: 0.9368421052631579, 32: 0.8673469387755102, 35: 0.7441860465116279, 37: 0.40559440559440557, 38: 0.42105263157894735, 39: 0.0}
Micro-average F1 score: 0.6238231682357757
Weighted-average F1 score: 0.6733372646712494
F1 score per class: {0: 0.972972972972973, 2: 0.5, 4: 0.93048128342246, 5: 0.8695652173913043, 6: 0.6914893617021277, 10: 0.5241379310344828, 11: 0.10309278350515463, 12: 0.7485380116959064, 13: 0.11764705882352941, 15: 0.5217391304347826, 16: 0.8571428571428571, 17: 0.0, 18: 0.4230769230769231, 19: 0.8160919540229885, 21: 0.4819277108433735, 23: 0.7560975609756098, 24: 0.24, 25: 0.5945945945945946, 26: 0.7391304347826086, 28: 0.36363636363636365, 29: 0.9479166666666666, 32: 0.8756218905472637, 35: 0.9108910891089109, 37: 0.36666666666666664, 38: 0.4943820224719101, 39: 0.25}
Micro-average F1 score: 0.6947291361639825
Weighted-average F1 score: 0.7030310516599324
F1 score per class: {0: 0.96, 2: 0.45161290322580644, 4: 0.9130434782608695, 5: 0.8733624454148472, 6: 0.6777777777777778, 10: 0.46715328467153283, 11: 0.10309278350515463, 12: 0.7602339181286549, 13: 0.11428571428571428, 15: 0.56, 16: 0.8571428571428571, 17: 0.1111111111111111, 18: 0.375, 19: 0.8092485549132948, 21: 0.47619047619047616, 23: 0.759493670886076, 24: 0.09523809523809523, 25: 0.5866666666666667, 26: 0.745945945945946, 28: 0.4444444444444444, 29: 0.9479166666666666, 32: 0.88, 35: 0.9108910891089109, 37: 0.358695652173913, 38: 0.48484848484848486, 39: 0.13333333333333333}
Micro-average F1 score: 0.6877528503126149
Weighted-average F1 score: 0.6984125862345285
cur_acc:  ['0.8135', '0.7445', '0.4291', '0.7960', '0.5964']
his_acc:  ['0.8135', '0.7696', '0.6838', '0.6887', '0.6238']
cur_acc des:  ['0.8490', '0.8474', '0.6196', '0.8618', '0.6917']
his_acc des:  ['0.8490', '0.8245', '0.7459', '0.7702', '0.6947']
cur_acc rrf:  ['0.8467', '0.8474', '0.6576', '0.8592', '0.7097']
his_acc rrf:  ['0.8467', '0.8146', '0.7390', '0.7545', '0.6878']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 74.2721354
CurrentTrain: epoch  0, batch     1 | loss: 95.9848141
CurrentTrain: epoch  0, batch     2 | loss: 78.1856636
CurrentTrain: epoch  0, batch     3 | loss: 48.2434007
CurrentTrain: epoch  1, batch     0 | loss: 91.1043172
CurrentTrain: epoch  1, batch     1 | loss: 121.9399040
CurrentTrain: epoch  1, batch     2 | loss: 57.3216284
CurrentTrain: epoch  1, batch     3 | loss: 44.2196511
CurrentTrain: epoch  2, batch     0 | loss: 73.0486644
CurrentTrain: epoch  2, batch     1 | loss: 69.1723654
CurrentTrain: epoch  2, batch     2 | loss: 67.7601470
CurrentTrain: epoch  2, batch     3 | loss: 42.9381126
CurrentTrain: epoch  3, batch     0 | loss: 58.0203465
CurrentTrain: epoch  3, batch     1 | loss: 67.6836554
CurrentTrain: epoch  3, batch     2 | loss: 54.3550688
CurrentTrain: epoch  3, batch     3 | loss: 54.0291992
CurrentTrain: epoch  4, batch     0 | loss: 69.1749292
CurrentTrain: epoch  4, batch     1 | loss: 86.6000473
CurrentTrain: epoch  4, batch     2 | loss: 63.2665406
CurrentTrain: epoch  4, batch     3 | loss: 40.0068697
CurrentTrain: epoch  5, batch     0 | loss: 67.6254365
CurrentTrain: epoch  5, batch     1 | loss: 86.4785603
CurrentTrain: epoch  5, batch     2 | loss: 48.8667322
CurrentTrain: epoch  5, batch     3 | loss: 52.2366875
CurrentTrain: epoch  6, batch     0 | loss: 66.0711266
CurrentTrain: epoch  6, batch     1 | loss: 80.8884553
CurrentTrain: epoch  6, batch     2 | loss: 81.3856394
CurrentTrain: epoch  6, batch     3 | loss: 49.6286463
CurrentTrain: epoch  7, batch     0 | loss: 66.1439712
CurrentTrain: epoch  7, batch     1 | loss: 52.7547652
CurrentTrain: epoch  7, batch     2 | loss: 65.0957294
CurrentTrain: epoch  7, batch     3 | loss: 38.8115664
CurrentTrain: epoch  8, batch     0 | loss: 50.5386897
CurrentTrain: epoch  8, batch     1 | loss: 115.9705131
CurrentTrain: epoch  8, batch     2 | loss: 52.8731164
CurrentTrain: epoch  8, batch     3 | loss: 37.8662497
CurrentTrain: epoch  9, batch     0 | loss: 63.7590927
CurrentTrain: epoch  9, batch     1 | loss: 81.4990863
CurrentTrain: epoch  9, batch     2 | loss: 61.3228783
CurrentTrain: epoch  9, batch     3 | loss: 69.4871934
MemoryTrain:  epoch  0, batch     0 | loss: 0.1509713
MemoryTrain:  epoch  1, batch     0 | loss: 0.1802469
MemoryTrain:  epoch  2, batch     0 | loss: 0.1330340
MemoryTrain:  epoch  3, batch     0 | loss: 0.1199166
MemoryTrain:  epoch  4, batch     0 | loss: 0.0926867
MemoryTrain:  epoch  5, batch     0 | loss: 0.0816930
MemoryTrain:  epoch  6, batch     0 | loss: 0.0727486
MemoryTrain:  epoch  7, batch     0 | loss: 0.0659844
MemoryTrain:  epoch  8, batch     0 | loss: 0.0567587
MemoryTrain:  epoch  9, batch     0 | loss: 0.0535330

F1 score per class: {33: 0.0, 36: 0.4423076923076923, 5: 0.0, 38: 0.0, 39: 0.0, 8: 0.0, 37: 0.8387096774193549, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.0, 20: 0.9444444444444444, 21: 0.42857142857142855, 26: 0.2631578947368421, 28: 0.0, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5212464589235127
Weighted-average F1 score: 0.5152098617300822
F1 score per class: {5: 0.0, 6: 0.0, 8: 0.6721311475409836, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.9411764705882353, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.42857142857142855, 35: 0.0, 36: 0.8666666666666667, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7534883720930232
Weighted-average F1 score: 0.695141082596198
F1 score per class: {5: 0.0, 6: 0.0, 8: 0.6721311475409836, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.9411764705882353, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.42857142857142855, 35: 0.0, 36: 0.6990291262135923, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.6903073286052009
Weighted-average F1 score: 0.6114435057632622

F1 score per class: {0: 0.9142857142857143, 2: 0.4, 4: 0.907103825136612, 5: 0.8521739130434782, 6: 0.4935064935064935, 8: 0.35658914728682173, 10: 0.21238938053097345, 11: 0.12, 12: 0.34108527131782945, 13: 0.16, 15: 0.6666666666666666, 16: 0.847457627118644, 17: 0.08695652173913043, 18: 0.2608695652173913, 19: 0.7169811320754716, 20: 0.8297872340425532, 21: 0.26666666666666666, 23: 0.6933333333333334, 24: 0.10526315789473684, 25: 0.47058823529411764, 26: 0.723404255319149, 28: 0.3076923076923077, 29: 0.9319371727748691, 30: 0.9444444444444444, 32: 0.8783068783068783, 33: 0.24, 35: 0.691358024691358, 36: 0.25, 37: 0.43243243243243246, 38: 0.4727272727272727, 39: 0.0}
Micro-average F1 score: 0.6119186046511628
Weighted-average F1 score: 0.6735041182654247
F1 score per class: {0: 0.958904109589041, 2: 0.5, 4: 0.9361702127659575, 5: 0.8583690987124464, 6: 0.6480446927374302, 8: 0.5290322580645161, 10: 0.5255474452554745, 11: 0.1523809523809524, 12: 0.6503067484662577, 13: 0.08333333333333333, 15: 0.5454545454545454, 16: 0.8387096774193549, 17: 0.16, 18: 0.38596491228070173, 19: 0.8202247191011236, 20: 0.9230769230769231, 21: 0.4864864864864865, 23: 0.7560975609756098, 24: 0.23076923076923078, 25: 0.631578947368421, 26: 0.7391304347826086, 28: 0.3333333333333333, 29: 0.9375, 30: 0.9230769230769231, 32: 0.8787878787878788, 33: 0.15, 35: 0.9019607843137255, 36: 0.611764705882353, 37: 0.3486238532110092, 38: 0.5063291139240507, 39: 0.1}
Micro-average F1 score: 0.6842439644218552
Weighted-average F1 score: 0.6941416500926294
F1 score per class: {0: 0.958904109589041, 2: 0.5, 4: 0.9361702127659575, 5: 0.8583690987124464, 6: 0.6242774566473989, 8: 0.5061728395061729, 10: 0.421875, 11: 0.14545454545454545, 12: 0.6545454545454545, 13: 0.14285714285714285, 15: 0.56, 16: 0.8387096774193549, 17: 0.15384615384615385, 18: 0.38596491228070173, 19: 0.8089887640449438, 20: 0.9320388349514563, 21: 0.5142857142857142, 23: 0.7654320987654321, 24: 0.18181818181818182, 25: 0.6493506493506493, 26: 0.7379679144385026, 28: 0.3157894736842105, 29: 0.9319371727748691, 30: 0.9, 32: 0.883248730964467, 33: 0.13953488372093023, 35: 0.9019607843137255, 36: 0.5806451612903226, 37: 0.43478260869565216, 38: 0.49382716049382713, 39: 0.11764705882352941}
Micro-average F1 score: 0.6777493606138107
Weighted-average F1 score: 0.6883181779727278
cur_acc:  ['0.8135', '0.7445', '0.4291', '0.7960', '0.5964', '0.5212']
his_acc:  ['0.8135', '0.7696', '0.6838', '0.6887', '0.6238', '0.6119']
cur_acc des:  ['0.8490', '0.8474', '0.6196', '0.8618', '0.6917', '0.7535']
his_acc des:  ['0.8490', '0.8245', '0.7459', '0.7702', '0.6947', '0.6842']
cur_acc rrf:  ['0.8467', '0.8474', '0.6576', '0.8592', '0.7097', '0.6903']
his_acc rrf:  ['0.8467', '0.8146', '0.7390', '0.7545', '0.6878', '0.6777']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 72.9580747
CurrentTrain: epoch  0, batch     1 | loss: 69.3598370
CurrentTrain: epoch  0, batch     2 | loss: 63.5877516
CurrentTrain: epoch  0, batch     3 | loss: 10.6427656
CurrentTrain: epoch  1, batch     0 | loss: 59.9607243
CurrentTrain: epoch  1, batch     1 | loss: 94.7074155
CurrentTrain: epoch  1, batch     2 | loss: 62.7874641
CurrentTrain: epoch  1, batch     3 | loss: 10.0178692
CurrentTrain: epoch  2, batch     0 | loss: 90.4016108
CurrentTrain: epoch  2, batch     1 | loss: 84.9388958
CurrentTrain: epoch  2, batch     2 | loss: 56.7166736
CurrentTrain: epoch  2, batch     3 | loss: 12.1053133
CurrentTrain: epoch  3, batch     0 | loss: 66.1488741
CurrentTrain: epoch  3, batch     1 | loss: 68.5106181
CurrentTrain: epoch  3, batch     2 | loss: 54.1116933
CurrentTrain: epoch  3, batch     3 | loss: 6.2219125
CurrentTrain: epoch  4, batch     0 | loss: 85.7222093
CurrentTrain: epoch  4, batch     1 | loss: 55.0547629
CurrentTrain: epoch  4, batch     2 | loss: 52.8265518
CurrentTrain: epoch  4, batch     3 | loss: 11.4695542
CurrentTrain: epoch  5, batch     0 | loss: 64.2455866
CurrentTrain: epoch  5, batch     1 | loss: 64.1232583
CurrentTrain: epoch  5, batch     2 | loss: 65.8140303
CurrentTrain: epoch  5, batch     3 | loss: 3.4105478
CurrentTrain: epoch  6, batch     0 | loss: 53.4143509
CurrentTrain: epoch  6, batch     1 | loss: 82.7904769
CurrentTrain: epoch  6, batch     2 | loss: 60.8450255
CurrentTrain: epoch  6, batch     3 | loss: 10.9525106
CurrentTrain: epoch  7, batch     0 | loss: 49.7590102
CurrentTrain: epoch  7, batch     1 | loss: 66.1726812
CurrentTrain: epoch  7, batch     2 | loss: 64.1468957
CurrentTrain: epoch  7, batch     3 | loss: 11.0531441
CurrentTrain: epoch  8, batch     0 | loss: 48.2941804
CurrentTrain: epoch  8, batch     1 | loss: 65.1714167
CurrentTrain: epoch  8, batch     2 | loss: 53.0986823
CurrentTrain: epoch  8, batch     3 | loss: 27.5343141
CurrentTrain: epoch  9, batch     0 | loss: 62.9716691
CurrentTrain: epoch  9, batch     1 | loss: 48.2541186
CurrentTrain: epoch  9, batch     2 | loss: 65.9615194
CurrentTrain: epoch  9, batch     3 | loss: 27.4202418
MemoryTrain:  epoch  0, batch     0 | loss: 0.3101548
MemoryTrain:  epoch  1, batch     0 | loss: 0.2474572
MemoryTrain:  epoch  2, batch     0 | loss: 0.1709446
MemoryTrain:  epoch  3, batch     0 | loss: 0.1226340
MemoryTrain:  epoch  4, batch     0 | loss: 0.1092785
MemoryTrain:  epoch  5, batch     0 | loss: 0.1016964
MemoryTrain:  epoch  6, batch     0 | loss: 0.0856615
MemoryTrain:  epoch  7, batch     0 | loss: 0.0736784
MemoryTrain:  epoch  8, batch     0 | loss: 0.0666491
MemoryTrain:  epoch  9, batch     0 | loss: 0.0594491

F1 score per class: {35: 0.0, 6: 0.0, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.6666666666666666, 26: 0.6666666666666666, 27: 0.0, 31: 0.5773195876288659}
Micro-average F1 score: 0.5714285714285714
Weighted-average F1 score: 0.4781754559495703
F1 score per class: {0: 0.0, 35: 0.0, 6: 0.0, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.72, 26: 0.0, 27: 1.0, 28: 0.0, 31: 0.8205128205128205}
Micro-average F1 score: 0.7466666666666667
Weighted-average F1 score: 0.6788732624026741
F1 score per class: {0: 0.0, 35: 0.0, 6: 0.0, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.7692307692307693, 26: 1.0, 27: 0.0, 31: 0.8205128205128205}
Micro-average F1 score: 0.7623318385650224
Weighted-average F1 score: 0.7038067167586864

F1 score per class: {0: 0.9142857142857143, 2: 0.48, 4: 0.8636363636363636, 5: 0.8646288209606987, 6: 0.373134328358209, 7: 0.0, 8: 0.3106796116504854, 9: 0.9803921568627451, 10: 0.09523809523809523, 11: 0.13725490196078433, 12: 0.3064516129032258, 13: 0.19047619047619047, 15: 0.7368421052631579, 16: 0.8275862068965517, 17: 0.0, 18: 0.21739130434782608, 19: 0.6185567010309279, 20: 0.8, 21: 0.20512820512820512, 23: 0.6197183098591549, 24: 0.10526315789473684, 25: 0.5142857142857142, 26: 0.7120418848167539, 27: 0.41025641025641024, 28: 0.3333333333333333, 29: 0.9206349206349206, 30: 0.9142857142857143, 31: 0.4, 32: 0.8432432432432433, 33: 0.25, 35: 0.5753424657534246, 36: 0.21333333333333335, 37: 0.4144144144144144, 38: 0.2, 39: 0.0, 40: 0.49557522123893805}
Micro-average F1 score: 0.5749656121045392
Weighted-average F1 score: 0.6433464002484086
F1 score per class: {0: 0.9315068493150684, 2: 0.4827586206896552, 4: 0.9247311827956989, 5: 0.8658008658008658, 6: 0.52, 7: 0.0, 8: 0.4925373134328358, 9: 0.9803921568627451, 10: 0.5285714285714286, 11: 0.16666666666666666, 12: 0.527027027027027, 13: 0.08, 15: 0.5454545454545454, 16: 0.8571428571428571, 17: 0.13333333333333333, 18: 0.4067796610169492, 19: 0.6190476190476191, 20: 0.9215686274509803, 21: 0.6071428571428571, 23: 0.6666666666666666, 24: 0.15384615384615385, 25: 0.6493506493506493, 26: 0.7120418848167539, 27: 0.46153846153846156, 28: 0.25, 29: 0.9319371727748691, 30: 0.9473684210526315, 31: 0.6666666666666666, 32: 0.875, 33: 0.11764705882352941, 35: 0.8712871287128713, 36: 0.6901408450704225, 37: 0.3448275862068966, 38: 0.5507246376811594, 39: 0.1, 40: 0.5925925925925926}
Micro-average F1 score: 0.6477102663873092
Weighted-average F1 score: 0.6491857062614179
F1 score per class: {0: 0.9315068493150684, 2: 0.4666666666666667, 4: 0.918918918918919, 5: 0.8771929824561403, 6: 0.5, 7: 0.0, 8: 0.5174825174825175, 9: 0.9803921568627451, 10: 0.40310077519379844, 11: 0.14545454545454545, 12: 0.527027027027027, 13: 0.16666666666666666, 15: 0.56, 16: 0.8571428571428571, 17: 0.125, 18: 0.32142857142857145, 19: 0.6190476190476191, 20: 0.9215686274509803, 21: 0.5098039215686274, 23: 0.6666666666666666, 24: 0.1, 25: 0.6493506493506493, 26: 0.7120418848167539, 27: 0.46511627906976744, 28: 0.36363636363636365, 29: 0.9319371727748691, 30: 0.9473684210526315, 31: 0.6666666666666666, 32: 0.8808290155440415, 33: 0.13333333333333333, 35: 0.8712871287128713, 36: 0.5094339622641509, 37: 0.3724137931034483, 38: 0.5135135135135135, 39: 0.1111111111111111, 40: 0.5925925925925926}
Micro-average F1 score: 0.6360338573155986
Weighted-average F1 score: 0.6436205433843435
cur_acc:  ['0.8135', '0.7445', '0.4291', '0.7960', '0.5964', '0.5212', '0.5714']
his_acc:  ['0.8135', '0.7696', '0.6838', '0.6887', '0.6238', '0.6119', '0.5750']
cur_acc des:  ['0.8490', '0.8474', '0.6196', '0.8618', '0.6917', '0.7535', '0.7467']
his_acc des:  ['0.8490', '0.8245', '0.7459', '0.7702', '0.6947', '0.6842', '0.6477']
cur_acc rrf:  ['0.8467', '0.8474', '0.6576', '0.8592', '0.7097', '0.6903', '0.7623']
his_acc rrf:  ['0.8467', '0.8146', '0.7390', '0.7545', '0.6878', '0.6777', '0.6360']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 97.3359216
CurrentTrain: epoch  0, batch     1 | loss: 81.4675887
CurrentTrain: epoch  0, batch     2 | loss: 83.5872476
CurrentTrain: epoch  0, batch     3 | loss: 68.3756772
CurrentTrain: epoch  0, batch     4 | loss: 110.4019087
CurrentTrain: epoch  1, batch     0 | loss: 76.3656917
CurrentTrain: epoch  1, batch     1 | loss: 91.5403359
CurrentTrain: epoch  1, batch     2 | loss: 92.5599008
CurrentTrain: epoch  1, batch     3 | loss: 91.1724782
CurrentTrain: epoch  1, batch     4 | loss: 51.5894987
CurrentTrain: epoch  2, batch     0 | loss: 73.4241969
CurrentTrain: epoch  2, batch     1 | loss: 56.8352349
CurrentTrain: epoch  2, batch     2 | loss: 68.2774979
CurrentTrain: epoch  2, batch     3 | loss: 74.1773676
CurrentTrain: epoch  2, batch     4 | loss: 214.4150203
CurrentTrain: epoch  3, batch     0 | loss: 89.9562750
CurrentTrain: epoch  3, batch     1 | loss: 63.7248232
CurrentTrain: epoch  3, batch     2 | loss: 119.4377238
CurrentTrain: epoch  3, batch     3 | loss: 86.6076257
CurrentTrain: epoch  3, batch     4 | loss: 38.3916124
CurrentTrain: epoch  4, batch     0 | loss: 68.5805257
CurrentTrain: epoch  4, batch     1 | loss: 70.0615405
CurrentTrain: epoch  4, batch     2 | loss: 67.8287967
CurrentTrain: epoch  4, batch     3 | loss: 87.3469129
CurrentTrain: epoch  4, batch     4 | loss: 48.5720506
CurrentTrain: epoch  5, batch     0 | loss: 121.5644002
CurrentTrain: epoch  5, batch     1 | loss: 83.3501585
CurrentTrain: epoch  5, batch     2 | loss: 64.4275047
CurrentTrain: epoch  5, batch     3 | loss: 64.4564285
CurrentTrain: epoch  5, batch     4 | loss: 103.7036895
CurrentTrain: epoch  6, batch     0 | loss: 53.7931207
CurrentTrain: epoch  6, batch     1 | loss: 66.0002326
CurrentTrain: epoch  6, batch     2 | loss: 66.5869251
CurrentTrain: epoch  6, batch     3 | loss: 86.3609732
CurrentTrain: epoch  6, batch     4 | loss: 65.8641001
CurrentTrain: epoch  7, batch     0 | loss: 86.6784913
CurrentTrain: epoch  7, batch     1 | loss: 64.3010951
CurrentTrain: epoch  7, batch     2 | loss: 68.2915206
CurrentTrain: epoch  7, batch     3 | loss: 68.0701880
CurrentTrain: epoch  7, batch     4 | loss: 45.0389896
CurrentTrain: epoch  8, batch     0 | loss: 65.2494998
CurrentTrain: epoch  8, batch     1 | loss: 54.3728536
CurrentTrain: epoch  8, batch     2 | loss: 119.7364331
CurrentTrain: epoch  8, batch     3 | loss: 62.4110840
CurrentTrain: epoch  8, batch     4 | loss: 103.0884518
CurrentTrain: epoch  9, batch     0 | loss: 84.4968608
CurrentTrain: epoch  9, batch     1 | loss: 65.2143916
CurrentTrain: epoch  9, batch     2 | loss: 66.2031112
CurrentTrain: epoch  9, batch     3 | loss: 52.7672157
CurrentTrain: epoch  9, batch     4 | loss: 63.7902022
MemoryTrain:  epoch  0, batch     0 | loss: 0.4339129
MemoryTrain:  epoch  1, batch     0 | loss: 0.3488406
MemoryTrain:  epoch  2, batch     0 | loss: 0.2581135
MemoryTrain:  epoch  3, batch     0 | loss: 0.2322364
MemoryTrain:  epoch  4, batch     0 | loss: 0.1937525
MemoryTrain:  epoch  5, batch     0 | loss: 0.1666097
MemoryTrain:  epoch  6, batch     0 | loss: 0.1386285
MemoryTrain:  epoch  7, batch     0 | loss: 0.1170546
MemoryTrain:  epoch  8, batch     0 | loss: 0.1073952
MemoryTrain:  epoch  9, batch     0 | loss: 0.0896215

F1 score per class: {32: 0.29310344827586204, 1: 0.6333333333333333, 34: 0.0, 3: 0.2, 35: 0.0, 37: 0.0, 11: 0.4626865671641791, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.625, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.41567291311754684
Weighted-average F1 score: 0.35297728781192955
F1 score per class: {0: 0.0, 1: 0.41509433962264153, 3: 0.9066666666666666, 10: 0.0, 11: 0.0, 14: 0.3119266055045872, 18: 0.0, 21: 0.0, 22: 0.5637583892617449, 23: 0.0, 24: 0.0, 27: 0.0, 32: 0.0, 33: 0.0, 34: 0.9215686274509803, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.5842026825633383
Weighted-average F1 score: 0.5274012786971777
F1 score per class: {0: 0.0, 1: 0.36893203883495146, 3: 0.9139072847682119, 10: 0.0, 11: 0.0, 14: 0.2909090909090909, 18: 0.0, 21: 0.0, 22: 0.5578231292517006, 23: 0.0, 24: 0.0, 27: 0.0, 32: 0.0, 33: 0.0, 34: 0.8775510204081632, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5671191553544495
Weighted-average F1 score: 0.508570657050894

F1 score per class: {0: 0.9142857142857143, 1: 0.26356589147286824, 2: 0.631578947368421, 3: 0.6129032258064516, 4: 0.8165680473372781, 5: 0.8878923766816144, 6: 0.3333333333333333, 7: 0.0, 8: 0.20833333333333334, 9: 0.9803921568627451, 10: 0.11320754716981132, 11: 0.04395604395604396, 12: 0.14678899082568808, 13: 0.10526315789473684, 14: 0.18333333333333332, 15: 0.7058823529411765, 16: 0.7931034482758621, 17: 0.0, 18: 0.13636363636363635, 19: 0.2903225806451613, 20: 0.7865168539325843, 21: 0.1, 22: 0.4460431654676259, 23: 0.6486486486486487, 24: 0.08, 25: 0.47058823529411764, 26: 0.71875, 27: 0.2727272727272727, 28: 0.4, 29: 0.9032258064516129, 30: 0.9444444444444444, 31: 0.0, 32: 0.8306010928961749, 33: 0.3157894736842105, 34: 0.31645569620253167, 35: 0.5, 36: 0.3614457831325301, 37: 0.2926829268292683, 38: 0.15789473684210525, 39: 0.0, 40: 0.603448275862069}
Micro-average F1 score: 0.5080668817835142
Weighted-average F1 score: 0.5656765379060562
F1 score per class: {0: 0.9473684210526315, 1: 0.37606837606837606, 2: 0.4, 3: 0.8717948717948718, 4: 0.8950276243093923, 5: 0.8620689655172413, 6: 0.4794520547945205, 7: 0.0392156862745098, 8: 0.4794520547945205, 9: 0.9803921568627451, 10: 0.44776119402985076, 11: 0.021739130434782608, 12: 0.6103896103896104, 13: 0.09090909090909091, 14: 0.2833333333333333, 15: 0.6666666666666666, 16: 0.84375, 17: 0.13333333333333333, 18: 0.13333333333333333, 19: 0.3025210084033613, 20: 0.9108910891089109, 21: 0.18518518518518517, 22: 0.535031847133758, 23: 0.725, 24: 0.08695652173913043, 25: 0.631578947368421, 26: 0.7120418848167539, 27: 0.12121212121212122, 28: 0.3333333333333333, 29: 0.9148936170212766, 30: 0.9444444444444444, 31: 1.0, 32: 0.8691099476439791, 33: 0.2222222222222222, 34: 0.4177777777777778, 35: 0.819047619047619, 36: 0.6779661016949152, 37: 0.2716049382716049, 38: 0.41025641025641024, 39: 0.10526315789473684, 40: 0.6666666666666666}
Micro-average F1 score: 0.5996971226653205
Weighted-average F1 score: 0.6140145656013335
F1 score per class: {0: 0.9473684210526315, 1: 0.33043478260869563, 2: 0.36363636363636365, 3: 0.8734177215189873, 4: 0.8888888888888888, 5: 0.8620689655172413, 6: 0.39416058394160586, 7: 0.03571428571428571, 8: 0.4722222222222222, 9: 0.9803921568627451, 10: 0.3937007874015748, 11: 0.02197802197802198, 12: 0.5540540540540541, 13: 0.07407407407407407, 14: 0.2601626016260163, 15: 0.631578947368421, 16: 0.8387096774193549, 17: 0.125, 18: 0.13333333333333333, 19: 0.32786885245901637, 20: 0.9019607843137255, 21: 0.1568627450980392, 22: 0.5324675324675324, 23: 0.6923076923076923, 24: 0.08333333333333333, 25: 0.631578947368421, 26: 0.7120418848167539, 27: 0.12121212121212122, 28: 0.36363636363636365, 29: 0.9206349206349206, 30: 0.9444444444444444, 31: 0.6666666666666666, 32: 0.8571428571428571, 33: 0.23076923076923078, 34: 0.36909871244635195, 35: 0.7878787878787878, 36: 0.5490196078431373, 37: 0.3111111111111111, 38: 0.3902439024390244, 39: 0.0, 40: 0.6707317073170732}
Micro-average F1 score: 0.5801682385929136
Weighted-average F1 score: 0.5933662137726088
cur_acc:  ['0.8135', '0.7445', '0.4291', '0.7960', '0.5964', '0.5212', '0.5714', '0.4157']
his_acc:  ['0.8135', '0.7696', '0.6838', '0.6887', '0.6238', '0.6119', '0.5750', '0.5081']
cur_acc des:  ['0.8490', '0.8474', '0.6196', '0.8618', '0.6917', '0.7535', '0.7467', '0.5842']
his_acc des:  ['0.8490', '0.8245', '0.7459', '0.7702', '0.6947', '0.6842', '0.6477', '0.5997']
cur_acc rrf:  ['0.8467', '0.8474', '0.6576', '0.8592', '0.7097', '0.6903', '0.7623', '0.5671']
his_acc rrf:  ['0.8467', '0.8146', '0.7390', '0.7545', '0.6878', '0.6777', '0.6360', '0.5802']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 57.6563662
CurrentTrain: epoch  0, batch     1 | loss: 67.5602620
CurrentTrain: epoch  0, batch     2 | loss: 79.2540761
CurrentTrain: epoch  0, batch     3 | loss: 97.6035758
CurrentTrain: epoch  0, batch     4 | loss: 78.4590832
CurrentTrain: epoch  0, batch     5 | loss: 66.0597701
CurrentTrain: epoch  0, batch     6 | loss: 97.0270390
CurrentTrain: epoch  0, batch     7 | loss: 66.5816107
CurrentTrain: epoch  0, batch     8 | loss: 96.6278336
CurrentTrain: epoch  0, batch     9 | loss: 65.4300813
CurrentTrain: epoch  0, batch    10 | loss: 77.6890900
CurrentTrain: epoch  0, batch    11 | loss: 95.4000270
CurrentTrain: epoch  0, batch    12 | loss: 128.2489072
CurrentTrain: epoch  0, batch    13 | loss: 65.9760728
CurrentTrain: epoch  0, batch    14 | loss: 64.9915563
CurrentTrain: epoch  0, batch    15 | loss: 77.3531666
CurrentTrain: epoch  0, batch    16 | loss: 65.0833222
CurrentTrain: epoch  0, batch    17 | loss: 126.5839648
CurrentTrain: epoch  0, batch    18 | loss: 77.2507858
CurrentTrain: epoch  0, batch    19 | loss: 77.8083346
CurrentTrain: epoch  0, batch    20 | loss: 56.0149077
CurrentTrain: epoch  0, batch    21 | loss: 64.7771270
CurrentTrain: epoch  0, batch    22 | loss: 55.7092975
CurrentTrain: epoch  0, batch    23 | loss: 94.9907828
CurrentTrain: epoch  0, batch    24 | loss: 78.9684869
CurrentTrain: epoch  0, batch    25 | loss: 77.1005696
CurrentTrain: epoch  0, batch    26 | loss: 125.8517708
CurrentTrain: epoch  0, batch    27 | loss: 76.5117667
CurrentTrain: epoch  0, batch    28 | loss: 64.9614839
CurrentTrain: epoch  0, batch    29 | loss: 95.5868965
CurrentTrain: epoch  0, batch    30 | loss: 188.0993489
CurrentTrain: epoch  0, batch    31 | loss: 56.2561562
CurrentTrain: epoch  0, batch    32 | loss: 64.0818334
CurrentTrain: epoch  0, batch    33 | loss: 75.9456455
CurrentTrain: epoch  0, batch    34 | loss: 76.8831856
CurrentTrain: epoch  0, batch    35 | loss: 94.7199242
CurrentTrain: epoch  0, batch    36 | loss: 64.5606886
CurrentTrain: epoch  0, batch    37 | loss: 77.4588870
CurrentTrain: epoch  0, batch    38 | loss: 64.4967439
CurrentTrain: epoch  0, batch    39 | loss: 75.9069804
CurrentTrain: epoch  0, batch    40 | loss: 55.8390462
CurrentTrain: epoch  0, batch    41 | loss: 125.9652149
CurrentTrain: epoch  0, batch    42 | loss: 76.4596803
CurrentTrain: epoch  0, batch    43 | loss: 64.3854588
CurrentTrain: epoch  0, batch    44 | loss: 76.2620872
CurrentTrain: epoch  0, batch    45 | loss: 64.5518330
CurrentTrain: epoch  0, batch    46 | loss: 126.1887973
CurrentTrain: epoch  0, batch    47 | loss: 75.8222104
CurrentTrain: epoch  0, batch    48 | loss: 55.5053568
CurrentTrain: epoch  0, batch    49 | loss: 76.4395527
CurrentTrain: epoch  0, batch    50 | loss: 76.1692744
CurrentTrain: epoch  0, batch    51 | loss: 63.8752674
CurrentTrain: epoch  0, batch    52 | loss: 64.2838067
CurrentTrain: epoch  0, batch    53 | loss: 63.9515481
CurrentTrain: epoch  0, batch    54 | loss: 63.3753914
CurrentTrain: epoch  0, batch    55 | loss: 75.4493683
CurrentTrain: epoch  0, batch    56 | loss: 76.0479135
CurrentTrain: epoch  0, batch    57 | loss: 95.1741684
CurrentTrain: epoch  0, batch    58 | loss: 93.3759000
CurrentTrain: epoch  0, batch    59 | loss: 93.4818096
CurrentTrain: epoch  0, batch    60 | loss: 63.0481628
CurrentTrain: epoch  0, batch    61 | loss: 63.3081434
CurrentTrain: epoch  0, batch    62 | loss: 372.2083324
CurrentTrain: epoch  0, batch    63 | loss: 62.1519560
CurrentTrain: epoch  0, batch    64 | loss: 63.2885960
CurrentTrain: epoch  0, batch    65 | loss: 63.4667002
CurrentTrain: epoch  0, batch    66 | loss: 73.4798430
CurrentTrain: epoch  0, batch    67 | loss: 125.6470729
CurrentTrain: epoch  0, batch    68 | loss: 74.7284059
CurrentTrain: epoch  0, batch    69 | loss: 75.6802023
CurrentTrain: epoch  0, batch    70 | loss: 122.2333102
CurrentTrain: epoch  0, batch    71 | loss: 94.2609376
CurrentTrain: epoch  0, batch    72 | loss: 73.9343229
CurrentTrain: epoch  0, batch    73 | loss: 92.2091141
CurrentTrain: epoch  0, batch    74 | loss: 76.4262810
CurrentTrain: epoch  0, batch    75 | loss: 72.9641460
CurrentTrain: epoch  0, batch    76 | loss: 74.8409329
CurrentTrain: epoch  0, batch    77 | loss: 93.9675932
CurrentTrain: epoch  0, batch    78 | loss: 74.4618020
CurrentTrain: epoch  0, batch    79 | loss: 121.4789743
CurrentTrain: epoch  0, batch    80 | loss: 74.0307037
CurrentTrain: epoch  0, batch    81 | loss: 91.9437364
CurrentTrain: epoch  0, batch    82 | loss: 92.8376020
CurrentTrain: epoch  0, batch    83 | loss: 72.8493185
CurrentTrain: epoch  0, batch    84 | loss: 92.7080734
CurrentTrain: epoch  0, batch    85 | loss: 74.0861642
CurrentTrain: epoch  0, batch    86 | loss: 92.1971710
CurrentTrain: epoch  0, batch    87 | loss: 91.7695547
CurrentTrain: epoch  0, batch    88 | loss: 72.4464609
CurrentTrain: epoch  0, batch    89 | loss: 58.9449000
CurrentTrain: epoch  0, batch    90 | loss: 62.1376425
CurrentTrain: epoch  0, batch    91 | loss: 60.0674475
CurrentTrain: epoch  0, batch    92 | loss: 60.6759168
CurrentTrain: epoch  0, batch    93 | loss: 61.7882746
CurrentTrain: epoch  0, batch    94 | loss: 59.7184703
CurrentTrain: epoch  0, batch    95 | loss: 103.4201190
CurrentTrain: epoch  1, batch     0 | loss: 72.9630437
CurrentTrain: epoch  1, batch     1 | loss: 73.7971883
CurrentTrain: epoch  1, batch     2 | loss: 73.8230885
CurrentTrain: epoch  1, batch     3 | loss: 72.9914971
CurrentTrain: epoch  1, batch     4 | loss: 58.9459976
CurrentTrain: epoch  1, batch     5 | loss: 86.9118510
CurrentTrain: epoch  1, batch     6 | loss: 58.0223420
CurrentTrain: epoch  1, batch     7 | loss: 57.7726136
CurrentTrain: epoch  1, batch     8 | loss: 70.1883929
CurrentTrain: epoch  1, batch     9 | loss: 58.3749669
CurrentTrain: epoch  1, batch    10 | loss: 186.7750023
CurrentTrain: epoch  1, batch    11 | loss: 120.3202363
CurrentTrain: epoch  1, batch    12 | loss: 60.1499896
CurrentTrain: epoch  1, batch    13 | loss: 62.2096874
CurrentTrain: epoch  1, batch    14 | loss: 60.0911596
CurrentTrain: epoch  1, batch    15 | loss: 73.2338888
CurrentTrain: epoch  1, batch    16 | loss: 89.8472411
CurrentTrain: epoch  1, batch    17 | loss: 68.1137162
CurrentTrain: epoch  1, batch    18 | loss: 70.0995015
CurrentTrain: epoch  1, batch    19 | loss: 50.2504863
CurrentTrain: epoch  1, batch    20 | loss: 56.8171476
CurrentTrain: epoch  1, batch    21 | loss: 60.8540456
CurrentTrain: epoch  1, batch    22 | loss: 71.5233540
CurrentTrain: epoch  1, batch    23 | loss: 51.4366641
CurrentTrain: epoch  1, batch    24 | loss: 86.9059374
CurrentTrain: epoch  1, batch    25 | loss: 60.2876534
CurrentTrain: epoch  1, batch    26 | loss: 181.8968269
CurrentTrain: epoch  1, batch    27 | loss: 72.3555145
CurrentTrain: epoch  1, batch    28 | loss: 73.4581499
CurrentTrain: epoch  1, batch    29 | loss: 71.3011542
CurrentTrain: epoch  1, batch    30 | loss: 59.5571989
CurrentTrain: epoch  1, batch    31 | loss: 73.1466222
CurrentTrain: epoch  1, batch    32 | loss: 56.6770663
CurrentTrain: epoch  1, batch    33 | loss: 73.1989793
CurrentTrain: epoch  1, batch    34 | loss: 179.8981284
CurrentTrain: epoch  1, batch    35 | loss: 88.8050760
CurrentTrain: epoch  1, batch    36 | loss: 88.4544825
CurrentTrain: epoch  1, batch    37 | loss: 68.7668279
CurrentTrain: epoch  1, batch    38 | loss: 56.9845017
CurrentTrain: epoch  1, batch    39 | loss: 87.3713234
CurrentTrain: epoch  1, batch    40 | loss: 49.7832400
CurrentTrain: epoch  1, batch    41 | loss: 91.5028438
CurrentTrain: epoch  1, batch    42 | loss: 73.2767033
CurrentTrain: epoch  1, batch    43 | loss: 58.1787551
CurrentTrain: epoch  1, batch    44 | loss: 72.5555098
CurrentTrain: epoch  1, batch    45 | loss: 52.1132404
CurrentTrain: epoch  1, batch    46 | loss: 74.4707747
CurrentTrain: epoch  1, batch    47 | loss: 87.8302864
CurrentTrain: epoch  1, batch    48 | loss: 117.0922071
CurrentTrain: epoch  1, batch    49 | loss: 49.4446753
CurrentTrain: epoch  1, batch    50 | loss: 120.7617291
CurrentTrain: epoch  1, batch    51 | loss: 53.2172043
CurrentTrain: epoch  1, batch    52 | loss: 49.3035862
CurrentTrain: epoch  1, batch    53 | loss: 89.7735685
CurrentTrain: epoch  1, batch    54 | loss: 54.0757670
CurrentTrain: epoch  1, batch    55 | loss: 91.5113605
CurrentTrain: epoch  1, batch    56 | loss: 57.1985842
CurrentTrain: epoch  1, batch    57 | loss: 89.3281207
CurrentTrain: epoch  1, batch    58 | loss: 87.8444460
CurrentTrain: epoch  1, batch    59 | loss: 71.3060657
CurrentTrain: epoch  1, batch    60 | loss: 68.6031396
CurrentTrain: epoch  1, batch    61 | loss: 59.0582371
CurrentTrain: epoch  1, batch    62 | loss: 71.9141006
CurrentTrain: epoch  1, batch    63 | loss: 86.1240477
CurrentTrain: epoch  1, batch    64 | loss: 57.0814377
CurrentTrain: epoch  1, batch    65 | loss: 69.4047964
CurrentTrain: epoch  1, batch    66 | loss: 55.1094617
CurrentTrain: epoch  1, batch    67 | loss: 68.5854235
CurrentTrain: epoch  1, batch    68 | loss: 71.0513709
CurrentTrain: epoch  1, batch    69 | loss: 53.9041380
CurrentTrain: epoch  1, batch    70 | loss: 68.5784528
CurrentTrain: epoch  1, batch    71 | loss: 70.4794436
CurrentTrain: epoch  1, batch    72 | loss: 70.6919378
CurrentTrain: epoch  1, batch    73 | loss: 83.5111583
CurrentTrain: epoch  1, batch    74 | loss: 71.2969772
CurrentTrain: epoch  1, batch    75 | loss: 71.7759017
CurrentTrain: epoch  1, batch    76 | loss: 71.8672391
CurrentTrain: epoch  1, batch    77 | loss: 67.0586944
CurrentTrain: epoch  1, batch    78 | loss: 57.9721155
CurrentTrain: epoch  1, batch    79 | loss: 71.5426618
CurrentTrain: epoch  1, batch    80 | loss: 120.2037053
CurrentTrain: epoch  1, batch    81 | loss: 59.2152284
CurrentTrain: epoch  1, batch    82 | loss: 67.4560722
CurrentTrain: epoch  1, batch    83 | loss: 68.8351026
CurrentTrain: epoch  1, batch    84 | loss: 69.8166407
CurrentTrain: epoch  1, batch    85 | loss: 87.1795983
CurrentTrain: epoch  1, batch    86 | loss: 87.1587662
CurrentTrain: epoch  1, batch    87 | loss: 50.0133782
CurrentTrain: epoch  1, batch    88 | loss: 57.2744541
CurrentTrain: epoch  1, batch    89 | loss: 71.2656560
CurrentTrain: epoch  1, batch    90 | loss: 92.6699621
CurrentTrain: epoch  1, batch    91 | loss: 87.4332941
CurrentTrain: epoch  1, batch    92 | loss: 119.3377070
CurrentTrain: epoch  1, batch    93 | loss: 88.5957435
CurrentTrain: epoch  1, batch    94 | loss: 69.0586546
CurrentTrain: epoch  1, batch    95 | loss: 72.4854418
CurrentTrain: epoch  2, batch     0 | loss: 88.0362289
CurrentTrain: epoch  2, batch     1 | loss: 59.5648861
CurrentTrain: epoch  2, batch     2 | loss: 89.9148047
CurrentTrain: epoch  2, batch     3 | loss: 74.0536385
CurrentTrain: epoch  2, batch     4 | loss: 51.7182976
CurrentTrain: epoch  2, batch     5 | loss: 47.7589288
CurrentTrain: epoch  2, batch     6 | loss: 47.1939990
CurrentTrain: epoch  2, batch     7 | loss: 68.2783734
CurrentTrain: epoch  2, batch     8 | loss: 83.8608849
CurrentTrain: epoch  2, batch     9 | loss: 52.9624546
CurrentTrain: epoch  2, batch    10 | loss: 55.0362331
CurrentTrain: epoch  2, batch    11 | loss: 69.1796888
CurrentTrain: epoch  2, batch    12 | loss: 70.3564585
CurrentTrain: epoch  2, batch    13 | loss: 85.3588901
CurrentTrain: epoch  2, batch    14 | loss: 113.5487285
CurrentTrain: epoch  2, batch    15 | loss: 46.2440558
CurrentTrain: epoch  2, batch    16 | loss: 120.2145842
CurrentTrain: epoch  2, batch    17 | loss: 67.5731138
CurrentTrain: epoch  2, batch    18 | loss: 87.9210734
CurrentTrain: epoch  2, batch    19 | loss: 53.9390214
CurrentTrain: epoch  2, batch    20 | loss: 89.2060251
CurrentTrain: epoch  2, batch    21 | loss: 57.9205230
CurrentTrain: epoch  2, batch    22 | loss: 116.7726020
CurrentTrain: epoch  2, batch    23 | loss: 119.6405439
CurrentTrain: epoch  2, batch    24 | loss: 55.7130363
CurrentTrain: epoch  2, batch    25 | loss: 59.4306564
CurrentTrain: epoch  2, batch    26 | loss: 87.3687059
CurrentTrain: epoch  2, batch    27 | loss: 55.8284970
CurrentTrain: epoch  2, batch    28 | loss: 182.7282719
CurrentTrain: epoch  2, batch    29 | loss: 88.0998837
CurrentTrain: epoch  2, batch    30 | loss: 114.2115109
CurrentTrain: epoch  2, batch    31 | loss: 88.7653258
CurrentTrain: epoch  2, batch    32 | loss: 83.6481637
CurrentTrain: epoch  2, batch    33 | loss: 49.6882333
CurrentTrain: epoch  2, batch    34 | loss: 43.4649530
CurrentTrain: epoch  2, batch    35 | loss: 84.6894155
CurrentTrain: epoch  2, batch    36 | loss: 116.8425656
CurrentTrain: epoch  2, batch    37 | loss: 70.4395388
CurrentTrain: epoch  2, batch    38 | loss: 55.5592559
CurrentTrain: epoch  2, batch    39 | loss: 68.0837079
CurrentTrain: epoch  2, batch    40 | loss: 67.6028132
CurrentTrain: epoch  2, batch    41 | loss: 58.0223254
CurrentTrain: epoch  2, batch    42 | loss: 55.7253275
CurrentTrain: epoch  2, batch    43 | loss: 56.4633148
CurrentTrain: epoch  2, batch    44 | loss: 54.5176606
CurrentTrain: epoch  2, batch    45 | loss: 84.4023153
CurrentTrain: epoch  2, batch    46 | loss: 68.0636285
CurrentTrain: epoch  2, batch    47 | loss: 71.1712346
CurrentTrain: epoch  2, batch    48 | loss: 68.7674907
CurrentTrain: epoch  2, batch    49 | loss: 58.2257868
CurrentTrain: epoch  2, batch    50 | loss: 69.4700061
CurrentTrain: epoch  2, batch    51 | loss: 55.0845914
CurrentTrain: epoch  2, batch    52 | loss: 64.9529939
CurrentTrain: epoch  2, batch    53 | loss: 119.0723252
CurrentTrain: epoch  2, batch    54 | loss: 51.4479363
CurrentTrain: epoch  2, batch    55 | loss: 84.6760473
CurrentTrain: epoch  2, batch    56 | loss: 114.9895519
CurrentTrain: epoch  2, batch    57 | loss: 55.5898991
CurrentTrain: epoch  2, batch    58 | loss: 57.0958713
CurrentTrain: epoch  2, batch    59 | loss: 44.8350667
CurrentTrain: epoch  2, batch    60 | loss: 86.9794236
CurrentTrain: epoch  2, batch    61 | loss: 52.2374794
CurrentTrain: epoch  2, batch    62 | loss: 67.5530162
CurrentTrain: epoch  2, batch    63 | loss: 72.7351016
CurrentTrain: epoch  2, batch    64 | loss: 89.1792564
CurrentTrain: epoch  2, batch    65 | loss: 54.1909107
CurrentTrain: epoch  2, batch    66 | loss: 68.4498112
CurrentTrain: epoch  2, batch    67 | loss: 70.8263606
CurrentTrain: epoch  2, batch    68 | loss: 91.3387695
CurrentTrain: epoch  2, batch    69 | loss: 55.1532021
CurrentTrain: epoch  2, batch    70 | loss: 64.0056609
CurrentTrain: epoch  2, batch    71 | loss: 68.4924969
CurrentTrain: epoch  2, batch    72 | loss: 91.2785708
CurrentTrain: epoch  2, batch    73 | loss: 87.7694085
CurrentTrain: epoch  2, batch    74 | loss: 89.5663754
CurrentTrain: epoch  2, batch    75 | loss: 56.2964877
CurrentTrain: epoch  2, batch    76 | loss: 86.9876899
CurrentTrain: epoch  2, batch    77 | loss: 66.3399180
CurrentTrain: epoch  2, batch    78 | loss: 54.9132956
CurrentTrain: epoch  2, batch    79 | loss: 89.5449808
CurrentTrain: epoch  2, batch    80 | loss: 68.3035743
CurrentTrain: epoch  2, batch    81 | loss: 54.9289911
CurrentTrain: epoch  2, batch    82 | loss: 65.8082220
CurrentTrain: epoch  2, batch    83 | loss: 119.0471576
CurrentTrain: epoch  2, batch    84 | loss: 116.9703314
CurrentTrain: epoch  2, batch    85 | loss: 70.1088229
CurrentTrain: epoch  2, batch    86 | loss: 68.2823086
CurrentTrain: epoch  2, batch    87 | loss: 69.4457208
CurrentTrain: epoch  2, batch    88 | loss: 81.0260769
CurrentTrain: epoch  2, batch    89 | loss: 70.8965625
CurrentTrain: epoch  2, batch    90 | loss: 55.2126383
CurrentTrain: epoch  2, batch    91 | loss: 66.7036800
CurrentTrain: epoch  2, batch    92 | loss: 59.3835979
CurrentTrain: epoch  2, batch    93 | loss: 56.4689204
CurrentTrain: epoch  2, batch    94 | loss: 123.1877116
CurrentTrain: epoch  2, batch    95 | loss: 59.5734646
CurrentTrain: epoch  3, batch     0 | loss: 48.0595971
CurrentTrain: epoch  3, batch     1 | loss: 87.2747057
CurrentTrain: epoch  3, batch     2 | loss: 64.9561107
CurrentTrain: epoch  3, batch     3 | loss: 45.2319940
CurrentTrain: epoch  3, batch     4 | loss: 68.8646458
CurrentTrain: epoch  3, batch     5 | loss: 88.3225853
CurrentTrain: epoch  3, batch     6 | loss: 54.1406296
CurrentTrain: epoch  3, batch     7 | loss: 67.8801344
CurrentTrain: epoch  3, batch     8 | loss: 119.6045357
CurrentTrain: epoch  3, batch     9 | loss: 55.5704263
CurrentTrain: epoch  3, batch    10 | loss: 67.3521932
CurrentTrain: epoch  3, batch    11 | loss: 69.1185531
CurrentTrain: epoch  3, batch    12 | loss: 85.2298503
CurrentTrain: epoch  3, batch    13 | loss: 46.7564484
CurrentTrain: epoch  3, batch    14 | loss: 62.0917323
CurrentTrain: epoch  3, batch    15 | loss: 86.3656340
CurrentTrain: epoch  3, batch    16 | loss: 89.6327423
CurrentTrain: epoch  3, batch    17 | loss: 46.3202918
CurrentTrain: epoch  3, batch    18 | loss: 46.5021447
CurrentTrain: epoch  3, batch    19 | loss: 82.4935506
CurrentTrain: epoch  3, batch    20 | loss: 55.4798632
CurrentTrain: epoch  3, batch    21 | loss: 117.9719014
CurrentTrain: epoch  3, batch    22 | loss: 82.8078182
CurrentTrain: epoch  3, batch    23 | loss: 87.4709856
CurrentTrain: epoch  3, batch    24 | loss: 68.0435705
CurrentTrain: epoch  3, batch    25 | loss: 67.5944231
CurrentTrain: epoch  3, batch    26 | loss: 49.2386467
CurrentTrain: epoch  3, batch    27 | loss: 182.6569954
CurrentTrain: epoch  3, batch    28 | loss: 55.1520805
CurrentTrain: epoch  3, batch    29 | loss: 70.0290773
CurrentTrain: epoch  3, batch    30 | loss: 55.2023487
CurrentTrain: epoch  3, batch    31 | loss: 45.1147444
CurrentTrain: epoch  3, batch    32 | loss: 65.8802461
CurrentTrain: epoch  3, batch    33 | loss: 115.9846963
CurrentTrain: epoch  3, batch    34 | loss: 55.7159009
CurrentTrain: epoch  3, batch    35 | loss: 53.5564681
CurrentTrain: epoch  3, batch    36 | loss: 83.1991614
CurrentTrain: epoch  3, batch    37 | loss: 66.0499237
CurrentTrain: epoch  3, batch    38 | loss: 84.3076339
CurrentTrain: epoch  3, batch    39 | loss: 54.4143154
CurrentTrain: epoch  3, batch    40 | loss: 87.6985089
CurrentTrain: epoch  3, batch    41 | loss: 56.3005951
CurrentTrain: epoch  3, batch    42 | loss: 85.0479651
CurrentTrain: epoch  3, batch    43 | loss: 54.2240134
CurrentTrain: epoch  3, batch    44 | loss: 47.0056208
CurrentTrain: epoch  3, batch    45 | loss: 69.7492330
CurrentTrain: epoch  3, batch    46 | loss: 71.1624045
CurrentTrain: epoch  3, batch    47 | loss: 45.7817238
CurrentTrain: epoch  3, batch    48 | loss: 57.3900058
CurrentTrain: epoch  3, batch    49 | loss: 85.3278894
CurrentTrain: epoch  3, batch    50 | loss: 88.7759876
CurrentTrain: epoch  3, batch    51 | loss: 66.6284251
CurrentTrain: epoch  3, batch    52 | loss: 113.5338974
CurrentTrain: epoch  3, batch    53 | loss: 55.0302839
CurrentTrain: epoch  3, batch    54 | loss: 54.1341620
CurrentTrain: epoch  3, batch    55 | loss: 66.3449356
CurrentTrain: epoch  3, batch    56 | loss: 70.8173959
CurrentTrain: epoch  3, batch    57 | loss: 67.8606487
CurrentTrain: epoch  3, batch    58 | loss: 56.3359414
CurrentTrain: epoch  3, batch    59 | loss: 115.7649131
CurrentTrain: epoch  3, batch    60 | loss: 68.3077441
CurrentTrain: epoch  3, batch    61 | loss: 85.2539451
CurrentTrain: epoch  3, batch    62 | loss: 68.5137747
CurrentTrain: epoch  3, batch    63 | loss: 84.1349481
CurrentTrain: epoch  3, batch    64 | loss: 50.9819984
CurrentTrain: epoch  3, batch    65 | loss: 47.8233717
CurrentTrain: epoch  3, batch    66 | loss: 70.0352664
CurrentTrain: epoch  3, batch    67 | loss: 64.7311762
CurrentTrain: epoch  3, batch    68 | loss: 46.3832349
CurrentTrain: epoch  3, batch    69 | loss: 90.6312288
CurrentTrain: epoch  3, batch    70 | loss: 55.0276124
CurrentTrain: epoch  3, batch    71 | loss: 65.9524583
CurrentTrain: epoch  3, batch    72 | loss: 87.6249141
CurrentTrain: epoch  3, batch    73 | loss: 50.2314275
CurrentTrain: epoch  3, batch    74 | loss: 83.6572424
CurrentTrain: epoch  3, batch    75 | loss: 53.3009299
CurrentTrain: epoch  3, batch    76 | loss: 67.8310290
CurrentTrain: epoch  3, batch    77 | loss: 54.5627054
CurrentTrain: epoch  3, batch    78 | loss: 118.8966296
CurrentTrain: epoch  3, batch    79 | loss: 66.7765357
CurrentTrain: epoch  3, batch    80 | loss: 56.2953707
CurrentTrain: epoch  3, batch    81 | loss: 65.5023562
CurrentTrain: epoch  3, batch    82 | loss: 67.0197531
CurrentTrain: epoch  3, batch    83 | loss: 118.6101145
CurrentTrain: epoch  3, batch    84 | loss: 69.6462101
CurrentTrain: epoch  3, batch    85 | loss: 83.3891252
CurrentTrain: epoch  3, batch    86 | loss: 65.0692406
CurrentTrain: epoch  3, batch    87 | loss: 65.8984852
CurrentTrain: epoch  3, batch    88 | loss: 86.4416490
CurrentTrain: epoch  3, batch    89 | loss: 66.4756243
CurrentTrain: epoch  3, batch    90 | loss: 54.2028310
CurrentTrain: epoch  3, batch    91 | loss: 87.4620225
CurrentTrain: epoch  3, batch    92 | loss: 57.8117794
CurrentTrain: epoch  3, batch    93 | loss: 56.6742945
CurrentTrain: epoch  3, batch    94 | loss: 45.9514828
CurrentTrain: epoch  3, batch    95 | loss: 55.8956635
CurrentTrain: epoch  4, batch     0 | loss: 87.0863719
CurrentTrain: epoch  4, batch     1 | loss: 66.6695685
CurrentTrain: epoch  4, batch     2 | loss: 52.9371028
CurrentTrain: epoch  4, batch     3 | loss: 55.0159603
CurrentTrain: epoch  4, batch     4 | loss: 53.4710435
CurrentTrain: epoch  4, batch     5 | loss: 116.0179945
CurrentTrain: epoch  4, batch     6 | loss: 63.1432402
CurrentTrain: epoch  4, batch     7 | loss: 87.5114344
CurrentTrain: epoch  4, batch     8 | loss: 121.1259708
CurrentTrain: epoch  4, batch     9 | loss: 54.1470449
CurrentTrain: epoch  4, batch    10 | loss: 81.6747123
CurrentTrain: epoch  4, batch    11 | loss: 80.1443184
CurrentTrain: epoch  4, batch    12 | loss: 115.4035082
CurrentTrain: epoch  4, batch    13 | loss: 182.8227651
CurrentTrain: epoch  4, batch    14 | loss: 53.8138444
CurrentTrain: epoch  4, batch    15 | loss: 55.1832107
CurrentTrain: epoch  4, batch    16 | loss: 54.0752394
CurrentTrain: epoch  4, batch    17 | loss: 47.6065310
CurrentTrain: epoch  4, batch    18 | loss: 89.2460257
CurrentTrain: epoch  4, batch    19 | loss: 86.5873720
CurrentTrain: epoch  4, batch    20 | loss: 68.6422709
CurrentTrain: epoch  4, batch    21 | loss: 66.9909517
CurrentTrain: epoch  4, batch    22 | loss: 66.1283960
CurrentTrain: epoch  4, batch    23 | loss: 54.7861324
CurrentTrain: epoch  4, batch    24 | loss: 61.8623027
CurrentTrain: epoch  4, batch    25 | loss: 67.0330006
CurrentTrain: epoch  4, batch    26 | loss: 88.3895648
CurrentTrain: epoch  4, batch    27 | loss: 66.9916425
CurrentTrain: epoch  4, batch    28 | loss: 54.8235809
CurrentTrain: epoch  4, batch    29 | loss: 54.1347995
CurrentTrain: epoch  4, batch    30 | loss: 64.8846483
CurrentTrain: epoch  4, batch    31 | loss: 66.8664486
CurrentTrain: epoch  4, batch    32 | loss: 64.7963714
CurrentTrain: epoch  4, batch    33 | loss: 183.2619424
CurrentTrain: epoch  4, batch    34 | loss: 67.5485802
CurrentTrain: epoch  4, batch    35 | loss: 67.0825722
CurrentTrain: epoch  4, batch    36 | loss: 66.2383660
CurrentTrain: epoch  4, batch    37 | loss: 85.5945416
CurrentTrain: epoch  4, batch    38 | loss: 81.6230811
CurrentTrain: epoch  4, batch    39 | loss: 86.4834406
CurrentTrain: epoch  4, batch    40 | loss: 85.2359644
CurrentTrain: epoch  4, batch    41 | loss: 57.3381644
CurrentTrain: epoch  4, batch    42 | loss: 63.1114148
CurrentTrain: epoch  4, batch    43 | loss: 69.1318858
CurrentTrain: epoch  4, batch    44 | loss: 67.4635483
CurrentTrain: epoch  4, batch    45 | loss: 67.9865836
CurrentTrain: epoch  4, batch    46 | loss: 63.9872101
CurrentTrain: epoch  4, batch    47 | loss: 45.1609777
CurrentTrain: epoch  4, batch    48 | loss: 53.3736559
CurrentTrain: epoch  4, batch    49 | loss: 56.2356741
CurrentTrain: epoch  4, batch    50 | loss: 54.2149824
CurrentTrain: epoch  4, batch    51 | loss: 67.0343976
CurrentTrain: epoch  4, batch    52 | loss: 54.3925694
CurrentTrain: epoch  4, batch    53 | loss: 48.0497362
CurrentTrain: epoch  4, batch    54 | loss: 85.8318823
CurrentTrain: epoch  4, batch    55 | loss: 54.7838766
CurrentTrain: epoch  4, batch    56 | loss: 64.1575737
CurrentTrain: epoch  4, batch    57 | loss: 79.9358706
CurrentTrain: epoch  4, batch    58 | loss: 65.8602705
CurrentTrain: epoch  4, batch    59 | loss: 82.4969233
CurrentTrain: epoch  4, batch    60 | loss: 65.8899304
CurrentTrain: epoch  4, batch    61 | loss: 81.5913170
CurrentTrain: epoch  4, batch    62 | loss: 86.5718718
CurrentTrain: epoch  4, batch    63 | loss: 120.0763747
CurrentTrain: epoch  4, batch    64 | loss: 62.4794295
CurrentTrain: epoch  4, batch    65 | loss: 53.5903462
CurrentTrain: epoch  4, batch    66 | loss: 54.2233466
CurrentTrain: epoch  4, batch    67 | loss: 69.1097318
CurrentTrain: epoch  4, batch    68 | loss: 88.1291542
CurrentTrain: epoch  4, batch    69 | loss: 83.7198822
CurrentTrain: epoch  4, batch    70 | loss: 85.1812738
CurrentTrain: epoch  4, batch    71 | loss: 53.8590985
CurrentTrain: epoch  4, batch    72 | loss: 53.8310850
CurrentTrain: epoch  4, batch    73 | loss: 54.8077716
CurrentTrain: epoch  4, batch    74 | loss: 62.0867341
CurrentTrain: epoch  4, batch    75 | loss: 69.3644283
CurrentTrain: epoch  4, batch    76 | loss: 65.7211740
CurrentTrain: epoch  4, batch    77 | loss: 119.0269555
CurrentTrain: epoch  4, batch    78 | loss: 66.5050619
CurrentTrain: epoch  4, batch    79 | loss: 67.7503845
CurrentTrain: epoch  4, batch    80 | loss: 54.0666798
CurrentTrain: epoch  4, batch    81 | loss: 66.0280313
CurrentTrain: epoch  4, batch    82 | loss: 88.0203840
CurrentTrain: epoch  4, batch    83 | loss: 83.3975887
CurrentTrain: epoch  4, batch    84 | loss: 67.9010870
CurrentTrain: epoch  4, batch    85 | loss: 54.1465141
CurrentTrain: epoch  4, batch    86 | loss: 83.9178613
CurrentTrain: epoch  4, batch    87 | loss: 67.4977983
CurrentTrain: epoch  4, batch    88 | loss: 82.0938358
CurrentTrain: epoch  4, batch    89 | loss: 51.3131237
CurrentTrain: epoch  4, batch    90 | loss: 55.1581763
CurrentTrain: epoch  4, batch    91 | loss: 64.5807680
CurrentTrain: epoch  4, batch    92 | loss: 86.3990595
CurrentTrain: epoch  4, batch    93 | loss: 84.3990694
CurrentTrain: epoch  4, batch    94 | loss: 54.6656681
CurrentTrain: epoch  4, batch    95 | loss: 55.4356146
CurrentTrain: epoch  5, batch     0 | loss: 55.2550041
CurrentTrain: epoch  5, batch     1 | loss: 53.2489277
CurrentTrain: epoch  5, batch     2 | loss: 80.9180983
CurrentTrain: epoch  5, batch     3 | loss: 64.9077090
CurrentTrain: epoch  5, batch     4 | loss: 50.8507926
CurrentTrain: epoch  5, batch     5 | loss: 67.6358511
CurrentTrain: epoch  5, batch     6 | loss: 84.1079792
CurrentTrain: epoch  5, batch     7 | loss: 68.1610492
CurrentTrain: epoch  5, batch     8 | loss: 54.5168977
CurrentTrain: epoch  5, batch     9 | loss: 62.8108780
CurrentTrain: epoch  5, batch    10 | loss: 50.5314968
CurrentTrain: epoch  5, batch    11 | loss: 84.3394945
CurrentTrain: epoch  5, batch    12 | loss: 55.5561665
CurrentTrain: epoch  5, batch    13 | loss: 65.1808591
CurrentTrain: epoch  5, batch    14 | loss: 51.6635360
CurrentTrain: epoch  5, batch    15 | loss: 86.0894579
CurrentTrain: epoch  5, batch    16 | loss: 66.5189036
CurrentTrain: epoch  5, batch    17 | loss: 46.0787275
CurrentTrain: epoch  5, batch    18 | loss: 84.4972900
CurrentTrain: epoch  5, batch    19 | loss: 53.9055579
CurrentTrain: epoch  5, batch    20 | loss: 48.5573608
CurrentTrain: epoch  5, batch    21 | loss: 80.4488607
CurrentTrain: epoch  5, batch    22 | loss: 115.3731792
CurrentTrain: epoch  5, batch    23 | loss: 86.7887198
CurrentTrain: epoch  5, batch    24 | loss: 85.2425830
CurrentTrain: epoch  5, batch    25 | loss: 65.7703112
CurrentTrain: epoch  5, batch    26 | loss: 45.7870004
CurrentTrain: epoch  5, batch    27 | loss: 113.7241909
CurrentTrain: epoch  5, batch    28 | loss: 84.8289901
CurrentTrain: epoch  5, batch    29 | loss: 52.8261371
CurrentTrain: epoch  5, batch    30 | loss: 65.7755254
CurrentTrain: epoch  5, batch    31 | loss: 67.0045983
CurrentTrain: epoch  5, batch    32 | loss: 53.8048296
CurrentTrain: epoch  5, batch    33 | loss: 65.7743036
CurrentTrain: epoch  5, batch    34 | loss: 86.6347868
CurrentTrain: epoch  5, batch    35 | loss: 50.1186167
CurrentTrain: epoch  5, batch    36 | loss: 116.6534816
CurrentTrain: epoch  5, batch    37 | loss: 116.1380969
CurrentTrain: epoch  5, batch    38 | loss: 117.8916908
CurrentTrain: epoch  5, batch    39 | loss: 64.7376165
CurrentTrain: epoch  5, batch    40 | loss: 53.3457554
CurrentTrain: epoch  5, batch    41 | loss: 85.1661560
CurrentTrain: epoch  5, batch    42 | loss: 52.3164486
CurrentTrain: epoch  5, batch    43 | loss: 66.5641131
CurrentTrain: epoch  5, batch    44 | loss: 64.5061258
CurrentTrain: epoch  5, batch    45 | loss: 67.8062434
CurrentTrain: epoch  5, batch    46 | loss: 44.2955799
CurrentTrain: epoch  5, batch    47 | loss: 86.2427175
CurrentTrain: epoch  5, batch    48 | loss: 63.6127381
CurrentTrain: epoch  5, batch    49 | loss: 54.7183986
CurrentTrain: epoch  5, batch    50 | loss: 84.1419347
CurrentTrain: epoch  5, batch    51 | loss: 177.8087537
CurrentTrain: epoch  5, batch    52 | loss: 62.6806488
CurrentTrain: epoch  5, batch    53 | loss: 65.0300702
CurrentTrain: epoch  5, batch    54 | loss: 64.2542112
CurrentTrain: epoch  5, batch    55 | loss: 84.9627681
CurrentTrain: epoch  5, batch    56 | loss: 51.7593311
CurrentTrain: epoch  5, batch    57 | loss: 51.8169519
CurrentTrain: epoch  5, batch    58 | loss: 182.2840750
CurrentTrain: epoch  5, batch    59 | loss: 45.7850832
CurrentTrain: epoch  5, batch    60 | loss: 63.7312106
CurrentTrain: epoch  5, batch    61 | loss: 61.3878995
CurrentTrain: epoch  5, batch    62 | loss: 53.8697908
CurrentTrain: epoch  5, batch    63 | loss: 50.8879551
CurrentTrain: epoch  5, batch    64 | loss: 44.4076410
CurrentTrain: epoch  5, batch    65 | loss: 115.7435702
CurrentTrain: epoch  5, batch    66 | loss: 67.0808017
CurrentTrain: epoch  5, batch    67 | loss: 48.1705426
CurrentTrain: epoch  5, batch    68 | loss: 61.9428080
CurrentTrain: epoch  5, batch    69 | loss: 372.5254211
CurrentTrain: epoch  5, batch    70 | loss: 86.8873650
CurrentTrain: epoch  5, batch    71 | loss: 82.7122712
CurrentTrain: epoch  5, batch    72 | loss: 43.9852346
CurrentTrain: epoch  5, batch    73 | loss: 68.1427636
CurrentTrain: epoch  5, batch    74 | loss: 82.8529615
CurrentTrain: epoch  5, batch    75 | loss: 64.1167678
CurrentTrain: epoch  5, batch    76 | loss: 52.4181967
CurrentTrain: epoch  5, batch    77 | loss: 44.8035926
CurrentTrain: epoch  5, batch    78 | loss: 55.1421021
CurrentTrain: epoch  5, batch    79 | loss: 117.7006205
CurrentTrain: epoch  5, batch    80 | loss: 51.6822477
CurrentTrain: epoch  5, batch    81 | loss: 65.3513541
CurrentTrain: epoch  5, batch    82 | loss: 44.4137360
CurrentTrain: epoch  5, batch    83 | loss: 84.6295292
CurrentTrain: epoch  5, batch    84 | loss: 52.9762279
CurrentTrain: epoch  5, batch    85 | loss: 52.3483647
CurrentTrain: epoch  5, batch    86 | loss: 88.4602423
CurrentTrain: epoch  5, batch    87 | loss: 111.4131014
CurrentTrain: epoch  5, batch    88 | loss: 54.6436244
CurrentTrain: epoch  5, batch    89 | loss: 81.7242900
CurrentTrain: epoch  5, batch    90 | loss: 51.1061007
CurrentTrain: epoch  5, batch    91 | loss: 84.8273904
CurrentTrain: epoch  5, batch    92 | loss: 64.7986594
CurrentTrain: epoch  5, batch    93 | loss: 52.0458901
CurrentTrain: epoch  5, batch    94 | loss: 66.6702559
CurrentTrain: epoch  5, batch    95 | loss: 54.7468099
CurrentTrain: epoch  6, batch     0 | loss: 83.1389456
CurrentTrain: epoch  6, batch     1 | loss: 63.1908471
CurrentTrain: epoch  6, batch     2 | loss: 84.4267944
CurrentTrain: epoch  6, batch     3 | loss: 54.3451196
CurrentTrain: epoch  6, batch     4 | loss: 81.2926909
CurrentTrain: epoch  6, batch     5 | loss: 49.9635756
CurrentTrain: epoch  6, batch     6 | loss: 84.2148160
CurrentTrain: epoch  6, batch     7 | loss: 68.8026145
CurrentTrain: epoch  6, batch     8 | loss: 52.8099053
CurrentTrain: epoch  6, batch     9 | loss: 82.2634409
CurrentTrain: epoch  6, batch    10 | loss: 62.2957690
CurrentTrain: epoch  6, batch    11 | loss: 64.4289648
CurrentTrain: epoch  6, batch    12 | loss: 120.1436232
CurrentTrain: epoch  6, batch    13 | loss: 119.7564780
CurrentTrain: epoch  6, batch    14 | loss: 64.3607137
CurrentTrain: epoch  6, batch    15 | loss: 54.8826666
CurrentTrain: epoch  6, batch    16 | loss: 86.1512834
CurrentTrain: epoch  6, batch    17 | loss: 56.9915307
CurrentTrain: epoch  6, batch    18 | loss: 82.0669588
CurrentTrain: epoch  6, batch    19 | loss: 60.9341258
CurrentTrain: epoch  6, batch    20 | loss: 85.6049307
CurrentTrain: epoch  6, batch    21 | loss: 42.9042521
CurrentTrain: epoch  6, batch    22 | loss: 63.2043489
CurrentTrain: epoch  6, batch    23 | loss: 83.1467730
CurrentTrain: epoch  6, batch    24 | loss: 53.4676433
CurrentTrain: epoch  6, batch    25 | loss: 64.4410939
CurrentTrain: epoch  6, batch    26 | loss: 60.6750952
CurrentTrain: epoch  6, batch    27 | loss: 53.2813109
CurrentTrain: epoch  6, batch    28 | loss: 64.8666484
CurrentTrain: epoch  6, batch    29 | loss: 176.2633822
CurrentTrain: epoch  6, batch    30 | loss: 67.5756786
CurrentTrain: epoch  6, batch    31 | loss: 48.9400933
CurrentTrain: epoch  6, batch    32 | loss: 51.4938031
CurrentTrain: epoch  6, batch    33 | loss: 49.0184163
CurrentTrain: epoch  6, batch    34 | loss: 85.7859336
CurrentTrain: epoch  6, batch    35 | loss: 84.9742830
CurrentTrain: epoch  6, batch    36 | loss: 82.4947530
CurrentTrain: epoch  6, batch    37 | loss: 53.2938582
CurrentTrain: epoch  6, batch    38 | loss: 63.2019258
CurrentTrain: epoch  6, batch    39 | loss: 118.2953969
CurrentTrain: epoch  6, batch    40 | loss: 45.3551917
CurrentTrain: epoch  6, batch    41 | loss: 64.5015016
CurrentTrain: epoch  6, batch    42 | loss: 65.1296822
CurrentTrain: epoch  6, batch    43 | loss: 53.9400873
CurrentTrain: epoch  6, batch    44 | loss: 42.9124200
CurrentTrain: epoch  6, batch    45 | loss: 71.0300945
CurrentTrain: epoch  6, batch    46 | loss: 82.0088892
CurrentTrain: epoch  6, batch    47 | loss: 117.1856220
CurrentTrain: epoch  6, batch    48 | loss: 43.7913959
CurrentTrain: epoch  6, batch    49 | loss: 81.2434674
CurrentTrain: epoch  6, batch    50 | loss: 181.7356433
CurrentTrain: epoch  6, batch    51 | loss: 54.3860001
CurrentTrain: epoch  6, batch    52 | loss: 69.9611298
CurrentTrain: epoch  6, batch    53 | loss: 49.7085925
CurrentTrain: epoch  6, batch    54 | loss: 54.9672736
CurrentTrain: epoch  6, batch    55 | loss: 51.8477794
CurrentTrain: epoch  6, batch    56 | loss: 42.1700090
CurrentTrain: epoch  6, batch    57 | loss: 85.9387860
CurrentTrain: epoch  6, batch    58 | loss: 53.2798264
CurrentTrain: epoch  6, batch    59 | loss: 69.3253653
CurrentTrain: epoch  6, batch    60 | loss: 181.5601177
CurrentTrain: epoch  6, batch    61 | loss: 82.3610299
CurrentTrain: epoch  6, batch    62 | loss: 88.2327707
CurrentTrain: epoch  6, batch    63 | loss: 48.6417303
CurrentTrain: epoch  6, batch    64 | loss: 67.7839679
CurrentTrain: epoch  6, batch    65 | loss: 65.8212027
CurrentTrain: epoch  6, batch    66 | loss: 68.3760834
CurrentTrain: epoch  6, batch    67 | loss: 63.1755713
CurrentTrain: epoch  6, batch    68 | loss: 44.5822377
CurrentTrain: epoch  6, batch    69 | loss: 83.5978371
CurrentTrain: epoch  6, batch    70 | loss: 44.2661584
CurrentTrain: epoch  6, batch    71 | loss: 85.4649392
CurrentTrain: epoch  6, batch    72 | loss: 51.0657310
CurrentTrain: epoch  6, batch    73 | loss: 82.7505285
CurrentTrain: epoch  6, batch    74 | loss: 116.0783190
CurrentTrain: epoch  6, batch    75 | loss: 88.1833742
CurrentTrain: epoch  6, batch    76 | loss: 52.3084010
CurrentTrain: epoch  6, batch    77 | loss: 42.7918899
CurrentTrain: epoch  6, batch    78 | loss: 66.3083786
CurrentTrain: epoch  6, batch    79 | loss: 53.0038910
CurrentTrain: epoch  6, batch    80 | loss: 183.4388058
CurrentTrain: epoch  6, batch    81 | loss: 81.7331946
CurrentTrain: epoch  6, batch    82 | loss: 86.2067415
CurrentTrain: epoch  6, batch    83 | loss: 54.8270817
CurrentTrain: epoch  6, batch    84 | loss: 64.7261434
CurrentTrain: epoch  6, batch    85 | loss: 71.3395358
CurrentTrain: epoch  6, batch    86 | loss: 118.0538272
CurrentTrain: epoch  6, batch    87 | loss: 66.6540449
CurrentTrain: epoch  6, batch    88 | loss: 65.8477180
CurrentTrain: epoch  6, batch    89 | loss: 44.4724048
CurrentTrain: epoch  6, batch    90 | loss: 115.2873502
CurrentTrain: epoch  6, batch    91 | loss: 65.0268131
CurrentTrain: epoch  6, batch    92 | loss: 65.2930089
CurrentTrain: epoch  6, batch    93 | loss: 82.9700221
CurrentTrain: epoch  6, batch    94 | loss: 62.9430890
CurrentTrain: epoch  6, batch    95 | loss: 54.3252964
CurrentTrain: epoch  7, batch     0 | loss: 51.6422366
CurrentTrain: epoch  7, batch     1 | loss: 52.8891177
CurrentTrain: epoch  7, batch     2 | loss: 65.6420166
CurrentTrain: epoch  7, batch     3 | loss: 52.0679204
CurrentTrain: epoch  7, batch     4 | loss: 83.2781607
CurrentTrain: epoch  7, batch     5 | loss: 49.3063632
CurrentTrain: epoch  7, batch     6 | loss: 66.4435209
CurrentTrain: epoch  7, batch     7 | loss: 50.4770764
CurrentTrain: epoch  7, batch     8 | loss: 49.2182037
CurrentTrain: epoch  7, batch     9 | loss: 117.6635258
CurrentTrain: epoch  7, batch    10 | loss: 44.8888826
CurrentTrain: epoch  7, batch    11 | loss: 64.7836412
CurrentTrain: epoch  7, batch    12 | loss: 109.4855446
CurrentTrain: epoch  7, batch    13 | loss: 118.6668323
CurrentTrain: epoch  7, batch    14 | loss: 54.3058579
CurrentTrain: epoch  7, batch    15 | loss: 50.9672878
CurrentTrain: epoch  7, batch    16 | loss: 62.0200121
CurrentTrain: epoch  7, batch    17 | loss: 63.0330334
CurrentTrain: epoch  7, batch    18 | loss: 83.1377813
CurrentTrain: epoch  7, batch    19 | loss: 112.9874550
CurrentTrain: epoch  7, batch    20 | loss: 63.1905405
CurrentTrain: epoch  7, batch    21 | loss: 85.7751114
CurrentTrain: epoch  7, batch    22 | loss: 53.6652643
CurrentTrain: epoch  7, batch    23 | loss: 52.8403414
CurrentTrain: epoch  7, batch    24 | loss: 64.1017251
CurrentTrain: epoch  7, batch    25 | loss: 50.5697966
CurrentTrain: epoch  7, batch    26 | loss: 177.6584800
CurrentTrain: epoch  7, batch    27 | loss: 42.3363174
CurrentTrain: epoch  7, batch    28 | loss: 82.9621483
CurrentTrain: epoch  7, batch    29 | loss: 83.7532968
CurrentTrain: epoch  7, batch    30 | loss: 82.8804899
CurrentTrain: epoch  7, batch    31 | loss: 65.4820578
CurrentTrain: epoch  7, batch    32 | loss: 52.7541757
CurrentTrain: epoch  7, batch    33 | loss: 60.1830279
CurrentTrain: epoch  7, batch    34 | loss: 64.6488613
CurrentTrain: epoch  7, batch    35 | loss: 114.3359010
CurrentTrain: epoch  7, batch    36 | loss: 51.3790784
CurrentTrain: epoch  7, batch    37 | loss: 53.8307526
CurrentTrain: epoch  7, batch    38 | loss: 64.4835719
CurrentTrain: epoch  7, batch    39 | loss: 82.7932641
CurrentTrain: epoch  7, batch    40 | loss: 52.5041527
CurrentTrain: epoch  7, batch    41 | loss: 117.6407859
CurrentTrain: epoch  7, batch    42 | loss: 65.4912505
CurrentTrain: epoch  7, batch    43 | loss: 83.1814329
CurrentTrain: epoch  7, batch    44 | loss: 52.3314947
CurrentTrain: epoch  7, batch    45 | loss: 67.8858210
CurrentTrain: epoch  7, batch    46 | loss: 80.1391846
CurrentTrain: epoch  7, batch    47 | loss: 118.3010282
CurrentTrain: epoch  7, batch    48 | loss: 41.8776020
CurrentTrain: epoch  7, batch    49 | loss: 64.4378514
CurrentTrain: epoch  7, batch    50 | loss: 86.4909168
CurrentTrain: epoch  7, batch    51 | loss: 67.3068794
CurrentTrain: epoch  7, batch    52 | loss: 65.6639423
CurrentTrain: epoch  7, batch    53 | loss: 53.8325726
CurrentTrain: epoch  7, batch    54 | loss: 115.5645711
CurrentTrain: epoch  7, batch    55 | loss: 53.7966564
CurrentTrain: epoch  7, batch    56 | loss: 67.3327131
CurrentTrain: epoch  7, batch    57 | loss: 61.0064343
CurrentTrain: epoch  7, batch    58 | loss: 53.7501941
CurrentTrain: epoch  7, batch    59 | loss: 60.5980656
CurrentTrain: epoch  7, batch    60 | loss: 83.1127913
CurrentTrain: epoch  7, batch    61 | loss: 85.8478553
CurrentTrain: epoch  7, batch    62 | loss: 69.7873760
CurrentTrain: epoch  7, batch    63 | loss: 63.6252094
CurrentTrain: epoch  7, batch    64 | loss: 117.6577191
CurrentTrain: epoch  7, batch    65 | loss: 62.4600828
CurrentTrain: epoch  7, batch    66 | loss: 85.7972484
CurrentTrain: epoch  7, batch    67 | loss: 181.6670375
CurrentTrain: epoch  7, batch    68 | loss: 49.9224982
CurrentTrain: epoch  7, batch    69 | loss: 50.9739610
CurrentTrain: epoch  7, batch    70 | loss: 63.2883846
CurrentTrain: epoch  7, batch    71 | loss: 83.7275855
CurrentTrain: epoch  7, batch    72 | loss: 43.7587493
CurrentTrain: epoch  7, batch    73 | loss: 52.1625549
CurrentTrain: epoch  7, batch    74 | loss: 80.4073131
CurrentTrain: epoch  7, batch    75 | loss: 85.9109303
CurrentTrain: epoch  7, batch    76 | loss: 43.5178468
CurrentTrain: epoch  7, batch    77 | loss: 62.5474581
CurrentTrain: epoch  7, batch    78 | loss: 62.9183060
CurrentTrain: epoch  7, batch    79 | loss: 66.8035071
CurrentTrain: epoch  7, batch    80 | loss: 63.2933335
CurrentTrain: epoch  7, batch    81 | loss: 64.6448961
CurrentTrain: epoch  7, batch    82 | loss: 66.1087706
CurrentTrain: epoch  7, batch    83 | loss: 82.0183969
CurrentTrain: epoch  7, batch    84 | loss: 82.5430113
CurrentTrain: epoch  7, batch    85 | loss: 50.6256884
CurrentTrain: epoch  7, batch    86 | loss: 83.5944015
CurrentTrain: epoch  7, batch    87 | loss: 52.8412430
CurrentTrain: epoch  7, batch    88 | loss: 64.3192798
CurrentTrain: epoch  7, batch    89 | loss: 72.4999950
CurrentTrain: epoch  7, batch    90 | loss: 79.0815971
CurrentTrain: epoch  7, batch    91 | loss: 52.2448745
CurrentTrain: epoch  7, batch    92 | loss: 66.0786472
CurrentTrain: epoch  7, batch    93 | loss: 54.0807519
CurrentTrain: epoch  7, batch    94 | loss: 89.1521100
CurrentTrain: epoch  7, batch    95 | loss: 43.5394426
CurrentTrain: epoch  8, batch     0 | loss: 50.7587058
CurrentTrain: epoch  8, batch     1 | loss: 82.0402320
CurrentTrain: epoch  8, batch     2 | loss: 43.1119852
CurrentTrain: epoch  8, batch     3 | loss: 51.9686276
CurrentTrain: epoch  8, batch     4 | loss: 62.3129258
CurrentTrain: epoch  8, batch     5 | loss: 42.5028732
CurrentTrain: epoch  8, batch     6 | loss: 60.7766126
CurrentTrain: epoch  8, batch     7 | loss: 66.5902972
CurrentTrain: epoch  8, batch     8 | loss: 177.5950495
CurrentTrain: epoch  8, batch     9 | loss: 84.8978498
CurrentTrain: epoch  8, batch    10 | loss: 63.1778696
CurrentTrain: epoch  8, batch    11 | loss: 63.2616570
CurrentTrain: epoch  8, batch    12 | loss: 115.2183464
CurrentTrain: epoch  8, batch    13 | loss: 82.3788143
CurrentTrain: epoch  8, batch    14 | loss: 52.9240499
CurrentTrain: epoch  8, batch    15 | loss: 52.0155959
CurrentTrain: epoch  8, batch    16 | loss: 64.5136478
CurrentTrain: epoch  8, batch    17 | loss: 177.6168539
CurrentTrain: epoch  8, batch    18 | loss: 51.4164036
CurrentTrain: epoch  8, batch    19 | loss: 87.3626466
CurrentTrain: epoch  8, batch    20 | loss: 44.8627519
CurrentTrain: epoch  8, batch    21 | loss: 51.3436589
CurrentTrain: epoch  8, batch    22 | loss: 117.5526658
CurrentTrain: epoch  8, batch    23 | loss: 63.3254994
CurrentTrain: epoch  8, batch    24 | loss: 83.4027416
CurrentTrain: epoch  8, batch    25 | loss: 54.3374498
CurrentTrain: epoch  8, batch    26 | loss: 52.3094529
CurrentTrain: epoch  8, batch    27 | loss: 43.3068324
CurrentTrain: epoch  8, batch    28 | loss: 51.5579452
CurrentTrain: epoch  8, batch    29 | loss: 52.1564512
CurrentTrain: epoch  8, batch    30 | loss: 82.8027722
CurrentTrain: epoch  8, batch    31 | loss: 42.2802644
CurrentTrain: epoch  8, batch    32 | loss: 53.3820652
CurrentTrain: epoch  8, batch    33 | loss: 68.2648477
CurrentTrain: epoch  8, batch    34 | loss: 62.9764608
CurrentTrain: epoch  8, batch    35 | loss: 83.4489504
CurrentTrain: epoch  8, batch    36 | loss: 79.6910823
CurrentTrain: epoch  8, batch    37 | loss: 63.3670379
CurrentTrain: epoch  8, batch    38 | loss: 52.5867203
CurrentTrain: epoch  8, batch    39 | loss: 52.1599715
CurrentTrain: epoch  8, batch    40 | loss: 64.3427926
CurrentTrain: epoch  8, batch    41 | loss: 66.7270649
CurrentTrain: epoch  8, batch    42 | loss: 50.8975756
CurrentTrain: epoch  8, batch    43 | loss: 52.3074052
CurrentTrain: epoch  8, batch    44 | loss: 62.4231503
CurrentTrain: epoch  8, batch    45 | loss: 64.5889597
CurrentTrain: epoch  8, batch    46 | loss: 64.5376965
CurrentTrain: epoch  8, batch    47 | loss: 54.3388935
CurrentTrain: epoch  8, batch    48 | loss: 82.6584568
CurrentTrain: epoch  8, batch    49 | loss: 85.2605023
CurrentTrain: epoch  8, batch    50 | loss: 64.3094863
CurrentTrain: epoch  8, batch    51 | loss: 48.2790269
CurrentTrain: epoch  8, batch    52 | loss: 117.4677988
CurrentTrain: epoch  8, batch    53 | loss: 64.5110471
CurrentTrain: epoch  8, batch    54 | loss: 64.3289679
CurrentTrain: epoch  8, batch    55 | loss: 67.3380858
CurrentTrain: epoch  8, batch    56 | loss: 80.1305091
CurrentTrain: epoch  8, batch    57 | loss: 61.1937705
CurrentTrain: epoch  8, batch    58 | loss: 56.4891337
CurrentTrain: epoch  8, batch    59 | loss: 66.8680916
CurrentTrain: epoch  8, batch    60 | loss: 87.4673703
CurrentTrain: epoch  8, batch    61 | loss: 42.5588428
CurrentTrain: epoch  8, batch    62 | loss: 63.1734456
CurrentTrain: epoch  8, batch    63 | loss: 115.7784646
CurrentTrain: epoch  8, batch    64 | loss: 82.4194261
CurrentTrain: epoch  8, batch    65 | loss: 58.2712170
CurrentTrain: epoch  8, batch    66 | loss: 84.0563245
CurrentTrain: epoch  8, batch    67 | loss: 121.1064182
CurrentTrain: epoch  8, batch    68 | loss: 63.1275611
CurrentTrain: epoch  8, batch    69 | loss: 63.0148837
CurrentTrain: epoch  8, batch    70 | loss: 86.2882481
CurrentTrain: epoch  8, batch    71 | loss: 112.4587497
CurrentTrain: epoch  8, batch    72 | loss: 77.1499770
CurrentTrain: epoch  8, batch    73 | loss: 66.7658512
CurrentTrain: epoch  8, batch    74 | loss: 53.4830936
CurrentTrain: epoch  8, batch    75 | loss: 63.1627796
CurrentTrain: epoch  8, batch    76 | loss: 64.0072945
CurrentTrain: epoch  8, batch    77 | loss: 66.7114389
CurrentTrain: epoch  8, batch    78 | loss: 53.7175934
CurrentTrain: epoch  8, batch    79 | loss: 61.0054718
CurrentTrain: epoch  8, batch    80 | loss: 51.1311407
CurrentTrain: epoch  8, batch    81 | loss: 86.1553565
CurrentTrain: epoch  8, batch    82 | loss: 48.2254050
CurrentTrain: epoch  8, batch    83 | loss: 52.6965982
CurrentTrain: epoch  8, batch    84 | loss: 50.6218383
CurrentTrain: epoch  8, batch    85 | loss: 85.7754426
CurrentTrain: epoch  8, batch    86 | loss: 53.3400033
CurrentTrain: epoch  8, batch    87 | loss: 65.3967578
CurrentTrain: epoch  8, batch    88 | loss: 64.7714609
CurrentTrain: epoch  8, batch    89 | loss: 79.4596547
CurrentTrain: epoch  8, batch    90 | loss: 67.7535891
CurrentTrain: epoch  8, batch    91 | loss: 89.2570365
CurrentTrain: epoch  8, batch    92 | loss: 62.9886975
CurrentTrain: epoch  8, batch    93 | loss: 65.7565829
CurrentTrain: epoch  8, batch    94 | loss: 84.2215930
CurrentTrain: epoch  8, batch    95 | loss: 149.0049456
CurrentTrain: epoch  9, batch     0 | loss: 64.5729012
CurrentTrain: epoch  9, batch     1 | loss: 62.4986191
CurrentTrain: epoch  9, batch     2 | loss: 61.3730084
CurrentTrain: epoch  9, batch     3 | loss: 52.1775394
CurrentTrain: epoch  9, batch     4 | loss: 76.5599979
CurrentTrain: epoch  9, batch     5 | loss: 61.2044843
CurrentTrain: epoch  9, batch     6 | loss: 52.1384694
CurrentTrain: epoch  9, batch     7 | loss: 63.3143984
CurrentTrain: epoch  9, batch     8 | loss: 51.7071607
CurrentTrain: epoch  9, batch     9 | loss: 51.5616354
CurrentTrain: epoch  9, batch    10 | loss: 117.6437301
CurrentTrain: epoch  9, batch    11 | loss: 50.2294626
CurrentTrain: epoch  9, batch    12 | loss: 81.0242023
CurrentTrain: epoch  9, batch    13 | loss: 61.9276510
CurrentTrain: epoch  9, batch    14 | loss: 81.2073252
CurrentTrain: epoch  9, batch    15 | loss: 65.5011595
CurrentTrain: epoch  9, batch    16 | loss: 53.0254993
CurrentTrain: epoch  9, batch    17 | loss: 54.0810379
CurrentTrain: epoch  9, batch    18 | loss: 64.3143281
CurrentTrain: epoch  9, batch    19 | loss: 50.0590786
CurrentTrain: epoch  9, batch    20 | loss: 42.1019230
CurrentTrain: epoch  9, batch    21 | loss: 63.6107898
CurrentTrain: epoch  9, batch    22 | loss: 82.6059119
CurrentTrain: epoch  9, batch    23 | loss: 66.7254039
CurrentTrain: epoch  9, batch    24 | loss: 63.1527296
CurrentTrain: epoch  9, batch    25 | loss: 62.0460485
CurrentTrain: epoch  9, batch    26 | loss: 63.4317676
CurrentTrain: epoch  9, batch    27 | loss: 66.7473980
CurrentTrain: epoch  9, batch    28 | loss: 54.1916249
CurrentTrain: epoch  9, batch    29 | loss: 81.1474790
CurrentTrain: epoch  9, batch    30 | loss: 117.4962028
CurrentTrain: epoch  9, batch    31 | loss: 82.7700723
CurrentTrain: epoch  9, batch    32 | loss: 85.7299492
CurrentTrain: epoch  9, batch    33 | loss: 82.2927258
CurrentTrain: epoch  9, batch    34 | loss: 64.1639138
CurrentTrain: epoch  9, batch    35 | loss: 49.9136985
CurrentTrain: epoch  9, batch    36 | loss: 52.3384842
CurrentTrain: epoch  9, batch    37 | loss: 52.9916133
CurrentTrain: epoch  9, batch    38 | loss: 62.1284797
CurrentTrain: epoch  9, batch    39 | loss: 62.6032276
CurrentTrain: epoch  9, batch    40 | loss: 115.7230510
CurrentTrain: epoch  9, batch    41 | loss: 51.0824420
CurrentTrain: epoch  9, batch    42 | loss: 53.4144212
CurrentTrain: epoch  9, batch    43 | loss: 115.1979143
CurrentTrain: epoch  9, batch    44 | loss: 64.9218565
CurrentTrain: epoch  9, batch    45 | loss: 115.2632367
CurrentTrain: epoch  9, batch    46 | loss: 65.5279492
CurrentTrain: epoch  9, batch    47 | loss: 82.7038097
CurrentTrain: epoch  9, batch    48 | loss: 52.1445661
CurrentTrain: epoch  9, batch    49 | loss: 66.1443644
CurrentTrain: epoch  9, batch    50 | loss: 111.0782368
CurrentTrain: epoch  9, batch    51 | loss: 52.4241772
CurrentTrain: epoch  9, batch    52 | loss: 83.5277428
CurrentTrain: epoch  9, batch    53 | loss: 62.9315910
CurrentTrain: epoch  9, batch    54 | loss: 40.7107459
CurrentTrain: epoch  9, batch    55 | loss: 61.6207823
CurrentTrain: epoch  9, batch    56 | loss: 48.6135207
CurrentTrain: epoch  9, batch    57 | loss: 117.4860147
CurrentTrain: epoch  9, batch    58 | loss: 65.2786138
CurrentTrain: epoch  9, batch    59 | loss: 82.8834451
CurrentTrain: epoch  9, batch    60 | loss: 62.9101868
CurrentTrain: epoch  9, batch    61 | loss: 117.7549008
CurrentTrain: epoch  9, batch    62 | loss: 86.0053092
CurrentTrain: epoch  9, batch    63 | loss: 81.0287017
CurrentTrain: epoch  9, batch    64 | loss: 41.9743627
CurrentTrain: epoch  9, batch    65 | loss: 181.7380202
CurrentTrain: epoch  9, batch    66 | loss: 53.3036677
CurrentTrain: epoch  9, batch    67 | loss: 50.9926018
CurrentTrain: epoch  9, batch    68 | loss: 41.1853178
CurrentTrain: epoch  9, batch    69 | loss: 82.4662287
CurrentTrain: epoch  9, batch    70 | loss: 82.4286534
CurrentTrain: epoch  9, batch    71 | loss: 64.0038134
CurrentTrain: epoch  9, batch    72 | loss: 51.9887461
CurrentTrain: epoch  9, batch    73 | loss: 50.9928441
CurrentTrain: epoch  9, batch    74 | loss: 81.5390252
CurrentTrain: epoch  9, batch    75 | loss: 81.8455320
CurrentTrain: epoch  9, batch    76 | loss: 84.1328849
CurrentTrain: epoch  9, batch    77 | loss: 117.4894365
CurrentTrain: epoch  9, batch    78 | loss: 55.1965850
CurrentTrain: epoch  9, batch    79 | loss: 62.8949534
CurrentTrain: epoch  9, batch    80 | loss: 56.3716828
CurrentTrain: epoch  9, batch    81 | loss: 60.8423564
CurrentTrain: epoch  9, batch    82 | loss: 64.4214766
CurrentTrain: epoch  9, batch    83 | loss: 64.6570314
CurrentTrain: epoch  9, batch    84 | loss: 84.2336402
CurrentTrain: epoch  9, batch    85 | loss: 115.1966222
CurrentTrain: epoch  9, batch    86 | loss: 62.8099033
CurrentTrain: epoch  9, batch    87 | loss: 117.5367360
CurrentTrain: epoch  9, batch    88 | loss: 181.4064564
CurrentTrain: epoch  9, batch    89 | loss: 79.2824759
CurrentTrain: epoch  9, batch    90 | loss: 42.8190779
CurrentTrain: epoch  9, batch    91 | loss: 44.2830391
CurrentTrain: epoch  9, batch    92 | loss: 82.3224010
CurrentTrain: epoch  9, batch    93 | loss: 63.0389689
CurrentTrain: epoch  9, batch    94 | loss: 62.9833756
CurrentTrain: epoch  9, batch    95 | loss: 90.7205189

F1 score per class: {32: 0.6590909090909091, 6: 0.8505747126436781, 19: 0.4, 24: 0.7624309392265194, 26: 0.93048128342246, 29: 0.8979591836734694}
Micro-average F1 score: 0.8115015974440895
Weighted-average F1 score: 0.8188748151883525
F1 score per class: {32: 0.7216494845360825, 6: 0.9130434782608695, 19: 0.5454545454545454, 24: 0.7613636363636364, 26: 0.9690721649484536, 29: 0.8775510204081632}
Micro-average F1 score: 0.8393039918116684
Weighted-average F1 score: 0.8407708926581647
F1 score per class: {32: 0.7216494845360825, 6: 0.9130434782608695, 19: 0.5454545454545454, 24: 0.7613636363636364, 26: 0.9690721649484536, 29: 0.8775510204081632}
Micro-average F1 score: 0.8393039918116684
Weighted-average F1 score: 0.8407708926581647

F1 score per class: {32: 0.6590909090909091, 6: 0.8505747126436781, 19: 0.4, 24: 0.7624309392265194, 26: 0.93048128342246, 29: 0.8979591836734694}
Micro-average F1 score: 0.8115015974440895
Weighted-average F1 score: 0.8188748151883525
F1 score per class: {32: 0.7216494845360825, 6: 0.9130434782608695, 19: 0.5454545454545454, 24: 0.7613636363636364, 26: 0.9690721649484536, 29: 0.8775510204081632}
Micro-average F1 score: 0.8393039918116684
Weighted-average F1 score: 0.8407708926581647
F1 score per class: {32: 0.7216494845360825, 6: 0.9130434782608695, 19: 0.5454545454545454, 24: 0.7613636363636364, 26: 0.9690721649484536, 29: 0.8775510204081632}
Micro-average F1 score: 0.8393039918116684
Weighted-average F1 score: 0.8407708926581647
cur_acc:  ['0.8115']
his_acc:  ['0.8115']
cur_acc des:  ['0.8393']
his_acc des:  ['0.8393']
cur_acc rrf:  ['0.8393']
his_acc rrf:  ['0.8393']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 105.1219367
CurrentTrain: epoch  0, batch     1 | loss: 76.5192127
CurrentTrain: epoch  0, batch     2 | loss: 95.1184804
CurrentTrain: epoch  0, batch     3 | loss: 17.2559254
CurrentTrain: epoch  1, batch     0 | loss: 73.0397406
CurrentTrain: epoch  1, batch     1 | loss: 58.5269109
CurrentTrain: epoch  1, batch     2 | loss: 94.2229084
CurrentTrain: epoch  1, batch     3 | loss: 11.0899864
CurrentTrain: epoch  2, batch     0 | loss: 58.1583469
CurrentTrain: epoch  2, batch     1 | loss: 60.1165884
CurrentTrain: epoch  2, batch     2 | loss: 57.4652886
CurrentTrain: epoch  2, batch     3 | loss: 11.8862030
CurrentTrain: epoch  3, batch     0 | loss: 58.4886268
CurrentTrain: epoch  3, batch     1 | loss: 56.7990641
CurrentTrain: epoch  3, batch     2 | loss: 66.8304035
CurrentTrain: epoch  3, batch     3 | loss: 7.1561327
CurrentTrain: epoch  4, batch     0 | loss: 70.0304829
CurrentTrain: epoch  4, batch     1 | loss: 64.9891979
CurrentTrain: epoch  4, batch     2 | loss: 85.1981409
CurrentTrain: epoch  4, batch     3 | loss: 11.3320249
CurrentTrain: epoch  5, batch     0 | loss: 66.3916987
CurrentTrain: epoch  5, batch     1 | loss: 79.2131069
CurrentTrain: epoch  5, batch     2 | loss: 84.9778303
CurrentTrain: epoch  5, batch     3 | loss: 10.9085517
CurrentTrain: epoch  6, batch     0 | loss: 51.8292484
CurrentTrain: epoch  6, batch     1 | loss: 54.1301611
CurrentTrain: epoch  6, batch     2 | loss: 53.8430501
CurrentTrain: epoch  6, batch     3 | loss: 27.3737304
CurrentTrain: epoch  7, batch     0 | loss: 64.8518946
CurrentTrain: epoch  7, batch     1 | loss: 65.1583483
CurrentTrain: epoch  7, batch     2 | loss: 50.8180167
CurrentTrain: epoch  7, batch     3 | loss: 27.3779566
CurrentTrain: epoch  8, batch     0 | loss: 51.1270752
CurrentTrain: epoch  8, batch     1 | loss: 67.5273505
CurrentTrain: epoch  8, batch     2 | loss: 51.6271420
CurrentTrain: epoch  8, batch     3 | loss: 6.3975331
CurrentTrain: epoch  9, batch     0 | loss: 51.1237630
CurrentTrain: epoch  9, batch     1 | loss: 51.3270270
CurrentTrain: epoch  9, batch     2 | loss: 52.5332508
CurrentTrain: epoch  9, batch     3 | loss: 27.3657395
MemoryTrain:  epoch  0, batch     0 | loss: 0.8372409
MemoryTrain:  epoch  1, batch     0 | loss: 0.5824888
MemoryTrain:  epoch  2, batch     0 | loss: 0.7608375
MemoryTrain:  epoch  3, batch     0 | loss: 0.4849098
MemoryTrain:  epoch  4, batch     0 | loss: 0.3981454
MemoryTrain:  epoch  5, batch     0 | loss: 0.3323089
MemoryTrain:  epoch  6, batch     0 | loss: 0.2774924
MemoryTrain:  epoch  7, batch     0 | loss: 0.2644972
MemoryTrain:  epoch  8, batch     0 | loss: 0.2103477
MemoryTrain:  epoch  9, batch     0 | loss: 0.1931593

F1 score per class: {6: 0.0, 7: 0.75, 40: 0.9166666666666666, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6, 27: 0.0, 31: 0.34146341463414637}
Micro-average F1 score: 0.4205607476635514
Weighted-average F1 score: 0.31397154471544714
F1 score per class: {6: 0.0, 7: 0.75, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6956521739130435, 27: 1.0, 31: 0.4888888888888889}
Micro-average F1 score: 0.5381165919282511
Weighted-average F1 score: 0.42634753409407905
F1 score per class: {6: 0.0, 7: 0.75, 40: 0.96, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6956521739130435, 27: 1.0, 31: 0.4888888888888889}
Micro-average F1 score: 0.5315315315315315
Weighted-average F1 score: 0.41668590087672214

F1 score per class: {32: 0.47619047619047616, 6: 0.09836065573770492, 7: 0.9166666666666666, 40: 0.703862660944206, 9: 0.18181818181818182, 19: 0.7630057803468208, 24: 0.42857142857142855, 26: 0.9247311827956989, 27: 0.0, 29: 0.8947368421052632, 31: 0.3146067415730337}
Micro-average F1 score: 0.6802374893977947
Weighted-average F1 score: 0.678582732867659
F1 score per class: {32: 0.5641025641025641, 6: 0.09375, 7: 0.9803921568627451, 40: 0.6968325791855203, 9: 0.3333333333333333, 19: 0.7640449438202247, 24: 0.5517241379310345, 26: 0.9533678756476683, 27: 0.8, 29: 0.8911917098445595, 31: 0.4}
Micro-average F1 score: 0.7024390243902439
Weighted-average F1 score: 0.6841382489438318
F1 score per class: {32: 0.5569620253164557, 6: 0.0967741935483871, 7: 0.96, 40: 0.6968325791855203, 9: 0.27586206896551724, 19: 0.7640449438202247, 24: 0.5333333333333333, 26: 0.9533678756476683, 27: 1.0, 29: 0.8736842105263158, 31: 0.4}
Micro-average F1 score: 0.6971428571428572
Weighted-average F1 score: 0.679541509288241
cur_acc:  ['0.8115', '0.4206']
his_acc:  ['0.8115', '0.6802']
cur_acc des:  ['0.8393', '0.5381']
his_acc des:  ['0.8393', '0.7024']
cur_acc rrf:  ['0.8393', '0.5315']
his_acc rrf:  ['0.8393', '0.6971']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 91.5293543
CurrentTrain: epoch  0, batch     1 | loss: 101.5764525
CurrentTrain: epoch  0, batch     2 | loss: 80.0099757
CurrentTrain: epoch  0, batch     3 | loss: 66.3650913
CurrentTrain: epoch  1, batch     0 | loss: 92.8465513
CurrentTrain: epoch  1, batch     1 | loss: 97.3764205
CurrentTrain: epoch  1, batch     2 | loss: 74.3213459
CurrentTrain: epoch  1, batch     3 | loss: 59.3468161
CurrentTrain: epoch  2, batch     0 | loss: 95.8542010
CurrentTrain: epoch  2, batch     1 | loss: 61.1684714
CurrentTrain: epoch  2, batch     2 | loss: 58.6225980
CurrentTrain: epoch  2, batch     3 | loss: 48.9281577
CurrentTrain: epoch  3, batch     0 | loss: 85.0902387
CurrentTrain: epoch  3, batch     1 | loss: 72.6685183
CurrentTrain: epoch  3, batch     2 | loss: 85.0815917
CurrentTrain: epoch  3, batch     3 | loss: 102.3882215
CurrentTrain: epoch  4, batch     0 | loss: 59.3669586
CurrentTrain: epoch  4, batch     1 | loss: 58.9681236
CurrentTrain: epoch  4, batch     2 | loss: 85.3439414
CurrentTrain: epoch  4, batch     3 | loss: 71.9473212
CurrentTrain: epoch  5, batch     0 | loss: 65.9767030
CurrentTrain: epoch  5, batch     1 | loss: 181.4770829
CurrentTrain: epoch  5, batch     2 | loss: 58.9648964
CurrentTrain: epoch  5, batch     3 | loss: 54.7468657
CurrentTrain: epoch  6, batch     0 | loss: 54.0375446
CurrentTrain: epoch  6, batch     1 | loss: 88.6460752
CurrentTrain: epoch  6, batch     2 | loss: 65.5247753
CurrentTrain: epoch  6, batch     3 | loss: 96.1384901
CurrentTrain: epoch  7, batch     0 | loss: 68.4533259
CurrentTrain: epoch  7, batch     1 | loss: 66.9687089
CurrentTrain: epoch  7, batch     2 | loss: 68.7153085
CurrentTrain: epoch  7, batch     3 | loss: 54.3373755
CurrentTrain: epoch  8, batch     0 | loss: 88.2750769
CurrentTrain: epoch  8, batch     1 | loss: 68.2854112
CurrentTrain: epoch  8, batch     2 | loss: 81.3202331
CurrentTrain: epoch  8, batch     3 | loss: 42.9648813
CurrentTrain: epoch  9, batch     0 | loss: 53.9131804
CurrentTrain: epoch  9, batch     1 | loss: 116.2180150
CurrentTrain: epoch  9, batch     2 | loss: 67.8029842
CurrentTrain: epoch  9, batch     3 | loss: 53.2391957
MemoryTrain:  epoch  0, batch     0 | loss: 0.8002523
MemoryTrain:  epoch  1, batch     0 | loss: 0.6497842
MemoryTrain:  epoch  2, batch     0 | loss: 0.4332357
MemoryTrain:  epoch  3, batch     0 | loss: 0.3565927
MemoryTrain:  epoch  4, batch     0 | loss: 0.2529518
MemoryTrain:  epoch  5, batch     0 | loss: 0.1984471
MemoryTrain:  epoch  6, batch     0 | loss: 0.1788228
MemoryTrain:  epoch  7, batch     0 | loss: 0.1358625
MemoryTrain:  epoch  8, batch     0 | loss: 0.1157094
MemoryTrain:  epoch  9, batch     0 | loss: 0.0897122

F1 score per class: {0: 0.9722222222222222, 32: 0.98989898989899, 4: 0.5714285714285714, 13: 0.76, 21: 0.810126582278481, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8985507246376812
Weighted-average F1 score: 0.8894061163074604
F1 score per class: {0: 0.9577464788732394, 32: 1.0, 4: 0.5714285714285714, 40: 0.7843137254901961, 13: 0.8674698795180723, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.9014084507042254
Weighted-average F1 score: 0.8768030465873874
F1 score per class: {0: 0.9577464788732394, 32: 1.0, 4: 0.5714285714285714, 13: 0.7843137254901961, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8957345971563981
Weighted-average F1 score: 0.8743441184868871

F1 score per class: {32: 0.958904109589041, 0: 0.98989898989899, 4: 0.5277777777777778, 6: 0.125, 7: 0.9166666666666666, 40: 0.25, 9: 0.7037037037037037, 13: 0.48717948717948717, 19: 0.7901234567901234, 21: 0.09523809523809523, 23: 0.7666666666666667, 24: 0.5714285714285714, 26: 0.9533678756476683, 27: 0.8, 29: 0.8156424581005587, 31: 0.45161290322580644}
Micro-average F1 score: 0.7382885696439725
Weighted-average F1 score: 0.7362023582088435
F1 score per class: {32: 0.9444444444444444, 0: 0.9950248756218906, 4: 0.543046357615894, 6: 0.1016949152542373, 7: 0.9803921568627451, 40: 0.23529411764705882, 9: 0.7102803738317757, 13: 0.6666666666666666, 19: 0.8571428571428571, 21: 0.16, 23: 0.75, 24: 0.47058823529411764, 26: 0.9538461538461539, 27: 0.6666666666666666, 29: 0.8923076923076924, 31: 0.47368421052631576}
Micro-average F1 score: 0.7521058965102286
Weighted-average F1 score: 0.7406617053456169
F1 score per class: {32: 0.9444444444444444, 0: 1.0, 4: 0.543046357615894, 6: 0.10714285714285714, 7: 0.96, 40: 0.21052631578947367, 9: 0.7102803738317757, 13: 0.625, 19: 0.8148148148148148, 21: 0.16, 23: 0.75, 24: 0.47058823529411764, 26: 0.9591836734693877, 27: 0.6666666666666666, 29: 0.8808290155440415, 31: 0.4954128440366973}
Micro-average F1 score: 0.7496977025392987
Weighted-average F1 score: 0.7387567550646327
cur_acc:  ['0.8115', '0.4206', '0.8986']
his_acc:  ['0.8115', '0.6802', '0.7383']
cur_acc des:  ['0.8393', '0.5381', '0.9014']
his_acc des:  ['0.8393', '0.7024', '0.7521']
cur_acc rrf:  ['0.8393', '0.5315', '0.8957']
his_acc rrf:  ['0.8393', '0.6971', '0.7497']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 76.0766481
CurrentTrain: epoch  0, batch     1 | loss: 64.7073274
CurrentTrain: epoch  0, batch     2 | loss: 99.3294192
CurrentTrain: epoch  0, batch     3 | loss: 91.7922434
CurrentTrain: epoch  0, batch     4 | loss: 80.1783960
CurrentTrain: epoch  1, batch     0 | loss: 121.8249383
CurrentTrain: epoch  1, batch     1 | loss: 123.5846210
CurrentTrain: epoch  1, batch     2 | loss: 69.6893565
CurrentTrain: epoch  1, batch     3 | loss: 58.3173889
CurrentTrain: epoch  1, batch     4 | loss: 47.4837555
CurrentTrain: epoch  2, batch     0 | loss: 67.5912141
CurrentTrain: epoch  2, batch     1 | loss: 92.2862003
CurrentTrain: epoch  2, batch     2 | loss: 57.8153593
CurrentTrain: epoch  2, batch     3 | loss: 69.6659261
CurrentTrain: epoch  2, batch     4 | loss: 78.4288784
CurrentTrain: epoch  3, batch     0 | loss: 67.9709061
CurrentTrain: epoch  3, batch     1 | loss: 86.1781292
CurrentTrain: epoch  3, batch     2 | loss: 69.4541614
CurrentTrain: epoch  3, batch     3 | loss: 68.9833934
CurrentTrain: epoch  3, batch     4 | loss: 114.2698179
CurrentTrain: epoch  4, batch     0 | loss: 118.6426405
CurrentTrain: epoch  4, batch     1 | loss: 116.1972607
CurrentTrain: epoch  4, batch     2 | loss: 118.5509710
CurrentTrain: epoch  4, batch     3 | loss: 53.3624401
CurrentTrain: epoch  4, batch     4 | loss: 39.7684468
CurrentTrain: epoch  5, batch     0 | loss: 84.3310019
CurrentTrain: epoch  5, batch     1 | loss: 53.0241591
CurrentTrain: epoch  5, batch     2 | loss: 68.3489601
CurrentTrain: epoch  5, batch     3 | loss: 84.7588705
CurrentTrain: epoch  5, batch     4 | loss: 54.5719523
CurrentTrain: epoch  6, batch     0 | loss: 67.2560800
CurrentTrain: epoch  6, batch     1 | loss: 66.2296655
CurrentTrain: epoch  6, batch     2 | loss: 81.1559046
CurrentTrain: epoch  6, batch     3 | loss: 64.0990012
CurrentTrain: epoch  6, batch     4 | loss: 116.4678942
CurrentTrain: epoch  7, batch     0 | loss: 86.1828457
CurrentTrain: epoch  7, batch     1 | loss: 67.6631833
CurrentTrain: epoch  7, batch     2 | loss: 52.8398022
CurrentTrain: epoch  7, batch     3 | loss: 51.9556157
CurrentTrain: epoch  7, batch     4 | loss: 73.2302156
CurrentTrain: epoch  8, batch     0 | loss: 67.1746188
CurrentTrain: epoch  8, batch     1 | loss: 52.5127528
CurrentTrain: epoch  8, batch     2 | loss: 64.7251718
CurrentTrain: epoch  8, batch     3 | loss: 67.4508195
CurrentTrain: epoch  8, batch     4 | loss: 41.1774741
CurrentTrain: epoch  9, batch     0 | loss: 54.3495546
CurrentTrain: epoch  9, batch     1 | loss: 64.4043849
CurrentTrain: epoch  9, batch     2 | loss: 66.1777039
CurrentTrain: epoch  9, batch     3 | loss: 65.6438884
CurrentTrain: epoch  9, batch     4 | loss: 41.0667778
MemoryTrain:  epoch  0, batch     0 | loss: 0.3688558
MemoryTrain:  epoch  1, batch     0 | loss: 0.3167582
MemoryTrain:  epoch  2, batch     0 | loss: 0.2156716
MemoryTrain:  epoch  3, batch     0 | loss: 0.1421489
MemoryTrain:  epoch  4, batch     0 | loss: 0.1046131
MemoryTrain:  epoch  5, batch     0 | loss: 0.0909996
MemoryTrain:  epoch  6, batch     0 | loss: 0.0743084
MemoryTrain:  epoch  7, batch     0 | loss: 0.0657998
MemoryTrain:  epoch  8, batch     0 | loss: 0.0563908
MemoryTrain:  epoch  9, batch     0 | loss: 0.0462309

F1 score per class: {5: 0.9637305699481865, 7: 0.0, 10: 0.6351351351351351, 13: 0.0, 16: 0.8461538461538461, 17: 0.0, 18: 0.52}
Micro-average F1 score: 0.7510729613733905
Weighted-average F1 score: 0.7657306005634339
F1 score per class: {5: 1.0, 6: 0.0, 7: 0.0, 40: 0.7636363636363637, 10: 0.0, 13: 0.9473684210526315, 16: 0.875, 17: 0.9428571428571428, 18: 0.0}
Micro-average F1 score: 0.8778625954198473
Weighted-average F1 score: 0.8566717609639937
F1 score per class: {5: 1.0, 6: 0.0, 7: 0.0, 40: 0.7636363636363637, 10: 0.0, 13: 0.9473684210526315, 16: 0.36363636363636365, 17: 0.7868852459016393, 18: 0.0}
Micro-average F1 score: 0.8388349514563107
Weighted-average F1 score: 0.8151792034882709

F1 score per class: {0: 0.9295774647887324, 4: 0.9583333333333334, 5: 0.9637305699481865, 6: 0.39705882352941174, 7: 0.0, 9: 0.96, 10: 0.6308724832214765, 13: 0.16666666666666666, 16: 0.8461538461538461, 17: 0.0, 18: 0.52, 19: 0.7064220183486238, 21: 0.4489795918367347, 23: 0.8, 24: 0.09090909090909091, 26: 0.75, 27: 0.4864864864864865, 29: 0.9533678756476683, 31: 0.5, 32: 0.7745664739884393, 40: 0.4175824175824176}
Micro-average F1 score: 0.7277277277277278
Weighted-average F1 score: 0.7435377788274976
F1 score per class: {0: 0.9444444444444444, 4: 0.9949748743718593, 5: 0.9900990099009901, 6: 0.5241379310344828, 7: 0.11428571428571428, 9: 0.9803921568627451, 10: 0.7544910179640718, 13: 0.19047619047619047, 16: 0.9152542372881356, 17: 0.32558139534883723, 18: 0.9428571428571428, 19: 0.7256637168141593, 21: 0.7241379310344828, 23: 0.9565217391304348, 24: 0.21428571428571427, 26: 0.7597765363128491, 27: 0.45454545454545453, 29: 0.9538461538461539, 31: 0.6666666666666666, 32: 0.8645833333333334, 40: 0.4}
Micro-average F1 score: 0.7838827838827839
Weighted-average F1 score: 0.7786991987994888
F1 score per class: {0: 0.9315068493150684, 4: 0.9949748743718593, 5: 0.9852216748768473, 6: 0.5263157894736842, 7: 0.09090909090909091, 9: 0.9803921568627451, 10: 0.7544910179640718, 13: 0.18181818181818182, 16: 0.9152542372881356, 17: 0.16666666666666666, 18: 0.7868852459016393, 19: 0.7288888888888889, 21: 0.6885245901639344, 23: 0.9090909090909091, 24: 0.21428571428571427, 26: 0.7597765363128491, 27: 0.4186046511627907, 29: 0.9538461538461539, 31: 0.6666666666666666, 32: 0.8465608465608465, 40: 0.3958333333333333}
Micro-average F1 score: 0.7713625866050808
Weighted-average F1 score: 0.7656508354311824
cur_acc:  ['0.8115', '0.4206', '0.8986', '0.7511']
his_acc:  ['0.8115', '0.6802', '0.7383', '0.7277']
cur_acc des:  ['0.8393', '0.5381', '0.9014', '0.8779']
his_acc des:  ['0.8393', '0.7024', '0.7521', '0.7839']
cur_acc rrf:  ['0.8393', '0.5315', '0.8957', '0.8388']
his_acc rrf:  ['0.8393', '0.6971', '0.7497', '0.7714']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 65.9680358
CurrentTrain: epoch  0, batch     1 | loss: 70.1833769
CurrentTrain: epoch  0, batch     2 | loss: 97.6491178
CurrentTrain: epoch  0, batch     3 | loss: 61.7284719
CurrentTrain: epoch  1, batch     0 | loss: 58.5126904
CurrentTrain: epoch  1, batch     1 | loss: 71.7300844
CurrentTrain: epoch  1, batch     2 | loss: 90.4553018
CurrentTrain: epoch  1, batch     3 | loss: 50.0445499
CurrentTrain: epoch  2, batch     0 | loss: 73.0521077
CurrentTrain: epoch  2, batch     1 | loss: 68.0462177
CurrentTrain: epoch  2, batch     2 | loss: 53.4495954
CurrentTrain: epoch  2, batch     3 | loss: 58.5473365
CurrentTrain: epoch  3, batch     0 | loss: 89.0995083
CurrentTrain: epoch  3, batch     1 | loss: 55.2997049
CurrentTrain: epoch  3, batch     2 | loss: 116.0821534
CurrentTrain: epoch  3, batch     3 | loss: 41.2829622
CurrentTrain: epoch  4, batch     0 | loss: 67.3805301
CurrentTrain: epoch  4, batch     1 | loss: 53.1837937
CurrentTrain: epoch  4, batch     2 | loss: 65.6611368
CurrentTrain: epoch  4, batch     3 | loss: 58.4506176
CurrentTrain: epoch  5, batch     0 | loss: 53.2854008
CurrentTrain: epoch  5, batch     1 | loss: 66.0905740
CurrentTrain: epoch  5, batch     2 | loss: 53.8827707
CurrentTrain: epoch  5, batch     3 | loss: 45.0486703
CurrentTrain: epoch  6, batch     0 | loss: 61.8926964
CurrentTrain: epoch  6, batch     1 | loss: 67.8941438
CurrentTrain: epoch  6, batch     2 | loss: 52.4477009
CurrentTrain: epoch  6, batch     3 | loss: 58.1566079
CurrentTrain: epoch  7, batch     0 | loss: 85.4023138
CurrentTrain: epoch  7, batch     1 | loss: 64.7646020
CurrentTrain: epoch  7, batch     2 | loss: 51.4226163
CurrentTrain: epoch  7, batch     3 | loss: 42.6884559
CurrentTrain: epoch  8, batch     0 | loss: 59.2069054
CurrentTrain: epoch  8, batch     1 | loss: 53.4866977
CurrentTrain: epoch  8, batch     2 | loss: 115.8424941
CurrentTrain: epoch  8, batch     3 | loss: 57.7660617
CurrentTrain: epoch  9, batch     0 | loss: 62.8594120
CurrentTrain: epoch  9, batch     1 | loss: 82.9942632
CurrentTrain: epoch  9, batch     2 | loss: 84.2712010
CurrentTrain: epoch  9, batch     3 | loss: 51.6658546
MemoryTrain:  epoch  0, batch     0 | loss: 0.2736353
MemoryTrain:  epoch  1, batch     0 | loss: 0.2113570
MemoryTrain:  epoch  2, batch     0 | loss: 0.1560359
MemoryTrain:  epoch  3, batch     0 | loss: 0.1166313
MemoryTrain:  epoch  4, batch     0 | loss: 0.0970880
MemoryTrain:  epoch  5, batch     0 | loss: 0.0858581
MemoryTrain:  epoch  6, batch     0 | loss: 0.0731776
MemoryTrain:  epoch  7, batch     0 | loss: 0.0636384
MemoryTrain:  epoch  8, batch     0 | loss: 0.0521927
MemoryTrain:  epoch  9, batch     0 | loss: 0.0438964

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.8888888888888888, 38: 0.0, 10: 0.0, 13: 0.0, 15: 0.4927536231884058, 18: 0.0, 21: 0.5555555555555556, 23: 0.42105263157894735, 25: 0.4444444444444444}
Micro-average F1 score: 0.46
Weighted-average F1 score: 0.37825239427070095
F1 score per class: {35: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 10: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.6666666666666666, 21: 0.0, 23: 0.9702970297029703, 25: 0.723404255319149, 27: 0.782608695652174}
Micro-average F1 score: 0.7169811320754716
Weighted-average F1 score: 0.6413581530651297
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.75, 38: 0.0, 10: 0.0, 13: 0.0, 15: 0.631578947368421, 18: 0.0, 21: 0.0, 23: 0.9166666666666666, 25: 0.723404255319149, 27: 0.782608695652174}
Micro-average F1 score: 0.6866485013623979
Weighted-average F1 score: 0.5999433490327248

F1 score per class: {0: 0.9577464788732394, 4: 0.9473684210526315, 5: 0.8744186046511628, 6: 0.43609022556390975, 7: 0.09302325581395349, 9: 0.96, 10: 0.7044025157232704, 13: 0.11764705882352941, 15: 0.7272727272727273, 16: 0.8888888888888888, 17: 0.0, 18: 0.34782608695652173, 19: 0.5444444444444444, 21: 0.5, 23: 0.8235294117647058, 24: 0.10526315789473684, 25: 0.4927536231884058, 26: 0.7528089887640449, 27: 0.5333333333333333, 29: 0.9479166666666666, 31: 0.5, 32: 0.7528089887640449, 35: 0.5405405405405406, 37: 0.36363636363636365, 38: 0.3902439024390244, 40: 0.2716049382716049}
Micro-average F1 score: 0.6768558951965066
Weighted-average F1 score: 0.7047620484890862
F1 score per class: {0: 0.9444444444444444, 4: 0.9795918367346939, 5: 0.8810572687224669, 6: 0.5679012345679012, 7: 0.1276595744680851, 9: 0.9803921568627451, 10: 0.7558139534883721, 13: 0.2222222222222222, 15: 0.5454545454545454, 16: 0.8852459016393442, 17: 0.3076923076923077, 18: 0.47058823529411764, 19: 0.6391752577319587, 21: 0.5217391304347826, 23: 0.8764044943820225, 24: 0.09090909090909091, 25: 0.6666666666666666, 26: 0.770949720670391, 27: 0.46153846153846156, 29: 0.9484536082474226, 31: 0.6666666666666666, 32: 0.8844221105527639, 35: 0.9245283018867925, 37: 0.5483870967741935, 38: 0.6, 40: 0.41237113402061853}
Micro-average F1 score: 0.7417582417582418
Weighted-average F1 score: 0.7440441667033303
F1 score per class: {0: 0.958904109589041, 4: 0.9795918367346939, 5: 0.8810572687224669, 6: 0.55, 7: 0.12244897959183673, 9: 0.9803921568627451, 10: 0.7398843930635838, 13: 0.2222222222222222, 15: 0.5, 16: 0.8727272727272727, 17: 0.0, 18: 0.47058823529411764, 19: 0.6031746031746031, 21: 0.4931506849315068, 23: 0.8505747126436781, 24: 0.09523809523809523, 25: 0.631578947368421, 26: 0.770949720670391, 27: 0.5, 29: 0.9484536082474226, 31: 0.8, 32: 0.8756218905472637, 35: 0.8888888888888888, 37: 0.5354330708661418, 38: 0.5901639344262295, 40: 0.40860215053763443}
Micro-average F1 score: 0.7299327797548438
Weighted-average F1 score: 0.73345846096966
cur_acc:  ['0.8115', '0.4206', '0.8986', '0.7511', '0.4600']
his_acc:  ['0.8115', '0.6802', '0.7383', '0.7277', '0.6769']
cur_acc des:  ['0.8393', '0.5381', '0.9014', '0.8779', '0.7170']
his_acc des:  ['0.8393', '0.7024', '0.7521', '0.7839', '0.7418']
cur_acc rrf:  ['0.8393', '0.5315', '0.8957', '0.8388', '0.6866']
his_acc rrf:  ['0.8393', '0.6971', '0.7497', '0.7714', '0.7299']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 113.6367829
CurrentTrain: epoch  0, batch     1 | loss: 86.5974408
CurrentTrain: epoch  0, batch     2 | loss: 80.6884424
CurrentTrain: epoch  0, batch     3 | loss: 69.4261448
CurrentTrain: epoch  0, batch     4 | loss: 34.5046337
CurrentTrain: epoch  1, batch     0 | loss: 64.9013814
CurrentTrain: epoch  1, batch     1 | loss: 97.8735647
CurrentTrain: epoch  1, batch     2 | loss: 62.6847511
CurrentTrain: epoch  1, batch     3 | loss: 78.6582018
CurrentTrain: epoch  1, batch     4 | loss: 60.1788423
CurrentTrain: epoch  2, batch     0 | loss: 74.7616612
CurrentTrain: epoch  2, batch     1 | loss: 74.7032853
CurrentTrain: epoch  2, batch     2 | loss: 56.7121650
CurrentTrain: epoch  2, batch     3 | loss: 123.2474383
CurrentTrain: epoch  2, batch     4 | loss: 30.2472373
CurrentTrain: epoch  3, batch     0 | loss: 90.8680213
CurrentTrain: epoch  3, batch     1 | loss: 124.1055432
CurrentTrain: epoch  3, batch     2 | loss: 56.2600343
CurrentTrain: epoch  3, batch     3 | loss: 73.5565444
CurrentTrain: epoch  3, batch     4 | loss: 12.1444291
CurrentTrain: epoch  4, batch     0 | loss: 67.9340154
CurrentTrain: epoch  4, batch     1 | loss: 70.5032966
CurrentTrain: epoch  4, batch     2 | loss: 87.9839606
CurrentTrain: epoch  4, batch     3 | loss: 67.9238795
CurrentTrain: epoch  4, batch     4 | loss: 60.1471328
CurrentTrain: epoch  5, batch     0 | loss: 87.6442977
CurrentTrain: epoch  5, batch     1 | loss: 65.6064841
CurrentTrain: epoch  5, batch     2 | loss: 54.6698235
CurrentTrain: epoch  5, batch     3 | loss: 69.3604007
CurrentTrain: epoch  5, batch     4 | loss: 60.1772357
CurrentTrain: epoch  6, batch     0 | loss: 55.4672158
CurrentTrain: epoch  6, batch     1 | loss: 86.3613849
CurrentTrain: epoch  6, batch     2 | loss: 67.7688209
CurrentTrain: epoch  6, batch     3 | loss: 84.0258286
CurrentTrain: epoch  6, batch     4 | loss: 27.3312424
CurrentTrain: epoch  7, batch     0 | loss: 65.0940772
CurrentTrain: epoch  7, batch     1 | loss: 56.2874954
CurrentTrain: epoch  7, batch     2 | loss: 67.9190545
CurrentTrain: epoch  7, batch     3 | loss: 82.6484404
CurrentTrain: epoch  7, batch     4 | loss: 60.0928523
CurrentTrain: epoch  8, batch     0 | loss: 54.0786388
CurrentTrain: epoch  8, batch     1 | loss: 86.5615493
CurrentTrain: epoch  8, batch     2 | loss: 86.8682084
CurrentTrain: epoch  8, batch     3 | loss: 62.6241147
CurrentTrain: epoch  8, batch     4 | loss: 26.9904508
CurrentTrain: epoch  9, batch     0 | loss: 53.2586391
CurrentTrain: epoch  9, batch     1 | loss: 84.0113266
CurrentTrain: epoch  9, batch     2 | loss: 83.8730171
CurrentTrain: epoch  9, batch     3 | loss: 82.8230986
CurrentTrain: epoch  9, batch     4 | loss: 60.0425384
MemoryTrain:  epoch  0, batch     0 | loss: 0.3250894
MemoryTrain:  epoch  1, batch     0 | loss: 0.2960600
MemoryTrain:  epoch  2, batch     0 | loss: 0.2273917
MemoryTrain:  epoch  3, batch     0 | loss: 0.1823420
MemoryTrain:  epoch  4, batch     0 | loss: 0.1503957
MemoryTrain:  epoch  5, batch     0 | loss: 0.1279946
MemoryTrain:  epoch  6, batch     0 | loss: 0.1095638
MemoryTrain:  epoch  7, batch     0 | loss: 0.0924929
MemoryTrain:  epoch  8, batch     0 | loss: 0.0802483
MemoryTrain:  epoch  9, batch     0 | loss: 0.0684556

F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 38: 0.4424778761061947, 39: 0.4496124031007752, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.42857142857142855, 21: 0.0, 27: 0.0, 28: 0.0}
Micro-average F1 score: 0.41830065359477125
Weighted-average F1 score: 0.3769785651035163
F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 38: 0.8187919463087249, 39: 0.7215189873417721, 10: 0.0, 11: 0.0, 12: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.6, 21: 0.0, 27: 0.0, 28: 0.0, 29: 0.125}
Micro-average F1 score: 0.6683544303797468
Weighted-average F1 score: 0.5986898294923578
F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 38: 0.8496732026143791, 39: 0.7295597484276729, 10: 0.0, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.6, 21: 0.0, 27: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.6956521739130435
Weighted-average F1 score: 0.6488243216695775

F1 score per class: {0: 0.9142857142857143, 2: 0.875, 4: 0.9583333333333334, 5: 0.8636363636363636, 6: 0.3387096774193548, 7: 0.03333333333333333, 9: 0.96, 10: 0.359375, 11: 0.3597122302158273, 12: 0.44274809160305345, 13: 0.13333333333333333, 15: 0.5517241379310345, 16: 0.8275862068965517, 17: 0.0, 18: 0.15, 19: 0.5222222222222223, 21: 0.34615384615384615, 23: 0.75, 24: 0.10526315789473684, 25: 0.4, 26: 0.7398843930635838, 27: 0.48, 28: 0.2608695652173913, 29: 0.9424083769633508, 31: 0.0, 32: 0.8235294117647058, 35: 0.44776119402985076, 37: 0.29333333333333333, 38: 0.22857142857142856, 39: 0.0, 40: 0.20512820512820512}
Micro-average F1 score: 0.5974941268598277
Weighted-average F1 score: 0.6501808180451738
F1 score per class: {0: 0.958904109589041, 2: 0.7777777777777778, 4: 0.9847715736040609, 5: 0.8810572687224669, 6: 0.40298507462686567, 7: 0.05714285714285714, 9: 0.9803921568627451, 10: 0.618421052631579, 11: 0.5781990521327014, 12: 0.6826347305389222, 13: 0.18181818181818182, 15: 0.46153846153846156, 16: 0.8387096774193549, 17: 0.0, 18: 0.37037037037037035, 19: 0.6507177033492823, 21: 0.6388888888888888, 23: 0.8372093023255814, 24: 0.09090909090909091, 25: 0.631578947368421, 26: 0.7597765363128491, 27: 0.48484848484848486, 28: 0.24489795918367346, 29: 0.9381443298969072, 31: 0.8, 32: 0.8855721393034826, 35: 0.9019607843137255, 37: 0.17391304347826086, 38: 0.5, 39: 0.125, 40: 0.37894736842105264}
Micro-average F1 score: 0.6834924965893588
Weighted-average F1 score: 0.6926425803770047
F1 score per class: {0: 0.958904109589041, 2: 0.7777777777777778, 4: 0.9847715736040609, 5: 0.8810572687224669, 6: 0.40601503759398494, 7: 0.056338028169014086, 9: 0.9803921568627451, 10: 0.6274509803921569, 11: 0.6161137440758294, 12: 0.6863905325443787, 13: 0.15384615384615385, 15: 0.46153846153846156, 16: 0.8666666666666667, 17: 0.0, 18: 0.3333333333333333, 19: 0.6161616161616161, 21: 0.5974025974025974, 23: 0.8333333333333334, 24: 0.1, 25: 0.6133333333333333, 26: 0.7570621468926554, 27: 0.5333333333333333, 28: 0.2857142857142857, 29: 0.9430051813471503, 31: 0.8, 32: 0.8682926829268293, 35: 0.8421052631578947, 37: 0.24324324324324326, 38: 0.4827586206896552, 39: 0.0, 40: 0.3617021276595745}
Micro-average F1 score: 0.6818025455796354
Weighted-average F1 score: 0.6923756617339519
cur_acc:  ['0.8115', '0.4206', '0.8986', '0.7511', '0.4600', '0.4183']
his_acc:  ['0.8115', '0.6802', '0.7383', '0.7277', '0.6769', '0.5975']
cur_acc des:  ['0.8393', '0.5381', '0.9014', '0.8779', '0.7170', '0.6684']
his_acc des:  ['0.8393', '0.7024', '0.7521', '0.7839', '0.7418', '0.6835']
cur_acc rrf:  ['0.8393', '0.5315', '0.8957', '0.8388', '0.6866', '0.6957']
his_acc rrf:  ['0.8393', '0.6971', '0.7497', '0.7714', '0.7299', '0.6818']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 70.5192097
CurrentTrain: epoch  0, batch     1 | loss: 71.6761958
CurrentTrain: epoch  0, batch     2 | loss: 190.4568397
CurrentTrain: epoch  0, batch     3 | loss: 65.9751114
CurrentTrain: epoch  0, batch     4 | loss: 111.6792399
CurrentTrain: epoch  1, batch     0 | loss: 185.7441721
CurrentTrain: epoch  1, batch     1 | loss: 94.1371880
CurrentTrain: epoch  1, batch     2 | loss: 94.0782574
CurrentTrain: epoch  1, batch     3 | loss: 60.9336274
CurrentTrain: epoch  1, batch     4 | loss: 40.8586515
CurrentTrain: epoch  2, batch     0 | loss: 73.2085972
CurrentTrain: epoch  2, batch     1 | loss: 70.4525273
CurrentTrain: epoch  2, batch     2 | loss: 91.7712114
CurrentTrain: epoch  2, batch     3 | loss: 87.1847685
CurrentTrain: epoch  2, batch     4 | loss: 70.1039990
CurrentTrain: epoch  3, batch     0 | loss: 68.0279627
CurrentTrain: epoch  3, batch     1 | loss: 59.6964834
CurrentTrain: epoch  3, batch     2 | loss: 68.3996000
CurrentTrain: epoch  3, batch     3 | loss: 89.8337410
CurrentTrain: epoch  3, batch     4 | loss: 48.6756040
CurrentTrain: epoch  4, batch     0 | loss: 68.4361218
CurrentTrain: epoch  4, batch     1 | loss: 70.5375247
CurrentTrain: epoch  4, batch     2 | loss: 67.6818742
CurrentTrain: epoch  4, batch     3 | loss: 71.4218285
CurrentTrain: epoch  4, batch     4 | loss: 46.4466914
CurrentTrain: epoch  5, batch     0 | loss: 87.3120563
CurrentTrain: epoch  5, batch     1 | loss: 116.2133671
CurrentTrain: epoch  5, batch     2 | loss: 84.8879488
CurrentTrain: epoch  5, batch     3 | loss: 66.6609390
CurrentTrain: epoch  5, batch     4 | loss: 28.9784896
CurrentTrain: epoch  6, batch     0 | loss: 55.5572548
CurrentTrain: epoch  6, batch     1 | loss: 67.1548387
CurrentTrain: epoch  6, batch     2 | loss: 67.2776894
CurrentTrain: epoch  6, batch     3 | loss: 65.4848394
CurrentTrain: epoch  6, batch     4 | loss: 103.0553034
CurrentTrain: epoch  7, batch     0 | loss: 65.0539945
CurrentTrain: epoch  7, batch     1 | loss: 80.6647364
CurrentTrain: epoch  7, batch     2 | loss: 182.7293947
CurrentTrain: epoch  7, batch     3 | loss: 66.4341007
CurrentTrain: epoch  7, batch     4 | loss: 46.2474707
CurrentTrain: epoch  8, batch     0 | loss: 55.5099679
CurrentTrain: epoch  8, batch     1 | loss: 178.9070177
CurrentTrain: epoch  8, batch     2 | loss: 83.6320624
CurrentTrain: epoch  8, batch     3 | loss: 84.8578218
CurrentTrain: epoch  8, batch     4 | loss: 32.4735518
CurrentTrain: epoch  9, batch     0 | loss: 67.5132210
CurrentTrain: epoch  9, batch     1 | loss: 49.9716995
CurrentTrain: epoch  9, batch     2 | loss: 67.9300803
CurrentTrain: epoch  9, batch     3 | loss: 84.7057513
CurrentTrain: epoch  9, batch     4 | loss: 65.5871202
MemoryTrain:  epoch  0, batch     0 | loss: 0.5367610
MemoryTrain:  epoch  1, batch     0 | loss: 0.4758598
MemoryTrain:  epoch  2, batch     0 | loss: 0.3528650
MemoryTrain:  epoch  3, batch     0 | loss: 0.2965190
MemoryTrain:  epoch  4, batch     0 | loss: 0.2394920
MemoryTrain:  epoch  5, batch     0 | loss: 0.2061940
MemoryTrain:  epoch  6, batch     0 | loss: 0.1683230
MemoryTrain:  epoch  7, batch     0 | loss: 0.1282756
MemoryTrain:  epoch  8, batch     0 | loss: 0.1104242
MemoryTrain:  epoch  9, batch     0 | loss: 0.0942545

F1 score per class: {32: 0.43478260869565216, 1: 0.6333333333333333, 34: 0.0, 35: 0.1038961038961039, 3: 0.0, 37: 0.0, 11: 0.711864406779661, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.5526315789473685, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.4866666666666667
Weighted-average F1 score: 0.43935212421178277
F1 score per class: {32: 0.43037974683544306, 1: 0.6333333333333333, 34: 0.0, 3: 0.0, 35: 0.08888888888888889, 10: 0.0, 11: 0.0, 14: 0.6947368421052632, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.9215686274509803, 27: 0.0}
Micro-average F1 score: 0.5134328358208955
Weighted-average F1 score: 0.45094461154270893
F1 score per class: {32: 0.42857142857142855, 1: 0.7286821705426356, 34: 0.0, 3: 0.0, 35: 0.1111111111111111, 10: 0.0, 11: 0.0, 14: 0.6881720430107527, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.9108910891089109, 27: 0.0}
Micro-average F1 score: 0.5397301349325337
Weighted-average F1 score: 0.48020313601051473

F1 score per class: {0: 0.8307692307692308, 1: 0.34782608695652173, 2: 0.7368421052631579, 3: 0.6031746031746031, 4: 0.9130434782608695, 5: 0.8744186046511628, 6: 0.3114754098360656, 7: 0.046511627906976744, 9: 0.96, 10: 0.4496124031007752, 11: 0.19047619047619047, 12: 0.21428571428571427, 13: 0.18181818181818182, 14: 0.09195402298850575, 15: 0.5454545454545454, 16: 0.7777777777777778, 17: 0.0, 18: 0.0, 19: 0.302158273381295, 21: 0.1111111111111111, 22: 0.6810810810810811, 23: 0.7804878048780488, 24: 0.08695652173913043, 25: 0.44776119402985076, 26: 0.7471264367816092, 27: 0.14285714285714285, 28: 0.23076923076923078, 29: 0.9197860962566845, 31: 0.0, 32: 0.6233766233766234, 34: 0.27450980392156865, 35: 0.20588235294117646, 37: 0.12307692307692308, 38: 0.2222222222222222, 39: 0.0, 40: 0.1111111111111111}
Micro-average F1 score: 0.5095081967213114
Weighted-average F1 score: 0.5655188958299703
F1 score per class: {0: 0.9142857142857143, 1: 0.35789473684210527, 2: 0.6363636363636364, 3: 0.5984251968503937, 4: 0.9583333333333334, 5: 0.8583690987124464, 6: 0.4117647058823529, 7: 0.0, 9: 0.9803921568627451, 10: 0.6496815286624203, 11: 0.29508196721311475, 12: 0.6878980891719745, 13: 0.14285714285714285, 14: 0.07476635514018691, 15: 0.48, 16: 0.8666666666666667, 17: 0.125, 18: 0.046511627906976744, 19: 0.5269461077844312, 21: 0.25, 22: 0.6534653465346535, 23: 0.7804878048780488, 24: 0.07692307692307693, 25: 0.6133333333333333, 26: 0.7391304347826086, 27: 0.14035087719298245, 28: 0.20512820512820512, 29: 0.9326424870466321, 31: 0.0, 32: 0.8186528497409327, 34: 0.41228070175438597, 35: 0.6511627906976745, 37: 0.11594202898550725, 38: 0.5862068965517241, 39: 0.2222222222222222, 40: 0.46938775510204084}
Micro-average F1 score: 0.5931773329574288
Weighted-average F1 score: 0.5978402096274269
F1 score per class: {0: 0.8985507246376812, 1: 0.32432432432432434, 2: 0.6363636363636364, 3: 0.6527777777777778, 4: 0.9361702127659575, 5: 0.8660714285714286, 6: 0.4, 7: 0.0, 9: 0.96, 10: 0.632258064516129, 11: 0.2857142857142857, 12: 0.631578947368421, 13: 0.14285714285714285, 14: 0.09345794392523364, 15: 0.48, 16: 0.8421052631578947, 17: 0.0, 18: 0.0, 19: 0.4875, 21: 0.2, 22: 0.6432160804020101, 23: 0.7804878048780488, 24: 0.07692307692307693, 25: 0.6133333333333333, 26: 0.7403314917127072, 27: 0.14285714285714285, 28: 0.20512820512820512, 29: 0.9368421052631579, 31: 0.0, 32: 0.7262569832402235, 34: 0.3948497854077253, 35: 0.425531914893617, 37: 0.11594202898550725, 38: 0.5666666666666667, 39: 0.0, 40: 0.41304347826086957}
Micro-average F1 score: 0.5668387837062536
Weighted-average F1 score: 0.5696194501090385
cur_acc:  ['0.8115', '0.4206', '0.8986', '0.7511', '0.4600', '0.4183', '0.4867']
his_acc:  ['0.8115', '0.6802', '0.7383', '0.7277', '0.6769', '0.5975', '0.5095']
cur_acc des:  ['0.8393', '0.5381', '0.9014', '0.8779', '0.7170', '0.6684', '0.5134']
his_acc des:  ['0.8393', '0.7024', '0.7521', '0.7839', '0.7418', '0.6835', '0.5932']
cur_acc rrf:  ['0.8393', '0.5315', '0.8957', '0.8388', '0.6866', '0.6957', '0.5397']
his_acc rrf:  ['0.8393', '0.6971', '0.7497', '0.7714', '0.7299', '0.6818', '0.5668']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 62.4032026
CurrentTrain: epoch  0, batch     1 | loss: 121.3588816
CurrentTrain: epoch  0, batch     2 | loss: 77.2570859
CurrentTrain: epoch  0, batch     3 | loss: 46.9160247
CurrentTrain: epoch  1, batch     0 | loss: 74.8175728
CurrentTrain: epoch  1, batch     1 | loss: 71.0678880
CurrentTrain: epoch  1, batch     2 | loss: 88.4754250
CurrentTrain: epoch  1, batch     3 | loss: 46.1971482
CurrentTrain: epoch  2, batch     0 | loss: 125.2810457
CurrentTrain: epoch  2, batch     1 | loss: 68.6495859
CurrentTrain: epoch  2, batch     2 | loss: 54.0269665
CurrentTrain: epoch  2, batch     3 | loss: 37.5114085
CurrentTrain: epoch  3, batch     0 | loss: 88.8677412
CurrentTrain: epoch  3, batch     1 | loss: 86.7986564
CurrentTrain: epoch  3, batch     2 | loss: 51.6793125
CurrentTrain: epoch  3, batch     3 | loss: 40.3146845
CurrentTrain: epoch  4, batch     0 | loss: 64.0140260
CurrentTrain: epoch  4, batch     1 | loss: 54.9649497
CurrentTrain: epoch  4, batch     2 | loss: 119.2378709
CurrentTrain: epoch  4, batch     3 | loss: 48.3028668
CurrentTrain: epoch  5, batch     0 | loss: 63.9286562
CurrentTrain: epoch  5, batch     1 | loss: 114.9228673
CurrentTrain: epoch  5, batch     2 | loss: 62.0714993
CurrentTrain: epoch  5, batch     3 | loss: 52.4817178
CurrentTrain: epoch  6, batch     0 | loss: 52.8539189
CurrentTrain: epoch  6, batch     1 | loss: 84.6577451
CurrentTrain: epoch  6, batch     2 | loss: 50.9515819
CurrentTrain: epoch  6, batch     3 | loss: 52.1665819
CurrentTrain: epoch  7, batch     0 | loss: 51.8649662
CurrentTrain: epoch  7, batch     1 | loss: 65.2407755
CurrentTrain: epoch  7, batch     2 | loss: 64.7647698
CurrentTrain: epoch  7, batch     3 | loss: 48.9943261
CurrentTrain: epoch  8, batch     0 | loss: 59.6537179
CurrentTrain: epoch  8, batch     1 | loss: 183.2872867
CurrentTrain: epoch  8, batch     2 | loss: 62.8055926
CurrentTrain: epoch  8, batch     3 | loss: 38.4606448
CurrentTrain: epoch  9, batch     0 | loss: 60.0627627
CurrentTrain: epoch  9, batch     1 | loss: 113.8142443
CurrentTrain: epoch  9, batch     2 | loss: 52.5204947
CurrentTrain: epoch  9, batch     3 | loss: 50.2249524
MemoryTrain:  epoch  0, batch     0 | loss: 0.2706729
MemoryTrain:  epoch  1, batch     0 | loss: 0.2151805
MemoryTrain:  epoch  2, batch     0 | loss: 0.1735351
MemoryTrain:  epoch  3, batch     0 | loss: 0.1452704
MemoryTrain:  epoch  4, batch     0 | loss: 0.1125861
MemoryTrain:  epoch  5, batch     0 | loss: 0.1019968
MemoryTrain:  epoch  6, batch     0 | loss: 0.1007294
MemoryTrain:  epoch  7, batch     0 | loss: 0.0819991
MemoryTrain:  epoch  8, batch     0 | loss: 0.0765084
MemoryTrain:  epoch  9, batch     0 | loss: 0.0657054

F1 score per class: {33: 0.0, 34: 0.1590909090909091, 36: 0.0, 37: 0.0, 6: 0.8131868131868132, 8: 0.0, 11: 0.0, 12: 0.0, 20: 0.0, 21: 0.9444444444444444, 26: 0.42857142857142855, 28: 0.0, 29: 0.2857142857142857, 30: 0.0}
Micro-average F1 score: 0.4322766570605187
Weighted-average F1 score: 0.44556616754892614
F1 score per class: {2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.65, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 20: 0.8, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9444444444444444, 33: 0.5333333333333333, 34: 0.0, 35: 0.0, 36: 0.8925619834710744, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.6944444444444444
Weighted-average F1 score: 0.6041283481581989
F1 score per class: {2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.6386554621848739, 11: 0.0, 12: 0.0, 16: 0.0, 20: 0.8131868131868132, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9444444444444444, 33: 0.5333333333333333, 34: 0.0, 36: 0.4943820224719101, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5526932084309133
Weighted-average F1 score: 0.4281453829422456

F1 score per class: {0: 0.8307692307692308, 1: 0.4380952380952381, 2: 0.75, 3: 0.5263157894736842, 4: 0.9528795811518325, 5: 0.8687782805429864, 6: 0.359375, 7: 0.044444444444444446, 8: 0.15217391304347827, 9: 0.96, 10: 0.24561403508771928, 11: 0.20967741935483872, 12: 0.19642857142857142, 13: 0.14285714285714285, 14: 0.0759493670886076, 15: 0.631578947368421, 16: 0.7857142857142857, 17: 0.0, 18: 0.0, 19: 0.48520710059171596, 20: 0.8043478260869565, 21: 0.05555555555555555, 22: 0.7135135135135136, 23: 0.7951807228915663, 24: 0.08333333333333333, 25: 0.44776119402985076, 26: 0.7457627118644068, 27: 0.1791044776119403, 28: 0.2727272727272727, 29: 0.9263157894736842, 30: 0.9444444444444444, 31: 0.0, 32: 0.6211180124223602, 33: 0.3157894736842105, 34: 0.36470588235294116, 35: 0.3132530120481928, 36: 0.28205128205128205, 37: 0.15584415584415584, 38: 0.125, 39: 0.0, 40: 0.2077922077922078}
Micro-average F1 score: 0.5188187608569774
Weighted-average F1 score: 0.5814075749685236
F1 score per class: {0: 0.9444444444444444, 1: 0.43956043956043955, 2: 0.4375, 3: 0.5826771653543307, 4: 0.9583333333333334, 5: 0.8438818565400844, 6: 0.5806451612903226, 7: 0.0, 8: 0.4875, 9: 0.9803921568627451, 10: 0.4626865671641791, 11: 0.3181818181818182, 12: 0.6748466257668712, 13: 0.18181818181818182, 14: 0.1188118811881188, 15: 0.5454545454545454, 16: 0.819672131147541, 17: 0.11764705882352941, 18: 0.08888888888888889, 19: 0.5978260869565217, 20: 0.7741935483870968, 21: 0.2857142857142857, 22: 0.6868686868686869, 23: 0.7951807228915663, 24: 0.08695652173913043, 25: 0.631578947368421, 26: 0.73224043715847, 27: 0.14084507042253522, 28: 0.2631578947368421, 29: 0.9270833333333334, 30: 0.8292682926829268, 31: 0.8, 32: 0.826530612244898, 33: 0.38095238095238093, 34: 0.430622009569378, 35: 0.6046511627906976, 36: 0.7397260273972602, 37: 0.16666666666666666, 38: 0.5185185185185185, 39: 0.23529411764705882, 40: 0.48598130841121495}
Micro-average F1 score: 0.6082857851649714
Weighted-average F1 score: 0.6124296052708543
F1 score per class: {0: 0.9444444444444444, 1: 0.4158415841584158, 2: 0.45161290322580644, 3: 0.5777777777777777, 4: 0.9528795811518325, 5: 0.8583690987124464, 6: 0.5769230769230769, 7: 0.0, 8: 0.5135135135135135, 9: 0.9803921568627451, 10: 0.48175182481751827, 11: 0.30656934306569344, 12: 0.6625766871165644, 13: 0.13333333333333333, 14: 0.15053763440860216, 15: 0.5454545454545454, 16: 0.847457627118644, 17: 0.0, 18: 0.05, 19: 0.6054054054054054, 20: 0.7872340425531915, 21: 0.22727272727272727, 22: 0.7070707070707071, 23: 0.7951807228915663, 24: 0.08695652173913043, 25: 0.631578947368421, 26: 0.7391304347826086, 27: 0.1388888888888889, 28: 0.2564102564102564, 29: 0.9319371727748691, 30: 0.8717948717948718, 31: 1.0, 32: 0.8144329896907216, 33: 0.38095238095238093, 34: 0.36507936507936506, 35: 0.4864864864864865, 36: 0.4731182795698925, 37: 0.1282051282051282, 38: 0.5357142857142857, 39: 0.0, 40: 0.48484848484848486}
Micro-average F1 score: 0.591295647823912
Weighted-average F1 score: 0.5915517346116129
cur_acc:  ['0.8115', '0.4206', '0.8986', '0.7511', '0.4600', '0.4183', '0.4867', '0.4323']
his_acc:  ['0.8115', '0.6802', '0.7383', '0.7277', '0.6769', '0.5975', '0.5095', '0.5188']
cur_acc des:  ['0.8393', '0.5381', '0.9014', '0.8779', '0.7170', '0.6684', '0.5134', '0.6944']
his_acc des:  ['0.8393', '0.7024', '0.7521', '0.7839', '0.7418', '0.6835', '0.5932', '0.6083']
cur_acc rrf:  ['0.8393', '0.5315', '0.8957', '0.8388', '0.6866', '0.6957', '0.5397', '0.5527']
his_acc rrf:  ['0.8393', '0.6971', '0.7497', '0.7714', '0.7299', '0.6818', '0.5668', '0.5913']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 68.4156926
CurrentTrain: epoch  0, batch     1 | loss: 97.6646292
CurrentTrain: epoch  0, batch     2 | loss: 66.6647428
CurrentTrain: epoch  0, batch     3 | loss: 66.6611328
CurrentTrain: epoch  0, batch     4 | loss: 128.4922829
CurrentTrain: epoch  0, batch     5 | loss: 80.3285258
CurrentTrain: epoch  0, batch     6 | loss: 78.3569335
CurrentTrain: epoch  0, batch     7 | loss: 96.3996857
CurrentTrain: epoch  0, batch     8 | loss: 127.4967321
CurrentTrain: epoch  0, batch     9 | loss: 78.6244211
CurrentTrain: epoch  0, batch    10 | loss: 57.1765895
CurrentTrain: epoch  0, batch    11 | loss: 126.9298491
CurrentTrain: epoch  0, batch    12 | loss: 65.1056801
CurrentTrain: epoch  0, batch    13 | loss: 56.1124955
CurrentTrain: epoch  0, batch    14 | loss: 77.4332140
CurrentTrain: epoch  0, batch    15 | loss: 65.7595999
CurrentTrain: epoch  0, batch    16 | loss: 96.2171900
CurrentTrain: epoch  0, batch    17 | loss: 95.7602983
CurrentTrain: epoch  0, batch    18 | loss: 78.3746237
CurrentTrain: epoch  0, batch    19 | loss: 96.1996524
CurrentTrain: epoch  0, batch    20 | loss: 77.7664904
CurrentTrain: epoch  0, batch    21 | loss: 94.7328183
CurrentTrain: epoch  0, batch    22 | loss: 64.7261869
CurrentTrain: epoch  0, batch    23 | loss: 77.3311336
CurrentTrain: epoch  0, batch    24 | loss: 65.1227938
CurrentTrain: epoch  0, batch    25 | loss: 64.3151007
CurrentTrain: epoch  0, batch    26 | loss: 55.8703358
CurrentTrain: epoch  0, batch    27 | loss: 125.3737294
CurrentTrain: epoch  0, batch    28 | loss: 126.4964234
CurrentTrain: epoch  0, batch    29 | loss: 76.8262722
CurrentTrain: epoch  0, batch    30 | loss: 76.5898056
CurrentTrain: epoch  0, batch    31 | loss: 55.7146791
CurrentTrain: epoch  0, batch    32 | loss: 55.7320566
CurrentTrain: epoch  0, batch    33 | loss: 95.7588743
CurrentTrain: epoch  0, batch    34 | loss: 76.9451425
CurrentTrain: epoch  0, batch    35 | loss: 63.9326197
CurrentTrain: epoch  0, batch    36 | loss: 95.2306984
CurrentTrain: epoch  0, batch    37 | loss: 76.4703838
CurrentTrain: epoch  0, batch    38 | loss: 76.5010509
CurrentTrain: epoch  0, batch    39 | loss: 94.4811466
CurrentTrain: epoch  0, batch    40 | loss: 94.6825228
CurrentTrain: epoch  0, batch    41 | loss: 76.2748139
CurrentTrain: epoch  0, batch    42 | loss: 75.9753405
CurrentTrain: epoch  0, batch    43 | loss: 94.7898330
CurrentTrain: epoch  0, batch    44 | loss: 64.4369037
CurrentTrain: epoch  0, batch    45 | loss: 64.1405471
CurrentTrain: epoch  0, batch    46 | loss: 63.8533215
CurrentTrain: epoch  0, batch    47 | loss: 76.1600717
CurrentTrain: epoch  0, batch    48 | loss: 95.3123814
CurrentTrain: epoch  0, batch    49 | loss: 124.7448036
CurrentTrain: epoch  0, batch    50 | loss: 76.5656137
CurrentTrain: epoch  0, batch    51 | loss: 77.5502617
CurrentTrain: epoch  0, batch    52 | loss: 125.0017857
CurrentTrain: epoch  0, batch    53 | loss: 187.0800092
CurrentTrain: epoch  0, batch    54 | loss: 63.4701243
CurrentTrain: epoch  0, batch    55 | loss: 124.9369525
CurrentTrain: epoch  0, batch    56 | loss: 75.8785962
CurrentTrain: epoch  0, batch    57 | loss: 54.5706203
CurrentTrain: epoch  0, batch    58 | loss: 74.9469625
CurrentTrain: epoch  0, batch    59 | loss: 125.9594360
CurrentTrain: epoch  0, batch    60 | loss: 75.2171564
CurrentTrain: epoch  0, batch    61 | loss: 93.3608149
CurrentTrain: epoch  0, batch    62 | loss: 75.3920633
CurrentTrain: epoch  0, batch    63 | loss: 76.4709431
CurrentTrain: epoch  0, batch    64 | loss: 63.1067302
CurrentTrain: epoch  0, batch    65 | loss: 94.8958092
CurrentTrain: epoch  0, batch    66 | loss: 54.7748674
CurrentTrain: epoch  0, batch    67 | loss: 93.8970671
CurrentTrain: epoch  0, batch    68 | loss: 92.8386088
CurrentTrain: epoch  0, batch    69 | loss: 95.6544179
CurrentTrain: epoch  0, batch    70 | loss: 127.1019904
CurrentTrain: epoch  0, batch    71 | loss: 62.5309384
CurrentTrain: epoch  0, batch    72 | loss: 63.2306932
CurrentTrain: epoch  0, batch    73 | loss: 63.7888877
CurrentTrain: epoch  0, batch    74 | loss: 63.7175668
CurrentTrain: epoch  0, batch    75 | loss: 53.4685924
CurrentTrain: epoch  0, batch    76 | loss: 74.4707249
CurrentTrain: epoch  0, batch    77 | loss: 75.0479388
CurrentTrain: epoch  0, batch    78 | loss: 73.3001132
CurrentTrain: epoch  0, batch    79 | loss: 60.5123695
CurrentTrain: epoch  0, batch    80 | loss: 75.0166536
CurrentTrain: epoch  0, batch    81 | loss: 64.2953200
CurrentTrain: epoch  0, batch    82 | loss: 60.7504398
CurrentTrain: epoch  0, batch    83 | loss: 90.0556161
CurrentTrain: epoch  0, batch    84 | loss: 93.7464096
CurrentTrain: epoch  0, batch    85 | loss: 60.1173735
CurrentTrain: epoch  0, batch    86 | loss: 61.7224588
CurrentTrain: epoch  0, batch    87 | loss: 70.7404908
CurrentTrain: epoch  0, batch    88 | loss: 120.5806803
CurrentTrain: epoch  0, batch    89 | loss: 90.2918133
CurrentTrain: epoch  0, batch    90 | loss: 61.3281059
CurrentTrain: epoch  0, batch    91 | loss: 59.3241187
CurrentTrain: epoch  0, batch    92 | loss: 73.7150776
CurrentTrain: epoch  0, batch    93 | loss: 61.1479572
CurrentTrain: epoch  0, batch    94 | loss: 72.9123336
CurrentTrain: epoch  0, batch    95 | loss: 61.4457781
CurrentTrain: epoch  1, batch     0 | loss: 68.3206344
CurrentTrain: epoch  1, batch     1 | loss: 61.0045493
CurrentTrain: epoch  1, batch     2 | loss: 90.7341271
CurrentTrain: epoch  1, batch     3 | loss: 74.6533178
CurrentTrain: epoch  1, batch     4 | loss: 118.3975846
CurrentTrain: epoch  1, batch     5 | loss: 50.3153551
CurrentTrain: epoch  1, batch     6 | loss: 60.0047244
CurrentTrain: epoch  1, batch     7 | loss: 56.7942160
CurrentTrain: epoch  1, batch     8 | loss: 73.3836459
CurrentTrain: epoch  1, batch     9 | loss: 70.2146399
CurrentTrain: epoch  1, batch    10 | loss: 70.1034561
CurrentTrain: epoch  1, batch    11 | loss: 88.5015263
CurrentTrain: epoch  1, batch    12 | loss: 51.3868339
CurrentTrain: epoch  1, batch    13 | loss: 71.4221417
CurrentTrain: epoch  1, batch    14 | loss: 71.6662712
CurrentTrain: epoch  1, batch    15 | loss: 91.4872694
CurrentTrain: epoch  1, batch    16 | loss: 91.6726340
CurrentTrain: epoch  1, batch    17 | loss: 58.6952483
CurrentTrain: epoch  1, batch    18 | loss: 58.0376559
CurrentTrain: epoch  1, batch    19 | loss: 53.7586451
CurrentTrain: epoch  1, batch    20 | loss: 71.4378227
CurrentTrain: epoch  1, batch    21 | loss: 56.1320256
CurrentTrain: epoch  1, batch    22 | loss: 56.5583017
CurrentTrain: epoch  1, batch    23 | loss: 59.8604223
CurrentTrain: epoch  1, batch    24 | loss: 91.6489048
CurrentTrain: epoch  1, batch    25 | loss: 69.0351221
CurrentTrain: epoch  1, batch    26 | loss: 91.6226999
CurrentTrain: epoch  1, batch    27 | loss: 89.5610483
CurrentTrain: epoch  1, batch    28 | loss: 70.9890937
CurrentTrain: epoch  1, batch    29 | loss: 72.5766065
CurrentTrain: epoch  1, batch    30 | loss: 69.6283401
CurrentTrain: epoch  1, batch    31 | loss: 125.1368163
CurrentTrain: epoch  1, batch    32 | loss: 68.8860295
CurrentTrain: epoch  1, batch    33 | loss: 87.7445276
CurrentTrain: epoch  1, batch    34 | loss: 86.8168059
CurrentTrain: epoch  1, batch    35 | loss: 89.3730583
CurrentTrain: epoch  1, batch    36 | loss: 91.3904686
CurrentTrain: epoch  1, batch    37 | loss: 91.3543491
CurrentTrain: epoch  1, batch    38 | loss: 94.1697803
CurrentTrain: epoch  1, batch    39 | loss: 71.0989205
CurrentTrain: epoch  1, batch    40 | loss: 57.5593171
CurrentTrain: epoch  1, batch    41 | loss: 59.6288734
CurrentTrain: epoch  1, batch    42 | loss: 56.0906162
CurrentTrain: epoch  1, batch    43 | loss: 186.6035559
CurrentTrain: epoch  1, batch    44 | loss: 60.7574508
CurrentTrain: epoch  1, batch    45 | loss: 58.1747338
CurrentTrain: epoch  1, batch    46 | loss: 89.2015986
CurrentTrain: epoch  1, batch    47 | loss: 70.9391884
CurrentTrain: epoch  1, batch    48 | loss: 69.8922712
CurrentTrain: epoch  1, batch    49 | loss: 92.2459900
CurrentTrain: epoch  1, batch    50 | loss: 60.3403343
CurrentTrain: epoch  1, batch    51 | loss: 58.1050932
CurrentTrain: epoch  1, batch    52 | loss: 57.5003487
CurrentTrain: epoch  1, batch    53 | loss: 71.9429664
CurrentTrain: epoch  1, batch    54 | loss: 70.3342330
CurrentTrain: epoch  1, batch    55 | loss: 67.8077571
CurrentTrain: epoch  1, batch    56 | loss: 60.1804556
CurrentTrain: epoch  1, batch    57 | loss: 59.8049024
CurrentTrain: epoch  1, batch    58 | loss: 89.1896356
CurrentTrain: epoch  1, batch    59 | loss: 58.0829863
CurrentTrain: epoch  1, batch    60 | loss: 58.2535260
CurrentTrain: epoch  1, batch    61 | loss: 87.6258146
CurrentTrain: epoch  1, batch    62 | loss: 61.3471972
CurrentTrain: epoch  1, batch    63 | loss: 188.9743028
CurrentTrain: epoch  1, batch    64 | loss: 86.5864308
CurrentTrain: epoch  1, batch    65 | loss: 88.8471898
CurrentTrain: epoch  1, batch    66 | loss: 70.2972827
CurrentTrain: epoch  1, batch    67 | loss: 67.2222618
CurrentTrain: epoch  1, batch    68 | loss: 86.7419291
CurrentTrain: epoch  1, batch    69 | loss: 69.2825684
CurrentTrain: epoch  1, batch    70 | loss: 58.0241092
CurrentTrain: epoch  1, batch    71 | loss: 59.6866818
CurrentTrain: epoch  1, batch    72 | loss: 70.3569085
CurrentTrain: epoch  1, batch    73 | loss: 62.2751926
CurrentTrain: epoch  1, batch    74 | loss: 88.0666429
CurrentTrain: epoch  1, batch    75 | loss: 55.3235123
CurrentTrain: epoch  1, batch    76 | loss: 87.1557268
CurrentTrain: epoch  1, batch    77 | loss: 69.4736431
CurrentTrain: epoch  1, batch    78 | loss: 67.7836825
CurrentTrain: epoch  1, batch    79 | loss: 65.5539241
CurrentTrain: epoch  1, batch    80 | loss: 87.5264117
CurrentTrain: epoch  1, batch    81 | loss: 55.8237517
CurrentTrain: epoch  1, batch    82 | loss: 57.6155757
CurrentTrain: epoch  1, batch    83 | loss: 71.7482864
CurrentTrain: epoch  1, batch    84 | loss: 124.8780534
CurrentTrain: epoch  1, batch    85 | loss: 73.5228171
CurrentTrain: epoch  1, batch    86 | loss: 68.6282722
CurrentTrain: epoch  1, batch    87 | loss: 68.4662292
CurrentTrain: epoch  1, batch    88 | loss: 55.3082437
CurrentTrain: epoch  1, batch    89 | loss: 55.2019283
CurrentTrain: epoch  1, batch    90 | loss: 58.1729947
CurrentTrain: epoch  1, batch    91 | loss: 123.1518384
CurrentTrain: epoch  1, batch    92 | loss: 59.2755415
CurrentTrain: epoch  1, batch    93 | loss: 66.9754563
CurrentTrain: epoch  1, batch    94 | loss: 89.3458818
CurrentTrain: epoch  1, batch    95 | loss: 60.1773603
CurrentTrain: epoch  2, batch     0 | loss: 67.5128398
CurrentTrain: epoch  2, batch     1 | loss: 89.2118467
CurrentTrain: epoch  2, batch     2 | loss: 88.5219139
CurrentTrain: epoch  2, batch     3 | loss: 56.9141108
CurrentTrain: epoch  2, batch     4 | loss: 69.0614799
CurrentTrain: epoch  2, batch     5 | loss: 54.1661377
CurrentTrain: epoch  2, batch     6 | loss: 67.1935209
CurrentTrain: epoch  2, batch     7 | loss: 84.9992499
CurrentTrain: epoch  2, batch     8 | loss: 85.4940178
CurrentTrain: epoch  2, batch     9 | loss: 67.9466082
CurrentTrain: epoch  2, batch    10 | loss: 117.6730987
CurrentTrain: epoch  2, batch    11 | loss: 55.5648461
CurrentTrain: epoch  2, batch    12 | loss: 72.6699235
CurrentTrain: epoch  2, batch    13 | loss: 87.2093719
CurrentTrain: epoch  2, batch    14 | loss: 70.2628102
CurrentTrain: epoch  2, batch    15 | loss: 65.5202197
CurrentTrain: epoch  2, batch    16 | loss: 119.6985489
CurrentTrain: epoch  2, batch    17 | loss: 54.8338794
CurrentTrain: epoch  2, batch    18 | loss: 70.9613469
CurrentTrain: epoch  2, batch    19 | loss: 53.7270543
CurrentTrain: epoch  2, batch    20 | loss: 82.0966783
CurrentTrain: epoch  2, batch    21 | loss: 51.6273485
CurrentTrain: epoch  2, batch    22 | loss: 116.5519010
CurrentTrain: epoch  2, batch    23 | loss: 59.0739312
CurrentTrain: epoch  2, batch    24 | loss: 55.7097816
CurrentTrain: epoch  2, batch    25 | loss: 89.2412534
CurrentTrain: epoch  2, batch    26 | loss: 55.1921924
CurrentTrain: epoch  2, batch    27 | loss: 73.6628067
CurrentTrain: epoch  2, batch    28 | loss: 84.4893539
CurrentTrain: epoch  2, batch    29 | loss: 125.9891338
CurrentTrain: epoch  2, batch    30 | loss: 54.4852056
CurrentTrain: epoch  2, batch    31 | loss: 54.2700135
CurrentTrain: epoch  2, batch    32 | loss: 69.3498274
CurrentTrain: epoch  2, batch    33 | loss: 115.7748899
CurrentTrain: epoch  2, batch    34 | loss: 53.9664856
CurrentTrain: epoch  2, batch    35 | loss: 54.2904651
CurrentTrain: epoch  2, batch    36 | loss: 86.0892243
CurrentTrain: epoch  2, batch    37 | loss: 70.4839037
CurrentTrain: epoch  2, batch    38 | loss: 60.9342412
CurrentTrain: epoch  2, batch    39 | loss: 70.2724557
CurrentTrain: epoch  2, batch    40 | loss: 56.8519834
CurrentTrain: epoch  2, batch    41 | loss: 45.8743509
CurrentTrain: epoch  2, batch    42 | loss: 88.2518701
CurrentTrain: epoch  2, batch    43 | loss: 56.1446720
CurrentTrain: epoch  2, batch    44 | loss: 86.4147072
CurrentTrain: epoch  2, batch    45 | loss: 54.7687777
CurrentTrain: epoch  2, batch    46 | loss: 69.2418902
CurrentTrain: epoch  2, batch    47 | loss: 53.7392749
CurrentTrain: epoch  2, batch    48 | loss: 59.7159319
CurrentTrain: epoch  2, batch    49 | loss: 59.5866849
CurrentTrain: epoch  2, batch    50 | loss: 84.6157132
CurrentTrain: epoch  2, batch    51 | loss: 55.4513815
CurrentTrain: epoch  2, batch    52 | loss: 56.8061437
CurrentTrain: epoch  2, batch    53 | loss: 66.3674741
CurrentTrain: epoch  2, batch    54 | loss: 70.5740745
CurrentTrain: epoch  2, batch    55 | loss: 72.5750157
CurrentTrain: epoch  2, batch    56 | loss: 71.2711095
CurrentTrain: epoch  2, batch    57 | loss: 88.6790559
CurrentTrain: epoch  2, batch    58 | loss: 65.4888644
CurrentTrain: epoch  2, batch    59 | loss: 84.7999765
CurrentTrain: epoch  2, batch    60 | loss: 46.2406613
CurrentTrain: epoch  2, batch    61 | loss: 70.9011440
CurrentTrain: epoch  2, batch    62 | loss: 45.7138928
CurrentTrain: epoch  2, batch    63 | loss: 56.8161749
CurrentTrain: epoch  2, batch    64 | loss: 48.0699791
CurrentTrain: epoch  2, batch    65 | loss: 90.2965671
CurrentTrain: epoch  2, batch    66 | loss: 87.3039071
CurrentTrain: epoch  2, batch    67 | loss: 90.8950857
CurrentTrain: epoch  2, batch    68 | loss: 66.2820872
CurrentTrain: epoch  2, batch    69 | loss: 88.5899483
CurrentTrain: epoch  2, batch    70 | loss: 54.9645379
CurrentTrain: epoch  2, batch    71 | loss: 87.2125322
CurrentTrain: epoch  2, batch    72 | loss: 116.2753054
CurrentTrain: epoch  2, batch    73 | loss: 88.2865918
CurrentTrain: epoch  2, batch    74 | loss: 70.3260245
CurrentTrain: epoch  2, batch    75 | loss: 70.5829236
CurrentTrain: epoch  2, batch    76 | loss: 84.3334032
CurrentTrain: epoch  2, batch    77 | loss: 82.7659923
CurrentTrain: epoch  2, batch    78 | loss: 66.8331971
CurrentTrain: epoch  2, batch    79 | loss: 55.1325245
CurrentTrain: epoch  2, batch    80 | loss: 68.1340375
CurrentTrain: epoch  2, batch    81 | loss: 67.0970207
CurrentTrain: epoch  2, batch    82 | loss: 69.0109581
CurrentTrain: epoch  2, batch    83 | loss: 86.6495584
CurrentTrain: epoch  2, batch    84 | loss: 71.3955567
CurrentTrain: epoch  2, batch    85 | loss: 66.3339615
CurrentTrain: epoch  2, batch    86 | loss: 67.1650891
CurrentTrain: epoch  2, batch    87 | loss: 86.3928053
CurrentTrain: epoch  2, batch    88 | loss: 71.1225985
CurrentTrain: epoch  2, batch    89 | loss: 59.9463726
CurrentTrain: epoch  2, batch    90 | loss: 55.5902287
CurrentTrain: epoch  2, batch    91 | loss: 68.8401229
CurrentTrain: epoch  2, batch    92 | loss: 70.3635915
CurrentTrain: epoch  2, batch    93 | loss: 84.2668657
CurrentTrain: epoch  2, batch    94 | loss: 54.7104308
CurrentTrain: epoch  2, batch    95 | loss: 99.2143351
CurrentTrain: epoch  3, batch     0 | loss: 55.0610610
CurrentTrain: epoch  3, batch     1 | loss: 68.9095922
CurrentTrain: epoch  3, batch     2 | loss: 55.6104495
CurrentTrain: epoch  3, batch     3 | loss: 63.7797296
CurrentTrain: epoch  3, batch     4 | loss: 90.3705787
CurrentTrain: epoch  3, batch     5 | loss: 56.1420870
CurrentTrain: epoch  3, batch     6 | loss: 53.2216064
CurrentTrain: epoch  3, batch     7 | loss: 69.2114036
CurrentTrain: epoch  3, batch     8 | loss: 87.4592956
CurrentTrain: epoch  3, batch     9 | loss: 67.8853114
CurrentTrain: epoch  3, batch    10 | loss: 53.1410025
CurrentTrain: epoch  3, batch    11 | loss: 69.2473115
CurrentTrain: epoch  3, batch    12 | loss: 45.2888288
CurrentTrain: epoch  3, batch    13 | loss: 79.6761443
CurrentTrain: epoch  3, batch    14 | loss: 83.4258860
CurrentTrain: epoch  3, batch    15 | loss: 69.3249793
CurrentTrain: epoch  3, batch    16 | loss: 56.5113945
CurrentTrain: epoch  3, batch    17 | loss: 121.1649248
CurrentTrain: epoch  3, batch    18 | loss: 45.7233329
CurrentTrain: epoch  3, batch    19 | loss: 55.3392231
CurrentTrain: epoch  3, batch    20 | loss: 83.0135567
CurrentTrain: epoch  3, batch    21 | loss: 47.2234806
CurrentTrain: epoch  3, batch    22 | loss: 71.9397910
CurrentTrain: epoch  3, batch    23 | loss: 69.3330813
CurrentTrain: epoch  3, batch    24 | loss: 65.9263161
CurrentTrain: epoch  3, batch    25 | loss: 112.5648468
CurrentTrain: epoch  3, batch    26 | loss: 67.0237737
CurrentTrain: epoch  3, batch    27 | loss: 88.6775233
CurrentTrain: epoch  3, batch    28 | loss: 64.1358480
CurrentTrain: epoch  3, batch    29 | loss: 84.4211818
CurrentTrain: epoch  3, batch    30 | loss: 52.8513328
CurrentTrain: epoch  3, batch    31 | loss: 118.8403837
CurrentTrain: epoch  3, batch    32 | loss: 68.7342111
CurrentTrain: epoch  3, batch    33 | loss: 57.2067998
CurrentTrain: epoch  3, batch    34 | loss: 85.1221715
CurrentTrain: epoch  3, batch    35 | loss: 56.5296889
CurrentTrain: epoch  3, batch    36 | loss: 53.6133033
CurrentTrain: epoch  3, batch    37 | loss: 69.9114017
CurrentTrain: epoch  3, batch    38 | loss: 67.8203380
CurrentTrain: epoch  3, batch    39 | loss: 57.3743513
CurrentTrain: epoch  3, batch    40 | loss: 68.6522637
CurrentTrain: epoch  3, batch    41 | loss: 67.5909658
CurrentTrain: epoch  3, batch    42 | loss: 91.0331486
CurrentTrain: epoch  3, batch    43 | loss: 85.6565933
CurrentTrain: epoch  3, batch    44 | loss: 87.4042000
CurrentTrain: epoch  3, batch    45 | loss: 68.6235451
CurrentTrain: epoch  3, batch    46 | loss: 115.6559841
CurrentTrain: epoch  3, batch    47 | loss: 54.7560130
CurrentTrain: epoch  3, batch    48 | loss: 67.9144605
CurrentTrain: epoch  3, batch    49 | loss: 61.9208959
CurrentTrain: epoch  3, batch    50 | loss: 48.8167837
CurrentTrain: epoch  3, batch    51 | loss: 65.6765715
CurrentTrain: epoch  3, batch    52 | loss: 87.0369942
CurrentTrain: epoch  3, batch    53 | loss: 67.8765042
CurrentTrain: epoch  3, batch    54 | loss: 53.2900745
CurrentTrain: epoch  3, batch    55 | loss: 64.9941586
CurrentTrain: epoch  3, batch    56 | loss: 81.4482916
CurrentTrain: epoch  3, batch    57 | loss: 52.9475827
CurrentTrain: epoch  3, batch    58 | loss: 66.4696590
CurrentTrain: epoch  3, batch    59 | loss: 69.3665990
CurrentTrain: epoch  3, batch    60 | loss: 51.5983839
CurrentTrain: epoch  3, batch    61 | loss: 67.1134181
CurrentTrain: epoch  3, batch    62 | loss: 88.3196720
CurrentTrain: epoch  3, batch    63 | loss: 69.4027400
CurrentTrain: epoch  3, batch    64 | loss: 81.6122460
CurrentTrain: epoch  3, batch    65 | loss: 55.3552692
CurrentTrain: epoch  3, batch    66 | loss: 52.3879810
CurrentTrain: epoch  3, batch    67 | loss: 88.6907030
CurrentTrain: epoch  3, batch    68 | loss: 88.4904764
CurrentTrain: epoch  3, batch    69 | loss: 67.3234245
CurrentTrain: epoch  3, batch    70 | loss: 51.4978394
CurrentTrain: epoch  3, batch    71 | loss: 50.7569059
CurrentTrain: epoch  3, batch    72 | loss: 87.5154021
CurrentTrain: epoch  3, batch    73 | loss: 69.9257842
CurrentTrain: epoch  3, batch    74 | loss: 55.1721738
CurrentTrain: epoch  3, batch    75 | loss: 121.5366440
CurrentTrain: epoch  3, batch    76 | loss: 54.6182620
CurrentTrain: epoch  3, batch    77 | loss: 62.7792690
CurrentTrain: epoch  3, batch    78 | loss: 54.8270838
CurrentTrain: epoch  3, batch    79 | loss: 82.5112314
CurrentTrain: epoch  3, batch    80 | loss: 88.5671040
CurrentTrain: epoch  3, batch    81 | loss: 86.2821704
CurrentTrain: epoch  3, batch    82 | loss: 87.2733862
CurrentTrain: epoch  3, batch    83 | loss: 118.1427659
CurrentTrain: epoch  3, batch    84 | loss: 85.8237894
CurrentTrain: epoch  3, batch    85 | loss: 70.0482495
CurrentTrain: epoch  3, batch    86 | loss: 53.8202200
CurrentTrain: epoch  3, batch    87 | loss: 64.7723288
CurrentTrain: epoch  3, batch    88 | loss: 53.2479385
CurrentTrain: epoch  3, batch    89 | loss: 53.1743247
CurrentTrain: epoch  3, batch    90 | loss: 54.5376080
CurrentTrain: epoch  3, batch    91 | loss: 85.0057424
CurrentTrain: epoch  3, batch    92 | loss: 63.1994125
CurrentTrain: epoch  3, batch    93 | loss: 54.1659201
CurrentTrain: epoch  3, batch    94 | loss: 53.5709227
CurrentTrain: epoch  3, batch    95 | loss: 57.0241603
CurrentTrain: epoch  4, batch     0 | loss: 48.4538961
CurrentTrain: epoch  4, batch     1 | loss: 55.1632831
CurrentTrain: epoch  4, batch     2 | loss: 42.8708767
CurrentTrain: epoch  4, batch     3 | loss: 86.7027875
CurrentTrain: epoch  4, batch     4 | loss: 68.6347501
CurrentTrain: epoch  4, batch     5 | loss: 65.4229961
CurrentTrain: epoch  4, batch     6 | loss: 120.4411169
CurrentTrain: epoch  4, batch     7 | loss: 87.1941771
CurrentTrain: epoch  4, batch     8 | loss: 85.0107006
CurrentTrain: epoch  4, batch     9 | loss: 118.0674182
CurrentTrain: epoch  4, batch    10 | loss: 44.1355751
CurrentTrain: epoch  4, batch    11 | loss: 57.9309511
CurrentTrain: epoch  4, batch    12 | loss: 66.3830030
CurrentTrain: epoch  4, batch    13 | loss: 65.8100150
CurrentTrain: epoch  4, batch    14 | loss: 66.7622371
CurrentTrain: epoch  4, batch    15 | loss: 64.5305991
CurrentTrain: epoch  4, batch    16 | loss: 116.7759281
CurrentTrain: epoch  4, batch    17 | loss: 55.1223149
CurrentTrain: epoch  4, batch    18 | loss: 83.3360593
CurrentTrain: epoch  4, batch    19 | loss: 56.1571577
CurrentTrain: epoch  4, batch    20 | loss: 54.6207725
CurrentTrain: epoch  4, batch    21 | loss: 58.6477321
CurrentTrain: epoch  4, batch    22 | loss: 53.1875743
CurrentTrain: epoch  4, batch    23 | loss: 116.4308033
CurrentTrain: epoch  4, batch    24 | loss: 67.4100815
CurrentTrain: epoch  4, batch    25 | loss: 116.3904034
CurrentTrain: epoch  4, batch    26 | loss: 63.9382940
CurrentTrain: epoch  4, batch    27 | loss: 52.1697018
CurrentTrain: epoch  4, batch    28 | loss: 46.2109892
CurrentTrain: epoch  4, batch    29 | loss: 179.6872465
CurrentTrain: epoch  4, batch    30 | loss: 55.0672962
CurrentTrain: epoch  4, batch    31 | loss: 53.0818031
CurrentTrain: epoch  4, batch    32 | loss: 44.7876318
CurrentTrain: epoch  4, batch    33 | loss: 85.2692453
CurrentTrain: epoch  4, batch    34 | loss: 42.7773224
CurrentTrain: epoch  4, batch    35 | loss: 43.2194307
CurrentTrain: epoch  4, batch    36 | loss: 117.2873237
CurrentTrain: epoch  4, batch    37 | loss: 83.3060715
CurrentTrain: epoch  4, batch    38 | loss: 85.9829459
CurrentTrain: epoch  4, batch    39 | loss: 85.9408253
CurrentTrain: epoch  4, batch    40 | loss: 68.2375684
CurrentTrain: epoch  4, batch    41 | loss: 64.8361655
CurrentTrain: epoch  4, batch    42 | loss: 44.8798262
CurrentTrain: epoch  4, batch    43 | loss: 69.4660103
CurrentTrain: epoch  4, batch    44 | loss: 66.7963413
CurrentTrain: epoch  4, batch    45 | loss: 54.1568691
CurrentTrain: epoch  4, batch    46 | loss: 84.4471035
CurrentTrain: epoch  4, batch    47 | loss: 54.1745819
CurrentTrain: epoch  4, batch    48 | loss: 45.7167822
CurrentTrain: epoch  4, batch    49 | loss: 65.9737651
CurrentTrain: epoch  4, batch    50 | loss: 67.8666693
CurrentTrain: epoch  4, batch    51 | loss: 88.3894463
CurrentTrain: epoch  4, batch    52 | loss: 84.4181305
CurrentTrain: epoch  4, batch    53 | loss: 65.3852696
CurrentTrain: epoch  4, batch    54 | loss: 64.0570857
CurrentTrain: epoch  4, batch    55 | loss: 55.3565482
CurrentTrain: epoch  4, batch    56 | loss: 51.3428353
CurrentTrain: epoch  4, batch    57 | loss: 64.5174890
CurrentTrain: epoch  4, batch    58 | loss: 54.9558061
CurrentTrain: epoch  4, batch    59 | loss: 66.8890120
CurrentTrain: epoch  4, batch    60 | loss: 53.3772617
CurrentTrain: epoch  4, batch    61 | loss: 118.0886595
CurrentTrain: epoch  4, batch    62 | loss: 44.6864293
CurrentTrain: epoch  4, batch    63 | loss: 85.4505973
CurrentTrain: epoch  4, batch    64 | loss: 84.4080632
CurrentTrain: epoch  4, batch    65 | loss: 54.6306643
CurrentTrain: epoch  4, batch    66 | loss: 66.4865800
CurrentTrain: epoch  4, batch    67 | loss: 68.3920372
CurrentTrain: epoch  4, batch    68 | loss: 41.0018588
CurrentTrain: epoch  4, batch    69 | loss: 80.3482644
CurrentTrain: epoch  4, batch    70 | loss: 50.2677256
CurrentTrain: epoch  4, batch    71 | loss: 81.8778737
CurrentTrain: epoch  4, batch    72 | loss: 53.2049515
CurrentTrain: epoch  4, batch    73 | loss: 86.9448879
CurrentTrain: epoch  4, batch    74 | loss: 53.8058336
CurrentTrain: epoch  4, batch    75 | loss: 66.2381970
CurrentTrain: epoch  4, batch    76 | loss: 87.2154024
CurrentTrain: epoch  4, batch    77 | loss: 113.5639594
CurrentTrain: epoch  4, batch    78 | loss: 68.7699730
CurrentTrain: epoch  4, batch    79 | loss: 53.5420291
CurrentTrain: epoch  4, batch    80 | loss: 85.3240330
CurrentTrain: epoch  4, batch    81 | loss: 54.3654770
CurrentTrain: epoch  4, batch    82 | loss: 88.7604771
CurrentTrain: epoch  4, batch    83 | loss: 83.6832294
CurrentTrain: epoch  4, batch    84 | loss: 66.5286327
CurrentTrain: epoch  4, batch    85 | loss: 65.8144397
CurrentTrain: epoch  4, batch    86 | loss: 68.4186065
CurrentTrain: epoch  4, batch    87 | loss: 64.6310894
CurrentTrain: epoch  4, batch    88 | loss: 62.8921543
CurrentTrain: epoch  4, batch    89 | loss: 67.0383159
CurrentTrain: epoch  4, batch    90 | loss: 85.8733342
CurrentTrain: epoch  4, batch    91 | loss: 54.1133256
CurrentTrain: epoch  4, batch    92 | loss: 119.4067817
CurrentTrain: epoch  4, batch    93 | loss: 87.7472916
CurrentTrain: epoch  4, batch    94 | loss: 68.6331174
CurrentTrain: epoch  4, batch    95 | loss: 46.6085086
CurrentTrain: epoch  5, batch     0 | loss: 52.1823040
CurrentTrain: epoch  5, batch     1 | loss: 51.9493509
CurrentTrain: epoch  5, batch     2 | loss: 64.5624499
CurrentTrain: epoch  5, batch     3 | loss: 45.6176833
CurrentTrain: epoch  5, batch     4 | loss: 63.6441201
CurrentTrain: epoch  5, batch     5 | loss: 113.7266208
CurrentTrain: epoch  5, batch     6 | loss: 66.7217685
CurrentTrain: epoch  5, batch     7 | loss: 64.5633848
CurrentTrain: epoch  5, batch     8 | loss: 68.3377297
CurrentTrain: epoch  5, batch     9 | loss: 63.5389191
CurrentTrain: epoch  5, batch    10 | loss: 68.8968238
CurrentTrain: epoch  5, batch    11 | loss: 66.2090459
CurrentTrain: epoch  5, batch    12 | loss: 66.6840329
CurrentTrain: epoch  5, batch    13 | loss: 67.5119893
CurrentTrain: epoch  5, batch    14 | loss: 54.7120837
CurrentTrain: epoch  5, batch    15 | loss: 83.0388984
CurrentTrain: epoch  5, batch    16 | loss: 55.6179102
CurrentTrain: epoch  5, batch    17 | loss: 53.7037147
CurrentTrain: epoch  5, batch    18 | loss: 68.1440219
CurrentTrain: epoch  5, batch    19 | loss: 66.2183585
CurrentTrain: epoch  5, batch    20 | loss: 62.3861690
CurrentTrain: epoch  5, batch    21 | loss: 66.4912345
CurrentTrain: epoch  5, batch    22 | loss: 52.3006667
CurrentTrain: epoch  5, batch    23 | loss: 44.2268114
CurrentTrain: epoch  5, batch    24 | loss: 83.0722859
CurrentTrain: epoch  5, batch    25 | loss: 117.7906676
CurrentTrain: epoch  5, batch    26 | loss: 65.1387330
CurrentTrain: epoch  5, batch    27 | loss: 52.3762676
CurrentTrain: epoch  5, batch    28 | loss: 85.7416441
CurrentTrain: epoch  5, batch    29 | loss: 65.5571163
CurrentTrain: epoch  5, batch    30 | loss: 66.5800312
CurrentTrain: epoch  5, batch    31 | loss: 66.2099073
CurrentTrain: epoch  5, batch    32 | loss: 83.6256549
CurrentTrain: epoch  5, batch    33 | loss: 52.8216781
CurrentTrain: epoch  5, batch    34 | loss: 63.3972964
CurrentTrain: epoch  5, batch    35 | loss: 62.5071536
CurrentTrain: epoch  5, batch    36 | loss: 44.9275707
CurrentTrain: epoch  5, batch    37 | loss: 82.0379394
CurrentTrain: epoch  5, batch    38 | loss: 82.6731676
CurrentTrain: epoch  5, batch    39 | loss: 69.1357244
CurrentTrain: epoch  5, batch    40 | loss: 86.8797061
CurrentTrain: epoch  5, batch    41 | loss: 63.9073055
CurrentTrain: epoch  5, batch    42 | loss: 65.9308835
CurrentTrain: epoch  5, batch    43 | loss: 114.1821890
CurrentTrain: epoch  5, batch    44 | loss: 85.2580784
CurrentTrain: epoch  5, batch    45 | loss: 62.2440101
CurrentTrain: epoch  5, batch    46 | loss: 54.1917710
CurrentTrain: epoch  5, batch    47 | loss: 67.0103966
CurrentTrain: epoch  5, batch    48 | loss: 65.1148842
CurrentTrain: epoch  5, batch    49 | loss: 53.9494850
CurrentTrain: epoch  5, batch    50 | loss: 41.6568464
CurrentTrain: epoch  5, batch    51 | loss: 84.4822597
CurrentTrain: epoch  5, batch    52 | loss: 86.6707260
CurrentTrain: epoch  5, batch    53 | loss: 63.4558662
CurrentTrain: epoch  5, batch    54 | loss: 66.8013746
CurrentTrain: epoch  5, batch    55 | loss: 65.1181810
CurrentTrain: epoch  5, batch    56 | loss: 83.0772635
CurrentTrain: epoch  5, batch    57 | loss: 63.4209714
CurrentTrain: epoch  5, batch    58 | loss: 115.3586460
CurrentTrain: epoch  5, batch    59 | loss: 86.1611024
CurrentTrain: epoch  5, batch    60 | loss: 85.6074015
CurrentTrain: epoch  5, batch    61 | loss: 62.6858016
CurrentTrain: epoch  5, batch    62 | loss: 49.9497264
CurrentTrain: epoch  5, batch    63 | loss: 67.3104985
CurrentTrain: epoch  5, batch    64 | loss: 67.3429697
CurrentTrain: epoch  5, batch    65 | loss: 52.4411274
CurrentTrain: epoch  5, batch    66 | loss: 45.1352251
CurrentTrain: epoch  5, batch    67 | loss: 67.7287070
CurrentTrain: epoch  5, batch    68 | loss: 65.6699881
CurrentTrain: epoch  5, batch    69 | loss: 66.3104322
CurrentTrain: epoch  5, batch    70 | loss: 66.0887562
CurrentTrain: epoch  5, batch    71 | loss: 88.0023951
CurrentTrain: epoch  5, batch    72 | loss: 81.3586540
CurrentTrain: epoch  5, batch    73 | loss: 83.7986835
CurrentTrain: epoch  5, batch    74 | loss: 45.6618588
CurrentTrain: epoch  5, batch    75 | loss: 83.5574353
CurrentTrain: epoch  5, batch    76 | loss: 69.1786131
CurrentTrain: epoch  5, batch    77 | loss: 51.8390167
CurrentTrain: epoch  5, batch    78 | loss: 66.4542025
CurrentTrain: epoch  5, batch    79 | loss: 53.9735526
CurrentTrain: epoch  5, batch    80 | loss: 55.3903223
CurrentTrain: epoch  5, batch    81 | loss: 88.0345447
CurrentTrain: epoch  5, batch    82 | loss: 54.5024118
CurrentTrain: epoch  5, batch    83 | loss: 50.9042151
CurrentTrain: epoch  5, batch    84 | loss: 64.8708640
CurrentTrain: epoch  5, batch    85 | loss: 52.4090976
CurrentTrain: epoch  5, batch    86 | loss: 89.1953240
CurrentTrain: epoch  5, batch    87 | loss: 51.7874510
CurrentTrain: epoch  5, batch    88 | loss: 118.0070225
CurrentTrain: epoch  5, batch    89 | loss: 85.7370582
CurrentTrain: epoch  5, batch    90 | loss: 49.5094031
CurrentTrain: epoch  5, batch    91 | loss: 69.8401488
CurrentTrain: epoch  5, batch    92 | loss: 61.4788073
CurrentTrain: epoch  5, batch    93 | loss: 114.5568487
CurrentTrain: epoch  5, batch    94 | loss: 85.4034722
CurrentTrain: epoch  5, batch    95 | loss: 46.7681559
CurrentTrain: epoch  6, batch     0 | loss: 83.5059513
CurrentTrain: epoch  6, batch     1 | loss: 82.0980620
CurrentTrain: epoch  6, batch     2 | loss: 85.8652352
CurrentTrain: epoch  6, batch     3 | loss: 83.5566614
CurrentTrain: epoch  6, batch     4 | loss: 65.8533145
CurrentTrain: epoch  6, batch     5 | loss: 55.4552726
CurrentTrain: epoch  6, batch     6 | loss: 84.8156246
CurrentTrain: epoch  6, batch     7 | loss: 81.3395701
CurrentTrain: epoch  6, batch     8 | loss: 64.4780877
CurrentTrain: epoch  6, batch     9 | loss: 53.2386324
CurrentTrain: epoch  6, batch    10 | loss: 82.9873613
CurrentTrain: epoch  6, batch    11 | loss: 63.5390646
CurrentTrain: epoch  6, batch    12 | loss: 117.6727671
CurrentTrain: epoch  6, batch    13 | loss: 84.0615211
CurrentTrain: epoch  6, batch    14 | loss: 65.6297773
CurrentTrain: epoch  6, batch    15 | loss: 117.2506794
CurrentTrain: epoch  6, batch    16 | loss: 44.1790840
CurrentTrain: epoch  6, batch    17 | loss: 84.9714850
CurrentTrain: epoch  6, batch    18 | loss: 50.4490601
CurrentTrain: epoch  6, batch    19 | loss: 42.2518000
CurrentTrain: epoch  6, batch    20 | loss: 64.9068306
CurrentTrain: epoch  6, batch    21 | loss: 56.8605916
CurrentTrain: epoch  6, batch    22 | loss: 53.6973114
CurrentTrain: epoch  6, batch    23 | loss: 182.3066840
CurrentTrain: epoch  6, batch    24 | loss: 61.2306406
CurrentTrain: epoch  6, batch    25 | loss: 66.4330194
CurrentTrain: epoch  6, batch    26 | loss: 87.1432424
CurrentTrain: epoch  6, batch    27 | loss: 181.6904932
CurrentTrain: epoch  6, batch    28 | loss: 63.5682887
CurrentTrain: epoch  6, batch    29 | loss: 69.9243449
CurrentTrain: epoch  6, batch    30 | loss: 65.1809895
CurrentTrain: epoch  6, batch    31 | loss: 51.6973150
CurrentTrain: epoch  6, batch    32 | loss: 113.4712298
CurrentTrain: epoch  6, batch    33 | loss: 65.9800865
CurrentTrain: epoch  6, batch    34 | loss: 81.7980138
CurrentTrain: epoch  6, batch    35 | loss: 181.6375532
CurrentTrain: epoch  6, batch    36 | loss: 49.7028353
CurrentTrain: epoch  6, batch    37 | loss: 68.4543361
CurrentTrain: epoch  6, batch    38 | loss: 84.6094159
CurrentTrain: epoch  6, batch    39 | loss: 57.5337396
CurrentTrain: epoch  6, batch    40 | loss: 52.7528884
CurrentTrain: epoch  6, batch    41 | loss: 50.2426199
CurrentTrain: epoch  6, batch    42 | loss: 80.0102056
CurrentTrain: epoch  6, batch    43 | loss: 65.3385574
CurrentTrain: epoch  6, batch    44 | loss: 83.2653209
CurrentTrain: epoch  6, batch    45 | loss: 83.8852558
CurrentTrain: epoch  6, batch    46 | loss: 63.3483048
CurrentTrain: epoch  6, batch    47 | loss: 62.4570174
CurrentTrain: epoch  6, batch    48 | loss: 53.9310561
CurrentTrain: epoch  6, batch    49 | loss: 54.1838801
CurrentTrain: epoch  6, batch    50 | loss: 50.2510748
CurrentTrain: epoch  6, batch    51 | loss: 116.9419552
CurrentTrain: epoch  6, batch    52 | loss: 81.9348058
CurrentTrain: epoch  6, batch    53 | loss: 46.7343110
CurrentTrain: epoch  6, batch    54 | loss: 68.0370975
CurrentTrain: epoch  6, batch    55 | loss: 64.5801841
CurrentTrain: epoch  6, batch    56 | loss: 53.6062291
CurrentTrain: epoch  6, batch    57 | loss: 53.1348953
CurrentTrain: epoch  6, batch    58 | loss: 113.1283210
CurrentTrain: epoch  6, batch    59 | loss: 66.8344289
CurrentTrain: epoch  6, batch    60 | loss: 67.3034071
CurrentTrain: epoch  6, batch    61 | loss: 55.5645221
CurrentTrain: epoch  6, batch    62 | loss: 65.2838356
CurrentTrain: epoch  6, batch    63 | loss: 83.3212564
CurrentTrain: epoch  6, batch    64 | loss: 84.0504097
CurrentTrain: epoch  6, batch    65 | loss: 54.9754254
CurrentTrain: epoch  6, batch    66 | loss: 120.2291586
CurrentTrain: epoch  6, batch    67 | loss: 45.4594601
CurrentTrain: epoch  6, batch    68 | loss: 52.0442475
CurrentTrain: epoch  6, batch    69 | loss: 52.8133959
CurrentTrain: epoch  6, batch    70 | loss: 51.7485335
CurrentTrain: epoch  6, batch    71 | loss: 64.9692704
CurrentTrain: epoch  6, batch    72 | loss: 85.8822040
CurrentTrain: epoch  6, batch    73 | loss: 112.1736347
CurrentTrain: epoch  6, batch    74 | loss: 53.4374780
CurrentTrain: epoch  6, batch    75 | loss: 52.4356274
CurrentTrain: epoch  6, batch    76 | loss: 66.2451812
CurrentTrain: epoch  6, batch    77 | loss: 112.1645066
CurrentTrain: epoch  6, batch    78 | loss: 117.4850801
CurrentTrain: epoch  6, batch    79 | loss: 66.0679768
CurrentTrain: epoch  6, batch    80 | loss: 51.2831245
CurrentTrain: epoch  6, batch    81 | loss: 72.4834078
CurrentTrain: epoch  6, batch    82 | loss: 50.3109012
CurrentTrain: epoch  6, batch    83 | loss: 67.1572623
CurrentTrain: epoch  6, batch    84 | loss: 81.3415454
CurrentTrain: epoch  6, batch    85 | loss: 53.0320016
CurrentTrain: epoch  6, batch    86 | loss: 61.0506031
CurrentTrain: epoch  6, batch    87 | loss: 50.1785867
CurrentTrain: epoch  6, batch    88 | loss: 42.2687129
CurrentTrain: epoch  6, batch    89 | loss: 114.5830802
CurrentTrain: epoch  6, batch    90 | loss: 177.6231748
CurrentTrain: epoch  6, batch    91 | loss: 42.5871166
CurrentTrain: epoch  6, batch    92 | loss: 80.4977503
CurrentTrain: epoch  6, batch    93 | loss: 65.3673926
CurrentTrain: epoch  6, batch    94 | loss: 43.3617855
CurrentTrain: epoch  6, batch    95 | loss: 42.5952435
CurrentTrain: epoch  7, batch     0 | loss: 42.0592156
CurrentTrain: epoch  7, batch     1 | loss: 175.0316861
CurrentTrain: epoch  7, batch     2 | loss: 52.0886002
CurrentTrain: epoch  7, batch     3 | loss: 64.3845852
CurrentTrain: epoch  7, batch     4 | loss: 53.4290722
CurrentTrain: epoch  7, batch     5 | loss: 181.3843227
CurrentTrain: epoch  7, batch     6 | loss: 115.2740827
CurrentTrain: epoch  7, batch     7 | loss: 65.5782194
CurrentTrain: epoch  7, batch     8 | loss: 81.5222984
CurrentTrain: epoch  7, batch     9 | loss: 65.5325787
CurrentTrain: epoch  7, batch    10 | loss: 51.6567258
CurrentTrain: epoch  7, batch    11 | loss: 84.6926709
CurrentTrain: epoch  7, batch    12 | loss: 49.7715041
CurrentTrain: epoch  7, batch    13 | loss: 115.5881840
CurrentTrain: epoch  7, batch    14 | loss: 82.7587001
CurrentTrain: epoch  7, batch    15 | loss: 114.9214443
CurrentTrain: epoch  7, batch    16 | loss: 67.0796974
CurrentTrain: epoch  7, batch    17 | loss: 66.4785343
CurrentTrain: epoch  7, batch    18 | loss: 53.5640652
CurrentTrain: epoch  7, batch    19 | loss: 53.6120830
CurrentTrain: epoch  7, batch    20 | loss: 114.2175135
CurrentTrain: epoch  7, batch    21 | loss: 86.5430283
CurrentTrain: epoch  7, batch    22 | loss: 79.0633816
CurrentTrain: epoch  7, batch    23 | loss: 54.1794351
CurrentTrain: epoch  7, batch    24 | loss: 62.0315505
CurrentTrain: epoch  7, batch    25 | loss: 50.5648611
CurrentTrain: epoch  7, batch    26 | loss: 81.5630281
CurrentTrain: epoch  7, batch    27 | loss: 51.4117049
CurrentTrain: epoch  7, batch    28 | loss: 85.9061892
CurrentTrain: epoch  7, batch    29 | loss: 64.8584679
CurrentTrain: epoch  7, batch    30 | loss: 111.7216118
CurrentTrain: epoch  7, batch    31 | loss: 66.0314348
CurrentTrain: epoch  7, batch    32 | loss: 50.3169901
CurrentTrain: epoch  7, batch    33 | loss: 79.7044383
CurrentTrain: epoch  7, batch    34 | loss: 53.5144660
CurrentTrain: epoch  7, batch    35 | loss: 64.8247006
CurrentTrain: epoch  7, batch    36 | loss: 40.4970225
CurrentTrain: epoch  7, batch    37 | loss: 51.4443846
CurrentTrain: epoch  7, batch    38 | loss: 53.1322241
CurrentTrain: epoch  7, batch    39 | loss: 51.3061397
CurrentTrain: epoch  7, batch    40 | loss: 82.6363146
CurrentTrain: epoch  7, batch    41 | loss: 64.1460885
CurrentTrain: epoch  7, batch    42 | loss: 48.5544585
CurrentTrain: epoch  7, batch    43 | loss: 170.1417181
CurrentTrain: epoch  7, batch    44 | loss: 66.5446353
CurrentTrain: epoch  7, batch    45 | loss: 84.7478589
CurrentTrain: epoch  7, batch    46 | loss: 117.8050511
CurrentTrain: epoch  7, batch    47 | loss: 42.7110990
CurrentTrain: epoch  7, batch    48 | loss: 67.7137092
CurrentTrain: epoch  7, batch    49 | loss: 67.4930781
CurrentTrain: epoch  7, batch    50 | loss: 54.5818307
CurrentTrain: epoch  7, batch    51 | loss: 62.9284965
CurrentTrain: epoch  7, batch    52 | loss: 67.6813360
CurrentTrain: epoch  7, batch    53 | loss: 66.4771702
CurrentTrain: epoch  7, batch    54 | loss: 53.4699956
CurrentTrain: epoch  7, batch    55 | loss: 85.7661465
CurrentTrain: epoch  7, batch    56 | loss: 66.7312747
CurrentTrain: epoch  7, batch    57 | loss: 67.5490711
CurrentTrain: epoch  7, batch    58 | loss: 63.6587316
CurrentTrain: epoch  7, batch    59 | loss: 56.1324420
CurrentTrain: epoch  7, batch    60 | loss: 61.1838286
CurrentTrain: epoch  7, batch    61 | loss: 65.4385488
CurrentTrain: epoch  7, batch    62 | loss: 60.2816733
CurrentTrain: epoch  7, batch    63 | loss: 114.1539364
CurrentTrain: epoch  7, batch    64 | loss: 54.3482349
CurrentTrain: epoch  7, batch    65 | loss: 54.4597688
CurrentTrain: epoch  7, batch    66 | loss: 81.1416808
CurrentTrain: epoch  7, batch    67 | loss: 51.2806945
CurrentTrain: epoch  7, batch    68 | loss: 66.5990708
CurrentTrain: epoch  7, batch    69 | loss: 43.0522127
CurrentTrain: epoch  7, batch    70 | loss: 64.9874643
CurrentTrain: epoch  7, batch    71 | loss: 83.4861513
CurrentTrain: epoch  7, batch    72 | loss: 85.7652055
CurrentTrain: epoch  7, batch    73 | loss: 82.9968749
CurrentTrain: epoch  7, batch    74 | loss: 51.5310569
CurrentTrain: epoch  7, batch    75 | loss: 64.8594256
CurrentTrain: epoch  7, batch    76 | loss: 39.7828759
CurrentTrain: epoch  7, batch    77 | loss: 60.8716147
CurrentTrain: epoch  7, batch    78 | loss: 65.9147160
CurrentTrain: epoch  7, batch    79 | loss: 83.2262083
CurrentTrain: epoch  7, batch    80 | loss: 54.3043353
CurrentTrain: epoch  7, batch    81 | loss: 51.3492725
CurrentTrain: epoch  7, batch    82 | loss: 64.4453755
CurrentTrain: epoch  7, batch    83 | loss: 118.4154073
CurrentTrain: epoch  7, batch    84 | loss: 85.3598717
CurrentTrain: epoch  7, batch    85 | loss: 52.3579487
CurrentTrain: epoch  7, batch    86 | loss: 117.6077974
CurrentTrain: epoch  7, batch    87 | loss: 52.9413951
CurrentTrain: epoch  7, batch    88 | loss: 81.4662726
CurrentTrain: epoch  7, batch    89 | loss: 84.1234314
CurrentTrain: epoch  7, batch    90 | loss: 80.8961087
CurrentTrain: epoch  7, batch    91 | loss: 85.8960179
CurrentTrain: epoch  7, batch    92 | loss: 54.0657709
CurrentTrain: epoch  7, batch    93 | loss: 49.3479405
CurrentTrain: epoch  7, batch    94 | loss: 81.3342735
CurrentTrain: epoch  7, batch    95 | loss: 71.9562048
CurrentTrain: epoch  8, batch     0 | loss: 55.8073784
CurrentTrain: epoch  8, batch     1 | loss: 84.5830723
CurrentTrain: epoch  8, batch     2 | loss: 85.8325384
CurrentTrain: epoch  8, batch     3 | loss: 61.9095494
CurrentTrain: epoch  8, batch     4 | loss: 64.7228319
CurrentTrain: epoch  8, batch     5 | loss: 52.4508349
CurrentTrain: epoch  8, batch     6 | loss: 60.5866765
CurrentTrain: epoch  8, batch     7 | loss: 117.4643538
CurrentTrain: epoch  8, batch     8 | loss: 64.1973661
CurrentTrain: epoch  8, batch     9 | loss: 70.3593097
CurrentTrain: epoch  8, batch    10 | loss: 49.0174608
CurrentTrain: epoch  8, batch    11 | loss: 48.9951523
CurrentTrain: epoch  8, batch    12 | loss: 64.9660483
CurrentTrain: epoch  8, batch    13 | loss: 82.5480365
CurrentTrain: epoch  8, batch    14 | loss: 51.8497086
CurrentTrain: epoch  8, batch    15 | loss: 65.5674461
CurrentTrain: epoch  8, batch    16 | loss: 119.0455246
CurrentTrain: epoch  8, batch    17 | loss: 84.1101659
CurrentTrain: epoch  8, batch    18 | loss: 62.0893505
CurrentTrain: epoch  8, batch    19 | loss: 87.6893972
CurrentTrain: epoch  8, batch    20 | loss: 84.0591586
CurrentTrain: epoch  8, batch    21 | loss: 67.3551286
CurrentTrain: epoch  8, batch    22 | loss: 83.0943643
CurrentTrain: epoch  8, batch    23 | loss: 64.1499050
CurrentTrain: epoch  8, batch    24 | loss: 51.1991679
CurrentTrain: epoch  8, batch    25 | loss: 83.8670093
CurrentTrain: epoch  8, batch    26 | loss: 64.7598649
CurrentTrain: epoch  8, batch    27 | loss: 62.8984144
CurrentTrain: epoch  8, batch    28 | loss: 64.1533326
CurrentTrain: epoch  8, batch    29 | loss: 53.0416209
CurrentTrain: epoch  8, batch    30 | loss: 52.1564063
CurrentTrain: epoch  8, batch    31 | loss: 50.7792459
CurrentTrain: epoch  8, batch    32 | loss: 52.5059417
CurrentTrain: epoch  8, batch    33 | loss: 65.1051976
CurrentTrain: epoch  8, batch    34 | loss: 51.1588784
CurrentTrain: epoch  8, batch    35 | loss: 66.9265122
CurrentTrain: epoch  8, batch    36 | loss: 51.6345457
CurrentTrain: epoch  8, batch    37 | loss: 60.4842849
CurrentTrain: epoch  8, batch    38 | loss: 52.7135504
CurrentTrain: epoch  8, batch    39 | loss: 53.0977477
CurrentTrain: epoch  8, batch    40 | loss: 84.9528104
CurrentTrain: epoch  8, batch    41 | loss: 65.3550038
CurrentTrain: epoch  8, batch    42 | loss: 116.7463756
CurrentTrain: epoch  8, batch    43 | loss: 51.5569061
CurrentTrain: epoch  8, batch    44 | loss: 63.5417516
CurrentTrain: epoch  8, batch    45 | loss: 67.6536696
CurrentTrain: epoch  8, batch    46 | loss: 84.8025009
CurrentTrain: epoch  8, batch    47 | loss: 53.7069868
CurrentTrain: epoch  8, batch    48 | loss: 54.5024621
CurrentTrain: epoch  8, batch    49 | loss: 64.0202581
CurrentTrain: epoch  8, batch    50 | loss: 66.4769892
CurrentTrain: epoch  8, batch    51 | loss: 65.5271099
CurrentTrain: epoch  8, batch    52 | loss: 82.7578216
CurrentTrain: epoch  8, batch    53 | loss: 52.9421707
CurrentTrain: epoch  8, batch    54 | loss: 52.9494038
CurrentTrain: epoch  8, batch    55 | loss: 66.9599612
CurrentTrain: epoch  8, batch    56 | loss: 62.4033142
CurrentTrain: epoch  8, batch    57 | loss: 61.8801913
CurrentTrain: epoch  8, batch    58 | loss: 53.2231136
CurrentTrain: epoch  8, batch    59 | loss: 82.3402312
CurrentTrain: epoch  8, batch    60 | loss: 44.4010165
CurrentTrain: epoch  8, batch    61 | loss: 53.5170414
CurrentTrain: epoch  8, batch    62 | loss: 69.8682579
CurrentTrain: epoch  8, batch    63 | loss: 51.4790061
CurrentTrain: epoch  8, batch    64 | loss: 59.4490997
CurrentTrain: epoch  8, batch    65 | loss: 65.5535143
CurrentTrain: epoch  8, batch    66 | loss: 65.5663434
CurrentTrain: epoch  8, batch    67 | loss: 115.5776778
CurrentTrain: epoch  8, batch    68 | loss: 64.6225152
CurrentTrain: epoch  8, batch    69 | loss: 51.4744747
CurrentTrain: epoch  8, batch    70 | loss: 84.8106769
CurrentTrain: epoch  8, batch    71 | loss: 44.0964555
CurrentTrain: epoch  8, batch    72 | loss: 55.8031218
CurrentTrain: epoch  8, batch    73 | loss: 44.6290272
CurrentTrain: epoch  8, batch    74 | loss: 64.4391252
CurrentTrain: epoch  8, batch    75 | loss: 84.8414649
CurrentTrain: epoch  8, batch    76 | loss: 63.2200384
CurrentTrain: epoch  8, batch    77 | loss: 66.6217701
CurrentTrain: epoch  8, batch    78 | loss: 64.3416477
CurrentTrain: epoch  8, batch    79 | loss: 52.4835701
CurrentTrain: epoch  8, batch    80 | loss: 82.5126858
CurrentTrain: epoch  8, batch    81 | loss: 115.5404199
CurrentTrain: epoch  8, batch    82 | loss: 40.2019503
CurrentTrain: epoch  8, batch    83 | loss: 42.8351940
CurrentTrain: epoch  8, batch    84 | loss: 84.1625946
CurrentTrain: epoch  8, batch    85 | loss: 84.1349056
CurrentTrain: epoch  8, batch    86 | loss: 82.9593742
CurrentTrain: epoch  8, batch    87 | loss: 89.0052168
CurrentTrain: epoch  8, batch    88 | loss: 53.0229911
CurrentTrain: epoch  8, batch    89 | loss: 54.5958823
CurrentTrain: epoch  8, batch    90 | loss: 81.0392287
CurrentTrain: epoch  8, batch    91 | loss: 63.1985959
CurrentTrain: epoch  8, batch    92 | loss: 63.7744341
CurrentTrain: epoch  8, batch    93 | loss: 66.9967401
CurrentTrain: epoch  8, batch    94 | loss: 117.5712109
CurrentTrain: epoch  8, batch    95 | loss: 51.9721169
CurrentTrain: epoch  9, batch     0 | loss: 61.9970147
CurrentTrain: epoch  9, batch     1 | loss: 42.3778309
CurrentTrain: epoch  9, batch     2 | loss: 82.3569739
CurrentTrain: epoch  9, batch     3 | loss: 64.8072400
CurrentTrain: epoch  9, batch     4 | loss: 175.2917097
CurrentTrain: epoch  9, batch     5 | loss: 42.6645028
CurrentTrain: epoch  9, batch     6 | loss: 83.4601720
CurrentTrain: epoch  9, batch     7 | loss: 63.0291130
CurrentTrain: epoch  9, batch     8 | loss: 113.4287891
CurrentTrain: epoch  9, batch     9 | loss: 64.2672652
CurrentTrain: epoch  9, batch    10 | loss: 81.1152876
CurrentTrain: epoch  9, batch    11 | loss: 62.1736562
CurrentTrain: epoch  9, batch    12 | loss: 181.9315248
CurrentTrain: epoch  9, batch    13 | loss: 181.7254901
CurrentTrain: epoch  9, batch    14 | loss: 51.0328147
CurrentTrain: epoch  9, batch    15 | loss: 115.9060504
CurrentTrain: epoch  9, batch    16 | loss: 117.6449613
CurrentTrain: epoch  9, batch    17 | loss: 50.3652385
CurrentTrain: epoch  9, batch    18 | loss: 50.0894094
CurrentTrain: epoch  9, batch    19 | loss: 115.2096408
CurrentTrain: epoch  9, batch    20 | loss: 65.5703541
CurrentTrain: epoch  9, batch    21 | loss: 64.8597593
CurrentTrain: epoch  9, batch    22 | loss: 84.2157946
CurrentTrain: epoch  9, batch    23 | loss: 53.3438683
CurrentTrain: epoch  9, batch    24 | loss: 82.8414531
CurrentTrain: epoch  9, batch    25 | loss: 115.4558087
CurrentTrain: epoch  9, batch    26 | loss: 111.6252622
CurrentTrain: epoch  9, batch    27 | loss: 51.4105372
CurrentTrain: epoch  9, batch    28 | loss: 77.8532642
CurrentTrain: epoch  9, batch    29 | loss: 117.5389061
CurrentTrain: epoch  9, batch    30 | loss: 79.3579135
CurrentTrain: epoch  9, batch    31 | loss: 84.0995388
CurrentTrain: epoch  9, batch    32 | loss: 66.1685032
CurrentTrain: epoch  9, batch    33 | loss: 82.5255418
CurrentTrain: epoch  9, batch    34 | loss: 66.2153833
CurrentTrain: epoch  9, batch    35 | loss: 52.6404306
CurrentTrain: epoch  9, batch    36 | loss: 52.4078895
CurrentTrain: epoch  9, batch    37 | loss: 82.4675424
CurrentTrain: epoch  9, batch    38 | loss: 65.3120953
CurrentTrain: epoch  9, batch    39 | loss: 51.8790526
CurrentTrain: epoch  9, batch    40 | loss: 41.9113706
CurrentTrain: epoch  9, batch    41 | loss: 84.3964805
CurrentTrain: epoch  9, batch    42 | loss: 82.4668017
CurrentTrain: epoch  9, batch    43 | loss: 62.9886391
CurrentTrain: epoch  9, batch    44 | loss: 51.7233199
CurrentTrain: epoch  9, batch    45 | loss: 50.2311110
CurrentTrain: epoch  9, batch    46 | loss: 66.9304884
CurrentTrain: epoch  9, batch    47 | loss: 83.1168647
CurrentTrain: epoch  9, batch    48 | loss: 64.5439106
CurrentTrain: epoch  9, batch    49 | loss: 63.4662344
CurrentTrain: epoch  9, batch    50 | loss: 52.9376120
CurrentTrain: epoch  9, batch    51 | loss: 51.1128455
CurrentTrain: epoch  9, batch    52 | loss: 64.1850455
CurrentTrain: epoch  9, batch    53 | loss: 53.2483012
CurrentTrain: epoch  9, batch    54 | loss: 84.0652365
CurrentTrain: epoch  9, batch    55 | loss: 52.9640060
CurrentTrain: epoch  9, batch    56 | loss: 52.8313477
CurrentTrain: epoch  9, batch    57 | loss: 43.1610751
CurrentTrain: epoch  9, batch    58 | loss: 117.6392132
CurrentTrain: epoch  9, batch    59 | loss: 83.3863819
CurrentTrain: epoch  9, batch    60 | loss: 42.8042421
CurrentTrain: epoch  9, batch    61 | loss: 44.2790959
CurrentTrain: epoch  9, batch    62 | loss: 54.5645421
CurrentTrain: epoch  9, batch    63 | loss: 50.3872300
CurrentTrain: epoch  9, batch    64 | loss: 63.8080960
CurrentTrain: epoch  9, batch    65 | loss: 50.4661980
CurrentTrain: epoch  9, batch    66 | loss: 50.2506804
CurrentTrain: epoch  9, batch    67 | loss: 52.3103969
CurrentTrain: epoch  9, batch    68 | loss: 52.9467274
CurrentTrain: epoch  9, batch    69 | loss: 49.3211945
CurrentTrain: epoch  9, batch    70 | loss: 62.8966094
CurrentTrain: epoch  9, batch    71 | loss: 66.6057670
CurrentTrain: epoch  9, batch    72 | loss: 62.6666758
CurrentTrain: epoch  9, batch    73 | loss: 52.4605487
CurrentTrain: epoch  9, batch    74 | loss: 65.4324796
CurrentTrain: epoch  9, batch    75 | loss: 85.4878043
CurrentTrain: epoch  9, batch    76 | loss: 80.9387834
CurrentTrain: epoch  9, batch    77 | loss: 51.5600885
CurrentTrain: epoch  9, batch    78 | loss: 60.8691904
CurrentTrain: epoch  9, batch    79 | loss: 81.0552483
CurrentTrain: epoch  9, batch    80 | loss: 66.2567905
CurrentTrain: epoch  9, batch    81 | loss: 48.9484689
CurrentTrain: epoch  9, batch    82 | loss: 51.9337215
CurrentTrain: epoch  9, batch    83 | loss: 82.8533922
CurrentTrain: epoch  9, batch    84 | loss: 64.2023277
CurrentTrain: epoch  9, batch    85 | loss: 372.1820180
CurrentTrain: epoch  9, batch    86 | loss: 53.7874842
CurrentTrain: epoch  9, batch    87 | loss: 53.0983107
CurrentTrain: epoch  9, batch    88 | loss: 63.4311515
CurrentTrain: epoch  9, batch    89 | loss: 87.0253972
CurrentTrain: epoch  9, batch    90 | loss: 80.9422632
CurrentTrain: epoch  9, batch    91 | loss: 65.4170412
CurrentTrain: epoch  9, batch    92 | loss: 52.9204725
CurrentTrain: epoch  9, batch    93 | loss: 80.7317225
CurrentTrain: epoch  9, batch    94 | loss: 44.1083286
CurrentTrain: epoch  9, batch    95 | loss: 41.7411993

F1 score per class: {32: 0.5875, 6: 0.9247311827956989, 19: 0.34782608695652173, 24: 0.774869109947644, 26: 0.9130434782608695, 29: 0.8736842105263158}
Micro-average F1 score: 0.8094218415417559
Weighted-average F1 score: 0.8229719526003118
F1 score per class: {32: 0.7046632124352331, 6: 0.9361702127659575, 19: 0.5517241379310345, 24: 0.7486631016042781, 26: 0.9637305699481865, 29: 0.9015544041450777}
Micro-average F1 score: 0.8423194303153612
Weighted-average F1 score: 0.8442813234997163
F1 score per class: {32: 0.7046632124352331, 6: 0.9361702127659575, 19: 0.5, 24: 0.7486631016042781, 26: 0.9528795811518325, 29: 0.9015544041450777}
Micro-average F1 score: 0.8387755102040816
Weighted-average F1 score: 0.8411405817960219

F1 score per class: {32: 0.5875, 6: 0.9247311827956989, 19: 0.34782608695652173, 24: 0.774869109947644, 26: 0.9130434782608695, 29: 0.8736842105263158}
Micro-average F1 score: 0.8094218415417559
Weighted-average F1 score: 0.8229719526003118
F1 score per class: {32: 0.7046632124352331, 6: 0.9361702127659575, 19: 0.5517241379310345, 24: 0.7486631016042781, 26: 0.9637305699481865, 29: 0.9015544041450777}
Micro-average F1 score: 0.8423194303153612
Weighted-average F1 score: 0.8442813234997163
F1 score per class: {32: 0.7046632124352331, 6: 0.9361702127659575, 19: 0.5, 24: 0.7486631016042781, 26: 0.9528795811518325, 29: 0.9015544041450777}
Micro-average F1 score: 0.8387755102040816
Weighted-average F1 score: 0.8411405817960219
cur_acc:  ['0.8094']
his_acc:  ['0.8094']
cur_acc des:  ['0.8423']
his_acc des:  ['0.8423']
cur_acc rrf:  ['0.8388']
his_acc rrf:  ['0.8388']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 90.5697904
CurrentTrain: epoch  0, batch     1 | loss: 77.9464531
CurrentTrain: epoch  0, batch     2 | loss: 97.8359713
CurrentTrain: epoch  0, batch     3 | loss: 54.6712870
CurrentTrain: epoch  1, batch     0 | loss: 120.2733845
CurrentTrain: epoch  1, batch     1 | loss: 63.3143741
CurrentTrain: epoch  1, batch     2 | loss: 62.6879170
CurrentTrain: epoch  1, batch     3 | loss: 42.4312936
CurrentTrain: epoch  2, batch     0 | loss: 73.9884796
CurrentTrain: epoch  2, batch     1 | loss: 69.5277928
CurrentTrain: epoch  2, batch     2 | loss: 71.4933673
CurrentTrain: epoch  2, batch     3 | loss: 61.9572643
CurrentTrain: epoch  3, batch     0 | loss: 72.5843010
CurrentTrain: epoch  3, batch     1 | loss: 56.3852354
CurrentTrain: epoch  3, batch     2 | loss: 87.8282713
CurrentTrain: epoch  3, batch     3 | loss: 83.1273472
CurrentTrain: epoch  4, batch     0 | loss: 56.4701210
CurrentTrain: epoch  4, batch     1 | loss: 87.7471634
CurrentTrain: epoch  4, batch     2 | loss: 57.1913606
CurrentTrain: epoch  4, batch     3 | loss: 87.2471620
CurrentTrain: epoch  5, batch     0 | loss: 86.3136667
CurrentTrain: epoch  5, batch     1 | loss: 56.2801712
CurrentTrain: epoch  5, batch     2 | loss: 57.5101143
CurrentTrain: epoch  5, batch     3 | loss: 82.4812093
CurrentTrain: epoch  6, batch     0 | loss: 57.9127834
CurrentTrain: epoch  6, batch     1 | loss: 117.6261147
CurrentTrain: epoch  6, batch     2 | loss: 55.8715493
CurrentTrain: epoch  6, batch     3 | loss: 43.7853762
CurrentTrain: epoch  7, batch     0 | loss: 64.2143078
CurrentTrain: epoch  7, batch     1 | loss: 66.7188548
CurrentTrain: epoch  7, batch     2 | loss: 56.5238951
CurrentTrain: epoch  7, batch     3 | loss: 80.7462564
CurrentTrain: epoch  8, batch     0 | loss: 65.5251636
CurrentTrain: epoch  8, batch     1 | loss: 64.9857587
CurrentTrain: epoch  8, batch     2 | loss: 86.8961032
CurrentTrain: epoch  8, batch     3 | loss: 43.9902668
CurrentTrain: epoch  9, batch     0 | loss: 54.4044683
CurrentTrain: epoch  9, batch     1 | loss: 117.8230516
CurrentTrain: epoch  9, batch     2 | loss: 85.1221881
CurrentTrain: epoch  9, batch     3 | loss: 32.9174404
MemoryTrain:  epoch  0, batch     0 | loss: 0.4815046
MemoryTrain:  epoch  1, batch     0 | loss: 0.4420361
MemoryTrain:  epoch  2, batch     0 | loss: 0.2703791
MemoryTrain:  epoch  3, batch     0 | loss: 0.2260404
MemoryTrain:  epoch  4, batch     0 | loss: 0.1513865
MemoryTrain:  epoch  5, batch     0 | loss: 0.0966189
MemoryTrain:  epoch  6, batch     0 | loss: 0.0871102
MemoryTrain:  epoch  7, batch     0 | loss: 0.0739099
MemoryTrain:  epoch  8, batch     0 | loss: 0.0614814
MemoryTrain:  epoch  9, batch     0 | loss: 0.0541031

F1 score per class: {32: 0.9473684210526315, 35: 0.6666666666666666, 37: 0.0, 38: 0.0, 15: 0.8387096774193549, 25: 0.6363636363636364, 26: 0.8085106382978723}
Micro-average F1 score: 0.7400611620795107
Weighted-average F1 score: 0.7481376152401691
F1 score per class: {35: 0.0, 37: 0.8235294117647058, 38: 0.0, 6: 0.9387755102040817, 15: 0.0, 24: 0.9702970297029703, 25: 0.7755102040816326, 26: 0.9056603773584906}
Micro-average F1 score: 0.8817204301075269
Weighted-average F1 score: 0.8747025030757517
F1 score per class: {35: 0.0, 37: 0.8235294117647058, 6: 0.0, 38: 0.8936170212765957, 15: 0.0, 24: 0.9607843137254902, 25: 0.7878787878787878, 26: 0.88}
Micro-average F1 score: 0.8664850136239782
Weighted-average F1 score: 0.8577342982123054

F1 score per class: {32: 0.5911949685534591, 35: 0.9473684210526315, 37: 0.8764044943820225, 6: 0.2727272727272727, 38: 0.6666666666666666, 15: 0.7684210526315789, 19: 0.9583333333333334, 24: 0.8279569892473119, 25: 0.8297872340425532, 26: 0.6086956521739131, 29: 0.76}
Micro-average F1 score: 0.7793650793650794
Weighted-average F1 score: 0.7959308434978568
F1 score per class: {32: 0.7368421052631579, 35: 0.8235294117647058, 37: 0.93048128342246, 6: 0.5333333333333333, 38: 0.9387755102040817, 15: 0.7578947368421053, 19: 0.9587628865979382, 24: 0.8586387434554974, 25: 0.8909090909090909, 26: 0.7378640776699029, 29: 0.8275862068965517}
Micro-average F1 score: 0.8421052631578947
Weighted-average F1 score: 0.844693038064659
F1 score per class: {32: 0.7103825136612022, 35: 0.8235294117647058, 37: 0.918918918918919, 38: 0.4827586206896552, 6: 0.8936170212765957, 15: 0.7578947368421053, 19: 0.9583333333333334, 24: 0.8631578947368421, 25: 0.875, 26: 0.7289719626168224, 29: 0.7333333333333333}
Micro-average F1 score: 0.8270787343635025
Weighted-average F1 score: 0.8294399605687083
cur_acc:  ['0.8094', '0.7401']
his_acc:  ['0.8094', '0.7794']
cur_acc des:  ['0.8423', '0.8817']
his_acc des:  ['0.8423', '0.8421']
cur_acc rrf:  ['0.8388', '0.8665']
his_acc rrf:  ['0.8388', '0.8271']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 78.7813247
CurrentTrain: epoch  0, batch     1 | loss: 100.1233301
CurrentTrain: epoch  0, batch     2 | loss: 74.9694561
CurrentTrain: epoch  0, batch     3 | loss: 44.7606734
CurrentTrain: epoch  1, batch     0 | loss: 74.8319563
CurrentTrain: epoch  1, batch     1 | loss: 71.5156237
CurrentTrain: epoch  1, batch     2 | loss: 59.7138398
CurrentTrain: epoch  1, batch     3 | loss: 77.6250015
CurrentTrain: epoch  2, batch     0 | loss: 73.9291140
CurrentTrain: epoch  2, batch     1 | loss: 84.7424716
CurrentTrain: epoch  2, batch     2 | loss: 86.6027501
CurrentTrain: epoch  2, batch     3 | loss: 36.8076170
CurrentTrain: epoch  3, batch     0 | loss: 70.5692481
CurrentTrain: epoch  3, batch     1 | loss: 86.9542138
CurrentTrain: epoch  3, batch     2 | loss: 87.6867849
CurrentTrain: epoch  3, batch     3 | loss: 38.3380408
CurrentTrain: epoch  4, batch     0 | loss: 55.0115736
CurrentTrain: epoch  4, batch     1 | loss: 68.1634639
CurrentTrain: epoch  4, batch     2 | loss: 66.9278006
CurrentTrain: epoch  4, batch     3 | loss: 74.2364439
CurrentTrain: epoch  5, batch     0 | loss: 68.7938129
CurrentTrain: epoch  5, batch     1 | loss: 55.1789838
CurrentTrain: epoch  5, batch     2 | loss: 52.2184888
CurrentTrain: epoch  5, batch     3 | loss: 71.4215994
CurrentTrain: epoch  6, batch     0 | loss: 65.4471821
CurrentTrain: epoch  6, batch     1 | loss: 66.6518860
CurrentTrain: epoch  6, batch     2 | loss: 81.4842218
CurrentTrain: epoch  6, batch     3 | loss: 52.4124946
CurrentTrain: epoch  7, batch     0 | loss: 65.8676825
CurrentTrain: epoch  7, batch     1 | loss: 65.1328231
CurrentTrain: epoch  7, batch     2 | loss: 64.0464966
CurrentTrain: epoch  7, batch     3 | loss: 52.6354412
CurrentTrain: epoch  8, batch     0 | loss: 61.2509742
CurrentTrain: epoch  8, batch     1 | loss: 84.5565906
CurrentTrain: epoch  8, batch     2 | loss: 65.4021799
CurrentTrain: epoch  8, batch     3 | loss: 50.0629844
CurrentTrain: epoch  9, batch     0 | loss: 53.6327504
CurrentTrain: epoch  9, batch     1 | loss: 63.0708427
CurrentTrain: epoch  9, batch     2 | loss: 62.8604561
CurrentTrain: epoch  9, batch     3 | loss: 71.2956268
MemoryTrain:  epoch  0, batch     0 | loss: 0.3304934
MemoryTrain:  epoch  1, batch     0 | loss: 0.3261656
MemoryTrain:  epoch  2, batch     0 | loss: 0.2297674
MemoryTrain:  epoch  3, batch     0 | loss: 0.2029564
MemoryTrain:  epoch  4, batch     0 | loss: 0.1531113
MemoryTrain:  epoch  5, batch     0 | loss: 0.1387793
MemoryTrain:  epoch  6, batch     0 | loss: 0.1127773
MemoryTrain:  epoch  7, batch     0 | loss: 0.0817113
MemoryTrain:  epoch  8, batch     0 | loss: 0.0666604
MemoryTrain:  epoch  9, batch     0 | loss: 0.0616930

F1 score per class: {33: 0.0, 36: 0.6829268292682927, 37: 0.0, 6: 0.8979591836734694, 38: 0.0, 8: 0.0, 19: 0.8823529411764706, 20: 0.42857142857142855, 26: 0.11428571428571428, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.6189111747851003
Weighted-average F1 score: 0.704841374333942
F1 score per class: {33: 0.0, 35: 0.8175182481751825, 36: 0.0, 37: 0.8979591836734694, 6: 0.0, 38: 0.0, 8: 0.972972972972973, 19: 0.42857142857142855, 20: 0.0, 26: 0.8, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7857142857142857
Weighted-average F1 score: 0.7369865279294744
F1 score per class: {33: 0.0, 35: 0.8175182481751825, 36: 0.0, 37: 0.8979591836734694, 6: 0.0, 38: 0.0, 8: 0.972972972972973, 19: 0.42857142857142855, 20: 0.0, 26: 0.7058823529411765, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7602905569007264
Weighted-average F1 score: 0.7115506510140276

F1 score per class: {32: 0.5194805194805194, 33: 0.6461538461538462, 35: 0.9473684210526315, 36: 0.8457142857142858, 37: 0.6984126984126984, 6: 0.2, 38: 0.375, 8: 0.7619047619047619, 15: 0.9479166666666666, 19: 0.8823529411764706, 20: 0.8324324324324325, 24: 0.42857142857142855, 25: 0.6666666666666666, 26: 0.11428571428571428, 29: 0.5777777777777777, 30: 0.3783783783783784}
Micro-average F1 score: 0.6899175649968294
Weighted-average F1 score: 0.7437728738693824
F1 score per class: {32: 0.5977011494252874, 33: 0.7044025157232704, 35: 0.8888888888888888, 36: 0.8864864864864865, 37: 0.6984126984126984, 6: 0.5, 38: 0.5945945945945946, 8: 0.7553191489361702, 15: 0.9484536082474226, 19: 0.8372093023255814, 20: 0.8775510204081632, 24: 0.4, 25: 0.9074074074074074, 26: 0.7586206896551724, 29: 0.6666666666666666, 30: 0.7083333333333334}
Micro-average F1 score: 0.7717330342120022
Weighted-average F1 score: 0.7795715582009963
F1 score per class: {32: 0.5882352941176471, 33: 0.6829268292682927, 35: 0.9473684210526315, 36: 0.8864864864864865, 37: 0.6875, 6: 0.4166666666666667, 38: 0.5555555555555556, 8: 0.7553191489361702, 15: 0.9484536082474226, 19: 0.8181818181818182, 20: 0.8775510204081632, 24: 0.4, 25: 0.9174311926605505, 26: 0.6792452830188679, 29: 0.6548672566371682, 30: 0.5581395348837209}
Micro-average F1 score: 0.7581920903954802
Weighted-average F1 score: 0.770251489615734
cur_acc:  ['0.8094', '0.7401', '0.6189']
his_acc:  ['0.8094', '0.7794', '0.6899']
cur_acc des:  ['0.8423', '0.8817', '0.7857']
his_acc des:  ['0.8423', '0.8421', '0.7717']
cur_acc rrf:  ['0.8388', '0.8665', '0.7603']
his_acc rrf:  ['0.8388', '0.8271', '0.7582']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 99.6604194
CurrentTrain: epoch  0, batch     1 | loss: 67.5001160
CurrentTrain: epoch  0, batch     2 | loss: 102.3155429
CurrentTrain: epoch  0, batch     3 | loss: 78.6312915
CurrentTrain: epoch  0, batch     4 | loss: 48.5305397
CurrentTrain: epoch  1, batch     0 | loss: 95.0570364
CurrentTrain: epoch  1, batch     1 | loss: 92.2416788
CurrentTrain: epoch  1, batch     2 | loss: 122.0895524
CurrentTrain: epoch  1, batch     3 | loss: 73.0915022
CurrentTrain: epoch  1, batch     4 | loss: 70.1369417
CurrentTrain: epoch  2, batch     0 | loss: 91.3425691
CurrentTrain: epoch  2, batch     1 | loss: 88.8582008
CurrentTrain: epoch  2, batch     2 | loss: 57.7201220
CurrentTrain: epoch  2, batch     3 | loss: 71.6036207
CurrentTrain: epoch  2, batch     4 | loss: 105.6040109
CurrentTrain: epoch  3, batch     0 | loss: 87.4491514
CurrentTrain: epoch  3, batch     1 | loss: 86.8195056
CurrentTrain: epoch  3, batch     2 | loss: 55.2849282
CurrentTrain: epoch  3, batch     3 | loss: 87.7080543
CurrentTrain: epoch  3, batch     4 | loss: 40.8527826
CurrentTrain: epoch  4, batch     0 | loss: 178.9346228
CurrentTrain: epoch  4, batch     1 | loss: 185.2964090
CurrentTrain: epoch  4, batch     2 | loss: 66.3699657
CurrentTrain: epoch  4, batch     3 | loss: 63.9555787
CurrentTrain: epoch  4, batch     4 | loss: 49.6532582
CurrentTrain: epoch  5, batch     0 | loss: 117.0693607
CurrentTrain: epoch  5, batch     1 | loss: 56.7281290
CurrentTrain: epoch  5, batch     2 | loss: 68.4178304
CurrentTrain: epoch  5, batch     3 | loss: 55.2467483
CurrentTrain: epoch  5, batch     4 | loss: 67.5469285
CurrentTrain: epoch  6, batch     0 | loss: 66.4609156
CurrentTrain: epoch  6, batch     1 | loss: 84.2356349
CurrentTrain: epoch  6, batch     2 | loss: 67.4500608
CurrentTrain: epoch  6, batch     3 | loss: 68.0672676
CurrentTrain: epoch  6, batch     4 | loss: 64.5948714
CurrentTrain: epoch  7, batch     0 | loss: 65.7259288
CurrentTrain: epoch  7, batch     1 | loss: 69.1734104
CurrentTrain: epoch  7, batch     2 | loss: 86.6621642
CurrentTrain: epoch  7, batch     3 | loss: 65.5622566
CurrentTrain: epoch  7, batch     4 | loss: 46.4533342
CurrentTrain: epoch  8, batch     0 | loss: 85.4125769
CurrentTrain: epoch  8, batch     1 | loss: 118.6426255
CurrentTrain: epoch  8, batch     2 | loss: 64.0103502
CurrentTrain: epoch  8, batch     3 | loss: 53.0254507
CurrentTrain: epoch  8, batch     4 | loss: 46.8941710
CurrentTrain: epoch  9, batch     0 | loss: 51.5171644
CurrentTrain: epoch  9, batch     1 | loss: 68.4103149
CurrentTrain: epoch  9, batch     2 | loss: 54.7887027
CurrentTrain: epoch  9, batch     3 | loss: 84.6306771
CurrentTrain: epoch  9, batch     4 | loss: 48.0618496
MemoryTrain:  epoch  0, batch     0 | loss: 0.6707476
MemoryTrain:  epoch  1, batch     0 | loss: 0.5694848
MemoryTrain:  epoch  2, batch     0 | loss: 0.4199433
MemoryTrain:  epoch  3, batch     0 | loss: 0.4497151
MemoryTrain:  epoch  4, batch     0 | loss: 0.2715808
MemoryTrain:  epoch  5, batch     0 | loss: 0.3298346
MemoryTrain:  epoch  6, batch     0 | loss: 0.2396532
MemoryTrain:  epoch  7, batch     0 | loss: 0.1608921
MemoryTrain:  epoch  8, batch     0 | loss: 0.1614991
MemoryTrain:  epoch  9, batch     0 | loss: 0.1424752

F1 score per class: {32: 0.3418803418803419, 1: 0.4230769230769231, 34: 0.14285714285714285, 3: 0.0, 35: 0.7613636363636364, 37: 0.0, 14: 0.0, 20: 0.0, 22: 0.3076923076923077, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.4244482173174873
Weighted-average F1 score: 0.39979013759501564
F1 score per class: {32: 0.40404040404040403, 1: 0.6101694915254238, 34: 0.0, 3: 0.13186813186813187, 35: 0.7549019607843137, 37: 0.0, 8: 0.0, 14: 0.0, 22: 0.7640449438202247, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.521870286576169
Weighted-average F1 score: 0.47656197422740487
F1 score per class: {32: 0.4117647058823529, 1: 0.5982905982905983, 34: 0.17391304347826086, 3: 0.0, 35: 0.7661691542288557, 37: 0.0, 14: 0.0, 20: 0.0, 22: 0.625, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.503030303030303
Weighted-average F1 score: 0.4512555197564973

F1 score per class: {1: 0.3225806451612903, 3: 0.3893805309734513, 6: 0.5100671140939598, 8: 0.5853658536585366, 14: 0.13636363636363635, 15: 0.75, 19: 0.34710743801652894, 20: 0.5984251968503937, 22: 0.7486033519553073, 24: 0.09090909090909091, 25: 0.375, 26: 0.7525773195876289, 29: 0.9312169312169312, 30: 0.8823529411764706, 32: 0.7602339181286549, 33: 0.4, 34: 0.2222222222222222, 35: 0.28865979381443296, 36: 0.2857142857142857, 37: 0.45652173913043476, 38: 0.47368421052631576}
Micro-average F1 score: 0.5426283560998587
Weighted-average F1 score: 0.5740433183339642
F1 score per class: {1: 0.39215686274509803, 3: 0.5106382978723404, 6: 0.5276073619631901, 8: 0.6794871794871795, 14: 0.125, 15: 0.75, 19: 0.6301369863013698, 20: 0.6466165413533834, 22: 0.7298578199052133, 24: 0.06896551724137931, 25: 0.5142857142857142, 26: 0.7448979591836735, 29: 0.9479166666666666, 30: 0.9473684210526315, 32: 0.7912087912087912, 33: 0.42857142857142855, 34: 0.5230769230769231, 35: 0.48, 36: 0.6346153846153846, 37: 0.3673469387755102, 38: 0.6666666666666666}
Micro-average F1 score: 0.6167989970748015
Weighted-average F1 score: 0.6254223572770164
F1 score per class: {1: 0.4, 3: 0.5, 6: 0.525, 8: 0.6835443037974683, 14: 0.16326530612244897, 15: 0.75, 19: 0.6301369863013698, 20: 0.6277372262773723, 22: 0.7403846153846154, 24: 0.07407407407407407, 25: 0.47058823529411764, 26: 0.7411167512690355, 29: 0.9424083769633508, 30: 0.972972972972973, 32: 0.7888888888888889, 33: 0.42857142857142855, 34: 0.43478260869565216, 35: 0.47619047619047616, 36: 0.5918367346938775, 37: 0.3853211009174312, 38: 0.6530612244897959}
Micro-average F1 score: 0.6094997898276587
Weighted-average F1 score: 0.6169884290508086
cur_acc:  ['0.8094', '0.7401', '0.6189', '0.4244']
his_acc:  ['0.8094', '0.7794', '0.6899', '0.5426']
cur_acc des:  ['0.8423', '0.8817', '0.7857', '0.5219']
his_acc des:  ['0.8423', '0.8421', '0.7717', '0.6168']
cur_acc rrf:  ['0.8388', '0.8665', '0.7603', '0.5030']
his_acc rrf:  ['0.8388', '0.8271', '0.7582', '0.6095']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 83.9057127
CurrentTrain: epoch  0, batch     1 | loss: 90.0105171
CurrentTrain: epoch  0, batch     2 | loss: 74.4203311
CurrentTrain: epoch  0, batch     3 | loss: 94.4097700
CurrentTrain: epoch  0, batch     4 | loss: 60.1311199
CurrentTrain: epoch  1, batch     0 | loss: 90.8940898
CurrentTrain: epoch  1, batch     1 | loss: 90.2901873
CurrentTrain: epoch  1, batch     2 | loss: 68.5081328
CurrentTrain: epoch  1, batch     3 | loss: 73.4158947
CurrentTrain: epoch  1, batch     4 | loss: 46.9595319
CurrentTrain: epoch  2, batch     0 | loss: 120.8655228
CurrentTrain: epoch  2, batch     1 | loss: 72.9789310
CurrentTrain: epoch  2, batch     2 | loss: 86.3880721
CurrentTrain: epoch  2, batch     3 | loss: 68.9310227
CurrentTrain: epoch  2, batch     4 | loss: 43.1412024
CurrentTrain: epoch  3, batch     0 | loss: 84.9890221
CurrentTrain: epoch  3, batch     1 | loss: 57.0028978
CurrentTrain: epoch  3, batch     2 | loss: 120.3966683
CurrentTrain: epoch  3, batch     3 | loss: 57.2263054
CurrentTrain: epoch  3, batch     4 | loss: 42.2404228
CurrentTrain: epoch  4, batch     0 | loss: 65.4214594
CurrentTrain: epoch  4, batch     1 | loss: 56.1088089
CurrentTrain: epoch  4, batch     2 | loss: 84.6836759
CurrentTrain: epoch  4, batch     3 | loss: 88.9360673
CurrentTrain: epoch  4, batch     4 | loss: 117.4015479
CurrentTrain: epoch  5, batch     0 | loss: 67.6529721
CurrentTrain: epoch  5, batch     1 | loss: 119.0866482
CurrentTrain: epoch  5, batch     2 | loss: 63.5710189
CurrentTrain: epoch  5, batch     3 | loss: 68.2853858
CurrentTrain: epoch  5, batch     4 | loss: 52.8477023
CurrentTrain: epoch  6, batch     0 | loss: 85.1604234
CurrentTrain: epoch  6, batch     1 | loss: 65.3376919
CurrentTrain: epoch  6, batch     2 | loss: 85.1583413
CurrentTrain: epoch  6, batch     3 | loss: 81.2545556
CurrentTrain: epoch  6, batch     4 | loss: 52.8068229
CurrentTrain: epoch  7, batch     0 | loss: 84.6462482
CurrentTrain: epoch  7, batch     1 | loss: 65.5239275
CurrentTrain: epoch  7, batch     2 | loss: 67.2722742
CurrentTrain: epoch  7, batch     3 | loss: 82.0961498
CurrentTrain: epoch  7, batch     4 | loss: 70.3573303
CurrentTrain: epoch  8, batch     0 | loss: 84.1139936
CurrentTrain: epoch  8, batch     1 | loss: 52.2013729
CurrentTrain: epoch  8, batch     2 | loss: 67.4596546
CurrentTrain: epoch  8, batch     3 | loss: 86.0749942
CurrentTrain: epoch  8, batch     4 | loss: 51.3454843
CurrentTrain: epoch  9, batch     0 | loss: 64.7446772
CurrentTrain: epoch  9, batch     1 | loss: 63.5221005
CurrentTrain: epoch  9, batch     2 | loss: 85.0171492
CurrentTrain: epoch  9, batch     3 | loss: 54.0287004
CurrentTrain: epoch  9, batch     4 | loss: 74.1957207
MemoryTrain:  epoch  0, batch     0 | loss: 0.3116927
MemoryTrain:  epoch  1, batch     0 | loss: 0.3341520
MemoryTrain:  epoch  2, batch     0 | loss: 0.2395621
MemoryTrain:  epoch  3, batch     0 | loss: 0.1854928
MemoryTrain:  epoch  4, batch     0 | loss: 0.1495032
MemoryTrain:  epoch  5, batch     0 | loss: 0.1269115
MemoryTrain:  epoch  6, batch     0 | loss: 0.1043619
MemoryTrain:  epoch  7, batch     0 | loss: 0.1142930
MemoryTrain:  epoch  8, batch     0 | loss: 0.1005459
MemoryTrain:  epoch  9, batch     0 | loss: 0.0696678

F1 score per class: {34: 0.9743589743589743, 37: 0.0, 38: 0.0, 6: 0.5815602836879432, 8: 0.8, 10: 0.0, 5: 0.4583333333333333, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.7276688453159041
Weighted-average F1 score: 0.7510914801200814
F1 score per class: {34: 1.0, 37: 0.0, 38: 0.0, 6: 0.7295597484276729, 8: 0.9473684210526315, 10: 0.8, 5: 0.8064516129032258, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.8089887640449438
Weighted-average F1 score: 0.7503266001869551
F1 score per class: {34: 1.0, 37: 0.0, 38: 0.0, 6: 0.7453416149068323, 8: 0.9473684210526315, 10: 0.7142857142857143, 5: 0.7868852459016393, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.8105065666041276
Weighted-average F1 score: 0.7529239702365982

F1 score per class: {1: 0.32, 3: 0.31683168316831684, 5: 0.9134615384615384, 6: 0.40559440559440557, 8: 0.32075471698113206, 10: 0.5503355704697986, 14: 0.05405405405405406, 15: 0.8235294117647058, 16: 0.8, 17: 0.0, 18: 0.3283582089552239, 19: 0.36065573770491804, 20: 0.6349206349206349, 22: 0.7126436781609196, 24: 0.08333333333333333, 25: 0.375, 26: 0.7604166666666666, 29: 0.9081081081081082, 30: 0.8823529411764706, 32: 0.797752808988764, 33: 0.42857142857142855, 34: 0.17142857142857143, 35: 0.18666666666666668, 36: 0.21621621621621623, 37: 0.4186046511627907, 38: 0.125}
Micro-average F1 score: 0.543859649122807
Weighted-average F1 score: 0.6078020155888293
F1 score per class: {1: 0.36036036036036034, 3: 0.6524822695035462, 5: 0.8968609865470852, 6: 0.4429530201342282, 8: 0.6436781609195402, 10: 0.6904761904761905, 14: 0.046511627906976744, 15: 0.8235294117647058, 16: 0.9, 17: 0.2926829268292683, 18: 0.42016806722689076, 19: 0.5294117647058824, 20: 0.7522935779816514, 22: 0.7487684729064039, 24: 0.0625, 25: 0.5753424657534246, 26: 0.7461139896373057, 29: 0.9424083769633508, 30: 0.972972972972973, 32: 0.8205128205128205, 33: 0.5, 34: 0.2682926829268293, 35: 0.5739130434782609, 36: 0.6213592233009708, 37: 0.4235294117647059, 38: 0.34146341463414637}
Micro-average F1 score: 0.6344827586206897
Weighted-average F1 score: 0.6497306274956895
F1 score per class: {1: 0.3826086956521739, 3: 0.6524822695035462, 5: 0.9090909090909091, 6: 0.4563758389261745, 8: 0.6549707602339181, 10: 0.7058823529411765, 14: 0.04597701149425287, 15: 0.8235294117647058, 16: 0.9, 17: 0.2631578947368421, 18: 0.39669421487603307, 19: 0.5074626865671642, 20: 0.7068965517241379, 22: 0.7575757575757576, 24: 0.06451612903225806, 25: 0.4927536231884058, 26: 0.75, 29: 0.9312169312169312, 30: 0.9473684210526315, 32: 0.8125, 33: 0.4, 34: 0.23684210526315788, 35: 0.5789473684210527, 36: 0.5376344086021505, 37: 0.44680851063829785, 38: 0.34146341463414637}
Micro-average F1 score: 0.6282540784449844
Weighted-average F1 score: 0.6453534565028162
cur_acc:  ['0.8094', '0.7401', '0.6189', '0.4244', '0.7277']
his_acc:  ['0.8094', '0.7794', '0.6899', '0.5426', '0.5439']
cur_acc des:  ['0.8423', '0.8817', '0.7857', '0.5219', '0.8090']
his_acc des:  ['0.8423', '0.8421', '0.7717', '0.6168', '0.6345']
cur_acc rrf:  ['0.8388', '0.8665', '0.7603', '0.5030', '0.8105']
his_acc rrf:  ['0.8388', '0.8271', '0.7582', '0.6095', '0.6283']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 89.6441427
CurrentTrain: epoch  0, batch     1 | loss: 67.0624889
CurrentTrain: epoch  0, batch     2 | loss: 83.2667115
CurrentTrain: epoch  0, batch     3 | loss: 76.3547362
CurrentTrain: epoch  1, batch     0 | loss: 94.8370791
CurrentTrain: epoch  1, batch     1 | loss: 74.5763773
CurrentTrain: epoch  1, batch     2 | loss: 75.1563797
CurrentTrain: epoch  1, batch     3 | loss: 69.4431672
CurrentTrain: epoch  2, batch     0 | loss: 73.3330222
CurrentTrain: epoch  2, batch     1 | loss: 90.5912486
CurrentTrain: epoch  2, batch     2 | loss: 60.1436354
CurrentTrain: epoch  2, batch     3 | loss: 71.3428597
CurrentTrain: epoch  3, batch     0 | loss: 96.0084129
CurrentTrain: epoch  3, batch     1 | loss: 56.7650415
CurrentTrain: epoch  3, batch     2 | loss: 58.5132879
CurrentTrain: epoch  3, batch     3 | loss: 46.6319854
CurrentTrain: epoch  4, batch     0 | loss: 123.2019459
CurrentTrain: epoch  4, batch     1 | loss: 71.9392497
CurrentTrain: epoch  4, batch     2 | loss: 81.4771424
CurrentTrain: epoch  4, batch     3 | loss: 53.7799239
CurrentTrain: epoch  5, batch     0 | loss: 64.8237849
CurrentTrain: epoch  5, batch     1 | loss: 52.0997348
CurrentTrain: epoch  5, batch     2 | loss: 89.5302959
CurrentTrain: epoch  5, batch     3 | loss: 96.5001188
CurrentTrain: epoch  6, batch     0 | loss: 88.0166315
CurrentTrain: epoch  6, batch     1 | loss: 66.2235861
CurrentTrain: epoch  6, batch     2 | loss: 63.2435128
CurrentTrain: epoch  6, batch     3 | loss: 44.3157578
CurrentTrain: epoch  7, batch     0 | loss: 65.2292532
CurrentTrain: epoch  7, batch     1 | loss: 118.1156599
CurrentTrain: epoch  7, batch     2 | loss: 52.4005923
CurrentTrain: epoch  7, batch     3 | loss: 51.9309175
CurrentTrain: epoch  8, batch     0 | loss: 53.0236772
CurrentTrain: epoch  8, batch     1 | loss: 66.4341780
CurrentTrain: epoch  8, batch     2 | loss: 53.6288598
CurrentTrain: epoch  8, batch     3 | loss: 67.1399362
CurrentTrain: epoch  9, batch     0 | loss: 53.9760216
CurrentTrain: epoch  9, batch     1 | loss: 66.7086420
CurrentTrain: epoch  9, batch     2 | loss: 64.4087235
CurrentTrain: epoch  9, batch     3 | loss: 42.2998723
MemoryTrain:  epoch  0, batch     0 | loss: 0.3308283
MemoryTrain:  epoch  1, batch     0 | loss: 0.3163554
MemoryTrain:  epoch  2, batch     0 | loss: 0.2202727
MemoryTrain:  epoch  3, batch     0 | loss: 0.1766938
MemoryTrain:  epoch  4, batch     0 | loss: 0.1471909
MemoryTrain:  epoch  5, batch     0 | loss: 0.1120112
MemoryTrain:  epoch  6, batch     0 | loss: 0.0965245
MemoryTrain:  epoch  7, batch     0 | loss: 0.0858541
MemoryTrain:  epoch  8, batch     0 | loss: 0.0738361
MemoryTrain:  epoch  9, batch     0 | loss: 0.0608175

F1 score per class: {0: 0.9577464788732394, 32: 0.0, 1: 0.93048128342246, 4: 0.8888888888888888, 13: 0.0, 14: 0.0, 18: 0.0, 20: 0.41025641025641024, 21: 0.0, 22: 0.825, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8238213399503722
Weighted-average F1 score: 0.8064363867598414
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 34: 0.9637305699481865, 1: 0.8888888888888888, 4: 0.0, 37: 0.0, 13: 0.0, 14: 0.5909090909090909, 15: 0.0, 18: 0.8395061728395061, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.851063829787234
Weighted-average F1 score: 0.8123860883456493
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 34: 0.9528795811518325, 1: 0.8888888888888888, 4: 0.0, 37: 0.0, 13: 0.0, 14: 0.5909090909090909, 18: 0.0, 20: 0.825, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8428571428571429
Weighted-average F1 score: 0.8034063246862949

F1 score per class: {0: 0.9577464788732394, 1: 0.3308270676691729, 3: 0.5573770491803278, 4: 0.93048128342246, 5: 0.9333333333333333, 6: 0.4225352112676056, 8: 0.4107142857142857, 10: 0.3064516129032258, 13: 0.08602150537634409, 14: 0.028169014084507043, 15: 0.75, 16: 0.8679245283018868, 17: 0.17391304347826086, 18: 0.35555555555555557, 19: 0.5915492957746479, 20: 0.684931506849315, 21: 0.2, 22: 0.6624203821656051, 23: 0.7857142857142857, 24: 0.0, 25: 0.375, 26: 0.743455497382199, 29: 0.9032258064516129, 30: 0.972972972972973, 32: 0.7455621301775148, 33: 0.4, 34: 0.06557377049180328, 35: 0.25, 36: 0.40963855421686746, 37: 0.4329896907216495, 38: 0.2857142857142857}
Micro-average F1 score: 0.5748387096774193
Weighted-average F1 score: 0.5940688184295668
F1 score per class: {0: 0.935064935064935, 1: 0.3582089552238806, 3: 0.7361111111111112, 4: 0.9538461538461539, 5: 0.9049773755656109, 6: 0.47058823529411764, 8: 0.7397260273972602, 10: 0.38095238095238093, 13: 0.10126582278481013, 14: 0.07894736842105263, 15: 0.7058823529411765, 16: 0.9, 17: 0.3157894736842105, 18: 0.4411764705882353, 19: 0.7295597484276729, 20: 0.8214285714285714, 21: 0.24528301886792453, 22: 0.6225165562913907, 23: 0.8095238095238095, 24: 0.09523809523809523, 25: 0.631578947368421, 26: 0.7208121827411168, 29: 0.9430051813471503, 30: 1.0, 32: 0.845360824742268, 33: 0.5, 34: 0.19718309859154928, 35: 0.6386554621848739, 36: 0.6476190476190476, 37: 0.4, 38: 0.47058823529411764}
Micro-average F1 score: 0.6469719350073855
Weighted-average F1 score: 0.6460992122613661
F1 score per class: {0: 0.935064935064935, 1: 0.35555555555555557, 3: 0.7297297297297297, 4: 0.9479166666666666, 5: 0.9259259259259259, 6: 0.49673202614379086, 8: 0.6944444444444444, 10: 0.3779527559055118, 13: 0.09302325581395349, 14: 0.07894736842105263, 15: 0.75, 16: 0.8813559322033898, 17: 0.30303030303030304, 18: 0.44776119402985076, 19: 0.7133757961783439, 20: 0.7833333333333333, 21: 0.24761904761904763, 22: 0.6225165562913907, 23: 0.7857142857142857, 24: 0.09523809523809523, 25: 0.631578947368421, 26: 0.7319587628865979, 29: 0.9319371727748691, 30: 1.0, 32: 0.8272251308900523, 33: 0.47058823529411764, 34: 0.1791044776119403, 35: 0.5555555555555556, 36: 0.6078431372549019, 37: 0.425531914893617, 38: 0.45454545454545453}
Micro-average F1 score: 0.6382598331346842
Weighted-average F1 score: 0.6364535609497142
cur_acc:  ['0.8094', '0.7401', '0.6189', '0.4244', '0.7277', '0.8238']
his_acc:  ['0.8094', '0.7794', '0.6899', '0.5426', '0.5439', '0.5748']
cur_acc des:  ['0.8423', '0.8817', '0.7857', '0.5219', '0.8090', '0.8511']
his_acc des:  ['0.8423', '0.8421', '0.7717', '0.6168', '0.6345', '0.6470']
cur_acc rrf:  ['0.8388', '0.8665', '0.7603', '0.5030', '0.8105', '0.8429']
his_acc rrf:  ['0.8388', '0.8271', '0.7582', '0.6095', '0.6283', '0.6383']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 108.0619202
CurrentTrain: epoch  0, batch     1 | loss: 78.1592325
CurrentTrain: epoch  0, batch     2 | loss: 99.6312014
CurrentTrain: epoch  0, batch     3 | loss: 73.3158685
CurrentTrain: epoch  0, batch     4 | loss: 60.4584528
CurrentTrain: epoch  1, batch     0 | loss: 76.9112439
CurrentTrain: epoch  1, batch     1 | loss: 63.7483454
CurrentTrain: epoch  1, batch     2 | loss: 73.9919434
CurrentTrain: epoch  1, batch     3 | loss: 74.8378247
CurrentTrain: epoch  1, batch     4 | loss: 60.2371769
CurrentTrain: epoch  2, batch     0 | loss: 94.1471353
CurrentTrain: epoch  2, batch     1 | loss: 122.6906815
CurrentTrain: epoch  2, batch     2 | loss: 73.4933611
CurrentTrain: epoch  2, batch     3 | loss: 60.9625298
CurrentTrain: epoch  2, batch     4 | loss: 18.7771269
CurrentTrain: epoch  3, batch     0 | loss: 88.5378979
CurrentTrain: epoch  3, batch     1 | loss: 57.4049981
CurrentTrain: epoch  3, batch     2 | loss: 70.1830504
CurrentTrain: epoch  3, batch     3 | loss: 119.6361262
CurrentTrain: epoch  3, batch     4 | loss: 28.6741992
CurrentTrain: epoch  4, batch     0 | loss: 57.9173228
CurrentTrain: epoch  4, batch     1 | loss: 183.7970747
CurrentTrain: epoch  4, batch     2 | loss: 71.1458320
CurrentTrain: epoch  4, batch     3 | loss: 64.6629254
CurrentTrain: epoch  4, batch     4 | loss: 27.4557243
CurrentTrain: epoch  5, batch     0 | loss: 86.2346510
CurrentTrain: epoch  5, batch     1 | loss: 116.6949767
CurrentTrain: epoch  5, batch     2 | loss: 70.9295786
CurrentTrain: epoch  5, batch     3 | loss: 68.8963770
CurrentTrain: epoch  5, batch     4 | loss: 9.7250563
CurrentTrain: epoch  6, batch     0 | loss: 86.5694878
CurrentTrain: epoch  6, batch     1 | loss: 67.6425717
CurrentTrain: epoch  6, batch     2 | loss: 55.3684185
CurrentTrain: epoch  6, batch     3 | loss: 65.8192180
CurrentTrain: epoch  6, batch     4 | loss: 60.0158417
CurrentTrain: epoch  7, batch     0 | loss: 86.7500905
CurrentTrain: epoch  7, batch     1 | loss: 64.3803595
CurrentTrain: epoch  7, batch     2 | loss: 64.9742435
CurrentTrain: epoch  7, batch     3 | loss: 68.3534495
CurrentTrain: epoch  7, batch     4 | loss: 27.0950730
CurrentTrain: epoch  8, batch     0 | loss: 66.6882624
CurrentTrain: epoch  8, batch     1 | loss: 66.0648176
CurrentTrain: epoch  8, batch     2 | loss: 117.5627388
CurrentTrain: epoch  8, batch     3 | loss: 62.7564376
CurrentTrain: epoch  8, batch     4 | loss: 15.8620227
CurrentTrain: epoch  9, batch     0 | loss: 63.2157967
CurrentTrain: epoch  9, batch     1 | loss: 65.4031114
CurrentTrain: epoch  9, batch     2 | loss: 83.1961408
CurrentTrain: epoch  9, batch     3 | loss: 86.5727940
CurrentTrain: epoch  9, batch     4 | loss: 16.1129597
MemoryTrain:  epoch  0, batch     0 | loss: 0.3179064
MemoryTrain:  epoch  1, batch     0 | loss: 0.2873775
MemoryTrain:  epoch  2, batch     0 | loss: 0.2216245
MemoryTrain:  epoch  3, batch     0 | loss: 0.1713336
MemoryTrain:  epoch  4, batch     0 | loss: 0.1456937
MemoryTrain:  epoch  5, batch     0 | loss: 0.1267186
MemoryTrain:  epoch  6, batch     0 | loss: 0.0994984
MemoryTrain:  epoch  7, batch     0 | loss: 0.0920048
MemoryTrain:  epoch  8, batch     0 | loss: 0.0850457
MemoryTrain:  epoch  9, batch     0 | loss: 0.0716824

F1 score per class: {0: 0.0, 2: 0.875, 34: 0.0, 5: 0.0, 38: 0.4827586206896552, 39: 0.5185185185185185, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.4, 18: 0.0, 19: 0.0, 28: 0.0}
Micro-average F1 score: 0.44648318042813456
Weighted-average F1 score: 0.3758369400690601
F1 score per class: {0: 0.0, 2: 0.875, 34: 0.0, 5: 0.0, 6: 0.0, 39: 0.0, 8: 0.7945205479452054, 10: 0.8372093023255814, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.625, 21: 0.0, 28: 0.4}
Micro-average F1 score: 0.7070217917675545
Weighted-average F1 score: 0.6179704203518751
F1 score per class: {0: 0.0, 2: 0.875, 34: 0.0, 5: 0.0, 38: 0.0, 39: 0.7862068965517242, 8: 0.8372093023255814, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 18: 0.5, 19: 0.0, 21: 0.0, 28: 0.0}
Micro-average F1 score: 0.6878048780487804
Weighted-average F1 score: 0.6137060883292677

F1 score per class: {0: 0.9295774647887324, 1: 0.34108527131782945, 2: 0.6666666666666666, 3: 0.2857142857142857, 4: 0.918918918918919, 5: 0.9082125603864735, 6: 0.5644171779141104, 8: 0.15217391304347827, 10: 0.24561403508771928, 11: 0.4, 12: 0.5, 13: 0.12903225806451613, 14: 0.0, 15: 0.75, 16: 0.8, 17: 0.3333333333333333, 18: 0.19047619047619047, 19: 0.5974025974025974, 20: 0.7301587301587301, 21: 0.2, 22: 0.589041095890411, 23: 0.7906976744186046, 24: 0.0, 25: 0.375, 26: 0.7526881720430108, 28: 0.1016949152542373, 29: 0.9263157894736842, 30: 1.0, 32: 0.768361581920904, 33: 0.42857142857142855, 34: 0.11428571428571428, 35: 0.14285714285714285, 36: 0.2631578947368421, 37: 0.5098039215686274, 38: 0.358974358974359, 39: 0.0}
Micro-average F1 score: 0.5533453887884268
Weighted-average F1 score: 0.6045455542793812
F1 score per class: {0: 0.961038961038961, 1: 0.35294117647058826, 2: 0.5833333333333334, 3: 0.6764705882352942, 4: 0.9247311827956989, 5: 0.9009009009009009, 6: 0.5333333333333333, 8: 0.4927536231884058, 10: 0.47761194029850745, 11: 0.5550239234449761, 12: 0.7422680412371134, 13: 0.0625, 14: 0.05555555555555555, 15: 0.75, 16: 0.8253968253968254, 17: 0.24, 18: 0.32989690721649484, 19: 0.7954545454545454, 20: 0.8490566037735849, 21: 0.23333333333333334, 22: 0.5, 23: 0.8235294117647058, 24: 0.09090909090909091, 25: 0.6493506493506493, 26: 0.743455497382199, 28: 0.13157894736842105, 29: 0.9319371727748691, 30: 1.0, 32: 0.845771144278607, 33: 0.4, 34: 0.11764705882352941, 35: 0.5370370370370371, 36: 0.6601941747572816, 37: 0.4897959183673469, 38: 0.5306122448979592, 39: 0.32}
Micro-average F1 score: 0.6261730969760166
Weighted-average F1 score: 0.6201883518557236
F1 score per class: {0: 0.958904109589041, 1: 0.35036496350364965, 2: 0.5833333333333334, 3: 0.6470588235294118, 4: 0.9247311827956989, 5: 0.9174311926605505, 6: 0.5357142857142857, 8: 0.3709677419354839, 10: 0.4580152671755725, 11: 0.5533980582524272, 12: 0.72, 13: 0.1, 14: 0.0547945205479452, 15: 0.75, 16: 0.8253968253968254, 17: 0.3157894736842105, 18: 0.35051546391752575, 19: 0.7954545454545454, 20: 0.8035714285714286, 21: 0.23728813559322035, 22: 0.5109489051094891, 23: 0.8095238095238095, 24: 0.09090909090909091, 25: 0.6133333333333333, 26: 0.7395833333333334, 28: 0.11904761904761904, 29: 0.9263157894736842, 30: 1.0, 32: 0.845771144278607, 33: 0.4, 34: 0.11764705882352941, 35: 0.4489795918367347, 36: 0.5773195876288659, 37: 0.47058823529411764, 38: 0.5, 39: 0.0}
Micro-average F1 score: 0.6120734908136483
Weighted-average F1 score: 0.6094871931737076
cur_acc:  ['0.8094', '0.7401', '0.6189', '0.4244', '0.7277', '0.8238', '0.4465']
his_acc:  ['0.8094', '0.7794', '0.6899', '0.5426', '0.5439', '0.5748', '0.5533']
cur_acc des:  ['0.8423', '0.8817', '0.7857', '0.5219', '0.8090', '0.8511', '0.7070']
his_acc des:  ['0.8423', '0.8421', '0.7717', '0.6168', '0.6345', '0.6470', '0.6262']
cur_acc rrf:  ['0.8388', '0.8665', '0.7603', '0.5030', '0.8105', '0.8429', '0.6878']
his_acc rrf:  ['0.8388', '0.8271', '0.7582', '0.6095', '0.6283', '0.6383', '0.6121']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 72.0418487
CurrentTrain: epoch  0, batch     1 | loss: 84.9100397
CurrentTrain: epoch  0, batch     2 | loss: 65.5118838
CurrentTrain: epoch  0, batch     3 | loss: 12.4711435
CurrentTrain: epoch  1, batch     0 | loss: 79.3032070
CurrentTrain: epoch  1, batch     1 | loss: 59.2208198
CurrentTrain: epoch  1, batch     2 | loss: 59.9529184
CurrentTrain: epoch  1, batch     3 | loss: 9.9370555
CurrentTrain: epoch  2, batch     0 | loss: 89.2286291
CurrentTrain: epoch  2, batch     1 | loss: 70.3711615
CurrentTrain: epoch  2, batch     2 | loss: 68.2096218
CurrentTrain: epoch  2, batch     3 | loss: 27.6342266
CurrentTrain: epoch  3, batch     0 | loss: 67.5361439
CurrentTrain: epoch  3, batch     1 | loss: 68.8334249
CurrentTrain: epoch  3, batch     2 | loss: 65.0239034
CurrentTrain: epoch  3, batch     3 | loss: 27.6364521
CurrentTrain: epoch  4, batch     0 | loss: 66.6653224
CurrentTrain: epoch  4, batch     1 | loss: 118.9727344
CurrentTrain: epoch  4, batch     2 | loss: 49.7528750
CurrentTrain: epoch  4, batch     3 | loss: 10.1767712
CurrentTrain: epoch  5, batch     0 | loss: 52.6204758
CurrentTrain: epoch  5, batch     1 | loss: 84.9534803
CurrentTrain: epoch  5, batch     2 | loss: 79.2048215
CurrentTrain: epoch  5, batch     3 | loss: 11.1842369
CurrentTrain: epoch  6, batch     0 | loss: 86.0254540
CurrentTrain: epoch  6, batch     1 | loss: 64.5188776
CurrentTrain: epoch  6, batch     2 | loss: 49.6442011
CurrentTrain: epoch  6, batch     3 | loss: 5.7956878
CurrentTrain: epoch  7, batch     0 | loss: 64.1959343
CurrentTrain: epoch  7, batch     1 | loss: 63.7407868
CurrentTrain: epoch  7, batch     2 | loss: 82.2692074
CurrentTrain: epoch  7, batch     3 | loss: 5.3764247
CurrentTrain: epoch  8, batch     0 | loss: 48.8611148
CurrentTrain: epoch  8, batch     1 | loss: 86.4043483
CurrentTrain: epoch  8, batch     2 | loss: 52.1231331
CurrentTrain: epoch  8, batch     3 | loss: 5.5251627
CurrentTrain: epoch  9, batch     0 | loss: 63.6299765
CurrentTrain: epoch  9, batch     1 | loss: 48.2596270
CurrentTrain: epoch  9, batch     2 | loss: 66.0910152
CurrentTrain: epoch  9, batch     3 | loss: 27.4947432
MemoryTrain:  epoch  0, batch     0 | loss: 0.3952176
MemoryTrain:  epoch  1, batch     0 | loss: 0.2563339
MemoryTrain:  epoch  2, batch     0 | loss: 0.1857789
MemoryTrain:  epoch  3, batch     0 | loss: 0.1913083
MemoryTrain:  epoch  4, batch     0 | loss: 0.1551223
MemoryTrain:  epoch  5, batch     0 | loss: 0.1245725
MemoryTrain:  epoch  6, batch     0 | loss: 0.1209331
MemoryTrain:  epoch  7, batch     0 | loss: 0.1024055
MemoryTrain:  epoch  8, batch     0 | loss: 0.0801364
MemoryTrain:  epoch  9, batch     0 | loss: 0.0711817

F1 score per class: {7: 0.75, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 26: 0.5263157894736842, 27: 0.0, 31: 0.3170731707317073}
Micro-average F1 score: 0.45544554455445546
Weighted-average F1 score: 0.39041730737107533
F1 score per class: {1: 0.0, 3: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 17: 0.0, 19: 0.0, 22: 0.0, 26: 0.6, 27: 1.0, 31: 0.7222222222222222}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.5624217588269876
F1 score per class: {1: 0.0, 3: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 12: 0.0, 17: 0.0, 19: 0.0, 22: 0.0, 26: 0.6, 27: 0.6666666666666666, 31: 0.6857142857142857}
Micro-average F1 score: 0.6422018348623854
Weighted-average F1 score: 0.5374111182934712

F1 score per class: {0: 0.8656716417910447, 1: 0.26229508196721313, 2: 0.5714285714285714, 3: 0.25263157894736843, 4: 0.8888888888888888, 5: 0.9038461538461539, 6: 0.019417475728155338, 7: 0.07142857142857142, 8: 0.09411764705882353, 9: 0.9803921568627451, 10: 0.07692307692307693, 11: 0.43478260869565216, 12: 0.45925925925925926, 13: 0.06451612903225806, 14: 0.030303030303030304, 15: 0.75, 16: 0.7037037037037037, 17: 0.0, 18: 0.14925373134328357, 19: 0.6138613861386139, 20: 0.6865671641791045, 21: 0.11538461538461539, 22: 0.5428571428571428, 23: 0.7727272727272727, 24: 0.1, 25: 0.375, 26: 0.73224043715847, 27: 0.17857142857142858, 28: 0.12121212121212122, 29: 0.9032258064516129, 30: 0.972972972972973, 31: 0.0, 32: 0.6193548387096774, 33: 0.42857142857142855, 34: 0.09230769230769231, 35: 0.03571428571428571, 36: 0.2857142857142857, 37: 0.4897959183673469, 38: 0.3157894736842105, 39: 0.1111111111111111, 40: 0.2826086956521739}
Micro-average F1 score: 0.49631159634110356
Weighted-average F1 score: 0.5674795071775361
F1 score per class: {0: 0.8985507246376812, 1: 0.3373493975903614, 2: 0.5454545454545454, 3: 0.75, 4: 0.9247311827956989, 5: 0.8849557522123894, 6: 0.019230769230769232, 7: 0.044444444444444446, 8: 0.34, 9: 0.9803921568627451, 10: 0.2905982905982906, 11: 0.5673076923076923, 12: 0.7431693989071039, 13: 0.0425531914893617, 14: 0.056338028169014086, 15: 0.7058823529411765, 16: 0.84375, 17: 0.48, 18: 0.2608695652173913, 19: 0.717948717948718, 20: 0.822429906542056, 21: 0.22807017543859648, 22: 0.589041095890411, 23: 0.8235294117647058, 24: 0.08695652173913043, 25: 0.6133333333333333, 26: 0.7340425531914894, 27: 0.17142857142857143, 28: 0.11764705882352941, 29: 0.9206349206349206, 30: 1.0, 31: 0.5, 32: 0.8229166666666666, 33: 0.375, 34: 0.08823529411764706, 35: 0.5217391304347826, 36: 0.6538461538461539, 37: 0.5252525252525253, 38: 0.5454545454545454, 39: 0.2222222222222222, 40: 0.5909090909090909}
Micro-average F1 score: 0.5875875875875876
Weighted-average F1 score: 0.5869331084181841
F1 score per class: {0: 0.9142857142857143, 1: 0.3218390804597701, 2: 0.5454545454545454, 3: 0.6618705035971223, 4: 0.9247311827956989, 5: 0.9090909090909091, 6: 0.019230769230769232, 7: 0.041666666666666664, 8: 0.2857142857142857, 9: 0.9803921568627451, 10: 0.23008849557522124, 11: 0.5615763546798029, 12: 0.7407407407407407, 13: 0.034482758620689655, 14: 0.07692307692307693, 15: 0.7058823529411765, 16: 0.8253968253968254, 17: 0.3157894736842105, 18: 0.21505376344086022, 19: 0.7319587628865979, 20: 0.8103448275862069, 21: 0.2391304347826087, 22: 0.589041095890411, 23: 0.7951807228915663, 24: 0.08695652173913043, 25: 0.6133333333333333, 26: 0.7340425531914894, 27: 0.1643835616438356, 28: 0.10126582278481013, 29: 0.9206349206349206, 30: 1.0, 31: 0.4, 32: 0.7802197802197802, 33: 0.375, 34: 0.08955223880597014, 35: 0.3614457831325301, 36: 0.4888888888888889, 37: 0.5094339622641509, 38: 0.5217391304347826, 39: 0.19047619047619047, 40: 0.5806451612903226}
Micro-average F1 score: 0.5687468290208016
Weighted-average F1 score: 0.5709899314041943
cur_acc:  ['0.8094', '0.7401', '0.6189', '0.4244', '0.7277', '0.8238', '0.4465', '0.4554']
his_acc:  ['0.8094', '0.7794', '0.6899', '0.5426', '0.5439', '0.5748', '0.5533', '0.4963']
cur_acc des:  ['0.8423', '0.8817', '0.7857', '0.5219', '0.8090', '0.8511', '0.7070', '0.6667']
his_acc des:  ['0.8423', '0.8421', '0.7717', '0.6168', '0.6345', '0.6470', '0.6262', '0.5876']
cur_acc rrf:  ['0.8388', '0.8665', '0.7603', '0.5030', '0.8105', '0.8429', '0.6878', '0.6422']
his_acc rrf:  ['0.8388', '0.8271', '0.7582', '0.6095', '0.6283', '0.6383', '0.6121', '0.5687']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 80.5313675
CurrentTrain: epoch  0, batch     1 | loss: 67.8532295
CurrentTrain: epoch  0, batch     2 | loss: 79.1638019
CurrentTrain: epoch  0, batch     3 | loss: 127.9652337
CurrentTrain: epoch  0, batch     4 | loss: 66.3252986
CurrentTrain: epoch  0, batch     5 | loss: 97.3696739
CurrentTrain: epoch  0, batch     6 | loss: 80.4893594
CurrentTrain: epoch  0, batch     7 | loss: 77.7023602
CurrentTrain: epoch  0, batch     8 | loss: 65.4893305
CurrentTrain: epoch  0, batch     9 | loss: 78.1788986
CurrentTrain: epoch  0, batch    10 | loss: 96.9046313
CurrentTrain: epoch  0, batch    11 | loss: 78.9562192
CurrentTrain: epoch  0, batch    12 | loss: 96.4307998
CurrentTrain: epoch  0, batch    13 | loss: 97.1975306
CurrentTrain: epoch  0, batch    14 | loss: 79.1928057
CurrentTrain: epoch  0, batch    15 | loss: 56.3251288
CurrentTrain: epoch  0, batch    16 | loss: 56.4203268
CurrentTrain: epoch  0, batch    17 | loss: 64.8968832
CurrentTrain: epoch  0, batch    18 | loss: 77.3157913
CurrentTrain: epoch  0, batch    19 | loss: 78.4564552
CurrentTrain: epoch  0, batch    20 | loss: 125.9621556
CurrentTrain: epoch  0, batch    21 | loss: 64.7578662
CurrentTrain: epoch  0, batch    22 | loss: 77.1809135
CurrentTrain: epoch  0, batch    23 | loss: 76.5796771
CurrentTrain: epoch  0, batch    24 | loss: 77.4884462
CurrentTrain: epoch  0, batch    25 | loss: 95.8212774
CurrentTrain: epoch  0, batch    26 | loss: 77.2203940
CurrentTrain: epoch  0, batch    27 | loss: 64.6606176
CurrentTrain: epoch  0, batch    28 | loss: 76.8879823
CurrentTrain: epoch  0, batch    29 | loss: 64.3316348
CurrentTrain: epoch  0, batch    30 | loss: 76.6736851
CurrentTrain: epoch  0, batch    31 | loss: 65.2719624
CurrentTrain: epoch  0, batch    32 | loss: 126.4232132
CurrentTrain: epoch  0, batch    33 | loss: 77.1833922
CurrentTrain: epoch  0, batch    34 | loss: 55.6026097
CurrentTrain: epoch  0, batch    35 | loss: 95.3595440
CurrentTrain: epoch  0, batch    36 | loss: 64.6556403
CurrentTrain: epoch  0, batch    37 | loss: 66.1381099
CurrentTrain: epoch  0, batch    38 | loss: 76.6683260
CurrentTrain: epoch  0, batch    39 | loss: 63.6959031
CurrentTrain: epoch  0, batch    40 | loss: 77.3628780
CurrentTrain: epoch  0, batch    41 | loss: 55.5633795
CurrentTrain: epoch  0, batch    42 | loss: 94.3810785
CurrentTrain: epoch  0, batch    43 | loss: 95.4139582
CurrentTrain: epoch  0, batch    44 | loss: 125.6936879
CurrentTrain: epoch  0, batch    45 | loss: 64.4665424
CurrentTrain: epoch  0, batch    46 | loss: 187.2717670
CurrentTrain: epoch  0, batch    47 | loss: 64.1228297
CurrentTrain: epoch  0, batch    48 | loss: 77.1350778
CurrentTrain: epoch  0, batch    49 | loss: 76.5020133
CurrentTrain: epoch  0, batch    50 | loss: 76.4945280
CurrentTrain: epoch  0, batch    51 | loss: 96.4782566
CurrentTrain: epoch  0, batch    52 | loss: 65.4905247
CurrentTrain: epoch  0, batch    53 | loss: 63.8513279
CurrentTrain: epoch  0, batch    54 | loss: 63.9381335
CurrentTrain: epoch  0, batch    55 | loss: 63.1480763
CurrentTrain: epoch  0, batch    56 | loss: 94.3094895
CurrentTrain: epoch  0, batch    57 | loss: 63.8252223
CurrentTrain: epoch  0, batch    58 | loss: 54.2100874
CurrentTrain: epoch  0, batch    59 | loss: 75.8336977
CurrentTrain: epoch  0, batch    60 | loss: 124.7837585
CurrentTrain: epoch  0, batch    61 | loss: 76.3685384
CurrentTrain: epoch  0, batch    62 | loss: 63.3284438
CurrentTrain: epoch  0, batch    63 | loss: 94.1085866
CurrentTrain: epoch  0, batch    64 | loss: 75.8831955
CurrentTrain: epoch  0, batch    65 | loss: 75.9284521
CurrentTrain: epoch  0, batch    66 | loss: 75.8746367
CurrentTrain: epoch  0, batch    67 | loss: 76.8548546
CurrentTrain: epoch  0, batch    68 | loss: 94.4769267
CurrentTrain: epoch  0, batch    69 | loss: 123.8344042
CurrentTrain: epoch  0, batch    70 | loss: 76.3409744
CurrentTrain: epoch  0, batch    71 | loss: 63.5857590
CurrentTrain: epoch  0, batch    72 | loss: 95.1123765
CurrentTrain: epoch  0, batch    73 | loss: 123.5285570
CurrentTrain: epoch  0, batch    74 | loss: 54.0973141
CurrentTrain: epoch  0, batch    75 | loss: 75.5820240
CurrentTrain: epoch  0, batch    76 | loss: 92.3056807
CurrentTrain: epoch  0, batch    77 | loss: 75.6630930
CurrentTrain: epoch  0, batch    78 | loss: 74.6591084
CurrentTrain: epoch  0, batch    79 | loss: 75.7629346
CurrentTrain: epoch  0, batch    80 | loss: 123.3224583
CurrentTrain: epoch  0, batch    81 | loss: 93.5415431
CurrentTrain: epoch  0, batch    82 | loss: 92.5092018
CurrentTrain: epoch  0, batch    83 | loss: 92.4149199
CurrentTrain: epoch  0, batch    84 | loss: 62.5666213
CurrentTrain: epoch  0, batch    85 | loss: 61.3558363
CurrentTrain: epoch  0, batch    86 | loss: 53.6451653
CurrentTrain: epoch  0, batch    87 | loss: 62.3210341
CurrentTrain: epoch  0, batch    88 | loss: 52.4694477
CurrentTrain: epoch  0, batch    89 | loss: 75.7191391
CurrentTrain: epoch  0, batch    90 | loss: 94.5083496
CurrentTrain: epoch  0, batch    91 | loss: 63.8752358
CurrentTrain: epoch  0, batch    92 | loss: 60.9203840
CurrentTrain: epoch  0, batch    93 | loss: 90.1303565
CurrentTrain: epoch  0, batch    94 | loss: 75.3206782
CurrentTrain: epoch  0, batch    95 | loss: 52.6375209
CurrentTrain: epoch  1, batch     0 | loss: 123.9539453
CurrentTrain: epoch  1, batch     1 | loss: 60.3859008
CurrentTrain: epoch  1, batch     2 | loss: 59.4681538
CurrentTrain: epoch  1, batch     3 | loss: 121.8471059
CurrentTrain: epoch  1, batch     4 | loss: 61.6693512
CurrentTrain: epoch  1, batch     5 | loss: 71.9044160
CurrentTrain: epoch  1, batch     6 | loss: 59.8494743
CurrentTrain: epoch  1, batch     7 | loss: 124.6206423
CurrentTrain: epoch  1, batch     8 | loss: 90.3746542
CurrentTrain: epoch  1, batch     9 | loss: 51.4521688
CurrentTrain: epoch  1, batch    10 | loss: 63.3777720
CurrentTrain: epoch  1, batch    11 | loss: 71.9290677
CurrentTrain: epoch  1, batch    12 | loss: 90.1429804
CurrentTrain: epoch  1, batch    13 | loss: 92.3241809
CurrentTrain: epoch  1, batch    14 | loss: 94.3196615
CurrentTrain: epoch  1, batch    15 | loss: 58.8309762
CurrentTrain: epoch  1, batch    16 | loss: 118.6310000
CurrentTrain: epoch  1, batch    17 | loss: 59.3182995
CurrentTrain: epoch  1, batch    18 | loss: 71.5630325
CurrentTrain: epoch  1, batch    19 | loss: 122.2493917
CurrentTrain: epoch  1, batch    20 | loss: 184.9697667
CurrentTrain: epoch  1, batch    21 | loss: 72.5054418
CurrentTrain: epoch  1, batch    22 | loss: 59.5488363
CurrentTrain: epoch  1, batch    23 | loss: 71.9124532
CurrentTrain: epoch  1, batch    24 | loss: 86.4082312
CurrentTrain: epoch  1, batch    25 | loss: 92.5482370
CurrentTrain: epoch  1, batch    26 | loss: 59.4331737
CurrentTrain: epoch  1, batch    27 | loss: 73.1911953
CurrentTrain: epoch  1, batch    28 | loss: 72.2665516
CurrentTrain: epoch  1, batch    29 | loss: 57.8802507
CurrentTrain: epoch  1, batch    30 | loss: 69.9518755
CurrentTrain: epoch  1, batch    31 | loss: 59.8208676
CurrentTrain: epoch  1, batch    32 | loss: 58.6670020
CurrentTrain: epoch  1, batch    33 | loss: 86.8729928
CurrentTrain: epoch  1, batch    34 | loss: 59.0349983
CurrentTrain: epoch  1, batch    35 | loss: 57.1419653
CurrentTrain: epoch  1, batch    36 | loss: 121.3651006
CurrentTrain: epoch  1, batch    37 | loss: 58.4694261
CurrentTrain: epoch  1, batch    38 | loss: 89.3114309
CurrentTrain: epoch  1, batch    39 | loss: 72.7164376
CurrentTrain: epoch  1, batch    40 | loss: 60.1685084
CurrentTrain: epoch  1, batch    41 | loss: 184.9151465
CurrentTrain: epoch  1, batch    42 | loss: 60.0884455
CurrentTrain: epoch  1, batch    43 | loss: 122.0209333
CurrentTrain: epoch  1, batch    44 | loss: 57.6922645
CurrentTrain: epoch  1, batch    45 | loss: 70.9772819
CurrentTrain: epoch  1, batch    46 | loss: 68.2354863
CurrentTrain: epoch  1, batch    47 | loss: 188.6353950
CurrentTrain: epoch  1, batch    48 | loss: 83.7986901
CurrentTrain: epoch  1, batch    49 | loss: 69.5038946
CurrentTrain: epoch  1, batch    50 | loss: 116.7759799
CurrentTrain: epoch  1, batch    51 | loss: 85.1806879
CurrentTrain: epoch  1, batch    52 | loss: 67.0365096
CurrentTrain: epoch  1, batch    53 | loss: 72.4542908
CurrentTrain: epoch  1, batch    54 | loss: 60.2696355
CurrentTrain: epoch  1, batch    55 | loss: 120.7690263
CurrentTrain: epoch  1, batch    56 | loss: 57.4882310
CurrentTrain: epoch  1, batch    57 | loss: 85.7113758
CurrentTrain: epoch  1, batch    58 | loss: 73.6848681
CurrentTrain: epoch  1, batch    59 | loss: 67.6241778
CurrentTrain: epoch  1, batch    60 | loss: 62.6936295
CurrentTrain: epoch  1, batch    61 | loss: 58.4183351
CurrentTrain: epoch  1, batch    62 | loss: 68.3738114
CurrentTrain: epoch  1, batch    63 | loss: 71.0653008
CurrentTrain: epoch  1, batch    64 | loss: 56.6079682
CurrentTrain: epoch  1, batch    65 | loss: 70.6724106
CurrentTrain: epoch  1, batch    66 | loss: 92.4178473
CurrentTrain: epoch  1, batch    67 | loss: 86.4959652
CurrentTrain: epoch  1, batch    68 | loss: 55.8997558
CurrentTrain: epoch  1, batch    69 | loss: 70.6765093
CurrentTrain: epoch  1, batch    70 | loss: 70.3173381
CurrentTrain: epoch  1, batch    71 | loss: 45.6317003
CurrentTrain: epoch  1, batch    72 | loss: 90.7624136
CurrentTrain: epoch  1, batch    73 | loss: 56.0374452
CurrentTrain: epoch  1, batch    74 | loss: 71.7363941
CurrentTrain: epoch  1, batch    75 | loss: 55.8615296
CurrentTrain: epoch  1, batch    76 | loss: 62.7153328
CurrentTrain: epoch  1, batch    77 | loss: 68.8858710
CurrentTrain: epoch  1, batch    78 | loss: 88.3671618
CurrentTrain: epoch  1, batch    79 | loss: 57.6418454
CurrentTrain: epoch  1, batch    80 | loss: 66.3165146
CurrentTrain: epoch  1, batch    81 | loss: 59.3322349
CurrentTrain: epoch  1, batch    82 | loss: 119.0666648
CurrentTrain: epoch  1, batch    83 | loss: 56.0362197
CurrentTrain: epoch  1, batch    84 | loss: 123.3261008
CurrentTrain: epoch  1, batch    85 | loss: 91.4699875
CurrentTrain: epoch  1, batch    86 | loss: 87.0314895
CurrentTrain: epoch  1, batch    87 | loss: 47.4017297
CurrentTrain: epoch  1, batch    88 | loss: 68.4808907
CurrentTrain: epoch  1, batch    89 | loss: 69.5020419
CurrentTrain: epoch  1, batch    90 | loss: 56.2177713
CurrentTrain: epoch  1, batch    91 | loss: 88.4121292
CurrentTrain: epoch  1, batch    92 | loss: 56.1927796
CurrentTrain: epoch  1, batch    93 | loss: 60.9469949
CurrentTrain: epoch  1, batch    94 | loss: 90.2743296
CurrentTrain: epoch  1, batch    95 | loss: 56.9834873
CurrentTrain: epoch  2, batch     0 | loss: 117.3570509
CurrentTrain: epoch  2, batch     1 | loss: 87.2518558
CurrentTrain: epoch  2, batch     2 | loss: 86.3329537
CurrentTrain: epoch  2, batch     3 | loss: 72.2888050
CurrentTrain: epoch  2, batch     4 | loss: 69.6638460
CurrentTrain: epoch  2, batch     5 | loss: 52.7314044
CurrentTrain: epoch  2, batch     6 | loss: 56.7686649
CurrentTrain: epoch  2, batch     7 | loss: 69.9200291
CurrentTrain: epoch  2, batch     8 | loss: 91.8558446
CurrentTrain: epoch  2, batch     9 | loss: 70.9928323
CurrentTrain: epoch  2, batch    10 | loss: 49.5282937
CurrentTrain: epoch  2, batch    11 | loss: 49.1677741
CurrentTrain: epoch  2, batch    12 | loss: 55.2882472
CurrentTrain: epoch  2, batch    13 | loss: 113.1171426
CurrentTrain: epoch  2, batch    14 | loss: 67.7378167
CurrentTrain: epoch  2, batch    15 | loss: 70.0739088
CurrentTrain: epoch  2, batch    16 | loss: 66.5695485
CurrentTrain: epoch  2, batch    17 | loss: 53.0277835
CurrentTrain: epoch  2, batch    18 | loss: 67.5920423
CurrentTrain: epoch  2, batch    19 | loss: 117.2708161
CurrentTrain: epoch  2, batch    20 | loss: 46.0870180
CurrentTrain: epoch  2, batch    21 | loss: 56.9701928
CurrentTrain: epoch  2, batch    22 | loss: 64.6435080
CurrentTrain: epoch  2, batch    23 | loss: 118.7379834
CurrentTrain: epoch  2, batch    24 | loss: 88.8142127
CurrentTrain: epoch  2, batch    25 | loss: 68.1541180
CurrentTrain: epoch  2, batch    26 | loss: 55.7268669
CurrentTrain: epoch  2, batch    27 | loss: 88.5000418
CurrentTrain: epoch  2, batch    28 | loss: 55.9696951
CurrentTrain: epoch  2, batch    29 | loss: 69.4600749
CurrentTrain: epoch  2, batch    30 | loss: 64.0988061
CurrentTrain: epoch  2, batch    31 | loss: 88.6674359
CurrentTrain: epoch  2, batch    32 | loss: 119.2457614
CurrentTrain: epoch  2, batch    33 | loss: 88.9371397
CurrentTrain: epoch  2, batch    34 | loss: 87.7310216
CurrentTrain: epoch  2, batch    35 | loss: 120.1399187
CurrentTrain: epoch  2, batch    36 | loss: 64.5956458
CurrentTrain: epoch  2, batch    37 | loss: 68.1474497
CurrentTrain: epoch  2, batch    38 | loss: 58.2466481
CurrentTrain: epoch  2, batch    39 | loss: 67.8283236
CurrentTrain: epoch  2, batch    40 | loss: 87.7753036
CurrentTrain: epoch  2, batch    41 | loss: 58.4465241
CurrentTrain: epoch  2, batch    42 | loss: 87.0924806
CurrentTrain: epoch  2, batch    43 | loss: 87.2382695
CurrentTrain: epoch  2, batch    44 | loss: 69.9837621
CurrentTrain: epoch  2, batch    45 | loss: 68.6017914
CurrentTrain: epoch  2, batch    46 | loss: 57.9683291
CurrentTrain: epoch  2, batch    47 | loss: 90.9873062
CurrentTrain: epoch  2, batch    48 | loss: 45.4946327
CurrentTrain: epoch  2, batch    49 | loss: 91.6564595
CurrentTrain: epoch  2, batch    50 | loss: 66.7603911
CurrentTrain: epoch  2, batch    51 | loss: 66.2775373
CurrentTrain: epoch  2, batch    52 | loss: 88.5948148
CurrentTrain: epoch  2, batch    53 | loss: 87.5226052
CurrentTrain: epoch  2, batch    54 | loss: 52.6187117
CurrentTrain: epoch  2, batch    55 | loss: 45.4620881
CurrentTrain: epoch  2, batch    56 | loss: 184.2569788
CurrentTrain: epoch  2, batch    57 | loss: 87.4164001
CurrentTrain: epoch  2, batch    58 | loss: 59.1849839
CurrentTrain: epoch  2, batch    59 | loss: 87.5961755
CurrentTrain: epoch  2, batch    60 | loss: 69.2729041
CurrentTrain: epoch  2, batch    61 | loss: 66.7592485
CurrentTrain: epoch  2, batch    62 | loss: 180.7366612
CurrentTrain: epoch  2, batch    63 | loss: 85.4367414
CurrentTrain: epoch  2, batch    64 | loss: 87.1045104
CurrentTrain: epoch  2, batch    65 | loss: 117.5349008
CurrentTrain: epoch  2, batch    66 | loss: 118.6389139
CurrentTrain: epoch  2, batch    67 | loss: 68.7693141
CurrentTrain: epoch  2, batch    68 | loss: 50.7928951
CurrentTrain: epoch  2, batch    69 | loss: 89.2849750
CurrentTrain: epoch  2, batch    70 | loss: 86.5043778
CurrentTrain: epoch  2, batch    71 | loss: 66.8970497
CurrentTrain: epoch  2, batch    72 | loss: 65.3805229
CurrentTrain: epoch  2, batch    73 | loss: 67.5379918
CurrentTrain: epoch  2, batch    74 | loss: 88.8288755
CurrentTrain: epoch  2, batch    75 | loss: 66.2402058
CurrentTrain: epoch  2, batch    76 | loss: 90.8384206
CurrentTrain: epoch  2, batch    77 | loss: 111.5796637
CurrentTrain: epoch  2, batch    78 | loss: 57.1238679
CurrentTrain: epoch  2, batch    79 | loss: 65.4052888
CurrentTrain: epoch  2, batch    80 | loss: 50.2688387
CurrentTrain: epoch  2, batch    81 | loss: 48.5693123
CurrentTrain: epoch  2, batch    82 | loss: 70.9005205
CurrentTrain: epoch  2, batch    83 | loss: 87.1445150
CurrentTrain: epoch  2, batch    84 | loss: 51.4526614
CurrentTrain: epoch  2, batch    85 | loss: 88.9034143
CurrentTrain: epoch  2, batch    86 | loss: 45.4945648
CurrentTrain: epoch  2, batch    87 | loss: 67.2394729
CurrentTrain: epoch  2, batch    88 | loss: 48.5450516
CurrentTrain: epoch  2, batch    89 | loss: 68.6278642
CurrentTrain: epoch  2, batch    90 | loss: 68.6378912
CurrentTrain: epoch  2, batch    91 | loss: 90.1496123
CurrentTrain: epoch  2, batch    92 | loss: 55.6790911
CurrentTrain: epoch  2, batch    93 | loss: 68.3939040
CurrentTrain: epoch  2, batch    94 | loss: 86.5310800
CurrentTrain: epoch  2, batch    95 | loss: 54.6898291
CurrentTrain: epoch  3, batch     0 | loss: 65.7795878
CurrentTrain: epoch  3, batch     1 | loss: 56.9666907
CurrentTrain: epoch  3, batch     2 | loss: 68.7976724
CurrentTrain: epoch  3, batch     3 | loss: 55.2222053
CurrentTrain: epoch  3, batch     4 | loss: 117.4946655
CurrentTrain: epoch  3, batch     5 | loss: 78.8461424
CurrentTrain: epoch  3, batch     6 | loss: 66.4886632
CurrentTrain: epoch  3, batch     7 | loss: 71.2218217
CurrentTrain: epoch  3, batch     8 | loss: 54.5790298
CurrentTrain: epoch  3, batch     9 | loss: 88.4742551
CurrentTrain: epoch  3, batch    10 | loss: 69.6634965
CurrentTrain: epoch  3, batch    11 | loss: 52.8290622
CurrentTrain: epoch  3, batch    12 | loss: 65.8240625
CurrentTrain: epoch  3, batch    13 | loss: 68.1795539
CurrentTrain: epoch  3, batch    14 | loss: 68.6776657
CurrentTrain: epoch  3, batch    15 | loss: 89.4046389
CurrentTrain: epoch  3, batch    16 | loss: 84.3126821
CurrentTrain: epoch  3, batch    17 | loss: 55.5507444
CurrentTrain: epoch  3, batch    18 | loss: 67.4501616
CurrentTrain: epoch  3, batch    19 | loss: 68.8351069
CurrentTrain: epoch  3, batch    20 | loss: 117.5712221
CurrentTrain: epoch  3, batch    21 | loss: 53.9828292
CurrentTrain: epoch  3, batch    22 | loss: 55.1330485
CurrentTrain: epoch  3, batch    23 | loss: 67.3516190
CurrentTrain: epoch  3, batch    24 | loss: 49.4408039
CurrentTrain: epoch  3, batch    25 | loss: 68.3291473
CurrentTrain: epoch  3, batch    26 | loss: 86.4728078
CurrentTrain: epoch  3, batch    27 | loss: 60.5032249
CurrentTrain: epoch  3, batch    28 | loss: 52.1095348
CurrentTrain: epoch  3, batch    29 | loss: 64.9179003
CurrentTrain: epoch  3, batch    30 | loss: 53.0099641
CurrentTrain: epoch  3, batch    31 | loss: 69.8610653
CurrentTrain: epoch  3, batch    32 | loss: 64.3140585
CurrentTrain: epoch  3, batch    33 | loss: 55.1806460
CurrentTrain: epoch  3, batch    34 | loss: 55.3419073
CurrentTrain: epoch  3, batch    35 | loss: 46.5446041
CurrentTrain: epoch  3, batch    36 | loss: 48.0583720
CurrentTrain: epoch  3, batch    37 | loss: 57.2380014
CurrentTrain: epoch  3, batch    38 | loss: 62.3035785
CurrentTrain: epoch  3, batch    39 | loss: 53.1036104
CurrentTrain: epoch  3, batch    40 | loss: 66.3979925
CurrentTrain: epoch  3, batch    41 | loss: 81.6038020
CurrentTrain: epoch  3, batch    42 | loss: 83.7207905
CurrentTrain: epoch  3, batch    43 | loss: 85.6811804
CurrentTrain: epoch  3, batch    44 | loss: 83.5287507
CurrentTrain: epoch  3, batch    45 | loss: 66.9265828
CurrentTrain: epoch  3, batch    46 | loss: 85.0858932
CurrentTrain: epoch  3, batch    47 | loss: 47.9820488
CurrentTrain: epoch  3, batch    48 | loss: 69.0371976
CurrentTrain: epoch  3, batch    49 | loss: 86.8975692
CurrentTrain: epoch  3, batch    50 | loss: 119.0492403
CurrentTrain: epoch  3, batch    51 | loss: 86.6866801
CurrentTrain: epoch  3, batch    52 | loss: 44.6543912
CurrentTrain: epoch  3, batch    53 | loss: 65.9207044
CurrentTrain: epoch  3, batch    54 | loss: 86.4157690
CurrentTrain: epoch  3, batch    55 | loss: 51.4494664
CurrentTrain: epoch  3, batch    56 | loss: 68.6019613
CurrentTrain: epoch  3, batch    57 | loss: 43.0755112
CurrentTrain: epoch  3, batch    58 | loss: 112.1895364
CurrentTrain: epoch  3, batch    59 | loss: 53.7082471
CurrentTrain: epoch  3, batch    60 | loss: 82.7683144
CurrentTrain: epoch  3, batch    61 | loss: 117.6669746
CurrentTrain: epoch  3, batch    62 | loss: 116.3758175
CurrentTrain: epoch  3, batch    63 | loss: 54.8004113
CurrentTrain: epoch  3, batch    64 | loss: 85.0712804
CurrentTrain: epoch  3, batch    65 | loss: 87.0268480
CurrentTrain: epoch  3, batch    66 | loss: 84.7301635
CurrentTrain: epoch  3, batch    67 | loss: 56.1146051
CurrentTrain: epoch  3, batch    68 | loss: 52.8640991
CurrentTrain: epoch  3, batch    69 | loss: 68.9776315
CurrentTrain: epoch  3, batch    70 | loss: 87.7727107
CurrentTrain: epoch  3, batch    71 | loss: 84.8162642
CurrentTrain: epoch  3, batch    72 | loss: 86.9640522
CurrentTrain: epoch  3, batch    73 | loss: 86.3385254
CurrentTrain: epoch  3, batch    74 | loss: 48.4277511
CurrentTrain: epoch  3, batch    75 | loss: 66.6239562
CurrentTrain: epoch  3, batch    76 | loss: 85.9982998
CurrentTrain: epoch  3, batch    77 | loss: 85.1215142
CurrentTrain: epoch  3, batch    78 | loss: 86.7984073
CurrentTrain: epoch  3, batch    79 | loss: 57.8750504
CurrentTrain: epoch  3, batch    80 | loss: 66.5003270
CurrentTrain: epoch  3, batch    81 | loss: 120.1775853
CurrentTrain: epoch  3, batch    82 | loss: 119.5082256
CurrentTrain: epoch  3, batch    83 | loss: 85.2575475
CurrentTrain: epoch  3, batch    84 | loss: 64.4755108
CurrentTrain: epoch  3, batch    85 | loss: 67.2527079
CurrentTrain: epoch  3, batch    86 | loss: 65.9878439
CurrentTrain: epoch  3, batch    87 | loss: 90.5593260
CurrentTrain: epoch  3, batch    88 | loss: 56.2144259
CurrentTrain: epoch  3, batch    89 | loss: 85.8659774
CurrentTrain: epoch  3, batch    90 | loss: 59.0133781
CurrentTrain: epoch  3, batch    91 | loss: 53.9942201
CurrentTrain: epoch  3, batch    92 | loss: 65.9757723
CurrentTrain: epoch  3, batch    93 | loss: 87.3113652
CurrentTrain: epoch  3, batch    94 | loss: 69.5647527
CurrentTrain: epoch  3, batch    95 | loss: 72.4958138
CurrentTrain: epoch  4, batch     0 | loss: 85.1732739
CurrentTrain: epoch  4, batch     1 | loss: 65.6835986
CurrentTrain: epoch  4, batch     2 | loss: 68.7843055
CurrentTrain: epoch  4, batch     3 | loss: 84.1668075
CurrentTrain: epoch  4, batch     4 | loss: 46.4264225
CurrentTrain: epoch  4, batch     5 | loss: 86.0633589
CurrentTrain: epoch  4, batch     6 | loss: 83.6974644
CurrentTrain: epoch  4, batch     7 | loss: 181.9106327
CurrentTrain: epoch  4, batch     8 | loss: 51.4978640
CurrentTrain: epoch  4, batch     9 | loss: 83.4400925
CurrentTrain: epoch  4, batch    10 | loss: 65.1540044
CurrentTrain: epoch  4, batch    11 | loss: 53.4483008
CurrentTrain: epoch  4, batch    12 | loss: 53.5373988
CurrentTrain: epoch  4, batch    13 | loss: 54.4215853
CurrentTrain: epoch  4, batch    14 | loss: 55.3420725
CurrentTrain: epoch  4, batch    15 | loss: 50.8081406
CurrentTrain: epoch  4, batch    16 | loss: 44.3742619
CurrentTrain: epoch  4, batch    17 | loss: 55.0721365
CurrentTrain: epoch  4, batch    18 | loss: 51.3851461
CurrentTrain: epoch  4, batch    19 | loss: 65.8599404
CurrentTrain: epoch  4, batch    20 | loss: 85.3731675
CurrentTrain: epoch  4, batch    21 | loss: 54.1478747
CurrentTrain: epoch  4, batch    22 | loss: 65.7380204
CurrentTrain: epoch  4, batch    23 | loss: 117.4106115
CurrentTrain: epoch  4, batch    24 | loss: 123.2841810
CurrentTrain: epoch  4, batch    25 | loss: 66.1859477
CurrentTrain: epoch  4, batch    26 | loss: 77.3200033
CurrentTrain: epoch  4, batch    27 | loss: 53.0556888
CurrentTrain: epoch  4, batch    28 | loss: 118.5197688
CurrentTrain: epoch  4, batch    29 | loss: 67.1374722
CurrentTrain: epoch  4, batch    30 | loss: 82.3588931
CurrentTrain: epoch  4, batch    31 | loss: 119.0649585
CurrentTrain: epoch  4, batch    32 | loss: 67.7171877
CurrentTrain: epoch  4, batch    33 | loss: 53.2048922
CurrentTrain: epoch  4, batch    34 | loss: 66.2231653
CurrentTrain: epoch  4, batch    35 | loss: 87.3816931
CurrentTrain: epoch  4, batch    36 | loss: 65.6671366
CurrentTrain: epoch  4, batch    37 | loss: 61.7207580
CurrentTrain: epoch  4, batch    38 | loss: 63.1502788
CurrentTrain: epoch  4, batch    39 | loss: 66.0559353
CurrentTrain: epoch  4, batch    40 | loss: 45.8375222
CurrentTrain: epoch  4, batch    41 | loss: 177.8803987
CurrentTrain: epoch  4, batch    42 | loss: 85.4171773
CurrentTrain: epoch  4, batch    43 | loss: 51.3536353
CurrentTrain: epoch  4, batch    44 | loss: 115.1922884
CurrentTrain: epoch  4, batch    45 | loss: 58.2027567
CurrentTrain: epoch  4, batch    46 | loss: 50.4971751
CurrentTrain: epoch  4, batch    47 | loss: 117.7745847
CurrentTrain: epoch  4, batch    48 | loss: 85.8558900
CurrentTrain: epoch  4, batch    49 | loss: 54.0238879
CurrentTrain: epoch  4, batch    50 | loss: 87.6751178
CurrentTrain: epoch  4, batch    51 | loss: 55.8846022
CurrentTrain: epoch  4, batch    52 | loss: 121.8632262
CurrentTrain: epoch  4, batch    53 | loss: 87.1699653
CurrentTrain: epoch  4, batch    54 | loss: 86.3510026
CurrentTrain: epoch  4, batch    55 | loss: 53.5762501
CurrentTrain: epoch  4, batch    56 | loss: 84.0303133
CurrentTrain: epoch  4, batch    57 | loss: 48.4733036
CurrentTrain: epoch  4, batch    58 | loss: 86.3552425
CurrentTrain: epoch  4, batch    59 | loss: 90.8840114
CurrentTrain: epoch  4, batch    60 | loss: 54.8088053
CurrentTrain: epoch  4, batch    61 | loss: 84.7126819
CurrentTrain: epoch  4, batch    62 | loss: 82.1549649
CurrentTrain: epoch  4, batch    63 | loss: 83.3710538
CurrentTrain: epoch  4, batch    64 | loss: 83.4629483
CurrentTrain: epoch  4, batch    65 | loss: 83.1539129
CurrentTrain: epoch  4, batch    66 | loss: 81.7591216
CurrentTrain: epoch  4, batch    67 | loss: 48.4195728
CurrentTrain: epoch  4, batch    68 | loss: 53.2686785
CurrentTrain: epoch  4, batch    69 | loss: 45.3146766
CurrentTrain: epoch  4, batch    70 | loss: 68.4598629
CurrentTrain: epoch  4, batch    71 | loss: 71.4052529
CurrentTrain: epoch  4, batch    72 | loss: 65.7780841
CurrentTrain: epoch  4, batch    73 | loss: 69.5865314
CurrentTrain: epoch  4, batch    74 | loss: 84.5666584
CurrentTrain: epoch  4, batch    75 | loss: 86.1878803
CurrentTrain: epoch  4, batch    76 | loss: 67.5448888
CurrentTrain: epoch  4, batch    77 | loss: 115.6155538
CurrentTrain: epoch  4, batch    78 | loss: 62.9809989
CurrentTrain: epoch  4, batch    79 | loss: 82.6371562
CurrentTrain: epoch  4, batch    80 | loss: 61.8496878
CurrentTrain: epoch  4, batch    81 | loss: 51.0770616
CurrentTrain: epoch  4, batch    82 | loss: 84.8696984
CurrentTrain: epoch  4, batch    83 | loss: 65.4078468
CurrentTrain: epoch  4, batch    84 | loss: 118.0949157
CurrentTrain: epoch  4, batch    85 | loss: 63.2496462
CurrentTrain: epoch  4, batch    86 | loss: 67.9779711
CurrentTrain: epoch  4, batch    87 | loss: 68.5720423
CurrentTrain: epoch  4, batch    88 | loss: 86.9999732
CurrentTrain: epoch  4, batch    89 | loss: 62.1079170
CurrentTrain: epoch  4, batch    90 | loss: 66.2193937
CurrentTrain: epoch  4, batch    91 | loss: 66.7514143
CurrentTrain: epoch  4, batch    92 | loss: 66.7108513
CurrentTrain: epoch  4, batch    93 | loss: 115.9148012
CurrentTrain: epoch  4, batch    94 | loss: 82.5012992
CurrentTrain: epoch  4, batch    95 | loss: 52.4805857
CurrentTrain: epoch  5, batch     0 | loss: 65.3616221
CurrentTrain: epoch  5, batch     1 | loss: 54.3684017
CurrentTrain: epoch  5, batch     2 | loss: 63.8535741
CurrentTrain: epoch  5, batch     3 | loss: 67.8396151
CurrentTrain: epoch  5, batch     4 | loss: 64.8974423
CurrentTrain: epoch  5, batch     5 | loss: 64.5954862
CurrentTrain: epoch  5, batch     6 | loss: 117.3089171
CurrentTrain: epoch  5, batch     7 | loss: 67.1118164
CurrentTrain: epoch  5, batch     8 | loss: 55.2696724
CurrentTrain: epoch  5, batch     9 | loss: 67.3031649
CurrentTrain: epoch  5, batch    10 | loss: 56.8010188
CurrentTrain: epoch  5, batch    11 | loss: 82.0671033
CurrentTrain: epoch  5, batch    12 | loss: 52.5633374
CurrentTrain: epoch  5, batch    13 | loss: 50.8795495
CurrentTrain: epoch  5, batch    14 | loss: 82.4879126
CurrentTrain: epoch  5, batch    15 | loss: 64.5029018
CurrentTrain: epoch  5, batch    16 | loss: 67.1823773
CurrentTrain: epoch  5, batch    17 | loss: 41.3404131
CurrentTrain: epoch  5, batch    18 | loss: 87.7523701
CurrentTrain: epoch  5, batch    19 | loss: 182.1206653
CurrentTrain: epoch  5, batch    20 | loss: 67.0820166
CurrentTrain: epoch  5, batch    21 | loss: 69.6863789
CurrentTrain: epoch  5, batch    22 | loss: 67.2842523
CurrentTrain: epoch  5, batch    23 | loss: 82.2520326
CurrentTrain: epoch  5, batch    24 | loss: 65.2330102
CurrentTrain: epoch  5, batch    25 | loss: 64.8089369
CurrentTrain: epoch  5, batch    26 | loss: 50.5547303
CurrentTrain: epoch  5, batch    27 | loss: 66.7247486
CurrentTrain: epoch  5, batch    28 | loss: 65.9054926
CurrentTrain: epoch  5, batch    29 | loss: 115.0234552
CurrentTrain: epoch  5, batch    30 | loss: 54.9398684
CurrentTrain: epoch  5, batch    31 | loss: 115.6009055
CurrentTrain: epoch  5, batch    32 | loss: 87.3925726
CurrentTrain: epoch  5, batch    33 | loss: 84.5600006
CurrentTrain: epoch  5, batch    34 | loss: 116.3008867
CurrentTrain: epoch  5, batch    35 | loss: 63.7107187
CurrentTrain: epoch  5, batch    36 | loss: 65.9581328
CurrentTrain: epoch  5, batch    37 | loss: 116.3672320
CurrentTrain: epoch  5, batch    38 | loss: 118.3826532
CurrentTrain: epoch  5, batch    39 | loss: 117.6292365
CurrentTrain: epoch  5, batch    40 | loss: 67.4671755
CurrentTrain: epoch  5, batch    41 | loss: 49.4672709
CurrentTrain: epoch  5, batch    42 | loss: 63.3183672
CurrentTrain: epoch  5, batch    43 | loss: 54.4473365
CurrentTrain: epoch  5, batch    44 | loss: 62.4516464
CurrentTrain: epoch  5, batch    45 | loss: 70.3369980
CurrentTrain: epoch  5, batch    46 | loss: 83.7187423
CurrentTrain: epoch  5, batch    47 | loss: 63.5947317
CurrentTrain: epoch  5, batch    48 | loss: 62.0271484
CurrentTrain: epoch  5, batch    49 | loss: 66.1229443
CurrentTrain: epoch  5, batch    50 | loss: 72.9685670
CurrentTrain: epoch  5, batch    51 | loss: 62.0058497
CurrentTrain: epoch  5, batch    52 | loss: 65.6793759
CurrentTrain: epoch  5, batch    53 | loss: 82.6496539
CurrentTrain: epoch  5, batch    54 | loss: 115.9819431
CurrentTrain: epoch  5, batch    55 | loss: 43.7008048
CurrentTrain: epoch  5, batch    56 | loss: 49.5853431
CurrentTrain: epoch  5, batch    57 | loss: 68.7441862
CurrentTrain: epoch  5, batch    58 | loss: 65.0678743
CurrentTrain: epoch  5, batch    59 | loss: 51.7766323
CurrentTrain: epoch  5, batch    60 | loss: 65.5165092
CurrentTrain: epoch  5, batch    61 | loss: 83.7748196
CurrentTrain: epoch  5, batch    62 | loss: 63.5225623
CurrentTrain: epoch  5, batch    63 | loss: 171.2371752
CurrentTrain: epoch  5, batch    64 | loss: 50.9541719
CurrentTrain: epoch  5, batch    65 | loss: 50.9575919
CurrentTrain: epoch  5, batch    66 | loss: 64.2684797
CurrentTrain: epoch  5, batch    67 | loss: 50.5137984
CurrentTrain: epoch  5, batch    68 | loss: 85.7960040
CurrentTrain: epoch  5, batch    69 | loss: 62.0514002
CurrentTrain: epoch  5, batch    70 | loss: 53.2974729
CurrentTrain: epoch  5, batch    71 | loss: 64.6330074
CurrentTrain: epoch  5, batch    72 | loss: 84.5774318
CurrentTrain: epoch  5, batch    73 | loss: 67.9577390
CurrentTrain: epoch  5, batch    74 | loss: 66.3505584
CurrentTrain: epoch  5, batch    75 | loss: 53.0154335
CurrentTrain: epoch  5, batch    76 | loss: 67.6454375
CurrentTrain: epoch  5, batch    77 | loss: 119.2409862
CurrentTrain: epoch  5, batch    78 | loss: 50.2993152
CurrentTrain: epoch  5, batch    79 | loss: 57.0153061
CurrentTrain: epoch  5, batch    80 | loss: 62.9099980
CurrentTrain: epoch  5, batch    81 | loss: 64.3254115
CurrentTrain: epoch  5, batch    82 | loss: 66.3224172
CurrentTrain: epoch  5, batch    83 | loss: 66.3293561
CurrentTrain: epoch  5, batch    84 | loss: 43.2421757
CurrentTrain: epoch  5, batch    85 | loss: 67.5236259
CurrentTrain: epoch  5, batch    86 | loss: 83.8358232
CurrentTrain: epoch  5, batch    87 | loss: 85.9777360
CurrentTrain: epoch  5, batch    88 | loss: 66.8155386
CurrentTrain: epoch  5, batch    89 | loss: 51.2221376
CurrentTrain: epoch  5, batch    90 | loss: 84.5644844
CurrentTrain: epoch  5, batch    91 | loss: 52.2225120
CurrentTrain: epoch  5, batch    92 | loss: 61.7605724
CurrentTrain: epoch  5, batch    93 | loss: 89.6984994
CurrentTrain: epoch  5, batch    94 | loss: 66.1640187
CurrentTrain: epoch  5, batch    95 | loss: 67.3505560
CurrentTrain: epoch  6, batch     0 | loss: 53.9538039
CurrentTrain: epoch  6, batch     1 | loss: 81.1255561
CurrentTrain: epoch  6, batch     2 | loss: 45.7095619
CurrentTrain: epoch  6, batch     3 | loss: 82.6132872
CurrentTrain: epoch  6, batch     4 | loss: 83.0700052
CurrentTrain: epoch  6, batch     5 | loss: 40.0405491
CurrentTrain: epoch  6, batch     6 | loss: 44.6726977
CurrentTrain: epoch  6, batch     7 | loss: 84.9893617
CurrentTrain: epoch  6, batch     8 | loss: 68.5737757
CurrentTrain: epoch  6, batch     9 | loss: 43.3850805
CurrentTrain: epoch  6, batch    10 | loss: 116.5156318
CurrentTrain: epoch  6, batch    11 | loss: 85.3071639
CurrentTrain: epoch  6, batch    12 | loss: 55.0995725
CurrentTrain: epoch  6, batch    13 | loss: 60.6475530
CurrentTrain: epoch  6, batch    14 | loss: 66.9312466
CurrentTrain: epoch  6, batch    15 | loss: 66.5483152
CurrentTrain: epoch  6, batch    16 | loss: 47.4145091
CurrentTrain: epoch  6, batch    17 | loss: 50.1564786
CurrentTrain: epoch  6, batch    18 | loss: 42.6031275
CurrentTrain: epoch  6, batch    19 | loss: 48.8597885
CurrentTrain: epoch  6, batch    20 | loss: 54.0834971
CurrentTrain: epoch  6, batch    21 | loss: 66.0361710
CurrentTrain: epoch  6, batch    22 | loss: 65.0806566
CurrentTrain: epoch  6, batch    23 | loss: 86.3062707
CurrentTrain: epoch  6, batch    24 | loss: 44.1237389
CurrentTrain: epoch  6, batch    25 | loss: 115.6494737
CurrentTrain: epoch  6, batch    26 | loss: 67.0864605
CurrentTrain: epoch  6, batch    27 | loss: 83.1993262
CurrentTrain: epoch  6, batch    28 | loss: 63.0459227
CurrentTrain: epoch  6, batch    29 | loss: 84.3505602
CurrentTrain: epoch  6, batch    30 | loss: 64.9241062
CurrentTrain: epoch  6, batch    31 | loss: 63.5666512
CurrentTrain: epoch  6, batch    32 | loss: 50.4919326
CurrentTrain: epoch  6, batch    33 | loss: 81.3697222
CurrentTrain: epoch  6, batch    34 | loss: 52.5288518
CurrentTrain: epoch  6, batch    35 | loss: 54.8751891
CurrentTrain: epoch  6, batch    36 | loss: 64.1538652
CurrentTrain: epoch  6, batch    37 | loss: 67.3529235
CurrentTrain: epoch  6, batch    38 | loss: 82.7918738
CurrentTrain: epoch  6, batch    39 | loss: 66.4463167
CurrentTrain: epoch  6, batch    40 | loss: 52.0451789
CurrentTrain: epoch  6, batch    41 | loss: 65.5449239
CurrentTrain: epoch  6, batch    42 | loss: 44.5116371
CurrentTrain: epoch  6, batch    43 | loss: 64.0167955
CurrentTrain: epoch  6, batch    44 | loss: 55.2818563
CurrentTrain: epoch  6, batch    45 | loss: 64.8112976
CurrentTrain: epoch  6, batch    46 | loss: 61.9234624
CurrentTrain: epoch  6, batch    47 | loss: 67.1063142
CurrentTrain: epoch  6, batch    48 | loss: 83.9774880
CurrentTrain: epoch  6, batch    49 | loss: 65.4944675
CurrentTrain: epoch  6, batch    50 | loss: 44.0133542
CurrentTrain: epoch  6, batch    51 | loss: 63.5321128
CurrentTrain: epoch  6, batch    52 | loss: 118.1171801
CurrentTrain: epoch  6, batch    53 | loss: 116.4921805
CurrentTrain: epoch  6, batch    54 | loss: 64.7089603
CurrentTrain: epoch  6, batch    55 | loss: 44.6025099
CurrentTrain: epoch  6, batch    56 | loss: 52.6165498
CurrentTrain: epoch  6, batch    57 | loss: 56.0592810
CurrentTrain: epoch  6, batch    58 | loss: 85.7539578
CurrentTrain: epoch  6, batch    59 | loss: 65.6073725
CurrentTrain: epoch  6, batch    60 | loss: 65.4938838
CurrentTrain: epoch  6, batch    61 | loss: 85.1641709
CurrentTrain: epoch  6, batch    62 | loss: 80.6128718
CurrentTrain: epoch  6, batch    63 | loss: 66.1406753
CurrentTrain: epoch  6, batch    64 | loss: 68.9292019
CurrentTrain: epoch  6, batch    65 | loss: 63.4842911
CurrentTrain: epoch  6, batch    66 | loss: 62.5534777
CurrentTrain: epoch  6, batch    67 | loss: 79.2969582
CurrentTrain: epoch  6, batch    68 | loss: 84.3204511
CurrentTrain: epoch  6, batch    69 | loss: 82.6906352
CurrentTrain: epoch  6, batch    70 | loss: 80.1622016
CurrentTrain: epoch  6, batch    71 | loss: 69.6320348
CurrentTrain: epoch  6, batch    72 | loss: 115.5464727
CurrentTrain: epoch  6, batch    73 | loss: 51.2356713
CurrentTrain: epoch  6, batch    74 | loss: 118.2854470
CurrentTrain: epoch  6, batch    75 | loss: 64.7110815
CurrentTrain: epoch  6, batch    76 | loss: 50.3833732
CurrentTrain: epoch  6, batch    77 | loss: 64.3892128
CurrentTrain: epoch  6, batch    78 | loss: 111.6981405
CurrentTrain: epoch  6, batch    79 | loss: 85.2310962
CurrentTrain: epoch  6, batch    80 | loss: 117.7003597
CurrentTrain: epoch  6, batch    81 | loss: 48.5296673
CurrentTrain: epoch  6, batch    82 | loss: 118.7066914
CurrentTrain: epoch  6, batch    83 | loss: 43.5329917
CurrentTrain: epoch  6, batch    84 | loss: 66.9955019
CurrentTrain: epoch  6, batch    85 | loss: 65.6195130
CurrentTrain: epoch  6, batch    86 | loss: 90.1568000
CurrentTrain: epoch  6, batch    87 | loss: 69.9360186
CurrentTrain: epoch  6, batch    88 | loss: 64.1155687
CurrentTrain: epoch  6, batch    89 | loss: 84.6290645
CurrentTrain: epoch  6, batch    90 | loss: 118.9721713
CurrentTrain: epoch  6, batch    91 | loss: 114.2888440
CurrentTrain: epoch  6, batch    92 | loss: 48.2951581
CurrentTrain: epoch  6, batch    93 | loss: 81.4967396
CurrentTrain: epoch  6, batch    94 | loss: 53.2555481
CurrentTrain: epoch  6, batch    95 | loss: 54.3522880
CurrentTrain: epoch  7, batch     0 | loss: 65.4772400
CurrentTrain: epoch  7, batch     1 | loss: 113.9642501
CurrentTrain: epoch  7, batch     2 | loss: 84.0732498
CurrentTrain: epoch  7, batch     3 | loss: 66.3278783
CurrentTrain: epoch  7, batch     4 | loss: 85.2965124
CurrentTrain: epoch  7, batch     5 | loss: 65.4463456
CurrentTrain: epoch  7, batch     6 | loss: 83.7880776
CurrentTrain: epoch  7, batch     7 | loss: 52.2601400
CurrentTrain: epoch  7, batch     8 | loss: 47.6111753
CurrentTrain: epoch  7, batch     9 | loss: 81.6271150
CurrentTrain: epoch  7, batch    10 | loss: 53.0528683
CurrentTrain: epoch  7, batch    11 | loss: 41.5940194
CurrentTrain: epoch  7, batch    12 | loss: 66.6620750
CurrentTrain: epoch  7, batch    13 | loss: 117.7418450
CurrentTrain: epoch  7, batch    14 | loss: 82.8742035
CurrentTrain: epoch  7, batch    15 | loss: 40.6127419
CurrentTrain: epoch  7, batch    16 | loss: 62.2847212
CurrentTrain: epoch  7, batch    17 | loss: 54.5070012
CurrentTrain: epoch  7, batch    18 | loss: 64.2530999
CurrentTrain: epoch  7, batch    19 | loss: 66.9143942
CurrentTrain: epoch  7, batch    20 | loss: 43.6176039
CurrentTrain: epoch  7, batch    21 | loss: 84.0893944
CurrentTrain: epoch  7, batch    22 | loss: 65.5970014
CurrentTrain: epoch  7, batch    23 | loss: 85.3019576
CurrentTrain: epoch  7, batch    24 | loss: 54.0271056
CurrentTrain: epoch  7, batch    25 | loss: 65.3380215
CurrentTrain: epoch  7, batch    26 | loss: 61.9491093
CurrentTrain: epoch  7, batch    27 | loss: 64.5027301
CurrentTrain: epoch  7, batch    28 | loss: 50.1102983
CurrentTrain: epoch  7, batch    29 | loss: 61.8071702
CurrentTrain: epoch  7, batch    30 | loss: 117.4770866
CurrentTrain: epoch  7, batch    31 | loss: 52.4265274
CurrentTrain: epoch  7, batch    32 | loss: 65.2101704
CurrentTrain: epoch  7, batch    33 | loss: 70.7356664
CurrentTrain: epoch  7, batch    34 | loss: 115.4251328
CurrentTrain: epoch  7, batch    35 | loss: 85.4194800
CurrentTrain: epoch  7, batch    36 | loss: 50.5004707
CurrentTrain: epoch  7, batch    37 | loss: 67.4470815
CurrentTrain: epoch  7, batch    38 | loss: 41.5252483
CurrentTrain: epoch  7, batch    39 | loss: 64.3406127
CurrentTrain: epoch  7, batch    40 | loss: 65.4051385
CurrentTrain: epoch  7, batch    41 | loss: 53.2616226
CurrentTrain: epoch  7, batch    42 | loss: 65.4398817
CurrentTrain: epoch  7, batch    43 | loss: 53.4726527
CurrentTrain: epoch  7, batch    44 | loss: 81.6638365
CurrentTrain: epoch  7, batch    45 | loss: 52.6366824
CurrentTrain: epoch  7, batch    46 | loss: 65.7007057
CurrentTrain: epoch  7, batch    47 | loss: 56.8379432
CurrentTrain: epoch  7, batch    48 | loss: 54.0836910
CurrentTrain: epoch  7, batch    49 | loss: 115.6284903
CurrentTrain: epoch  7, batch    50 | loss: 53.0143345
CurrentTrain: epoch  7, batch    51 | loss: 67.3717841
CurrentTrain: epoch  7, batch    52 | loss: 52.1561751
CurrentTrain: epoch  7, batch    53 | loss: 44.3787355
CurrentTrain: epoch  7, batch    54 | loss: 52.8189365
CurrentTrain: epoch  7, batch    55 | loss: 64.9894065
CurrentTrain: epoch  7, batch    56 | loss: 90.6344856
CurrentTrain: epoch  7, batch    57 | loss: 53.8625734
CurrentTrain: epoch  7, batch    58 | loss: 63.4423036
CurrentTrain: epoch  7, batch    59 | loss: 82.9330022
CurrentTrain: epoch  7, batch    60 | loss: 84.5141682
CurrentTrain: epoch  7, batch    61 | loss: 63.6451938
CurrentTrain: epoch  7, batch    62 | loss: 63.6616634
CurrentTrain: epoch  7, batch    63 | loss: 62.0419930
CurrentTrain: epoch  7, batch    64 | loss: 87.4936640
CurrentTrain: epoch  7, batch    65 | loss: 53.4380139
CurrentTrain: epoch  7, batch    66 | loss: 113.0167895
CurrentTrain: epoch  7, batch    67 | loss: 119.3428995
CurrentTrain: epoch  7, batch    68 | loss: 70.7328613
CurrentTrain: epoch  7, batch    69 | loss: 53.6551388
CurrentTrain: epoch  7, batch    70 | loss: 63.0986753
CurrentTrain: epoch  7, batch    71 | loss: 66.3625095
CurrentTrain: epoch  7, batch    72 | loss: 83.1218751
CurrentTrain: epoch  7, batch    73 | loss: 82.9316980
CurrentTrain: epoch  7, batch    74 | loss: 115.4852821
CurrentTrain: epoch  7, batch    75 | loss: 64.4781871
CurrentTrain: epoch  7, batch    76 | loss: 69.3893587
CurrentTrain: epoch  7, batch    77 | loss: 49.7306578
CurrentTrain: epoch  7, batch    78 | loss: 53.2284829
CurrentTrain: epoch  7, batch    79 | loss: 78.5502745
CurrentTrain: epoch  7, batch    80 | loss: 85.8079562
CurrentTrain: epoch  7, batch    81 | loss: 52.9563229
CurrentTrain: epoch  7, batch    82 | loss: 67.9108830
CurrentTrain: epoch  7, batch    83 | loss: 85.8521072
CurrentTrain: epoch  7, batch    84 | loss: 51.4320510
CurrentTrain: epoch  7, batch    85 | loss: 64.3932064
CurrentTrain: epoch  7, batch    86 | loss: 63.3766907
CurrentTrain: epoch  7, batch    87 | loss: 86.0704689
CurrentTrain: epoch  7, batch    88 | loss: 54.2805244
CurrentTrain: epoch  7, batch    89 | loss: 54.1392352
CurrentTrain: epoch  7, batch    90 | loss: 53.3220058
CurrentTrain: epoch  7, batch    91 | loss: 53.7257039
CurrentTrain: epoch  7, batch    92 | loss: 64.6732815
CurrentTrain: epoch  7, batch    93 | loss: 51.9244814
CurrentTrain: epoch  7, batch    94 | loss: 63.4803049
CurrentTrain: epoch  7, batch    95 | loss: 55.8026225
CurrentTrain: epoch  8, batch     0 | loss: 53.1554053
CurrentTrain: epoch  8, batch     1 | loss: 66.5768511
CurrentTrain: epoch  8, batch     2 | loss: 53.8213709
CurrentTrain: epoch  8, batch     3 | loss: 115.3542013
CurrentTrain: epoch  8, batch     4 | loss: 83.4305099
CurrentTrain: epoch  8, batch     5 | loss: 65.8998032
CurrentTrain: epoch  8, batch     6 | loss: 50.9860601
CurrentTrain: epoch  8, batch     7 | loss: 84.4660521
CurrentTrain: epoch  8, batch     8 | loss: 67.4829244
CurrentTrain: epoch  8, batch     9 | loss: 53.5227618
CurrentTrain: epoch  8, batch    10 | loss: 115.2509669
CurrentTrain: epoch  8, batch    11 | loss: 65.1583793
CurrentTrain: epoch  8, batch    12 | loss: 52.0017160
CurrentTrain: epoch  8, batch    13 | loss: 54.7218361
CurrentTrain: epoch  8, batch    14 | loss: 52.2376907
CurrentTrain: epoch  8, batch    15 | loss: 66.5245819
CurrentTrain: epoch  8, batch    16 | loss: 85.8075638
CurrentTrain: epoch  8, batch    17 | loss: 51.2594146
CurrentTrain: epoch  8, batch    18 | loss: 53.8845374
CurrentTrain: epoch  8, batch    19 | loss: 82.6534003
CurrentTrain: epoch  8, batch    20 | loss: 51.3570515
CurrentTrain: epoch  8, batch    21 | loss: 65.4795080
CurrentTrain: epoch  8, batch    22 | loss: 82.7539378
CurrentTrain: epoch  8, batch    23 | loss: 64.5318154
CurrentTrain: epoch  8, batch    24 | loss: 44.0645473
CurrentTrain: epoch  8, batch    25 | loss: 59.8936984
CurrentTrain: epoch  8, batch    26 | loss: 64.8680779
CurrentTrain: epoch  8, batch    27 | loss: 61.7416773
CurrentTrain: epoch  8, batch    28 | loss: 64.4236523
CurrentTrain: epoch  8, batch    29 | loss: 86.6834257
CurrentTrain: epoch  8, batch    30 | loss: 51.5598936
CurrentTrain: epoch  8, batch    31 | loss: 82.3959173
CurrentTrain: epoch  8, batch    32 | loss: 48.6071214
CurrentTrain: epoch  8, batch    33 | loss: 41.0272955
CurrentTrain: epoch  8, batch    34 | loss: 60.1207118
CurrentTrain: epoch  8, batch    35 | loss: 52.5329515
CurrentTrain: epoch  8, batch    36 | loss: 43.3888093
CurrentTrain: epoch  8, batch    37 | loss: 51.6308593
CurrentTrain: epoch  8, batch    38 | loss: 53.6552120
CurrentTrain: epoch  8, batch    39 | loss: 63.0986892
CurrentTrain: epoch  8, batch    40 | loss: 80.7766436
CurrentTrain: epoch  8, batch    41 | loss: 52.0476395
CurrentTrain: epoch  8, batch    42 | loss: 65.4804474
CurrentTrain: epoch  8, batch    43 | loss: 50.1122517
CurrentTrain: epoch  8, batch    44 | loss: 84.0951178
CurrentTrain: epoch  8, batch    45 | loss: 85.7945198
CurrentTrain: epoch  8, batch    46 | loss: 52.3166633
CurrentTrain: epoch  8, batch    47 | loss: 117.5071679
CurrentTrain: epoch  8, batch    48 | loss: 82.5006300
CurrentTrain: epoch  8, batch    49 | loss: 84.7310506
CurrentTrain: epoch  8, batch    50 | loss: 52.6564114
CurrentTrain: epoch  8, batch    51 | loss: 64.5967149
CurrentTrain: epoch  8, batch    52 | loss: 50.1662094
CurrentTrain: epoch  8, batch    53 | loss: 40.8296770
CurrentTrain: epoch  8, batch    54 | loss: 64.7042998
CurrentTrain: epoch  8, batch    55 | loss: 79.9844603
CurrentTrain: epoch  8, batch    56 | loss: 84.0598387
CurrentTrain: epoch  8, batch    57 | loss: 181.6008516
CurrentTrain: epoch  8, batch    58 | loss: 62.8382748
CurrentTrain: epoch  8, batch    59 | loss: 84.0810750
CurrentTrain: epoch  8, batch    60 | loss: 49.9868595
CurrentTrain: epoch  8, batch    61 | loss: 115.2725039
CurrentTrain: epoch  8, batch    62 | loss: 86.0801295
CurrentTrain: epoch  8, batch    63 | loss: 54.8674375
CurrentTrain: epoch  8, batch    64 | loss: 84.1873458
CurrentTrain: epoch  8, batch    65 | loss: 44.0957333
CurrentTrain: epoch  8, batch    66 | loss: 63.0636322
CurrentTrain: epoch  8, batch    67 | loss: 65.7217669
CurrentTrain: epoch  8, batch    68 | loss: 51.5831603
CurrentTrain: epoch  8, batch    69 | loss: 65.4138240
CurrentTrain: epoch  8, batch    70 | loss: 82.5430156
CurrentTrain: epoch  8, batch    71 | loss: 53.8241249
CurrentTrain: epoch  8, batch    72 | loss: 63.1271769
CurrentTrain: epoch  8, batch    73 | loss: 64.8373469
CurrentTrain: epoch  8, batch    74 | loss: 50.7901549
CurrentTrain: epoch  8, batch    75 | loss: 54.9943493
CurrentTrain: epoch  8, batch    76 | loss: 84.4618299
CurrentTrain: epoch  8, batch    77 | loss: 119.8951506
CurrentTrain: epoch  8, batch    78 | loss: 82.3734230
CurrentTrain: epoch  8, batch    79 | loss: 52.6207254
CurrentTrain: epoch  8, batch    80 | loss: 84.0653334
CurrentTrain: epoch  8, batch    81 | loss: 82.9975629
CurrentTrain: epoch  8, batch    82 | loss: 63.2848757
CurrentTrain: epoch  8, batch    83 | loss: 83.0633942
CurrentTrain: epoch  8, batch    84 | loss: 84.3467801
CurrentTrain: epoch  8, batch    85 | loss: 62.6868152
CurrentTrain: epoch  8, batch    86 | loss: 113.3794338
CurrentTrain: epoch  8, batch    87 | loss: 86.4171605
CurrentTrain: epoch  8, batch    88 | loss: 40.0146730
CurrentTrain: epoch  8, batch    89 | loss: 82.9975733
CurrentTrain: epoch  8, batch    90 | loss: 43.9202476
CurrentTrain: epoch  8, batch    91 | loss: 117.6456956
CurrentTrain: epoch  8, batch    92 | loss: 81.2531269
CurrentTrain: epoch  8, batch    93 | loss: 64.2987191
CurrentTrain: epoch  8, batch    94 | loss: 42.7196894
CurrentTrain: epoch  8, batch    95 | loss: 94.6313491
CurrentTrain: epoch  9, batch     0 | loss: 83.0479935
CurrentTrain: epoch  9, batch     1 | loss: 64.7985351
CurrentTrain: epoch  9, batch     2 | loss: 64.4051835
CurrentTrain: epoch  9, batch     3 | loss: 64.6121622
CurrentTrain: epoch  9, batch     4 | loss: 64.4276570
CurrentTrain: epoch  9, batch     5 | loss: 82.6617868
CurrentTrain: epoch  9, batch     6 | loss: 45.9348281
CurrentTrain: epoch  9, batch     7 | loss: 66.7340130
CurrentTrain: epoch  9, batch     8 | loss: 81.2612677
CurrentTrain: epoch  9, batch     9 | loss: 51.5700791
CurrentTrain: epoch  9, batch    10 | loss: 66.7674319
CurrentTrain: epoch  9, batch    11 | loss: 111.6858880
CurrentTrain: epoch  9, batch    12 | loss: 65.6215441
CurrentTrain: epoch  9, batch    13 | loss: 64.0772561
CurrentTrain: epoch  9, batch    14 | loss: 64.1272272
CurrentTrain: epoch  9, batch    15 | loss: 65.7670470
CurrentTrain: epoch  9, batch    16 | loss: 81.4712457
CurrentTrain: epoch  9, batch    17 | loss: 52.2716471
CurrentTrain: epoch  9, batch    18 | loss: 65.8082614
CurrentTrain: epoch  9, batch    19 | loss: 78.7087915
CurrentTrain: epoch  9, batch    20 | loss: 115.7913244
CurrentTrain: epoch  9, batch    21 | loss: 50.5971599
CurrentTrain: epoch  9, batch    22 | loss: 50.0810082
CurrentTrain: epoch  9, batch    23 | loss: 49.4584041
CurrentTrain: epoch  9, batch    24 | loss: 113.0277437
CurrentTrain: epoch  9, batch    25 | loss: 61.7604059
CurrentTrain: epoch  9, batch    26 | loss: 51.9030969
CurrentTrain: epoch  9, batch    27 | loss: 49.4051800
CurrentTrain: epoch  9, batch    28 | loss: 84.3894421
CurrentTrain: epoch  9, batch    29 | loss: 41.5988770
CurrentTrain: epoch  9, batch    30 | loss: 52.0370086
CurrentTrain: epoch  9, batch    31 | loss: 52.8925347
CurrentTrain: epoch  9, batch    32 | loss: 85.7610479
CurrentTrain: epoch  9, batch    33 | loss: 66.7433279
CurrentTrain: epoch  9, batch    34 | loss: 79.7554889
CurrentTrain: epoch  9, batch    35 | loss: 52.9631980
CurrentTrain: epoch  9, batch    36 | loss: 84.3315633
CurrentTrain: epoch  9, batch    37 | loss: 66.0996715
CurrentTrain: epoch  9, batch    38 | loss: 49.9668343
CurrentTrain: epoch  9, batch    39 | loss: 81.9903101
CurrentTrain: epoch  9, batch    40 | loss: 62.9968291
CurrentTrain: epoch  9, batch    41 | loss: 65.4631986
CurrentTrain: epoch  9, batch    42 | loss: 82.3056707
CurrentTrain: epoch  9, batch    43 | loss: 80.3202378
CurrentTrain: epoch  9, batch    44 | loss: 112.9991280
CurrentTrain: epoch  9, batch    45 | loss: 61.9283435
CurrentTrain: epoch  9, batch    46 | loss: 181.6687055
CurrentTrain: epoch  9, batch    47 | loss: 62.9593852
CurrentTrain: epoch  9, batch    48 | loss: 84.2271441
CurrentTrain: epoch  9, batch    49 | loss: 64.0232525
CurrentTrain: epoch  9, batch    50 | loss: 53.3009546
CurrentTrain: epoch  9, batch    51 | loss: 51.3627587
CurrentTrain: epoch  9, batch    52 | loss: 62.8735204
CurrentTrain: epoch  9, batch    53 | loss: 53.1364192
CurrentTrain: epoch  9, batch    54 | loss: 84.1694534
CurrentTrain: epoch  9, batch    55 | loss: 177.6008818
CurrentTrain: epoch  9, batch    56 | loss: 66.7099437
CurrentTrain: epoch  9, batch    57 | loss: 62.8712056
CurrentTrain: epoch  9, batch    58 | loss: 48.4563950
CurrentTrain: epoch  9, batch    59 | loss: 65.3343829
CurrentTrain: epoch  9, batch    60 | loss: 115.3973188
CurrentTrain: epoch  9, batch    61 | loss: 48.9641801
CurrentTrain: epoch  9, batch    62 | loss: 54.4214981
CurrentTrain: epoch  9, batch    63 | loss: 84.2643513
CurrentTrain: epoch  9, batch    64 | loss: 47.9388626
CurrentTrain: epoch  9, batch    65 | loss: 81.0485384
CurrentTrain: epoch  9, batch    66 | loss: 84.1044292
CurrentTrain: epoch  9, batch    67 | loss: 64.7248502
CurrentTrain: epoch  9, batch    68 | loss: 62.5566789
CurrentTrain: epoch  9, batch    69 | loss: 51.4396880
CurrentTrain: epoch  9, batch    70 | loss: 52.0266656
CurrentTrain: epoch  9, batch    71 | loss: 115.2514187
CurrentTrain: epoch  9, batch    72 | loss: 115.2417567
CurrentTrain: epoch  9, batch    73 | loss: 83.7045447
CurrentTrain: epoch  9, batch    74 | loss: 64.4711399
CurrentTrain: epoch  9, batch    75 | loss: 53.7907905
CurrentTrain: epoch  9, batch    76 | loss: 62.9343546
CurrentTrain: epoch  9, batch    77 | loss: 61.8426540
CurrentTrain: epoch  9, batch    78 | loss: 44.3882990
CurrentTrain: epoch  9, batch    79 | loss: 65.4269154
CurrentTrain: epoch  9, batch    80 | loss: 52.0868761
CurrentTrain: epoch  9, batch    81 | loss: 40.1930324
CurrentTrain: epoch  9, batch    82 | loss: 63.4568447
CurrentTrain: epoch  9, batch    83 | loss: 54.1768181
CurrentTrain: epoch  9, batch    84 | loss: 82.7291147
CurrentTrain: epoch  9, batch    85 | loss: 64.1688212
CurrentTrain: epoch  9, batch    86 | loss: 80.9178171
CurrentTrain: epoch  9, batch    87 | loss: 52.7423753
CurrentTrain: epoch  9, batch    88 | loss: 115.2332198
CurrentTrain: epoch  9, batch    89 | loss: 48.0665607
CurrentTrain: epoch  9, batch    90 | loss: 63.3937302
CurrentTrain: epoch  9, batch    91 | loss: 62.8692949
CurrentTrain: epoch  9, batch    92 | loss: 117.4534378
CurrentTrain: epoch  9, batch    93 | loss: 59.6373490
CurrentTrain: epoch  9, batch    94 | loss: 64.1948763
CurrentTrain: epoch  9, batch    95 | loss: 71.5250844

F1 score per class: {32: 0.573170731707317, 6: 0.7096774193548387, 19: 0.32, 24: 0.7513812154696132, 26: 0.9010989010989011, 29: 0.8854166666666666}
Micro-average F1 score: 0.7586206896551724
Weighted-average F1 score: 0.7720879437539798
F1 score per class: {32: 0.6881720430107527, 6: 0.7577639751552795, 19: 0.46153846153846156, 24: 0.7540983606557377, 26: 0.9473684210526315, 29: 0.9035532994923858}
Micro-average F1 score: 0.8038176033934252
Weighted-average F1 score: 0.8108157594250093
F1 score per class: {32: 0.6881720430107527, 6: 0.7577639751552795, 19: 0.4444444444444444, 24: 0.7540983606557377, 26: 0.9473684210526315, 29: 0.8979591836734694}
Micro-average F1 score: 0.8016967126193001
Weighted-average F1 score: 0.808150121518148

F1 score per class: {32: 0.573170731707317, 6: 0.7096774193548387, 19: 0.32, 24: 0.7513812154696132, 26: 0.9010989010989011, 29: 0.8854166666666666}
Micro-average F1 score: 0.7586206896551724
Weighted-average F1 score: 0.7720879437539798
F1 score per class: {32: 0.6881720430107527, 6: 0.7577639751552795, 19: 0.46153846153846156, 24: 0.7540983606557377, 26: 0.9473684210526315, 29: 0.9035532994923858}
Micro-average F1 score: 0.8038176033934252
Weighted-average F1 score: 0.8108157594250093
F1 score per class: {32: 0.6881720430107527, 6: 0.7577639751552795, 19: 0.4444444444444444, 24: 0.7540983606557377, 26: 0.9473684210526315, 29: 0.8979591836734694}
Micro-average F1 score: 0.8016967126193001
Weighted-average F1 score: 0.808150121518148
cur_acc:  ['0.7586']
his_acc:  ['0.7586']
cur_acc des:  ['0.8038']
his_acc des:  ['0.8038']
cur_acc rrf:  ['0.8017']
his_acc rrf:  ['0.8017']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 76.1946317
CurrentTrain: epoch  0, batch     1 | loss: 127.6689473
CurrentTrain: epoch  0, batch     2 | loss: 98.4537509
CurrentTrain: epoch  0, batch     3 | loss: 63.5994003
CurrentTrain: epoch  0, batch     4 | loss: 81.1426206
CurrentTrain: epoch  1, batch     0 | loss: 73.5869274
CurrentTrain: epoch  1, batch     1 | loss: 75.0673666
CurrentTrain: epoch  1, batch     2 | loss: 94.3318965
CurrentTrain: epoch  1, batch     3 | loss: 72.6040917
CurrentTrain: epoch  1, batch     4 | loss: 59.7303132
CurrentTrain: epoch  2, batch     0 | loss: 91.2491082
CurrentTrain: epoch  2, batch     1 | loss: 60.6447684
CurrentTrain: epoch  2, batch     2 | loss: 72.7294673
CurrentTrain: epoch  2, batch     3 | loss: 73.6146483
CurrentTrain: epoch  2, batch     4 | loss: 58.7630115
CurrentTrain: epoch  3, batch     0 | loss: 88.2703303
CurrentTrain: epoch  3, batch     1 | loss: 90.7118673
CurrentTrain: epoch  3, batch     2 | loss: 71.1161641
CurrentTrain: epoch  3, batch     3 | loss: 118.3318515
CurrentTrain: epoch  3, batch     4 | loss: 76.7386997
CurrentTrain: epoch  4, batch     0 | loss: 69.6497578
CurrentTrain: epoch  4, batch     1 | loss: 58.2810853
CurrentTrain: epoch  4, batch     2 | loss: 86.5855469
CurrentTrain: epoch  4, batch     3 | loss: 119.0537372
CurrentTrain: epoch  4, batch     4 | loss: 54.5972150
CurrentTrain: epoch  5, batch     0 | loss: 70.4588540
CurrentTrain: epoch  5, batch     1 | loss: 86.3598647
CurrentTrain: epoch  5, batch     2 | loss: 57.4270553
CurrentTrain: epoch  5, batch     3 | loss: 56.1404041
CurrentTrain: epoch  5, batch     4 | loss: 43.0883401
CurrentTrain: epoch  6, batch     0 | loss: 68.0048868
CurrentTrain: epoch  6, batch     1 | loss: 53.6297698
CurrentTrain: epoch  6, batch     2 | loss: 68.2798274
CurrentTrain: epoch  6, batch     3 | loss: 118.8537641
CurrentTrain: epoch  6, batch     4 | loss: 73.1048439
CurrentTrain: epoch  7, batch     0 | loss: 64.9633409
CurrentTrain: epoch  7, batch     1 | loss: 84.6313775
CurrentTrain: epoch  7, batch     2 | loss: 81.4583826
CurrentTrain: epoch  7, batch     3 | loss: 86.5145811
CurrentTrain: epoch  7, batch     4 | loss: 41.8553035
CurrentTrain: epoch  8, batch     0 | loss: 66.4644631
CurrentTrain: epoch  8, batch     1 | loss: 65.5204427
CurrentTrain: epoch  8, batch     2 | loss: 116.1456498
CurrentTrain: epoch  8, batch     3 | loss: 115.3387487
CurrentTrain: epoch  8, batch     4 | loss: 67.4964925
CurrentTrain: epoch  9, batch     0 | loss: 52.9070090
CurrentTrain: epoch  9, batch     1 | loss: 86.8967539
CurrentTrain: epoch  9, batch     2 | loss: 82.8768667
CurrentTrain: epoch  9, batch     3 | loss: 63.0684527
CurrentTrain: epoch  9, batch     4 | loss: 112.5511756
MemoryTrain:  epoch  0, batch     0 | loss: 0.4134722
MemoryTrain:  epoch  1, batch     0 | loss: 0.4306416
MemoryTrain:  epoch  2, batch     0 | loss: 0.3371612
MemoryTrain:  epoch  3, batch     0 | loss: 0.2661148
MemoryTrain:  epoch  4, batch     0 | loss: 0.2081027
MemoryTrain:  epoch  5, batch     0 | loss: 0.1528758
MemoryTrain:  epoch  6, batch     0 | loss: 0.1440114
MemoryTrain:  epoch  7, batch     0 | loss: 0.0965777
MemoryTrain:  epoch  8, batch     0 | loss: 0.0936100
MemoryTrain:  epoch  9, batch     0 | loss: 0.0667290

F1 score per class: {5: 0.9690721649484536, 6: 0.0, 10: 0.6013986013986014, 16: 0.7843137254901961, 17: 0.5, 18: 0.425531914893617}
Micro-average F1 score: 0.7505518763796909
Weighted-average F1 score: 0.7863210776809308
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.7530864197530864, 16: 0.8301886792452831, 17: 0.18181818181818182, 18: 0.84375}
Micro-average F1 score: 0.8410462776659959
Weighted-average F1 score: 0.8413605657062003
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.7530864197530864, 16: 0.8076923076923077, 17: 0.18181818181818182, 18: 0.84375}
Micro-average F1 score: 0.8387096774193549
Weighted-average F1 score: 0.8391617098529979

F1 score per class: {32: 0.9591836734693877, 5: 0.3157894736842105, 6: 0.593103448275862, 10: 0.7843137254901961, 16: 0.08823529411764706, 17: 0.4166666666666667, 18: 0.7804878048780488, 19: 0.3333333333333333, 24: 0.7558139534883721, 26: 0.9528795811518325, 29: 0.8947368421052632}
Micro-average F1 score: 0.723589001447178
Weighted-average F1 score: 0.7296944323405969
F1 score per class: {32: 0.98989898989899, 5: 0.5730994152046783, 6: 0.7484662576687117, 10: 0.8, 16: 0.03508771929824561, 17: 0.7941176470588235, 18: 0.8522727272727273, 19: 0.5714285714285714, 24: 0.7542857142857143, 26: 0.9743589743589743, 29: 0.9072164948453608}
Micro-average F1 score: 0.7972972972972973
Weighted-average F1 score: 0.781423157355553
F1 score per class: {32: 0.98, 5: 0.5217391304347826, 6: 0.7439024390243902, 10: 0.7777777777777778, 16: 0.03278688524590164, 17: 0.7941176470588235, 18: 0.8522727272727273, 19: 0.5714285714285714, 24: 0.7630057803468208, 26: 0.9637305699481865, 29: 0.9072164948453608}
Micro-average F1 score: 0.7880434782608695
Weighted-average F1 score: 0.772148677412
cur_acc:  ['0.7586', '0.7506']
his_acc:  ['0.7586', '0.7236']
cur_acc des:  ['0.8038', '0.8410']
his_acc des:  ['0.8038', '0.7973']
cur_acc rrf:  ['0.8017', '0.8387']
his_acc rrf:  ['0.8017', '0.7880']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 72.0561698
CurrentTrain: epoch  0, batch     1 | loss: 85.8282352
CurrentTrain: epoch  0, batch     2 | loss: 66.3924252
CurrentTrain: epoch  0, batch     3 | loss: 27.6500229
CurrentTrain: epoch  1, batch     0 | loss: 62.2608546
CurrentTrain: epoch  1, batch     1 | loss: 62.5338984
CurrentTrain: epoch  1, batch     2 | loss: 97.6705862
CurrentTrain: epoch  1, batch     3 | loss: 11.3762017
CurrentTrain: epoch  2, batch     0 | loss: 72.0525533
CurrentTrain: epoch  2, batch     1 | loss: 58.0042613
CurrentTrain: epoch  2, batch     2 | loss: 70.1453459
CurrentTrain: epoch  2, batch     3 | loss: 12.9762459
CurrentTrain: epoch  3, batch     0 | loss: 90.8050056
CurrentTrain: epoch  3, batch     1 | loss: 68.8677375
CurrentTrain: epoch  3, batch     2 | loss: 53.0467539
CurrentTrain: epoch  3, batch     3 | loss: 11.5804013
CurrentTrain: epoch  4, batch     0 | loss: 52.9256186
CurrentTrain: epoch  4, batch     1 | loss: 57.2056278
CurrentTrain: epoch  4, batch     2 | loss: 87.2446907
CurrentTrain: epoch  4, batch     3 | loss: 27.3954532
CurrentTrain: epoch  5, batch     0 | loss: 51.2582627
CurrentTrain: epoch  5, batch     1 | loss: 83.5382611
CurrentTrain: epoch  5, batch     2 | loss: 70.1222850
CurrentTrain: epoch  5, batch     3 | loss: 3.6780434
CurrentTrain: epoch  6, batch     0 | loss: 66.9622549
CurrentTrain: epoch  6, batch     1 | loss: 50.4011696
CurrentTrain: epoch  6, batch     2 | loss: 83.8766837
CurrentTrain: epoch  6, batch     3 | loss: 10.9843297
CurrentTrain: epoch  7, batch     0 | loss: 182.1303223
CurrentTrain: epoch  7, batch     1 | loss: 48.0769316
CurrentTrain: epoch  7, batch     2 | loss: 52.1100928
CurrentTrain: epoch  7, batch     3 | loss: 11.3350316
CurrentTrain: epoch  8, batch     0 | loss: 85.1122432
CurrentTrain: epoch  8, batch     1 | loss: 51.8407082
CurrentTrain: epoch  8, batch     2 | loss: 50.1961615
CurrentTrain: epoch  8, batch     3 | loss: 10.9555370
CurrentTrain: epoch  9, batch     0 | loss: 62.9273447
CurrentTrain: epoch  9, batch     1 | loss: 61.0636763
CurrentTrain: epoch  9, batch     2 | loss: 64.4427977
CurrentTrain: epoch  9, batch     3 | loss: 11.5401582
MemoryTrain:  epoch  0, batch     0 | loss: 0.5785226
MemoryTrain:  epoch  1, batch     0 | loss: 0.4187554
MemoryTrain:  epoch  2, batch     0 | loss: 0.3609572
MemoryTrain:  epoch  3, batch     0 | loss: 0.2724990
MemoryTrain:  epoch  4, batch     0 | loss: 0.2374797
MemoryTrain:  epoch  5, batch     0 | loss: 0.2080732
MemoryTrain:  epoch  6, batch     0 | loss: 0.1603138
MemoryTrain:  epoch  7, batch     0 | loss: 0.1368992
MemoryTrain:  epoch  8, batch     0 | loss: 0.1115677
MemoryTrain:  epoch  9, batch     0 | loss: 0.0878521

F1 score per class: {7: 0.3333333333333333, 40: 0.9387755102040817, 9: 0.0, 17: 0.0, 19: 0.0, 26: 0.6666666666666666, 27: 0.0, 31: 0.5773195876288659}
Micro-average F1 score: 0.5566037735849056
Weighted-average F1 score: 0.4489874636999761
F1 score per class: {5: 0.0, 7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 17: 0.0, 19: 0.0, 24: 0.0, 26: 0.6, 27: 0.8, 31: 0.7102803738317757}
Micro-average F1 score: 0.6545454545454545
Weighted-average F1 score: 0.5579629418334078
F1 score per class: {7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 17: 0.0, 19: 0.0, 24: 0.0, 26: 0.5714285714285714, 27: 1.0, 31: 0.7102803738317757}
Micro-average F1 score: 0.6575342465753424
Weighted-average F1 score: 0.563004765769236

F1 score per class: {32: 0.9591836734693877, 5: 0.21238938053097345, 6: 0.037037037037037035, 7: 0.9387755102040817, 40: 0.4375, 10: 0.7083333333333334, 9: 0.0, 16: 0.391304347826087, 17: 0.532608695652174, 18: 0.2, 19: 0.7630057803468208, 24: 0.5185185185185185, 26: 0.9010989010989011, 27: 0.0, 29: 0.7692307692307693, 31: 0.4628099173553719}
Micro-average F1 score: 0.625242718446602
Weighted-average F1 score: 0.6379354089892226
F1 score per class: {32: 0.9615384615384616, 5: 0.07692307692307693, 6: 0.024691358024691357, 7: 0.9803921568627451, 40: 0.5531914893617021, 10: 0.8148148148148148, 9: 0.0, 16: 0.7666666666666667, 17: 0.5795454545454546, 18: 0.17391304347826086, 19: 0.7640449438202247, 24: 0.42857142857142855, 26: 0.9473684210526315, 27: 0.4444444444444444, 29: 0.8783068783068783, 31: 0.5314685314685315}
Micro-average F1 score: 0.6614925373134328
Weighted-average F1 score: 0.6619983716854039
F1 score per class: {32: 0.975609756097561, 5: 0.14545454545454545, 6: 0.029850746268656716, 7: 0.9803921568627451, 40: 0.5531914893617021, 10: 0.7843137254901961, 9: 0.0, 16: 0.6037735849056604, 17: 0.5714285714285714, 18: 0.17391304347826086, 19: 0.768361581920904, 24: 0.4, 26: 0.9417989417989417, 27: 0.8, 29: 0.8333333333333334, 31: 0.5205479452054794}
Micro-average F1 score: 0.654126213592233
Weighted-average F1 score: 0.651683782145774
cur_acc:  ['0.7586', '0.7506', '0.5566']
his_acc:  ['0.7586', '0.7236', '0.6252']
cur_acc des:  ['0.8038', '0.8410', '0.6545']
his_acc des:  ['0.8038', '0.7973', '0.6615']
cur_acc rrf:  ['0.8017', '0.8387', '0.6575']
his_acc rrf:  ['0.8017', '0.7880', '0.6541']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 113.2505648
CurrentTrain: epoch  0, batch     1 | loss: 89.2887133
CurrentTrain: epoch  0, batch     2 | loss: 72.0647234
CurrentTrain: epoch  0, batch     3 | loss: 62.1176703
CurrentTrain: epoch  1, batch     0 | loss: 75.2834985
CurrentTrain: epoch  1, batch     1 | loss: 91.5100104
CurrentTrain: epoch  1, batch     2 | loss: 67.7818024
CurrentTrain: epoch  1, batch     3 | loss: 51.5838793
CurrentTrain: epoch  2, batch     0 | loss: 58.3979337
CurrentTrain: epoch  2, batch     1 | loss: 130.6960674
CurrentTrain: epoch  2, batch     2 | loss: 74.5761659
CurrentTrain: epoch  2, batch     3 | loss: 65.8695585
CurrentTrain: epoch  3, batch     0 | loss: 67.6101091
CurrentTrain: epoch  3, batch     1 | loss: 57.8803878
CurrentTrain: epoch  3, batch     2 | loss: 57.7482650
CurrentTrain: epoch  3, batch     3 | loss: 75.6236500
CurrentTrain: epoch  4, batch     0 | loss: 175.6478026
CurrentTrain: epoch  4, batch     1 | loss: 57.4987716
CurrentTrain: epoch  4, batch     2 | loss: 52.8827517
CurrentTrain: epoch  4, batch     3 | loss: 101.9533001
CurrentTrain: epoch  5, batch     0 | loss: 80.9846610
CurrentTrain: epoch  5, batch     1 | loss: 65.9896896
CurrentTrain: epoch  5, batch     2 | loss: 89.5721206
CurrentTrain: epoch  5, batch     3 | loss: 49.3576711
CurrentTrain: epoch  6, batch     0 | loss: 82.2909445
CurrentTrain: epoch  6, batch     1 | loss: 84.1765124
CurrentTrain: epoch  6, batch     2 | loss: 67.6696604
CurrentTrain: epoch  6, batch     3 | loss: 57.7244031
CurrentTrain: epoch  7, batch     0 | loss: 68.5336797
CurrentTrain: epoch  7, batch     1 | loss: 64.5712037
CurrentTrain: epoch  7, batch     2 | loss: 68.6726417
CurrentTrain: epoch  7, batch     3 | loss: 68.2932964
CurrentTrain: epoch  8, batch     0 | loss: 54.1919268
CurrentTrain: epoch  8, batch     1 | loss: 68.4056346
CurrentTrain: epoch  8, batch     2 | loss: 84.6692601
CurrentTrain: epoch  8, batch     3 | loss: 64.7273567
CurrentTrain: epoch  9, batch     0 | loss: 54.9690771
CurrentTrain: epoch  9, batch     1 | loss: 66.6775479
CurrentTrain: epoch  9, batch     2 | loss: 52.8239332
CurrentTrain: epoch  9, batch     3 | loss: 55.2874505
MemoryTrain:  epoch  0, batch     0 | loss: 0.5919489
MemoryTrain:  epoch  1, batch     0 | loss: 0.4331559
MemoryTrain:  epoch  2, batch     0 | loss: 0.3418119
MemoryTrain:  epoch  3, batch     0 | loss: 0.2497644
MemoryTrain:  epoch  4, batch     0 | loss: 0.1760951
MemoryTrain:  epoch  5, batch     0 | loss: 0.1512806
MemoryTrain:  epoch  6, batch     0 | loss: 0.1065130
MemoryTrain:  epoch  7, batch     0 | loss: 0.0913391
MemoryTrain:  epoch  8, batch     0 | loss: 0.0712876
MemoryTrain:  epoch  9, batch     0 | loss: 0.0722309

F1 score per class: {0: 0.9428571428571428, 4: 0.9361702127659575, 13: 0.5714285714285714, 21: 0.7083333333333334, 23: 0.8674698795180723, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8669950738916257
Weighted-average F1 score: 0.8489822790852923
F1 score per class: {0: 0.9428571428571428, 4: 0.98989898989899, 5: 0.0, 10: 0.0, 13: 0.5714285714285714, 18: 0.0, 21: 0.8076923076923077, 23: 0.8809523809523809, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8904428904428905
Weighted-average F1 score: 0.8556173226029685
F1 score per class: {0: 0.9577464788732394, 4: 0.9949748743718593, 5: 0.0, 10: 0.0, 13: 0.5714285714285714, 21: 0.8076923076923077, 23: 0.8536585365853658, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8946135831381733
Weighted-average F1 score: 0.8649678127023555

F1 score per class: {0: 0.9428571428571428, 4: 0.9361702127659575, 5: 0.9746192893401016, 6: 0.12962962962962962, 7: 0.03773584905660377, 9: 0.96, 10: 0.4888888888888889, 13: 0.1, 16: 0.8, 17: 0.08695652173913043, 18: 0.52, 19: 0.6601941747572816, 21: 0.576271186440678, 23: 0.8470588235294118, 24: 0.09090909090909091, 26: 0.7486033519553073, 27: 0.5161290322580645, 29: 0.9424083769633508, 31: 0.8, 32: 0.6174496644295302, 40: 0.4329896907216495}
Micro-average F1 score: 0.6780684104627767
Weighted-average F1 score: 0.693698985337223
F1 score per class: {0: 0.9295774647887324, 4: 0.98989898989899, 5: 0.9523809523809523, 6: 0.25862068965517243, 7: 0.09090909090909091, 9: 0.9803921568627451, 10: 0.5571428571428572, 13: 0.06557377049180328, 16: 0.896551724137931, 17: 0.0, 18: 0.8695652173913043, 19: 0.7019230769230769, 21: 0.6363636363636364, 23: 0.8809523809523809, 24: 0.08695652173913043, 26: 0.7351351351351352, 27: 0.6206896551724138, 29: 0.9479166666666666, 31: 0.5, 32: 0.8723404255319149, 40: 0.5042016806722689}
Micro-average F1 score: 0.7220946256316031
Weighted-average F1 score: 0.7098901310676775
F1 score per class: {0: 0.9444444444444444, 4: 0.9949748743718593, 5: 0.9611650485436893, 6: 0.25210084033613445, 7: 0.05128205128205128, 9: 0.9803921568627451, 10: 0.5594405594405595, 13: 0.06666666666666667, 16: 0.8301886792452831, 17: 0.11764705882352941, 18: 0.8307692307692308, 19: 0.6926829268292682, 21: 0.6086956521739131, 23: 0.8536585365853658, 24: 0.08695652173913043, 26: 0.7351351351351352, 27: 0.5161290322580645, 29: 0.9533678756476683, 31: 0.6666666666666666, 32: 0.7455621301775148, 40: 0.5217391304347826}
Micro-average F1 score: 0.7071461933675852
Weighted-average F1 score: 0.6937331944160953
cur_acc:  ['0.7586', '0.7506', '0.5566', '0.8670']
his_acc:  ['0.7586', '0.7236', '0.6252', '0.6781']
cur_acc des:  ['0.8038', '0.8410', '0.6545', '0.8904']
his_acc des:  ['0.8038', '0.7973', '0.6615', '0.7221']
cur_acc rrf:  ['0.8017', '0.8387', '0.6575', '0.8946']
his_acc rrf:  ['0.8017', '0.7880', '0.6541', '0.7071']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 80.3032058
CurrentTrain: epoch  0, batch     1 | loss: 78.4202728
CurrentTrain: epoch  0, batch     2 | loss: 63.8243813
CurrentTrain: epoch  0, batch     3 | loss: 76.6108047
CurrentTrain: epoch  1, batch     0 | loss: 61.3969084
CurrentTrain: epoch  1, batch     1 | loss: 91.8031306
CurrentTrain: epoch  1, batch     2 | loss: 89.4951019
CurrentTrain: epoch  1, batch     3 | loss: 114.2250364
CurrentTrain: epoch  2, batch     0 | loss: 57.3303856
CurrentTrain: epoch  2, batch     1 | loss: 89.2696514
CurrentTrain: epoch  2, batch     2 | loss: 70.4826475
CurrentTrain: epoch  2, batch     3 | loss: 41.3495102
CurrentTrain: epoch  3, batch     0 | loss: 54.4229518
CurrentTrain: epoch  3, batch     1 | loss: 69.8514986
CurrentTrain: epoch  3, batch     2 | loss: 56.1049734
CurrentTrain: epoch  3, batch     3 | loss: 54.1684751
CurrentTrain: epoch  4, batch     0 | loss: 86.6551843
CurrentTrain: epoch  4, batch     1 | loss: 66.0864443
CurrentTrain: epoch  4, batch     2 | loss: 53.3776995
CurrentTrain: epoch  4, batch     3 | loss: 40.2292087
CurrentTrain: epoch  5, batch     0 | loss: 87.3442981
CurrentTrain: epoch  5, batch     1 | loss: 63.2885574
CurrentTrain: epoch  5, batch     2 | loss: 65.2337897
CurrentTrain: epoch  5, batch     3 | loss: 49.9155995
CurrentTrain: epoch  6, batch     0 | loss: 63.5114268
CurrentTrain: epoch  6, batch     1 | loss: 63.0390465
CurrentTrain: epoch  6, batch     2 | loss: 118.6466846
CurrentTrain: epoch  6, batch     3 | loss: 39.1695280
CurrentTrain: epoch  7, batch     0 | loss: 63.0464625
CurrentTrain: epoch  7, batch     1 | loss: 51.2863576
CurrentTrain: epoch  7, batch     2 | loss: 82.8507434
CurrentTrain: epoch  7, batch     3 | loss: 51.8978683
CurrentTrain: epoch  8, batch     0 | loss: 81.5542846
CurrentTrain: epoch  8, batch     1 | loss: 63.9071705
CurrentTrain: epoch  8, batch     2 | loss: 64.1954277
CurrentTrain: epoch  8, batch     3 | loss: 48.9703021
CurrentTrain: epoch  9, batch     0 | loss: 64.9418112
CurrentTrain: epoch  9, batch     1 | loss: 80.2639668
CurrentTrain: epoch  9, batch     2 | loss: 79.8886158
CurrentTrain: epoch  9, batch     3 | loss: 49.8275594
MemoryTrain:  epoch  0, batch     0 | loss: 0.1818796
MemoryTrain:  epoch  1, batch     0 | loss: 0.1745195
MemoryTrain:  epoch  2, batch     0 | loss: 0.1378874
MemoryTrain:  epoch  3, batch     0 | loss: 0.0983033
MemoryTrain:  epoch  4, batch     0 | loss: 0.0872951
MemoryTrain:  epoch  5, batch     0 | loss: 0.0707547
MemoryTrain:  epoch  6, batch     0 | loss: 0.0797302
MemoryTrain:  epoch  7, batch     0 | loss: 0.0613162
MemoryTrain:  epoch  8, batch     0 | loss: 0.0508138
MemoryTrain:  epoch  9, batch     0 | loss: 0.0472234

F1 score per class: {33: 0.0, 36: 0.4423076923076923, 6: 0.0, 8: 0.0, 13: 0.8260869565217391, 18: 0.0, 20: 0.0, 26: 0.9444444444444444, 29: 0.4, 30: 0.1917808219178082}
Micro-average F1 score: 0.5382262996941896
Weighted-average F1 score: 0.6308583602071169
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.7910447761194029, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.0, 16: 0.9090909090909091, 18: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.972972972972973, 29: 0.5555555555555556, 30: 0.8376068376068376}
Micro-average F1 score: 0.7962529274004684
Weighted-average F1 score: 0.7497694651212776
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.8, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.92, 18: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.972972972972973, 29: 0.5555555555555556, 30: 0.8376068376068376}
Micro-average F1 score: 0.8056206088992974
Weighted-average F1 score: 0.763471179185465

F1 score per class: {0: 0.9428571428571428, 4: 0.9417989417989417, 5: 0.964824120603015, 6: 0.07547169811320754, 7: 0.05263157894736842, 8: 0.4107142857142857, 9: 0.96, 10: 0.1981981981981982, 13: 0.13333333333333333, 16: 0.7346938775510204, 17: 0.1, 18: 0.2978723404255319, 19: 0.6130653266331658, 20: 0.8260869565217391, 21: 0.4090909090909091, 23: 0.8571428571428571, 24: 0.09090909090909091, 26: 0.7362637362637363, 27: 0.5333333333333333, 29: 0.9319371727748691, 30: 0.9444444444444444, 31: 0.6666666666666666, 32: 0.6266666666666667, 33: 0.3333333333333333, 36: 0.1917808219178082, 40: 0.3956043956043956}
Micro-average F1 score: 0.6393516434038722
Weighted-average F1 score: 0.7129036221589881
F1 score per class: {0: 0.9577464788732394, 4: 0.9847715736040609, 5: 0.9259259259259259, 6: 0.2222222222222222, 7: 0.07017543859649122, 8: 0.5, 9: 0.9803921568627451, 10: 0.31666666666666665, 13: 0.15384615384615385, 16: 0.847457627118644, 17: 0.15384615384615385, 18: 0.5538461538461539, 19: 0.6699507389162561, 20: 0.9090909090909091, 21: 0.6571428571428571, 23: 0.8048780487804879, 24: 0.16666666666666666, 26: 0.7127659574468085, 27: 0.6428571428571429, 29: 0.9381443298969072, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.8691099476439791, 33: 0.4166666666666667, 36: 0.7596899224806202, 40: 0.5043478260869565}
Micro-average F1 score: 0.7067028283611003
Weighted-average F1 score: 0.7223482807654091
F1 score per class: {0: 0.9577464788732394, 4: 0.9847715736040609, 5: 0.9345794392523364, 6: 0.21052631578947367, 7: 0.037037037037037035, 8: 0.5046728971962616, 9: 0.9803921568627451, 10: 0.3442622950819672, 13: 0.13333333333333333, 16: 0.8518518518518519, 17: 0.26666666666666666, 18: 0.5161290322580645, 19: 0.6633663366336634, 20: 0.92, 21: 0.6176470588235294, 23: 0.8048780487804879, 24: 0.16666666666666666, 26: 0.7165775401069518, 27: 0.6206896551724138, 29: 0.9435897435897436, 30: 0.9473684210526315, 31: 0.6666666666666666, 32: 0.8241758241758241, 33: 0.4166666666666667, 36: 0.7538461538461538, 40: 0.5043478260869565}
Micro-average F1 score: 0.7025341130604289
Weighted-average F1 score: 0.7167613047082291
cur_acc:  ['0.7586', '0.7506', '0.5566', '0.8670', '0.5382']
his_acc:  ['0.7586', '0.7236', '0.6252', '0.6781', '0.6394']
cur_acc des:  ['0.8038', '0.8410', '0.6545', '0.8904', '0.7963']
his_acc des:  ['0.8038', '0.7973', '0.6615', '0.7221', '0.7067']
cur_acc rrf:  ['0.8017', '0.8387', '0.6575', '0.8946', '0.8056']
his_acc rrf:  ['0.8017', '0.7880', '0.6541', '0.7071', '0.7025']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 91.7260574
CurrentTrain: epoch  0, batch     1 | loss: 99.6253577
CurrentTrain: epoch  0, batch     2 | loss: 70.1223917
CurrentTrain: epoch  0, batch     3 | loss: 80.6616872
CurrentTrain: epoch  0, batch     4 | loss: 60.4432694
CurrentTrain: epoch  1, batch     0 | loss: 64.2994587
CurrentTrain: epoch  1, batch     1 | loss: 126.9943831
CurrentTrain: epoch  1, batch     2 | loss: 63.0233556
CurrentTrain: epoch  1, batch     3 | loss: 75.9112058
CurrentTrain: epoch  1, batch     4 | loss: 29.9890281
CurrentTrain: epoch  2, batch     0 | loss: 71.5962159
CurrentTrain: epoch  2, batch     1 | loss: 87.3579613
CurrentTrain: epoch  2, batch     2 | loss: 124.2204278
CurrentTrain: epoch  2, batch     3 | loss: 85.9556994
CurrentTrain: epoch  2, batch     4 | loss: 29.9582399
CurrentTrain: epoch  3, batch     0 | loss: 114.7213897
CurrentTrain: epoch  3, batch     1 | loss: 121.6009690
CurrentTrain: epoch  3, batch     2 | loss: 91.4103894
CurrentTrain: epoch  3, batch     3 | loss: 68.5485522
CurrentTrain: epoch  3, batch     4 | loss: 18.5431946
CurrentTrain: epoch  4, batch     0 | loss: 88.6630633
CurrentTrain: epoch  4, batch     1 | loss: 67.9810834
CurrentTrain: epoch  4, batch     2 | loss: 69.0670385
CurrentTrain: epoch  4, batch     3 | loss: 85.6269218
CurrentTrain: epoch  4, batch     4 | loss: 60.0733655
CurrentTrain: epoch  5, batch     0 | loss: 67.8938821
CurrentTrain: epoch  5, batch     1 | loss: 54.3114975
CurrentTrain: epoch  5, batch     2 | loss: 68.3066703
CurrentTrain: epoch  5, batch     3 | loss: 88.8437060
CurrentTrain: epoch  5, batch     4 | loss: 27.6492356
CurrentTrain: epoch  6, batch     0 | loss: 56.8875487
CurrentTrain: epoch  6, batch     1 | loss: 86.6805052
CurrentTrain: epoch  6, batch     2 | loss: 53.9152777
CurrentTrain: epoch  6, batch     3 | loss: 68.3422640
CurrentTrain: epoch  6, batch     4 | loss: 27.4540627
CurrentTrain: epoch  7, batch     0 | loss: 86.7739498
CurrentTrain: epoch  7, batch     1 | loss: 119.1003778
CurrentTrain: epoch  7, batch     2 | loss: 60.8042442
CurrentTrain: epoch  7, batch     3 | loss: 83.1831765
CurrentTrain: epoch  7, batch     4 | loss: 27.1213464
CurrentTrain: epoch  8, batch     0 | loss: 52.9620391
CurrentTrain: epoch  8, batch     1 | loss: 83.8060948
CurrentTrain: epoch  8, batch     2 | loss: 62.6470682
CurrentTrain: epoch  8, batch     3 | loss: 182.8107607
CurrentTrain: epoch  8, batch     4 | loss: 25.7746789
CurrentTrain: epoch  9, batch     0 | loss: 84.6955785
CurrentTrain: epoch  9, batch     1 | loss: 84.2880035
CurrentTrain: epoch  9, batch     2 | loss: 66.2609028
CurrentTrain: epoch  9, batch     3 | loss: 54.3685310
CurrentTrain: epoch  9, batch     4 | loss: 14.0499625
MemoryTrain:  epoch  0, batch     0 | loss: 0.2550358
MemoryTrain:  epoch  1, batch     0 | loss: 0.2373556
MemoryTrain:  epoch  2, batch     0 | loss: 0.2201017
MemoryTrain:  epoch  3, batch     0 | loss: 0.1858435
MemoryTrain:  epoch  4, batch     0 | loss: 0.1269458
MemoryTrain:  epoch  5, batch     0 | loss: 0.1163541
MemoryTrain:  epoch  6, batch     0 | loss: 0.0957226
MemoryTrain:  epoch  7, batch     0 | loss: 0.0860296
MemoryTrain:  epoch  8, batch     0 | loss: 0.0776449
MemoryTrain:  epoch  9, batch     0 | loss: 0.0738519

F1 score per class: {0: 0.0, 33: 0.875, 2: 0.0, 5: 0.5210084033613446, 39: 0.3333333333333333, 40: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.5714285714285714, 18: 0.0, 19: 0.13333333333333333, 28: 0.0}
Micro-average F1 score: 0.42
Weighted-average F1 score: 0.40129422139226056
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.5691056910569106, 12: 0.7215189873417721, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.9333333333333333, 33: 0.0, 39: 0.5263157894736842, 40: 0.0}
Micro-average F1 score: 0.5888594164456233
Weighted-average F1 score: 0.4877007135032685
F1 score per class: {0: 0.0, 33: 0.875, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.0, 8: 0.592, 10: 0.7215189873417721, 11: 0.0, 12: 0.0, 40: 0.0, 16: 0.0, 18: 0.0, 19: 0.8235294117647058, 21: 0.0, 26: 0.13333333333333333, 28: 0.0}
Micro-average F1 score: 0.5828877005347594
Weighted-average F1 score: 0.4994980600325063

F1 score per class: {0: 0.9295774647887324, 2: 0.6363636363636364, 4: 0.93048128342246, 5: 0.9595959595959596, 6: 0.07547169811320754, 7: 0.03508771929824561, 8: 0.15053763440860216, 9: 0.96, 10: 0.0196078431372549, 11: 0.4105960264900662, 12: 0.3225806451612903, 13: 0.2222222222222222, 16: 0.6923076923076923, 17: 0.0, 18: 0.04878048780487805, 19: 0.6130653266331658, 20: 0.6329113924050633, 21: 0.46511627906976744, 23: 0.896551724137931, 24: 0.09523809523809523, 26: 0.7386363636363636, 27: 0.5333333333333333, 28: 0.23529411764705882, 29: 0.9270833333333334, 30: 0.9142857142857143, 31: 0.6666666666666666, 32: 0.6875, 33: 0.375, 36: 0.029850746268656716, 39: 0.125, 40: 0.46464646464646464}
Micro-average F1 score: 0.5777426992896606
Weighted-average F1 score: 0.6693722436293275
F1 score per class: {0: 0.972972972972973, 2: 0.4666666666666667, 4: 0.8950276243093923, 5: 0.9389671361502347, 6: 0.2975206611570248, 7: 0.028985507246376812, 8: 0.48322147651006714, 9: 0.9803921568627451, 10: 0.31666666666666665, 11: 0.4166666666666667, 12: 0.6589595375722543, 13: 0.18181818181818182, 16: 0.8709677419354839, 17: 0.08695652173913043, 18: 0.38095238095238093, 19: 0.6889952153110048, 20: 0.851063829787234, 21: 0.6388888888888888, 23: 0.9230769230769231, 24: 0.24, 26: 0.7362637362637363, 27: 0.6153846153846154, 28: 0.2978723404255319, 29: 0.9263157894736842, 30: 0.972972972972973, 31: 1.0, 32: 0.8865979381443299, 33: 0.24, 36: 0.6274509803921569, 39: 0.37037037037037035, 40: 0.5344827586206896}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6695776872521003
F1 score per class: {0: 0.958904109589041, 2: 0.45161290322580644, 4: 0.9010989010989011, 5: 0.9389671361502347, 6: 0.23931623931623933, 7: 0.029850746268656716, 8: 0.45714285714285713, 9: 0.9803921568627451, 10: 0.32786885245901637, 11: 0.43023255813953487, 12: 0.6551724137931034, 13: 0.16666666666666666, 16: 0.8333333333333334, 17: 0.0, 18: 0.3448275862068966, 19: 0.6859903381642513, 20: 0.851063829787234, 21: 0.5714285714285714, 23: 0.9213483146067416, 24: 0.08695652173913043, 26: 0.7362637362637363, 27: 0.5517241379310345, 28: 0.2641509433962264, 29: 0.9263157894736842, 30: 0.972972972972973, 31: 1.0, 32: 0.8808290155440415, 33: 0.24, 36: 0.6060606060606061, 39: 0.09090909090909091, 40: 0.5254237288135594}
Micro-average F1 score: 0.6548310003414135
Weighted-average F1 score: 0.6603992448947537
cur_acc:  ['0.7586', '0.7506', '0.5566', '0.8670', '0.5382', '0.4200']
his_acc:  ['0.7586', '0.7236', '0.6252', '0.6781', '0.6394', '0.5777']
cur_acc des:  ['0.8038', '0.8410', '0.6545', '0.8904', '0.7963', '0.5889']
his_acc des:  ['0.8038', '0.7973', '0.6615', '0.7221', '0.7067', '0.6667']
cur_acc rrf:  ['0.8017', '0.8387', '0.6575', '0.8946', '0.8056', '0.5829']
his_acc rrf:  ['0.8017', '0.7880', '0.6541', '0.7071', '0.7025', '0.6548']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 83.3644020
CurrentTrain: epoch  0, batch     1 | loss: 98.2912228
CurrentTrain: epoch  0, batch     2 | loss: 130.3083121
CurrentTrain: epoch  0, batch     3 | loss: 99.1425775
CurrentTrain: epoch  0, batch     4 | loss: 67.9731236
CurrentTrain: epoch  1, batch     0 | loss: 93.0685505
CurrentTrain: epoch  1, batch     1 | loss: 93.2655020
CurrentTrain: epoch  1, batch     2 | loss: 73.1840114
CurrentTrain: epoch  1, batch     3 | loss: 60.5836110
CurrentTrain: epoch  1, batch     4 | loss: 107.0619679
CurrentTrain: epoch  2, batch     0 | loss: 74.0620524
CurrentTrain: epoch  2, batch     1 | loss: 115.8893969
CurrentTrain: epoch  2, batch     2 | loss: 88.5727847
CurrentTrain: epoch  2, batch     3 | loss: 58.3902148
CurrentTrain: epoch  2, batch     4 | loss: 44.6384806
CurrentTrain: epoch  3, batch     0 | loss: 71.1819762
CurrentTrain: epoch  3, batch     1 | loss: 70.1314749
CurrentTrain: epoch  3, batch     2 | loss: 69.5227783
CurrentTrain: epoch  3, batch     3 | loss: 68.8274138
CurrentTrain: epoch  3, batch     4 | loss: 104.5540497
CurrentTrain: epoch  4, batch     0 | loss: 69.1162477
CurrentTrain: epoch  4, batch     1 | loss: 70.5862366
CurrentTrain: epoch  4, batch     2 | loss: 54.8209616
CurrentTrain: epoch  4, batch     3 | loss: 86.4244912
CurrentTrain: epoch  4, batch     4 | loss: 67.1206269
CurrentTrain: epoch  5, batch     0 | loss: 119.2289761
CurrentTrain: epoch  5, batch     1 | loss: 65.6556758
CurrentTrain: epoch  5, batch     2 | loss: 85.7436762
CurrentTrain: epoch  5, batch     3 | loss: 67.4833436
CurrentTrain: epoch  5, batch     4 | loss: 66.0445042
CurrentTrain: epoch  6, batch     0 | loss: 85.2642139
CurrentTrain: epoch  6, batch     1 | loss: 66.8998405
CurrentTrain: epoch  6, batch     2 | loss: 54.7342932
CurrentTrain: epoch  6, batch     3 | loss: 86.4672221
CurrentTrain: epoch  6, batch     4 | loss: 47.5914078
CurrentTrain: epoch  7, batch     0 | loss: 177.9023892
CurrentTrain: epoch  7, batch     1 | loss: 55.2170352
CurrentTrain: epoch  7, batch     2 | loss: 63.3031699
CurrentTrain: epoch  7, batch     3 | loss: 85.2607425
CurrentTrain: epoch  7, batch     4 | loss: 46.5873395
CurrentTrain: epoch  8, batch     0 | loss: 66.8624680
CurrentTrain: epoch  8, batch     1 | loss: 49.4830580
CurrentTrain: epoch  8, batch     2 | loss: 114.4404906
CurrentTrain: epoch  8, batch     3 | loss: 115.8340711
CurrentTrain: epoch  8, batch     4 | loss: 102.9399691
CurrentTrain: epoch  9, batch     0 | loss: 84.9417632
CurrentTrain: epoch  9, batch     1 | loss: 111.2814162
CurrentTrain: epoch  9, batch     2 | loss: 63.0561145
CurrentTrain: epoch  9, batch     3 | loss: 115.8989927
CurrentTrain: epoch  9, batch     4 | loss: 44.8328299
MemoryTrain:  epoch  0, batch     0 | loss: 0.4474761
MemoryTrain:  epoch  1, batch     0 | loss: 0.3821504
MemoryTrain:  epoch  2, batch     0 | loss: 0.2994062
MemoryTrain:  epoch  3, batch     0 | loss: 0.2527384
MemoryTrain:  epoch  4, batch     0 | loss: 0.1809577
MemoryTrain:  epoch  5, batch     0 | loss: 0.1488979
MemoryTrain:  epoch  6, batch     0 | loss: 0.1312198
MemoryTrain:  epoch  7, batch     0 | loss: 0.1003539
MemoryTrain:  epoch  8, batch     0 | loss: 0.0945684
MemoryTrain:  epoch  9, batch     0 | loss: 0.0774611

F1 score per class: {32: 0.39669421487603307, 1: 0.7480916030534351, 34: 0.0, 3: 0.16901408450704225, 11: 0.0, 14: 0.6982248520710059, 18: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.22580645161290322}
Micro-average F1 score: 0.5
Weighted-average F1 score: 0.49992080088534174
F1 score per class: {32: 0.39603960396039606, 1: 0.9554140127388535, 34: 0.0, 3: 0.0, 36: 0.0, 5: 0.11363636363636363, 40: 0.0, 10: 0.0, 11: 0.6555555555555556, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.6075949367088608, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.5545454545454546
Weighted-average F1 score: 0.5180175923064958
F1 score per class: {32: 0.45714285714285713, 1: 0.9487179487179487, 34: 0.0, 3: 0.0, 33: 0.0, 36: 0.1686746987951807, 40: 0.0, 10: 0.0, 11: 0.6815642458100558, 12: 0.0, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.5135135135135135, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.5648854961832062
Weighted-average F1 score: 0.5295940491708091

F1 score per class: {0: 0.8823529411764706, 1: 0.35036496350364965, 2: 0.5454545454545454, 3: 0.7424242424242424, 4: 0.918918918918919, 5: 0.9595959595959596, 6: 0.07547169811320754, 7: 0.058823529411764705, 8: 0.34615384615384615, 9: 0.96, 10: 0.19642857142857142, 11: 0.17886178861788618, 12: 0.24347826086956523, 13: 0.1111111111111111, 14: 0.16216216216216217, 16: 0.7058823529411765, 17: 0.0, 18: 0.044444444444444446, 19: 0.125, 20: 0.6829268292682927, 21: 0.12121212121212122, 22: 0.659217877094972, 23: 0.898876404494382, 24: 0.07407407407407407, 26: 0.72, 27: 0.10526315789473684, 28: 0.20512820512820512, 29: 0.8972972972972973, 30: 0.9444444444444444, 31: 0.0, 32: 0.6496815286624203, 33: 0.3157894736842105, 34: 0.175, 36: 0.08695652173913043, 39: 0.125, 40: 0.225}
Micro-average F1 score: 0.5150612380006621
Weighted-average F1 score: 0.5946538153673755
F1 score per class: {0: 0.9295774647887324, 1: 0.3508771929824561, 2: 0.4375, 3: 0.8982035928143712, 4: 0.9010989010989011, 5: 0.9433962264150944, 6: 0.2833333333333333, 7: 0.06153846153846154, 8: 0.4785276073619632, 9: 0.9803921568627451, 10: 0.4, 11: 0.2814814814814815, 12: 0.6823529411764706, 13: 0.13333333333333333, 14: 0.10989010989010989, 16: 0.8813559322033898, 17: 0.0, 18: 0.3333333333333333, 19: 0.31496062992125984, 20: 0.8, 21: 0.3728813559322034, 22: 0.6082474226804123, 23: 0.8837209302325582, 24: 0.08333333333333333, 26: 0.7272727272727273, 27: 0.17142857142857143, 28: 0.23076923076923078, 29: 0.9206349206349206, 30: 1.0, 31: 0.6666666666666666, 32: 0.8556701030927835, 33: 0.23076923076923078, 34: 0.4, 36: 0.6728971962616822, 39: 0.2608695652173913, 40: 0.6187050359712231}
Micro-average F1 score: 0.6114864864864865
Weighted-average F1 score: 0.6182840007310927
F1 score per class: {0: 0.9295774647887324, 1: 0.40336134453781514, 2: 0.4375, 3: 0.8862275449101796, 4: 0.907103825136612, 5: 0.9569377990430622, 6: 0.17857142857142858, 7: 0.06153846153846154, 8: 0.46357615894039733, 9: 0.9803921568627451, 10: 0.3875968992248062, 11: 0.3018867924528302, 12: 0.6341463414634146, 13: 0.11764705882352941, 14: 0.16091954022988506, 16: 0.8813559322033898, 17: 0.0, 18: 0.22641509433962265, 19: 0.296875, 20: 0.8131868131868132, 21: 0.39285714285714285, 22: 0.6387434554973822, 23: 0.9090909090909091, 24: 0.08333333333333333, 26: 0.7351351351351352, 27: 0.17142857142857143, 28: 0.21052631578947367, 29: 0.9032258064516129, 30: 0.9743589743589743, 31: 0.6666666666666666, 32: 0.7912087912087912, 33: 0.2222222222222222, 34: 0.37254901960784315, 36: 0.6601941747572816, 39: 0.10526315789473684, 40: 0.5967741935483871}
Micro-average F1 score: 0.6017191977077364
Weighted-average F1 score: 0.6120838726886324
cur_acc:  ['0.7586', '0.7506', '0.5566', '0.8670', '0.5382', '0.4200', '0.5000']
his_acc:  ['0.7586', '0.7236', '0.6252', '0.6781', '0.6394', '0.5777', '0.5151']
cur_acc des:  ['0.8038', '0.8410', '0.6545', '0.8904', '0.7963', '0.5889', '0.5545']
his_acc des:  ['0.8038', '0.7973', '0.6615', '0.7221', '0.7067', '0.6667', '0.6115']
cur_acc rrf:  ['0.8017', '0.8387', '0.6575', '0.8946', '0.8056', '0.5829', '0.5649']
his_acc rrf:  ['0.8017', '0.7880', '0.6541', '0.7071', '0.7025', '0.6548', '0.6017']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 67.9395203
CurrentTrain: epoch  0, batch     1 | loss: 63.2465207
CurrentTrain: epoch  0, batch     2 | loss: 78.7499004
CurrentTrain: epoch  0, batch     3 | loss: 55.2374504
CurrentTrain: epoch  1, batch     0 | loss: 61.8486405
CurrentTrain: epoch  1, batch     1 | loss: 72.5851244
CurrentTrain: epoch  1, batch     2 | loss: 90.3962628
CurrentTrain: epoch  1, batch     3 | loss: 47.5323637
CurrentTrain: epoch  2, batch     0 | loss: 93.5746695
CurrentTrain: epoch  2, batch     1 | loss: 55.7550298
CurrentTrain: epoch  2, batch     2 | loss: 65.6134122
CurrentTrain: epoch  2, batch     3 | loss: 79.0292602
CurrentTrain: epoch  3, batch     0 | loss: 65.5607540
CurrentTrain: epoch  3, batch     1 | loss: 69.4225942
CurrentTrain: epoch  3, batch     2 | loss: 68.6023814
CurrentTrain: epoch  3, batch     3 | loss: 58.4635169
CurrentTrain: epoch  4, batch     0 | loss: 88.1680870
CurrentTrain: epoch  4, batch     1 | loss: 55.4246407
CurrentTrain: epoch  4, batch     2 | loss: 48.8844778
CurrentTrain: epoch  4, batch     3 | loss: 60.7708038
CurrentTrain: epoch  5, batch     0 | loss: 61.7011369
CurrentTrain: epoch  5, batch     1 | loss: 65.8112850
CurrentTrain: epoch  5, batch     2 | loss: 65.6113071
CurrentTrain: epoch  5, batch     3 | loss: 82.7807025
CurrentTrain: epoch  6, batch     0 | loss: 65.2051621
CurrentTrain: epoch  6, batch     1 | loss: 63.6858545
CurrentTrain: epoch  6, batch     2 | loss: 52.0427780
CurrentTrain: epoch  6, batch     3 | loss: 122.4867570
CurrentTrain: epoch  7, batch     0 | loss: 53.9741659
CurrentTrain: epoch  7, batch     1 | loss: 64.6328465
CurrentTrain: epoch  7, batch     2 | loss: 63.9176921
CurrentTrain: epoch  7, batch     3 | loss: 57.9141659
CurrentTrain: epoch  8, batch     0 | loss: 59.8112461
CurrentTrain: epoch  8, batch     1 | loss: 117.7198251
CurrentTrain: epoch  8, batch     2 | loss: 64.7832082
CurrentTrain: epoch  8, batch     3 | loss: 55.5941832
CurrentTrain: epoch  9, batch     0 | loss: 62.8020658
CurrentTrain: epoch  9, batch     1 | loss: 115.7767755
CurrentTrain: epoch  9, batch     2 | loss: 51.7414691
CurrentTrain: epoch  9, batch     3 | loss: 55.2324880
MemoryTrain:  epoch  0, batch     0 | loss: 0.3619854
MemoryTrain:  epoch  1, batch     0 | loss: 0.3596430
MemoryTrain:  epoch  2, batch     0 | loss: 0.2439298
MemoryTrain:  epoch  3, batch     0 | loss: 0.1854319
MemoryTrain:  epoch  4, batch     0 | loss: 0.1496938
MemoryTrain:  epoch  5, batch     0 | loss: 0.1280121
MemoryTrain:  epoch  6, batch     0 | loss: 0.1068093
MemoryTrain:  epoch  7, batch     0 | loss: 0.0964168
MemoryTrain:  epoch  8, batch     0 | loss: 0.0821433
MemoryTrain:  epoch  9, batch     0 | loss: 0.0712744

F1 score per class: {1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 33: 0.8235294117647058, 36: 0.0, 38: 0.0, 11: 0.0, 8: 0.5753424657534246, 13: 0.0, 15: 0.0, 18: 0.6753246753246753, 20: 0.0, 23: 0.5365853658536586, 25: 0.6}
Micro-average F1 score: 0.5238095238095238
Weighted-average F1 score: 0.40303864644738446
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 11: 0.0, 13: 0.0, 15: 0.75, 18: 0.0, 20: 0.0, 22: 0.0, 23: 0.0, 25: 0.5555555555555556, 33: 0.0, 34: 0.0, 35: 0.9375, 36: 0.0, 37: 0.651685393258427, 38: 0.782608695652174, 39: 0.0}
Micro-average F1 score: 0.6082474226804123
Weighted-average F1 score: 0.48547831036013167
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 11: 0.0, 13: 0.0, 14: 0.0, 15: 0.75, 18: 0.0, 20: 0.0, 23: 0.0, 25: 0.5753424657534246, 33: 0.0, 34: 0.0, 35: 0.9375, 36: 0.0, 37: 0.6666666666666666, 38: 0.8085106382978723, 39: 0.0}
Micro-average F1 score: 0.6285714285714286
Weighted-average F1 score: 0.511583662546095

F1 score per class: {0: 0.9142857142857143, 1: 0.3698630136986301, 2: 0.5384615384615384, 3: 0.8590604026845637, 4: 0.8505747126436781, 5: 0.8687782805429864, 6: 0.019417475728155338, 7: 0.0392156862745098, 8: 0.17582417582417584, 9: 0.96, 10: 0.09345794392523364, 11: 0.192, 12: 0.2711864406779661, 13: 0.125, 14: 0.11267605633802817, 15: 0.7368421052631579, 16: 0.8, 17: 0.0, 18: 0.2, 19: 0.43312101910828027, 20: 0.7058823529411765, 21: 0.17647058823529413, 22: 0.7252747252747253, 23: 0.8666666666666667, 24: 0.08695652173913043, 25: 0.5753424657534246, 26: 0.7167630057803468, 27: 0.11764705882352941, 28: 0.34782608695652173, 29: 0.9263157894736842, 30: 0.9444444444444444, 31: 0.6666666666666666, 32: 0.4782608695652174, 33: 0.2727272727272727, 34: 0.2127659574468085, 35: 0.611764705882353, 36: 0.11267605633802817, 37: 0.3308270676691729, 38: 0.3, 39: 0.13333333333333333, 40: 0.43010752688172044}
Micro-average F1 score: 0.5204375359815774
Weighted-average F1 score: 0.5839670171619272
F1 score per class: {0: 0.9295774647887324, 1: 0.38181818181818183, 2: 0.3888888888888889, 3: 0.9101796407185628, 4: 0.8700564971751412, 5: 0.8368200836820083, 6: 0.2564102564102564, 7: 0.03508771929824561, 8: 0.5238095238095238, 9: 0.9803921568627451, 10: 0.3225806451612903, 11: 0.323943661971831, 12: 0.6467065868263473, 13: 0.15384615384615385, 14: 0.10256410256410256, 15: 0.6666666666666666, 16: 0.847457627118644, 17: 0.09523809523809523, 18: 0.3870967741935484, 19: 0.5263157894736842, 20: 0.6904761904761905, 21: 0.30434782608695654, 22: 0.7330316742081447, 23: 0.9032258064516129, 24: 0.09090909090909091, 25: 0.5405405405405406, 26: 0.7444444444444445, 27: 0.2, 28: 0.34146341463414637, 29: 0.9263157894736842, 30: 1.0, 31: 0.6666666666666666, 32: 0.7218934911242604, 33: 0.3225806451612903, 34: 0.1523809523809524, 35: 0.743801652892562, 36: 0.42696629213483145, 37: 0.30526315789473685, 38: 0.3829787234042553, 39: 0.18181818181818182, 40: 0.6797385620915033}
Micro-average F1 score: 0.5898389095415117
Weighted-average F1 score: 0.5963315803726492
F1 score per class: {0: 0.9295774647887324, 1: 0.42735042735042733, 2: 0.4, 3: 0.8944099378881988, 4: 0.8700564971751412, 5: 0.851063829787234, 6: 0.22608695652173913, 7: 0.03389830508474576, 8: 0.4666666666666667, 9: 0.9803921568627451, 10: 0.2542372881355932, 11: 0.3401360544217687, 12: 0.6341463414634146, 13: 0.13333333333333333, 14: 0.18823529411764706, 15: 0.6666666666666666, 16: 0.847457627118644, 17: 0.0, 18: 0.36363636363636365, 19: 0.535031847133758, 20: 0.6904761904761905, 21: 0.3404255319148936, 22: 0.7572815533980582, 23: 0.9032258064516129, 24: 0.09090909090909091, 25: 0.56, 26: 0.7444444444444445, 27: 0.2, 28: 0.2857142857142857, 29: 0.9263157894736842, 30: 1.0, 31: 0.6666666666666666, 32: 0.6909090909090909, 33: 0.3448275862068966, 34: 0.1834862385321101, 35: 0.7142857142857143, 36: 0.34146341463414637, 37: 0.30612244897959184, 38: 0.3619047619047619, 39: 0.1, 40: 0.6762589928057554}
Micro-average F1 score: 0.5814824057898678
Weighted-average F1 score: 0.5860091007639611
cur_acc:  ['0.7586', '0.7506', '0.5566', '0.8670', '0.5382', '0.4200', '0.5000', '0.5238']
his_acc:  ['0.7586', '0.7236', '0.6252', '0.6781', '0.6394', '0.5777', '0.5151', '0.5204']
cur_acc des:  ['0.8038', '0.8410', '0.6545', '0.8904', '0.7963', '0.5889', '0.5545', '0.6082']
his_acc des:  ['0.8038', '0.7973', '0.6615', '0.7221', '0.7067', '0.6667', '0.6115', '0.5898']
cur_acc rrf:  ['0.8017', '0.8387', '0.6575', '0.8946', '0.8056', '0.5829', '0.5649', '0.6286']
his_acc rrf:  ['0.8017', '0.7880', '0.6541', '0.7071', '0.7025', '0.6548', '0.6017', '0.5815']
----------END
his_acc mean:  [0.8037 0.7268 0.6502 0.6286 0.5756 0.5675 0.5258 0.5126]
his_acc des mean:  [0.8323 0.7894 0.7213 0.7092 0.6427 0.6374 0.5896 0.5664]
his_acc rrf mean:  [0.8315 0.7785 0.7123 0.6906 0.6387 0.6276 0.5747 0.5713]
