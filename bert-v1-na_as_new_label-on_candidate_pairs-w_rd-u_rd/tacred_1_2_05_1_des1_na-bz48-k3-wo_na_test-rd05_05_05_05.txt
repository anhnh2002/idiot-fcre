#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 260.6909352CurrentTrain: epoch  0, batch     1 | loss: 196.6768015CurrentTrain: epoch  0, batch     2 | loss: 187.2307392CurrentTrain: epoch  0, batch     3 | loss: 194.4124209CurrentTrain: epoch  0, batch     4 | loss: 200.5584525CurrentTrain: epoch  0, batch     5 | loss: 187.4505819CurrentTrain: epoch  0, batch     6 | loss: 227.8345084CurrentTrain: epoch  0, batch     7 | loss: 226.5831597CurrentTrain: epoch  0, batch     8 | loss: 187.0117034CurrentTrain: epoch  0, batch     9 | loss: 187.8540207CurrentTrain: epoch  0, batch    10 | loss: 145.6959548CurrentTrain: epoch  0, batch    11 | loss: 225.8835254CurrentTrain: epoch  0, batch    12 | loss: 206.3044904CurrentTrain: epoch  0, batch    13 | loss: 360.1189282CurrentTrain: epoch  0, batch    14 | loss: 217.5818261CurrentTrain: epoch  0, batch    15 | loss: 211.6422684CurrentTrain: epoch  0, batch    16 | loss: 191.8524395CurrentTrain: epoch  0, batch    17 | loss: 256.5509305CurrentTrain: epoch  0, batch    18 | loss: 210.6085312CurrentTrain: epoch  0, batch    19 | loss: 197.5107602CurrentTrain: epoch  0, batch    20 | loss: 296.9052790CurrentTrain: epoch  0, batch    21 | loss: 288.6609915CurrentTrain: epoch  0, batch    22 | loss: 365.2424527CurrentTrain: epoch  0, batch    23 | loss: 280.7785497CurrentTrain: epoch  0, batch    24 | loss: 210.4111986CurrentTrain: epoch  0, batch    25 | loss: 358.5172762CurrentTrain: epoch  0, batch    26 | loss: 191.6368258CurrentTrain: epoch  0, batch    27 | loss: 251.1078989CurrentTrain: epoch  0, batch    28 | loss: 281.1798731CurrentTrain: epoch  0, batch    29 | loss: 193.4756981CurrentTrain: epoch  0, batch    30 | loss: 294.7795018CurrentTrain: epoch  0, batch    31 | loss: 280.9148761CurrentTrain: epoch  0, batch    32 | loss: 248.5855752CurrentTrain: epoch  0, batch    33 | loss: 190.6512258CurrentTrain: epoch  0, batch    34 | loss: 287.8101888CurrentTrain: epoch  0, batch    35 | loss: 211.2344294CurrentTrain: epoch  0, batch    36 | loss: 204.4723047CurrentTrain: epoch  0, batch    37 | loss: 216.6064448CurrentTrain: epoch  0, batch    38 | loss: 209.1264841CurrentTrain: epoch  0, batch    39 | loss: 196.5306107CurrentTrain: epoch  0, batch    40 | loss: 256.1530427CurrentTrain: epoch  0, batch    41 | loss: 203.3515855CurrentTrain: epoch  0, batch    42 | loss: 196.6046614CurrentTrain: epoch  0, batch    43 | loss: 158.0465422CurrentTrain: epoch  0, batch    44 | loss: 216.2680118CurrentTrain: epoch  0, batch    45 | loss: 172.0531608CurrentTrain: epoch  0, batch    46 | loss: 240.8073205CurrentTrain: epoch  0, batch    47 | loss: 190.6856152CurrentTrain: epoch  0, batch    48 | loss: 230.3075874CurrentTrain: epoch  0, batch    49 | loss: 209.3171023CurrentTrain: epoch  0, batch    50 | loss: 229.0733010CurrentTrain: epoch  0, batch    51 | loss: 209.5807682CurrentTrain: epoch  0, batch    52 | loss: 229.7694887CurrentTrain: epoch  0, batch    53 | loss: 288.2295085CurrentTrain: epoch  0, batch    54 | loss: 256.3405033CurrentTrain: epoch  0, batch    55 | loss: 172.7572998CurrentTrain: epoch  0, batch    56 | loss: 196.4747599CurrentTrain: epoch  0, batch    57 | loss: 191.0408747CurrentTrain: epoch  0, batch    58 | loss: 189.5472120CurrentTrain: epoch  0, batch    59 | loss: 214.3753451CurrentTrain: epoch  0, batch    60 | loss: 190.5651553CurrentTrain: epoch  0, batch    61 | loss: 214.9467202CurrentTrain: epoch  0, batch    62 | loss: 195.2061788CurrentTrain: epoch  0, batch    63 | loss: 228.8943904CurrentTrain: epoch  0, batch    64 | loss: 209.4420192CurrentTrain: epoch  0, batch    65 | loss: 167.5474329CurrentTrain: epoch  0, batch    66 | loss: 200.8637224CurrentTrain: epoch  0, batch    67 | loss: 215.2063968CurrentTrain: epoch  0, batch    68 | loss: 196.1107562CurrentTrain: epoch  0, batch    69 | loss: 294.5017722CurrentTrain: epoch  0, batch    70 | loss: 256.6530342CurrentTrain: epoch  0, batch    71 | loss: 189.3183238CurrentTrain: epoch  0, batch    72 | loss: 222.3762804CurrentTrain: epoch  0, batch    73 | loss: 247.4620981CurrentTrain: epoch  0, batch    74 | loss: 280.2837111CurrentTrain: epoch  0, batch    75 | loss: 230.0807969CurrentTrain: epoch  0, batch    76 | loss: 237.7161109CurrentTrain: epoch  0, batch    77 | loss: 212.8644637CurrentTrain: epoch  0, batch    78 | loss: 245.8796184CurrentTrain: epoch  0, batch    79 | loss: 239.3837448CurrentTrain: epoch  0, batch    80 | loss: 193.2118111CurrentTrain: epoch  0, batch    81 | loss: 221.5629015CurrentTrain: epoch  0, batch    82 | loss: 246.6793842CurrentTrain: epoch  0, batch    83 | loss: 200.8134809CurrentTrain: epoch  0, batch    84 | loss: 229.8058558CurrentTrain: epoch  0, batch    85 | loss: 170.8798130CurrentTrain: epoch  0, batch    86 | loss: 261.9666988CurrentTrain: epoch  0, batch    87 | loss: 199.8832431CurrentTrain: epoch  0, batch    88 | loss: 202.3636313CurrentTrain: epoch  0, batch    89 | loss: 160.1023579CurrentTrain: epoch  0, batch    90 | loss: 220.7077668CurrentTrain: epoch  0, batch    91 | loss: 175.0148328CurrentTrain: epoch  0, batch    92 | loss: 193.3904822CurrentTrain: epoch  0, batch    93 | loss: 260.8207921CurrentTrain: epoch  0, batch    94 | loss: 193.6739982CurrentTrain: epoch  0, batch    95 | loss: 205.4110987CurrentTrain: epoch  1, batch     0 | loss: 193.1381499CurrentTrain: epoch  1, batch     1 | loss: 220.7646926CurrentTrain: epoch  1, batch     2 | loss: 200.3505665CurrentTrain: epoch  1, batch     3 | loss: 192.7205134CurrentTrain: epoch  1, batch     4 | loss: 196.3188698CurrentTrain: epoch  1, batch     5 | loss: 170.1138244CurrentTrain: epoch  1, batch     6 | loss: 220.6963319CurrentTrain: epoch  1, batch     7 | loss: 275.1127286CurrentTrain: epoch  1, batch     8 | loss: 189.5884089CurrentTrain: epoch  1, batch     9 | loss: 252.2689425CurrentTrain: epoch  1, batch    10 | loss: 196.2558757CurrentTrain: epoch  1, batch    11 | loss: 183.8615283CurrentTrain: epoch  1, batch    12 | loss: 169.5683294CurrentTrain: epoch  1, batch    13 | loss: 238.4962474CurrentTrain: epoch  1, batch    14 | loss: 213.5737922CurrentTrain: epoch  1, batch    15 | loss: 218.5402072CurrentTrain: epoch  1, batch    16 | loss: 281.6558268CurrentTrain: epoch  1, batch    17 | loss: 170.5862963CurrentTrain: epoch  1, batch    18 | loss: 185.9235262CurrentTrain: epoch  1, batch    19 | loss: 246.0211593CurrentTrain: epoch  1, batch    20 | loss: 291.8720923CurrentTrain: epoch  1, batch    21 | loss: 189.9781185CurrentTrain: epoch  1, batch    22 | loss: 189.1951806CurrentTrain: epoch  1, batch    23 | loss: 210.0813916CurrentTrain: epoch  1, batch    24 | loss: 276.4574377CurrentTrain: epoch  1, batch    25 | loss: 211.4946863CurrentTrain: epoch  1, batch    26 | loss: 168.0406425CurrentTrain: epoch  1, batch    27 | loss: 189.8126018CurrentTrain: epoch  1, batch    28 | loss: 162.5438307CurrentTrain: epoch  1, batch    29 | loss: 190.7717043CurrentTrain: epoch  1, batch    30 | loss: 223.2498333CurrentTrain: epoch  1, batch    31 | loss: 197.2695820CurrentTrain: epoch  1, batch    32 | loss: 251.3839308CurrentTrain: epoch  1, batch    33 | loss: 217.5013324CurrentTrain: epoch  1, batch    34 | loss: 200.8674215CurrentTrain: epoch  1, batch    35 | loss: 291.6160876CurrentTrain: epoch  1, batch    36 | loss: 204.9990886CurrentTrain: epoch  1, batch    37 | loss: 208.8339037CurrentTrain: epoch  1, batch    38 | loss: 201.8809473CurrentTrain: epoch  1, batch    39 | loss: 175.3504177CurrentTrain: epoch  1, batch    40 | loss: 157.3641400CurrentTrain: epoch  1, batch    41 | loss: 176.2615786CurrentTrain: epoch  1, batch    42 | loss: 204.1387049CurrentTrain: epoch  1, batch    43 | loss: 216.3386700CurrentTrain: epoch  1, batch    44 | loss: 279.3055383CurrentTrain: epoch  1, batch    45 | loss: 210.2635412CurrentTrain: epoch  1, batch    46 | loss: 251.4288556CurrentTrain: epoch  1, batch    47 | loss: 174.5731379CurrentTrain: epoch  1, batch    48 | loss: 215.6321524CurrentTrain: epoch  1, batch    49 | loss: 207.6471343CurrentTrain: epoch  1, batch    50 | loss: 202.8185026CurrentTrain: epoch  1, batch    51 | loss: 253.5390221CurrentTrain: epoch  1, batch    52 | loss: 246.4381456CurrentTrain: epoch  1, batch    53 | loss: 274.4353127CurrentTrain: epoch  1, batch    54 | loss: 198.2863226CurrentTrain: epoch  1, batch    55 | loss: 215.9399627CurrentTrain: epoch  1, batch    56 | loss: 200.2397604CurrentTrain: epoch  1, batch    57 | loss: 240.7462243CurrentTrain: epoch  1, batch    58 | loss: 210.2723851CurrentTrain: epoch  1, batch    59 | loss: 169.1151386CurrentTrain: epoch  1, batch    60 | loss: 195.4920529CurrentTrain: epoch  1, batch    61 | loss: 287.9226709CurrentTrain: epoch  1, batch    62 | loss: 290.8036159CurrentTrain: epoch  1, batch    63 | loss: 184.2303811CurrentTrain: epoch  1, batch    64 | loss: 194.2370807CurrentTrain: epoch  1, batch    65 | loss: 240.5713792CurrentTrain: epoch  1, batch    66 | loss: 173.6963936CurrentTrain: epoch  1, batch    67 | loss: 280.6841975CurrentTrain: epoch  1, batch    68 | loss: 167.1742268CurrentTrain: epoch  1, batch    69 | loss: 200.5519523CurrentTrain: epoch  1, batch    70 | loss: 167.5999944CurrentTrain: epoch  1, batch    71 | loss: 208.3295666CurrentTrain: epoch  1, batch    72 | loss: 241.8051830CurrentTrain: epoch  1, batch    73 | loss: 211.0169687CurrentTrain: epoch  1, batch    74 | loss: 226.3012322CurrentTrain: epoch  1, batch    75 | loss: 199.6194321CurrentTrain: epoch  1, batch    76 | loss: 223.5507810CurrentTrain: epoch  1, batch    77 | loss: 220.9563173CurrentTrain: epoch  1, batch    78 | loss: 195.9208661CurrentTrain: epoch  1, batch    79 | loss: 192.8483040CurrentTrain: epoch  1, batch    80 | loss: 159.4558601CurrentTrain: epoch  1, batch    81 | loss: 219.6975071CurrentTrain: epoch  1, batch    82 | loss: 161.5892713CurrentTrain: epoch  1, batch    83 | loss: 364.2720629CurrentTrain: epoch  1, batch    84 | loss: 227.9099700CurrentTrain: epoch  1, batch    85 | loss: 187.5449248CurrentTrain: epoch  1, batch    86 | loss: 164.6188949CurrentTrain: epoch  1, batch    87 | loss: 194.6698151CurrentTrain: epoch  1, batch    88 | loss: 242.9836537CurrentTrain: epoch  1, batch    89 | loss: 226.4629141CurrentTrain: epoch  1, batch    90 | loss: 168.4852295CurrentTrain: epoch  1, batch    91 | loss: 222.8507562CurrentTrain: epoch  1, batch    92 | loss: 200.9423296CurrentTrain: epoch  1, batch    93 | loss: 290.7502323CurrentTrain: epoch  1, batch    94 | loss: 202.5884630CurrentTrain: epoch  1, batch    95 | loss: 154.0301525CurrentTrain: epoch  2, batch     0 | loss: 171.4826982CurrentTrain: epoch  2, batch     1 | loss: 244.3617221CurrentTrain: epoch  2, batch     2 | loss: 206.9295275CurrentTrain: epoch  2, batch     3 | loss: 215.1824071CurrentTrain: epoch  2, batch     4 | loss: 270.8347263CurrentTrain: epoch  2, batch     5 | loss: 172.5475652CurrentTrain: epoch  2, batch     6 | loss: 274.8212443CurrentTrain: epoch  2, batch     7 | loss: 185.3541615CurrentTrain: epoch  2, batch     8 | loss: 225.0770746CurrentTrain: epoch  2, batch     9 | loss: 158.3359746CurrentTrain: epoch  2, batch    10 | loss: 224.2990215CurrentTrain: epoch  2, batch    11 | loss: 151.4744783CurrentTrain: epoch  2, batch    12 | loss: 158.2445941CurrentTrain: epoch  2, batch    13 | loss: 241.9694403CurrentTrain: epoch  2, batch    14 | loss: 240.4500039CurrentTrain: epoch  2, batch    15 | loss: 242.5750973CurrentTrain: epoch  2, batch    16 | loss: 205.0284410CurrentTrain: epoch  2, batch    17 | loss: 212.3111916CurrentTrain: epoch  2, batch    18 | loss: 191.9709279CurrentTrain: epoch  2, batch    19 | loss: 226.1063810CurrentTrain: epoch  2, batch    20 | loss: 173.9094498CurrentTrain: epoch  2, batch    21 | loss: 223.0774270CurrentTrain: epoch  2, batch    22 | loss: 186.5142747CurrentTrain: epoch  2, batch    23 | loss: 205.7441264CurrentTrain: epoch  2, batch    24 | loss: 194.7662440CurrentTrain: epoch  2, batch    25 | loss: 182.2934515CurrentTrain: epoch  2, batch    26 | loss: 208.1513715CurrentTrain: epoch  2, batch    27 | loss: 185.0716095CurrentTrain: epoch  2, batch    28 | loss: 240.1972378CurrentTrain: epoch  2, batch    29 | loss: 234.4477802CurrentTrain: epoch  2, batch    30 | loss: 215.4805583CurrentTrain: epoch  2, batch    31 | loss: 196.1792183CurrentTrain: epoch  2, batch    32 | loss: 202.5818697CurrentTrain: epoch  2, batch    33 | loss: 257.7287130CurrentTrain: epoch  2, batch    34 | loss: 247.8715591CurrentTrain: epoch  2, batch    35 | loss: 226.7487914CurrentTrain: epoch  2, batch    36 | loss: 207.5614091CurrentTrain: epoch  2, batch    37 | loss: 221.3168468CurrentTrain: epoch  2, batch    38 | loss: 187.4958698CurrentTrain: epoch  2, batch    39 | loss: 240.1067485CurrentTrain: epoch  2, batch    40 | loss: 156.4988394CurrentTrain: epoch  2, batch    41 | loss: 248.2980020CurrentTrain: epoch  2, batch    42 | loss: 224.9438714CurrentTrain: epoch  2, batch    43 | loss: 181.5195671CurrentTrain: epoch  2, batch    44 | loss: 198.8128139CurrentTrain: epoch  2, batch    45 | loss: 189.2447782CurrentTrain: epoch  2, batch    46 | loss: 222.8487347CurrentTrain: epoch  2, batch    47 | loss: 181.9181779CurrentTrain: epoch  2, batch    48 | loss: 287.3352716CurrentTrain: epoch  2, batch    49 | loss: 282.2274207CurrentTrain: epoch  2, batch    50 | loss: 166.6564807CurrentTrain: epoch  2, batch    51 | loss: 291.3566348CurrentTrain: epoch  2, batch    52 | loss: 194.5175640CurrentTrain: epoch  2, batch    53 | loss: 215.0669768CurrentTrain: epoch  2, batch    54 | loss: 239.2360912CurrentTrain: epoch  2, batch    55 | loss: 213.2815390CurrentTrain: epoch  2, batch    56 | loss: 189.2300759CurrentTrain: epoch  2, batch    57 | loss: 166.0508189CurrentTrain: epoch  2, batch    58 | loss: 195.2211451CurrentTrain: epoch  2, batch    59 | loss: 182.8241967CurrentTrain: epoch  2, batch    60 | loss: 192.9753098CurrentTrain: epoch  2, batch    61 | loss: 169.4000941CurrentTrain: epoch  2, batch    62 | loss: 212.5417250CurrentTrain: epoch  2, batch    63 | loss: 172.7028083CurrentTrain: epoch  2, batch    64 | loss: 186.0914508CurrentTrain: epoch  2, batch    65 | loss: 197.7514986CurrentTrain: epoch  2, batch    66 | loss: 288.3364373CurrentTrain: epoch  2, batch    67 | loss: 236.2587812CurrentTrain: epoch  2, batch    68 | loss: 187.3092694CurrentTrain: epoch  2, batch    69 | loss: 169.6831218CurrentTrain: epoch  2, batch    70 | loss: 213.8262261CurrentTrain: epoch  2, batch    71 | loss: 184.9042684CurrentTrain: epoch  2, batch    72 | loss: 359.7266771CurrentTrain: epoch  2, batch    73 | loss: 209.1741613CurrentTrain: epoch  2, batch    74 | loss: 204.3906605CurrentTrain: epoch  2, batch    75 | loss: 191.5910738CurrentTrain: epoch  2, batch    76 | loss: 230.4981339CurrentTrain: epoch  2, batch    77 | loss: 163.3276674CurrentTrain: epoch  2, batch    78 | loss: 205.5248377CurrentTrain: epoch  2, batch    79 | loss: 205.8400007CurrentTrain: epoch  2, batch    80 | loss: 177.3303720CurrentTrain: epoch  2, batch    81 | loss: 212.2759239CurrentTrain: epoch  2, batch    82 | loss: 233.8445670CurrentTrain: epoch  2, batch    83 | loss: 250.3990699CurrentTrain: epoch  2, batch    84 | loss: 172.7117727CurrentTrain: epoch  2, batch    85 | loss: 262.4965587CurrentTrain: epoch  2, batch    86 | loss: 164.1182569CurrentTrain: epoch  2, batch    87 | loss: 170.4715044CurrentTrain: epoch  2, batch    88 | loss: 186.6205488CurrentTrain: epoch  2, batch    89 | loss: 200.9171708CurrentTrain: epoch  2, batch    90 | loss: 221.0429229CurrentTrain: epoch  2, batch    91 | loss: 204.7871404CurrentTrain: epoch  2, batch    92 | loss: 197.9602406CurrentTrain: epoch  2, batch    93 | loss: 207.6289627CurrentTrain: epoch  2, batch    94 | loss: 205.9805575CurrentTrain: epoch  2, batch    95 | loss: 154.8379015CurrentTrain: epoch  3, batch     0 | loss: 212.8515862CurrentTrain: epoch  3, batch     1 | loss: 223.7109454CurrentTrain: epoch  3, batch     2 | loss: 215.2763341CurrentTrain: epoch  3, batch     3 | loss: 213.9035738CurrentTrain: epoch  3, batch     4 | loss: 204.7923021CurrentTrain: epoch  3, batch     5 | loss: 230.8587398CurrentTrain: epoch  3, batch     6 | loss: 187.9991058CurrentTrain: epoch  3, batch     7 | loss: 172.3757309CurrentTrain: epoch  3, batch     8 | loss: 186.8309305CurrentTrain: epoch  3, batch     9 | loss: 259.7534472CurrentTrain: epoch  3, batch    10 | loss: 201.1975693CurrentTrain: epoch  3, batch    11 | loss: 195.8496347CurrentTrain: epoch  3, batch    12 | loss: 230.2808048CurrentTrain: epoch  3, batch    13 | loss: 230.0373314CurrentTrain: epoch  3, batch    14 | loss: 214.0560069CurrentTrain: epoch  3, batch    15 | loss: 231.0426046CurrentTrain: epoch  3, batch    16 | loss: 224.4197459CurrentTrain: epoch  3, batch    17 | loss: 169.8732420CurrentTrain: epoch  3, batch    18 | loss: 178.6444683CurrentTrain: epoch  3, batch    19 | loss: 200.1826046CurrentTrain: epoch  3, batch    20 | loss: 232.8668973CurrentTrain: epoch  3, batch    21 | loss: 189.1629907CurrentTrain: epoch  3, batch    22 | loss: 229.1169803CurrentTrain: epoch  3, batch    23 | loss: 196.9882775CurrentTrain: epoch  3, batch    24 | loss: 185.2345086CurrentTrain: epoch  3, batch    25 | loss: 206.8184455CurrentTrain: epoch  3, batch    26 | loss: 238.7767752CurrentTrain: epoch  3, batch    27 | loss: 251.0892434CurrentTrain: epoch  3, batch    28 | loss: 229.5923369CurrentTrain: epoch  3, batch    29 | loss: 193.1267231CurrentTrain: epoch  3, batch    30 | loss: 222.9834731CurrentTrain: epoch  3, batch    31 | loss: 194.2409489CurrentTrain: epoch  3, batch    32 | loss: 197.8441987CurrentTrain: epoch  3, batch    33 | loss: 276.7412210CurrentTrain: epoch  3, batch    34 | loss: 214.7858690CurrentTrain: epoch  3, batch    35 | loss: 268.0239212CurrentTrain: epoch  3, batch    36 | loss: 154.6053332CurrentTrain: epoch  3, batch    37 | loss: 269.8091373CurrentTrain: epoch  3, batch    38 | loss: 148.4709798CurrentTrain: epoch  3, batch    39 | loss: 205.1660124CurrentTrain: epoch  3, batch    40 | loss: 197.1115184CurrentTrain: epoch  3, batch    41 | loss: 182.3699475CurrentTrain: epoch  3, batch    42 | loss: 196.9564937CurrentTrain: epoch  3, batch    43 | loss: 169.0528116CurrentTrain: epoch  3, batch    44 | loss: 179.2004631CurrentTrain: epoch  3, batch    45 | loss: 194.0479779CurrentTrain: epoch  3, batch    46 | loss: 238.4793572CurrentTrain: epoch  3, batch    47 | loss: 213.3224518CurrentTrain: epoch  3, batch    48 | loss: 279.4190502CurrentTrain: epoch  3, batch    49 | loss: 178.1405257CurrentTrain: epoch  3, batch    50 | loss: 201.5593406CurrentTrain: epoch  3, batch    51 | loss: 192.6394272CurrentTrain: epoch  3, batch    52 | loss: 277.0812915CurrentTrain: epoch  3, batch    53 | loss: 186.6710846CurrentTrain: epoch  3, batch    54 | loss: 237.1747347CurrentTrain: epoch  3, batch    55 | loss: 189.3956146CurrentTrain: epoch  3, batch    56 | loss: 200.8930440CurrentTrain: epoch  3, batch    57 | loss: 194.9352447CurrentTrain: epoch  3, batch    58 | loss: 207.5865355CurrentTrain: epoch  3, batch    59 | loss: 266.6684414CurrentTrain: epoch  3, batch    60 | loss: 212.5308443CurrentTrain: epoch  3, batch    61 | loss: 179.4318671CurrentTrain: epoch  3, batch    62 | loss: 181.2528739CurrentTrain: epoch  3, batch    63 | loss: 214.6199501CurrentTrain: epoch  3, batch    64 | loss: 182.5300942CurrentTrain: epoch  3, batch    65 | loss: 233.0897501CurrentTrain: epoch  3, batch    66 | loss: 239.5910925CurrentTrain: epoch  3, batch    67 | loss: 199.5705351CurrentTrain: epoch  3, batch    68 | loss: 204.1648856CurrentTrain: epoch  3, batch    69 | loss: 184.8834512CurrentTrain: epoch  3, batch    70 | loss: 206.8459257CurrentTrain: epoch  3, batch    71 | loss: 143.9949781CurrentTrain: epoch  3, batch    72 | loss: 200.0471493CurrentTrain: epoch  3, batch    73 | loss: 141.9640317CurrentTrain: epoch  3, batch    74 | loss: 214.0697145CurrentTrain: epoch  3, batch    75 | loss: 238.8578414CurrentTrain: epoch  3, batch    76 | loss: 212.4896922CurrentTrain: epoch  3, batch    77 | loss: 214.9164490CurrentTrain: epoch  3, batch    78 | loss: 258.6853486CurrentTrain: epoch  3, batch    79 | loss: 181.6179827CurrentTrain: epoch  3, batch    80 | loss: 225.5650801CurrentTrain: epoch  3, batch    81 | loss: 143.6927601CurrentTrain: epoch  3, batch    82 | loss: 251.4732122CurrentTrain: epoch  3, batch    83 | loss: 191.5349027CurrentTrain: epoch  3, batch    84 | loss: 184.5754081CurrentTrain: epoch  3, batch    85 | loss: 233.1500039CurrentTrain: epoch  3, batch    86 | loss: 193.6791730CurrentTrain: epoch  3, batch    87 | loss: 222.0015375CurrentTrain: epoch  3, batch    88 | loss: 168.8554124CurrentTrain: epoch  3, batch    89 | loss: 164.6634446CurrentTrain: epoch  3, batch    90 | loss: 214.9512277CurrentTrain: epoch  3, batch    91 | loss: 224.3498590CurrentTrain: epoch  3, batch    92 | loss: 186.1452330CurrentTrain: epoch  3, batch    93 | loss: 206.7238474CurrentTrain: epoch  3, batch    94 | loss: 247.8480184CurrentTrain: epoch  3, batch    95 | loss: 133.1109263CurrentTrain: epoch  4, batch     0 | loss: 196.3244263CurrentTrain: epoch  4, batch     1 | loss: 247.3163695CurrentTrain: epoch  4, batch     2 | loss: 219.8325055CurrentTrain: epoch  4, batch     3 | loss: 178.7900847CurrentTrain: epoch  4, batch     4 | loss: 290.8330878CurrentTrain: epoch  4, batch     5 | loss: 220.1623218CurrentTrain: epoch  4, batch     6 | loss: 191.6422773CurrentTrain: epoch  4, batch     7 | loss: 160.6985364CurrentTrain: epoch  4, batch     8 | loss: 247.4631000CurrentTrain: epoch  4, batch     9 | loss: 272.1778689CurrentTrain: epoch  4, batch    10 | loss: 247.3179842CurrentTrain: epoch  4, batch    11 | loss: 192.6220752CurrentTrain: epoch  4, batch    12 | loss: 198.7009991CurrentTrain: epoch  4, batch    13 | loss: 179.6046932CurrentTrain: epoch  4, batch    14 | loss: 179.1737404CurrentTrain: epoch  4, batch    15 | loss: 237.8414596CurrentTrain: epoch  4, batch    16 | loss: 220.4050891CurrentTrain: epoch  4, batch    17 | loss: 286.8106149CurrentTrain: epoch  4, batch    18 | loss: 196.5826051CurrentTrain: epoch  4, batch    19 | loss: 195.9980119CurrentTrain: epoch  4, batch    20 | loss: 195.6067144CurrentTrain: epoch  4, batch    21 | loss: 202.2709189CurrentTrain: epoch  4, batch    22 | loss: 239.1434856CurrentTrain: epoch  4, batch    23 | loss: 173.5215448CurrentTrain: epoch  4, batch    24 | loss: 212.0508948CurrentTrain: epoch  4, batch    25 | loss: 213.9568073CurrentTrain: epoch  4, batch    26 | loss: 211.5976681CurrentTrain: epoch  4, batch    27 | loss: 184.2571587CurrentTrain: epoch  4, batch    28 | loss: 229.8641627CurrentTrain: epoch  4, batch    29 | loss: 194.8887712CurrentTrain: epoch  4, batch    30 | loss: 200.5026674CurrentTrain: epoch  4, batch    31 | loss: 184.0339822CurrentTrain: epoch  4, batch    32 | loss: 196.6683196CurrentTrain: epoch  4, batch    33 | loss: 153.0766143CurrentTrain: epoch  4, batch    34 | loss: 228.9306403CurrentTrain: epoch  4, batch    35 | loss: 220.9020757CurrentTrain: epoch  4, batch    36 | loss: 190.9206840CurrentTrain: epoch  4, batch    37 | loss: 162.9560143CurrentTrain: epoch  4, batch    38 | loss: 187.0008051CurrentTrain: epoch  4, batch    39 | loss: 193.4112047CurrentTrain: epoch  4, batch    40 | loss: 204.8831895CurrentTrain: epoch  4, batch    41 | loss: 184.4613609CurrentTrain: epoch  4, batch    42 | loss: 214.8428273CurrentTrain: epoch  4, batch    43 | loss: 154.5399925CurrentTrain: epoch  4, batch    44 | loss: 167.9729720CurrentTrain: epoch  4, batch    45 | loss: 212.2418871CurrentTrain: epoch  4, batch    46 | loss: 194.7106590CurrentTrain: epoch  4, batch    47 | loss: 213.9420010CurrentTrain: epoch  4, batch    48 | loss: 200.5274002CurrentTrain: epoch  4, batch    49 | loss: 205.9021273CurrentTrain: epoch  4, batch    50 | loss: 189.7801990CurrentTrain: epoch  4, batch    51 | loss: 193.8034200CurrentTrain: epoch  4, batch    52 | loss: 212.9049208CurrentTrain: epoch  4, batch    53 | loss: 266.6802709CurrentTrain: epoch  4, batch    54 | loss: 204.7067776CurrentTrain: epoch  4, batch    55 | loss: 231.5876773CurrentTrain: epoch  4, batch    56 | loss: 286.8293813CurrentTrain: epoch  4, batch    57 | loss: 178.4595689CurrentTrain: epoch  4, batch    58 | loss: 266.9833556CurrentTrain: epoch  4, batch    59 | loss: 233.1022362CurrentTrain: epoch  4, batch    60 | loss: 221.6315947CurrentTrain: epoch  4, batch    61 | loss: 161.5358577CurrentTrain: epoch  4, batch    62 | loss: 147.7021269CurrentTrain: epoch  4, batch    63 | loss: 184.0500125CurrentTrain: epoch  4, batch    64 | loss: 212.0516440CurrentTrain: epoch  4, batch    65 | loss: 277.9989518CurrentTrain: epoch  4, batch    66 | loss: 185.8808634CurrentTrain: epoch  4, batch    67 | loss: 176.3368296CurrentTrain: epoch  4, batch    68 | loss: 233.8467755CurrentTrain: epoch  4, batch    69 | loss: 175.6742939CurrentTrain: epoch  4, batch    70 | loss: 196.2570853CurrentTrain: epoch  4, batch    71 | loss: 225.3602434CurrentTrain: epoch  4, batch    72 | loss: 153.4438281CurrentTrain: epoch  4, batch    73 | loss: 194.5770125CurrentTrain: epoch  4, batch    74 | loss: 286.9934022CurrentTrain: epoch  4, batch    75 | loss: 204.1036023CurrentTrain: epoch  4, batch    76 | loss: 287.3058581CurrentTrain: epoch  4, batch    77 | loss: 220.3714652CurrentTrain: epoch  4, batch    78 | loss: 171.2994370CurrentTrain: epoch  4, batch    79 | loss: 219.6222447CurrentTrain: epoch  4, batch    80 | loss: 170.9720177CurrentTrain: epoch  4, batch    81 | loss: 276.2570293CurrentTrain: epoch  4, batch    82 | loss: 179.3323157CurrentTrain: epoch  4, batch    83 | loss: 175.2970280CurrentTrain: epoch  4, batch    84 | loss: 155.9859329CurrentTrain: epoch  4, batch    85 | loss: 202.7094006CurrentTrain: epoch  4, batch    86 | loss: 247.7175573CurrentTrain: epoch  4, batch    87 | loss: 269.2447144CurrentTrain: epoch  4, batch    88 | loss: 239.4832929CurrentTrain: epoch  4, batch    89 | loss: 183.0530759CurrentTrain: epoch  4, batch    90 | loss: 252.9440430CurrentTrain: epoch  4, batch    91 | loss: 213.1718006CurrentTrain: epoch  4, batch    92 | loss: 212.7470200CurrentTrain: epoch  4, batch    93 | loss: 188.3641372CurrentTrain: epoch  4, batch    94 | loss: 225.4268716CurrentTrain: epoch  4, batch    95 | loss: 163.3834310CurrentTrain: epoch  5, batch     0 | loss: 194.4251696CurrentTrain: epoch  5, batch     1 | loss: 187.0092912CurrentTrain: epoch  5, batch     2 | loss: 228.9111839CurrentTrain: epoch  5, batch     3 | loss: 204.0523495CurrentTrain: epoch  5, batch     4 | loss: 187.0368862CurrentTrain: epoch  5, batch     5 | loss: 176.2339965CurrentTrain: epoch  5, batch     6 | loss: 289.7796561CurrentTrain: epoch  5, batch     7 | loss: 181.0998160CurrentTrain: epoch  5, batch     8 | loss: 195.0481585CurrentTrain: epoch  5, batch     9 | loss: 187.4993346CurrentTrain: epoch  5, batch    10 | loss: 184.9094406CurrentTrain: epoch  5, batch    11 | loss: 237.9806098CurrentTrain: epoch  5, batch    12 | loss: 286.3911706CurrentTrain: epoch  5, batch    13 | loss: 184.0109662CurrentTrain: epoch  5, batch    14 | loss: 228.7113156CurrentTrain: epoch  5, batch    15 | loss: 170.1048530CurrentTrain: epoch  5, batch    16 | loss: 203.1799277CurrentTrain: epoch  5, batch    17 | loss: 155.6728203CurrentTrain: epoch  5, batch    18 | loss: 190.2826623CurrentTrain: epoch  5, batch    19 | loss: 227.9908674CurrentTrain: epoch  5, batch    20 | loss: 202.4038171CurrentTrain: epoch  5, batch    21 | loss: 171.7734696CurrentTrain: epoch  5, batch    22 | loss: 211.0287278CurrentTrain: epoch  5, batch    23 | loss: 219.8777192CurrentTrain: epoch  5, batch    24 | loss: 175.9808846CurrentTrain: epoch  5, batch    25 | loss: 203.4833014CurrentTrain: epoch  5, batch    26 | loss: 246.6464392CurrentTrain: epoch  5, batch    27 | loss: 212.6948590CurrentTrain: epoch  5, batch    28 | loss: 287.4392791CurrentTrain: epoch  5, batch    29 | loss: 558.5412347CurrentTrain: epoch  5, batch    30 | loss: 184.7908830CurrentTrain: epoch  5, batch    31 | loss: 346.4673034CurrentTrain: epoch  5, batch    32 | loss: 163.2901160CurrentTrain: epoch  5, batch    33 | loss: 164.2232596CurrentTrain: epoch  5, batch    34 | loss: 195.3157109CurrentTrain: epoch  5, batch    35 | loss: 151.6204313CurrentTrain: epoch  5, batch    36 | loss: 277.1311753CurrentTrain: epoch  5, batch    37 | loss: 196.4297408CurrentTrain: epoch  5, batch    38 | loss: 202.3420574CurrentTrain: epoch  5, batch    39 | loss: 248.3036401CurrentTrain: epoch  5, batch    40 | loss: 193.8068260CurrentTrain: epoch  5, batch    41 | loss: 247.1651334CurrentTrain: epoch  5, batch    42 | loss: 287.2484113CurrentTrain: epoch  5, batch    43 | loss: 191.6907333CurrentTrain: epoch  5, batch    44 | loss: 183.4211154CurrentTrain: epoch  5, batch    45 | loss: 210.7192520CurrentTrain: epoch  5, batch    46 | loss: 211.3010181CurrentTrain: epoch  5, batch    47 | loss: 169.7160296CurrentTrain: epoch  5, batch    48 | loss: 167.5920084CurrentTrain: epoch  5, batch    49 | loss: 191.7852246CurrentTrain: epoch  5, batch    50 | loss: 168.2671025CurrentTrain: epoch  5, batch    51 | loss: 179.9235094CurrentTrain: epoch  5, batch    52 | loss: 221.9555852CurrentTrain: epoch  5, batch    53 | loss: 178.4152995CurrentTrain: epoch  5, batch    54 | loss: 188.8261931CurrentTrain: epoch  5, batch    55 | loss: 246.6735659CurrentTrain: epoch  5, batch    56 | loss: 286.5832933CurrentTrain: epoch  5, batch    57 | loss: 212.7363755CurrentTrain: epoch  5, batch    58 | loss: 212.0855123CurrentTrain: epoch  5, batch    59 | loss: 277.8224931CurrentTrain: epoch  5, batch    60 | loss: 240.0516875CurrentTrain: epoch  5, batch    61 | loss: 203.8913557CurrentTrain: epoch  5, batch    62 | loss: 250.0165155CurrentTrain: epoch  5, batch    63 | loss: 195.5048478CurrentTrain: epoch  5, batch    64 | loss: 153.5547079CurrentTrain: epoch  5, batch    65 | loss: 174.8633384CurrentTrain: epoch  5, batch    66 | loss: 193.9791408CurrentTrain: epoch  5, batch    67 | loss: 188.7298104CurrentTrain: epoch  5, batch    68 | loss: 163.9057522CurrentTrain: epoch  5, batch    69 | loss: 196.0702054CurrentTrain: epoch  5, batch    70 | loss: 238.3988453CurrentTrain: epoch  5, batch    71 | loss: 241.1570666CurrentTrain: epoch  5, batch    72 | loss: 230.7634667CurrentTrain: epoch  5, batch    73 | loss: 155.6830145CurrentTrain: epoch  5, batch    74 | loss: 160.8662193CurrentTrain: epoch  5, batch    75 | loss: 175.3429545CurrentTrain: epoch  5, batch    76 | loss: 270.4529494CurrentTrain: epoch  5, batch    77 | loss: 211.3048901CurrentTrain: epoch  5, batch    78 | loss: 186.9770638CurrentTrain: epoch  5, batch    79 | loss: 194.9288575CurrentTrain: epoch  5, batch    80 | loss: 211.4590581CurrentTrain: epoch  5, batch    81 | loss: 237.1055690CurrentTrain: epoch  5, batch    82 | loss: 203.3979018CurrentTrain: epoch  5, batch    83 | loss: 237.7798749CurrentTrain: epoch  5, batch    84 | loss: 228.0404854CurrentTrain: epoch  5, batch    85 | loss: 203.6669498CurrentTrain: epoch  5, batch    86 | loss: 237.5547910CurrentTrain: epoch  5, batch    87 | loss: 169.0447282CurrentTrain: epoch  5, batch    88 | loss: 229.2778152CurrentTrain: epoch  5, batch    89 | loss: 197.1994449CurrentTrain: epoch  5, batch    90 | loss: 202.8812161CurrentTrain: epoch  5, batch    91 | loss: 220.4164383CurrentTrain: epoch  5, batch    92 | loss: 187.7590799CurrentTrain: epoch  5, batch    93 | loss: 168.6687982CurrentTrain: epoch  5, batch    94 | loss: 276.6474183CurrentTrain: epoch  5, batch    95 | loss: 163.6638992CurrentTrain: epoch  6, batch     0 | loss: 155.4356198CurrentTrain: epoch  6, batch     1 | loss: 203.6515729CurrentTrain: epoch  6, batch     2 | loss: 185.8727079CurrentTrain: epoch  6, batch     3 | loss: 246.3929253CurrentTrain: epoch  6, batch     4 | loss: 287.2684310CurrentTrain: epoch  6, batch     5 | loss: 359.1124948CurrentTrain: epoch  6, batch     6 | loss: 212.2420960CurrentTrain: epoch  6, batch     7 | loss: 212.2658575CurrentTrain: epoch  6, batch     8 | loss: 237.8564472CurrentTrain: epoch  6, batch     9 | loss: 220.0704813CurrentTrain: epoch  6, batch    10 | loss: 257.5205891CurrentTrain: epoch  6, batch    11 | loss: 212.8160342CurrentTrain: epoch  6, batch    12 | loss: 246.4036029CurrentTrain: epoch  6, batch    13 | loss: 229.7644105CurrentTrain: epoch  6, batch    14 | loss: 161.3883103CurrentTrain: epoch  6, batch    15 | loss: 175.1033803CurrentTrain: epoch  6, batch    16 | loss: 174.5592481CurrentTrain: epoch  6, batch    17 | loss: 203.0559070CurrentTrain: epoch  6, batch    18 | loss: 174.7034324CurrentTrain: epoch  6, batch    19 | loss: 168.8116107CurrentTrain: epoch  6, batch    20 | loss: 168.1403626CurrentTrain: epoch  6, batch    21 | loss: 211.0479698CurrentTrain: epoch  6, batch    22 | loss: 220.7283972CurrentTrain: epoch  6, batch    23 | loss: 221.4149512CurrentTrain: epoch  6, batch    24 | loss: 238.3648690CurrentTrain: epoch  6, batch    25 | loss: 167.3118364CurrentTrain: epoch  6, batch    26 | loss: 183.5261672CurrentTrain: epoch  6, batch    27 | loss: 167.6932727CurrentTrain: epoch  6, batch    28 | loss: 200.8723683CurrentTrain: epoch  6, batch    29 | loss: 218.9268618CurrentTrain: epoch  6, batch    30 | loss: 237.1608972CurrentTrain: epoch  6, batch    31 | loss: 152.3648530CurrentTrain: epoch  6, batch    32 | loss: 220.0575630CurrentTrain: epoch  6, batch    33 | loss: 219.9899003CurrentTrain: epoch  6, batch    34 | loss: 219.8507267CurrentTrain: epoch  6, batch    35 | loss: 174.9590841CurrentTrain: epoch  6, batch    36 | loss: 182.4866467CurrentTrain: epoch  6, batch    37 | loss: 200.1779849CurrentTrain: epoch  6, batch    38 | loss: 228.4695549CurrentTrain: epoch  6, batch    39 | loss: 195.3626039CurrentTrain: epoch  6, batch    40 | loss: 168.3527364CurrentTrain: epoch  6, batch    41 | loss: 214.3700288CurrentTrain: epoch  6, batch    42 | loss: 187.3649974CurrentTrain: epoch  6, batch    43 | loss: 168.0198670CurrentTrain: epoch  6, batch    44 | loss: 144.8263716CurrentTrain: epoch  6, batch    45 | loss: 203.5926915CurrentTrain: epoch  6, batch    46 | loss: 194.5323576CurrentTrain: epoch  6, batch    47 | loss: 286.5441364CurrentTrain: epoch  6, batch    48 | loss: 191.3329077CurrentTrain: epoch  6, batch    49 | loss: 176.0226630CurrentTrain: epoch  6, batch    50 | loss: 247.2266527CurrentTrain: epoch  6, batch    51 | loss: 220.9322776CurrentTrain: epoch  6, batch    52 | loss: 286.4474392CurrentTrain: epoch  6, batch    53 | loss: 249.4165424CurrentTrain: epoch  6, batch    54 | loss: 211.9414638CurrentTrain: epoch  6, batch    55 | loss: 210.5378571CurrentTrain: epoch  6, batch    56 | loss: 229.9532359CurrentTrain: epoch  6, batch    57 | loss: 147.3552588CurrentTrain: epoch  6, batch    58 | loss: 238.9202373CurrentTrain: epoch  6, batch    59 | loss: 166.8018475CurrentTrain: epoch  6, batch    60 | loss: 193.4975249CurrentTrain: epoch  6, batch    61 | loss: 220.9757909CurrentTrain: epoch  6, batch    62 | loss: 219.3305135CurrentTrain: epoch  6, batch    63 | loss: 181.5740298CurrentTrain: epoch  6, batch    64 | loss: 193.7213433CurrentTrain: epoch  6, batch    65 | loss: 237.2644737CurrentTrain: epoch  6, batch    66 | loss: 213.8203899CurrentTrain: epoch  6, batch    67 | loss: 202.5438746CurrentTrain: epoch  6, batch    68 | loss: 228.2501223CurrentTrain: epoch  6, batch    69 | loss: 194.0375750CurrentTrain: epoch  6, batch    70 | loss: 238.4949976CurrentTrain: epoch  6, batch    71 | loss: 167.1522182CurrentTrain: epoch  6, batch    72 | loss: 169.9746814CurrentTrain: epoch  6, batch    73 | loss: 153.6758104CurrentTrain: epoch  6, batch    74 | loss: 246.4010754CurrentTrain: epoch  6, batch    75 | loss: 228.6541504CurrentTrain: epoch  6, batch    76 | loss: 146.6321433CurrentTrain: epoch  6, batch    77 | loss: 167.0552646CurrentTrain: epoch  6, batch    78 | loss: 237.0339102CurrentTrain: epoch  6, batch    79 | loss: 231.4095231CurrentTrain: epoch  6, batch    80 | loss: 194.8078640CurrentTrain: epoch  6, batch    81 | loss: 161.6236924CurrentTrain: epoch  6, batch    82 | loss: 183.8605447CurrentTrain: epoch  6, batch    83 | loss: 210.2151049CurrentTrain: epoch  6, batch    84 | loss: 203.0249229CurrentTrain: epoch  6, batch    85 | loss: 160.3544867CurrentTrain: epoch  6, batch    86 | loss: 186.7136066CurrentTrain: epoch  6, batch    87 | loss: 228.5445952CurrentTrain: epoch  6, batch    88 | loss: 220.0394642CurrentTrain: epoch  6, batch    89 | loss: 248.8239041CurrentTrain: epoch  6, batch    90 | loss: 228.8014544CurrentTrain: epoch  6, batch    91 | loss: 159.5384582CurrentTrain: epoch  6, batch    92 | loss: 202.7918164CurrentTrain: epoch  6, batch    93 | loss: 187.7971571CurrentTrain: epoch  6, batch    94 | loss: 187.7025096CurrentTrain: epoch  6, batch    95 | loss: 211.7576357CurrentTrain: epoch  7, batch     0 | loss: 210.4523964CurrentTrain: epoch  7, batch     1 | loss: 227.8031354CurrentTrain: epoch  7, batch     2 | loss: 211.8041743CurrentTrain: epoch  7, batch     3 | loss: 203.5186598CurrentTrain: epoch  7, batch     4 | loss: 194.3320833CurrentTrain: epoch  7, batch     5 | loss: 227.9287691CurrentTrain: epoch  7, batch     6 | loss: 211.0409510CurrentTrain: epoch  7, batch     7 | loss: 194.3452314CurrentTrain: epoch  7, batch     8 | loss: 174.6492828CurrentTrain: epoch  7, batch     9 | loss: 169.0060830CurrentTrain: epoch  7, batch    10 | loss: 182.9606796CurrentTrain: epoch  7, batch    11 | loss: 182.6251129CurrentTrain: epoch  7, batch    12 | loss: 243.8426192CurrentTrain: epoch  7, batch    13 | loss: 210.4789008CurrentTrain: epoch  7, batch    14 | loss: 215.2063912CurrentTrain: epoch  7, batch    15 | loss: 199.3817531CurrentTrain: epoch  7, batch    16 | loss: 237.2733216CurrentTrain: epoch  7, batch    17 | loss: 157.8977786CurrentTrain: epoch  7, batch    18 | loss: 183.2199958CurrentTrain: epoch  7, batch    19 | loss: 160.5399384CurrentTrain: epoch  7, batch    20 | loss: 179.0853519CurrentTrain: epoch  7, batch    21 | loss: 186.9055606CurrentTrain: epoch  7, batch    22 | loss: 237.0669607CurrentTrain: epoch  7, batch    23 | loss: 233.1494472CurrentTrain: epoch  7, batch    24 | loss: 184.6297623CurrentTrain: epoch  7, batch    25 | loss: 159.5729282CurrentTrain: epoch  7, batch    26 | loss: 203.1691042CurrentTrain: epoch  7, batch    27 | loss: 246.5422261CurrentTrain: epoch  7, batch    28 | loss: 183.8467382CurrentTrain: epoch  7, batch    29 | loss: 247.8696123CurrentTrain: epoch  7, batch    30 | loss: 194.2209433CurrentTrain: epoch  7, batch    31 | loss: 219.7182137CurrentTrain: epoch  7, batch    32 | loss: 256.7791227CurrentTrain: epoch  7, batch    33 | loss: 210.8420359CurrentTrain: epoch  7, batch    34 | loss: 219.5967904CurrentTrain: epoch  7, batch    35 | loss: 247.1502337CurrentTrain: epoch  7, batch    36 | loss: 210.9371788CurrentTrain: epoch  7, batch    37 | loss: 220.2669921CurrentTrain: epoch  7, batch    38 | loss: 210.6199574CurrentTrain: epoch  7, batch    39 | loss: 240.2300110CurrentTrain: epoch  7, batch    40 | loss: 204.3604266CurrentTrain: epoch  7, batch    41 | loss: 160.1386613CurrentTrain: epoch  7, batch    42 | loss: 222.7197009CurrentTrain: epoch  7, batch    43 | loss: 175.5932672CurrentTrain: epoch  7, batch    44 | loss: 186.1950015CurrentTrain: epoch  7, batch    45 | loss: 237.2407647CurrentTrain: epoch  7, batch    46 | loss: 192.4690623CurrentTrain: epoch  7, batch    47 | loss: 170.7030301CurrentTrain: epoch  7, batch    48 | loss: 257.4037953CurrentTrain: epoch  7, batch    49 | loss: 175.4744717CurrentTrain: epoch  7, batch    50 | loss: 218.8029012CurrentTrain: epoch  7, batch    51 | loss: 177.7440538CurrentTrain: epoch  7, batch    52 | loss: 175.1444318CurrentTrain: epoch  7, batch    53 | loss: 228.6694548CurrentTrain: epoch  7, batch    54 | loss: 175.2459979CurrentTrain: epoch  7, batch    55 | loss: 246.5654342CurrentTrain: epoch  7, batch    56 | loss: 228.0706032CurrentTrain: epoch  7, batch    57 | loss: 202.7450753CurrentTrain: epoch  7, batch    58 | loss: 194.4960218CurrentTrain: epoch  7, batch    59 | loss: 286.3916867CurrentTrain: epoch  7, batch    60 | loss: 169.8072020CurrentTrain: epoch  7, batch    61 | loss: 190.6152205CurrentTrain: epoch  7, batch    62 | loss: 266.0294373CurrentTrain: epoch  7, batch    63 | loss: 197.9777635CurrentTrain: epoch  7, batch    64 | loss: 152.8068408CurrentTrain: epoch  7, batch    65 | loss: 178.8679830CurrentTrain: epoch  7, batch    66 | loss: 201.7565340CurrentTrain: epoch  7, batch    67 | loss: 219.4172944CurrentTrain: epoch  7, batch    68 | loss: 167.6337298CurrentTrain: epoch  7, batch    69 | loss: 203.0484442CurrentTrain: epoch  7, batch    70 | loss: 191.0437460CurrentTrain: epoch  7, batch    71 | loss: 246.9711258CurrentTrain: epoch  7, batch    72 | loss: 167.1713986CurrentTrain: epoch  7, batch    73 | loss: 201.7943570CurrentTrain: epoch  7, batch    74 | loss: 146.1149243CurrentTrain: epoch  7, batch    75 | loss: 160.1262506CurrentTrain: epoch  7, batch    76 | loss: 176.9811727CurrentTrain: epoch  7, batch    77 | loss: 237.9104111CurrentTrain: epoch  7, batch    78 | loss: 211.0800602CurrentTrain: epoch  7, batch    79 | loss: 286.9662980CurrentTrain: epoch  7, batch    80 | loss: 190.8052861CurrentTrain: epoch  7, batch    81 | loss: 174.2921889CurrentTrain: epoch  7, batch    82 | loss: 202.3209672CurrentTrain: epoch  7, batch    83 | loss: 159.6081646CurrentTrain: epoch  7, batch    84 | loss: 194.0401307CurrentTrain: epoch  7, batch    85 | loss: 210.7835956CurrentTrain: epoch  7, batch    86 | loss: 178.0488650CurrentTrain: epoch  7, batch    87 | loss: 228.9393498CurrentTrain: epoch  7, batch    88 | loss: 210.7744496CurrentTrain: epoch  7, batch    89 | loss: 259.7341821CurrentTrain: epoch  7, batch    90 | loss: 210.5355586CurrentTrain: epoch  7, batch    91 | loss: 194.5174139CurrentTrain: epoch  7, batch    92 | loss: 228.4998216CurrentTrain: epoch  7, batch    93 | loss: 175.0547729CurrentTrain: epoch  7, batch    94 | loss: 183.4094227CurrentTrain: epoch  7, batch    95 | loss: 195.4861058CurrentTrain: epoch  8, batch     0 | loss: 175.3471684CurrentTrain: epoch  8, batch     1 | loss: 174.7132452CurrentTrain: epoch  8, batch     2 | loss: 238.3024587CurrentTrain: epoch  8, batch     3 | loss: 286.2742217CurrentTrain: epoch  8, batch     4 | loss: 203.8391136CurrentTrain: epoch  8, batch     5 | loss: 199.1742428CurrentTrain: epoch  8, batch     6 | loss: 220.3592008CurrentTrain: epoch  8, batch     7 | loss: 194.0110986CurrentTrain: epoch  8, batch     8 | loss: 191.7576461CurrentTrain: epoch  8, batch     9 | loss: 240.6756849CurrentTrain: epoch  8, batch    10 | loss: 167.0507901CurrentTrain: epoch  8, batch    11 | loss: 186.2749975CurrentTrain: epoch  8, batch    12 | loss: 202.0908926CurrentTrain: epoch  8, batch    13 | loss: 210.9680372CurrentTrain: epoch  8, batch    14 | loss: 193.8778190CurrentTrain: epoch  8, batch    15 | loss: 210.7309462CurrentTrain: epoch  8, batch    16 | loss: 227.9677233CurrentTrain: epoch  8, batch    17 | loss: 201.7850889CurrentTrain: epoch  8, batch    18 | loss: 237.9744157CurrentTrain: epoch  8, batch    19 | loss: 182.9602892CurrentTrain: epoch  8, batch    20 | loss: 179.3024471CurrentTrain: epoch  8, batch    21 | loss: 186.0277047CurrentTrain: epoch  8, batch    22 | loss: 211.5205840CurrentTrain: epoch  8, batch    23 | loss: 190.5322083CurrentTrain: epoch  8, batch    24 | loss: 169.0572125CurrentTrain: epoch  8, batch    25 | loss: 227.5169108CurrentTrain: epoch  8, batch    26 | loss: 160.1051771CurrentTrain: epoch  8, batch    27 | loss: 195.1495526CurrentTrain: epoch  8, batch    28 | loss: 175.0547212CurrentTrain: epoch  8, batch    29 | loss: 182.4559187CurrentTrain: epoch  8, batch    30 | loss: 247.4215785CurrentTrain: epoch  8, batch    31 | loss: 151.5848851CurrentTrain: epoch  8, batch    32 | loss: 346.4188309CurrentTrain: epoch  8, batch    33 | loss: 236.7359469CurrentTrain: epoch  8, batch    34 | loss: 220.8551893CurrentTrain: epoch  8, batch    35 | loss: 166.6440128CurrentTrain: epoch  8, batch    36 | loss: 211.4447520CurrentTrain: epoch  8, batch    37 | loss: 191.2929873CurrentTrain: epoch  8, batch    38 | loss: 193.8153449CurrentTrain: epoch  8, batch    39 | loss: 220.1019354CurrentTrain: epoch  8, batch    40 | loss: 210.8050492CurrentTrain: epoch  8, batch    41 | loss: 246.6368256CurrentTrain: epoch  8, batch    42 | loss: 210.4286218CurrentTrain: epoch  8, batch    43 | loss: 246.4279980CurrentTrain: epoch  8, batch    44 | loss: 276.1612687CurrentTrain: epoch  8, batch    45 | loss: 210.7191826CurrentTrain: epoch  8, batch    46 | loss: 166.9694095CurrentTrain: epoch  8, batch    47 | loss: 211.0391914CurrentTrain: epoch  8, batch    48 | loss: 174.0732010CurrentTrain: epoch  8, batch    49 | loss: 194.0699312CurrentTrain: epoch  8, batch    50 | loss: 228.4031577CurrentTrain: epoch  8, batch    51 | loss: 174.3976578CurrentTrain: epoch  8, batch    52 | loss: 210.4145262CurrentTrain: epoch  8, batch    53 | loss: 219.4958988CurrentTrain: epoch  8, batch    54 | loss: 227.8639186CurrentTrain: epoch  8, batch    55 | loss: 218.8421466CurrentTrain: epoch  8, batch    56 | loss: 182.6347769CurrentTrain: epoch  8, batch    57 | loss: 171.6931974CurrentTrain: epoch  8, batch    58 | loss: 202.0700787CurrentTrain: epoch  8, batch    59 | loss: 133.6686713CurrentTrain: epoch  8, batch    60 | loss: 276.2201052CurrentTrain: epoch  8, batch    61 | loss: 237.3262167CurrentTrain: epoch  8, batch    62 | loss: 152.7439360CurrentTrain: epoch  8, batch    63 | loss: 210.6089211CurrentTrain: epoch  8, batch    64 | loss: 207.2884807CurrentTrain: epoch  8, batch    65 | loss: 193.6828731CurrentTrain: epoch  8, batch    66 | loss: 165.5598004CurrentTrain: epoch  8, batch    67 | loss: 211.6902284CurrentTrain: epoch  8, batch    68 | loss: 210.7592523CurrentTrain: epoch  8, batch    69 | loss: 211.8215625CurrentTrain: epoch  8, batch    70 | loss: 227.5066092CurrentTrain: epoch  8, batch    71 | loss: 257.8286151CurrentTrain: epoch  8, batch    72 | loss: 200.3464702CurrentTrain: epoch  8, batch    73 | loss: 193.3494866CurrentTrain: epoch  8, batch    74 | loss: 236.8524127CurrentTrain: epoch  8, batch    75 | loss: 201.8981121CurrentTrain: epoch  8, batch    76 | loss: 159.3256787CurrentTrain: epoch  8, batch    77 | loss: 266.3552035CurrentTrain: epoch  8, batch    78 | loss: 149.7223560CurrentTrain: epoch  8, batch    79 | loss: 193.3338419CurrentTrain: epoch  8, batch    80 | loss: 219.4035056CurrentTrain: epoch  8, batch    81 | loss: 212.1233907CurrentTrain: epoch  8, batch    82 | loss: 199.4366012CurrentTrain: epoch  8, batch    83 | loss: 175.5072578CurrentTrain: epoch  8, batch    84 | loss: 277.6849873CurrentTrain: epoch  8, batch    85 | loss: 256.6755464CurrentTrain: epoch  8, batch    86 | loss: 237.0653148CurrentTrain: epoch  8, batch    87 | loss: 204.0888829CurrentTrain: epoch  8, batch    88 | loss: 125.9852200CurrentTrain: epoch  8, batch    89 | loss: 195.7682752CurrentTrain: epoch  8, batch    90 | loss: 193.4582846CurrentTrain: epoch  8, batch    91 | loss: 222.4742425CurrentTrain: epoch  8, batch    92 | loss: 202.2445628CurrentTrain: epoch  8, batch    93 | loss: 201.9790317CurrentTrain: epoch  8, batch    94 | loss: 219.9467214CurrentTrain: epoch  8, batch    95 | loss: 229.0667977CurrentTrain: epoch  9, batch     0 | loss: 190.7716913CurrentTrain: epoch  9, batch     1 | loss: 256.3724650CurrentTrain: epoch  9, batch     2 | loss: 189.8476779CurrentTrain: epoch  9, batch     3 | loss: 184.3608741CurrentTrain: epoch  9, batch     4 | loss: 203.1540281CurrentTrain: epoch  9, batch     5 | loss: 144.8185062CurrentTrain: epoch  9, batch     6 | loss: 227.5359715CurrentTrain: epoch  9, batch     7 | loss: 193.8699803CurrentTrain: epoch  9, batch     8 | loss: 227.4734192CurrentTrain: epoch  9, batch     9 | loss: 174.8684393CurrentTrain: epoch  9, batch    10 | loss: 246.9837243CurrentTrain: epoch  9, batch    11 | loss: 190.6727820CurrentTrain: epoch  9, batch    12 | loss: 228.9236008CurrentTrain: epoch  9, batch    13 | loss: 236.9555111CurrentTrain: epoch  9, batch    14 | loss: 220.9846467CurrentTrain: epoch  9, batch    15 | loss: 185.8637542CurrentTrain: epoch  9, batch    16 | loss: 286.3140481CurrentTrain: epoch  9, batch    17 | loss: 210.8686576CurrentTrain: epoch  9, batch    18 | loss: 194.7010025CurrentTrain: epoch  9, batch    19 | loss: 224.7310736CurrentTrain: epoch  9, batch    20 | loss: 160.9653769CurrentTrain: epoch  9, batch    21 | loss: 237.6458627CurrentTrain: epoch  9, batch    22 | loss: 190.5430580CurrentTrain: epoch  9, batch    23 | loss: 183.3304816CurrentTrain: epoch  9, batch    24 | loss: 275.9661062CurrentTrain: epoch  9, batch    25 | loss: 227.6224316CurrentTrain: epoch  9, batch    26 | loss: 145.7746213CurrentTrain: epoch  9, batch    27 | loss: 256.6118765CurrentTrain: epoch  9, batch    28 | loss: 175.5040581CurrentTrain: epoch  9, batch    29 | loss: 183.3370905CurrentTrain: epoch  9, batch    30 | loss: 174.8464494CurrentTrain: epoch  9, batch    31 | loss: 194.1694212CurrentTrain: epoch  9, batch    32 | loss: 236.9131584CurrentTrain: epoch  9, batch    33 | loss: 192.9496302CurrentTrain: epoch  9, batch    34 | loss: 182.7246903CurrentTrain: epoch  9, batch    35 | loss: 267.3044937CurrentTrain: epoch  9, batch    36 | loss: 193.5981108CurrentTrain: epoch  9, batch    37 | loss: 210.6721394CurrentTrain: epoch  9, batch    38 | loss: 201.7311448CurrentTrain: epoch  9, batch    39 | loss: 191.9674453CurrentTrain: epoch  9, batch    40 | loss: 160.1519423CurrentTrain: epoch  9, batch    41 | loss: 219.4624111CurrentTrain: epoch  9, batch    42 | loss: 166.5328529CurrentTrain: epoch  9, batch    43 | loss: 161.0773560CurrentTrain: epoch  9, batch    44 | loss: 266.1469051CurrentTrain: epoch  9, batch    45 | loss: 199.3324971CurrentTrain: epoch  9, batch    46 | loss: 210.9416982CurrentTrain: epoch  9, batch    47 | loss: 237.0419403CurrentTrain: epoch  9, batch    48 | loss: 193.3101863CurrentTrain: epoch  9, batch    49 | loss: 190.8564598CurrentTrain: epoch  9, batch    50 | loss: 185.7767678CurrentTrain: epoch  9, batch    51 | loss: 211.7238444CurrentTrain: epoch  9, batch    52 | loss: 183.0841075CurrentTrain: epoch  9, batch    53 | loss: 211.2439410CurrentTrain: epoch  9, batch    54 | loss: 182.7237166CurrentTrain: epoch  9, batch    55 | loss: 177.5932475CurrentTrain: epoch  9, batch    56 | loss: 210.8226067CurrentTrain: epoch  9, batch    57 | loss: 144.3076012CurrentTrain: epoch  9, batch    58 | loss: 210.3789424CurrentTrain: epoch  9, batch    59 | loss: 238.5051383CurrentTrain: epoch  9, batch    60 | loss: 178.8607582CurrentTrain: epoch  9, batch    61 | loss: 160.0027495CurrentTrain: epoch  9, batch    62 | loss: 227.9551600CurrentTrain: epoch  9, batch    63 | loss: 153.0232044CurrentTrain: epoch  9, batch    64 | loss: 161.4169230CurrentTrain: epoch  9, batch    65 | loss: 236.9934672CurrentTrain: epoch  9, batch    66 | loss: 228.0979531CurrentTrain: epoch  9, batch    67 | loss: 194.5070123CurrentTrain: epoch  9, batch    68 | loss: 286.3137998CurrentTrain: epoch  9, batch    69 | loss: 237.3946797CurrentTrain: epoch  9, batch    70 | loss: 202.1490070CurrentTrain: epoch  9, batch    71 | loss: 228.0496783CurrentTrain: epoch  9, batch    72 | loss: 167.8244930CurrentTrain: epoch  9, batch    73 | loss: 240.1107031CurrentTrain: epoch  9, batch    74 | loss: 286.3346816CurrentTrain: epoch  9, batch    75 | loss: 241.9597999CurrentTrain: epoch  9, batch    76 | loss: 227.4647720CurrentTrain: epoch  9, batch    77 | loss: 182.2795865CurrentTrain: epoch  9, batch    78 | loss: 257.0402917CurrentTrain: epoch  9, batch    79 | loss: 194.2906792CurrentTrain: epoch  9, batch    80 | loss: 166.9665188CurrentTrain: epoch  9, batch    81 | loss: 145.2193388CurrentTrain: epoch  9, batch    82 | loss: 246.7256153CurrentTrain: epoch  9, batch    83 | loss: 286.5044078CurrentTrain: epoch  9, batch    84 | loss: 286.3023879CurrentTrain: epoch  9, batch    85 | loss: 182.2899151CurrentTrain: epoch  9, batch    86 | loss: 187.7537408CurrentTrain: epoch  9, batch    87 | loss: 175.8360958CurrentTrain: epoch  9, batch    88 | loss: 286.2541610CurrentTrain: epoch  9, batch    89 | loss: 167.1136192CurrentTrain: epoch  9, batch    90 | loss: 203.0144620CurrentTrain: epoch  9, batch    91 | loss: 194.0501086CurrentTrain: epoch  9, batch    92 | loss: 146.3137726CurrentTrain: epoch  9, batch    93 | loss: 169.2988469CurrentTrain: epoch  9, batch    94 | loss: 210.0081404CurrentTrain: epoch  9, batch    95 | loss: 156.2541798

F1 score per class: {32: 0.6740331491712708, 6: 0.8571428571428571, 19: 0.16, 24: 0.7582417582417582, 26: 0.907103825136612, 29: 0.8795811518324608}
Micro-average F1 score: 0.7982924226254002
Weighted-average F1 score: 0.8055124721897473
F1 score per class: {32: 0.7309644670050761, 6: 0.918918918918919, 19: 0.3870967741935484, 24: 0.7582417582417582, 26: 0.9528795811518325, 29: 0.8736842105263158}
Micro-average F1 score: 0.8319672131147541
Weighted-average F1 score: 0.8331077827076988
F1 score per class: {32: 0.7309644670050761, 6: 0.918918918918919, 19: 0.41379310344827586, 24: 0.7582417582417582, 26: 0.9528795811518325, 29: 0.8854166666666666}
Micro-average F1 score: 0.8360655737704918
Weighted-average F1 score: 0.8382305348024144

F1 score per class: {32: 0.6740331491712708, 6: 0.8571428571428571, 19: 0.16, 24: 0.7582417582417582, 26: 0.907103825136612, 29: 0.8795811518324608}
Micro-average F1 score: 0.7982924226254002
Weighted-average F1 score: 0.8055124721897473
F1 score per class: {32: 0.7309644670050761, 6: 0.918918918918919, 19: 0.3870967741935484, 24: 0.7582417582417582, 26: 0.9528795811518325, 29: 0.8736842105263158}
Micro-average F1 score: 0.8319672131147541
Weighted-average F1 score: 0.8331077827076988
F1 score per class: {32: 0.7309644670050761, 6: 0.918918918918919, 19: 0.41379310344827586, 24: 0.7582417582417582, 26: 0.9528795811518325, 29: 0.8854166666666666}
Micro-average F1 score: 0.8360655737704918
Weighted-average F1 score: 0.8382305348024144
cur_acc:  ['0.7983']
his_acc:  ['0.7983']
cur_acc des:  ['0.8320']
his_acc des:  ['0.8320']
cur_acc rrf:  ['0.8361']
his_acc rrf:  ['0.8361']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 210.6820529CurrentTrain: epoch  0, batch     1 | loss: 225.1892510CurrentTrain: epoch  0, batch     2 | loss: 257.0226631CurrentTrain: epoch  0, batch     3 | loss: 200.6888616CurrentTrain: epoch  0, batch     4 | loss: 36.7536807CurrentTrain: epoch  1, batch     0 | loss: 246.5962498CurrentTrain: epoch  1, batch     1 | loss: 216.3495412CurrentTrain: epoch  1, batch     2 | loss: 239.1759666CurrentTrain: epoch  1, batch     3 | loss: 196.2937776CurrentTrain: epoch  1, batch     4 | loss: 40.0600775CurrentTrain: epoch  2, batch     0 | loss: 177.2739278CurrentTrain: epoch  2, batch     1 | loss: 284.0607320CurrentTrain: epoch  2, batch     2 | loss: 244.4387731CurrentTrain: epoch  2, batch     3 | loss: 234.1326299CurrentTrain: epoch  2, batch     4 | loss: 31.0537437CurrentTrain: epoch  3, batch     0 | loss: 252.8571830CurrentTrain: epoch  3, batch     1 | loss: 193.9939722CurrentTrain: epoch  3, batch     2 | loss: 250.3797878CurrentTrain: epoch  3, batch     3 | loss: 179.4333227CurrentTrain: epoch  3, batch     4 | loss: 53.5245934CurrentTrain: epoch  4, batch     0 | loss: 259.6739647CurrentTrain: epoch  4, batch     1 | loss: 195.4858483CurrentTrain: epoch  4, batch     2 | loss: 208.7132522CurrentTrain: epoch  4, batch     3 | loss: 206.6044210CurrentTrain: epoch  4, batch     4 | loss: 35.9266080CurrentTrain: epoch  5, batch     0 | loss: 190.3646832CurrentTrain: epoch  5, batch     1 | loss: 191.4458794CurrentTrain: epoch  5, batch     2 | loss: 212.5108903CurrentTrain: epoch  5, batch     3 | loss: 248.5345056CurrentTrain: epoch  5, batch     4 | loss: 52.0687881CurrentTrain: epoch  6, batch     0 | loss: 214.0575724CurrentTrain: epoch  6, batch     1 | loss: 247.8659786CurrentTrain: epoch  6, batch     2 | loss: 178.3491841CurrentTrain: epoch  6, batch     3 | loss: 228.7578497CurrentTrain: epoch  6, batch     4 | loss: 21.4405264CurrentTrain: epoch  7, batch     0 | loss: 204.2998549CurrentTrain: epoch  7, batch     1 | loss: 196.1491310CurrentTrain: epoch  7, batch     2 | loss: 194.4961494CurrentTrain: epoch  7, batch     3 | loss: 288.0609724CurrentTrain: epoch  7, batch     4 | loss: 30.1793067CurrentTrain: epoch  8, batch     0 | loss: 195.1025258CurrentTrain: epoch  8, batch     1 | loss: 237.7518956CurrentTrain: epoch  8, batch     2 | loss: 203.3903000CurrentTrain: epoch  8, batch     3 | loss: 211.3930281CurrentTrain: epoch  8, batch     4 | loss: 51.0957914CurrentTrain: epoch  9, batch     0 | loss: 266.3511702CurrentTrain: epoch  9, batch     1 | loss: 196.8778928CurrentTrain: epoch  9, batch     2 | loss: 237.7463009CurrentTrain: epoch  9, batch     3 | loss: 211.0430815CurrentTrain: epoch  9, batch     4 | loss: 29.4700468
MemoryTrain:  epoch  0, batch     0 | loss: 1.5629278MemoryTrain:  epoch  1, batch     0 | loss: 1.5063490MemoryTrain:  epoch  2, batch     0 | loss: 1.1596001MemoryTrain:  epoch  3, batch     0 | loss: 1.0031795MemoryTrain:  epoch  4, batch     0 | loss: 0.8228149MemoryTrain:  epoch  5, batch     0 | loss: 0.5862079MemoryTrain:  epoch  6, batch     0 | loss: 0.4854698MemoryTrain:  epoch  7, batch     0 | loss: 0.4015844MemoryTrain:  epoch  8, batch     0 | loss: 0.3305042MemoryTrain:  epoch  9, batch     0 | loss: 0.2874434

F1 score per class: {2: 0.875, 6: 0.0, 39: 0.5528455284552846, 11: 0.37398373983739835, 12: 0.0, 19: 0.5333333333333333, 28: 0.13333333333333333}
Micro-average F1 score: 0.46153846153846156
Weighted-average F1 score: 0.4742860772357724
F1 score per class: {2: 0.9411764705882353, 6: 0.0, 39: 0.7285714285714285, 11: 0.6394557823129252, 12: 0.0, 19: 0.0, 24: 0.4, 28: 0.0, 29: 0.13333333333333333}
Micro-average F1 score: 0.6288951841359773
Weighted-average F1 score: 0.6000215011377686
F1 score per class: {2: 0.9411764705882353, 6: 0.0, 39: 0.723404255319149, 11: 0.6301369863013698, 12: 0.0, 19: 0.4, 28: 0.0, 29: 0.13333333333333333}
Micro-average F1 score: 0.6285714285714286
Weighted-average F1 score: 0.6090799389299018

F1 score per class: {32: 0.875, 2: 0.7058823529411765, 6: 0.544, 39: 0.35658914728682173, 11: 0.8723404255319149, 12: 0.3076923076923077, 19: 0.7734806629834254, 24: 0.36363636363636365, 26: 0.93048128342246, 28: 0.8923076923076924, 29: 0.13333333333333333}
Micro-average F1 score: 0.7317073170731707
Weighted-average F1 score: 0.7693457386487487
F1 score per class: {32: 0.9411764705882353, 2: 0.7586206896551724, 6: 0.723404255319149, 39: 0.5987261146496815, 11: 0.8795811518324608, 12: 0.4827586206896552, 19: 0.7692307692307693, 24: 0.3333333333333333, 26: 0.9368421052631579, 28: 0.9081632653061225, 29: 0.13333333333333333}
Micro-average F1 score: 0.7836431226765799
Weighted-average F1 score: 0.7952803276268705
F1 score per class: {32: 0.9411764705882353, 2: 0.7309644670050761, 6: 0.7132867132867133, 39: 0.5859872611464968, 11: 0.8795811518324608, 12: 0.4, 19: 0.7624309392265194, 24: 0.26666666666666666, 26: 0.925531914893617, 28: 0.9035532994923858, 29: 0.13333333333333333}
Micro-average F1 score: 0.7695749440715883
Weighted-average F1 score: 0.7793961550261913
cur_acc:  ['0.7983', '0.4615']
his_acc:  ['0.7983', '0.7317']
cur_acc des:  ['0.8320', '0.6289']
his_acc des:  ['0.8320', '0.7836']
cur_acc rrf:  ['0.8361', '0.6286']
his_acc rrf:  ['0.8361', '0.7696']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 189.1553832CurrentTrain: epoch  0, batch     1 | loss: 197.2811704CurrentTrain: epoch  0, batch     2 | loss: 186.7333968CurrentTrain: epoch  0, batch     3 | loss: 26.3489610CurrentTrain: epoch  1, batch     0 | loss: 206.6460561CurrentTrain: epoch  1, batch     1 | loss: 169.9406934CurrentTrain: epoch  1, batch     2 | loss: 200.3334678CurrentTrain: epoch  1, batch     3 | loss: 20.6172794CurrentTrain: epoch  2, batch     0 | loss: 225.2652686CurrentTrain: epoch  2, batch     1 | loss: 240.3796416CurrentTrain: epoch  2, batch     2 | loss: 134.6746503CurrentTrain: epoch  2, batch     3 | loss: 41.3417346CurrentTrain: epoch  3, batch     0 | loss: 214.4398730CurrentTrain: epoch  3, batch     1 | loss: 180.2637767CurrentTrain: epoch  3, batch     2 | loss: 151.4775854CurrentTrain: epoch  3, batch     3 | loss: 41.2185436CurrentTrain: epoch  4, batch     0 | loss: 229.7559200CurrentTrain: epoch  4, batch     1 | loss: 156.2864410CurrentTrain: epoch  4, batch     2 | loss: 215.8452083CurrentTrain: epoch  4, batch     3 | loss: 11.7422070CurrentTrain: epoch  5, batch     0 | loss: 177.6654783CurrentTrain: epoch  5, batch     1 | loss: 169.4342775CurrentTrain: epoch  5, batch     2 | loss: 196.8785728CurrentTrain: epoch  5, batch     3 | loss: 20.8201299CurrentTrain: epoch  6, batch     0 | loss: 161.1359287CurrentTrain: epoch  6, batch     1 | loss: 203.6641218CurrentTrain: epoch  6, batch     2 | loss: 178.6774439CurrentTrain: epoch  6, batch     3 | loss: 20.5146080CurrentTrain: epoch  7, batch     0 | loss: 268.2438460CurrentTrain: epoch  7, batch     1 | loss: 195.0183083CurrentTrain: epoch  7, batch     2 | loss: 134.9743437CurrentTrain: epoch  7, batch     3 | loss: 41.2714331CurrentTrain: epoch  8, batch     0 | loss: 268.3815320CurrentTrain: epoch  8, batch     1 | loss: 152.8638473CurrentTrain: epoch  8, batch     2 | loss: 175.1206836CurrentTrain: epoch  8, batch     3 | loss: 11.6925860CurrentTrain: epoch  9, batch     0 | loss: 160.0198246CurrentTrain: epoch  9, batch     1 | loss: 211.2527294CurrentTrain: epoch  9, batch     2 | loss: 194.2485071CurrentTrain: epoch  9, batch     3 | loss: 20.3730034
MemoryTrain:  epoch  0, batch     0 | loss: 1.0119869MemoryTrain:  epoch  1, batch     0 | loss: 0.7827781MemoryTrain:  epoch  2, batch     0 | loss: 0.6707591MemoryTrain:  epoch  3, batch     0 | loss: 0.4680099MemoryTrain:  epoch  4, batch     0 | loss: 0.4046126MemoryTrain:  epoch  5, batch     0 | loss: 0.3294122MemoryTrain:  epoch  6, batch     0 | loss: 0.2536657MemoryTrain:  epoch  7, batch     0 | loss: 0.2210844MemoryTrain:  epoch  8, batch     0 | loss: 0.1617217MemoryTrain:  epoch  9, batch     0 | loss: 0.1161077

F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.5, 27: 0.0, 28: 0.0, 31: 0.3614457831325301}
Micro-average F1 score: 0.42790697674418604
Weighted-average F1 score: 0.3343051881375238
F1 score per class: {6: 0.0, 7: 0.0, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 24: 0.0, 26: 0.4444444444444444, 27: 0.0, 28: 0.8, 31: 0.5833333333333334}
Micro-average F1 score: 0.5339366515837104
Weighted-average F1 score: 0.4249099016553662
F1 score per class: {6: 0.0, 7: 0.0, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.5, 27: 0.0, 28: 0.8, 31: 0.5531914893617021}
Micro-average F1 score: 0.5296803652968036
Weighted-average F1 score: 0.4265979299521227

F1 score per class: {32: 0.8, 2: 0.4444444444444444, 6: 0.030303030303030304, 7: 0.9803921568627451, 40: 0.6412213740458015, 39: 0.20512820512820512, 11: 0.7033898305084746, 12: 0.16, 9: 0.7542857142857143, 19: 0.4, 24: 0.38095238095238093, 26: 0.9130434782608695, 27: 0.0, 28: 0.8617021276595744, 29: 0.13333333333333333, 31: 0.32967032967032966}
Micro-average F1 score: 0.6188219363574814
Weighted-average F1 score: 0.6407660697978059
F1 score per class: {32: 0.875, 2: 0.569620253164557, 6: 0.0, 7: 0.9615384615384616, 40: 0.7724137931034483, 9: 0.47368421052631576, 11: 0.7149321266968326, 12: 0.35294117647058826, 39: 0.7640449438202247, 19: 0.34782608695652173, 24: 0.2962962962962963, 26: 0.8888888888888888, 27: 0.6666666666666666, 28: 0.8911917098445595, 29: 0.13333333333333333, 31: 0.4444444444444444}
Micro-average F1 score: 0.6704834605597965
Weighted-average F1 score: 0.6605396671473797
F1 score per class: {32: 0.875, 2: 0.567741935483871, 6: 0.0, 7: 0.9803921568627451, 40: 0.7552447552447552, 39: 0.4429530201342282, 11: 0.7149321266968326, 12: 0.2857142857142857, 9: 0.7640449438202247, 19: 0.3333333333333333, 24: 0.2962962962962963, 26: 0.8888888888888888, 27: 0.8, 28: 0.8911917098445595, 29: 0.13333333333333333, 31: 0.41935483870967744}
Micro-average F1 score: 0.6624040920716112
Weighted-average F1 score: 0.6526909487064293
cur_acc:  ['0.7983', '0.4615', '0.4279']
his_acc:  ['0.7983', '0.7317', '0.6188']
cur_acc des:  ['0.8320', '0.6289', '0.5339']
his_acc des:  ['0.8320', '0.7836', '0.6705']
cur_acc rrf:  ['0.8361', '0.6286', '0.5297']
his_acc rrf:  ['0.8361', '0.7696', '0.6624']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 209.2112891CurrentTrain: epoch  0, batch     1 | loss: 220.6721321CurrentTrain: epoch  0, batch     2 | loss: 206.3643032CurrentTrain: epoch  0, batch     3 | loss: 147.1306293CurrentTrain: epoch  1, batch     0 | loss: 180.2735735CurrentTrain: epoch  1, batch     1 | loss: 304.0096094CurrentTrain: epoch  1, batch     2 | loss: 186.3713178CurrentTrain: epoch  1, batch     3 | loss: 139.4887475CurrentTrain: epoch  2, batch     0 | loss: 222.7743440CurrentTrain: epoch  2, batch     1 | loss: 188.9482294CurrentTrain: epoch  2, batch     2 | loss: 221.9464116CurrentTrain: epoch  2, batch     3 | loss: 132.5540831CurrentTrain: epoch  3, batch     0 | loss: 164.5525686CurrentTrain: epoch  3, batch     1 | loss: 264.1372293CurrentTrain: epoch  3, batch     2 | loss: 224.6325493CurrentTrain: epoch  3, batch     3 | loss: 171.9581465CurrentTrain: epoch  4, batch     0 | loss: 224.8990100CurrentTrain: epoch  4, batch     1 | loss: 172.5737712CurrentTrain: epoch  4, batch     2 | loss: 189.1110615CurrentTrain: epoch  4, batch     3 | loss: 136.6417129CurrentTrain: epoch  5, batch     0 | loss: 194.9748774CurrentTrain: epoch  5, batch     1 | loss: 268.4783715CurrentTrain: epoch  5, batch     2 | loss: 175.3768153CurrentTrain: epoch  5, batch     3 | loss: 142.0060953CurrentTrain: epoch  6, batch     0 | loss: 165.0850588CurrentTrain: epoch  6, batch     1 | loss: 188.3721206CurrentTrain: epoch  6, batch     2 | loss: 233.0796163CurrentTrain: epoch  6, batch     3 | loss: 236.3230250CurrentTrain: epoch  7, batch     0 | loss: 177.4571334CurrentTrain: epoch  7, batch     1 | loss: 222.8248179CurrentTrain: epoch  7, batch     2 | loss: 196.4414153CurrentTrain: epoch  7, batch     3 | loss: 158.9070221CurrentTrain: epoch  8, batch     0 | loss: 195.8659396CurrentTrain: epoch  8, batch     1 | loss: 184.8837540CurrentTrain: epoch  8, batch     2 | loss: 184.5353222CurrentTrain: epoch  8, batch     3 | loss: 177.6478360CurrentTrain: epoch  9, batch     0 | loss: 220.9807780CurrentTrain: epoch  9, batch     1 | loss: 195.7815456CurrentTrain: epoch  9, batch     2 | loss: 204.9965025CurrentTrain: epoch  9, batch     3 | loss: 129.3282264
MemoryTrain:  epoch  0, batch     0 | loss: 0.9941740MemoryTrain:  epoch  1, batch     0 | loss: 0.8343100MemoryTrain:  epoch  2, batch     0 | loss: 0.6646708MemoryTrain:  epoch  3, batch     0 | loss: 0.5727145MemoryTrain:  epoch  4, batch     0 | loss: 0.4411031MemoryTrain:  epoch  5, batch     0 | loss: 0.3538186MemoryTrain:  epoch  6, batch     0 | loss: 0.2995482MemoryTrain:  epoch  7, batch     0 | loss: 0.2654755MemoryTrain:  epoch  8, batch     0 | loss: 0.1778965MemoryTrain:  epoch  9, batch     0 | loss: 0.1493517

F1 score per class: {32: 0.0, 35: 0.9473684210526315, 37: 0.42424242424242425, 38: 0.0, 39: 0.0, 11: 0.631578947368421, 15: 0.6363636363636364, 25: 0.6976744186046512, 27: 0.0}
Micro-average F1 score: 0.594059405940594
Weighted-average F1 score: 0.5739066814738208
F1 score per class: {32: 0.0, 35: 0.0, 37: 0.0, 38: 0.75, 6: 0.0, 39: 0.8181818181818182, 9: 0.0, 11: 0.0, 15: 0.0, 24: 0.0, 25: 0.94, 26: 0.7755102040816326, 27: 0.7111111111111111, 28: 0.0}
Micro-average F1 score: 0.772972972972973
Weighted-average F1 score: 0.719365811233943
F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.7619047619047619, 38: 0.0, 39: 0.0, 11: 0.0, 15: 0.0, 25: 0.9278350515463918, 26: 0.7755102040816326, 27: 0.7659574468085106, 28: 0.0}
Micro-average F1 score: 0.7713498622589532
Weighted-average F1 score: 0.7212879622154512

F1 score per class: {2: 0.875, 6: 0.5241379310344828, 7: 0.03125, 9: 0.9803921568627451, 11: 0.5801526717557252, 12: 0.2222222222222222, 15: 0.8571428571428571, 19: 0.6517857142857143, 24: 0.16, 25: 0.42424242424242425, 26: 0.7613636363636364, 27: 0.3448275862068966, 28: 0.6666666666666666, 29: 0.9247311827956989, 31: 0.0, 32: 0.8465608465608465, 35: 0.6075949367088608, 37: 0.5714285714285714, 38: 0.6, 39: 0.125, 40: 0.2727272727272727}
Micro-average F1 score: 0.6072829131652661
Weighted-average F1 score: 0.6284424669732964
F1 score per class: {2: 0.9411764705882353, 6: 0.6666666666666666, 7: 0.047619047619047616, 9: 0.9615384615384616, 11: 0.7901234567901234, 12: 0.5290322580645161, 15: 0.5454545454545454, 19: 0.7031963470319634, 24: 0.3333333333333333, 25: 0.8181818181818182, 26: 0.7513812154696132, 27: 0.4, 28: 0.5, 29: 0.9130434782608695, 31: 0.8, 32: 0.8465608465608465, 35: 0.8392857142857143, 37: 0.628099173553719, 38: 0.5517241379310345, 39: 0.125, 40: 0.4251968503937008}
Micro-average F1 score: 0.6927651139742319
Weighted-average F1 score: 0.6856844592060009
F1 score per class: {2: 0.9411764705882353, 6: 0.6742857142857143, 7: 0.045454545454545456, 9: 0.9803921568627451, 11: 0.7721518987341772, 12: 0.47297297297297297, 15: 0.5833333333333334, 19: 0.6995515695067265, 24: 0.2857142857142857, 25: 0.7619047619047619, 26: 0.7555555555555555, 27: 0.3783783783783784, 28: 0.47058823529411764, 29: 0.9130434782608695, 31: 0.8, 32: 0.84375, 35: 0.8411214953271028, 37: 0.6129032258064516, 38: 0.5625, 39: 0.125, 40: 0.4098360655737705}
Micro-average F1 score: 0.683
Weighted-average F1 score: 0.6772442422432645
cur_acc:  ['0.7983', '0.4615', '0.4279', '0.5941']
his_acc:  ['0.7983', '0.7317', '0.6188', '0.6073']
cur_acc des:  ['0.8320', '0.6289', '0.5339', '0.7730']
his_acc des:  ['0.8320', '0.7836', '0.6705', '0.6928']
cur_acc rrf:  ['0.8361', '0.6286', '0.5297', '0.7713']
his_acc rrf:  ['0.8361', '0.7696', '0.6624', '0.6830']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 274.1053025CurrentTrain: epoch  0, batch     1 | loss: 249.4434380CurrentTrain: epoch  0, batch     2 | loss: 227.9264328CurrentTrain: epoch  0, batch     3 | loss: 205.2806398CurrentTrain: epoch  0, batch     4 | loss: 116.1714435CurrentTrain: epoch  1, batch     0 | loss: 215.7319298CurrentTrain: epoch  1, batch     1 | loss: 198.9081684CurrentTrain: epoch  1, batch     2 | loss: 202.2838970CurrentTrain: epoch  1, batch     3 | loss: 281.3452505CurrentTrain: epoch  1, batch     4 | loss: 162.7969790CurrentTrain: epoch  2, batch     0 | loss: 279.4357036CurrentTrain: epoch  2, batch     1 | loss: 199.8179583CurrentTrain: epoch  2, batch     2 | loss: 282.2341560CurrentTrain: epoch  2, batch     3 | loss: 178.4564770CurrentTrain: epoch  2, batch     4 | loss: 134.3838361CurrentTrain: epoch  3, batch     0 | loss: 214.4965950CurrentTrain: epoch  3, batch     1 | loss: 216.0600788CurrentTrain: epoch  3, batch     2 | loss: 216.3118269CurrentTrain: epoch  3, batch     3 | loss: 212.7040038CurrentTrain: epoch  3, batch     4 | loss: 135.2772844CurrentTrain: epoch  4, batch     0 | loss: 176.2953150CurrentTrain: epoch  4, batch     1 | loss: 280.5254080CurrentTrain: epoch  4, batch     2 | loss: 215.2910726CurrentTrain: epoch  4, batch     3 | loss: 185.6883060CurrentTrain: epoch  4, batch     4 | loss: 321.1609950CurrentTrain: epoch  5, batch     0 | loss: 222.7493849CurrentTrain: epoch  5, batch     1 | loss: 203.6242183CurrentTrain: epoch  5, batch     2 | loss: 240.3437064CurrentTrain: epoch  5, batch     3 | loss: 189.2893376CurrentTrain: epoch  5, batch     4 | loss: 206.1346246CurrentTrain: epoch  6, batch     0 | loss: 259.0054737CurrentTrain: epoch  6, batch     1 | loss: 221.0171409CurrentTrain: epoch  6, batch     2 | loss: 230.2072436CurrentTrain: epoch  6, batch     3 | loss: 221.8543625CurrentTrain: epoch  6, batch     4 | loss: 135.1193418CurrentTrain: epoch  7, batch     0 | loss: 155.7850337CurrentTrain: epoch  7, batch     1 | loss: 204.1492420CurrentTrain: epoch  7, batch     2 | loss: 277.3626790CurrentTrain: epoch  7, batch     3 | loss: 238.0042890CurrentTrain: epoch  7, batch     4 | loss: 202.9998703CurrentTrain: epoch  8, batch     0 | loss: 266.4743533CurrentTrain: epoch  8, batch     1 | loss: 203.0730217CurrentTrain: epoch  8, batch     2 | loss: 202.8057477CurrentTrain: epoch  8, batch     3 | loss: 220.4405723CurrentTrain: epoch  8, batch     4 | loss: 148.4754176CurrentTrain: epoch  9, batch     0 | loss: 194.0327979CurrentTrain: epoch  9, batch     1 | loss: 228.6857205CurrentTrain: epoch  9, batch     2 | loss: 176.4000987CurrentTrain: epoch  9, batch     3 | loss: 238.0074688CurrentTrain: epoch  9, batch     4 | loss: 202.0482139
MemoryTrain:  epoch  0, batch     0 | loss: 1.0818371MemoryTrain:  epoch  1, batch     0 | loss: 1.0457481MemoryTrain:  epoch  2, batch     0 | loss: 0.8391177MemoryTrain:  epoch  3, batch     0 | loss: 0.6217330MemoryTrain:  epoch  4, batch     0 | loss: 0.6320537MemoryTrain:  epoch  5, batch     0 | loss: 0.5800684MemoryTrain:  epoch  6, batch     0 | loss: 0.4261359MemoryTrain:  epoch  7, batch     0 | loss: 0.3758610MemoryTrain:  epoch  8, batch     0 | loss: 0.3059226MemoryTrain:  epoch  9, batch     0 | loss: 0.2480648

F1 score per class: {32: 0.32786885245901637, 1: 0.2916666666666667, 34: 0.0, 35: 0.058823529411764705, 3: 0.7282608695652174, 37: 0.0, 38: 0.0, 11: 0.0, 14: 0.38235294117647056, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.3778501628664495
Weighted-average F1 score: 0.3548138239128398
F1 score per class: {32: 0.38095238095238093, 1: 0.5614035087719298, 34: 0.0, 35: 0.0, 3: 0.1411764705882353, 37: 0.0, 38: 0.7058823529411765, 9: 0.0, 11: 0.0, 14: 0.0, 15: 0.7777777777777778, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.46726190476190477
Weighted-average F1 score: 0.3923527455028297
F1 score per class: {32: 0.368, 1: 0.5614035087719298, 34: 0.0, 35: 0.14814814814814814, 3: 0.0, 37: 0.7231638418079096, 38: 0.0, 11: 0.0, 14: 0.0, 15: 0.5714285714285714, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.4388059701492537
Weighted-average F1 score: 0.3634335519454256

F1 score per class: {1: 0.2898550724637681, 2: 0.8, 3: 0.27184466019417475, 6: 0.4714285714285714, 7: 0.0, 9: 0.9803921568627451, 11: 0.6380368098159509, 12: 0.1592920353982301, 14: 0.056338028169014086, 15: 0.7777777777777778, 19: 0.5082872928176796, 22: 0.7015706806282722, 24: 0.06896551724137931, 25: 0.4, 26: 0.7624309392265194, 27: 0.11764705882352941, 28: 0.5333333333333333, 29: 0.9130434782608695, 31: 0.0, 32: 0.7932960893854749, 34: 0.29545454545454547, 35: 0.32142857142857145, 37: 0.4791666666666667, 38: 0.5925925925925926, 39: 0.125, 40: 0.16}
Micro-average F1 score: 0.5117071094082588
Weighted-average F1 score: 0.5424099870203892
F1 score per class: {1: 0.32, 2: 0.875, 3: 0.47761194029850745, 6: 0.6035502958579881, 7: 0.0, 9: 0.9259259259259259, 11: 0.5714285714285714, 12: 0.4246575342465753, 14: 0.13636363636363635, 15: 0.5714285714285714, 19: 0.65625, 22: 0.6666666666666666, 24: 0.1, 25: 0.7764705882352941, 26: 0.7540983606557377, 27: 0.0, 28: 0.3333333333333333, 29: 0.8950276243093923, 31: 0.5, 32: 0.8279569892473119, 34: 0.4375, 35: 0.5, 37: 0.5346534653465347, 38: 0.6666666666666666, 39: 0.125, 40: 0.59375}
Micro-average F1 score: 0.5787298008267568
Weighted-average F1 score: 0.5681318660554304
F1 score per class: {1: 0.32167832167832167, 2: 0.875, 3: 0.47761194029850745, 6: 0.5853658536585366, 7: 0.0, 9: 0.9803921568627451, 11: 0.6419753086419753, 12: 0.3308270676691729, 14: 0.14285714285714285, 15: 0.6, 19: 0.63, 22: 0.6808510638297872, 24: 0.10526315789473684, 25: 0.7469879518072289, 26: 0.7582417582417582, 27: 0.1, 28: 0.3333333333333333, 29: 0.8950276243093923, 31: 0.6666666666666666, 32: 0.8216216216216217, 34: 0.352, 35: 0.48226950354609927, 37: 0.6153846153846154, 38: 0.647887323943662, 39: 0.125, 40: 0.5}
Micro-average F1 score: 0.5748957148274555
Weighted-average F1 score: 0.5692145995991543
cur_acc:  ['0.7983', '0.4615', '0.4279', '0.5941', '0.3779']
his_acc:  ['0.7983', '0.7317', '0.6188', '0.6073', '0.5117']
cur_acc des:  ['0.8320', '0.6289', '0.5339', '0.7730', '0.4673']
his_acc des:  ['0.8320', '0.7836', '0.6705', '0.6928', '0.5787']
cur_acc rrf:  ['0.8361', '0.6286', '0.5297', '0.7713', '0.4388']
his_acc rrf:  ['0.8361', '0.7696', '0.6624', '0.6830', '0.5749']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 237.6525220CurrentTrain: epoch  0, batch     1 | loss: 209.3573177CurrentTrain: epoch  0, batch     2 | loss: 200.5664962CurrentTrain: epoch  0, batch     3 | loss: 247.1766329CurrentTrain: epoch  1, batch     0 | loss: 262.5843794CurrentTrain: epoch  1, batch     1 | loss: 216.2700048CurrentTrain: epoch  1, batch     2 | loss: 176.0166967CurrentTrain: epoch  1, batch     3 | loss: 150.3478637CurrentTrain: epoch  2, batch     0 | loss: 198.1629029CurrentTrain: epoch  2, batch     1 | loss: 192.3558792CurrentTrain: epoch  2, batch     2 | loss: 212.4417712CurrentTrain: epoch  2, batch     3 | loss: 186.8431214CurrentTrain: epoch  3, batch     0 | loss: 219.9685828CurrentTrain: epoch  3, batch     1 | loss: 177.8486374CurrentTrain: epoch  3, batch     2 | loss: 226.6365952CurrentTrain: epoch  3, batch     3 | loss: 148.0759372CurrentTrain: epoch  4, batch     0 | loss: 222.6144689CurrentTrain: epoch  4, batch     1 | loss: 202.4159234CurrentTrain: epoch  4, batch     2 | loss: 208.7593183CurrentTrain: epoch  4, batch     3 | loss: 160.2646001CurrentTrain: epoch  5, batch     0 | loss: 211.2964843CurrentTrain: epoch  5, batch     1 | loss: 221.5937252CurrentTrain: epoch  5, batch     2 | loss: 176.1289069CurrentTrain: epoch  5, batch     3 | loss: 293.5491066CurrentTrain: epoch  6, batch     0 | loss: 237.4286644CurrentTrain: epoch  6, batch     1 | loss: 160.2727935CurrentTrain: epoch  6, batch     2 | loss: 268.5392783CurrentTrain: epoch  6, batch     3 | loss: 174.0728819CurrentTrain: epoch  7, batch     0 | loss: 203.0112421CurrentTrain: epoch  7, batch     1 | loss: 186.7641701CurrentTrain: epoch  7, batch     2 | loss: 228.3011300CurrentTrain: epoch  7, batch     3 | loss: 167.3494344CurrentTrain: epoch  8, batch     0 | loss: 213.8089474CurrentTrain: epoch  8, batch     1 | loss: 199.2566903CurrentTrain: epoch  8, batch     2 | loss: 175.9462636CurrentTrain: epoch  8, batch     3 | loss: 180.2571943CurrentTrain: epoch  9, batch     0 | loss: 183.3902965CurrentTrain: epoch  9, batch     1 | loss: 200.3062926CurrentTrain: epoch  9, batch     2 | loss: 202.5056861CurrentTrain: epoch  9, batch     3 | loss: 180.0158974
MemoryTrain:  epoch  0, batch     0 | loss: 1.0732628MemoryTrain:  epoch  1, batch     0 | loss: 0.9734614MemoryTrain:  epoch  2, batch     0 | loss: 0.7008638MemoryTrain:  epoch  3, batch     0 | loss: 0.5612104MemoryTrain:  epoch  4, batch     0 | loss: 0.4277556MemoryTrain:  epoch  5, batch     0 | loss: 0.3879437MemoryTrain:  epoch  6, batch     0 | loss: 0.3304267MemoryTrain:  epoch  7, batch     0 | loss: 0.3002016MemoryTrain:  epoch  8, batch     0 | loss: 0.2619138MemoryTrain:  epoch  9, batch     0 | loss: 0.2130355

F1 score per class: {0: 0.9428571428571428, 32: 0.0, 2: 0.0, 1: 0.8235294117647058, 4: 0.0, 37: 0.5714285714285714, 11: 0.0, 13: 0.5238095238095238, 15: 0.0, 21: 0.825, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.7233009708737864
Weighted-average F1 score: 0.6400578460550886
F1 score per class: {0: 1.0, 1: 0.0, 2: 0.0, 4: 0.8571428571428571, 11: 0.0, 13: 0.75, 14: 0.0, 21: 0.7346938775510204, 22: 0.0, 23: 0.9318181818181818, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.7981651376146789
Weighted-average F1 score: 0.7174293960008244
F1 score per class: {0: 1.0, 1: 0.0, 2: 0.0, 4: 0.8636363636363636, 11: 0.0, 13: 0.75, 14: 0.0, 15: 0.0, 21: 0.7346938775510204, 22: 0.0, 23: 0.8809523809523809, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.7862068965517242
Weighted-average F1 score: 0.7009585652442796

F1 score per class: {0: 0.9295774647887324, 1: 0.2857142857142857, 2: 0.6363636363636364, 3: 0.38181818181818183, 4: 0.8235294117647058, 6: 0.5205479452054794, 7: 0.0, 9: 0.9803921568627451, 11: 0.5487804878048781, 12: 0.16216216216216217, 13: 0.19047619047619047, 14: 0.0, 15: 0.48, 19: 0.6009852216748769, 21: 0.29333333333333333, 22: 0.7134502923976608, 23: 0.7857142857142857, 24: 0.07142857142857142, 25: 0.44776119402985076, 26: 0.7362637362637363, 27: 0.0, 28: 0.0, 29: 0.9139784946236559, 31: 0.5, 32: 0.770949720670391, 34: 0.23529411764705882, 35: 0.31666666666666665, 37: 0.5098039215686274, 38: 0.6666666666666666, 39: 0.0, 40: 0.3132530120481928}
Micro-average F1 score: 0.5461018155927376
Weighted-average F1 score: 0.5653579554215203
F1 score per class: {0: 0.9866666666666667, 1: 0.31901840490797545, 2: 0.4117647058823529, 3: 0.5405405405405406, 4: 0.8571428571428571, 6: 0.6927374301675978, 7: 0.0, 9: 0.9433962264150944, 11: 0.6751592356687898, 12: 0.3546099290780142, 13: 0.2727272727272727, 14: 0.028169014084507043, 15: 0.7058823529411765, 19: 0.6540284360189573, 21: 0.37894736842105264, 22: 0.7272727272727273, 23: 0.8723404255319149, 24: 0.06666666666666667, 25: 0.8181818181818182, 26: 0.7431693989071039, 27: 0.0, 28: 0.18181818181818182, 29: 0.8839779005524862, 31: 1.0, 32: 0.8152173913043478, 34: 0.460431654676259, 35: 0.42748091603053434, 37: 0.5454545454545454, 38: 0.6376811594202898, 39: 0.13333333333333333, 40: 0.5128205128205128}
Micro-average F1 score: 0.61312
Weighted-average F1 score: 0.6066927241910381
F1 score per class: {0: 0.9866666666666667, 1: 0.3270440251572327, 2: 0.4375, 3: 0.5103448275862069, 4: 0.8636363636363636, 6: 0.6742857142857143, 7: 0.0, 9: 0.9433962264150944, 11: 0.6931818181818182, 12: 0.3111111111111111, 13: 0.25, 14: 0.027777777777777776, 15: 0.5714285714285714, 19: 0.6509433962264151, 21: 0.375, 22: 0.7344632768361582, 23: 0.8314606741573034, 24: 0.06896551724137931, 25: 0.7619047619047619, 26: 0.7431693989071039, 27: 0.0, 28: 0.18181818181818182, 29: 0.8901098901098901, 31: 1.0, 32: 0.8172043010752689, 34: 0.3119266055045872, 35: 0.3875968992248062, 37: 0.5409836065573771, 38: 0.6764705882352942, 39: 0.13333333333333333, 40: 0.5087719298245614}
Micro-average F1 score: 0.6033483580167418
Weighted-average F1 score: 0.6001375488470334
cur_acc:  ['0.7983', '0.4615', '0.4279', '0.5941', '0.3779', '0.7233']
his_acc:  ['0.7983', '0.7317', '0.6188', '0.6073', '0.5117', '0.5461']
cur_acc des:  ['0.8320', '0.6289', '0.5339', '0.7730', '0.4673', '0.7982']
his_acc des:  ['0.8320', '0.7836', '0.6705', '0.6928', '0.5787', '0.6131']
cur_acc rrf:  ['0.8361', '0.6286', '0.5297', '0.7713', '0.4388', '0.7862']
his_acc rrf:  ['0.8361', '0.7696', '0.6624', '0.6830', '0.5749', '0.6033']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 225.5903787CurrentTrain: epoch  0, batch     1 | loss: 215.2914515CurrentTrain: epoch  0, batch     2 | loss: 200.3813216CurrentTrain: epoch  0, batch     3 | loss: 137.2710417CurrentTrain: epoch  1, batch     0 | loss: 212.5079537CurrentTrain: epoch  1, batch     1 | loss: 193.7586899CurrentTrain: epoch  1, batch     2 | loss: 177.0864826CurrentTrain: epoch  1, batch     3 | loss: 149.8418432CurrentTrain: epoch  2, batch     0 | loss: 257.0286518CurrentTrain: epoch  2, batch     1 | loss: 183.9417072CurrentTrain: epoch  2, batch     2 | loss: 199.8065555CurrentTrain: epoch  2, batch     3 | loss: 109.0332883CurrentTrain: epoch  3, batch     0 | loss: 197.8240471CurrentTrain: epoch  3, batch     1 | loss: 215.2891265CurrentTrain: epoch  3, batch     2 | loss: 204.0624627CurrentTrain: epoch  3, batch     3 | loss: 106.2919961CurrentTrain: epoch  4, batch     0 | loss: 177.5844264CurrentTrain: epoch  4, batch     1 | loss: 212.4988790CurrentTrain: epoch  4, batch     2 | loss: 191.6118636CurrentTrain: epoch  4, batch     3 | loss: 172.4276820CurrentTrain: epoch  5, batch     0 | loss: 156.9339082CurrentTrain: epoch  5, batch     1 | loss: 238.7225215CurrentTrain: epoch  5, batch     2 | loss: 267.6100710CurrentTrain: epoch  5, batch     3 | loss: 122.3067173CurrentTrain: epoch  6, batch     0 | loss: 177.3533219CurrentTrain: epoch  6, batch     1 | loss: 229.3272880CurrentTrain: epoch  6, batch     2 | loss: 229.8325759CurrentTrain: epoch  6, batch     3 | loss: 104.9599366CurrentTrain: epoch  7, batch     0 | loss: 148.2527548CurrentTrain: epoch  7, batch     1 | loss: 195.4564727CurrentTrain: epoch  7, batch     2 | loss: 202.8253846CurrentTrain: epoch  7, batch     3 | loss: 347.7325465CurrentTrain: epoch  8, batch     0 | loss: 191.2071790CurrentTrain: epoch  8, batch     1 | loss: 267.0758793CurrentTrain: epoch  8, batch     2 | loss: 168.3888773CurrentTrain: epoch  8, batch     3 | loss: 110.5673903CurrentTrain: epoch  9, batch     0 | loss: 159.7995409CurrentTrain: epoch  9, batch     1 | loss: 201.9979199CurrentTrain: epoch  9, batch     2 | loss: 240.0895446CurrentTrain: epoch  9, batch     3 | loss: 208.2891134
MemoryTrain:  epoch  0, batch     0 | loss: 0.6041323MemoryTrain:  epoch  1, batch     0 | loss: 0.4799402MemoryTrain:  epoch  2, batch     0 | loss: 0.4020661MemoryTrain:  epoch  3, batch     0 | loss: 0.3290301MemoryTrain:  epoch  4, batch     0 | loss: 0.2842646MemoryTrain:  epoch  5, batch     0 | loss: 0.2465775MemoryTrain:  epoch  6, batch     0 | loss: 0.2444017MemoryTrain:  epoch  7, batch     0 | loss: 0.1763147MemoryTrain:  epoch  8, batch     0 | loss: 0.1546649MemoryTrain:  epoch  9, batch     0 | loss: 0.1362881

F1 score per class: {33: 0.0, 34: 0.5137614678899083, 36: 0.0, 37: 0.0, 38: 0.9306930693069307, 4: 0.0, 8: 0.0, 12: 0.0, 13: 0.0, 20: 0.9444444444444444, 21: 0.42857142857142855, 26: 0.0, 28: 0.4827586206896552, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.6236559139784946
Weighted-average F1 score: 0.6071425264829488
F1 score per class: {2: 0.0, 3: 0.0, 7: 0.0, 8: 0.7441860465116279, 12: 0.0, 13: 0.0, 20: 0.9514563106796117, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.5333333333333333, 34: 0.0, 35: 0.0, 36: 0.8275862068965517, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.7695852534562212
Weighted-average F1 score: 0.7062485056528746
F1 score per class: {2: 0.0, 4: 0.0, 7: 0.0, 8: 0.704, 12: 0.0, 13: 0.0, 20: 0.9514563106796117, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.5333333333333333, 34: 0.0, 36: 0.7747747747747747, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.7452830188679245
Weighted-average F1 score: 0.6845029582125011

F1 score per class: {0: 0.9295774647887324, 1: 0.2876712328767123, 2: 0.631578947368421, 3: 0.28865979381443296, 4: 0.9081081081081082, 6: 0.5135135135135135, 7: 0.0, 8: 0.4666666666666667, 9: 0.9803921568627451, 11: 0.3548387096774194, 12: 0.10909090909090909, 13: 0.1111111111111111, 14: 0.0, 15: 0.631578947368421, 19: 0.5652173913043478, 20: 0.7580645161290323, 21: 0.2028985507246377, 22: 0.6282051282051282, 23: 0.717948717948718, 24: 0.07692307692307693, 25: 0.3225806451612903, 26: 0.7243243243243244, 27: 0.0, 28: 0.0, 29: 0.9090909090909091, 30: 0.9444444444444444, 31: 0.5, 32: 0.7514450867052023, 33: 0.3, 34: 0.18604651162790697, 35: 0.25263157894736843, 36: 0.4666666666666667, 37: 0.54, 38: 0.5490196078431373, 39: 0.0, 40: 0.20253164556962025}
Micro-average F1 score: 0.5299145299145299
Weighted-average F1 score: 0.5739742890426031
F1 score per class: {0: 0.972972972972973, 1: 0.32051282051282054, 2: 0.4827586206896552, 3: 0.5555555555555556, 4: 0.9130434782608695, 6: 0.6627906976744186, 7: 0.0, 8: 0.6153846153846154, 9: 0.9433962264150944, 11: 0.5179856115107914, 12: 0.2878787878787879, 13: 0.2, 14: 0.028169014084507043, 15: 0.7058823529411765, 19: 0.6113989637305699, 20: 0.784, 21: 0.367816091954023, 22: 0.7167630057803468, 23: 0.8089887640449438, 24: 0.06896551724137931, 25: 0.5555555555555556, 26: 0.7272727272727273, 27: 0.0, 28: 0.16666666666666666, 29: 0.9090909090909091, 30: 0.9, 31: 0.8, 32: 0.8172043010752689, 33: 0.26666666666666666, 34: 0.43103448275862066, 35: 0.3969465648854962, 36: 0.7164179104477612, 37: 0.4854368932038835, 38: 0.6666666666666666, 39: 0.13333333333333333, 40: 0.5785123966942148}
Micro-average F1 score: 0.6089449541284404
Weighted-average F1 score: 0.6118145095937512
F1 score per class: {0: 0.972972972972973, 1: 0.3246753246753247, 2: 0.5384615384615384, 3: 0.5507246376811594, 4: 0.925531914893617, 6: 0.6627218934911243, 7: 0.0, 8: 0.5986394557823129, 9: 0.9433962264150944, 11: 0.527027027027027, 12: 0.2748091603053435, 13: 0.18181818181818182, 14: 0.0, 15: 0.5714285714285714, 19: 0.6122448979591837, 20: 0.7596899224806202, 21: 0.3516483516483517, 22: 0.7167630057803468, 23: 0.7857142857142857, 24: 0.07142857142857142, 25: 0.47058823529411764, 26: 0.7272727272727273, 27: 0.0, 28: 0.18181818181818182, 29: 0.9090909090909091, 30: 0.9230769230769231, 31: 0.8, 32: 0.8191489361702128, 33: 0.26666666666666666, 34: 0.2708333333333333, 35: 0.34375, 36: 0.671875, 37: 0.46017699115044247, 38: 0.6666666666666666, 39: 0.13333333333333333, 40: 0.5641025641025641}
Micro-average F1 score: 0.5969373013579891
Weighted-average F1 score: 0.6027994952256195
cur_acc:  ['0.7983', '0.4615', '0.4279', '0.5941', '0.3779', '0.7233', '0.6237']
his_acc:  ['0.7983', '0.7317', '0.6188', '0.6073', '0.5117', '0.5461', '0.5299']
cur_acc des:  ['0.8320', '0.6289', '0.5339', '0.7730', '0.4673', '0.7982', '0.7696']
his_acc des:  ['0.8320', '0.7836', '0.6705', '0.6928', '0.5787', '0.6131', '0.6089']
cur_acc rrf:  ['0.8361', '0.6286', '0.5297', '0.7713', '0.4388', '0.7862', '0.7453']
his_acc rrf:  ['0.8361', '0.7696', '0.6624', '0.6830', '0.5749', '0.6033', '0.5969']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 212.5224944CurrentTrain: epoch  0, batch     1 | loss: 253.1065173CurrentTrain: epoch  0, batch     2 | loss: 252.5654961CurrentTrain: epoch  0, batch     3 | loss: 244.3040372CurrentTrain: epoch  0, batch     4 | loss: 115.7922089CurrentTrain: epoch  1, batch     0 | loss: 198.5099071CurrentTrain: epoch  1, batch     1 | loss: 212.4765010CurrentTrain: epoch  1, batch     2 | loss: 211.8870125CurrentTrain: epoch  1, batch     3 | loss: 280.5116065CurrentTrain: epoch  1, batch     4 | loss: 172.0571430CurrentTrain: epoch  2, batch     0 | loss: 291.5079593CurrentTrain: epoch  2, batch     1 | loss: 183.8661794CurrentTrain: epoch  2, batch     2 | loss: 214.1633045CurrentTrain: epoch  2, batch     3 | loss: 280.4461693CurrentTrain: epoch  2, batch     4 | loss: 146.2597270CurrentTrain: epoch  3, batch     0 | loss: 363.3503207CurrentTrain: epoch  3, batch     1 | loss: 205.2581163CurrentTrain: epoch  3, batch     2 | loss: 195.3582112CurrentTrain: epoch  3, batch     3 | loss: 188.7633891CurrentTrain: epoch  3, batch     4 | loss: 211.0650927CurrentTrain: epoch  4, batch     0 | loss: 230.8259196CurrentTrain: epoch  4, batch     1 | loss: 237.8122969CurrentTrain: epoch  4, batch     2 | loss: 197.5502188CurrentTrain: epoch  4, batch     3 | loss: 205.1956860CurrentTrain: epoch  4, batch     4 | loss: 151.1861772CurrentTrain: epoch  5, batch     0 | loss: 228.6454418CurrentTrain: epoch  5, batch     1 | loss: 195.9321522CurrentTrain: epoch  5, batch     2 | loss: 187.8105559CurrentTrain: epoch  5, batch     3 | loss: 286.8386129CurrentTrain: epoch  5, batch     4 | loss: 154.1030042CurrentTrain: epoch  6, batch     0 | loss: 247.0660530CurrentTrain: epoch  6, batch     1 | loss: 204.8325920CurrentTrain: epoch  6, batch     2 | loss: 196.4860087CurrentTrain: epoch  6, batch     3 | loss: 193.0329311CurrentTrain: epoch  6, batch     4 | loss: 152.1749850CurrentTrain: epoch  7, batch     0 | loss: 276.3130741CurrentTrain: epoch  7, batch     1 | loss: 202.9106298CurrentTrain: epoch  7, batch     2 | loss: 238.5780471CurrentTrain: epoch  7, batch     3 | loss: 183.4147694CurrentTrain: epoch  7, batch     4 | loss: 125.5698750CurrentTrain: epoch  8, batch     0 | loss: 186.7074432CurrentTrain: epoch  8, batch     1 | loss: 248.7863808CurrentTrain: epoch  8, batch     2 | loss: 228.5964849CurrentTrain: epoch  8, batch     3 | loss: 246.5478394CurrentTrain: epoch  8, batch     4 | loss: 117.1016517CurrentTrain: epoch  9, batch     0 | loss: 227.7294759CurrentTrain: epoch  9, batch     1 | loss: 211.1377743CurrentTrain: epoch  9, batch     2 | loss: 286.9455562CurrentTrain: epoch  9, batch     3 | loss: 202.7924194CurrentTrain: epoch  9, batch     4 | loss: 123.8481145
MemoryTrain:  epoch  0, batch     0 | loss: 0.8892247MemoryTrain:  epoch  1, batch     0 | loss: 0.6688700MemoryTrain:  epoch  2, batch     0 | loss: 0.5213396MemoryTrain:  epoch  3, batch     0 | loss: 0.4134531MemoryTrain:  epoch  4, batch     0 | loss: 0.2993535MemoryTrain:  epoch  5, batch     0 | loss: 0.2540657MemoryTrain:  epoch  6, batch     0 | loss: 0.2282743MemoryTrain:  epoch  7, batch     0 | loss: 0.1853941MemoryTrain:  epoch  8, batch     0 | loss: 0.1445613MemoryTrain:  epoch  9, batch     0 | loss: 0.1426660

F1 score per class: {34: 0.9743589743589743, 5: 0.0, 38: 0.0, 6: 0.0, 8: 0.5714285714285714, 10: 0.0, 7: 0.0, 12: 0.0, 13: 0.8679245283018868, 11: 0.0, 16: 0.6037735849056604, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.71900826446281
Weighted-average F1 score: 0.6973263794744201
F1 score per class: {34: 0.9949748743718593, 36: 0.0, 5: 0.0, 38: 0.6027397260273972, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8461538461538461, 12: 0.8, 13: 0.7868852459016393, 16: 0.0, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7289719626168224
Weighted-average F1 score: 0.6506917779038544
F1 score per class: {34: 0.9949748743718593, 36: 0.0, 5: 0.0, 38: 0.0, 7: 0.6797385620915033, 8: 0.0, 10: 0.0, 6: 0.0, 12: 0.8461538461538461, 13: 0.36363636363636365, 11: 0.7868852459016393, 16: 0.0, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7453183520599251
Weighted-average F1 score: 0.6695990682091488

F1 score per class: {0: 0.8253968253968254, 1: 0.2896551724137931, 2: 0.45454545454545453, 3: 0.26804123711340205, 4: 0.8372093023255814, 5: 0.8715596330275229, 6: 0.30158730158730157, 7: 0.0, 8: 0.3333333333333333, 9: 0.9615384615384616, 10: 0.5405405405405406, 11: 0.34108527131782945, 12: 0.1896551724137931, 13: 0.1, 14: 0.0, 15: 0.631578947368421, 16: 0.8214285714285714, 17: 0.0, 18: 0.3855421686746988, 19: 0.5870646766169154, 20: 0.7107438016528925, 21: 0.2, 22: 0.6375, 23: 0.75, 24: 0.06896551724137931, 25: 0.3225806451612903, 26: 0.7039106145251397, 27: 0.0, 28: 0.0, 29: 0.8913043478260869, 30: 0.9444444444444444, 31: 0.4, 32: 0.7441860465116279, 33: 0.3333333333333333, 34: 0.08571428571428572, 35: 0.24242424242424243, 36: 0.23684210526315788, 37: 0.4782608695652174, 38: 0.28, 39: 0.0, 40: 0.2891566265060241}
Micro-average F1 score: 0.5184127890379675
Weighted-average F1 score: 0.5642936799480557
F1 score per class: {0: 0.9444444444444444, 1: 0.31645569620253167, 2: 0.41379310344827586, 3: 0.593103448275862, 4: 0.9010989010989011, 5: 0.8389830508474576, 6: 0.6162790697674418, 7: 0.0, 8: 0.5802469135802469, 9: 0.9090909090909091, 10: 0.5534591194968553, 11: 0.4307692307692308, 12: 0.3448275862068966, 13: 0.17391304347826086, 14: 0.027777777777777776, 15: 0.75, 16: 0.8301886792452831, 17: 0.6, 18: 0.3037974683544304, 19: 0.6267281105990783, 20: 0.8490566037735849, 21: 0.375, 22: 0.7052023121387283, 23: 0.8478260869565217, 24: 0.07142857142857142, 25: 0.5555555555555556, 26: 0.7182320441988951, 27: 0.0, 28: 0.2857142857142857, 29: 0.8972972972972973, 30: 0.9230769230769231, 31: 0.5714285714285714, 32: 0.8128342245989305, 33: 0.3076923076923077, 34: 0.12987012987012986, 35: 0.4186046511627907, 36: 0.6605504587155964, 37: 0.5904761904761905, 38: 0.5555555555555556, 39: 0.11764705882352941, 40: 0.5}
Micro-average F1 score: 0.5998522531396208
Weighted-average F1 score: 0.5995509839431521
F1 score per class: {0: 0.9444444444444444, 1: 0.3246753246753247, 2: 0.4444444444444444, 3: 0.49612403100775193, 4: 0.8764044943820225, 5: 0.853448275862069, 6: 0.6012269938650306, 7: 0.05, 8: 0.5562913907284768, 9: 0.9090909090909091, 10: 0.5977011494252874, 11: 0.49635036496350365, 12: 0.2978723404255319, 13: 0.17391304347826086, 14: 0.02857142857142857, 15: 0.5454545454545454, 16: 0.7857142857142857, 17: 0.2857142857142857, 18: 0.3356643356643357, 19: 0.6238532110091743, 20: 0.8, 21: 0.3953488372093023, 22: 0.7058823529411765, 23: 0.7857142857142857, 24: 0.07142857142857142, 25: 0.5142857142857142, 26: 0.7182320441988951, 27: 0.0, 28: 0.3333333333333333, 29: 0.8972972972972973, 30: 0.9473684210526315, 31: 0.5, 32: 0.8253968253968254, 33: 0.2962962962962963, 34: 0.10526315789473684, 35: 0.4094488188976378, 36: 0.6355140186915887, 37: 0.5849056603773585, 38: 0.4722222222222222, 39: 0.0, 40: 0.4444444444444444}
Micro-average F1 score: 0.5900497512437811
Weighted-average F1 score: 0.5935618764771525
cur_acc:  ['0.7983', '0.4615', '0.4279', '0.5941', '0.3779', '0.7233', '0.6237', '0.7190']
his_acc:  ['0.7983', '0.7317', '0.6188', '0.6073', '0.5117', '0.5461', '0.5299', '0.5184']
cur_acc des:  ['0.8320', '0.6289', '0.5339', '0.7730', '0.4673', '0.7982', '0.7696', '0.7290']
his_acc des:  ['0.8320', '0.7836', '0.6705', '0.6928', '0.5787', '0.6131', '0.6089', '0.5999']
cur_acc rrf:  ['0.8361', '0.6286', '0.5297', '0.7713', '0.4388', '0.7862', '0.7453', '0.7453']
his_acc rrf:  ['0.8361', '0.7696', '0.6624', '0.6830', '0.5749', '0.6033', '0.5969', '0.5900']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 236.0125107CurrentTrain: epoch  0, batch     1 | loss: 225.9160381CurrentTrain: epoch  0, batch     2 | loss: 187.4629542CurrentTrain: epoch  0, batch     3 | loss: 222.2812959CurrentTrain: epoch  0, batch     4 | loss: 222.5812116CurrentTrain: epoch  0, batch     5 | loss: 227.8977892CurrentTrain: epoch  0, batch     6 | loss: 227.4900340CurrentTrain: epoch  0, batch     7 | loss: 221.3200610CurrentTrain: epoch  0, batch     8 | loss: 234.4045434CurrentTrain: epoch  0, batch     9 | loss: 227.4073234CurrentTrain: epoch  0, batch    10 | loss: 247.1182593CurrentTrain: epoch  0, batch    11 | loss: 296.9874363CurrentTrain: epoch  0, batch    12 | loss: 193.0192866CurrentTrain: epoch  0, batch    13 | loss: 218.7527114CurrentTrain: epoch  0, batch    14 | loss: 195.3985612CurrentTrain: epoch  0, batch    15 | loss: 238.0667026CurrentTrain: epoch  0, batch    16 | loss: 212.2925914CurrentTrain: epoch  0, batch    17 | loss: 212.0847661CurrentTrain: epoch  0, batch    18 | loss: 196.2616039CurrentTrain: epoch  0, batch    19 | loss: 242.8363988CurrentTrain: epoch  0, batch    20 | loss: 251.0987068CurrentTrain: epoch  0, batch    21 | loss: 243.1877867CurrentTrain: epoch  0, batch    22 | loss: 186.4062633CurrentTrain: epoch  0, batch    23 | loss: 192.5216024CurrentTrain: epoch  0, batch    24 | loss: 154.1500658CurrentTrain: epoch  0, batch    25 | loss: 203.6374229CurrentTrain: epoch  0, batch    26 | loss: 178.5171296CurrentTrain: epoch  0, batch    27 | loss: 188.2948113CurrentTrain: epoch  0, batch    28 | loss: 231.7556727CurrentTrain: epoch  0, batch    29 | loss: 217.6488682CurrentTrain: epoch  0, batch    30 | loss: 280.5741871CurrentTrain: epoch  0, batch    31 | loss: 216.8546785CurrentTrain: epoch  0, batch    32 | loss: 288.6399978CurrentTrain: epoch  0, batch    33 | loss: 198.3952492CurrentTrain: epoch  0, batch    34 | loss: 256.4796917CurrentTrain: epoch  0, batch    35 | loss: 256.7612401CurrentTrain: epoch  0, batch    36 | loss: 187.5979162CurrentTrain: epoch  0, batch    37 | loss: 191.8364372CurrentTrain: epoch  0, batch    38 | loss: 223.2829732CurrentTrain: epoch  0, batch    39 | loss: 295.0221190CurrentTrain: epoch  0, batch    40 | loss: 223.0439378CurrentTrain: epoch  0, batch    41 | loss: 204.1436423CurrentTrain: epoch  0, batch    42 | loss: 211.5530110CurrentTrain: epoch  0, batch    43 | loss: 217.2458208CurrentTrain: epoch  0, batch    44 | loss: 216.2913079CurrentTrain: epoch  0, batch    45 | loss: 203.9374205CurrentTrain: epoch  0, batch    46 | loss: 196.0454671CurrentTrain: epoch  0, batch    47 | loss: 223.3616023CurrentTrain: epoch  0, batch    48 | loss: 295.0923791CurrentTrain: epoch  0, batch    49 | loss: 235.9616435CurrentTrain: epoch  0, batch    50 | loss: 191.2209913CurrentTrain: epoch  0, batch    51 | loss: 203.9665174CurrentTrain: epoch  0, batch    52 | loss: 204.7409134CurrentTrain: epoch  0, batch    53 | loss: 190.2137336CurrentTrain: epoch  0, batch    54 | loss: 190.5343976CurrentTrain: epoch  0, batch    55 | loss: 228.6287199CurrentTrain: epoch  0, batch    56 | loss: 216.5651766CurrentTrain: epoch  0, batch    57 | loss: 203.2067827CurrentTrain: epoch  0, batch    58 | loss: 216.3536186CurrentTrain: epoch  0, batch    59 | loss: 241.8110322CurrentTrain: epoch  0, batch    60 | loss: 191.0943983CurrentTrain: epoch  0, batch    61 | loss: 216.6226586CurrentTrain: epoch  0, batch    62 | loss: 173.9790940CurrentTrain: epoch  0, batch    63 | loss: 195.9209498CurrentTrain: epoch  0, batch    64 | loss: 295.6346631CurrentTrain: epoch  0, batch    65 | loss: 204.1022031CurrentTrain: epoch  0, batch    66 | loss: 202.0839635CurrentTrain: epoch  0, batch    67 | loss: 157.9013498CurrentTrain: epoch  0, batch    68 | loss: 223.0496244CurrentTrain: epoch  0, batch    69 | loss: 287.0536308CurrentTrain: epoch  0, batch    70 | loss: 189.1607225CurrentTrain: epoch  0, batch    71 | loss: 215.2432052CurrentTrain: epoch  0, batch    72 | loss: 273.0885301CurrentTrain: epoch  0, batch    73 | loss: 189.4720386CurrentTrain: epoch  0, batch    74 | loss: 286.6606608CurrentTrain: epoch  0, batch    75 | loss: 195.2923881CurrentTrain: epoch  0, batch    76 | loss: 208.2594930CurrentTrain: epoch  0, batch    77 | loss: 209.3698016CurrentTrain: epoch  0, batch    78 | loss: 364.5041681CurrentTrain: epoch  0, batch    79 | loss: 214.9700973CurrentTrain: epoch  0, batch    80 | loss: 256.1554660CurrentTrain: epoch  0, batch    81 | loss: 208.2523477CurrentTrain: epoch  0, batch    82 | loss: 198.1016535CurrentTrain: epoch  0, batch    83 | loss: 287.2216435CurrentTrain: epoch  0, batch    84 | loss: 241.7018590CurrentTrain: epoch  0, batch    85 | loss: 229.0649380CurrentTrain: epoch  0, batch    86 | loss: 241.0449116CurrentTrain: epoch  0, batch    87 | loss: 241.7158251CurrentTrain: epoch  0, batch    88 | loss: 214.8651245CurrentTrain: epoch  0, batch    89 | loss: 248.6290627CurrentTrain: epoch  0, batch    90 | loss: 254.3895345CurrentTrain: epoch  0, batch    91 | loss: 226.9349702CurrentTrain: epoch  0, batch    92 | loss: 202.7859275CurrentTrain: epoch  0, batch    93 | loss: 171.8193166CurrentTrain: epoch  0, batch    94 | loss: 208.6696192CurrentTrain: epoch  0, batch    95 | loss: 165.2199557CurrentTrain: epoch  1, batch     0 | loss: 181.2838158CurrentTrain: epoch  1, batch     1 | loss: 285.7435749CurrentTrain: epoch  1, batch     2 | loss: 206.4142508CurrentTrain: epoch  1, batch     3 | loss: 215.3425125CurrentTrain: epoch  1, batch     4 | loss: 194.4268617CurrentTrain: epoch  1, batch     5 | loss: 202.1412567CurrentTrain: epoch  1, batch     6 | loss: 239.4217426CurrentTrain: epoch  1, batch     7 | loss: 277.5533281CurrentTrain: epoch  1, batch     8 | loss: 225.3229830CurrentTrain: epoch  1, batch     9 | loss: 193.0383996CurrentTrain: epoch  1, batch    10 | loss: 172.5955890CurrentTrain: epoch  1, batch    11 | loss: 221.0562315CurrentTrain: epoch  1, batch    12 | loss: 277.0908634CurrentTrain: epoch  1, batch    13 | loss: 285.6728469CurrentTrain: epoch  1, batch    14 | loss: 201.6461716CurrentTrain: epoch  1, batch    15 | loss: 165.2382996CurrentTrain: epoch  1, batch    16 | loss: 219.3654731CurrentTrain: epoch  1, batch    17 | loss: 212.1819745CurrentTrain: epoch  1, batch    18 | loss: 246.8092326CurrentTrain: epoch  1, batch    19 | loss: 291.1348516CurrentTrain: epoch  1, batch    20 | loss: 192.6756418CurrentTrain: epoch  1, batch    21 | loss: 226.5453761CurrentTrain: epoch  1, batch    22 | loss: 178.6259131CurrentTrain: epoch  1, batch    23 | loss: 180.0848724CurrentTrain: epoch  1, batch    24 | loss: 247.3835544CurrentTrain: epoch  1, batch    25 | loss: 252.7626615CurrentTrain: epoch  1, batch    26 | loss: 236.5065922CurrentTrain: epoch  1, batch    27 | loss: 170.5494902CurrentTrain: epoch  1, batch    28 | loss: 230.2834313CurrentTrain: epoch  1, batch    29 | loss: 219.5437712CurrentTrain: epoch  1, batch    30 | loss: 217.7099743CurrentTrain: epoch  1, batch    31 | loss: 229.6510869CurrentTrain: epoch  1, batch    32 | loss: 237.5067579CurrentTrain: epoch  1, batch    33 | loss: 170.7163793CurrentTrain: epoch  1, batch    34 | loss: 245.0939569CurrentTrain: epoch  1, batch    35 | loss: 200.2801281CurrentTrain: epoch  1, batch    36 | loss: 218.7638296CurrentTrain: epoch  1, batch    37 | loss: 219.3035650CurrentTrain: epoch  1, batch    38 | loss: 191.1048638CurrentTrain: epoch  1, batch    39 | loss: 244.4632372CurrentTrain: epoch  1, batch    40 | loss: 187.1546277CurrentTrain: epoch  1, batch    41 | loss: 205.5376153CurrentTrain: epoch  1, batch    42 | loss: 163.7022357CurrentTrain: epoch  1, batch    43 | loss: 213.6014770CurrentTrain: epoch  1, batch    44 | loss: 201.9670029CurrentTrain: epoch  1, batch    45 | loss: 191.1411764CurrentTrain: epoch  1, batch    46 | loss: 176.4213772CurrentTrain: epoch  1, batch    47 | loss: 184.4385770CurrentTrain: epoch  1, batch    48 | loss: 174.3623267CurrentTrain: epoch  1, batch    49 | loss: 191.1354106CurrentTrain: epoch  1, batch    50 | loss: 353.1645586CurrentTrain: epoch  1, batch    51 | loss: 228.4986238CurrentTrain: epoch  1, batch    52 | loss: 251.1077713CurrentTrain: epoch  1, batch    53 | loss: 191.8493560CurrentTrain: epoch  1, batch    54 | loss: 185.0591182CurrentTrain: epoch  1, batch    55 | loss: 191.7034138CurrentTrain: epoch  1, batch    56 | loss: 243.7677614CurrentTrain: epoch  1, batch    57 | loss: 275.6845950CurrentTrain: epoch  1, batch    58 | loss: 291.8295726CurrentTrain: epoch  1, batch    59 | loss: 280.3770645CurrentTrain: epoch  1, batch    60 | loss: 218.2846933CurrentTrain: epoch  1, batch    61 | loss: 193.1848231CurrentTrain: epoch  1, batch    62 | loss: 173.8991243CurrentTrain: epoch  1, batch    63 | loss: 220.6455093CurrentTrain: epoch  1, batch    64 | loss: 224.2666147CurrentTrain: epoch  1, batch    65 | loss: 199.3070149CurrentTrain: epoch  1, batch    66 | loss: 175.1842770CurrentTrain: epoch  1, batch    67 | loss: 175.5870915CurrentTrain: epoch  1, batch    68 | loss: 200.6518116CurrentTrain: epoch  1, batch    69 | loss: 294.7045943CurrentTrain: epoch  1, batch    70 | loss: 199.5404831CurrentTrain: epoch  1, batch    71 | loss: 208.0365937CurrentTrain: epoch  1, batch    72 | loss: 191.4964478CurrentTrain: epoch  1, batch    73 | loss: 230.8428796CurrentTrain: epoch  1, batch    74 | loss: 214.8358892CurrentTrain: epoch  1, batch    75 | loss: 166.3477334CurrentTrain: epoch  1, batch    76 | loss: 182.8078203CurrentTrain: epoch  1, batch    77 | loss: 223.8737461CurrentTrain: epoch  1, batch    78 | loss: 238.1140691CurrentTrain: epoch  1, batch    79 | loss: 244.6269964CurrentTrain: epoch  1, batch    80 | loss: 161.5896719CurrentTrain: epoch  1, batch    81 | loss: 196.5761844CurrentTrain: epoch  1, batch    82 | loss: 226.8689432CurrentTrain: epoch  1, batch    83 | loss: 214.0544120CurrentTrain: epoch  1, batch    84 | loss: 273.5721423CurrentTrain: epoch  1, batch    85 | loss: 198.6619893CurrentTrain: epoch  1, batch    86 | loss: 203.9289243CurrentTrain: epoch  1, batch    87 | loss: 226.7702752CurrentTrain: epoch  1, batch    88 | loss: 247.0045891CurrentTrain: epoch  1, batch    89 | loss: 177.6800683CurrentTrain: epoch  1, batch    90 | loss: 282.1402680CurrentTrain: epoch  1, batch    91 | loss: 196.3924020CurrentTrain: epoch  1, batch    92 | loss: 144.7498319CurrentTrain: epoch  1, batch    93 | loss: 201.7712504CurrentTrain: epoch  1, batch    94 | loss: 198.8934477CurrentTrain: epoch  1, batch    95 | loss: 164.4809530CurrentTrain: epoch  2, batch     0 | loss: 215.2562262CurrentTrain: epoch  2, batch     1 | loss: 216.2321506CurrentTrain: epoch  2, batch     2 | loss: 181.7434560CurrentTrain: epoch  2, batch     3 | loss: 243.8995990CurrentTrain: epoch  2, batch     4 | loss: 233.5519523CurrentTrain: epoch  2, batch     5 | loss: 179.2344978CurrentTrain: epoch  2, batch     6 | loss: 232.6128453CurrentTrain: epoch  2, batch     7 | loss: 158.0978750CurrentTrain: epoch  2, batch     8 | loss: 224.2275145CurrentTrain: epoch  2, batch     9 | loss: 191.0596247CurrentTrain: epoch  2, batch    10 | loss: 225.9626373CurrentTrain: epoch  2, batch    11 | loss: 200.3894941CurrentTrain: epoch  2, batch    12 | loss: 204.1694735CurrentTrain: epoch  2, batch    13 | loss: 223.9141793CurrentTrain: epoch  2, batch    14 | loss: 366.6581219CurrentTrain: epoch  2, batch    15 | loss: 196.1214277CurrentTrain: epoch  2, batch    16 | loss: 216.5923944CurrentTrain: epoch  2, batch    17 | loss: 199.1796849CurrentTrain: epoch  2, batch    18 | loss: 225.4325135CurrentTrain: epoch  2, batch    19 | loss: 171.6490471CurrentTrain: epoch  2, batch    20 | loss: 189.4856475CurrentTrain: epoch  2, batch    21 | loss: 207.8527117CurrentTrain: epoch  2, batch    22 | loss: 190.3676893CurrentTrain: epoch  2, batch    23 | loss: 279.3210980CurrentTrain: epoch  2, batch    24 | loss: 275.2715459CurrentTrain: epoch  2, batch    25 | loss: 280.3144118CurrentTrain: epoch  2, batch    26 | loss: 218.6542019CurrentTrain: epoch  2, batch    27 | loss: 229.8758570CurrentTrain: epoch  2, batch    28 | loss: 231.6266150CurrentTrain: epoch  2, batch    29 | loss: 215.4045777CurrentTrain: epoch  2, batch    30 | loss: 175.8190215CurrentTrain: epoch  2, batch    31 | loss: 278.3089068CurrentTrain: epoch  2, batch    32 | loss: 197.1371970CurrentTrain: epoch  2, batch    33 | loss: 209.4997123CurrentTrain: epoch  2, batch    34 | loss: 196.3086020CurrentTrain: epoch  2, batch    35 | loss: 175.5177775CurrentTrain: epoch  2, batch    36 | loss: 238.8142202CurrentTrain: epoch  2, batch    37 | loss: 163.0398591CurrentTrain: epoch  2, batch    38 | loss: 226.2556728CurrentTrain: epoch  2, batch    39 | loss: 242.6110936CurrentTrain: epoch  2, batch    40 | loss: 206.2471562CurrentTrain: epoch  2, batch    41 | loss: 194.6049952CurrentTrain: epoch  2, batch    42 | loss: 263.0415167CurrentTrain: epoch  2, batch    43 | loss: 192.4285297CurrentTrain: epoch  2, batch    44 | loss: 224.0698763CurrentTrain: epoch  2, batch    45 | loss: 281.6984264CurrentTrain: epoch  2, batch    46 | loss: 183.9106999CurrentTrain: epoch  2, batch    47 | loss: 166.8482912CurrentTrain: epoch  2, batch    48 | loss: 193.1662672CurrentTrain: epoch  2, batch    49 | loss: 250.9828140CurrentTrain: epoch  2, batch    50 | loss: 214.7232589CurrentTrain: epoch  2, batch    51 | loss: 187.0585501CurrentTrain: epoch  2, batch    52 | loss: 197.2026965CurrentTrain: epoch  2, batch    53 | loss: 221.6680130CurrentTrain: epoch  2, batch    54 | loss: 228.9542781CurrentTrain: epoch  2, batch    55 | loss: 223.8730593CurrentTrain: epoch  2, batch    56 | loss: 181.9599862CurrentTrain: epoch  2, batch    57 | loss: 223.9466225CurrentTrain: epoch  2, batch    58 | loss: 173.7728611CurrentTrain: epoch  2, batch    59 | loss: 194.7156039CurrentTrain: epoch  2, batch    60 | loss: 197.2272646CurrentTrain: epoch  2, batch    61 | loss: 181.5739047CurrentTrain: epoch  2, batch    62 | loss: 149.8501374CurrentTrain: epoch  2, batch    63 | loss: 152.1326734CurrentTrain: epoch  2, batch    64 | loss: 222.5778995CurrentTrain: epoch  2, batch    65 | loss: 194.5336714CurrentTrain: epoch  2, batch    66 | loss: 178.5106321CurrentTrain: epoch  2, batch    67 | loss: 224.6109868CurrentTrain: epoch  2, batch    68 | loss: 269.7906150CurrentTrain: epoch  2, batch    69 | loss: 185.2677926CurrentTrain: epoch  2, batch    70 | loss: 181.6632284CurrentTrain: epoch  2, batch    71 | loss: 197.3183143CurrentTrain: epoch  2, batch    72 | loss: 194.3134011CurrentTrain: epoch  2, batch    73 | loss: 197.2264405CurrentTrain: epoch  2, batch    74 | loss: 199.6548878CurrentTrain: epoch  2, batch    75 | loss: 248.8287321CurrentTrain: epoch  2, batch    76 | loss: 172.4047353CurrentTrain: epoch  2, batch    77 | loss: 199.6482881CurrentTrain: epoch  2, batch    78 | loss: 159.8298795CurrentTrain: epoch  2, batch    79 | loss: 169.0892118CurrentTrain: epoch  2, batch    80 | loss: 231.0791608CurrentTrain: epoch  2, batch    81 | loss: 251.5457383CurrentTrain: epoch  2, batch    82 | loss: 169.0753356CurrentTrain: epoch  2, batch    83 | loss: 169.8049383CurrentTrain: epoch  2, batch    84 | loss: 189.8842306CurrentTrain: epoch  2, batch    85 | loss: 209.4805367CurrentTrain: epoch  2, batch    86 | loss: 222.8780950CurrentTrain: epoch  2, batch    87 | loss: 190.3632432CurrentTrain: epoch  2, batch    88 | loss: 249.0822210CurrentTrain: epoch  2, batch    89 | loss: 293.1098197CurrentTrain: epoch  2, batch    90 | loss: 224.5803134CurrentTrain: epoch  2, batch    91 | loss: 281.1416932CurrentTrain: epoch  2, batch    92 | loss: 288.0058363CurrentTrain: epoch  2, batch    93 | loss: 203.4455354CurrentTrain: epoch  2, batch    94 | loss: 207.9588964CurrentTrain: epoch  2, batch    95 | loss: 158.5874986CurrentTrain: epoch  3, batch     0 | loss: 212.3427351CurrentTrain: epoch  3, batch     1 | loss: 186.6582709CurrentTrain: epoch  3, batch     2 | loss: 250.2460183CurrentTrain: epoch  3, batch     3 | loss: 198.0511627CurrentTrain: epoch  3, batch     4 | loss: 178.3413693CurrentTrain: epoch  3, batch     5 | loss: 199.4275845CurrentTrain: epoch  3, batch     6 | loss: 233.4487267CurrentTrain: epoch  3, batch     7 | loss: 195.9761521CurrentTrain: epoch  3, batch     8 | loss: 174.6697833CurrentTrain: epoch  3, batch     9 | loss: 177.3482631CurrentTrain: epoch  3, batch    10 | loss: 189.3817469CurrentTrain: epoch  3, batch    11 | loss: 241.1538378CurrentTrain: epoch  3, batch    12 | loss: 204.0554016CurrentTrain: epoch  3, batch    13 | loss: 208.4692728CurrentTrain: epoch  3, batch    14 | loss: 223.1961398CurrentTrain: epoch  3, batch    15 | loss: 180.4638545CurrentTrain: epoch  3, batch    16 | loss: 216.8682415CurrentTrain: epoch  3, batch    17 | loss: 197.8870382CurrentTrain: epoch  3, batch    18 | loss: 214.2763483CurrentTrain: epoch  3, batch    19 | loss: 241.1264477CurrentTrain: epoch  3, batch    20 | loss: 171.0812777CurrentTrain: epoch  3, batch    21 | loss: 271.9403209CurrentTrain: epoch  3, batch    22 | loss: 186.4908161CurrentTrain: epoch  3, batch    23 | loss: 171.3692887CurrentTrain: epoch  3, batch    24 | loss: 226.4091397CurrentTrain: epoch  3, batch    25 | loss: 141.5153637CurrentTrain: epoch  3, batch    26 | loss: 239.8043284CurrentTrain: epoch  3, batch    27 | loss: 293.5620586CurrentTrain: epoch  3, batch    28 | loss: 213.0859873CurrentTrain: epoch  3, batch    29 | loss: 277.4926994CurrentTrain: epoch  3, batch    30 | loss: 206.9768745CurrentTrain: epoch  3, batch    31 | loss: 206.2301296CurrentTrain: epoch  3, batch    32 | loss: 208.4184410CurrentTrain: epoch  3, batch    33 | loss: 172.3234152CurrentTrain: epoch  3, batch    34 | loss: 220.6431840CurrentTrain: epoch  3, batch    35 | loss: 244.6532106CurrentTrain: epoch  3, batch    36 | loss: 177.5738835CurrentTrain: epoch  3, batch    37 | loss: 241.7546353CurrentTrain: epoch  3, batch    38 | loss: 209.8041383CurrentTrain: epoch  3, batch    39 | loss: 216.9801040CurrentTrain: epoch  3, batch    40 | loss: 192.9311341CurrentTrain: epoch  3, batch    41 | loss: 247.1427584CurrentTrain: epoch  3, batch    42 | loss: 247.4275006CurrentTrain: epoch  3, batch    43 | loss: 242.6533160CurrentTrain: epoch  3, batch    44 | loss: 194.6798758CurrentTrain: epoch  3, batch    45 | loss: 170.5691686CurrentTrain: epoch  3, batch    46 | loss: 263.2615136CurrentTrain: epoch  3, batch    47 | loss: 221.3549292CurrentTrain: epoch  3, batch    48 | loss: 196.0619553CurrentTrain: epoch  3, batch    49 | loss: 192.7523081CurrentTrain: epoch  3, batch    50 | loss: 250.3678323CurrentTrain: epoch  3, batch    51 | loss: 156.1394241CurrentTrain: epoch  3, batch    52 | loss: 238.7743072CurrentTrain: epoch  3, batch    53 | loss: 230.4120358CurrentTrain: epoch  3, batch    54 | loss: 229.6732518CurrentTrain: epoch  3, batch    55 | loss: 186.4039773CurrentTrain: epoch  3, batch    56 | loss: 261.7539640CurrentTrain: epoch  3, batch    57 | loss: 196.8849478CurrentTrain: epoch  3, batch    58 | loss: 163.5685961CurrentTrain: epoch  3, batch    59 | loss: 212.4861229CurrentTrain: epoch  3, batch    60 | loss: 185.7420676CurrentTrain: epoch  3, batch    61 | loss: 197.8141248CurrentTrain: epoch  3, batch    62 | loss: 225.3591569CurrentTrain: epoch  3, batch    63 | loss: 230.5989406CurrentTrain: epoch  3, batch    64 | loss: 224.7349758CurrentTrain: epoch  3, batch    65 | loss: 197.7638957CurrentTrain: epoch  3, batch    66 | loss: 172.4075956CurrentTrain: epoch  3, batch    67 | loss: 156.8938356CurrentTrain: epoch  3, batch    68 | loss: 237.8804076CurrentTrain: epoch  3, batch    69 | loss: 247.8507817CurrentTrain: epoch  3, batch    70 | loss: 223.7139855CurrentTrain: epoch  3, batch    71 | loss: 211.9055064CurrentTrain: epoch  3, batch    72 | loss: 172.6012412CurrentTrain: epoch  3, batch    73 | loss: 230.4501841CurrentTrain: epoch  3, batch    74 | loss: 213.4300888CurrentTrain: epoch  3, batch    75 | loss: 184.1789164CurrentTrain: epoch  3, batch    76 | loss: 188.8334818CurrentTrain: epoch  3, batch    77 | loss: 247.8816643CurrentTrain: epoch  3, batch    78 | loss: 222.2827308CurrentTrain: epoch  3, batch    79 | loss: 237.5708505CurrentTrain: epoch  3, batch    80 | loss: 159.2426406CurrentTrain: epoch  3, batch    81 | loss: 174.6347127CurrentTrain: epoch  3, batch    82 | loss: 239.9462286CurrentTrain: epoch  3, batch    83 | loss: 163.0003327CurrentTrain: epoch  3, batch    84 | loss: 237.9695118CurrentTrain: epoch  3, batch    85 | loss: 166.8315577CurrentTrain: epoch  3, batch    86 | loss: 238.4264453CurrentTrain: epoch  3, batch    87 | loss: 154.7244613CurrentTrain: epoch  3, batch    88 | loss: 186.5035212CurrentTrain: epoch  3, batch    89 | loss: 214.6508379CurrentTrain: epoch  3, batch    90 | loss: 205.8712048CurrentTrain: epoch  3, batch    91 | loss: 216.3949614CurrentTrain: epoch  3, batch    92 | loss: 230.3006016CurrentTrain: epoch  3, batch    93 | loss: 195.2598512CurrentTrain: epoch  3, batch    94 | loss: 171.8712937CurrentTrain: epoch  3, batch    95 | loss: 152.2503132CurrentTrain: epoch  4, batch     0 | loss: 270.8933316CurrentTrain: epoch  4, batch     1 | loss: 205.5422301CurrentTrain: epoch  4, batch     2 | loss: 164.1833896CurrentTrain: epoch  4, batch     3 | loss: 201.4152156CurrentTrain: epoch  4, batch     4 | loss: 203.6209100CurrentTrain: epoch  4, batch     5 | loss: 234.1573809CurrentTrain: epoch  4, batch     6 | loss: 204.0029563CurrentTrain: epoch  4, batch     7 | loss: 154.2779551CurrentTrain: epoch  4, batch     8 | loss: 241.9609073CurrentTrain: epoch  4, batch     9 | loss: 160.3010968CurrentTrain: epoch  4, batch    10 | loss: 222.5783323CurrentTrain: epoch  4, batch    11 | loss: 238.3733246CurrentTrain: epoch  4, batch    12 | loss: 164.0713295CurrentTrain: epoch  4, batch    13 | loss: 194.3117204CurrentTrain: epoch  4, batch    14 | loss: 221.7640927CurrentTrain: epoch  4, batch    15 | loss: 185.0728927CurrentTrain: epoch  4, batch    16 | loss: 200.5858547CurrentTrain: epoch  4, batch    17 | loss: 197.8907721CurrentTrain: epoch  4, batch    18 | loss: 180.9415692CurrentTrain: epoch  4, batch    19 | loss: 229.4621474CurrentTrain: epoch  4, batch    20 | loss: 212.8004640CurrentTrain: epoch  4, batch    21 | loss: 247.3031270CurrentTrain: epoch  4, batch    22 | loss: 220.7584625CurrentTrain: epoch  4, batch    23 | loss: 360.8033592CurrentTrain: epoch  4, batch    24 | loss: 359.8331258CurrentTrain: epoch  4, batch    25 | loss: 157.2861026CurrentTrain: epoch  4, batch    26 | loss: 232.2329482CurrentTrain: epoch  4, batch    27 | loss: 151.9482240CurrentTrain: epoch  4, batch    28 | loss: 203.4790917CurrentTrain: epoch  4, batch    29 | loss: 233.7398875CurrentTrain: epoch  4, batch    30 | loss: 154.4603912CurrentTrain: epoch  4, batch    31 | loss: 199.8529868CurrentTrain: epoch  4, batch    32 | loss: 170.3638140CurrentTrain: epoch  4, batch    33 | loss: 238.1512463CurrentTrain: epoch  4, batch    34 | loss: 193.5202718CurrentTrain: epoch  4, batch    35 | loss: 157.0759109CurrentTrain: epoch  4, batch    36 | loss: 212.7019062CurrentTrain: epoch  4, batch    37 | loss: 196.1682647CurrentTrain: epoch  4, batch    38 | loss: 212.7904713CurrentTrain: epoch  4, batch    39 | loss: 206.3213027CurrentTrain: epoch  4, batch    40 | loss: 229.7175096CurrentTrain: epoch  4, batch    41 | loss: 204.1290704CurrentTrain: epoch  4, batch    42 | loss: 238.0408778CurrentTrain: epoch  4, batch    43 | loss: 137.2232650CurrentTrain: epoch  4, batch    44 | loss: 187.0880083CurrentTrain: epoch  4, batch    45 | loss: 239.2879839CurrentTrain: epoch  4, batch    46 | loss: 177.2356961CurrentTrain: epoch  4, batch    47 | loss: 168.3991840CurrentTrain: epoch  4, batch    48 | loss: 220.5720665CurrentTrain: epoch  4, batch    49 | loss: 292.5183534CurrentTrain: epoch  4, batch    50 | loss: 201.9291143CurrentTrain: epoch  4, batch    51 | loss: 188.8288274CurrentTrain: epoch  4, batch    52 | loss: 204.1482749CurrentTrain: epoch  4, batch    53 | loss: 287.9249685CurrentTrain: epoch  4, batch    54 | loss: 195.9593636CurrentTrain: epoch  4, batch    55 | loss: 238.9541592CurrentTrain: epoch  4, batch    56 | loss: 196.0338070CurrentTrain: epoch  4, batch    57 | loss: 281.0344703CurrentTrain: epoch  4, batch    58 | loss: 206.8720677CurrentTrain: epoch  4, batch    59 | loss: 209.2818055CurrentTrain: epoch  4, batch    60 | loss: 284.0417212CurrentTrain: epoch  4, batch    61 | loss: 195.7934637CurrentTrain: epoch  4, batch    62 | loss: 178.5707666CurrentTrain: epoch  4, batch    63 | loss: 184.8818547CurrentTrain: epoch  4, batch    64 | loss: 168.8829060CurrentTrain: epoch  4, batch    65 | loss: 219.6798361CurrentTrain: epoch  4, batch    66 | loss: 257.4537192CurrentTrain: epoch  4, batch    67 | loss: 269.2888536CurrentTrain: epoch  4, batch    68 | loss: 162.6981340CurrentTrain: epoch  4, batch    69 | loss: 207.6009417CurrentTrain: epoch  4, batch    70 | loss: 279.1332414CurrentTrain: epoch  4, batch    71 | loss: 195.5617603CurrentTrain: epoch  4, batch    72 | loss: 177.5714790CurrentTrain: epoch  4, batch    73 | loss: 201.5920216CurrentTrain: epoch  4, batch    74 | loss: 203.8209913CurrentTrain: epoch  4, batch    75 | loss: 185.5961889CurrentTrain: epoch  4, batch    76 | loss: 196.9051105CurrentTrain: epoch  4, batch    77 | loss: 237.2837471CurrentTrain: epoch  4, batch    78 | loss: 187.9504392CurrentTrain: epoch  4, batch    79 | loss: 277.7482888CurrentTrain: epoch  4, batch    80 | loss: 205.9991637CurrentTrain: epoch  4, batch    81 | loss: 239.2818177CurrentTrain: epoch  4, batch    82 | loss: 230.2057245CurrentTrain: epoch  4, batch    83 | loss: 166.0375351CurrentTrain: epoch  4, batch    84 | loss: 359.2364659CurrentTrain: epoch  4, batch    85 | loss: 199.9823344CurrentTrain: epoch  4, batch    86 | loss: 215.9944317CurrentTrain: epoch  4, batch    87 | loss: 162.7915003CurrentTrain: epoch  4, batch    88 | loss: 186.2795855CurrentTrain: epoch  4, batch    89 | loss: 186.7975663CurrentTrain: epoch  4, batch    90 | loss: 174.0290397CurrentTrain: epoch  4, batch    91 | loss: 169.3503725CurrentTrain: epoch  4, batch    92 | loss: 172.6435947CurrentTrain: epoch  4, batch    93 | loss: 177.1590425CurrentTrain: epoch  4, batch    94 | loss: 191.5716673CurrentTrain: epoch  4, batch    95 | loss: 150.7729227CurrentTrain: epoch  5, batch     0 | loss: 178.3207382CurrentTrain: epoch  5, batch     1 | loss: 230.4457530CurrentTrain: epoch  5, batch     2 | loss: 213.1689893CurrentTrain: epoch  5, batch     3 | loss: 196.0328561CurrentTrain: epoch  5, batch     4 | loss: 229.7523289CurrentTrain: epoch  5, batch     5 | loss: 192.9446370CurrentTrain: epoch  5, batch     6 | loss: 191.6124287CurrentTrain: epoch  5, batch     7 | loss: 187.8857996CurrentTrain: epoch  5, batch     8 | loss: 148.9757814CurrentTrain: epoch  5, batch     9 | loss: 203.7207835CurrentTrain: epoch  5, batch    10 | loss: 279.5745059CurrentTrain: epoch  5, batch    11 | loss: 204.0204178CurrentTrain: epoch  5, batch    12 | loss: 267.9139207CurrentTrain: epoch  5, batch    13 | loss: 204.0033711CurrentTrain: epoch  5, batch    14 | loss: 239.2626378CurrentTrain: epoch  5, batch    15 | loss: 239.8875162CurrentTrain: epoch  5, batch    16 | loss: 212.7503959CurrentTrain: epoch  5, batch    17 | loss: 203.8491812CurrentTrain: epoch  5, batch    18 | loss: 149.0513211CurrentTrain: epoch  5, batch    19 | loss: 212.4550085CurrentTrain: epoch  5, batch    20 | loss: 192.8998777CurrentTrain: epoch  5, batch    21 | loss: 193.1727529CurrentTrain: epoch  5, batch    22 | loss: 276.1973075CurrentTrain: epoch  5, batch    23 | loss: 169.0878261CurrentTrain: epoch  5, batch    24 | loss: 183.3048945CurrentTrain: epoch  5, batch    25 | loss: 177.9805819CurrentTrain: epoch  5, batch    26 | loss: 204.0838999CurrentTrain: epoch  5, batch    27 | loss: 205.2698294CurrentTrain: epoch  5, batch    28 | loss: 205.5649935CurrentTrain: epoch  5, batch    29 | loss: 204.1857491CurrentTrain: epoch  5, batch    30 | loss: 161.8926233CurrentTrain: epoch  5, batch    31 | loss: 220.7816118CurrentTrain: epoch  5, batch    32 | loss: 175.3610105CurrentTrain: epoch  5, batch    33 | loss: 287.4010147CurrentTrain: epoch  5, batch    34 | loss: 171.0903626CurrentTrain: epoch  5, batch    35 | loss: 246.9820887CurrentTrain: epoch  5, batch    36 | loss: 229.1741725CurrentTrain: epoch  5, batch    37 | loss: 240.2744526CurrentTrain: epoch  5, batch    38 | loss: 194.6236003CurrentTrain: epoch  5, batch    39 | loss: 237.3629616CurrentTrain: epoch  5, batch    40 | loss: 276.0434635CurrentTrain: epoch  5, batch    41 | loss: 170.5982904CurrentTrain: epoch  5, batch    42 | loss: 188.4911205CurrentTrain: epoch  5, batch    43 | loss: 175.0583842CurrentTrain: epoch  5, batch    44 | loss: 276.6406849CurrentTrain: epoch  5, batch    45 | loss: 179.9053428CurrentTrain: epoch  5, batch    46 | loss: 170.7112767CurrentTrain: epoch  5, batch    47 | loss: 178.9505752CurrentTrain: epoch  5, batch    48 | loss: 147.8355483CurrentTrain: epoch  5, batch    49 | loss: 186.7188241CurrentTrain: epoch  5, batch    50 | loss: 189.0062028CurrentTrain: epoch  5, batch    51 | loss: 184.7150372CurrentTrain: epoch  5, batch    52 | loss: 210.9605236CurrentTrain: epoch  5, batch    53 | loss: 336.0164941CurrentTrain: epoch  5, batch    54 | loss: 258.7099249CurrentTrain: epoch  5, batch    55 | loss: 182.0178618CurrentTrain: epoch  5, batch    56 | loss: 277.4973237CurrentTrain: epoch  5, batch    57 | loss: 230.6401925CurrentTrain: epoch  5, batch    58 | loss: 202.1546112CurrentTrain: epoch  5, batch    59 | loss: 363.6261713CurrentTrain: epoch  5, batch    60 | loss: 215.1564337CurrentTrain: epoch  5, batch    61 | loss: 267.1964537CurrentTrain: epoch  5, batch    62 | loss: 212.2628867CurrentTrain: epoch  5, batch    63 | loss: 195.1672313CurrentTrain: epoch  5, batch    64 | loss: 214.2821075CurrentTrain: epoch  5, batch    65 | loss: 202.4390844CurrentTrain: epoch  5, batch    66 | loss: 231.7949045CurrentTrain: epoch  5, batch    67 | loss: 195.1482501CurrentTrain: epoch  5, batch    68 | loss: 196.7495096CurrentTrain: epoch  5, batch    69 | loss: 195.1921983CurrentTrain: epoch  5, batch    70 | loss: 247.4274448CurrentTrain: epoch  5, batch    71 | loss: 346.6605855CurrentTrain: epoch  5, batch    72 | loss: 183.2539325CurrentTrain: epoch  5, batch    73 | loss: 228.8135144CurrentTrain: epoch  5, batch    74 | loss: 202.7119110CurrentTrain: epoch  5, batch    75 | loss: 176.8910652CurrentTrain: epoch  5, batch    76 | loss: 196.0405298CurrentTrain: epoch  5, batch    77 | loss: 167.6931385CurrentTrain: epoch  5, batch    78 | loss: 250.0677854CurrentTrain: epoch  5, batch    79 | loss: 168.2183276CurrentTrain: epoch  5, batch    80 | loss: 257.2738383CurrentTrain: epoch  5, batch    81 | loss: 196.5450668CurrentTrain: epoch  5, batch    82 | loss: 204.9501116CurrentTrain: epoch  5, batch    83 | loss: 277.0320100CurrentTrain: epoch  5, batch    84 | loss: 182.0634700CurrentTrain: epoch  5, batch    85 | loss: 240.2547019CurrentTrain: epoch  5, batch    86 | loss: 230.9296081CurrentTrain: epoch  5, batch    87 | loss: 246.9143374CurrentTrain: epoch  5, batch    88 | loss: 180.2734790CurrentTrain: epoch  5, batch    89 | loss: 229.0189521CurrentTrain: epoch  5, batch    90 | loss: 212.4167794CurrentTrain: epoch  5, batch    91 | loss: 261.1564372CurrentTrain: epoch  5, batch    92 | loss: 186.8749719CurrentTrain: epoch  5, batch    93 | loss: 216.2497013CurrentTrain: epoch  5, batch    94 | loss: 186.2942161CurrentTrain: epoch  5, batch    95 | loss: 146.9783706CurrentTrain: epoch  6, batch     0 | loss: 228.3995270CurrentTrain: epoch  6, batch     1 | loss: 203.8214001CurrentTrain: epoch  6, batch     2 | loss: 276.0009554CurrentTrain: epoch  6, batch     3 | loss: 210.8299333CurrentTrain: epoch  6, batch     4 | loss: 194.1919314CurrentTrain: epoch  6, batch     5 | loss: 204.8923280CurrentTrain: epoch  6, batch     6 | loss: 277.9725904CurrentTrain: epoch  6, batch     7 | loss: 266.4553088CurrentTrain: epoch  6, batch     8 | loss: 162.2503004CurrentTrain: epoch  6, batch     9 | loss: 159.8893476CurrentTrain: epoch  6, batch    10 | loss: 211.9052430CurrentTrain: epoch  6, batch    11 | loss: 210.9582473CurrentTrain: epoch  6, batch    12 | loss: 179.9950375CurrentTrain: epoch  6, batch    13 | loss: 213.1584711CurrentTrain: epoch  6, batch    14 | loss: 246.7311328CurrentTrain: epoch  6, batch    15 | loss: 152.3997766CurrentTrain: epoch  6, batch    16 | loss: 220.0179663CurrentTrain: epoch  6, batch    17 | loss: 239.4094779CurrentTrain: epoch  6, batch    18 | loss: 187.1803691CurrentTrain: epoch  6, batch    19 | loss: 228.2822591CurrentTrain: epoch  6, batch    20 | loss: 212.8642770CurrentTrain: epoch  6, batch    21 | loss: 193.6890209CurrentTrain: epoch  6, batch    22 | loss: 171.7882557CurrentTrain: epoch  6, batch    23 | loss: 228.8638810CurrentTrain: epoch  6, batch    24 | loss: 194.7127240CurrentTrain: epoch  6, batch    25 | loss: 184.8129903CurrentTrain: epoch  6, batch    26 | loss: 221.0896867CurrentTrain: epoch  6, batch    27 | loss: 188.1539670CurrentTrain: epoch  6, batch    28 | loss: 220.7749333CurrentTrain: epoch  6, batch    29 | loss: 248.4590395CurrentTrain: epoch  6, batch    30 | loss: 167.6933951CurrentTrain: epoch  6, batch    31 | loss: 140.0132061CurrentTrain: epoch  6, batch    32 | loss: 186.5404695CurrentTrain: epoch  6, batch    33 | loss: 175.9904986CurrentTrain: epoch  6, batch    34 | loss: 276.2429987CurrentTrain: epoch  6, batch    35 | loss: 212.3089525CurrentTrain: epoch  6, batch    36 | loss: 246.8923325CurrentTrain: epoch  6, batch    37 | loss: 197.7795390CurrentTrain: epoch  6, batch    38 | loss: 266.8172591CurrentTrain: epoch  6, batch    39 | loss: 246.7360766CurrentTrain: epoch  6, batch    40 | loss: 169.3756263CurrentTrain: epoch  6, batch    41 | loss: 203.2852494CurrentTrain: epoch  6, batch    42 | loss: 220.1057989CurrentTrain: epoch  6, batch    43 | loss: 247.1612594CurrentTrain: epoch  6, batch    44 | loss: 246.7734246CurrentTrain: epoch  6, batch    45 | loss: 211.0444697CurrentTrain: epoch  6, batch    46 | loss: 347.1892850CurrentTrain: epoch  6, batch    47 | loss: 212.0460642CurrentTrain: epoch  6, batch    48 | loss: 267.6972443CurrentTrain: epoch  6, batch    49 | loss: 187.9569097CurrentTrain: epoch  6, batch    50 | loss: 286.6072368CurrentTrain: epoch  6, batch    51 | loss: 186.3087843CurrentTrain: epoch  6, batch    52 | loss: 196.0439909CurrentTrain: epoch  6, batch    53 | loss: 213.7669726CurrentTrain: epoch  6, batch    54 | loss: 161.3601510CurrentTrain: epoch  6, batch    55 | loss: 228.6849241CurrentTrain: epoch  6, batch    56 | loss: 204.2832739CurrentTrain: epoch  6, batch    57 | loss: 184.7823049CurrentTrain: epoch  6, batch    58 | loss: 238.1658695CurrentTrain: epoch  6, batch    59 | loss: 229.7542186CurrentTrain: epoch  6, batch    60 | loss: 147.5953562CurrentTrain: epoch  6, batch    61 | loss: 167.7680071CurrentTrain: epoch  6, batch    62 | loss: 160.1986741CurrentTrain: epoch  6, batch    63 | loss: 187.5969530CurrentTrain: epoch  6, batch    64 | loss: 205.9977502CurrentTrain: epoch  6, batch    65 | loss: 194.9702149CurrentTrain: epoch  6, batch    66 | loss: 176.1876218CurrentTrain: epoch  6, batch    67 | loss: 237.0790658CurrentTrain: epoch  6, batch    68 | loss: 202.9085616CurrentTrain: epoch  6, batch    69 | loss: 173.8175858CurrentTrain: epoch  6, batch    70 | loss: 159.9011464CurrentTrain: epoch  6, batch    71 | loss: 183.0612457CurrentTrain: epoch  6, batch    72 | loss: 193.1591980CurrentTrain: epoch  6, batch    73 | loss: 177.6524954CurrentTrain: epoch  6, batch    74 | loss: 184.0765347CurrentTrain: epoch  6, batch    75 | loss: 183.5970080CurrentTrain: epoch  6, batch    76 | loss: 177.1383507CurrentTrain: epoch  6, batch    77 | loss: 206.2886825CurrentTrain: epoch  6, batch    78 | loss: 223.5739916CurrentTrain: epoch  6, batch    79 | loss: 186.3915049CurrentTrain: epoch  6, batch    80 | loss: 286.8744671CurrentTrain: epoch  6, batch    81 | loss: 169.4172662CurrentTrain: epoch  6, batch    82 | loss: 211.6187679CurrentTrain: epoch  6, batch    83 | loss: 232.2908709CurrentTrain: epoch  6, batch    84 | loss: 204.7110468CurrentTrain: epoch  6, batch    85 | loss: 238.8458883CurrentTrain: epoch  6, batch    86 | loss: 228.5995748CurrentTrain: epoch  6, batch    87 | loss: 195.9710736CurrentTrain: epoch  6, batch    88 | loss: 180.9980185CurrentTrain: epoch  6, batch    89 | loss: 220.3197607CurrentTrain: epoch  6, batch    90 | loss: 172.0280352CurrentTrain: epoch  6, batch    91 | loss: 219.3219412CurrentTrain: epoch  6, batch    92 | loss: 187.8881495CurrentTrain: epoch  6, batch    93 | loss: 160.1011860CurrentTrain: epoch  6, batch    94 | loss: 228.1441952CurrentTrain: epoch  6, batch    95 | loss: 173.3236950CurrentTrain: epoch  7, batch     0 | loss: 204.2148436CurrentTrain: epoch  7, batch     1 | loss: 238.8999464CurrentTrain: epoch  7, batch     2 | loss: 175.6108358CurrentTrain: epoch  7, batch     3 | loss: 183.6381791CurrentTrain: epoch  7, batch     4 | loss: 138.6714601CurrentTrain: epoch  7, batch     5 | loss: 201.7570780CurrentTrain: epoch  7, batch     6 | loss: 229.1568122CurrentTrain: epoch  7, batch     7 | loss: 160.6187566CurrentTrain: epoch  7, batch     8 | loss: 177.9591452CurrentTrain: epoch  7, batch     9 | loss: 170.0680815CurrentTrain: epoch  7, batch    10 | loss: 211.3510912CurrentTrain: epoch  7, batch    11 | loss: 168.5730613CurrentTrain: epoch  7, batch    12 | loss: 175.2679785CurrentTrain: epoch  7, batch    13 | loss: 239.6708376CurrentTrain: epoch  7, batch    14 | loss: 202.7491033CurrentTrain: epoch  7, batch    15 | loss: 133.7250045CurrentTrain: epoch  7, batch    16 | loss: 195.0416146CurrentTrain: epoch  7, batch    17 | loss: 202.4472625CurrentTrain: epoch  7, batch    18 | loss: 211.4203024CurrentTrain: epoch  7, batch    19 | loss: 237.5958933CurrentTrain: epoch  7, batch    20 | loss: 177.0414441CurrentTrain: epoch  7, batch    21 | loss: 276.1023289CurrentTrain: epoch  7, batch    22 | loss: 212.7551140CurrentTrain: epoch  7, batch    23 | loss: 286.5110204CurrentTrain: epoch  7, batch    24 | loss: 194.5247600CurrentTrain: epoch  7, batch    25 | loss: 280.2593891CurrentTrain: epoch  7, batch    26 | loss: 248.2935207CurrentTrain: epoch  7, batch    27 | loss: 219.0546649CurrentTrain: epoch  7, batch    28 | loss: 175.4179622CurrentTrain: epoch  7, batch    29 | loss: 175.4780552CurrentTrain: epoch  7, batch    30 | loss: 146.2959362CurrentTrain: epoch  7, batch    31 | loss: 196.8880352CurrentTrain: epoch  7, batch    32 | loss: 215.2720915CurrentTrain: epoch  7, batch    33 | loss: 200.7750194CurrentTrain: epoch  7, batch    34 | loss: 210.9721904CurrentTrain: epoch  7, batch    35 | loss: 237.5046952CurrentTrain: epoch  7, batch    36 | loss: 192.6234377CurrentTrain: epoch  7, batch    37 | loss: 195.8487738CurrentTrain: epoch  7, batch    38 | loss: 183.1668867CurrentTrain: epoch  7, batch    39 | loss: 186.6671595CurrentTrain: epoch  7, batch    40 | loss: 182.7530796CurrentTrain: epoch  7, batch    41 | loss: 203.9650407CurrentTrain: epoch  7, batch    42 | loss: 218.7461352CurrentTrain: epoch  7, batch    43 | loss: 212.0885604CurrentTrain: epoch  7, batch    44 | loss: 174.5570571CurrentTrain: epoch  7, batch    45 | loss: 167.4940408CurrentTrain: epoch  7, batch    46 | loss: 182.8404138CurrentTrain: epoch  7, batch    47 | loss: 187.0882215CurrentTrain: epoch  7, batch    48 | loss: 228.1166327CurrentTrain: epoch  7, batch    49 | loss: 202.2148061CurrentTrain: epoch  7, batch    50 | loss: 220.7400338CurrentTrain: epoch  7, batch    51 | loss: 210.6447549CurrentTrain: epoch  7, batch    52 | loss: 210.8864222CurrentTrain: epoch  7, batch    53 | loss: 203.0511793CurrentTrain: epoch  7, batch    54 | loss: 202.2568044CurrentTrain: epoch  7, batch    55 | loss: 201.9989650CurrentTrain: epoch  7, batch    56 | loss: 202.3450523CurrentTrain: epoch  7, batch    57 | loss: 219.8183982CurrentTrain: epoch  7, batch    58 | loss: 258.5843407CurrentTrain: epoch  7, batch    59 | loss: 191.0532474CurrentTrain: epoch  7, batch    60 | loss: 249.0002329CurrentTrain: epoch  7, batch    61 | loss: 155.8935900CurrentTrain: epoch  7, batch    62 | loss: 193.8652138CurrentTrain: epoch  7, batch    63 | loss: 240.6306057CurrentTrain: epoch  7, batch    64 | loss: 237.9281468CurrentTrain: epoch  7, batch    65 | loss: 190.8101409CurrentTrain: epoch  7, batch    66 | loss: 183.8613366CurrentTrain: epoch  7, batch    67 | loss: 160.6906148CurrentTrain: epoch  7, batch    68 | loss: 276.0073244CurrentTrain: epoch  7, batch    69 | loss: 194.1541364CurrentTrain: epoch  7, batch    70 | loss: 231.0246726CurrentTrain: epoch  7, batch    71 | loss: 178.6804609CurrentTrain: epoch  7, batch    72 | loss: 246.6073969CurrentTrain: epoch  7, batch    73 | loss: 238.2740673CurrentTrain: epoch  7, batch    74 | loss: 246.6975057CurrentTrain: epoch  7, batch    75 | loss: 277.1915716CurrentTrain: epoch  7, batch    76 | loss: 172.2916321CurrentTrain: epoch  7, batch    77 | loss: 276.5340617CurrentTrain: epoch  7, batch    78 | loss: 187.2892827CurrentTrain: epoch  7, batch    79 | loss: 191.3724335CurrentTrain: epoch  7, batch    80 | loss: 185.9028417CurrentTrain: epoch  7, batch    81 | loss: 196.0250800CurrentTrain: epoch  7, batch    82 | loss: 237.0585161CurrentTrain: epoch  7, batch    83 | loss: 211.8175712CurrentTrain: epoch  7, batch    84 | loss: 204.8758670CurrentTrain: epoch  7, batch    85 | loss: 219.7987410CurrentTrain: epoch  7, batch    86 | loss: 161.5011844CurrentTrain: epoch  7, batch    87 | loss: 237.5081347CurrentTrain: epoch  7, batch    88 | loss: 175.8150270CurrentTrain: epoch  7, batch    89 | loss: 177.2153504CurrentTrain: epoch  7, batch    90 | loss: 219.9489652CurrentTrain: epoch  7, batch    91 | loss: 267.7229048CurrentTrain: epoch  7, batch    92 | loss: 167.4650860CurrentTrain: epoch  7, batch    93 | loss: 176.2636401CurrentTrain: epoch  7, batch    94 | loss: 167.6196118CurrentTrain: epoch  7, batch    95 | loss: 163.9562661CurrentTrain: epoch  8, batch     0 | loss: 227.7016664CurrentTrain: epoch  8, batch     1 | loss: 208.6507044CurrentTrain: epoch  8, batch     2 | loss: 182.8185927CurrentTrain: epoch  8, batch     3 | loss: 174.7865504CurrentTrain: epoch  8, batch     4 | loss: 171.0277355CurrentTrain: epoch  8, batch     5 | loss: 219.5152194CurrentTrain: epoch  8, batch     6 | loss: 138.5001585CurrentTrain: epoch  8, batch     7 | loss: 237.0710967CurrentTrain: epoch  8, batch     8 | loss: 146.8198049CurrentTrain: epoch  8, batch     9 | loss: 168.2835551CurrentTrain: epoch  8, batch    10 | loss: 286.3590980CurrentTrain: epoch  8, batch    11 | loss: 190.9834669CurrentTrain: epoch  8, batch    12 | loss: 193.4060837CurrentTrain: epoch  8, batch    13 | loss: 220.2852467CurrentTrain: epoch  8, batch    14 | loss: 227.9161603CurrentTrain: epoch  8, batch    15 | loss: 286.7455788CurrentTrain: epoch  8, batch    16 | loss: 177.9401157CurrentTrain: epoch  8, batch    17 | loss: 220.5308830CurrentTrain: epoch  8, batch    18 | loss: 202.8904128CurrentTrain: epoch  8, batch    19 | loss: 176.3997757CurrentTrain: epoch  8, batch    20 | loss: 182.3689326CurrentTrain: epoch  8, batch    21 | loss: 202.3398573CurrentTrain: epoch  8, batch    22 | loss: 197.0040556CurrentTrain: epoch  8, batch    23 | loss: 154.5251605CurrentTrain: epoch  8, batch    24 | loss: 346.7427247CurrentTrain: epoch  8, batch    25 | loss: 286.3927814CurrentTrain: epoch  8, batch    26 | loss: 195.4145175CurrentTrain: epoch  8, batch    27 | loss: 210.7366170CurrentTrain: epoch  8, batch    28 | loss: 219.6173332CurrentTrain: epoch  8, batch    29 | loss: 210.5299919CurrentTrain: epoch  8, batch    30 | loss: 227.8963719CurrentTrain: epoch  8, batch    31 | loss: 236.9842571CurrentTrain: epoch  8, batch    32 | loss: 236.8213239CurrentTrain: epoch  8, batch    33 | loss: 167.9830912CurrentTrain: epoch  8, batch    34 | loss: 183.3891424CurrentTrain: epoch  8, batch    35 | loss: 210.5695949CurrentTrain: epoch  8, batch    36 | loss: 159.7300690CurrentTrain: epoch  8, batch    37 | loss: 179.1025945CurrentTrain: epoch  8, batch    38 | loss: 359.1114293CurrentTrain: epoch  8, batch    39 | loss: 177.2632896CurrentTrain: epoch  8, batch    40 | loss: 152.3067697CurrentTrain: epoch  8, batch    41 | loss: 210.7631539CurrentTrain: epoch  8, batch    42 | loss: 205.1634716CurrentTrain: epoch  8, batch    43 | loss: 227.5998140CurrentTrain: epoch  8, batch    44 | loss: 210.5120983CurrentTrain: epoch  8, batch    45 | loss: 202.9837339CurrentTrain: epoch  8, batch    46 | loss: 246.4111851CurrentTrain: epoch  8, batch    47 | loss: 237.0094851CurrentTrain: epoch  8, batch    48 | loss: 211.1511239CurrentTrain: epoch  8, batch    49 | loss: 196.1310915CurrentTrain: epoch  8, batch    50 | loss: 202.1038059CurrentTrain: epoch  8, batch    51 | loss: 194.0093104CurrentTrain: epoch  8, batch    52 | loss: 211.6306450CurrentTrain: epoch  8, batch    53 | loss: 187.0035721CurrentTrain: epoch  8, batch    54 | loss: 175.8925808CurrentTrain: epoch  8, batch    55 | loss: 247.5973190CurrentTrain: epoch  8, batch    56 | loss: 187.9964349CurrentTrain: epoch  8, batch    57 | loss: 227.7603260CurrentTrain: epoch  8, batch    58 | loss: 195.3093391CurrentTrain: epoch  8, batch    59 | loss: 276.2039942CurrentTrain: epoch  8, batch    60 | loss: 195.2638030CurrentTrain: epoch  8, batch    61 | loss: 152.6555758CurrentTrain: epoch  8, batch    62 | loss: 219.9290091CurrentTrain: epoch  8, batch    63 | loss: 212.8795161CurrentTrain: epoch  8, batch    64 | loss: 170.1323110CurrentTrain: epoch  8, batch    65 | loss: 191.5189499CurrentTrain: epoch  8, batch    66 | loss: 202.0212919CurrentTrain: epoch  8, batch    67 | loss: 219.4909514CurrentTrain: epoch  8, batch    68 | loss: 218.8760583CurrentTrain: epoch  8, batch    69 | loss: 167.8645881CurrentTrain: epoch  8, batch    70 | loss: 220.4624318CurrentTrain: epoch  8, batch    71 | loss: 194.8894788CurrentTrain: epoch  8, batch    72 | loss: 174.5330645CurrentTrain: epoch  8, batch    73 | loss: 227.8269101CurrentTrain: epoch  8, batch    74 | loss: 199.7558502CurrentTrain: epoch  8, batch    75 | loss: 183.9061421CurrentTrain: epoch  8, batch    76 | loss: 266.8346678CurrentTrain: epoch  8, batch    77 | loss: 153.3926442CurrentTrain: epoch  8, batch    78 | loss: 184.0749798CurrentTrain: epoch  8, batch    79 | loss: 243.5755316CurrentTrain: epoch  8, batch    80 | loss: 211.6748302CurrentTrain: epoch  8, batch    81 | loss: 268.5462956CurrentTrain: epoch  8, batch    82 | loss: 286.5020222CurrentTrain: epoch  8, batch    83 | loss: 159.9280098CurrentTrain: epoch  8, batch    84 | loss: 155.8624754CurrentTrain: epoch  8, batch    85 | loss: 236.9538832CurrentTrain: epoch  8, batch    86 | loss: 202.2240777CurrentTrain: epoch  8, batch    87 | loss: 219.4107820CurrentTrain: epoch  8, batch    88 | loss: 195.0216870CurrentTrain: epoch  8, batch    89 | loss: 193.1410767CurrentTrain: epoch  8, batch    90 | loss: 211.2961411CurrentTrain: epoch  8, batch    91 | loss: 153.4321763CurrentTrain: epoch  8, batch    92 | loss: 153.5936729CurrentTrain: epoch  8, batch    93 | loss: 276.7629305CurrentTrain: epoch  8, batch    94 | loss: 202.9624670CurrentTrain: epoch  8, batch    95 | loss: 184.4531414CurrentTrain: epoch  9, batch     0 | loss: 175.6230996CurrentTrain: epoch  9, batch     1 | loss: 221.4678925CurrentTrain: epoch  9, batch     2 | loss: 182.2161010CurrentTrain: epoch  9, batch     3 | loss: 237.0925359CurrentTrain: epoch  9, batch     4 | loss: 152.3972563CurrentTrain: epoch  9, batch     5 | loss: 214.2766201CurrentTrain: epoch  9, batch     6 | loss: 211.6720113CurrentTrain: epoch  9, batch     7 | loss: 286.4841142CurrentTrain: epoch  9, batch     8 | loss: 228.0399689CurrentTrain: epoch  9, batch     9 | loss: 194.0932343CurrentTrain: epoch  9, batch    10 | loss: 227.4393488CurrentTrain: epoch  9, batch    11 | loss: 174.8439767CurrentTrain: epoch  9, batch    12 | loss: 202.3548412CurrentTrain: epoch  9, batch    13 | loss: 238.1769301CurrentTrain: epoch  9, batch    14 | loss: 183.6785761CurrentTrain: epoch  9, batch    15 | loss: 168.1992486CurrentTrain: epoch  9, batch    16 | loss: 276.8599056CurrentTrain: epoch  9, batch    17 | loss: 237.0042848CurrentTrain: epoch  9, batch    18 | loss: 220.8963164CurrentTrain: epoch  9, batch    19 | loss: 246.6600251CurrentTrain: epoch  9, batch    20 | loss: 210.4981673CurrentTrain: epoch  9, batch    21 | loss: 178.1813681CurrentTrain: epoch  9, batch    22 | loss: 210.5343189CurrentTrain: epoch  9, batch    23 | loss: 197.5728483CurrentTrain: epoch  9, batch    24 | loss: 193.5220684CurrentTrain: epoch  9, batch    25 | loss: 203.9354287CurrentTrain: epoch  9, batch    26 | loss: 201.8181282CurrentTrain: epoch  9, batch    27 | loss: 182.4775273CurrentTrain: epoch  9, batch    28 | loss: 210.5814717CurrentTrain: epoch  9, batch    29 | loss: 228.0200968CurrentTrain: epoch  9, batch    30 | loss: 210.1914305CurrentTrain: epoch  9, batch    31 | loss: 219.4131728CurrentTrain: epoch  9, batch    32 | loss: 246.4891926CurrentTrain: epoch  9, batch    33 | loss: 186.7508424CurrentTrain: epoch  9, batch    34 | loss: 176.7977487CurrentTrain: epoch  9, batch    35 | loss: 286.4552266CurrentTrain: epoch  9, batch    36 | loss: 182.7837306CurrentTrain: epoch  9, batch    37 | loss: 202.5231953CurrentTrain: epoch  9, batch    38 | loss: 199.0936072CurrentTrain: epoch  9, batch    39 | loss: 205.3907934CurrentTrain: epoch  9, batch    40 | loss: 160.1118879CurrentTrain: epoch  9, batch    41 | loss: 147.1658463CurrentTrain: epoch  9, batch    42 | loss: 219.8590527CurrentTrain: epoch  9, batch    43 | loss: 236.7975196CurrentTrain: epoch  9, batch    44 | loss: 146.5434277CurrentTrain: epoch  9, batch    45 | loss: 228.1285122CurrentTrain: epoch  9, batch    46 | loss: 202.3206359CurrentTrain: epoch  9, batch    47 | loss: 171.0576303CurrentTrain: epoch  9, batch    48 | loss: 237.7521976CurrentTrain: epoch  9, batch    49 | loss: 167.0809396CurrentTrain: epoch  9, batch    50 | loss: 219.5656671CurrentTrain: epoch  9, batch    51 | loss: 249.4553726CurrentTrain: epoch  9, batch    52 | loss: 240.8824008CurrentTrain: epoch  9, batch    53 | loss: 193.8578391CurrentTrain: epoch  9, batch    54 | loss: 202.4254299CurrentTrain: epoch  9, batch    55 | loss: 186.5874403CurrentTrain: epoch  9, batch    56 | loss: 218.6824748CurrentTrain: epoch  9, batch    57 | loss: 185.3842564CurrentTrain: epoch  9, batch    58 | loss: 256.3719427CurrentTrain: epoch  9, batch    59 | loss: 202.0810899CurrentTrain: epoch  9, batch    60 | loss: 202.4657552CurrentTrain: epoch  9, batch    61 | loss: 276.0004886CurrentTrain: epoch  9, batch    62 | loss: 202.4578045CurrentTrain: epoch  9, batch    63 | loss: 186.3101082CurrentTrain: epoch  9, batch    64 | loss: 182.8170164CurrentTrain: epoch  9, batch    65 | loss: 193.5119045CurrentTrain: epoch  9, batch    66 | loss: 160.7112620CurrentTrain: epoch  9, batch    67 | loss: 190.8990512CurrentTrain: epoch  9, batch    68 | loss: 196.6289981CurrentTrain: epoch  9, batch    69 | loss: 219.2119109CurrentTrain: epoch  9, batch    70 | loss: 227.4640907CurrentTrain: epoch  9, batch    71 | loss: 210.9626982CurrentTrain: epoch  9, batch    72 | loss: 194.0156314CurrentTrain: epoch  9, batch    73 | loss: 193.1531830CurrentTrain: epoch  9, batch    74 | loss: 179.7916882CurrentTrain: epoch  9, batch    75 | loss: 183.1288195CurrentTrain: epoch  9, batch    76 | loss: 184.0830241CurrentTrain: epoch  9, batch    77 | loss: 210.9597900CurrentTrain: epoch  9, batch    78 | loss: 185.8724857CurrentTrain: epoch  9, batch    79 | loss: 199.3814043CurrentTrain: epoch  9, batch    80 | loss: 219.7221357CurrentTrain: epoch  9, batch    81 | loss: 196.8238305CurrentTrain: epoch  9, batch    82 | loss: 236.9325487CurrentTrain: epoch  9, batch    83 | loss: 220.2051782CurrentTrain: epoch  9, batch    84 | loss: 156.3134830CurrentTrain: epoch  9, batch    85 | loss: 276.0165882CurrentTrain: epoch  9, batch    86 | loss: 210.4891686CurrentTrain: epoch  9, batch    87 | loss: 168.2083126CurrentTrain: epoch  9, batch    88 | loss: 227.9754540CurrentTrain: epoch  9, batch    89 | loss: 246.3742920CurrentTrain: epoch  9, batch    90 | loss: 202.7678811CurrentTrain: epoch  9, batch    91 | loss: 236.8160155CurrentTrain: epoch  9, batch    92 | loss: 195.1118245CurrentTrain: epoch  9, batch    93 | loss: 201.8082155CurrentTrain: epoch  9, batch    94 | loss: 210.8131812CurrentTrain: epoch  9, batch    95 | loss: 169.9122290

F1 score per class: {32: 0.6704545454545454, 6: 0.918918918918919, 19: 0.4166666666666667, 24: 0.7578947368421053, 26: 0.9361702127659575, 29: 0.914572864321608}
Micro-average F1 score: 0.8316008316008316
Weighted-average F1 score: 0.8394118742560507
F1 score per class: {32: 0.7395833333333334, 6: 0.9417989417989417, 19: 0.48484848484848486, 24: 0.7487179487179487, 26: 0.9795918367346939, 29: 0.8762886597938144}
Micro-average F1 score: 0.8448448448448449
Weighted-average F1 score: 0.8454774537107153
F1 score per class: {32: 0.7058823529411765, 6: 0.9417989417989417, 19: 0.48484848484848486, 24: 0.7487179487179487, 26: 0.9743589743589743, 29: 0.8762886597938144}
Micro-average F1 score: 0.837865055387714
Weighted-average F1 score: 0.839090609498193

F1 score per class: {32: 0.6704545454545454, 6: 0.918918918918919, 19: 0.4166666666666667, 24: 0.7578947368421053, 26: 0.9361702127659575, 29: 0.914572864321608}
Micro-average F1 score: 0.8316008316008316
Weighted-average F1 score: 0.8394118742560507
F1 score per class: {32: 0.7395833333333334, 6: 0.9417989417989417, 19: 0.48484848484848486, 24: 0.7487179487179487, 26: 0.9795918367346939, 29: 0.8762886597938144}
Micro-average F1 score: 0.8448448448448449
Weighted-average F1 score: 0.8454774537107153
F1 score per class: {32: 0.7058823529411765, 6: 0.9417989417989417, 19: 0.48484848484848486, 24: 0.7487179487179487, 26: 0.9743589743589743, 29: 0.8762886597938144}
Micro-average F1 score: 0.837865055387714
Weighted-average F1 score: 0.839090609498193
cur_acc:  ['0.8316']
his_acc:  ['0.8316']
cur_acc des:  ['0.8448']
his_acc des:  ['0.8448']
cur_acc rrf:  ['0.8379']
his_acc rrf:  ['0.8379']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 224.2282851CurrentTrain: epoch  0, batch     1 | loss: 188.9175112CurrentTrain: epoch  0, batch     2 | loss: 219.5697197CurrentTrain: epoch  0, batch     3 | loss: 129.6958450CurrentTrain: epoch  1, batch     0 | loss: 248.7938573CurrentTrain: epoch  1, batch     1 | loss: 203.7016751CurrentTrain: epoch  1, batch     2 | loss: 207.1751920CurrentTrain: epoch  1, batch     3 | loss: 126.7897367CurrentTrain: epoch  2, batch     0 | loss: 276.6342647CurrentTrain: epoch  2, batch     1 | loss: 206.3851113CurrentTrain: epoch  2, batch     2 | loss: 208.5701080CurrentTrain: epoch  2, batch     3 | loss: 100.4160355CurrentTrain: epoch  3, batch     0 | loss: 176.1040006CurrentTrain: epoch  3, batch     1 | loss: 217.5326818CurrentTrain: epoch  3, batch     2 | loss: 202.8742581CurrentTrain: epoch  3, batch     3 | loss: 128.0122604CurrentTrain: epoch  4, batch     0 | loss: 217.1701375CurrentTrain: epoch  4, batch     1 | loss: 200.8116592CurrentTrain: epoch  4, batch     2 | loss: 180.4402097CurrentTrain: epoch  4, batch     3 | loss: 123.0488379CurrentTrain: epoch  5, batch     0 | loss: 289.7017113CurrentTrain: epoch  5, batch     1 | loss: 178.7426281CurrentTrain: epoch  5, batch     2 | loss: 151.8062969CurrentTrain: epoch  5, batch     3 | loss: 166.4551193CurrentTrain: epoch  6, batch     0 | loss: 205.8993029CurrentTrain: epoch  6, batch     1 | loss: 196.6268500CurrentTrain: epoch  6, batch     2 | loss: 163.6845894CurrentTrain: epoch  6, batch     3 | loss: 171.9935769CurrentTrain: epoch  7, batch     0 | loss: 197.4703058CurrentTrain: epoch  7, batch     1 | loss: 196.3089775CurrentTrain: epoch  7, batch     2 | loss: 186.5692149CurrentTrain: epoch  7, batch     3 | loss: 145.2305571CurrentTrain: epoch  8, batch     0 | loss: 184.7298022CurrentTrain: epoch  8, batch     1 | loss: 160.2042709CurrentTrain: epoch  8, batch     2 | loss: 247.0313593CurrentTrain: epoch  8, batch     3 | loss: 118.0671804CurrentTrain: epoch  9, batch     0 | loss: 211.1849620CurrentTrain: epoch  9, batch     1 | loss: 191.6016102CurrentTrain: epoch  9, batch     2 | loss: 220.2199740CurrentTrain: epoch  9, batch     3 | loss: 79.1253647
MemoryTrain:  epoch  0, batch     0 | loss: 1.4519330MemoryTrain:  epoch  1, batch     0 | loss: 1.1620820MemoryTrain:  epoch  2, batch     0 | loss: 0.8325427MemoryTrain:  epoch  3, batch     0 | loss: 0.7129026MemoryTrain:  epoch  4, batch     0 | loss: 0.5443736MemoryTrain:  epoch  5, batch     0 | loss: 0.4823285MemoryTrain:  epoch  6, batch     0 | loss: 0.3484060MemoryTrain:  epoch  7, batch     0 | loss: 0.2879932MemoryTrain:  epoch  8, batch     0 | loss: 0.2536856MemoryTrain:  epoch  9, batch     0 | loss: 0.1895158

F1 score per class: {33: 0.6829268292682927, 36: 0.7727272727272727, 8: 0.0, 20: 0.0, 26: 0.9142857142857143, 29: 0.375, 30: 0.693069306930693}
Micro-average F1 score: 0.7123287671232876
Weighted-average F1 score: 0.7143033677100094
F1 score per class: {33: 0.8671328671328671, 36: 0.8888888888888888, 8: 0.0, 20: 0.0, 26: 0.9473684210526315, 29: 0.47058823529411764, 30: 0.9032258064516129}
Micro-average F1 score: 0.8699763593380615
Weighted-average F1 score: 0.8696503330635208
F1 score per class: {33: 0.8671328671328671, 36: 0.8888888888888888, 8: 0.0, 20: 0.0, 26: 0.972972972972973, 29: 0.5263157894736842, 30: 0.9016393442622951}
Micro-average F1 score: 0.8720379146919431
Weighted-average F1 score: 0.8689951878022475

F1 score per class: {32: 0.4857142857142857, 33: 0.5526315789473685, 36: 0.8950276243093923, 6: 0.7727272727272727, 8: 0.5925925925925926, 19: 0.7643979057591623, 20: 0.9312169312169312, 24: 0.9142857142857143, 26: 0.8808290155440415, 29: 0.375, 30: 0.693069306930693}
Micro-average F1 score: 0.7600913937547601
Weighted-average F1 score: 0.7763924177802982
F1 score per class: {32: 0.7065217391304348, 33: 0.7469879518072289, 36: 0.9130434782608695, 6: 0.8888888888888888, 8: 0.625, 19: 0.7717391304347826, 20: 0.9424083769633508, 24: 0.8181818181818182, 26: 0.8865979381443299, 29: 0.3333333333333333, 30: 0.9032258064516129}
Micro-average F1 score: 0.8274894810659187
Weighted-average F1 score: 0.8262745718941064
F1 score per class: {32: 0.6704545454545454, 33: 0.7209302325581395, 36: 0.9197860962566845, 6: 0.8888888888888888, 8: 0.625, 19: 0.7675675675675676, 20: 0.9424083769633508, 24: 0.8780487804878049, 26: 0.8808290155440415, 29: 0.4, 30: 0.9016393442622951}
Micro-average F1 score: 0.8222066057624736
Weighted-average F1 score: 0.82145247393708
cur_acc:  ['0.8316', '0.7123']
his_acc:  ['0.8316', '0.7601']
cur_acc des:  ['0.8448', '0.8700']
his_acc des:  ['0.8448', '0.8275']
cur_acc rrf:  ['0.8379', '0.8720']
his_acc rrf:  ['0.8379', '0.8222']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 222.2983582CurrentTrain: epoch  0, batch     1 | loss: 209.7660206CurrentTrain: epoch  0, batch     2 | loss: 220.2095092CurrentTrain: epoch  0, batch     3 | loss: 242.2336058CurrentTrain: epoch  0, batch     4 | loss: 55.8888601CurrentTrain: epoch  1, batch     0 | loss: 181.4471665CurrentTrain: epoch  1, batch     1 | loss: 348.2776499CurrentTrain: epoch  1, batch     2 | loss: 253.6708777CurrentTrain: epoch  1, batch     3 | loss: 183.9334242CurrentTrain: epoch  1, batch     4 | loss: 41.3415026CurrentTrain: epoch  2, batch     0 | loss: 214.2842110CurrentTrain: epoch  2, batch     1 | loss: 178.4076609CurrentTrain: epoch  2, batch     2 | loss: 292.3271702CurrentTrain: epoch  2, batch     3 | loss: 212.0517179CurrentTrain: epoch  2, batch     4 | loss: 52.1748413CurrentTrain: epoch  3, batch     0 | loss: 180.6784750CurrentTrain: epoch  3, batch     1 | loss: 347.1930396CurrentTrain: epoch  3, batch     2 | loss: 227.5682085CurrentTrain: epoch  3, batch     3 | loss: 215.5519539CurrentTrain: epoch  3, batch     4 | loss: 37.3115088CurrentTrain: epoch  4, batch     0 | loss: 249.7842225CurrentTrain: epoch  4, batch     1 | loss: 200.1629358CurrentTrain: epoch  4, batch     2 | loss: 191.3558021CurrentTrain: epoch  4, batch     3 | loss: 206.9883172CurrentTrain: epoch  4, batch     4 | loss: 51.6933885CurrentTrain: epoch  5, batch     0 | loss: 198.2670685CurrentTrain: epoch  5, batch     1 | loss: 362.9273393CurrentTrain: epoch  5, batch     2 | loss: 172.3018342CurrentTrain: epoch  5, batch     3 | loss: 212.7401138CurrentTrain: epoch  5, batch     4 | loss: 31.8755535CurrentTrain: epoch  6, batch     0 | loss: 248.1555244CurrentTrain: epoch  6, batch     1 | loss: 206.7309488CurrentTrain: epoch  6, batch     2 | loss: 213.3472373CurrentTrain: epoch  6, batch     3 | loss: 171.6968818CurrentTrain: epoch  6, batch     4 | loss: 35.6732224CurrentTrain: epoch  7, batch     0 | loss: 185.4342600CurrentTrain: epoch  7, batch     1 | loss: 189.0543981CurrentTrain: epoch  7, batch     2 | loss: 193.4227561CurrentTrain: epoch  7, batch     3 | loss: 238.5345256CurrentTrain: epoch  7, batch     4 | loss: 89.7297195CurrentTrain: epoch  8, batch     0 | loss: 212.7814574CurrentTrain: epoch  8, batch     1 | loss: 223.7800251CurrentTrain: epoch  8, batch     2 | loss: 238.5196432CurrentTrain: epoch  8, batch     3 | loss: 196.7250330CurrentTrain: epoch  8, batch     4 | loss: 30.4071939CurrentTrain: epoch  9, batch     0 | loss: 267.4443051CurrentTrain: epoch  9, batch     1 | loss: 229.2660033CurrentTrain: epoch  9, batch     2 | loss: 203.2686345CurrentTrain: epoch  9, batch     3 | loss: 179.1793279CurrentTrain: epoch  9, batch     4 | loss: 51.2580841
MemoryTrain:  epoch  0, batch     0 | loss: 1.2299568MemoryTrain:  epoch  1, batch     0 | loss: 0.9893461MemoryTrain:  epoch  2, batch     0 | loss: 0.8087617MemoryTrain:  epoch  3, batch     0 | loss: 0.6610869MemoryTrain:  epoch  4, batch     0 | loss: 0.4755466MemoryTrain:  epoch  5, batch     0 | loss: 0.3973406MemoryTrain:  epoch  6, batch     0 | loss: 0.3220582MemoryTrain:  epoch  7, batch     0 | loss: 0.2695136MemoryTrain:  epoch  8, batch     0 | loss: 0.2337281MemoryTrain:  epoch  9, batch     0 | loss: 0.2305064

F1 score per class: {2: 0.875, 39: 0.0, 8: 0.5691056910569106, 11: 0.37398373983739835, 12: 0.0, 19: 0.7692307692307693, 28: 0.0}
Micro-average F1 score: 0.4605263157894737
Weighted-average F1 score: 0.4528409299930103
F1 score per class: {33: 0.9411764705882353, 2: 0.0, 39: 0.8516129032258064, 8: 0.8323699421965318, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.5454545454545454, 26: 0.0, 28: 0.13333333333333333}
Micro-average F1 score: 0.7614213197969543
Weighted-average F1 score: 0.7264010230335926
F1 score per class: {33: 0.9411764705882353, 2: 0.0, 39: 0.8516129032258064, 8: 0.8323699421965318, 11: 0.0, 12: 0.0, 19: 0.5454545454545454, 26: 0.0, 28: 0.13333333333333333}
Micro-average F1 score: 0.7614213197969543
Weighted-average F1 score: 0.7264010230335926

F1 score per class: {32: 0.875, 33: 0.5679012345679012, 2: 0.39285714285714285, 36: 0.5223880597014925, 6: 0.36220472440944884, 39: 0.7888888888888889, 8: 0.65, 11: 0.34782608695652173, 12: 0.7340425531914894, 19: 0.2564102564102564, 20: 0.8839779005524862, 24: 0.9142857142857143, 26: 0.8969072164948454, 28: 0.16666666666666666, 29: 0.4827586206896552, 30: 0.0}
Micro-average F1 score: 0.645689112649465
Weighted-average F1 score: 0.680293378224727
F1 score per class: {32: 0.7619047619047619, 33: 0.7745098039215687, 2: 0.6933333333333334, 36: 0.7951807228915663, 6: 0.7093596059113301, 39: 0.8351648351648352, 8: 0.8043478260869565, 11: 0.6060606060606061, 12: 0.745945945945946, 19: 0.2727272727272727, 20: 0.9021739130434783, 24: 0.8837209302325582, 26: 0.9035532994923858, 28: 0.3157894736842105, 29: 0.8035714285714286, 30: 0.1111111111111111}
Micro-average F1 score: 0.7777170944838886
Weighted-average F1 score: 0.7793219811756957
F1 score per class: {32: 0.9411764705882353, 33: 0.7204301075268817, 2: 0.6447368421052632, 36: 0.7810650887573964, 6: 0.7024390243902439, 39: 0.8369565217391305, 8: 0.8631578947368421, 11: 0.4827586206896552, 12: 0.7486631016042781, 19: 0.2, 20: 0.9021739130434783, 24: 0.9047619047619048, 26: 0.9035532994923858, 28: 0.3333333333333333, 29: 0.7818181818181819, 30: 0.1111111111111111}
Micro-average F1 score: 0.7657707076247943
Weighted-average F1 score: 0.7651900854537862
cur_acc:  ['0.8316', '0.7123', '0.4605']
his_acc:  ['0.8316', '0.7601', '0.6457']
cur_acc des:  ['0.8448', '0.8700', '0.7614']
his_acc des:  ['0.8448', '0.8275', '0.7777']
cur_acc rrf:  ['0.8379', '0.8720', '0.7614']
his_acc rrf:  ['0.8379', '0.8222', '0.7658']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 166.8345654CurrentTrain: epoch  0, batch     1 | loss: 294.6479653CurrentTrain: epoch  0, batch     2 | loss: 255.1513747CurrentTrain: epoch  0, batch     3 | loss: 298.1551717CurrentTrain: epoch  0, batch     4 | loss: 148.6442080CurrentTrain: epoch  1, batch     0 | loss: 249.5399677CurrentTrain: epoch  1, batch     1 | loss: 179.6454586CurrentTrain: epoch  1, batch     2 | loss: 285.2763380CurrentTrain: epoch  1, batch     3 | loss: 217.4209314CurrentTrain: epoch  1, batch     4 | loss: 156.0082980CurrentTrain: epoch  2, batch     0 | loss: 241.6694305CurrentTrain: epoch  2, batch     1 | loss: 223.8168803CurrentTrain: epoch  2, batch     2 | loss: 197.1264209CurrentTrain: epoch  2, batch     3 | loss: 225.9764704CurrentTrain: epoch  2, batch     4 | loss: 139.6753004CurrentTrain: epoch  3, batch     0 | loss: 250.5495410CurrentTrain: epoch  3, batch     1 | loss: 200.7351031CurrentTrain: epoch  3, batch     2 | loss: 242.8662660CurrentTrain: epoch  3, batch     3 | loss: 231.6922794CurrentTrain: epoch  3, batch     4 | loss: 119.6018202CurrentTrain: epoch  4, batch     0 | loss: 215.1586073CurrentTrain: epoch  4, batch     1 | loss: 221.4381158CurrentTrain: epoch  4, batch     2 | loss: 281.6703241CurrentTrain: epoch  4, batch     3 | loss: 174.2496008CurrentTrain: epoch  4, batch     4 | loss: 154.1706145CurrentTrain: epoch  5, batch     0 | loss: 248.0488780CurrentTrain: epoch  5, batch     1 | loss: 203.9038440CurrentTrain: epoch  5, batch     2 | loss: 185.0680257CurrentTrain: epoch  5, batch     3 | loss: 229.7000110CurrentTrain: epoch  5, batch     4 | loss: 143.3876543CurrentTrain: epoch  6, batch     0 | loss: 238.6512638CurrentTrain: epoch  6, batch     1 | loss: 277.0844183CurrentTrain: epoch  6, batch     2 | loss: 221.2041829CurrentTrain: epoch  6, batch     3 | loss: 169.2851261CurrentTrain: epoch  6, batch     4 | loss: 170.2499234CurrentTrain: epoch  7, batch     0 | loss: 237.2878495CurrentTrain: epoch  7, batch     1 | loss: 238.0920646CurrentTrain: epoch  7, batch     2 | loss: 221.3233896CurrentTrain: epoch  7, batch     3 | loss: 220.5624224CurrentTrain: epoch  7, batch     4 | loss: 136.6236478CurrentTrain: epoch  8, batch     0 | loss: 175.9831620CurrentTrain: epoch  8, batch     1 | loss: 276.3937797CurrentTrain: epoch  8, batch     2 | loss: 191.2719181CurrentTrain: epoch  8, batch     3 | loss: 195.1479381CurrentTrain: epoch  8, batch     4 | loss: 228.9294883CurrentTrain: epoch  9, batch     0 | loss: 211.4044534CurrentTrain: epoch  9, batch     1 | loss: 238.7964817CurrentTrain: epoch  9, batch     2 | loss: 178.2532924CurrentTrain: epoch  9, batch     3 | loss: 266.4062034CurrentTrain: epoch  9, batch     4 | loss: 150.9441020
MemoryTrain:  epoch  0, batch     0 | loss: 0.9580313MemoryTrain:  epoch  1, batch     0 | loss: 0.7646569MemoryTrain:  epoch  2, batch     0 | loss: 0.6024900MemoryTrain:  epoch  3, batch     0 | loss: 0.4145962MemoryTrain:  epoch  4, batch     0 | loss: 0.3290897MemoryTrain:  epoch  5, batch     0 | loss: 0.3045132MemoryTrain:  epoch  6, batch     0 | loss: 0.2541072MemoryTrain:  epoch  7, batch     0 | loss: 0.1931653MemoryTrain:  epoch  8, batch     0 | loss: 0.1635326MemoryTrain:  epoch  9, batch     0 | loss: 0.1241725

F1 score per class: {5: 0.9743589743589743, 6: 0.0, 39: 0.0, 8: 0.2608695652173913, 10: 0.0, 11: 0.8518518518518519, 16: 0.6153846153846154, 17: 0.23809523809523808, 18: 0.0, 28: 0.0}
Micro-average F1 score: 0.6228070175438597
Weighted-average F1 score: 0.6698533618823473
F1 score per class: {5: 0.9950248756218906, 6: 0.0, 8: 0.0, 10: 0.7133757961783439, 11: 0.0, 12: 0.0, 16: 0.9310344827586207, 17: 0.8, 18: 0.5769230769230769, 28: 0.0}
Micro-average F1 score: 0.7771428571428571
Weighted-average F1 score: 0.7256335120123055
F1 score per class: {5: 0.9950248756218906, 6: 0.0, 8: 0.0, 10: 0.6923076923076923, 11: 0.0, 12: 0.0, 16: 0.9491525423728814, 17: 0.8, 18: 0.4583333333333333, 28: 0.0}
Micro-average F1 score: 0.7624521072796935
Weighted-average F1 score: 0.7180237138313901

F1 score per class: {2: 0.8, 5: 0.9644670050761421, 6: 0.5868263473053892, 8: 0.17582417582417584, 10: 0.23076923076923078, 11: 0.5859872611464968, 12: 0.3, 16: 0.7796610169491526, 17: 0.5333333333333333, 18: 0.18867924528301888, 19: 0.7415730337078652, 20: 0.6666666666666666, 24: 0.4166666666666667, 26: 0.7422680412371134, 28: 0.19672131147540983, 29: 0.8777777777777778, 30: 0.918918918918919, 32: 0.9081632653061225, 33: 0.16666666666666666, 36: 0.3076923076923077, 39: 0.10526315789473684}
Micro-average F1 score: 0.624031007751938
Weighted-average F1 score: 0.6810829149666157
F1 score per class: {2: 0.8, 5: 0.9523809523809523, 6: 0.7655502392344498, 8: 0.45161290322580644, 10: 0.6153846153846154, 11: 0.6424242424242425, 12: 0.6914893617021277, 16: 0.8709677419354839, 17: 0.6, 18: 0.375, 19: 0.8324324324324325, 20: 0.7209302325581395, 24: 0.5945945945945946, 26: 0.7593582887700535, 28: 0.3333333333333333, 29: 0.9021739130434783, 30: 0.8372093023255814, 32: 0.8865979381443299, 33: 0.3157894736842105, 36: 0.6464646464646465, 39: 0.19047619047619047}
Micro-average F1 score: 0.7297527706734868
Weighted-average F1 score: 0.7338762422164368
F1 score per class: {2: 0.8, 5: 0.9615384615384616, 6: 0.7604166666666666, 8: 0.4067796610169492, 10: 0.5806451612903226, 11: 0.6742857142857143, 12: 0.6994535519125683, 16: 0.875, 17: 0.631578947368421, 18: 0.3235294117647059, 19: 0.8342245989304813, 20: 0.7209302325581395, 24: 0.5294117647058824, 26: 0.7619047619047619, 28: 0.24, 29: 0.9021739130434783, 30: 0.8181818181818182, 32: 0.882051282051282, 33: 0.25, 36: 0.6326530612244898, 39: 0.2857142857142857}
Micro-average F1 score: 0.7238421955403087
Weighted-average F1 score: 0.7278840672756948
cur_acc:  ['0.8316', '0.7123', '0.4605', '0.6228']
his_acc:  ['0.8316', '0.7601', '0.6457', '0.6240']
cur_acc des:  ['0.8448', '0.8700', '0.7614', '0.7771']
his_acc des:  ['0.8448', '0.8275', '0.7777', '0.7298']
cur_acc rrf:  ['0.8379', '0.8720', '0.7614', '0.7625']
his_acc rrf:  ['0.8379', '0.8222', '0.7658', '0.7238']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 260.9189859CurrentTrain: epoch  0, batch     1 | loss: 213.7023982CurrentTrain: epoch  0, batch     2 | loss: 206.1218254CurrentTrain: epoch  0, batch     3 | loss: 244.5587866CurrentTrain: epoch  0, batch     4 | loss: 207.1540889CurrentTrain: epoch  1, batch     0 | loss: 213.4025556CurrentTrain: epoch  1, batch     1 | loss: 294.4720992CurrentTrain: epoch  1, batch     2 | loss: 218.2892894CurrentTrain: epoch  1, batch     3 | loss: 193.8975373CurrentTrain: epoch  1, batch     4 | loss: 142.7277029CurrentTrain: epoch  2, batch     0 | loss: 361.8935519CurrentTrain: epoch  2, batch     1 | loss: 201.8712543CurrentTrain: epoch  2, batch     2 | loss: 169.1739158CurrentTrain: epoch  2, batch     3 | loss: 214.9511781CurrentTrain: epoch  2, batch     4 | loss: 158.2431119CurrentTrain: epoch  3, batch     0 | loss: 232.8349460CurrentTrain: epoch  3, batch     1 | loss: 189.6172061CurrentTrain: epoch  3, batch     2 | loss: 243.9897342CurrentTrain: epoch  3, batch     3 | loss: 231.0453577CurrentTrain: epoch  3, batch     4 | loss: 184.0372368CurrentTrain: epoch  4, batch     0 | loss: 204.3408429CurrentTrain: epoch  4, batch     1 | loss: 186.2261662CurrentTrain: epoch  4, batch     2 | loss: 202.0859766CurrentTrain: epoch  4, batch     3 | loss: 248.7055138CurrentTrain: epoch  4, batch     4 | loss: 126.8166339CurrentTrain: epoch  5, batch     0 | loss: 278.1309186CurrentTrain: epoch  5, batch     1 | loss: 185.5273577CurrentTrain: epoch  5, batch     2 | loss: 230.1918934CurrentTrain: epoch  5, batch     3 | loss: 213.8009010CurrentTrain: epoch  5, batch     4 | loss: 117.0332069CurrentTrain: epoch  6, batch     0 | loss: 176.7556268CurrentTrain: epoch  6, batch     1 | loss: 238.2570281CurrentTrain: epoch  6, batch     2 | loss: 205.0094660CurrentTrain: epoch  6, batch     3 | loss: 287.3193897CurrentTrain: epoch  6, batch     4 | loss: 106.8060761CurrentTrain: epoch  7, batch     0 | loss: 212.8906331CurrentTrain: epoch  7, batch     1 | loss: 185.1059507CurrentTrain: epoch  7, batch     2 | loss: 203.9000668CurrentTrain: epoch  7, batch     3 | loss: 219.9309952CurrentTrain: epoch  7, batch     4 | loss: 148.4241201CurrentTrain: epoch  8, batch     0 | loss: 169.9728910CurrentTrain: epoch  8, batch     1 | loss: 228.4173148CurrentTrain: epoch  8, batch     2 | loss: 203.1705966CurrentTrain: epoch  8, batch     3 | loss: 237.8110964CurrentTrain: epoch  8, batch     4 | loss: 202.6309895CurrentTrain: epoch  9, batch     0 | loss: 246.9582107CurrentTrain: epoch  9, batch     1 | loss: 194.4015971CurrentTrain: epoch  9, batch     2 | loss: 277.2634985CurrentTrain: epoch  9, batch     3 | loss: 219.8680227CurrentTrain: epoch  9, batch     4 | loss: 93.8341440
MemoryTrain:  epoch  0, batch     0 | loss: 1.0693977MemoryTrain:  epoch  1, batch     0 | loss: 0.8974621MemoryTrain:  epoch  2, batch     0 | loss: 0.6288842MemoryTrain:  epoch  3, batch     0 | loss: 0.5697381MemoryTrain:  epoch  4, batch     0 | loss: 0.4893412MemoryTrain:  epoch  5, batch     0 | loss: 0.3887862MemoryTrain:  epoch  6, batch     0 | loss: 0.3178841MemoryTrain:  epoch  7, batch     0 | loss: 0.2585938MemoryTrain:  epoch  8, batch     0 | loss: 0.2322018MemoryTrain:  epoch  9, batch     0 | loss: 0.1864999

F1 score per class: {32: 0.368, 1: 0.7086614173228346, 34: 0.0, 3: 0.0, 11: 0.7, 14: 0.0, 22: 0.0, 24: 0.0, 26: 0.735632183908046}
Micro-average F1 score: 0.5442404006677797
Weighted-average F1 score: 0.5517109127522852
F1 score per class: {32: 0.4580152671755725, 1: 0.9419354838709677, 34: 0.0, 3: 0.0, 8: 0.0, 10: 0.0, 11: 0.057971014492753624, 12: 0.0, 14: 0.7513812154696132, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.8888888888888888}
Micro-average F1 score: 0.6555891238670695
Weighted-average F1 score: 0.6551329317631802
F1 score per class: {32: 0.4461538461538462, 1: 0.9554140127388535, 34: 0.0, 3: 0.0, 10: 0.0, 11: 0.058823529411764705, 12: 0.0, 14: 0.7540983606557377, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.8888888888888888}
Micro-average F1 score: 0.6626323751891074
Weighted-average F1 score: 0.6669166602095996

F1 score per class: {1: 0.3458646616541353, 2: 0.8, 3: 0.6923076923076923, 5: 0.9696969696969697, 6: 0.6107784431137725, 8: 0.20618556701030927, 10: 0.46987951807228917, 11: 0.42424242424242425, 12: 0.24561403508771928, 14: 0.0, 16: 0.7931034482758621, 17: 0.15384615384615385, 18: 0.0425531914893617, 19: 0.3787878787878788, 20: 0.6329113924050633, 22: 0.6774193548387096, 24: 0.06060606060606061, 26: 0.7604166666666666, 28: 0.2857142857142857, 29: 0.8901098901098901, 30: 0.8823529411764706, 32: 0.8216216216216217, 33: 0.16666666666666666, 34: 0.39751552795031053, 36: 0.11428571428571428, 39: 0.1111111111111111}
Micro-average F1 score: 0.5566002256487401
Weighted-average F1 score: 0.6067911293110138
F1 score per class: {1: 0.4316546762589928, 2: 0.875, 3: 0.9125, 5: 0.970873786407767, 6: 0.7263681592039801, 8: 0.4852941176470588, 10: 0.6596858638743456, 11: 0.5394736842105263, 12: 0.7252747252747253, 14: 0.056338028169014086, 16: 0.8524590163934426, 17: 0.4, 18: 0.14035087719298245, 19: 0.6790123456790124, 20: 0.6904761904761905, 22: 0.7311827956989247, 24: 0.05555555555555555, 26: 0.75, 28: 0.3076923076923077, 29: 0.9139784946236559, 30: 1.0, 32: 0.8497409326424871, 33: 0.3157894736842105, 34: 0.4835164835164835, 36: 0.40476190476190477, 39: 0.1111111111111111}
Micro-average F1 score: 0.667332002661344
Weighted-average F1 score: 0.6772157016551987
F1 score per class: {1: 0.4233576642335766, 2: 0.875, 3: 0.9259259259259259, 5: 0.975609756097561, 6: 0.6984126984126984, 8: 0.37606837606837606, 10: 0.6532663316582915, 11: 0.5384615384615384, 12: 0.6589595375722543, 14: 0.05714285714285714, 16: 0.8524590163934426, 17: 0.4, 18: 0.1509433962264151, 19: 0.6871165644171779, 20: 0.6823529411764706, 22: 0.7340425531914894, 24: 0.05714285714285714, 26: 0.7564766839378239, 28: 0.23529411764705882, 29: 0.9021739130434783, 30: 1.0, 32: 0.8497409326424871, 33: 0.125, 34: 0.47058823529411764, 36: 0.3855421686746988, 39: 0.1111111111111111}
Micro-average F1 score: 0.6548376297288249
Weighted-average F1 score: 0.6663050555619568
cur_acc:  ['0.8316', '0.7123', '0.4605', '0.6228', '0.5442']
his_acc:  ['0.8316', '0.7601', '0.6457', '0.6240', '0.5566']
cur_acc des:  ['0.8448', '0.8700', '0.7614', '0.7771', '0.6556']
his_acc des:  ['0.8448', '0.8275', '0.7777', '0.7298', '0.6673']
cur_acc rrf:  ['0.8379', '0.8720', '0.7614', '0.7625', '0.6626']
his_acc rrf:  ['0.8379', '0.8222', '0.7658', '0.7238', '0.6548']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 200.2070327CurrentTrain: epoch  0, batch     1 | loss: 208.1124953CurrentTrain: epoch  0, batch     2 | loss: 203.4099878CurrentTrain: epoch  0, batch     3 | loss: 20.4011704CurrentTrain: epoch  1, batch     0 | loss: 205.3893198CurrentTrain: epoch  1, batch     1 | loss: 165.0860273CurrentTrain: epoch  1, batch     2 | loss: 184.4348224CurrentTrain: epoch  1, batch     3 | loss: 41.9574946CurrentTrain: epoch  2, batch     0 | loss: 251.4514286CurrentTrain: epoch  2, batch     1 | loss: 169.8540621CurrentTrain: epoch  2, batch     2 | loss: 167.4396689CurrentTrain: epoch  2, batch     3 | loss: 13.4322427CurrentTrain: epoch  3, batch     0 | loss: 173.2787003CurrentTrain: epoch  3, batch     1 | loss: 206.3590975CurrentTrain: epoch  3, batch     2 | loss: 201.6845300CurrentTrain: epoch  3, batch     3 | loss: 7.0086031CurrentTrain: epoch  4, batch     0 | loss: 200.0893442CurrentTrain: epoch  4, batch     1 | loss: 163.5437694CurrentTrain: epoch  4, batch     2 | loss: 189.7785776CurrentTrain: epoch  4, batch     3 | loss: 41.2810033CurrentTrain: epoch  5, batch     0 | loss: 212.6889641CurrentTrain: epoch  5, batch     1 | loss: 169.9849084CurrentTrain: epoch  5, batch     2 | loss: 189.9653283CurrentTrain: epoch  5, batch     3 | loss: 21.3594545CurrentTrain: epoch  6, batch     0 | loss: 142.6347755CurrentTrain: epoch  6, batch     1 | loss: 193.6569071CurrentTrain: epoch  6, batch     2 | loss: 267.3207556CurrentTrain: epoch  6, batch     3 | loss: 12.9362314CurrentTrain: epoch  7, batch     0 | loss: 166.1145184CurrentTrain: epoch  7, batch     1 | loss: 220.8053703CurrentTrain: epoch  7, batch     2 | loss: 171.7134008CurrentTrain: epoch  7, batch     3 | loss: 41.1150109CurrentTrain: epoch  8, batch     0 | loss: 176.3547350CurrentTrain: epoch  8, batch     1 | loss: 187.4615460CurrentTrain: epoch  8, batch     2 | loss: 175.5097426CurrentTrain: epoch  8, batch     3 | loss: 20.4953724CurrentTrain: epoch  9, batch     0 | loss: 203.4198418CurrentTrain: epoch  9, batch     1 | loss: 167.2342509CurrentTrain: epoch  9, batch     2 | loss: 175.7845876CurrentTrain: epoch  9, batch     3 | loss: 16.2574638
MemoryTrain:  epoch  0, batch     0 | loss: 0.6908982MemoryTrain:  epoch  1, batch     0 | loss: 0.6122298MemoryTrain:  epoch  2, batch     0 | loss: 0.4423534MemoryTrain:  epoch  3, batch     0 | loss: 0.3578697MemoryTrain:  epoch  4, batch     0 | loss: 0.3011622MemoryTrain:  epoch  5, batch     0 | loss: 0.2525967MemoryTrain:  epoch  6, batch     0 | loss: 0.1950608MemoryTrain:  epoch  7, batch     0 | loss: 0.1772263MemoryTrain:  epoch  8, batch     0 | loss: 0.1416551MemoryTrain:  epoch  9, batch     0 | loss: 0.1174768

F1 score per class: {1: 0.0, 6: 0.0, 7: 0.75, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 22: 0.0, 26: 0.25, 27: 0.6666666666666666, 31: 0.6534653465346535}
Micro-average F1 score: 0.5925925925925926
Weighted-average F1 score: 0.4876551669007731
F1 score per class: {1: 0.0, 3: 0.0, 6: 0.0, 7: 0.75, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 22: 0.0, 26: 0.13333333333333333, 27: 1.0, 31: 0.7964601769911505}
Micro-average F1 score: 0.672566371681416
Weighted-average F1 score: 0.5791712370789023
F1 score per class: {1: 0.0, 3: 0.0, 6: 0.0, 7: 0.75, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 22: 0.0, 26: 0.25, 27: 1.0, 31: 0.7857142857142857}
Micro-average F1 score: 0.672566371681416
Weighted-average F1 score: 0.5721538615446179

F1 score per class: {1: 0.336, 2: 0.8, 3: 0.6666666666666666, 5: 0.964824120603015, 6: 0.16071428571428573, 7: 0.05454545454545454, 8: 0.17777777777777778, 9: 0.9803921568627451, 10: 0.3007518796992481, 11: 0.5419354838709678, 12: 0.1651376146788991, 14: 0.058823529411764705, 16: 0.7719298245614035, 17: 0.0, 18: 0.19230769230769232, 19: 0.6217616580310881, 20: 0.6987951807228916, 22: 0.7126436781609196, 24: 0.05405405405405406, 26: 0.7624309392265194, 27: 0.13333333333333333, 28: 0.23529411764705882, 29: 0.8715083798882681, 30: 0.8823529411764706, 31: 0.6666666666666666, 32: 0.8156424581005587, 33: 0.42857142857142855, 34: 0.42105263157894735, 36: 0.14084507042253522, 39: 0.10526315789473684, 40: 0.48175182481751827}
Micro-average F1 score: 0.5300136425648022
Weighted-average F1 score: 0.5574426045460238
F1 score per class: {1: 0.4, 2: 0.8235294117647058, 3: 0.926829268292683, 5: 0.9519230769230769, 6: 0.352, 7: 0.06, 8: 0.32075471698113206, 9: 0.9259259259259259, 10: 0.5783132530120482, 11: 0.648936170212766, 12: 0.6583850931677019, 14: 0.07894736842105263, 16: 0.84375, 17: 0.3333333333333333, 18: 0.17647058823529413, 19: 0.6702127659574468, 20: 0.8421052631578947, 22: 0.7446808510638298, 24: 0.05405405405405406, 26: 0.7659574468085106, 27: 0.10526315789473684, 28: 0.1917808219178082, 29: 0.9032258064516129, 30: 1.0, 31: 1.0, 32: 0.837696335078534, 33: 0.3, 34: 0.44594594594594594, 36: 0.3076923076923077, 39: 0.18181818181818182, 40: 0.5590062111801242}
Micro-average F1 score: 0.6151975683890577
Weighted-average F1 score: 0.6107669060169073
F1 score per class: {1: 0.3795620437956204, 2: 0.75, 3: 0.9156626506024096, 5: 0.9519230769230769, 6: 0.3252032520325203, 7: 0.0594059405940594, 8: 0.20833333333333334, 9: 0.9433962264150944, 10: 0.5389221556886228, 11: 0.6455026455026455, 12: 0.6013071895424836, 14: 0.07894736842105263, 16: 0.8709677419354839, 17: 0.18181818181818182, 18: 0.16393442622950818, 19: 0.6595744680851063, 20: 0.8247422680412371, 22: 0.7379679144385026, 24: 0.05405405405405406, 26: 0.7659574468085106, 27: 0.15384615384615385, 28: 0.1794871794871795, 29: 0.9139784946236559, 30: 1.0, 31: 1.0, 32: 0.8421052631578947, 33: 0.35294117647058826, 34: 0.45161290322580644, 36: 0.3076923076923077, 39: 0.18181818181818182, 40: 0.5398773006134969}
Micro-average F1 score: 0.6037274671555148
Weighted-average F1 score: 0.6032505832671623
cur_acc:  ['0.8316', '0.7123', '0.4605', '0.6228', '0.5442', '0.5926']
his_acc:  ['0.8316', '0.7601', '0.6457', '0.6240', '0.5566', '0.5300']
cur_acc des:  ['0.8448', '0.8700', '0.7614', '0.7771', '0.6556', '0.6726']
his_acc des:  ['0.8448', '0.8275', '0.7777', '0.7298', '0.6673', '0.6152']
cur_acc rrf:  ['0.8379', '0.8720', '0.7614', '0.7625', '0.6626', '0.6726']
his_acc rrf:  ['0.8379', '0.8222', '0.7658', '0.7238', '0.6548', '0.6037']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 233.1323387CurrentTrain: epoch  0, batch     1 | loss: 206.6361248CurrentTrain: epoch  0, batch     2 | loss: 191.0236967CurrentTrain: epoch  0, batch     3 | loss: 252.2648621CurrentTrain: epoch  1, batch     0 | loss: 173.1044511CurrentTrain: epoch  1, batch     1 | loss: 214.4964098CurrentTrain: epoch  1, batch     2 | loss: 188.6597379CurrentTrain: epoch  1, batch     3 | loss: 248.9544155CurrentTrain: epoch  2, batch     0 | loss: 205.3655496CurrentTrain: epoch  2, batch     1 | loss: 218.7504966CurrentTrain: epoch  2, batch     2 | loss: 215.0586079CurrentTrain: epoch  2, batch     3 | loss: 122.7696261CurrentTrain: epoch  3, batch     0 | loss: 250.1300302CurrentTrain: epoch  3, batch     1 | loss: 173.8550435CurrentTrain: epoch  3, batch     2 | loss: 242.5156752CurrentTrain: epoch  3, batch     3 | loss: 147.0205081CurrentTrain: epoch  4, batch     0 | loss: 187.2145340CurrentTrain: epoch  4, batch     1 | loss: 207.7587573CurrentTrain: epoch  4, batch     2 | loss: 180.3779706CurrentTrain: epoch  4, batch     3 | loss: 187.1817790CurrentTrain: epoch  5, batch     0 | loss: 230.0859795CurrentTrain: epoch  5, batch     1 | loss: 189.3868339CurrentTrain: epoch  5, batch     2 | loss: 239.0243516CurrentTrain: epoch  5, batch     3 | loss: 116.3342460CurrentTrain: epoch  6, batch     0 | loss: 247.6247619CurrentTrain: epoch  6, batch     1 | loss: 161.8637663CurrentTrain: epoch  6, batch     2 | loss: 213.1687496CurrentTrain: epoch  6, batch     3 | loss: 138.2764847CurrentTrain: epoch  7, batch     0 | loss: 182.5209681CurrentTrain: epoch  7, batch     1 | loss: 230.1871944CurrentTrain: epoch  7, batch     2 | loss: 185.4856106CurrentTrain: epoch  7, batch     3 | loss: 144.0386120CurrentTrain: epoch  8, batch     0 | loss: 212.1065632CurrentTrain: epoch  8, batch     1 | loss: 168.4842472CurrentTrain: epoch  8, batch     2 | loss: 187.8820914CurrentTrain: epoch  8, batch     3 | loss: 184.1884418CurrentTrain: epoch  9, batch     0 | loss: 178.9342469CurrentTrain: epoch  9, batch     1 | loss: 204.1631015CurrentTrain: epoch  9, batch     2 | loss: 229.2285760CurrentTrain: epoch  9, batch     3 | loss: 147.3121941
MemoryTrain:  epoch  0, batch     0 | loss: 0.9049216MemoryTrain:  epoch  1, batch     0 | loss: 0.7837958MemoryTrain:  epoch  2, batch     0 | loss: 0.6090615MemoryTrain:  epoch  3, batch     0 | loss: 0.4939371MemoryTrain:  epoch  4, batch     0 | loss: 0.4097288MemoryTrain:  epoch  5, batch     0 | loss: 0.3222295MemoryTrain:  epoch  6, batch     0 | loss: 0.3097512MemoryTrain:  epoch  7, batch     0 | loss: 0.2719737MemoryTrain:  epoch  8, batch     0 | loss: 0.2287684MemoryTrain:  epoch  9, batch     0 | loss: 0.1653838

F1 score per class: {1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 7: 0.8235294117647058, 38: 0.0, 8: 0.0, 11: 0.4927536231884058, 15: 0.0, 18: 0.0, 20: 0.0, 25: 0.6027397260273972, 26: 0.5365853658536586, 28: 0.6666666666666666}
Micro-average F1 score: 0.4939759036144578
Weighted-average F1 score: 0.3705343727578046
F1 score per class: {1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 8: 0.0, 10: 0.0, 11: 0.6835443037974683, 15: 0.0, 18: 0.0, 20: 0.0, 25: 0.8666666666666667, 26: 0.5882352941176471, 28: 0.7111111111111111}
Micro-average F1 score: 0.5721518987341773
Weighted-average F1 score: 0.43067251259980266
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 15: 0.75, 18: 0.0, 20: 0.0, 25: 0.6666666666666666, 26: 0.0, 28: 0.0, 34: 0.0, 35: 0.8409090909090909, 37: 0.6206896551724138, 38: 0.7111111111111111, 40: 0.0}
Micro-average F1 score: 0.5685279187817259
Weighted-average F1 score: 0.4238055921011075

F1 score per class: {1: 0.3157894736842105, 2: 0.8, 3: 0.759493670886076, 5: 0.8909952606635071, 6: 0.1111111111111111, 7: 0.05263157894736842, 8: 0.16842105263157894, 9: 0.9803921568627451, 10: 0.23931623931623933, 11: 0.5217391304347826, 12: 0.11320754716981132, 14: 0.0, 15: 0.7777777777777778, 16: 0.8135593220338984, 17: 0.0, 18: 0.0, 19: 0.5414364640883977, 20: 0.625, 22: 0.6976744186046512, 24: 0.06666666666666667, 25: 0.4927536231884058, 26: 0.7540983606557377, 27: 0.21428571428571427, 28: 0.23529411764705882, 29: 0.88268156424581, 30: 0.8484848484848485, 31: 1.0, 32: 0.7932960893854749, 33: 0.3076923076923077, 34: 0.4, 35: 0.55, 36: 0.0, 37: 0.352, 38: 0.25925925925925924, 39: 0.0, 40: 0.5225225225225225}
Micro-average F1 score: 0.5051131081499846
Weighted-average F1 score: 0.5479578434252639
F1 score per class: {1: 0.3673469387755102, 2: 0.875, 3: 0.8491620111731844, 5: 0.8839285714285714, 6: 0.3492063492063492, 7: 0.061855670103092786, 8: 0.3448275862068966, 9: 0.9259259259259259, 10: 0.5394736842105263, 11: 0.6334841628959276, 12: 0.6296296296296297, 14: 0.075, 15: 0.75, 16: 0.84375, 17: 0.0, 18: 0.18461538461538463, 19: 0.6492146596858639, 20: 0.7640449438202247, 22: 0.7011494252873564, 24: 0.05128205128205128, 25: 0.6835443037974683, 26: 0.7486631016042781, 27: 0.09523809523809523, 28: 0.16666666666666666, 29: 0.9032258064516129, 30: 1.0, 31: 1.0, 32: 0.8148148148148148, 33: 0.42105263157894735, 34: 0.48695652173913045, 35: 0.7572815533980582, 36: 0.14084507042253522, 37: 0.43859649122807015, 38: 0.3595505617977528, 39: 0.0, 40: 0.5419354838709678}
Micro-average F1 score: 0.5893195988072648
Weighted-average F1 score: 0.5887981013951288
F1 score per class: {1: 0.352112676056338, 2: 0.875, 3: 0.8287292817679558, 5: 0.8878923766816144, 6: 0.25210084033613445, 7: 0.05405405405405406, 8: 0.2962962962962963, 9: 0.9803921568627451, 10: 0.4397163120567376, 11: 0.6244343891402715, 12: 0.5882352941176471, 14: 0.07692307692307693, 15: 0.7058823529411765, 16: 0.8571428571428571, 17: 0.0, 18: 0.19230769230769232, 19: 0.6458333333333334, 20: 0.7640449438202247, 22: 0.7344632768361582, 24: 0.05405405405405406, 25: 0.6666666666666666, 26: 0.7486631016042781, 27: 0.16666666666666666, 28: 0.2028985507246377, 29: 0.9021739130434783, 30: 1.0, 31: 1.0, 32: 0.8167539267015707, 33: 0.4, 34: 0.4485981308411215, 35: 0.7474747474747475, 36: 0.058823529411764705, 37: 0.39705882352941174, 38: 0.2909090909090909, 39: 0.0, 40: 0.5316455696202531}
Micro-average F1 score: 0.57400327689787
Weighted-average F1 score: 0.5788859771748027
cur_acc:  ['0.8316', '0.7123', '0.4605', '0.6228', '0.5442', '0.5926', '0.4940']
his_acc:  ['0.8316', '0.7601', '0.6457', '0.6240', '0.5566', '0.5300', '0.5051']
cur_acc des:  ['0.8448', '0.8700', '0.7614', '0.7771', '0.6556', '0.6726', '0.5722']
his_acc des:  ['0.8448', '0.8275', '0.7777', '0.7298', '0.6673', '0.6152', '0.5893']
cur_acc rrf:  ['0.8379', '0.8720', '0.7614', '0.7625', '0.6626', '0.6726', '0.5685']
his_acc rrf:  ['0.8379', '0.8222', '0.7658', '0.7238', '0.6548', '0.6037', '0.5740']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 212.7163271CurrentTrain: epoch  0, batch     1 | loss: 224.3896300CurrentTrain: epoch  0, batch     2 | loss: 290.7876294CurrentTrain: epoch  0, batch     3 | loss: 154.0825300CurrentTrain: epoch  1, batch     0 | loss: 224.0718161CurrentTrain: epoch  1, batch     1 | loss: 189.9667389CurrentTrain: epoch  1, batch     2 | loss: 176.5542439CurrentTrain: epoch  1, batch     3 | loss: 199.4579681CurrentTrain: epoch  2, batch     0 | loss: 237.6298525CurrentTrain: epoch  2, batch     1 | loss: 194.5312791CurrentTrain: epoch  2, batch     2 | loss: 179.3603641CurrentTrain: epoch  2, batch     3 | loss: 191.5598371CurrentTrain: epoch  3, batch     0 | loss: 223.0612408CurrentTrain: epoch  3, batch     1 | loss: 188.9122823CurrentTrain: epoch  3, batch     2 | loss: 228.8155838CurrentTrain: epoch  3, batch     3 | loss: 133.5450194CurrentTrain: epoch  4, batch     0 | loss: 266.6681209CurrentTrain: epoch  4, batch     1 | loss: 190.3979009CurrentTrain: epoch  4, batch     2 | loss: 206.9892562CurrentTrain: epoch  4, batch     3 | loss: 160.3908200CurrentTrain: epoch  5, batch     0 | loss: 283.4988907CurrentTrain: epoch  5, batch     1 | loss: 176.4805312CurrentTrain: epoch  5, batch     2 | loss: 219.3798464CurrentTrain: epoch  5, batch     3 | loss: 145.4394251CurrentTrain: epoch  6, batch     0 | loss: 287.1757009CurrentTrain: epoch  6, batch     1 | loss: 228.7055337CurrentTrain: epoch  6, batch     2 | loss: 161.3206567CurrentTrain: epoch  6, batch     3 | loss: 147.3252697CurrentTrain: epoch  7, batch     0 | loss: 195.3235441CurrentTrain: epoch  7, batch     1 | loss: 228.4531697CurrentTrain: epoch  7, batch     2 | loss: 203.1285080CurrentTrain: epoch  7, batch     3 | loss: 153.5348721CurrentTrain: epoch  8, batch     0 | loss: 251.2103459CurrentTrain: epoch  8, batch     1 | loss: 202.8079434CurrentTrain: epoch  8, batch     2 | loss: 152.7405569CurrentTrain: epoch  8, batch     3 | loss: 166.2445260CurrentTrain: epoch  9, batch     0 | loss: 185.6228442CurrentTrain: epoch  9, batch     1 | loss: 187.0376704CurrentTrain: epoch  9, batch     2 | loss: 252.2822233CurrentTrain: epoch  9, batch     3 | loss: 221.9422339
MemoryTrain:  epoch  0, batch     0 | loss: 0.7376118MemoryTrain:  epoch  1, batch     0 | loss: 0.6402472MemoryTrain:  epoch  2, batch     0 | loss: 0.4746591MemoryTrain:  epoch  3, batch     0 | loss: 0.4249303MemoryTrain:  epoch  4, batch     0 | loss: 0.3263081MemoryTrain:  epoch  5, batch     0 | loss: 0.2625054MemoryTrain:  epoch  6, batch     0 | loss: 0.2323754MemoryTrain:  epoch  7, batch     0 | loss: 0.1794960MemoryTrain:  epoch  8, batch     0 | loss: 0.1632692MemoryTrain:  epoch  9, batch     0 | loss: 0.1681122

F1 score per class: {0: 0.9428571428571428, 32: 0.0, 2: 0.0, 1: 0.8950276243093923, 4: 0.0, 11: 0.8888888888888888, 13: 0.0, 15: 0.6222222222222222, 21: 0.7466666666666667, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.7785888077858881
Weighted-average F1 score: 0.7161510817627215
F1 score per class: {0: 0.9722222222222222, 32: 0.0, 2: 0.0, 1: 0.8571428571428571, 4: 0.0, 37: 1.0, 40: 0.0, 11: 0.7346938775510204, 13: 0.7631578947368421, 15: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7552447552447552
Weighted-average F1 score: 0.6634907215635351
F1 score per class: {0: 0.9722222222222222, 32: 0.0, 2: 0.0, 1: 0.8700564971751412, 4: 0.0, 37: 0.0, 5: 1.0, 40: 0.0, 11: 0.7346938775510204, 13: 0.7297297297297297, 15: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7570093457943925
Weighted-average F1 score: 0.6680255796100227

F1 score per class: {0: 0.9295774647887324, 1: 0.3181818181818182, 2: 0.5833333333333334, 3: 0.6906474820143885, 4: 0.8950276243093923, 5: 0.8732394366197183, 6: 0.05825242718446602, 7: 0.061224489795918366, 8: 0.11363636363636363, 9: 0.9803921568627451, 10: 0.09433962264150944, 11: 0.4503311258278146, 12: 0.05714285714285714, 13: 0.07476635514018691, 14: 0.0, 15: 0.6666666666666666, 16: 0.8524590163934426, 17: 0.0, 18: 0.047619047619047616, 19: 0.5434782608695652, 20: 0.5822784810126582, 21: 0.3010752688172043, 22: 0.5454545454545454, 23: 0.717948717948718, 24: 0.0, 25: 0.42424242424242425, 26: 0.7150259067357513, 27: 0.15384615384615385, 28: 0.3076923076923077, 29: 0.8777777777777778, 30: 0.8823529411764706, 31: 0.8, 32: 0.6993865030674846, 33: 0.16666666666666666, 34: 0.44155844155844154, 35: 0.4, 36: 0.029850746268656716, 37: 0.39655172413793105, 38: 0.3333333333333333, 39: 0.0, 40: 0.5142857142857142}
Micro-average F1 score: 0.4963707426018984
Weighted-average F1 score: 0.5389109545647464
F1 score per class: {0: 0.958904109589041, 1: 0.3472222222222222, 2: 0.4117647058823529, 3: 0.8823529411764706, 4: 0.847457627118644, 5: 0.8722466960352423, 6: 0.2833333333333333, 7: 0.06382978723404255, 8: 0.43103448275862066, 9: 0.9803921568627451, 10: 0.3064516129032258, 11: 0.6232558139534884, 12: 0.5789473684210527, 13: 0.09523809523809523, 14: 0.1038961038961039, 15: 0.7058823529411765, 16: 0.84375, 17: 0.0, 18: 0.19718309859154928, 19: 0.656084656084656, 20: 0.7586206896551724, 21: 0.3103448275862069, 22: 0.4927536231884058, 23: 0.725, 24: 0.0, 25: 0.6153846153846154, 26: 0.6965174129353234, 27: 0.16, 28: 0.25806451612903225, 29: 0.9042553191489362, 30: 0.9444444444444444, 31: 1.0, 32: 0.7888888888888889, 33: 0.42857142857142855, 34: 0.5454545454545454, 35: 0.76, 36: 0.2631578947368421, 37: 0.3888888888888889, 38: 0.5333333333333333, 39: 0.0, 40: 0.5584415584415584}
Micro-average F1 score: 0.5809617271835132
Weighted-average F1 score: 0.5775344058923243
F1 score per class: {0: 0.958904109589041, 1: 0.3404255319148936, 2: 0.4375, 3: 0.872093023255814, 4: 0.8603351955307262, 5: 0.868421052631579, 6: 0.2857142857142857, 7: 0.08333333333333333, 8: 0.2830188679245283, 9: 0.9803921568627451, 10: 0.25, 11: 0.6079295154185022, 12: 0.496551724137931, 13: 0.08333333333333333, 14: 0.05555555555555555, 15: 0.6666666666666666, 16: 0.84375, 17: 0.0, 18: 0.1875, 19: 0.6458333333333334, 20: 0.75, 21: 0.3, 22: 0.4927536231884058, 23: 0.6923076923076923, 24: 0.0, 25: 0.5974025974025974, 26: 0.7, 27: 0.16, 28: 0.36363636363636365, 29: 0.9032258064516129, 30: 0.9444444444444444, 31: 1.0, 32: 0.7912087912087912, 33: 0.42857142857142855, 34: 0.5094339622641509, 35: 0.7368421052631579, 36: 0.1917808219178082, 37: 0.41379310344827586, 38: 0.4473684210526316, 39: 0.0, 40: 0.5584415584415584}
Micro-average F1 score: 0.5664939550949913
Weighted-average F1 score: 0.5691076406274352
cur_acc:  ['0.8316', '0.7123', '0.4605', '0.6228', '0.5442', '0.5926', '0.4940', '0.7786']
his_acc:  ['0.8316', '0.7601', '0.6457', '0.6240', '0.5566', '0.5300', '0.5051', '0.4964']
cur_acc des:  ['0.8448', '0.8700', '0.7614', '0.7771', '0.6556', '0.6726', '0.5722', '0.7552']
his_acc des:  ['0.8448', '0.8275', '0.7777', '0.7298', '0.6673', '0.6152', '0.5893', '0.5810']
cur_acc rrf:  ['0.8379', '0.8720', '0.7614', '0.7625', '0.6626', '0.6726', '0.5685', '0.7570']
his_acc rrf:  ['0.8379', '0.8222', '0.7658', '0.7238', '0.6548', '0.6037', '0.5740', '0.5665']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 224.7275540CurrentTrain: epoch  0, batch     1 | loss: 202.6416089CurrentTrain: epoch  0, batch     2 | loss: 252.3753395CurrentTrain: epoch  0, batch     3 | loss: 239.3071136CurrentTrain: epoch  0, batch     4 | loss: 290.7438498CurrentTrain: epoch  0, batch     5 | loss: 258.5913903CurrentTrain: epoch  0, batch     6 | loss: 253.5817671CurrentTrain: epoch  0, batch     7 | loss: 232.9839714CurrentTrain: epoch  0, batch     8 | loss: 244.5416689CurrentTrain: epoch  0, batch     9 | loss: 233.7800715CurrentTrain: epoch  0, batch    10 | loss: 294.6659734CurrentTrain: epoch  0, batch    11 | loss: 193.9497006CurrentTrain: epoch  0, batch    12 | loss: 180.4165397CurrentTrain: epoch  0, batch    13 | loss: 196.1779220CurrentTrain: epoch  0, batch    14 | loss: 224.6199357CurrentTrain: epoch  0, batch    15 | loss: 224.4595602CurrentTrain: epoch  0, batch    16 | loss: 204.2770828CurrentTrain: epoch  0, batch    17 | loss: 296.0852661CurrentTrain: epoch  0, batch    18 | loss: 357.0775430CurrentTrain: epoch  0, batch    19 | loss: 217.4488905CurrentTrain: epoch  0, batch    20 | loss: 280.6242364CurrentTrain: epoch  0, batch    21 | loss: 173.5681388CurrentTrain: epoch  0, batch    22 | loss: 209.9409793CurrentTrain: epoch  0, batch    23 | loss: 163.6609973CurrentTrain: epoch  0, batch    24 | loss: 216.9150673CurrentTrain: epoch  0, batch    25 | loss: 223.2949529CurrentTrain: epoch  0, batch    26 | loss: 203.9203154CurrentTrain: epoch  0, batch    27 | loss: 288.4237184CurrentTrain: epoch  0, batch    28 | loss: 230.9503134CurrentTrain: epoch  0, batch    29 | loss: 208.8576759CurrentTrain: epoch  0, batch    30 | loss: 224.1248252CurrentTrain: epoch  0, batch    31 | loss: 174.9902174CurrentTrain: epoch  0, batch    32 | loss: 216.5879244CurrentTrain: epoch  0, batch    33 | loss: 287.3036827CurrentTrain: epoch  0, batch    34 | loss: 286.9609005CurrentTrain: epoch  0, batch    35 | loss: 162.7953115CurrentTrain: epoch  0, batch    36 | loss: 196.2814124CurrentTrain: epoch  0, batch    37 | loss: 222.9197868CurrentTrain: epoch  0, batch    38 | loss: 190.0619831CurrentTrain: epoch  0, batch    39 | loss: 227.4914694CurrentTrain: epoch  0, batch    40 | loss: 152.2777425CurrentTrain: epoch  0, batch    41 | loss: 229.1779948CurrentTrain: epoch  0, batch    42 | loss: 201.4890247CurrentTrain: epoch  0, batch    43 | loss: 204.3312358CurrentTrain: epoch  0, batch    44 | loss: 171.7319837CurrentTrain: epoch  0, batch    45 | loss: 203.4413299CurrentTrain: epoch  0, batch    46 | loss: 222.6679955CurrentTrain: epoch  0, batch    47 | loss: 280.7123554CurrentTrain: epoch  0, batch    48 | loss: 190.2704553CurrentTrain: epoch  0, batch    49 | loss: 222.3518571CurrentTrain: epoch  0, batch    50 | loss: 169.4497173CurrentTrain: epoch  0, batch    51 | loss: 166.0873936CurrentTrain: epoch  0, batch    52 | loss: 256.0661341CurrentTrain: epoch  0, batch    53 | loss: 248.3474569CurrentTrain: epoch  0, batch    54 | loss: 208.7049764CurrentTrain: epoch  0, batch    55 | loss: 214.2042412CurrentTrain: epoch  0, batch    56 | loss: 286.2787831CurrentTrain: epoch  0, batch    57 | loss: 220.5552538CurrentTrain: epoch  0, batch    58 | loss: 247.9246358CurrentTrain: epoch  0, batch    59 | loss: 229.2307936CurrentTrain: epoch  0, batch    60 | loss: 224.0899427CurrentTrain: epoch  0, batch    61 | loss: 229.1110179CurrentTrain: epoch  0, batch    62 | loss: 200.6811575CurrentTrain: epoch  0, batch    63 | loss: 229.1117536CurrentTrain: epoch  0, batch    64 | loss: 172.8636093CurrentTrain: epoch  0, batch    65 | loss: 239.3769273CurrentTrain: epoch  0, batch    66 | loss: 213.8046209CurrentTrain: epoch  0, batch    67 | loss: 214.4242070CurrentTrain: epoch  0, batch    68 | loss: 196.2927824CurrentTrain: epoch  0, batch    69 | loss: 198.4832248CurrentTrain: epoch  0, batch    70 | loss: 188.9885792CurrentTrain: epoch  0, batch    71 | loss: 238.2851110CurrentTrain: epoch  0, batch    72 | loss: 199.1915863CurrentTrain: epoch  0, batch    73 | loss: 192.4101110CurrentTrain: epoch  0, batch    74 | loss: 193.2709946CurrentTrain: epoch  0, batch    75 | loss: 224.4963289CurrentTrain: epoch  0, batch    76 | loss: 214.6647943CurrentTrain: epoch  0, batch    77 | loss: 230.1859334CurrentTrain: epoch  0, batch    78 | loss: 252.9619374CurrentTrain: epoch  0, batch    79 | loss: 362.1233813CurrentTrain: epoch  0, batch    80 | loss: 184.9467476CurrentTrain: epoch  0, batch    81 | loss: 174.0291404CurrentTrain: epoch  0, batch    82 | loss: 187.5351856CurrentTrain: epoch  0, batch    83 | loss: 227.6990257CurrentTrain: epoch  0, batch    84 | loss: 202.1571863CurrentTrain: epoch  0, batch    85 | loss: 196.9903830CurrentTrain: epoch  0, batch    86 | loss: 238.0030469CurrentTrain: epoch  0, batch    87 | loss: 231.9661612CurrentTrain: epoch  0, batch    88 | loss: 223.8815488CurrentTrain: epoch  0, batch    89 | loss: 176.5095898CurrentTrain: epoch  0, batch    90 | loss: 200.0226175CurrentTrain: epoch  0, batch    91 | loss: 218.1399926CurrentTrain: epoch  0, batch    92 | loss: 218.7137481CurrentTrain: epoch  0, batch    93 | loss: 247.1208916CurrentTrain: epoch  0, batch    94 | loss: 192.4923192CurrentTrain: epoch  0, batch    95 | loss: 188.5550969CurrentTrain: epoch  1, batch     0 | loss: 273.0078359CurrentTrain: epoch  1, batch     1 | loss: 191.9688075CurrentTrain: epoch  1, batch     2 | loss: 210.6009284CurrentTrain: epoch  1, batch     3 | loss: 360.9517275CurrentTrain: epoch  1, batch     4 | loss: 193.8527067CurrentTrain: epoch  1, batch     5 | loss: 245.4566529CurrentTrain: epoch  1, batch     6 | loss: 243.3488246CurrentTrain: epoch  1, batch     7 | loss: 211.1829217CurrentTrain: epoch  1, batch     8 | loss: 176.7045766CurrentTrain: epoch  1, batch     9 | loss: 214.8582389CurrentTrain: epoch  1, batch    10 | loss: 208.7051343CurrentTrain: epoch  1, batch    11 | loss: 218.9957538CurrentTrain: epoch  1, batch    12 | loss: 219.4323045CurrentTrain: epoch  1, batch    13 | loss: 190.6274111CurrentTrain: epoch  1, batch    14 | loss: 203.7834230CurrentTrain: epoch  1, batch    15 | loss: 184.5578977CurrentTrain: epoch  1, batch    16 | loss: 251.6454025CurrentTrain: epoch  1, batch    17 | loss: 192.7096413CurrentTrain: epoch  1, batch    18 | loss: 228.4225620CurrentTrain: epoch  1, batch    19 | loss: 180.9575112CurrentTrain: epoch  1, batch    20 | loss: 216.3019894CurrentTrain: epoch  1, batch    21 | loss: 190.7456197CurrentTrain: epoch  1, batch    22 | loss: 180.3021318CurrentTrain: epoch  1, batch    23 | loss: 282.0630588CurrentTrain: epoch  1, batch    24 | loss: 177.7593405CurrentTrain: epoch  1, batch    25 | loss: 190.7508133CurrentTrain: epoch  1, batch    26 | loss: 208.8668362CurrentTrain: epoch  1, batch    27 | loss: 177.0416394CurrentTrain: epoch  1, batch    28 | loss: 287.8358650CurrentTrain: epoch  1, batch    29 | loss: 241.9869547CurrentTrain: epoch  1, batch    30 | loss: 198.7465318CurrentTrain: epoch  1, batch    31 | loss: 200.8809270CurrentTrain: epoch  1, batch    32 | loss: 233.6514540CurrentTrain: epoch  1, batch    33 | loss: 207.4068465CurrentTrain: epoch  1, batch    34 | loss: 232.4322656CurrentTrain: epoch  1, batch    35 | loss: 292.7713501CurrentTrain: epoch  1, batch    36 | loss: 240.0016812CurrentTrain: epoch  1, batch    37 | loss: 169.0681401CurrentTrain: epoch  1, batch    38 | loss: 292.3322424CurrentTrain: epoch  1, batch    39 | loss: 231.7297298CurrentTrain: epoch  1, batch    40 | loss: 280.0158314CurrentTrain: epoch  1, batch    41 | loss: 279.8207500CurrentTrain: epoch  1, batch    42 | loss: 154.2817833CurrentTrain: epoch  1, batch    43 | loss: 188.6914288CurrentTrain: epoch  1, batch    44 | loss: 214.8608369CurrentTrain: epoch  1, batch    45 | loss: 220.2153873CurrentTrain: epoch  1, batch    46 | loss: 183.0811552CurrentTrain: epoch  1, batch    47 | loss: 185.0875468CurrentTrain: epoch  1, batch    48 | loss: 203.1837073CurrentTrain: epoch  1, batch    49 | loss: 180.9880994CurrentTrain: epoch  1, batch    50 | loss: 136.7005140CurrentTrain: epoch  1, batch    51 | loss: 182.2597466CurrentTrain: epoch  1, batch    52 | loss: 236.8935988CurrentTrain: epoch  1, batch    53 | loss: 217.7704942CurrentTrain: epoch  1, batch    54 | loss: 160.4900613CurrentTrain: epoch  1, batch    55 | loss: 209.4866877CurrentTrain: epoch  1, batch    56 | loss: 188.5138673CurrentTrain: epoch  1, batch    57 | loss: 291.5192509CurrentTrain: epoch  1, batch    58 | loss: 276.6199314CurrentTrain: epoch  1, batch    59 | loss: 189.4486106CurrentTrain: epoch  1, batch    60 | loss: 166.5183816CurrentTrain: epoch  1, batch    61 | loss: 194.7062231CurrentTrain: epoch  1, batch    62 | loss: 180.7486484CurrentTrain: epoch  1, batch    63 | loss: 181.1624667CurrentTrain: epoch  1, batch    64 | loss: 192.1260965CurrentTrain: epoch  1, batch    65 | loss: 274.3184246CurrentTrain: epoch  1, batch    66 | loss: 222.0229257CurrentTrain: epoch  1, batch    67 | loss: 278.0642565CurrentTrain: epoch  1, batch    68 | loss: 233.6015542CurrentTrain: epoch  1, batch    69 | loss: 235.8213905CurrentTrain: epoch  1, batch    70 | loss: 232.9994642CurrentTrain: epoch  1, batch    71 | loss: 209.0266372CurrentTrain: epoch  1, batch    72 | loss: 208.2306938CurrentTrain: epoch  1, batch    73 | loss: 179.8340160CurrentTrain: epoch  1, batch    74 | loss: 279.5096910CurrentTrain: epoch  1, batch    75 | loss: 209.5705304CurrentTrain: epoch  1, batch    76 | loss: 206.2454038CurrentTrain: epoch  1, batch    77 | loss: 196.0552177CurrentTrain: epoch  1, batch    78 | loss: 225.5727782CurrentTrain: epoch  1, batch    79 | loss: 225.9320461CurrentTrain: epoch  1, batch    80 | loss: 206.1884168CurrentTrain: epoch  1, batch    81 | loss: 185.7705803CurrentTrain: epoch  1, batch    82 | loss: 174.9216593CurrentTrain: epoch  1, batch    83 | loss: 198.4370580CurrentTrain: epoch  1, batch    84 | loss: 192.0435193CurrentTrain: epoch  1, batch    85 | loss: 224.0652791CurrentTrain: epoch  1, batch    86 | loss: 249.4750067CurrentTrain: epoch  1, batch    87 | loss: 245.3343584CurrentTrain: epoch  1, batch    88 | loss: 188.7667978CurrentTrain: epoch  1, batch    89 | loss: 196.8861298CurrentTrain: epoch  1, batch    90 | loss: 187.2074590CurrentTrain: epoch  1, batch    91 | loss: 196.0673401CurrentTrain: epoch  1, batch    92 | loss: 186.5983368CurrentTrain: epoch  1, batch    93 | loss: 199.8416924CurrentTrain: epoch  1, batch    94 | loss: 215.9138312CurrentTrain: epoch  1, batch    95 | loss: 155.5924284CurrentTrain: epoch  2, batch     0 | loss: 196.8525929CurrentTrain: epoch  2, batch     1 | loss: 195.5858271CurrentTrain: epoch  2, batch     2 | loss: 195.9747047CurrentTrain: epoch  2, batch     3 | loss: 267.2592395CurrentTrain: epoch  2, batch     4 | loss: 241.2446243CurrentTrain: epoch  2, batch     5 | loss: 186.5904250CurrentTrain: epoch  2, batch     6 | loss: 249.1567797CurrentTrain: epoch  2, batch     7 | loss: 232.8341934CurrentTrain: epoch  2, batch     8 | loss: 182.6896780CurrentTrain: epoch  2, batch     9 | loss: 230.7577208CurrentTrain: epoch  2, batch    10 | loss: 218.3855026CurrentTrain: epoch  2, batch    11 | loss: 190.2144145CurrentTrain: epoch  2, batch    12 | loss: 187.9414448CurrentTrain: epoch  2, batch    13 | loss: 221.7254683CurrentTrain: epoch  2, batch    14 | loss: 213.2057316CurrentTrain: epoch  2, batch    15 | loss: 239.6749390CurrentTrain: epoch  2, batch    16 | loss: 230.1513157CurrentTrain: epoch  2, batch    17 | loss: 205.7883904CurrentTrain: epoch  2, batch    18 | loss: 224.3688328CurrentTrain: epoch  2, batch    19 | loss: 196.4289433CurrentTrain: epoch  2, batch    20 | loss: 228.9311550CurrentTrain: epoch  2, batch    21 | loss: 238.8746555CurrentTrain: epoch  2, batch    22 | loss: 214.1649175CurrentTrain: epoch  2, batch    23 | loss: 290.8834270CurrentTrain: epoch  2, batch    24 | loss: 216.3558524CurrentTrain: epoch  2, batch    25 | loss: 288.2944514CurrentTrain: epoch  2, batch    26 | loss: 223.0717876CurrentTrain: epoch  2, batch    27 | loss: 222.8718787CurrentTrain: epoch  2, batch    28 | loss: 268.8855270CurrentTrain: epoch  2, batch    29 | loss: 171.0560402CurrentTrain: epoch  2, batch    30 | loss: 277.2924342CurrentTrain: epoch  2, batch    31 | loss: 178.6063063CurrentTrain: epoch  2, batch    32 | loss: 247.8942923CurrentTrain: epoch  2, batch    33 | loss: 177.3613838CurrentTrain: epoch  2, batch    34 | loss: 205.2251025CurrentTrain: epoch  2, batch    35 | loss: 182.6566615CurrentTrain: epoch  2, batch    36 | loss: 136.5410616CurrentTrain: epoch  2, batch    37 | loss: 241.9289603CurrentTrain: epoch  2, batch    38 | loss: 204.6206355CurrentTrain: epoch  2, batch    39 | loss: 158.9488978CurrentTrain: epoch  2, batch    40 | loss: 240.7869803CurrentTrain: epoch  2, batch    41 | loss: 196.4066736CurrentTrain: epoch  2, batch    42 | loss: 265.4276655CurrentTrain: epoch  2, batch    43 | loss: 229.3658946CurrentTrain: epoch  2, batch    44 | loss: 217.5491219CurrentTrain: epoch  2, batch    45 | loss: 234.5558005CurrentTrain: epoch  2, batch    46 | loss: 258.8215050CurrentTrain: epoch  2, batch    47 | loss: 267.5586339CurrentTrain: epoch  2, batch    48 | loss: 193.5386088CurrentTrain: epoch  2, batch    49 | loss: 188.3235548CurrentTrain: epoch  2, batch    50 | loss: 180.0317957CurrentTrain: epoch  2, batch    51 | loss: 267.8113848CurrentTrain: epoch  2, batch    52 | loss: 281.0954411CurrentTrain: epoch  2, batch    53 | loss: 168.4791346CurrentTrain: epoch  2, batch    54 | loss: 252.2317611CurrentTrain: epoch  2, batch    55 | loss: 162.9428789CurrentTrain: epoch  2, batch    56 | loss: 157.3099558CurrentTrain: epoch  2, batch    57 | loss: 215.6724470CurrentTrain: epoch  2, batch    58 | loss: 169.0107368CurrentTrain: epoch  2, batch    59 | loss: 185.1494576CurrentTrain: epoch  2, batch    60 | loss: 221.5437415CurrentTrain: epoch  2, batch    61 | loss: 208.6107710CurrentTrain: epoch  2, batch    62 | loss: 214.8544577CurrentTrain: epoch  2, batch    63 | loss: 206.6931493CurrentTrain: epoch  2, batch    64 | loss: 220.4962101CurrentTrain: epoch  2, batch    65 | loss: 206.1317450CurrentTrain: epoch  2, batch    66 | loss: 216.3679379CurrentTrain: epoch  2, batch    67 | loss: 228.4973959CurrentTrain: epoch  2, batch    68 | loss: 240.7289459CurrentTrain: epoch  2, batch    69 | loss: 195.8107014CurrentTrain: epoch  2, batch    70 | loss: 250.8972172CurrentTrain: epoch  2, batch    71 | loss: 179.6814138CurrentTrain: epoch  2, batch    72 | loss: 215.5766655CurrentTrain: epoch  2, batch    73 | loss: 162.3670998CurrentTrain: epoch  2, batch    74 | loss: 205.6435126CurrentTrain: epoch  2, batch    75 | loss: 184.6904155CurrentTrain: epoch  2, batch    76 | loss: 163.8417219CurrentTrain: epoch  2, batch    77 | loss: 199.8644153CurrentTrain: epoch  2, batch    78 | loss: 186.6375037CurrentTrain: epoch  2, batch    79 | loss: 208.9859299CurrentTrain: epoch  2, batch    80 | loss: 270.6695660CurrentTrain: epoch  2, batch    81 | loss: 157.3363905CurrentTrain: epoch  2, batch    82 | loss: 232.5417654CurrentTrain: epoch  2, batch    83 | loss: 187.9184040CurrentTrain: epoch  2, batch    84 | loss: 212.8172715CurrentTrain: epoch  2, batch    85 | loss: 166.2003933CurrentTrain: epoch  2, batch    86 | loss: 174.1180092CurrentTrain: epoch  2, batch    87 | loss: 197.2931484CurrentTrain: epoch  2, batch    88 | loss: 251.5382485CurrentTrain: epoch  2, batch    89 | loss: 178.2726654CurrentTrain: epoch  2, batch    90 | loss: 164.8179615CurrentTrain: epoch  2, batch    91 | loss: 206.9799997CurrentTrain: epoch  2, batch    92 | loss: 216.4543977CurrentTrain: epoch  2, batch    93 | loss: 178.2704545CurrentTrain: epoch  2, batch    94 | loss: 178.1458953CurrentTrain: epoch  2, batch    95 | loss: 177.8410951CurrentTrain: epoch  3, batch     0 | loss: 279.1633699CurrentTrain: epoch  3, batch     1 | loss: 221.2412703CurrentTrain: epoch  3, batch     2 | loss: 231.3959763CurrentTrain: epoch  3, batch     3 | loss: 192.9787275CurrentTrain: epoch  3, batch     4 | loss: 232.0321115CurrentTrain: epoch  3, batch     5 | loss: 213.2515597CurrentTrain: epoch  3, batch     6 | loss: 195.2989741CurrentTrain: epoch  3, batch     7 | loss: 239.5708045CurrentTrain: epoch  3, batch     8 | loss: 172.0051132CurrentTrain: epoch  3, batch     9 | loss: 186.1772996CurrentTrain: epoch  3, batch    10 | loss: 156.0549502CurrentTrain: epoch  3, batch    11 | loss: 200.7403256CurrentTrain: epoch  3, batch    12 | loss: 238.7257444CurrentTrain: epoch  3, batch    13 | loss: 204.2823771CurrentTrain: epoch  3, batch    14 | loss: 179.9424613CurrentTrain: epoch  3, batch    15 | loss: 215.4211056CurrentTrain: epoch  3, batch    16 | loss: 230.5228674CurrentTrain: epoch  3, batch    17 | loss: 278.7612535CurrentTrain: epoch  3, batch    18 | loss: 172.7874689CurrentTrain: epoch  3, batch    19 | loss: 288.6165089CurrentTrain: epoch  3, batch    20 | loss: 267.1863703CurrentTrain: epoch  3, batch    21 | loss: 196.4974723CurrentTrain: epoch  3, batch    22 | loss: 195.7955753CurrentTrain: epoch  3, batch    23 | loss: 200.8817121CurrentTrain: epoch  3, batch    24 | loss: 231.6072766CurrentTrain: epoch  3, batch    25 | loss: 231.4948240CurrentTrain: epoch  3, batch    26 | loss: 171.0223885CurrentTrain: epoch  3, batch    27 | loss: 262.6126956CurrentTrain: epoch  3, batch    28 | loss: 213.5419803CurrentTrain: epoch  3, batch    29 | loss: 170.1215557CurrentTrain: epoch  3, batch    30 | loss: 196.8588575CurrentTrain: epoch  3, batch    31 | loss: 222.6216511CurrentTrain: epoch  3, batch    32 | loss: 195.5564606CurrentTrain: epoch  3, batch    33 | loss: 144.9207241CurrentTrain: epoch  3, batch    34 | loss: 271.8653600CurrentTrain: epoch  3, batch    35 | loss: 160.9168262CurrentTrain: epoch  3, batch    36 | loss: 212.7924750CurrentTrain: epoch  3, batch    37 | loss: 200.3300134CurrentTrain: epoch  3, batch    38 | loss: 157.8480353CurrentTrain: epoch  3, batch    39 | loss: 211.2580881CurrentTrain: epoch  3, batch    40 | loss: 205.6267056CurrentTrain: epoch  3, batch    41 | loss: 214.5806132CurrentTrain: epoch  3, batch    42 | loss: 212.4939474CurrentTrain: epoch  3, batch    43 | loss: 269.8682052CurrentTrain: epoch  3, batch    44 | loss: 176.9295796CurrentTrain: epoch  3, batch    45 | loss: 176.8696377CurrentTrain: epoch  3, batch    46 | loss: 163.5842130CurrentTrain: epoch  3, batch    47 | loss: 206.8482866CurrentTrain: epoch  3, batch    48 | loss: 187.7684093CurrentTrain: epoch  3, batch    49 | loss: 212.3230581CurrentTrain: epoch  3, batch    50 | loss: 186.0908880CurrentTrain: epoch  3, batch    51 | loss: 182.5826455CurrentTrain: epoch  3, batch    52 | loss: 225.7057411CurrentTrain: epoch  3, batch    53 | loss: 287.8951152CurrentTrain: epoch  3, batch    54 | loss: 156.6907425CurrentTrain: epoch  3, batch    55 | loss: 177.7937103CurrentTrain: epoch  3, batch    56 | loss: 249.7223982CurrentTrain: epoch  3, batch    57 | loss: 161.5152173CurrentTrain: epoch  3, batch    58 | loss: 204.3331866CurrentTrain: epoch  3, batch    59 | loss: 176.6125035CurrentTrain: epoch  3, batch    60 | loss: 172.4247662CurrentTrain: epoch  3, batch    61 | loss: 232.1667536CurrentTrain: epoch  3, batch    62 | loss: 185.7444212CurrentTrain: epoch  3, batch    63 | loss: 203.0836251CurrentTrain: epoch  3, batch    64 | loss: 239.1009251CurrentTrain: epoch  3, batch    65 | loss: 224.7951961CurrentTrain: epoch  3, batch    66 | loss: 238.6602382CurrentTrain: epoch  3, batch    67 | loss: 195.0513593CurrentTrain: epoch  3, batch    68 | loss: 220.8066166CurrentTrain: epoch  3, batch    69 | loss: 198.3272181CurrentTrain: epoch  3, batch    70 | loss: 148.2364709CurrentTrain: epoch  3, batch    71 | loss: 175.4316162CurrentTrain: epoch  3, batch    72 | loss: 203.0964280CurrentTrain: epoch  3, batch    73 | loss: 205.7710548CurrentTrain: epoch  3, batch    74 | loss: 242.1610970CurrentTrain: epoch  3, batch    75 | loss: 336.7589870CurrentTrain: epoch  3, batch    76 | loss: 250.7178448CurrentTrain: epoch  3, batch    77 | loss: 238.3287809CurrentTrain: epoch  3, batch    78 | loss: 221.1795776CurrentTrain: epoch  3, batch    79 | loss: 189.5344446CurrentTrain: epoch  3, batch    80 | loss: 155.5063955CurrentTrain: epoch  3, batch    81 | loss: 195.3237036CurrentTrain: epoch  3, batch    82 | loss: 203.3600430CurrentTrain: epoch  3, batch    83 | loss: 270.1561674CurrentTrain: epoch  3, batch    84 | loss: 360.2228654CurrentTrain: epoch  3, batch    85 | loss: 198.2423545CurrentTrain: epoch  3, batch    86 | loss: 233.2357338CurrentTrain: epoch  3, batch    87 | loss: 221.0249815CurrentTrain: epoch  3, batch    88 | loss: 239.5574998CurrentTrain: epoch  3, batch    89 | loss: 195.4781494CurrentTrain: epoch  3, batch    90 | loss: 216.7290455CurrentTrain: epoch  3, batch    91 | loss: 165.1696253CurrentTrain: epoch  3, batch    92 | loss: 269.1934663CurrentTrain: epoch  3, batch    93 | loss: 289.4287284CurrentTrain: epoch  3, batch    94 | loss: 154.9234451CurrentTrain: epoch  3, batch    95 | loss: 180.4040254CurrentTrain: epoch  4, batch     0 | loss: 186.0927617CurrentTrain: epoch  4, batch     1 | loss: 241.3313228CurrentTrain: epoch  4, batch     2 | loss: 183.7096066CurrentTrain: epoch  4, batch     3 | loss: 211.9112791CurrentTrain: epoch  4, batch     4 | loss: 161.9366024CurrentTrain: epoch  4, batch     5 | loss: 220.7838191CurrentTrain: epoch  4, batch     6 | loss: 269.6297327CurrentTrain: epoch  4, batch     7 | loss: 204.6349041CurrentTrain: epoch  4, batch     8 | loss: 202.7212340CurrentTrain: epoch  4, batch     9 | loss: 179.7010513CurrentTrain: epoch  4, batch    10 | loss: 266.3068739CurrentTrain: epoch  4, batch    11 | loss: 213.8214091CurrentTrain: epoch  4, batch    12 | loss: 277.9199403CurrentTrain: epoch  4, batch    13 | loss: 203.7475996CurrentTrain: epoch  4, batch    14 | loss: 201.4158673CurrentTrain: epoch  4, batch    15 | loss: 258.5236932CurrentTrain: epoch  4, batch    16 | loss: 558.9805619CurrentTrain: epoch  4, batch    17 | loss: 195.0034035CurrentTrain: epoch  4, batch    18 | loss: 204.1963444CurrentTrain: epoch  4, batch    19 | loss: 164.4275716CurrentTrain: epoch  4, batch    20 | loss: 155.0125830CurrentTrain: epoch  4, batch    21 | loss: 220.4747671CurrentTrain: epoch  4, batch    22 | loss: 161.8122167CurrentTrain: epoch  4, batch    23 | loss: 212.4871316CurrentTrain: epoch  4, batch    24 | loss: 222.0944913CurrentTrain: epoch  4, batch    25 | loss: 198.6693106CurrentTrain: epoch  4, batch    26 | loss: 220.9716170CurrentTrain: epoch  4, batch    27 | loss: 194.8970152CurrentTrain: epoch  4, batch    28 | loss: 203.2588380CurrentTrain: epoch  4, batch    29 | loss: 178.7139110CurrentTrain: epoch  4, batch    30 | loss: 204.7062238CurrentTrain: epoch  4, batch    31 | loss: 219.7268355CurrentTrain: epoch  4, batch    32 | loss: 151.8630617CurrentTrain: epoch  4, batch    33 | loss: 347.1146839CurrentTrain: epoch  4, batch    34 | loss: 191.2837426CurrentTrain: epoch  4, batch    35 | loss: 237.9566194CurrentTrain: epoch  4, batch    36 | loss: 156.4548454CurrentTrain: epoch  4, batch    37 | loss: 287.2886963CurrentTrain: epoch  4, batch    38 | loss: 213.6342320CurrentTrain: epoch  4, batch    39 | loss: 183.9906201CurrentTrain: epoch  4, batch    40 | loss: 363.7223467CurrentTrain: epoch  4, batch    41 | loss: 197.9367065CurrentTrain: epoch  4, batch    42 | loss: 193.8806583CurrentTrain: epoch  4, batch    43 | loss: 172.6658479CurrentTrain: epoch  4, batch    44 | loss: 179.2587841CurrentTrain: epoch  4, batch    45 | loss: 248.7316320CurrentTrain: epoch  4, batch    46 | loss: 162.5315431CurrentTrain: epoch  4, batch    47 | loss: 266.2980992CurrentTrain: epoch  4, batch    48 | loss: 157.6718668CurrentTrain: epoch  4, batch    49 | loss: 212.9593064CurrentTrain: epoch  4, batch    50 | loss: 212.1007888CurrentTrain: epoch  4, batch    51 | loss: 213.0603995CurrentTrain: epoch  4, batch    52 | loss: 196.1769199CurrentTrain: epoch  4, batch    53 | loss: 169.2108307CurrentTrain: epoch  4, batch    54 | loss: 276.2285940CurrentTrain: epoch  4, batch    55 | loss: 212.2073453CurrentTrain: epoch  4, batch    56 | loss: 220.2197205CurrentTrain: epoch  4, batch    57 | loss: 242.2204777CurrentTrain: epoch  4, batch    58 | loss: 197.3912446CurrentTrain: epoch  4, batch    59 | loss: 203.2597105CurrentTrain: epoch  4, batch    60 | loss: 169.8808966CurrentTrain: epoch  4, batch    61 | loss: 177.7334781CurrentTrain: epoch  4, batch    62 | loss: 197.4285666CurrentTrain: epoch  4, batch    63 | loss: 221.8473183CurrentTrain: epoch  4, batch    64 | loss: 229.6067863CurrentTrain: epoch  4, batch    65 | loss: 248.1696275CurrentTrain: epoch  4, batch    66 | loss: 205.2934851CurrentTrain: epoch  4, batch    67 | loss: 161.5656576CurrentTrain: epoch  4, batch    68 | loss: 210.9192014CurrentTrain: epoch  4, batch    69 | loss: 246.8977771CurrentTrain: epoch  4, batch    70 | loss: 155.1660096CurrentTrain: epoch  4, batch    71 | loss: 204.9713266CurrentTrain: epoch  4, batch    72 | loss: 176.6614477CurrentTrain: epoch  4, batch    73 | loss: 221.0150164CurrentTrain: epoch  4, batch    74 | loss: 212.1746319CurrentTrain: epoch  4, batch    75 | loss: 222.1752422CurrentTrain: epoch  4, batch    76 | loss: 211.1159116CurrentTrain: epoch  4, batch    77 | loss: 211.1445598CurrentTrain: epoch  4, batch    78 | loss: 204.0115146CurrentTrain: epoch  4, batch    79 | loss: 169.5523166CurrentTrain: epoch  4, batch    80 | loss: 287.7036380CurrentTrain: epoch  4, batch    81 | loss: 179.3769916CurrentTrain: epoch  4, batch    82 | loss: 185.0187023CurrentTrain: epoch  4, batch    83 | loss: 178.1850903CurrentTrain: epoch  4, batch    84 | loss: 247.8824635CurrentTrain: epoch  4, batch    85 | loss: 220.5547315CurrentTrain: epoch  4, batch    86 | loss: 176.1348542CurrentTrain: epoch  4, batch    87 | loss: 197.4795558CurrentTrain: epoch  4, batch    88 | loss: 238.2509402CurrentTrain: epoch  4, batch    89 | loss: 213.6241748CurrentTrain: epoch  4, batch    90 | loss: 167.9956645CurrentTrain: epoch  4, batch    91 | loss: 196.5864563CurrentTrain: epoch  4, batch    92 | loss: 196.6480584CurrentTrain: epoch  4, batch    93 | loss: 193.4174373CurrentTrain: epoch  4, batch    94 | loss: 239.0745923CurrentTrain: epoch  4, batch    95 | loss: 183.6076994CurrentTrain: epoch  5, batch     0 | loss: 186.6994479CurrentTrain: epoch  5, batch     1 | loss: 195.1198047CurrentTrain: epoch  5, batch     2 | loss: 194.3326448CurrentTrain: epoch  5, batch     3 | loss: 191.4496870CurrentTrain: epoch  5, batch     4 | loss: 213.4917744CurrentTrain: epoch  5, batch     5 | loss: 220.5805749CurrentTrain: epoch  5, batch     6 | loss: 169.2041980CurrentTrain: epoch  5, batch     7 | loss: 183.6005494CurrentTrain: epoch  5, batch     8 | loss: 211.2700275CurrentTrain: epoch  5, batch     9 | loss: 188.3853082CurrentTrain: epoch  5, batch    10 | loss: 211.4269783CurrentTrain: epoch  5, batch    11 | loss: 191.1979142CurrentTrain: epoch  5, batch    12 | loss: 220.6368066CurrentTrain: epoch  5, batch    13 | loss: 169.7959162CurrentTrain: epoch  5, batch    14 | loss: 162.2319967CurrentTrain: epoch  5, batch    15 | loss: 222.5203336CurrentTrain: epoch  5, batch    16 | loss: 168.6817278CurrentTrain: epoch  5, batch    17 | loss: 221.1913205CurrentTrain: epoch  5, batch    18 | loss: 194.1335497CurrentTrain: epoch  5, batch    19 | loss: 194.7897380CurrentTrain: epoch  5, batch    20 | loss: 154.8077948CurrentTrain: epoch  5, batch    21 | loss: 167.6571698CurrentTrain: epoch  5, batch    22 | loss: 228.7751038CurrentTrain: epoch  5, batch    23 | loss: 194.5758497CurrentTrain: epoch  5, batch    24 | loss: 192.0846532CurrentTrain: epoch  5, batch    25 | loss: 191.4006123CurrentTrain: epoch  5, batch    26 | loss: 276.1996147CurrentTrain: epoch  5, batch    27 | loss: 218.4751789CurrentTrain: epoch  5, batch    28 | loss: 202.3827711CurrentTrain: epoch  5, batch    29 | loss: 216.4296585CurrentTrain: epoch  5, batch    30 | loss: 177.0233502CurrentTrain: epoch  5, batch    31 | loss: 202.7667419CurrentTrain: epoch  5, batch    32 | loss: 173.2027760CurrentTrain: epoch  5, batch    33 | loss: 211.1262051CurrentTrain: epoch  5, batch    34 | loss: 223.0818356CurrentTrain: epoch  5, batch    35 | loss: 247.0290508CurrentTrain: epoch  5, batch    36 | loss: 220.2252560CurrentTrain: epoch  5, batch    37 | loss: 239.8550410CurrentTrain: epoch  5, batch    38 | loss: 203.5097201CurrentTrain: epoch  5, batch    39 | loss: 194.7471181CurrentTrain: epoch  5, batch    40 | loss: 211.4590206CurrentTrain: epoch  5, batch    41 | loss: 195.1531829CurrentTrain: epoch  5, batch    42 | loss: 186.6867205CurrentTrain: epoch  5, batch    43 | loss: 160.3517802CurrentTrain: epoch  5, batch    44 | loss: 286.5790524CurrentTrain: epoch  5, batch    45 | loss: 194.1186882CurrentTrain: epoch  5, batch    46 | loss: 203.0087348CurrentTrain: epoch  5, batch    47 | loss: 220.2040736CurrentTrain: epoch  5, batch    48 | loss: 160.2814728CurrentTrain: epoch  5, batch    49 | loss: 153.9296625CurrentTrain: epoch  5, batch    50 | loss: 286.9898793CurrentTrain: epoch  5, batch    51 | loss: 193.4706920CurrentTrain: epoch  5, batch    52 | loss: 204.7552690CurrentTrain: epoch  5, batch    53 | loss: 168.1437266CurrentTrain: epoch  5, batch    54 | loss: 211.9418999CurrentTrain: epoch  5, batch    55 | loss: 288.5662318CurrentTrain: epoch  5, batch    56 | loss: 228.4651225CurrentTrain: epoch  5, batch    57 | loss: 170.7026607CurrentTrain: epoch  5, batch    58 | loss: 240.1926491CurrentTrain: epoch  5, batch    59 | loss: 195.8137611CurrentTrain: epoch  5, batch    60 | loss: 266.5747060CurrentTrain: epoch  5, batch    61 | loss: 219.8655338CurrentTrain: epoch  5, batch    62 | loss: 219.7354331CurrentTrain: epoch  5, batch    63 | loss: 227.8375508CurrentTrain: epoch  5, batch    64 | loss: 276.7824463CurrentTrain: epoch  5, batch    65 | loss: 199.8924579CurrentTrain: epoch  5, batch    66 | loss: 219.5244839CurrentTrain: epoch  5, batch    67 | loss: 167.5289018CurrentTrain: epoch  5, batch    68 | loss: 247.4556091CurrentTrain: epoch  5, batch    69 | loss: 171.4086183CurrentTrain: epoch  5, batch    70 | loss: 213.2159510CurrentTrain: epoch  5, batch    71 | loss: 281.1856644CurrentTrain: epoch  5, batch    72 | loss: 213.2141081CurrentTrain: epoch  5, batch    73 | loss: 179.3921336CurrentTrain: epoch  5, batch    74 | loss: 187.5730500CurrentTrain: epoch  5, batch    75 | loss: 278.2517456CurrentTrain: epoch  5, batch    76 | loss: 153.1458376CurrentTrain: epoch  5, batch    77 | loss: 182.3759436CurrentTrain: epoch  5, batch    78 | loss: 178.7690180CurrentTrain: epoch  5, batch    79 | loss: 202.7371529CurrentTrain: epoch  5, batch    80 | loss: 227.9368745CurrentTrain: epoch  5, batch    81 | loss: 183.2365011CurrentTrain: epoch  5, batch    82 | loss: 266.7998800CurrentTrain: epoch  5, batch    83 | loss: 168.1490506CurrentTrain: epoch  5, batch    84 | loss: 158.8834173CurrentTrain: epoch  5, batch    85 | loss: 246.8149795CurrentTrain: epoch  5, batch    86 | loss: 148.2672481CurrentTrain: epoch  5, batch    87 | loss: 240.1746220CurrentTrain: epoch  5, batch    88 | loss: 202.7064405CurrentTrain: epoch  5, batch    89 | loss: 257.5056500CurrentTrain: epoch  5, batch    90 | loss: 280.7940635CurrentTrain: epoch  5, batch    91 | loss: 191.6610991CurrentTrain: epoch  5, batch    92 | loss: 183.4559995CurrentTrain: epoch  5, batch    93 | loss: 207.0363704CurrentTrain: epoch  5, batch    94 | loss: 229.2530368CurrentTrain: epoch  5, batch    95 | loss: 187.8866516CurrentTrain: epoch  6, batch     0 | loss: 211.9316578CurrentTrain: epoch  6, batch     1 | loss: 180.7673898CurrentTrain: epoch  6, batch     2 | loss: 192.4645503CurrentTrain: epoch  6, batch     3 | loss: 187.6111177CurrentTrain: epoch  6, batch     4 | loss: 248.5776177CurrentTrain: epoch  6, batch     5 | loss: 214.7864407CurrentTrain: epoch  6, batch     6 | loss: 220.5043770CurrentTrain: epoch  6, batch     7 | loss: 191.9048655CurrentTrain: epoch  6, batch     8 | loss: 139.1953900CurrentTrain: epoch  6, batch     9 | loss: 193.4672976CurrentTrain: epoch  6, batch    10 | loss: 155.5289968CurrentTrain: epoch  6, batch    11 | loss: 228.2247453CurrentTrain: epoch  6, batch    12 | loss: 195.9976559CurrentTrain: epoch  6, batch    13 | loss: 196.0262406CurrentTrain: epoch  6, batch    14 | loss: 161.2245747CurrentTrain: epoch  6, batch    15 | loss: 228.6626412CurrentTrain: epoch  6, batch    16 | loss: 161.2229085CurrentTrain: epoch  6, batch    17 | loss: 200.0009334CurrentTrain: epoch  6, batch    18 | loss: 167.1764415CurrentTrain: epoch  6, batch    19 | loss: 219.4129406CurrentTrain: epoch  6, batch    20 | loss: 268.6525551CurrentTrain: epoch  6, batch    21 | loss: 268.2571005CurrentTrain: epoch  6, batch    22 | loss: 180.2263604CurrentTrain: epoch  6, batch    23 | loss: 228.0433947CurrentTrain: epoch  6, batch    24 | loss: 246.6735135CurrentTrain: epoch  6, batch    25 | loss: 286.6024860CurrentTrain: epoch  6, batch    26 | loss: 228.1657817CurrentTrain: epoch  6, batch    27 | loss: 228.3844834CurrentTrain: epoch  6, batch    28 | loss: 229.0692728CurrentTrain: epoch  6, batch    29 | loss: 202.6663862CurrentTrain: epoch  6, batch    30 | loss: 211.5028786CurrentTrain: epoch  6, batch    31 | loss: 181.4782634CurrentTrain: epoch  6, batch    32 | loss: 176.6569621CurrentTrain: epoch  6, batch    33 | loss: 168.8693098CurrentTrain: epoch  6, batch    34 | loss: 163.8475616CurrentTrain: epoch  6, batch    35 | loss: 175.8541197CurrentTrain: epoch  6, batch    36 | loss: 219.1484565CurrentTrain: epoch  6, batch    37 | loss: 237.9068884CurrentTrain: epoch  6, batch    38 | loss: 219.9793416CurrentTrain: epoch  6, batch    39 | loss: 211.6908731CurrentTrain: epoch  6, batch    40 | loss: 266.3137934CurrentTrain: epoch  6, batch    41 | loss: 238.8940060CurrentTrain: epoch  6, batch    42 | loss: 202.3730441CurrentTrain: epoch  6, batch    43 | loss: 211.9253194CurrentTrain: epoch  6, batch    44 | loss: 186.6495625CurrentTrain: epoch  6, batch    45 | loss: 219.7604563CurrentTrain: epoch  6, batch    46 | loss: 184.3831892CurrentTrain: epoch  6, batch    47 | loss: 238.2173216CurrentTrain: epoch  6, batch    48 | loss: 237.3032904CurrentTrain: epoch  6, batch    49 | loss: 151.9944833CurrentTrain: epoch  6, batch    50 | loss: 180.5189812CurrentTrain: epoch  6, batch    51 | loss: 185.5388379CurrentTrain: epoch  6, batch    52 | loss: 175.7190839CurrentTrain: epoch  6, batch    53 | loss: 177.3993866CurrentTrain: epoch  6, batch    54 | loss: 227.8387934CurrentTrain: epoch  6, batch    55 | loss: 237.0881027CurrentTrain: epoch  6, batch    56 | loss: 221.1453173CurrentTrain: epoch  6, batch    57 | loss: 181.9365108CurrentTrain: epoch  6, batch    58 | loss: 210.3170015CurrentTrain: epoch  6, batch    59 | loss: 190.7634380CurrentTrain: epoch  6, batch    60 | loss: 227.8145882CurrentTrain: epoch  6, batch    61 | loss: 238.8094067CurrentTrain: epoch  6, batch    62 | loss: 265.9297699CurrentTrain: epoch  6, batch    63 | loss: 229.1499760CurrentTrain: epoch  6, batch    64 | loss: 237.1140347CurrentTrain: epoch  6, batch    65 | loss: 266.1763211CurrentTrain: epoch  6, batch    66 | loss: 218.9490279CurrentTrain: epoch  6, batch    67 | loss: 176.8223302CurrentTrain: epoch  6, batch    68 | loss: 204.7434922CurrentTrain: epoch  6, batch    69 | loss: 154.4058857CurrentTrain: epoch  6, batch    70 | loss: 220.2785930CurrentTrain: epoch  6, batch    71 | loss: 194.2274315CurrentTrain: epoch  6, batch    72 | loss: 184.2973217CurrentTrain: epoch  6, batch    73 | loss: 154.1029184CurrentTrain: epoch  6, batch    74 | loss: 202.4673721CurrentTrain: epoch  6, batch    75 | loss: 220.3133863CurrentTrain: epoch  6, batch    76 | loss: 204.1343276CurrentTrain: epoch  6, batch    77 | loss: 175.8143113CurrentTrain: epoch  6, batch    78 | loss: 230.2125804CurrentTrain: epoch  6, batch    79 | loss: 237.6871723CurrentTrain: epoch  6, batch    80 | loss: 139.3423276CurrentTrain: epoch  6, batch    81 | loss: 188.4594138CurrentTrain: epoch  6, batch    82 | loss: 174.6766110CurrentTrain: epoch  6, batch    83 | loss: 213.3345096CurrentTrain: epoch  6, batch    84 | loss: 276.6945520CurrentTrain: epoch  6, batch    85 | loss: 212.1081834CurrentTrain: epoch  6, batch    86 | loss: 286.3492903CurrentTrain: epoch  6, batch    87 | loss: 154.5967202CurrentTrain: epoch  6, batch    88 | loss: 211.0920730CurrentTrain: epoch  6, batch    89 | loss: 266.7955819CurrentTrain: epoch  6, batch    90 | loss: 193.6237787CurrentTrain: epoch  6, batch    91 | loss: 277.4936652CurrentTrain: epoch  6, batch    92 | loss: 180.4281367CurrentTrain: epoch  6, batch    93 | loss: 202.9070813CurrentTrain: epoch  6, batch    94 | loss: 191.4920061CurrentTrain: epoch  6, batch    95 | loss: 172.3601333CurrentTrain: epoch  7, batch     0 | loss: 276.1215368CurrentTrain: epoch  7, batch     1 | loss: 167.2730027CurrentTrain: epoch  7, batch     2 | loss: 202.3917854CurrentTrain: epoch  7, batch     3 | loss: 219.7181118CurrentTrain: epoch  7, batch     4 | loss: 222.0148607CurrentTrain: epoch  7, batch     5 | loss: 179.3801606CurrentTrain: epoch  7, batch     6 | loss: 182.9960223CurrentTrain: epoch  7, batch     7 | loss: 237.0741118CurrentTrain: epoch  7, batch     8 | loss: 196.7050063CurrentTrain: epoch  7, batch     9 | loss: 186.9310185CurrentTrain: epoch  7, batch    10 | loss: 219.4731298CurrentTrain: epoch  7, batch    11 | loss: 268.0353326CurrentTrain: epoch  7, batch    12 | loss: 175.3171944CurrentTrain: epoch  7, batch    13 | loss: 160.5413735CurrentTrain: epoch  7, batch    14 | loss: 146.7853126CurrentTrain: epoch  7, batch    15 | loss: 159.5147191CurrentTrain: epoch  7, batch    16 | loss: 237.8943219CurrentTrain: epoch  7, batch    17 | loss: 185.9281147CurrentTrain: epoch  7, batch    18 | loss: 165.9437937CurrentTrain: epoch  7, batch    19 | loss: 196.1345516CurrentTrain: epoch  7, batch    20 | loss: 266.0268136CurrentTrain: epoch  7, batch    21 | loss: 286.4834162CurrentTrain: epoch  7, batch    22 | loss: 204.0809370CurrentTrain: epoch  7, batch    23 | loss: 219.2428891CurrentTrain: epoch  7, batch    24 | loss: 178.1798109CurrentTrain: epoch  7, batch    25 | loss: 227.8806107CurrentTrain: epoch  7, batch    26 | loss: 212.2682913CurrentTrain: epoch  7, batch    27 | loss: 183.0105582CurrentTrain: epoch  7, batch    28 | loss: 203.4364592CurrentTrain: epoch  7, batch    29 | loss: 246.7573040CurrentTrain: epoch  7, batch    30 | loss: 211.0693686CurrentTrain: epoch  7, batch    31 | loss: 256.4675823CurrentTrain: epoch  7, batch    32 | loss: 227.9392215CurrentTrain: epoch  7, batch    33 | loss: 159.3180925CurrentTrain: epoch  7, batch    34 | loss: 153.4319090CurrentTrain: epoch  7, batch    35 | loss: 178.5032901CurrentTrain: epoch  7, batch    36 | loss: 277.4760325CurrentTrain: epoch  7, batch    37 | loss: 220.5584666CurrentTrain: epoch  7, batch    38 | loss: 203.6211499CurrentTrain: epoch  7, batch    39 | loss: 219.9865545CurrentTrain: epoch  7, batch    40 | loss: 207.9744017CurrentTrain: epoch  7, batch    41 | loss: 182.3343261CurrentTrain: epoch  7, batch    42 | loss: 180.0106584CurrentTrain: epoch  7, batch    43 | loss: 175.8038027CurrentTrain: epoch  7, batch    44 | loss: 237.0667426CurrentTrain: epoch  7, batch    45 | loss: 286.3049399CurrentTrain: epoch  7, batch    46 | loss: 228.3544315CurrentTrain: epoch  7, batch    47 | loss: 194.5652657CurrentTrain: epoch  7, batch    48 | loss: 202.3239990CurrentTrain: epoch  7, batch    49 | loss: 141.7095571CurrentTrain: epoch  7, batch    50 | loss: 210.9426335CurrentTrain: epoch  7, batch    51 | loss: 210.9157232CurrentTrain: epoch  7, batch    52 | loss: 179.0202094CurrentTrain: epoch  7, batch    53 | loss: 248.7321840CurrentTrain: epoch  7, batch    54 | loss: 210.4181449CurrentTrain: epoch  7, batch    55 | loss: 247.0240506CurrentTrain: epoch  7, batch    56 | loss: 202.1892742CurrentTrain: epoch  7, batch    57 | loss: 193.3971268CurrentTrain: epoch  7, batch    58 | loss: 159.5958966CurrentTrain: epoch  7, batch    59 | loss: 202.4623820CurrentTrain: epoch  7, batch    60 | loss: 237.5352817CurrentTrain: epoch  7, batch    61 | loss: 266.4357034CurrentTrain: epoch  7, batch    62 | loss: 174.5588349CurrentTrain: epoch  7, batch    63 | loss: 151.7459170CurrentTrain: epoch  7, batch    64 | loss: 287.0157426CurrentTrain: epoch  7, batch    65 | loss: 174.4980720CurrentTrain: epoch  7, batch    66 | loss: 238.1443664CurrentTrain: epoch  7, batch    67 | loss: 186.7712326CurrentTrain: epoch  7, batch    68 | loss: 202.1729084CurrentTrain: epoch  7, batch    69 | loss: 210.8677143CurrentTrain: epoch  7, batch    70 | loss: 153.4011836CurrentTrain: epoch  7, batch    71 | loss: 174.4813927CurrentTrain: epoch  7, batch    72 | loss: 237.3678702CurrentTrain: epoch  7, batch    73 | loss: 172.1295692CurrentTrain: epoch  7, batch    74 | loss: 219.5969797CurrentTrain: epoch  7, batch    75 | loss: 210.6852860CurrentTrain: epoch  7, batch    76 | loss: 237.0893102CurrentTrain: epoch  7, batch    77 | loss: 237.0346063CurrentTrain: epoch  7, batch    78 | loss: 219.7988087CurrentTrain: epoch  7, batch    79 | loss: 201.9154606CurrentTrain: epoch  7, batch    80 | loss: 204.3900675CurrentTrain: epoch  7, batch    81 | loss: 185.9686503CurrentTrain: epoch  7, batch    82 | loss: 220.3157421CurrentTrain: epoch  7, batch    83 | loss: 247.4693886CurrentTrain: epoch  7, batch    84 | loss: 202.3138262CurrentTrain: epoch  7, batch    85 | loss: 275.9974423CurrentTrain: epoch  7, batch    86 | loss: 229.2415127CurrentTrain: epoch  7, batch    87 | loss: 183.2541564CurrentTrain: epoch  7, batch    88 | loss: 195.4637263CurrentTrain: epoch  7, batch    89 | loss: 228.1510881CurrentTrain: epoch  7, batch    90 | loss: 202.1906946CurrentTrain: epoch  7, batch    91 | loss: 220.0173054CurrentTrain: epoch  7, batch    92 | loss: 153.4591400CurrentTrain: epoch  7, batch    93 | loss: 194.9594001CurrentTrain: epoch  7, batch    94 | loss: 174.3667107CurrentTrain: epoch  7, batch    95 | loss: 187.9005064CurrentTrain: epoch  8, batch     0 | loss: 166.4447496CurrentTrain: epoch  8, batch     1 | loss: 201.9203705CurrentTrain: epoch  8, batch     2 | loss: 159.7246166CurrentTrain: epoch  8, batch     3 | loss: 276.3566693CurrentTrain: epoch  8, batch     4 | loss: 202.8381219CurrentTrain: epoch  8, batch     5 | loss: 236.7755980CurrentTrain: epoch  8, batch     6 | loss: 167.9572839CurrentTrain: epoch  8, batch     7 | loss: 210.5484752CurrentTrain: epoch  8, batch     8 | loss: 210.5418328CurrentTrain: epoch  8, batch     9 | loss: 190.8049760CurrentTrain: epoch  8, batch    10 | loss: 168.3409786CurrentTrain: epoch  8, batch    11 | loss: 167.0262263CurrentTrain: epoch  8, batch    12 | loss: 190.6361085CurrentTrain: epoch  8, batch    13 | loss: 227.9209849CurrentTrain: epoch  8, batch    14 | loss: 220.0300368CurrentTrain: epoch  8, batch    15 | loss: 237.0944248CurrentTrain: epoch  8, batch    16 | loss: 212.7084324CurrentTrain: epoch  8, batch    17 | loss: 185.2438082CurrentTrain: epoch  8, batch    18 | loss: 238.5851041CurrentTrain: epoch  8, batch    19 | loss: 228.6366061CurrentTrain: epoch  8, batch    20 | loss: 149.8640501CurrentTrain: epoch  8, batch    21 | loss: 193.6498784CurrentTrain: epoch  8, batch    22 | loss: 202.4764838CurrentTrain: epoch  8, batch    23 | loss: 246.4999447CurrentTrain: epoch  8, batch    24 | loss: 227.7900163CurrentTrain: epoch  8, batch    25 | loss: 152.3553567CurrentTrain: epoch  8, batch    26 | loss: 210.4146942CurrentTrain: epoch  8, batch    27 | loss: 202.1784540CurrentTrain: epoch  8, batch    28 | loss: 199.9294339CurrentTrain: epoch  8, batch    29 | loss: 153.7098482CurrentTrain: epoch  8, batch    30 | loss: 210.5301210CurrentTrain: epoch  8, batch    31 | loss: 159.1957960CurrentTrain: epoch  8, batch    32 | loss: 162.3220564CurrentTrain: epoch  8, batch    33 | loss: 219.8708574CurrentTrain: epoch  8, batch    34 | loss: 236.8156670CurrentTrain: epoch  8, batch    35 | loss: 168.7102042CurrentTrain: epoch  8, batch    36 | loss: 211.0007955CurrentTrain: epoch  8, batch    37 | loss: 159.6754293CurrentTrain: epoch  8, batch    38 | loss: 276.0102165CurrentTrain: epoch  8, batch    39 | loss: 165.7697506CurrentTrain: epoch  8, batch    40 | loss: 190.5796849CurrentTrain: epoch  8, batch    41 | loss: 202.0692359CurrentTrain: epoch  8, batch    42 | loss: 204.4493599CurrentTrain: epoch  8, batch    43 | loss: 203.8875788CurrentTrain: epoch  8, batch    44 | loss: 211.3297097CurrentTrain: epoch  8, batch    45 | loss: 174.5275731CurrentTrain: epoch  8, batch    46 | loss: 202.5187649CurrentTrain: epoch  8, batch    47 | loss: 276.5148138CurrentTrain: epoch  8, batch    48 | loss: 185.3983312CurrentTrain: epoch  8, batch    49 | loss: 286.4355675CurrentTrain: epoch  8, batch    50 | loss: 170.3433603CurrentTrain: epoch  8, batch    51 | loss: 286.2590569CurrentTrain: epoch  8, batch    52 | loss: 159.4086290CurrentTrain: epoch  8, batch    53 | loss: 227.8053077CurrentTrain: epoch  8, batch    54 | loss: 199.0634249CurrentTrain: epoch  8, batch    55 | loss: 139.5468406CurrentTrain: epoch  8, batch    56 | loss: 210.3861901CurrentTrain: epoch  8, batch    57 | loss: 190.9759014CurrentTrain: epoch  8, batch    58 | loss: 152.4504485CurrentTrain: epoch  8, batch    59 | loss: 190.6675285CurrentTrain: epoch  8, batch    60 | loss: 201.8764789CurrentTrain: epoch  8, batch    61 | loss: 286.2996376CurrentTrain: epoch  8, batch    62 | loss: 246.4613105CurrentTrain: epoch  8, batch    63 | loss: 199.0688796CurrentTrain: epoch  8, batch    64 | loss: 236.8547126CurrentTrain: epoch  8, batch    65 | loss: 228.0659873CurrentTrain: epoch  8, batch    66 | loss: 219.0696430CurrentTrain: epoch  8, batch    67 | loss: 260.4762014CurrentTrain: epoch  8, batch    68 | loss: 182.6875992CurrentTrain: epoch  8, batch    69 | loss: 246.4998484CurrentTrain: epoch  8, batch    70 | loss: 219.2278512CurrentTrain: epoch  8, batch    71 | loss: 193.3860945CurrentTrain: epoch  8, batch    72 | loss: 182.8598586CurrentTrain: epoch  8, batch    73 | loss: 167.3449862CurrentTrain: epoch  8, batch    74 | loss: 203.2205417CurrentTrain: epoch  8, batch    75 | loss: 219.4584398CurrentTrain: epoch  8, batch    76 | loss: 168.4162716CurrentTrain: epoch  8, batch    77 | loss: 210.9404069CurrentTrain: epoch  8, batch    78 | loss: 213.1981454CurrentTrain: epoch  8, batch    79 | loss: 202.6875620CurrentTrain: epoch  8, batch    80 | loss: 247.2252280CurrentTrain: epoch  8, batch    81 | loss: 199.0205325CurrentTrain: epoch  8, batch    82 | loss: 221.1515256CurrentTrain: epoch  8, batch    83 | loss: 212.6953965CurrentTrain: epoch  8, batch    84 | loss: 174.8822136CurrentTrain: epoch  8, batch    85 | loss: 172.2870708CurrentTrain: epoch  8, batch    86 | loss: 167.3840251CurrentTrain: epoch  8, batch    87 | loss: 201.8860646CurrentTrain: epoch  8, batch    88 | loss: 201.7280888CurrentTrain: epoch  8, batch    89 | loss: 185.9548286CurrentTrain: epoch  8, batch    90 | loss: 210.8300499CurrentTrain: epoch  8, batch    91 | loss: 276.7880677CurrentTrain: epoch  8, batch    92 | loss: 159.3453657CurrentTrain: epoch  8, batch    93 | loss: 159.0329952CurrentTrain: epoch  8, batch    94 | loss: 228.4625505CurrentTrain: epoch  8, batch    95 | loss: 204.5586512CurrentTrain: epoch  9, batch     0 | loss: 220.2542207CurrentTrain: epoch  9, batch     1 | loss: 286.2493839CurrentTrain: epoch  9, batch     2 | loss: 193.5756628CurrentTrain: epoch  9, batch     3 | loss: 182.1983324CurrentTrain: epoch  9, batch     4 | loss: 212.6111903CurrentTrain: epoch  9, batch     5 | loss: 237.6239868CurrentTrain: epoch  9, batch     6 | loss: 201.7210104CurrentTrain: epoch  9, batch     7 | loss: 218.9667905CurrentTrain: epoch  9, batch     8 | loss: 227.4254642CurrentTrain: epoch  9, batch     9 | loss: 247.9996551CurrentTrain: epoch  9, batch    10 | loss: 335.6120296CurrentTrain: epoch  9, batch    11 | loss: 237.7414201CurrentTrain: epoch  9, batch    12 | loss: 174.7388961CurrentTrain: epoch  9, batch    13 | loss: 266.1629941CurrentTrain: epoch  9, batch    14 | loss: 193.3878304CurrentTrain: epoch  9, batch    15 | loss: 182.3435697CurrentTrain: epoch  9, batch    16 | loss: 190.9044904CurrentTrain: epoch  9, batch    17 | loss: 167.2092823CurrentTrain: epoch  9, batch    18 | loss: 174.6712963CurrentTrain: epoch  9, batch    19 | loss: 218.9982606CurrentTrain: epoch  9, batch    20 | loss: 238.5799717CurrentTrain: epoch  9, batch    21 | loss: 202.4025401CurrentTrain: epoch  9, batch    22 | loss: 210.7697718CurrentTrain: epoch  9, batch    23 | loss: 167.2755535CurrentTrain: epoch  9, batch    24 | loss: 165.2267940CurrentTrain: epoch  9, batch    25 | loss: 227.3935866CurrentTrain: epoch  9, batch    26 | loss: 174.7020607CurrentTrain: epoch  9, batch    27 | loss: 185.1195055CurrentTrain: epoch  9, batch    28 | loss: 177.3623809CurrentTrain: epoch  9, batch    29 | loss: 286.3220007CurrentTrain: epoch  9, batch    30 | loss: 196.8697946CurrentTrain: epoch  9, batch    31 | loss: 219.3458957CurrentTrain: epoch  9, batch    32 | loss: 218.9580791CurrentTrain: epoch  9, batch    33 | loss: 227.7716860CurrentTrain: epoch  9, batch    34 | loss: 174.3509361CurrentTrain: epoch  9, batch    35 | loss: 219.1781862CurrentTrain: epoch  9, batch    36 | loss: 202.3562261CurrentTrain: epoch  9, batch    37 | loss: 227.8776760CurrentTrain: epoch  9, batch    38 | loss: 227.4732401CurrentTrain: epoch  9, batch    39 | loss: 227.8906324CurrentTrain: epoch  9, batch    40 | loss: 185.3762002CurrentTrain: epoch  9, batch    41 | loss: 177.8166853CurrentTrain: epoch  9, batch    42 | loss: 278.8312488CurrentTrain: epoch  9, batch    43 | loss: 180.2269995CurrentTrain: epoch  9, batch    44 | loss: 228.5167057CurrentTrain: epoch  9, batch    45 | loss: 219.6707678CurrentTrain: epoch  9, batch    46 | loss: 201.6604679CurrentTrain: epoch  9, batch    47 | loss: 168.8885113CurrentTrain: epoch  9, batch    48 | loss: 126.3883278CurrentTrain: epoch  9, batch    49 | loss: 164.0754064CurrentTrain: epoch  9, batch    50 | loss: 246.4117338CurrentTrain: epoch  9, batch    51 | loss: 201.7989597CurrentTrain: epoch  9, batch    52 | loss: 194.0852997CurrentTrain: epoch  9, batch    53 | loss: 238.0847392CurrentTrain: epoch  9, batch    54 | loss: 152.9865043CurrentTrain: epoch  9, batch    55 | loss: 193.5197004CurrentTrain: epoch  9, batch    56 | loss: 228.0437074CurrentTrain: epoch  9, batch    57 | loss: 237.1082288CurrentTrain: epoch  9, batch    58 | loss: 286.6404895CurrentTrain: epoch  9, batch    59 | loss: 201.8095784CurrentTrain: epoch  9, batch    60 | loss: 211.5408172CurrentTrain: epoch  9, batch    61 | loss: 236.9927719CurrentTrain: epoch  9, batch    62 | loss: 286.3328150CurrentTrain: epoch  9, batch    63 | loss: 275.9349871CurrentTrain: epoch  9, batch    64 | loss: 190.5801726CurrentTrain: epoch  9, batch    65 | loss: 227.6976539CurrentTrain: epoch  9, batch    66 | loss: 159.6811545CurrentTrain: epoch  9, batch    67 | loss: 316.3114016CurrentTrain: epoch  9, batch    68 | loss: 186.0812981CurrentTrain: epoch  9, batch    69 | loss: 160.7103542CurrentTrain: epoch  9, batch    70 | loss: 203.8643965CurrentTrain: epoch  9, batch    71 | loss: 199.5173687CurrentTrain: epoch  9, batch    72 | loss: 227.5117724CurrentTrain: epoch  9, batch    73 | loss: 236.8612921CurrentTrain: epoch  9, batch    74 | loss: 176.3028585CurrentTrain: epoch  9, batch    75 | loss: 210.7558023CurrentTrain: epoch  9, batch    76 | loss: 161.0373030CurrentTrain: epoch  9, batch    77 | loss: 246.5818455CurrentTrain: epoch  9, batch    78 | loss: 210.7416264CurrentTrain: epoch  9, batch    79 | loss: 175.0343304CurrentTrain: epoch  9, batch    80 | loss: 220.4729809CurrentTrain: epoch  9, batch    81 | loss: 159.1130234CurrentTrain: epoch  9, batch    82 | loss: 210.6278307CurrentTrain: epoch  9, batch    83 | loss: 191.1127112CurrentTrain: epoch  9, batch    84 | loss: 227.8301133CurrentTrain: epoch  9, batch    85 | loss: 186.3348727CurrentTrain: epoch  9, batch    86 | loss: 159.1698671CurrentTrain: epoch  9, batch    87 | loss: 170.8461183CurrentTrain: epoch  9, batch    88 | loss: 161.4107138CurrentTrain: epoch  9, batch    89 | loss: 166.3992187CurrentTrain: epoch  9, batch    90 | loss: 182.4214586CurrentTrain: epoch  9, batch    91 | loss: 201.8823496CurrentTrain: epoch  9, batch    92 | loss: 167.0321035CurrentTrain: epoch  9, batch    93 | loss: 193.1313572CurrentTrain: epoch  9, batch    94 | loss: 178.8075565CurrentTrain: epoch  9, batch    95 | loss: 186.8158293

F1 score per class: {32: 0.6703910614525139, 6: 0.8023952095808383, 19: 0.2727272727272727, 24: 0.7624309392265194, 26: 0.9361702127659575, 29: 0.9081632653061225}
Micro-average F1 score: 0.8060021436227224
Weighted-average F1 score: 0.8171947958017202
F1 score per class: {32: 0.7961165048543689, 6: 0.8764044943820225, 19: 0.6896551724137931, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.92}
Micro-average F1 score: 0.8640973630831643
Weighted-average F1 score: 0.8661754693362708
F1 score per class: {32: 0.7961165048543689, 6: 0.8764044943820225, 19: 0.6896551724137931, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.92}
Micro-average F1 score: 0.8640973630831643
Weighted-average F1 score: 0.8661754693362708

F1 score per class: {32: 0.6703910614525139, 6: 0.8023952095808383, 19: 0.2727272727272727, 24: 0.7624309392265194, 26: 0.9361702127659575, 29: 0.9081632653061225}
Micro-average F1 score: 0.8060021436227224
Weighted-average F1 score: 0.8171947958017202
F1 score per class: {32: 0.7961165048543689, 6: 0.8764044943820225, 19: 0.6896551724137931, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.92}
Micro-average F1 score: 0.8640973630831643
Weighted-average F1 score: 0.8661754693362708
F1 score per class: {32: 0.7961165048543689, 6: 0.8764044943820225, 19: 0.6896551724137931, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.92}
Micro-average F1 score: 0.8640973630831643
Weighted-average F1 score: 0.8661754693362708
cur_acc:  ['0.8060']
his_acc:  ['0.8060']
cur_acc des:  ['0.8641']
his_acc des:  ['0.8641']
cur_acc rrf:  ['0.8641']
his_acc rrf:  ['0.8641']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 225.4683684CurrentTrain: epoch  0, batch     1 | loss: 240.2242323CurrentTrain: epoch  0, batch     2 | loss: 212.3273153CurrentTrain: epoch  0, batch     3 | loss: 258.2134617CurrentTrain: epoch  0, batch     4 | loss: 158.6911821CurrentTrain: epoch  1, batch     0 | loss: 197.3321243CurrentTrain: epoch  1, batch     1 | loss: 200.9529081CurrentTrain: epoch  1, batch     2 | loss: 365.8481541CurrentTrain: epoch  1, batch     3 | loss: 218.5672912CurrentTrain: epoch  1, batch     4 | loss: 130.3082936CurrentTrain: epoch  2, batch     0 | loss: 196.2075661CurrentTrain: epoch  2, batch     1 | loss: 243.5072068CurrentTrain: epoch  2, batch     2 | loss: 210.5520528CurrentTrain: epoch  2, batch     3 | loss: 227.0660495CurrentTrain: epoch  2, batch     4 | loss: 231.0931395CurrentTrain: epoch  3, batch     0 | loss: 183.1521982CurrentTrain: epoch  3, batch     1 | loss: 240.6404779CurrentTrain: epoch  3, batch     2 | loss: 250.9455951CurrentTrain: epoch  3, batch     3 | loss: 199.7680961CurrentTrain: epoch  3, batch     4 | loss: 152.7847369CurrentTrain: epoch  4, batch     0 | loss: 158.5496686CurrentTrain: epoch  4, batch     1 | loss: 224.6606243CurrentTrain: epoch  4, batch     2 | loss: 359.8556357CurrentTrain: epoch  4, batch     3 | loss: 289.1640565CurrentTrain: epoch  4, batch     4 | loss: 111.5771502CurrentTrain: epoch  5, batch     0 | loss: 185.7116445CurrentTrain: epoch  5, batch     1 | loss: 238.9671334CurrentTrain: epoch  5, batch     2 | loss: 287.9034856CurrentTrain: epoch  5, batch     3 | loss: 184.7976462CurrentTrain: epoch  5, batch     4 | loss: 135.7880828CurrentTrain: epoch  6, batch     0 | loss: 278.1366261CurrentTrain: epoch  6, batch     1 | loss: 221.3891179CurrentTrain: epoch  6, batch     2 | loss: 211.7075590CurrentTrain: epoch  6, batch     3 | loss: 180.2510608CurrentTrain: epoch  6, batch     4 | loss: 133.2942191CurrentTrain: epoch  7, batch     0 | loss: 212.5058530CurrentTrain: epoch  7, batch     1 | loss: 203.3575597CurrentTrain: epoch  7, batch     2 | loss: 202.8872030CurrentTrain: epoch  7, batch     3 | loss: 247.4777908CurrentTrain: epoch  7, batch     4 | loss: 207.5747164CurrentTrain: epoch  8, batch     0 | loss: 222.7309980CurrentTrain: epoch  8, batch     1 | loss: 277.7857098CurrentTrain: epoch  8, batch     2 | loss: 237.1849574CurrentTrain: epoch  8, batch     3 | loss: 175.1758267CurrentTrain: epoch  8, batch     4 | loss: 116.2752580CurrentTrain: epoch  9, batch     0 | loss: 359.9173687CurrentTrain: epoch  9, batch     1 | loss: 219.7033544CurrentTrain: epoch  9, batch     2 | loss: 168.0955692CurrentTrain: epoch  9, batch     3 | loss: 182.5197292CurrentTrain: epoch  9, batch     4 | loss: 131.8513698
MemoryTrain:  epoch  0, batch     0 | loss: 1.7807014MemoryTrain:  epoch  1, batch     0 | loss: 1.4955359MemoryTrain:  epoch  2, batch     0 | loss: 1.0288638MemoryTrain:  epoch  3, batch     0 | loss: 0.9500602MemoryTrain:  epoch  4, batch     0 | loss: 0.7108541MemoryTrain:  epoch  5, batch     0 | loss: 0.5737797MemoryTrain:  epoch  6, batch     0 | loss: 0.3562001MemoryTrain:  epoch  7, batch     0 | loss: 0.3244420MemoryTrain:  epoch  8, batch     0 | loss: 0.2431851MemoryTrain:  epoch  9, batch     0 | loss: 0.2080447

F1 score per class: {5: 0.9690721649484536, 6: 0.0, 10: 0.23008849557522124, 16: 0.8235294117647058, 17: 0.0, 18: 0.52}
Micro-average F1 score: 0.6604215456674473
Weighted-average F1 score: 0.78237120261385
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.6442953020134228, 16: 0.8679245283018868, 17: 0.0, 18: 0.7457627118644068}
Micro-average F1 score: 0.7941787941787942
Weighted-average F1 score: 0.804632266442532
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.6442953020134228, 16: 0.8679245283018868, 17: 0.0, 18: 0.7457627118644068}
Micro-average F1 score: 0.7941787941787942
Weighted-average F1 score: 0.804632266442532

F1 score per class: {32: 0.9591836734693877, 5: 0.6382978723404256, 6: 0.23008849557522124, 10: 0.8235294117647058, 16: 0.0, 17: 0.52, 18: 0.7804878048780488, 19: 0.2857142857142857, 24: 0.7727272727272727, 26: 0.918918918918919, 29: 0.9090909090909091}
Micro-average F1 score: 0.7470760233918129
Weighted-average F1 score: 0.7875117871153019
F1 score per class: {32: 0.9607843137254902, 5: 0.7162790697674418, 6: 0.64, 10: 0.8518518518518519, 16: 0.0, 17: 0.7333333333333333, 18: 0.8505747126436781, 19: 0.6896551724137931, 24: 0.7727272727272727, 26: 0.9417989417989417, 29: 0.9191919191919192}
Micro-average F1 score: 0.8191126279863481
Weighted-average F1 score: 0.8267685469779261
F1 score per class: {32: 0.9607843137254902, 5: 0.7102803738317757, 6: 0.64, 10: 0.8518518518518519, 16: 0.0, 17: 0.7457627118644068, 18: 0.8505747126436781, 19: 0.6666666666666666, 24: 0.7727272727272727, 26: 0.9417989417989417, 29: 0.914572864321608}
Micro-average F1 score: 0.8180574555403557
Weighted-average F1 score: 0.8260140362284203
cur_acc:  ['0.8060', '0.6604']
his_acc:  ['0.8060', '0.7471']
cur_acc des:  ['0.8641', '0.7942']
his_acc des:  ['0.8641', '0.8191']
cur_acc rrf:  ['0.8641', '0.7942']
his_acc rrf:  ['0.8641', '0.8181']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 220.2280754CurrentTrain: epoch  0, batch     1 | loss: 246.5514636CurrentTrain: epoch  0, batch     2 | loss: 190.7215003CurrentTrain: epoch  0, batch     3 | loss: 219.2670524CurrentTrain: epoch  0, batch     4 | loss: 90.3199320CurrentTrain: epoch  1, batch     0 | loss: 242.0131953CurrentTrain: epoch  1, batch     1 | loss: 194.2334329CurrentTrain: epoch  1, batch     2 | loss: 167.7686432CurrentTrain: epoch  1, batch     3 | loss: 253.6563691CurrentTrain: epoch  1, batch     4 | loss: 90.0521699CurrentTrain: epoch  2, batch     0 | loss: 196.6182238CurrentTrain: epoch  2, batch     1 | loss: 218.2020293CurrentTrain: epoch  2, batch     2 | loss: 192.0799201CurrentTrain: epoch  2, batch     3 | loss: 284.1195129CurrentTrain: epoch  2, batch     4 | loss: 53.9189768CurrentTrain: epoch  3, batch     0 | loss: 235.4054929CurrentTrain: epoch  3, batch     1 | loss: 181.6417550CurrentTrain: epoch  3, batch     2 | loss: 214.8554701CurrentTrain: epoch  3, batch     3 | loss: 222.8099507CurrentTrain: epoch  3, batch     4 | loss: 52.8346860CurrentTrain: epoch  4, batch     0 | loss: 243.6368569CurrentTrain: epoch  4, batch     1 | loss: 207.0820955CurrentTrain: epoch  4, batch     2 | loss: 196.6437376CurrentTrain: epoch  4, batch     3 | loss: 222.8912093CurrentTrain: epoch  4, batch     4 | loss: 36.0351053CurrentTrain: epoch  5, batch     0 | loss: 206.5114064CurrentTrain: epoch  5, batch     1 | loss: 204.6881566CurrentTrain: epoch  5, batch     2 | loss: 199.0414206CurrentTrain: epoch  5, batch     3 | loss: 213.2247459CurrentTrain: epoch  5, batch     4 | loss: 51.9945081CurrentTrain: epoch  6, batch     0 | loss: 222.2044377CurrentTrain: epoch  6, batch     1 | loss: 238.0932552CurrentTrain: epoch  6, batch     2 | loss: 177.5634875CurrentTrain: epoch  6, batch     3 | loss: 185.3186355CurrentTrain: epoch  6, batch     4 | loss: 45.6012956CurrentTrain: epoch  7, batch     0 | loss: 197.1010578CurrentTrain: epoch  7, batch     1 | loss: 220.3793417CurrentTrain: epoch  7, batch     2 | loss: 220.3172539CurrentTrain: epoch  7, batch     3 | loss: 204.1778956CurrentTrain: epoch  7, batch     4 | loss: 89.8766905CurrentTrain: epoch  8, batch     0 | loss: 220.3530227CurrentTrain: epoch  8, batch     1 | loss: 175.8367013CurrentTrain: epoch  8, batch     2 | loss: 239.8937637CurrentTrain: epoch  8, batch     3 | loss: 195.6854968CurrentTrain: epoch  8, batch     4 | loss: 34.8891243CurrentTrain: epoch  9, batch     0 | loss: 237.6585675CurrentTrain: epoch  9, batch     1 | loss: 246.8034079CurrentTrain: epoch  9, batch     2 | loss: 176.3250781CurrentTrain: epoch  9, batch     3 | loss: 167.9826680CurrentTrain: epoch  9, batch     4 | loss: 51.1532668
MemoryTrain:  epoch  0, batch     0 | loss: 1.2342760MemoryTrain:  epoch  1, batch     0 | loss: 1.0680778MemoryTrain:  epoch  2, batch     0 | loss: 0.8130952MemoryTrain:  epoch  3, batch     0 | loss: 0.6575397MemoryTrain:  epoch  4, batch     0 | loss: 0.4998947MemoryTrain:  epoch  5, batch     0 | loss: 0.4508830MemoryTrain:  epoch  6, batch     0 | loss: 0.3065453MemoryTrain:  epoch  7, batch     0 | loss: 0.2804438MemoryTrain:  epoch  8, batch     0 | loss: 0.2028784MemoryTrain:  epoch  9, batch     0 | loss: 0.1686254

F1 score per class: {32: 0.8, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.25742574257425743, 10: 0.3870967741935484, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.46153846153846156, 19: 0.0, 28: 0.25}
Micro-average F1 score: 0.3404255319148936
Weighted-average F1 score: 0.3213420562190923
F1 score per class: {32: 0.9411764705882353, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.42857142857142855, 10: 0.6710526315789473, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.625, 24: 0.0, 28: 0.5263157894736842}
Micro-average F1 score: 0.510989010989011
Weighted-average F1 score: 0.4161478747578886
F1 score per class: {32: 0.9411764705882353, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.49572649572649574, 10: 0.6797385620915033, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.625, 19: 0.0, 28: 0.5263157894736842}
Micro-average F1 score: 0.5469613259668509
Weighted-average F1 score: 0.45848393621672967

F1 score per class: {32: 0.8, 2: 0.964824120603015, 5: 0.6868686868686869, 6: 0.09433962264150944, 39: 0.24528301886792453, 10: 0.384, 11: 0.75, 12: 0.0, 16: 0.40816326530612246, 17: 0.8070175438596491, 18: 0.36363636363636365, 19: 0.7727272727272727, 24: 0.2857142857142857, 26: 0.93048128342246, 28: 0.91, 29: 0.25}
Micro-average F1 score: 0.6823104693140795
Weighted-average F1 score: 0.7708105264712912
F1 score per class: {32: 0.9411764705882353, 2: 0.970873786407767, 5: 0.7230046948356808, 6: 0.3937007874015748, 39: 0.3902439024390244, 10: 0.6219512195121951, 11: 0.8070175438596491, 12: 0.25, 16: 0.47619047619047616, 17: 0.8369565217391305, 18: 0.6060606060606061, 19: 0.7555555555555555, 24: 0.37037037037037035, 26: 0.918918918918919, 28: 0.916256157635468, 29: 0.5}
Micro-average F1 score: 0.7319195214790647
Weighted-average F1 score: 0.7560708914558737
F1 score per class: {32: 0.9411764705882353, 2: 0.970873786407767, 5: 0.7307692307692307, 6: 0.3548387096774194, 39: 0.453125, 10: 0.6303030303030303, 11: 0.8070175438596491, 12: 0.23529411764705882, 16: 0.5063291139240507, 17: 0.8324324324324325, 18: 0.5925925925925926, 19: 0.7597765363128491, 24: 0.3333333333333333, 26: 0.918918918918919, 28: 0.916256157635468, 29: 0.5263157894736842}
Micro-average F1 score: 0.7359212684527064
Weighted-average F1 score: 0.7603841011559149
cur_acc:  ['0.8060', '0.6604', '0.3404']
his_acc:  ['0.8060', '0.7471', '0.6823']
cur_acc des:  ['0.8641', '0.7942', '0.5110']
his_acc des:  ['0.8641', '0.8191', '0.7319']
cur_acc rrf:  ['0.8641', '0.7942', '0.5470']
his_acc rrf:  ['0.8641', '0.8181', '0.7359']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 249.8974971CurrentTrain: epoch  0, batch     1 | loss: 193.7701013CurrentTrain: epoch  0, batch     2 | loss: 208.9320208CurrentTrain: epoch  0, batch     3 | loss: 206.7728442CurrentTrain: epoch  1, batch     0 | loss: 194.2573147CurrentTrain: epoch  1, batch     1 | loss: 250.2127220CurrentTrain: epoch  1, batch     2 | loss: 199.3191621CurrentTrain: epoch  1, batch     3 | loss: 192.9333169CurrentTrain: epoch  2, batch     0 | loss: 363.2426441CurrentTrain: epoch  2, batch     1 | loss: 196.8127627CurrentTrain: epoch  2, batch     2 | loss: 160.5457099CurrentTrain: epoch  2, batch     3 | loss: 197.5326962CurrentTrain: epoch  3, batch     0 | loss: 231.2665008CurrentTrain: epoch  3, batch     1 | loss: 224.4139399CurrentTrain: epoch  3, batch     2 | loss: 175.0766409CurrentTrain: epoch  3, batch     3 | loss: 193.3593227CurrentTrain: epoch  4, batch     0 | loss: 207.5706912CurrentTrain: epoch  4, batch     1 | loss: 202.4004095CurrentTrain: epoch  4, batch     2 | loss: 179.5257426CurrentTrain: epoch  4, batch     3 | loss: 181.0155701CurrentTrain: epoch  5, batch     0 | loss: 192.7796474CurrentTrain: epoch  5, batch     1 | loss: 194.9340027CurrentTrain: epoch  5, batch     2 | loss: 203.5551766CurrentTrain: epoch  5, batch     3 | loss: 188.3091789CurrentTrain: epoch  6, batch     0 | loss: 250.1522353CurrentTrain: epoch  6, batch     1 | loss: 181.5942438CurrentTrain: epoch  6, batch     2 | loss: 188.4195978CurrentTrain: epoch  6, batch     3 | loss: 161.7652193CurrentTrain: epoch  7, batch     0 | loss: 208.6798003CurrentTrain: epoch  7, batch     1 | loss: 223.1184443CurrentTrain: epoch  7, batch     2 | loss: 239.5307518CurrentTrain: epoch  7, batch     3 | loss: 137.9297465CurrentTrain: epoch  8, batch     0 | loss: 216.2201905CurrentTrain: epoch  8, batch     1 | loss: 184.9480004CurrentTrain: epoch  8, batch     2 | loss: 237.6757271CurrentTrain: epoch  8, batch     3 | loss: 143.9864445CurrentTrain: epoch  9, batch     0 | loss: 203.4197233CurrentTrain: epoch  9, batch     1 | loss: 179.9720615CurrentTrain: epoch  9, batch     2 | loss: 220.0975725CurrentTrain: epoch  9, batch     3 | loss: 167.6960827
MemoryTrain:  epoch  0, batch     0 | loss: 0.9921223MemoryTrain:  epoch  1, batch     0 | loss: 0.8138165MemoryTrain:  epoch  2, batch     0 | loss: 0.6422036MemoryTrain:  epoch  3, batch     0 | loss: 0.4888484MemoryTrain:  epoch  4, batch     0 | loss: 0.3705271MemoryTrain:  epoch  5, batch     0 | loss: 0.3290105MemoryTrain:  epoch  6, batch     0 | loss: 0.3200028MemoryTrain:  epoch  7, batch     0 | loss: 0.2379677MemoryTrain:  epoch  8, batch     0 | loss: 0.2117861MemoryTrain:  epoch  9, batch     0 | loss: 0.1583141

F1 score per class: {0: 0.9117647058823529, 32: 0.0, 2: 0.7951807228915663, 4: 0.0, 11: 0.5714285714285714, 13: 0.41025641025641024, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7466666666666667
Weighted-average F1 score: 0.725138980761966
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.8165680473372781, 4: 0.0, 5: 0.0, 11: 0.5714285714285714, 13: 0.0, 18: 0.76, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7464788732394366
Weighted-average F1 score: 0.6536791345834845
F1 score per class: {0: 0.9722222222222222, 32: 0.0, 2: 0.8571428571428571, 4: 0.0, 11: 0.5714285714285714, 13: 0.76, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7717647058823529
Weighted-average F1 score: 0.6883968253968252

F1 score per class: {0: 0.8985507246376812, 2: 0.7058823529411765, 4: 0.7951807228915663, 5: 0.9748743718592965, 6: 0.7208121827411168, 10: 0.18018018018018017, 11: 0.22429906542056074, 12: 0.25806451612903225, 13: 0.11428571428571428, 16: 0.7719298245614035, 17: 0.0, 18: 0.4528301886792453, 19: 0.7734806629834254, 21: 0.34782608695652173, 23: 0.8148148148148148, 24: 0.1, 26: 0.7431693989071039, 28: 0.5454545454545454, 29: 0.9081081081081082, 32: 0.898989898989899, 39: 0.23529411764705882}
Micro-average F1 score: 0.6802128688921142
Weighted-average F1 score: 0.746229808973509
F1 score per class: {0: 0.972972972972973, 2: 0.3684210526315789, 4: 0.8165680473372781, 5: 0.9615384615384616, 6: 0.7288888888888889, 10: 0.524822695035461, 11: 0.5333333333333333, 12: 0.6303030303030303, 13: 0.1111111111111111, 16: 0.8666666666666667, 17: 0.3333333333333333, 18: 0.5333333333333333, 19: 0.8469387755102041, 21: 0.5205479452054794, 23: 0.7857142857142857, 24: 0.1, 26: 0.7120418848167539, 28: 0.5333333333333333, 29: 0.9032258064516129, 32: 0.9207920792079208, 39: 0.48}
Micro-average F1 score: 0.7368869936034115
Weighted-average F1 score: 0.7392965800346443
F1 score per class: {0: 0.958904109589041, 2: 0.4117647058823529, 4: 0.8571428571428571, 5: 0.966183574879227, 6: 0.7431192660550459, 10: 0.5072463768115942, 11: 0.5620915032679739, 12: 0.5962732919254659, 13: 0.1, 16: 0.8333333333333334, 17: 0.3333333333333333, 18: 0.5797101449275363, 19: 0.8426395939086294, 21: 0.5205479452054794, 23: 0.8048780487804879, 24: 0.1, 26: 0.7157894736842105, 28: 0.47058823529411764, 29: 0.9081081081081082, 32: 0.916256157635468, 39: 0.3333333333333333}
Micro-average F1 score: 0.7406451612903225
Weighted-average F1 score: 0.7453551655533248
cur_acc:  ['0.8060', '0.6604', '0.3404', '0.7467']
his_acc:  ['0.8060', '0.7471', '0.6823', '0.6802']
cur_acc des:  ['0.8641', '0.7942', '0.5110', '0.7465']
his_acc des:  ['0.8641', '0.8191', '0.7319', '0.7369']
cur_acc rrf:  ['0.8641', '0.7942', '0.5470', '0.7718']
his_acc rrf:  ['0.8641', '0.8181', '0.7359', '0.7406']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 216.4428099CurrentTrain: epoch  0, batch     1 | loss: 224.2206507CurrentTrain: epoch  0, batch     2 | loss: 185.1289638CurrentTrain: epoch  0, batch     3 | loss: 198.5251079CurrentTrain: epoch  1, batch     0 | loss: 221.9204076CurrentTrain: epoch  1, batch     1 | loss: 230.5843523CurrentTrain: epoch  1, batch     2 | loss: 177.3298897CurrentTrain: epoch  1, batch     3 | loss: 118.9514286CurrentTrain: epoch  2, batch     0 | loss: 180.5357422CurrentTrain: epoch  2, batch     1 | loss: 216.2051848CurrentTrain: epoch  2, batch     2 | loss: 186.3094618CurrentTrain: epoch  2, batch     3 | loss: 198.1632538CurrentTrain: epoch  3, batch     0 | loss: 185.2629638CurrentTrain: epoch  3, batch     1 | loss: 179.5702307CurrentTrain: epoch  3, batch     2 | loss: 240.1189734CurrentTrain: epoch  3, batch     3 | loss: 140.7551249CurrentTrain: epoch  4, batch     0 | loss: 161.2428759CurrentTrain: epoch  4, batch     1 | loss: 188.2413527CurrentTrain: epoch  4, batch     2 | loss: 238.5038528CurrentTrain: epoch  4, batch     3 | loss: 187.0337595CurrentTrain: epoch  5, batch     0 | loss: 213.4447069CurrentTrain: epoch  5, batch     1 | loss: 184.3003703CurrentTrain: epoch  5, batch     2 | loss: 229.7185073CurrentTrain: epoch  5, batch     3 | loss: 129.1171230CurrentTrain: epoch  6, batch     0 | loss: 220.4600894CurrentTrain: epoch  6, batch     1 | loss: 168.1971632CurrentTrain: epoch  6, batch     2 | loss: 229.9840012CurrentTrain: epoch  6, batch     3 | loss: 149.7138107CurrentTrain: epoch  7, batch     0 | loss: 212.6339527CurrentTrain: epoch  7, batch     1 | loss: 183.4172542CurrentTrain: epoch  7, batch     2 | loss: 177.9600839CurrentTrain: epoch  7, batch     3 | loss: 155.8290779CurrentTrain: epoch  8, batch     0 | loss: 182.8375558CurrentTrain: epoch  8, batch     1 | loss: 179.6680892CurrentTrain: epoch  8, batch     2 | loss: 220.0076632CurrentTrain: epoch  8, batch     3 | loss: 183.9611823CurrentTrain: epoch  9, batch     0 | loss: 177.6723112CurrentTrain: epoch  9, batch     1 | loss: 167.7094779CurrentTrain: epoch  9, batch     2 | loss: 238.1336671CurrentTrain: epoch  9, batch     3 | loss: 164.2426454
MemoryTrain:  epoch  0, batch     0 | loss: 1.0023805MemoryTrain:  epoch  1, batch     0 | loss: 0.8450568MemoryTrain:  epoch  2, batch     0 | loss: 0.6949679MemoryTrain:  epoch  3, batch     0 | loss: 0.5907871MemoryTrain:  epoch  4, batch     0 | loss: 0.4934731MemoryTrain:  epoch  5, batch     0 | loss: 0.4161765MemoryTrain:  epoch  6, batch     0 | loss: 0.3523639MemoryTrain:  epoch  7, batch     0 | loss: 0.2840635MemoryTrain:  epoch  8, batch     0 | loss: 0.2240147MemoryTrain:  epoch  9, batch     0 | loss: 0.2236599

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.75, 37: 0.0, 38: 0.0, 13: 0.0, 15: 0.3225806451612903, 18: 0.0, 21: 0.5833333333333334, 23: 0.5542168674698795, 25: 0.5263157894736842}
Micro-average F1 score: 0.45751633986928103
Weighted-average F1 score: 0.3617709742681617
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.75, 38: 0.0, 11: 0.0, 13: 0.0, 15: 0.5753424657534246, 18: 0.0, 21: 0.9148936170212766, 23: 0.7755102040816326, 25: 0.8333333333333334}
Micro-average F1 score: 0.6808510638297872
Weighted-average F1 score: 0.5831924099114602
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.7058823529411765, 38: 0.0, 11: 0.0, 13: 0.0, 15: 0.5753424657534246, 18: 0.0, 21: 0.9148936170212766, 23: 0.7755102040816326, 25: 0.8571428571428571}
Micro-average F1 score: 0.688
Weighted-average F1 score: 0.5967424226346384

F1 score per class: {0: 0.8985507246376812, 2: 0.48, 4: 0.7577639751552795, 5: 0.8482142857142857, 6: 0.6781609195402298, 10: 0.14678899082568808, 11: 0.06521739130434782, 12: 0.21487603305785125, 13: 0.125, 15: 0.4, 16: 0.7931034482758621, 17: 0.0, 18: 0.3673469387755102, 19: 0.6625, 21: 0.2608695652173913, 23: 0.75, 24: 0.10526315789473684, 25: 0.3225806451612903, 26: 0.7486033519553073, 28: 0.5454545454545454, 29: 0.9081081081081082, 32: 0.882051282051282, 35: 0.5675675675675675, 37: 0.39655172413793105, 38: 0.23809523809523808, 39: 0.0}
Micro-average F1 score: 0.5968894493484658
Weighted-average F1 score: 0.6605318715270835
F1 score per class: {0: 0.96, 2: 0.358974358974359, 4: 0.8304093567251462, 5: 0.8547008547008547, 6: 0.7298578199052133, 10: 0.43609022556390975, 11: 0.17647058823529413, 12: 0.5490196078431373, 13: 0.16326530612244897, 15: 0.5, 16: 0.8666666666666667, 17: 0.0, 18: 0.5, 19: 0.7570621468926554, 21: 0.4175824175824176, 23: 0.7865168539325843, 24: 0.09523809523809523, 25: 0.5753424657534246, 26: 0.7472527472527473, 28: 0.6666666666666666, 29: 0.9197860962566845, 32: 0.8910891089108911, 35: 0.8775510204081632, 37: 0.41304347826086957, 38: 0.4819277108433735, 39: 0.5}
Micro-average F1 score: 0.6710430342815463
Weighted-average F1 score: 0.6781102714694359
F1 score per class: {0: 0.96, 2: 0.3888888888888889, 4: 0.8372093023255814, 5: 0.8620689655172413, 6: 0.7393364928909952, 10: 0.32, 11: 0.17647058823529413, 12: 0.543046357615894, 13: 0.125, 15: 0.4444444444444444, 16: 0.8666666666666667, 17: 0.0, 18: 0.42105263157894735, 19: 0.7570621468926554, 21: 0.3953488372093023, 23: 0.8235294117647058, 24: 0.09523809523809523, 25: 0.5753424657534246, 26: 0.7513812154696132, 28: 0.6666666666666666, 29: 0.9197860962566845, 32: 0.8910891089108911, 35: 0.8865979381443299, 37: 0.40425531914893614, 38: 0.45161290322580644, 39: 0.3333333333333333}
Micro-average F1 score: 0.6634897360703812
Weighted-average F1 score: 0.673874391275085
cur_acc:  ['0.8060', '0.6604', '0.3404', '0.7467', '0.4575']
his_acc:  ['0.8060', '0.7471', '0.6823', '0.6802', '0.5969']
cur_acc des:  ['0.8641', '0.7942', '0.5110', '0.7465', '0.6809']
his_acc des:  ['0.8641', '0.8191', '0.7319', '0.7369', '0.6710']
cur_acc rrf:  ['0.8641', '0.7942', '0.5470', '0.7718', '0.6880']
his_acc rrf:  ['0.8641', '0.8181', '0.7359', '0.7406', '0.6635']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 217.9556088CurrentTrain: epoch  0, batch     1 | loss: 236.8236495CurrentTrain: epoch  0, batch     2 | loss: 212.1177634CurrentTrain: epoch  0, batch     3 | loss: 123.4579317CurrentTrain: epoch  1, batch     0 | loss: 231.5274514CurrentTrain: epoch  1, batch     1 | loss: 271.0607554CurrentTrain: epoch  1, batch     2 | loss: 168.8239772CurrentTrain: epoch  1, batch     3 | loss: 131.6401097CurrentTrain: epoch  2, batch     0 | loss: 224.9970979CurrentTrain: epoch  2, batch     1 | loss: 200.2677177CurrentTrain: epoch  2, batch     2 | loss: 186.5132250CurrentTrain: epoch  2, batch     3 | loss: 121.7616250CurrentTrain: epoch  3, batch     0 | loss: 196.6034156CurrentTrain: epoch  3, batch     1 | loss: 207.3122812CurrentTrain: epoch  3, batch     2 | loss: 172.2758089CurrentTrain: epoch  3, batch     3 | loss: 140.5960814CurrentTrain: epoch  4, batch     0 | loss: 214.3648077CurrentTrain: epoch  4, batch     1 | loss: 224.8663109CurrentTrain: epoch  4, batch     2 | loss: 182.0665851CurrentTrain: epoch  4, batch     3 | loss: 119.0153565CurrentTrain: epoch  5, batch     0 | loss: 212.2782240CurrentTrain: epoch  5, batch     1 | loss: 238.9137022CurrentTrain: epoch  5, batch     2 | loss: 148.5365372CurrentTrain: epoch  5, batch     3 | loss: 140.2837525CurrentTrain: epoch  6, batch     0 | loss: 203.7502956CurrentTrain: epoch  6, batch     1 | loss: 212.1889976CurrentTrain: epoch  6, batch     2 | loss: 215.4840207CurrentTrain: epoch  6, batch     3 | loss: 129.6376294CurrentTrain: epoch  7, batch     0 | loss: 203.6063830CurrentTrain: epoch  7, batch     1 | loss: 177.2384862CurrentTrain: epoch  7, batch     2 | loss: 202.6473866CurrentTrain: epoch  7, batch     3 | loss: 117.9081061CurrentTrain: epoch  8, batch     0 | loss: 167.6663996CurrentTrain: epoch  8, batch     1 | loss: 276.3475229CurrentTrain: epoch  8, batch     2 | loss: 183.8379142CurrentTrain: epoch  8, batch     3 | loss: 110.9623552CurrentTrain: epoch  9, batch     0 | loss: 194.8302804CurrentTrain: epoch  9, batch     1 | loss: 218.9693002CurrentTrain: epoch  9, batch     2 | loss: 178.9026857CurrentTrain: epoch  9, batch     3 | loss: 163.1664788
MemoryTrain:  epoch  0, batch     0 | loss: 0.6926744MemoryTrain:  epoch  1, batch     0 | loss: 0.5618121MemoryTrain:  epoch  2, batch     0 | loss: 0.4245454MemoryTrain:  epoch  3, batch     0 | loss: 0.3324206MemoryTrain:  epoch  4, batch     0 | loss: 0.2885371MemoryTrain:  epoch  5, batch     0 | loss: 0.2302068MemoryTrain:  epoch  6, batch     0 | loss: 0.1916084MemoryTrain:  epoch  7, batch     0 | loss: 0.1488207MemoryTrain:  epoch  8, batch     0 | loss: 0.1415890MemoryTrain:  epoch  9, batch     0 | loss: 0.1168169

F1 score per class: {33: 0.0, 36: 0.0, 5: 0.0, 6: 0.5137614678899083, 37: 0.0, 8: 0.0, 4: 0.6153846153846154, 11: 0.0, 12: 0.0, 20: 0.0, 26: 0.972972972972973, 28: 0.42857142857142855, 29: 0.21333333333333335, 30: 0.0}
Micro-average F1 score: 0.4864864864864865
Weighted-average F1 score: 0.4889586242095687
F1 score per class: {5: 0.0, 6: 0.0, 8: 0.7441860465116279, 10: 0.0, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.9090909090909091, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.42857142857142855, 35: 0.0, 36: 0.85, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7597254004576659
Weighted-average F1 score: 0.6932802690648706
F1 score per class: {4: 0.0, 5: 0.0, 6: 0.0, 8: 0.7633587786259542, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.9090909090909091, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.42857142857142855, 35: 0.0, 36: 0.7222222222222222, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7188940092165899
Weighted-average F1 score: 0.6388969332977156

F1 score per class: {0: 0.8857142857142857, 2: 0.631578947368421, 4: 0.8961748633879781, 5: 0.8471615720524017, 6: 0.6818181818181818, 8: 0.3971631205673759, 10: 0.18018018018018017, 11: 0.18867924528301888, 12: 0.06837606837606838, 13: 0.1, 15: 0.4444444444444444, 16: 0.7931034482758621, 17: 0.0, 18: 0.3829787234042553, 19: 0.6624203821656051, 20: 0.6153846153846154, 21: 0.23809523809523808, 23: 0.6933333333333334, 24: 0.10526315789473684, 25: 0.4, 26: 0.7351351351351352, 28: 0.42857142857142855, 29: 0.8603351955307262, 30: 0.9, 32: 0.8465608465608465, 33: 0.2857142857142857, 35: 0.45454545454545453, 36: 0.21333333333333335, 37: 0.5048543689320388, 38: 0.16216216216216217, 39: 0.0}
Micro-average F1 score: 0.590347923681257
Weighted-average F1 score: 0.6730041635136899
F1 score per class: {0: 0.958904109589041, 2: 0.45161290322580644, 4: 0.8700564971751412, 5: 0.8438818565400844, 6: 0.736318407960199, 8: 0.5333333333333333, 10: 0.42748091603053434, 11: 0.2782608695652174, 12: 0.4305555555555556, 13: 0.2, 15: 0.5, 16: 0.8666666666666667, 17: 0.0, 18: 0.5277777777777778, 19: 0.6867469879518072, 20: 0.9, 21: 0.45, 23: 0.75, 24: 0.09523809523809523, 25: 0.56, 26: 0.7351351351351352, 28: 0.4, 29: 0.9148936170212766, 30: 0.7307692307692307, 32: 0.8955223880597015, 33: 0.2, 35: 0.8712871287128713, 36: 0.6986301369863014, 37: 0.3597122302158273, 38: 0.4918032786885246, 39: 0.26666666666666666}
Micro-average F1 score: 0.6673009831906122
Weighted-average F1 score: 0.6812039440321221
F1 score per class: {0: 0.958904109589041, 2: 0.41379310344827586, 4: 0.8777777777777778, 5: 0.8438818565400844, 6: 0.73, 8: 0.5376344086021505, 10: 0.34146341463414637, 11: 0.2786885245901639, 12: 0.4195804195804196, 13: 0.19047619047619047, 15: 0.46153846153846156, 16: 0.8524590163934426, 17: 0.0, 18: 0.5217391304347826, 19: 0.6867469879518072, 20: 0.9, 21: 0.45, 23: 0.7857142857142857, 24: 0.09523809523809523, 25: 0.5405405405405406, 26: 0.7351351351351352, 28: 0.4, 29: 0.9148936170212766, 30: 0.7307692307692307, 32: 0.8955223880597015, 33: 0.1875, 35: 0.86, 36: 0.6290322580645161, 37: 0.3624161073825503, 38: 0.46875, 39: 0.16666666666666666}
Micro-average F1 score: 0.6575342465753424
Weighted-average F1 score: 0.6721505290051877
cur_acc:  ['0.8060', '0.6604', '0.3404', '0.7467', '0.4575', '0.4865']
his_acc:  ['0.8060', '0.7471', '0.6823', '0.6802', '0.5969', '0.5903']
cur_acc des:  ['0.8641', '0.7942', '0.5110', '0.7465', '0.6809', '0.7597']
his_acc des:  ['0.8641', '0.8191', '0.7319', '0.7369', '0.6710', '0.6673']
cur_acc rrf:  ['0.8641', '0.7942', '0.5470', '0.7718', '0.6880', '0.7189']
his_acc rrf:  ['0.8641', '0.8181', '0.7359', '0.7406', '0.6635', '0.6575']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 200.2817510CurrentTrain: epoch  0, batch     1 | loss: 185.8038715CurrentTrain: epoch  0, batch     2 | loss: 195.8950901CurrentTrain: epoch  0, batch     3 | loss: 15.6396299CurrentTrain: epoch  1, batch     0 | loss: 175.9461654CurrentTrain: epoch  1, batch     1 | loss: 231.6281657CurrentTrain: epoch  1, batch     2 | loss: 191.4535693CurrentTrain: epoch  1, batch     3 | loss: 13.8253300CurrentTrain: epoch  2, batch     0 | loss: 221.5312818CurrentTrain: epoch  2, batch     1 | loss: 206.3824651CurrentTrain: epoch  2, batch     2 | loss: 174.5812942CurrentTrain: epoch  2, batch     3 | loss: 22.3859029CurrentTrain: epoch  3, batch     0 | loss: 189.5568961CurrentTrain: epoch  3, batch     1 | loss: 201.1370966CurrentTrain: epoch  3, batch     2 | loss: 182.9780414CurrentTrain: epoch  3, batch     3 | loss: 11.8168422CurrentTrain: epoch  4, batch     0 | loss: 221.8501799CurrentTrain: epoch  4, batch     1 | loss: 172.0192994CurrentTrain: epoch  4, batch     2 | loss: 168.4710010CurrentTrain: epoch  4, batch     3 | loss: 20.6589258CurrentTrain: epoch  5, batch     0 | loss: 188.3111954CurrentTrain: epoch  5, batch     1 | loss: 188.7692333CurrentTrain: epoch  5, batch     2 | loss: 203.5571929CurrentTrain: epoch  5, batch     3 | loss: 7.6161589CurrentTrain: epoch  6, batch     0 | loss: 184.8337338CurrentTrain: epoch  6, batch     1 | loss: 221.7384046CurrentTrain: epoch  6, batch     2 | loss: 165.4740054CurrentTrain: epoch  6, batch     3 | loss: 20.2357108CurrentTrain: epoch  7, batch     0 | loss: 153.6972674CurrentTrain: epoch  7, batch     1 | loss: 205.0348768CurrentTrain: epoch  7, batch     2 | loss: 194.6122108CurrentTrain: epoch  7, batch     3 | loss: 20.5006520CurrentTrain: epoch  8, batch     0 | loss: 147.5485382CurrentTrain: epoch  8, batch     1 | loss: 202.6092774CurrentTrain: epoch  8, batch     2 | loss: 183.3970937CurrentTrain: epoch  8, batch     3 | loss: 41.2270744CurrentTrain: epoch  9, batch     0 | loss: 186.7791085CurrentTrain: epoch  9, batch     1 | loss: 146.1068262CurrentTrain: epoch  9, batch     2 | loss: 210.6603581CurrentTrain: epoch  9, batch     3 | loss: 41.2211698
MemoryTrain:  epoch  0, batch     0 | loss: 0.5824832MemoryTrain:  epoch  1, batch     0 | loss: 0.5195186MemoryTrain:  epoch  2, batch     0 | loss: 0.3270877MemoryTrain:  epoch  3, batch     0 | loss: 0.2278086MemoryTrain:  epoch  4, batch     0 | loss: 0.1930393MemoryTrain:  epoch  5, batch     0 | loss: 0.1853676MemoryTrain:  epoch  6, batch     0 | loss: 0.1420710MemoryTrain:  epoch  7, batch     0 | loss: 0.1238178MemoryTrain:  epoch  8, batch     0 | loss: 0.1093399MemoryTrain:  epoch  9, batch     0 | loss: 0.1051288

F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 26: 0.4166666666666667, 27: 0.0, 31: 0.449438202247191}
Micro-average F1 score: 0.49514563106796117
Weighted-average F1 score: 0.4224420960372424
F1 score per class: {35: 0.0, 6: 0.5714285714285714, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 21: 0.64, 26: 1.0, 27: 0.0, 31: 0.9133858267716536}
Micro-average F1 score: 0.8482142857142857
Weighted-average F1 score: 0.8111129531268486
F1 score per class: {35: 0.0, 6: 0.5714285714285714, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 21: 0.64, 26: 1.0, 27: 0.0, 31: 0.8870967741935484}
Micro-average F1 score: 0.8251121076233184
Weighted-average F1 score: 0.7799089946721512

F1 score per class: {0: 0.8656716417910447, 2: 0.5263157894736842, 4: 0.8636363636363636, 5: 0.8722466960352423, 6: 0.5503355704697986, 7: 0.047619047619047616, 8: 0.45454545454545453, 9: 0.9803921568627451, 10: 0.11214953271028037, 11: 0.2037037037037037, 12: 0.01694915254237288, 13: 0.08, 15: 0.46153846153846156, 16: 0.7586206896551724, 17: 0.0, 18: 0.5161290322580645, 19: 0.6185567010309279, 20: 0.5789473684210527, 21: 0.2631578947368421, 23: 0.6388888888888888, 24: 0.1, 25: 0.45714285714285713, 26: 0.7157894736842105, 27: 0.2631578947368421, 28: 0.36363636363636365, 29: 0.8409090909090909, 30: 0.8888888888888888, 31: 0.0, 32: 0.797752808988764, 33: 0.20689655172413793, 35: 0.40625, 36: 0.28205128205128205, 37: 0.5, 38: 0.16216216216216217, 39: 0.1111111111111111, 40: 0.40404040404040403}
Micro-average F1 score: 0.556241426611797
Weighted-average F1 score: 0.6118183446992109
F1 score per class: {0: 0.9295774647887324, 2: 0.41379310344827586, 4: 0.8636363636363636, 5: 0.8403361344537815, 6: 0.5512820512820513, 7: 0.07407407407407407, 8: 0.5838509316770186, 9: 0.9803921568627451, 10: 0.3252032520325203, 11: 0.2644628099173554, 12: 0.4027777777777778, 13: 0.06896551724137931, 15: 0.5, 16: 0.847457627118644, 17: 0.0, 18: 0.5185185185185185, 19: 0.6666666666666666, 20: 0.9, 21: 0.5671641791044776, 23: 0.6582278481012658, 24: 0.09090909090909091, 25: 0.56, 26: 0.7046632124352331, 27: 0.42105263157894735, 28: 0.3, 29: 0.8983957219251337, 30: 1.0, 31: 0.6666666666666666, 32: 0.8481675392670157, 33: 0.15789473684210525, 35: 0.845360824742268, 36: 0.7164179104477612, 37: 0.43548387096774194, 38: 0.36065573770491804, 39: 0.21428571428571427, 40: 0.6705202312138728}
Micro-average F1 score: 0.638285203929741
Weighted-average F1 score: 0.6432600561174499
F1 score per class: {0: 0.9295774647887324, 2: 0.42857142857142855, 4: 0.8715083798882681, 5: 0.8368200836820083, 6: 0.5512820512820513, 7: 0.07017543859649122, 8: 0.5662650602409639, 9: 0.9803921568627451, 10: 0.2833333333333333, 11: 0.27419354838709675, 12: 0.3829787234042553, 13: 0.06666666666666667, 15: 0.5, 16: 0.8333333333333334, 17: 0.0, 18: 0.5135135135135135, 19: 0.6904761904761905, 20: 0.8888888888888888, 21: 0.5614035087719298, 23: 0.6753246753246753, 24: 0.09090909090909091, 25: 0.56, 26: 0.7046632124352331, 27: 0.36363636363636365, 28: 0.375, 29: 0.8983957219251337, 30: 0.95, 31: 0.6666666666666666, 32: 0.8481675392670157, 33: 0.15, 35: 0.8210526315789474, 36: 0.5818181818181818, 37: 0.40559440559440557, 38: 0.3448275862068966, 39: 0.16, 40: 0.6790123456790124}
Micro-average F1 score: 0.6275097392867846
Weighted-average F1 score: 0.6318850834137866
cur_acc:  ['0.8060', '0.6604', '0.3404', '0.7467', '0.4575', '0.4865', '0.4951']
his_acc:  ['0.8060', '0.7471', '0.6823', '0.6802', '0.5969', '0.5903', '0.5562']
cur_acc des:  ['0.8641', '0.7942', '0.5110', '0.7465', '0.6809', '0.7597', '0.8482']
his_acc des:  ['0.8641', '0.8191', '0.7319', '0.7369', '0.6710', '0.6673', '0.6383']
cur_acc rrf:  ['0.8641', '0.7942', '0.5470', '0.7718', '0.6880', '0.7189', '0.8251']
his_acc rrf:  ['0.8641', '0.8181', '0.7359', '0.7406', '0.6635', '0.6575', '0.6275']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 253.0873704CurrentTrain: epoch  0, batch     1 | loss: 213.3983836CurrentTrain: epoch  0, batch     2 | loss: 224.1007500CurrentTrain: epoch  0, batch     3 | loss: 205.0798426CurrentTrain: epoch  0, batch     4 | loss: 208.0549398CurrentTrain: epoch  1, batch     0 | loss: 205.3854610CurrentTrain: epoch  1, batch     1 | loss: 246.0727467CurrentTrain: epoch  1, batch     2 | loss: 246.5881665CurrentTrain: epoch  1, batch     3 | loss: 238.5554367CurrentTrain: epoch  1, batch     4 | loss: 130.9795076CurrentTrain: epoch  2, batch     0 | loss: 224.4717234CurrentTrain: epoch  2, batch     1 | loss: 174.7201768CurrentTrain: epoch  2, batch     2 | loss: 199.7848234CurrentTrain: epoch  2, batch     3 | loss: 215.4597471CurrentTrain: epoch  2, batch     4 | loss: 321.1917277CurrentTrain: epoch  3, batch     0 | loss: 245.3239419CurrentTrain: epoch  3, batch     1 | loss: 182.2744192CurrentTrain: epoch  3, batch     2 | loss: 291.4024546CurrentTrain: epoch  3, batch     3 | loss: 229.7846612CurrentTrain: epoch  3, batch     4 | loss: 109.1219799CurrentTrain: epoch  4, batch     0 | loss: 213.2730864CurrentTrain: epoch  4, batch     1 | loss: 198.3880294CurrentTrain: epoch  4, batch     2 | loss: 213.2969848CurrentTrain: epoch  4, batch     3 | loss: 239.2600263CurrentTrain: epoch  4, batch     4 | loss: 126.0522704CurrentTrain: epoch  5, batch     0 | loss: 287.5421280CurrentTrain: epoch  5, batch     1 | loss: 220.9556533CurrentTrain: epoch  5, batch     2 | loss: 189.8053050CurrentTrain: epoch  5, batch     3 | loss: 188.0475796CurrentTrain: epoch  5, batch     4 | loss: 203.8311355CurrentTrain: epoch  6, batch     0 | loss: 176.4407198CurrentTrain: epoch  6, batch     1 | loss: 196.6881322CurrentTrain: epoch  6, batch     2 | loss: 212.0670557CurrentTrain: epoch  6, batch     3 | loss: 247.3684118CurrentTrain: epoch  6, batch     4 | loss: 157.4646968CurrentTrain: epoch  7, batch     0 | loss: 247.7609321CurrentTrain: epoch  7, batch     1 | loss: 179.9986521CurrentTrain: epoch  7, batch     2 | loss: 222.2510533CurrentTrain: epoch  7, batch     3 | loss: 221.1677216CurrentTrain: epoch  7, batch     4 | loss: 118.2931557CurrentTrain: epoch  8, batch     0 | loss: 195.7015211CurrentTrain: epoch  8, batch     1 | loss: 191.8800436CurrentTrain: epoch  8, batch     2 | loss: 287.0352430CurrentTrain: epoch  8, batch     3 | loss: 179.6742726CurrentTrain: epoch  8, batch     4 | loss: 201.9560666CurrentTrain: epoch  9, batch     0 | loss: 237.6622424CurrentTrain: epoch  9, batch     1 | loss: 203.5309143CurrentTrain: epoch  9, batch     2 | loss: 211.1926275CurrentTrain: epoch  9, batch     3 | loss: 183.1490786CurrentTrain: epoch  9, batch     4 | loss: 148.5536051
MemoryTrain:  epoch  0, batch     0 | loss: 0.9360632MemoryTrain:  epoch  1, batch     0 | loss: 0.7461954MemoryTrain:  epoch  2, batch     0 | loss: 0.5713530MemoryTrain:  epoch  3, batch     0 | loss: 0.4859894MemoryTrain:  epoch  4, batch     0 | loss: 0.4152304MemoryTrain:  epoch  5, batch     0 | loss: 0.3175426MemoryTrain:  epoch  6, batch     0 | loss: 0.2774557MemoryTrain:  epoch  7, batch     0 | loss: 0.2231011MemoryTrain:  epoch  8, batch     0 | loss: 0.1893451MemoryTrain:  epoch  9, batch     0 | loss: 0.1694094

F1 score per class: {32: 0.3937007874015748, 1: 0.7761194029850746, 34: 0.0, 3: 0.1282051282051282, 35: 0.0, 37: 0.7912087912087912, 40: 0.0, 11: 0.0, 14: 0.0, 18: 0.0, 22: 0.4507042253521127, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.5354330708661418
Weighted-average F1 score: 0.5040309754986004
F1 score per class: {32: 0.3893805309734513, 1: 0.8689655172413793, 34: 0.0, 3: 0.17391304347826086, 35: 0.0, 37: 0.7525773195876289, 38: 0.0, 5: 0.0, 40: 0.0, 14: 0.8775510204081632, 18: 0.0, 22: 0.0, 23: 0.0, 24: 0.0}
Micro-average F1 score: 0.6192592592592593
Weighted-average F1 score: 0.5882724019473222
F1 score per class: {32: 0.38596491228070173, 1: 0.8689655172413793, 34: 0.0, 3: 0.0, 35: 0.20224719101123595, 37: 0.0, 38: 0.7692307692307693, 5: 0.0, 40: 0.0, 11: 0.0, 14: 0.0, 18: 0.75, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.6020864381520119
Weighted-average F1 score: 0.5641767337807393

F1 score per class: {0: 0.7017543859649122, 1: 0.3424657534246575, 2: 0.5333333333333333, 3: 0.7074829931972789, 4: 0.8764044943820225, 5: 0.8495575221238938, 6: 0.48226950354609927, 7: 0.09302325581395349, 8: 0.47761194029850745, 9: 0.9803921568627451, 10: 0.21428571428571427, 11: 0.08080808080808081, 12: 0.017391304347826087, 13: 0.10526315789473684, 14: 0.10989010989010989, 15: 0.5, 16: 0.7719298245614035, 17: 0.0, 18: 0.4375, 19: 0.4968152866242038, 20: 0.5, 21: 0.0, 22: 0.7384615384615385, 23: 0.7341772151898734, 24: 0.08, 25: 0.3492063492063492, 26: 0.7311827956989247, 27: 0.0, 28: 0.5, 29: 0.8342857142857143, 30: 0.9473684210526315, 31: 0.0, 32: 0.7294117647058823, 33: 0.3, 34: 0.24615384615384617, 35: 0.5416666666666666, 36: 0.3037974683544304, 37: 0.42424242424242425, 38: 0.18181818181818182, 39: 0.1111111111111111, 40: 0.4583333333333333}
Micro-average F1 score: 0.5270002827254736
Weighted-average F1 score: 0.5716054465143291
F1 score per class: {0: 0.8985507246376812, 1: 0.3188405797101449, 2: 0.41379310344827586, 3: 0.7924528301886793, 4: 0.8764044943820225, 5: 0.8298755186721992, 6: 0.5859872611464968, 7: 0.09302325581395349, 8: 0.5371428571428571, 9: 0.9803921568627451, 10: 0.34146341463414637, 11: 0.044444444444444446, 12: 0.35294117647058826, 13: 0.09523809523809523, 14: 0.1509433962264151, 15: 0.5714285714285714, 16: 0.847457627118644, 17: 0.0, 18: 0.42424242424242425, 19: 0.5070422535211268, 20: 0.8297872340425532, 21: 0.12121212121212122, 22: 0.6854460093896714, 23: 0.7906976744186046, 24: 0.08, 25: 0.56, 26: 0.7195767195767195, 27: 0.0, 28: 0.3333333333333333, 29: 0.9090909090909091, 30: 1.0, 31: 0.6666666666666666, 32: 0.8253968253968254, 33: 0.24, 34: 0.3426294820717131, 35: 0.7586206896551724, 36: 0.6728971962616822, 37: 0.2564102564102564, 38: 0.2571428571428571, 39: 0.14814814814814814, 40: 0.6091954022988506}
Micro-average F1 score: 0.5858736059479553
Weighted-average F1 score: 0.5983763218683573
F1 score per class: {0: 0.8985507246376812, 1: 0.3142857142857143, 2: 0.42857142857142855, 3: 0.7730061349693251, 4: 0.8888888888888888, 5: 0.8264462809917356, 6: 0.5859872611464968, 7: 0.08888888888888889, 8: 0.5340909090909091, 9: 0.9803921568627451, 10: 0.3, 11: 0.06315789473684211, 12: 0.32592592592592595, 13: 0.09090909090909091, 14: 0.16981132075471697, 15: 0.5, 16: 0.8333333333333334, 17: 0.0, 18: 0.4411764705882353, 19: 0.5859872611464968, 20: 0.8297872340425532, 21: 0.0625, 22: 0.7009345794392523, 23: 0.7654320987654321, 24: 0.08, 25: 0.5405405405405406, 26: 0.7195767195767195, 27: 0.0, 28: 0.375, 29: 0.9090909090909091, 30: 0.926829268292683, 31: 0.8, 32: 0.8253968253968254, 33: 0.24, 34: 0.3113207547169811, 35: 0.7478260869565218, 36: 0.6857142857142857, 37: 0.32967032967032966, 38: 0.27692307692307694, 39: 0.08333333333333333, 40: 0.6428571428571429}
Micro-average F1 score: 0.5878818227341012
Weighted-average F1 score: 0.6008250792859922
cur_acc:  ['0.8060', '0.6604', '0.3404', '0.7467', '0.4575', '0.4865', '0.4951', '0.5354']
his_acc:  ['0.8060', '0.7471', '0.6823', '0.6802', '0.5969', '0.5903', '0.5562', '0.5270']
cur_acc des:  ['0.8641', '0.7942', '0.5110', '0.7465', '0.6809', '0.7597', '0.8482', '0.6193']
his_acc des:  ['0.8641', '0.8191', '0.7319', '0.7369', '0.6710', '0.6673', '0.6383', '0.5859']
cur_acc rrf:  ['0.8641', '0.7942', '0.5470', '0.7718', '0.6880', '0.7189', '0.8251', '0.6021']
his_acc rrf:  ['0.8641', '0.8181', '0.7359', '0.7406', '0.6635', '0.6575', '0.6275', '0.5879']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 188.0400244CurrentTrain: epoch  0, batch     1 | loss: 200.1278915CurrentTrain: epoch  0, batch     2 | loss: 220.9906136CurrentTrain: epoch  0, batch     3 | loss: 255.0894096CurrentTrain: epoch  0, batch     4 | loss: 221.3392450CurrentTrain: epoch  0, batch     5 | loss: 171.1806854CurrentTrain: epoch  0, batch     6 | loss: 245.4393019CurrentTrain: epoch  0, batch     7 | loss: 192.8672440CurrentTrain: epoch  0, batch     8 | loss: 252.0917841CurrentTrain: epoch  0, batch     9 | loss: 198.0899424CurrentTrain: epoch  0, batch    10 | loss: 218.0512370CurrentTrain: epoch  0, batch    11 | loss: 244.7510346CurrentTrain: epoch  0, batch    12 | loss: 297.0634161CurrentTrain: epoch  0, batch    13 | loss: 198.6495733CurrentTrain: epoch  0, batch    14 | loss: 197.7194142CurrentTrain: epoch  0, batch    15 | loss: 231.7298969CurrentTrain: epoch  0, batch    16 | loss: 198.1196303CurrentTrain: epoch  0, batch    17 | loss: 295.7044763CurrentTrain: epoch  0, batch    18 | loss: 191.9880735CurrentTrain: epoch  0, batch    19 | loss: 204.3533319CurrentTrain: epoch  0, batch    20 | loss: 181.9015188CurrentTrain: epoch  0, batch    21 | loss: 203.8926818CurrentTrain: epoch  0, batch    22 | loss: 181.4408126CurrentTrain: epoch  0, batch    23 | loss: 256.5899964CurrentTrain: epoch  0, batch    24 | loss: 211.6518801CurrentTrain: epoch  0, batch    25 | loss: 223.4972949CurrentTrain: epoch  0, batch    26 | loss: 295.6913405CurrentTrain: epoch  0, batch    27 | loss: 196.9455411CurrentTrain: epoch  0, batch    28 | loss: 184.8113141CurrentTrain: epoch  0, batch    29 | loss: 249.6745597CurrentTrain: epoch  0, batch    30 | loss: 366.0307794CurrentTrain: epoch  0, batch    31 | loss: 162.8648912CurrentTrain: epoch  0, batch    32 | loss: 203.2227285CurrentTrain: epoch  0, batch    33 | loss: 201.7023364CurrentTrain: epoch  0, batch    34 | loss: 208.9523657CurrentTrain: epoch  0, batch    35 | loss: 222.6780571CurrentTrain: epoch  0, batch    36 | loss: 190.8254260CurrentTrain: epoch  0, batch    37 | loss: 210.6538308CurrentTrain: epoch  0, batch    38 | loss: 196.7765191CurrentTrain: epoch  0, batch    39 | loss: 215.2707153CurrentTrain: epoch  0, batch    40 | loss: 169.4945649CurrentTrain: epoch  0, batch    41 | loss: 294.3644459CurrentTrain: epoch  0, batch    42 | loss: 209.3140072CurrentTrain: epoch  0, batch    43 | loss: 190.3525683CurrentTrain: epoch  0, batch    44 | loss: 216.3351522CurrentTrain: epoch  0, batch    45 | loss: 210.9127622CurrentTrain: epoch  0, batch    46 | loss: 279.9149078CurrentTrain: epoch  0, batch    47 | loss: 222.8376193CurrentTrain: epoch  0, batch    48 | loss: 181.7530223CurrentTrain: epoch  0, batch    49 | loss: 191.6478085CurrentTrain: epoch  0, batch    50 | loss: 208.5180086CurrentTrain: epoch  0, batch    51 | loss: 195.7774406CurrentTrain: epoch  0, batch    52 | loss: 177.7394851CurrentTrain: epoch  0, batch    53 | loss: 190.1775332CurrentTrain: epoch  0, batch    54 | loss: 182.0565624CurrentTrain: epoch  0, batch    55 | loss: 221.3920322CurrentTrain: epoch  0, batch    56 | loss: 222.9051945CurrentTrain: epoch  0, batch    57 | loss: 256.0415285CurrentTrain: epoch  0, batch    58 | loss: 240.4170174CurrentTrain: epoch  0, batch    59 | loss: 254.7423878CurrentTrain: epoch  0, batch    60 | loss: 201.2880230CurrentTrain: epoch  0, batch    61 | loss: 176.3293261CurrentTrain: epoch  0, batch    62 | loss: 558.7381511CurrentTrain: epoch  0, batch    63 | loss: 163.1843392CurrentTrain: epoch  0, batch    64 | loss: 194.0071429CurrentTrain: epoch  0, batch    65 | loss: 188.1422628CurrentTrain: epoch  0, batch    66 | loss: 187.8198913CurrentTrain: epoch  0, batch    67 | loss: 287.4955290CurrentTrain: epoch  0, batch    68 | loss: 187.6695402CurrentTrain: epoch  0, batch    69 | loss: 214.6372794CurrentTrain: epoch  0, batch    70 | loss: 283.7009316CurrentTrain: epoch  0, batch    71 | loss: 256.2007256CurrentTrain: epoch  0, batch    72 | loss: 226.9245988CurrentTrain: epoch  0, batch    73 | loss: 247.2781499CurrentTrain: epoch  0, batch    74 | loss: 214.7900145CurrentTrain: epoch  0, batch    75 | loss: 206.1860841CurrentTrain: epoch  0, batch    76 | loss: 212.6066203CurrentTrain: epoch  0, batch    77 | loss: 255.1761954CurrentTrain: epoch  0, batch    78 | loss: 219.9118376CurrentTrain: epoch  0, batch    79 | loss: 281.8303784CurrentTrain: epoch  0, batch    80 | loss: 219.1957692CurrentTrain: epoch  0, batch    81 | loss: 253.7434307CurrentTrain: epoch  0, batch    82 | loss: 245.9303287CurrentTrain: epoch  0, batch    83 | loss: 204.7448543CurrentTrain: epoch  0, batch    84 | loss: 251.9973141CurrentTrain: epoch  0, batch    85 | loss: 218.8734143CurrentTrain: epoch  0, batch    86 | loss: 237.1456636CurrentTrain: epoch  0, batch    87 | loss: 230.9114106CurrentTrain: epoch  0, batch    88 | loss: 211.4492987CurrentTrain: epoch  0, batch    89 | loss: 170.8608781CurrentTrain: epoch  0, batch    90 | loss: 207.2528992CurrentTrain: epoch  0, batch    91 | loss: 166.1523381CurrentTrain: epoch  0, batch    92 | loss: 180.4060392CurrentTrain: epoch  0, batch    93 | loss: 181.0114985CurrentTrain: epoch  0, batch    94 | loss: 176.3863878CurrentTrain: epoch  0, batch    95 | loss: 237.0832546CurrentTrain: epoch  1, batch     0 | loss: 197.4925832CurrentTrain: epoch  1, batch     1 | loss: 224.4167218CurrentTrain: epoch  1, batch     2 | loss: 220.1081895CurrentTrain: epoch  1, batch     3 | loss: 220.0125965CurrentTrain: epoch  1, batch     4 | loss: 195.8936274CurrentTrain: epoch  1, batch     5 | loss: 232.7864569CurrentTrain: epoch  1, batch     6 | loss: 175.2490272CurrentTrain: epoch  1, batch     7 | loss: 182.7917218CurrentTrain: epoch  1, batch     8 | loss: 208.0769890CurrentTrain: epoch  1, batch     9 | loss: 188.3283213CurrentTrain: epoch  1, batch    10 | loss: 363.5269460CurrentTrain: epoch  1, batch    11 | loss: 282.7473264CurrentTrain: epoch  1, batch    12 | loss: 191.7355479CurrentTrain: epoch  1, batch    13 | loss: 192.8800584CurrentTrain: epoch  1, batch    14 | loss: 190.8308956CurrentTrain: epoch  1, batch    15 | loss: 225.8097218CurrentTrain: epoch  1, batch    16 | loss: 226.9618188CurrentTrain: epoch  1, batch    17 | loss: 187.6399924CurrentTrain: epoch  1, batch    18 | loss: 200.2759853CurrentTrain: epoch  1, batch    19 | loss: 172.4292590CurrentTrain: epoch  1, batch    20 | loss: 180.5897680CurrentTrain: epoch  1, batch    21 | loss: 190.2902171CurrentTrain: epoch  1, batch    22 | loss: 202.8770099CurrentTrain: epoch  1, batch    23 | loss: 168.7768175CurrentTrain: epoch  1, batch    24 | loss: 220.0497245CurrentTrain: epoch  1, batch    25 | loss: 168.9862260CurrentTrain: epoch  1, batch    26 | loss: 355.5007395CurrentTrain: epoch  1, batch    27 | loss: 210.1028278CurrentTrain: epoch  1, batch    28 | loss: 205.7696771CurrentTrain: epoch  1, batch    29 | loss: 202.2044325CurrentTrain: epoch  1, batch    30 | loss: 175.0785089CurrentTrain: epoch  1, batch    31 | loss: 211.7606894CurrentTrain: epoch  1, batch    32 | loss: 202.2378594CurrentTrain: epoch  1, batch    33 | loss: 223.6921391CurrentTrain: epoch  1, batch    34 | loss: 342.2593728CurrentTrain: epoch  1, batch    35 | loss: 240.4735318CurrentTrain: epoch  1, batch    36 | loss: 248.3843750CurrentTrain: epoch  1, batch    37 | loss: 199.7973905CurrentTrain: epoch  1, batch    38 | loss: 167.6618271CurrentTrain: epoch  1, batch    39 | loss: 239.8046189CurrentTrain: epoch  1, batch    40 | loss: 180.7118137CurrentTrain: epoch  1, batch    41 | loss: 251.3757696CurrentTrain: epoch  1, batch    42 | loss: 218.6595387CurrentTrain: epoch  1, batch    43 | loss: 181.4692854CurrentTrain: epoch  1, batch    44 | loss: 217.5918748CurrentTrain: epoch  1, batch    45 | loss: 162.1681112CurrentTrain: epoch  1, batch    46 | loss: 214.2938359CurrentTrain: epoch  1, batch    47 | loss: 231.6017157CurrentTrain: epoch  1, batch    48 | loss: 277.9638959CurrentTrain: epoch  1, batch    49 | loss: 172.7601604CurrentTrain: epoch  1, batch    50 | loss: 272.9417299CurrentTrain: epoch  1, batch    51 | loss: 177.1986485CurrentTrain: epoch  1, batch    52 | loss: 180.1543172CurrentTrain: epoch  1, batch    53 | loss: 235.7432005CurrentTrain: epoch  1, batch    54 | loss: 149.8919840CurrentTrain: epoch  1, batch    55 | loss: 237.9197879CurrentTrain: epoch  1, batch    56 | loss: 168.8049367CurrentTrain: epoch  1, batch    57 | loss: 249.9063078CurrentTrain: epoch  1, batch    58 | loss: 227.5273672CurrentTrain: epoch  1, batch    59 | loss: 210.4062257CurrentTrain: epoch  1, batch    60 | loss: 198.4072038CurrentTrain: epoch  1, batch    61 | loss: 197.9221713CurrentTrain: epoch  1, batch    62 | loss: 217.1737067CurrentTrain: epoch  1, batch    63 | loss: 224.1985158CurrentTrain: epoch  1, batch    64 | loss: 193.6763103CurrentTrain: epoch  1, batch    65 | loss: 199.7522559CurrentTrain: epoch  1, batch    66 | loss: 171.7017442CurrentTrain: epoch  1, batch    67 | loss: 201.8925501CurrentTrain: epoch  1, batch    68 | loss: 216.9193392CurrentTrain: epoch  1, batch    69 | loss: 178.8977703CurrentTrain: epoch  1, batch    70 | loss: 192.2731416CurrentTrain: epoch  1, batch    71 | loss: 213.1570867CurrentTrain: epoch  1, batch    72 | loss: 201.0382956CurrentTrain: epoch  1, batch    73 | loss: 222.8137882CurrentTrain: epoch  1, batch    74 | loss: 208.8146574CurrentTrain: epoch  1, batch    75 | loss: 201.0539959CurrentTrain: epoch  1, batch    76 | loss: 200.4940930CurrentTrain: epoch  1, batch    77 | loss: 192.2858164CurrentTrain: epoch  1, batch    78 | loss: 202.1704363CurrentTrain: epoch  1, batch    79 | loss: 224.9084149CurrentTrain: epoch  1, batch    80 | loss: 282.8589283CurrentTrain: epoch  1, batch    81 | loss: 169.4105332CurrentTrain: epoch  1, batch    82 | loss: 189.9229853CurrentTrain: epoch  1, batch    83 | loss: 195.0838920CurrentTrain: epoch  1, batch    84 | loss: 216.1835512CurrentTrain: epoch  1, batch    85 | loss: 224.7650598CurrentTrain: epoch  1, batch    86 | loss: 241.8123077CurrentTrain: epoch  1, batch    87 | loss: 174.9056924CurrentTrain: epoch  1, batch    88 | loss: 194.7466262CurrentTrain: epoch  1, batch    89 | loss: 198.5088902CurrentTrain: epoch  1, batch    90 | loss: 252.7888153CurrentTrain: epoch  1, batch    91 | loss: 226.1990540CurrentTrain: epoch  1, batch    92 | loss: 273.1898229CurrentTrain: epoch  1, batch    93 | loss: 233.9306668CurrentTrain: epoch  1, batch    94 | loss: 200.5625118CurrentTrain: epoch  1, batch    95 | loss: 186.2536758CurrentTrain: epoch  2, batch     0 | loss: 239.6361070CurrentTrain: epoch  2, batch     1 | loss: 196.6969017CurrentTrain: epoch  2, batch     2 | loss: 234.7664916CurrentTrain: epoch  2, batch     3 | loss: 220.3121569CurrentTrain: epoch  2, batch     4 | loss: 144.7429220CurrentTrain: epoch  2, batch     5 | loss: 157.5796298CurrentTrain: epoch  2, batch     6 | loss: 163.6139768CurrentTrain: epoch  2, batch     7 | loss: 213.3930068CurrentTrain: epoch  2, batch     8 | loss: 223.0481824CurrentTrain: epoch  2, batch     9 | loss: 162.7926714CurrentTrain: epoch  2, batch    10 | loss: 186.6003577CurrentTrain: epoch  2, batch    11 | loss: 215.6146102CurrentTrain: epoch  2, batch    12 | loss: 209.9377195CurrentTrain: epoch  2, batch    13 | loss: 229.7934904CurrentTrain: epoch  2, batch    14 | loss: 259.6404393CurrentTrain: epoch  2, batch    15 | loss: 145.4916005CurrentTrain: epoch  2, batch    16 | loss: 287.5803524CurrentTrain: epoch  2, batch    17 | loss: 207.3940513CurrentTrain: epoch  2, batch    18 | loss: 248.2734797CurrentTrain: epoch  2, batch    19 | loss: 178.3011428CurrentTrain: epoch  2, batch    20 | loss: 248.6564278CurrentTrain: epoch  2, batch    21 | loss: 194.4005134CurrentTrain: epoch  2, batch    22 | loss: 268.2261950CurrentTrain: epoch  2, batch    23 | loss: 273.5738574CurrentTrain: epoch  2, batch    24 | loss: 194.4528077CurrentTrain: epoch  2, batch    25 | loss: 190.6607080CurrentTrain: epoch  2, batch    26 | loss: 239.0113473CurrentTrain: epoch  2, batch    27 | loss: 159.1359390CurrentTrain: epoch  2, batch    28 | loss: 360.4198292CurrentTrain: epoch  2, batch    29 | loss: 239.7568619CurrentTrain: epoch  2, batch    30 | loss: 267.4392416CurrentTrain: epoch  2, batch    31 | loss: 248.8230567CurrentTrain: epoch  2, batch    32 | loss: 229.6074065CurrentTrain: epoch  2, batch    33 | loss: 180.4769964CurrentTrain: epoch  2, batch    34 | loss: 136.9111178CurrentTrain: epoch  2, batch    35 | loss: 230.4237556CurrentTrain: epoch  2, batch    36 | loss: 263.4282906CurrentTrain: epoch  2, batch    37 | loss: 222.3368811CurrentTrain: epoch  2, batch    38 | loss: 193.7754348CurrentTrain: epoch  2, batch    39 | loss: 221.3220294CurrentTrain: epoch  2, batch    40 | loss: 213.7875351CurrentTrain: epoch  2, batch    41 | loss: 189.0791502CurrentTrain: epoch  2, batch    42 | loss: 185.2477073CurrentTrain: epoch  2, batch    43 | loss: 179.5023242CurrentTrain: epoch  2, batch    44 | loss: 171.4530618CurrentTrain: epoch  2, batch    45 | loss: 214.8799821CurrentTrain: epoch  2, batch    46 | loss: 198.6534146CurrentTrain: epoch  2, batch    47 | loss: 195.5714057CurrentTrain: epoch  2, batch    48 | loss: 212.8483738CurrentTrain: epoch  2, batch    49 | loss: 162.9300006CurrentTrain: epoch  2, batch    50 | loss: 213.1763036CurrentTrain: epoch  2, batch    51 | loss: 167.7398307CurrentTrain: epoch  2, batch    52 | loss: 177.3660495CurrentTrain: epoch  2, batch    53 | loss: 277.5278500CurrentTrain: epoch  2, batch    54 | loss: 168.6044264CurrentTrain: epoch  2, batch    55 | loss: 216.9539192CurrentTrain: epoch  2, batch    56 | loss: 268.9377316CurrentTrain: epoch  2, batch    57 | loss: 200.8314946CurrentTrain: epoch  2, batch    58 | loss: 196.3646476CurrentTrain: epoch  2, batch    59 | loss: 169.1740277CurrentTrain: epoch  2, batch    60 | loss: 239.6552131CurrentTrain: epoch  2, batch    61 | loss: 157.3642068CurrentTrain: epoch  2, batch    62 | loss: 185.0043567CurrentTrain: epoch  2, batch    63 | loss: 214.4156338CurrentTrain: epoch  2, batch    64 | loss: 243.0492341CurrentTrain: epoch  2, batch    65 | loss: 170.6022261CurrentTrain: epoch  2, batch    66 | loss: 212.1686304CurrentTrain: epoch  2, batch    67 | loss: 202.4313832CurrentTrain: epoch  2, batch    68 | loss: 243.1904267CurrentTrain: epoch  2, batch    69 | loss: 186.5021761CurrentTrain: epoch  2, batch    70 | loss: 187.8235261CurrentTrain: epoch  2, batch    71 | loss: 178.9021603CurrentTrain: epoch  2, batch    72 | loss: 241.1191241CurrentTrain: epoch  2, batch    73 | loss: 248.4553747CurrentTrain: epoch  2, batch    74 | loss: 242.8680590CurrentTrain: epoch  2, batch    75 | loss: 168.2132549CurrentTrain: epoch  2, batch    76 | loss: 247.8069154CurrentTrain: epoch  2, batch    77 | loss: 198.5755410CurrentTrain: epoch  2, batch    78 | loss: 193.4183900CurrentTrain: epoch  2, batch    79 | loss: 250.2169487CurrentTrain: epoch  2, batch    80 | loss: 205.9002922CurrentTrain: epoch  2, batch    81 | loss: 172.3650181CurrentTrain: epoch  2, batch    82 | loss: 196.1473136CurrentTrain: epoch  2, batch    83 | loss: 278.5257010CurrentTrain: epoch  2, batch    84 | loss: 277.7235684CurrentTrain: epoch  2, batch    85 | loss: 192.6630958CurrentTrain: epoch  2, batch    86 | loss: 214.6796690CurrentTrain: epoch  2, batch    87 | loss: 200.0933027CurrentTrain: epoch  2, batch    88 | loss: 212.5087736CurrentTrain: epoch  2, batch    89 | loss: 207.5001278CurrentTrain: epoch  2, batch    90 | loss: 184.2465817CurrentTrain: epoch  2, batch    91 | loss: 204.5266541CurrentTrain: epoch  2, batch    92 | loss: 180.5159522CurrentTrain: epoch  2, batch    93 | loss: 170.5915658CurrentTrain: epoch  2, batch    94 | loss: 290.9442912CurrentTrain: epoch  2, batch    95 | loss: 175.7524150CurrentTrain: epoch  3, batch     0 | loss: 178.8802490CurrentTrain: epoch  3, batch     1 | loss: 239.7557519CurrentTrain: epoch  3, batch     2 | loss: 187.6571016CurrentTrain: epoch  3, batch     3 | loss: 162.5506719CurrentTrain: epoch  3, batch     4 | loss: 205.5343344CurrentTrain: epoch  3, batch     5 | loss: 241.0865201CurrentTrain: epoch  3, batch     6 | loss: 170.4572730CurrentTrain: epoch  3, batch     7 | loss: 204.7830389CurrentTrain: epoch  3, batch     8 | loss: 287.7952525CurrentTrain: epoch  3, batch     9 | loss: 180.2199264CurrentTrain: epoch  3, batch    10 | loss: 198.6502578CurrentTrain: epoch  3, batch    11 | loss: 204.7945687CurrentTrain: epoch  3, batch    12 | loss: 223.0597828CurrentTrain: epoch  3, batch    13 | loss: 169.2388380CurrentTrain: epoch  3, batch    14 | loss: 167.2522216CurrentTrain: epoch  3, batch    15 | loss: 238.3715731CurrentTrain: epoch  3, batch    16 | loss: 252.3053436CurrentTrain: epoch  3, batch    17 | loss: 175.7893773CurrentTrain: epoch  3, batch    18 | loss: 169.2304085CurrentTrain: epoch  3, batch    19 | loss: 211.7782880CurrentTrain: epoch  3, batch    20 | loss: 179.7770543CurrentTrain: epoch  3, batch    21 | loss: 279.2483739CurrentTrain: epoch  3, batch    22 | loss: 215.1295208CurrentTrain: epoch  3, batch    23 | loss: 223.3830914CurrentTrain: epoch  3, batch    24 | loss: 204.9972034CurrentTrain: epoch  3, batch    25 | loss: 212.5048571CurrentTrain: epoch  3, batch    26 | loss: 172.3547240CurrentTrain: epoch  3, batch    27 | loss: 361.3073663CurrentTrain: epoch  3, batch    28 | loss: 186.4394471CurrentTrain: epoch  3, batch    29 | loss: 205.1373159CurrentTrain: epoch  3, batch    30 | loss: 191.9984746CurrentTrain: epoch  3, batch    31 | loss: 162.2888789CurrentTrain: epoch  3, batch    32 | loss: 195.2786548CurrentTrain: epoch  3, batch    33 | loss: 276.8274651CurrentTrain: epoch  3, batch    34 | loss: 186.3778367CurrentTrain: epoch  3, batch    35 | loss: 176.2825325CurrentTrain: epoch  3, batch    36 | loss: 207.7860183CurrentTrain: epoch  3, batch    37 | loss: 202.7639447CurrentTrain: epoch  3, batch    38 | loss: 229.6579850CurrentTrain: epoch  3, batch    39 | loss: 187.9381789CurrentTrain: epoch  3, batch    40 | loss: 238.0698992CurrentTrain: epoch  3, batch    41 | loss: 186.8377051CurrentTrain: epoch  3, batch    42 | loss: 222.8168468CurrentTrain: epoch  3, batch    43 | loss: 185.1028789CurrentTrain: epoch  3, batch    44 | loss: 163.7393468CurrentTrain: epoch  3, batch    45 | loss: 217.8820383CurrentTrain: epoch  3, batch    46 | loss: 221.6073959CurrentTrain: epoch  3, batch    47 | loss: 175.6941326CurrentTrain: epoch  3, batch    48 | loss: 180.7474660CurrentTrain: epoch  3, batch    49 | loss: 223.7762800CurrentTrain: epoch  3, batch    50 | loss: 239.4743233CurrentTrain: epoch  3, batch    51 | loss: 211.3148232CurrentTrain: epoch  3, batch    52 | loss: 259.3066354CurrentTrain: epoch  3, batch    53 | loss: 186.5324559CurrentTrain: epoch  3, batch    54 | loss: 186.3934414CurrentTrain: epoch  3, batch    55 | loss: 204.4905303CurrentTrain: epoch  3, batch    56 | loss: 222.7443574CurrentTrain: epoch  3, batch    57 | loss: 207.1040644CurrentTrain: epoch  3, batch    58 | loss: 186.2466523CurrentTrain: epoch  3, batch    59 | loss: 270.5354731CurrentTrain: epoch  3, batch    60 | loss: 205.1258960CurrentTrain: epoch  3, batch    61 | loss: 238.2963519CurrentTrain: epoch  3, batch    62 | loss: 221.5129489CurrentTrain: epoch  3, batch    63 | loss: 231.5972186CurrentTrain: epoch  3, batch    64 | loss: 162.1453135CurrentTrain: epoch  3, batch    65 | loss: 163.0537857CurrentTrain: epoch  3, batch    66 | loss: 210.6291516CurrentTrain: epoch  3, batch    67 | loss: 181.2047670CurrentTrain: epoch  3, batch    68 | loss: 176.7248660CurrentTrain: epoch  3, batch    69 | loss: 252.2380054CurrentTrain: epoch  3, batch    70 | loss: 184.2278929CurrentTrain: epoch  3, batch    71 | loss: 203.7072273CurrentTrain: epoch  3, batch    72 | loss: 241.1611180CurrentTrain: epoch  3, batch    73 | loss: 154.1989718CurrentTrain: epoch  3, batch    74 | loss: 220.9298015CurrentTrain: epoch  3, batch    75 | loss: 170.3586725CurrentTrain: epoch  3, batch    76 | loss: 206.2031902CurrentTrain: epoch  3, batch    77 | loss: 176.5987101CurrentTrain: epoch  3, batch    78 | loss: 270.1482123CurrentTrain: epoch  3, batch    79 | loss: 211.8086921CurrentTrain: epoch  3, batch    80 | loss: 188.4072957CurrentTrain: epoch  3, batch    81 | loss: 195.5760014CurrentTrain: epoch  3, batch    82 | loss: 183.0959348CurrentTrain: epoch  3, batch    83 | loss: 287.0571247CurrentTrain: epoch  3, batch    84 | loss: 198.6290575CurrentTrain: epoch  3, batch    85 | loss: 219.7095014CurrentTrain: epoch  3, batch    86 | loss: 194.7514001CurrentTrain: epoch  3, batch    87 | loss: 182.2637039CurrentTrain: epoch  3, batch    88 | loss: 238.4753685CurrentTrain: epoch  3, batch    89 | loss: 211.3391808CurrentTrain: epoch  3, batch    90 | loss: 184.2838485CurrentTrain: epoch  3, batch    91 | loss: 240.5035239CurrentTrain: epoch  3, batch    92 | loss: 195.1324801CurrentTrain: epoch  3, batch    93 | loss: 194.4160112CurrentTrain: epoch  3, batch    94 | loss: 166.4162307CurrentTrain: epoch  3, batch    95 | loss: 174.1340977CurrentTrain: epoch  4, batch     0 | loss: 229.6339980CurrentTrain: epoch  4, batch     1 | loss: 194.9249333CurrentTrain: epoch  4, batch     2 | loss: 176.9319364CurrentTrain: epoch  4, batch     3 | loss: 184.9611941CurrentTrain: epoch  4, batch     4 | loss: 176.5682234CurrentTrain: epoch  4, batch     5 | loss: 276.6868822CurrentTrain: epoch  4, batch     6 | loss: 187.0248692CurrentTrain: epoch  4, batch     7 | loss: 247.2576252CurrentTrain: epoch  4, batch     8 | loss: 282.4095752CurrentTrain: epoch  4, batch     9 | loss: 184.2253046CurrentTrain: epoch  4, batch    10 | loss: 211.3870801CurrentTrain: epoch  4, batch    11 | loss: 204.2024501CurrentTrain: epoch  4, batch    12 | loss: 275.9793035CurrentTrain: epoch  4, batch    13 | loss: 360.9797438CurrentTrain: epoch  4, batch    14 | loss: 166.2819184CurrentTrain: epoch  4, batch    15 | loss: 191.7978358CurrentTrain: epoch  4, batch    16 | loss: 184.3104257CurrentTrain: epoch  4, batch    17 | loss: 168.5593311CurrentTrain: epoch  4, batch    18 | loss: 231.4696471CurrentTrain: epoch  4, batch    19 | loss: 237.7264251CurrentTrain: epoch  4, batch    20 | loss: 196.2468756CurrentTrain: epoch  4, batch    21 | loss: 204.1502018CurrentTrain: epoch  4, batch    22 | loss: 211.2109571CurrentTrain: epoch  4, batch    23 | loss: 184.6687240CurrentTrain: epoch  4, batch    24 | loss: 178.7752582CurrentTrain: epoch  4, batch    25 | loss: 205.2685191CurrentTrain: epoch  4, batch    26 | loss: 241.3726877CurrentTrain: epoch  4, batch    27 | loss: 203.8640324CurrentTrain: epoch  4, batch    28 | loss: 199.8339585CurrentTrain: epoch  4, batch    29 | loss: 169.8004928CurrentTrain: epoch  4, batch    30 | loss: 196.0415410CurrentTrain: epoch  4, batch    31 | loss: 212.4278727CurrentTrain: epoch  4, batch    32 | loss: 187.5016277CurrentTrain: epoch  4, batch    33 | loss: 360.0471850CurrentTrain: epoch  4, batch    34 | loss: 214.4885745CurrentTrain: epoch  4, batch    35 | loss: 195.5917992CurrentTrain: epoch  4, batch    36 | loss: 203.7306487CurrentTrain: epoch  4, batch    37 | loss: 222.1356395CurrentTrain: epoch  4, batch    38 | loss: 211.7524798CurrentTrain: epoch  4, batch    39 | loss: 228.1516201CurrentTrain: epoch  4, batch    40 | loss: 215.6249293CurrentTrain: epoch  4, batch    41 | loss: 193.2997124CurrentTrain: epoch  4, batch    42 | loss: 176.2060129CurrentTrain: epoch  4, batch    43 | loss: 214.3952868CurrentTrain: epoch  4, batch    44 | loss: 220.2655980CurrentTrain: epoch  4, batch    45 | loss: 220.3480890CurrentTrain: epoch  4, batch    46 | loss: 188.0237296CurrentTrain: epoch  4, batch    47 | loss: 163.8972432CurrentTrain: epoch  4, batch    48 | loss: 175.6145037CurrentTrain: epoch  4, batch    49 | loss: 192.9802708CurrentTrain: epoch  4, batch    50 | loss: 183.6388634CurrentTrain: epoch  4, batch    51 | loss: 212.2069127CurrentTrain: epoch  4, batch    52 | loss: 177.8314573CurrentTrain: epoch  4, batch    53 | loss: 147.8684154CurrentTrain: epoch  4, batch    54 | loss: 239.1390833CurrentTrain: epoch  4, batch    55 | loss: 192.3888108CurrentTrain: epoch  4, batch    56 | loss: 195.2372405CurrentTrain: epoch  4, batch    57 | loss: 197.7059359CurrentTrain: epoch  4, batch    58 | loss: 205.5096578CurrentTrain: epoch  4, batch    59 | loss: 221.8781532CurrentTrain: epoch  4, batch    60 | loss: 204.5692550CurrentTrain: epoch  4, batch    61 | loss: 221.2594762CurrentTrain: epoch  4, batch    62 | loss: 229.4303514CurrentTrain: epoch  4, batch    63 | loss: 282.4047005CurrentTrain: epoch  4, batch    64 | loss: 180.7609192CurrentTrain: epoch  4, batch    65 | loss: 177.1435173CurrentTrain: epoch  4, batch    66 | loss: 193.2617083CurrentTrain: epoch  4, batch    67 | loss: 212.7233903CurrentTrain: epoch  4, batch    68 | loss: 238.8492932CurrentTrain: epoch  4, batch    69 | loss: 230.5398595CurrentTrain: epoch  4, batch    70 | loss: 237.7560120CurrentTrain: epoch  4, batch    71 | loss: 177.2879779CurrentTrain: epoch  4, batch    72 | loss: 185.2332033CurrentTrain: epoch  4, batch    73 | loss: 164.3619694CurrentTrain: epoch  4, batch    74 | loss: 174.6500457CurrentTrain: epoch  4, batch    75 | loss: 222.7189997CurrentTrain: epoch  4, batch    76 | loss: 212.1658553CurrentTrain: epoch  4, batch    77 | loss: 279.1924127CurrentTrain: epoch  4, batch    78 | loss: 203.9708035CurrentTrain: epoch  4, batch    79 | loss: 212.8761045CurrentTrain: epoch  4, batch    80 | loss: 176.5089631CurrentTrain: epoch  4, batch    81 | loss: 188.2566889CurrentTrain: epoch  4, batch    82 | loss: 246.7911106CurrentTrain: epoch  4, batch    83 | loss: 229.1730513CurrentTrain: epoch  4, batch    84 | loss: 204.5083449CurrentTrain: epoch  4, batch    85 | loss: 170.0547827CurrentTrain: epoch  4, batch    86 | loss: 229.5817730CurrentTrain: epoch  4, batch    87 | loss: 204.7898151CurrentTrain: epoch  4, batch    88 | loss: 219.6680189CurrentTrain: epoch  4, batch    89 | loss: 151.1366855CurrentTrain: epoch  4, batch    90 | loss: 179.7545710CurrentTrain: epoch  4, batch    91 | loss: 194.6644840CurrentTrain: epoch  4, batch    92 | loss: 237.8336979CurrentTrain: epoch  4, batch    93 | loss: 237.5868678CurrentTrain: epoch  4, batch    94 | loss: 177.5659096CurrentTrain: epoch  4, batch    95 | loss: 158.7764233CurrentTrain: epoch  5, batch     0 | loss: 200.3029942CurrentTrain: epoch  5, batch     1 | loss: 176.2102980CurrentTrain: epoch  5, batch     2 | loss: 211.4121541CurrentTrain: epoch  5, batch     3 | loss: 195.4443096CurrentTrain: epoch  5, batch     4 | loss: 168.6078032CurrentTrain: epoch  5, batch     5 | loss: 204.7206098CurrentTrain: epoch  5, batch     6 | loss: 230.3954355CurrentTrain: epoch  5, batch     7 | loss: 221.9702824CurrentTrain: epoch  5, batch     8 | loss: 192.0468034CurrentTrain: epoch  5, batch     9 | loss: 179.8908719CurrentTrain: epoch  5, batch    10 | loss: 160.5727219CurrentTrain: epoch  5, batch    11 | loss: 227.8656754CurrentTrain: epoch  5, batch    12 | loss: 192.8484856CurrentTrain: epoch  5, batch    13 | loss: 203.1986116CurrentTrain: epoch  5, batch    14 | loss: 174.7237671CurrentTrain: epoch  5, batch    15 | loss: 246.4399858CurrentTrain: epoch  5, batch    16 | loss: 205.7529668CurrentTrain: epoch  5, batch    17 | loss: 156.9558260CurrentTrain: epoch  5, batch    18 | loss: 229.0752168CurrentTrain: epoch  5, batch    19 | loss: 184.0791718CurrentTrain: epoch  5, batch    20 | loss: 185.3401896CurrentTrain: epoch  5, batch    21 | loss: 211.5548233CurrentTrain: epoch  5, batch    22 | loss: 267.8319153CurrentTrain: epoch  5, batch    23 | loss: 239.2511646CurrentTrain: epoch  5, batch    24 | loss: 237.5019963CurrentTrain: epoch  5, batch    25 | loss: 202.3087959CurrentTrain: epoch  5, batch    26 | loss: 157.1445837CurrentTrain: epoch  5, batch    27 | loss: 266.3068330CurrentTrain: epoch  5, batch    28 | loss: 237.4275293CurrentTrain: epoch  5, batch    29 | loss: 176.9436357CurrentTrain: epoch  5, batch    30 | loss: 202.9887838CurrentTrain: epoch  5, batch    31 | loss: 211.8318549CurrentTrain: epoch  5, batch    32 | loss: 191.5452295CurrentTrain: epoch  5, batch    33 | loss: 212.1802220CurrentTrain: epoch  5, batch    34 | loss: 237.6515953CurrentTrain: epoch  5, batch    35 | loss: 160.4092659CurrentTrain: epoch  5, batch    36 | loss: 277.9403516CurrentTrain: epoch  5, batch    37 | loss: 276.3371482CurrentTrain: epoch  5, batch    38 | loss: 287.5753846CurrentTrain: epoch  5, batch    39 | loss: 189.2170065CurrentTrain: epoch  5, batch    40 | loss: 182.9117373CurrentTrain: epoch  5, batch    41 | loss: 228.2656256CurrentTrain: epoch  5, batch    42 | loss: 176.6793058CurrentTrain: epoch  5, batch    43 | loss: 195.7735375CurrentTrain: epoch  5, batch    44 | loss: 202.0710259CurrentTrain: epoch  5, batch    45 | loss: 211.5398080CurrentTrain: epoch  5, batch    46 | loss: 167.2531947CurrentTrain: epoch  5, batch    47 | loss: 238.1178102CurrentTrain: epoch  5, batch    48 | loss: 188.3444458CurrentTrain: epoch  5, batch    49 | loss: 177.0059715CurrentTrain: epoch  5, batch    50 | loss: 212.4305133CurrentTrain: epoch  5, batch    51 | loss: 346.9300833CurrentTrain: epoch  5, batch    52 | loss: 186.1138247CurrentTrain: epoch  5, batch    53 | loss: 196.0187588CurrentTrain: epoch  5, batch    54 | loss: 195.9737464CurrentTrain: epoch  5, batch    55 | loss: 228.2655444CurrentTrain: epoch  5, batch    56 | loss: 175.6351301CurrentTrain: epoch  5, batch    57 | loss: 167.8128446CurrentTrain: epoch  5, batch    58 | loss: 359.4733185CurrentTrain: epoch  5, batch    59 | loss: 168.3957088CurrentTrain: epoch  5, batch    60 | loss: 194.3966610CurrentTrain: epoch  5, batch    61 | loss: 172.3870979CurrentTrain: epoch  5, batch    62 | loss: 176.0671238CurrentTrain: epoch  5, batch    63 | loss: 168.2052642CurrentTrain: epoch  5, batch    64 | loss: 152.9351620CurrentTrain: epoch  5, batch    65 | loss: 277.1885683CurrentTrain: epoch  5, batch    66 | loss: 220.2703418CurrentTrain: epoch  5, batch    67 | loss: 184.2783835CurrentTrain: epoch  5, batch    68 | loss: 180.1057311CurrentTrain: epoch  5, batch    69 | loss: 558.5767291CurrentTrain: epoch  5, batch    70 | loss: 246.7791766CurrentTrain: epoch  5, batch    71 | loss: 226.1275171CurrentTrain: epoch  5, batch    72 | loss: 167.1660010CurrentTrain: epoch  5, batch    73 | loss: 220.1280951CurrentTrain: epoch  5, batch    74 | loss: 229.1835153CurrentTrain: epoch  5, batch    75 | loss: 194.5238172CurrentTrain: epoch  5, batch    76 | loss: 175.5112787CurrentTrain: epoch  5, batch    77 | loss: 168.3185423CurrentTrain: epoch  5, batch    78 | loss: 199.8714610CurrentTrain: epoch  5, batch    79 | loss: 286.4212880CurrentTrain: epoch  5, batch    80 | loss: 175.7659548CurrentTrain: epoch  5, batch    81 | loss: 192.0221151CurrentTrain: epoch  5, batch    82 | loss: 159.9457548CurrentTrain: epoch  5, batch    83 | loss: 238.5572453CurrentTrain: epoch  5, batch    84 | loss: 183.2433666CurrentTrain: epoch  5, batch    85 | loss: 177.1203972CurrentTrain: epoch  5, batch    86 | loss: 247.7058200CurrentTrain: epoch  5, batch    87 | loss: 256.5820018CurrentTrain: epoch  5, batch    88 | loss: 184.1412143CurrentTrain: epoch  5, batch    89 | loss: 219.2218360CurrentTrain: epoch  5, batch    90 | loss: 155.2921306CurrentTrain: epoch  5, batch    91 | loss: 228.2698694CurrentTrain: epoch  5, batch    92 | loss: 186.8829967CurrentTrain: epoch  5, batch    93 | loss: 167.5194399CurrentTrain: epoch  5, batch    94 | loss: 203.1960665CurrentTrain: epoch  5, batch    95 | loss: 175.2839202CurrentTrain: epoch  6, batch     0 | loss: 228.0811523CurrentTrain: epoch  6, batch     1 | loss: 187.4773618CurrentTrain: epoch  6, batch     2 | loss: 237.6092034CurrentTrain: epoch  6, batch     3 | loss: 190.7258733CurrentTrain: epoch  6, batch     4 | loss: 211.9226795CurrentTrain: epoch  6, batch     5 | loss: 161.4860100CurrentTrain: epoch  6, batch     6 | loss: 238.3983254CurrentTrain: epoch  6, batch     7 | loss: 220.1642011CurrentTrain: epoch  6, batch     8 | loss: 175.1381093CurrentTrain: epoch  6, batch     9 | loss: 220.3960427CurrentTrain: epoch  6, batch    10 | loss: 186.8958206CurrentTrain: epoch  6, batch    11 | loss: 202.1105395CurrentTrain: epoch  6, batch    12 | loss: 286.4510923CurrentTrain: epoch  6, batch    13 | loss: 288.4761456CurrentTrain: epoch  6, batch    14 | loss: 195.1897271CurrentTrain: epoch  6, batch    15 | loss: 191.3664514CurrentTrain: epoch  6, batch    16 | loss: 239.7878321CurrentTrain: epoch  6, batch    17 | loss: 174.8416653CurrentTrain: epoch  6, batch    18 | loss: 221.7457089CurrentTrain: epoch  6, batch    19 | loss: 171.3116935CurrentTrain: epoch  6, batch    20 | loss: 231.3935082CurrentTrain: epoch  6, batch    21 | loss: 153.7529247CurrentTrain: epoch  6, batch    22 | loss: 188.5891576CurrentTrain: epoch  6, batch    23 | loss: 228.2961010CurrentTrain: epoch  6, batch    24 | loss: 190.7927258CurrentTrain: epoch  6, batch    25 | loss: 203.8342123CurrentTrain: epoch  6, batch    26 | loss: 177.7106545CurrentTrain: epoch  6, batch    27 | loss: 190.7868302CurrentTrain: epoch  6, batch    28 | loss: 202.3040589CurrentTrain: epoch  6, batch    29 | loss: 336.4339554CurrentTrain: epoch  6, batch    30 | loss: 210.8643186CurrentTrain: epoch  6, batch    31 | loss: 147.1188972CurrentTrain: epoch  6, batch    32 | loss: 175.1931597CurrentTrain: epoch  6, batch    33 | loss: 152.4737045CurrentTrain: epoch  6, batch    34 | loss: 247.6402833CurrentTrain: epoch  6, batch    35 | loss: 236.9068290CurrentTrain: epoch  6, batch    36 | loss: 227.6782045CurrentTrain: epoch  6, batch    37 | loss: 190.7659586CurrentTrain: epoch  6, batch    38 | loss: 193.7414501CurrentTrain: epoch  6, batch    39 | loss: 286.3741584CurrentTrain: epoch  6, batch    40 | loss: 162.6289263CurrentTrain: epoch  6, batch    41 | loss: 201.7037783CurrentTrain: epoch  6, batch    42 | loss: 195.3666898CurrentTrain: epoch  6, batch    43 | loss: 168.7589158CurrentTrain: epoch  6, batch    44 | loss: 159.4881679CurrentTrain: epoch  6, batch    45 | loss: 203.3150750CurrentTrain: epoch  6, batch    46 | loss: 210.4434801CurrentTrain: epoch  6, batch    47 | loss: 256.6412318CurrentTrain: epoch  6, batch    48 | loss: 160.4151619CurrentTrain: epoch  6, batch    49 | loss: 219.6253086CurrentTrain: epoch  6, batch    50 | loss: 359.6314212CurrentTrain: epoch  6, batch    51 | loss: 199.4109016CurrentTrain: epoch  6, batch    52 | loss: 222.8092833CurrentTrain: epoch  6, batch    53 | loss: 159.3996321CurrentTrain: epoch  6, batch    54 | loss: 170.4059920CurrentTrain: epoch  6, batch    55 | loss: 175.0059112CurrentTrain: epoch  6, batch    56 | loss: 145.9572751CurrentTrain: epoch  6, batch    57 | loss: 246.7354950CurrentTrain: epoch  6, batch    58 | loss: 190.7820736CurrentTrain: epoch  6, batch    59 | loss: 186.3572768CurrentTrain: epoch  6, batch    60 | loss: 359.0997878CurrentTrain: epoch  6, batch    61 | loss: 210.9015759CurrentTrain: epoch  6, batch    62 | loss: 247.0941741CurrentTrain: epoch  6, batch    63 | loss: 139.6548615CurrentTrain: epoch  6, batch    64 | loss: 208.4661065CurrentTrain: epoch  6, batch    65 | loss: 197.3992029CurrentTrain: epoch  6, batch    66 | loss: 211.1044902CurrentTrain: epoch  6, batch    67 | loss: 194.6129136CurrentTrain: epoch  6, batch    68 | loss: 175.5913004CurrentTrain: epoch  6, batch    69 | loss: 213.6368004CurrentTrain: epoch  6, batch    70 | loss: 160.0474237CurrentTrain: epoch  6, batch    71 | loss: 222.3059429CurrentTrain: epoch  6, batch    72 | loss: 168.6270048CurrentTrain: epoch  6, batch    73 | loss: 228.0320611CurrentTrain: epoch  6, batch    74 | loss: 278.0411571CurrentTrain: epoch  6, batch    75 | loss: 249.0261733CurrentTrain: epoch  6, batch    76 | loss: 182.3853167CurrentTrain: epoch  6, batch    77 | loss: 160.9574091CurrentTrain: epoch  6, batch    78 | loss: 196.7795838CurrentTrain: epoch  6, batch    79 | loss: 176.7739983CurrentTrain: epoch  6, batch    80 | loss: 358.9797165CurrentTrain: epoch  6, batch    81 | loss: 220.9535807CurrentTrain: epoch  6, batch    82 | loss: 237.4675741CurrentTrain: epoch  6, batch    83 | loss: 200.1039226CurrentTrain: epoch  6, batch    84 | loss: 202.4634323CurrentTrain: epoch  6, batch    85 | loss: 224.1547872CurrentTrain: epoch  6, batch    86 | loss: 286.8484400CurrentTrain: epoch  6, batch    87 | loss: 183.1761331CurrentTrain: epoch  6, batch    88 | loss: 202.6506894CurrentTrain: epoch  6, batch    89 | loss: 168.7217044CurrentTrain: epoch  6, batch    90 | loss: 267.1430893CurrentTrain: epoch  6, batch    91 | loss: 203.4747380CurrentTrain: epoch  6, batch    92 | loss: 202.4927864CurrentTrain: epoch  6, batch    93 | loss: 228.0136489CurrentTrain: epoch  6, batch    94 | loss: 185.7805545CurrentTrain: epoch  6, batch    95 | loss: 172.2131068CurrentTrain: epoch  7, batch     0 | loss: 175.3880051CurrentTrain: epoch  7, batch     1 | loss: 175.7344896CurrentTrain: epoch  7, batch     2 | loss: 202.8450351CurrentTrain: epoch  7, batch     3 | loss: 183.2024987CurrentTrain: epoch  7, batch     4 | loss: 212.9738342CurrentTrain: epoch  7, batch     5 | loss: 159.5682954CurrentTrain: epoch  7, batch     6 | loss: 211.3074995CurrentTrain: epoch  7, batch     7 | loss: 168.6442285CurrentTrain: epoch  7, batch     8 | loss: 153.1136258CurrentTrain: epoch  7, batch     9 | loss: 288.2970879CurrentTrain: epoch  7, batch    10 | loss: 175.1003291CurrentTrain: epoch  7, batch    11 | loss: 201.8953557CurrentTrain: epoch  7, batch    12 | loss: 247.3872891CurrentTrain: epoch  7, batch    13 | loss: 278.1371617CurrentTrain: epoch  7, batch    14 | loss: 183.2201265CurrentTrain: epoch  7, batch    15 | loss: 174.3837716CurrentTrain: epoch  7, batch    16 | loss: 185.7739333CurrentTrain: epoch  7, batch    17 | loss: 193.7551235CurrentTrain: epoch  7, batch    18 | loss: 228.8190166CurrentTrain: epoch  7, batch    19 | loss: 265.7211762CurrentTrain: epoch  7, batch    20 | loss: 194.4752722CurrentTrain: epoch  7, batch    21 | loss: 246.7706067CurrentTrain: epoch  7, batch    22 | loss: 191.4182232CurrentTrain: epoch  7, batch    23 | loss: 175.3619853CurrentTrain: epoch  7, batch    24 | loss: 195.0908897CurrentTrain: epoch  7, batch    25 | loss: 167.4088941CurrentTrain: epoch  7, batch    26 | loss: 346.4656661CurrentTrain: epoch  7, batch    27 | loss: 153.0794417CurrentTrain: epoch  7, batch    28 | loss: 228.7066343CurrentTrain: epoch  7, batch    29 | loss: 230.6629892CurrentTrain: epoch  7, batch    30 | loss: 228.1309016CurrentTrain: epoch  7, batch    31 | loss: 210.5606704CurrentTrain: epoch  7, batch    32 | loss: 183.3060138CurrentTrain: epoch  7, batch    33 | loss: 170.8324171CurrentTrain: epoch  7, batch    34 | loss: 201.8419161CurrentTrain: epoch  7, batch    35 | loss: 267.4482540CurrentTrain: epoch  7, batch    36 | loss: 174.9452692CurrentTrain: epoch  7, batch    37 | loss: 184.2174581CurrentTrain: epoch  7, batch    38 | loss: 194.7972093CurrentTrain: epoch  7, batch    39 | loss: 227.8962283CurrentTrain: epoch  7, batch    40 | loss: 184.3629255CurrentTrain: epoch  7, batch    41 | loss: 287.0386811CurrentTrain: epoch  7, batch    42 | loss: 211.5630005CurrentTrain: epoch  7, batch    43 | loss: 229.9861955CurrentTrain: epoch  7, batch    44 | loss: 183.0775613CurrentTrain: epoch  7, batch    45 | loss: 187.5710468CurrentTrain: epoch  7, batch    46 | loss: 210.9184750CurrentTrain: epoch  7, batch    47 | loss: 287.9150587CurrentTrain: epoch  7, batch    48 | loss: 137.8723863CurrentTrain: epoch  7, batch    49 | loss: 203.7093623CurrentTrain: epoch  7, batch    50 | loss: 247.1249040CurrentTrain: epoch  7, batch    51 | loss: 220.1156320CurrentTrain: epoch  7, batch    52 | loss: 210.5393408CurrentTrain: epoch  7, batch    53 | loss: 191.6390788CurrentTrain: epoch  7, batch    54 | loss: 266.0981488CurrentTrain: epoch  7, batch    55 | loss: 191.1579144CurrentTrain: epoch  7, batch    56 | loss: 220.4716738CurrentTrain: epoch  7, batch    57 | loss: 178.0871515CurrentTrain: epoch  7, batch    58 | loss: 190.9344782CurrentTrain: epoch  7, batch    59 | loss: 174.1843882CurrentTrain: epoch  7, batch    60 | loss: 227.6579042CurrentTrain: epoch  7, batch    61 | loss: 246.4741712CurrentTrain: epoch  7, batch    62 | loss: 215.3277292CurrentTrain: epoch  7, batch    63 | loss: 195.5757896CurrentTrain: epoch  7, batch    64 | loss: 286.4196341CurrentTrain: epoch  7, batch    65 | loss: 178.4169293CurrentTrain: epoch  7, batch    66 | loss: 246.7926715CurrentTrain: epoch  7, batch    67 | loss: 359.3560789CurrentTrain: epoch  7, batch    68 | loss: 153.7891235CurrentTrain: epoch  7, batch    69 | loss: 174.3448122CurrentTrain: epoch  7, batch    70 | loss: 194.0607173CurrentTrain: epoch  7, batch    71 | loss: 228.9859615CurrentTrain: epoch  7, batch    72 | loss: 167.1593484CurrentTrain: epoch  7, batch    73 | loss: 160.8760713CurrentTrain: epoch  7, batch    74 | loss: 212.5801974CurrentTrain: epoch  7, batch    75 | loss: 248.4238415CurrentTrain: epoch  7, batch    76 | loss: 167.0393150CurrentTrain: epoch  7, batch    77 | loss: 179.5430747CurrentTrain: epoch  7, batch    78 | loss: 195.0460195CurrentTrain: epoch  7, batch    79 | loss: 219.3778369CurrentTrain: epoch  7, batch    80 | loss: 185.7942809CurrentTrain: epoch  7, batch    81 | loss: 202.0908620CurrentTrain: epoch  7, batch    82 | loss: 211.9119522CurrentTrain: epoch  7, batch    83 | loss: 220.2844506CurrentTrain: epoch  7, batch    84 | loss: 228.2945903CurrentTrain: epoch  7, batch    85 | loss: 167.7472949CurrentTrain: epoch  7, batch    86 | loss: 227.9324028CurrentTrain: epoch  7, batch    87 | loss: 160.9353853CurrentTrain: epoch  7, batch    88 | loss: 201.9246447CurrentTrain: epoch  7, batch    89 | loss: 223.6499916CurrentTrain: epoch  7, batch    90 | loss: 202.7077246CurrentTrain: epoch  7, batch    91 | loss: 182.7311393CurrentTrain: epoch  7, batch    92 | loss: 211.4332007CurrentTrain: epoch  7, batch    93 | loss: 199.1964432CurrentTrain: epoch  7, batch    94 | loss: 251.6031075CurrentTrain: epoch  7, batch    95 | loss: 141.5689757CurrentTrain: epoch  8, batch     0 | loss: 168.1378422CurrentTrain: epoch  8, batch     1 | loss: 220.2410861CurrentTrain: epoch  8, batch     2 | loss: 161.1254187CurrentTrain: epoch  8, batch     3 | loss: 182.2392400CurrentTrain: epoch  8, batch     4 | loss: 186.0067958CurrentTrain: epoch  8, batch     5 | loss: 152.1743539CurrentTrain: epoch  8, batch     6 | loss: 177.7369504CurrentTrain: epoch  8, batch     7 | loss: 212.1727779CurrentTrain: epoch  8, batch     8 | loss: 346.4581964CurrentTrain: epoch  8, batch     9 | loss: 237.3331545CurrentTrain: epoch  8, batch    10 | loss: 188.2634137CurrentTrain: epoch  8, batch    11 | loss: 194.4347316CurrentTrain: epoch  8, batch    12 | loss: 275.9112203CurrentTrain: epoch  8, batch    13 | loss: 227.8096914CurrentTrain: epoch  8, batch    14 | loss: 168.8180175CurrentTrain: epoch  8, batch    15 | loss: 182.8070207CurrentTrain: epoch  8, batch    16 | loss: 203.0479538CurrentTrain: epoch  8, batch    17 | loss: 346.4784384CurrentTrain: epoch  8, batch    18 | loss: 168.4823887CurrentTrain: epoch  8, batch    19 | loss: 238.4827260CurrentTrain: epoch  8, batch    20 | loss: 167.7474697CurrentTrain: epoch  8, batch    21 | loss: 174.6925596CurrentTrain: epoch  8, batch    22 | loss: 286.8619970CurrentTrain: epoch  8, batch    23 | loss: 194.4271930CurrentTrain: epoch  8, batch    24 | loss: 219.1047002CurrentTrain: epoch  8, batch    25 | loss: 182.5598098CurrentTrain: epoch  8, batch    26 | loss: 187.9092942CurrentTrain: epoch  8, batch    27 | loss: 161.5488186CurrentTrain: epoch  8, batch    28 | loss: 174.6861255CurrentTrain: epoch  8, batch    29 | loss: 182.8367463CurrentTrain: epoch  8, batch    30 | loss: 227.9750560CurrentTrain: epoch  8, batch    31 | loss: 153.3274381CurrentTrain: epoch  8, batch    32 | loss: 191.4554756CurrentTrain: epoch  8, batch    33 | loss: 219.8750749CurrentTrain: epoch  8, batch    34 | loss: 193.7256172CurrentTrain: epoch  8, batch    35 | loss: 220.5938128CurrentTrain: epoch  8, batch    36 | loss: 202.7879508CurrentTrain: epoch  8, batch    37 | loss: 195.3403784CurrentTrain: epoch  8, batch    38 | loss: 182.8596280CurrentTrain: epoch  8, batch    39 | loss: 182.2916432CurrentTrain: epoch  8, batch    40 | loss: 201.7296761CurrentTrain: epoch  8, batch    41 | loss: 219.8961978CurrentTrain: epoch  8, batch    42 | loss: 174.1995751CurrentTrain: epoch  8, batch    43 | loss: 175.9853294CurrentTrain: epoch  8, batch    44 | loss: 190.2490900CurrentTrain: epoch  8, batch    45 | loss: 194.6378938CurrentTrain: epoch  8, batch    46 | loss: 202.3424385CurrentTrain: epoch  8, batch    47 | loss: 199.3514951CurrentTrain: epoch  8, batch    48 | loss: 228.6810514CurrentTrain: epoch  8, batch    49 | loss: 238.2540636CurrentTrain: epoch  8, batch    50 | loss: 201.9924114CurrentTrain: epoch  8, batch    51 | loss: 152.6782750CurrentTrain: epoch  8, batch    52 | loss: 286.3066209CurrentTrain: epoch  8, batch    53 | loss: 202.5239701CurrentTrain: epoch  8, batch    54 | loss: 203.4430228CurrentTrain: epoch  8, batch    55 | loss: 210.7587420CurrentTrain: epoch  8, batch    56 | loss: 211.0022256CurrentTrain: epoch  8, batch    57 | loss: 178.4752989CurrentTrain: epoch  8, batch    58 | loss: 203.7797517CurrentTrain: epoch  8, batch    59 | loss: 211.7373469CurrentTrain: epoch  8, batch    60 | loss: 237.6271796CurrentTrain: epoch  8, batch    61 | loss: 159.0236577CurrentTrain: epoch  8, batch    62 | loss: 186.5770450CurrentTrain: epoch  8, batch    63 | loss: 276.6374995CurrentTrain: epoch  8, batch    64 | loss: 228.3306187CurrentTrain: epoch  8, batch    65 | loss: 192.9113049CurrentTrain: epoch  8, batch    66 | loss: 236.9214134CurrentTrain: epoch  8, batch    67 | loss: 276.7104857CurrentTrain: epoch  8, batch    68 | loss: 195.5507810CurrentTrain: epoch  8, batch    69 | loss: 193.9558610CurrentTrain: epoch  8, batch    70 | loss: 246.4823464CurrentTrain: epoch  8, batch    71 | loss: 257.3239995CurrentTrain: epoch  8, batch    72 | loss: 194.2925104CurrentTrain: epoch  8, batch    73 | loss: 211.2767428CurrentTrain: epoch  8, batch    74 | loss: 191.3795908CurrentTrain: epoch  8, batch    75 | loss: 194.1577640CurrentTrain: epoch  8, batch    76 | loss: 193.8750471CurrentTrain: epoch  8, batch    77 | loss: 202.3930100CurrentTrain: epoch  8, batch    78 | loss: 192.4031845CurrentTrain: epoch  8, batch    79 | loss: 178.9403412CurrentTrain: epoch  8, batch    80 | loss: 174.7253085CurrentTrain: epoch  8, batch    81 | loss: 249.7343098CurrentTrain: epoch  8, batch    82 | loss: 145.6081317CurrentTrain: epoch  8, batch    83 | loss: 182.5517325CurrentTrain: epoch  8, batch    84 | loss: 167.1624994CurrentTrain: epoch  8, batch    85 | loss: 246.5256199CurrentTrain: epoch  8, batch    86 | loss: 175.2031837CurrentTrain: epoch  8, batch    87 | loss: 210.4585783CurrentTrain: epoch  8, batch    88 | loss: 202.3344756CurrentTrain: epoch  8, batch    89 | loss: 210.3272028CurrentTrain: epoch  8, batch    90 | loss: 219.4878468CurrentTrain: epoch  8, batch    91 | loss: 249.9517564CurrentTrain: epoch  8, batch    92 | loss: 193.5141050CurrentTrain: epoch  8, batch    93 | loss: 210.3528934CurrentTrain: epoch  8, batch    94 | loss: 238.8748662CurrentTrain: epoch  8, batch    95 | loss: 289.6087318CurrentTrain: epoch  9, batch     0 | loss: 205.2174161CurrentTrain: epoch  9, batch     1 | loss: 186.2461525CurrentTrain: epoch  9, batch     2 | loss: 177.7475730CurrentTrain: epoch  9, batch     3 | loss: 184.0507704CurrentTrain: epoch  9, batch     4 | loss: 193.8252180CurrentTrain: epoch  9, batch     5 | loss: 178.2443065CurrentTrain: epoch  9, batch     6 | loss: 182.7644734CurrentTrain: epoch  9, batch     7 | loss: 194.0682887CurrentTrain: epoch  9, batch     8 | loss: 174.9677978CurrentTrain: epoch  9, batch     9 | loss: 174.9669994CurrentTrain: epoch  9, batch    10 | loss: 258.4210806CurrentTrain: epoch  9, batch    11 | loss: 161.4250561CurrentTrain: epoch  9, batch    12 | loss: 218.6976742CurrentTrain: epoch  9, batch    13 | loss: 185.3305151CurrentTrain: epoch  9, batch    14 | loss: 219.5165825CurrentTrain: epoch  9, batch    15 | loss: 210.4896445CurrentTrain: epoch  9, batch    16 | loss: 190.8569212CurrentTrain: epoch  9, batch    17 | loss: 199.2059770CurrentTrain: epoch  9, batch    18 | loss: 203.0794430CurrentTrain: epoch  9, batch    19 | loss: 166.9217762CurrentTrain: epoch  9, batch    20 | loss: 155.1449940CurrentTrain: epoch  9, batch    21 | loss: 194.0445389CurrentTrain: epoch  9, batch    22 | loss: 227.7757213CurrentTrain: epoch  9, batch    23 | loss: 219.5413564CurrentTrain: epoch  9, batch    24 | loss: 193.6891056CurrentTrain: epoch  9, batch    25 | loss: 185.7877771CurrentTrain: epoch  9, batch    26 | loss: 194.9221828CurrentTrain: epoch  9, batch    27 | loss: 219.4767139CurrentTrain: epoch  9, batch    28 | loss: 199.6557502CurrentTrain: epoch  9, batch    29 | loss: 218.8990227CurrentTrain: epoch  9, batch    30 | loss: 286.3046474CurrentTrain: epoch  9, batch    31 | loss: 227.8267238CurrentTrain: epoch  9, batch    32 | loss: 246.4003263CurrentTrain: epoch  9, batch    33 | loss: 225.3064416CurrentTrain: epoch  9, batch    34 | loss: 201.7927452CurrentTrain: epoch  9, batch    35 | loss: 166.6876250CurrentTrain: epoch  9, batch    36 | loss: 182.6614453CurrentTrain: epoch  9, batch    37 | loss: 190.7079401CurrentTrain: epoch  9, batch    38 | loss: 185.8299757CurrentTrain: epoch  9, batch    39 | loss: 174.2894982CurrentTrain: epoch  9, batch    40 | loss: 277.0556636CurrentTrain: epoch  9, batch    41 | loss: 178.3615242CurrentTrain: epoch  9, batch    42 | loss: 192.6648374CurrentTrain: epoch  9, batch    43 | loss: 276.1564933CurrentTrain: epoch  9, batch    44 | loss: 201.7560010CurrentTrain: epoch  9, batch    45 | loss: 276.0298268CurrentTrain: epoch  9, batch    46 | loss: 211.0272962CurrentTrain: epoch  9, batch    47 | loss: 228.7022042CurrentTrain: epoch  9, batch    48 | loss: 182.6648096CurrentTrain: epoch  9, batch    49 | loss: 210.8338166CurrentTrain: epoch  9, batch    50 | loss: 260.2654580CurrentTrain: epoch  9, batch    51 | loss: 167.2366616CurrentTrain: epoch  9, batch    52 | loss: 228.7698282CurrentTrain: epoch  9, batch    53 | loss: 193.7283167CurrentTrain: epoch  9, batch    54 | loss: 145.4258943CurrentTrain: epoch  9, batch    55 | loss: 185.4418264CurrentTrain: epoch  9, batch    56 | loss: 152.6194891CurrentTrain: epoch  9, batch    57 | loss: 286.3547919CurrentTrain: epoch  9, batch    58 | loss: 202.2700330CurrentTrain: epoch  9, batch    59 | loss: 228.1776459CurrentTrain: epoch  9, batch    60 | loss: 193.4770003CurrentTrain: epoch  9, batch    61 | loss: 277.5973623CurrentTrain: epoch  9, batch    62 | loss: 246.4144389CurrentTrain: epoch  9, batch    63 | loss: 219.1500375CurrentTrain: epoch  9, batch    64 | loss: 152.9245741CurrentTrain: epoch  9, batch    65 | loss: 359.6998613CurrentTrain: epoch  9, batch    66 | loss: 191.6815970CurrentTrain: epoch  9, batch    67 | loss: 175.0873721CurrentTrain: epoch  9, batch    68 | loss: 147.0760700CurrentTrain: epoch  9, batch    69 | loss: 227.6296932CurrentTrain: epoch  9, batch    70 | loss: 227.6902173CurrentTrain: epoch  9, batch    71 | loss: 201.6468940CurrentTrain: epoch  9, batch    72 | loss: 182.3674644CurrentTrain: epoch  9, batch    73 | loss: 174.6809999CurrentTrain: epoch  9, batch    74 | loss: 219.6927338CurrentTrain: epoch  9, batch    75 | loss: 220.3021159CurrentTrain: epoch  9, batch    76 | loss: 236.7591387CurrentTrain: epoch  9, batch    77 | loss: 286.5539593CurrentTrain: epoch  9, batch    78 | loss: 192.6631407CurrentTrain: epoch  9, batch    79 | loss: 193.5594307CurrentTrain: epoch  9, batch    80 | loss: 185.2404783CurrentTrain: epoch  9, batch    81 | loss: 177.8120500CurrentTrain: epoch  9, batch    82 | loss: 202.0692278CurrentTrain: epoch  9, batch    83 | loss: 202.7568414CurrentTrain: epoch  9, batch    84 | loss: 237.3145378CurrentTrain: epoch  9, batch    85 | loss: 275.9774115CurrentTrain: epoch  9, batch    86 | loss: 193.1141305CurrentTrain: epoch  9, batch    87 | loss: 286.4761895CurrentTrain: epoch  9, batch    88 | loss: 359.0805149CurrentTrain: epoch  9, batch    89 | loss: 209.9013313CurrentTrain: epoch  9, batch    90 | loss: 152.8783350CurrentTrain: epoch  9, batch    91 | loss: 174.4729634CurrentTrain: epoch  9, batch    92 | loss: 227.5908876CurrentTrain: epoch  9, batch    93 | loss: 193.8254259CurrentTrain: epoch  9, batch    94 | loss: 194.0831132CurrentTrain: epoch  9, batch    95 | loss: 202.2973852

F1 score per class: {32: 0.5228758169934641, 6: 0.8636363636363636, 19: 0.4, 24: 0.7608695652173914, 26: 0.8950276243093923, 29: 0.8911917098445595}
Micro-average F1 score: 0.7850877192982456
Weighted-average F1 score: 0.8006088629438654
F1 score per class: {32: 0.6740331491712708, 6: 0.9081081081081082, 19: 0.3448275862068966, 24: 0.7777777777777778, 26: 0.9583333333333334, 29: 0.8730964467005076}
Micro-average F1 score: 0.8257261410788381
Weighted-average F1 score: 0.8310722465003949
F1 score per class: {32: 0.6629213483146067, 6: 0.9081081081081082, 19: 0.3448275862068966, 24: 0.7734806629834254, 26: 0.9583333333333334, 29: 0.8730964467005076}
Micro-average F1 score: 0.8232848232848233
Weighted-average F1 score: 0.8292772826672923

F1 score per class: {32: 0.5228758169934641, 6: 0.8636363636363636, 19: 0.4, 24: 0.7608695652173914, 26: 0.8950276243093923, 29: 0.8911917098445595}
Micro-average F1 score: 0.7850877192982456
Weighted-average F1 score: 0.8006088629438654
F1 score per class: {32: 0.6740331491712708, 6: 0.9081081081081082, 19: 0.3448275862068966, 24: 0.7777777777777778, 26: 0.9583333333333334, 29: 0.8730964467005076}
Micro-average F1 score: 0.8257261410788381
Weighted-average F1 score: 0.8310722465003949
F1 score per class: {32: 0.6629213483146067, 6: 0.9081081081081082, 19: 0.3448275862068966, 24: 0.7734806629834254, 26: 0.9583333333333334, 29: 0.8730964467005076}
Micro-average F1 score: 0.8232848232848233
Weighted-average F1 score: 0.8292772826672923
cur_acc:  ['0.7851']
his_acc:  ['0.7851']
cur_acc des:  ['0.8257']
his_acc des:  ['0.8257']
cur_acc rrf:  ['0.8233']
his_acc rrf:  ['0.8233']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 243.6454745CurrentTrain: epoch  0, batch     1 | loss: 191.5596795CurrentTrain: epoch  0, batch     2 | loss: 221.4031550CurrentTrain: epoch  0, batch     3 | loss: 23.4983100CurrentTrain: epoch  1, batch     0 | loss: 205.4426722CurrentTrain: epoch  1, batch     1 | loss: 156.7694573CurrentTrain: epoch  1, batch     2 | loss: 243.6947701CurrentTrain: epoch  1, batch     3 | loss: 20.2779193CurrentTrain: epoch  2, batch     0 | loss: 190.5295847CurrentTrain: epoch  2, batch     1 | loss: 177.4990648CurrentTrain: epoch  2, batch     2 | loss: 183.2419918CurrentTrain: epoch  2, batch     3 | loss: 22.2180294CurrentTrain: epoch  3, batch     0 | loss: 190.6230184CurrentTrain: epoch  3, batch     1 | loss: 175.5162116CurrentTrain: epoch  3, batch     2 | loss: 199.0384773CurrentTrain: epoch  3, batch     3 | loss: 14.4531353CurrentTrain: epoch  4, batch     0 | loss: 207.0291475CurrentTrain: epoch  4, batch     1 | loss: 170.6405399CurrentTrain: epoch  4, batch     2 | loss: 217.3388461CurrentTrain: epoch  4, batch     3 | loss: 21.1290913CurrentTrain: epoch  5, batch     0 | loss: 197.9480767CurrentTrain: epoch  5, batch     1 | loss: 192.5191209CurrentTrain: epoch  5, batch     2 | loss: 215.2992101CurrentTrain: epoch  5, batch     3 | loss: 20.1366434CurrentTrain: epoch  6, batch     0 | loss: 169.2797816CurrentTrain: epoch  6, batch     1 | loss: 177.9417630CurrentTrain: epoch  6, batch     2 | loss: 178.2049729CurrentTrain: epoch  6, batch     3 | loss: 41.1549865CurrentTrain: epoch  7, batch     0 | loss: 188.5843131CurrentTrain: epoch  7, batch     1 | loss: 195.9404500CurrentTrain: epoch  7, batch     2 | loss: 161.5784884CurrentTrain: epoch  7, batch     3 | loss: 41.0162435CurrentTrain: epoch  8, batch     0 | loss: 161.8311846CurrentTrain: epoch  8, batch     1 | loss: 212.6358707CurrentTrain: epoch  8, batch     2 | loss: 175.6349846CurrentTrain: epoch  8, batch     3 | loss: 11.8112563CurrentTrain: epoch  9, batch     0 | loss: 161.7685645CurrentTrain: epoch  9, batch     1 | loss: 175.1999901CurrentTrain: epoch  9, batch     2 | loss: 183.2058401CurrentTrain: epoch  9, batch     3 | loss: 41.0591377
MemoryTrain:  epoch  0, batch     0 | loss: 2.0046288MemoryTrain:  epoch  1, batch     0 | loss: 1.5292567MemoryTrain:  epoch  2, batch     0 | loss: 1.2008925MemoryTrain:  epoch  3, batch     0 | loss: 1.0496864MemoryTrain:  epoch  4, batch     0 | loss: 0.8178041MemoryTrain:  epoch  5, batch     0 | loss: 0.6429025MemoryTrain:  epoch  6, batch     0 | loss: 0.4918672MemoryTrain:  epoch  7, batch     0 | loss: 0.3939480MemoryTrain:  epoch  8, batch     0 | loss: 0.3613353MemoryTrain:  epoch  9, batch     0 | loss: 0.2858325

F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.96, 9: 0.0, 19: 0.0, 26: 0.4444444444444444, 27: 0.0, 31: 0.47191011235955055}
Micro-average F1 score: 0.4672897196261682
Weighted-average F1 score: 0.35505667915106115
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 26: 0.8, 27: 1.0, 31: 0.4186046511627907}
Micro-average F1 score: 0.5
Weighted-average F1 score: 0.3998001906893836
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.96, 9: 0.0, 19: 0.0, 26: 0.75, 27: 1.0, 31: 0.4186046511627907}
Micro-average F1 score: 0.4864864864864865
Weighted-average F1 score: 0.3810946023542922

F1 score per class: {32: 0.48366013071895425, 6: 0.04081632653061224, 7: 0.96, 40: 0.641860465116279, 9: 0.34782608695652173, 19: 0.7752808988764045, 24: 0.38095238095238093, 26: 0.8888888888888888, 27: 0.0, 29: 0.8829787234042553, 31: 0.3652173913043478}
Micro-average F1 score: 0.6678023850085179
Weighted-average F1 score: 0.6559427574549348
F1 score per class: {32: 0.5128205128205128, 6: 0.04, 7: 0.9803921568627451, 40: 0.6844444444444444, 9: 0.3448275862068966, 19: 0.770949720670391, 24: 0.6451612903225806, 26: 0.9479166666666666, 27: 0.8, 29: 0.8783068783068783, 31: 0.33962264150943394}
Micro-average F1 score: 0.6941467436108821
Weighted-average F1 score: 0.6842988789709468
F1 score per class: {32: 0.5283018867924528, 6: 0.0425531914893617, 7: 0.96, 40: 0.6666666666666666, 9: 0.3448275862068966, 19: 0.770949720670391, 24: 0.6, 26: 0.9417989417989417, 27: 1.0, 29: 0.8770053475935828, 31: 0.32727272727272727}
Micro-average F1 score: 0.6882255389718076
Weighted-average F1 score: 0.6761211423067597
cur_acc:  ['0.7851', '0.4673']
his_acc:  ['0.7851', '0.6678']
cur_acc des:  ['0.8257', '0.5000']
his_acc des:  ['0.8257', '0.6941']
cur_acc rrf:  ['0.8233', '0.4865']
his_acc rrf:  ['0.8233', '0.6882']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 214.6655461CurrentTrain: epoch  0, batch     1 | loss: 252.4956292CurrentTrain: epoch  0, batch     2 | loss: 225.3578668CurrentTrain: epoch  0, batch     3 | loss: 163.2061535CurrentTrain: epoch  1, batch     0 | loss: 244.8426132CurrentTrain: epoch  1, batch     1 | loss: 243.9385997CurrentTrain: epoch  1, batch     2 | loss: 194.0194502CurrentTrain: epoch  1, batch     3 | loss: 162.0282835CurrentTrain: epoch  2, batch     0 | loss: 258.3869270CurrentTrain: epoch  2, batch     1 | loss: 198.2687751CurrentTrain: epoch  2, batch     2 | loss: 169.6196699CurrentTrain: epoch  2, batch     3 | loss: 155.8909962CurrentTrain: epoch  3, batch     0 | loss: 217.4665496CurrentTrain: epoch  3, batch     1 | loss: 199.2834629CurrentTrain: epoch  3, batch     2 | loss: 222.6092869CurrentTrain: epoch  3, batch     3 | loss: 223.0592671CurrentTrain: epoch  4, batch     0 | loss: 196.5266730CurrentTrain: epoch  4, batch     1 | loss: 177.8653579CurrentTrain: epoch  4, batch     2 | loss: 222.3943733CurrentTrain: epoch  4, batch     3 | loss: 191.2127225CurrentTrain: epoch  5, batch     0 | loss: 183.7853736CurrentTrain: epoch  5, batch     1 | loss: 349.7315379CurrentTrain: epoch  5, batch     2 | loss: 183.5819973CurrentTrain: epoch  5, batch     3 | loss: 161.4262296CurrentTrain: epoch  6, batch     0 | loss: 170.8862985CurrentTrain: epoch  6, batch     1 | loss: 214.2549672CurrentTrain: epoch  6, batch     2 | loss: 203.3145098CurrentTrain: epoch  6, batch     3 | loss: 232.8713244CurrentTrain: epoch  7, batch     0 | loss: 199.4227984CurrentTrain: epoch  7, batch     1 | loss: 190.7338344CurrentTrain: epoch  7, batch     2 | loss: 212.6730909CurrentTrain: epoch  7, batch     3 | loss: 167.4740720CurrentTrain: epoch  8, batch     0 | loss: 241.0887656CurrentTrain: epoch  8, batch     1 | loss: 199.8624300CurrentTrain: epoch  8, batch     2 | loss: 219.1769976CurrentTrain: epoch  8, batch     3 | loss: 137.6768473CurrentTrain: epoch  9, batch     0 | loss: 175.8401600CurrentTrain: epoch  9, batch     1 | loss: 276.5682155CurrentTrain: epoch  9, batch     2 | loss: 199.0997449CurrentTrain: epoch  9, batch     3 | loss: 153.6712888
MemoryTrain:  epoch  0, batch     0 | loss: 1.4204253MemoryTrain:  epoch  1, batch     0 | loss: 1.0950029MemoryTrain:  epoch  2, batch     0 | loss: 0.7530062MemoryTrain:  epoch  3, batch     0 | loss: 0.6369443MemoryTrain:  epoch  4, batch     0 | loss: 0.5068036MemoryTrain:  epoch  5, batch     0 | loss: 0.4042174MemoryTrain:  epoch  6, batch     0 | loss: 0.3109491MemoryTrain:  epoch  7, batch     0 | loss: 0.2446843MemoryTrain:  epoch  8, batch     0 | loss: 0.2053482MemoryTrain:  epoch  9, batch     0 | loss: 0.1605417

F1 score per class: {0: 0.8955223880597015, 32: 0.8950276243093923, 4: 0.3333333333333333, 13: 0.76, 21: 0.7631578947368421, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8205128205128205
Weighted-average F1 score: 0.8015754205385502
F1 score per class: {0: 0.9577464788732394, 32: 0.9847715736040609, 4: 0.5714285714285714, 40: 0.7843137254901961, 13: 0.9195402298850575, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.908235294117647
Weighted-average F1 score: 0.8863266525334019
F1 score per class: {0: 0.9428571428571428, 32: 0.9847715736040609, 4: 0.5714285714285714, 40: 0.7843137254901961, 13: 0.8809523809523809, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8957345971563981
Weighted-average F1 score: 0.8715915747612811

F1 score per class: {32: 0.8955223880597015, 0: 0.8950276243093923, 4: 0.567741935483871, 6: 0.05128205128205128, 7: 0.96, 40: 0.125, 9: 0.67, 13: 0.5507246376811594, 19: 0.7631578947368421, 21: 0.1, 23: 0.7431693989071039, 24: 0.3, 26: 0.925531914893617, 27: 0.6666666666666666, 29: 0.875, 31: 0.5081967213114754}
Micro-average F1 score: 0.7222222222222222
Weighted-average F1 score: 0.7142699164244826
F1 score per class: {32: 0.9577464788732394, 0: 0.9847715736040609, 4: 0.6358381502890174, 6: 0.05128205128205128, 7: 0.9803921568627451, 40: 0.3076923076923077, 9: 0.6820276497695853, 13: 0.6451612903225806, 19: 0.9090909090909091, 21: 0.15384615384615385, 23: 0.7513812154696132, 24: 0.5294117647058824, 26: 0.9591836734693877, 27: 0.6666666666666666, 29: 0.8865979381443299, 31: 0.40350877192982454}
Micro-average F1 score: 0.7605294825511432
Weighted-average F1 score: 0.7532685260142544
F1 score per class: {32: 0.9428571428571428, 0: 0.9847715736040609, 4: 0.6190476190476191, 6: 0.05263157894736842, 7: 0.9803921568627451, 40: 0.26666666666666666, 9: 0.6851851851851852, 13: 0.5970149253731343, 19: 0.8705882352941177, 21: 0.16, 23: 0.7513812154696132, 24: 0.42857142857142855, 26: 0.9591836734693877, 27: 0.6666666666666666, 29: 0.8979591836734694, 31: 0.41379310344827586}
Micro-average F1 score: 0.7552870090634441
Weighted-average F1 score: 0.7483523669716254
cur_acc:  ['0.7851', '0.4673', '0.8205']
his_acc:  ['0.7851', '0.6678', '0.7222']
cur_acc des:  ['0.8257', '0.5000', '0.9082']
his_acc des:  ['0.8257', '0.6941', '0.7605']
cur_acc rrf:  ['0.8233', '0.4865', '0.8957']
his_acc rrf:  ['0.8233', '0.6882', '0.7553']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 211.0291195CurrentTrain: epoch  0, batch     1 | loss: 196.4725137CurrentTrain: epoch  0, batch     2 | loss: 247.1820747CurrentTrain: epoch  0, batch     3 | loss: 244.3563874CurrentTrain: epoch  0, batch     4 | loss: 187.8263148CurrentTrain: epoch  1, batch     0 | loss: 282.6979576CurrentTrain: epoch  1, batch     1 | loss: 291.3857989CurrentTrain: epoch  1, batch     2 | loss: 210.4686282CurrentTrain: epoch  1, batch     3 | loss: 181.6763853CurrentTrain: epoch  1, batch     4 | loss: 123.9307975CurrentTrain: epoch  2, batch     0 | loss: 179.6297913CurrentTrain: epoch  2, batch     1 | loss: 243.7480762CurrentTrain: epoch  2, batch     2 | loss: 205.3258314CurrentTrain: epoch  2, batch     3 | loss: 222.8781444CurrentTrain: epoch  2, batch     4 | loss: 180.9047435CurrentTrain: epoch  3, batch     0 | loss: 201.1618344CurrentTrain: epoch  3, batch     1 | loss: 232.3333734CurrentTrain: epoch  3, batch     2 | loss: 213.3545664CurrentTrain: epoch  3, batch     3 | loss: 205.7030969CurrentTrain: epoch  3, batch     4 | loss: 219.6611535CurrentTrain: epoch  4, batch     0 | loss: 278.8851746CurrentTrain: epoch  4, batch     1 | loss: 270.0001835CurrentTrain: epoch  4, batch     2 | loss: 287.2432858CurrentTrain: epoch  4, batch     3 | loss: 171.1881135CurrentTrain: epoch  4, batch     4 | loss: 111.6706684CurrentTrain: epoch  5, batch     0 | loss: 229.7227265CurrentTrain: epoch  5, batch     1 | loss: 178.3930779CurrentTrain: epoch  5, batch     2 | loss: 221.8323268CurrentTrain: epoch  5, batch     3 | loss: 238.4157233CurrentTrain: epoch  5, batch     4 | loss: 145.7394609CurrentTrain: epoch  6, batch     0 | loss: 213.5429597CurrentTrain: epoch  6, batch     1 | loss: 211.7487390CurrentTrain: epoch  6, batch     2 | loss: 212.7679323CurrentTrain: epoch  6, batch     3 | loss: 195.8935015CurrentTrain: epoch  6, batch     4 | loss: 228.8340222CurrentTrain: epoch  7, batch     0 | loss: 246.9152474CurrentTrain: epoch  7, batch     1 | loss: 220.4142281CurrentTrain: epoch  7, batch     2 | loss: 183.7827035CurrentTrain: epoch  7, batch     3 | loss: 175.8388911CurrentTrain: epoch  7, batch     4 | loss: 172.3844119CurrentTrain: epoch  8, batch     0 | loss: 221.6573505CurrentTrain: epoch  8, batch     1 | loss: 182.9569691CurrentTrain: epoch  8, batch     2 | loss: 202.7469640CurrentTrain: epoch  8, batch     3 | loss: 220.2548772CurrentTrain: epoch  8, batch     4 | loss: 131.5396302CurrentTrain: epoch  9, batch     0 | loss: 199.8659301CurrentTrain: epoch  9, batch     1 | loss: 202.1153151CurrentTrain: epoch  9, batch     2 | loss: 210.7139264CurrentTrain: epoch  9, batch     3 | loss: 211.8088794CurrentTrain: epoch  9, batch     4 | loss: 131.1794767
MemoryTrain:  epoch  0, batch     0 | loss: 1.1255976MemoryTrain:  epoch  1, batch     0 | loss: 0.8561396MemoryTrain:  epoch  2, batch     0 | loss: 0.5776524MemoryTrain:  epoch  3, batch     0 | loss: 0.4420212MemoryTrain:  epoch  4, batch     0 | loss: 0.3595578MemoryTrain:  epoch  5, batch     0 | loss: 0.2714183MemoryTrain:  epoch  6, batch     0 | loss: 0.2112464MemoryTrain:  epoch  7, batch     0 | loss: 0.1508234MemoryTrain:  epoch  8, batch     0 | loss: 0.1386285MemoryTrain:  epoch  9, batch     0 | loss: 0.1062031

F1 score per class: {5: 0.9690721649484536, 7: 0.0, 10: 0.4375, 13: 0.0, 16: 0.7755102040816326, 17: 0.36363636363636365, 18: 0.5769230769230769}
Micro-average F1 score: 0.6854663774403471
Weighted-average F1 score: 0.6889653852098624
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 7: 0.0, 10: 0.7757575757575758, 13: 0.0, 16: 0.9310344827586207, 17: 0.7142857142857143, 18: 0.9428571428571428}
Micro-average F1 score: 0.8652751423149905
Weighted-average F1 score: 0.8353842023506459
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 7: 0.0, 10: 0.7831325301204819, 13: 0.0, 16: 0.9122807017543859, 17: 0.6153846153846154, 18: 0.84375}
Micro-average F1 score: 0.85
Weighted-average F1 score: 0.8199328973757081

F1 score per class: {0: 0.7457627118644068, 4: 0.88268156424581, 5: 0.9641025641025641, 6: 0.48226950354609927, 7: 0.05, 9: 0.96, 10: 0.43410852713178294, 13: 0.07692307692307693, 16: 0.7755102040816326, 17: 0.2857142857142857, 18: 0.5769230769230769, 19: 0.6597938144329897, 21: 0.5538461538461539, 23: 0.8395061728395061, 24: 0.1, 26: 0.7582417582417582, 27: 0.3, 29: 0.9312169312169312, 31: 0.6666666666666666, 32: 0.8478260869565217, 40: 0.5210084033613446}
Micro-average F1 score: 0.7091273821464393
Weighted-average F1 score: 0.7142231814022311
F1 score per class: {0: 0.9295774647887324, 4: 0.9473684210526315, 5: 0.9658536585365853, 6: 0.6033519553072626, 7: 0.0, 9: 0.9803921568627451, 10: 0.7664670658682635, 13: 0.21052631578947367, 16: 0.9, 17: 0.37037037037037035, 18: 0.9166666666666666, 19: 0.6822429906542056, 21: 0.6129032258064516, 23: 0.9782608695652174, 24: 0.2222222222222222, 26: 0.7666666666666667, 27: 0.5454545454545454, 29: 0.9430051813471503, 31: 0.5, 32: 0.875, 40: 0.3963963963963964}
Micro-average F1 score: 0.7785419532324622
Weighted-average F1 score: 0.7728329413929081
F1 score per class: {0: 0.9142857142857143, 4: 0.9473684210526315, 5: 0.9658536585365853, 6: 0.5977011494252874, 7: 0.0, 9: 0.9803921568627451, 10: 0.7738095238095238, 13: 0.2, 16: 0.8813559322033898, 17: 0.3076923076923077, 18: 0.84375, 19: 0.6854460093896714, 21: 0.6031746031746031, 23: 0.9318181818181818, 24: 0.2222222222222222, 26: 0.7666666666666667, 27: 0.41379310344827586, 29: 0.9430051813471503, 31: 0.5714285714285714, 32: 0.8808290155440415, 40: 0.40350877192982454}
Micro-average F1 score: 0.7704387990762125
Weighted-average F1 score: 0.7625168562755342
cur_acc:  ['0.7851', '0.4673', '0.8205', '0.6855']
his_acc:  ['0.7851', '0.6678', '0.7222', '0.7091']
cur_acc des:  ['0.8257', '0.5000', '0.9082', '0.8653']
his_acc des:  ['0.8257', '0.6941', '0.7605', '0.7785']
cur_acc rrf:  ['0.8233', '0.4865', '0.8957', '0.8500']
his_acc rrf:  ['0.8233', '0.6882', '0.7553', '0.7704']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 188.5999572CurrentTrain: epoch  0, batch     1 | loss: 192.8165299CurrentTrain: epoch  0, batch     2 | loss: 245.7154395CurrentTrain: epoch  0, batch     3 | loss: 153.8638928CurrentTrain: epoch  1, batch     0 | loss: 186.9219751CurrentTrain: epoch  1, batch     1 | loss: 215.0630019CurrentTrain: epoch  1, batch     2 | loss: 219.7656466CurrentTrain: epoch  1, batch     3 | loss: 135.3807939CurrentTrain: epoch  2, batch     0 | loss: 223.9717832CurrentTrain: epoch  2, batch     1 | loss: 205.2562544CurrentTrain: epoch  2, batch     2 | loss: 155.8518890CurrentTrain: epoch  2, batch     3 | loss: 155.7603502CurrentTrain: epoch  3, batch     0 | loss: 241.6705877CurrentTrain: epoch  3, batch     1 | loss: 171.6743046CurrentTrain: epoch  3, batch     2 | loss: 276.5544242CurrentTrain: epoch  3, batch     3 | loss: 108.5228357CurrentTrain: epoch  4, batch     0 | loss: 194.7318969CurrentTrain: epoch  4, batch     1 | loss: 176.2491619CurrentTrain: epoch  4, batch     2 | loss: 195.7466139CurrentTrain: epoch  4, batch     3 | loss: 164.0470082CurrentTrain: epoch  5, batch     0 | loss: 183.2775122CurrentTrain: epoch  5, batch     1 | loss: 203.4183022CurrentTrain: epoch  5, batch     2 | loss: 183.2697455CurrentTrain: epoch  5, batch     3 | loss: 135.6614821CurrentTrain: epoch  6, batch     0 | loss: 178.4498931CurrentTrain: epoch  6, batch     1 | loss: 211.3651486CurrentTrain: epoch  6, batch     2 | loss: 174.9938506CurrentTrain: epoch  6, batch     3 | loss: 163.6961017CurrentTrain: epoch  7, batch     0 | loss: 237.2280088CurrentTrain: epoch  7, batch     1 | loss: 201.8087532CurrentTrain: epoch  7, batch     2 | loss: 169.8375589CurrentTrain: epoch  7, batch     3 | loss: 127.8868585CurrentTrain: epoch  8, batch     0 | loss: 162.8741591CurrentTrain: epoch  8, batch     1 | loss: 183.1548076CurrentTrain: epoch  8, batch     2 | loss: 276.0401450CurrentTrain: epoch  8, batch     3 | loss: 155.4601173CurrentTrain: epoch  9, batch     0 | loss: 185.4852354CurrentTrain: epoch  9, batch     1 | loss: 228.1300973CurrentTrain: epoch  9, batch     2 | loss: 236.9178153CurrentTrain: epoch  9, batch     3 | loss: 125.0537587
MemoryTrain:  epoch  0, batch     0 | loss: 0.7686454MemoryTrain:  epoch  1, batch     0 | loss: 0.5701354MemoryTrain:  epoch  2, batch     0 | loss: 0.4470386MemoryTrain:  epoch  3, batch     0 | loss: 0.3665968MemoryTrain:  epoch  4, batch     0 | loss: 0.2726868MemoryTrain:  epoch  5, batch     0 | loss: 0.2201383MemoryTrain:  epoch  6, batch     0 | loss: 0.1656657MemoryTrain:  epoch  7, batch     0 | loss: 0.1462388MemoryTrain:  epoch  8, batch     0 | loss: 0.1116096MemoryTrain:  epoch  9, batch     0 | loss: 0.1097421

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.8888888888888888, 38: 0.0, 10: 0.0, 13: 0.0, 15: 0.47058823529411764, 18: 0.0, 21: 0.3492063492063492, 23: 0.5714285714285714, 25: 0.5263157894736842}
Micro-average F1 score: 0.45394736842105265
Weighted-average F1 score: 0.3670722589836479
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 9: 0.0, 10: 0.0, 13: 0.0, 15: 0.6493506493506493, 18: 0.0, 21: 0.88, 23: 0.7368421052631579, 25: 0.782608695652174}
Micro-average F1 score: 0.6826666666666666
Weighted-average F1 score: 0.5961505542512407
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.75, 38: 0.0, 10: 0.0, 13: 0.0, 15: 0.5945945945945946, 18: 0.0, 21: 0.7912087912087912, 23: 0.7368421052631579, 25: 0.8085106382978723}
Micro-average F1 score: 0.6378378378378379
Weighted-average F1 score: 0.5347965002443954

F1 score per class: {0: 0.7457627118644068, 4: 0.8700564971751412, 5: 0.8715596330275229, 6: 0.5066666666666667, 7: 0.0, 9: 0.96, 10: 0.5874125874125874, 13: 0.07692307692307693, 15: 0.8, 16: 0.8518518518518519, 17: 0.0, 18: 0.44, 19: 0.4779874213836478, 21: 0.32653061224489793, 23: 0.8735632183908046, 24: 0.1, 25: 0.47058823529411764, 26: 0.7666666666666667, 27: 0.2857142857142857, 29: 0.9197860962566845, 31: 0.0, 32: 0.7471264367816092, 35: 0.34375, 37: 0.45714285714285713, 38: 0.4444444444444444, 40: 0.5172413793103449}
Micro-average F1 score: 0.6519823788546255
Weighted-average F1 score: 0.6681816517791014
F1 score per class: {0: 0.9444444444444444, 4: 0.9417989417989417, 5: 0.8771929824561403, 6: 0.5632183908045977, 7: 0.0, 9: 0.9615384615384616, 10: 0.7375, 13: 0.2, 15: 0.7058823529411765, 16: 0.8666666666666667, 17: 0.3333333333333333, 18: 0.5818181818181818, 19: 0.5652173913043478, 21: 0.5714285714285714, 23: 0.9148936170212766, 24: 0.09090909090909091, 25: 0.6493506493506493, 26: 0.7555555555555555, 27: 0.5384615384615384, 29: 0.9484536082474226, 31: 0.8, 32: 0.8484848484848485, 35: 0.8461538461538461, 37: 0.603448275862069, 38: 0.5217391304347826, 40: 0.43103448275862066}
Micro-average F1 score: 0.7301461872777558
Weighted-average F1 score: 0.730225142372273
F1 score per class: {0: 0.927536231884058, 4: 0.907103825136612, 5: 0.8658008658008658, 6: 0.5632183908045977, 7: 0.0, 9: 0.9803921568627451, 10: 0.7375, 13: 0.2, 15: 0.7058823529411765, 16: 0.8620689655172413, 17: 0.3333333333333333, 18: 0.5555555555555556, 19: 0.5251396648044693, 21: 0.5373134328358209, 23: 0.9130434782608695, 24: 0.09523809523809523, 25: 0.5945945945945946, 26: 0.7555555555555555, 27: 0.5, 29: 0.9375, 31: 0.6666666666666666, 32: 0.8349514563106796, 35: 0.7659574468085106, 37: 0.5691056910569106, 38: 0.5428571428571428, 40: 0.423728813559322}
Micro-average F1 score: 0.7128634010354441
Weighted-average F1 score: 0.71300973859112
cur_acc:  ['0.7851', '0.4673', '0.8205', '0.6855', '0.4539']
his_acc:  ['0.7851', '0.6678', '0.7222', '0.7091', '0.6520']
cur_acc des:  ['0.8257', '0.5000', '0.9082', '0.8653', '0.6827']
his_acc des:  ['0.8257', '0.6941', '0.7605', '0.7785', '0.7301']
cur_acc rrf:  ['0.8233', '0.4865', '0.8957', '0.8500', '0.6378']
his_acc rrf:  ['0.8233', '0.6882', '0.7553', '0.7704', '0.7129']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 265.3419635CurrentTrain: epoch  0, batch     1 | loss: 204.3922234CurrentTrain: epoch  0, batch     2 | loss: 229.3284123CurrentTrain: epoch  0, batch     3 | loss: 218.3627623CurrentTrain: epoch  0, batch     4 | loss: 62.2231827CurrentTrain: epoch  1, batch     0 | loss: 196.7125957CurrentTrain: epoch  1, batch     1 | loss: 245.8961860CurrentTrain: epoch  1, batch     2 | loss: 185.5241919CurrentTrain: epoch  1, batch     3 | loss: 225.5395756CurrentTrain: epoch  1, batch     4 | loss: 90.1428270CurrentTrain: epoch  2, batch     0 | loss: 220.2988237CurrentTrain: epoch  2, batch     1 | loss: 219.8438638CurrentTrain: epoch  2, batch     2 | loss: 158.5739457CurrentTrain: epoch  2, batch     3 | loss: 291.3041994CurrentTrain: epoch  2, batch     4 | loss: 56.6018222CurrentTrain: epoch  3, batch     0 | loss: 249.3590604CurrentTrain: epoch  3, batch     1 | loss: 286.1208240CurrentTrain: epoch  3, batch     2 | loss: 168.8772371CurrentTrain: epoch  3, batch     3 | loss: 208.7914008CurrentTrain: epoch  3, batch     4 | loss: 30.0425827CurrentTrain: epoch  4, batch     0 | loss: 183.9159436CurrentTrain: epoch  4, batch     1 | loss: 213.6032619CurrentTrain: epoch  4, batch     2 | loss: 231.5878530CurrentTrain: epoch  4, batch     3 | loss: 207.6025794CurrentTrain: epoch  4, batch     4 | loss: 90.0884778CurrentTrain: epoch  5, batch     0 | loss: 250.4076991CurrentTrain: epoch  5, batch     1 | loss: 191.9574947CurrentTrain: epoch  5, batch     2 | loss: 172.4964936CurrentTrain: epoch  5, batch     3 | loss: 214.7690564CurrentTrain: epoch  5, batch     4 | loss: 90.1291867CurrentTrain: epoch  6, batch     0 | loss: 171.8772412CurrentTrain: epoch  6, batch     1 | loss: 231.6107314CurrentTrain: epoch  6, batch     2 | loss: 213.2562896CurrentTrain: epoch  6, batch     3 | loss: 230.5256286CurrentTrain: epoch  6, batch     4 | loss: 51.4982297CurrentTrain: epoch  7, batch     0 | loss: 190.4300686CurrentTrain: epoch  7, batch     1 | loss: 192.8700414CurrentTrain: epoch  7, batch     2 | loss: 212.9903510CurrentTrain: epoch  7, batch     3 | loss: 220.7089579CurrentTrain: epoch  7, batch     4 | loss: 89.8397102CurrentTrain: epoch  8, batch     0 | loss: 177.6743239CurrentTrain: epoch  8, batch     1 | loss: 238.9130374CurrentTrain: epoch  8, batch     2 | loss: 247.9567311CurrentTrain: epoch  8, batch     3 | loss: 180.0798153CurrentTrain: epoch  8, batch     4 | loss: 51.4345735CurrentTrain: epoch  9, batch     0 | loss: 183.7153576CurrentTrain: epoch  9, batch     1 | loss: 229.5631141CurrentTrain: epoch  9, batch     2 | loss: 213.4194704CurrentTrain: epoch  9, batch     3 | loss: 219.5944390CurrentTrain: epoch  9, batch     4 | loss: 90.1028590
MemoryTrain:  epoch  0, batch     0 | loss: 0.7450732MemoryTrain:  epoch  1, batch     0 | loss: 0.6465356MemoryTrain:  epoch  2, batch     0 | loss: 0.5347559MemoryTrain:  epoch  3, batch     0 | loss: 0.4243840MemoryTrain:  epoch  4, batch     0 | loss: 0.4092662MemoryTrain:  epoch  5, batch     0 | loss: 0.3178427MemoryTrain:  epoch  6, batch     0 | loss: 0.3028065MemoryTrain:  epoch  7, batch     0 | loss: 0.2234824MemoryTrain:  epoch  8, batch     0 | loss: 0.1859778MemoryTrain:  epoch  9, batch     0 | loss: 0.1709195

F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 6: 0.0, 5: 0.0, 40: 0.6031746031746031, 10: 0.4732824427480916, 11: 0.0, 12: 0.0, 13: 0.0, 39: 0.0, 16: 0.6666666666666666, 18: 0.0, 19: 0.25, 28: 0.0}
Micro-average F1 score: 0.5075528700906344
Weighted-average F1 score: 0.4543219402901109
F1 score per class: {0: 0.0, 2: 0.9411764705882353, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.8571428571428571, 12: 0.7375, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 27: 0.0, 28: 0.8235294117647058, 35: 0.0, 37: 0.0, 39: 0.782608695652174, 40: 0.0}
Micro-average F1 score: 0.711217183770883
Weighted-average F1 score: 0.6140304165144319
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.8344370860927153, 12: 0.7682926829268293, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 27: 0.0, 28: 0.7777777777777778, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.6666666666666666, 40: 0.0}
Micro-average F1 score: 0.708433734939759
Weighted-average F1 score: 0.6138250641612377

F1 score per class: {0: 0.8656716417910447, 2: 0.7368421052631579, 4: 0.88268156424581, 5: 0.8636363636363636, 6: 0.5100671140939598, 7: 0.045454545454545456, 9: 0.96, 10: 0.3140495867768595, 11: 0.5066666666666667, 12: 0.4492753623188406, 13: 0.11764705882352941, 15: 0.7, 16: 0.8135593220338984, 17: 0.0, 18: 0.22727272727272727, 19: 0.582010582010582, 21: 0.3673469387755102, 23: 0.8505747126436781, 24: 0.1, 25: 0.44776119402985076, 26: 0.7570621468926554, 27: 0.36363636363636365, 28: 0.36363636363636365, 29: 0.925531914893617, 31: 0.0, 32: 0.7978142076502732, 35: 0.3582089552238806, 37: 0.5773195876288659, 38: 0.20512820512820512, 39: 0.18181818181818182, 40: 0.48739495798319327}
Micro-average F1 score: 0.6239155035835534
Weighted-average F1 score: 0.6514262874596929
F1 score per class: {0: 0.96, 2: 0.64, 4: 0.9247311827956989, 5: 0.8771929824561403, 6: 0.6219512195121951, 7: 0.0392156862745098, 9: 0.9803921568627451, 10: 0.6369426751592356, 11: 0.7058823529411765, 12: 0.6941176470588235, 13: 0.16666666666666666, 15: 0.7058823529411765, 16: 0.8387096774193549, 17: 0.0, 18: 0.34615384615384615, 19: 0.6320754716981132, 21: 0.5797101449275363, 23: 0.9183673469387755, 24: 0.09090909090909091, 25: 0.631578947368421, 26: 0.7597765363128491, 27: 0.5, 28: 0.35, 29: 0.9263157894736842, 31: 1.0, 32: 0.8756218905472637, 35: 0.8653846153846154, 37: 0.6782608695652174, 38: 0.5352112676056338, 39: 0.6206896551724138, 40: 0.4247787610619469}
Micro-average F1 score: 0.7184789859906604
Weighted-average F1 score: 0.7171578322499615
F1 score per class: {0: 0.9315068493150684, 2: 0.5185185185185185, 4: 0.9130434782608695, 5: 0.8620689655172413, 6: 0.6190476190476191, 7: 0.0425531914893617, 9: 0.9803921568627451, 10: 0.46715328467153283, 11: 0.6885245901639344, 12: 0.7078651685393258, 13: 0.10526315789473684, 15: 0.7058823529411765, 16: 0.8387096774193549, 17: 0.0, 18: 0.34615384615384615, 19: 0.6161137440758294, 21: 0.5625, 23: 0.9010989010989011, 24: 0.09090909090909091, 25: 0.5753424657534246, 26: 0.7640449438202247, 27: 0.5185185185185185, 28: 0.2916666666666667, 29: 0.9312169312169312, 31: 0.8, 32: 0.8405797101449275, 35: 0.6436781609195402, 37: 0.6949152542372882, 38: 0.45569620253164556, 39: 0.5185185185185185, 40: 0.41379310344827586}
Micro-average F1 score: 0.688359610868836
Weighted-average F1 score: 0.6868181212510375
cur_acc:  ['0.7851', '0.4673', '0.8205', '0.6855', '0.4539', '0.5076']
his_acc:  ['0.7851', '0.6678', '0.7222', '0.7091', '0.6520', '0.6239']
cur_acc des:  ['0.8257', '0.5000', '0.9082', '0.8653', '0.6827', '0.7112']
his_acc des:  ['0.8257', '0.6941', '0.7605', '0.7785', '0.7301', '0.7185']
cur_acc rrf:  ['0.8233', '0.4865', '0.8957', '0.8500', '0.6378', '0.7084']
his_acc rrf:  ['0.8233', '0.6882', '0.7553', '0.7704', '0.7129', '0.6884']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 200.7370772CurrentTrain: epoch  0, batch     1 | loss: 202.0209585CurrentTrain: epoch  0, batch     2 | loss: 367.4827376CurrentTrain: epoch  0, batch     3 | loss: 183.6013527CurrentTrain: epoch  0, batch     4 | loss: 206.1439863CurrentTrain: epoch  1, batch     0 | loss: 363.6753234CurrentTrain: epoch  1, batch     1 | loss: 221.1571527CurrentTrain: epoch  1, batch     2 | loss: 234.6842005CurrentTrain: epoch  1, batch     3 | loss: 194.3184057CurrentTrain: epoch  1, batch     4 | loss: 111.9562167CurrentTrain: epoch  2, batch     0 | loss: 203.6124350CurrentTrain: epoch  2, batch     1 | loss: 216.1218243CurrentTrain: epoch  2, batch     2 | loss: 240.6571684CurrentTrain: epoch  2, batch     3 | loss: 232.6725772CurrentTrain: epoch  2, batch     4 | loss: 148.5637409CurrentTrain: epoch  3, batch     0 | loss: 214.1459430CurrentTrain: epoch  3, batch     1 | loss: 188.2851437CurrentTrain: epoch  3, batch     2 | loss: 212.0827937CurrentTrain: epoch  3, batch     3 | loss: 240.7099985CurrentTrain: epoch  3, batch     4 | loss: 125.8946789CurrentTrain: epoch  4, batch     0 | loss: 213.3274187CurrentTrain: epoch  4, batch     1 | loss: 213.4105466CurrentTrain: epoch  4, batch     2 | loss: 198.8545570CurrentTrain: epoch  4, batch     3 | loss: 221.1609817CurrentTrain: epoch  4, batch     4 | loss: 124.2556066CurrentTrain: epoch  5, batch     0 | loss: 238.1403435CurrentTrain: epoch  5, batch     1 | loss: 276.8024723CurrentTrain: epoch  5, batch     2 | loss: 228.7880214CurrentTrain: epoch  5, batch     3 | loss: 204.4655731CurrentTrain: epoch  5, batch     4 | loss: 88.9809493CurrentTrain: epoch  6, batch     0 | loss: 185.9085688CurrentTrain: epoch  6, batch     1 | loss: 203.7023114CurrentTrain: epoch  6, batch     2 | loss: 212.8705921CurrentTrain: epoch  6, batch     3 | loss: 202.3602988CurrentTrain: epoch  6, batch     4 | loss: 202.1488984CurrentTrain: epoch  7, batch     0 | loss: 196.3092177CurrentTrain: epoch  7, batch     1 | loss: 210.7577873CurrentTrain: epoch  7, batch     2 | loss: 360.4212576CurrentTrain: epoch  7, batch     3 | loss: 202.7671418CurrentTrain: epoch  7, batch     4 | loss: 124.1601766CurrentTrain: epoch  8, batch     0 | loss: 200.8390843CurrentTrain: epoch  8, batch     1 | loss: 348.0838097CurrentTrain: epoch  8, batch     2 | loss: 228.7289110CurrentTrain: epoch  8, batch     3 | loss: 237.8510633CurrentTrain: epoch  8, batch     4 | loss: 80.7229125CurrentTrain: epoch  9, batch     0 | loss: 220.2451093CurrentTrain: epoch  9, batch     1 | loss: 154.1241921CurrentTrain: epoch  9, batch     2 | loss: 220.8038072CurrentTrain: epoch  9, batch     3 | loss: 237.7652542CurrentTrain: epoch  9, batch     4 | loss: 156.8690061
MemoryTrain:  epoch  0, batch     0 | loss: 0.9689717MemoryTrain:  epoch  1, batch     0 | loss: 0.7997154MemoryTrain:  epoch  2, batch     0 | loss: 0.6387414MemoryTrain:  epoch  3, batch     0 | loss: 0.5805319MemoryTrain:  epoch  4, batch     0 | loss: 0.4805845MemoryTrain:  epoch  5, batch     0 | loss: 0.4128085MemoryTrain:  epoch  6, batch     0 | loss: 0.4073631MemoryTrain:  epoch  7, batch     0 | loss: 0.3151161MemoryTrain:  epoch  8, batch     0 | loss: 0.2458264MemoryTrain:  epoch  9, batch     0 | loss: 0.2415110

F1 score per class: {32: 0.2905982905982906, 1: 0.5486725663716814, 34: 0.0, 3: 0.0, 35: 0.08, 37: 0.0, 9: 0.7159090909090909, 11: 0.0, 14: 0.0, 18: 0.0, 22: 0.0, 23: 0.25396825396825395, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.4046434494195688
Weighted-average F1 score: 0.3680898527385934
F1 score per class: {32: 0.3968253968253968, 1: 0.7669172932330827, 34: 0.0, 3: 0.0, 35: 0.0975609756097561, 37: 0.0, 38: 0.0, 9: 0.650887573964497, 11: 0.0, 14: 0.0, 18: 0.0, 21: 0.5526315789473685, 22: 0.0, 23: 0.0, 24: 0.0}
Micro-average F1 score: 0.46706586826347307
Weighted-average F1 score: 0.40036981467141514
F1 score per class: {32: 0.3968253968253968, 1: 0.7575757575757576, 34: 0.0, 3: 0.11627906976744186, 35: 0.0, 37: 0.0, 38: 0.6666666666666666, 11: 0.0, 14: 0.0, 18: 0.0, 21: 0.5333333333333333, 22: 0.0, 23: 0.0, 24: 0.0}
Micro-average F1 score: 0.4743202416918429
Weighted-average F1 score: 0.41131400656078054

F1 score per class: {0: 0.8307692307692308, 1: 0.26153846153846155, 2: 0.6666666666666666, 3: 0.49206349206349204, 4: 0.8571428571428571, 5: 0.8703703703703703, 6: 0.4827586206896552, 7: 0.04878048780487805, 9: 0.9615384615384616, 10: 0.2975206611570248, 11: 0.42038216560509556, 12: 0.32, 13: 0.13333333333333333, 14: 0.07792207792207792, 15: 0.75, 16: 0.8275862068965517, 17: 0.0, 18: 0.17777777777777778, 19: 0.32116788321167883, 21: 0.06060606060606061, 22: 0.6494845360824743, 23: 0.8666666666666667, 24: 0.07692307692307693, 25: 0.42424242424242425, 26: 0.7640449438202247, 27: 0.125, 28: 0.3, 29: 0.9139784946236559, 31: 0.0, 32: 0.6335403726708074, 34: 0.16842105263157894, 35: 0.27906976744186046, 37: 0.4782608695652174, 38: 0.23255813953488372, 39: 0.10526315789473684, 40: 0.512}
Micro-average F1 score: 0.5353440150801131
Weighted-average F1 score: 0.5670959832429967
F1 score per class: {0: 0.9444444444444444, 1: 0.3448275862068966, 2: 0.45161290322580644, 3: 0.6623376623376623, 4: 0.8764044943820225, 5: 0.8571428571428571, 6: 0.5590062111801242, 7: 0.045454545454545456, 9: 0.9259259259259259, 10: 0.6225165562913907, 11: 0.6666666666666666, 12: 0.6867469879518072, 13: 0.13333333333333333, 14: 0.09302325581395349, 15: 0.7058823529411765, 16: 0.8387096774193549, 17: 0.0, 18: 0.38596491228070173, 19: 0.4503311258278146, 21: 0.25, 22: 0.6043956043956044, 23: 0.9278350515463918, 24: 0.08, 25: 0.5789473684210527, 26: 0.7597765363128491, 27: 0.1111111111111111, 28: 0.3111111111111111, 29: 0.9319371727748691, 31: 0.0, 32: 0.8235294117647058, 34: 0.2916666666666667, 35: 0.5964912280701754, 37: 0.25, 38: 0.5, 39: 0.2857142857142857, 40: 0.593103448275862}
Micro-average F1 score: 0.6179806362378977
Weighted-average F1 score: 0.6182103864401918
F1 score per class: {0: 0.9142857142857143, 1: 0.3472222222222222, 2: 0.4666666666666667, 3: 0.6535947712418301, 4: 0.88268156424581, 5: 0.853448275862069, 6: 0.5644171779141104, 7: 0.04878048780487805, 9: 0.9803921568627451, 10: 0.5035971223021583, 11: 0.6735751295336787, 12: 0.6909090909090909, 13: 0.10526315789473684, 14: 0.1111111111111111, 15: 0.7058823529411765, 16: 0.8387096774193549, 17: 0.0, 18: 0.38596491228070173, 19: 0.4444444444444444, 21: 0.22641509433962265, 22: 0.6195652173913043, 23: 0.9375, 24: 0.08, 25: 0.5205479452054794, 26: 0.7597765363128491, 27: 0.125, 28: 0.2692307692307692, 29: 0.9312169312169312, 31: 0.0, 32: 0.8191489361702128, 34: 0.2898550724637681, 35: 0.5486725663716814, 37: 0.26666666666666666, 38: 0.48, 39: 0.2857142857142857, 40: 0.589041095890411}
Micro-average F1 score: 0.6111884219315336
Weighted-average F1 score: 0.6124945875756123
cur_acc:  ['0.7851', '0.4673', '0.8205', '0.6855', '0.4539', '0.5076', '0.4046']
his_acc:  ['0.7851', '0.6678', '0.7222', '0.7091', '0.6520', '0.6239', '0.5353']
cur_acc des:  ['0.8257', '0.5000', '0.9082', '0.8653', '0.6827', '0.7112', '0.4671']
his_acc des:  ['0.8257', '0.6941', '0.7605', '0.7785', '0.7301', '0.7185', '0.6180']
cur_acc rrf:  ['0.8233', '0.4865', '0.8957', '0.8500', '0.6378', '0.7084', '0.4743']
his_acc rrf:  ['0.8233', '0.6882', '0.7553', '0.7704', '0.7129', '0.6884', '0.6112']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 177.2944331CurrentTrain: epoch  0, batch     1 | loss: 289.7719235CurrentTrain: epoch  0, batch     2 | loss: 202.1111006CurrentTrain: epoch  0, batch     3 | loss: 129.1955153CurrentTrain: epoch  1, batch     0 | loss: 212.5172018CurrentTrain: epoch  1, batch     1 | loss: 210.3533083CurrentTrain: epoch  1, batch     2 | loss: 213.3319513CurrentTrain: epoch  1, batch     3 | loss: 121.3040894CurrentTrain: epoch  2, batch     0 | loss: 272.3582263CurrentTrain: epoch  2, batch     1 | loss: 206.7481517CurrentTrain: epoch  2, batch     2 | loss: 185.7211013CurrentTrain: epoch  2, batch     3 | loss: 100.5931061CurrentTrain: epoch  3, batch     0 | loss: 250.2648352CurrentTrain: epoch  3, batch     1 | loss: 232.1144576CurrentTrain: epoch  3, batch     2 | loss: 156.6199001CurrentTrain: epoch  3, batch     3 | loss: 113.9619355CurrentTrain: epoch  4, batch     0 | loss: 181.4058729CurrentTrain: epoch  4, batch     1 | loss: 185.9558249CurrentTrain: epoch  4, batch     2 | loss: 286.9386255CurrentTrain: epoch  4, batch     3 | loss: 116.8135623CurrentTrain: epoch  5, batch     0 | loss: 181.1680425CurrentTrain: epoch  5, batch     1 | loss: 267.3838706CurrentTrain: epoch  5, batch     2 | loss: 178.3991324CurrentTrain: epoch  5, batch     3 | loss: 144.8848466CurrentTrain: epoch  6, batch     0 | loss: 168.5624990CurrentTrain: epoch  6, batch     1 | loss: 237.3566582CurrentTrain: epoch  6, batch     2 | loss: 167.3157102CurrentTrain: epoch  6, batch     3 | loss: 144.3912209CurrentTrain: epoch  7, batch     0 | loss: 175.0822429CurrentTrain: epoch  7, batch     1 | loss: 202.6434360CurrentTrain: epoch  7, batch     2 | loss: 202.7475374CurrentTrain: epoch  7, batch     3 | loss: 128.5744472CurrentTrain: epoch  8, batch     0 | loss: 163.2430723CurrentTrain: epoch  8, batch     1 | loss: 360.7187279CurrentTrain: epoch  8, batch     2 | loss: 186.5012171CurrentTrain: epoch  8, batch     3 | loss: 118.2627322CurrentTrain: epoch  9, batch     0 | loss: 170.3522500CurrentTrain: epoch  9, batch     1 | loss: 266.1929997CurrentTrain: epoch  9, batch     2 | loss: 182.3632923CurrentTrain: epoch  9, batch     3 | loss: 136.2083219
MemoryTrain:  epoch  0, batch     0 | loss: 0.5014549MemoryTrain:  epoch  1, batch     0 | loss: 0.4541356MemoryTrain:  epoch  2, batch     0 | loss: 0.3648036MemoryTrain:  epoch  3, batch     0 | loss: 0.2949873MemoryTrain:  epoch  4, batch     0 | loss: 0.2510150MemoryTrain:  epoch  5, batch     0 | loss: 0.2047709MemoryTrain:  epoch  6, batch     0 | loss: 0.1952646MemoryTrain:  epoch  7, batch     0 | loss: 0.1635069MemoryTrain:  epoch  8, batch     0 | loss: 0.1420180MemoryTrain:  epoch  9, batch     0 | loss: 0.1143045

F1 score per class: {33: 0.3469387755102041, 34: 0.0, 36: 0.8631578947368421, 37: 0.0, 39: 0.0, 8: 0.0, 18: 0.8823529411764706, 20: 0.42857142857142855, 26: 0.0, 28: 0.42857142857142855, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5340909090909091
Weighted-average F1 score: 0.524981214756455
F1 score per class: {2: 0.0, 5: 0.0, 8: 0.6935483870967742, 12: 0.0, 18: 0.0, 20: 0.8888888888888888, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.42857142857142855, 34: 0.0, 35: 0.0, 36: 0.8421052631578947, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7255813953488373
Weighted-average F1 score: 0.6484565883214474
F1 score per class: {2: 0.0, 5: 0.0, 8: 0.6721311475409836, 12: 0.0, 18: 0.0, 20: 0.8888888888888888, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.42857142857142855, 34: 0.0, 35: 0.0, 36: 0.8214285714285714, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7086247086247086
Weighted-average F1 score: 0.626981409768295

F1 score per class: {0: 0.8484848484848485, 1: 0.27906976744186046, 2: 0.75, 3: 0.4745762711864407, 4: 0.8372093023255814, 5: 0.8789237668161435, 6: 0.34108527131782945, 7: 0.0, 8: 0.288135593220339, 9: 0.9615384615384616, 10: 0.09523809523809523, 11: 0.3225806451612903, 12: 0.16071428571428573, 13: 0.2222222222222222, 14: 0.0, 15: 0.75, 16: 0.7636363636363637, 17: 0.0, 18: 0.2, 19: 0.3511450381679389, 20: 0.82, 21: 0.0, 22: 0.6666666666666666, 23: 0.8666666666666667, 24: 0.07407407407407407, 25: 0.44776119402985076, 26: 0.7597765363128491, 27: 0.17391304347826086, 28: 0.45454545454545453, 29: 0.8729281767955801, 30: 0.8823529411764706, 31: 0.6666666666666666, 32: 0.6075949367088608, 33: 0.2727272727272727, 34: 0.23008849557522124, 35: 0.2682926829268293, 36: 0.41379310344827586, 37: 0.46511627906976744, 38: 0.11764705882352941, 39: 0.10526315789473684, 40: 0.5354330708661418}
Micro-average F1 score: 0.5178466939730837
Weighted-average F1 score: 0.577568950341094
F1 score per class: {0: 0.9315068493150684, 1: 0.33557046979865773, 2: 0.5185185185185185, 3: 0.6068965517241379, 4: 0.88268156424581, 5: 0.8425531914893617, 6: 0.52, 7: 0.0, 8: 0.45989304812834225, 9: 0.9259259259259259, 10: 0.16363636363636364, 11: 0.6097560975609756, 12: 0.5853658536585366, 13: 0.2222222222222222, 14: 0.075, 15: 0.75, 16: 0.8771929824561403, 17: 0.16666666666666666, 18: 0.41379310344827586, 19: 0.4444444444444444, 20: 0.8148148148148148, 21: 0.3137254901960784, 22: 0.673469387755102, 23: 0.9263157894736842, 24: 0.08, 25: 0.6329113924050633, 26: 0.7555555555555555, 27: 0.21052631578947367, 28: 0.26666666666666666, 29: 0.9032258064516129, 30: 0.8571428571428571, 31: 0.8, 32: 0.7914438502673797, 33: 0.21428571428571427, 34: 0.35668789808917195, 35: 0.5409836065573771, 36: 0.768, 37: 0.2597402597402597, 38: 0.4482758620689655, 39: 0.2, 40: 0.5611510791366906}
Micro-average F1 score: 0.6017034068136272
Weighted-average F1 score: 0.6133748655372679
F1 score per class: {0: 0.9166666666666666, 1: 0.33557046979865773, 2: 0.5185185185185185, 3: 0.6133333333333333, 4: 0.8888888888888888, 5: 0.8284518828451883, 6: 0.5359477124183006, 7: 0.0, 8: 0.44808743169398907, 9: 0.9615384615384616, 10: 0.16363636363636364, 11: 0.5838509316770186, 12: 0.5748502994011976, 13: 0.2222222222222222, 14: 0.09411764705882353, 15: 0.75, 16: 0.847457627118644, 17: 0.0, 18: 0.41379310344827586, 19: 0.4266666666666667, 20: 0.7927927927927928, 21: 0.2, 22: 0.6804123711340206, 23: 0.8913043478260869, 24: 0.08, 25: 0.5205479452054794, 26: 0.7555555555555555, 27: 0.21052631578947367, 28: 0.24242424242424243, 29: 0.9032258064516129, 30: 0.9230769230769231, 31: 0.8, 32: 0.7891891891891892, 33: 0.21428571428571427, 34: 0.34210526315789475, 35: 0.38095238095238093, 36: 0.7603305785123967, 37: 0.3037974683544304, 38: 0.4406779661016949, 39: 0.08, 40: 0.5555555555555556}
Micro-average F1 score: 0.5901970692268823
Weighted-average F1 score: 0.6025860999211607
cur_acc:  ['0.7851', '0.4673', '0.8205', '0.6855', '0.4539', '0.5076', '0.4046', '0.5341']
his_acc:  ['0.7851', '0.6678', '0.7222', '0.7091', '0.6520', '0.6239', '0.5353', '0.5178']
cur_acc des:  ['0.8257', '0.5000', '0.9082', '0.8653', '0.6827', '0.7112', '0.4671', '0.7256']
his_acc des:  ['0.8257', '0.6941', '0.7605', '0.7785', '0.7301', '0.7185', '0.6180', '0.6017']
cur_acc rrf:  ['0.8233', '0.4865', '0.8957', '0.8500', '0.6378', '0.7084', '0.4743', '0.7086']
his_acc rrf:  ['0.8233', '0.6882', '0.7553', '0.7704', '0.7129', '0.6884', '0.6112', '0.5902']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 217.3613468CurrentTrain: epoch  0, batch     1 | loss: 238.9061148CurrentTrain: epoch  0, batch     2 | loss: 208.7347467CurrentTrain: epoch  0, batch     3 | loss: 206.3009226CurrentTrain: epoch  0, batch     4 | loss: 292.9672657CurrentTrain: epoch  0, batch     5 | loss: 227.1824021CurrentTrain: epoch  0, batch     6 | loss: 213.0006154CurrentTrain: epoch  0, batch     7 | loss: 224.6792351CurrentTrain: epoch  0, batch     8 | loss: 297.1773980CurrentTrain: epoch  0, batch     9 | loss: 219.0724793CurrentTrain: epoch  0, batch    10 | loss: 177.8744189CurrentTrain: epoch  0, batch    11 | loss: 289.1371418CurrentTrain: epoch  0, batch    12 | loss: 205.6579582CurrentTrain: epoch  0, batch    13 | loss: 170.2123878CurrentTrain: epoch  0, batch    14 | loss: 223.6891252CurrentTrain: epoch  0, batch    15 | loss: 198.2728138CurrentTrain: epoch  0, batch    16 | loss: 236.3631206CurrentTrain: epoch  0, batch    17 | loss: 250.2254677CurrentTrain: epoch  0, batch    18 | loss: 224.4316522CurrentTrain: epoch  0, batch    19 | loss: 256.9508297CurrentTrain: epoch  0, batch    20 | loss: 217.8293494CurrentTrain: epoch  0, batch    21 | loss: 249.3871420CurrentTrain: epoch  0, batch    22 | loss: 204.4031656CurrentTrain: epoch  0, batch    23 | loss: 210.6025847CurrentTrain: epoch  0, batch    24 | loss: 191.1625215CurrentTrain: epoch  0, batch    25 | loss: 190.2244605CurrentTrain: epoch  0, batch    26 | loss: 163.7474106CurrentTrain: epoch  0, batch    27 | loss: 294.5787155CurrentTrain: epoch  0, batch    28 | loss: 296.4676106CurrentTrain: epoch  0, batch    29 | loss: 191.1822959CurrentTrain: epoch  0, batch    30 | loss: 230.8830577CurrentTrain: epoch  0, batch    31 | loss: 180.7151202CurrentTrain: epoch  0, batch    32 | loss: 175.5664052CurrentTrain: epoch  0, batch    33 | loss: 235.6817610CurrentTrain: epoch  0, batch    34 | loss: 223.0859840CurrentTrain: epoch  0, batch    35 | loss: 182.9310823CurrentTrain: epoch  0, batch    36 | loss: 241.1105532CurrentTrain: epoch  0, batch    37 | loss: 209.9896907CurrentTrain: epoch  0, batch    38 | loss: 221.9373385CurrentTrain: epoch  0, batch    39 | loss: 248.3660166CurrentTrain: epoch  0, batch    40 | loss: 256.6640113CurrentTrain: epoch  0, batch    41 | loss: 221.6603874CurrentTrain: epoch  0, batch    42 | loss: 222.1992723CurrentTrain: epoch  0, batch    43 | loss: 229.3879276CurrentTrain: epoch  0, batch    44 | loss: 195.8899990CurrentTrain: epoch  0, batch    45 | loss: 170.8367509CurrentTrain: epoch  0, batch    46 | loss: 183.6900558CurrentTrain: epoch  0, batch    47 | loss: 222.6830469CurrentTrain: epoch  0, batch    48 | loss: 255.9116135CurrentTrain: epoch  0, batch    49 | loss: 278.5905613CurrentTrain: epoch  0, batch    50 | loss: 202.4743510CurrentTrain: epoch  0, batch    51 | loss: 223.2294998CurrentTrain: epoch  0, batch    52 | loss: 295.2048069CurrentTrain: epoch  0, batch    53 | loss: 341.8808121CurrentTrain: epoch  0, batch    54 | loss: 189.3343713CurrentTrain: epoch  0, batch    55 | loss: 294.3849422CurrentTrain: epoch  0, batch    56 | loss: 228.5284860CurrentTrain: epoch  0, batch    57 | loss: 172.8454462CurrentTrain: epoch  0, batch    58 | loss: 220.7107985CurrentTrain: epoch  0, batch    59 | loss: 280.1524976CurrentTrain: epoch  0, batch    60 | loss: 200.7299723CurrentTrain: epoch  0, batch    61 | loss: 246.4221065CurrentTrain: epoch  0, batch    62 | loss: 213.4775748CurrentTrain: epoch  0, batch    63 | loss: 221.7009020CurrentTrain: epoch  0, batch    64 | loss: 188.6313366CurrentTrain: epoch  0, batch    65 | loss: 256.2111030CurrentTrain: epoch  0, batch    66 | loss: 185.7241976CurrentTrain: epoch  0, batch    67 | loss: 225.1666416CurrentTrain: epoch  0, batch    68 | loss: 231.0598295CurrentTrain: epoch  0, batch    69 | loss: 227.6713672CurrentTrain: epoch  0, batch    70 | loss: 281.1490864CurrentTrain: epoch  0, batch    71 | loss: 187.8036273CurrentTrain: epoch  0, batch    72 | loss: 182.2753253CurrentTrain: epoch  0, batch    73 | loss: 166.0414694CurrentTrain: epoch  0, batch    74 | loss: 194.9861352CurrentTrain: epoch  0, batch    75 | loss: 170.5516492CurrentTrain: epoch  0, batch    76 | loss: 186.8219342CurrentTrain: epoch  0, batch    77 | loss: 212.6882406CurrentTrain: epoch  0, batch    78 | loss: 199.4690565CurrentTrain: epoch  0, batch    79 | loss: 184.9418352CurrentTrain: epoch  0, batch    80 | loss: 213.8593787CurrentTrain: epoch  0, batch    81 | loss: 187.5188754CurrentTrain: epoch  0, batch    82 | loss: 178.6494655CurrentTrain: epoch  0, batch    83 | loss: 234.8275208CurrentTrain: epoch  0, batch    84 | loss: 255.5800861CurrentTrain: epoch  0, batch    85 | loss: 172.6592760CurrentTrain: epoch  0, batch    86 | loss: 193.8562364CurrentTrain: epoch  0, batch    87 | loss: 209.2682243CurrentTrain: epoch  0, batch    88 | loss: 280.7893253CurrentTrain: epoch  0, batch    89 | loss: 229.9461532CurrentTrain: epoch  0, batch    90 | loss: 193.3817566CurrentTrain: epoch  0, batch    91 | loss: 184.9518760CurrentTrain: epoch  0, batch    92 | loss: 211.3885615CurrentTrain: epoch  0, batch    93 | loss: 184.7606137CurrentTrain: epoch  0, batch    94 | loss: 203.9647005CurrentTrain: epoch  0, batch    95 | loss: 160.1285243CurrentTrain: epoch  1, batch     0 | loss: 173.8618969CurrentTrain: epoch  1, batch     1 | loss: 185.6319813CurrentTrain: epoch  1, batch     2 | loss: 234.5998781CurrentTrain: epoch  1, batch     3 | loss: 228.0561456CurrentTrain: epoch  1, batch     4 | loss: 271.7685097CurrentTrain: epoch  1, batch     5 | loss: 174.7154721CurrentTrain: epoch  1, batch     6 | loss: 189.6083903CurrentTrain: epoch  1, batch     7 | loss: 159.6822424CurrentTrain: epoch  1, batch     8 | loss: 209.7828704CurrentTrain: epoch  1, batch     9 | loss: 209.5916527CurrentTrain: epoch  1, batch    10 | loss: 201.6225188CurrentTrain: epoch  1, batch    11 | loss: 216.0584176CurrentTrain: epoch  1, batch    12 | loss: 161.2140311CurrentTrain: epoch  1, batch    13 | loss: 215.7184111CurrentTrain: epoch  1, batch    14 | loss: 207.8803485CurrentTrain: epoch  1, batch    15 | loss: 244.6375676CurrentTrain: epoch  1, batch    16 | loss: 236.4159758CurrentTrain: epoch  1, batch    17 | loss: 203.8028463CurrentTrain: epoch  1, batch    18 | loss: 175.5692068CurrentTrain: epoch  1, batch    19 | loss: 190.7560851CurrentTrain: epoch  1, batch    20 | loss: 203.3439434CurrentTrain: epoch  1, batch    21 | loss: 187.0659640CurrentTrain: epoch  1, batch    22 | loss: 166.4563458CurrentTrain: epoch  1, batch    23 | loss: 183.6580607CurrentTrain: epoch  1, batch    24 | loss: 223.3728761CurrentTrain: epoch  1, batch    25 | loss: 199.5480624CurrentTrain: epoch  1, batch    26 | loss: 243.7595715CurrentTrain: epoch  1, batch    27 | loss: 242.3228774CurrentTrain: epoch  1, batch    28 | loss: 217.4447765CurrentTrain: epoch  1, batch    29 | loss: 204.3153060CurrentTrain: epoch  1, batch    30 | loss: 194.5443012CurrentTrain: epoch  1, batch    31 | loss: 293.5197064CurrentTrain: epoch  1, batch    32 | loss: 192.9182036CurrentTrain: epoch  1, batch    33 | loss: 218.1580158CurrentTrain: epoch  1, batch    34 | loss: 239.6166081CurrentTrain: epoch  1, batch    35 | loss: 232.3137739CurrentTrain: epoch  1, batch    36 | loss: 235.7992983CurrentTrain: epoch  1, batch    37 | loss: 235.6237041CurrentTrain: epoch  1, batch    38 | loss: 253.3874928CurrentTrain: epoch  1, batch    39 | loss: 207.5126403CurrentTrain: epoch  1, batch    40 | loss: 180.9021610CurrentTrain: epoch  1, batch    41 | loss: 197.6431932CurrentTrain: epoch  1, batch    42 | loss: 161.7871363CurrentTrain: epoch  1, batch    43 | loss: 359.5036717CurrentTrain: epoch  1, batch    44 | loss: 194.0078525CurrentTrain: epoch  1, batch    45 | loss: 197.1392100CurrentTrain: epoch  1, batch    46 | loss: 228.9452609CurrentTrain: epoch  1, batch    47 | loss: 216.5612609CurrentTrain: epoch  1, batch    48 | loss: 206.3494404CurrentTrain: epoch  1, batch    49 | loss: 254.4631410CurrentTrain: epoch  1, batch    50 | loss: 190.2971449CurrentTrain: epoch  1, batch    51 | loss: 195.5394817CurrentTrain: epoch  1, batch    52 | loss: 189.0367436CurrentTrain: epoch  1, batch    53 | loss: 225.4291648CurrentTrain: epoch  1, batch    54 | loss: 207.9923022CurrentTrain: epoch  1, batch    55 | loss: 199.4985856CurrentTrain: epoch  1, batch    56 | loss: 190.0346544CurrentTrain: epoch  1, batch    57 | loss: 205.5387657CurrentTrain: epoch  1, batch    58 | loss: 233.6622930CurrentTrain: epoch  1, batch    59 | loss: 195.3551652CurrentTrain: epoch  1, batch    60 | loss: 188.6527181CurrentTrain: epoch  1, batch    61 | loss: 234.0326847CurrentTrain: epoch  1, batch    62 | loss: 198.4817849CurrentTrain: epoch  1, batch    63 | loss: 364.9118253CurrentTrain: epoch  1, batch    64 | loss: 219.1698847CurrentTrain: epoch  1, batch    65 | loss: 232.6149762CurrentTrain: epoch  1, batch    66 | loss: 207.7681631CurrentTrain: epoch  1, batch    67 | loss: 185.2767673CurrentTrain: epoch  1, batch    68 | loss: 223.2731806CurrentTrain: epoch  1, batch    69 | loss: 186.9173704CurrentTrain: epoch  1, batch    70 | loss: 182.4469491CurrentTrain: epoch  1, batch    71 | loss: 190.8832442CurrentTrain: epoch  1, batch    72 | loss: 214.7850919CurrentTrain: epoch  1, batch    73 | loss: 191.7482290CurrentTrain: epoch  1, batch    74 | loss: 234.0187949CurrentTrain: epoch  1, batch    75 | loss: 177.9459240CurrentTrain: epoch  1, batch    76 | loss: 225.8412618CurrentTrain: epoch  1, batch    77 | loss: 203.5919293CurrentTrain: epoch  1, batch    78 | loss: 204.5907520CurrentTrain: epoch  1, batch    79 | loss: 184.3622738CurrentTrain: epoch  1, batch    80 | loss: 232.0635420CurrentTrain: epoch  1, batch    81 | loss: 166.9250426CurrentTrain: epoch  1, batch    82 | loss: 195.0996628CurrentTrain: epoch  1, batch    83 | loss: 217.6608123CurrentTrain: epoch  1, batch    84 | loss: 287.6128793CurrentTrain: epoch  1, batch    85 | loss: 218.3029691CurrentTrain: epoch  1, batch    86 | loss: 207.4905317CurrentTrain: epoch  1, batch    87 | loss: 206.6705674CurrentTrain: epoch  1, batch    88 | loss: 166.1104592CurrentTrain: epoch  1, batch    89 | loss: 172.8423653CurrentTrain: epoch  1, batch    90 | loss: 180.7158223CurrentTrain: epoch  1, batch    91 | loss: 292.6747344CurrentTrain: epoch  1, batch    92 | loss: 203.6485626CurrentTrain: epoch  1, batch    93 | loss: 184.3725948CurrentTrain: epoch  1, batch    94 | loss: 250.2468045CurrentTrain: epoch  1, batch    95 | loss: 170.9145603CurrentTrain: epoch  2, batch     0 | loss: 205.2172920CurrentTrain: epoch  2, batch     1 | loss: 242.0764835CurrentTrain: epoch  2, batch     2 | loss: 240.6022267CurrentTrain: epoch  2, batch     3 | loss: 168.5951875CurrentTrain: epoch  2, batch     4 | loss: 206.4032663CurrentTrain: epoch  2, batch     5 | loss: 177.8898446CurrentTrain: epoch  2, batch     6 | loss: 198.4891570CurrentTrain: epoch  2, batch     7 | loss: 223.7036279CurrentTrain: epoch  2, batch     8 | loss: 222.9086018CurrentTrain: epoch  2, batch     9 | loss: 213.9070609CurrentTrain: epoch  2, batch    10 | loss: 277.3655437CurrentTrain: epoch  2, batch    11 | loss: 174.5578507CurrentTrain: epoch  2, batch    12 | loss: 209.5574886CurrentTrain: epoch  2, batch    13 | loss: 240.2650689CurrentTrain: epoch  2, batch    14 | loss: 214.9236671CurrentTrain: epoch  2, batch    15 | loss: 171.4935998CurrentTrain: epoch  2, batch    16 | loss: 278.4463027CurrentTrain: epoch  2, batch    17 | loss: 179.2671795CurrentTrain: epoch  2, batch    18 | loss: 209.2055389CurrentTrain: epoch  2, batch    19 | loss: 172.3744678CurrentTrain: epoch  2, batch    20 | loss: 205.8640165CurrentTrain: epoch  2, batch    21 | loss: 184.2763019CurrentTrain: epoch  2, batch    22 | loss: 260.2799570CurrentTrain: epoch  2, batch    23 | loss: 202.5621775CurrentTrain: epoch  2, batch    24 | loss: 180.0128693CurrentTrain: epoch  2, batch    25 | loss: 242.9266489CurrentTrain: epoch  2, batch    26 | loss: 180.8097943CurrentTrain: epoch  2, batch    27 | loss: 225.1297712CurrentTrain: epoch  2, batch    28 | loss: 223.6606615CurrentTrain: epoch  2, batch    29 | loss: 292.6502612CurrentTrain: epoch  2, batch    30 | loss: 178.0110306CurrentTrain: epoch  2, batch    31 | loss: 178.0572635CurrentTrain: epoch  2, batch    32 | loss: 213.5093577CurrentTrain: epoch  2, batch    33 | loss: 268.4888560CurrentTrain: epoch  2, batch    34 | loss: 166.1710130CurrentTrain: epoch  2, batch    35 | loss: 178.8141617CurrentTrain: epoch  2, batch    36 | loss: 239.3461421CurrentTrain: epoch  2, batch    37 | loss: 214.7706726CurrentTrain: epoch  2, batch    38 | loss: 154.2265379CurrentTrain: epoch  2, batch    39 | loss: 222.0929379CurrentTrain: epoch  2, batch    40 | loss: 186.3018508CurrentTrain: epoch  2, batch    41 | loss: 147.4285654CurrentTrain: epoch  2, batch    42 | loss: 240.9443687CurrentTrain: epoch  2, batch    43 | loss: 193.8905907CurrentTrain: epoch  2, batch    44 | loss: 246.9944391CurrentTrain: epoch  2, batch    45 | loss: 174.5273745CurrentTrain: epoch  2, batch    46 | loss: 213.8047769CurrentTrain: epoch  2, batch    47 | loss: 163.0004972CurrentTrain: epoch  2, batch    48 | loss: 204.8577781CurrentTrain: epoch  2, batch    49 | loss: 187.5466845CurrentTrain: epoch  2, batch    50 | loss: 229.5128682CurrentTrain: epoch  2, batch    51 | loss: 193.3313989CurrentTrain: epoch  2, batch    52 | loss: 178.2960214CurrentTrain: epoch  2, batch    53 | loss: 196.4833582CurrentTrain: epoch  2, batch    54 | loss: 215.8258216CurrentTrain: epoch  2, batch    55 | loss: 211.4659422CurrentTrain: epoch  2, batch    56 | loss: 217.5120740CurrentTrain: epoch  2, batch    57 | loss: 248.8376228CurrentTrain: epoch  2, batch    58 | loss: 196.5132081CurrentTrain: epoch  2, batch    59 | loss: 230.5376537CurrentTrain: epoch  2, batch    60 | loss: 163.2561916CurrentTrain: epoch  2, batch    61 | loss: 223.9893970CurrentTrain: epoch  2, batch    62 | loss: 137.7099661CurrentTrain: epoch  2, batch    63 | loss: 174.9952096CurrentTrain: epoch  2, batch    64 | loss: 169.7762721CurrentTrain: epoch  2, batch    65 | loss: 250.0466243CurrentTrain: epoch  2, batch    66 | loss: 239.5140819CurrentTrain: epoch  2, batch    67 | loss: 243.0454811CurrentTrain: epoch  2, batch    68 | loss: 198.0508080CurrentTrain: epoch  2, batch    69 | loss: 227.7234922CurrentTrain: epoch  2, batch    70 | loss: 179.7800580CurrentTrain: epoch  2, batch    71 | loss: 239.6998204CurrentTrain: epoch  2, batch    72 | loss: 277.3859239CurrentTrain: epoch  2, batch    73 | loss: 251.9025027CurrentTrain: epoch  2, batch    74 | loss: 210.3876431CurrentTrain: epoch  2, batch    75 | loss: 217.1544038CurrentTrain: epoch  2, batch    76 | loss: 216.1222640CurrentTrain: epoch  2, batch    77 | loss: 212.6633259CurrentTrain: epoch  2, batch    78 | loss: 197.3996391CurrentTrain: epoch  2, batch    79 | loss: 166.8982418CurrentTrain: epoch  2, batch    80 | loss: 199.4571247CurrentTrain: epoch  2, batch    81 | loss: 211.8247013CurrentTrain: epoch  2, batch    82 | loss: 206.4339873CurrentTrain: epoch  2, batch    83 | loss: 239.2343075CurrentTrain: epoch  2, batch    84 | loss: 203.5561449CurrentTrain: epoch  2, batch    85 | loss: 189.6768802CurrentTrain: epoch  2, batch    86 | loss: 207.6157315CurrentTrain: epoch  2, batch    87 | loss: 223.5526351CurrentTrain: epoch  2, batch    88 | loss: 208.3674734CurrentTrain: epoch  2, batch    89 | loss: 189.8404550CurrentTrain: epoch  2, batch    90 | loss: 186.4726847CurrentTrain: epoch  2, batch    91 | loss: 222.3815004CurrentTrain: epoch  2, batch    92 | loss: 216.2042807CurrentTrain: epoch  2, batch    93 | loss: 223.0750151CurrentTrain: epoch  2, batch    94 | loss: 179.9573037CurrentTrain: epoch  2, batch    95 | loss: 228.8606178CurrentTrain: epoch  3, batch     0 | loss: 179.6430900CurrentTrain: epoch  3, batch     1 | loss: 221.7489613CurrentTrain: epoch  3, batch     2 | loss: 178.1300182CurrentTrain: epoch  3, batch     3 | loss: 186.3447308CurrentTrain: epoch  3, batch     4 | loss: 250.5007242CurrentTrain: epoch  3, batch     5 | loss: 200.4921977CurrentTrain: epoch  3, batch     6 | loss: 170.4753749CurrentTrain: epoch  3, batch     7 | loss: 220.9844847CurrentTrain: epoch  3, batch     8 | loss: 240.1547470CurrentTrain: epoch  3, batch     9 | loss: 221.2280283CurrentTrain: epoch  3, batch    10 | loss: 169.5400899CurrentTrain: epoch  3, batch    11 | loss: 215.4183770CurrentTrain: epoch  3, batch    12 | loss: 156.6399937CurrentTrain: epoch  3, batch    13 | loss: 204.6273593CurrentTrain: epoch  3, batch    14 | loss: 222.7741474CurrentTrain: epoch  3, batch    15 | loss: 214.3430536CurrentTrain: epoch  3, batch    16 | loss: 201.1480613CurrentTrain: epoch  3, batch    17 | loss: 288.6375334CurrentTrain: epoch  3, batch    18 | loss: 156.3504970CurrentTrain: epoch  3, batch    19 | loss: 192.7477777CurrentTrain: epoch  3, batch    20 | loss: 213.5245011CurrentTrain: epoch  3, batch    21 | loss: 179.7903802CurrentTrain: epoch  3, batch    22 | loss: 204.8566951CurrentTrain: epoch  3, batch    23 | loss: 214.8352261CurrentTrain: epoch  3, batch    24 | loss: 203.4440162CurrentTrain: epoch  3, batch    25 | loss: 258.5014358CurrentTrain: epoch  3, batch    26 | loss: 213.6385691CurrentTrain: epoch  3, batch    27 | loss: 224.1741559CurrentTrain: epoch  3, batch    28 | loss: 194.3606604CurrentTrain: epoch  3, batch    29 | loss: 222.4591854CurrentTrain: epoch  3, batch    30 | loss: 177.2212229CurrentTrain: epoch  3, batch    31 | loss: 286.9739055CurrentTrain: epoch  3, batch    32 | loss: 222.1019947CurrentTrain: epoch  3, batch    33 | loss: 193.5669879CurrentTrain: epoch  3, batch    34 | loss: 231.7791808CurrentTrain: epoch  3, batch    35 | loss: 186.6676043CurrentTrain: epoch  3, batch    36 | loss: 177.8412363CurrentTrain: epoch  3, batch    37 | loss: 213.2980426CurrentTrain: epoch  3, batch    38 | loss: 214.8305298CurrentTrain: epoch  3, batch    39 | loss: 193.8268996CurrentTrain: epoch  3, batch    40 | loss: 207.8357459CurrentTrain: epoch  3, batch    41 | loss: 221.3510900CurrentTrain: epoch  3, batch    42 | loss: 223.4329907CurrentTrain: epoch  3, batch    43 | loss: 225.4357652CurrentTrain: epoch  3, batch    44 | loss: 218.7732185CurrentTrain: epoch  3, batch    45 | loss: 199.9022453CurrentTrain: epoch  3, batch    46 | loss: 276.3806557CurrentTrain: epoch  3, batch    47 | loss: 179.7669518CurrentTrain: epoch  3, batch    48 | loss: 212.0807503CurrentTrain: epoch  3, batch    49 | loss: 179.8787176CurrentTrain: epoch  3, batch    50 | loss: 178.3416587CurrentTrain: epoch  3, batch    51 | loss: 195.9779634CurrentTrain: epoch  3, batch    52 | loss: 241.3683418CurrentTrain: epoch  3, batch    53 | loss: 204.3971710CurrentTrain: epoch  3, batch    54 | loss: 170.8046707CurrentTrain: epoch  3, batch    55 | loss: 195.8301013CurrentTrain: epoch  3, batch    56 | loss: 212.8822322CurrentTrain: epoch  3, batch    57 | loss: 170.8550312CurrentTrain: epoch  3, batch    58 | loss: 198.0256914CurrentTrain: epoch  3, batch    59 | loss: 184.0285474CurrentTrain: epoch  3, batch    60 | loss: 157.8293812CurrentTrain: epoch  3, batch    61 | loss: 204.6745783CurrentTrain: epoch  3, batch    62 | loss: 249.6143466CurrentTrain: epoch  3, batch    63 | loss: 202.1152596CurrentTrain: epoch  3, batch    64 | loss: 221.8661716CurrentTrain: epoch  3, batch    65 | loss: 187.5542688CurrentTrain: epoch  3, batch    66 | loss: 177.9840297CurrentTrain: epoch  3, batch    67 | loss: 241.8775997CurrentTrain: epoch  3, batch    68 | loss: 248.8599454CurrentTrain: epoch  3, batch    69 | loss: 204.9630285CurrentTrain: epoch  3, batch    70 | loss: 143.7905280CurrentTrain: epoch  3, batch    71 | loss: 179.8444429CurrentTrain: epoch  3, batch    72 | loss: 247.9941430CurrentTrain: epoch  3, batch    73 | loss: 222.9528332CurrentTrain: epoch  3, batch    74 | loss: 177.7817619CurrentTrain: epoch  3, batch    75 | loss: 274.3601786CurrentTrain: epoch  3, batch    76 | loss: 179.4580325CurrentTrain: epoch  3, batch    77 | loss: 175.4826548CurrentTrain: epoch  3, batch    78 | loss: 185.6952598CurrentTrain: epoch  3, batch    79 | loss: 221.0427227CurrentTrain: epoch  3, batch    80 | loss: 249.4896082CurrentTrain: epoch  3, batch    81 | loss: 247.0095142CurrentTrain: epoch  3, batch    82 | loss: 248.1929573CurrentTrain: epoch  3, batch    83 | loss: 288.9198010CurrentTrain: epoch  3, batch    84 | loss: 231.5809866CurrentTrain: epoch  3, batch    85 | loss: 202.5926997CurrentTrain: epoch  3, batch    86 | loss: 185.2080614CurrentTrain: epoch  3, batch    87 | loss: 188.2625431CurrentTrain: epoch  3, batch    88 | loss: 155.3027067CurrentTrain: epoch  3, batch    89 | loss: 176.7860664CurrentTrain: epoch  3, batch    90 | loss: 177.3040690CurrentTrain: epoch  3, batch    91 | loss: 237.8462457CurrentTrain: epoch  3, batch    92 | loss: 187.2552173CurrentTrain: epoch  3, batch    93 | loss: 185.7314936CurrentTrain: epoch  3, batch    94 | loss: 184.2857005CurrentTrain: epoch  3, batch    95 | loss: 173.3612175CurrentTrain: epoch  4, batch     0 | loss: 184.7430701CurrentTrain: epoch  4, batch     1 | loss: 200.0669378CurrentTrain: epoch  4, batch     2 | loss: 147.5224397CurrentTrain: epoch  4, batch     3 | loss: 239.1513003CurrentTrain: epoch  4, batch     4 | loss: 204.5979472CurrentTrain: epoch  4, batch     5 | loss: 203.6767787CurrentTrain: epoch  4, batch     6 | loss: 287.5211503CurrentTrain: epoch  4, batch     7 | loss: 240.7592609CurrentTrain: epoch  4, batch     8 | loss: 231.1221225CurrentTrain: epoch  4, batch     9 | loss: 286.4655776CurrentTrain: epoch  4, batch    10 | loss: 161.1229779CurrentTrain: epoch  4, batch    11 | loss: 201.6213779CurrentTrain: epoch  4, batch    12 | loss: 182.4435210CurrentTrain: epoch  4, batch    13 | loss: 196.7820709CurrentTrain: epoch  4, batch    14 | loss: 211.4555391CurrentTrain: epoch  4, batch    15 | loss: 194.4628204CurrentTrain: epoch  4, batch    16 | loss: 277.3290249CurrentTrain: epoch  4, batch    17 | loss: 185.3720658CurrentTrain: epoch  4, batch    18 | loss: 229.8636009CurrentTrain: epoch  4, batch    19 | loss: 178.9765815CurrentTrain: epoch  4, batch    20 | loss: 191.4806918CurrentTrain: epoch  4, batch    21 | loss: 195.3841775CurrentTrain: epoch  4, batch    22 | loss: 170.3474165CurrentTrain: epoch  4, batch    23 | loss: 260.2487576CurrentTrain: epoch  4, batch    24 | loss: 212.7468327CurrentTrain: epoch  4, batch    25 | loss: 277.2389761CurrentTrain: epoch  4, batch    26 | loss: 187.5413751CurrentTrain: epoch  4, batch    27 | loss: 169.0125061CurrentTrain: epoch  4, batch    28 | loss: 168.7077615CurrentTrain: epoch  4, batch    29 | loss: 349.0349078CurrentTrain: epoch  4, batch    30 | loss: 185.9198434CurrentTrain: epoch  4, batch    31 | loss: 176.5825847CurrentTrain: epoch  4, batch    32 | loss: 143.5879631CurrentTrain: epoch  4, batch    33 | loss: 221.1446760CurrentTrain: epoch  4, batch    34 | loss: 135.1658344CurrentTrain: epoch  4, batch    35 | loss: 155.8177557CurrentTrain: epoch  4, batch    36 | loss: 276.2379745CurrentTrain: epoch  4, batch    37 | loss: 220.8688721CurrentTrain: epoch  4, batch    38 | loss: 238.6738993CurrentTrain: epoch  4, batch    39 | loss: 238.9343806CurrentTrain: epoch  4, batch    40 | loss: 203.7520260CurrentTrain: epoch  4, batch    41 | loss: 203.8050770CurrentTrain: epoch  4, batch    42 | loss: 169.2667603CurrentTrain: epoch  4, batch    43 | loss: 221.1598669CurrentTrain: epoch  4, batch    44 | loss: 212.0296616CurrentTrain: epoch  4, batch    45 | loss: 186.4627781CurrentTrain: epoch  4, batch    46 | loss: 237.4243006CurrentTrain: epoch  4, batch    47 | loss: 184.0818435CurrentTrain: epoch  4, batch    48 | loss: 147.1021500CurrentTrain: epoch  4, batch    49 | loss: 211.4432595CurrentTrain: epoch  4, batch    50 | loss: 221.0988196CurrentTrain: epoch  4, batch    51 | loss: 247.7465422CurrentTrain: epoch  4, batch    52 | loss: 237.1675742CurrentTrain: epoch  4, batch    53 | loss: 203.0469939CurrentTrain: epoch  4, batch    54 | loss: 187.8261692CurrentTrain: epoch  4, batch    55 | loss: 184.5249194CurrentTrain: epoch  4, batch    56 | loss: 167.7471797CurrentTrain: epoch  4, batch    57 | loss: 195.8660789CurrentTrain: epoch  4, batch    58 | loss: 191.9549818CurrentTrain: epoch  4, batch    59 | loss: 206.7131473CurrentTrain: epoch  4, batch    60 | loss: 163.8913169CurrentTrain: epoch  4, batch    61 | loss: 287.1790680CurrentTrain: epoch  4, batch    62 | loss: 162.0774490CurrentTrain: epoch  4, batch    63 | loss: 229.7413825CurrentTrain: epoch  4, batch    64 | loss: 229.4048702CurrentTrain: epoch  4, batch    65 | loss: 170.7679930CurrentTrain: epoch  4, batch    66 | loss: 197.5716170CurrentTrain: epoch  4, batch    67 | loss: 221.2556761CurrentTrain: epoch  4, batch    68 | loss: 139.2344191CurrentTrain: epoch  4, batch    69 | loss: 204.3968235CurrentTrain: epoch  4, batch    70 | loss: 147.3852187CurrentTrain: epoch  4, batch    71 | loss: 219.4677632CurrentTrain: epoch  4, batch    72 | loss: 190.7602774CurrentTrain: epoch  4, batch    73 | loss: 243.5637458CurrentTrain: epoch  4, batch    74 | loss: 191.4807455CurrentTrain: epoch  4, batch    75 | loss: 202.9042722CurrentTrain: epoch  4, batch    76 | loss: 233.3562195CurrentTrain: epoch  4, batch    77 | loss: 267.2123305CurrentTrain: epoch  4, batch    78 | loss: 208.5991884CurrentTrain: epoch  4, batch    79 | loss: 191.7248322CurrentTrain: epoch  4, batch    80 | loss: 217.9723705CurrentTrain: epoch  4, batch    81 | loss: 171.0707766CurrentTrain: epoch  4, batch    82 | loss: 242.0126540CurrentTrain: epoch  4, batch    83 | loss: 228.7892357CurrentTrain: epoch  4, batch    84 | loss: 207.6061076CurrentTrain: epoch  4, batch    85 | loss: 204.3041792CurrentTrain: epoch  4, batch    86 | loss: 203.8178738CurrentTrain: epoch  4, batch    87 | loss: 195.9731220CurrentTrain: epoch  4, batch    88 | loss: 179.5636985CurrentTrain: epoch  4, batch    89 | loss: 205.7132048CurrentTrain: epoch  4, batch    90 | loss: 231.4122071CurrentTrain: epoch  4, batch    91 | loss: 174.9371743CurrentTrain: epoch  4, batch    92 | loss: 288.1842867CurrentTrain: epoch  4, batch    93 | loss: 247.2807034CurrentTrain: epoch  4, batch    94 | loss: 213.2002476CurrentTrain: epoch  4, batch    95 | loss: 156.2889331CurrentTrain: epoch  5, batch     0 | loss: 176.7939624CurrentTrain: epoch  5, batch     1 | loss: 170.2432858CurrentTrain: epoch  5, batch     2 | loss: 195.4727921CurrentTrain: epoch  5, batch     3 | loss: 161.6237279CurrentTrain: epoch  5, batch     4 | loss: 188.7507586CurrentTrain: epoch  5, batch     5 | loss: 266.8233375CurrentTrain: epoch  5, batch     6 | loss: 203.9566140CurrentTrain: epoch  5, batch     7 | loss: 196.8522100CurrentTrain: epoch  5, batch     8 | loss: 213.5368700CurrentTrain: epoch  5, batch     9 | loss: 188.2499442CurrentTrain: epoch  5, batch    10 | loss: 206.2044662CurrentTrain: epoch  5, batch    11 | loss: 211.3756311CurrentTrain: epoch  5, batch    12 | loss: 212.1377888CurrentTrain: epoch  5, batch    13 | loss: 220.4269909CurrentTrain: epoch  5, batch    14 | loss: 192.7196322CurrentTrain: epoch  5, batch    15 | loss: 228.4045680CurrentTrain: epoch  5, batch    16 | loss: 186.3806796CurrentTrain: epoch  5, batch    17 | loss: 184.6175220CurrentTrain: epoch  5, batch    18 | loss: 212.4186558CurrentTrain: epoch  5, batch    19 | loss: 211.1785718CurrentTrain: epoch  5, batch    20 | loss: 186.0151819CurrentTrain: epoch  5, batch    21 | loss: 206.2996284CurrentTrain: epoch  5, batch    22 | loss: 175.7213150CurrentTrain: epoch  5, batch    23 | loss: 168.4899855CurrentTrain: epoch  5, batch    24 | loss: 220.3950240CurrentTrain: epoch  5, batch    25 | loss: 286.8233735CurrentTrain: epoch  5, batch    26 | loss: 202.8139117CurrentTrain: epoch  5, batch    27 | loss: 175.4660079CurrentTrain: epoch  5, batch    28 | loss: 237.6471492CurrentTrain: epoch  5, batch    29 | loss: 203.4129064CurrentTrain: epoch  5, batch    30 | loss: 203.0994021CurrentTrain: epoch  5, batch    31 | loss: 211.8618514CurrentTrain: epoch  5, batch    32 | loss: 230.4428142CurrentTrain: epoch  5, batch    33 | loss: 183.0009396CurrentTrain: epoch  5, batch    34 | loss: 194.1107690CurrentTrain: epoch  5, batch    35 | loss: 185.9098711CurrentTrain: epoch  5, batch    36 | loss: 153.8248163CurrentTrain: epoch  5, batch    37 | loss: 220.3959571CurrentTrain: epoch  5, batch    38 | loss: 212.1903969CurrentTrain: epoch  5, batch    39 | loss: 206.2405165CurrentTrain: epoch  5, batch    40 | loss: 240.2126601CurrentTrain: epoch  5, batch    41 | loss: 181.0704281CurrentTrain: epoch  5, batch    42 | loss: 202.7831470CurrentTrain: epoch  5, batch    43 | loss: 268.1718218CurrentTrain: epoch  5, batch    44 | loss: 240.9626794CurrentTrain: epoch  5, batch    45 | loss: 174.6868164CurrentTrain: epoch  5, batch    46 | loss: 192.7235568CurrentTrain: epoch  5, batch    47 | loss: 211.5667938CurrentTrain: epoch  5, batch    48 | loss: 202.4592228CurrentTrain: epoch  5, batch    49 | loss: 184.4816522CurrentTrain: epoch  5, batch    50 | loss: 145.9857831CurrentTrain: epoch  5, batch    51 | loss: 237.6963478CurrentTrain: epoch  5, batch    52 | loss: 246.9048012CurrentTrain: epoch  5, batch    53 | loss: 187.7898941CurrentTrain: epoch  5, batch    54 | loss: 212.6201919CurrentTrain: epoch  5, batch    55 | loss: 194.4358468CurrentTrain: epoch  5, batch    56 | loss: 228.3831433CurrentTrain: epoch  5, batch    57 | loss: 187.8995490CurrentTrain: epoch  5, batch    58 | loss: 275.9732591CurrentTrain: epoch  5, batch    59 | loss: 230.3827880CurrentTrain: epoch  5, batch    60 | loss: 227.7601013CurrentTrain: epoch  5, batch    61 | loss: 186.4412350CurrentTrain: epoch  5, batch    62 | loss: 167.9159542CurrentTrain: epoch  5, batch    63 | loss: 213.9268367CurrentTrain: epoch  5, batch    64 | loss: 221.3519272CurrentTrain: epoch  5, batch    65 | loss: 183.9166135CurrentTrain: epoch  5, batch    66 | loss: 154.1801614CurrentTrain: epoch  5, batch    67 | loss: 219.9764743CurrentTrain: epoch  5, batch    68 | loss: 203.2569193CurrentTrain: epoch  5, batch    69 | loss: 212.4422455CurrentTrain: epoch  5, batch    70 | loss: 213.1482436CurrentTrain: epoch  5, batch    71 | loss: 248.4883600CurrentTrain: epoch  5, batch    72 | loss: 220.1728772CurrentTrain: epoch  5, batch    73 | loss: 220.2973365CurrentTrain: epoch  5, batch    74 | loss: 161.8464213CurrentTrain: epoch  5, batch    75 | loss: 221.5233737CurrentTrain: epoch  5, batch    76 | loss: 209.7928697CurrentTrain: epoch  5, batch    77 | loss: 161.5875334CurrentTrain: epoch  5, batch    78 | loss: 211.1274442CurrentTrain: epoch  5, batch    79 | loss: 178.9825086CurrentTrain: epoch  5, batch    80 | loss: 192.2303112CurrentTrain: epoch  5, batch    81 | loss: 239.8130779CurrentTrain: epoch  5, batch    82 | loss: 169.2485887CurrentTrain: epoch  5, batch    83 | loss: 169.1553937CurrentTrain: epoch  5, batch    84 | loss: 202.9753097CurrentTrain: epoch  5, batch    85 | loss: 176.0958725CurrentTrain: epoch  5, batch    86 | loss: 240.5398357CurrentTrain: epoch  5, batch    87 | loss: 175.2523269CurrentTrain: epoch  5, batch    88 | loss: 287.7757429CurrentTrain: epoch  5, batch    89 | loss: 237.5912378CurrentTrain: epoch  5, batch    90 | loss: 159.4471908CurrentTrain: epoch  5, batch    91 | loss: 221.6000810CurrentTrain: epoch  5, batch    92 | loss: 170.4959527CurrentTrain: epoch  5, batch    93 | loss: 270.0354103CurrentTrain: epoch  5, batch    94 | loss: 237.6708145CurrentTrain: epoch  5, batch    95 | loss: 156.5772656CurrentTrain: epoch  6, batch     0 | loss: 228.3574270CurrentTrain: epoch  6, batch     1 | loss: 212.1760765CurrentTrain: epoch  6, batch     2 | loss: 246.5139672CurrentTrain: epoch  6, batch     3 | loss: 229.0449877CurrentTrain: epoch  6, batch     4 | loss: 202.6706072CurrentTrain: epoch  6, batch     5 | loss: 191.9565069CurrentTrain: epoch  6, batch     6 | loss: 237.2199021CurrentTrain: epoch  6, batch     7 | loss: 219.8415769CurrentTrain: epoch  6, batch     8 | loss: 201.9053037CurrentTrain: epoch  6, batch     9 | loss: 191.0303700CurrentTrain: epoch  6, batch    10 | loss: 227.8937486CurrentTrain: epoch  6, batch    11 | loss: 194.2745546CurrentTrain: epoch  6, batch    12 | loss: 286.4372261CurrentTrain: epoch  6, batch    13 | loss: 236.7982603CurrentTrain: epoch  6, batch    14 | loss: 202.9065139CurrentTrain: epoch  6, batch    15 | loss: 266.3913976CurrentTrain: epoch  6, batch    16 | loss: 163.8028921CurrentTrain: epoch  6, batch    17 | loss: 237.1971866CurrentTrain: epoch  6, batch    18 | loss: 167.8413531CurrentTrain: epoch  6, batch    19 | loss: 146.4151450CurrentTrain: epoch  6, batch    20 | loss: 188.9250031CurrentTrain: epoch  6, batch    21 | loss: 183.7558179CurrentTrain: epoch  6, batch    22 | loss: 191.2900894CurrentTrain: epoch  6, batch    23 | loss: 359.2443964CurrentTrain: epoch  6, batch    24 | loss: 178.2604831CurrentTrain: epoch  6, batch    25 | loss: 211.6738969CurrentTrain: epoch  6, batch    26 | loss: 238.7247900CurrentTrain: epoch  6, batch    27 | loss: 359.2791560CurrentTrain: epoch  6, batch    28 | loss: 189.0196434CurrentTrain: epoch  6, batch    29 | loss: 214.1344720CurrentTrain: epoch  6, batch    30 | loss: 204.2829477CurrentTrain: epoch  6, batch    31 | loss: 175.0996949CurrentTrain: epoch  6, batch    32 | loss: 266.5108019CurrentTrain: epoch  6, batch    33 | loss: 210.9624537CurrentTrain: epoch  6, batch    34 | loss: 220.0854505CurrentTrain: epoch  6, batch    35 | loss: 359.5244975CurrentTrain: epoch  6, batch    36 | loss: 160.1568547CurrentTrain: epoch  6, batch    37 | loss: 220.0844504CurrentTrain: epoch  6, batch    38 | loss: 233.2820602CurrentTrain: epoch  6, batch    39 | loss: 200.3636857CurrentTrain: epoch  6, batch    40 | loss: 175.1204979CurrentTrain: epoch  6, batch    41 | loss: 155.9362723CurrentTrain: epoch  6, batch    42 | loss: 203.8651043CurrentTrain: epoch  6, batch    43 | loss: 203.4629586CurrentTrain: epoch  6, batch    44 | loss: 228.4899508CurrentTrain: epoch  6, batch    45 | loss: 228.8405216CurrentTrain: epoch  6, batch    46 | loss: 197.3178828CurrentTrain: epoch  6, batch    47 | loss: 187.0751853CurrentTrain: epoch  6, batch    48 | loss: 191.9926002CurrentTrain: epoch  6, batch    49 | loss: 199.5570594CurrentTrain: epoch  6, batch    50 | loss: 160.2634513CurrentTrain: epoch  6, batch    51 | loss: 268.5370007CurrentTrain: epoch  6, batch    52 | loss: 219.7398295CurrentTrain: epoch  6, batch    53 | loss: 169.9145437CurrentTrain: epoch  6, batch    54 | loss: 219.6456845CurrentTrain: epoch  6, batch    55 | loss: 203.8018690CurrentTrain: epoch  6, batch    56 | loss: 190.8756361CurrentTrain: epoch  6, batch    57 | loss: 170.8689680CurrentTrain: epoch  6, batch    58 | loss: 265.9770144CurrentTrain: epoch  6, batch    59 | loss: 198.7323664CurrentTrain: epoch  6, batch    60 | loss: 219.7463504CurrentTrain: epoch  6, batch    61 | loss: 176.6671472CurrentTrain: epoch  6, batch    62 | loss: 194.4459024CurrentTrain: epoch  6, batch    63 | loss: 229.3570653CurrentTrain: epoch  6, batch    64 | loss: 230.0711659CurrentTrain: epoch  6, batch    65 | loss: 185.3748743CurrentTrain: epoch  6, batch    66 | loss: 286.3696227CurrentTrain: epoch  6, batch    67 | loss: 163.9812062CurrentTrain: epoch  6, batch    68 | loss: 160.1591705CurrentTrain: epoch  6, batch    69 | loss: 182.8425970CurrentTrain: epoch  6, batch    70 | loss: 159.9800182CurrentTrain: epoch  6, batch    71 | loss: 194.6536804CurrentTrain: epoch  6, batch    72 | loss: 246.6089750CurrentTrain: epoch  6, batch    73 | loss: 257.2247578CurrentTrain: epoch  6, batch    74 | loss: 178.8432792CurrentTrain: epoch  6, batch    75 | loss: 183.2974612CurrentTrain: epoch  6, batch    76 | loss: 195.1383813CurrentTrain: epoch  6, batch    77 | loss: 258.7228102CurrentTrain: epoch  6, batch    78 | loss: 286.2978448CurrentTrain: epoch  6, batch    79 | loss: 210.6977395CurrentTrain: epoch  6, batch    80 | loss: 168.8413701CurrentTrain: epoch  6, batch    81 | loss: 224.8533853CurrentTrain: epoch  6, batch    82 | loss: 161.0368142CurrentTrain: epoch  6, batch    83 | loss: 196.3779741CurrentTrain: epoch  6, batch    84 | loss: 219.0790050CurrentTrain: epoch  6, batch    85 | loss: 183.7236219CurrentTrain: epoch  6, batch    86 | loss: 179.0088750CurrentTrain: epoch  6, batch    87 | loss: 154.8363435CurrentTrain: epoch  6, batch    88 | loss: 153.4061862CurrentTrain: epoch  6, batch    89 | loss: 268.7238711CurrentTrain: epoch  6, batch    90 | loss: 346.5915060CurrentTrain: epoch  6, batch    91 | loss: 146.4718662CurrentTrain: epoch  6, batch    92 | loss: 204.7509726CurrentTrain: epoch  6, batch    93 | loss: 202.7618745CurrentTrain: epoch  6, batch    94 | loss: 160.1678446CurrentTrain: epoch  6, batch    95 | loss: 140.7167568CurrentTrain: epoch  7, batch     0 | loss: 146.7109867CurrentTrain: epoch  7, batch     1 | loss: 335.6268279CurrentTrain: epoch  7, batch     2 | loss: 167.5037185CurrentTrain: epoch  7, batch     3 | loss: 196.6196708CurrentTrain: epoch  7, batch     4 | loss: 190.9055909CurrentTrain: epoch  7, batch     5 | loss: 346.9623145CurrentTrain: epoch  7, batch     6 | loss: 276.4413917CurrentTrain: epoch  7, batch     7 | loss: 210.8967910CurrentTrain: epoch  7, batch     8 | loss: 221.1692403CurrentTrain: epoch  7, batch     9 | loss: 210.7439949CurrentTrain: epoch  7, batch    10 | loss: 175.7343279CurrentTrain: epoch  7, batch    11 | loss: 237.5162992CurrentTrain: epoch  7, batch    12 | loss: 160.9052948CurrentTrain: epoch  7, batch    13 | loss: 276.6392235CurrentTrain: epoch  7, batch    14 | loss: 227.8191341CurrentTrain: epoch  7, batch    15 | loss: 267.0588627CurrentTrain: epoch  7, batch    16 | loss: 220.3320142CurrentTrain: epoch  7, batch    17 | loss: 210.9354023CurrentTrain: epoch  7, batch    18 | loss: 182.9411551CurrentTrain: epoch  7, batch    19 | loss: 179.1540075CurrentTrain: epoch  7, batch    20 | loss: 268.3582872CurrentTrain: epoch  7, batch    21 | loss: 237.1823196CurrentTrain: epoch  7, batch    22 | loss: 196.1802885CurrentTrain: epoch  7, batch    23 | loss: 199.5414758CurrentTrain: epoch  7, batch    24 | loss: 185.5902482CurrentTrain: epoch  7, batch    25 | loss: 166.8918545CurrentTrain: epoch  7, batch    26 | loss: 220.3445116CurrentTrain: epoch  7, batch    27 | loss: 167.2850241CurrentTrain: epoch  7, batch    28 | loss: 246.6170275CurrentTrain: epoch  7, batch    29 | loss: 194.4669966CurrentTrain: epoch  7, batch    30 | loss: 257.4820416CurrentTrain: epoch  7, batch    31 | loss: 204.6641369CurrentTrain: epoch  7, batch    32 | loss: 167.0432913CurrentTrain: epoch  7, batch    33 | loss: 210.6094323CurrentTrain: epoch  7, batch    34 | loss: 159.9535619CurrentTrain: epoch  7, batch    35 | loss: 194.0217380CurrentTrain: epoch  7, batch    36 | loss: 138.7350975CurrentTrain: epoch  7, batch    37 | loss: 174.8448098CurrentTrain: epoch  7, batch    38 | loss: 190.6490694CurrentTrain: epoch  7, batch    39 | loss: 175.2447201CurrentTrain: epoch  7, batch    40 | loss: 228.2995245CurrentTrain: epoch  7, batch    41 | loss: 204.4021386CurrentTrain: epoch  7, batch    42 | loss: 152.8202519CurrentTrain: epoch  7, batch    43 | loss: 316.5896708CurrentTrain: epoch  7, batch    44 | loss: 211.1838635CurrentTrain: epoch  7, batch    45 | loss: 230.5210699CurrentTrain: epoch  7, batch    46 | loss: 286.3754872CurrentTrain: epoch  7, batch    47 | loss: 153.6578299CurrentTrain: epoch  7, batch    48 | loss: 219.5823866CurrentTrain: epoch  7, batch    49 | loss: 218.2960879CurrentTrain: epoch  7, batch    50 | loss: 191.3086220CurrentTrain: epoch  7, batch    51 | loss: 193.6742721CurrentTrain: epoch  7, batch    52 | loss: 210.5835669CurrentTrain: epoch  7, batch    53 | loss: 210.7668719CurrentTrain: epoch  7, batch    54 | loss: 190.9645034CurrentTrain: epoch  7, batch    55 | loss: 237.8121731CurrentTrain: epoch  7, batch    56 | loss: 219.7744771CurrentTrain: epoch  7, batch    57 | loss: 211.1254006CurrentTrain: epoch  7, batch    58 | loss: 196.3737348CurrentTrain: epoch  7, batch    59 | loss: 190.7582547CurrentTrain: epoch  7, batch    60 | loss: 181.0753230CurrentTrain: epoch  7, batch    61 | loss: 186.7525127CurrentTrain: epoch  7, batch    62 | loss: 170.7515286CurrentTrain: epoch  7, batch    63 | loss: 267.9376800CurrentTrain: epoch  7, batch    64 | loss: 194.6331824CurrentTrain: epoch  7, batch    65 | loss: 192.1690925CurrentTrain: epoch  7, batch    66 | loss: 219.6609802CurrentTrain: epoch  7, batch    67 | loss: 174.7201837CurrentTrain: epoch  7, batch    68 | loss: 211.9104595CurrentTrain: epoch  7, batch    69 | loss: 158.9486087CurrentTrain: epoch  7, batch    70 | loss: 202.4507090CurrentTrain: epoch  7, batch    71 | loss: 229.0334913CurrentTrain: epoch  7, batch    72 | loss: 246.3912198CurrentTrain: epoch  7, batch    73 | loss: 229.0284726CurrentTrain: epoch  7, batch    74 | loss: 175.3829558CurrentTrain: epoch  7, batch    75 | loss: 202.3483184CurrentTrain: epoch  7, batch    76 | loss: 121.2152555CurrentTrain: epoch  7, batch    77 | loss: 177.6631726CurrentTrain: epoch  7, batch    78 | loss: 202.2390589CurrentTrain: epoch  7, batch    79 | loss: 228.0003848CurrentTrain: epoch  7, batch    80 | loss: 193.8369656CurrentTrain: epoch  7, batch    81 | loss: 174.3765475CurrentTrain: epoch  7, batch    82 | loss: 181.9437078CurrentTrain: epoch  7, batch    83 | loss: 286.7799537CurrentTrain: epoch  7, batch    84 | loss: 239.4460917CurrentTrain: epoch  7, batch    85 | loss: 183.2488681CurrentTrain: epoch  7, batch    86 | loss: 287.6205848CurrentTrain: epoch  7, batch    87 | loss: 183.4937631CurrentTrain: epoch  7, batch    88 | loss: 219.4880943CurrentTrain: epoch  7, batch    89 | loss: 228.9350290CurrentTrain: epoch  7, batch    90 | loss: 211.6746670CurrentTrain: epoch  7, batch    91 | loss: 247.4019774CurrentTrain: epoch  7, batch    92 | loss: 193.5085855CurrentTrain: epoch  7, batch    93 | loss: 159.9127510CurrentTrain: epoch  7, batch    94 | loss: 219.3730075CurrentTrain: epoch  7, batch    95 | loss: 204.3050573CurrentTrain: epoch  8, batch     0 | loss: 176.5652205CurrentTrain: epoch  8, batch     1 | loss: 237.0554781CurrentTrain: epoch  8, batch     2 | loss: 246.4220354CurrentTrain: epoch  8, batch     3 | loss: 185.2155172CurrentTrain: epoch  8, batch     4 | loss: 195.7294213CurrentTrain: epoch  8, batch     5 | loss: 182.5686403CurrentTrain: epoch  8, batch     6 | loss: 171.1106875CurrentTrain: epoch  8, batch     7 | loss: 286.2768464CurrentTrain: epoch  8, batch     8 | loss: 202.2453047CurrentTrain: epoch  8, batch     9 | loss: 202.1801346CurrentTrain: epoch  8, batch    10 | loss: 159.9657782CurrentTrain: epoch  8, batch    11 | loss: 159.5017464CurrentTrain: epoch  8, batch    12 | loss: 201.7546486CurrentTrain: epoch  8, batch    13 | loss: 227.8606775CurrentTrain: epoch  8, batch    14 | loss: 183.3147491CurrentTrain: epoch  8, batch    15 | loss: 211.1110996CurrentTrain: epoch  8, batch    16 | loss: 276.3495683CurrentTrain: epoch  8, batch    17 | loss: 237.0866523CurrentTrain: epoch  8, batch    18 | loss: 185.7887218CurrentTrain: epoch  8, batch    19 | loss: 248.3021225CurrentTrain: epoch  8, batch    20 | loss: 237.0793384CurrentTrain: epoch  8, batch    21 | loss: 204.6089269CurrentTrain: epoch  8, batch    22 | loss: 227.9205683CurrentTrain: epoch  8, batch    23 | loss: 202.1293788CurrentTrain: epoch  8, batch    24 | loss: 175.1116879CurrentTrain: epoch  8, batch    25 | loss: 228.5029722CurrentTrain: epoch  8, batch    26 | loss: 201.7969360CurrentTrain: epoch  8, batch    27 | loss: 194.8525555CurrentTrain: epoch  8, batch    28 | loss: 201.9668582CurrentTrain: epoch  8, batch    29 | loss: 175.4805800CurrentTrain: epoch  8, batch    30 | loss: 182.7163349CurrentTrain: epoch  8, batch    31 | loss: 174.5668895CurrentTrain: epoch  8, batch    32 | loss: 183.1509125CurrentTrain: epoch  8, batch    33 | loss: 202.6291815CurrentTrain: epoch  8, batch    34 | loss: 174.3513618CurrentTrain: epoch  8, batch    35 | loss: 212.5769551CurrentTrain: epoch  8, batch    36 | loss: 175.2917398CurrentTrain: epoch  8, batch    37 | loss: 178.2240012CurrentTrain: epoch  8, batch    38 | loss: 184.6969110CurrentTrain: epoch  8, batch    39 | loss: 190.7971630CurrentTrain: epoch  8, batch    40 | loss: 237.0248989CurrentTrain: epoch  8, batch    41 | loss: 210.6009993CurrentTrain: epoch  8, batch    42 | loss: 276.1590497CurrentTrain: epoch  8, batch    43 | loss: 175.6847902CurrentTrain: epoch  8, batch    44 | loss: 194.7300044CurrentTrain: epoch  8, batch    45 | loss: 212.6183039CurrentTrain: epoch  8, batch    46 | loss: 237.8390990CurrentTrain: epoch  8, batch    47 | loss: 182.2769626CurrentTrain: epoch  8, batch    48 | loss: 192.8915862CurrentTrain: epoch  8, batch    49 | loss: 201.5909035CurrentTrain: epoch  8, batch    50 | loss: 211.2298028CurrentTrain: epoch  8, batch    51 | loss: 210.6799404CurrentTrain: epoch  8, batch    52 | loss: 228.2294568CurrentTrain: epoch  8, batch    53 | loss: 190.5947237CurrentTrain: epoch  8, batch    54 | loss: 190.5551318CurrentTrain: epoch  8, batch    55 | loss: 214.3940741CurrentTrain: epoch  8, batch    56 | loss: 185.7426695CurrentTrain: epoch  8, batch    57 | loss: 185.3275640CurrentTrain: epoch  8, batch    58 | loss: 185.7411495CurrentTrain: epoch  8, batch    59 | loss: 227.5006439CurrentTrain: epoch  8, batch    60 | loss: 161.1831476CurrentTrain: epoch  8, batch    61 | loss: 191.1525572CurrentTrain: epoch  8, batch    62 | loss: 210.6843515CurrentTrain: epoch  8, batch    63 | loss: 174.7102983CurrentTrain: epoch  8, batch    64 | loss: 164.9274925CurrentTrain: epoch  8, batch    65 | loss: 203.2719532CurrentTrain: epoch  8, batch    66 | loss: 211.4475853CurrentTrain: epoch  8, batch    67 | loss: 276.0612556CurrentTrain: epoch  8, batch    68 | loss: 202.7584663CurrentTrain: epoch  8, batch    69 | loss: 175.4634143CurrentTrain: epoch  8, batch    70 | loss: 227.4555273CurrentTrain: epoch  8, batch    71 | loss: 175.9310385CurrentTrain: epoch  8, batch    72 | loss: 190.8555515CurrentTrain: epoch  8, batch    73 | loss: 159.8558046CurrentTrain: epoch  8, batch    74 | loss: 173.4567646CurrentTrain: epoch  8, batch    75 | loss: 236.9683287CurrentTrain: epoch  8, batch    76 | loss: 193.9837241CurrentTrain: epoch  8, batch    77 | loss: 186.3367448CurrentTrain: epoch  8, batch    78 | loss: 203.1681027CurrentTrain: epoch  8, batch    79 | loss: 183.2843899CurrentTrain: epoch  8, batch    80 | loss: 218.8791123CurrentTrain: epoch  8, batch    81 | loss: 276.0800977CurrentTrain: epoch  8, batch    82 | loss: 133.2889479CurrentTrain: epoch  8, batch    83 | loss: 147.3082360CurrentTrain: epoch  8, batch    84 | loss: 237.6759065CurrentTrain: epoch  8, batch    85 | loss: 237.1135493CurrentTrain: epoch  8, batch    86 | loss: 218.7393952CurrentTrain: epoch  8, batch    87 | loss: 237.2515737CurrentTrain: epoch  8, batch    88 | loss: 190.6989259CurrentTrain: epoch  8, batch    89 | loss: 200.0933894CurrentTrain: epoch  8, batch    90 | loss: 218.7155310CurrentTrain: epoch  8, batch    91 | loss: 193.4087614CurrentTrain: epoch  8, batch    92 | loss: 193.9301878CurrentTrain: epoch  8, batch    93 | loss: 219.8339799CurrentTrain: epoch  8, batch    94 | loss: 287.0936411CurrentTrain: epoch  8, batch    95 | loss: 156.3977989CurrentTrain: epoch  9, batch     0 | loss: 185.1775573CurrentTrain: epoch  9, batch     1 | loss: 152.9460611CurrentTrain: epoch  9, batch     2 | loss: 227.5387333CurrentTrain: epoch  9, batch     3 | loss: 202.4930937CurrentTrain: epoch  9, batch     4 | loss: 335.8124281CurrentTrain: epoch  9, batch     5 | loss: 159.6395227CurrentTrain: epoch  9, batch     6 | loss: 228.7902977CurrentTrain: epoch  9, batch     7 | loss: 193.8238577CurrentTrain: epoch  9, batch     8 | loss: 265.7144181CurrentTrain: epoch  9, batch     9 | loss: 201.6596627CurrentTrain: epoch  9, batch    10 | loss: 218.6430639CurrentTrain: epoch  9, batch    11 | loss: 186.8576309CurrentTrain: epoch  9, batch    12 | loss: 359.3496322CurrentTrain: epoch  9, batch    13 | loss: 359.2146021CurrentTrain: epoch  9, batch    14 | loss: 160.3251610CurrentTrain: epoch  9, batch    15 | loss: 276.0063607CurrentTrain: epoch  9, batch    16 | loss: 286.3548056CurrentTrain: epoch  9, batch    17 | loss: 159.9431261CurrentTrain: epoch  9, batch    18 | loss: 161.4056532CurrentTrain: epoch  9, batch    19 | loss: 276.2648365CurrentTrain: epoch  9, batch    20 | loss: 210.8915211CurrentTrain: epoch  9, batch    21 | loss: 202.5743581CurrentTrain: epoch  9, batch    22 | loss: 236.9078337CurrentTrain: epoch  9, batch    23 | loss: 190.5267472CurrentTrain: epoch  9, batch    24 | loss: 228.6244399CurrentTrain: epoch  9, batch    25 | loss: 276.3045426CurrentTrain: epoch  9, batch    26 | loss: 257.5260670CurrentTrain: epoch  9, batch    27 | loss: 175.4320496CurrentTrain: epoch  9, batch    28 | loss: 194.1879677CurrentTrain: epoch  9, batch    29 | loss: 286.2738788CurrentTrain: epoch  9, batch    30 | loss: 209.9123790CurrentTrain: epoch  9, batch    31 | loss: 236.8195150CurrentTrain: epoch  9, batch    32 | loss: 211.0915576CurrentTrain: epoch  9, batch    33 | loss: 227.6457584CurrentTrain: epoch  9, batch    34 | loss: 210.6667751CurrentTrain: epoch  9, batch    35 | loss: 175.0436768CurrentTrain: epoch  9, batch    36 | loss: 183.1667215CurrentTrain: epoch  9, batch    37 | loss: 227.5065979CurrentTrain: epoch  9, batch    38 | loss: 210.3703342CurrentTrain: epoch  9, batch    39 | loss: 182.1100824CurrentTrain: epoch  9, batch    40 | loss: 146.1204645CurrentTrain: epoch  9, batch    41 | loss: 237.2851050CurrentTrain: epoch  9, batch    42 | loss: 227.8316191CurrentTrain: epoch  9, batch    43 | loss: 193.5329707CurrentTrain: epoch  9, batch    44 | loss: 176.4317774CurrentTrain: epoch  9, batch    45 | loss: 161.6689377CurrentTrain: epoch  9, batch    46 | loss: 219.4602970CurrentTrain: epoch  9, batch    47 | loss: 231.9141341CurrentTrain: epoch  9, batch    48 | loss: 202.3410169CurrentTrain: epoch  9, batch    49 | loss: 193.4843092CurrentTrain: epoch  9, batch    50 | loss: 190.5772314CurrentTrain: epoch  9, batch    51 | loss: 175.3901385CurrentTrain: epoch  9, batch    52 | loss: 201.7118960CurrentTrain: epoch  9, batch    53 | loss: 191.1138225CurrentTrain: epoch  9, batch    54 | loss: 238.2020738CurrentTrain: epoch  9, batch    55 | loss: 190.8315519CurrentTrain: epoch  9, batch    56 | loss: 185.0162504CurrentTrain: epoch  9, batch    57 | loss: 159.9049321CurrentTrain: epoch  9, batch    58 | loss: 286.2851087CurrentTrain: epoch  9, batch    59 | loss: 229.0592651CurrentTrain: epoch  9, batch    60 | loss: 159.3722368CurrentTrain: epoch  9, batch    61 | loss: 160.4141358CurrentTrain: epoch  9, batch    62 | loss: 182.8176005CurrentTrain: epoch  9, batch    63 | loss: 167.4508177CurrentTrain: epoch  9, batch    64 | loss: 193.2488231CurrentTrain: epoch  9, batch    65 | loss: 167.4622735CurrentTrain: epoch  9, batch    66 | loss: 166.9228456CurrentTrain: epoch  9, batch    67 | loss: 183.6242955CurrentTrain: epoch  9, batch    68 | loss: 190.5591507CurrentTrain: epoch  9, batch    69 | loss: 152.5015485CurrentTrain: epoch  9, batch    70 | loss: 193.3599591CurrentTrain: epoch  9, batch    71 | loss: 203.3566299CurrentTrain: epoch  9, batch    72 | loss: 187.1447081CurrentTrain: epoch  9, batch    73 | loss: 169.4550402CurrentTrain: epoch  9, batch    74 | loss: 210.8913835CurrentTrain: epoch  9, batch    75 | loss: 237.2565970CurrentTrain: epoch  9, batch    76 | loss: 219.1984642CurrentTrain: epoch  9, batch    77 | loss: 174.5691411CurrentTrain: epoch  9, batch    78 | loss: 178.3647433CurrentTrain: epoch  9, batch    79 | loss: 218.8021250CurrentTrain: epoch  9, batch    80 | loss: 202.3105640CurrentTrain: epoch  9, batch    81 | loss: 159.2510292CurrentTrain: epoch  9, batch    82 | loss: 182.6670316CurrentTrain: epoch  9, batch    83 | loss: 229.4792540CurrentTrain: epoch  9, batch    84 | loss: 201.8263911CurrentTrain: epoch  9, batch    85 | loss: 558.2061138CurrentTrain: epoch  9, batch    86 | loss: 191.2312416CurrentTrain: epoch  9, batch    87 | loss: 191.7216728CurrentTrain: epoch  9, batch    88 | loss: 193.6960514CurrentTrain: epoch  9, batch    89 | loss: 247.2489981CurrentTrain: epoch  9, batch    90 | loss: 218.7358003CurrentTrain: epoch  9, batch    91 | loss: 210.6392782CurrentTrain: epoch  9, batch    92 | loss: 184.9179082CurrentTrain: epoch  9, batch    93 | loss: 218.8335594CurrentTrain: epoch  9, batch    94 | loss: 175.0724982CurrentTrain: epoch  9, batch    95 | loss: 140.6041055

F1 score per class: {32: 0.5194805194805194, 6: 0.8571428571428571, 19: 0.4166666666666667, 24: 0.7513227513227513, 26: 0.907103825136612, 29: 0.8415300546448088}
Micro-average F1 score: 0.7731277533039648
Weighted-average F1 score: 0.7867743569929361
F1 score per class: {32: 0.6235294117647059, 6: 0.8950276243093923, 19: 0.46153846153846156, 24: 0.7578947368421053, 26: 0.9473684210526315, 29: 0.8709677419354839}
Micro-average F1 score: 0.8123011664899258
Weighted-average F1 score: 0.8193259155338254
F1 score per class: {32: 0.6272189349112426, 6: 0.8950276243093923, 19: 0.46153846153846156, 24: 0.7578947368421053, 26: 0.9473684210526315, 29: 0.8663101604278075}
Micro-average F1 score: 0.8123011664899258
Weighted-average F1 score: 0.8195536939717405

F1 score per class: {32: 0.5194805194805194, 6: 0.8571428571428571, 19: 0.4166666666666667, 24: 0.7513227513227513, 26: 0.907103825136612, 29: 0.8415300546448088}
Micro-average F1 score: 0.7731277533039648
Weighted-average F1 score: 0.7867743569929361
F1 score per class: {32: 0.6235294117647059, 6: 0.8950276243093923, 19: 0.46153846153846156, 24: 0.7578947368421053, 26: 0.9473684210526315, 29: 0.8709677419354839}
Micro-average F1 score: 0.8123011664899258
Weighted-average F1 score: 0.8193259155338254
F1 score per class: {32: 0.6272189349112426, 6: 0.8950276243093923, 19: 0.46153846153846156, 24: 0.7578947368421053, 26: 0.9473684210526315, 29: 0.8663101604278075}
Micro-average F1 score: 0.8123011664899258
Weighted-average F1 score: 0.8195536939717405
cur_acc:  ['0.7731']
his_acc:  ['0.7731']
cur_acc des:  ['0.8123']
his_acc des:  ['0.8123']
cur_acc rrf:  ['0.8123']
his_acc rrf:  ['0.8123']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 224.3974012CurrentTrain: epoch  0, batch     1 | loss: 194.4993538CurrentTrain: epoch  0, batch     2 | loss: 251.0083383CurrentTrain: epoch  0, batch     3 | loss: 144.1851020CurrentTrain: epoch  1, batch     0 | loss: 289.3991876CurrentTrain: epoch  1, batch     1 | loss: 181.1059067CurrentTrain: epoch  1, batch     2 | loss: 201.5145876CurrentTrain: epoch  1, batch     3 | loss: 117.0653616CurrentTrain: epoch  2, batch     0 | loss: 227.4013838CurrentTrain: epoch  2, batch     1 | loss: 177.4300291CurrentTrain: epoch  2, batch     2 | loss: 209.9420203CurrentTrain: epoch  2, batch     3 | loss: 161.5076024CurrentTrain: epoch  3, batch     0 | loss: 209.9410159CurrentTrain: epoch  3, batch     1 | loss: 175.5046828CurrentTrain: epoch  3, batch     2 | loss: 227.2816133CurrentTrain: epoch  3, batch     3 | loss: 182.0766078CurrentTrain: epoch  4, batch     0 | loss: 174.6740704CurrentTrain: epoch  4, batch     1 | loss: 233.3214975CurrentTrain: epoch  4, batch     2 | loss: 180.2964268CurrentTrain: epoch  4, batch     3 | loss: 191.1545044CurrentTrain: epoch  5, batch     0 | loss: 224.3756461CurrentTrain: epoch  5, batch     1 | loss: 166.3483719CurrentTrain: epoch  5, batch     2 | loss: 189.5081606CurrentTrain: epoch  5, batch     3 | loss: 195.6696259CurrentTrain: epoch  6, batch     0 | loss: 196.5040020CurrentTrain: epoch  6, batch     1 | loss: 277.7728012CurrentTrain: epoch  6, batch     2 | loss: 173.6433762CurrentTrain: epoch  6, batch     3 | loss: 123.1802089CurrentTrain: epoch  7, batch     0 | loss: 187.1209349CurrentTrain: epoch  7, batch     1 | loss: 204.7841090CurrentTrain: epoch  7, batch     2 | loss: 186.7358112CurrentTrain: epoch  7, batch     3 | loss: 179.8289116CurrentTrain: epoch  8, batch     0 | loss: 187.3439098CurrentTrain: epoch  8, batch     1 | loss: 196.9473900CurrentTrain: epoch  8, batch     2 | loss: 238.7461654CurrentTrain: epoch  8, batch     3 | loss: 128.8893266CurrentTrain: epoch  9, batch     0 | loss: 184.0342151CurrentTrain: epoch  9, batch     1 | loss: 286.4515512CurrentTrain: epoch  9, batch     2 | loss: 237.9185429CurrentTrain: epoch  9, batch     3 | loss: 85.4766630
MemoryTrain:  epoch  0, batch     0 | loss: 2.6108530MemoryTrain:  epoch  1, batch     0 | loss: 1.6619164MemoryTrain:  epoch  2, batch     0 | loss: 1.4292181MemoryTrain:  epoch  3, batch     0 | loss: 1.0940896MemoryTrain:  epoch  4, batch     0 | loss: 1.0245782MemoryTrain:  epoch  5, batch     0 | loss: 0.8873560MemoryTrain:  epoch  6, batch     0 | loss: 0.7729103MemoryTrain:  epoch  7, batch     0 | loss: 0.7503565MemoryTrain:  epoch  8, batch     0 | loss: 0.5667498MemoryTrain:  epoch  9, batch     0 | loss: 0.5617624

F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.0, 6: 0.42424242424242425, 38: 0.0, 15: 0.0, 24: 0.8723404255319149, 25: 0.7096774193548387, 26: 0.88}
Micro-average F1 score: 0.7134146341463414
Weighted-average F1 score: 0.7243629366797552
F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.0, 6: 0.9166666666666666, 38: 0.0, 15: 0.0, 24: 0.96, 25: 0.8349514563106796, 26: 0.9818181818181818}
Micro-average F1 score: 0.8918205804749341
Weighted-average F1 score: 0.8738766446412157
F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.0, 6: 0.8045977011494253, 38: 0.0, 15: 0.0, 24: 0.9702970297029703, 25: 0.8269230769230769, 26: 0.9818181818181818}
Micro-average F1 score: 0.8679245283018868
Weighted-average F1 score: 0.8549069600084567

F1 score per class: {32: 0.7487179487179487, 35: 0.8235294117647058, 37: 0.8165680473372781, 6: 0.5, 38: 0.42424242424242425, 15: 0.7619047619047619, 19: 0.9473684210526315, 24: 0.8601036269430051, 25: 0.82, 26: 0.6804123711340206, 29: 0.8461538461538461}
Micro-average F1 score: 0.7885802469135802
Weighted-average F1 score: 0.8022543661452952
F1 score per class: {32: 0.780952380952381, 35: 0.8235294117647058, 37: 0.93048128342246, 38: 0.4666666666666667, 6: 0.9166666666666666, 15: 0.7567567567567568, 19: 0.9743589743589743, 24: 0.8736842105263158, 25: 0.8347826086956521, 26: 0.8037383177570093, 29: 0.9152542372881356}
Micro-average F1 score: 0.8526240115025162
Weighted-average F1 score: 0.8536448628884388
F1 score per class: {32: 0.7766990291262136, 35: 0.8235294117647058, 37: 0.9010989010989011, 38: 0.4827586206896552, 6: 0.8045977011494253, 15: 0.7567567567567568, 19: 0.9690721649484536, 24: 0.8795811518324608, 25: 0.8448275862068966, 26: 0.7818181818181819, 29: 0.8852459016393442}
Micro-average F1 score: 0.8388969521044993
Weighted-average F1 score: 0.8408694792448349
cur_acc:  ['0.7731', '0.7134']
his_acc:  ['0.7731', '0.7886']
cur_acc des:  ['0.8123', '0.8918']
his_acc des:  ['0.8123', '0.8526']
cur_acc rrf:  ['0.8123', '0.8679']
his_acc rrf:  ['0.8123', '0.8389']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 220.3643652CurrentTrain: epoch  0, batch     1 | loss: 252.4133892CurrentTrain: epoch  0, batch     2 | loss: 207.1996882CurrentTrain: epoch  0, batch     3 | loss: 109.2519410CurrentTrain: epoch  1, batch     0 | loss: 215.1960006CurrentTrain: epoch  1, batch     1 | loss: 195.2281482CurrentTrain: epoch  1, batch     2 | loss: 177.8895291CurrentTrain: epoch  1, batch     3 | loss: 178.1122880CurrentTrain: epoch  2, batch     0 | loss: 219.5242947CurrentTrain: epoch  2, batch     1 | loss: 204.7721693CurrentTrain: epoch  2, batch     2 | loss: 232.8057731CurrentTrain: epoch  2, batch     3 | loss: 110.1355254CurrentTrain: epoch  3, batch     0 | loss: 194.8742215CurrentTrain: epoch  3, batch     1 | loss: 223.4574123CurrentTrain: epoch  3, batch     2 | loss: 248.7073698CurrentTrain: epoch  3, batch     3 | loss: 106.6549335CurrentTrain: epoch  4, batch     0 | loss: 172.1061052CurrentTrain: epoch  4, batch     1 | loss: 201.1532447CurrentTrain: epoch  4, batch     2 | loss: 198.0815956CurrentTrain: epoch  4, batch     3 | loss: 175.1525283CurrentTrain: epoch  5, batch     0 | loss: 221.3942618CurrentTrain: epoch  5, batch     1 | loss: 172.0492952CurrentTrain: epoch  5, batch     2 | loss: 163.8867428CurrentTrain: epoch  5, batch     3 | loss: 172.4501875CurrentTrain: epoch  6, batch     0 | loss: 197.7168751CurrentTrain: epoch  6, batch     1 | loss: 197.1159166CurrentTrain: epoch  6, batch     2 | loss: 212.7697907CurrentTrain: epoch  6, batch     3 | loss: 139.0499913CurrentTrain: epoch  7, batch     0 | loss: 205.0878354CurrentTrain: epoch  7, batch     1 | loss: 195.8241573CurrentTrain: epoch  7, batch     2 | loss: 182.3752203CurrentTrain: epoch  7, batch     3 | loss: 144.8463386CurrentTrain: epoch  8, batch     0 | loss: 165.8306135CurrentTrain: epoch  8, batch     1 | loss: 237.9947976CurrentTrain: epoch  8, batch     2 | loss: 203.2606747CurrentTrain: epoch  8, batch     3 | loss: 136.8336892CurrentTrain: epoch  9, batch     0 | loss: 184.0179325CurrentTrain: epoch  9, batch     1 | loss: 187.7170445CurrentTrain: epoch  9, batch     2 | loss: 187.3040234CurrentTrain: epoch  9, batch     3 | loss: 172.1626375
MemoryTrain:  epoch  0, batch     0 | loss: 1.3540884MemoryTrain:  epoch  1, batch     0 | loss: 1.0747588MemoryTrain:  epoch  2, batch     0 | loss: 0.9287577MemoryTrain:  epoch  3, batch     0 | loss: 0.7462035MemoryTrain:  epoch  4, batch     0 | loss: 0.6270468MemoryTrain:  epoch  5, batch     0 | loss: 0.5198815MemoryTrain:  epoch  6, batch     0 | loss: 0.4222644MemoryTrain:  epoch  7, batch     0 | loss: 0.3920133MemoryTrain:  epoch  8, batch     0 | loss: 0.2615440MemoryTrain:  epoch  9, batch     0 | loss: 0.2485984

F1 score per class: {33: 0.734375, 36: 0.8631578947368421, 37: 0.0, 38: 0.0, 8: 0.9444444444444444, 20: 0.3076923076923077, 26: 0.4772727272727273, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.6684073107049608
Weighted-average F1 score: 0.6386581503628336
F1 score per class: {33: 0.8979591836734694, 35: 0.0, 36: 0.92, 37: 0.0, 38: 0.0, 8: 0.0, 19: 0.972972972972973, 20: 0.5333333333333333, 25: 0.0, 26: 0.9147286821705426, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.8654708520179372
Weighted-average F1 score: 0.8319072555629767
F1 score per class: {33: 0.8979591836734694, 35: 0.0, 36: 0.92, 37: 0.0, 38: 0.0, 8: 0.0, 19: 0.972972972972973, 20: 0.5333333333333333, 25: 0.0, 26: 0.8666666666666667, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.8378378378378378
Weighted-average F1 score: 0.7888833472736894

F1 score per class: {32: 0.5584415584415584, 33: 0.6225165562913907, 35: 0.8235294117647058, 36: 0.8505747126436781, 37: 0.7321428571428571, 6: 0.4444444444444444, 38: 0.42424242424242425, 8: 0.7619047619047619, 15: 0.9430051813471503, 19: 0.9444444444444444, 20: 0.851063829787234, 24: 0.3076923076923077, 25: 0.8297872340425532, 26: 0.47191011235955055, 29: 0.5765765765765766, 30: 0.55}
Micro-average F1 score: 0.7218863361547763
Weighted-average F1 score: 0.7449668597847111
F1 score per class: {32: 0.6629213483146067, 33: 0.7058823529411765, 35: 0.8235294117647058, 36: 0.9368421052631579, 37: 0.7419354838709677, 6: 0.48484848484848486, 38: 0.65, 8: 0.75, 15: 0.9538461538461539, 19: 0.9230769230769231, 20: 0.8704663212435233, 24: 0.42105263157894735, 25: 0.8363636363636363, 26: 0.7564102564102564, 29: 0.5918367346938775, 30: 0.6222222222222222}
Micro-average F1 score: 0.775974025974026
Weighted-average F1 score: 0.7805220093583088
F1 score per class: {32: 0.6629213483146067, 33: 0.7058823529411765, 35: 0.8235294117647058, 36: 0.9197860962566845, 37: 0.71875, 6: 0.5333333333333333, 38: 0.5789473684210527, 8: 0.75, 15: 0.9538461538461539, 19: 0.9, 20: 0.8704663212435233, 24: 0.47058823529411764, 25: 0.8571428571428571, 26: 0.7272727272727273, 29: 0.5740740740740741, 30: 0.6222222222222222}
Micro-average F1 score: 0.7684782608695652
Weighted-average F1 score: 0.7736079294392566
cur_acc:  ['0.7731', '0.7134', '0.6684']
his_acc:  ['0.7731', '0.7886', '0.7219']
cur_acc des:  ['0.8123', '0.8918', '0.8655']
his_acc des:  ['0.8123', '0.8526', '0.7760']
cur_acc rrf:  ['0.8123', '0.8679', '0.8378']
his_acc rrf:  ['0.8123', '0.8389', '0.7685']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 253.3965063CurrentTrain: epoch  0, batch     1 | loss: 196.3269725CurrentTrain: epoch  0, batch     2 | loss: 267.6966347CurrentTrain: epoch  0, batch     3 | loss: 207.8717816CurrentTrain: epoch  0, batch     4 | loss: 124.2223407CurrentTrain: epoch  1, batch     0 | loss: 221.7570709CurrentTrain: epoch  1, batch     1 | loss: 241.4700334CurrentTrain: epoch  1, batch     2 | loss: 273.7205697CurrentTrain: epoch  1, batch     3 | loss: 209.8631059CurrentTrain: epoch  1, batch     4 | loss: 144.7862819CurrentTrain: epoch  2, batch     0 | loss: 228.1982039CurrentTrain: epoch  2, batch     1 | loss: 234.4490146CurrentTrain: epoch  2, batch     2 | loss: 188.2704231CurrentTrain: epoch  2, batch     3 | loss: 207.7401711CurrentTrain: epoch  2, batch     4 | loss: 203.5969663CurrentTrain: epoch  3, batch     0 | loss: 240.3632184CurrentTrain: epoch  3, batch     1 | loss: 233.3514203CurrentTrain: epoch  3, batch     2 | loss: 178.4155861CurrentTrain: epoch  3, batch     3 | loss: 239.2016571CurrentTrain: epoch  3, batch     4 | loss: 118.8820241CurrentTrain: epoch  4, batch     0 | loss: 338.5021779CurrentTrain: epoch  4, batch     1 | loss: 362.4464943CurrentTrain: epoch  4, batch     2 | loss: 189.6982657CurrentTrain: epoch  4, batch     3 | loss: 181.0093114CurrentTrain: epoch  4, batch     4 | loss: 127.4926747CurrentTrain: epoch  5, batch     0 | loss: 276.8665207CurrentTrain: epoch  5, batch     1 | loss: 178.0602810CurrentTrain: epoch  5, batch     2 | loss: 212.8336717CurrentTrain: epoch  5, batch     3 | loss: 184.0487722CurrentTrain: epoch  5, batch     4 | loss: 158.2042655CurrentTrain: epoch  6, batch     0 | loss: 196.8715808CurrentTrain: epoch  6, batch     1 | loss: 229.0322191CurrentTrain: epoch  6, batch     2 | loss: 213.1053370CurrentTrain: epoch  6, batch     3 | loss: 212.7414088CurrentTrain: epoch  6, batch     4 | loss: 149.2312464CurrentTrain: epoch  7, batch     0 | loss: 203.5023840CurrentTrain: epoch  7, batch     1 | loss: 220.8296668CurrentTrain: epoch  7, batch     2 | loss: 239.2377407CurrentTrain: epoch  7, batch     3 | loss: 196.0868414CurrentTrain: epoch  7, batch     4 | loss: 124.7331743CurrentTrain: epoch  8, batch     0 | loss: 238.1014463CurrentTrain: epoch  8, batch     1 | loss: 287.3238205CurrentTrain: epoch  8, batch     2 | loss: 188.1716444CurrentTrain: epoch  8, batch     3 | loss: 184.1001342CurrentTrain: epoch  8, batch     4 | loss: 124.2598067CurrentTrain: epoch  9, batch     0 | loss: 167.9826088CurrentTrain: epoch  9, batch     1 | loss: 220.7813761CurrentTrain: epoch  9, batch     2 | loss: 200.9021657CurrentTrain: epoch  9, batch     3 | loss: 237.6820531CurrentTrain: epoch  9, batch     4 | loss: 132.1915857
MemoryTrain:  epoch  0, batch     0 | loss: 1.7659397MemoryTrain:  epoch  1, batch     0 | loss: 1.5496983MemoryTrain:  epoch  2, batch     0 | loss: 1.4146464MemoryTrain:  epoch  3, batch     0 | loss: 1.1234648MemoryTrain:  epoch  4, batch     0 | loss: 0.9347619MemoryTrain:  epoch  5, batch     0 | loss: 0.8079941MemoryTrain:  epoch  6, batch     0 | loss: 0.6680235MemoryTrain:  epoch  7, batch     0 | loss: 0.6469493MemoryTrain:  epoch  8, batch     0 | loss: 0.5396492MemoryTrain:  epoch  9, batch     0 | loss: 0.3980465

F1 score per class: {32: 0.3937007874015748, 1: 0.7575757575757576, 34: 0.05555555555555555, 3: 0.768361581920904, 35: 0.0, 37: 0.0, 14: 0.0, 22: 0.7912087912087912, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.5709779179810726
Weighted-average F1 score: 0.5455631171137465
F1 score per class: {32: 0.3793103448275862, 1: 0.9139072847682119, 34: 0.0, 3: 0.12345679012345678, 35: 0.6625, 37: 0.0, 36: 0.0, 8: 0.0, 14: 0.8043478260869565, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.5511111111111111
Weighted-average F1 score: 0.48863402425950575
F1 score per class: {32: 0.39655172413793105, 1: 0.9210526315789473, 34: 0.125, 3: 0.7176470588235294, 35: 0.0, 37: 0.0, 36: 0.0, 14: 0.8421052631578947, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.5905044510385756
Weighted-average F1 score: 0.5413743594937166

F1 score per class: {1: 0.3597122302158273, 3: 0.6993006993006993, 6: 0.5, 8: 0.515625, 14: 0.05333333333333334, 15: 0.8888888888888888, 19: 0.5507246376811594, 20: 0.7090909090909091, 22: 0.768361581920904, 24: 0.20512820512820512, 25: 0.42424242424242425, 26: 0.7628865979381443, 29: 0.9424083769633508, 30: 0.9444444444444444, 32: 0.8021978021978022, 33: 0.16666666666666666, 34: 0.42857142857142855, 35: 0.4, 36: 0.325, 37: 0.3076923076923077, 38: 0.4444444444444444}
Micro-average F1 score: 0.5868210151380232
Weighted-average F1 score: 0.6065707368343572
F1 score per class: {1: 0.3464566929133858, 3: 0.7931034482758621, 6: 0.6358381502890174, 8: 0.6885245901639344, 14: 0.11904761904761904, 15: 0.8235294117647058, 19: 0.7133757961783439, 20: 0.7611940298507462, 22: 0.6583850931677019, 24: 0.15584415584415584, 25: 0.6133333333333333, 26: 0.7564766839378239, 29: 0.9641025641025641, 30: 0.9230769230769231, 32: 0.7912087912087912, 33: 0.5, 34: 0.556390977443609, 35: 0.6222222222222222, 36: 0.7703703703703704, 37: 0.3469387755102041, 38: 0.6222222222222222}
Micro-average F1 score: 0.6581059390048154
Weighted-average F1 score: 0.6556602827222245
F1 score per class: {1: 0.35384615384615387, 3: 0.7909604519774012, 6: 0.6235294117647059, 8: 0.6847826086956522, 14: 0.12048192771084337, 15: 0.8235294117647058, 19: 0.717948717948718, 20: 0.7555555555555555, 22: 0.7134502923976608, 24: 0.15151515151515152, 25: 0.5753424657534246, 26: 0.7564766839378239, 29: 0.9587628865979382, 30: 0.972972972972973, 32: 0.7912087912087912, 33: 0.4, 34: 0.45977011494252873, 35: 0.5909090909090909, 36: 0.4444444444444444, 37: 0.3584905660377358, 38: 0.6222222222222222}
Micro-average F1 score: 0.6379726468222043
Weighted-average F1 score: 0.6355754333059779
cur_acc:  ['0.7731', '0.7134', '0.6684', '0.5710']
his_acc:  ['0.7731', '0.7886', '0.7219', '0.5868']
cur_acc des:  ['0.8123', '0.8918', '0.8655', '0.5511']
his_acc des:  ['0.8123', '0.8526', '0.7760', '0.6581']
cur_acc rrf:  ['0.8123', '0.8679', '0.8378', '0.5905']
his_acc rrf:  ['0.8123', '0.8389', '0.7685', '0.6380']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 218.8781222CurrentTrain: epoch  0, batch     1 | loss: 237.3464258CurrentTrain: epoch  0, batch     2 | loss: 228.9609075CurrentTrain: epoch  0, batch     3 | loss: 228.3686509CurrentTrain: epoch  0, batch     4 | loss: 157.1273792CurrentTrain: epoch  1, batch     0 | loss: 238.0788022CurrentTrain: epoch  1, batch     1 | loss: 250.8810616CurrentTrain: epoch  1, batch     2 | loss: 193.7814543CurrentTrain: epoch  1, batch     3 | loss: 219.9382520CurrentTrain: epoch  1, batch     4 | loss: 137.4823671CurrentTrain: epoch  2, batch     0 | loss: 280.0873365CurrentTrain: epoch  2, batch     1 | loss: 219.8700251CurrentTrain: epoch  2, batch     2 | loss: 232.1246397CurrentTrain: epoch  2, batch     3 | loss: 205.8406240CurrentTrain: epoch  2, batch     4 | loss: 121.6433403CurrentTrain: epoch  3, batch     0 | loss: 230.9612701CurrentTrain: epoch  3, batch     1 | loss: 188.8534360CurrentTrain: epoch  3, batch     2 | loss: 289.1449843CurrentTrain: epoch  3, batch     3 | loss: 195.3044033CurrentTrain: epoch  3, batch     4 | loss: 126.3699177CurrentTrain: epoch  4, batch     0 | loss: 189.5778991CurrentTrain: epoch  4, batch     1 | loss: 193.9792039CurrentTrain: epoch  4, batch     2 | loss: 223.9445099CurrentTrain: epoch  4, batch     3 | loss: 239.6330853CurrentTrain: epoch  4, batch     4 | loss: 230.1635070CurrentTrain: epoch  5, batch     0 | loss: 213.3616113CurrentTrain: epoch  5, batch     1 | loss: 277.5488924CurrentTrain: epoch  5, batch     2 | loss: 187.5883108CurrentTrain: epoch  5, batch     3 | loss: 214.6552108CurrentTrain: epoch  5, batch     4 | loss: 143.5162496CurrentTrain: epoch  6, batch     0 | loss: 240.7408977CurrentTrain: epoch  6, batch     1 | loss: 203.9438792CurrentTrain: epoch  6, batch     2 | loss: 238.5831017CurrentTrain: epoch  6, batch     3 | loss: 212.0730024CurrentTrain: epoch  6, batch     4 | loss: 142.6776451CurrentTrain: epoch  7, batch     0 | loss: 231.1341351CurrentTrain: epoch  7, batch     1 | loss: 203.6765474CurrentTrain: epoch  7, batch     2 | loss: 220.4013632CurrentTrain: epoch  7, batch     3 | loss: 220.0802041CurrentTrain: epoch  7, batch     4 | loss: 161.1859069CurrentTrain: epoch  8, batch     0 | loss: 228.5836460CurrentTrain: epoch  8, batch     1 | loss: 176.2878717CurrentTrain: epoch  8, batch     2 | loss: 220.5956203CurrentTrain: epoch  8, batch     3 | loss: 247.2042387CurrentTrain: epoch  8, batch     4 | loss: 135.7050405CurrentTrain: epoch  9, batch     0 | loss: 202.7032251CurrentTrain: epoch  9, batch     1 | loss: 194.4307851CurrentTrain: epoch  9, batch     2 | loss: 238.3342657CurrentTrain: epoch  9, batch     3 | loss: 191.2066100CurrentTrain: epoch  9, batch     4 | loss: 178.5408769
MemoryTrain:  epoch  0, batch     0 | loss: 1.1993858MemoryTrain:  epoch  1, batch     0 | loss: 1.0767237MemoryTrain:  epoch  2, batch     0 | loss: 0.7828299MemoryTrain:  epoch  3, batch     0 | loss: 0.6297007MemoryTrain:  epoch  4, batch     0 | loss: 0.7877883MemoryTrain:  epoch  5, batch     0 | loss: 0.4778325MemoryTrain:  epoch  6, batch     0 | loss: 0.3689910MemoryTrain:  epoch  7, batch     0 | loss: 0.2995694MemoryTrain:  epoch  8, batch     0 | loss: 0.2860530MemoryTrain:  epoch  9, batch     0 | loss: 0.2592534

F1 score per class: {34: 0.9743589743589743, 5: 0.0, 38: 0.0, 6: 0.4732824427480916, 8: 0.8, 10: 0.0, 16: 0.23809523809523808, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.6726057906458798
Weighted-average F1 score: 0.7192273669350845
F1 score per class: {33: 1.0, 34: 0.0, 36: 0.0, 5: 0.6308724832214765, 38: 0.9473684210526315, 6: 0.6153846153846154, 8: 0.7666666666666667, 10: 0.0, 16: 0.0, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7486033519553073
Weighted-average F1 score: 0.6765768997745026
F1 score per class: {33: 1.0, 34: 0.0, 5: 0.0, 38: 0.64, 6: 0.9473684210526315, 8: 0.2, 10: 0.6785714285714286, 16: 0.0, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7289719626168224
Weighted-average F1 score: 0.6589644961825412

F1 score per class: {1: 0.3382352941176471, 3: 0.43243243243243246, 5: 0.8837209302325582, 6: 0.4935064935064935, 8: 0.2, 10: 0.4492753623188406, 14: 0.0759493670886076, 15: 0.8888888888888888, 16: 0.8, 17: 0.0, 18: 0.2127659574468085, 19: 0.5611510791366906, 20: 0.5777777777777777, 22: 0.7738095238095238, 24: 0.21052631578947367, 25: 0.4, 26: 0.7564766839378239, 29: 0.9081081081081082, 30: 0.9142857142857143, 32: 0.797752808988764, 33: 0.2857142857142857, 34: 0.4380952380952381, 35: 0.3373493975903614, 36: 0.11428571428571428, 37: 0.379746835443038, 38: 0.23529411764705882}
Micro-average F1 score: 0.5606000789577577
Weighted-average F1 score: 0.6131621153393874
F1 score per class: {1: 0.336, 3: 0.7469879518072289, 5: 0.8888888888888888, 6: 0.6956521739130435, 8: 0.6162162162162163, 10: 0.5987261146496815, 14: 0.12048192771084337, 15: 0.8888888888888888, 16: 0.8852459016393442, 17: 0.4444444444444444, 18: 0.4946236559139785, 19: 0.7730061349693251, 20: 0.8155339805825242, 22: 0.6463414634146342, 24: 0.15384615384615385, 25: 0.6153846153846154, 26: 0.7659574468085106, 29: 0.9484536082474226, 30: 0.9473684210526315, 32: 0.7845303867403315, 33: 0.38095238095238093, 34: 0.5087719298245614, 35: 0.5625, 36: 0.7272727272727273, 37: 0.4418604651162791, 38: 0.43137254901960786}
Micro-average F1 score: 0.6639919759277834
Weighted-average F1 score: 0.6613347922778561
F1 score per class: {1: 0.328125, 3: 0.7195121951219512, 5: 0.8733624454148472, 6: 0.6774193548387096, 8: 0.5945945945945946, 10: 0.5962732919254659, 14: 0.11904761904761904, 15: 0.8888888888888888, 16: 0.8852459016393442, 17: 0.2, 18: 0.42696629213483145, 19: 0.7530864197530864, 20: 0.8, 22: 0.7441860465116279, 24: 0.15625, 25: 0.5405405405405406, 26: 0.7619047619047619, 29: 0.9479166666666666, 30: 0.9473684210526315, 32: 0.7777777777777778, 33: 0.3157894736842105, 34: 0.47244094488188976, 35: 0.5416666666666666, 36: 0.5454545454545454, 37: 0.42696629213483145, 38: 0.3829787234042553}
Micro-average F1 score: 0.648921832884097
Weighted-average F1 score: 0.6499197742387318
cur_acc:  ['0.7731', '0.7134', '0.6684', '0.5710', '0.6726']
his_acc:  ['0.7731', '0.7886', '0.7219', '0.5868', '0.5606']
cur_acc des:  ['0.8123', '0.8918', '0.8655', '0.5511', '0.7486']
his_acc des:  ['0.8123', '0.8526', '0.7760', '0.6581', '0.6640']
cur_acc rrf:  ['0.8123', '0.8679', '0.8378', '0.5905', '0.7290']
his_acc rrf:  ['0.8123', '0.8389', '0.7685', '0.6380', '0.6489']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 219.9035150CurrentTrain: epoch  0, batch     1 | loss: 199.8393043CurrentTrain: epoch  0, batch     2 | loss: 212.9601922CurrentTrain: epoch  0, batch     3 | loss: 198.4257269CurrentTrain: epoch  1, batch     0 | loss: 247.1493977CurrentTrain: epoch  1, batch     1 | loss: 205.1018300CurrentTrain: epoch  1, batch     2 | loss: 202.0621602CurrentTrain: epoch  1, batch     3 | loss: 176.5952410CurrentTrain: epoch  2, batch     0 | loss: 203.4181477CurrentTrain: epoch  2, batch     1 | loss: 235.4654157CurrentTrain: epoch  2, batch     2 | loss: 172.0103944CurrentTrain: epoch  2, batch     3 | loss: 200.1543002CurrentTrain: epoch  3, batch     0 | loss: 256.5047242CurrentTrain: epoch  3, batch     1 | loss: 166.3128982CurrentTrain: epoch  3, batch     2 | loss: 196.6082567CurrentTrain: epoch  3, batch     3 | loss: 154.0661617CurrentTrain: epoch  4, batch     0 | loss: 285.7024016CurrentTrain: epoch  4, batch     1 | loss: 217.9284193CurrentTrain: epoch  4, batch     2 | loss: 198.2402194CurrentTrain: epoch  4, batch     3 | loss: 147.5430762CurrentTrain: epoch  5, batch     0 | loss: 195.0342501CurrentTrain: epoch  5, batch     1 | loss: 158.1034709CurrentTrain: epoch  5, batch     2 | loss: 236.1482381CurrentTrain: epoch  5, batch     3 | loss: 232.7533931CurrentTrain: epoch  6, batch     0 | loss: 240.9095650CurrentTrain: epoch  6, batch     1 | loss: 204.1600238CurrentTrain: epoch  6, batch     2 | loss: 186.8765535CurrentTrain: epoch  6, batch     3 | loss: 145.8540543CurrentTrain: epoch  7, batch     0 | loss: 189.8099772CurrentTrain: epoch  7, batch     1 | loss: 286.9833302CurrentTrain: epoch  7, batch     2 | loss: 177.0792277CurrentTrain: epoch  7, batch     3 | loss: 152.1351233CurrentTrain: epoch  8, batch     0 | loss: 183.5584252CurrentTrain: epoch  8, batch     1 | loss: 204.8722224CurrentTrain: epoch  8, batch     2 | loss: 185.6610958CurrentTrain: epoch  8, batch     3 | loss: 180.6066930CurrentTrain: epoch  9, batch     0 | loss: 185.7976167CurrentTrain: epoch  9, batch     1 | loss: 211.9643081CurrentTrain: epoch  9, batch     2 | loss: 201.9281388CurrentTrain: epoch  9, batch     3 | loss: 142.2268974
MemoryTrain:  epoch  0, batch     0 | loss: 0.7712672MemoryTrain:  epoch  1, batch     0 | loss: 0.6754849MemoryTrain:  epoch  2, batch     0 | loss: 0.5228451MemoryTrain:  epoch  3, batch     0 | loss: 0.3894090MemoryTrain:  epoch  4, batch     0 | loss: 0.3121415MemoryTrain:  epoch  5, batch     0 | loss: 0.2580546MemoryTrain:  epoch  6, batch     0 | loss: 0.1966762MemoryTrain:  epoch  7, batch     0 | loss: 0.1905520MemoryTrain:  epoch  8, batch     0 | loss: 0.1328588MemoryTrain:  epoch  9, batch     0 | loss: 0.1295175

F1 score per class: {0: 0.9117647058823529, 32: 0.0, 1: 0.9528795811518325, 4: 0.75, 13: 0.0, 14: 0.2777777777777778, 21: 0.0, 22: 0.6944444444444444, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7848101265822784
Weighted-average F1 score: 0.7770099872409696
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 34: 0.9528795811518325, 1: 0.8888888888888888, 4: 0.0, 37: 0.6808510638297872, 13: 0.0, 18: 0.7848101265822784, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8298368298368298
Weighted-average F1 score: 0.7740764967815803
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 34: 0.9528795811518325, 1: 0.8888888888888888, 4: 0.0, 37: 0.0, 13: 0.6808510638297872, 15: 0.0, 18: 0.72, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8130841121495327
Weighted-average F1 score: 0.7539810758496028

F1 score per class: {0: 0.9117647058823529, 1: 0.32592592592592595, 3: 0.37254901960784315, 4: 0.9528795811518325, 5: 0.8778280542986425, 6: 0.5569620253164557, 8: 0.20833333333333334, 10: 0.19130434782608696, 13: 0.07058823529411765, 14: 0.0, 15: 0.75, 16: 0.8461538461538461, 17: 0.36363636363636365, 18: 0.2727272727272727, 19: 0.7577639751552795, 20: 0.6666666666666666, 21: 0.136986301369863, 22: 0.5555555555555556, 23: 0.6666666666666666, 24: 0.0625, 25: 0.47058823529411764, 26: 0.7121951219512195, 29: 0.9148936170212766, 30: 0.972972972972973, 32: 0.7529411764705882, 33: 0.3076923076923077, 34: 0.2962962962962963, 35: 0.35185185185185186, 36: 0.2597402597402597, 37: 0.46938775510204084, 38: 0.23529411764705882}
Micro-average F1 score: 0.5659377070907886
Weighted-average F1 score: 0.5968142472484296
F1 score per class: {0: 0.972972972972973, 1: 0.32857142857142857, 3: 0.7612903225806451, 4: 0.9528795811518325, 5: 0.8658008658008658, 6: 0.6777777777777778, 8: 0.5977011494252874, 10: 0.5138888888888888, 13: 0.1702127659574468, 14: 0.07228915662650602, 15: 0.75, 16: 0.9180327868852459, 17: 0.5333333333333333, 18: 0.5476190476190477, 19: 0.8700564971751412, 20: 0.8269230769230769, 21: 0.34408602150537637, 22: 0.6455696202531646, 23: 0.7469879518072289, 24: 0.0, 25: 0.6329113924050633, 26: 0.7121951219512195, 29: 0.9489795918367347, 30: 0.9473684210526315, 32: 0.7934782608695652, 33: 0.4444444444444444, 34: 0.48695652173913045, 35: 0.625, 36: 0.7580645161290323, 37: 0.5094339622641509, 38: 0.425531914893617}
Micro-average F1 score: 0.67994227994228
Weighted-average F1 score: 0.6764944696573633
F1 score per class: {0: 0.972972972972973, 1: 0.34965034965034963, 3: 0.7083333333333334, 4: 0.9528795811518325, 5: 0.8658008658008658, 6: 0.6740331491712708, 8: 0.5987261146496815, 10: 0.463768115942029, 13: 0.11764705882352941, 14: 0.07317073170731707, 15: 0.6, 16: 0.9032258064516129, 17: 0.3333333333333333, 18: 0.4444444444444444, 19: 0.8505747126436781, 20: 0.8113207547169812, 21: 0.3333333333333333, 22: 0.631578947368421, 23: 0.6923076923076923, 24: 0.0, 25: 0.5974025974025974, 26: 0.7121951219512195, 29: 0.9489795918367347, 30: 0.9473684210526315, 32: 0.8, 33: 0.3333333333333333, 34: 0.4838709677419355, 35: 0.6218487394957983, 36: 0.6605504587155964, 37: 0.5272727272727272, 38: 0.4}
Micro-average F1 score: 0.6598955310504934
Weighted-average F1 score: 0.6543232274599029
cur_acc:  ['0.7731', '0.7134', '0.6684', '0.5710', '0.6726', '0.7848']
his_acc:  ['0.7731', '0.7886', '0.7219', '0.5868', '0.5606', '0.5659']
cur_acc des:  ['0.8123', '0.8918', '0.8655', '0.5511', '0.7486', '0.8298']
his_acc des:  ['0.8123', '0.8526', '0.7760', '0.6581', '0.6640', '0.6799']
cur_acc rrf:  ['0.8123', '0.8679', '0.8378', '0.5905', '0.7290', '0.8131']
his_acc rrf:  ['0.8123', '0.8389', '0.7685', '0.6380', '0.6489', '0.6599']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 271.8370878CurrentTrain: epoch  0, batch     1 | loss: 220.3772519CurrentTrain: epoch  0, batch     2 | loss: 259.2499011CurrentTrain: epoch  0, batch     3 | loss: 172.2675378CurrentTrain: epoch  0, batch     4 | loss: 90.6355001CurrentTrain: epoch  1, batch     0 | loss: 227.5676801CurrentTrain: epoch  1, batch     1 | loss: 190.3458625CurrentTrain: epoch  1, batch     2 | loss: 207.9003682CurrentTrain: epoch  1, batch     3 | loss: 217.2872945CurrentTrain: epoch  1, batch     4 | loss: 90.1298334CurrentTrain: epoch  2, batch     0 | loss: 245.5302166CurrentTrain: epoch  2, batch     1 | loss: 274.3860910CurrentTrain: epoch  2, batch     2 | loss: 219.9910491CurrentTrain: epoch  2, batch     3 | loss: 172.3418102CurrentTrain: epoch  2, batch     4 | loss: 37.4559948CurrentTrain: epoch  3, batch     0 | loss: 226.4360319CurrentTrain: epoch  3, batch     1 | loss: 180.4697383CurrentTrain: epoch  3, batch     2 | loss: 202.5778205CurrentTrain: epoch  3, batch     3 | loss: 279.8178198CurrentTrain: epoch  3, batch     4 | loss: 53.3949812CurrentTrain: epoch  4, batch     0 | loss: 187.5172608CurrentTrain: epoch  4, batch     1 | loss: 361.6076197CurrentTrain: epoch  4, batch     2 | loss: 209.0261001CurrentTrain: epoch  4, batch     3 | loss: 176.9650426CurrentTrain: epoch  4, batch     4 | loss: 53.0662032CurrentTrain: epoch  5, batch     0 | loss: 224.8952491CurrentTrain: epoch  5, batch     1 | loss: 278.5523492CurrentTrain: epoch  5, batch     2 | loss: 216.2689006CurrentTrain: epoch  5, batch     3 | loss: 214.8516605CurrentTrain: epoch  5, batch     4 | loss: 17.7910973CurrentTrain: epoch  6, batch     0 | loss: 239.6669710CurrentTrain: epoch  6, batch     1 | loss: 192.0050020CurrentTrain: epoch  6, batch     2 | loss: 192.9563351CurrentTrain: epoch  6, batch     3 | loss: 196.6981556CurrentTrain: epoch  6, batch     4 | loss: 89.8687343CurrentTrain: epoch  7, batch     0 | loss: 247.9751391CurrentTrain: epoch  7, batch     1 | loss: 187.5555445CurrentTrain: epoch  7, batch     2 | loss: 196.7087577CurrentTrain: epoch  7, batch     3 | loss: 207.8444040CurrentTrain: epoch  7, batch     4 | loss: 51.4779692CurrentTrain: epoch  8, batch     0 | loss: 213.0196653CurrentTrain: epoch  8, batch     1 | loss: 203.1831073CurrentTrain: epoch  8, batch     2 | loss: 278.9135747CurrentTrain: epoch  8, batch     3 | loss: 180.7315005CurrentTrain: epoch  8, batch     4 | loss: 35.2737988CurrentTrain: epoch  9, batch     0 | loss: 180.8156640CurrentTrain: epoch  9, batch     1 | loss: 202.9439552CurrentTrain: epoch  9, batch     2 | loss: 228.4111379CurrentTrain: epoch  9, batch     3 | loss: 247.2566391CurrentTrain: epoch  9, batch     4 | loss: 35.4627589
MemoryTrain:  epoch  0, batch     0 | loss: 0.8080243MemoryTrain:  epoch  1, batch     0 | loss: 0.7212141MemoryTrain:  epoch  2, batch     0 | loss: 0.6011359MemoryTrain:  epoch  3, batch     0 | loss: 0.5165080MemoryTrain:  epoch  4, batch     0 | loss: 0.3613399MemoryTrain:  epoch  5, batch     0 | loss: 0.2941487MemoryTrain:  epoch  6, batch     0 | loss: 0.2796204MemoryTrain:  epoch  7, batch     0 | loss: 0.2198709MemoryTrain:  epoch  8, batch     0 | loss: 0.1782678MemoryTrain:  epoch  9, batch     0 | loss: 0.1737627

F1 score per class: {0: 0.0, 2: 0.875, 34: 0.0, 5: 0.0, 39: 0.6141732283464567, 8: 0.6442953020134228, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 18: 0.5, 19: 0.0, 28: 0.4444444444444444}
Micro-average F1 score: 0.5906432748538012
Weighted-average F1 score: 0.5319187274955065
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.7945205479452054, 12: 0.6928104575163399, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.8, 33: 0.0, 34: 0.0, 39: 0.7272727272727273}
Micro-average F1 score: 0.6616541353383458
Weighted-average F1 score: 0.557468488040943
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8187919463087249, 12: 0.7261146496815286, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 26: 0.0, 28: 0.8, 34: 0.0, 39: 0.7272727272727273}
Micro-average F1 score: 0.7002518891687658
Weighted-average F1 score: 0.6116743009823661

F1 score per class: {0: 0.8985507246376812, 1: 0.32592592592592595, 2: 0.7368421052631579, 3: 0.25, 4: 0.918918918918919, 5: 0.8858447488584474, 6: 0.625, 8: 0.3148148148148148, 10: 0.12727272727272726, 11: 0.3577981651376147, 12: 0.5423728813559322, 13: 0.06896551724137931, 14: 0.0, 15: 0.75, 16: 0.7719298245614035, 17: 0.0, 18: 0.04878048780487805, 19: 0.7857142857142857, 20: 0.5777777777777777, 21: 0.1927710843373494, 22: 0.5323741007194245, 23: 0.6666666666666666, 24: 0.0, 25: 0.42424242424242425, 26: 0.7070707070707071, 28: 0.15384615384615385, 29: 0.9090909090909091, 30: 0.972972972972973, 32: 0.8043478260869565, 33: 0.16666666666666666, 34: 0.11594202898550725, 35: 0.4090909090909091, 36: 0.08695652173913043, 37: 0.32432432432432434, 38: 0.18181818181818182, 39: 0.26666666666666666}
Micro-average F1 score: 0.5428824049513705
Weighted-average F1 score: 0.5922765644420332
F1 score per class: {0: 0.972972972972973, 1: 0.34532374100719426, 2: 0.4666666666666667, 3: 0.8050314465408805, 4: 0.88268156424581, 5: 0.8722466960352423, 6: 0.7076923076923077, 8: 0.509090909090909, 10: 0.4740740740740741, 11: 0.5248868778280543, 12: 0.5955056179775281, 13: 0.09523809523809523, 14: 0.07058823529411765, 15: 0.75, 16: 0.8333333333333334, 17: 0.0, 18: 0.38095238095238093, 19: 0.8617021276595744, 20: 0.7572815533980582, 21: 0.3058823529411765, 22: 0.5390070921985816, 23: 0.7674418604651163, 24: 0.17777777777777778, 25: 0.4788732394366197, 26: 0.7010309278350515, 28: 0.2857142857142857, 29: 0.9090909090909091, 30: 1.0, 32: 0.8315789473684211, 33: 0.35294117647058826, 34: 0.46601941747572817, 35: 0.6851851851851852, 36: 0.6972477064220184, 37: 0.4186046511627907, 38: 0.43478260869565216, 39: 0.37209302325581395}
Micro-average F1 score: 0.640458214006769
Weighted-average F1 score: 0.6401798619226777
F1 score per class: {0: 0.972972972972973, 1: 0.3333333333333333, 2: 0.5185185185185185, 3: 0.7948717948717948, 4: 0.907103825136612, 5: 0.8761061946902655, 6: 0.6907216494845361, 8: 0.4626865671641791, 10: 0.3875968992248062, 11: 0.48221343873517786, 12: 0.6, 13: 0.0625, 14: 0.075, 15: 0.6666666666666666, 16: 0.819672131147541, 17: 0.0, 18: 0.3076923076923077, 19: 0.8248587570621468, 20: 0.7572815533980582, 21: 0.2857142857142857, 22: 0.5594405594405595, 23: 0.7, 24: 0.20512820512820512, 25: 0.4788732394366197, 26: 0.7010309278350515, 28: 0.20689655172413793, 29: 0.9090909090909091, 30: 1.0, 32: 0.837696335078534, 33: 0.4, 34: 0.40425531914893614, 35: 0.6542056074766355, 36: 0.23684210526315788, 37: 0.3902439024390244, 38: 0.46808510638297873, 39: 0.34782608695652173}
Micro-average F1 score: 0.6148577449947313
Weighted-average F1 score: 0.6195113580125098
cur_acc:  ['0.7731', '0.7134', '0.6684', '0.5710', '0.6726', '0.7848', '0.5906']
his_acc:  ['0.7731', '0.7886', '0.7219', '0.5868', '0.5606', '0.5659', '0.5429']
cur_acc des:  ['0.8123', '0.8918', '0.8655', '0.5511', '0.7486', '0.8298', '0.6617']
his_acc des:  ['0.8123', '0.8526', '0.7760', '0.6581', '0.6640', '0.6799', '0.6405']
cur_acc rrf:  ['0.8123', '0.8679', '0.8378', '0.5905', '0.7290', '0.8131', '0.7003']
his_acc rrf:  ['0.8123', '0.8389', '0.7685', '0.6380', '0.6489', '0.6599', '0.6149']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 207.0274698CurrentTrain: epoch  0, batch     1 | loss: 215.3788305CurrentTrain: epoch  0, batch     2 | loss: 170.8894072CurrentTrain: epoch  0, batch     3 | loss: 21.6027166CurrentTrain: epoch  1, batch     0 | loss: 213.8204644CurrentTrain: epoch  1, batch     1 | loss: 170.1241367CurrentTrain: epoch  1, batch     2 | loss: 190.7087746CurrentTrain: epoch  1, batch     3 | loss: 17.1418752CurrentTrain: epoch  2, batch     0 | loss: 216.9588171CurrentTrain: epoch  2, batch     1 | loss: 184.4344958CurrentTrain: epoch  2, batch     2 | loss: 184.4683970CurrentTrain: epoch  2, batch     3 | loss: 41.4667351CurrentTrain: epoch  3, batch     0 | loss: 185.0312887CurrentTrain: epoch  3, batch     1 | loss: 200.4697832CurrentTrain: epoch  3, batch     2 | loss: 180.7438757CurrentTrain: epoch  3, batch     3 | loss: 41.3340195CurrentTrain: epoch  4, batch     0 | loss: 198.9626039CurrentTrain: epoch  4, batch     1 | loss: 278.5379641CurrentTrain: epoch  4, batch     2 | loss: 142.7441284CurrentTrain: epoch  4, batch     3 | loss: 16.7024795CurrentTrain: epoch  5, batch     0 | loss: 169.6872470CurrentTrain: epoch  5, batch     1 | loss: 229.9296686CurrentTrain: epoch  5, batch     2 | loss: 190.5050430CurrentTrain: epoch  5, batch     3 | loss: 20.3221126CurrentTrain: epoch  6, batch     0 | loss: 230.3186366CurrentTrain: epoch  6, batch     1 | loss: 196.4511093CurrentTrain: epoch  6, batch     2 | loss: 153.9017810CurrentTrain: epoch  6, batch     3 | loss: 11.7938626CurrentTrain: epoch  7, batch     0 | loss: 186.7326024CurrentTrain: epoch  7, batch     1 | loss: 187.5339200CurrentTrain: epoch  7, batch     2 | loss: 219.8175290CurrentTrain: epoch  7, batch     3 | loss: 10.8490003CurrentTrain: epoch  8, batch     0 | loss: 147.5235711CurrentTrain: epoch  8, batch     1 | loss: 247.4967710CurrentTrain: epoch  8, batch     2 | loss: 175.4727764CurrentTrain: epoch  8, batch     3 | loss: 11.4784532CurrentTrain: epoch  9, batch     0 | loss: 194.2859764CurrentTrain: epoch  9, batch     1 | loss: 140.2371942CurrentTrain: epoch  9, batch     2 | loss: 211.1027573CurrentTrain: epoch  9, batch     3 | loss: 41.1809444
MemoryTrain:  epoch  0, batch     0 | loss: 0.6381669MemoryTrain:  epoch  1, batch     0 | loss: 0.6478060MemoryTrain:  epoch  2, batch     0 | loss: 0.4571408MemoryTrain:  epoch  3, batch     0 | loss: 0.3438062MemoryTrain:  epoch  4, batch     0 | loss: 0.2791754MemoryTrain:  epoch  5, batch     0 | loss: 0.2350483MemoryTrain:  epoch  6, batch     0 | loss: 0.2257334MemoryTrain:  epoch  7, batch     0 | loss: 0.1930791MemoryTrain:  epoch  8, batch     0 | loss: 0.1527579MemoryTrain:  epoch  9, batch     0 | loss: 0.1312182

F1 score per class: {1: 0.0, 6: 0.0, 7: 0.0, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 21: 0.0, 26: 0.5714285714285714, 27: 0.0, 28: 0.0, 31: 0.7222222222222222}
Micro-average F1 score: 0.6451612903225806
Weighted-average F1 score: 0.562816712445496
F1 score per class: {3: 0.0, 6: 0.0, 7: 0.0, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 22: 0.0, 26: 0.6956521739130435, 27: 0.0, 28: 1.0, 31: 0.921875}
Micro-average F1 score: 0.8392857142857143
Weighted-average F1 score: 0.8070802332790823
F1 score per class: {3: 0.0, 6: 0.0, 7: 0.0, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.6956521739130435, 27: 0.0, 28: 1.0, 31: 0.921875}
Micro-average F1 score: 0.8392857142857143
Weighted-average F1 score: 0.8070802332790823

F1 score per class: {0: 0.9142857142857143, 1: 0.21568627450980393, 2: 0.5714285714285714, 3: 0.39622641509433965, 4: 0.9010989010989011, 5: 0.8818181818181818, 6: 0.4788732394366197, 7: 0.0, 8: 0.26262626262626265, 9: 0.9803921568627451, 10: 0.11009174311926606, 11: 0.41284403669724773, 12: 0.4645161290322581, 13: 0.05714285714285714, 14: 0.028169014084507043, 15: 0.75, 16: 0.7457627118644068, 17: 0.0, 18: 0.04878048780487805, 19: 0.6914893617021277, 20: 0.6666666666666666, 21: 0.15384615384615385, 22: 0.5793103448275863, 23: 0.7, 24: 0.0, 25: 0.43478260869565216, 26: 0.700507614213198, 27: 0.1935483870967742, 28: 0.14285714285714285, 29: 0.8913043478260869, 30: 0.972972972972973, 31: 0.0, 32: 0.6832298136645962, 33: 0.3076923076923077, 34: 0.11940298507462686, 35: 0.43243243243243246, 36: 0.14084507042253522, 37: 0.36363636363636365, 38: 0.125, 39: 0.1935483870967742, 40: 0.6}
Micro-average F1 score: 0.5281118881118881
Weighted-average F1 score: 0.5667362862409738
F1 score per class: {0: 0.958904109589041, 1: 0.3404255319148936, 2: 0.5185185185185185, 3: 0.8152866242038217, 4: 0.8950276243093923, 5: 0.88, 6: 0.4647887323943662, 7: 0.0, 8: 0.5142857142857142, 9: 0.9803921568627451, 10: 0.5179856115107914, 11: 0.6042553191489362, 12: 0.5909090909090909, 13: 0.08, 14: 0.07142857142857142, 15: 0.75, 16: 0.8333333333333334, 17: 0.0, 18: 0.34375, 19: 0.7931034482758621, 20: 0.7964601769911505, 21: 0.3466666666666667, 22: 0.6369426751592356, 23: 0.7441860465116279, 24: 0.125, 25: 0.5205479452054794, 26: 0.68, 27: 0.22857142857142856, 28: 0.23529411764705882, 29: 0.9032258064516129, 30: 0.9473684210526315, 31: 0.8, 32: 0.7325581395348837, 33: 0.375, 34: 0.4418604651162791, 35: 0.7157894736842105, 36: 0.7936507936507936, 37: 0.46153846153846156, 38: 0.42857142857142855, 39: 0.3111111111111111, 40: 0.7329192546583851}
Micro-average F1 score: 0.6321754126632175
Weighted-average F1 score: 0.6249323513822372
F1 score per class: {0: 0.958904109589041, 1: 0.26373626373626374, 2: 0.4827586206896552, 3: 0.7843137254901961, 4: 0.9010989010989011, 5: 0.8918918918918919, 6: 0.4755244755244755, 7: 0.0, 8: 0.416, 9: 0.9803921568627451, 10: 0.375, 11: 0.5576208178438662, 12: 0.5760869565217391, 13: 0.05555555555555555, 14: 0.07317073170731707, 15: 0.6666666666666666, 16: 0.819672131147541, 17: 0.0, 18: 0.23076923076923078, 19: 0.8, 20: 0.7894736842105263, 21: 0.3291139240506329, 22: 0.6081081081081081, 23: 0.7317073170731707, 24: 0.1276595744680851, 25: 0.5, 26: 0.68, 27: 0.2077922077922078, 28: 0.20689655172413793, 29: 0.8972972972972973, 30: 0.9473684210526315, 31: 0.8, 32: 0.7251461988304093, 33: 0.3076923076923077, 34: 0.38461538461538464, 35: 0.6666666666666666, 36: 0.5416666666666666, 37: 0.4318181818181818, 38: 0.4186046511627907, 39: 0.3181818181818182, 40: 0.7375}
Micro-average F1 score: 0.6025832091405862
Weighted-average F1 score: 0.597885507835975
cur_acc:  ['0.7731', '0.7134', '0.6684', '0.5710', '0.6726', '0.7848', '0.5906', '0.6452']
his_acc:  ['0.7731', '0.7886', '0.7219', '0.5868', '0.5606', '0.5659', '0.5429', '0.5281']
cur_acc des:  ['0.8123', '0.8918', '0.8655', '0.5511', '0.7486', '0.8298', '0.6617', '0.8393']
his_acc des:  ['0.8123', '0.8526', '0.7760', '0.6581', '0.6640', '0.6799', '0.6405', '0.6322']
cur_acc rrf:  ['0.8123', '0.8679', '0.8378', '0.5905', '0.7290', '0.8131', '0.7003', '0.8393']
his_acc rrf:  ['0.8123', '0.8389', '0.7685', '0.6380', '0.6489', '0.6599', '0.6149', '0.6026']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 233.4501344CurrentTrain: epoch  0, batch     1 | loss: 211.3254790CurrentTrain: epoch  0, batch     2 | loss: 234.8605443CurrentTrain: epoch  0, batch     3 | loss: 277.6110650CurrentTrain: epoch  0, batch     4 | loss: 188.5828580CurrentTrain: epoch  0, batch     5 | loss: 261.9490312CurrentTrain: epoch  0, batch     6 | loss: 206.3842866CurrentTrain: epoch  0, batch     7 | loss: 219.0808477CurrentTrain: epoch  0, batch     8 | loss: 200.2635529CurrentTrain: epoch  0, batch     9 | loss: 211.4814366CurrentTrain: epoch  0, batch    10 | loss: 236.4808861CurrentTrain: epoch  0, batch    11 | loss: 224.9990099CurrentTrain: epoch  0, batch    12 | loss: 243.2106056CurrentTrain: epoch  0, batch    13 | loss: 259.6659916CurrentTrain: epoch  0, batch    14 | loss: 224.7557627CurrentTrain: epoch  0, batch    15 | loss: 188.9092715CurrentTrain: epoch  0, batch    16 | loss: 165.9554086CurrentTrain: epoch  0, batch    17 | loss: 179.0108897CurrentTrain: epoch  0, batch    18 | loss: 217.1490742CurrentTrain: epoch  0, batch    19 | loss: 205.3301723CurrentTrain: epoch  0, batch    20 | loss: 280.5397611CurrentTrain: epoch  0, batch    21 | loss: 197.5559588CurrentTrain: epoch  0, batch    22 | loss: 224.3856890CurrentTrain: epoch  0, batch    23 | loss: 223.4197323CurrentTrain: epoch  0, batch    24 | loss: 192.3046298CurrentTrain: epoch  0, batch    25 | loss: 235.6854077CurrentTrain: epoch  0, batch    26 | loss: 222.9752772CurrentTrain: epoch  0, batch    27 | loss: 190.7283844CurrentTrain: epoch  0, batch    28 | loss: 216.4456397CurrentTrain: epoch  0, batch    29 | loss: 184.9981553CurrentTrain: epoch  0, batch    30 | loss: 222.8641445CurrentTrain: epoch  0, batch    31 | loss: 190.8706813CurrentTrain: epoch  0, batch    32 | loss: 295.5819784CurrentTrain: epoch  0, batch    33 | loss: 223.0824209CurrentTrain: epoch  0, batch    34 | loss: 187.4799760CurrentTrain: epoch  0, batch    35 | loss: 249.2621617CurrentTrain: epoch  0, batch    36 | loss: 204.3228502CurrentTrain: epoch  0, batch    37 | loss: 196.2279313CurrentTrain: epoch  0, batch    38 | loss: 215.5090979CurrentTrain: epoch  0, batch    39 | loss: 189.3636357CurrentTrain: epoch  0, batch    40 | loss: 223.1434935CurrentTrain: epoch  0, batch    41 | loss: 174.6276889CurrentTrain: epoch  0, batch    42 | loss: 248.0550416CurrentTrain: epoch  0, batch    43 | loss: 255.9423187CurrentTrain: epoch  0, batch    44 | loss: 288.3506946CurrentTrain: epoch  0, batch    45 | loss: 189.5392991CurrentTrain: epoch  0, batch    46 | loss: 363.5897006CurrentTrain: epoch  0, batch    47 | loss: 189.3382798CurrentTrain: epoch  0, batch    48 | loss: 216.6223038CurrentTrain: epoch  0, batch    49 | loss: 202.3137276CurrentTrain: epoch  0, batch    50 | loss: 215.4575823CurrentTrain: epoch  0, batch    51 | loss: 217.0070887CurrentTrain: epoch  0, batch    52 | loss: 188.9675281CurrentTrain: epoch  0, batch    53 | loss: 188.4392849CurrentTrain: epoch  0, batch    54 | loss: 202.3899432CurrentTrain: epoch  0, batch    55 | loss: 175.8679341CurrentTrain: epoch  0, batch    56 | loss: 233.2778695CurrentTrain: epoch  0, batch    57 | loss: 196.0788760CurrentTrain: epoch  0, batch    58 | loss: 179.1111947CurrentTrain: epoch  0, batch    59 | loss: 188.5546615CurrentTrain: epoch  0, batch    60 | loss: 294.7772826CurrentTrain: epoch  0, batch    61 | loss: 230.3745389CurrentTrain: epoch  0, batch    62 | loss: 169.3056902CurrentTrain: epoch  0, batch    63 | loss: 254.2136961CurrentTrain: epoch  0, batch    64 | loss: 214.9587649CurrentTrain: epoch  0, batch    65 | loss: 213.8600678CurrentTrain: epoch  0, batch    66 | loss: 215.1158392CurrentTrain: epoch  0, batch    67 | loss: 215.1769304CurrentTrain: epoch  0, batch    68 | loss: 248.1864291CurrentTrain: epoch  0, batch    69 | loss: 284.7107412CurrentTrain: epoch  0, batch    70 | loss: 227.0937409CurrentTrain: epoch  0, batch    71 | loss: 182.6668861CurrentTrain: epoch  0, batch    72 | loss: 241.4049834CurrentTrain: epoch  0, batch    73 | loss: 284.1483214CurrentTrain: epoch  0, batch    74 | loss: 191.9544587CurrentTrain: epoch  0, batch    75 | loss: 220.4362307CurrentTrain: epoch  0, batch    76 | loss: 238.2732821CurrentTrain: epoch  0, batch    77 | loss: 207.3826849CurrentTrain: epoch  0, batch    78 | loss: 193.6650703CurrentTrain: epoch  0, batch    79 | loss: 222.1096196CurrentTrain: epoch  0, batch    80 | loss: 256.6820028CurrentTrain: epoch  0, batch    81 | loss: 238.8450287CurrentTrain: epoch  0, batch    82 | loss: 246.3757193CurrentTrain: epoch  0, batch    83 | loss: 230.0205584CurrentTrain: epoch  0, batch    84 | loss: 193.6401512CurrentTrain: epoch  0, batch    85 | loss: 193.0212757CurrentTrain: epoch  0, batch    86 | loss: 177.7366462CurrentTrain: epoch  0, batch    87 | loss: 173.1197295CurrentTrain: epoch  0, batch    88 | loss: 156.2387481CurrentTrain: epoch  0, batch    89 | loss: 214.6494123CurrentTrain: epoch  0, batch    90 | loss: 248.3239697CurrentTrain: epoch  0, batch    91 | loss: 193.8257203CurrentTrain: epoch  0, batch    92 | loss: 190.8940752CurrentTrain: epoch  0, batch    93 | loss: 229.7202423CurrentTrain: epoch  0, batch    94 | loss: 221.2373579CurrentTrain: epoch  0, batch    95 | loss: 150.8867710CurrentTrain: epoch  1, batch     0 | loss: 271.2906260CurrentTrain: epoch  1, batch     1 | loss: 189.9640002CurrentTrain: epoch  1, batch     2 | loss: 182.0249203CurrentTrain: epoch  1, batch     3 | loss: 290.6297047CurrentTrain: epoch  1, batch     4 | loss: 199.7020589CurrentTrain: epoch  1, batch     5 | loss: 225.7571174CurrentTrain: epoch  1, batch     6 | loss: 171.3358318CurrentTrain: epoch  1, batch     7 | loss: 292.8301464CurrentTrain: epoch  1, batch     8 | loss: 229.3519285CurrentTrain: epoch  1, batch     9 | loss: 174.1322141CurrentTrain: epoch  1, batch    10 | loss: 207.3374433CurrentTrain: epoch  1, batch    11 | loss: 224.4342725CurrentTrain: epoch  1, batch    12 | loss: 237.4214582CurrentTrain: epoch  1, batch    13 | loss: 230.8435761CurrentTrain: epoch  1, batch    14 | loss: 243.9804503CurrentTrain: epoch  1, batch    15 | loss: 190.0647343CurrentTrain: epoch  1, batch    16 | loss: 278.3327201CurrentTrain: epoch  1, batch    17 | loss: 196.8610738CurrentTrain: epoch  1, batch    18 | loss: 218.6747594CurrentTrain: epoch  1, batch    19 | loss: 285.5077432CurrentTrain: epoch  1, batch    20 | loss: 363.0840397CurrentTrain: epoch  1, batch    21 | loss: 204.9504327CurrentTrain: epoch  1, batch    22 | loss: 196.1569470CurrentTrain: epoch  1, batch    23 | loss: 225.7385006CurrentTrain: epoch  1, batch    24 | loss: 230.9343803CurrentTrain: epoch  1, batch    25 | loss: 252.7207495CurrentTrain: epoch  1, batch    26 | loss: 169.3129414CurrentTrain: epoch  1, batch    27 | loss: 216.3458589CurrentTrain: epoch  1, batch    28 | loss: 211.5322252CurrentTrain: epoch  1, batch    29 | loss: 156.5551401CurrentTrain: epoch  1, batch    30 | loss: 193.5336310CurrentTrain: epoch  1, batch    31 | loss: 183.8794670CurrentTrain: epoch  1, batch    32 | loss: 165.5538150CurrentTrain: epoch  1, batch    33 | loss: 199.2592105CurrentTrain: epoch  1, batch    34 | loss: 180.5638340CurrentTrain: epoch  1, batch    35 | loss: 155.4392254CurrentTrain: epoch  1, batch    36 | loss: 291.1501768CurrentTrain: epoch  1, batch    37 | loss: 197.0179783CurrentTrain: epoch  1, batch    38 | loss: 234.1974789CurrentTrain: epoch  1, batch    39 | loss: 223.6984442CurrentTrain: epoch  1, batch    40 | loss: 206.1253289CurrentTrain: epoch  1, batch    41 | loss: 355.1163825CurrentTrain: epoch  1, batch    42 | loss: 197.7220689CurrentTrain: epoch  1, batch    43 | loss: 275.6715416CurrentTrain: epoch  1, batch    44 | loss: 174.7384886CurrentTrain: epoch  1, batch    45 | loss: 186.6141975CurrentTrain: epoch  1, batch    46 | loss: 191.6056424CurrentTrain: epoch  1, batch    47 | loss: 365.4761644CurrentTrain: epoch  1, batch    48 | loss: 222.2923056CurrentTrain: epoch  1, batch    49 | loss: 208.0204062CurrentTrain: epoch  1, batch    50 | loss: 254.6126538CurrentTrain: epoch  1, batch    51 | loss: 215.5719406CurrentTrain: epoch  1, batch    52 | loss: 189.4086414CurrentTrain: epoch  1, batch    53 | loss: 215.7382022CurrentTrain: epoch  1, batch    54 | loss: 171.1938727CurrentTrain: epoch  1, batch    55 | loss: 282.8263357CurrentTrain: epoch  1, batch    56 | loss: 181.6443821CurrentTrain: epoch  1, batch    57 | loss: 232.1803186CurrentTrain: epoch  1, batch    58 | loss: 226.5134434CurrentTrain: epoch  1, batch    59 | loss: 204.2633133CurrentTrain: epoch  1, batch    60 | loss: 192.4089746CurrentTrain: epoch  1, batch    61 | loss: 169.6043852CurrentTrain: epoch  1, batch    62 | loss: 200.1527882CurrentTrain: epoch  1, batch    63 | loss: 193.8905683CurrentTrain: epoch  1, batch    64 | loss: 186.8283771CurrentTrain: epoch  1, batch    65 | loss: 210.2628016CurrentTrain: epoch  1, batch    66 | loss: 251.5072996CurrentTrain: epoch  1, batch    67 | loss: 217.3401047CurrentTrain: epoch  1, batch    68 | loss: 166.6567785CurrentTrain: epoch  1, batch    69 | loss: 222.1941242CurrentTrain: epoch  1, batch    70 | loss: 223.4763359CurrentTrain: epoch  1, batch    71 | loss: 145.9835950CurrentTrain: epoch  1, batch    72 | loss: 227.9836651CurrentTrain: epoch  1, batch    73 | loss: 179.9610525CurrentTrain: epoch  1, batch    74 | loss: 201.3493452CurrentTrain: epoch  1, batch    75 | loss: 179.6501819CurrentTrain: epoch  1, batch    76 | loss: 187.6979764CurrentTrain: epoch  1, batch    77 | loss: 198.9815641CurrentTrain: epoch  1, batch    78 | loss: 239.8961013CurrentTrain: epoch  1, batch    79 | loss: 189.0059720CurrentTrain: epoch  1, batch    80 | loss: 197.2250053CurrentTrain: epoch  1, batch    81 | loss: 191.2141999CurrentTrain: epoch  1, batch    82 | loss: 279.4278972CurrentTrain: epoch  1, batch    83 | loss: 179.4068642CurrentTrain: epoch  1, batch    84 | loss: 272.6563624CurrentTrain: epoch  1, batch    85 | loss: 248.8875861CurrentTrain: epoch  1, batch    86 | loss: 223.8789456CurrentTrain: epoch  1, batch    87 | loss: 163.4790417CurrentTrain: epoch  1, batch    88 | loss: 194.0351758CurrentTrain: epoch  1, batch    89 | loss: 192.8805249CurrentTrain: epoch  1, batch    90 | loss: 174.4705770CurrentTrain: epoch  1, batch    91 | loss: 233.9105504CurrentTrain: epoch  1, batch    92 | loss: 186.0155284CurrentTrain: epoch  1, batch    93 | loss: 199.4774056CurrentTrain: epoch  1, batch    94 | loss: 234.8510144CurrentTrain: epoch  1, batch    95 | loss: 155.7318652CurrentTrain: epoch  2, batch     0 | loss: 279.8150823CurrentTrain: epoch  2, batch     1 | loss: 231.8177570CurrentTrain: epoch  2, batch     2 | loss: 231.4474786CurrentTrain: epoch  2, batch     3 | loss: 222.8628917CurrentTrain: epoch  2, batch     4 | loss: 214.6793294CurrentTrain: epoch  2, batch     5 | loss: 152.1677747CurrentTrain: epoch  2, batch     6 | loss: 180.7562198CurrentTrain: epoch  2, batch     7 | loss: 177.9537065CurrentTrain: epoch  2, batch     8 | loss: 251.0213323CurrentTrain: epoch  2, batch     9 | loss: 207.2439616CurrentTrain: epoch  2, batch    10 | loss: 179.1237291CurrentTrain: epoch  2, batch    11 | loss: 174.0283401CurrentTrain: epoch  2, batch    12 | loss: 186.8752769CurrentTrain: epoch  2, batch    13 | loss: 251.5631804CurrentTrain: epoch  2, batch    14 | loss: 192.4661076CurrentTrain: epoch  2, batch    15 | loss: 221.9456740CurrentTrain: epoch  2, batch    16 | loss: 195.9540782CurrentTrain: epoch  2, batch    17 | loss: 151.7712618CurrentTrain: epoch  2, batch    18 | loss: 199.2455144CurrentTrain: epoch  2, batch    19 | loss: 278.6604453CurrentTrain: epoch  2, batch    20 | loss: 158.9299147CurrentTrain: epoch  2, batch    21 | loss: 174.9125060CurrentTrain: epoch  2, batch    22 | loss: 163.3478652CurrentTrain: epoch  2, batch    23 | loss: 281.4418620CurrentTrain: epoch  2, batch    24 | loss: 250.1158473CurrentTrain: epoch  2, batch    25 | loss: 198.2406366CurrentTrain: epoch  2, batch    26 | loss: 186.8268778CurrentTrain: epoch  2, batch    27 | loss: 239.7018745CurrentTrain: epoch  2, batch    28 | loss: 180.6873196CurrentTrain: epoch  2, batch    29 | loss: 213.4299556CurrentTrain: epoch  2, batch    30 | loss: 181.4306636CurrentTrain: epoch  2, batch    31 | loss: 250.8612035CurrentTrain: epoch  2, batch    32 | loss: 287.1801318CurrentTrain: epoch  2, batch    33 | loss: 240.3025105CurrentTrain: epoch  2, batch    34 | loss: 222.3288096CurrentTrain: epoch  2, batch    35 | loss: 288.3012723CurrentTrain: epoch  2, batch    36 | loss: 189.3198236CurrentTrain: epoch  2, batch    37 | loss: 205.7082832CurrentTrain: epoch  2, batch    38 | loss: 194.1105282CurrentTrain: epoch  2, batch    39 | loss: 220.7532248CurrentTrain: epoch  2, batch    40 | loss: 216.7558421CurrentTrain: epoch  2, batch    41 | loss: 197.2819684CurrentTrain: epoch  2, batch    42 | loss: 223.4704488CurrentTrain: epoch  2, batch    43 | loss: 222.4185175CurrentTrain: epoch  2, batch    44 | loss: 201.5467403CurrentTrain: epoch  2, batch    45 | loss: 205.5304257CurrentTrain: epoch  2, batch    46 | loss: 187.1497408CurrentTrain: epoch  2, batch    47 | loss: 240.6236900CurrentTrain: epoch  2, batch    48 | loss: 150.6062747CurrentTrain: epoch  2, batch    49 | loss: 237.9929357CurrentTrain: epoch  2, batch    50 | loss: 177.9991097CurrentTrain: epoch  2, batch    51 | loss: 203.5128694CurrentTrain: epoch  2, batch    52 | loss: 241.3046348CurrentTrain: epoch  2, batch    53 | loss: 231.9784840CurrentTrain: epoch  2, batch    54 | loss: 164.1046558CurrentTrain: epoch  2, batch    55 | loss: 143.9997931CurrentTrain: epoch  2, batch    56 | loss: 351.9510756CurrentTrain: epoch  2, batch    57 | loss: 233.2414737CurrentTrain: epoch  2, batch    58 | loss: 190.0794611CurrentTrain: epoch  2, batch    59 | loss: 240.0885242CurrentTrain: epoch  2, batch    60 | loss: 206.7531107CurrentTrain: epoch  2, batch    61 | loss: 190.3242336CurrentTrain: epoch  2, batch    62 | loss: 353.3311565CurrentTrain: epoch  2, batch    63 | loss: 230.1650659CurrentTrain: epoch  2, batch    64 | loss: 241.1026369CurrentTrain: epoch  2, batch    65 | loss: 279.2363213CurrentTrain: epoch  2, batch    66 | loss: 287.2888590CurrentTrain: epoch  2, batch    67 | loss: 213.9879927CurrentTrain: epoch  2, batch    68 | loss: 173.0582710CurrentTrain: epoch  2, batch    69 | loss: 242.4925631CurrentTrain: epoch  2, batch    70 | loss: 231.1530677CurrentTrain: epoch  2, batch    71 | loss: 212.3925844CurrentTrain: epoch  2, batch    72 | loss: 171.6750542CurrentTrain: epoch  2, batch    73 | loss: 206.2662091CurrentTrain: epoch  2, batch    74 | loss: 232.8564635CurrentTrain: epoch  2, batch    75 | loss: 185.2386461CurrentTrain: epoch  2, batch    76 | loss: 249.7819807CurrentTrain: epoch  2, batch    77 | loss: 249.0051339CurrentTrain: epoch  2, batch    78 | loss: 174.4387392CurrentTrain: epoch  2, batch    79 | loss: 196.9948386CurrentTrain: epoch  2, batch    80 | loss: 182.4712659CurrentTrain: epoch  2, batch    81 | loss: 171.7074859CurrentTrain: epoch  2, batch    82 | loss: 224.6744877CurrentTrain: epoch  2, batch    83 | loss: 239.9056108CurrentTrain: epoch  2, batch    84 | loss: 156.2578178CurrentTrain: epoch  2, batch    85 | loss: 236.5689882CurrentTrain: epoch  2, batch    86 | loss: 149.2735991CurrentTrain: epoch  2, batch    87 | loss: 213.6713161CurrentTrain: epoch  2, batch    88 | loss: 164.6204451CurrentTrain: epoch  2, batch    89 | loss: 186.8644956CurrentTrain: epoch  2, batch    90 | loss: 221.5994417CurrentTrain: epoch  2, batch    91 | loss: 234.7145143CurrentTrain: epoch  2, batch    92 | loss: 179.9805352CurrentTrain: epoch  2, batch    93 | loss: 207.4175231CurrentTrain: epoch  2, batch    94 | loss: 215.6975895CurrentTrain: epoch  2, batch    95 | loss: 166.0968213CurrentTrain: epoch  3, batch     0 | loss: 196.5028368CurrentTrain: epoch  3, batch     1 | loss: 193.3581821CurrentTrain: epoch  3, batch     2 | loss: 205.1412649CurrentTrain: epoch  3, batch     3 | loss: 187.0659507CurrentTrain: epoch  3, batch     4 | loss: 276.7587494CurrentTrain: epoch  3, batch     5 | loss: 195.5242777CurrentTrain: epoch  3, batch     6 | loss: 202.9690153CurrentTrain: epoch  3, batch     7 | loss: 223.4299939CurrentTrain: epoch  3, batch     8 | loss: 176.9796655CurrentTrain: epoch  3, batch     9 | loss: 241.9745079CurrentTrain: epoch  3, batch    10 | loss: 204.0331912CurrentTrain: epoch  3, batch    11 | loss: 148.6907149CurrentTrain: epoch  3, batch    12 | loss: 203.0087189CurrentTrain: epoch  3, batch    13 | loss: 206.3930808CurrentTrain: epoch  3, batch    14 | loss: 206.6659012CurrentTrain: epoch  3, batch    15 | loss: 242.8140150CurrentTrain: epoch  3, batch    16 | loss: 222.1620261CurrentTrain: epoch  3, batch    17 | loss: 200.2052317CurrentTrain: epoch  3, batch    18 | loss: 205.4510831CurrentTrain: epoch  3, batch    19 | loss: 221.3111379CurrentTrain: epoch  3, batch    20 | loss: 278.4458878CurrentTrain: epoch  3, batch    21 | loss: 162.5931718CurrentTrain: epoch  3, batch    22 | loss: 179.5879529CurrentTrain: epoch  3, batch    23 | loss: 212.9091453CurrentTrain: epoch  3, batch    24 | loss: 149.0825886CurrentTrain: epoch  3, batch    25 | loss: 204.5992183CurrentTrain: epoch  3, batch    26 | loss: 238.2217290CurrentTrain: epoch  3, batch    27 | loss: 164.7829160CurrentTrain: epoch  3, batch    28 | loss: 157.3899766CurrentTrain: epoch  3, batch    29 | loss: 189.8188875CurrentTrain: epoch  3, batch    30 | loss: 156.2288667CurrentTrain: epoch  3, batch    31 | loss: 205.0093111CurrentTrain: epoch  3, batch    32 | loss: 187.3993348CurrentTrain: epoch  3, batch    33 | loss: 185.1892409CurrentTrain: epoch  3, batch    34 | loss: 186.2334793CurrentTrain: epoch  3, batch    35 | loss: 163.8086283CurrentTrain: epoch  3, batch    36 | loss: 163.6698007CurrentTrain: epoch  3, batch    37 | loss: 172.8600801CurrentTrain: epoch  3, batch    38 | loss: 180.5385193CurrentTrain: epoch  3, batch    39 | loss: 178.3166098CurrentTrain: epoch  3, batch    40 | loss: 204.3240198CurrentTrain: epoch  3, batch    41 | loss: 207.8242535CurrentTrain: epoch  3, batch    42 | loss: 223.4119953CurrentTrain: epoch  3, batch    43 | loss: 238.3421955CurrentTrain: epoch  3, batch    44 | loss: 228.7196272CurrentTrain: epoch  3, batch    45 | loss: 197.0558953CurrentTrain: epoch  3, batch    46 | loss: 237.7848176CurrentTrain: epoch  3, batch    47 | loss: 172.5643653CurrentTrain: epoch  3, batch    48 | loss: 209.7340834CurrentTrain: epoch  3, batch    49 | loss: 239.5957032CurrentTrain: epoch  3, batch    50 | loss: 282.6270974CurrentTrain: epoch  3, batch    51 | loss: 240.1564437CurrentTrain: epoch  3, batch    52 | loss: 161.4143596CurrentTrain: epoch  3, batch    53 | loss: 199.5301812CurrentTrain: epoch  3, batch    54 | loss: 238.8291020CurrentTrain: epoch  3, batch    55 | loss: 155.3892658CurrentTrain: epoch  3, batch    56 | loss: 220.7577447CurrentTrain: epoch  3, batch    57 | loss: 136.7761703CurrentTrain: epoch  3, batch    58 | loss: 257.8978167CurrentTrain: epoch  3, batch    59 | loss: 177.0072599CurrentTrain: epoch  3, batch    60 | loss: 222.3014289CurrentTrain: epoch  3, batch    61 | loss: 277.3101418CurrentTrain: epoch  3, batch    62 | loss: 276.5338759CurrentTrain: epoch  3, batch    63 | loss: 157.8802882CurrentTrain: epoch  3, batch    64 | loss: 237.4506650CurrentTrain: epoch  3, batch    65 | loss: 246.7491313CurrentTrain: epoch  3, batch    66 | loss: 225.1869418CurrentTrain: epoch  3, batch    67 | loss: 186.3173248CurrentTrain: epoch  3, batch    68 | loss: 176.1218341CurrentTrain: epoch  3, batch    69 | loss: 221.2612672CurrentTrain: epoch  3, batch    70 | loss: 231.8215928CurrentTrain: epoch  3, batch    71 | loss: 237.9410692CurrentTrain: epoch  3, batch    72 | loss: 231.6496133CurrentTrain: epoch  3, batch    73 | loss: 246.8631678CurrentTrain: epoch  3, batch    74 | loss: 185.4006148CurrentTrain: epoch  3, batch    75 | loss: 197.9015550CurrentTrain: epoch  3, batch    76 | loss: 238.7172677CurrentTrain: epoch  3, batch    77 | loss: 217.7472004CurrentTrain: epoch  3, batch    78 | loss: 247.7882622CurrentTrain: epoch  3, batch    79 | loss: 197.2958510CurrentTrain: epoch  3, batch    80 | loss: 197.7167887CurrentTrain: epoch  3, batch    81 | loss: 289.2328932CurrentTrain: epoch  3, batch    82 | loss: 287.9766481CurrentTrain: epoch  3, batch    83 | loss: 229.5022655CurrentTrain: epoch  3, batch    84 | loss: 183.6345424CurrentTrain: epoch  3, batch    85 | loss: 199.1665374CurrentTrain: epoch  3, batch    86 | loss: 204.7413035CurrentTrain: epoch  3, batch    87 | loss: 239.7243117CurrentTrain: epoch  3, batch    88 | loss: 201.7515564CurrentTrain: epoch  3, batch    89 | loss: 237.9193118CurrentTrain: epoch  3, batch    90 | loss: 190.5490485CurrentTrain: epoch  3, batch    91 | loss: 184.7771886CurrentTrain: epoch  3, batch    92 | loss: 197.0987880CurrentTrain: epoch  3, batch    93 | loss: 247.5346181CurrentTrain: epoch  3, batch    94 | loss: 192.9686929CurrentTrain: epoch  3, batch    95 | loss: 197.2608919CurrentTrain: epoch  4, batch     0 | loss: 237.7032387CurrentTrain: epoch  4, batch     1 | loss: 196.8361385CurrentTrain: epoch  4, batch     2 | loss: 222.1577283CurrentTrain: epoch  4, batch     3 | loss: 221.5670148CurrentTrain: epoch  4, batch     4 | loss: 176.1362630CurrentTrain: epoch  4, batch     5 | loss: 238.9442063CurrentTrain: epoch  4, batch     6 | loss: 228.7724555CurrentTrain: epoch  4, batch     7 | loss: 359.5623226CurrentTrain: epoch  4, batch     8 | loss: 168.8576873CurrentTrain: epoch  4, batch     9 | loss: 230.1437107CurrentTrain: epoch  4, batch    10 | loss: 197.3227387CurrentTrain: epoch  4, batch    11 | loss: 169.9564666CurrentTrain: epoch  4, batch    12 | loss: 177.8701234CurrentTrain: epoch  4, batch    13 | loss: 192.0751751CurrentTrain: epoch  4, batch    14 | loss: 193.1100071CurrentTrain: epoch  4, batch    15 | loss: 150.3357634CurrentTrain: epoch  4, batch    16 | loss: 153.7579946CurrentTrain: epoch  4, batch    17 | loss: 194.1999727CurrentTrain: epoch  4, batch    18 | loss: 167.8174107CurrentTrain: epoch  4, batch    19 | loss: 210.7725292CurrentTrain: epoch  4, batch    20 | loss: 238.3125714CurrentTrain: epoch  4, batch    21 | loss: 170.8594847CurrentTrain: epoch  4, batch    22 | loss: 210.6703813CurrentTrain: epoch  4, batch    23 | loss: 278.4691017CurrentTrain: epoch  4, batch    24 | loss: 287.5201397CurrentTrain: epoch  4, batch    25 | loss: 197.3495735CurrentTrain: epoch  4, batch    26 | loss: 195.4997588CurrentTrain: epoch  4, batch    27 | loss: 178.8157280CurrentTrain: epoch  4, batch    28 | loss: 278.1319709CurrentTrain: epoch  4, batch    29 | loss: 219.9492262CurrentTrain: epoch  4, batch    30 | loss: 219.5418110CurrentTrain: epoch  4, batch    31 | loss: 268.9443244CurrentTrain: epoch  4, batch    32 | loss: 220.1373532CurrentTrain: epoch  4, batch    33 | loss: 175.7869548CurrentTrain: epoch  4, batch    34 | loss: 195.6221544CurrentTrain: epoch  4, batch    35 | loss: 247.6492656CurrentTrain: epoch  4, batch    36 | loss: 198.3702005CurrentTrain: epoch  4, batch    37 | loss: 179.8070712CurrentTrain: epoch  4, batch    38 | loss: 173.2973425CurrentTrain: epoch  4, batch    39 | loss: 188.2450529CurrentTrain: epoch  4, batch    40 | loss: 155.2478119CurrentTrain: epoch  4, batch    41 | loss: 346.7145256CurrentTrain: epoch  4, batch    42 | loss: 233.9353953CurrentTrain: epoch  4, batch    43 | loss: 162.9513662CurrentTrain: epoch  4, batch    44 | loss: 267.1724770CurrentTrain: epoch  4, batch    45 | loss: 193.0052140CurrentTrain: epoch  4, batch    46 | loss: 161.7562783CurrentTrain: epoch  4, batch    47 | loss: 276.7297443CurrentTrain: epoch  4, batch    48 | loss: 242.1152068CurrentTrain: epoch  4, batch    49 | loss: 170.6030013CurrentTrain: epoch  4, batch    50 | loss: 248.7637391CurrentTrain: epoch  4, batch    51 | loss: 201.3211733CurrentTrain: epoch  4, batch    52 | loss: 285.0912419CurrentTrain: epoch  4, batch    53 | loss: 241.8961969CurrentTrain: epoch  4, batch    54 | loss: 246.7113189CurrentTrain: epoch  4, batch    55 | loss: 175.1950661CurrentTrain: epoch  4, batch    56 | loss: 230.7797933CurrentTrain: epoch  4, batch    57 | loss: 163.8376254CurrentTrain: epoch  4, batch    58 | loss: 215.7888505CurrentTrain: epoch  4, batch    59 | loss: 249.7889675CurrentTrain: epoch  4, batch    60 | loss: 176.7512031CurrentTrain: epoch  4, batch    61 | loss: 221.0254759CurrentTrain: epoch  4, batch    62 | loss: 219.5299160CurrentTrain: epoch  4, batch    63 | loss: 228.5571019CurrentTrain: epoch  4, batch    64 | loss: 214.1467140CurrentTrain: epoch  4, batch    65 | loss: 228.5721720CurrentTrain: epoch  4, batch    66 | loss: 212.6927470CurrentTrain: epoch  4, batch    67 | loss: 156.0102432CurrentTrain: epoch  4, batch    68 | loss: 170.6178009CurrentTrain: epoch  4, batch    69 | loss: 154.9779457CurrentTrain: epoch  4, batch    70 | loss: 214.1237253CurrentTrain: epoch  4, batch    71 | loss: 207.8205962CurrentTrain: epoch  4, batch    72 | loss: 190.5659240CurrentTrain: epoch  4, batch    73 | loss: 222.0680871CurrentTrain: epoch  4, batch    74 | loss: 225.1736145CurrentTrain: epoch  4, batch    75 | loss: 239.5638211CurrentTrain: epoch  4, batch    76 | loss: 182.9248979CurrentTrain: epoch  4, batch    77 | loss: 258.5411410CurrentTrain: epoch  4, batch    78 | loss: 179.5502464CurrentTrain: epoch  4, batch    79 | loss: 219.7159063CurrentTrain: epoch  4, batch    80 | loss: 172.2575045CurrentTrain: epoch  4, batch    81 | loss: 167.9599634CurrentTrain: epoch  4, batch    82 | loss: 228.9270555CurrentTrain: epoch  4, batch    83 | loss: 194.9987728CurrentTrain: epoch  4, batch    84 | loss: 269.7896379CurrentTrain: epoch  4, batch    85 | loss: 186.3564514CurrentTrain: epoch  4, batch    86 | loss: 223.7675598CurrentTrain: epoch  4, batch    87 | loss: 220.8216839CurrentTrain: epoch  4, batch    88 | loss: 247.0582293CurrentTrain: epoch  4, batch    89 | loss: 173.9626766CurrentTrain: epoch  4, batch    90 | loss: 195.7513279CurrentTrain: epoch  4, batch    91 | loss: 203.9923175CurrentTrain: epoch  4, batch    92 | loss: 203.6361415CurrentTrain: epoch  4, batch    93 | loss: 276.7545456CurrentTrain: epoch  4, batch    94 | loss: 220.3281659CurrentTrain: epoch  4, batch    95 | loss: 138.7560302CurrentTrain: epoch  5, batch     0 | loss: 202.7229856CurrentTrain: epoch  5, batch     1 | loss: 183.2991922CurrentTrain: epoch  5, batch     2 | loss: 195.3730798CurrentTrain: epoch  5, batch     3 | loss: 213.2031425CurrentTrain: epoch  5, batch     4 | loss: 207.0473201CurrentTrain: epoch  5, batch     5 | loss: 195.5427179CurrentTrain: epoch  5, batch     6 | loss: 260.2423527CurrentTrain: epoch  5, batch     7 | loss: 212.3013062CurrentTrain: epoch  5, batch     8 | loss: 200.1101622CurrentTrain: epoch  5, batch     9 | loss: 211.5474992CurrentTrain: epoch  5, batch    10 | loss: 200.6674010CurrentTrain: epoch  5, batch    11 | loss: 212.9072809CurrentTrain: epoch  5, batch    12 | loss: 163.4252717CurrentTrain: epoch  5, batch    13 | loss: 161.3231680CurrentTrain: epoch  5, batch    14 | loss: 222.9179068CurrentTrain: epoch  5, batch    15 | loss: 195.7681101CurrentTrain: epoch  5, batch    16 | loss: 211.6862663CurrentTrain: epoch  5, batch    17 | loss: 133.5228817CurrentTrain: epoch  5, batch    18 | loss: 239.1744423CurrentTrain: epoch  5, batch    19 | loss: 359.6160143CurrentTrain: epoch  5, batch    20 | loss: 219.9156694CurrentTrain: epoch  5, batch    21 | loss: 212.9005172CurrentTrain: epoch  5, batch    22 | loss: 221.5777973CurrentTrain: epoch  5, batch    23 | loss: 219.7381972CurrentTrain: epoch  5, batch    24 | loss: 202.8155354CurrentTrain: epoch  5, batch    25 | loss: 202.2108275CurrentTrain: epoch  5, batch    26 | loss: 168.1864911CurrentTrain: epoch  5, batch    27 | loss: 212.1623401CurrentTrain: epoch  5, batch    28 | loss: 212.1138674CurrentTrain: epoch  5, batch    29 | loss: 266.2164227CurrentTrain: epoch  5, batch    30 | loss: 200.0655164CurrentTrain: epoch  5, batch    31 | loss: 276.1684093CurrentTrain: epoch  5, batch    32 | loss: 232.3201558CurrentTrain: epoch  5, batch    33 | loss: 237.7690883CurrentTrain: epoch  5, batch    34 | loss: 276.2055330CurrentTrain: epoch  5, batch    35 | loss: 194.3766849CurrentTrain: epoch  5, batch    36 | loss: 211.9237948CurrentTrain: epoch  5, batch    37 | loss: 278.5617911CurrentTrain: epoch  5, batch    38 | loss: 287.2305115CurrentTrain: epoch  5, batch    39 | loss: 286.4035695CurrentTrain: epoch  5, batch    40 | loss: 212.8944398CurrentTrain: epoch  5, batch    41 | loss: 155.5052760CurrentTrain: epoch  5, batch    42 | loss: 194.7435515CurrentTrain: epoch  5, batch    43 | loss: 184.7955356CurrentTrain: epoch  5, batch    44 | loss: 181.1913791CurrentTrain: epoch  5, batch    45 | loss: 196.8196919CurrentTrain: epoch  5, batch    46 | loss: 229.7782177CurrentTrain: epoch  5, batch    47 | loss: 194.1866830CurrentTrain: epoch  5, batch    48 | loss: 172.6384487CurrentTrain: epoch  5, batch    49 | loss: 203.9359949CurrentTrain: epoch  5, batch    50 | loss: 222.1182026CurrentTrain: epoch  5, batch    51 | loss: 185.8675844CurrentTrain: epoch  5, batch    52 | loss: 210.9036512CurrentTrain: epoch  5, batch    53 | loss: 227.9560259CurrentTrain: epoch  5, batch    54 | loss: 277.1471570CurrentTrain: epoch  5, batch    55 | loss: 146.5992433CurrentTrain: epoch  5, batch    56 | loss: 160.8181813CurrentTrain: epoch  5, batch    57 | loss: 212.1683779CurrentTrain: epoch  5, batch    58 | loss: 203.6559070CurrentTrain: epoch  5, batch    59 | loss: 160.3911528CurrentTrain: epoch  5, batch    60 | loss: 203.2952920CurrentTrain: epoch  5, batch    61 | loss: 230.0499619CurrentTrain: epoch  5, batch    62 | loss: 195.4376362CurrentTrain: epoch  5, batch    63 | loss: 317.2965614CurrentTrain: epoch  5, batch    64 | loss: 167.6109541CurrentTrain: epoch  5, batch    65 | loss: 169.0342777CurrentTrain: epoch  5, batch    66 | loss: 182.6474701CurrentTrain: epoch  5, batch    67 | loss: 160.1942216CurrentTrain: epoch  5, batch    68 | loss: 238.4549474CurrentTrain: epoch  5, batch    69 | loss: 179.5127167CurrentTrain: epoch  5, batch    70 | loss: 185.2205833CurrentTrain: epoch  5, batch    71 | loss: 195.1119694CurrentTrain: epoch  5, batch    72 | loss: 237.1128025CurrentTrain: epoch  5, batch    73 | loss: 221.0187539CurrentTrain: epoch  5, batch    74 | loss: 212.4976668CurrentTrain: epoch  5, batch    75 | loss: 185.2514386CurrentTrain: epoch  5, batch    76 | loss: 196.6126669CurrentTrain: epoch  5, batch    77 | loss: 287.1023608CurrentTrain: epoch  5, batch    78 | loss: 160.0079883CurrentTrain: epoch  5, batch    79 | loss: 201.8975475CurrentTrain: epoch  5, batch    80 | loss: 186.6831145CurrentTrain: epoch  5, batch    81 | loss: 202.6872096CurrentTrain: epoch  5, batch    82 | loss: 211.8130779CurrentTrain: epoch  5, batch    83 | loss: 212.3929825CurrentTrain: epoch  5, batch    84 | loss: 153.8756029CurrentTrain: epoch  5, batch    85 | loss: 211.2881795CurrentTrain: epoch  5, batch    86 | loss: 230.1989191CurrentTrain: epoch  5, batch    87 | loss: 237.7650742CurrentTrain: epoch  5, batch    88 | loss: 211.2400279CurrentTrain: epoch  5, batch    89 | loss: 168.3695342CurrentTrain: epoch  5, batch    90 | loss: 207.1018635CurrentTrain: epoch  5, batch    91 | loss: 175.3776753CurrentTrain: epoch  5, batch    92 | loss: 173.0700369CurrentTrain: epoch  5, batch    93 | loss: 238.6429402CurrentTrain: epoch  5, batch    94 | loss: 179.2767307CurrentTrain: epoch  5, batch    95 | loss: 162.9706824CurrentTrain: epoch  6, batch     0 | loss: 191.2344474CurrentTrain: epoch  6, batch     1 | loss: 204.4545257CurrentTrain: epoch  6, batch     2 | loss: 183.7321517CurrentTrain: epoch  6, batch     3 | loss: 219.7680148CurrentTrain: epoch  6, batch     4 | loss: 228.1069271CurrentTrain: epoch  6, batch     5 | loss: 127.9327669CurrentTrain: epoch  6, batch     6 | loss: 159.5767260CurrentTrain: epoch  6, batch     7 | loss: 241.0758244CurrentTrain: epoch  6, batch     8 | loss: 224.2522896CurrentTrain: epoch  6, batch     9 | loss: 160.2705692CurrentTrain: epoch  6, batch    10 | loss: 277.5147962CurrentTrain: epoch  6, batch    11 | loss: 237.0285647CurrentTrain: epoch  6, batch    12 | loss: 200.0804836CurrentTrain: epoch  6, batch    13 | loss: 164.6136990CurrentTrain: epoch  6, batch    14 | loss: 219.5309354CurrentTrain: epoch  6, batch    15 | loss: 202.7630969CurrentTrain: epoch  6, batch    16 | loss: 174.9133445CurrentTrain: epoch  6, batch    17 | loss: 139.6064356CurrentTrain: epoch  6, batch    18 | loss: 152.5003385CurrentTrain: epoch  6, batch    19 | loss: 145.8359908CurrentTrain: epoch  6, batch    20 | loss: 175.2054615CurrentTrain: epoch  6, batch    21 | loss: 204.4183471CurrentTrain: epoch  6, batch    22 | loss: 202.6773510CurrentTrain: epoch  6, batch    23 | loss: 247.3021341CurrentTrain: epoch  6, batch    24 | loss: 159.3271629CurrentTrain: epoch  6, batch    25 | loss: 276.8096889CurrentTrain: epoch  6, batch    26 | loss: 219.4768891CurrentTrain: epoch  6, batch    27 | loss: 219.3669751CurrentTrain: epoch  6, batch    28 | loss: 186.5083351CurrentTrain: epoch  6, batch    29 | loss: 218.9918387CurrentTrain: epoch  6, batch    30 | loss: 202.5312645CurrentTrain: epoch  6, batch    31 | loss: 193.6765116CurrentTrain: epoch  6, batch    32 | loss: 166.8560372CurrentTrain: epoch  6, batch    33 | loss: 219.7016890CurrentTrain: epoch  6, batch    34 | loss: 174.9560376CurrentTrain: epoch  6, batch    35 | loss: 203.1034221CurrentTrain: epoch  6, batch    36 | loss: 187.7485903CurrentTrain: epoch  6, batch    37 | loss: 220.0793824CurrentTrain: epoch  6, batch    38 | loss: 228.0027702CurrentTrain: epoch  6, batch    39 | loss: 212.3259897CurrentTrain: epoch  6, batch    40 | loss: 182.2021644CurrentTrain: epoch  6, batch    41 | loss: 195.4188308CurrentTrain: epoch  6, batch    42 | loss: 159.5469010CurrentTrain: epoch  6, batch    43 | loss: 194.6961827CurrentTrain: epoch  6, batch    44 | loss: 184.0812834CurrentTrain: epoch  6, batch    45 | loss: 202.7216029CurrentTrain: epoch  6, batch    46 | loss: 179.5623806CurrentTrain: epoch  6, batch    47 | loss: 220.0759990CurrentTrain: epoch  6, batch    48 | loss: 228.2687237CurrentTrain: epoch  6, batch    49 | loss: 210.5171328CurrentTrain: epoch  6, batch    50 | loss: 140.2901407CurrentTrain: epoch  6, batch    51 | loss: 185.4660148CurrentTrain: epoch  6, batch    52 | loss: 286.3394383CurrentTrain: epoch  6, batch    53 | loss: 276.9613752CurrentTrain: epoch  6, batch    54 | loss: 202.1099723CurrentTrain: epoch  6, batch    55 | loss: 175.4650103CurrentTrain: epoch  6, batch    56 | loss: 185.1570209CurrentTrain: epoch  6, batch    57 | loss: 202.9259313CurrentTrain: epoch  6, batch    58 | loss: 246.4661270CurrentTrain: epoch  6, batch    59 | loss: 210.4788243CurrentTrain: epoch  6, batch    60 | loss: 210.3917709CurrentTrain: epoch  6, batch    61 | loss: 237.9532167CurrentTrain: epoch  6, batch    62 | loss: 212.3336739CurrentTrain: epoch  6, batch    63 | loss: 210.7139814CurrentTrain: epoch  6, batch    64 | loss: 219.4827934CurrentTrain: epoch  6, batch    65 | loss: 193.8913713CurrentTrain: epoch  6, batch    66 | loss: 180.5491697CurrentTrain: epoch  6, batch    67 | loss: 197.3657887CurrentTrain: epoch  6, batch    68 | loss: 237.5333757CurrentTrain: epoch  6, batch    69 | loss: 227.8480117CurrentTrain: epoch  6, batch    70 | loss: 212.4540637CurrentTrain: epoch  6, batch    71 | loss: 190.4937023CurrentTrain: epoch  6, batch    72 | loss: 268.3031800CurrentTrain: epoch  6, batch    73 | loss: 167.5185924CurrentTrain: epoch  6, batch    74 | loss: 288.0435571CurrentTrain: epoch  6, batch    75 | loss: 202.5239550CurrentTrain: epoch  6, batch    76 | loss: 161.4152353CurrentTrain: epoch  6, batch    77 | loss: 202.4098516CurrentTrain: epoch  6, batch    78 | loss: 257.2711204CurrentTrain: epoch  6, batch    79 | loss: 237.3293856CurrentTrain: epoch  6, batch    80 | loss: 286.5667551CurrentTrain: epoch  6, batch    81 | loss: 164.9496035CurrentTrain: epoch  6, batch    82 | loss: 296.6837758CurrentTrain: epoch  6, batch    83 | loss: 160.0864053CurrentTrain: epoch  6, batch    84 | loss: 219.5620076CurrentTrain: epoch  6, batch    85 | loss: 210.9232570CurrentTrain: epoch  6, batch    86 | loss: 246.7279273CurrentTrain: epoch  6, batch    87 | loss: 205.1445483CurrentTrain: epoch  6, batch    88 | loss: 196.0553579CurrentTrain: epoch  6, batch    89 | loss: 237.8427296CurrentTrain: epoch  6, batch    90 | loss: 286.4297986CurrentTrain: epoch  6, batch    91 | loss: 266.6802507CurrentTrain: epoch  6, batch    92 | loss: 146.2043939CurrentTrain: epoch  6, batch    93 | loss: 221.0262167CurrentTrain: epoch  6, batch    94 | loss: 183.1637565CurrentTrain: epoch  6, batch    95 | loss: 165.2637978CurrentTrain: epoch  7, batch     0 | loss: 210.6931198CurrentTrain: epoch  7, batch     1 | loss: 270.4989475CurrentTrain: epoch  7, batch     2 | loss: 236.7921596CurrentTrain: epoch  7, batch     3 | loss: 211.8668390CurrentTrain: epoch  7, batch     4 | loss: 237.6446277CurrentTrain: epoch  7, batch     5 | loss: 210.6652007CurrentTrain: epoch  7, batch     6 | loss: 230.1434389CurrentTrain: epoch  7, batch     7 | loss: 176.0423559CurrentTrain: epoch  7, batch     8 | loss: 140.8124623CurrentTrain: epoch  7, batch     9 | loss: 219.8490908CurrentTrain: epoch  7, batch    10 | loss: 183.4674252CurrentTrain: epoch  7, batch    11 | loss: 149.3829145CurrentTrain: epoch  7, batch    12 | loss: 210.9817792CurrentTrain: epoch  7, batch    13 | loss: 286.3351844CurrentTrain: epoch  7, batch    14 | loss: 230.2186140CurrentTrain: epoch  7, batch    15 | loss: 138.9635427CurrentTrain: epoch  7, batch    16 | loss: 185.9569346CurrentTrain: epoch  7, batch    17 | loss: 182.6347064CurrentTrain: epoch  7, batch    18 | loss: 202.3588740CurrentTrain: epoch  7, batch    19 | loss: 219.8465375CurrentTrain: epoch  7, batch    20 | loss: 159.4231607CurrentTrain: epoch  7, batch    21 | loss: 228.1335315CurrentTrain: epoch  7, batch    22 | loss: 210.5415594CurrentTrain: epoch  7, batch    23 | loss: 239.2022957CurrentTrain: epoch  7, batch    24 | loss: 194.7876063CurrentTrain: epoch  7, batch    25 | loss: 203.4729234CurrentTrain: epoch  7, batch    26 | loss: 185.4533966CurrentTrain: epoch  7, batch    27 | loss: 194.2157818CurrentTrain: epoch  7, batch    28 | loss: 167.0956570CurrentTrain: epoch  7, batch    29 | loss: 185.5568989CurrentTrain: epoch  7, batch    30 | loss: 278.2646375CurrentTrain: epoch  7, batch    31 | loss: 182.8743992CurrentTrain: epoch  7, batch    32 | loss: 203.9774520CurrentTrain: epoch  7, batch    33 | loss: 215.8131958CurrentTrain: epoch  7, batch    34 | loss: 276.2312295CurrentTrain: epoch  7, batch    35 | loss: 229.4849743CurrentTrain: epoch  7, batch    36 | loss: 167.0046589CurrentTrain: epoch  7, batch    37 | loss: 221.4450722CurrentTrain: epoch  7, batch    38 | loss: 145.9350535CurrentTrain: epoch  7, batch    39 | loss: 201.7999159CurrentTrain: epoch  7, batch    40 | loss: 210.5582242CurrentTrain: epoch  7, batch    41 | loss: 191.0233379CurrentTrain: epoch  7, batch    42 | loss: 211.1051557CurrentTrain: epoch  7, batch    43 | loss: 183.6666917CurrentTrain: epoch  7, batch    44 | loss: 219.3996519CurrentTrain: epoch  7, batch    45 | loss: 174.7247469CurrentTrain: epoch  7, batch    46 | loss: 204.9897847CurrentTrain: epoch  7, batch    47 | loss: 169.6605947CurrentTrain: epoch  7, batch    48 | loss: 191.7001457CurrentTrain: epoch  7, batch    49 | loss: 276.1013549CurrentTrain: epoch  7, batch    50 | loss: 175.9200627CurrentTrain: epoch  7, batch    51 | loss: 219.9918927CurrentTrain: epoch  7, batch    52 | loss: 176.0829363CurrentTrain: epoch  7, batch    53 | loss: 145.8678508CurrentTrain: epoch  7, batch    54 | loss: 182.6306010CurrentTrain: epoch  7, batch    55 | loss: 202.7314540CurrentTrain: epoch  7, batch    56 | loss: 251.3300278CurrentTrain: epoch  7, batch    57 | loss: 175.2601434CurrentTrain: epoch  7, batch    58 | loss: 193.6106402CurrentTrain: epoch  7, batch    59 | loss: 228.2693328CurrentTrain: epoch  7, batch    60 | loss: 228.0999041CurrentTrain: epoch  7, batch    61 | loss: 193.6099113CurrentTrain: epoch  7, batch    62 | loss: 194.8660547CurrentTrain: epoch  7, batch    63 | loss: 187.8597620CurrentTrain: epoch  7, batch    64 | loss: 236.8785305CurrentTrain: epoch  7, batch    65 | loss: 162.1960305CurrentTrain: epoch  7, batch    66 | loss: 265.7713662CurrentTrain: epoch  7, batch    67 | loss: 286.3336622CurrentTrain: epoch  7, batch    68 | loss: 196.0916475CurrentTrain: epoch  7, batch    69 | loss: 191.7328091CurrentTrain: epoch  7, batch    70 | loss: 193.9028906CurrentTrain: epoch  7, batch    71 | loss: 194.4349551CurrentTrain: epoch  7, batch    72 | loss: 229.0182610CurrentTrain: epoch  7, batch    73 | loss: 220.3284470CurrentTrain: epoch  7, batch    74 | loss: 276.0166102CurrentTrain: epoch  7, batch    75 | loss: 202.3501435CurrentTrain: epoch  7, batch    76 | loss: 210.8530802CurrentTrain: epoch  7, batch    77 | loss: 160.1243548CurrentTrain: epoch  7, batch    78 | loss: 167.8094258CurrentTrain: epoch  7, batch    79 | loss: 202.1682263CurrentTrain: epoch  7, batch    80 | loss: 246.3895225CurrentTrain: epoch  7, batch    81 | loss: 183.5221555CurrentTrain: epoch  7, batch    82 | loss: 219.7753966CurrentTrain: epoch  7, batch    83 | loss: 246.3790684CurrentTrain: epoch  7, batch    84 | loss: 174.7995330CurrentTrain: epoch  7, batch    85 | loss: 201.6548587CurrentTrain: epoch  7, batch    86 | loss: 193.2883679CurrentTrain: epoch  7, batch    87 | loss: 246.7065493CurrentTrain: epoch  7, batch    88 | loss: 178.5515518CurrentTrain: epoch  7, batch    89 | loss: 190.7218156CurrentTrain: epoch  7, batch    90 | loss: 183.4540450CurrentTrain: epoch  7, batch    91 | loss: 183.0556126CurrentTrain: epoch  7, batch    92 | loss: 202.0766062CurrentTrain: epoch  7, batch    93 | loss: 183.8637498CurrentTrain: epoch  7, batch    94 | loss: 188.5141007CurrentTrain: epoch  7, batch    95 | loss: 181.4476586CurrentTrain: epoch  8, batch     0 | loss: 182.9136098CurrentTrain: epoch  8, batch     1 | loss: 193.4400370CurrentTrain: epoch  8, batch     2 | loss: 182.6355141CurrentTrain: epoch  8, batch     3 | loss: 276.3638383CurrentTrain: epoch  8, batch     4 | loss: 230.4928531CurrentTrain: epoch  8, batch     5 | loss: 213.5916045CurrentTrain: epoch  8, batch     6 | loss: 174.3095277CurrentTrain: epoch  8, batch     7 | loss: 236.7708939CurrentTrain: epoch  8, batch     8 | loss: 219.5648332CurrentTrain: epoch  8, batch     9 | loss: 190.8204386CurrentTrain: epoch  8, batch    10 | loss: 276.1098533CurrentTrain: epoch  8, batch    11 | loss: 178.7629966CurrentTrain: epoch  8, batch    12 | loss: 174.9699131CurrentTrain: epoch  8, batch    13 | loss: 190.6663175CurrentTrain: epoch  8, batch    14 | loss: 183.0677829CurrentTrain: epoch  8, batch    15 | loss: 211.1882959CurrentTrain: epoch  8, batch    16 | loss: 246.6456838CurrentTrain: epoch  8, batch    17 | loss: 174.8776692CurrentTrain: epoch  8, batch    18 | loss: 190.7984785CurrentTrain: epoch  8, batch    19 | loss: 227.8190143CurrentTrain: epoch  8, batch    20 | loss: 175.0975709CurrentTrain: epoch  8, batch    21 | loss: 210.3957015CurrentTrain: epoch  8, batch    22 | loss: 227.8442786CurrentTrain: epoch  8, batch    23 | loss: 201.7926782CurrentTrain: epoch  8, batch    24 | loss: 167.2426638CurrentTrain: epoch  8, batch    25 | loss: 164.8213758CurrentTrain: epoch  8, batch    26 | loss: 201.7589466CurrentTrain: epoch  8, batch    27 | loss: 185.7628990CurrentTrain: epoch  8, batch    28 | loss: 202.8719527CurrentTrain: epoch  8, batch    29 | loss: 246.6598017CurrentTrain: epoch  8, batch    30 | loss: 176.5759052CurrentTrain: epoch  8, batch    31 | loss: 227.7171098CurrentTrain: epoch  8, batch    32 | loss: 147.5673391CurrentTrain: epoch  8, batch    33 | loss: 138.9502641CurrentTrain: epoch  8, batch    34 | loss: 170.8594061CurrentTrain: epoch  8, batch    35 | loss: 182.6966641CurrentTrain: epoch  8, batch    36 | loss: 159.4323475CurrentTrain: epoch  8, batch    37 | loss: 175.0271053CurrentTrain: epoch  8, batch    38 | loss: 191.1580986CurrentTrain: epoch  8, batch    39 | loss: 194.1309579CurrentTrain: epoch  8, batch    40 | loss: 218.3849206CurrentTrain: epoch  8, batch    41 | loss: 175.6489031CurrentTrain: epoch  8, batch    42 | loss: 210.3325417CurrentTrain: epoch  8, batch    43 | loss: 159.7488565CurrentTrain: epoch  8, batch    44 | loss: 236.7398419CurrentTrain: epoch  8, batch    45 | loss: 246.3937848CurrentTrain: epoch  8, batch    46 | loss: 182.5895961CurrentTrain: epoch  8, batch    47 | loss: 286.2682218CurrentTrain: epoch  8, batch    48 | loss: 228.1213602CurrentTrain: epoch  8, batch    49 | loss: 238.5822822CurrentTrain: epoch  8, batch    50 | loss: 182.4895452CurrentTrain: epoch  8, batch    51 | loss: 195.1687576CurrentTrain: epoch  8, batch    52 | loss: 167.5323439CurrentTrain: epoch  8, batch    53 | loss: 138.7587293CurrentTrain: epoch  8, batch    54 | loss: 204.0497263CurrentTrain: epoch  8, batch    55 | loss: 211.4044800CurrentTrain: epoch  8, batch    56 | loss: 236.8642313CurrentTrain: epoch  8, batch    57 | loss: 359.3457278CurrentTrain: epoch  8, batch    58 | loss: 193.2396020CurrentTrain: epoch  8, batch    59 | loss: 237.0167005CurrentTrain: epoch  8, batch    60 | loss: 159.9444328CurrentTrain: epoch  8, batch    61 | loss: 275.9922736CurrentTrain: epoch  8, batch    62 | loss: 246.7014632CurrentTrain: epoch  8, batch    63 | loss: 183.2541245CurrentTrain: epoch  8, batch    64 | loss: 236.8510292CurrentTrain: epoch  8, batch    65 | loss: 166.9491596CurrentTrain: epoch  8, batch    66 | loss: 193.3935404CurrentTrain: epoch  8, batch    67 | loss: 211.6483360CurrentTrain: epoch  8, batch    68 | loss: 175.0604867CurrentTrain: epoch  8, batch    69 | loss: 210.7452376CurrentTrain: epoch  8, batch    70 | loss: 227.5265889CurrentTrain: epoch  8, batch    71 | loss: 184.9938435CurrentTrain: epoch  8, batch    72 | loss: 193.5341647CurrentTrain: epoch  8, batch    73 | loss: 202.3743792CurrentTrain: epoch  8, batch    74 | loss: 161.0648429CurrentTrain: epoch  8, batch    75 | loss: 183.1951480CurrentTrain: epoch  8, batch    76 | loss: 236.9093783CurrentTrain: epoch  8, batch    77 | loss: 288.7101844CurrentTrain: epoch  8, batch    78 | loss: 228.8015064CurrentTrain: epoch  8, batch    79 | loss: 182.8293635CurrentTrain: epoch  8, batch    80 | loss: 236.9145979CurrentTrain: epoch  8, batch    81 | loss: 227.9744424CurrentTrain: epoch  8, batch    82 | loss: 193.5612854CurrentTrain: epoch  8, batch    83 | loss: 228.1003331CurrentTrain: epoch  8, batch    84 | loss: 237.2883756CurrentTrain: epoch  8, batch    85 | loss: 187.0483065CurrentTrain: epoch  8, batch    86 | loss: 266.4753081CurrentTrain: epoch  8, batch    87 | loss: 247.9152312CurrentTrain: epoch  8, batch    88 | loss: 132.9294990CurrentTrain: epoch  8, batch    89 | loss: 228.0213573CurrentTrain: epoch  8, batch    90 | loss: 153.6390076CurrentTrain: epoch  8, batch    91 | loss: 286.2732072CurrentTrain: epoch  8, batch    92 | loss: 219.0820583CurrentTrain: epoch  8, batch    93 | loss: 194.1210355CurrentTrain: epoch  8, batch    94 | loss: 159.2278327CurrentTrain: epoch  8, batch    95 | loss: 224.5730705CurrentTrain: epoch  9, batch     0 | loss: 228.1574769CurrentTrain: epoch  9, batch     1 | loss: 203.0829747CurrentTrain: epoch  9, batch     2 | loss: 203.6025978CurrentTrain: epoch  9, batch     3 | loss: 203.8123722CurrentTrain: epoch  9, batch     4 | loss: 202.6066361CurrentTrain: epoch  9, batch     5 | loss: 228.0854960CurrentTrain: epoch  9, batch     6 | loss: 166.9069828CurrentTrain: epoch  9, batch     7 | loss: 219.7801820CurrentTrain: epoch  9, batch     8 | loss: 210.3979948CurrentTrain: epoch  9, batch     9 | loss: 167.0080673CurrentTrain: epoch  9, batch    10 | loss: 220.2151132CurrentTrain: epoch  9, batch    11 | loss: 257.4613952CurrentTrain: epoch  9, batch    12 | loss: 212.3892993CurrentTrain: epoch  9, batch    13 | loss: 204.2835123CurrentTrain: epoch  9, batch    14 | loss: 201.6843701CurrentTrain: epoch  9, batch    15 | loss: 210.9234739CurrentTrain: epoch  9, batch    16 | loss: 219.1157947CurrentTrain: epoch  9, batch    17 | loss: 182.9029493CurrentTrain: epoch  9, batch    18 | loss: 212.7438453CurrentTrain: epoch  9, batch    19 | loss: 202.9191024CurrentTrain: epoch  9, batch    20 | loss: 277.4000334CurrentTrain: epoch  9, batch    21 | loss: 168.5188267CurrentTrain: epoch  9, batch    22 | loss: 166.9439237CurrentTrain: epoch  9, batch    23 | loss: 159.6622983CurrentTrain: epoch  9, batch    24 | loss: 265.8237655CurrentTrain: epoch  9, batch    25 | loss: 185.4569548CurrentTrain: epoch  9, batch    26 | loss: 174.9251605CurrentTrain: epoch  9, batch    27 | loss: 159.5561942CurrentTrain: epoch  9, batch    28 | loss: 237.2354087CurrentTrain: epoch  9, batch    29 | loss: 152.7772658CurrentTrain: epoch  9, batch    30 | loss: 182.7154689CurrentTrain: epoch  9, batch    31 | loss: 190.6240644CurrentTrain: epoch  9, batch    32 | loss: 251.2344781CurrentTrain: epoch  9, batch    33 | loss: 219.5213283CurrentTrain: epoch  9, batch    34 | loss: 210.4705614CurrentTrain: epoch  9, batch    35 | loss: 190.8124465CurrentTrain: epoch  9, batch    36 | loss: 243.1882305CurrentTrain: epoch  9, batch    37 | loss: 201.9489633CurrentTrain: epoch  9, batch    38 | loss: 152.6307625CurrentTrain: epoch  9, batch    39 | loss: 218.8066803CurrentTrain: epoch  9, batch    40 | loss: 193.7104474CurrentTrain: epoch  9, batch    41 | loss: 210.8218395CurrentTrain: epoch  9, batch    42 | loss: 227.4741638CurrentTrain: epoch  9, batch    43 | loss: 211.8358000CurrentTrain: epoch  9, batch    44 | loss: 266.0458770CurrentTrain: epoch  9, batch    45 | loss: 185.6663484CurrentTrain: epoch  9, batch    46 | loss: 359.3031260CurrentTrain: epoch  9, batch    47 | loss: 193.4601713CurrentTrain: epoch  9, batch    48 | loss: 237.5578130CurrentTrain: epoch  9, batch    49 | loss: 193.6868479CurrentTrain: epoch  9, batch    50 | loss: 190.7927703CurrentTrain: epoch  9, batch    51 | loss: 174.6030120CurrentTrain: epoch  9, batch    52 | loss: 193.2939132CurrentTrain: epoch  9, batch    53 | loss: 191.7956526CurrentTrain: epoch  9, batch    54 | loss: 237.3806982CurrentTrain: epoch  9, batch    55 | loss: 346.4312341CurrentTrain: epoch  9, batch    56 | loss: 219.7143980CurrentTrain: epoch  9, batch    57 | loss: 188.3369466CurrentTrain: epoch  9, batch    58 | loss: 153.4119820CurrentTrain: epoch  9, batch    59 | loss: 210.4406617CurrentTrain: epoch  9, batch    60 | loss: 275.8804572CurrentTrain: epoch  9, batch    61 | loss: 152.8522640CurrentTrain: epoch  9, batch    62 | loss: 200.7092057CurrentTrain: epoch  9, batch    63 | loss: 236.8528195CurrentTrain: epoch  9, batch    64 | loss: 151.9662360CurrentTrain: epoch  9, batch    65 | loss: 218.7007082CurrentTrain: epoch  9, batch    66 | loss: 237.0626741CurrentTrain: epoch  9, batch    67 | loss: 196.4509287CurrentTrain: epoch  9, batch    68 | loss: 186.1493743CurrentTrain: epoch  9, batch    69 | loss: 167.5529446CurrentTrain: epoch  9, batch    70 | loss: 183.0050847CurrentTrain: epoch  9, batch    71 | loss: 276.0101916CurrentTrain: epoch  9, batch    72 | loss: 275.9163506CurrentTrain: epoch  9, batch    73 | loss: 229.4513140CurrentTrain: epoch  9, batch    74 | loss: 202.2719161CurrentTrain: epoch  9, batch    75 | loss: 191.0810923CurrentTrain: epoch  9, batch    76 | loss: 195.5761351CurrentTrain: epoch  9, batch    77 | loss: 185.8567393CurrentTrain: epoch  9, batch    78 | loss: 175.0611179CurrentTrain: epoch  9, batch    79 | loss: 210.3680269CurrentTrain: epoch  9, batch    80 | loss: 182.5726785CurrentTrain: epoch  9, batch    81 | loss: 138.0422636CurrentTrain: epoch  9, batch    82 | loss: 193.6878252CurrentTrain: epoch  9, batch    83 | loss: 191.2117903CurrentTrain: epoch  9, batch    84 | loss: 230.5959346CurrentTrain: epoch  9, batch    85 | loss: 201.9146344CurrentTrain: epoch  9, batch    86 | loss: 218.7018770CurrentTrain: epoch  9, batch    87 | loss: 182.8825727CurrentTrain: epoch  9, batch    88 | loss: 275.9940796CurrentTrain: epoch  9, batch    89 | loss: 152.1391737CurrentTrain: epoch  9, batch    90 | loss: 194.3130055CurrentTrain: epoch  9, batch    91 | loss: 194.1258426CurrentTrain: epoch  9, batch    92 | loss: 286.3177855CurrentTrain: epoch  9, batch    93 | loss: 170.2013747CurrentTrain: epoch  9, batch    94 | loss: 202.6493035CurrentTrain: epoch  9, batch    95 | loss: 204.3294999

F1 score per class: {32: 0.6741573033707865, 6: 0.8764044943820225, 19: 0.4166666666666667, 24: 0.7708333333333334, 26: 0.93048128342246, 29: 0.9081632653061225}
Micro-average F1 score: 0.8230366492146597
Weighted-average F1 score: 0.8295103707517791
F1 score per class: {32: 0.7628865979381443, 6: 0.9247311827956989, 19: 0.5517241379310345, 24: 0.774869109947644, 26: 0.9743589743589743, 29: 0.9081632653061225}
Micro-average F1 score: 0.8597376387487387
Weighted-average F1 score: 0.8616660729018667
F1 score per class: {32: 0.7628865979381443, 6: 0.9247311827956989, 19: 0.5714285714285714, 24: 0.774869109947644, 26: 0.9743589743589743, 29: 0.9137055837563451}
Micro-average F1 score: 0.8617558022199798
Weighted-average F1 score: 0.8639728136038162

F1 score per class: {32: 0.6741573033707865, 6: 0.8764044943820225, 19: 0.4166666666666667, 24: 0.7708333333333334, 26: 0.93048128342246, 29: 0.9081632653061225}
Micro-average F1 score: 0.8230366492146597
Weighted-average F1 score: 0.8295103707517791
F1 score per class: {32: 0.7628865979381443, 6: 0.9247311827956989, 19: 0.5517241379310345, 24: 0.774869109947644, 26: 0.9743589743589743, 29: 0.9081632653061225}
Micro-average F1 score: 0.8597376387487387
Weighted-average F1 score: 0.8616660729018667
F1 score per class: {32: 0.7628865979381443, 6: 0.9247311827956989, 19: 0.5714285714285714, 24: 0.774869109947644, 26: 0.9743589743589743, 29: 0.9137055837563451}
Micro-average F1 score: 0.8617558022199798
Weighted-average F1 score: 0.8639728136038162
cur_acc:  ['0.8230']
his_acc:  ['0.8230']
cur_acc des:  ['0.8597']
his_acc des:  ['0.8597']
cur_acc rrf:  ['0.8618']
his_acc rrf:  ['0.8618']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 191.3227786CurrentTrain: epoch  0, batch     1 | loss: 291.0415042CurrentTrain: epoch  0, batch     2 | loss: 258.9660876CurrentTrain: epoch  0, batch     3 | loss: 200.1862865CurrentTrain: epoch  0, batch     4 | loss: 177.3487025CurrentTrain: epoch  1, batch     0 | loss: 213.0660107CurrentTrain: epoch  1, batch     1 | loss: 212.5976387CurrentTrain: epoch  1, batch     2 | loss: 255.8652275CurrentTrain: epoch  1, batch     3 | loss: 203.6878093CurrentTrain: epoch  1, batch     4 | loss: 156.1922664CurrentTrain: epoch  2, batch     0 | loss: 243.0336191CurrentTrain: epoch  2, batch     1 | loss: 198.3195740CurrentTrain: epoch  2, batch     2 | loss: 216.3633260CurrentTrain: epoch  2, batch     3 | loss: 217.3886766CurrentTrain: epoch  2, batch     4 | loss: 142.2930932CurrentTrain: epoch  3, batch     0 | loss: 219.4733829CurrentTrain: epoch  3, batch     1 | loss: 227.8137219CurrentTrain: epoch  3, batch     2 | loss: 208.9075534CurrentTrain: epoch  3, batch     3 | loss: 270.1179669CurrentTrain: epoch  3, batch     4 | loss: 180.3287451CurrentTrain: epoch  4, batch     0 | loss: 214.1620250CurrentTrain: epoch  4, batch     1 | loss: 180.6229454CurrentTrain: epoch  4, batch     2 | loss: 233.1534446CurrentTrain: epoch  4, batch     3 | loss: 278.4428608CurrentTrain: epoch  4, batch     4 | loss: 146.0225648CurrentTrain: epoch  5, batch     0 | loss: 222.2273619CurrentTrain: epoch  5, batch     1 | loss: 238.2809388CurrentTrain: epoch  5, batch     2 | loss: 194.0357574CurrentTrain: epoch  5, batch     3 | loss: 193.8654917CurrentTrain: epoch  5, batch     4 | loss: 126.3871737CurrentTrain: epoch  6, batch     0 | loss: 206.0289286CurrentTrain: epoch  6, batch     1 | loss: 169.6774001CurrentTrain: epoch  6, batch     2 | loss: 211.9688770CurrentTrain: epoch  6, batch     3 | loss: 287.7090977CurrentTrain: epoch  6, batch     4 | loss: 170.7441716CurrentTrain: epoch  7, batch     0 | loss: 196.1627846CurrentTrain: epoch  7, batch     1 | loss: 237.8707925CurrentTrain: epoch  7, batch     2 | loss: 210.6944680CurrentTrain: epoch  7, batch     3 | loss: 247.4859221CurrentTrain: epoch  7, batch     4 | loss: 132.0998321CurrentTrain: epoch  8, batch     0 | loss: 211.5893380CurrentTrain: epoch  8, batch     1 | loss: 196.8561871CurrentTrain: epoch  8, batch     2 | loss: 266.6551378CurrentTrain: epoch  8, batch     3 | loss: 276.1127003CurrentTrain: epoch  8, batch     4 | loss: 145.3660446CurrentTrain: epoch  9, batch     0 | loss: 183.7984387CurrentTrain: epoch  9, batch     1 | loss: 248.7546535CurrentTrain: epoch  9, batch     2 | loss: 227.8795032CurrentTrain: epoch  9, batch     3 | loss: 186.8095512CurrentTrain: epoch  9, batch     4 | loss: 216.9129236
MemoryTrain:  epoch  0, batch     0 | loss: 1.2704706MemoryTrain:  epoch  1, batch     0 | loss: 0.9928654MemoryTrain:  epoch  2, batch     0 | loss: 0.8394087MemoryTrain:  epoch  3, batch     0 | loss: 0.6123535MemoryTrain:  epoch  4, batch     0 | loss: 0.5076820MemoryTrain:  epoch  5, batch     0 | loss: 0.3691992MemoryTrain:  epoch  6, batch     0 | loss: 0.2313690MemoryTrain:  epoch  7, batch     0 | loss: 0.2744886MemoryTrain:  epoch  8, batch     0 | loss: 0.1400260MemoryTrain:  epoch  9, batch     0 | loss: 0.1743375

F1 score per class: {5: 0.9690721649484536, 6: 0.0, 10: 0.5815602836879432, 16: 0.8, 17: 0.0, 18: 0.391304347826087}
Micro-average F1 score: 0.7288888888888889
Weighted-average F1 score: 0.7727499670505465
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.7088607594936709, 16: 0.8679245283018868, 17: 0.0, 18: 0.8615384615384616}
Micro-average F1 score: 0.8266129032258065
Weighted-average F1 score: 0.8282243915216101
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.725, 16: 0.8679245283018868, 17: 0.0, 18: 0.8615384615384616}
Micro-average F1 score: 0.8346774193548387
Weighted-average F1 score: 0.8390701912914607

F1 score per class: {32: 0.9641025641025641, 5: 0.6598984771573604, 6: 0.5815602836879432, 10: 0.8, 16: 0.0, 17: 0.391304347826087, 18: 0.8372093023255814, 19: 0.43478260869565216, 24: 0.7555555555555555, 26: 0.9417989417989417, 29: 0.9109947643979057}
Micro-average F1 score: 0.7834757834757835
Weighted-average F1 score: 0.8009568250349637
F1 score per class: {32: 0.9751243781094527, 5: 0.7075471698113207, 6: 0.7044025157232704, 10: 0.8363636363636363, 16: 0.0, 17: 0.8484848484848485, 18: 0.907103825136612, 19: 0.6, 24: 0.7624309392265194, 26: 0.9528795811518325, 29: 0.9119170984455959}
Micro-average F1 score: 0.8322147651006712
Weighted-average F1 score: 0.8341589091303033
F1 score per class: {32: 0.9751243781094527, 5: 0.7081339712918661, 6: 0.7204968944099379, 10: 0.8363636363636363, 16: 0.0, 17: 0.8484848484848485, 18: 0.907103825136612, 19: 0.5517241379310345, 24: 0.7624309392265194, 26: 0.9528795811518325, 29: 0.9119170984455959}
Micro-average F1 score: 0.8333333333333334
Weighted-average F1 score: 0.8354113087250884
cur_acc:  ['0.8230', '0.7289']
his_acc:  ['0.8230', '0.7835']
cur_acc des:  ['0.8597', '0.8266']
his_acc des:  ['0.8597', '0.8322']
cur_acc rrf:  ['0.8618', '0.8347']
his_acc rrf:  ['0.8618', '0.8333']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 179.9540751CurrentTrain: epoch  0, batch     1 | loss: 224.3450609CurrentTrain: epoch  0, batch     2 | loss: 186.3580564CurrentTrain: epoch  0, batch     3 | loss: 41.5279119CurrentTrain: epoch  1, batch     0 | loss: 174.9496786CurrentTrain: epoch  1, batch     1 | loss: 190.7113924CurrentTrain: epoch  1, batch     2 | loss: 222.0476091CurrentTrain: epoch  1, batch     3 | loss: 20.4572867CurrentTrain: epoch  2, batch     0 | loss: 199.1967297CurrentTrain: epoch  2, batch     1 | loss: 182.5821519CurrentTrain: epoch  2, batch     2 | loss: 186.3761439CurrentTrain: epoch  2, batch     3 | loss: 21.2592070CurrentTrain: epoch  3, batch     0 | loss: 222.4823172CurrentTrain: epoch  3, batch     1 | loss: 211.8470345CurrentTrain: epoch  3, batch     2 | loss: 145.8259710CurrentTrain: epoch  3, batch     3 | loss: 20.8410907CurrentTrain: epoch  4, batch     0 | loss: 138.1170130CurrentTrain: epoch  4, batch     1 | loss: 187.6297729CurrentTrain: epoch  4, batch     2 | loss: 238.7469853CurrentTrain: epoch  4, batch     3 | loss: 41.1352999CurrentTrain: epoch  5, batch     0 | loss: 154.7380772CurrentTrain: epoch  5, batch     1 | loss: 221.3488576CurrentTrain: epoch  5, batch     2 | loss: 215.4673599CurrentTrain: epoch  5, batch     3 | loss: 6.8725371CurrentTrain: epoch  6, batch     0 | loss: 204.7293559CurrentTrain: epoch  6, batch     1 | loss: 140.5359984CurrentTrain: epoch  6, batch     2 | loss: 229.3274778CurrentTrain: epoch  6, batch     3 | loss: 20.3342133CurrentTrain: epoch  7, batch     0 | loss: 360.2111562CurrentTrain: epoch  7, batch     1 | loss: 140.5450127CurrentTrain: epoch  7, batch     2 | loss: 163.2448786CurrentTrain: epoch  7, batch     3 | loss: 20.5424090CurrentTrain: epoch  8, batch     0 | loss: 238.5597646CurrentTrain: epoch  8, batch     1 | loss: 168.2029870CurrentTrain: epoch  8, batch     2 | loss: 153.7836854CurrentTrain: epoch  8, batch     3 | loss: 20.4606869CurrentTrain: epoch  9, batch     0 | loss: 181.1037005CurrentTrain: epoch  9, batch     1 | loss: 178.0004731CurrentTrain: epoch  9, batch     2 | loss: 202.2505531CurrentTrain: epoch  9, batch     3 | loss: 20.3937778
MemoryTrain:  epoch  0, batch     0 | loss: 0.6699017MemoryTrain:  epoch  1, batch     0 | loss: 0.5666837MemoryTrain:  epoch  2, batch     0 | loss: 0.3808639MemoryTrain:  epoch  3, batch     0 | loss: 0.3369701MemoryTrain:  epoch  4, batch     0 | loss: 0.2845054MemoryTrain:  epoch  5, batch     0 | loss: 0.1848626MemoryTrain:  epoch  6, batch     0 | loss: 0.1689403MemoryTrain:  epoch  7, batch     0 | loss: 0.1330597MemoryTrain:  epoch  8, batch     0 | loss: 0.0929180MemoryTrain:  epoch  9, batch     0 | loss: 0.0890450

F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.96, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.5263157894736842, 27: 0.6666666666666666, 31: 0.23376623376623376}
Micro-average F1 score: 0.3886255924170616
Weighted-average F1 score: 0.3104259242295054
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.5833333333333334, 27: 0.8, 31: 0.5531914893617021}
Micro-average F1 score: 0.5560538116591929
Weighted-average F1 score: 0.44818629774767604
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.5217391304347826, 27: 0.8, 31: 0.5376344086021505}
Micro-average F1 score: 0.5405405405405406
Weighted-average F1 score: 0.43250844985025333

F1 score per class: {32: 0.9746192893401016, 5: 0.366412213740458, 6: 0.043010752688172046, 7: 0.96, 40: 0.48484848484848486, 10: 0.7843137254901961, 9: 0.0, 16: 0.7241379310344828, 17: 0.6724890829694323, 18: 0.17391304347826086, 19: 0.7218934911242604, 24: 0.4166666666666667, 26: 0.8888888888888888, 27: 0.6666666666666666, 29: 0.8756756756756757, 31: 0.2222222222222222}
Micro-average F1 score: 0.6621287128712872
Weighted-average F1 score: 0.6652940289140219
F1 score per class: {32: 0.9519230769230769, 5: 0.4666666666666667, 6: 0.04819277108433735, 7: 0.9803921568627451, 40: 0.6883116883116883, 10: 0.8771929824561403, 9: 0.0, 16: 0.927536231884058, 17: 0.6857142857142857, 18: 0.4117647058823529, 19: 0.7597765363128491, 24: 0.4666666666666667, 26: 0.925531914893617, 27: 0.8, 29: 0.8709677419354839, 31: 0.4406779661016949}
Micro-average F1 score: 0.7170900692840647
Weighted-average F1 score: 0.698665638191167
F1 score per class: {32: 0.9611650485436893, 5: 0.4666666666666667, 6: 0.04878048780487805, 7: 0.9803921568627451, 40: 0.6883116883116883, 10: 0.8771929824561403, 9: 0.0, 16: 0.8787878787878788, 17: 0.7069767441860465, 18: 0.42424242424242425, 19: 0.7597765363128491, 24: 0.41379310344827586, 26: 0.93048128342246, 27: 0.8, 29: 0.8709677419354839, 31: 0.4424778761061947}
Micro-average F1 score: 0.7196749854904236
Weighted-average F1 score: 0.7022039509195186
cur_acc:  ['0.8230', '0.7289', '0.3886']
his_acc:  ['0.8230', '0.7835', '0.6621']
cur_acc des:  ['0.8597', '0.8266', '0.5561']
his_acc des:  ['0.8597', '0.8322', '0.7171']
cur_acc rrf:  ['0.8618', '0.8347', '0.5405']
his_acc rrf:  ['0.8618', '0.8333', '0.7197']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 256.9957456CurrentTrain: epoch  0, batch     1 | loss: 228.9175504CurrentTrain: epoch  0, batch     2 | loss: 198.2663182CurrentTrain: epoch  0, batch     3 | loss: 169.2604573CurrentTrain: epoch  1, batch     0 | loss: 215.3991927CurrentTrain: epoch  1, batch     1 | loss: 225.4431183CurrentTrain: epoch  1, batch     2 | loss: 204.2303052CurrentTrain: epoch  1, batch     3 | loss: 152.2877929CurrentTrain: epoch  2, batch     0 | loss: 176.4231018CurrentTrain: epoch  2, batch     1 | loss: 300.0889983CurrentTrain: epoch  2, batch     2 | loss: 215.0487512CurrentTrain: epoch  2, batch     3 | loss: 163.2569006CurrentTrain: epoch  3, batch     0 | loss: 205.4309420CurrentTrain: epoch  3, batch     1 | loss: 205.7138717CurrentTrain: epoch  3, batch     2 | loss: 176.1340014CurrentTrain: epoch  3, batch     3 | loss: 191.6384632CurrentTrain: epoch  4, batch     0 | loss: 336.4983569CurrentTrain: epoch  4, batch     1 | loss: 167.3328691CurrentTrain: epoch  4, batch     2 | loss: 170.4630670CurrentTrain: epoch  4, batch     3 | loss: 230.9385979CurrentTrain: epoch  5, batch     0 | loss: 212.4959083CurrentTrain: epoch  5, batch     1 | loss: 183.2281538CurrentTrain: epoch  5, batch     2 | loss: 243.2764069CurrentTrain: epoch  5, batch     3 | loss: 162.6963815CurrentTrain: epoch  6, batch     0 | loss: 220.3995867CurrentTrain: epoch  6, batch     1 | loss: 230.0381866CurrentTrain: epoch  6, batch     2 | loss: 189.6020872CurrentTrain: epoch  6, batch     3 | loss: 164.2420389CurrentTrain: epoch  7, batch     0 | loss: 214.4781060CurrentTrain: epoch  7, batch     1 | loss: 181.0437050CurrentTrain: epoch  7, batch     2 | loss: 199.7187847CurrentTrain: epoch  7, batch     3 | loss: 188.9962901CurrentTrain: epoch  8, batch     0 | loss: 191.9460907CurrentTrain: epoch  8, batch     1 | loss: 198.5001475CurrentTrain: epoch  8, batch     2 | loss: 221.4503696CurrentTrain: epoch  8, batch     3 | loss: 171.7081124CurrentTrain: epoch  9, batch     0 | loss: 191.7519438CurrentTrain: epoch  9, batch     1 | loss: 211.7799145CurrentTrain: epoch  9, batch     2 | loss: 183.0051740CurrentTrain: epoch  9, batch     3 | loss: 153.8209328
MemoryTrain:  epoch  0, batch     0 | loss: 0.7349828MemoryTrain:  epoch  1, batch     0 | loss: 0.6459697MemoryTrain:  epoch  2, batch     0 | loss: 0.4216037MemoryTrain:  epoch  3, batch     0 | loss: 0.3669495MemoryTrain:  epoch  4, batch     0 | loss: 0.2636416MemoryTrain:  epoch  5, batch     0 | loss: 0.2207128MemoryTrain:  epoch  6, batch     0 | loss: 0.1600484MemoryTrain:  epoch  7, batch     0 | loss: 0.1360322MemoryTrain:  epoch  8, batch     0 | loss: 0.1250358MemoryTrain:  epoch  9, batch     0 | loss: 0.1014502

F1 score per class: {0: 0.927536231884058, 32: 0.9583333333333334, 4: 0.3333333333333333, 13: 0.6222222222222222, 21: 0.825, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8664987405541562
Weighted-average F1 score: 0.870719042550288
F1 score per class: {0: 0.9577464788732394, 32: 0.9637305699481865, 4: 0.0, 5: 0.5714285714285714, 13: 0.0, 18: 0.8301886792452831, 21: 0.7948717948717948, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8524590163934426
Weighted-average F1 score: 0.8030838503099058
F1 score per class: {0: 0.9577464788732394, 32: 0.9949748743718593, 4: 0.5714285714285714, 40: 0.8301886792452831, 13: 0.7466666666666667, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8705882352941177
Weighted-average F1 score: 0.8359978907191425

F1 score per class: {0: 0.927536231884058, 4: 0.9583333333333334, 5: 0.9746192893401016, 6: 0.4153846153846154, 7: 0.05714285714285714, 9: 0.96, 10: 0.2608695652173913, 13: 0.058823529411764705, 16: 0.8301886792452831, 17: 0.0, 18: 0.6296296296296297, 19: 0.6635514018691588, 21: 0.5283018867924528, 23: 0.8048780487804879, 24: 0.10526315789473684, 26: 0.7251461988304093, 27: 0.5384615384615384, 29: 0.8901098901098901, 31: 0.6666666666666666, 32: 0.7692307692307693, 40: 0.3218390804597701}
Micro-average F1 score: 0.6841574167507568
Weighted-average F1 score: 0.6977694497100035
F1 score per class: {0: 0.9577464788732394, 4: 0.9637305699481865, 5: 0.9615384615384616, 6: 0.5802469135802469, 7: 0.08695652173913043, 9: 0.9803921568627451, 10: 0.5255474452554745, 13: 0.07142857142857142, 16: 0.9152542372881356, 17: 0.0, 18: 0.8285714285714286, 19: 0.7087378640776699, 21: 0.6875, 23: 0.7848101265822784, 24: 0.09090909090909091, 26: 0.7046632124352331, 27: 0.5405405405405406, 29: 0.9368421052631579, 31: 0.6666666666666666, 32: 0.8842105263157894, 40: 0.512396694214876}
Micro-average F1 score: 0.7359781121751026
Weighted-average F1 score: 0.7179122101103369
F1 score per class: {0: 0.9577464788732394, 4: 0.99, 5: 0.9803921568627451, 6: 0.5477707006369427, 7: 0.0821917808219178, 9: 0.9803921568627451, 10: 0.5147058823529411, 13: 0.06896551724137931, 16: 0.9152542372881356, 17: 0.0, 18: 0.8253968253968254, 19: 0.7087378640776699, 21: 0.6666666666666666, 23: 0.7368421052631579, 24: 0.09523809523809523, 26: 0.7046632124352331, 27: 0.5, 29: 0.9368421052631579, 31: 0.6666666666666666, 32: 0.8783068783068783, 40: 0.5344827586206896}
Micro-average F1 score: 0.7339449541284404
Weighted-average F1 score: 0.7157676005828877
cur_acc:  ['0.8230', '0.7289', '0.3886', '0.8665']
his_acc:  ['0.8230', '0.7835', '0.6621', '0.6842']
cur_acc des:  ['0.8597', '0.8266', '0.5561', '0.8525']
his_acc des:  ['0.8597', '0.8322', '0.7171', '0.7360']
cur_acc rrf:  ['0.8618', '0.8347', '0.5405', '0.8706']
his_acc rrf:  ['0.8618', '0.8333', '0.7197', '0.7339']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 206.1455087CurrentTrain: epoch  0, batch     1 | loss: 214.5310750CurrentTrain: epoch  0, batch     2 | loss: 196.1401913CurrentTrain: epoch  0, batch     3 | loss: 172.9459728CurrentTrain: epoch  1, batch     0 | loss: 172.1692409CurrentTrain: epoch  1, batch     1 | loss: 221.0758553CurrentTrain: epoch  1, batch     2 | loss: 220.5876497CurrentTrain: epoch  1, batch     3 | loss: 221.8172252CurrentTrain: epoch  2, batch     0 | loss: 165.4692731CurrentTrain: epoch  2, batch     1 | loss: 244.4966703CurrentTrain: epoch  2, batch     2 | loss: 217.5654218CurrentTrain: epoch  2, batch     3 | loss: 115.8772843CurrentTrain: epoch  3, batch     0 | loss: 170.7003723CurrentTrain: epoch  3, batch     1 | loss: 214.3846362CurrentTrain: epoch  3, batch     2 | loss: 187.3331482CurrentTrain: epoch  3, batch     3 | loss: 141.2178151CurrentTrain: epoch  4, batch     0 | loss: 239.9316849CurrentTrain: epoch  4, batch     1 | loss: 205.9859347CurrentTrain: epoch  4, batch     2 | loss: 164.8070145CurrentTrain: epoch  4, batch     3 | loss: 118.6158471CurrentTrain: epoch  5, batch     0 | loss: 238.4810466CurrentTrain: epoch  5, batch     1 | loss: 180.9959920CurrentTrain: epoch  5, batch     2 | loss: 195.0046880CurrentTrain: epoch  5, batch     3 | loss: 129.5350019CurrentTrain: epoch  6, batch     0 | loss: 180.2105180CurrentTrain: epoch  6, batch     1 | loss: 180.7514609CurrentTrain: epoch  6, batch     2 | loss: 287.0627378CurrentTrain: epoch  6, batch     3 | loss: 118.8566624CurrentTrain: epoch  7, batch     0 | loss: 186.2888741CurrentTrain: epoch  7, batch     1 | loss: 167.4909405CurrentTrain: epoch  7, batch     2 | loss: 228.2661279CurrentTrain: epoch  7, batch     3 | loss: 144.8250026CurrentTrain: epoch  8, batch     0 | loss: 219.0325973CurrentTrain: epoch  8, batch     1 | loss: 194.4367254CurrentTrain: epoch  8, batch     2 | loss: 194.3643889CurrentTrain: epoch  8, batch     3 | loss: 128.3733895CurrentTrain: epoch  9, batch     0 | loss: 202.6068089CurrentTrain: epoch  9, batch     1 | loss: 210.8030426CurrentTrain: epoch  9, batch     2 | loss: 210.6504058CurrentTrain: epoch  9, batch     3 | loss: 128.8363498
MemoryTrain:  epoch  0, batch     0 | loss: 0.5193734MemoryTrain:  epoch  1, batch     0 | loss: 0.4318165MemoryTrain:  epoch  2, batch     0 | loss: 0.3586121MemoryTrain:  epoch  3, batch     0 | loss: 0.2885103MemoryTrain:  epoch  4, batch     0 | loss: 0.2008304MemoryTrain:  epoch  5, batch     0 | loss: 0.2023665MemoryTrain:  epoch  6, batch     0 | loss: 0.1394769MemoryTrain:  epoch  7, batch     0 | loss: 0.1320433MemoryTrain:  epoch  8, batch     0 | loss: 0.1042681MemoryTrain:  epoch  9, batch     0 | loss: 0.0968579

F1 score per class: {33: 0.0, 36: 0.4716981132075472, 5: 0.0, 8: 0.0, 40: 0.7865168539325843, 13: 0.0, 18: 0.0, 20: 0.9444444444444444, 26: 0.42857142857142855, 29: 0.6105263157894737, 30: 0.0}
Micro-average F1 score: 0.6246418338108882
Weighted-average F1 score: 0.6302294552287603
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.0, 6: 0.704, 7: 0.0, 8: 0.0, 10: 0.0, 13: 0.8979591836734694, 18: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.9444444444444444, 29: 0.625, 30: 0.8925619834710744}
Micro-average F1 score: 0.784688995215311
Weighted-average F1 score: 0.7420196188668304
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.0, 6: 0.7142857142857143, 4: 0.0, 8: 0.0, 10: 0.0, 40: 0.8979591836734694, 13: 0.0, 18: 0.0, 20: 0.0, 21: 0.9444444444444444, 26: 0.625, 29: 0.8571428571428571, 30: 0.0}
Micro-average F1 score: 0.7769784172661871
Weighted-average F1 score: 0.7325924097237461

F1 score per class: {0: 0.9428571428571428, 4: 0.9743589743589743, 5: 0.9696969696969697, 6: 0.23008849557522124, 7: 0.06779661016949153, 8: 0.4032258064516129, 9: 0.96, 10: 0.1651376146788991, 13: 0.13333333333333333, 16: 0.7843137254901961, 17: 0.0, 18: 0.5555555555555556, 19: 0.5684210526315789, 20: 0.7777777777777778, 21: 0.2857142857142857, 23: 0.7848101265822784, 24: 0.10526315789473684, 26: 0.7444444444444445, 27: 0.5454545454545454, 29: 0.861878453038674, 30: 0.9444444444444444, 31: 1.0, 32: 0.7393939393939394, 33: 0.4, 36: 0.5918367346938775, 40: 0.2988505747126437}
Micro-average F1 score: 0.6527716186252772
Weighted-average F1 score: 0.7015669768056193
F1 score per class: {0: 0.958904109589041, 4: 0.9795918367346939, 5: 0.91324200913242, 6: 0.5341614906832298, 7: 0.06896551724137931, 8: 0.4861878453038674, 9: 0.9803921568627451, 10: 0.288135593220339, 13: 0.14285714285714285, 16: 0.8928571428571429, 17: 0.0, 18: 0.8169014084507042, 19: 0.6767676767676768, 20: 0.8888888888888888, 21: 0.65625, 23: 0.7848101265822784, 24: 0.09090909090909091, 26: 0.7157894736842105, 27: 0.48484848484848486, 29: 0.9042553191489362, 30: 0.8717948717948718, 31: 0.6666666666666666, 32: 0.8602150537634409, 33: 0.5263157894736842, 36: 0.84375, 40: 0.5263157894736842}
Micro-average F1 score: 0.7231726283048211
Weighted-average F1 score: 0.7304451840314635
F1 score per class: {0: 0.958904109589041, 4: 0.9746192893401016, 5: 0.9166666666666666, 6: 0.43537414965986393, 7: 0.06666666666666667, 8: 0.4891304347826087, 9: 0.9803921568627451, 10: 0.27350427350427353, 13: 0.14285714285714285, 16: 0.8928571428571429, 17: 0.0, 18: 0.782608695652174, 19: 0.6834170854271356, 20: 0.8888888888888888, 21: 0.6461538461538462, 23: 0.7792207792207793, 24: 0.09523809523809523, 26: 0.7157894736842105, 27: 0.5161290322580645, 29: 0.9042553191489362, 30: 0.85, 31: 0.8, 32: 0.8602150537634409, 33: 0.5, 36: 0.8031496062992126, 40: 0.5454545454545454}
Micro-average F1 score: 0.715797726381811
Weighted-average F1 score: 0.7263505942642723
cur_acc:  ['0.8230', '0.7289', '0.3886', '0.8665', '0.6246']
his_acc:  ['0.8230', '0.7835', '0.6621', '0.6842', '0.6528']
cur_acc des:  ['0.8597', '0.8266', '0.5561', '0.8525', '0.7847']
his_acc des:  ['0.8597', '0.8322', '0.7171', '0.7360', '0.7232']
cur_acc rrf:  ['0.8618', '0.8347', '0.5405', '0.8706', '0.7770']
his_acc rrf:  ['0.8618', '0.8333', '0.7197', '0.7339', '0.7158']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 226.6770248CurrentTrain: epoch  0, batch     1 | loss: 247.0460330CurrentTrain: epoch  0, batch     2 | loss: 202.8283637CurrentTrain: epoch  0, batch     3 | loss: 210.7554969CurrentTrain: epoch  0, batch     4 | loss: 90.4359945CurrentTrain: epoch  1, batch     0 | loss: 201.7885132CurrentTrain: epoch  1, batch     1 | loss: 286.2295444CurrentTrain: epoch  1, batch     2 | loss: 195.7938161CurrentTrain: epoch  1, batch     3 | loss: 195.0131562CurrentTrain: epoch  1, batch     4 | loss: 51.9433029CurrentTrain: epoch  2, batch     0 | loss: 188.9053545CurrentTrain: epoch  2, batch     1 | loss: 226.0685892CurrentTrain: epoch  2, batch     2 | loss: 291.5159293CurrentTrain: epoch  2, batch     3 | loss: 216.0877568CurrentTrain: epoch  2, batch     4 | loss: 53.3790084CurrentTrain: epoch  3, batch     0 | loss: 245.6773948CurrentTrain: epoch  3, batch     1 | loss: 290.9392554CurrentTrain: epoch  3, batch     2 | loss: 226.2010514CurrentTrain: epoch  3, batch     3 | loss: 191.7853247CurrentTrain: epoch  3, batch     4 | loss: 36.1881262CurrentTrain: epoch  4, batch     0 | loss: 241.1389843CurrentTrain: epoch  4, batch     1 | loss: 204.9296431CurrentTrain: epoch  4, batch     2 | loss: 199.8903008CurrentTrain: epoch  4, batch     3 | loss: 210.3855632CurrentTrain: epoch  4, batch     4 | loss: 90.0028201CurrentTrain: epoch  5, batch     0 | loss: 205.2309482CurrentTrain: epoch  5, batch     1 | loss: 171.2938576CurrentTrain: epoch  5, batch     2 | loss: 212.9437971CurrentTrain: epoch  5, batch     3 | loss: 241.1804507CurrentTrain: epoch  5, batch     4 | loss: 51.1979777CurrentTrain: epoch  6, batch     0 | loss: 193.4889550CurrentTrain: epoch  6, batch     1 | loss: 232.0756466CurrentTrain: epoch  6, batch     2 | loss: 169.9802915CurrentTrain: epoch  6, batch     3 | loss: 220.4086192CurrentTrain: epoch  6, batch     4 | loss: 51.4404736CurrentTrain: epoch  7, batch     0 | loss: 230.7458428CurrentTrain: epoch  7, batch     1 | loss: 287.0236861CurrentTrain: epoch  7, batch     2 | loss: 164.3066706CurrentTrain: epoch  7, batch     3 | loss: 220.8186927CurrentTrain: epoch  7, batch     4 | loss: 51.1010204CurrentTrain: epoch  8, batch     0 | loss: 175.7569432CurrentTrain: epoch  8, batch     1 | loss: 228.2827735CurrentTrain: epoch  8, batch     2 | loss: 179.7688075CurrentTrain: epoch  8, batch     3 | loss: 359.9267144CurrentTrain: epoch  8, batch     4 | loss: 45.7400639CurrentTrain: epoch  9, batch     0 | loss: 228.5650456CurrentTrain: epoch  9, batch     1 | loss: 228.4990234CurrentTrain: epoch  9, batch     2 | loss: 210.9861838CurrentTrain: epoch  9, batch     3 | loss: 191.3772301CurrentTrain: epoch  9, batch     4 | loss: 25.2600283
MemoryTrain:  epoch  0, batch     0 | loss: 0.6063098MemoryTrain:  epoch  1, batch     0 | loss: 0.5767016MemoryTrain:  epoch  2, batch     0 | loss: 0.4187015MemoryTrain:  epoch  3, batch     0 | loss: 0.3562234MemoryTrain:  epoch  4, batch     0 | loss: 0.3388499MemoryTrain:  epoch  5, batch     0 | loss: 0.2397540MemoryTrain:  epoch  6, batch     0 | loss: 0.2187485MemoryTrain:  epoch  7, batch     0 | loss: 0.1793852MemoryTrain:  epoch  8, batch     0 | loss: 0.1621587MemoryTrain:  epoch  9, batch     0 | loss: 0.1267427

F1 score per class: {0: 0.0, 2: 0.875, 6: 0.0, 39: 0.4144144144144144, 11: 0.36065573770491804, 12: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.6666666666666666, 28: 0.25}
Micro-average F1 score: 0.4
Weighted-average F1 score: 0.37686794968600557
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.7724137931034483, 12: 0.7757575757575758, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 28: 0.7142857142857143, 33: 0.0, 36: 0.0, 39: 0.4444444444444444, 40: 0.0}
Micro-average F1 score: 0.6868686868686869
Weighted-average F1 score: 0.6024876875402883
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.7808219178082192, 12: 0.7682926829268293, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 28: 0.6153846153846154, 33: 0.0, 36: 0.0, 39: 0.35294117647058826, 40: 0.0}
Micro-average F1 score: 0.6802030456852792
Weighted-average F1 score: 0.5983951402658778

F1 score per class: {0: 0.9444444444444444, 2: 0.7, 4: 0.9010989010989011, 5: 0.9538461538461539, 6: 0.2564102564102564, 7: 0.03225806451612903, 8: 0.2127659574468085, 9: 0.96, 10: 0.12962962962962962, 11: 0.32857142857142857, 12: 0.352, 13: 0.0, 16: 0.7547169811320755, 17: 0.0, 18: 0.35294117647058826, 19: 0.5670103092783505, 20: 0.6829268292682927, 21: 0.32558139534883723, 23: 0.8292682926829268, 24: 0.10526315789473684, 26: 0.7314285714285714, 27: 0.38095238095238093, 28: 0.26666666666666666, 29: 0.9090909090909091, 30: 0.9444444444444444, 31: 1.0, 32: 0.7954545454545454, 33: 0.4, 36: 0.2631578947368421, 39: 0.17391304347826086, 40: 0.27906976744186046}
Micro-average F1 score: 0.5866141732283464
Weighted-average F1 score: 0.6487912174286433
F1 score per class: {0: 0.972972972972973, 2: 0.358974358974359, 4: 0.8571428571428571, 5: 0.9428571428571428, 6: 0.5098039215686274, 7: 0.06896551724137931, 8: 0.5063291139240507, 9: 0.9803921568627451, 10: 0.24347826086956523, 11: 0.5714285714285714, 12: 0.6666666666666666, 13: 0.15384615384615385, 16: 0.8421052631578947, 17: 0.0, 18: 0.45614035087719296, 19: 0.6633663366336634, 20: 0.8, 21: 0.5882352941176471, 23: 0.8809523809523809, 24: 0.09090909090909091, 26: 0.7362637362637363, 27: 0.47058823529411764, 28: 0.2702702702702703, 29: 0.9148936170212766, 30: 1.0, 31: 0.8, 32: 0.8900523560209425, 33: 0.35294117647058826, 36: 0.5625, 39: 0.27586206896551724, 40: 0.5128205128205128}
Micro-average F1 score: 0.6709502874535002
Weighted-average F1 score: 0.6738044363900737
F1 score per class: {0: 0.972972972972973, 2: 0.42424242424242425, 4: 0.88268156424581, 5: 0.9473684210526315, 6: 0.4689655172413793, 7: 0.06557377049180328, 8: 0.4444444444444444, 9: 0.9803921568627451, 10: 0.22807017543859648, 11: 0.5757575757575758, 12: 0.6596858638743456, 13: 0.14285714285714285, 16: 0.8421052631578947, 17: 0.0, 18: 0.43636363636363634, 19: 0.6534653465346535, 20: 0.8, 21: 0.5970149253731343, 23: 0.8809523809523809, 24: 0.09090909090909091, 26: 0.7362637362637363, 27: 0.4444444444444444, 28: 0.18604651162790697, 29: 0.9148936170212766, 30: 0.9743589743589743, 31: 1.0, 32: 0.8900523560209425, 33: 0.35294117647058826, 36: 0.5773195876288659, 39: 0.23076923076923078, 40: 0.5217391304347826}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6712343592524771
cur_acc:  ['0.8230', '0.7289', '0.3886', '0.8665', '0.6246', '0.4000']
his_acc:  ['0.8230', '0.7835', '0.6621', '0.6842', '0.6528', '0.5866']
cur_acc des:  ['0.8597', '0.8266', '0.5561', '0.8525', '0.7847', '0.6869']
his_acc des:  ['0.8597', '0.8322', '0.7171', '0.7360', '0.7232', '0.6710']
cur_acc rrf:  ['0.8618', '0.8347', '0.5405', '0.8706', '0.7770', '0.6802']
his_acc rrf:  ['0.8618', '0.8333', '0.7197', '0.7339', '0.7158', '0.6667']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 201.5098173CurrentTrain: epoch  0, batch     1 | loss: 256.5632860CurrentTrain: epoch  0, batch     2 | loss: 286.4131110CurrentTrain: epoch  0, batch     3 | loss: 246.1142135CurrentTrain: epoch  0, batch     4 | loss: 147.4207294CurrentTrain: epoch  1, batch     0 | loss: 247.6586928CurrentTrain: epoch  1, batch     1 | loss: 246.1828268CurrentTrain: epoch  1, batch     2 | loss: 210.6251188CurrentTrain: epoch  1, batch     3 | loss: 182.3921343CurrentTrain: epoch  1, batch     4 | loss: 199.2322586CurrentTrain: epoch  2, batch     0 | loss: 206.9546355CurrentTrain: epoch  2, batch     1 | loss: 262.5063268CurrentTrain: epoch  2, batch     2 | loss: 254.2974942CurrentTrain: epoch  2, batch     3 | loss: 187.5767001CurrentTrain: epoch  2, batch     4 | loss: 119.6146783CurrentTrain: epoch  3, batch     0 | loss: 207.9431880CurrentTrain: epoch  3, batch     1 | loss: 214.3130108CurrentTrain: epoch  3, batch     2 | loss: 197.8290133CurrentTrain: epoch  3, batch     3 | loss: 204.8662463CurrentTrain: epoch  3, batch     4 | loss: 203.2976554CurrentTrain: epoch  4, batch     0 | loss: 222.1651749CurrentTrain: epoch  4, batch     1 | loss: 214.3029117CurrentTrain: epoch  4, batch     2 | loss: 163.9766989CurrentTrain: epoch  4, batch     3 | loss: 238.4660918CurrentTrain: epoch  4, batch     4 | loss: 157.6607569CurrentTrain: epoch  5, batch     0 | loss: 287.3047741CurrentTrain: epoch  5, batch     1 | loss: 175.4076161CurrentTrain: epoch  5, batch     2 | loss: 230.4777607CurrentTrain: epoch  5, batch     3 | loss: 204.1700105CurrentTrain: epoch  5, batch     4 | loss: 156.9149475CurrentTrain: epoch  6, batch     0 | loss: 237.6181425CurrentTrain: epoch  6, batch     1 | loss: 197.7135934CurrentTrain: epoch  6, batch     2 | loss: 176.8489698CurrentTrain: epoch  6, batch     3 | loss: 247.0406752CurrentTrain: epoch  6, batch     4 | loss: 131.9816953CurrentTrain: epoch  7, batch     0 | loss: 346.8615972CurrentTrain: epoch  7, batch     1 | loss: 184.3662109CurrentTrain: epoch  7, batch     2 | loss: 186.2264253CurrentTrain: epoch  7, batch     3 | loss: 240.2560467CurrentTrain: epoch  7, batch     4 | loss: 123.7443247CurrentTrain: epoch  8, batch     0 | loss: 211.5566609CurrentTrain: epoch  8, batch     1 | loss: 147.8940119CurrentTrain: epoch  8, batch     2 | loss: 267.0259065CurrentTrain: epoch  8, batch     3 | loss: 276.2819921CurrentTrain: epoch  8, batch     4 | loss: 202.0955194CurrentTrain: epoch  9, batch     0 | loss: 237.9332748CurrentTrain: epoch  9, batch     1 | loss: 256.6406732CurrentTrain: epoch  9, batch     2 | loss: 186.6299238CurrentTrain: epoch  9, batch     3 | loss: 276.3816482CurrentTrain: epoch  9, batch     4 | loss: 116.3952958
MemoryTrain:  epoch  0, batch     0 | loss: 0.9188928MemoryTrain:  epoch  1, batch     0 | loss: 0.7635070MemoryTrain:  epoch  2, batch     0 | loss: 0.6851887MemoryTrain:  epoch  3, batch     0 | loss: 0.5672436MemoryTrain:  epoch  4, batch     0 | loss: 0.4636533MemoryTrain:  epoch  5, batch     0 | loss: 0.4172552MemoryTrain:  epoch  6, batch     0 | loss: 0.3284236MemoryTrain:  epoch  7, batch     0 | loss: 0.2996013MemoryTrain:  epoch  8, batch     0 | loss: 0.2158349MemoryTrain:  epoch  9, batch     0 | loss: 0.1816240

F1 score per class: {32: 0.37606837606837606, 1: 0.7480916030534351, 34: 0.0, 3: 0.029850746268656716, 11: 0.7593582887700535, 14: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.5135135135135135}
Micro-average F1 score: 0.5382059800664452
Weighted-average F1 score: 0.5408481143386272
F1 score per class: {32: 0.42276422764227645, 1: 0.9210526315789473, 34: 0.0, 3: 0.0, 36: 0.1, 40: 0.0, 10: 0.0, 11: 0.7391304347826086, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.7777777777777778, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.6160849772382397
Weighted-average F1 score: 0.5980418890450236
F1 score per class: {32: 0.4032258064516129, 1: 0.9210526315789473, 34: 0.0, 3: 0.0, 40: 0.10526315789473684, 10: 0.0, 11: 0.0, 14: 0.7724867724867724, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.75, 27: 0.0}
Micro-average F1 score: 0.625
Weighted-average F1 score: 0.6124816475671068

F1 score per class: {0: 0.8656716417910447, 1: 0.34108527131782945, 2: 0.48, 3: 0.6712328767123288, 4: 0.8700564971751412, 5: 0.9587628865979382, 6: 0.19469026548672566, 7: 0.03225806451612903, 8: 0.19148936170212766, 9: 0.96, 10: 0.14678899082568808, 11: 0.2937853107344633, 12: 0.16363636363636364, 13: 0.125, 14: 0.02666666666666667, 16: 0.7636363636363637, 17: 0.0, 18: 0.0, 19: 0.3717948717948718, 20: 0.43478260869565216, 21: 0.0, 22: 0.696078431372549, 23: 0.8571428571428571, 24: 0.0, 26: 0.7528089887640449, 27: 0.09090909090909091, 28: 0.21621621621621623, 29: 0.8791208791208791, 30: 0.9142857142857143, 31: 0.0, 32: 0.7586206896551724, 33: 0.375, 34: 0.3619047619047619, 36: 0.21621621621621623, 39: 0.09523809523809523, 40: 0.15789473684210525}
Micro-average F1 score: 0.5152866242038217
Weighted-average F1 score: 0.5793894959171483
F1 score per class: {0: 0.9295774647887324, 1: 0.3611111111111111, 2: 0.3684210526315789, 3: 0.8974358974358975, 4: 0.8372093023255814, 5: 0.9428571428571428, 6: 0.46258503401360546, 7: 0.06666666666666667, 8: 0.4161073825503356, 9: 0.9803921568627451, 10: 0.3140495867768595, 11: 0.272, 12: 0.6049382716049383, 13: 0.1111111111111111, 14: 0.09523809523809523, 16: 0.8148148148148148, 17: 0.0, 18: 0.5671641791044776, 19: 0.4444444444444444, 20: 0.6987951807228916, 21: 0.26666666666666666, 22: 0.6732673267326733, 23: 0.8433734939759037, 24: 0.08695652173913043, 26: 0.708994708994709, 27: 0.09090909090909091, 28: 0.16666666666666666, 29: 0.9032258064516129, 30: 0.9473684210526315, 31: 1.0, 32: 0.8601036269430051, 33: 0.3157894736842105, 34: 0.4794520547945205, 36: 0.8, 39: 0.26666666666666666, 40: 0.5428571428571428}
Micro-average F1 score: 0.61175147596289
Weighted-average F1 score: 0.6144730605044031
F1 score per class: {0: 0.9295774647887324, 1: 0.3401360544217687, 2: 0.4827586206896552, 3: 0.875, 4: 0.8700564971751412, 5: 0.9753694581280788, 6: 0.41134751773049644, 7: 0.0625, 8: 0.391304347826087, 9: 0.9803921568627451, 10: 0.2857142857142857, 11: 0.25, 12: 0.5641025641025641, 13: 0.1111111111111111, 14: 0.1, 16: 0.8363636363636363, 17: 0.0, 18: 0.19607843137254902, 19: 0.5149700598802395, 20: 0.6976744186046512, 21: 0.16666666666666666, 22: 0.7019230769230769, 23: 0.7848101265822784, 24: 0.08695652173913043, 26: 0.708994708994709, 27: 0.1, 28: 0.14814814814814814, 29: 0.9090909090909091, 30: 0.9473684210526315, 31: 1.0, 32: 0.8601036269430051, 33: 0.3333333333333333, 34: 0.4177215189873418, 36: 0.6041666666666666, 39: 0.15384615384615385, 40: 0.5344827586206896}
Micro-average F1 score: 0.5952720022785531
Weighted-average F1 score: 0.6018775136767359
cur_acc:  ['0.8230', '0.7289', '0.3886', '0.8665', '0.6246', '0.4000', '0.5382']
his_acc:  ['0.8230', '0.7835', '0.6621', '0.6842', '0.6528', '0.5866', '0.5153']
cur_acc des:  ['0.8597', '0.8266', '0.5561', '0.8525', '0.7847', '0.6869', '0.6161']
his_acc des:  ['0.8597', '0.8322', '0.7171', '0.7360', '0.7232', '0.6710', '0.6118']
cur_acc rrf:  ['0.8618', '0.8347', '0.5405', '0.8706', '0.7770', '0.6802', '0.6250']
his_acc rrf:  ['0.8618', '0.8333', '0.7197', '0.7339', '0.7158', '0.6667', '0.5953']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 202.8462168CurrentTrain: epoch  0, batch     1 | loss: 180.9878792CurrentTrain: epoch  0, batch     2 | loss: 229.7149581CurrentTrain: epoch  0, batch     3 | loss: 154.8686644CurrentTrain: epoch  1, batch     0 | loss: 190.9267714CurrentTrain: epoch  1, batch     1 | loss: 190.1324724CurrentTrain: epoch  1, batch     2 | loss: 243.3862180CurrentTrain: epoch  1, batch     3 | loss: 139.2204256CurrentTrain: epoch  2, batch     0 | loss: 252.4229198CurrentTrain: epoch  2, batch     1 | loss: 177.8396992CurrentTrain: epoch  2, batch     2 | loss: 177.1717533CurrentTrain: epoch  2, batch     3 | loss: 178.1512029CurrentTrain: epoch  3, batch     0 | loss: 183.2985891CurrentTrain: epoch  3, batch     1 | loss: 213.4857394CurrentTrain: epoch  3, batch     2 | loss: 205.5009079CurrentTrain: epoch  3, batch     3 | loss: 149.7198827CurrentTrain: epoch  4, batch     0 | loss: 240.4141718CurrentTrain: epoch  4, batch     1 | loss: 193.1441087CurrentTrain: epoch  4, batch     2 | loss: 146.9843396CurrentTrain: epoch  4, batch     3 | loss: 165.4340464CurrentTrain: epoch  5, batch     0 | loss: 165.2753248CurrentTrain: epoch  5, batch     1 | loss: 203.6917344CurrentTrain: epoch  5, batch     2 | loss: 202.6280041CurrentTrain: epoch  5, batch     3 | loss: 196.0173107CurrentTrain: epoch  6, batch     0 | loss: 202.7002082CurrentTrain: epoch  6, batch     1 | loss: 194.3546018CurrentTrain: epoch  6, batch     2 | loss: 162.4019119CurrentTrain: epoch  6, batch     3 | loss: 235.2094094CurrentTrain: epoch  7, batch     0 | loss: 191.0678175CurrentTrain: epoch  7, batch     1 | loss: 194.2456383CurrentTrain: epoch  7, batch     2 | loss: 179.6353686CurrentTrain: epoch  7, batch     3 | loss: 163.7646751CurrentTrain: epoch  8, batch     0 | loss: 157.7101311CurrentTrain: epoch  8, batch     1 | loss: 286.4557605CurrentTrain: epoch  8, batch     2 | loss: 202.3606169CurrentTrain: epoch  8, batch     3 | loss: 147.4818147CurrentTrain: epoch  9, batch     0 | loss: 179.7690354CurrentTrain: epoch  9, batch     1 | loss: 276.2235738CurrentTrain: epoch  9, batch     2 | loss: 174.6030244CurrentTrain: epoch  9, batch     3 | loss: 147.1227648
MemoryTrain:  epoch  0, batch     0 | loss: 0.8969694MemoryTrain:  epoch  1, batch     0 | loss: 0.8436103MemoryTrain:  epoch  2, batch     0 | loss: 0.5880533MemoryTrain:  epoch  3, batch     0 | loss: 0.4814338MemoryTrain:  epoch  4, batch     0 | loss: 0.3926622MemoryTrain:  epoch  5, batch     0 | loss: 0.3236141MemoryTrain:  epoch  6, batch     0 | loss: 0.2478118MemoryTrain:  epoch  7, batch     0 | loss: 0.2278167MemoryTrain:  epoch  8, batch     0 | loss: 0.1834857MemoryTrain:  epoch  9, batch     0 | loss: 0.1750518

F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.75, 38: 0.0, 8: 0.0, 39: 0.0, 11: 0.4927536231884058, 13: 0.0, 15: 0.0, 18: 0.631578947368421, 20: 0.5882352941176471, 23: 0.4864864864864865, 25: 0.0}
Micro-average F1 score: 0.4879518072289157
Weighted-average F1 score: 0.364507984807122
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.0, 13: 0.0, 14: 0.0, 15: 0.75, 18: 0.0, 20: 0.0, 23: 0.0, 25: 0.6835443037974683, 33: 0.0, 34: 0.0, 35: 0.8695652173913043, 36: 0.0, 37: 0.6813186813186813, 38: 0.7272727272727273, 39: 0.0}
Micro-average F1 score: 0.6060606060606061
Weighted-average F1 score: 0.4685390403734251
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.0, 13: 0.0, 15: 0.75, 18: 0.0, 20: 0.0, 22: 0.0, 23: 0.0, 25: 0.6493506493506493, 33: 0.0, 34: 0.0, 35: 0.8695652173913043, 36: 0.0, 37: 0.723404255319149, 38: 0.7272727272727273, 39: 0.0}
Micro-average F1 score: 0.6126582278481013
Weighted-average F1 score: 0.47741262094857945

F1 score per class: {0: 0.9142857142857143, 1: 0.32167832167832167, 2: 0.5185185185185185, 3: 0.7248322147651006, 4: 0.8304093567251462, 5: 0.9004739336492891, 6: 0.2413793103448276, 7: 0.06451612903225806, 8: 0.3364485981308411, 9: 0.96, 10: 0.2413793103448276, 11: 0.3108108108108108, 12: 0.09433962264150944, 13: 0.06060606060606061, 14: 0.029411764705882353, 15: 0.5454545454545454, 16: 0.8275862068965517, 17: 0.0, 18: 0.28, 19: 0.518918918918919, 20: 0.38461538461538464, 21: 0.0, 22: 0.7422680412371134, 23: 0.8275862068965517, 24: 0.0, 25: 0.4927536231884058, 26: 0.7457627118644068, 27: 0.18181818181818182, 28: 0.15384615384615385, 29: 0.8666666666666667, 30: 0.9444444444444444, 31: 0.0, 32: 0.7325581395348837, 33: 0.3333333333333333, 34: 0.4375, 35: 0.42105263157894735, 36: 0.14084507042253522, 37: 0.37037037037037035, 38: 0.2857142857142857, 39: 0.07692307692307693, 40: 0.26506024096385544}
Micro-average F1 score: 0.5197112715158245
Weighted-average F1 score: 0.5616178547150589
F1 score per class: {0: 0.972972972972973, 1: 0.3055555555555556, 2: 0.4117647058823529, 3: 0.8636363636363636, 4: 0.8235294117647058, 5: 0.8839285714285714, 6: 0.49673202614379086, 7: 0.037037037037037035, 8: 0.48148148148148145, 9: 0.9803921568627451, 10: 0.352, 11: 0.33540372670807456, 12: 0.6060606060606061, 13: 0.06896551724137931, 14: 0.07317073170731707, 15: 0.631578947368421, 16: 0.8571428571428571, 17: 0.0, 18: 0.5714285714285714, 19: 0.6464646464646465, 20: 0.7191011235955056, 21: 0.16666666666666666, 22: 0.75, 23: 0.8817204301075269, 24: 0.0, 25: 0.6835443037974683, 26: 0.7472527472527473, 27: 0.2222222222222222, 28: 0.15, 29: 0.9148936170212766, 30: 0.972972972972973, 31: 1.0, 32: 0.84375, 33: 0.3076923076923077, 34: 0.43537414965986393, 35: 0.7547169811320755, 36: 0.6666666666666666, 37: 0.512396694214876, 38: 0.43243243243243246, 39: 0.3333333333333333, 40: 0.5555555555555556}
Micro-average F1 score: 0.6211271041717492
Weighted-average F1 score: 0.6184366685311952
F1 score per class: {0: 0.972972972972973, 1: 0.31724137931034485, 2: 0.4117647058823529, 3: 0.8554913294797688, 4: 0.8235294117647058, 5: 0.8918918918918919, 6: 0.46258503401360546, 7: 0.03508771929824561, 8: 0.4520547945205479, 9: 0.9803921568627451, 10: 0.32786885245901637, 11: 0.34408602150537637, 12: 0.543046357615894, 13: 0.06451612903225806, 14: 0.07317073170731707, 15: 0.6, 16: 0.8771929824561403, 17: 0.0, 18: 0.4482758620689655, 19: 0.64, 20: 0.6956521739130435, 21: 0.17142857142857143, 22: 0.7428571428571429, 23: 0.8791208791208791, 24: 0.0, 25: 0.6493506493506493, 26: 0.7472527472527473, 27: 0.23076923076923078, 28: 0.14634146341463414, 29: 0.9148936170212766, 30: 0.972972972972973, 31: 1.0, 32: 0.8359788359788359, 33: 0.24, 34: 0.4316546762589928, 35: 0.7017543859649122, 36: 0.4666666666666667, 37: 0.5151515151515151, 38: 0.3764705882352941, 39: 0.24242424242424243, 40: 0.5862068965517241}
Micro-average F1 score: 0.603643525356967
Weighted-average F1 score: 0.6023575363648663
cur_acc:  ['0.8230', '0.7289', '0.3886', '0.8665', '0.6246', '0.4000', '0.5382', '0.4880']
his_acc:  ['0.8230', '0.7835', '0.6621', '0.6842', '0.6528', '0.5866', '0.5153', '0.5197']
cur_acc des:  ['0.8597', '0.8266', '0.5561', '0.8525', '0.7847', '0.6869', '0.6161', '0.6061']
his_acc des:  ['0.8597', '0.8322', '0.7171', '0.7360', '0.7232', '0.6710', '0.6118', '0.6211']
cur_acc rrf:  ['0.8618', '0.8347', '0.5405', '0.8706', '0.7770', '0.6802', '0.6250', '0.6127']
his_acc rrf:  ['0.8618', '0.8333', '0.7197', '0.7339', '0.7158', '0.6667', '0.5953', '0.6036']
----------END
his_acc mean:  [0.8029 0.7465 0.6755 0.6486 0.5884 0.5738 0.5308 0.5179]
his_acc des mean:  [0.8398 0.8015 0.739  0.722  0.6724 0.6608 0.6178 0.6036]
his_acc rrf mean:  [0.8392 0.795  0.7346 0.715  0.6618 0.6466 0.6033 0.5901]
