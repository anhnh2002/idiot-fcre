#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 59.7954196CurrentTrain: epoch  0, batch     1 | loss: 60.2536139CurrentTrain: epoch  0, batch     2 | loss: 59.5208405CurrentTrain: epoch  0, batch     3 | loss: 37.8029690CurrentTrain: epoch  0, batch     4 | loss: 47.9592725CurrentTrain: epoch  0, batch     5 | loss: 49.3831855CurrentTrain: epoch  0, batch     6 | loss: 58.2006759CurrentTrain: epoch  0, batch     7 | loss: 40.1287835CurrentTrain: epoch  0, batch     8 | loss: 57.9971164CurrentTrain: epoch  0, batch     9 | loss: 76.6701900CurrentTrain: epoch  0, batch    10 | loss: 58.9587221CurrentTrain: epoch  0, batch    11 | loss: 58.1682726CurrentTrain: epoch  0, batch    12 | loss: 58.5963267CurrentTrain: epoch  0, batch    13 | loss: 47.0509582CurrentTrain: epoch  0, batch    14 | loss: 39.9881252CurrentTrain: epoch  0, batch    15 | loss: 41.6627049CurrentTrain: epoch  0, batch    16 | loss: 34.6114767CurrentTrain: epoch  0, batch    17 | loss: 76.9654778CurrentTrain: epoch  0, batch    18 | loss: 58.0125527CurrentTrain: epoch  0, batch    19 | loss: 75.9412670CurrentTrain: epoch  0, batch    20 | loss: 114.3327390CurrentTrain: epoch  0, batch    21 | loss: 77.2484832CurrentTrain: epoch  0, batch    22 | loss: 58.2141145CurrentTrain: epoch  0, batch    23 | loss: 47.2396202CurrentTrain: epoch  0, batch    24 | loss: 57.6201118CurrentTrain: epoch  0, batch    25 | loss: 47.0593649CurrentTrain: epoch  0, batch    26 | loss: 75.5082376CurrentTrain: epoch  0, batch    27 | loss: 57.3572416CurrentTrain: epoch  0, batch    28 | loss: 47.0062258CurrentTrain: epoch  0, batch    29 | loss: 47.8409747CurrentTrain: epoch  0, batch    30 | loss: 75.2018179CurrentTrain: epoch  0, batch    31 | loss: 113.0791164CurrentTrain: epoch  0, batch    32 | loss: 76.4402398CurrentTrain: epoch  0, batch    33 | loss: 112.7250740CurrentTrain: epoch  0, batch    34 | loss: 112.2596126CurrentTrain: epoch  0, batch    35 | loss: 75.9965977CurrentTrain: epoch  0, batch    36 | loss: 46.8205280CurrentTrain: epoch  0, batch    37 | loss: 76.2620907CurrentTrain: epoch  0, batch    38 | loss: 222.2791152CurrentTrain: epoch  0, batch    39 | loss: 39.0082417CurrentTrain: epoch  0, batch    40 | loss: 111.7227073CurrentTrain: epoch  0, batch    41 | loss: 57.3404376CurrentTrain: epoch  0, batch    42 | loss: 76.4777227CurrentTrain: epoch  0, batch    43 | loss: 75.6292459CurrentTrain: epoch  0, batch    44 | loss: 46.4206385CurrentTrain: epoch  0, batch    45 | loss: 111.7337231CurrentTrain: epoch  0, batch    46 | loss: 75.4393110CurrentTrain: epoch  0, batch    47 | loss: 76.3036833CurrentTrain: epoch  0, batch    48 | loss: 75.6688606CurrentTrain: epoch  0, batch    49 | loss: 46.4986535CurrentTrain: epoch  0, batch    50 | loss: 57.0444904CurrentTrain: epoch  0, batch    51 | loss: 75.4216440CurrentTrain: epoch  0, batch    52 | loss: 112.8032143CurrentTrain: epoch  0, batch    53 | loss: 38.4991802CurrentTrain: epoch  0, batch    54 | loss: 45.6256333CurrentTrain: epoch  0, batch    55 | loss: 57.0999347CurrentTrain: epoch  0, batch    56 | loss: 46.1515319CurrentTrain: epoch  0, batch    57 | loss: 56.8548201CurrentTrain: epoch  0, batch    58 | loss: 45.0480690CurrentTrain: epoch  0, batch    59 | loss: 76.3266730CurrentTrain: epoch  0, batch    60 | loss: 112.4607563CurrentTrain: epoch  0, batch    61 | loss: 74.7751986CurrentTrain: epoch  0, batch    62 | loss: 39.1513693CurrentTrain: epoch  0, batch    63 | loss: 38.5000824CurrentTrain: epoch  0, batch    64 | loss: 39.1726599CurrentTrain: epoch  0, batch    65 | loss: 57.7329405CurrentTrain: epoch  0, batch    66 | loss: 57.5005244CurrentTrain: epoch  0, batch    67 | loss: 34.4154067CurrentTrain: epoch  0, batch    68 | loss: 38.4082885CurrentTrain: epoch  0, batch    69 | loss: 74.2558004CurrentTrain: epoch  0, batch    70 | loss: 56.8086097CurrentTrain: epoch  0, batch    71 | loss: 46.6718523CurrentTrain: epoch  0, batch    72 | loss: 75.9750415CurrentTrain: epoch  0, batch    73 | loss: 56.5857873CurrentTrain: epoch  0, batch    74 | loss: 57.2211647CurrentTrain: epoch  0, batch    75 | loss: 56.8049546CurrentTrain: epoch  0, batch    76 | loss: 111.3271034CurrentTrain: epoch  0, batch    77 | loss: 46.6906219CurrentTrain: epoch  0, batch    78 | loss: 57.0971988CurrentTrain: epoch  0, batch    79 | loss: 111.3228630CurrentTrain: epoch  0, batch    80 | loss: 73.9240455CurrentTrain: epoch  0, batch    81 | loss: 76.5603824CurrentTrain: epoch  0, batch    82 | loss: 72.3267106CurrentTrain: epoch  0, batch    83 | loss: 46.4853385CurrentTrain: epoch  0, batch    84 | loss: 56.6072222CurrentTrain: epoch  0, batch    85 | loss: 55.6007819CurrentTrain: epoch  0, batch    86 | loss: 44.6912137CurrentTrain: epoch  0, batch    87 | loss: 45.0255593CurrentTrain: epoch  0, batch    88 | loss: 55.6853641CurrentTrain: epoch  0, batch    89 | loss: 54.3214026CurrentTrain: epoch  0, batch    90 | loss: 55.1017508CurrentTrain: epoch  0, batch    91 | loss: 45.1173282CurrentTrain: epoch  0, batch    92 | loss: 56.3997946CurrentTrain: epoch  0, batch    93 | loss: 54.7583892CurrentTrain: epoch  0, batch    94 | loss: 45.2308378CurrentTrain: epoch  0, batch    95 | loss: 73.2712977CurrentTrain: epoch  0, batch    96 | loss: 45.6961283CurrentTrain: epoch  0, batch    97 | loss: 45.4110551CurrentTrain: epoch  0, batch    98 | loss: 55.8716689CurrentTrain: epoch  0, batch    99 | loss: 42.4140410CurrentTrain: epoch  0, batch   100 | loss: 44.1861920CurrentTrain: epoch  0, batch   101 | loss: 109.1038480CurrentTrain: epoch  0, batch   102 | loss: 45.8997761CurrentTrain: epoch  0, batch   103 | loss: 44.1075633CurrentTrain: epoch  0, batch   104 | loss: 222.3970637CurrentTrain: epoch  0, batch   105 | loss: 55.6765970CurrentTrain: epoch  0, batch   106 | loss: 73.4361470CurrentTrain: epoch  0, batch   107 | loss: 38.2950500CurrentTrain: epoch  0, batch   108 | loss: 46.1880443CurrentTrain: epoch  0, batch   109 | loss: 75.0102099CurrentTrain: epoch  0, batch   110 | loss: 70.8027584CurrentTrain: epoch  0, batch   111 | loss: 72.5485006CurrentTrain: epoch  0, batch   112 | loss: 53.7947010CurrentTrain: epoch  0, batch   113 | loss: 74.7419772CurrentTrain: epoch  0, batch   114 | loss: 51.7183867CurrentTrain: epoch  0, batch   115 | loss: 72.2244119CurrentTrain: epoch  0, batch   116 | loss: 53.4286013CurrentTrain: epoch  0, batch   117 | loss: 71.5724940CurrentTrain: epoch  0, batch   118 | loss: 108.5669558CurrentTrain: epoch  0, batch   119 | loss: 52.4620104CurrentTrain: epoch  0, batch   120 | loss: 34.0359589CurrentTrain: epoch  0, batch   121 | loss: 109.7168540CurrentTrain: epoch  0, batch   122 | loss: 52.7891146CurrentTrain: epoch  0, batch   123 | loss: 73.0471967CurrentTrain: epoch  0, batch   124 | loss: 69.8818895CurrentTrain: epoch  0, batch   125 | loss: 54.1030226CurrentTrain: epoch  0, batch   126 | loss: 71.3893547CurrentTrain: epoch  0, batch   127 | loss: 53.3980452CurrentTrain: epoch  0, batch   128 | loss: 34.4613252CurrentTrain: epoch  0, batch   129 | loss: 72.4142243CurrentTrain: epoch  0, batch   130 | loss: 72.1519254CurrentTrain: epoch  0, batch   131 | loss: 34.5915121CurrentTrain: epoch  0, batch   132 | loss: 53.9864241CurrentTrain: epoch  0, batch   133 | loss: 53.9301796CurrentTrain: epoch  0, batch   134 | loss: 30.5683324CurrentTrain: epoch  0, batch   135 | loss: 54.2941083CurrentTrain: epoch  0, batch   136 | loss: 45.4270344CurrentTrain: epoch  0, batch   137 | loss: 36.1518690CurrentTrain: epoch  0, batch   138 | loss: 42.4089843CurrentTrain: epoch  0, batch   139 | loss: 55.7624540CurrentTrain: epoch  0, batch   140 | loss: 69.4861532CurrentTrain: epoch  0, batch   141 | loss: 44.0709789CurrentTrain: epoch  0, batch   142 | loss: 42.9882788CurrentTrain: epoch  0, batch   143 | loss: 83.8367276CurrentTrain: epoch  1, batch     0 | loss: 52.7807391CurrentTrain: epoch  1, batch     1 | loss: 55.4227431CurrentTrain: epoch  1, batch     2 | loss: 74.1965863CurrentTrain: epoch  1, batch     3 | loss: 44.8091697CurrentTrain: epoch  1, batch     4 | loss: 70.4990315CurrentTrain: epoch  1, batch     5 | loss: 42.5470493CurrentTrain: epoch  1, batch     6 | loss: 51.7779646CurrentTrain: epoch  1, batch     7 | loss: 51.4426052CurrentTrain: epoch  1, batch     8 | loss: 32.6836562CurrentTrain: epoch  1, batch     9 | loss: 56.5307645CurrentTrain: epoch  1, batch    10 | loss: 40.8942261CurrentTrain: epoch  1, batch    11 | loss: 73.9555612CurrentTrain: epoch  1, batch    12 | loss: 43.1595393CurrentTrain: epoch  1, batch    13 | loss: 40.5475584CurrentTrain: epoch  1, batch    14 | loss: 111.7190918CurrentTrain: epoch  1, batch    15 | loss: 42.8852493CurrentTrain: epoch  1, batch    16 | loss: 40.7828259CurrentTrain: epoch  1, batch    17 | loss: 51.0519268CurrentTrain: epoch  1, batch    18 | loss: 32.6247903CurrentTrain: epoch  1, batch    19 | loss: 51.3082547CurrentTrain: epoch  1, batch    20 | loss: 51.8167306CurrentTrain: epoch  1, batch    21 | loss: 49.2557394CurrentTrain: epoch  1, batch    22 | loss: 70.7283563CurrentTrain: epoch  1, batch    23 | loss: 54.5713131CurrentTrain: epoch  1, batch    24 | loss: 108.1381444CurrentTrain: epoch  1, batch    25 | loss: 48.5723640CurrentTrain: epoch  1, batch    26 | loss: 33.7324939CurrentTrain: epoch  1, batch    27 | loss: 38.5989900CurrentTrain: epoch  1, batch    28 | loss: 54.3573098CurrentTrain: epoch  1, batch    29 | loss: 71.5014704CurrentTrain: epoch  1, batch    30 | loss: 110.3025182CurrentTrain: epoch  1, batch    31 | loss: 107.7248831CurrentTrain: epoch  1, batch    32 | loss: 33.9442771CurrentTrain: epoch  1, batch    33 | loss: 40.6247399CurrentTrain: epoch  1, batch    34 | loss: 70.5545394CurrentTrain: epoch  1, batch    35 | loss: 51.0983661CurrentTrain: epoch  1, batch    36 | loss: 74.1399931CurrentTrain: epoch  1, batch    37 | loss: 109.3016564CurrentTrain: epoch  1, batch    38 | loss: 41.8433757CurrentTrain: epoch  1, batch    39 | loss: 31.0232309CurrentTrain: epoch  1, batch    40 | loss: 71.2905006CurrentTrain: epoch  1, batch    41 | loss: 51.5752804CurrentTrain: epoch  1, batch    42 | loss: 41.6782293CurrentTrain: epoch  1, batch    43 | loss: 33.9213747CurrentTrain: epoch  1, batch    44 | loss: 53.4325258CurrentTrain: epoch  1, batch    45 | loss: 53.0026338CurrentTrain: epoch  1, batch    46 | loss: 52.7319485CurrentTrain: epoch  1, batch    47 | loss: 55.3060992CurrentTrain: epoch  1, batch    48 | loss: 108.3379589CurrentTrain: epoch  1, batch    49 | loss: 72.5193605CurrentTrain: epoch  1, batch    50 | loss: 40.3252867CurrentTrain: epoch  1, batch    51 | loss: 47.2915439CurrentTrain: epoch  1, batch    52 | loss: 67.9824922CurrentTrain: epoch  1, batch    53 | loss: 72.7192243CurrentTrain: epoch  1, batch    54 | loss: 54.7565970CurrentTrain: epoch  1, batch    55 | loss: 50.7409860CurrentTrain: epoch  1, batch    56 | loss: 50.0626856CurrentTrain: epoch  1, batch    57 | loss: 39.8268733CurrentTrain: epoch  1, batch    58 | loss: 38.6337366CurrentTrain: epoch  1, batch    59 | loss: 54.5750167CurrentTrain: epoch  1, batch    60 | loss: 31.9724575CurrentTrain: epoch  1, batch    61 | loss: 51.3257940CurrentTrain: epoch  1, batch    62 | loss: 49.7780329CurrentTrain: epoch  1, batch    63 | loss: 40.1987789CurrentTrain: epoch  1, batch    64 | loss: 51.4418037CurrentTrain: epoch  1, batch    65 | loss: 69.6390880CurrentTrain: epoch  1, batch    66 | loss: 112.3097091CurrentTrain: epoch  1, batch    67 | loss: 53.2559080CurrentTrain: epoch  1, batch    68 | loss: 43.4882247CurrentTrain: epoch  1, batch    69 | loss: 51.2344110CurrentTrain: epoch  1, batch    70 | loss: 104.9603612CurrentTrain: epoch  1, batch    71 | loss: 40.1924822CurrentTrain: epoch  1, batch    72 | loss: 69.3588584CurrentTrain: epoch  1, batch    73 | loss: 51.5880859CurrentTrain: epoch  1, batch    74 | loss: 53.4954864CurrentTrain: epoch  1, batch    75 | loss: 49.0650937CurrentTrain: epoch  1, batch    76 | loss: 73.4926826CurrentTrain: epoch  1, batch    77 | loss: 54.7990467CurrentTrain: epoch  1, batch    78 | loss: 73.0927987CurrentTrain: epoch  1, batch    79 | loss: 71.7116031CurrentTrain: epoch  1, batch    80 | loss: 72.3233945CurrentTrain: epoch  1, batch    81 | loss: 42.6647892CurrentTrain: epoch  1, batch    82 | loss: 51.5009415CurrentTrain: epoch  1, batch    83 | loss: 52.4090044CurrentTrain: epoch  1, batch    84 | loss: 39.5056533CurrentTrain: epoch  1, batch    85 | loss: 73.6984710CurrentTrain: epoch  1, batch    86 | loss: 50.5837678CurrentTrain: epoch  1, batch    87 | loss: 51.9916003CurrentTrain: epoch  1, batch    88 | loss: 52.3296912CurrentTrain: epoch  1, batch    89 | loss: 34.6528580CurrentTrain: epoch  1, batch    90 | loss: 50.4148955CurrentTrain: epoch  1, batch    91 | loss: 68.9644523CurrentTrain: epoch  1, batch    92 | loss: 69.3415347CurrentTrain: epoch  1, batch    93 | loss: 70.2497933CurrentTrain: epoch  1, batch    94 | loss: 55.6541303CurrentTrain: epoch  1, batch    95 | loss: 48.9002857CurrentTrain: epoch  1, batch    96 | loss: 49.9069910CurrentTrain: epoch  1, batch    97 | loss: 72.9953374CurrentTrain: epoch  1, batch    98 | loss: 49.8166510CurrentTrain: epoch  1, batch    99 | loss: 52.8089601CurrentTrain: epoch  1, batch   100 | loss: 40.8288890CurrentTrain: epoch  1, batch   101 | loss: 68.7449018CurrentTrain: epoch  1, batch   102 | loss: 33.3202060CurrentTrain: epoch  1, batch   103 | loss: 33.2036305CurrentTrain: epoch  1, batch   104 | loss: 112.9236951CurrentTrain: epoch  1, batch   105 | loss: 40.3154618CurrentTrain: epoch  1, batch   106 | loss: 41.5800474CurrentTrain: epoch  1, batch   107 | loss: 53.9900925CurrentTrain: epoch  1, batch   108 | loss: 73.1473522CurrentTrain: epoch  1, batch   109 | loss: 49.1620867CurrentTrain: epoch  1, batch   110 | loss: 70.9312394CurrentTrain: epoch  1, batch   111 | loss: 71.6999998CurrentTrain: epoch  1, batch   112 | loss: 50.0895619CurrentTrain: epoch  1, batch   113 | loss: 49.5912501CurrentTrain: epoch  1, batch   114 | loss: 53.2096758CurrentTrain: epoch  1, batch   115 | loss: 55.9583437CurrentTrain: epoch  1, batch   116 | loss: 72.0475450CurrentTrain: epoch  1, batch   117 | loss: 53.5149412CurrentTrain: epoch  1, batch   118 | loss: 34.2890227CurrentTrain: epoch  1, batch   119 | loss: 48.0976473CurrentTrain: epoch  1, batch   120 | loss: 30.5376351CurrentTrain: epoch  1, batch   121 | loss: 52.1732308CurrentTrain: epoch  1, batch   122 | loss: 73.4347362CurrentTrain: epoch  1, batch   123 | loss: 39.9550857CurrentTrain: epoch  1, batch   124 | loss: 51.7445445CurrentTrain: epoch  1, batch   125 | loss: 222.5768927CurrentTrain: epoch  1, batch   126 | loss: 75.9218088CurrentTrain: epoch  1, batch   127 | loss: 47.8437117CurrentTrain: epoch  1, batch   128 | loss: 42.8851019CurrentTrain: epoch  1, batch   129 | loss: 40.6681727CurrentTrain: epoch  1, batch   130 | loss: 39.9959307CurrentTrain: epoch  1, batch   131 | loss: 32.5395202CurrentTrain: epoch  1, batch   132 | loss: 53.8608130CurrentTrain: epoch  1, batch   133 | loss: 52.7408321CurrentTrain: epoch  1, batch   134 | loss: 69.9104280CurrentTrain: epoch  1, batch   135 | loss: 36.4395671CurrentTrain: epoch  1, batch   136 | loss: 39.6420919CurrentTrain: epoch  1, batch   137 | loss: 70.4993828CurrentTrain: epoch  1, batch   138 | loss: 51.9713562CurrentTrain: epoch  1, batch   139 | loss: 69.5874238CurrentTrain: epoch  1, batch   140 | loss: 72.7093006CurrentTrain: epoch  1, batch   141 | loss: 43.2987578CurrentTrain: epoch  1, batch   142 | loss: 52.8449760CurrentTrain: epoch  1, batch   143 | loss: 29.5575662CurrentTrain: epoch  2, batch     0 | loss: 32.1113784CurrentTrain: epoch  2, batch     1 | loss: 71.9318813CurrentTrain: epoch  2, batch     2 | loss: 70.0905277CurrentTrain: epoch  2, batch     3 | loss: 39.4704244CurrentTrain: epoch  2, batch     4 | loss: 70.8755011CurrentTrain: epoch  2, batch     5 | loss: 70.1759475CurrentTrain: epoch  2, batch     6 | loss: 222.3911594CurrentTrain: epoch  2, batch     7 | loss: 39.4208178CurrentTrain: epoch  2, batch     8 | loss: 48.1358322CurrentTrain: epoch  2, batch     9 | loss: 72.3717543CurrentTrain: epoch  2, batch    10 | loss: 52.2861132CurrentTrain: epoch  2, batch    11 | loss: 39.5377603CurrentTrain: epoch  2, batch    12 | loss: 71.2297524CurrentTrain: epoch  2, batch    13 | loss: 39.6566678CurrentTrain: epoch  2, batch    14 | loss: 26.8846031CurrentTrain: epoch  2, batch    15 | loss: 47.7452119CurrentTrain: epoch  2, batch    16 | loss: 47.0185140CurrentTrain: epoch  2, batch    17 | loss: 38.7980978CurrentTrain: epoch  2, batch    18 | loss: 32.6886471CurrentTrain: epoch  2, batch    19 | loss: 50.3154491CurrentTrain: epoch  2, batch    20 | loss: 50.8102508CurrentTrain: epoch  2, batch    21 | loss: 51.5052575CurrentTrain: epoch  2, batch    22 | loss: 67.9152182CurrentTrain: epoch  2, batch    23 | loss: 73.6144722CurrentTrain: epoch  2, batch    24 | loss: 70.6679319CurrentTrain: epoch  2, batch    25 | loss: 38.5916744CurrentTrain: epoch  2, batch    26 | loss: 38.3683289CurrentTrain: epoch  2, batch    27 | loss: 38.3597078CurrentTrain: epoch  2, batch    28 | loss: 40.3487009CurrentTrain: epoch  2, batch    29 | loss: 70.5916343CurrentTrain: epoch  2, batch    30 | loss: 45.1850338CurrentTrain: epoch  2, batch    31 | loss: 39.2528245CurrentTrain: epoch  2, batch    32 | loss: 68.6400661CurrentTrain: epoch  2, batch    33 | loss: 52.1101747CurrentTrain: epoch  2, batch    34 | loss: 29.5624544CurrentTrain: epoch  2, batch    35 | loss: 69.2283138CurrentTrain: epoch  2, batch    36 | loss: 49.9840931CurrentTrain: epoch  2, batch    37 | loss: 35.6446945CurrentTrain: epoch  2, batch    38 | loss: 38.2844695CurrentTrain: epoch  2, batch    39 | loss: 50.6125230CurrentTrain: epoch  2, batch    40 | loss: 72.1061050CurrentTrain: epoch  2, batch    41 | loss: 37.9577780CurrentTrain: epoch  2, batch    42 | loss: 107.5524088CurrentTrain: epoch  2, batch    43 | loss: 38.9938880CurrentTrain: epoch  2, batch    44 | loss: 68.7711071CurrentTrain: epoch  2, batch    45 | loss: 52.3066509CurrentTrain: epoch  2, batch    46 | loss: 68.3190632CurrentTrain: epoch  2, batch    47 | loss: 37.1340559CurrentTrain: epoch  2, batch    48 | loss: 50.7094291CurrentTrain: epoch  2, batch    49 | loss: 51.8449508CurrentTrain: epoch  2, batch    50 | loss: 78.0377160CurrentTrain: epoch  2, batch    51 | loss: 108.0319683CurrentTrain: epoch  2, batch    52 | loss: 40.1725053CurrentTrain: epoch  2, batch    53 | loss: 50.1308580CurrentTrain: epoch  2, batch    54 | loss: 39.1004497CurrentTrain: epoch  2, batch    55 | loss: 51.7046594CurrentTrain: epoch  2, batch    56 | loss: 65.2478883CurrentTrain: epoch  2, batch    57 | loss: 43.0356098CurrentTrain: epoch  2, batch    58 | loss: 50.1899104CurrentTrain: epoch  2, batch    59 | loss: 49.3502497CurrentTrain: epoch  2, batch    60 | loss: 36.9058056CurrentTrain: epoch  2, batch    61 | loss: 50.0208956CurrentTrain: epoch  2, batch    62 | loss: 69.2054104CurrentTrain: epoch  2, batch    63 | loss: 51.6632991CurrentTrain: epoch  2, batch    64 | loss: 39.8029837CurrentTrain: epoch  2, batch    65 | loss: 31.8770590CurrentTrain: epoch  2, batch    66 | loss: 49.8544984CurrentTrain: epoch  2, batch    67 | loss: 39.2416850CurrentTrain: epoch  2, batch    68 | loss: 51.6831457CurrentTrain: epoch  2, batch    69 | loss: 51.0875673CurrentTrain: epoch  2, batch    70 | loss: 53.8579416CurrentTrain: epoch  2, batch    71 | loss: 32.9567522CurrentTrain: epoch  2, batch    72 | loss: 69.0896000CurrentTrain: epoch  2, batch    73 | loss: 222.4919204CurrentTrain: epoch  2, batch    74 | loss: 68.8278513CurrentTrain: epoch  2, batch    75 | loss: 31.2053427CurrentTrain: epoch  2, batch    76 | loss: 73.2115656CurrentTrain: epoch  2, batch    77 | loss: 70.4405127CurrentTrain: epoch  2, batch    78 | loss: 51.1858188CurrentTrain: epoch  2, batch    79 | loss: 39.4389232CurrentTrain: epoch  2, batch    80 | loss: 51.8126577CurrentTrain: epoch  2, batch    81 | loss: 52.3577024CurrentTrain: epoch  2, batch    82 | loss: 68.0119352CurrentTrain: epoch  2, batch    83 | loss: 50.9267924CurrentTrain: epoch  2, batch    84 | loss: 47.6811653CurrentTrain: epoch  2, batch    85 | loss: 30.9590151CurrentTrain: epoch  2, batch    86 | loss: 39.5896699CurrentTrain: epoch  2, batch    87 | loss: 51.8154941CurrentTrain: epoch  2, batch    88 | loss: 41.8854398CurrentTrain: epoch  2, batch    89 | loss: 40.7094965CurrentTrain: epoch  2, batch    90 | loss: 48.2442424CurrentTrain: epoch  2, batch    91 | loss: 48.2735643CurrentTrain: epoch  2, batch    92 | loss: 36.9409056CurrentTrain: epoch  2, batch    93 | loss: 49.8117324CurrentTrain: epoch  2, batch    94 | loss: 48.5969381CurrentTrain: epoch  2, batch    95 | loss: 49.5675261CurrentTrain: epoch  2, batch    96 | loss: 66.8718004CurrentTrain: epoch  2, batch    97 | loss: 30.9495303CurrentTrain: epoch  2, batch    98 | loss: 52.4467659CurrentTrain: epoch  2, batch    99 | loss: 110.5943311CurrentTrain: epoch  2, batch   100 | loss: 72.0133386CurrentTrain: epoch  2, batch   101 | loss: 52.2338496CurrentTrain: epoch  2, batch   102 | loss: 33.5574819CurrentTrain: epoch  2, batch   103 | loss: 49.9598439CurrentTrain: epoch  2, batch   104 | loss: 30.0539383CurrentTrain: epoch  2, batch   105 | loss: 103.7757007CurrentTrain: epoch  2, batch   106 | loss: 39.5212919CurrentTrain: epoch  2, batch   107 | loss: 38.2436545CurrentTrain: epoch  2, batch   108 | loss: 107.3969659CurrentTrain: epoch  2, batch   109 | loss: 71.3189404CurrentTrain: epoch  2, batch   110 | loss: 54.8732626CurrentTrain: epoch  2, batch   111 | loss: 39.6542602CurrentTrain: epoch  2, batch   112 | loss: 50.8109219CurrentTrain: epoch  2, batch   113 | loss: 39.9976360CurrentTrain: epoch  2, batch   114 | loss: 67.5979517CurrentTrain: epoch  2, batch   115 | loss: 49.4176380CurrentTrain: epoch  2, batch   116 | loss: 31.0955283CurrentTrain: epoch  2, batch   117 | loss: 51.6981700CurrentTrain: epoch  2, batch   118 | loss: 69.2710963CurrentTrain: epoch  2, batch   119 | loss: 49.2295551CurrentTrain: epoch  2, batch   120 | loss: 47.5425127CurrentTrain: epoch  2, batch   121 | loss: 52.3542688CurrentTrain: epoch  2, batch   122 | loss: 49.8551395CurrentTrain: epoch  2, batch   123 | loss: 49.3654686CurrentTrain: epoch  2, batch   124 | loss: 69.2073806CurrentTrain: epoch  2, batch   125 | loss: 111.9817100CurrentTrain: epoch  2, batch   126 | loss: 38.4682133CurrentTrain: epoch  2, batch   127 | loss: 50.3841048CurrentTrain: epoch  2, batch   128 | loss: 68.4256721CurrentTrain: epoch  2, batch   129 | loss: 48.5222174CurrentTrain: epoch  2, batch   130 | loss: 46.8211267CurrentTrain: epoch  2, batch   131 | loss: 30.6887978CurrentTrain: epoch  2, batch   132 | loss: 40.7104619CurrentTrain: epoch  2, batch   133 | loss: 45.3611115CurrentTrain: epoch  2, batch   134 | loss: 70.8863371CurrentTrain: epoch  2, batch   135 | loss: 69.9900107CurrentTrain: epoch  2, batch   136 | loss: 40.4590126CurrentTrain: epoch  2, batch   137 | loss: 68.6028677CurrentTrain: epoch  2, batch   138 | loss: 43.0896584CurrentTrain: epoch  2, batch   139 | loss: 38.3708475CurrentTrain: epoch  2, batch   140 | loss: 110.7287709CurrentTrain: epoch  2, batch   141 | loss: 50.4486824CurrentTrain: epoch  2, batch   142 | loss: 40.2180867CurrentTrain: epoch  2, batch   143 | loss: 36.6718695CurrentTrain: epoch  3, batch     0 | loss: 69.0535823CurrentTrain: epoch  3, batch     1 | loss: 66.1206746CurrentTrain: epoch  3, batch     2 | loss: 50.7634779CurrentTrain: epoch  3, batch     3 | loss: 40.7994273CurrentTrain: epoch  3, batch     4 | loss: 69.1722492CurrentTrain: epoch  3, batch     5 | loss: 50.7619024CurrentTrain: epoch  3, batch     6 | loss: 51.6119592CurrentTrain: epoch  3, batch     7 | loss: 65.7568379CurrentTrain: epoch  3, batch     8 | loss: 49.3453740CurrentTrain: epoch  3, batch     9 | loss: 66.0775112CurrentTrain: epoch  3, batch    10 | loss: 29.9233724CurrentTrain: epoch  3, batch    11 | loss: 37.4073618CurrentTrain: epoch  3, batch    12 | loss: 38.2159418CurrentTrain: epoch  3, batch    13 | loss: 52.0950071CurrentTrain: epoch  3, batch    14 | loss: 67.5539935CurrentTrain: epoch  3, batch    15 | loss: 38.2057702CurrentTrain: epoch  3, batch    16 | loss: 67.5475002CurrentTrain: epoch  3, batch    17 | loss: 36.9509606CurrentTrain: epoch  3, batch    18 | loss: 101.6474946CurrentTrain: epoch  3, batch    19 | loss: 64.6383422CurrentTrain: epoch  3, batch    20 | loss: 68.9183557CurrentTrain: epoch  3, batch    21 | loss: 71.1309171CurrentTrain: epoch  3, batch    22 | loss: 66.9911994CurrentTrain: epoch  3, batch    23 | loss: 49.0524969CurrentTrain: epoch  3, batch    24 | loss: 50.5845480CurrentTrain: epoch  3, batch    25 | loss: 38.2607118CurrentTrain: epoch  3, batch    26 | loss: 37.8252977CurrentTrain: epoch  3, batch    27 | loss: 40.8912239CurrentTrain: epoch  3, batch    28 | loss: 48.9893148CurrentTrain: epoch  3, batch    29 | loss: 31.1685295CurrentTrain: epoch  3, batch    30 | loss: 47.8743227CurrentTrain: epoch  3, batch    31 | loss: 50.9274179CurrentTrain: epoch  3, batch    32 | loss: 64.2047742CurrentTrain: epoch  3, batch    33 | loss: 66.3148402CurrentTrain: epoch  3, batch    34 | loss: 39.0115259CurrentTrain: epoch  3, batch    35 | loss: 49.4135162CurrentTrain: epoch  3, batch    36 | loss: 39.0316720CurrentTrain: epoch  3, batch    37 | loss: 37.7919605CurrentTrain: epoch  3, batch    38 | loss: 40.9314988CurrentTrain: epoch  3, batch    39 | loss: 51.1176627CurrentTrain: epoch  3, batch    40 | loss: 53.6078873CurrentTrain: epoch  3, batch    41 | loss: 107.2411394CurrentTrain: epoch  3, batch    42 | loss: 107.1580583CurrentTrain: epoch  3, batch    43 | loss: 35.5562772CurrentTrain: epoch  3, batch    44 | loss: 51.5539565CurrentTrain: epoch  3, batch    45 | loss: 50.6007696CurrentTrain: epoch  3, batch    46 | loss: 31.6268831CurrentTrain: epoch  3, batch    47 | loss: 66.2531534CurrentTrain: epoch  3, batch    48 | loss: 37.2275468CurrentTrain: epoch  3, batch    49 | loss: 107.0817603CurrentTrain: epoch  3, batch    50 | loss: 66.5350246CurrentTrain: epoch  3, batch    51 | loss: 50.0780667CurrentTrain: epoch  3, batch    52 | loss: 46.5391859CurrentTrain: epoch  3, batch    53 | loss: 222.4431900CurrentTrain: epoch  3, batch    54 | loss: 36.1290918CurrentTrain: epoch  3, batch    55 | loss: 39.1571726CurrentTrain: epoch  3, batch    56 | loss: 104.4432579CurrentTrain: epoch  3, batch    57 | loss: 47.2820150CurrentTrain: epoch  3, batch    58 | loss: 29.3927913CurrentTrain: epoch  3, batch    59 | loss: 38.7318325CurrentTrain: epoch  3, batch    60 | loss: 68.0561474CurrentTrain: epoch  3, batch    61 | loss: 30.4205851CurrentTrain: epoch  3, batch    62 | loss: 48.4691166CurrentTrain: epoch  3, batch    63 | loss: 36.2636159CurrentTrain: epoch  3, batch    64 | loss: 47.5973864CurrentTrain: epoch  3, batch    65 | loss: 47.9282565CurrentTrain: epoch  3, batch    66 | loss: 38.3090623CurrentTrain: epoch  3, batch    67 | loss: 69.3247951CurrentTrain: epoch  3, batch    68 | loss: 32.7181526CurrentTrain: epoch  3, batch    69 | loss: 69.6447127CurrentTrain: epoch  3, batch    70 | loss: 39.9484058CurrentTrain: epoch  3, batch    71 | loss: 69.6483866CurrentTrain: epoch  3, batch    72 | loss: 68.8503513CurrentTrain: epoch  3, batch    73 | loss: 104.5811209CurrentTrain: epoch  3, batch    74 | loss: 27.0869225CurrentTrain: epoch  3, batch    75 | loss: 41.6497118CurrentTrain: epoch  3, batch    76 | loss: 69.1132567CurrentTrain: epoch  3, batch    77 | loss: 31.1541169CurrentTrain: epoch  3, batch    78 | loss: 68.4134749CurrentTrain: epoch  3, batch    79 | loss: 39.1870479CurrentTrain: epoch  3, batch    80 | loss: 39.6475481CurrentTrain: epoch  3, batch    81 | loss: 66.1864570CurrentTrain: epoch  3, batch    82 | loss: 104.7829234CurrentTrain: epoch  3, batch    83 | loss: 35.9440663CurrentTrain: epoch  3, batch    84 | loss: 39.2502842CurrentTrain: epoch  3, batch    85 | loss: 68.1841575CurrentTrain: epoch  3, batch    86 | loss: 38.5058378CurrentTrain: epoch  3, batch    87 | loss: 48.4212698CurrentTrain: epoch  3, batch    88 | loss: 56.4514909CurrentTrain: epoch  3, batch    89 | loss: 67.0165378CurrentTrain: epoch  3, batch    90 | loss: 67.0082668CurrentTrain: epoch  3, batch    91 | loss: 48.5603576CurrentTrain: epoch  3, batch    92 | loss: 46.9410982CurrentTrain: epoch  3, batch    93 | loss: 54.0020935CurrentTrain: epoch  3, batch    94 | loss: 26.0107902CurrentTrain: epoch  3, batch    95 | loss: 48.0646674CurrentTrain: epoch  3, batch    96 | loss: 35.4833921CurrentTrain: epoch  3, batch    97 | loss: 67.2755079CurrentTrain: epoch  3, batch    98 | loss: 69.6350601CurrentTrain: epoch  3, batch    99 | loss: 67.4004819CurrentTrain: epoch  3, batch   100 | loss: 49.2843321CurrentTrain: epoch  3, batch   101 | loss: 49.1854957CurrentTrain: epoch  3, batch   102 | loss: 39.4146622CurrentTrain: epoch  3, batch   103 | loss: 50.7312298CurrentTrain: epoch  3, batch   104 | loss: 39.1343684CurrentTrain: epoch  3, batch   105 | loss: 68.7242541CurrentTrain: epoch  3, batch   106 | loss: 37.5612764CurrentTrain: epoch  3, batch   107 | loss: 31.2903332CurrentTrain: epoch  3, batch   108 | loss: 49.2091614CurrentTrain: epoch  3, batch   109 | loss: 37.4372263CurrentTrain: epoch  3, batch   110 | loss: 25.1902489CurrentTrain: epoch  3, batch   111 | loss: 51.4880447CurrentTrain: epoch  3, batch   112 | loss: 51.5017671CurrentTrain: epoch  3, batch   113 | loss: 50.3928298CurrentTrain: epoch  3, batch   114 | loss: 46.3751117CurrentTrain: epoch  3, batch   115 | loss: 70.1976715CurrentTrain: epoch  3, batch   116 | loss: 39.2820071CurrentTrain: epoch  3, batch   117 | loss: 67.1730642CurrentTrain: epoch  3, batch   118 | loss: 47.0284454CurrentTrain: epoch  3, batch   119 | loss: 47.7009195CurrentTrain: epoch  3, batch   120 | loss: 48.8765457CurrentTrain: epoch  3, batch   121 | loss: 47.5848414CurrentTrain: epoch  3, batch   122 | loss: 29.1278578CurrentTrain: epoch  3, batch   123 | loss: 54.1316475CurrentTrain: epoch  3, batch   124 | loss: 53.9014136CurrentTrain: epoch  3, batch   125 | loss: 47.6331276CurrentTrain: epoch  3, batch   126 | loss: 30.5049962CurrentTrain: epoch  3, batch   127 | loss: 50.8888699CurrentTrain: epoch  3, batch   128 | loss: 67.2210046CurrentTrain: epoch  3, batch   129 | loss: 68.2557219CurrentTrain: epoch  3, batch   130 | loss: 30.3451218CurrentTrain: epoch  3, batch   131 | loss: 107.5724227CurrentTrain: epoch  3, batch   132 | loss: 45.6521485CurrentTrain: epoch  3, batch   133 | loss: 37.7684271CurrentTrain: epoch  3, batch   134 | loss: 31.9763893CurrentTrain: epoch  3, batch   135 | loss: 49.7695258CurrentTrain: epoch  3, batch   136 | loss: 38.1360567CurrentTrain: epoch  3, batch   137 | loss: 53.3731599CurrentTrain: epoch  3, batch   138 | loss: 31.3417940CurrentTrain: epoch  3, batch   139 | loss: 52.3384632CurrentTrain: epoch  3, batch   140 | loss: 38.8996802CurrentTrain: epoch  3, batch   141 | loss: 49.6373933CurrentTrain: epoch  3, batch   142 | loss: 67.2583054CurrentTrain: epoch  3, batch   143 | loss: 21.4586420CurrentTrain: epoch  4, batch     0 | loss: 37.5032319CurrentTrain: epoch  4, batch     1 | loss: 66.7077983CurrentTrain: epoch  4, batch     2 | loss: 68.3084002CurrentTrain: epoch  4, batch     3 | loss: 46.5899451CurrentTrain: epoch  4, batch     4 | loss: 66.6851205CurrentTrain: epoch  4, batch     5 | loss: 43.8976408CurrentTrain: epoch  4, batch     6 | loss: 69.6363998CurrentTrain: epoch  4, batch     7 | loss: 49.8058623CurrentTrain: epoch  4, batch     8 | loss: 104.2293181CurrentTrain: epoch  4, batch     9 | loss: 37.9246272CurrentTrain: epoch  4, batch    10 | loss: 36.3850334CurrentTrain: epoch  4, batch    11 | loss: 45.2065705CurrentTrain: epoch  4, batch    12 | loss: 49.4919097CurrentTrain: epoch  4, batch    13 | loss: 103.3172685CurrentTrain: epoch  4, batch    14 | loss: 107.3661362CurrentTrain: epoch  4, batch    15 | loss: 107.0111646CurrentTrain: epoch  4, batch    16 | loss: 67.0078731CurrentTrain: epoch  4, batch    17 | loss: 38.8511934CurrentTrain: epoch  4, batch    18 | loss: 46.8597223CurrentTrain: epoch  4, batch    19 | loss: 38.9166885CurrentTrain: epoch  4, batch    20 | loss: 49.6137760CurrentTrain: epoch  4, batch    21 | loss: 64.5309178CurrentTrain: epoch  4, batch    22 | loss: 29.9330650CurrentTrain: epoch  4, batch    23 | loss: 68.6132541CurrentTrain: epoch  4, batch    24 | loss: 46.5697195CurrentTrain: epoch  4, batch    25 | loss: 109.6158317CurrentTrain: epoch  4, batch    26 | loss: 68.3797527CurrentTrain: epoch  4, batch    27 | loss: 66.8429523CurrentTrain: epoch  4, batch    28 | loss: 27.8816882CurrentTrain: epoch  4, batch    29 | loss: 106.3200298CurrentTrain: epoch  4, batch    30 | loss: 37.9710117CurrentTrain: epoch  4, batch    31 | loss: 67.5288060CurrentTrain: epoch  4, batch    32 | loss: 32.1484614CurrentTrain: epoch  4, batch    33 | loss: 69.1030044CurrentTrain: epoch  4, batch    34 | loss: 40.8087647CurrentTrain: epoch  4, batch    35 | loss: 35.3035343CurrentTrain: epoch  4, batch    36 | loss: 67.0073947CurrentTrain: epoch  4, batch    37 | loss: 48.3197350CurrentTrain: epoch  4, batch    38 | loss: 47.0762024CurrentTrain: epoch  4, batch    39 | loss: 48.2240501CurrentTrain: epoch  4, batch    40 | loss: 38.7037835CurrentTrain: epoch  4, batch    41 | loss: 68.9368238CurrentTrain: epoch  4, batch    42 | loss: 65.5088946CurrentTrain: epoch  4, batch    43 | loss: 49.8043273CurrentTrain: epoch  4, batch    44 | loss: 46.8470863CurrentTrain: epoch  4, batch    45 | loss: 40.0183685CurrentTrain: epoch  4, batch    46 | loss: 67.0333900CurrentTrain: epoch  4, batch    47 | loss: 47.0345580CurrentTrain: epoch  4, batch    48 | loss: 47.2130449CurrentTrain: epoch  4, batch    49 | loss: 65.9195911CurrentTrain: epoch  4, batch    50 | loss: 29.1840362CurrentTrain: epoch  4, batch    51 | loss: 69.6137577CurrentTrain: epoch  4, batch    52 | loss: 37.5008435CurrentTrain: epoch  4, batch    53 | loss: 68.4470670CurrentTrain: epoch  4, batch    54 | loss: 50.3385634CurrentTrain: epoch  4, batch    55 | loss: 37.7607385CurrentTrain: epoch  4, batch    56 | loss: 29.9681239CurrentTrain: epoch  4, batch    57 | loss: 39.3655342CurrentTrain: epoch  4, batch    58 | loss: 38.6426471CurrentTrain: epoch  4, batch    59 | loss: 37.9309865CurrentTrain: epoch  4, batch    60 | loss: 38.3819370CurrentTrain: epoch  4, batch    61 | loss: 50.5480905CurrentTrain: epoch  4, batch    62 | loss: 31.1930028CurrentTrain: epoch  4, batch    63 | loss: 69.5070912CurrentTrain: epoch  4, batch    64 | loss: 36.5634027CurrentTrain: epoch  4, batch    65 | loss: 36.1669373CurrentTrain: epoch  4, batch    66 | loss: 35.9826818CurrentTrain: epoch  4, batch    67 | loss: 51.3000288CurrentTrain: epoch  4, batch    68 | loss: 49.5110930CurrentTrain: epoch  4, batch    69 | loss: 40.3662011CurrentTrain: epoch  4, batch    70 | loss: 68.5639044CurrentTrain: epoch  4, batch    71 | loss: 62.2756149CurrentTrain: epoch  4, batch    72 | loss: 107.4572912CurrentTrain: epoch  4, batch    73 | loss: 40.0052891CurrentTrain: epoch  4, batch    74 | loss: 38.5096390CurrentTrain: epoch  4, batch    75 | loss: 47.8084445CurrentTrain: epoch  4, batch    76 | loss: 29.8712162CurrentTrain: epoch  4, batch    77 | loss: 104.1413578CurrentTrain: epoch  4, batch    78 | loss: 50.1712686CurrentTrain: epoch  4, batch    79 | loss: 38.7289507CurrentTrain: epoch  4, batch    80 | loss: 69.5771559CurrentTrain: epoch  4, batch    81 | loss: 50.6602422CurrentTrain: epoch  4, batch    82 | loss: 48.0716042CurrentTrain: epoch  4, batch    83 | loss: 66.6968445CurrentTrain: epoch  4, batch    84 | loss: 69.0367232CurrentTrain: epoch  4, batch    85 | loss: 70.7745349CurrentTrain: epoch  4, batch    86 | loss: 37.9752162CurrentTrain: epoch  4, batch    87 | loss: 65.0449999CurrentTrain: epoch  4, batch    88 | loss: 107.0121348CurrentTrain: epoch  4, batch    89 | loss: 71.8174992CurrentTrain: epoch  4, batch    90 | loss: 69.0039442CurrentTrain: epoch  4, batch    91 | loss: 37.7779190CurrentTrain: epoch  4, batch    92 | loss: 32.8110784CurrentTrain: epoch  4, batch    93 | loss: 36.0108438CurrentTrain: epoch  4, batch    94 | loss: 46.3597319CurrentTrain: epoch  4, batch    95 | loss: 30.8667192CurrentTrain: epoch  4, batch    96 | loss: 53.1727930CurrentTrain: epoch  4, batch    97 | loss: 68.3518079CurrentTrain: epoch  4, batch    98 | loss: 66.7205100CurrentTrain: epoch  4, batch    99 | loss: 69.1155778CurrentTrain: epoch  4, batch   100 | loss: 27.0366405CurrentTrain: epoch  4, batch   101 | loss: 36.8760282CurrentTrain: epoch  4, batch   102 | loss: 48.6887886CurrentTrain: epoch  4, batch   103 | loss: 69.0584790CurrentTrain: epoch  4, batch   104 | loss: 37.6335884CurrentTrain: epoch  4, batch   105 | loss: 45.5108820CurrentTrain: epoch  4, batch   106 | loss: 48.7717521CurrentTrain: epoch  4, batch   107 | loss: 103.9572092CurrentTrain: epoch  4, batch   108 | loss: 26.4268950CurrentTrain: epoch  4, batch   109 | loss: 106.9383032CurrentTrain: epoch  4, batch   110 | loss: 30.3114548CurrentTrain: epoch  4, batch   111 | loss: 107.5177492CurrentTrain: epoch  4, batch   112 | loss: 107.0708679CurrentTrain: epoch  4, batch   113 | loss: 36.0173754CurrentTrain: epoch  4, batch   114 | loss: 68.5433302CurrentTrain: epoch  4, batch   115 | loss: 66.3144483CurrentTrain: epoch  4, batch   116 | loss: 66.2936611CurrentTrain: epoch  4, batch   117 | loss: 30.0673174CurrentTrain: epoch  4, batch   118 | loss: 66.2238331CurrentTrain: epoch  4, batch   119 | loss: 46.5195960CurrentTrain: epoch  4, batch   120 | loss: 27.8940252CurrentTrain: epoch  4, batch   121 | loss: 108.7476536CurrentTrain: epoch  4, batch   122 | loss: 66.2717193CurrentTrain: epoch  4, batch   123 | loss: 34.1941204CurrentTrain: epoch  4, batch   124 | loss: 48.9941038CurrentTrain: epoch  4, batch   125 | loss: 31.0199647CurrentTrain: epoch  4, batch   126 | loss: 35.2476562CurrentTrain: epoch  4, batch   127 | loss: 34.7238196CurrentTrain: epoch  4, batch   128 | loss: 49.2171265CurrentTrain: epoch  4, batch   129 | loss: 49.7975160CurrentTrain: epoch  4, batch   130 | loss: 107.7569294CurrentTrain: epoch  4, batch   131 | loss: 67.3044102CurrentTrain: epoch  4, batch   132 | loss: 68.7351363CurrentTrain: epoch  4, batch   133 | loss: 38.6219673CurrentTrain: epoch  4, batch   134 | loss: 54.2742294CurrentTrain: epoch  4, batch   135 | loss: 67.0320405CurrentTrain: epoch  4, batch   136 | loss: 47.5978345CurrentTrain: epoch  4, batch   137 | loss: 66.1299987CurrentTrain: epoch  4, batch   138 | loss: 37.8960118CurrentTrain: epoch  4, batch   139 | loss: 66.3962474CurrentTrain: epoch  4, batch   140 | loss: 35.3410490CurrentTrain: epoch  4, batch   141 | loss: 51.3009876CurrentTrain: epoch  4, batch   142 | loss: 49.6959177CurrentTrain: epoch  4, batch   143 | loss: 51.3898366CurrentTrain: epoch  5, batch     0 | loss: 49.0111750CurrentTrain: epoch  5, batch     1 | loss: 36.9015905CurrentTrain: epoch  5, batch     2 | loss: 46.0601146CurrentTrain: epoch  5, batch     3 | loss: 66.5835445CurrentTrain: epoch  5, batch     4 | loss: 48.0127374CurrentTrain: epoch  5, batch     5 | loss: 47.9018514CurrentTrain: epoch  5, batch     6 | loss: 48.4632495CurrentTrain: epoch  5, batch     7 | loss: 34.7069204CurrentTrain: epoch  5, batch     8 | loss: 37.3236181CurrentTrain: epoch  5, batch     9 | loss: 73.8082150CurrentTrain: epoch  5, batch    10 | loss: 47.4782520CurrentTrain: epoch  5, batch    11 | loss: 65.6978292CurrentTrain: epoch  5, batch    12 | loss: 36.7962833CurrentTrain: epoch  5, batch    13 | loss: 49.1496068CurrentTrain: epoch  5, batch    14 | loss: 36.1158553CurrentTrain: epoch  5, batch    15 | loss: 28.5052354CurrentTrain: epoch  5, batch    16 | loss: 222.3650673CurrentTrain: epoch  5, batch    17 | loss: 49.1064924CurrentTrain: epoch  5, batch    18 | loss: 68.0801875CurrentTrain: epoch  5, batch    19 | loss: 106.9361512CurrentTrain: epoch  5, batch    20 | loss: 35.8087034CurrentTrain: epoch  5, batch    21 | loss: 68.3502613CurrentTrain: epoch  5, batch    22 | loss: 27.8769849CurrentTrain: epoch  5, batch    23 | loss: 67.6754255CurrentTrain: epoch  5, batch    24 | loss: 37.6252075CurrentTrain: epoch  5, batch    25 | loss: 47.2029736CurrentTrain: epoch  5, batch    26 | loss: 35.0372277CurrentTrain: epoch  5, batch    27 | loss: 40.1892478CurrentTrain: epoch  5, batch    28 | loss: 39.3005686CurrentTrain: epoch  5, batch    29 | loss: 47.5205618CurrentTrain: epoch  5, batch    30 | loss: 35.4976387CurrentTrain: epoch  5, batch    31 | loss: 68.1188691CurrentTrain: epoch  5, batch    32 | loss: 37.3582265CurrentTrain: epoch  5, batch    33 | loss: 36.7904015CurrentTrain: epoch  5, batch    34 | loss: 107.0993774CurrentTrain: epoch  5, batch    35 | loss: 46.4163902CurrentTrain: epoch  5, batch    36 | loss: 30.4412748CurrentTrain: epoch  5, batch    37 | loss: 36.8384151CurrentTrain: epoch  5, batch    38 | loss: 49.9269398CurrentTrain: epoch  5, batch    39 | loss: 106.8816259CurrentTrain: epoch  5, batch    40 | loss: 49.2052085CurrentTrain: epoch  5, batch    41 | loss: 48.1007468CurrentTrain: epoch  5, batch    42 | loss: 106.7907057CurrentTrain: epoch  5, batch    43 | loss: 106.8134529CurrentTrain: epoch  5, batch    44 | loss: 222.1057897CurrentTrain: epoch  5, batch    45 | loss: 37.6289795CurrentTrain: epoch  5, batch    46 | loss: 50.9592579CurrentTrain: epoch  5, batch    47 | loss: 106.7847444CurrentTrain: epoch  5, batch    48 | loss: 36.5090149CurrentTrain: epoch  5, batch    49 | loss: 27.5707527CurrentTrain: epoch  5, batch    50 | loss: 46.1669904CurrentTrain: epoch  5, batch    51 | loss: 63.2755273CurrentTrain: epoch  5, batch    52 | loss: 37.3639797CurrentTrain: epoch  5, batch    53 | loss: 32.6562709CurrentTrain: epoch  5, batch    54 | loss: 103.6585406CurrentTrain: epoch  5, batch    55 | loss: 64.9803695CurrentTrain: epoch  5, batch    56 | loss: 48.9936251CurrentTrain: epoch  5, batch    57 | loss: 47.8521540CurrentTrain: epoch  5, batch    58 | loss: 47.7944800CurrentTrain: epoch  5, batch    59 | loss: 68.3046932CurrentTrain: epoch  5, batch    60 | loss: 36.4459078CurrentTrain: epoch  5, batch    61 | loss: 47.4944981CurrentTrain: epoch  5, batch    62 | loss: 109.0385630CurrentTrain: epoch  5, batch    63 | loss: 107.1098337CurrentTrain: epoch  5, batch    64 | loss: 48.7221710CurrentTrain: epoch  5, batch    65 | loss: 68.3899773CurrentTrain: epoch  5, batch    66 | loss: 35.5612734CurrentTrain: epoch  5, batch    67 | loss: 47.5981127CurrentTrain: epoch  5, batch    68 | loss: 69.0035758CurrentTrain: epoch  5, batch    69 | loss: 48.9991517CurrentTrain: epoch  5, batch    70 | loss: 36.8349053CurrentTrain: epoch  5, batch    71 | loss: 30.3086993CurrentTrain: epoch  5, batch    72 | loss: 34.4119963CurrentTrain: epoch  5, batch    73 | loss: 46.2576496CurrentTrain: epoch  5, batch    74 | loss: 49.5730521CurrentTrain: epoch  5, batch    75 | loss: 24.9058009CurrentTrain: epoch  5, batch    76 | loss: 46.1872192CurrentTrain: epoch  5, batch    77 | loss: 48.2151923CurrentTrain: epoch  5, batch    78 | loss: 69.1924923CurrentTrain: epoch  5, batch    79 | loss: 46.7952330CurrentTrain: epoch  5, batch    80 | loss: 44.9296314CurrentTrain: epoch  5, batch    81 | loss: 36.4318714CurrentTrain: epoch  5, batch    82 | loss: 39.2996569CurrentTrain: epoch  5, batch    83 | loss: 222.3195562CurrentTrain: epoch  5, batch    84 | loss: 107.0729852CurrentTrain: epoch  5, batch    85 | loss: 48.9405126CurrentTrain: epoch  5, batch    86 | loss: 50.4300494CurrentTrain: epoch  5, batch    87 | loss: 51.2797535CurrentTrain: epoch  5, batch    88 | loss: 69.3470813CurrentTrain: epoch  5, batch    89 | loss: 68.7841386CurrentTrain: epoch  5, batch    90 | loss: 68.3388541CurrentTrain: epoch  5, batch    91 | loss: 67.7843812CurrentTrain: epoch  5, batch    92 | loss: 47.7320224CurrentTrain: epoch  5, batch    93 | loss: 69.1391562CurrentTrain: epoch  5, batch    94 | loss: 107.4691917CurrentTrain: epoch  5, batch    95 | loss: 34.6659790CurrentTrain: epoch  5, batch    96 | loss: 27.4672183CurrentTrain: epoch  5, batch    97 | loss: 47.7436200CurrentTrain: epoch  5, batch    98 | loss: 36.4608403CurrentTrain: epoch  5, batch    99 | loss: 35.6975975CurrentTrain: epoch  5, batch   100 | loss: 63.2257531CurrentTrain: epoch  5, batch   101 | loss: 50.0260180CurrentTrain: epoch  5, batch   102 | loss: 37.9397611CurrentTrain: epoch  5, batch   103 | loss: 48.2055100CurrentTrain: epoch  5, batch   104 | loss: 36.2283409CurrentTrain: epoch  5, batch   105 | loss: 66.4777406CurrentTrain: epoch  5, batch   106 | loss: 69.3200267CurrentTrain: epoch  5, batch   107 | loss: 49.7020618CurrentTrain: epoch  5, batch   108 | loss: 55.8711674CurrentTrain: epoch  5, batch   109 | loss: 36.6817144CurrentTrain: epoch  5, batch   110 | loss: 35.6560207CurrentTrain: epoch  5, batch   111 | loss: 46.4358630CurrentTrain: epoch  5, batch   112 | loss: 24.2983562CurrentTrain: epoch  5, batch   113 | loss: 49.3038081CurrentTrain: epoch  5, batch   114 | loss: 101.3934384CurrentTrain: epoch  5, batch   115 | loss: 37.7684465CurrentTrain: epoch  5, batch   116 | loss: 103.5005855CurrentTrain: epoch  5, batch   117 | loss: 34.7919544CurrentTrain: epoch  5, batch   118 | loss: 36.4083791CurrentTrain: epoch  5, batch   119 | loss: 68.4479061CurrentTrain: epoch  5, batch   120 | loss: 45.5843872CurrentTrain: epoch  5, batch   121 | loss: 107.0363664CurrentTrain: epoch  5, batch   122 | loss: 47.6105213CurrentTrain: epoch  5, batch   123 | loss: 41.5671564CurrentTrain: epoch  5, batch   124 | loss: 101.0138910CurrentTrain: epoch  5, batch   125 | loss: 49.4898200CurrentTrain: epoch  5, batch   126 | loss: 49.0570306CurrentTrain: epoch  5, batch   127 | loss: 39.2196512CurrentTrain: epoch  5, batch   128 | loss: 37.5702833CurrentTrain: epoch  5, batch   129 | loss: 47.8697434CurrentTrain: epoch  5, batch   130 | loss: 68.2835723CurrentTrain: epoch  5, batch   131 | loss: 31.2859489CurrentTrain: epoch  5, batch   132 | loss: 68.5090799CurrentTrain: epoch  5, batch   133 | loss: 28.8878499CurrentTrain: epoch  5, batch   134 | loss: 50.0420475CurrentTrain: epoch  5, batch   135 | loss: 36.5077876CurrentTrain: epoch  5, batch   136 | loss: 49.5946205CurrentTrain: epoch  5, batch   137 | loss: 68.1641789CurrentTrain: epoch  5, batch   138 | loss: 47.1955009CurrentTrain: epoch  5, batch   139 | loss: 47.6539772CurrentTrain: epoch  5, batch   140 | loss: 28.2474071CurrentTrain: epoch  5, batch   141 | loss: 68.2641209CurrentTrain: epoch  5, batch   142 | loss: 68.2573793CurrentTrain: epoch  5, batch   143 | loss: 27.6386047CurrentTrain: epoch  6, batch     0 | loss: 24.1890871CurrentTrain: epoch  6, batch     1 | loss: 46.8306327CurrentTrain: epoch  6, batch     2 | loss: 49.5593686CurrentTrain: epoch  6, batch     3 | loss: 34.3850829CurrentTrain: epoch  6, batch     4 | loss: 51.2452020CurrentTrain: epoch  6, batch     5 | loss: 106.7817098CurrentTrain: epoch  6, batch     6 | loss: 68.1628622CurrentTrain: epoch  6, batch     7 | loss: 222.2647323CurrentTrain: epoch  6, batch     8 | loss: 106.7828008CurrentTrain: epoch  6, batch     9 | loss: 49.1922673CurrentTrain: epoch  6, batch    10 | loss: 47.4486197CurrentTrain: epoch  6, batch    11 | loss: 49.5359701CurrentTrain: epoch  6, batch    12 | loss: 49.3777998CurrentTrain: epoch  6, batch    13 | loss: 67.1754686CurrentTrain: epoch  6, batch    14 | loss: 68.0785338CurrentTrain: epoch  6, batch    15 | loss: 103.4918953CurrentTrain: epoch  6, batch    16 | loss: 64.9740864CurrentTrain: epoch  6, batch    17 | loss: 37.9906057CurrentTrain: epoch  6, batch    18 | loss: 222.1863191CurrentTrain: epoch  6, batch    19 | loss: 36.3849684CurrentTrain: epoch  6, batch    20 | loss: 49.4103702CurrentTrain: epoch  6, batch    21 | loss: 37.5389460CurrentTrain: epoch  6, batch    22 | loss: 42.9426458CurrentTrain: epoch  6, batch    23 | loss: 35.8479905CurrentTrain: epoch  6, batch    24 | loss: 29.8287026CurrentTrain: epoch  6, batch    25 | loss: 68.1440607CurrentTrain: epoch  6, batch    26 | loss: 35.4821213CurrentTrain: epoch  6, batch    27 | loss: 29.8997667CurrentTrain: epoch  6, batch    28 | loss: 36.0587122CurrentTrain: epoch  6, batch    29 | loss: 63.8186980CurrentTrain: epoch  6, batch    30 | loss: 29.3273627CurrentTrain: epoch  6, batch    31 | loss: 48.1359666CurrentTrain: epoch  6, batch    32 | loss: 47.6131742CurrentTrain: epoch  6, batch    33 | loss: 52.0740306CurrentTrain: epoch  6, batch    34 | loss: 64.9545570CurrentTrain: epoch  6, batch    35 | loss: 66.6166603CurrentTrain: epoch  6, batch    36 | loss: 106.8600311CurrentTrain: epoch  6, batch    37 | loss: 41.6509108CurrentTrain: epoch  6, batch    38 | loss: 66.4364556CurrentTrain: epoch  6, batch    39 | loss: 37.5704905CurrentTrain: epoch  6, batch    40 | loss: 46.3101650CurrentTrain: epoch  6, batch    41 | loss: 28.4996407CurrentTrain: epoch  6, batch    42 | loss: 68.1638324CurrentTrain: epoch  6, batch    43 | loss: 28.1546108CurrentTrain: epoch  6, batch    44 | loss: 68.0887165CurrentTrain: epoch  6, batch    45 | loss: 68.0965169CurrentTrain: epoch  6, batch    46 | loss: 47.9808746CurrentTrain: epoch  6, batch    47 | loss: 24.2316868CurrentTrain: epoch  6, batch    48 | loss: 66.4738681CurrentTrain: epoch  6, batch    49 | loss: 47.6197177CurrentTrain: epoch  6, batch    50 | loss: 49.1732182CurrentTrain: epoch  6, batch    51 | loss: 64.1994832CurrentTrain: epoch  6, batch    52 | loss: 48.3176207CurrentTrain: epoch  6, batch    53 | loss: 36.4432799CurrentTrain: epoch  6, batch    54 | loss: 37.4873878CurrentTrain: epoch  6, batch    55 | loss: 47.7510445CurrentTrain: epoch  6, batch    56 | loss: 37.4573033CurrentTrain: epoch  6, batch    57 | loss: 66.1393983CurrentTrain: epoch  6, batch    58 | loss: 35.6767218CurrentTrain: epoch  6, batch    59 | loss: 67.0024305CurrentTrain: epoch  6, batch    60 | loss: 35.7480581CurrentTrain: epoch  6, batch    61 | loss: 38.2850683CurrentTrain: epoch  6, batch    62 | loss: 62.9186490CurrentTrain: epoch  6, batch    63 | loss: 36.1575972CurrentTrain: epoch  6, batch    64 | loss: 29.1478777CurrentTrain: epoch  6, batch    65 | loss: 47.5157206CurrentTrain: epoch  6, batch    66 | loss: 22.6708522CurrentTrain: epoch  6, batch    67 | loss: 47.6793361CurrentTrain: epoch  6, batch    68 | loss: 48.9264091CurrentTrain: epoch  6, batch    69 | loss: 47.5231298CurrentTrain: epoch  6, batch    70 | loss: 50.4285895CurrentTrain: epoch  6, batch    71 | loss: 106.7873126CurrentTrain: epoch  6, batch    72 | loss: 68.0860951CurrentTrain: epoch  6, batch    73 | loss: 24.0688497CurrentTrain: epoch  6, batch    74 | loss: 46.3605953CurrentTrain: epoch  6, batch    75 | loss: 48.9830513CurrentTrain: epoch  6, batch    76 | loss: 68.1185397CurrentTrain: epoch  6, batch    77 | loss: 64.7758405CurrentTrain: epoch  6, batch    78 | loss: 222.4291655CurrentTrain: epoch  6, batch    79 | loss: 49.3182029CurrentTrain: epoch  6, batch    80 | loss: 106.8738653CurrentTrain: epoch  6, batch    81 | loss: 49.1390231CurrentTrain: epoch  6, batch    82 | loss: 66.0815457CurrentTrain: epoch  6, batch    83 | loss: 37.4244148CurrentTrain: epoch  6, batch    84 | loss: 49.7164471CurrentTrain: epoch  6, batch    85 | loss: 64.7674308CurrentTrain: epoch  6, batch    86 | loss: 27.0503159CurrentTrain: epoch  6, batch    87 | loss: 66.4276933CurrentTrain: epoch  6, batch    88 | loss: 48.9879394CurrentTrain: epoch  6, batch    89 | loss: 28.8949351CurrentTrain: epoch  6, batch    90 | loss: 46.6560482CurrentTrain: epoch  6, batch    91 | loss: 101.0069917CurrentTrain: epoch  6, batch    92 | loss: 49.1123059CurrentTrain: epoch  6, batch    93 | loss: 46.1238403CurrentTrain: epoch  6, batch    94 | loss: 45.9884099CurrentTrain: epoch  6, batch    95 | loss: 47.3005038CurrentTrain: epoch  6, batch    96 | loss: 48.9753134CurrentTrain: epoch  6, batch    97 | loss: 47.4571081CurrentTrain: epoch  6, batch    98 | loss: 47.6826815CurrentTrain: epoch  6, batch    99 | loss: 38.9732930CurrentTrain: epoch  6, batch   100 | loss: 50.1893939CurrentTrain: epoch  6, batch   101 | loss: 48.4852062CurrentTrain: epoch  6, batch   102 | loss: 68.1409913CurrentTrain: epoch  6, batch   103 | loss: 34.7179075CurrentTrain: epoch  6, batch   104 | loss: 49.0936545CurrentTrain: epoch  6, batch   105 | loss: 72.6792997CurrentTrain: epoch  6, batch   106 | loss: 47.4574782CurrentTrain: epoch  6, batch   107 | loss: 32.2036417CurrentTrain: epoch  6, batch   108 | loss: 50.8071386CurrentTrain: epoch  6, batch   109 | loss: 35.5578236CurrentTrain: epoch  6, batch   110 | loss: 27.6885135CurrentTrain: epoch  6, batch   111 | loss: 68.1208594CurrentTrain: epoch  6, batch   112 | loss: 68.3629629CurrentTrain: epoch  6, batch   113 | loss: 47.5682971CurrentTrain: epoch  6, batch   114 | loss: 28.5757200CurrentTrain: epoch  6, batch   115 | loss: 27.5083801CurrentTrain: epoch  6, batch   116 | loss: 47.8806096CurrentTrain: epoch  6, batch   117 | loss: 68.1194202CurrentTrain: epoch  6, batch   118 | loss: 67.4633519CurrentTrain: epoch  6, batch   119 | loss: 48.4319190CurrentTrain: epoch  6, batch   120 | loss: 46.9649795CurrentTrain: epoch  6, batch   121 | loss: 36.8657543CurrentTrain: epoch  6, batch   122 | loss: 39.2249970CurrentTrain: epoch  6, batch   123 | loss: 35.5348116CurrentTrain: epoch  6, batch   124 | loss: 65.9996384CurrentTrain: epoch  6, batch   125 | loss: 64.4968976CurrentTrain: epoch  6, batch   126 | loss: 68.6183390CurrentTrain: epoch  6, batch   127 | loss: 45.6345642CurrentTrain: epoch  6, batch   128 | loss: 24.2868495CurrentTrain: epoch  6, batch   129 | loss: 35.1685266CurrentTrain: epoch  6, batch   130 | loss: 70.7285922CurrentTrain: epoch  6, batch   131 | loss: 47.7116092CurrentTrain: epoch  6, batch   132 | loss: 49.0095038CurrentTrain: epoch  6, batch   133 | loss: 71.0755139CurrentTrain: epoch  6, batch   134 | loss: 68.0827520CurrentTrain: epoch  6, batch   135 | loss: 52.2800431CurrentTrain: epoch  6, batch   136 | loss: 29.6421946CurrentTrain: epoch  6, batch   137 | loss: 47.6742283CurrentTrain: epoch  6, batch   138 | loss: 49.2214359CurrentTrain: epoch  6, batch   139 | loss: 36.3372306CurrentTrain: epoch  6, batch   140 | loss: 48.8347220CurrentTrain: epoch  6, batch   141 | loss: 31.0557659CurrentTrain: epoch  6, batch   142 | loss: 66.4001997CurrentTrain: epoch  6, batch   143 | loss: 77.8058160CurrentTrain: epoch  7, batch     0 | loss: 49.0869836CurrentTrain: epoch  7, batch     1 | loss: 68.0925719CurrentTrain: epoch  7, batch     2 | loss: 47.7470258CurrentTrain: epoch  7, batch     3 | loss: 68.4427590CurrentTrain: epoch  7, batch     4 | loss: 47.6005518CurrentTrain: epoch  7, batch     5 | loss: 48.5996316CurrentTrain: epoch  7, batch     6 | loss: 106.8632883CurrentTrain: epoch  7, batch     7 | loss: 28.3065277CurrentTrain: epoch  7, batch     8 | loss: 47.4296038CurrentTrain: epoch  7, batch     9 | loss: 50.9746008CurrentTrain: epoch  7, batch    10 | loss: 63.4772952CurrentTrain: epoch  7, batch    11 | loss: 47.5937628CurrentTrain: epoch  7, batch    12 | loss: 46.5010234CurrentTrain: epoch  7, batch    13 | loss: 36.9654662CurrentTrain: epoch  7, batch    14 | loss: 44.8677938CurrentTrain: epoch  7, batch    15 | loss: 30.5527399CurrentTrain: epoch  7, batch    16 | loss: 29.0634356CurrentTrain: epoch  7, batch    17 | loss: 49.5774596CurrentTrain: epoch  7, batch    18 | loss: 106.7828487CurrentTrain: epoch  7, batch    19 | loss: 32.4507421CurrentTrain: epoch  7, batch    20 | loss: 49.0922833CurrentTrain: epoch  7, batch    21 | loss: 49.6277689CurrentTrain: epoch  7, batch    22 | loss: 68.2605590CurrentTrain: epoch  7, batch    23 | loss: 29.9324276CurrentTrain: epoch  7, batch    24 | loss: 72.5671425CurrentTrain: epoch  7, batch    25 | loss: 37.0183997CurrentTrain: epoch  7, batch    26 | loss: 32.6469001CurrentTrain: epoch  7, batch    27 | loss: 28.1675045CurrentTrain: epoch  7, batch    28 | loss: 68.1528339CurrentTrain: epoch  7, batch    29 | loss: 30.1569520CurrentTrain: epoch  7, batch    30 | loss: 33.7641374CurrentTrain: epoch  7, batch    31 | loss: 48.9622447CurrentTrain: epoch  7, batch    32 | loss: 45.0193198CurrentTrain: epoch  7, batch    33 | loss: 68.0929848CurrentTrain: epoch  7, batch    34 | loss: 66.1639407CurrentTrain: epoch  7, batch    35 | loss: 47.7341573CurrentTrain: epoch  7, batch    36 | loss: 66.3846773CurrentTrain: epoch  7, batch    37 | loss: 36.4268319CurrentTrain: epoch  7, batch    38 | loss: 27.6131104CurrentTrain: epoch  7, batch    39 | loss: 37.2540514CurrentTrain: epoch  7, batch    40 | loss: 106.9084800CurrentTrain: epoch  7, batch    41 | loss: 68.1321007CurrentTrain: epoch  7, batch    42 | loss: 47.5298421CurrentTrain: epoch  7, batch    43 | loss: 49.2783571CurrentTrain: epoch  7, batch    44 | loss: 49.1868048CurrentTrain: epoch  7, batch    45 | loss: 67.2049105CurrentTrain: epoch  7, batch    46 | loss: 27.2302101CurrentTrain: epoch  7, batch    47 | loss: 222.1379069CurrentTrain: epoch  7, batch    48 | loss: 66.1514053CurrentTrain: epoch  7, batch    49 | loss: 48.9520025CurrentTrain: epoch  7, batch    50 | loss: 49.1085761CurrentTrain: epoch  7, batch    51 | loss: 68.0809906CurrentTrain: epoch  7, batch    52 | loss: 66.1744530CurrentTrain: epoch  7, batch    53 | loss: 68.0718446CurrentTrain: epoch  7, batch    54 | loss: 47.8263002CurrentTrain: epoch  7, batch    55 | loss: 68.1482440CurrentTrain: epoch  7, batch    56 | loss: 50.6662104CurrentTrain: epoch  7, batch    57 | loss: 48.9831162CurrentTrain: epoch  7, batch    58 | loss: 68.1684179CurrentTrain: epoch  7, batch    59 | loss: 47.7519415CurrentTrain: epoch  7, batch    60 | loss: 36.9016253CurrentTrain: epoch  7, batch    61 | loss: 45.0396521CurrentTrain: epoch  7, batch    62 | loss: 46.6434325CurrentTrain: epoch  7, batch    63 | loss: 48.9787848CurrentTrain: epoch  7, batch    64 | loss: 47.1864602CurrentTrain: epoch  7, batch    65 | loss: 45.9401672CurrentTrain: epoch  7, batch    66 | loss: 37.8014435CurrentTrain: epoch  7, batch    67 | loss: 47.4136465CurrentTrain: epoch  7, batch    68 | loss: 68.9514153CurrentTrain: epoch  7, batch    69 | loss: 38.6629820CurrentTrain: epoch  7, batch    70 | loss: 36.3155929CurrentTrain: epoch  7, batch    71 | loss: 34.7282342CurrentTrain: epoch  7, batch    72 | loss: 103.5844010CurrentTrain: epoch  7, batch    73 | loss: 47.6025826CurrentTrain: epoch  7, batch    74 | loss: 29.9536216CurrentTrain: epoch  7, batch    75 | loss: 49.3067038CurrentTrain: epoch  7, batch    76 | loss: 35.2134444CurrentTrain: epoch  7, batch    77 | loss: 36.9600483CurrentTrain: epoch  7, batch    78 | loss: 46.4356055CurrentTrain: epoch  7, batch    79 | loss: 47.6447099CurrentTrain: epoch  7, batch    80 | loss: 47.9302226CurrentTrain: epoch  7, batch    81 | loss: 37.5069515CurrentTrain: epoch  7, batch    82 | loss: 48.4120486CurrentTrain: epoch  7, batch    83 | loss: 49.2777999CurrentTrain: epoch  7, batch    84 | loss: 47.4556307CurrentTrain: epoch  7, batch    85 | loss: 106.9614744CurrentTrain: epoch  7, batch    86 | loss: 37.5969629CurrentTrain: epoch  7, batch    87 | loss: 34.9348787CurrentTrain: epoch  7, batch    88 | loss: 222.4272891CurrentTrain: epoch  7, batch    89 | loss: 68.8447427CurrentTrain: epoch  7, batch    90 | loss: 30.9314930CurrentTrain: epoch  7, batch    91 | loss: 47.1169828CurrentTrain: epoch  7, batch    92 | loss: 37.5857514CurrentTrain: epoch  7, batch    93 | loss: 68.1312209CurrentTrain: epoch  7, batch    94 | loss: 49.4495720CurrentTrain: epoch  7, batch    95 | loss: 48.2759765CurrentTrain: epoch  7, batch    96 | loss: 26.4075794CurrentTrain: epoch  7, batch    97 | loss: 46.0360331CurrentTrain: epoch  7, batch    98 | loss: 45.7296921CurrentTrain: epoch  7, batch    99 | loss: 47.5830691CurrentTrain: epoch  7, batch   100 | loss: 36.7328082CurrentTrain: epoch  7, batch   101 | loss: 106.9396464CurrentTrain: epoch  7, batch   102 | loss: 44.8969064CurrentTrain: epoch  7, batch   103 | loss: 37.4788422CurrentTrain: epoch  7, batch   104 | loss: 49.8985767CurrentTrain: epoch  7, batch   105 | loss: 37.5146167CurrentTrain: epoch  7, batch   106 | loss: 68.1415989CurrentTrain: epoch  7, batch   107 | loss: 50.5877704CurrentTrain: epoch  7, batch   108 | loss: 35.2392846CurrentTrain: epoch  7, batch   109 | loss: 47.7309426CurrentTrain: epoch  7, batch   110 | loss: 46.2706910CurrentTrain: epoch  7, batch   111 | loss: 34.8790002CurrentTrain: epoch  7, batch   112 | loss: 26.5048290CurrentTrain: epoch  7, batch   113 | loss: 37.5597070CurrentTrain: epoch  7, batch   114 | loss: 38.7564181CurrentTrain: epoch  7, batch   115 | loss: 46.0384328CurrentTrain: epoch  7, batch   116 | loss: 68.4122538CurrentTrain: epoch  7, batch   117 | loss: 66.8649621CurrentTrain: epoch  7, batch   118 | loss: 68.9114651CurrentTrain: epoch  7, batch   119 | loss: 68.1168188CurrentTrain: epoch  7, batch   120 | loss: 37.4542233CurrentTrain: epoch  7, batch   121 | loss: 49.1265365CurrentTrain: epoch  7, batch   122 | loss: 34.1274825CurrentTrain: epoch  7, batch   123 | loss: 48.2362569CurrentTrain: epoch  7, batch   124 | loss: 33.5261928CurrentTrain: epoch  7, batch   125 | loss: 47.6310473CurrentTrain: epoch  7, batch   126 | loss: 47.4303176CurrentTrain: epoch  7, batch   127 | loss: 47.4979227CurrentTrain: epoch  7, batch   128 | loss: 40.3816738CurrentTrain: epoch  7, batch   129 | loss: 29.8414357CurrentTrain: epoch  7, batch   130 | loss: 47.7914547CurrentTrain: epoch  7, batch   131 | loss: 66.4520153CurrentTrain: epoch  7, batch   132 | loss: 46.0623805CurrentTrain: epoch  7, batch   133 | loss: 66.1526582CurrentTrain: epoch  7, batch   134 | loss: 69.7204867CurrentTrain: epoch  7, batch   135 | loss: 37.5298719CurrentTrain: epoch  7, batch   136 | loss: 64.2601032CurrentTrain: epoch  7, batch   137 | loss: 47.6146422CurrentTrain: epoch  7, batch   138 | loss: 49.6548400CurrentTrain: epoch  7, batch   139 | loss: 49.2208227CurrentTrain: epoch  7, batch   140 | loss: 37.6946733CurrentTrain: epoch  7, batch   141 | loss: 48.9681641CurrentTrain: epoch  7, batch   142 | loss: 38.7490632CurrentTrain: epoch  7, batch   143 | loss: 54.2136639CurrentTrain: epoch  8, batch     0 | loss: 29.9052741CurrentTrain: epoch  8, batch     1 | loss: 68.1010101CurrentTrain: epoch  8, batch     2 | loss: 35.1807389CurrentTrain: epoch  8, batch     3 | loss: 68.5350422CurrentTrain: epoch  8, batch     4 | loss: 66.3996090CurrentTrain: epoch  8, batch     5 | loss: 106.7697078CurrentTrain: epoch  8, batch     6 | loss: 47.9897608CurrentTrain: epoch  8, batch     7 | loss: 48.9141941CurrentTrain: epoch  8, batch     8 | loss: 37.6604781CurrentTrain: epoch  8, batch     9 | loss: 49.0748493CurrentTrain: epoch  8, batch    10 | loss: 106.8358851CurrentTrain: epoch  8, batch    11 | loss: 34.9047559CurrentTrain: epoch  8, batch    12 | loss: 38.0091926CurrentTrain: epoch  8, batch    13 | loss: 47.5372205CurrentTrain: epoch  8, batch    14 | loss: 103.4741606CurrentTrain: epoch  8, batch    15 | loss: 45.6363030CurrentTrain: epoch  8, batch    16 | loss: 33.2088404CurrentTrain: epoch  8, batch    17 | loss: 66.3571604CurrentTrain: epoch  8, batch    18 | loss: 47.5181689CurrentTrain: epoch  8, batch    19 | loss: 49.2112017CurrentTrain: epoch  8, batch    20 | loss: 45.6487297CurrentTrain: epoch  8, batch    21 | loss: 47.8405971CurrentTrain: epoch  8, batch    22 | loss: 35.5041177CurrentTrain: epoch  8, batch    23 | loss: 68.0682211CurrentTrain: epoch  8, batch    24 | loss: 68.0716494CurrentTrain: epoch  8, batch    25 | loss: 64.8447620CurrentTrain: epoch  8, batch    26 | loss: 46.0551078CurrentTrain: epoch  8, batch    27 | loss: 103.7040266CurrentTrain: epoch  8, batch    28 | loss: 28.4228479CurrentTrain: epoch  8, batch    29 | loss: 103.4765017CurrentTrain: epoch  8, batch    30 | loss: 35.3919423CurrentTrain: epoch  8, batch    31 | loss: 62.5782927CurrentTrain: epoch  8, batch    32 | loss: 46.0409375CurrentTrain: epoch  8, batch    33 | loss: 103.7163663CurrentTrain: epoch  8, batch    34 | loss: 34.4040667CurrentTrain: epoch  8, batch    35 | loss: 47.4109320CurrentTrain: epoch  8, batch    36 | loss: 35.2273333CurrentTrain: epoch  8, batch    37 | loss: 64.5715688CurrentTrain: epoch  8, batch    38 | loss: 48.8797524CurrentTrain: epoch  8, batch    39 | loss: 33.6126979CurrentTrain: epoch  8, batch    40 | loss: 48.9227015CurrentTrain: epoch  8, batch    41 | loss: 36.4514395CurrentTrain: epoch  8, batch    42 | loss: 46.3024148CurrentTrain: epoch  8, batch    43 | loss: 35.6466933CurrentTrain: epoch  8, batch    44 | loss: 37.4182660CurrentTrain: epoch  8, batch    45 | loss: 49.7336493CurrentTrain: epoch  8, batch    46 | loss: 37.6642950CurrentTrain: epoch  8, batch    47 | loss: 35.4695406CurrentTrain: epoch  8, batch    48 | loss: 103.5388446CurrentTrain: epoch  8, batch    49 | loss: 68.0617937CurrentTrain: epoch  8, batch    50 | loss: 68.0734363CurrentTrain: epoch  8, batch    51 | loss: 64.4719045CurrentTrain: epoch  8, batch    52 | loss: 36.0612169CurrentTrain: epoch  8, batch    53 | loss: 36.5075090CurrentTrain: epoch  8, batch    54 | loss: 62.6158741CurrentTrain: epoch  8, batch    55 | loss: 29.0732673CurrentTrain: epoch  8, batch    56 | loss: 105.5669575CurrentTrain: epoch  8, batch    57 | loss: 46.5672577CurrentTrain: epoch  8, batch    58 | loss: 50.2619886CurrentTrain: epoch  8, batch    59 | loss: 49.0626331CurrentTrain: epoch  8, batch    60 | loss: 37.5501534CurrentTrain: epoch  8, batch    61 | loss: 106.9637668CurrentTrain: epoch  8, batch    62 | loss: 49.0246446CurrentTrain: epoch  8, batch    63 | loss: 47.4501065CurrentTrain: epoch  8, batch    64 | loss: 68.0870259CurrentTrain: epoch  8, batch    65 | loss: 69.5938854CurrentTrain: epoch  8, batch    66 | loss: 106.7605247CurrentTrain: epoch  8, batch    67 | loss: 46.5490286CurrentTrain: epoch  8, batch    68 | loss: 68.0917298CurrentTrain: epoch  8, batch    69 | loss: 47.5871237CurrentTrain: epoch  8, batch    70 | loss: 29.0007804CurrentTrain: epoch  8, batch    71 | loss: 43.3022059CurrentTrain: epoch  8, batch    72 | loss: 35.0845699CurrentTrain: epoch  8, batch    73 | loss: 37.5563186CurrentTrain: epoch  8, batch    74 | loss: 47.2383266CurrentTrain: epoch  8, batch    75 | loss: 66.0853741CurrentTrain: epoch  8, batch    76 | loss: 66.1842659CurrentTrain: epoch  8, batch    77 | loss: 28.2754276CurrentTrain: epoch  8, batch    78 | loss: 68.0876376CurrentTrain: epoch  8, batch    79 | loss: 35.1636448CurrentTrain: epoch  8, batch    80 | loss: 106.8443585CurrentTrain: epoch  8, batch    81 | loss: 68.2534213CurrentTrain: epoch  8, batch    82 | loss: 35.3308318CurrentTrain: epoch  8, batch    83 | loss: 48.8975663CurrentTrain: epoch  8, batch    84 | loss: 36.7616040CurrentTrain: epoch  8, batch    85 | loss: 37.0659128CurrentTrain: epoch  8, batch    86 | loss: 48.0075944CurrentTrain: epoch  8, batch    87 | loss: 47.4137032CurrentTrain: epoch  8, batch    88 | loss: 29.3408382CurrentTrain: epoch  8, batch    89 | loss: 29.8433689CurrentTrain: epoch  8, batch    90 | loss: 106.8654253CurrentTrain: epoch  8, batch    91 | loss: 68.0941191CurrentTrain: epoch  8, batch    92 | loss: 47.4243058CurrentTrain: epoch  8, batch    93 | loss: 66.0808815CurrentTrain: epoch  8, batch    94 | loss: 23.3090561CurrentTrain: epoch  8, batch    95 | loss: 47.6030981CurrentTrain: epoch  8, batch    96 | loss: 52.2107304CurrentTrain: epoch  8, batch    97 | loss: 63.0007012CurrentTrain: epoch  8, batch    98 | loss: 46.3790779CurrentTrain: epoch  8, batch    99 | loss: 33.7766216CurrentTrain: epoch  8, batch   100 | loss: 35.8853364CurrentTrain: epoch  8, batch   101 | loss: 48.2671743CurrentTrain: epoch  8, batch   102 | loss: 46.2747345CurrentTrain: epoch  8, batch   103 | loss: 47.4278220CurrentTrain: epoch  8, batch   104 | loss: 66.1599646CurrentTrain: epoch  8, batch   105 | loss: 103.5154668CurrentTrain: epoch  8, batch   106 | loss: 45.0392537CurrentTrain: epoch  8, batch   107 | loss: 68.3567263CurrentTrain: epoch  8, batch   108 | loss: 36.2983054CurrentTrain: epoch  8, batch   109 | loss: 65.4231097CurrentTrain: epoch  8, batch   110 | loss: 46.0825384CurrentTrain: epoch  8, batch   111 | loss: 66.3075527CurrentTrain: epoch  8, batch   112 | loss: 48.9629909CurrentTrain: epoch  8, batch   113 | loss: 47.5123929CurrentTrain: epoch  8, batch   114 | loss: 23.5369843CurrentTrain: epoch  8, batch   115 | loss: 68.1118680CurrentTrain: epoch  8, batch   116 | loss: 101.0472127CurrentTrain: epoch  8, batch   117 | loss: 47.9398597CurrentTrain: epoch  8, batch   118 | loss: 24.9618925CurrentTrain: epoch  8, batch   119 | loss: 36.3814500CurrentTrain: epoch  8, batch   120 | loss: 66.1004926CurrentTrain: epoch  8, batch   121 | loss: 46.8603813CurrentTrain: epoch  8, batch   122 | loss: 49.1071239CurrentTrain: epoch  8, batch   123 | loss: 30.1253614CurrentTrain: epoch  8, batch   124 | loss: 66.2598438CurrentTrain: epoch  8, batch   125 | loss: 47.1133801CurrentTrain: epoch  8, batch   126 | loss: 106.8013620CurrentTrain: epoch  8, batch   127 | loss: 47.4599186CurrentTrain: epoch  8, batch   128 | loss: 66.0770027CurrentTrain: epoch  8, batch   129 | loss: 66.1340554CurrentTrain: epoch  8, batch   130 | loss: 66.1163360CurrentTrain: epoch  8, batch   131 | loss: 47.8773262CurrentTrain: epoch  8, batch   132 | loss: 20.7751621CurrentTrain: epoch  8, batch   133 | loss: 47.7855646CurrentTrain: epoch  8, batch   134 | loss: 45.9851176CurrentTrain: epoch  8, batch   135 | loss: 66.0890462CurrentTrain: epoch  8, batch   136 | loss: 27.6844221CurrentTrain: epoch  8, batch   137 | loss: 222.1686321CurrentTrain: epoch  8, batch   138 | loss: 66.0619665CurrentTrain: epoch  8, batch   139 | loss: 29.2186275CurrentTrain: epoch  8, batch   140 | loss: 48.9412477CurrentTrain: epoch  8, batch   141 | loss: 68.4247798CurrentTrain: epoch  8, batch   142 | loss: 44.8276541CurrentTrain: epoch  8, batch   143 | loss: 80.8368071CurrentTrain: epoch  9, batch     0 | loss: 66.1850610CurrentTrain: epoch  9, batch     1 | loss: 36.2679169CurrentTrain: epoch  9, batch     2 | loss: 66.1997901CurrentTrain: epoch  9, batch     3 | loss: 48.3490967CurrentTrain: epoch  9, batch     4 | loss: 46.6203089CurrentTrain: epoch  9, batch     5 | loss: 30.3281454CurrentTrain: epoch  9, batch     6 | loss: 36.2965749CurrentTrain: epoch  9, batch     7 | loss: 48.9770201CurrentTrain: epoch  9, batch     8 | loss: 27.3548372CurrentTrain: epoch  9, batch     9 | loss: 106.7927344CurrentTrain: epoch  9, batch    10 | loss: 36.4828704CurrentTrain: epoch  9, batch    11 | loss: 35.1555956CurrentTrain: epoch  9, batch    12 | loss: 68.9247725CurrentTrain: epoch  9, batch    13 | loss: 36.5501111CurrentTrain: epoch  9, batch    14 | loss: 35.2917428CurrentTrain: epoch  9, batch    15 | loss: 68.4693900CurrentTrain: epoch  9, batch    16 | loss: 36.4849549CurrentTrain: epoch  9, batch    17 | loss: 68.0785672CurrentTrain: epoch  9, batch    18 | loss: 66.2907020CurrentTrain: epoch  9, batch    19 | loss: 103.4963647CurrentTrain: epoch  9, batch    20 | loss: 47.4039379CurrentTrain: epoch  9, batch    21 | loss: 70.7172976CurrentTrain: epoch  9, batch    22 | loss: 35.9394741CurrentTrain: epoch  9, batch    23 | loss: 35.1597410CurrentTrain: epoch  9, batch    24 | loss: 106.7838399CurrentTrain: epoch  9, batch    25 | loss: 49.4252916CurrentTrain: epoch  9, batch    26 | loss: 66.2784943CurrentTrain: epoch  9, batch    27 | loss: 64.5910590CurrentTrain: epoch  9, batch    28 | loss: 36.4779691CurrentTrain: epoch  9, batch    29 | loss: 68.0608929CurrentTrain: epoch  9, batch    30 | loss: 45.1108767CurrentTrain: epoch  9, batch    31 | loss: 29.6173749CurrentTrain: epoch  9, batch    32 | loss: 103.5088518CurrentTrain: epoch  9, batch    33 | loss: 48.9145401CurrentTrain: epoch  9, batch    34 | loss: 65.5570363CurrentTrain: epoch  9, batch    35 | loss: 35.9737951CurrentTrain: epoch  9, batch    36 | loss: 222.1066610CurrentTrain: epoch  9, batch    37 | loss: 36.6808924CurrentTrain: epoch  9, batch    38 | loss: 66.0878843CurrentTrain: epoch  9, batch    39 | loss: 28.2185313CurrentTrain: epoch  9, batch    40 | loss: 29.1536402CurrentTrain: epoch  9, batch    41 | loss: 64.4633080CurrentTrain: epoch  9, batch    42 | loss: 31.2658727CurrentTrain: epoch  9, batch    43 | loss: 46.2994570CurrentTrain: epoch  9, batch    44 | loss: 37.4046055CurrentTrain: epoch  9, batch    45 | loss: 28.8434588CurrentTrain: epoch  9, batch    46 | loss: 68.1652424CurrentTrain: epoch  9, batch    47 | loss: 35.3586769CurrentTrain: epoch  9, batch    48 | loss: 68.0852931CurrentTrain: epoch  9, batch    49 | loss: 38.2673088CurrentTrain: epoch  9, batch    50 | loss: 48.9638776CurrentTrain: epoch  9, batch    51 | loss: 49.1238053CurrentTrain: epoch  9, batch    52 | loss: 36.9308711CurrentTrain: epoch  9, batch    53 | loss: 103.4922130CurrentTrain: epoch  9, batch    54 | loss: 34.2135758CurrentTrain: epoch  9, batch    55 | loss: 106.7971006CurrentTrain: epoch  9, batch    56 | loss: 37.7601851CurrentTrain: epoch  9, batch    57 | loss: 46.0052611CurrentTrain: epoch  9, batch    58 | loss: 106.9346486CurrentTrain: epoch  9, batch    59 | loss: 29.0097342CurrentTrain: epoch  9, batch    60 | loss: 49.0583420CurrentTrain: epoch  9, batch    61 | loss: 33.4329464CurrentTrain: epoch  9, batch    62 | loss: 48.9600887CurrentTrain: epoch  9, batch    63 | loss: 35.1414243CurrentTrain: epoch  9, batch    64 | loss: 35.2372103CurrentTrain: epoch  9, batch    65 | loss: 36.5974863CurrentTrain: epoch  9, batch    66 | loss: 66.0896907CurrentTrain: epoch  9, batch    67 | loss: 68.0515710CurrentTrain: epoch  9, batch    68 | loss: 37.7139523CurrentTrain: epoch  9, batch    69 | loss: 66.1194621CurrentTrain: epoch  9, batch    70 | loss: 49.0224581CurrentTrain: epoch  9, batch    71 | loss: 66.1015276CurrentTrain: epoch  9, batch    72 | loss: 35.5216889CurrentTrain: epoch  9, batch    73 | loss: 106.9122615CurrentTrain: epoch  9, batch    74 | loss: 30.9525004CurrentTrain: epoch  9, batch    75 | loss: 47.5692462CurrentTrain: epoch  9, batch    76 | loss: 61.1421219CurrentTrain: epoch  9, batch    77 | loss: 68.0586958CurrentTrain: epoch  9, batch    78 | loss: 48.9588812CurrentTrain: epoch  9, batch    79 | loss: 36.8397855CurrentTrain: epoch  9, batch    80 | loss: 49.1692485CurrentTrain: epoch  9, batch    81 | loss: 28.1635214CurrentTrain: epoch  9, batch    82 | loss: 49.8258170CurrentTrain: epoch  9, batch    83 | loss: 47.5131789CurrentTrain: epoch  9, batch    84 | loss: 48.9819556CurrentTrain: epoch  9, batch    85 | loss: 35.4083202CurrentTrain: epoch  9, batch    86 | loss: 34.4777216CurrentTrain: epoch  9, batch    87 | loss: 44.8556696CurrentTrain: epoch  9, batch    88 | loss: 66.2020090CurrentTrain: epoch  9, batch    89 | loss: 103.4677899CurrentTrain: epoch  9, batch    90 | loss: 34.1762810CurrentTrain: epoch  9, batch    91 | loss: 35.3190753CurrentTrain: epoch  9, batch    92 | loss: 38.4095471CurrentTrain: epoch  9, batch    93 | loss: 49.5852851CurrentTrain: epoch  9, batch    94 | loss: 44.8704056CurrentTrain: epoch  9, batch    95 | loss: 28.2676848CurrentTrain: epoch  9, batch    96 | loss: 36.5022516CurrentTrain: epoch  9, batch    97 | loss: 35.1552514CurrentTrain: epoch  9, batch    98 | loss: 71.7542612CurrentTrain: epoch  9, batch    99 | loss: 47.4654348CurrentTrain: epoch  9, batch   100 | loss: 47.5124860CurrentTrain: epoch  9, batch   101 | loss: 47.4277434CurrentTrain: epoch  9, batch   102 | loss: 68.0753021CurrentTrain: epoch  9, batch   103 | loss: 68.4988090CurrentTrain: epoch  9, batch   104 | loss: 68.0780048CurrentTrain: epoch  9, batch   105 | loss: 36.4398714CurrentTrain: epoch  9, batch   106 | loss: 106.9725697CurrentTrain: epoch  9, batch   107 | loss: 46.3597613CurrentTrain: epoch  9, batch   108 | loss: 28.8551575CurrentTrain: epoch  9, batch   109 | loss: 66.0647998CurrentTrain: epoch  9, batch   110 | loss: 47.4993661CurrentTrain: epoch  9, batch   111 | loss: 106.7069336CurrentTrain: epoch  9, batch   112 | loss: 69.2818024CurrentTrain: epoch  9, batch   113 | loss: 103.4835615CurrentTrain: epoch  9, batch   114 | loss: 66.2310283CurrentTrain: epoch  9, batch   115 | loss: 47.3934822CurrentTrain: epoch  9, batch   116 | loss: 36.2998660CurrentTrain: epoch  9, batch   117 | loss: 65.2040731CurrentTrain: epoch  9, batch   118 | loss: 47.9524250CurrentTrain: epoch  9, batch   119 | loss: 47.5011826CurrentTrain: epoch  9, batch   120 | loss: 35.7393600CurrentTrain: epoch  9, batch   121 | loss: 33.3971199CurrentTrain: epoch  9, batch   122 | loss: 30.5101066CurrentTrain: epoch  9, batch   123 | loss: 68.3396600CurrentTrain: epoch  9, batch   124 | loss: 106.7938108CurrentTrain: epoch  9, batch   125 | loss: 68.0975867CurrentTrain: epoch  9, batch   126 | loss: 68.0642895CurrentTrain: epoch  9, batch   127 | loss: 68.0875483CurrentTrain: epoch  9, batch   128 | loss: 36.2582388CurrentTrain: epoch  9, batch   129 | loss: 36.7467949CurrentTrain: epoch  9, batch   130 | loss: 45.0608594CurrentTrain: epoch  9, batch   131 | loss: 64.6074341CurrentTrain: epoch  9, batch   132 | loss: 106.7196033CurrentTrain: epoch  9, batch   133 | loss: 47.4019624CurrentTrain: epoch  9, batch   134 | loss: 46.2418588CurrentTrain: epoch  9, batch   135 | loss: 35.6362414CurrentTrain: epoch  9, batch   136 | loss: 37.5670122CurrentTrain: epoch  9, batch   137 | loss: 48.8736922CurrentTrain: epoch  9, batch   138 | loss: 36.4042974CurrentTrain: epoch  9, batch   139 | loss: 27.2246045CurrentTrain: epoch  9, batch   140 | loss: 38.2432555CurrentTrain: epoch  9, batch   141 | loss: 46.0170573CurrentTrain: epoch  9, batch   142 | loss: 48.9265965CurrentTrain: epoch  9, batch   143 | loss: 25.3439284

F1 score per class: {32: 0.5224489795918368, 6: 0.6666666666666666, 19: 0.18604651162790697, 24: 0.6728110599078341, 26: 0.8783068783068783, 29: 0.7553648068669528}
Micro-average F1 score: 0.6717557251908397
Weighted-average F1 score: 0.6642833943368848
F1 score per class: {32: 0.463855421686747, 6: 0.6446886446886447, 19: 0.1651376146788991, 24: 0.6604651162790698, 26: 0.9306930693069307, 29: 0.7296137339055794}
Micro-average F1 score: 0.6217008797653959
Weighted-average F1 score: 0.5934940004126272
F1 score per class: {32: 0.47058823529411764, 6: 0.6446886446886447, 19: 0.18181818181818182, 24: 0.6543778801843319, 26: 0.9306930693069307, 29: 0.7203389830508474}
Micro-average F1 score: 0.6266666666666667
Weighted-average F1 score: 0.601753114753348

F1 score per class: {32: 0.5224489795918368, 6: 0.6666666666666666, 19: 0.18604651162790697, 24: 0.6728110599078341, 26: 0.8783068783068783, 29: 0.7553648068669528}
Micro-average F1 score: 0.6717557251908397
Weighted-average F1 score: 0.6642833943368848
F1 score per class: {32: 0.463855421686747, 6: 0.6446886446886447, 19: 0.1651376146788991, 24: 0.6604651162790698, 26: 0.9306930693069307, 29: 0.7296137339055794}
Micro-average F1 score: 0.6217008797653959
Weighted-average F1 score: 0.5934940004126272
F1 score per class: {32: 0.47058823529411764, 6: 0.6446886446886447, 19: 0.18181818181818182, 24: 0.6543778801843319, 26: 0.9306930693069307, 29: 0.7203389830508474}
Micro-average F1 score: 0.6266666666666667
Weighted-average F1 score: 0.601753114753348
cur_acc:  ['0.6718']
his_acc:  ['0.6718']
cur_acc des:  ['0.6217']
his_acc des:  ['0.6217']
cur_acc rrf:  ['0.6267']
his_acc rrf:  ['0.6267']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 63.8629545CurrentTrain: epoch  0, batch     1 | loss: 38.9199864CurrentTrain: epoch  0, batch     2 | loss: 79.4902874CurrentTrain: epoch  0, batch     3 | loss: 60.5913521CurrentTrain: epoch  0, batch     4 | loss: 59.9603604CurrentTrain: epoch  0, batch     5 | loss: 59.0835564CurrentTrain: epoch  0, batch     6 | loss: 17.7044706CurrentTrain: epoch  1, batch     0 | loss: 56.3494861CurrentTrain: epoch  1, batch     1 | loss: 56.1299369CurrentTrain: epoch  1, batch     2 | loss: 75.9114078CurrentTrain: epoch  1, batch     3 | loss: 113.1828908CurrentTrain: epoch  1, batch     4 | loss: 36.1649640CurrentTrain: epoch  1, batch     5 | loss: 53.5056693CurrentTrain: epoch  1, batch     6 | loss: 21.6070015CurrentTrain: epoch  2, batch     0 | loss: 35.2036390CurrentTrain: epoch  2, batch     1 | loss: 108.0667685CurrentTrain: epoch  2, batch     2 | loss: 74.9432357CurrentTrain: epoch  2, batch     3 | loss: 55.3150353CurrentTrain: epoch  2, batch     4 | loss: 56.1964632CurrentTrain: epoch  2, batch     5 | loss: 104.0735233CurrentTrain: epoch  2, batch     6 | loss: 19.0893420CurrentTrain: epoch  3, batch     0 | loss: 112.7540375CurrentTrain: epoch  3, batch     1 | loss: 52.8333294CurrentTrain: epoch  3, batch     2 | loss: 52.6207785CurrentTrain: epoch  3, batch     3 | loss: 52.9540023CurrentTrain: epoch  3, batch     4 | loss: 110.8896608CurrentTrain: epoch  3, batch     5 | loss: 41.1435177CurrentTrain: epoch  3, batch     6 | loss: 30.1679191CurrentTrain: epoch  4, batch     0 | loss: 71.2147501CurrentTrain: epoch  4, batch     1 | loss: 105.5161007CurrentTrain: epoch  4, batch     2 | loss: 35.4380518CurrentTrain: epoch  4, batch     3 | loss: 55.2367270CurrentTrain: epoch  4, batch     4 | loss: 52.6681973CurrentTrain: epoch  4, batch     5 | loss: 39.8781012CurrentTrain: epoch  4, batch     6 | loss: 17.3419330CurrentTrain: epoch  5, batch     0 | loss: 50.2110497CurrentTrain: epoch  5, batch     1 | loss: 41.4955134CurrentTrain: epoch  5, batch     2 | loss: 50.4734994CurrentTrain: epoch  5, batch     3 | loss: 67.2669217CurrentTrain: epoch  5, batch     4 | loss: 71.1564527CurrentTrain: epoch  5, batch     5 | loss: 53.9836387CurrentTrain: epoch  5, batch     6 | loss: 28.0611764CurrentTrain: epoch  6, batch     0 | loss: 40.6061834CurrentTrain: epoch  6, batch     1 | loss: 107.8464290CurrentTrain: epoch  6, batch     2 | loss: 50.6744139CurrentTrain: epoch  6, batch     3 | loss: 39.5309740CurrentTrain: epoch  6, batch     4 | loss: 39.1216644CurrentTrain: epoch  6, batch     5 | loss: 67.2125305CurrentTrain: epoch  6, batch     6 | loss: 10.5150371CurrentTrain: epoch  7, batch     0 | loss: 51.2767527CurrentTrain: epoch  7, batch     1 | loss: 38.6883571CurrentTrain: epoch  7, batch     2 | loss: 50.0015128CurrentTrain: epoch  7, batch     3 | loss: 36.6959020CurrentTrain: epoch  7, batch     4 | loss: 105.0015440CurrentTrain: epoch  7, batch     5 | loss: 107.4269607CurrentTrain: epoch  7, batch     6 | loss: 15.0196469CurrentTrain: epoch  8, batch     0 | loss: 37.7279250CurrentTrain: epoch  8, batch     1 | loss: 49.0000860CurrentTrain: epoch  8, batch     2 | loss: 106.9783094CurrentTrain: epoch  8, batch     3 | loss: 37.6686798CurrentTrain: epoch  8, batch     4 | loss: 49.0436596CurrentTrain: epoch  8, batch     5 | loss: 65.6049443CurrentTrain: epoch  8, batch     6 | loss: 26.6129347CurrentTrain: epoch  9, batch     0 | loss: 66.6580431CurrentTrain: epoch  9, batch     1 | loss: 46.6897511CurrentTrain: epoch  9, batch     2 | loss: 47.2235344CurrentTrain: epoch  9, batch     3 | loss: 50.1265115CurrentTrain: epoch  9, batch     4 | loss: 106.9272455CurrentTrain: epoch  9, batch     5 | loss: 48.0651613CurrentTrain: epoch  9, batch     6 | loss: 14.7832547
MemoryTrain:  epoch  0, batch     0 | loss: 0.7383159MemoryTrain:  epoch  1, batch     0 | loss: 0.5540194MemoryTrain:  epoch  2, batch     0 | loss: 0.4528017MemoryTrain:  epoch  3, batch     0 | loss: 0.4104879MemoryTrain:  epoch  4, batch     0 | loss: 0.2666641MemoryTrain:  epoch  5, batch     0 | loss: 0.2178786MemoryTrain:  epoch  6, batch     0 | loss: 0.1700123MemoryTrain:  epoch  7, batch     0 | loss: 0.1252082MemoryTrain:  epoch  8, batch     0 | loss: 0.1087264MemoryTrain:  epoch  9, batch     0 | loss: 0.0861017

F1 score per class: {32: 0.4166666666666667, 2: 0.0, 6: 0.17142857142857143, 39: 0.18487394957983194, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.13333333333333333, 26: 0.0, 28: 0.0, 29: 0.07407407407407407}
Micro-average F1 score: 0.15384615384615385
Weighted-average F1 score: 0.11429784174206081
F1 score per class: {32: 0.32, 2: 0.0, 6: 0.46060606060606063, 39: 0.4444444444444444, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.08333333333333333, 26: 0.0, 28: 0.0, 29: 0.0625}
Micro-average F1 score: 0.3049645390070922
Weighted-average F1 score: 0.2394219001610306
F1 score per class: {32: 0.32, 2: 0.0, 6: 0.45121951219512196, 39: 0.4539877300613497, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.07894736842105263, 26: 0.0, 28: 0.0, 29: 0.0625}
Micro-average F1 score: 0.3076923076923077
Weighted-average F1 score: 0.242668620509782

F1 score per class: {32: 0.37037037037037035, 2: 0.4591439688715953, 6: 0.1592920353982301, 39: 0.15384615384615385, 11: 0.6666666666666666, 12: 0.17142857142857143, 19: 0.7395833333333334, 24: 0.0975609756097561, 26: 0.8934010152284264, 28: 0.7096774193548387, 29: 0.06896551724137931}
Micro-average F1 score: 0.5479632063074902
Weighted-average F1 score: 0.572720245209617
F1 score per class: {32: 0.20253164556962025, 2: 0.4452296819787986, 6: 0.36538461538461536, 39: 0.291497975708502, 11: 0.6217228464419475, 12: 0.1702127659574468, 19: 0.7142857142857143, 24: 0.04054054054054054, 26: 0.8934010152284264, 28: 0.6825396825396826, 29: 0.05714285714285714}
Micro-average F1 score: 0.48255234297108673
Weighted-average F1 score: 0.44520687415773186
F1 score per class: {32: 0.20253164556962025, 2: 0.46545454545454545, 6: 0.3592233009708738, 39: 0.2971887550200803, 11: 0.6283524904214559, 12: 0.2222222222222222, 19: 0.7142857142857143, 24: 0.039473684210526314, 26: 0.8934010152284264, 28: 0.6852589641434262, 29: 0.05714285714285714}
Micro-average F1 score: 0.49185336048879835
Weighted-average F1 score: 0.4556280576816523
cur_acc:  ['0.6718', '0.1538']
his_acc:  ['0.6718', '0.5480']
cur_acc des:  ['0.6217', '0.3050']
his_acc des:  ['0.6217', '0.4826']
cur_acc rrf:  ['0.6267', '0.3077']
his_acc rrf:  ['0.6267', '0.4919']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 47.1871126CurrentTrain: epoch  0, batch     1 | loss: 61.0127527CurrentTrain: epoch  0, batch     2 | loss: 55.2065749CurrentTrain: epoch  0, batch     3 | loss: 49.4451043CurrentTrain: epoch  0, batch     4 | loss: 39.5869961CurrentTrain: epoch  1, batch     0 | loss: 75.5967949CurrentTrain: epoch  1, batch     1 | loss: 45.2095109CurrentTrain: epoch  1, batch     2 | loss: 37.7106239CurrentTrain: epoch  1, batch     3 | loss: 50.9100264CurrentTrain: epoch  1, batch     4 | loss: 35.9204883CurrentTrain: epoch  2, batch     0 | loss: 74.4496467CurrentTrain: epoch  2, batch     1 | loss: 50.7299859CurrentTrain: epoch  2, batch     2 | loss: 71.2923989CurrentTrain: epoch  2, batch     3 | loss: 32.5322350CurrentTrain: epoch  2, batch     4 | loss: 30.5016333CurrentTrain: epoch  3, batch     0 | loss: 71.3323002CurrentTrain: epoch  3, batch     1 | loss: 40.1054172CurrentTrain: epoch  3, batch     2 | loss: 40.6748058CurrentTrain: epoch  3, batch     3 | loss: 30.8885932CurrentTrain: epoch  3, batch     4 | loss: 31.8289187CurrentTrain: epoch  4, batch     0 | loss: 47.1643023CurrentTrain: epoch  4, batch     1 | loss: 34.4668268CurrentTrain: epoch  4, batch     2 | loss: 31.7367122CurrentTrain: epoch  4, batch     3 | loss: 67.2054247CurrentTrain: epoch  4, batch     4 | loss: 24.0416388CurrentTrain: epoch  5, batch     0 | loss: 49.7816811CurrentTrain: epoch  5, batch     1 | loss: 30.6914940CurrentTrain: epoch  5, batch     2 | loss: 35.9119827CurrentTrain: epoch  5, batch     3 | loss: 38.1087618CurrentTrain: epoch  5, batch     4 | loss: 32.6014122CurrentTrain: epoch  6, batch     0 | loss: 49.3364001CurrentTrain: epoch  6, batch     1 | loss: 34.9743138CurrentTrain: epoch  6, batch     2 | loss: 46.8932560CurrentTrain: epoch  6, batch     3 | loss: 38.9508900CurrentTrain: epoch  6, batch     4 | loss: 31.0998509CurrentTrain: epoch  7, batch     0 | loss: 67.7662413CurrentTrain: epoch  7, batch     1 | loss: 68.2452425CurrentTrain: epoch  7, batch     2 | loss: 46.3237571CurrentTrain: epoch  7, batch     3 | loss: 26.5858563CurrentTrain: epoch  7, batch     4 | loss: 63.3218999CurrentTrain: epoch  8, batch     0 | loss: 66.3914183CurrentTrain: epoch  8, batch     1 | loss: 30.0902466CurrentTrain: epoch  8, batch     2 | loss: 36.6605846CurrentTrain: epoch  8, batch     3 | loss: 38.7587942CurrentTrain: epoch  8, batch     4 | loss: 21.0242821CurrentTrain: epoch  9, batch     0 | loss: 36.0464406CurrentTrain: epoch  9, batch     1 | loss: 35.6021437CurrentTrain: epoch  9, batch     2 | loss: 46.5915337CurrentTrain: epoch  9, batch     3 | loss: 47.4580190CurrentTrain: epoch  9, batch     4 | loss: 30.7920721
MemoryTrain:  epoch  0, batch     0 | loss: 0.9592125MemoryTrain:  epoch  1, batch     0 | loss: 0.7586231MemoryTrain:  epoch  2, batch     0 | loss: 0.3430982MemoryTrain:  epoch  3, batch     0 | loss: 0.3470296MemoryTrain:  epoch  4, batch     0 | loss: 0.2222471MemoryTrain:  epoch  5, batch     0 | loss: 0.2002170MemoryTrain:  epoch  6, batch     0 | loss: 0.2761910MemoryTrain:  epoch  7, batch     0 | loss: 0.1878069MemoryTrain:  epoch  8, batch     0 | loss: 0.2066974MemoryTrain:  epoch  9, batch     0 | loss: 0.1682166

F1 score per class: {32: 0.0, 6: 0.0, 7: 0.92, 40: 0.0, 9: 0.0, 11: 0.0, 19: 0.5217391304347826, 26: 0.0, 27: 0.0, 31: 0.2857142857142857}
Micro-average F1 score: 0.3345724907063197
Weighted-average F1 score: 0.2572181927469445
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.5714285714285714, 7: 0.819672131147541, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.43478260869565216, 26: 0.0, 27: 0.13333333333333333, 28: 0.0, 31: 0.30344827586206896}
Micro-average F1 score: 0.3235294117647059
Weighted-average F1 score: 0.26229229378181523
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.5714285714285714, 7: 0.819672131147541, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 19: 0.48, 26: 0.0, 27: 0.2, 28: 0.0, 31: 0.3076923076923077}
Micro-average F1 score: 0.34146341463414637
Weighted-average F1 score: 0.28162759256983255

F1 score per class: {32: 0.2777777777777778, 2: 0.3225806451612903, 6: 0.0, 7: 0.92, 40: 0.24, 39: 0.15037593984962405, 11: 0.5217391304347826, 12: 0.09090909090909091, 9: 0.6979166666666666, 19: 0.19047619047619047, 24: 0.10810810810810811, 26: 0.8877551020408163, 27: 0.0, 28: 0.7909090909090909, 29: 0.08695652173913043, 31: 0.17582417582417584}
Micro-average F1 score: 0.46229508196721314
Weighted-average F1 score: 0.4526879348886232
F1 score per class: {32: 0.11570247933884298, 2: 0.2838709677419355, 6: 0.034482758620689655, 7: 0.819672131147541, 40: 0.44144144144144143, 39: 0.29365079365079366, 11: 0.5050505050505051, 12: 0.17777777777777778, 9: 0.68, 19: 0.16129032258064516, 24: 0.08264462809917356, 26: 0.8877551020408163, 27: 0.025, 28: 0.7327586206896551, 29: 0.047619047619047616, 31: 0.15602836879432624}
Micro-average F1 score: 0.39855072463768115
Weighted-average F1 score: 0.3509092564334175
F1 score per class: {32: 0.12173913043478261, 2: 0.2875816993464052, 6: 0.037383177570093455, 7: 0.819672131147541, 40: 0.4434389140271493, 39: 0.288135593220339, 11: 0.5121107266435986, 12: 0.13793103448275862, 9: 0.6767676767676768, 19: 0.15789473684210525, 24: 0.07462686567164178, 26: 0.8877551020408163, 27: 0.029411764705882353, 28: 0.723404255319149, 29: 0.05714285714285714, 31: 0.15602836879432624}
Micro-average F1 score: 0.4016427104722793
Weighted-average F1 score: 0.354928452669051
cur_acc:  ['0.6718', '0.1538', '0.3346']
his_acc:  ['0.6718', '0.5480', '0.4623']
cur_acc des:  ['0.6217', '0.3050', '0.3235']
his_acc des:  ['0.6217', '0.4826', '0.3986']
cur_acc rrf:  ['0.6267', '0.3077', '0.3415']
his_acc rrf:  ['0.6267', '0.4919', '0.4016']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 91.0029451CurrentTrain: epoch  0, batch     1 | loss: 43.2775565CurrentTrain: epoch  0, batch     2 | loss: 54.8918044CurrentTrain: epoch  0, batch     3 | loss: 50.5504239CurrentTrain: epoch  0, batch     4 | loss: 60.4674568CurrentTrain: epoch  0, batch     5 | loss: 44.4452009CurrentTrain: epoch  1, batch     0 | loss: 45.6761739CurrentTrain: epoch  1, batch     1 | loss: 44.8345701CurrentTrain: epoch  1, batch     2 | loss: 115.8594310CurrentTrain: epoch  1, batch     3 | loss: 42.6220053CurrentTrain: epoch  1, batch     4 | loss: 53.4671156CurrentTrain: epoch  1, batch     5 | loss: 29.1641392CurrentTrain: epoch  2, batch     0 | loss: 52.4073153CurrentTrain: epoch  2, batch     1 | loss: 52.3908270CurrentTrain: epoch  2, batch     2 | loss: 50.7559412CurrentTrain: epoch  2, batch     3 | loss: 69.9600321CurrentTrain: epoch  2, batch     4 | loss: 43.9358865CurrentTrain: epoch  2, batch     5 | loss: 34.4961852CurrentTrain: epoch  3, batch     0 | loss: 29.4613980CurrentTrain: epoch  3, batch     1 | loss: 51.2849369CurrentTrain: epoch  3, batch     2 | loss: 70.2023274CurrentTrain: epoch  3, batch     3 | loss: 42.0708890CurrentTrain: epoch  3, batch     4 | loss: 111.7104406CurrentTrain: epoch  3, batch     5 | loss: 37.3505593CurrentTrain: epoch  4, batch     0 | loss: 110.6679847CurrentTrain: epoch  4, batch     1 | loss: 38.6992907CurrentTrain: epoch  4, batch     2 | loss: 38.2250881CurrentTrain: epoch  4, batch     3 | loss: 72.2814929CurrentTrain: epoch  4, batch     4 | loss: 38.5766950CurrentTrain: epoch  4, batch     5 | loss: 34.2715362CurrentTrain: epoch  5, batch     0 | loss: 50.8891669CurrentTrain: epoch  5, batch     1 | loss: 47.9787975CurrentTrain: epoch  5, batch     2 | loss: 69.8946514CurrentTrain: epoch  5, batch     3 | loss: 30.4246433CurrentTrain: epoch  5, batch     4 | loss: 53.9454134CurrentTrain: epoch  5, batch     5 | loss: 23.8103118CurrentTrain: epoch  6, batch     0 | loss: 29.8376702CurrentTrain: epoch  6, batch     1 | loss: 38.4615879CurrentTrain: epoch  6, batch     2 | loss: 47.0699506CurrentTrain: epoch  6, batch     3 | loss: 51.5771226CurrentTrain: epoch  6, batch     4 | loss: 222.3878579CurrentTrain: epoch  6, batch     5 | loss: 54.6192789CurrentTrain: epoch  7, batch     0 | loss: 29.3436624CurrentTrain: epoch  7, batch     1 | loss: 69.4191952CurrentTrain: epoch  7, batch     2 | loss: 49.1895511CurrentTrain: epoch  7, batch     3 | loss: 48.2318781CurrentTrain: epoch  7, batch     4 | loss: 38.8389284CurrentTrain: epoch  7, batch     5 | loss: 34.4959824CurrentTrain: epoch  8, batch     0 | loss: 36.4234991CurrentTrain: epoch  8, batch     1 | loss: 37.7971513CurrentTrain: epoch  8, batch     2 | loss: 49.8421593CurrentTrain: epoch  8, batch     3 | loss: 36.0904268CurrentTrain: epoch  8, batch     4 | loss: 67.3756881CurrentTrain: epoch  8, batch     5 | loss: 55.1420222CurrentTrain: epoch  9, batch     0 | loss: 66.9190238CurrentTrain: epoch  9, batch     1 | loss: 35.7711545CurrentTrain: epoch  9, batch     2 | loss: 37.8364702CurrentTrain: epoch  9, batch     3 | loss: 48.8897025CurrentTrain: epoch  9, batch     4 | loss: 37.1561267CurrentTrain: epoch  9, batch     5 | loss: 24.3979852
MemoryTrain:  epoch  0, batch     0 | loss: 0.5699074MemoryTrain:  epoch  1, batch     0 | loss: 0.5310434MemoryTrain:  epoch  2, batch     0 | loss: 0.4086245MemoryTrain:  epoch  3, batch     0 | loss: 0.3202550MemoryTrain:  epoch  4, batch     0 | loss: 0.2948777MemoryTrain:  epoch  5, batch     0 | loss: 0.2174360MemoryTrain:  epoch  6, batch     0 | loss: 0.1803316MemoryTrain:  epoch  7, batch     0 | loss: 0.1498364MemoryTrain:  epoch  8, batch     0 | loss: 0.1295658MemoryTrain:  epoch  9, batch     0 | loss: 0.0939528

F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.9, 19: 0.0, 24: 0.0, 25: 0.44776119402985076, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.4854368932038835, 37: 0.425531914893617, 38: 0.37209302325581395, 40: 0.0}
Micro-average F1 score: 0.375609756097561
Weighted-average F1 score: 0.29190725747497553
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.5384615384615384, 19: 0.0, 24: 0.0, 25: 0.8, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5975609756097561, 37: 0.336283185840708, 38: 0.3516483516483517, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3460490463215259
Weighted-average F1 score: 0.27489048754726214
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.6363636363636364, 19: 0.0, 24: 0.0, 25: 0.6987951807228916, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5903614457831325, 37: 0.35772357723577236, 38: 0.3146067415730337, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.34277620396600567
Weighted-average F1 score: 0.27508549921909353

F1 score per class: {2: 0.37037037037037035, 6: 0.2602739726027397, 7: 0.0, 9: 0.8888888888888888, 11: 0.2014388489208633, 12: 0.14965986394557823, 15: 0.6428571428571429, 19: 0.4392156862745098, 24: 0.07142857142857142, 25: 0.44776119402985076, 26: 0.6903553299492385, 27: 0.1728395061728395, 28: 0.12307692307692308, 29: 0.8756218905472637, 31: 0.0, 32: 0.7522935779816514, 35: 0.3424657534246575, 37: 0.2702702702702703, 38: 0.2222222222222222, 39: 0.0, 40: 0.24761904761904763}
Micro-average F1 score: 0.41596452328159644
Weighted-average F1 score: 0.404085802868271
F1 score per class: {2: 0.17142857142857143, 6: 0.34615384615384615, 7: 0.0, 9: 0.746268656716418, 11: 0.26666666666666666, 12: 0.25748502994011974, 15: 0.25, 19: 0.44966442953020136, 24: 0.10526315789473684, 25: 0.8, 26: 0.6507177033492823, 27: 0.12, 28: 0.06578947368421052, 29: 0.864321608040201, 31: 0.041666666666666664, 32: 0.6798418972332015, 35: 0.2996941896024465, 37: 0.12582781456953643, 38: 0.14035087719298245, 39: 0.0625, 40: 0.16740088105726872}
Micro-average F1 score: 0.33619763694951665
Weighted-average F1 score: 0.2977479403851262
F1 score per class: {2: 0.2, 6: 0.3283582089552239, 7: 0.0, 9: 0.819672131147541, 11: 0.2586750788643533, 12: 0.24918032786885247, 15: 0.3181818181818182, 19: 0.45791245791245794, 24: 0.08333333333333333, 25: 0.6987951807228916, 26: 0.6601941747572816, 27: 0.10344827586206896, 28: 0.08064516129032258, 29: 0.864321608040201, 31: 0.05128205128205128, 32: 0.6798418972332015, 35: 0.29518072289156627, 37: 0.1301775147928994, 38: 0.1222707423580786, 39: 0.0625, 40: 0.16964285714285715}
Micro-average F1 score: 0.33545530030445614
Weighted-average F1 score: 0.2968892490386929
cur_acc:  ['0.6718', '0.1538', '0.3346', '0.3756']
his_acc:  ['0.6718', '0.5480', '0.4623', '0.4160']
cur_acc des:  ['0.6217', '0.3050', '0.3235', '0.3460']
his_acc des:  ['0.6217', '0.4826', '0.3986', '0.3362']
cur_acc rrf:  ['0.6267', '0.3077', '0.3415', '0.3428']
his_acc rrf:  ['0.6267', '0.4919', '0.4016', '0.3355']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 84.6223498CurrentTrain: epoch  0, batch     1 | loss: 119.6195518CurrentTrain: epoch  0, batch     2 | loss: 67.2202089CurrentTrain: epoch  0, batch     3 | loss: 112.4701751CurrentTrain: epoch  0, batch     4 | loss: 50.2225954CurrentTrain: epoch  0, batch     5 | loss: 42.7531968CurrentTrain: epoch  0, batch     6 | loss: 43.8886114CurrentTrain: epoch  1, batch     0 | loss: 61.8426982CurrentTrain: epoch  1, batch     1 | loss: 54.0584811CurrentTrain: epoch  1, batch     2 | loss: 42.9725750CurrentTrain: epoch  1, batch     3 | loss: 39.5681254CurrentTrain: epoch  1, batch     4 | loss: 74.1144404CurrentTrain: epoch  1, batch     5 | loss: 106.9413151CurrentTrain: epoch  1, batch     6 | loss: 70.8479367CurrentTrain: epoch  2, batch     0 | loss: 108.8174038CurrentTrain: epoch  2, batch     1 | loss: 50.6598031CurrentTrain: epoch  2, batch     2 | loss: 39.9990896CurrentTrain: epoch  2, batch     3 | loss: 113.1504949CurrentTrain: epoch  2, batch     4 | loss: 53.8607029CurrentTrain: epoch  2, batch     5 | loss: 40.6365101CurrentTrain: epoch  2, batch     6 | loss: 49.7100467CurrentTrain: epoch  3, batch     0 | loss: 40.2495269CurrentTrain: epoch  3, batch     1 | loss: 109.2142593CurrentTrain: epoch  3, batch     2 | loss: 41.2341055CurrentTrain: epoch  3, batch     3 | loss: 50.5671648CurrentTrain: epoch  3, batch     4 | loss: 51.8557624CurrentTrain: epoch  3, batch     5 | loss: 50.1208962CurrentTrain: epoch  3, batch     6 | loss: 51.3655135CurrentTrain: epoch  4, batch     0 | loss: 48.7323410CurrentTrain: epoch  4, batch     1 | loss: 38.4739322CurrentTrain: epoch  4, batch     2 | loss: 105.0200384CurrentTrain: epoch  4, batch     3 | loss: 39.6561741CurrentTrain: epoch  4, batch     4 | loss: 50.5111812CurrentTrain: epoch  4, batch     5 | loss: 49.3558951CurrentTrain: epoch  4, batch     6 | loss: 213.8323294CurrentTrain: epoch  5, batch     0 | loss: 64.7171033CurrentTrain: epoch  5, batch     1 | loss: 103.6611692CurrentTrain: epoch  5, batch     2 | loss: 37.8282894CurrentTrain: epoch  5, batch     3 | loss: 51.4681893CurrentTrain: epoch  5, batch     4 | loss: 48.1645362CurrentTrain: epoch  5, batch     5 | loss: 66.2788521CurrentTrain: epoch  5, batch     6 | loss: 105.2477983CurrentTrain: epoch  6, batch     0 | loss: 69.5399148CurrentTrain: epoch  6, batch     1 | loss: 66.7478979CurrentTrain: epoch  6, batch     2 | loss: 49.7600236CurrentTrain: epoch  6, batch     3 | loss: 50.1123542CurrentTrain: epoch  6, batch     4 | loss: 47.8098035CurrentTrain: epoch  6, batch     5 | loss: 101.1873173CurrentTrain: epoch  6, batch     6 | loss: 62.9666215CurrentTrain: epoch  7, batch     0 | loss: 38.6608427CurrentTrain: epoch  7, batch     1 | loss: 43.8317054CurrentTrain: epoch  7, batch     2 | loss: 48.9276646CurrentTrain: epoch  7, batch     3 | loss: 68.9814796CurrentTrain: epoch  7, batch     4 | loss: 104.0924815CurrentTrain: epoch  7, batch     5 | loss: 66.5079579CurrentTrain: epoch  7, batch     6 | loss: 103.3107074CurrentTrain: epoch  8, batch     0 | loss: 67.7465161CurrentTrain: epoch  8, batch     1 | loss: 103.6729869CurrentTrain: epoch  8, batch     2 | loss: 37.2573011CurrentTrain: epoch  8, batch     3 | loss: 49.9838582CurrentTrain: epoch  8, batch     4 | loss: 48.0128348CurrentTrain: epoch  8, batch     5 | loss: 47.1804866CurrentTrain: epoch  8, batch     6 | loss: 63.7731775CurrentTrain: epoch  9, batch     0 | loss: 35.6819460CurrentTrain: epoch  9, batch     1 | loss: 68.2139920CurrentTrain: epoch  9, batch     2 | loss: 48.0216438CurrentTrain: epoch  9, batch     3 | loss: 36.3372268CurrentTrain: epoch  9, batch     4 | loss: 37.8531837CurrentTrain: epoch  9, batch     5 | loss: 107.6078953CurrentTrain: epoch  9, batch     6 | loss: 102.9139218
MemoryTrain:  epoch  0, batch     0 | loss: 0.7399455MemoryTrain:  epoch  1, batch     0 | loss: 0.6069319MemoryTrain:  epoch  2, batch     0 | loss: 0.6014504MemoryTrain:  epoch  3, batch     0 | loss: 0.4203200MemoryTrain:  epoch  4, batch     0 | loss: 0.3199108MemoryTrain:  epoch  5, batch     0 | loss: 0.3462930MemoryTrain:  epoch  6, batch     0 | loss: 0.2553594MemoryTrain:  epoch  7, batch     0 | loss: 0.2239242MemoryTrain:  epoch  8, batch     0 | loss: 0.1915205MemoryTrain:  epoch  9, batch     0 | loss: 0.1593394

F1 score per class: {1: 0.14678899082568808, 2: 0.0, 3: 0.43859649122807015, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.07142857142857142, 15: 0.0, 19: 0.0, 22: 0.46048109965635736, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.36363636363636365, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2417391304347826
Weighted-average F1 score: 0.20458449904431328
F1 score per class: {1: 0.12612612612612611, 2: 0.0, 3: 0.49746192893401014, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.06756756756756757, 15: 0.0, 19: 0.0, 22: 0.4866666666666667, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.3389830508474576, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.20625724217844726
Weighted-average F1 score: 0.17220437355415444
F1 score per class: {1: 0.1286549707602339, 2: 0.0, 3: 0.5212765957446809, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.07692307692307693, 15: 0.0, 19: 0.0, 22: 0.47896440129449835, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.3352601156069364, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.21021726365237817
Weighted-average F1 score: 0.1753599435135469

F1 score per class: {1: 0.1095890410958904, 2: 0.35714285714285715, 3: 0.3424657534246575, 6: 0.18571428571428572, 7: 0.0, 9: 0.8771929824561403, 11: 0.15483870967741936, 12: 0.15584415584415584, 14: 0.05309734513274336, 15: 0.4, 19: 0.30612244897959184, 22: 0.3691460055096419, 24: 0.029850746268656716, 25: 0.4927536231884058, 26: 0.6538461538461539, 27: 0.0, 28: 0.12698412698412698, 29: 0.8055555555555556, 31: 0.0, 32: 0.5725490196078431, 34: 0.12779552715654952, 35: 0.2047244094488189, 37: 0.11235955056179775, 38: 0.2077922077922078, 39: 0.1, 40: 0.07317073170731707}
Micro-average F1 score: 0.29508196721311475
Weighted-average F1 score: 0.27797365583726297
F1 score per class: {1: 0.08842105263157894, 2: 0.24, 3: 0.310126582278481, 6: 0.27586206896551724, 7: 0.0, 9: 0.6578947368421053, 11: 0.08391608391608392, 12: 0.2571428571428571, 14: 0.041666666666666664, 15: 0.3333333333333333, 19: 0.26519337016574585, 22: 0.3492822966507177, 24: 0.03125, 25: 0.7727272727272727, 26: 0.6244343891402715, 27: 0.08333333333333333, 28: 0.06493506493506493, 29: 0.794392523364486, 31: 0.0, 32: 0.49079754601226994, 34: 0.10152284263959391, 35: 0.1895734597156398, 37: 0.15447154471544716, 38: 0.12274368231046931, 39: 0.06666666666666667, 40: 0.3088803088803089}
Micro-average F1 score: 0.2553648068669528
Weighted-average F1 score: 0.23067919910266618
F1 score per class: {1: 0.08906882591093117, 2: 0.2553191489361702, 3: 0.33793103448275863, 6: 0.2727272727272727, 7: 0.0, 9: 0.746268656716418, 11: 0.09333333333333334, 12: 0.25287356321839083, 14: 0.04878048780487805, 15: 0.34285714285714286, 19: 0.3015075376884422, 22: 0.3515439429928741, 24: 0.029850746268656716, 25: 0.7586206896551724, 26: 0.6359447004608295, 27: 0.08333333333333333, 28: 0.06451612903225806, 29: 0.8, 31: 0.0, 32: 0.4954128440366973, 34: 0.1001727115716753, 35: 0.18055555555555555, 37: 0.1532258064516129, 38: 0.10996563573883161, 39: 0.06896551724137931, 40: 0.2897196261682243}
Micro-average F1 score: 0.25719489981785065
Weighted-average F1 score: 0.23046656072286895
cur_acc:  ['0.6718', '0.1538', '0.3346', '0.3756', '0.2417']
his_acc:  ['0.6718', '0.5480', '0.4623', '0.4160', '0.2951']
cur_acc des:  ['0.6217', '0.3050', '0.3235', '0.3460', '0.2063']
his_acc des:  ['0.6217', '0.4826', '0.3986', '0.3362', '0.2554']
cur_acc rrf:  ['0.6267', '0.3077', '0.3415', '0.3428', '0.2102']
his_acc rrf:  ['0.6267', '0.4919', '0.4016', '0.3355', '0.2572']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 66.1410156CurrentTrain: epoch  0, batch     1 | loss: 64.2537450CurrentTrain: epoch  0, batch     2 | loss: 65.9026591CurrentTrain: epoch  0, batch     3 | loss: 51.6224883CurrentTrain: epoch  0, batch     4 | loss: 89.9310350CurrentTrain: epoch  0, batch     5 | loss: 86.2093513CurrentTrain: epoch  1, batch     0 | loss: 63.8275597CurrentTrain: epoch  1, batch     1 | loss: 74.1165177CurrentTrain: epoch  1, batch     2 | loss: 73.5740444CurrentTrain: epoch  1, batch     3 | loss: 35.5488110CurrentTrain: epoch  1, batch     4 | loss: 43.8078135CurrentTrain: epoch  1, batch     5 | loss: 29.7926091CurrentTrain: epoch  2, batch     0 | loss: 69.9180048CurrentTrain: epoch  2, batch     1 | loss: 37.7808210CurrentTrain: epoch  2, batch     2 | loss: 56.0622766CurrentTrain: epoch  2, batch     3 | loss: 108.2525966CurrentTrain: epoch  2, batch     4 | loss: 44.2068858CurrentTrain: epoch  2, batch     5 | loss: 37.1811481CurrentTrain: epoch  3, batch     0 | loss: 55.3876499CurrentTrain: epoch  3, batch     1 | loss: 50.4709379CurrentTrain: epoch  3, batch     2 | loss: 38.4735424CurrentTrain: epoch  3, batch     3 | loss: 51.7681534CurrentTrain: epoch  3, batch     4 | loss: 52.3835662CurrentTrain: epoch  3, batch     5 | loss: 25.5507212CurrentTrain: epoch  4, batch     0 | loss: 48.7495236CurrentTrain: epoch  4, batch     1 | loss: 104.7288763CurrentTrain: epoch  4, batch     2 | loss: 40.0825776CurrentTrain: epoch  4, batch     3 | loss: 49.5245678CurrentTrain: epoch  4, batch     4 | loss: 38.3870138CurrentTrain: epoch  4, batch     5 | loss: 71.9991906CurrentTrain: epoch  5, batch     0 | loss: 48.7754003CurrentTrain: epoch  5, batch     1 | loss: 70.3098544CurrentTrain: epoch  5, batch     2 | loss: 47.1393182CurrentTrain: epoch  5, batch     3 | loss: 46.1724105CurrentTrain: epoch  5, batch     4 | loss: 38.1921603CurrentTrain: epoch  5, batch     5 | loss: 161.9452505CurrentTrain: epoch  6, batch     0 | loss: 107.7693352CurrentTrain: epoch  6, batch     1 | loss: 35.1493487CurrentTrain: epoch  6, batch     2 | loss: 36.5481685CurrentTrain: epoch  6, batch     3 | loss: 102.0293243CurrentTrain: epoch  6, batch     4 | loss: 48.6489455CurrentTrain: epoch  6, batch     5 | loss: 47.0901090CurrentTrain: epoch  7, batch     0 | loss: 47.9690352CurrentTrain: epoch  7, batch     1 | loss: 46.7192673CurrentTrain: epoch  7, batch     2 | loss: 46.6213967CurrentTrain: epoch  7, batch     3 | loss: 67.9186735CurrentTrain: epoch  7, batch     4 | loss: 30.5911182CurrentTrain: epoch  7, batch     5 | loss: 46.6679041CurrentTrain: epoch  8, batch     0 | loss: 48.5219203CurrentTrain: epoch  8, batch     1 | loss: 47.8632648CurrentTrain: epoch  8, batch     2 | loss: 49.5609702CurrentTrain: epoch  8, batch     3 | loss: 35.9492381CurrentTrain: epoch  8, batch     4 | loss: 64.7620619CurrentTrain: epoch  8, batch     5 | loss: 34.3195228CurrentTrain: epoch  9, batch     0 | loss: 47.6387802CurrentTrain: epoch  9, batch     1 | loss: 36.1083283CurrentTrain: epoch  9, batch     2 | loss: 65.6304452CurrentTrain: epoch  9, batch     3 | loss: 46.1877716CurrentTrain: epoch  9, batch     4 | loss: 46.2938294CurrentTrain: epoch  9, batch     5 | loss: 77.1834793
MemoryTrain:  epoch  0, batch     0 | loss: 0.3933272MemoryTrain:  epoch  1, batch     0 | loss: 0.3422865MemoryTrain:  epoch  2, batch     0 | loss: 0.2815831MemoryTrain:  epoch  3, batch     0 | loss: 0.2292836MemoryTrain:  epoch  4, batch     0 | loss: 0.1761915MemoryTrain:  epoch  5, batch     0 | loss: 0.1446117MemoryTrain:  epoch  6, batch     0 | loss: 0.1190075MemoryTrain:  epoch  7, batch     0 | loss: 0.1016716MemoryTrain:  epoch  8, batch     0 | loss: 0.0847603MemoryTrain:  epoch  9, batch     0 | loss: 0.0798114

F1 score per class: {0: 0.9014084507042254, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9743589743589743, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.1, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.2222222222222222, 22: 0.0, 23: 0.7560975609756098, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.5670103092783505
Weighted-average F1 score: 0.43173070616934717
F1 score per class: {0: 0.6422018348623854, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9458128078817734, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.04, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.2037037037037037, 22: 0.0, 23: 0.7311827956989247, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3597560975609756
Weighted-average F1 score: 0.2549435145409033
F1 score per class: {0: 0.6542056074766355, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9746192893401016, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.07272727272727272, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.1875, 22: 0.0, 23: 0.7472527472527473, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.36935991605456453
Weighted-average F1 score: 0.257892835067993

F1 score per class: {0: 0.4383561643835616, 1: 0.09815950920245399, 2: 0.24390243902439024, 3: 0.3825136612021858, 4: 0.9547738693467337, 6: 0.26900584795321636, 7: 0.019417475728155338, 9: 0.7936507936507936, 11: 0.13986013986013987, 12: 0.12738853503184713, 13: 0.017167381974248927, 14: 0.037383177570093455, 15: 0.4666666666666667, 19: 0.3333333333333333, 21: 0.07633587786259542, 22: 0.45662100456621, 23: 0.6888888888888889, 24: 0.0392156862745098, 25: 0.5822784810126582, 26: 0.6568627450980392, 27: 0.09090909090909091, 28: 0.36363636363636365, 29: 0.7586206896551724, 31: 0.0, 32: 0.5, 34: 0.13580246913580246, 35: 0.14925373134328357, 37: 0.058823529411764705, 38: 0.2153846153846154, 39: 0.10526315789473684, 40: 0.05063291139240506}
Micro-average F1 score: 0.3210227272727273
Weighted-average F1 score: 0.2900546401716445
F1 score per class: {0: 0.2017291066282421, 1: 0.0900562851782364, 2: 0.11320754716981132, 3: 0.2585858585858586, 4: 0.9056603773584906, 6: 0.2595419847328244, 7: 0.05714285714285714, 9: 0.5681818181818182, 11: 0.11904761904761904, 12: 0.21262458471760798, 13: 0.007272727272727273, 14: 0.036585365853658534, 15: 0.27906976744186046, 19: 0.36764705882352944, 21: 0.05432098765432099, 22: 0.46808510638297873, 23: 0.6017699115044248, 24: 0.05405405405405406, 25: 0.7358490566037735, 26: 0.625, 27: 0.037037037037037035, 28: 0.0625, 29: 0.7404255319148936, 31: 0.026845637583892617, 32: 0.4235294117647059, 34: 0.088, 35: 0.20202020202020202, 37: 0.1306122448979592, 38: 0.10149253731343283, 39: 0.16, 40: 0.291970802919708}
Micro-average F1 score: 0.25
Weighted-average F1 score: 0.21754294264633872
F1 score per class: {0: 0.20057306590257878, 1: 0.08856088560885608, 2: 0.11428571428571428, 3: 0.30117647058823527, 4: 0.9504950495049505, 6: 0.26153846153846155, 7: 0.05714285714285714, 9: 0.6172839506172839, 11: 0.17560975609756097, 12: 0.22, 13: 0.013289036544850499, 14: 0.037267080745341616, 15: 0.2727272727272727, 19: 0.3682539682539683, 21: 0.04945054945054945, 22: 0.47804878048780486, 23: 0.6476190476190476, 24: 0.041666666666666664, 25: 0.6990291262135923, 26: 0.6216216216216216, 27: 0.03508771929824561, 28: 0.06451612903225806, 29: 0.7467811158798283, 31: 0.017241379310344827, 32: 0.4391691394658754, 34: 0.09223300970873786, 35: 0.18137254901960784, 37: 0.11235955056179775, 38: 0.09556313993174062, 39: 0.08333333333333333, 40: 0.26519337016574585}
Micro-average F1 score: 0.25572629747752973
Weighted-average F1 score: 0.22117280487385205
cur_acc:  ['0.6718', '0.1538', '0.3346', '0.3756', '0.2417', '0.5670']
his_acc:  ['0.6718', '0.5480', '0.4623', '0.4160', '0.2951', '0.3210']
cur_acc des:  ['0.6217', '0.3050', '0.3235', '0.3460', '0.2063', '0.3598']
his_acc des:  ['0.6217', '0.4826', '0.3986', '0.3362', '0.2554', '0.2500']
cur_acc rrf:  ['0.6267', '0.3077', '0.3415', '0.3428', '0.2102', '0.3694']
his_acc rrf:  ['0.6267', '0.4919', '0.4016', '0.3355', '0.2572', '0.2557']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 54.8667735CurrentTrain: epoch  0, batch     1 | loss: 60.8709386CurrentTrain: epoch  0, batch     2 | loss: 62.3973175CurrentTrain: epoch  0, batch     3 | loss: 60.2659231CurrentTrain: epoch  0, batch     4 | loss: 52.9297268CurrentTrain: epoch  0, batch     5 | loss: 26.2765019CurrentTrain: epoch  1, batch     0 | loss: 111.5527241CurrentTrain: epoch  1, batch     1 | loss: 54.4649020CurrentTrain: epoch  1, batch     2 | loss: 33.6818222CurrentTrain: epoch  1, batch     3 | loss: 49.5521516CurrentTrain: epoch  1, batch     4 | loss: 51.3988696CurrentTrain: epoch  1, batch     5 | loss: 29.9810150CurrentTrain: epoch  2, batch     0 | loss: 53.6654299CurrentTrain: epoch  2, batch     1 | loss: 72.3087961CurrentTrain: epoch  2, batch     2 | loss: 37.3143588CurrentTrain: epoch  2, batch     3 | loss: 38.9912455CurrentTrain: epoch  2, batch     4 | loss: 69.0454087CurrentTrain: epoch  2, batch     5 | loss: 20.5715031CurrentTrain: epoch  3, batch     0 | loss: 50.1713543CurrentTrain: epoch  3, batch     1 | loss: 47.8878302CurrentTrain: epoch  3, batch     2 | loss: 67.6491938CurrentTrain: epoch  3, batch     3 | loss: 49.8964225CurrentTrain: epoch  3, batch     4 | loss: 37.1790790CurrentTrain: epoch  3, batch     5 | loss: 27.6189390CurrentTrain: epoch  4, batch     0 | loss: 37.9962637CurrentTrain: epoch  4, batch     1 | loss: 37.1899634CurrentTrain: epoch  4, batch     2 | loss: 65.9881454CurrentTrain: epoch  4, batch     3 | loss: 37.8347707CurrentTrain: epoch  4, batch     4 | loss: 68.4337403CurrentTrain: epoch  4, batch     5 | loss: 45.3472595CurrentTrain: epoch  5, batch     0 | loss: 34.1853538CurrentTrain: epoch  5, batch     1 | loss: 67.2069849CurrentTrain: epoch  5, batch     2 | loss: 49.9741132CurrentTrain: epoch  5, batch     3 | loss: 64.9542387CurrentTrain: epoch  5, batch     4 | loss: 65.5108593CurrentTrain: epoch  5, batch     5 | loss: 27.5105726CurrentTrain: epoch  6, batch     0 | loss: 29.5779802CurrentTrain: epoch  6, batch     1 | loss: 49.3789598CurrentTrain: epoch  6, batch     2 | loss: 66.8584566CurrentTrain: epoch  6, batch     3 | loss: 48.4088018CurrentTrain: epoch  6, batch     4 | loss: 66.7903346CurrentTrain: epoch  6, batch     5 | loss: 17.0811697CurrentTrain: epoch  7, batch     0 | loss: 46.7119815CurrentTrain: epoch  7, batch     1 | loss: 25.8317290CurrentTrain: epoch  7, batch     2 | loss: 49.5468344CurrentTrain: epoch  7, batch     3 | loss: 36.7294002CurrentTrain: epoch  7, batch     4 | loss: 107.9496684CurrentTrain: epoch  7, batch     5 | loss: 97.0249506CurrentTrain: epoch  8, batch     0 | loss: 36.6691602CurrentTrain: epoch  8, batch     1 | loss: 101.9874976CurrentTrain: epoch  8, batch     2 | loss: 68.2682502CurrentTrain: epoch  8, batch     3 | loss: 35.8236575CurrentTrain: epoch  8, batch     4 | loss: 36.1373882CurrentTrain: epoch  8, batch     5 | loss: 18.7065593CurrentTrain: epoch  9, batch     0 | loss: 65.1468903CurrentTrain: epoch  9, batch     1 | loss: 26.8159197CurrentTrain: epoch  9, batch     2 | loss: 66.2077013CurrentTrain: epoch  9, batch     3 | loss: 63.1012793CurrentTrain: epoch  9, batch     4 | loss: 49.0060143CurrentTrain: epoch  9, batch     5 | loss: 44.9589087
MemoryTrain:  epoch  0, batch     0 | loss: 0.2046030MemoryTrain:  epoch  0, batch     1 | loss: 0.0520066MemoryTrain:  epoch  1, batch     0 | loss: 0.1488852MemoryTrain:  epoch  1, batch     1 | loss: 0.1484958MemoryTrain:  epoch  2, batch     0 | loss: 0.1351738MemoryTrain:  epoch  2, batch     1 | loss: 0.0602527MemoryTrain:  epoch  3, batch     0 | loss: 0.1085647MemoryTrain:  epoch  3, batch     1 | loss: 0.0565094MemoryTrain:  epoch  4, batch     0 | loss: 0.0908082MemoryTrain:  epoch  4, batch     1 | loss: 0.0498568MemoryTrain:  epoch  5, batch     0 | loss: 0.0720544MemoryTrain:  epoch  5, batch     1 | loss: 0.0813891MemoryTrain:  epoch  6, batch     0 | loss: 0.0605766MemoryTrain:  epoch  6, batch     1 | loss: 0.0388996MemoryTrain:  epoch  7, batch     0 | loss: 0.0587549MemoryTrain:  epoch  7, batch     1 | loss: 0.0530362MemoryTrain:  epoch  8, batch     0 | loss: 0.0543504MemoryTrain:  epoch  8, batch     1 | loss: 0.0418646MemoryTrain:  epoch  9, batch     0 | loss: 0.0486642MemoryTrain:  epoch  9, batch     1 | loss: 0.0291924

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.26, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 19: 0.0, 20: 0.6271186440677966, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9473684210526315, 31: 0.0, 32: 0.0, 33: 0.4, 34: 0.0, 36: 0.20512820512820512, 38: 0.0}
Micro-average F1 score: 0.2981132075471698
Weighted-average F1 score: 0.22453890194607842
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 6: 0.0, 7: 0.0, 8: 0.455026455026455, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 20: 0.5850340136054422, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.95, 31: 0.0, 32: 0.0, 33: 0.4444444444444444, 34: 0.0, 35: 0.0, 36: 0.4805194805194805, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2976554536187564
Weighted-average F1 score: 0.22519712773998488
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 6: 0.0, 7: 0.0, 8: 0.4594594594594595, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.589041095890411, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.95, 31: 0.0, 32: 0.0, 33: 0.4, 34: 0.0, 35: 0.0, 36: 0.4740740740740741, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.28633405639913234
Weighted-average F1 score: 0.20439461029929482

F1 score per class: {0: 0.4266666666666667, 1: 0.0995850622406639, 2: 0.23529411764705882, 3: 0.3393939393939394, 4: 0.96, 6: 0.273224043715847, 7: 0.0, 8: 0.203125, 9: 0.7692307692307693, 11: 0.10218978102189781, 12: 0.12154696132596685, 13: 0.01904761904761905, 14: 0.04597701149425287, 15: 0.45161290322580644, 19: 0.3522727272727273, 20: 0.2901960784313726, 21: 0.06504065040650407, 22: 0.46938775510204084, 23: 0.6190476190476191, 24: 0.03636363636363636, 25: 0.4857142857142857, 26: 0.6118721461187214, 27: 0.07692307692307693, 28: 0.36363636363636365, 29: 0.7228915662650602, 30: 0.9473684210526315, 31: 0.020833333333333332, 32: 0.5182186234817814, 33: 0.3333333333333333, 34: 0.08695652173913043, 35: 0.13740458015267176, 36: 0.1839080459770115, 37: 0.03225806451612903, 38: 0.18691588785046728, 39: 0.0, 40: 0.07407407407407407}
Micro-average F1 score: 0.3196095076400679
Weighted-average F1 score: 0.30201987176355577
F1 score per class: {0: 0.1452991452991453, 1: 0.09784735812133072, 2: 0.17391304347826086, 3: 0.24561403508771928, 4: 0.9186602870813397, 6: 0.2747603833865815, 7: 0.02, 8: 0.19068736141906872, 9: 0.5376344086021505, 11: 0.1827956989247312, 12: 0.17699115044247787, 13: 0.016129032258064516, 14: 0.046242774566473986, 15: 0.21818181818181817, 19: 0.34065934065934067, 20: 0.21717171717171718, 21: 0.06666666666666667, 22: 0.49760765550239233, 23: 0.52, 24: 0.05555555555555555, 25: 0.6597938144329897, 26: 0.5836909871244635, 27: 0.030303030303030304, 28: 0.10526315789473684, 29: 0.7063492063492064, 30: 0.8260869565217391, 31: 0.017543859649122806, 32: 0.42934782608695654, 33: 0.11267605633802817, 34: 0.15023474178403756, 35: 0.18719211822660098, 36: 0.29365079365079366, 37: 0.17964071856287425, 38: 0.11666666666666667, 39: 0.10526315789473684, 40: 0.1986754966887417}
Micro-average F1 score: 0.2574478649453823
Weighted-average F1 score: 0.23206084706533023
F1 score per class: {0: 0.15315315315315314, 1: 0.0946969696969697, 2: 0.18181818181818182, 3: 0.3237597911227154, 4: 0.9411764705882353, 6: 0.273015873015873, 7: 0.019230769230769232, 8: 0.22972972972972974, 9: 0.5747126436781609, 11: 0.19, 12: 0.17647058823529413, 13: 0.011494252873563218, 14: 0.05263157894736842, 15: 0.23529411764705882, 19: 0.33, 20: 0.21393034825870647, 21: 0.06646525679758308, 22: 0.5242718446601942, 23: 0.5591397849462365, 24: 0.05405405405405406, 25: 0.6363636363636364, 26: 0.5862068965517241, 27: 0.03076923076923077, 28: 0.1111111111111111, 29: 0.7142857142857143, 30: 0.8636363636363636, 31: 0.0213903743315508, 32: 0.41578947368421054, 33: 0.1095890410958904, 34: 0.15384615384615385, 35: 0.1671309192200557, 36: 0.3076923076923077, 37: 0.1910828025477707, 38: 0.11594202898550725, 39: 0.125, 40: 0.1693121693121693}
Micro-average F1 score: 0.26515550869794413
Weighted-average F1 score: 0.23748695910514742
cur_acc:  ['0.6718', '0.1538', '0.3346', '0.3756', '0.2417', '0.5670', '0.2981']
his_acc:  ['0.6718', '0.5480', '0.4623', '0.4160', '0.2951', '0.3210', '0.3196']
cur_acc des:  ['0.6217', '0.3050', '0.3235', '0.3460', '0.2063', '0.3598', '0.2977']
his_acc des:  ['0.6217', '0.4826', '0.3986', '0.3362', '0.2554', '0.2500', '0.2574']
cur_acc rrf:  ['0.6267', '0.3077', '0.3415', '0.3428', '0.2102', '0.3694', '0.2863']
his_acc rrf:  ['0.6267', '0.4919', '0.4016', '0.3355', '0.2572', '0.2557', '0.2652']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 60.3765349CurrentTrain: epoch  0, batch     1 | loss: 75.9928574CurrentTrain: epoch  0, batch     2 | loss: 74.5393119CurrentTrain: epoch  0, batch     3 | loss: 62.3257690CurrentTrain: epoch  0, batch     4 | loss: 115.6299508CurrentTrain: epoch  0, batch     5 | loss: 80.3094361CurrentTrain: epoch  0, batch     6 | loss: 36.6057612CurrentTrain: epoch  0, batch     7 | loss: 2.9819165CurrentTrain: epoch  1, batch     0 | loss: 49.2403501CurrentTrain: epoch  1, batch     1 | loss: 75.8377875CurrentTrain: epoch  1, batch     2 | loss: 48.8869754CurrentTrain: epoch  1, batch     3 | loss: 42.9092257CurrentTrain: epoch  1, batch     4 | loss: 57.7271992CurrentTrain: epoch  1, batch     5 | loss: 106.9557495CurrentTrain: epoch  1, batch     6 | loss: 71.2280265CurrentTrain: epoch  1, batch     7 | loss: 2.9141987CurrentTrain: epoch  2, batch     0 | loss: 110.5836971CurrentTrain: epoch  2, batch     1 | loss: 48.3383110CurrentTrain: epoch  2, batch     2 | loss: 40.6177732CurrentTrain: epoch  2, batch     3 | loss: 68.9225628CurrentTrain: epoch  2, batch     4 | loss: 51.4733937CurrentTrain: epoch  2, batch     5 | loss: 69.2093307CurrentTrain: epoch  2, batch     6 | loss: 52.8761113CurrentTrain: epoch  2, batch     7 | loss: 0.6265716CurrentTrain: epoch  3, batch     0 | loss: 111.1834792CurrentTrain: epoch  3, batch     1 | loss: 108.2460483CurrentTrain: epoch  3, batch     2 | loss: 46.4518882CurrentTrain: epoch  3, batch     3 | loss: 51.5891584CurrentTrain: epoch  3, batch     4 | loss: 37.2912182CurrentTrain: epoch  3, batch     5 | loss: 70.2360398CurrentTrain: epoch  3, batch     6 | loss: 104.8043406CurrentTrain: epoch  3, batch     7 | loss: 1.2365274CurrentTrain: epoch  4, batch     0 | loss: 48.7811471CurrentTrain: epoch  4, batch     1 | loss: 67.1464530CurrentTrain: epoch  4, batch     2 | loss: 69.2843907CurrentTrain: epoch  4, batch     3 | loss: 51.2365657CurrentTrain: epoch  4, batch     4 | loss: 49.3088547CurrentTrain: epoch  4, batch     5 | loss: 39.4328075CurrentTrain: epoch  4, batch     6 | loss: 49.6905648CurrentTrain: epoch  4, batch     7 | loss: 2.8970212CurrentTrain: epoch  5, batch     0 | loss: 66.6946776CurrentTrain: epoch  5, batch     1 | loss: 49.9576080CurrentTrain: epoch  5, batch     2 | loss: 39.9090668CurrentTrain: epoch  5, batch     3 | loss: 37.3019531CurrentTrain: epoch  5, batch     4 | loss: 50.7008431CurrentTrain: epoch  5, batch     5 | loss: 68.7776039CurrentTrain: epoch  5, batch     6 | loss: 68.3654696CurrentTrain: epoch  5, batch     7 | loss: 0.8552412CurrentTrain: epoch  6, batch     0 | loss: 107.9759569CurrentTrain: epoch  6, batch     1 | loss: 29.6884827CurrentTrain: epoch  6, batch     2 | loss: 68.6572931CurrentTrain: epoch  6, batch     3 | loss: 35.4242758CurrentTrain: epoch  6, batch     4 | loss: 68.3870443CurrentTrain: epoch  6, batch     5 | loss: 39.4075686CurrentTrain: epoch  6, batch     6 | loss: 49.6340523CurrentTrain: epoch  6, batch     7 | loss: 2.8736701CurrentTrain: epoch  7, batch     0 | loss: 103.5885120CurrentTrain: epoch  7, batch     1 | loss: 107.7363122CurrentTrain: epoch  7, batch     2 | loss: 35.5009888CurrentTrain: epoch  7, batch     3 | loss: 50.2940939CurrentTrain: epoch  7, batch     4 | loss: 49.3006140CurrentTrain: epoch  7, batch     5 | loss: 47.4715619CurrentTrain: epoch  7, batch     6 | loss: 36.9305922CurrentTrain: epoch  7, batch     7 | loss: 2.9162498CurrentTrain: epoch  8, batch     0 | loss: 47.9396039CurrentTrain: epoch  8, batch     1 | loss: 46.4499625CurrentTrain: epoch  8, batch     2 | loss: 68.8710811CurrentTrain: epoch  8, batch     3 | loss: 47.9204769CurrentTrain: epoch  8, batch     4 | loss: 66.2642065CurrentTrain: epoch  8, batch     5 | loss: 107.6652838CurrentTrain: epoch  8, batch     6 | loss: 35.6001871CurrentTrain: epoch  8, batch     7 | loss: 2.8779793CurrentTrain: epoch  9, batch     0 | loss: 68.2479154CurrentTrain: epoch  9, batch     1 | loss: 28.4022070CurrentTrain: epoch  9, batch     2 | loss: 49.2557733CurrentTrain: epoch  9, batch     3 | loss: 222.9506364CurrentTrain: epoch  9, batch     4 | loss: 36.5142848CurrentTrain: epoch  9, batch     5 | loss: 66.2206613CurrentTrain: epoch  9, batch     6 | loss: 37.4357048CurrentTrain: epoch  9, batch     7 | loss: 2.8361971
MemoryTrain:  epoch  0, batch     0 | loss: 0.2204329MemoryTrain:  epoch  0, batch     1 | loss: 0.0321498MemoryTrain:  epoch  1, batch     0 | loss: 0.1548439MemoryTrain:  epoch  1, batch     1 | loss: 0.0540551MemoryTrain:  epoch  2, batch     0 | loss: 0.1117388MemoryTrain:  epoch  2, batch     1 | loss: 0.0549809MemoryTrain:  epoch  3, batch     0 | loss: 0.0764035MemoryTrain:  epoch  3, batch     1 | loss: 0.0485714MemoryTrain:  epoch  4, batch     0 | loss: 0.0633605MemoryTrain:  epoch  4, batch     1 | loss: 0.0599470MemoryTrain:  epoch  5, batch     0 | loss: 0.0491428MemoryTrain:  epoch  5, batch     1 | loss: 0.0617462MemoryTrain:  epoch  6, batch     0 | loss: 0.0491639MemoryTrain:  epoch  6, batch     1 | loss: 0.0464331MemoryTrain:  epoch  7, batch     0 | loss: 0.0429615MemoryTrain:  epoch  7, batch     1 | loss: 0.0428338MemoryTrain:  epoch  8, batch     0 | loss: 0.0408884MemoryTrain:  epoch  8, batch     1 | loss: 0.0319076MemoryTrain:  epoch  9, batch     0 | loss: 0.0405441MemoryTrain:  epoch  9, batch     1 | loss: 0.0348977

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.803347280334728, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.384, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.391304347826087, 17: 0.2, 18: 0.208, 20: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 38: 0.0}
Micro-average F1 score: 0.40168539325842695
Weighted-average F1 score: 0.33493151727464354
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.526595744680851, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.4050632911392405, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.5070422535211268, 17: 0.7058823529411765, 18: 0.19463087248322147, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.24848075624577987
Weighted-average F1 score: 0.20420626170457035
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.532258064516129, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.46153846153846156, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.4927536231884058, 17: 0.6666666666666666, 18: 0.18360655737704917, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2590529247910863
Weighted-average F1 score: 0.21124508282408444

F1 score per class: {0: 0.5128205128205128, 1: 0.10569105691056911, 2: 0.20408163265306123, 3: 0.3977272727272727, 4: 0.9278350515463918, 5: 0.6808510638297872, 6: 0.12121212121212122, 7: 0.01652892561983471, 8: 0.11864406779661017, 9: 0.7246376811594203, 10: 0.3057324840764331, 11: 0.15503875968992248, 12: 0.1461187214611872, 13: 0.015625, 14: 0.024390243902439025, 15: 0.41379310344827586, 16: 0.2857142857142857, 17: 0.09090909090909091, 18: 0.12440191387559808, 19: 0.3435897435897436, 20: 0.3263157894736842, 21: 0.07079646017699115, 22: 0.5076142131979695, 23: 0.6666666666666666, 24: 0.029850746268656716, 25: 0.4931506849315068, 26: 0.5781990521327014, 27: 0.07142857142857142, 28: 0.36363636363636365, 29: 0.6926070038910506, 30: 0.918918918918919, 31: 0.029850746268656716, 32: 0.5376344086021505, 33: 0.375, 34: 0.06837606837606838, 35: 0.11940298507462686, 36: 0.1797752808988764, 37: 0.03278688524590164, 38: 0.10126582278481013, 39: 0.0, 40: 0.05263157894736842}
Micro-average F1 score: 0.3270767515331723
Weighted-average F1 score: 0.31389348985544235
F1 score per class: {0: 0.2214765100671141, 1: 0.08778625954198473, 2: 0.1276595744680851, 3: 0.23140495867768596, 4: 0.916256157635468, 5: 0.32405891980360063, 6: 0.24489795918367346, 7: 0.026845637583892617, 8: 0.257372654155496, 9: 0.49504950495049505, 10: 0.2077922077922078, 11: 0.13245033112582782, 12: 0.15555555555555556, 13: 0.007272727272727273, 14: 0.027972027972027972, 15: 0.2, 16: 0.2975206611570248, 17: 0.2727272727272727, 18: 0.06236559139784946, 19: 0.3198090692124105, 20: 0.20786516853932585, 21: 0.06179775280898876, 22: 0.4721030042918455, 23: 0.5321100917431193, 24: 0.03773584905660377, 25: 0.5227272727272727, 26: 0.5565217391304348, 27: 0.05405405405405406, 28: 0.06060606060606061, 29: 0.6742424242424242, 30: 0.76, 31: 0.019138755980861243, 32: 0.38222222222222224, 33: 0.13114754098360656, 34: 0.06451612903225806, 35: 0.185, 36: 0.30927835051546393, 37: 0.06153846153846154, 38: 0.10138248847926268, 39: 0.0625, 40: 0.14696485623003194}
Micro-average F1 score: 0.2302676399026764
Weighted-average F1 score: 0.20993395429235048
F1 score per class: {0: 0.2268041237113402, 1: 0.09107468123861566, 2: 0.1411764705882353, 3: 0.3285024154589372, 4: 0.9292929292929293, 5: 0.336734693877551, 6: 0.23197492163009403, 7: 0.025477707006369428, 8: 0.28307692307692306, 9: 0.5263157894736842, 10: 0.23841059602649006, 11: 0.12195121951219512, 12: 0.15584415584415584, 13: 0.006493506493506494, 14: 0.03125, 15: 0.2033898305084746, 16: 0.30357142857142855, 17: 0.25, 18: 0.06067172264355363, 19: 0.296137339055794, 20: 0.20994475138121546, 21: 0.05917159763313609, 22: 0.47863247863247865, 23: 0.5471698113207547, 24: 0.03571428571428571, 25: 0.5057471264367817, 26: 0.5614035087719298, 27: 0.05194805194805195, 28: 0.06060606060606061, 29: 0.6793893129770993, 30: 0.8085106382978723, 31: 0.02185792349726776, 32: 0.3648068669527897, 33: 0.13559322033898305, 34: 0.06329113924050633, 35: 0.18453865336658354, 36: 0.3157894736842105, 37: 0.04878048780487805, 38: 0.10084033613445378, 39: 0.0, 40: 0.13725490196078433}
Micro-average F1 score: 0.23502071334747904
Weighted-average F1 score: 0.21354941125312907
cur_acc:  ['0.6718', '0.1538', '0.3346', '0.3756', '0.2417', '0.5670', '0.2981', '0.4017']
his_acc:  ['0.6718', '0.5480', '0.4623', '0.4160', '0.2951', '0.3210', '0.3196', '0.3271']
cur_acc des:  ['0.6217', '0.3050', '0.3235', '0.3460', '0.2063', '0.3598', '0.2977', '0.2485']
his_acc des:  ['0.6217', '0.4826', '0.3986', '0.3362', '0.2554', '0.2500', '0.2574', '0.2303']
cur_acc rrf:  ['0.6267', '0.3077', '0.3415', '0.3428', '0.2102', '0.3694', '0.2863', '0.2591']
his_acc rrf:  ['0.6267', '0.4919', '0.4016', '0.3355', '0.2572', '0.2557', '0.2652', '0.2350']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 42.9796657CurrentTrain: epoch  0, batch     1 | loss: 50.3993850CurrentTrain: epoch  0, batch     2 | loss: 78.0706211CurrentTrain: epoch  0, batch     3 | loss: 49.4571913CurrentTrain: epoch  0, batch     4 | loss: 58.8650904CurrentTrain: epoch  0, batch     5 | loss: 47.7621571CurrentTrain: epoch  0, batch     6 | loss: 49.2104273CurrentTrain: epoch  0, batch     7 | loss: 76.4865605CurrentTrain: epoch  0, batch     8 | loss: 58.8155680CurrentTrain: epoch  0, batch     9 | loss: 47.5649647CurrentTrain: epoch  0, batch    10 | loss: 112.7835936CurrentTrain: epoch  0, batch    11 | loss: 59.0624391CurrentTrain: epoch  0, batch    12 | loss: 58.2531195CurrentTrain: epoch  0, batch    13 | loss: 58.3169275CurrentTrain: epoch  0, batch    14 | loss: 58.3331644CurrentTrain: epoch  0, batch    15 | loss: 76.3013520CurrentTrain: epoch  0, batch    16 | loss: 76.6022534CurrentTrain: epoch  0, batch    17 | loss: 112.7116842CurrentTrain: epoch  0, batch    18 | loss: 47.4589104CurrentTrain: epoch  0, batch    19 | loss: 57.3705785CurrentTrain: epoch  0, batch    20 | loss: 46.4990431CurrentTrain: epoch  0, batch    21 | loss: 47.0472446CurrentTrain: epoch  0, batch    22 | loss: 46.7388662CurrentTrain: epoch  0, batch    23 | loss: 76.2197633CurrentTrain: epoch  0, batch    24 | loss: 58.1896795CurrentTrain: epoch  0, batch    25 | loss: 47.0635186CurrentTrain: epoch  0, batch    26 | loss: 57.5688081CurrentTrain: epoch  0, batch    27 | loss: 57.3491706CurrentTrain: epoch  0, batch    28 | loss: 46.5996809CurrentTrain: epoch  0, batch    29 | loss: 56.8549139CurrentTrain: epoch  0, batch    30 | loss: 112.4083394CurrentTrain: epoch  0, batch    31 | loss: 76.6738740CurrentTrain: epoch  0, batch    32 | loss: 75.2508673CurrentTrain: epoch  0, batch    33 | loss: 46.9750317CurrentTrain: epoch  0, batch    34 | loss: 48.3251009CurrentTrain: epoch  0, batch    35 | loss: 46.7329155CurrentTrain: epoch  0, batch    36 | loss: 34.0867023CurrentTrain: epoch  0, batch    37 | loss: 45.5614154CurrentTrain: epoch  0, batch    38 | loss: 57.3950093CurrentTrain: epoch  0, batch    39 | loss: 38.7786403CurrentTrain: epoch  0, batch    40 | loss: 39.1327624CurrentTrain: epoch  0, batch    41 | loss: 57.3746890CurrentTrain: epoch  0, batch    42 | loss: 113.1371719CurrentTrain: epoch  0, batch    43 | loss: 46.6886133CurrentTrain: epoch  0, batch    44 | loss: 56.8654301CurrentTrain: epoch  0, batch    45 | loss: 111.7644981CurrentTrain: epoch  0, batch    46 | loss: 57.3838173CurrentTrain: epoch  0, batch    47 | loss: 57.5286102CurrentTrain: epoch  0, batch    48 | loss: 74.6866063CurrentTrain: epoch  0, batch    49 | loss: 46.2248713CurrentTrain: epoch  0, batch    50 | loss: 46.7424749CurrentTrain: epoch  0, batch    51 | loss: 75.2747775CurrentTrain: epoch  0, batch    52 | loss: 57.1361850CurrentTrain: epoch  0, batch    53 | loss: 110.9592761CurrentTrain: epoch  0, batch    54 | loss: 76.2825260CurrentTrain: epoch  0, batch    55 | loss: 45.4895997CurrentTrain: epoch  0, batch    56 | loss: 56.1107648CurrentTrain: epoch  0, batch    57 | loss: 56.8283411CurrentTrain: epoch  0, batch    58 | loss: 112.1461042CurrentTrain: epoch  0, batch    59 | loss: 111.5255678CurrentTrain: epoch  0, batch    60 | loss: 45.6553145CurrentTrain: epoch  0, batch    61 | loss: 45.7157114CurrentTrain: epoch  0, batch    62 | loss: 57.0398741CurrentTrain: epoch  0, batch    63 | loss: 38.9128232CurrentTrain: epoch  0, batch    64 | loss: 111.5390980CurrentTrain: epoch  0, batch    65 | loss: 47.3304154CurrentTrain: epoch  0, batch    66 | loss: 45.0915379CurrentTrain: epoch  0, batch    67 | loss: 112.0084755CurrentTrain: epoch  0, batch    68 | loss: 45.7682336CurrentTrain: epoch  0, batch    69 | loss: 73.9158391CurrentTrain: epoch  0, batch    70 | loss: 33.9884323CurrentTrain: epoch  0, batch    71 | loss: 74.1144754CurrentTrain: epoch  0, batch    72 | loss: 74.8097366CurrentTrain: epoch  0, batch    73 | loss: 109.3612794CurrentTrain: epoch  0, batch    74 | loss: 74.4563879CurrentTrain: epoch  0, batch    75 | loss: 44.9217435CurrentTrain: epoch  0, batch    76 | loss: 37.6136733CurrentTrain: epoch  0, batch    77 | loss: 110.9711278CurrentTrain: epoch  0, batch    78 | loss: 57.9293164CurrentTrain: epoch  0, batch    79 | loss: 37.7610140CurrentTrain: epoch  0, batch    80 | loss: 55.8792423CurrentTrain: epoch  0, batch    81 | loss: 38.7915257CurrentTrain: epoch  0, batch    82 | loss: 44.8192936CurrentTrain: epoch  0, batch    83 | loss: 57.0320837CurrentTrain: epoch  0, batch    84 | loss: 45.8066349CurrentTrain: epoch  0, batch    85 | loss: 73.0457347CurrentTrain: epoch  0, batch    86 | loss: 56.3671735CurrentTrain: epoch  0, batch    87 | loss: 54.7269841CurrentTrain: epoch  0, batch    88 | loss: 55.6729050CurrentTrain: epoch  0, batch    89 | loss: 53.9221552CurrentTrain: epoch  0, batch    90 | loss: 75.3183007CurrentTrain: epoch  0, batch    91 | loss: 53.6277476CurrentTrain: epoch  0, batch    92 | loss: 45.8420458CurrentTrain: epoch  0, batch    93 | loss: 54.2671810CurrentTrain: epoch  0, batch    94 | loss: 36.1636218CurrentTrain: epoch  0, batch    95 | loss: 54.3513314CurrentTrain: epoch  0, batch    96 | loss: 222.6309446CurrentTrain: epoch  0, batch    97 | loss: 33.0328358CurrentTrain: epoch  0, batch    98 | loss: 73.2769471CurrentTrain: epoch  0, batch    99 | loss: 43.2869565CurrentTrain: epoch  0, batch   100 | loss: 44.0008646CurrentTrain: epoch  0, batch   101 | loss: 31.3812419CurrentTrain: epoch  0, batch   102 | loss: 58.3580318CurrentTrain: epoch  0, batch   103 | loss: 76.4072406CurrentTrain: epoch  0, batch   104 | loss: 71.6445131CurrentTrain: epoch  0, batch   105 | loss: 44.9532681CurrentTrain: epoch  0, batch   106 | loss: 52.0538025CurrentTrain: epoch  0, batch   107 | loss: 54.7443418CurrentTrain: epoch  0, batch   108 | loss: 75.1352625CurrentTrain: epoch  0, batch   109 | loss: 55.4532996CurrentTrain: epoch  0, batch   110 | loss: 45.3360459CurrentTrain: epoch  0, batch   111 | loss: 73.3299770CurrentTrain: epoch  0, batch   112 | loss: 55.4995431CurrentTrain: epoch  0, batch   113 | loss: 43.4609795CurrentTrain: epoch  0, batch   114 | loss: 52.6582420CurrentTrain: epoch  0, batch   115 | loss: 72.3183611CurrentTrain: epoch  0, batch   116 | loss: 53.2382860CurrentTrain: epoch  0, batch   117 | loss: 108.7168328CurrentTrain: epoch  0, batch   118 | loss: 75.5205625CurrentTrain: epoch  0, batch   119 | loss: 54.7255278CurrentTrain: epoch  0, batch   120 | loss: 55.1090713CurrentTrain: epoch  0, batch   121 | loss: 72.9306719CurrentTrain: epoch  0, batch   122 | loss: 51.7373323CurrentTrain: epoch  0, batch   123 | loss: 36.1063016CurrentTrain: epoch  0, batch   124 | loss: 75.5435335CurrentTrain: epoch  0, batch   125 | loss: 110.1990939CurrentTrain: epoch  0, batch   126 | loss: 55.5826279CurrentTrain: epoch  0, batch   127 | loss: 56.6040986CurrentTrain: epoch  0, batch   128 | loss: 75.4533352CurrentTrain: epoch  0, batch   129 | loss: 57.8999674CurrentTrain: epoch  0, batch   130 | loss: 56.9544368CurrentTrain: epoch  0, batch   131 | loss: 72.0593429CurrentTrain: epoch  0, batch   132 | loss: 110.7447133CurrentTrain: epoch  0, batch   133 | loss: 35.4688275CurrentTrain: epoch  0, batch   134 | loss: 73.5576437CurrentTrain: epoch  0, batch   135 | loss: 71.7052080CurrentTrain: epoch  0, batch   136 | loss: 42.9940956CurrentTrain: epoch  0, batch   137 | loss: 72.7148661CurrentTrain: epoch  0, batch   138 | loss: 73.0067941CurrentTrain: epoch  0, batch   139 | loss: 36.5718911CurrentTrain: epoch  0, batch   140 | loss: 34.6535197CurrentTrain: epoch  0, batch   141 | loss: 42.7668668CurrentTrain: epoch  0, batch   142 | loss: 35.0199985CurrentTrain: epoch  0, batch   143 | loss: 56.3937776CurrentTrain: epoch  1, batch     0 | loss: 42.5756873CurrentTrain: epoch  1, batch     1 | loss: 54.0137686CurrentTrain: epoch  1, batch     2 | loss: 69.7309717CurrentTrain: epoch  1, batch     3 | loss: 40.4663662CurrentTrain: epoch  1, batch     4 | loss: 109.6967973CurrentTrain: epoch  1, batch     5 | loss: 52.7768154CurrentTrain: epoch  1, batch     6 | loss: 36.0066755CurrentTrain: epoch  1, batch     7 | loss: 71.2360979CurrentTrain: epoch  1, batch     8 | loss: 40.9285037CurrentTrain: epoch  1, batch     9 | loss: 53.7734934CurrentTrain: epoch  1, batch    10 | loss: 53.0737455CurrentTrain: epoch  1, batch    11 | loss: 73.0514318CurrentTrain: epoch  1, batch    12 | loss: 71.8829647CurrentTrain: epoch  1, batch    13 | loss: 50.7513332CurrentTrain: epoch  1, batch    14 | loss: 42.5220093CurrentTrain: epoch  1, batch    15 | loss: 43.0505134CurrentTrain: epoch  1, batch    16 | loss: 41.5151895CurrentTrain: epoch  1, batch    17 | loss: 43.5241756CurrentTrain: epoch  1, batch    18 | loss: 71.9252797CurrentTrain: epoch  1, batch    19 | loss: 51.9450114CurrentTrain: epoch  1, batch    20 | loss: 108.2957660CurrentTrain: epoch  1, batch    21 | loss: 51.0658543CurrentTrain: epoch  1, batch    22 | loss: 41.6623845CurrentTrain: epoch  1, batch    23 | loss: 39.9359403CurrentTrain: epoch  1, batch    24 | loss: 52.1873824CurrentTrain: epoch  1, batch    25 | loss: 40.7262342CurrentTrain: epoch  1, batch    26 | loss: 73.2637528CurrentTrain: epoch  1, batch    27 | loss: 73.0220156CurrentTrain: epoch  1, batch    28 | loss: 52.4120886CurrentTrain: epoch  1, batch    29 | loss: 107.4891441CurrentTrain: epoch  1, batch    30 | loss: 41.9096801CurrentTrain: epoch  1, batch    31 | loss: 50.0877807CurrentTrain: epoch  1, batch    32 | loss: 72.5060599CurrentTrain: epoch  1, batch    33 | loss: 40.3869282CurrentTrain: epoch  1, batch    34 | loss: 50.6205520CurrentTrain: epoch  1, batch    35 | loss: 33.0657273CurrentTrain: epoch  1, batch    36 | loss: 71.1443031CurrentTrain: epoch  1, batch    37 | loss: 70.8520849CurrentTrain: epoch  1, batch    38 | loss: 69.0815838CurrentTrain: epoch  1, batch    39 | loss: 53.3655505CurrentTrain: epoch  1, batch    40 | loss: 38.6534781CurrentTrain: epoch  1, batch    41 | loss: 53.0438953CurrentTrain: epoch  1, batch    42 | loss: 70.0227376CurrentTrain: epoch  1, batch    43 | loss: 51.3687186CurrentTrain: epoch  1, batch    44 | loss: 53.3709536CurrentTrain: epoch  1, batch    45 | loss: 72.0142149CurrentTrain: epoch  1, batch    46 | loss: 68.4079134CurrentTrain: epoch  1, batch    47 | loss: 55.3150723CurrentTrain: epoch  1, batch    48 | loss: 50.8715118CurrentTrain: epoch  1, batch    49 | loss: 52.5760637CurrentTrain: epoch  1, batch    50 | loss: 43.3874966CurrentTrain: epoch  1, batch    51 | loss: 110.8926164CurrentTrain: epoch  1, batch    52 | loss: 39.9760050CurrentTrain: epoch  1, batch    53 | loss: 68.8225218CurrentTrain: epoch  1, batch    54 | loss: 43.5662899CurrentTrain: epoch  1, batch    55 | loss: 105.5650815CurrentTrain: epoch  1, batch    56 | loss: 51.4265508CurrentTrain: epoch  1, batch    57 | loss: 31.1118705CurrentTrain: epoch  1, batch    58 | loss: 71.2281747CurrentTrain: epoch  1, batch    59 | loss: 70.9064465CurrentTrain: epoch  1, batch    60 | loss: 41.3116919CurrentTrain: epoch  1, batch    61 | loss: 39.9624672CurrentTrain: epoch  1, batch    62 | loss: 53.1870572CurrentTrain: epoch  1, batch    63 | loss: 33.8290441CurrentTrain: epoch  1, batch    64 | loss: 40.4869000CurrentTrain: epoch  1, batch    65 | loss: 52.6696830CurrentTrain: epoch  1, batch    66 | loss: 37.8019922CurrentTrain: epoch  1, batch    67 | loss: 74.0168988CurrentTrain: epoch  1, batch    68 | loss: 34.4453354CurrentTrain: epoch  1, batch    69 | loss: 46.1752956CurrentTrain: epoch  1, batch    70 | loss: 49.8333764CurrentTrain: epoch  1, batch    71 | loss: 53.9674574CurrentTrain: epoch  1, batch    72 | loss: 39.4213221CurrentTrain: epoch  1, batch    73 | loss: 42.4877318CurrentTrain: epoch  1, batch    74 | loss: 41.5665392CurrentTrain: epoch  1, batch    75 | loss: 111.6102261CurrentTrain: epoch  1, batch    76 | loss: 109.2165107CurrentTrain: epoch  1, batch    77 | loss: 49.2407063CurrentTrain: epoch  1, batch    78 | loss: 53.3877135CurrentTrain: epoch  1, batch    79 | loss: 73.1118292CurrentTrain: epoch  1, batch    80 | loss: 49.6315952CurrentTrain: epoch  1, batch    81 | loss: 33.3933360CurrentTrain: epoch  1, batch    82 | loss: 50.9391825CurrentTrain: epoch  1, batch    83 | loss: 32.2704885CurrentTrain: epoch  1, batch    84 | loss: 50.8246067CurrentTrain: epoch  1, batch    85 | loss: 222.4753332CurrentTrain: epoch  1, batch    86 | loss: 71.1065269CurrentTrain: epoch  1, batch    87 | loss: 72.9790342CurrentTrain: epoch  1, batch    88 | loss: 68.9470507CurrentTrain: epoch  1, batch    89 | loss: 108.4851479CurrentTrain: epoch  1, batch    90 | loss: 71.1054842CurrentTrain: epoch  1, batch    91 | loss: 41.3432147CurrentTrain: epoch  1, batch    92 | loss: 44.2450599CurrentTrain: epoch  1, batch    93 | loss: 48.8062365CurrentTrain: epoch  1, batch    94 | loss: 40.4360157CurrentTrain: epoch  1, batch    95 | loss: 54.7412081CurrentTrain: epoch  1, batch    96 | loss: 52.6637796CurrentTrain: epoch  1, batch    97 | loss: 109.6627878CurrentTrain: epoch  1, batch    98 | loss: 37.3058937CurrentTrain: epoch  1, batch    99 | loss: 34.0440644CurrentTrain: epoch  1, batch   100 | loss: 48.7411561CurrentTrain: epoch  1, batch   101 | loss: 39.0497253CurrentTrain: epoch  1, batch   102 | loss: 71.9747320CurrentTrain: epoch  1, batch   103 | loss: 67.0509178CurrentTrain: epoch  1, batch   104 | loss: 70.7554299CurrentTrain: epoch  1, batch   105 | loss: 46.2165565CurrentTrain: epoch  1, batch   106 | loss: 63.7480710CurrentTrain: epoch  1, batch   107 | loss: 42.4493866CurrentTrain: epoch  1, batch   108 | loss: 40.2494253CurrentTrain: epoch  1, batch   109 | loss: 42.5613543CurrentTrain: epoch  1, batch   110 | loss: 72.5184185CurrentTrain: epoch  1, batch   111 | loss: 40.2135943CurrentTrain: epoch  1, batch   112 | loss: 50.1393164CurrentTrain: epoch  1, batch   113 | loss: 32.4235545CurrentTrain: epoch  1, batch   114 | loss: 70.2080207CurrentTrain: epoch  1, batch   115 | loss: 31.4136182CurrentTrain: epoch  1, batch   116 | loss: 54.1116157CurrentTrain: epoch  1, batch   117 | loss: 69.1050307CurrentTrain: epoch  1, batch   118 | loss: 72.3364528CurrentTrain: epoch  1, batch   119 | loss: 53.0594986CurrentTrain: epoch  1, batch   120 | loss: 32.0064095CurrentTrain: epoch  1, batch   121 | loss: 48.8051816CurrentTrain: epoch  1, batch   122 | loss: 39.7677409CurrentTrain: epoch  1, batch   123 | loss: 50.4587545CurrentTrain: epoch  1, batch   124 | loss: 71.8957520CurrentTrain: epoch  1, batch   125 | loss: 52.1572332CurrentTrain: epoch  1, batch   126 | loss: 70.0298874CurrentTrain: epoch  1, batch   127 | loss: 73.8300275CurrentTrain: epoch  1, batch   128 | loss: 40.9992174CurrentTrain: epoch  1, batch   129 | loss: 70.7111485CurrentTrain: epoch  1, batch   130 | loss: 38.9348311CurrentTrain: epoch  1, batch   131 | loss: 49.6131517CurrentTrain: epoch  1, batch   132 | loss: 53.1940824CurrentTrain: epoch  1, batch   133 | loss: 73.9984110CurrentTrain: epoch  1, batch   134 | loss: 30.2502583CurrentTrain: epoch  1, batch   135 | loss: 72.8096797CurrentTrain: epoch  1, batch   136 | loss: 38.5840325CurrentTrain: epoch  1, batch   137 | loss: 52.1449456CurrentTrain: epoch  1, batch   138 | loss: 32.2887672CurrentTrain: epoch  1, batch   139 | loss: 49.4603413CurrentTrain: epoch  1, batch   140 | loss: 51.0992577CurrentTrain: epoch  1, batch   141 | loss: 38.5020865CurrentTrain: epoch  1, batch   142 | loss: 39.7295070CurrentTrain: epoch  1, batch   143 | loss: 51.7561761CurrentTrain: epoch  2, batch     0 | loss: 48.5540989CurrentTrain: epoch  2, batch     1 | loss: 104.4787491CurrentTrain: epoch  2, batch     2 | loss: 39.2999903CurrentTrain: epoch  2, batch     3 | loss: 31.7344584CurrentTrain: epoch  2, batch     4 | loss: 71.1753788CurrentTrain: epoch  2, batch     5 | loss: 74.7118174CurrentTrain: epoch  2, batch     6 | loss: 53.0817529CurrentTrain: epoch  2, batch     7 | loss: 50.4196837CurrentTrain: epoch  2, batch     8 | loss: 40.5472642CurrentTrain: epoch  2, batch     9 | loss: 109.6676520CurrentTrain: epoch  2, batch    10 | loss: 48.5231219CurrentTrain: epoch  2, batch    11 | loss: 24.6179633CurrentTrain: epoch  2, batch    12 | loss: 68.1715277CurrentTrain: epoch  2, batch    13 | loss: 46.3053741CurrentTrain: epoch  2, batch    14 | loss: 67.7700133CurrentTrain: epoch  2, batch    15 | loss: 50.2919967CurrentTrain: epoch  2, batch    16 | loss: 73.7174754CurrentTrain: epoch  2, batch    17 | loss: 37.2341513CurrentTrain: epoch  2, batch    18 | loss: 40.3257110CurrentTrain: epoch  2, batch    19 | loss: 71.9801583CurrentTrain: epoch  2, batch    20 | loss: 68.4412610CurrentTrain: epoch  2, batch    21 | loss: 111.7035288CurrentTrain: epoch  2, batch    22 | loss: 107.5989651CurrentTrain: epoch  2, batch    23 | loss: 36.3039692CurrentTrain: epoch  2, batch    24 | loss: 50.4828658CurrentTrain: epoch  2, batch    25 | loss: 49.5204480CurrentTrain: epoch  2, batch    26 | loss: 52.1500602CurrentTrain: epoch  2, batch    27 | loss: 47.3698561CurrentTrain: epoch  2, batch    28 | loss: 70.6535478CurrentTrain: epoch  2, batch    29 | loss: 37.3759278CurrentTrain: epoch  2, batch    30 | loss: 32.3559241CurrentTrain: epoch  2, batch    31 | loss: 70.0622683CurrentTrain: epoch  2, batch    32 | loss: 53.7004695CurrentTrain: epoch  2, batch    33 | loss: 42.6532456CurrentTrain: epoch  2, batch    34 | loss: 49.0296863CurrentTrain: epoch  2, batch    35 | loss: 107.1784139CurrentTrain: epoch  2, batch    36 | loss: 65.4615431CurrentTrain: epoch  2, batch    37 | loss: 109.9785333CurrentTrain: epoch  2, batch    38 | loss: 69.8184440CurrentTrain: epoch  2, batch    39 | loss: 47.5829245CurrentTrain: epoch  2, batch    40 | loss: 108.3854424CurrentTrain: epoch  2, batch    41 | loss: 64.9526466CurrentTrain: epoch  2, batch    42 | loss: 67.4733747CurrentTrain: epoch  2, batch    43 | loss: 49.9917417CurrentTrain: epoch  2, batch    44 | loss: 52.9183332CurrentTrain: epoch  2, batch    45 | loss: 41.0337843CurrentTrain: epoch  2, batch    46 | loss: 51.1565550CurrentTrain: epoch  2, batch    47 | loss: 70.0790849CurrentTrain: epoch  2, batch    48 | loss: 42.3429548CurrentTrain: epoch  2, batch    49 | loss: 68.6383062CurrentTrain: epoch  2, batch    50 | loss: 53.8508016CurrentTrain: epoch  2, batch    51 | loss: 33.0495126CurrentTrain: epoch  2, batch    52 | loss: 56.6605076CurrentTrain: epoch  2, batch    53 | loss: 31.7900845CurrentTrain: epoch  2, batch    54 | loss: 104.3353850CurrentTrain: epoch  2, batch    55 | loss: 37.9369127CurrentTrain: epoch  2, batch    56 | loss: 35.7391411CurrentTrain: epoch  2, batch    57 | loss: 73.7581190CurrentTrain: epoch  2, batch    58 | loss: 48.7570501CurrentTrain: epoch  2, batch    59 | loss: 73.0496099CurrentTrain: epoch  2, batch    60 | loss: 50.6523361CurrentTrain: epoch  2, batch    61 | loss: 36.4508868CurrentTrain: epoch  2, batch    62 | loss: 66.3486886CurrentTrain: epoch  2, batch    63 | loss: 104.6170500CurrentTrain: epoch  2, batch    64 | loss: 33.8134767CurrentTrain: epoch  2, batch    65 | loss: 48.9753856CurrentTrain: epoch  2, batch    66 | loss: 51.4142486CurrentTrain: epoch  2, batch    67 | loss: 51.3026712CurrentTrain: epoch  2, batch    68 | loss: 222.2780977CurrentTrain: epoch  2, batch    69 | loss: 37.0492423CurrentTrain: epoch  2, batch    70 | loss: 48.3499495CurrentTrain: epoch  2, batch    71 | loss: 32.7573314CurrentTrain: epoch  2, batch    72 | loss: 37.4730350CurrentTrain: epoch  2, batch    73 | loss: 52.4443448CurrentTrain: epoch  2, batch    74 | loss: 70.7279135CurrentTrain: epoch  2, batch    75 | loss: 51.7082000CurrentTrain: epoch  2, batch    76 | loss: 39.0055620CurrentTrain: epoch  2, batch    77 | loss: 49.9686800CurrentTrain: epoch  2, batch    78 | loss: 50.6789744CurrentTrain: epoch  2, batch    79 | loss: 37.3592339CurrentTrain: epoch  2, batch    80 | loss: 68.7102893CurrentTrain: epoch  2, batch    81 | loss: 50.5790654CurrentTrain: epoch  2, batch    82 | loss: 71.5117502CurrentTrain: epoch  2, batch    83 | loss: 39.7448182CurrentTrain: epoch  2, batch    84 | loss: 42.7503649CurrentTrain: epoch  2, batch    85 | loss: 38.7565061CurrentTrain: epoch  2, batch    86 | loss: 50.2002588CurrentTrain: epoch  2, batch    87 | loss: 40.3048090CurrentTrain: epoch  2, batch    88 | loss: 50.7491993CurrentTrain: epoch  2, batch    89 | loss: 40.8696911CurrentTrain: epoch  2, batch    90 | loss: 47.2373952CurrentTrain: epoch  2, batch    91 | loss: 73.6021950CurrentTrain: epoch  2, batch    92 | loss: 31.2610620CurrentTrain: epoch  2, batch    93 | loss: 25.9197770CurrentTrain: epoch  2, batch    94 | loss: 37.6689888CurrentTrain: epoch  2, batch    95 | loss: 34.2956782CurrentTrain: epoch  2, batch    96 | loss: 69.1902647CurrentTrain: epoch  2, batch    97 | loss: 67.8795536CurrentTrain: epoch  2, batch    98 | loss: 52.3126290CurrentTrain: epoch  2, batch    99 | loss: 36.8277806CurrentTrain: epoch  2, batch   100 | loss: 40.2857349CurrentTrain: epoch  2, batch   101 | loss: 51.1357185CurrentTrain: epoch  2, batch   102 | loss: 66.8201879CurrentTrain: epoch  2, batch   103 | loss: 68.9355469CurrentTrain: epoch  2, batch   104 | loss: 39.4303826CurrentTrain: epoch  2, batch   105 | loss: 33.3049262CurrentTrain: epoch  2, batch   106 | loss: 51.5821550CurrentTrain: epoch  2, batch   107 | loss: 63.3634683CurrentTrain: epoch  2, batch   108 | loss: 40.7137665CurrentTrain: epoch  2, batch   109 | loss: 51.0945616CurrentTrain: epoch  2, batch   110 | loss: 39.2831045CurrentTrain: epoch  2, batch   111 | loss: 41.5361099CurrentTrain: epoch  2, batch   112 | loss: 37.9889374CurrentTrain: epoch  2, batch   113 | loss: 107.1975424CurrentTrain: epoch  2, batch   114 | loss: 50.7145217CurrentTrain: epoch  2, batch   115 | loss: 44.9772973CurrentTrain: epoch  2, batch   116 | loss: 46.3755470CurrentTrain: epoch  2, batch   117 | loss: 39.9550741CurrentTrain: epoch  2, batch   118 | loss: 30.7680380CurrentTrain: epoch  2, batch   119 | loss: 47.9749969CurrentTrain: epoch  2, batch   120 | loss: 48.4306646CurrentTrain: epoch  2, batch   121 | loss: 109.8610619CurrentTrain: epoch  2, batch   122 | loss: 73.1396653CurrentTrain: epoch  2, batch   123 | loss: 37.1834722CurrentTrain: epoch  2, batch   124 | loss: 38.4013583CurrentTrain: epoch  2, batch   125 | loss: 36.2715855CurrentTrain: epoch  2, batch   126 | loss: 39.2580875CurrentTrain: epoch  2, batch   127 | loss: 35.3555843CurrentTrain: epoch  2, batch   128 | loss: 50.7434142CurrentTrain: epoch  2, batch   129 | loss: 74.8103733CurrentTrain: epoch  2, batch   130 | loss: 48.8687200CurrentTrain: epoch  2, batch   131 | loss: 38.6022694CurrentTrain: epoch  2, batch   132 | loss: 70.1694920CurrentTrain: epoch  2, batch   133 | loss: 71.6657702CurrentTrain: epoch  2, batch   134 | loss: 107.9348002CurrentTrain: epoch  2, batch   135 | loss: 70.6576054CurrentTrain: epoch  2, batch   136 | loss: 69.8917418CurrentTrain: epoch  2, batch   137 | loss: 69.5204816CurrentTrain: epoch  2, batch   138 | loss: 107.5898696CurrentTrain: epoch  2, batch   139 | loss: 46.6824780CurrentTrain: epoch  2, batch   140 | loss: 107.8708821CurrentTrain: epoch  2, batch   141 | loss: 51.7040469CurrentTrain: epoch  2, batch   142 | loss: 39.3355085CurrentTrain: epoch  2, batch   143 | loss: 43.0160996CurrentTrain: epoch  3, batch     0 | loss: 38.8965993CurrentTrain: epoch  3, batch     1 | loss: 107.0584012CurrentTrain: epoch  3, batch     2 | loss: 30.9728933CurrentTrain: epoch  3, batch     3 | loss: 53.0916679CurrentTrain: epoch  3, batch     4 | loss: 68.7094762CurrentTrain: epoch  3, batch     5 | loss: 49.2672231CurrentTrain: epoch  3, batch     6 | loss: 38.7596428CurrentTrain: epoch  3, batch     7 | loss: 51.5324204CurrentTrain: epoch  3, batch     8 | loss: 41.1609009CurrentTrain: epoch  3, batch     9 | loss: 67.9932774CurrentTrain: epoch  3, batch    10 | loss: 40.1622749CurrentTrain: epoch  3, batch    11 | loss: 46.9671152CurrentTrain: epoch  3, batch    12 | loss: 36.8002026CurrentTrain: epoch  3, batch    13 | loss: 45.3986558CurrentTrain: epoch  3, batch    14 | loss: 47.5713183CurrentTrain: epoch  3, batch    15 | loss: 37.8235112CurrentTrain: epoch  3, batch    16 | loss: 49.8631617CurrentTrain: epoch  3, batch    17 | loss: 69.5160663CurrentTrain: epoch  3, batch    18 | loss: 49.2745841CurrentTrain: epoch  3, batch    19 | loss: 49.2405767CurrentTrain: epoch  3, batch    20 | loss: 105.9982985CurrentTrain: epoch  3, batch    21 | loss: 66.5846770CurrentTrain: epoch  3, batch    22 | loss: 65.4924693CurrentTrain: epoch  3, batch    23 | loss: 37.0323878CurrentTrain: epoch  3, batch    24 | loss: 51.6993861CurrentTrain: epoch  3, batch    25 | loss: 39.5882476CurrentTrain: epoch  3, batch    26 | loss: 50.1154304CurrentTrain: epoch  3, batch    27 | loss: 66.8214222CurrentTrain: epoch  3, batch    28 | loss: 69.6087408CurrentTrain: epoch  3, batch    29 | loss: 67.0045691CurrentTrain: epoch  3, batch    30 | loss: 31.0578057CurrentTrain: epoch  3, batch    31 | loss: 48.5869221CurrentTrain: epoch  3, batch    32 | loss: 105.8341397CurrentTrain: epoch  3, batch    33 | loss: 39.1996440CurrentTrain: epoch  3, batch    34 | loss: 49.3607584CurrentTrain: epoch  3, batch    35 | loss: 37.2869825CurrentTrain: epoch  3, batch    36 | loss: 70.3961029CurrentTrain: epoch  3, batch    37 | loss: 48.6639848CurrentTrain: epoch  3, batch    38 | loss: 30.1713916CurrentTrain: epoch  3, batch    39 | loss: 107.2393373CurrentTrain: epoch  3, batch    40 | loss: 69.8400088CurrentTrain: epoch  3, batch    41 | loss: 71.0184923CurrentTrain: epoch  3, batch    42 | loss: 68.7874038CurrentTrain: epoch  3, batch    43 | loss: 37.4409097CurrentTrain: epoch  3, batch    44 | loss: 107.3467165CurrentTrain: epoch  3, batch    45 | loss: 50.4826646CurrentTrain: epoch  3, batch    46 | loss: 49.9793097CurrentTrain: epoch  3, batch    47 | loss: 50.8854862CurrentTrain: epoch  3, batch    48 | loss: 68.0817353CurrentTrain: epoch  3, batch    49 | loss: 33.1737306CurrentTrain: epoch  3, batch    50 | loss: 50.3706576CurrentTrain: epoch  3, batch    51 | loss: 49.4550778CurrentTrain: epoch  3, batch    52 | loss: 69.2961762CurrentTrain: epoch  3, batch    53 | loss: 51.7244958CurrentTrain: epoch  3, batch    54 | loss: 39.6596616CurrentTrain: epoch  3, batch    55 | loss: 40.1481501CurrentTrain: epoch  3, batch    56 | loss: 108.7248732CurrentTrain: epoch  3, batch    57 | loss: 101.7070499CurrentTrain: epoch  3, batch    58 | loss: 34.7480436CurrentTrain: epoch  3, batch    59 | loss: 65.7179339CurrentTrain: epoch  3, batch    60 | loss: 38.9841041CurrentTrain: epoch  3, batch    61 | loss: 38.5271256CurrentTrain: epoch  3, batch    62 | loss: 107.4918074CurrentTrain: epoch  3, batch    63 | loss: 68.5952847CurrentTrain: epoch  3, batch    64 | loss: 69.4925733CurrentTrain: epoch  3, batch    65 | loss: 69.8872424CurrentTrain: epoch  3, batch    66 | loss: 38.8299648CurrentTrain: epoch  3, batch    67 | loss: 40.8627186CurrentTrain: epoch  3, batch    68 | loss: 47.7620005CurrentTrain: epoch  3, batch    69 | loss: 71.9286134CurrentTrain: epoch  3, batch    70 | loss: 69.6891706CurrentTrain: epoch  3, batch    71 | loss: 38.0365446CurrentTrain: epoch  3, batch    72 | loss: 107.9121503CurrentTrain: epoch  3, batch    73 | loss: 30.8372468CurrentTrain: epoch  3, batch    74 | loss: 50.1152602CurrentTrain: epoch  3, batch    75 | loss: 107.6285182CurrentTrain: epoch  3, batch    76 | loss: 38.4882291CurrentTrain: epoch  3, batch    77 | loss: 36.8419323CurrentTrain: epoch  3, batch    78 | loss: 68.9868041CurrentTrain: epoch  3, batch    79 | loss: 46.6244749CurrentTrain: epoch  3, batch    80 | loss: 70.1629108CurrentTrain: epoch  3, batch    81 | loss: 49.9261979CurrentTrain: epoch  3, batch    82 | loss: 48.2500090CurrentTrain: epoch  3, batch    83 | loss: 32.0901150CurrentTrain: epoch  3, batch    84 | loss: 67.6141063CurrentTrain: epoch  3, batch    85 | loss: 37.0659268CurrentTrain: epoch  3, batch    86 | loss: 67.8671417CurrentTrain: epoch  3, batch    87 | loss: 39.4498939CurrentTrain: epoch  3, batch    88 | loss: 29.7988374CurrentTrain: epoch  3, batch    89 | loss: 38.9036862CurrentTrain: epoch  3, batch    90 | loss: 72.4692991CurrentTrain: epoch  3, batch    91 | loss: 30.0280182CurrentTrain: epoch  3, batch    92 | loss: 47.6244528CurrentTrain: epoch  3, batch    93 | loss: 65.6754890CurrentTrain: epoch  3, batch    94 | loss: 50.3082616CurrentTrain: epoch  3, batch    95 | loss: 49.6315371CurrentTrain: epoch  3, batch    96 | loss: 68.0150123CurrentTrain: epoch  3, batch    97 | loss: 37.2877106CurrentTrain: epoch  3, batch    98 | loss: 50.0733259CurrentTrain: epoch  3, batch    99 | loss: 48.2650586CurrentTrain: epoch  3, batch   100 | loss: 36.5228763CurrentTrain: epoch  3, batch   101 | loss: 34.6577603CurrentTrain: epoch  3, batch   102 | loss: 69.4167995CurrentTrain: epoch  3, batch   103 | loss: 73.2769468CurrentTrain: epoch  3, batch   104 | loss: 49.4826684CurrentTrain: epoch  3, batch   105 | loss: 48.1488179CurrentTrain: epoch  3, batch   106 | loss: 38.6114779CurrentTrain: epoch  3, batch   107 | loss: 64.6924949CurrentTrain: epoch  3, batch   108 | loss: 39.8831270CurrentTrain: epoch  3, batch   109 | loss: 37.7165003CurrentTrain: epoch  3, batch   110 | loss: 66.8223698CurrentTrain: epoch  3, batch   111 | loss: 50.0130235CurrentTrain: epoch  3, batch   112 | loss: 36.9998303CurrentTrain: epoch  3, batch   113 | loss: 48.3056402CurrentTrain: epoch  3, batch   114 | loss: 31.4899503CurrentTrain: epoch  3, batch   115 | loss: 70.4173630CurrentTrain: epoch  3, batch   116 | loss: 69.3122193CurrentTrain: epoch  3, batch   117 | loss: 49.8832463CurrentTrain: epoch  3, batch   118 | loss: 49.2127065CurrentTrain: epoch  3, batch   119 | loss: 49.9627379CurrentTrain: epoch  3, batch   120 | loss: 34.6036793CurrentTrain: epoch  3, batch   121 | loss: 46.8228819CurrentTrain: epoch  3, batch   122 | loss: 37.1444276CurrentTrain: epoch  3, batch   123 | loss: 68.4344321CurrentTrain: epoch  3, batch   124 | loss: 39.4080775CurrentTrain: epoch  3, batch   125 | loss: 29.7562283CurrentTrain: epoch  3, batch   126 | loss: 48.2040734CurrentTrain: epoch  3, batch   127 | loss: 30.7646150CurrentTrain: epoch  3, batch   128 | loss: 53.9641024CurrentTrain: epoch  3, batch   129 | loss: 68.2942797CurrentTrain: epoch  3, batch   130 | loss: 48.7861338CurrentTrain: epoch  3, batch   131 | loss: 28.8119750CurrentTrain: epoch  3, batch   132 | loss: 39.7777337CurrentTrain: epoch  3, batch   133 | loss: 53.6295171CurrentTrain: epoch  3, batch   134 | loss: 52.1823599CurrentTrain: epoch  3, batch   135 | loss: 47.3381066CurrentTrain: epoch  3, batch   136 | loss: 37.7664081CurrentTrain: epoch  3, batch   137 | loss: 68.9674896CurrentTrain: epoch  3, batch   138 | loss: 106.7819678CurrentTrain: epoch  3, batch   139 | loss: 40.1453499CurrentTrain: epoch  3, batch   140 | loss: 39.5200468CurrentTrain: epoch  3, batch   141 | loss: 33.1832552CurrentTrain: epoch  3, batch   142 | loss: 29.6508842CurrentTrain: epoch  3, batch   143 | loss: 36.7210729CurrentTrain: epoch  4, batch     0 | loss: 66.6499166CurrentTrain: epoch  4, batch     1 | loss: 68.6064555CurrentTrain: epoch  4, batch     2 | loss: 50.4866520CurrentTrain: epoch  4, batch     3 | loss: 49.0809162CurrentTrain: epoch  4, batch     4 | loss: 36.3911210CurrentTrain: epoch  4, batch     5 | loss: 49.7535736CurrentTrain: epoch  4, batch     6 | loss: 68.4338363CurrentTrain: epoch  4, batch     7 | loss: 38.2470387CurrentTrain: epoch  4, batch     8 | loss: 50.8351549CurrentTrain: epoch  4, batch     9 | loss: 47.7375641CurrentTrain: epoch  4, batch    10 | loss: 67.3907490CurrentTrain: epoch  4, batch    11 | loss: 26.6992620CurrentTrain: epoch  4, batch    12 | loss: 49.0994300CurrentTrain: epoch  4, batch    13 | loss: 68.7797452CurrentTrain: epoch  4, batch    14 | loss: 24.5190350CurrentTrain: epoch  4, batch    15 | loss: 47.4407130CurrentTrain: epoch  4, batch    16 | loss: 108.5751646CurrentTrain: epoch  4, batch    17 | loss: 48.3886306CurrentTrain: epoch  4, batch    18 | loss: 35.1350397CurrentTrain: epoch  4, batch    19 | loss: 30.5731785CurrentTrain: epoch  4, batch    20 | loss: 37.2718806CurrentTrain: epoch  4, batch    21 | loss: 69.0986636CurrentTrain: epoch  4, batch    22 | loss: 46.5524169CurrentTrain: epoch  4, batch    23 | loss: 48.0280815CurrentTrain: epoch  4, batch    24 | loss: 68.3829428CurrentTrain: epoch  4, batch    25 | loss: 39.4685053CurrentTrain: epoch  4, batch    26 | loss: 47.1347343CurrentTrain: epoch  4, batch    27 | loss: 36.5254215CurrentTrain: epoch  4, batch    28 | loss: 69.1171521CurrentTrain: epoch  4, batch    29 | loss: 48.2198401CurrentTrain: epoch  4, batch    30 | loss: 69.0392777CurrentTrain: epoch  4, batch    31 | loss: 47.3561392CurrentTrain: epoch  4, batch    32 | loss: 107.1373615CurrentTrain: epoch  4, batch    33 | loss: 222.2042164CurrentTrain: epoch  4, batch    34 | loss: 36.8152163CurrentTrain: epoch  4, batch    35 | loss: 222.3013054CurrentTrain: epoch  4, batch    36 | loss: 222.2243312CurrentTrain: epoch  4, batch    37 | loss: 38.9331568CurrentTrain: epoch  4, batch    38 | loss: 39.2698796CurrentTrain: epoch  4, batch    39 | loss: 48.5514107CurrentTrain: epoch  4, batch    40 | loss: 49.0871570CurrentTrain: epoch  4, batch    41 | loss: 28.5402217CurrentTrain: epoch  4, batch    42 | loss: 35.5475124CurrentTrain: epoch  4, batch    43 | loss: 69.1308802CurrentTrain: epoch  4, batch    44 | loss: 70.1987790CurrentTrain: epoch  4, batch    45 | loss: 29.9240879CurrentTrain: epoch  4, batch    46 | loss: 48.2892229CurrentTrain: epoch  4, batch    47 | loss: 30.1466343CurrentTrain: epoch  4, batch    48 | loss: 36.7153891CurrentTrain: epoch  4, batch    49 | loss: 64.5722292CurrentTrain: epoch  4, batch    50 | loss: 66.2975152CurrentTrain: epoch  4, batch    51 | loss: 49.0657668CurrentTrain: epoch  4, batch    52 | loss: 39.0397919CurrentTrain: epoch  4, batch    53 | loss: 29.7553950CurrentTrain: epoch  4, batch    54 | loss: 49.4268424CurrentTrain: epoch  4, batch    55 | loss: 47.9598033CurrentTrain: epoch  4, batch    56 | loss: 46.9757593CurrentTrain: epoch  4, batch    57 | loss: 47.3534288CurrentTrain: epoch  4, batch    58 | loss: 49.1760748CurrentTrain: epoch  4, batch    59 | loss: 38.2721197CurrentTrain: epoch  4, batch    60 | loss: 101.3623301CurrentTrain: epoch  4, batch    61 | loss: 47.7108802CurrentTrain: epoch  4, batch    62 | loss: 65.9237367CurrentTrain: epoch  4, batch    63 | loss: 66.2423895CurrentTrain: epoch  4, batch    64 | loss: 48.2172718CurrentTrain: epoch  4, batch    65 | loss: 27.8174084CurrentTrain: epoch  4, batch    66 | loss: 38.4237735CurrentTrain: epoch  4, batch    67 | loss: 66.1993606CurrentTrain: epoch  4, batch    68 | loss: 49.9143096CurrentTrain: epoch  4, batch    69 | loss: 36.3966738CurrentTrain: epoch  4, batch    70 | loss: 45.9306757CurrentTrain: epoch  4, batch    71 | loss: 46.8140362CurrentTrain: epoch  4, batch    72 | loss: 49.1977359CurrentTrain: epoch  4, batch    73 | loss: 69.0712055CurrentTrain: epoch  4, batch    74 | loss: 108.1960000CurrentTrain: epoch  4, batch    75 | loss: 40.1682591CurrentTrain: epoch  4, batch    76 | loss: 66.8502138CurrentTrain: epoch  4, batch    77 | loss: 37.1567803CurrentTrain: epoch  4, batch    78 | loss: 49.0956573CurrentTrain: epoch  4, batch    79 | loss: 44.5410186CurrentTrain: epoch  4, batch    80 | loss: 107.1314488CurrentTrain: epoch  4, batch    81 | loss: 37.3218073CurrentTrain: epoch  4, batch    82 | loss: 66.1550055CurrentTrain: epoch  4, batch    83 | loss: 50.6683291CurrentTrain: epoch  4, batch    84 | loss: 36.8823689CurrentTrain: epoch  4, batch    85 | loss: 68.6552524CurrentTrain: epoch  4, batch    86 | loss: 70.2804327CurrentTrain: epoch  4, batch    87 | loss: 103.6857146CurrentTrain: epoch  4, batch    88 | loss: 26.6663299CurrentTrain: epoch  4, batch    89 | loss: 50.0336895CurrentTrain: epoch  4, batch    90 | loss: 103.9141093CurrentTrain: epoch  4, batch    91 | loss: 51.4469768CurrentTrain: epoch  4, batch    92 | loss: 38.4775806CurrentTrain: epoch  4, batch    93 | loss: 50.3046305CurrentTrain: epoch  4, batch    94 | loss: 47.6573830CurrentTrain: epoch  4, batch    95 | loss: 37.5124831CurrentTrain: epoch  4, batch    96 | loss: 35.3635003CurrentTrain: epoch  4, batch    97 | loss: 65.0479907CurrentTrain: epoch  4, batch    98 | loss: 64.7911296CurrentTrain: epoch  4, batch    99 | loss: 66.6106037CurrentTrain: epoch  4, batch   100 | loss: 65.4423757CurrentTrain: epoch  4, batch   101 | loss: 69.3646361CurrentTrain: epoch  4, batch   102 | loss: 48.2075736CurrentTrain: epoch  4, batch   103 | loss: 35.2813584CurrentTrain: epoch  4, batch   104 | loss: 49.7905791CurrentTrain: epoch  4, batch   105 | loss: 106.8915381CurrentTrain: epoch  4, batch   106 | loss: 40.3297251CurrentTrain: epoch  4, batch   107 | loss: 36.7296060CurrentTrain: epoch  4, batch   108 | loss: 41.0022870CurrentTrain: epoch  4, batch   109 | loss: 49.2161226CurrentTrain: epoch  4, batch   110 | loss: 37.8900274CurrentTrain: epoch  4, batch   111 | loss: 65.3071663CurrentTrain: epoch  4, batch   112 | loss: 27.7139442CurrentTrain: epoch  4, batch   113 | loss: 107.5431376CurrentTrain: epoch  4, batch   114 | loss: 45.2570769CurrentTrain: epoch  4, batch   115 | loss: 65.1381874CurrentTrain: epoch  4, batch   116 | loss: 70.2935676CurrentTrain: epoch  4, batch   117 | loss: 47.1777828CurrentTrain: epoch  4, batch   118 | loss: 49.9201841CurrentTrain: epoch  4, batch   119 | loss: 103.9857112CurrentTrain: epoch  4, batch   120 | loss: 71.1961471CurrentTrain: epoch  4, batch   121 | loss: 32.6937139CurrentTrain: epoch  4, batch   122 | loss: 50.5199518CurrentTrain: epoch  4, batch   123 | loss: 68.7647924CurrentTrain: epoch  4, batch   124 | loss: 45.9044509CurrentTrain: epoch  4, batch   125 | loss: 47.4483359CurrentTrain: epoch  4, batch   126 | loss: 106.9407725CurrentTrain: epoch  4, batch   127 | loss: 107.0513582CurrentTrain: epoch  4, batch   128 | loss: 36.1462039CurrentTrain: epoch  4, batch   129 | loss: 107.3238664CurrentTrain: epoch  4, batch   130 | loss: 49.8389410CurrentTrain: epoch  4, batch   131 | loss: 28.9826703CurrentTrain: epoch  4, batch   132 | loss: 49.1145847CurrentTrain: epoch  4, batch   133 | loss: 37.5071727CurrentTrain: epoch  4, batch   134 | loss: 38.5646754CurrentTrain: epoch  4, batch   135 | loss: 35.5255495CurrentTrain: epoch  4, batch   136 | loss: 36.2522496CurrentTrain: epoch  4, batch   137 | loss: 37.8568889CurrentTrain: epoch  4, batch   138 | loss: 48.9492412CurrentTrain: epoch  4, batch   139 | loss: 30.2151459CurrentTrain: epoch  4, batch   140 | loss: 47.5088430CurrentTrain: epoch  4, batch   141 | loss: 30.4555135CurrentTrain: epoch  4, batch   142 | loss: 67.1802669CurrentTrain: epoch  4, batch   143 | loss: 25.9095231CurrentTrain: epoch  5, batch     0 | loss: 31.3462304CurrentTrain: epoch  5, batch     1 | loss: 49.9286771CurrentTrain: epoch  5, batch     2 | loss: 66.1802791CurrentTrain: epoch  5, batch     3 | loss: 37.9816786CurrentTrain: epoch  5, batch     4 | loss: 49.9545865CurrentTrain: epoch  5, batch     5 | loss: 64.7239063CurrentTrain: epoch  5, batch     6 | loss: 67.8351754CurrentTrain: epoch  5, batch     7 | loss: 48.3030059CurrentTrain: epoch  5, batch     8 | loss: 37.8806946CurrentTrain: epoch  5, batch     9 | loss: 29.0278637CurrentTrain: epoch  5, batch    10 | loss: 47.7025544CurrentTrain: epoch  5, batch    11 | loss: 38.6573954CurrentTrain: epoch  5, batch    12 | loss: 34.8044449CurrentTrain: epoch  5, batch    13 | loss: 36.5690704CurrentTrain: epoch  5, batch    14 | loss: 48.4135250CurrentTrain: epoch  5, batch    15 | loss: 68.8105277CurrentTrain: epoch  5, batch    16 | loss: 68.4167174CurrentTrain: epoch  5, batch    17 | loss: 46.2983292CurrentTrain: epoch  5, batch    18 | loss: 66.7027914CurrentTrain: epoch  5, batch    19 | loss: 68.4151343CurrentTrain: epoch  5, batch    20 | loss: 37.8126545CurrentTrain: epoch  5, batch    21 | loss: 49.1453206CurrentTrain: epoch  5, batch    22 | loss: 49.2776046CurrentTrain: epoch  5, batch    23 | loss: 68.4447269CurrentTrain: epoch  5, batch    24 | loss: 39.9180192CurrentTrain: epoch  5, batch    25 | loss: 66.3322725CurrentTrain: epoch  5, batch    26 | loss: 48.4298604CurrentTrain: epoch  5, batch    27 | loss: 36.3492010CurrentTrain: epoch  5, batch    28 | loss: 47.3472997CurrentTrain: epoch  5, batch    29 | loss: 64.5334885CurrentTrain: epoch  5, batch    30 | loss: 33.3295240CurrentTrain: epoch  5, batch    31 | loss: 106.8705321CurrentTrain: epoch  5, batch    32 | loss: 38.0302419CurrentTrain: epoch  5, batch    33 | loss: 67.6859690CurrentTrain: epoch  5, batch    34 | loss: 68.1878125CurrentTrain: epoch  5, batch    35 | loss: 34.6232913CurrentTrain: epoch  5, batch    36 | loss: 50.1585734CurrentTrain: epoch  5, batch    37 | loss: 35.4498009CurrentTrain: epoch  5, batch    38 | loss: 30.4330879CurrentTrain: epoch  5, batch    39 | loss: 46.3096565CurrentTrain: epoch  5, batch    40 | loss: 51.9166999CurrentTrain: epoch  5, batch    41 | loss: 50.0531321CurrentTrain: epoch  5, batch    42 | loss: 49.6599400CurrentTrain: epoch  5, batch    43 | loss: 45.9759896CurrentTrain: epoch  5, batch    44 | loss: 46.7934894CurrentTrain: epoch  5, batch    45 | loss: 35.5098594CurrentTrain: epoch  5, batch    46 | loss: 47.6467704CurrentTrain: epoch  5, batch    47 | loss: 38.3092375CurrentTrain: epoch  5, batch    48 | loss: 29.5264974CurrentTrain: epoch  5, batch    49 | loss: 107.0634150CurrentTrain: epoch  5, batch    50 | loss: 68.4880057CurrentTrain: epoch  5, batch    51 | loss: 38.2724960CurrentTrain: epoch  5, batch    52 | loss: 46.2150328CurrentTrain: epoch  5, batch    53 | loss: 106.9448907CurrentTrain: epoch  5, batch    54 | loss: 48.5178758CurrentTrain: epoch  5, batch    55 | loss: 107.1474116CurrentTrain: epoch  5, batch    56 | loss: 47.7164689CurrentTrain: epoch  5, batch    57 | loss: 37.6960622CurrentTrain: epoch  5, batch    58 | loss: 106.8464199CurrentTrain: epoch  5, batch    59 | loss: 47.5979503CurrentTrain: epoch  5, batch    60 | loss: 66.1913734CurrentTrain: epoch  5, batch    61 | loss: 106.8873037CurrentTrain: epoch  5, batch    62 | loss: 33.8954630CurrentTrain: epoch  5, batch    63 | loss: 37.3085633CurrentTrain: epoch  5, batch    64 | loss: 45.5702890CurrentTrain: epoch  5, batch    65 | loss: 47.8372497CurrentTrain: epoch  5, batch    66 | loss: 66.4082603CurrentTrain: epoch  5, batch    67 | loss: 47.6725536CurrentTrain: epoch  5, batch    68 | loss: 46.5278971CurrentTrain: epoch  5, batch    69 | loss: 29.1982524CurrentTrain: epoch  5, batch    70 | loss: 36.6876020CurrentTrain: epoch  5, batch    71 | loss: 47.7096543CurrentTrain: epoch  5, batch    72 | loss: 34.4610393CurrentTrain: epoch  5, batch    73 | loss: 29.0408025CurrentTrain: epoch  5, batch    74 | loss: 66.2788583CurrentTrain: epoch  5, batch    75 | loss: 38.2606208CurrentTrain: epoch  5, batch    76 | loss: 37.3748499CurrentTrain: epoch  5, batch    77 | loss: 36.9093163CurrentTrain: epoch  5, batch    78 | loss: 46.3750564CurrentTrain: epoch  5, batch    79 | loss: 66.2607679CurrentTrain: epoch  5, batch    80 | loss: 103.6355921CurrentTrain: epoch  5, batch    81 | loss: 69.1117482CurrentTrain: epoch  5, batch    82 | loss: 38.1756966CurrentTrain: epoch  5, batch    83 | loss: 47.5782378CurrentTrain: epoch  5, batch    84 | loss: 107.4023088CurrentTrain: epoch  5, batch    85 | loss: 101.1283688CurrentTrain: epoch  5, batch    86 | loss: 48.7056030CurrentTrain: epoch  5, batch    87 | loss: 30.1151576CurrentTrain: epoch  5, batch    88 | loss: 222.2579544CurrentTrain: epoch  5, batch    89 | loss: 109.2200556CurrentTrain: epoch  5, batch    90 | loss: 50.0838858CurrentTrain: epoch  5, batch    91 | loss: 46.2520155CurrentTrain: epoch  5, batch    92 | loss: 103.8863870CurrentTrain: epoch  5, batch    93 | loss: 107.0617596CurrentTrain: epoch  5, batch    94 | loss: 48.5458672CurrentTrain: epoch  5, batch    95 | loss: 37.1593419CurrentTrain: epoch  5, batch    96 | loss: 66.2494042CurrentTrain: epoch  5, batch    97 | loss: 36.7083619CurrentTrain: epoch  5, batch    98 | loss: 48.9227241CurrentTrain: epoch  5, batch    99 | loss: 47.9257922CurrentTrain: epoch  5, batch   100 | loss: 46.2017215CurrentTrain: epoch  5, batch   101 | loss: 49.1201496CurrentTrain: epoch  5, batch   102 | loss: 35.5902779CurrentTrain: epoch  5, batch   103 | loss: 48.9869022CurrentTrain: epoch  5, batch   104 | loss: 65.0473766CurrentTrain: epoch  5, batch   105 | loss: 49.0192406CurrentTrain: epoch  5, batch   106 | loss: 222.2863675CurrentTrain: epoch  5, batch   107 | loss: 103.7077556CurrentTrain: epoch  5, batch   108 | loss: 66.6041584CurrentTrain: epoch  5, batch   109 | loss: 35.5991881CurrentTrain: epoch  5, batch   110 | loss: 64.7408725CurrentTrain: epoch  5, batch   111 | loss: 49.0954651CurrentTrain: epoch  5, batch   112 | loss: 29.4908105CurrentTrain: epoch  5, batch   113 | loss: 46.4775712CurrentTrain: epoch  5, batch   114 | loss: 106.8829181CurrentTrain: epoch  5, batch   115 | loss: 35.3646451CurrentTrain: epoch  5, batch   116 | loss: 28.4249316CurrentTrain: epoch  5, batch   117 | loss: 107.6880366CurrentTrain: epoch  5, batch   118 | loss: 47.6468361CurrentTrain: epoch  5, batch   119 | loss: 28.3140321CurrentTrain: epoch  5, batch   120 | loss: 106.8770924CurrentTrain: epoch  5, batch   121 | loss: 38.9435687CurrentTrain: epoch  5, batch   122 | loss: 47.6901579CurrentTrain: epoch  5, batch   123 | loss: 46.8977590CurrentTrain: epoch  5, batch   124 | loss: 52.8291049CurrentTrain: epoch  5, batch   125 | loss: 222.3483945CurrentTrain: epoch  5, batch   126 | loss: 34.8850592CurrentTrain: epoch  5, batch   127 | loss: 46.3264307CurrentTrain: epoch  5, batch   128 | loss: 106.8310555CurrentTrain: epoch  5, batch   129 | loss: 49.2064714CurrentTrain: epoch  5, batch   130 | loss: 107.6031928CurrentTrain: epoch  5, batch   131 | loss: 48.9524860CurrentTrain: epoch  5, batch   132 | loss: 34.6276166CurrentTrain: epoch  5, batch   133 | loss: 47.6786684CurrentTrain: epoch  5, batch   134 | loss: 68.1846189CurrentTrain: epoch  5, batch   135 | loss: 63.8504526CurrentTrain: epoch  5, batch   136 | loss: 45.6005479CurrentTrain: epoch  5, batch   137 | loss: 107.3969337CurrentTrain: epoch  5, batch   138 | loss: 35.3858591CurrentTrain: epoch  5, batch   139 | loss: 65.4025849CurrentTrain: epoch  5, batch   140 | loss: 101.8051961CurrentTrain: epoch  5, batch   141 | loss: 45.1436199CurrentTrain: epoch  5, batch   142 | loss: 34.1661950CurrentTrain: epoch  5, batch   143 | loss: 33.6475856CurrentTrain: epoch  6, batch     0 | loss: 48.0648868CurrentTrain: epoch  6, batch     1 | loss: 48.9720825CurrentTrain: epoch  6, batch     2 | loss: 45.5833108CurrentTrain: epoch  6, batch     3 | loss: 68.0805134CurrentTrain: epoch  6, batch     4 | loss: 106.8193287CurrentTrain: epoch  6, batch     5 | loss: 36.3418880CurrentTrain: epoch  6, batch     6 | loss: 37.0936716CurrentTrain: epoch  6, batch     7 | loss: 36.0766756CurrentTrain: epoch  6, batch     8 | loss: 107.2976077CurrentTrain: epoch  6, batch     9 | loss: 68.8034008CurrentTrain: epoch  6, batch    10 | loss: 68.1163006CurrentTrain: epoch  6, batch    11 | loss: 66.3593007CurrentTrain: epoch  6, batch    12 | loss: 36.1204084CurrentTrain: epoch  6, batch    13 | loss: 35.8594899CurrentTrain: epoch  6, batch    14 | loss: 29.1106935CurrentTrain: epoch  6, batch    15 | loss: 68.0939117CurrentTrain: epoch  6, batch    16 | loss: 37.4410816CurrentTrain: epoch  6, batch    17 | loss: 37.5265059CurrentTrain: epoch  6, batch    18 | loss: 46.9116801CurrentTrain: epoch  6, batch    19 | loss: 28.5435875CurrentTrain: epoch  6, batch    20 | loss: 68.1908017CurrentTrain: epoch  6, batch    21 | loss: 68.5727475CurrentTrain: epoch  6, batch    22 | loss: 47.4770990CurrentTrain: epoch  6, batch    23 | loss: 28.3144550CurrentTrain: epoch  6, batch    24 | loss: 108.3255203CurrentTrain: epoch  6, batch    25 | loss: 35.3165970CurrentTrain: epoch  6, batch    26 | loss: 107.2871847CurrentTrain: epoch  6, batch    27 | loss: 36.4969522CurrentTrain: epoch  6, batch    28 | loss: 31.0213543CurrentTrain: epoch  6, batch    29 | loss: 103.4725418CurrentTrain: epoch  6, batch    30 | loss: 66.6218436CurrentTrain: epoch  6, batch    31 | loss: 35.9157886CurrentTrain: epoch  6, batch    32 | loss: 46.0564767CurrentTrain: epoch  6, batch    33 | loss: 44.9109858CurrentTrain: epoch  6, batch    34 | loss: 36.3327584CurrentTrain: epoch  6, batch    35 | loss: 48.6619974CurrentTrain: epoch  6, batch    36 | loss: 47.5380206CurrentTrain: epoch  6, batch    37 | loss: 29.9949369CurrentTrain: epoch  6, batch    38 | loss: 47.7599586CurrentTrain: epoch  6, batch    39 | loss: 46.5065911CurrentTrain: epoch  6, batch    40 | loss: 38.5475755CurrentTrain: epoch  6, batch    41 | loss: 36.9589702CurrentTrain: epoch  6, batch    42 | loss: 68.1417422CurrentTrain: epoch  6, batch    43 | loss: 68.1165718CurrentTrain: epoch  6, batch    44 | loss: 50.4673471CurrentTrain: epoch  6, batch    45 | loss: 28.2100887CurrentTrain: epoch  6, batch    46 | loss: 68.1676520CurrentTrain: epoch  6, batch    47 | loss: 21.3347437CurrentTrain: epoch  6, batch    48 | loss: 64.6136209CurrentTrain: epoch  6, batch    49 | loss: 29.8655566CurrentTrain: epoch  6, batch    50 | loss: 29.6209587CurrentTrain: epoch  6, batch    51 | loss: 66.3809786CurrentTrain: epoch  6, batch    52 | loss: 107.7002477CurrentTrain: epoch  6, batch    53 | loss: 44.8287450CurrentTrain: epoch  6, batch    54 | loss: 68.1282682CurrentTrain: epoch  6, batch    55 | loss: 38.1371570CurrentTrain: epoch  6, batch    56 | loss: 52.8238989CurrentTrain: epoch  6, batch    57 | loss: 68.1614480CurrentTrain: epoch  6, batch    58 | loss: 36.9797116CurrentTrain: epoch  6, batch    59 | loss: 222.1603936CurrentTrain: epoch  6, batch    60 | loss: 27.9969986CurrentTrain: epoch  6, batch    61 | loss: 47.8488948CurrentTrain: epoch  6, batch    62 | loss: 66.3389933CurrentTrain: epoch  6, batch    63 | loss: 106.8204493CurrentTrain: epoch  6, batch    64 | loss: 36.8826577CurrentTrain: epoch  6, batch    65 | loss: 106.8298986CurrentTrain: epoch  6, batch    66 | loss: 49.2156394CurrentTrain: epoch  6, batch    67 | loss: 106.8892551CurrentTrain: epoch  6, batch    68 | loss: 47.7167776CurrentTrain: epoch  6, batch    69 | loss: 103.8146990CurrentTrain: epoch  6, batch    70 | loss: 222.2092912CurrentTrain: epoch  6, batch    71 | loss: 39.3120546CurrentTrain: epoch  6, batch    72 | loss: 64.7189254CurrentTrain: epoch  6, batch    73 | loss: 222.1967721CurrentTrain: epoch  6, batch    74 | loss: 33.6478252CurrentTrain: epoch  6, batch    75 | loss: 106.8865705CurrentTrain: epoch  6, batch    76 | loss: 64.8942091CurrentTrain: epoch  6, batch    77 | loss: 47.6330838CurrentTrain: epoch  6, batch    78 | loss: 48.3586493CurrentTrain: epoch  6, batch    79 | loss: 37.8040780CurrentTrain: epoch  6, batch    80 | loss: 45.7140662CurrentTrain: epoch  6, batch    81 | loss: 30.1905855CurrentTrain: epoch  6, batch    82 | loss: 35.7006207CurrentTrain: epoch  6, batch    83 | loss: 68.3380480CurrentTrain: epoch  6, batch    84 | loss: 51.0849059CurrentTrain: epoch  6, batch    85 | loss: 29.6048122CurrentTrain: epoch  6, batch    86 | loss: 49.0098025CurrentTrain: epoch  6, batch    87 | loss: 47.5368338CurrentTrain: epoch  6, batch    88 | loss: 106.7723673CurrentTrain: epoch  6, batch    89 | loss: 49.9156106CurrentTrain: epoch  6, batch    90 | loss: 27.2416981CurrentTrain: epoch  6, batch    91 | loss: 29.2193159CurrentTrain: epoch  6, batch    92 | loss: 29.8534633CurrentTrain: epoch  6, batch    93 | loss: 31.1496679CurrentTrain: epoch  6, batch    94 | loss: 29.6275047CurrentTrain: epoch  6, batch    95 | loss: 52.2988586CurrentTrain: epoch  6, batch    96 | loss: 49.9092253CurrentTrain: epoch  6, batch    97 | loss: 71.4506077CurrentTrain: epoch  6, batch    98 | loss: 35.8410405CurrentTrain: epoch  6, batch    99 | loss: 35.5782266CurrentTrain: epoch  6, batch   100 | loss: 48.8784304CurrentTrain: epoch  6, batch   101 | loss: 49.1557780CurrentTrain: epoch  6, batch   102 | loss: 39.3586669CurrentTrain: epoch  6, batch   103 | loss: 38.7596687CurrentTrain: epoch  6, batch   104 | loss: 45.5974815CurrentTrain: epoch  6, batch   105 | loss: 47.8970371CurrentTrain: epoch  6, batch   106 | loss: 24.2034211CurrentTrain: epoch  6, batch   107 | loss: 49.1106059CurrentTrain: epoch  6, batch   108 | loss: 39.0527971CurrentTrain: epoch  6, batch   109 | loss: 36.8485765CurrentTrain: epoch  6, batch   110 | loss: 49.0710421CurrentTrain: epoch  6, batch   111 | loss: 29.6393720CurrentTrain: epoch  6, batch   112 | loss: 50.5636585CurrentTrain: epoch  6, batch   113 | loss: 36.7782488CurrentTrain: epoch  6, batch   114 | loss: 27.3117327CurrentTrain: epoch  6, batch   115 | loss: 107.0629667CurrentTrain: epoch  6, batch   116 | loss: 38.4361105CurrentTrain: epoch  6, batch   117 | loss: 68.5896258CurrentTrain: epoch  6, batch   118 | loss: 37.5835245CurrentTrain: epoch  6, batch   119 | loss: 45.8991461CurrentTrain: epoch  6, batch   120 | loss: 69.5188058CurrentTrain: epoch  6, batch   121 | loss: 106.8296558CurrentTrain: epoch  6, batch   122 | loss: 31.3741259CurrentTrain: epoch  6, batch   123 | loss: 68.4201875CurrentTrain: epoch  6, batch   124 | loss: 47.7423710CurrentTrain: epoch  6, batch   125 | loss: 66.2784403CurrentTrain: epoch  6, batch   126 | loss: 35.7825557CurrentTrain: epoch  6, batch   127 | loss: 222.2398617CurrentTrain: epoch  6, batch   128 | loss: 48.2292213CurrentTrain: epoch  6, batch   129 | loss: 103.4961861CurrentTrain: epoch  6, batch   130 | loss: 50.3019349CurrentTrain: epoch  6, batch   131 | loss: 38.1547373CurrentTrain: epoch  6, batch   132 | loss: 64.3053146CurrentTrain: epoch  6, batch   133 | loss: 46.5780736CurrentTrain: epoch  6, batch   134 | loss: 37.8948912CurrentTrain: epoch  6, batch   135 | loss: 46.4747006CurrentTrain: epoch  6, batch   136 | loss: 39.3373206CurrentTrain: epoch  6, batch   137 | loss: 47.8223083CurrentTrain: epoch  6, batch   138 | loss: 47.7528148CurrentTrain: epoch  6, batch   139 | loss: 31.3862172CurrentTrain: epoch  6, batch   140 | loss: 37.4492024CurrentTrain: epoch  6, batch   141 | loss: 66.2969615CurrentTrain: epoch  6, batch   142 | loss: 48.1278927CurrentTrain: epoch  6, batch   143 | loss: 36.2499686CurrentTrain: epoch  7, batch     0 | loss: 47.4991511CurrentTrain: epoch  7, batch     1 | loss: 46.2607597CurrentTrain: epoch  7, batch     2 | loss: 106.7358464CurrentTrain: epoch  7, batch     3 | loss: 29.9222494CurrentTrain: epoch  7, batch     4 | loss: 47.8170550CurrentTrain: epoch  7, batch     5 | loss: 37.6501708CurrentTrain: epoch  7, batch     6 | loss: 28.0868178CurrentTrain: epoch  7, batch     7 | loss: 36.5664412CurrentTrain: epoch  7, batch     8 | loss: 36.3006061CurrentTrain: epoch  7, batch     9 | loss: 48.0657570CurrentTrain: epoch  7, batch    10 | loss: 36.3352732CurrentTrain: epoch  7, batch    11 | loss: 37.1212221CurrentTrain: epoch  7, batch    12 | loss: 37.7376434CurrentTrain: epoch  7, batch    13 | loss: 35.6604699CurrentTrain: epoch  7, batch    14 | loss: 29.2362971CurrentTrain: epoch  7, batch    15 | loss: 68.2609861CurrentTrain: epoch  7, batch    16 | loss: 36.6895843CurrentTrain: epoch  7, batch    17 | loss: 43.4635679CurrentTrain: epoch  7, batch    18 | loss: 30.3199364CurrentTrain: epoch  7, batch    19 | loss: 64.2732132CurrentTrain: epoch  7, batch    20 | loss: 68.3255451CurrentTrain: epoch  7, batch    21 | loss: 47.1393224CurrentTrain: epoch  7, batch    22 | loss: 45.2995829CurrentTrain: epoch  7, batch    23 | loss: 26.7417234CurrentTrain: epoch  7, batch    24 | loss: 36.2929858CurrentTrain: epoch  7, batch    25 | loss: 49.0204743CurrentTrain: epoch  7, batch    26 | loss: 46.3204058CurrentTrain: epoch  7, batch    27 | loss: 37.8261749CurrentTrain: epoch  7, batch    28 | loss: 106.8916208CurrentTrain: epoch  7, batch    29 | loss: 47.8690188CurrentTrain: epoch  7, batch    30 | loss: 46.3201216CurrentTrain: epoch  7, batch    31 | loss: 47.7998078CurrentTrain: epoch  7, batch    32 | loss: 68.7732371CurrentTrain: epoch  7, batch    33 | loss: 48.9547784CurrentTrain: epoch  7, batch    34 | loss: 69.2746311CurrentTrain: epoch  7, batch    35 | loss: 68.0894482CurrentTrain: epoch  7, batch    36 | loss: 46.2876995CurrentTrain: epoch  7, batch    37 | loss: 47.4907679CurrentTrain: epoch  7, batch    38 | loss: 106.7934120CurrentTrain: epoch  7, batch    39 | loss: 68.0860736CurrentTrain: epoch  7, batch    40 | loss: 69.6787173CurrentTrain: epoch  7, batch    41 | loss: 46.2289622CurrentTrain: epoch  7, batch    42 | loss: 47.5482216CurrentTrain: epoch  7, batch    43 | loss: 27.6514280CurrentTrain: epoch  7, batch    44 | loss: 47.6942013CurrentTrain: epoch  7, batch    45 | loss: 26.4380521CurrentTrain: epoch  7, batch    46 | loss: 47.6490819CurrentTrain: epoch  7, batch    47 | loss: 49.2098717CurrentTrain: epoch  7, batch    48 | loss: 68.1422150CurrentTrain: epoch  7, batch    49 | loss: 54.0560715CurrentTrain: epoch  7, batch    50 | loss: 37.4838845CurrentTrain: epoch  7, batch    51 | loss: 47.4102115CurrentTrain: epoch  7, batch    52 | loss: 107.3090072CurrentTrain: epoch  7, batch    53 | loss: 47.5474042CurrentTrain: epoch  7, batch    54 | loss: 36.5663127CurrentTrain: epoch  7, batch    55 | loss: 67.1423002CurrentTrain: epoch  7, batch    56 | loss: 48.9200496CurrentTrain: epoch  7, batch    57 | loss: 31.3653991CurrentTrain: epoch  7, batch    58 | loss: 70.8455149CurrentTrain: epoch  7, batch    59 | loss: 46.4061160CurrentTrain: epoch  7, batch    60 | loss: 36.3600832CurrentTrain: epoch  7, batch    61 | loss: 47.8123695CurrentTrain: epoch  7, batch    62 | loss: 46.2574746CurrentTrain: epoch  7, batch    63 | loss: 68.0973438CurrentTrain: epoch  7, batch    64 | loss: 45.3160280CurrentTrain: epoch  7, batch    65 | loss: 49.5154747CurrentTrain: epoch  7, batch    66 | loss: 29.1725700CurrentTrain: epoch  7, batch    67 | loss: 48.9255791CurrentTrain: epoch  7, batch    68 | loss: 28.2543612CurrentTrain: epoch  7, batch    69 | loss: 30.9270316CurrentTrain: epoch  7, batch    70 | loss: 47.6835489CurrentTrain: epoch  7, batch    71 | loss: 45.3172336CurrentTrain: epoch  7, batch    72 | loss: 47.9432358CurrentTrain: epoch  7, batch    73 | loss: 49.0325507CurrentTrain: epoch  7, batch    74 | loss: 51.2697259CurrentTrain: epoch  7, batch    75 | loss: 69.5142326CurrentTrain: epoch  7, batch    76 | loss: 53.6537038CurrentTrain: epoch  7, batch    77 | loss: 37.5302831CurrentTrain: epoch  7, batch    78 | loss: 37.9721090CurrentTrain: epoch  7, batch    79 | loss: 49.1720945CurrentTrain: epoch  7, batch    80 | loss: 47.6195700CurrentTrain: epoch  7, batch    81 | loss: 35.4179724CurrentTrain: epoch  7, batch    82 | loss: 106.7203624CurrentTrain: epoch  7, batch    83 | loss: 46.3064846CurrentTrain: epoch  7, batch    84 | loss: 46.1448258CurrentTrain: epoch  7, batch    85 | loss: 47.8676485CurrentTrain: epoch  7, batch    86 | loss: 51.4663618CurrentTrain: epoch  7, batch    87 | loss: 103.6112883CurrentTrain: epoch  7, batch    88 | loss: 36.3430730CurrentTrain: epoch  7, batch    89 | loss: 47.8944287CurrentTrain: epoch  7, batch    90 | loss: 222.1577047CurrentTrain: epoch  7, batch    91 | loss: 36.8628024CurrentTrain: epoch  7, batch    92 | loss: 27.0907870CurrentTrain: epoch  7, batch    93 | loss: 35.5808989CurrentTrain: epoch  7, batch    94 | loss: 69.2793327CurrentTrain: epoch  7, batch    95 | loss: 49.0076550CurrentTrain: epoch  7, batch    96 | loss: 47.5339583CurrentTrain: epoch  7, batch    97 | loss: 106.9171008CurrentTrain: epoch  7, batch    98 | loss: 36.2856142CurrentTrain: epoch  7, batch    99 | loss: 29.3156610CurrentTrain: epoch  7, batch   100 | loss: 49.1196203CurrentTrain: epoch  7, batch   101 | loss: 29.2909269CurrentTrain: epoch  7, batch   102 | loss: 106.8130324CurrentTrain: epoch  7, batch   103 | loss: 46.2684331CurrentTrain: epoch  7, batch   104 | loss: 37.5369861CurrentTrain: epoch  7, batch   105 | loss: 65.2970563CurrentTrain: epoch  7, batch   106 | loss: 106.8871905CurrentTrain: epoch  7, batch   107 | loss: 32.3829994CurrentTrain: epoch  7, batch   108 | loss: 106.7919193CurrentTrain: epoch  7, batch   109 | loss: 48.9014716CurrentTrain: epoch  7, batch   110 | loss: 49.2396453CurrentTrain: epoch  7, batch   111 | loss: 48.9276291CurrentTrain: epoch  7, batch   112 | loss: 68.2645374CurrentTrain: epoch  7, batch   113 | loss: 107.0268884CurrentTrain: epoch  7, batch   114 | loss: 47.5553138CurrentTrain: epoch  7, batch   115 | loss: 34.3090194CurrentTrain: epoch  7, batch   116 | loss: 106.9836356CurrentTrain: epoch  7, batch   117 | loss: 48.3938091CurrentTrain: epoch  7, batch   118 | loss: 47.7182452CurrentTrain: epoch  7, batch   119 | loss: 37.5894668CurrentTrain: epoch  7, batch   120 | loss: 49.8702710CurrentTrain: epoch  7, batch   121 | loss: 45.2863785CurrentTrain: epoch  7, batch   122 | loss: 39.0554364CurrentTrain: epoch  7, batch   123 | loss: 66.0954751CurrentTrain: epoch  7, batch   124 | loss: 106.8527768CurrentTrain: epoch  7, batch   125 | loss: 36.9984318CurrentTrain: epoch  7, batch   126 | loss: 38.7658302CurrentTrain: epoch  7, batch   127 | loss: 222.2421277CurrentTrain: epoch  7, batch   128 | loss: 37.5886188CurrentTrain: epoch  7, batch   129 | loss: 27.4901142CurrentTrain: epoch  7, batch   130 | loss: 48.8934560CurrentTrain: epoch  7, batch   131 | loss: 66.3065035CurrentTrain: epoch  7, batch   132 | loss: 35.4319588CurrentTrain: epoch  7, batch   133 | loss: 36.3015455CurrentTrain: epoch  7, batch   134 | loss: 48.9004065CurrentTrain: epoch  7, batch   135 | loss: 49.4429275CurrentTrain: epoch  7, batch   136 | loss: 68.3396204CurrentTrain: epoch  7, batch   137 | loss: 66.1881173CurrentTrain: epoch  7, batch   138 | loss: 44.1017346CurrentTrain: epoch  7, batch   139 | loss: 32.1944524CurrentTrain: epoch  7, batch   140 | loss: 37.8922317CurrentTrain: epoch  7, batch   141 | loss: 47.4801014CurrentTrain: epoch  7, batch   142 | loss: 45.4276649CurrentTrain: epoch  7, batch   143 | loss: 34.8106858CurrentTrain: epoch  8, batch     0 | loss: 66.1162629CurrentTrain: epoch  8, batch     1 | loss: 35.4965834CurrentTrain: epoch  8, batch     2 | loss: 68.1492235CurrentTrain: epoch  8, batch     3 | loss: 47.5604232CurrentTrain: epoch  8, batch     4 | loss: 36.4532340CurrentTrain: epoch  8, batch     5 | loss: 46.0563284CurrentTrain: epoch  8, batch     6 | loss: 36.3380400CurrentTrain: epoch  8, batch     7 | loss: 47.4712730CurrentTrain: epoch  8, batch     8 | loss: 64.5249112CurrentTrain: epoch  8, batch     9 | loss: 29.3207121CurrentTrain: epoch  8, batch    10 | loss: 28.2095221CurrentTrain: epoch  8, batch    11 | loss: 66.2987333CurrentTrain: epoch  8, batch    12 | loss: 24.1565730CurrentTrain: epoch  8, batch    13 | loss: 46.5195098CurrentTrain: epoch  8, batch    14 | loss: 24.9071092CurrentTrain: epoch  8, batch    15 | loss: 68.0788564CurrentTrain: epoch  8, batch    16 | loss: 49.0401207CurrentTrain: epoch  8, batch    17 | loss: 49.0936425CurrentTrain: epoch  8, batch    18 | loss: 48.8878930CurrentTrain: epoch  8, batch    19 | loss: 35.9748272CurrentTrain: epoch  8, batch    20 | loss: 64.9963878CurrentTrain: epoch  8, batch    21 | loss: 46.2405272CurrentTrain: epoch  8, batch    22 | loss: 106.8916761CurrentTrain: epoch  8, batch    23 | loss: 106.7097480CurrentTrain: epoch  8, batch    24 | loss: 44.9698852CurrentTrain: epoch  8, batch    25 | loss: 49.3288033CurrentTrain: epoch  8, batch    26 | loss: 49.2667954CurrentTrain: epoch  8, batch    27 | loss: 36.9748649CurrentTrain: epoch  8, batch    28 | loss: 48.9586661CurrentTrain: epoch  8, batch    29 | loss: 29.5413057CurrentTrain: epoch  8, batch    30 | loss: 28.9352273CurrentTrain: epoch  8, batch    31 | loss: 47.4695504CurrentTrain: epoch  8, batch    32 | loss: 46.5611721CurrentTrain: epoch  8, batch    33 | loss: 63.0432261CurrentTrain: epoch  8, batch    34 | loss: 36.7554559CurrentTrain: epoch  8, batch    35 | loss: 22.4069873CurrentTrain: epoch  8, batch    36 | loss: 222.0829093CurrentTrain: epoch  8, batch    37 | loss: 106.4120687CurrentTrain: epoch  8, batch    38 | loss: 68.1436060CurrentTrain: epoch  8, batch    39 | loss: 36.0211697CurrentTrain: epoch  8, batch    40 | loss: 66.3652322CurrentTrain: epoch  8, batch    41 | loss: 49.3737205CurrentTrain: epoch  8, batch    42 | loss: 68.0744004CurrentTrain: epoch  8, batch    43 | loss: 68.3021887CurrentTrain: epoch  8, batch    44 | loss: 36.4392405CurrentTrain: epoch  8, batch    45 | loss: 68.0619785CurrentTrain: epoch  8, batch    46 | loss: 64.2091304CurrentTrain: epoch  8, batch    47 | loss: 66.0994690CurrentTrain: epoch  8, batch    48 | loss: 106.9762325CurrentTrain: epoch  8, batch    49 | loss: 64.5579007CurrentTrain: epoch  8, batch    50 | loss: 27.4214898CurrentTrain: epoch  8, batch    51 | loss: 37.4910065CurrentTrain: epoch  8, batch    52 | loss: 48.8996464CurrentTrain: epoch  8, batch    53 | loss: 36.5422591CurrentTrain: epoch  8, batch    54 | loss: 29.1028015CurrentTrain: epoch  8, batch    55 | loss: 28.7516936CurrentTrain: epoch  8, batch    56 | loss: 46.5272327CurrentTrain: epoch  8, batch    57 | loss: 222.1474884CurrentTrain: epoch  8, batch    58 | loss: 47.4180791CurrentTrain: epoch  8, batch    59 | loss: 44.9551391CurrentTrain: epoch  8, batch    60 | loss: 24.9180990CurrentTrain: epoch  8, batch    61 | loss: 69.5213510CurrentTrain: epoch  8, batch    62 | loss: 66.0927220CurrentTrain: epoch  8, batch    63 | loss: 35.3901097CurrentTrain: epoch  8, batch    64 | loss: 68.3678916CurrentTrain: epoch  8, batch    65 | loss: 68.1549680CurrentTrain: epoch  8, batch    66 | loss: 47.4135932CurrentTrain: epoch  8, batch    67 | loss: 47.4419789CurrentTrain: epoch  8, batch    68 | loss: 54.3942109CurrentTrain: epoch  8, batch    69 | loss: 68.1784592CurrentTrain: epoch  8, batch    70 | loss: 68.2762891CurrentTrain: epoch  8, batch    71 | loss: 66.0769254CurrentTrain: epoch  8, batch    72 | loss: 36.3173342CurrentTrain: epoch  8, batch    73 | loss: 101.0151282CurrentTrain: epoch  8, batch    74 | loss: 47.5373162CurrentTrain: epoch  8, batch    75 | loss: 47.5358820CurrentTrain: epoch  8, batch    76 | loss: 48.8852231CurrentTrain: epoch  8, batch    77 | loss: 35.3447692CurrentTrain: epoch  8, batch    78 | loss: 38.1854672CurrentTrain: epoch  8, batch    79 | loss: 47.4857374CurrentTrain: epoch  8, batch    80 | loss: 46.3557210CurrentTrain: epoch  8, batch    81 | loss: 29.9181036CurrentTrain: epoch  8, batch    82 | loss: 47.0958639CurrentTrain: epoch  8, batch    83 | loss: 106.7415809CurrentTrain: epoch  8, batch    84 | loss: 69.5607289CurrentTrain: epoch  8, batch    85 | loss: 36.4213912CurrentTrain: epoch  8, batch    86 | loss: 66.1682309CurrentTrain: epoch  8, batch    87 | loss: 46.9876465CurrentTrain: epoch  8, batch    88 | loss: 48.9048854CurrentTrain: epoch  8, batch    89 | loss: 68.0648334CurrentTrain: epoch  8, batch    90 | loss: 38.5818336CurrentTrain: epoch  8, batch    91 | loss: 28.0351678CurrentTrain: epoch  8, batch    92 | loss: 46.2463463CurrentTrain: epoch  8, batch    93 | loss: 49.0476417CurrentTrain: epoch  8, batch    94 | loss: 68.0708325CurrentTrain: epoch  8, batch    95 | loss: 37.4410230CurrentTrain: epoch  8, batch    96 | loss: 29.2806320CurrentTrain: epoch  8, batch    97 | loss: 36.9088788CurrentTrain: epoch  8, batch    98 | loss: 37.8184656CurrentTrain: epoch  8, batch    99 | loss: 46.0475679CurrentTrain: epoch  8, batch   100 | loss: 66.1882482CurrentTrain: epoch  8, batch   101 | loss: 68.4941899CurrentTrain: epoch  8, batch   102 | loss: 46.3618593CurrentTrain: epoch  8, batch   103 | loss: 66.1068862CurrentTrain: epoch  8, batch   104 | loss: 34.8229928CurrentTrain: epoch  8, batch   105 | loss: 48.9298979CurrentTrain: epoch  8, batch   106 | loss: 47.4652635CurrentTrain: epoch  8, batch   107 | loss: 48.2146783CurrentTrain: epoch  8, batch   108 | loss: 28.9122122CurrentTrain: epoch  8, batch   109 | loss: 66.0946146CurrentTrain: epoch  8, batch   110 | loss: 64.5213369CurrentTrain: epoch  8, batch   111 | loss: 49.0679663CurrentTrain: epoch  8, batch   112 | loss: 37.5349087CurrentTrain: epoch  8, batch   113 | loss: 37.6438476CurrentTrain: epoch  8, batch   114 | loss: 66.3254727CurrentTrain: epoch  8, batch   115 | loss: 68.1033465CurrentTrain: epoch  8, batch   116 | loss: 26.8335917CurrentTrain: epoch  8, batch   117 | loss: 47.6743703CurrentTrain: epoch  8, batch   118 | loss: 46.7402615CurrentTrain: epoch  8, batch   119 | loss: 106.7461634CurrentTrain: epoch  8, batch   120 | loss: 103.5782236CurrentTrain: epoch  8, batch   121 | loss: 48.9974596CurrentTrain: epoch  8, batch   122 | loss: 66.1957649CurrentTrain: epoch  8, batch   123 | loss: 106.7302182CurrentTrain: epoch  8, batch   124 | loss: 37.0113299CurrentTrain: epoch  8, batch   125 | loss: 35.1829555CurrentTrain: epoch  8, batch   126 | loss: 27.1270791CurrentTrain: epoch  8, batch   127 | loss: 47.5602047CurrentTrain: epoch  8, batch   128 | loss: 68.1363501CurrentTrain: epoch  8, batch   129 | loss: 36.4078078CurrentTrain: epoch  8, batch   130 | loss: 70.2202054CurrentTrain: epoch  8, batch   131 | loss: 46.2425147CurrentTrain: epoch  8, batch   132 | loss: 47.5126707CurrentTrain: epoch  8, batch   133 | loss: 34.9324995CurrentTrain: epoch  8, batch   134 | loss: 66.0942829CurrentTrain: epoch  8, batch   135 | loss: 36.3165822CurrentTrain: epoch  8, batch   136 | loss: 47.9440692CurrentTrain: epoch  8, batch   137 | loss: 34.3738983CurrentTrain: epoch  8, batch   138 | loss: 33.2532621CurrentTrain: epoch  8, batch   139 | loss: 37.5447597CurrentTrain: epoch  8, batch   140 | loss: 68.2111635CurrentTrain: epoch  8, batch   141 | loss: 47.2893270CurrentTrain: epoch  8, batch   142 | loss: 104.9652693CurrentTrain: epoch  8, batch   143 | loss: 36.1630218CurrentTrain: epoch  9, batch     0 | loss: 37.0613348CurrentTrain: epoch  9, batch     1 | loss: 66.1078520CurrentTrain: epoch  9, batch     2 | loss: 50.2816712CurrentTrain: epoch  9, batch     3 | loss: 47.4301007CurrentTrain: epoch  9, batch     4 | loss: 49.8676513CurrentTrain: epoch  9, batch     5 | loss: 66.3650767CurrentTrain: epoch  9, batch     6 | loss: 32.3265295CurrentTrain: epoch  9, batch     7 | loss: 37.6711226CurrentTrain: epoch  9, batch     8 | loss: 64.5164543CurrentTrain: epoch  9, batch     9 | loss: 66.3239168CurrentTrain: epoch  9, batch    10 | loss: 68.0901279CurrentTrain: epoch  9, batch    11 | loss: 69.2363661CurrentTrain: epoch  9, batch    12 | loss: 66.5612718CurrentTrain: epoch  9, batch    13 | loss: 47.6374356CurrentTrain: epoch  9, batch    14 | loss: 47.4889979CurrentTrain: epoch  9, batch    15 | loss: 47.4103998CurrentTrain: epoch  9, batch    16 | loss: 68.1706596CurrentTrain: epoch  9, batch    17 | loss: 34.4407555CurrentTrain: epoch  9, batch    18 | loss: 47.9507407CurrentTrain: epoch  9, batch    19 | loss: 49.3587742CurrentTrain: epoch  9, batch    20 | loss: 66.1207990CurrentTrain: epoch  9, batch    21 | loss: 29.9568154CurrentTrain: epoch  9, batch    22 | loss: 50.0247271CurrentTrain: epoch  9, batch    23 | loss: 35.3933881CurrentTrain: epoch  9, batch    24 | loss: 222.0450948CurrentTrain: epoch  9, batch    25 | loss: 36.6074619CurrentTrain: epoch  9, batch    26 | loss: 68.0699358CurrentTrain: epoch  9, batch    27 | loss: 68.1791569CurrentTrain: epoch  9, batch    28 | loss: 48.9473821CurrentTrain: epoch  9, batch    29 | loss: 68.0695612CurrentTrain: epoch  9, batch    30 | loss: 37.7656162CurrentTrain: epoch  9, batch    31 | loss: 37.7969781CurrentTrain: epoch  9, batch    32 | loss: 44.7997582CurrentTrain: epoch  9, batch    33 | loss: 49.0078585CurrentTrain: epoch  9, batch    34 | loss: 36.9060698CurrentTrain: epoch  9, batch    35 | loss: 47.4886107CurrentTrain: epoch  9, batch    36 | loss: 47.4926598CurrentTrain: epoch  9, batch    37 | loss: 37.5022375CurrentTrain: epoch  9, batch    38 | loss: 48.1540677CurrentTrain: epoch  9, batch    39 | loss: 38.5187186CurrentTrain: epoch  9, batch    40 | loss: 48.2182050CurrentTrain: epoch  9, batch    41 | loss: 36.2584944CurrentTrain: epoch  9, batch    42 | loss: 46.0268606CurrentTrain: epoch  9, batch    43 | loss: 62.6225112CurrentTrain: epoch  9, batch    44 | loss: 68.0754243CurrentTrain: epoch  9, batch    45 | loss: 44.9147365CurrentTrain: epoch  9, batch    46 | loss: 66.0703093CurrentTrain: epoch  9, batch    47 | loss: 68.5582923CurrentTrain: epoch  9, batch    48 | loss: 106.7620803CurrentTrain: epoch  9, batch    49 | loss: 46.3316067CurrentTrain: epoch  9, batch    50 | loss: 36.6076857CurrentTrain: epoch  9, batch    51 | loss: 37.6628504CurrentTrain: epoch  9, batch    52 | loss: 46.3787372CurrentTrain: epoch  9, batch    53 | loss: 106.7249327CurrentTrain: epoch  9, batch    54 | loss: 38.3104063CurrentTrain: epoch  9, batch    55 | loss: 46.0763760CurrentTrain: epoch  9, batch    56 | loss: 48.1633161CurrentTrain: epoch  9, batch    57 | loss: 68.0943692CurrentTrain: epoch  9, batch    58 | loss: 47.6737359CurrentTrain: epoch  9, batch    59 | loss: 46.2185174CurrentTrain: epoch  9, batch    60 | loss: 28.2328323CurrentTrain: epoch  9, batch    61 | loss: 28.8528944CurrentTrain: epoch  9, batch    62 | loss: 33.7302849CurrentTrain: epoch  9, batch    63 | loss: 67.1459710CurrentTrain: epoch  9, batch    64 | loss: 47.6247111CurrentTrain: epoch  9, batch    65 | loss: 48.9186212CurrentTrain: epoch  9, batch    66 | loss: 34.8109477CurrentTrain: epoch  9, batch    67 | loss: 35.5130968CurrentTrain: epoch  9, batch    68 | loss: 64.4652690CurrentTrain: epoch  9, batch    69 | loss: 37.7522654CurrentTrain: epoch  9, batch    70 | loss: 36.3057064CurrentTrain: epoch  9, batch    71 | loss: 34.0055288CurrentTrain: epoch  9, batch    72 | loss: 70.7683997CurrentTrain: epoch  9, batch    73 | loss: 47.9892047CurrentTrain: epoch  9, batch    74 | loss: 28.9031878CurrentTrain: epoch  9, batch    75 | loss: 46.4681504CurrentTrain: epoch  9, batch    76 | loss: 49.2203011CurrentTrain: epoch  9, batch    77 | loss: 64.6515674CurrentTrain: epoch  9, batch    78 | loss: 68.1307000CurrentTrain: epoch  9, batch    79 | loss: 37.8635021CurrentTrain: epoch  9, batch    80 | loss: 36.3516256CurrentTrain: epoch  9, batch    81 | loss: 66.0710459CurrentTrain: epoch  9, batch    82 | loss: 29.0183477CurrentTrain: epoch  9, batch    83 | loss: 46.2916189CurrentTrain: epoch  9, batch    84 | loss: 46.7676003CurrentTrain: epoch  9, batch    85 | loss: 36.5216423CurrentTrain: epoch  9, batch    86 | loss: 37.3948062CurrentTrain: epoch  9, batch    87 | loss: 101.0706137CurrentTrain: epoch  9, batch    88 | loss: 64.4598827CurrentTrain: epoch  9, batch    89 | loss: 47.5812216CurrentTrain: epoch  9, batch    90 | loss: 36.7712202CurrentTrain: epoch  9, batch    91 | loss: 68.2449702CurrentTrain: epoch  9, batch    92 | loss: 68.0534665CurrentTrain: epoch  9, batch    93 | loss: 47.9308127CurrentTrain: epoch  9, batch    94 | loss: 47.4253914CurrentTrain: epoch  9, batch    95 | loss: 36.3791050CurrentTrain: epoch  9, batch    96 | loss: 47.4543514CurrentTrain: epoch  9, batch    97 | loss: 29.8752479CurrentTrain: epoch  9, batch    98 | loss: 46.0919903CurrentTrain: epoch  9, batch    99 | loss: 27.1154039CurrentTrain: epoch  9, batch   100 | loss: 101.9845009CurrentTrain: epoch  9, batch   101 | loss: 37.5014499CurrentTrain: epoch  9, batch   102 | loss: 49.7230281CurrentTrain: epoch  9, batch   103 | loss: 45.9360556CurrentTrain: epoch  9, batch   104 | loss: 47.4727780CurrentTrain: epoch  9, batch   105 | loss: 46.0030534CurrentTrain: epoch  9, batch   106 | loss: 222.1210036CurrentTrain: epoch  9, batch   107 | loss: 43.7474118CurrentTrain: epoch  9, batch   108 | loss: 36.9581126CurrentTrain: epoch  9, batch   109 | loss: 44.9356097CurrentTrain: epoch  9, batch   110 | loss: 66.0923936CurrentTrain: epoch  9, batch   111 | loss: 38.8318690CurrentTrain: epoch  9, batch   112 | loss: 66.0695286CurrentTrain: epoch  9, batch   113 | loss: 36.3114442CurrentTrain: epoch  9, batch   114 | loss: 48.8020708CurrentTrain: epoch  9, batch   115 | loss: 29.0139756CurrentTrain: epoch  9, batch   116 | loss: 47.4719601CurrentTrain: epoch  9, batch   117 | loss: 35.4564151CurrentTrain: epoch  9, batch   118 | loss: 66.2348057CurrentTrain: epoch  9, batch   119 | loss: 37.5914961CurrentTrain: epoch  9, batch   120 | loss: 68.1099835CurrentTrain: epoch  9, batch   121 | loss: 35.3885896CurrentTrain: epoch  9, batch   122 | loss: 45.9769118CurrentTrain: epoch  9, batch   123 | loss: 48.8667983CurrentTrain: epoch  9, batch   124 | loss: 101.4417170CurrentTrain: epoch  9, batch   125 | loss: 47.9572981CurrentTrain: epoch  9, batch   126 | loss: 34.7237040CurrentTrain: epoch  9, batch   127 | loss: 45.3010082CurrentTrain: epoch  9, batch   128 | loss: 106.7359750CurrentTrain: epoch  9, batch   129 | loss: 44.7899030CurrentTrain: epoch  9, batch   130 | loss: 29.7728272CurrentTrain: epoch  9, batch   131 | loss: 48.1377992CurrentTrain: epoch  9, batch   132 | loss: 66.1535395CurrentTrain: epoch  9, batch   133 | loss: 48.8964910CurrentTrain: epoch  9, batch   134 | loss: 68.0686119CurrentTrain: epoch  9, batch   135 | loss: 47.6604157CurrentTrain: epoch  9, batch   136 | loss: 47.3844537CurrentTrain: epoch  9, batch   137 | loss: 103.4694423CurrentTrain: epoch  9, batch   138 | loss: 68.1685703CurrentTrain: epoch  9, batch   139 | loss: 23.3093694CurrentTrain: epoch  9, batch   140 | loss: 68.0847266CurrentTrain: epoch  9, batch   141 | loss: 106.9748519CurrentTrain: epoch  9, batch   142 | loss: 35.4166973CurrentTrain: epoch  9, batch   143 | loss: 33.5421675

F1 score per class: {32: 0.47761194029850745, 6: 0.6846846846846847, 19: 0.20833333333333334, 24: 0.7236180904522613, 26: 0.8571428571428571, 29: 0.7837837837837838}
Micro-average F1 score: 0.6827012025901943
Weighted-average F1 score: 0.6777368816408359
F1 score per class: {32: 0.5476190476190477, 6: 0.635036496350365, 19: 0.2222222222222222, 24: 0.7106598984771574, 26: 0.9019607843137255, 29: 0.6976744186046512}
Micro-average F1 score: 0.6587677725118484
Weighted-average F1 score: 0.6426536570367719
F1 score per class: {32: 0.5433070866141733, 6: 0.635036496350365, 19: 0.2222222222222222, 24: 0.7106598984771574, 26: 0.9019607843137255, 29: 0.7086614173228346}
Micro-average F1 score: 0.6598101265822784
Weighted-average F1 score: 0.6434818119198049

F1 score per class: {32: 0.47761194029850745, 6: 0.6846846846846847, 19: 0.20833333333333334, 24: 0.7236180904522613, 26: 0.8571428571428571, 29: 0.7837837837837838}
Micro-average F1 score: 0.6827012025901943
Weighted-average F1 score: 0.6777368816408359
F1 score per class: {32: 0.5476190476190477, 6: 0.635036496350365, 19: 0.2222222222222222, 24: 0.7106598984771574, 26: 0.9019607843137255, 29: 0.6976744186046512}
Micro-average F1 score: 0.6587677725118484
Weighted-average F1 score: 0.6426536570367719
F1 score per class: {32: 0.5433070866141733, 6: 0.635036496350365, 19: 0.2222222222222222, 24: 0.7106598984771574, 26: 0.9019607843137255, 29: 0.7086614173228346}
Micro-average F1 score: 0.6598101265822784
Weighted-average F1 score: 0.6434818119198049
cur_acc:  ['0.6827']
his_acc:  ['0.6827']
cur_acc des:  ['0.6588']
his_acc des:  ['0.6588']
cur_acc rrf:  ['0.6598']
his_acc rrf:  ['0.6598']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 63.8270997CurrentTrain: epoch  0, batch     1 | loss: 80.9976987CurrentTrain: epoch  0, batch     2 | loss: 43.6489461CurrentTrain: epoch  0, batch     3 | loss: 79.5711236CurrentTrain: epoch  0, batch     4 | loss: 49.4215680CurrentTrain: epoch  0, batch     5 | loss: 50.3902114CurrentTrain: epoch  1, batch     0 | loss: 58.2845792CurrentTrain: epoch  1, batch     1 | loss: 70.0961813CurrentTrain: epoch  1, batch     2 | loss: 57.2149624CurrentTrain: epoch  1, batch     3 | loss: 57.9144719CurrentTrain: epoch  1, batch     4 | loss: 45.2050494CurrentTrain: epoch  1, batch     5 | loss: 32.4966558CurrentTrain: epoch  2, batch     0 | loss: 112.7491492CurrentTrain: epoch  2, batch     1 | loss: 55.8737423CurrentTrain: epoch  2, batch     2 | loss: 44.0365386CurrentTrain: epoch  2, batch     3 | loss: 43.0929383CurrentTrain: epoch  2, batch     4 | loss: 43.8542540CurrentTrain: epoch  2, batch     5 | loss: 19.0225987CurrentTrain: epoch  3, batch     0 | loss: 42.0306421CurrentTrain: epoch  3, batch     1 | loss: 56.4519505CurrentTrain: epoch  3, batch     2 | loss: 53.3419176CurrentTrain: epoch  3, batch     3 | loss: 73.5403526CurrentTrain: epoch  3, batch     4 | loss: 35.0034102CurrentTrain: epoch  3, batch     5 | loss: 30.2267882CurrentTrain: epoch  4, batch     0 | loss: 54.4423763CurrentTrain: epoch  4, batch     1 | loss: 53.0010334CurrentTrain: epoch  4, batch     2 | loss: 40.4611911CurrentTrain: epoch  4, batch     3 | loss: 51.1996310CurrentTrain: epoch  4, batch     4 | loss: 34.1707073CurrentTrain: epoch  4, batch     5 | loss: 30.2120452CurrentTrain: epoch  5, batch     0 | loss: 71.9781812CurrentTrain: epoch  5, batch     1 | loss: 50.8916069CurrentTrain: epoch  5, batch     2 | loss: 40.6281093CurrentTrain: epoch  5, batch     3 | loss: 36.5475235CurrentTrain: epoch  5, batch     4 | loss: 52.5659349CurrentTrain: epoch  5, batch     5 | loss: 29.5567519CurrentTrain: epoch  6, batch     0 | loss: 51.8392643CurrentTrain: epoch  6, batch     1 | loss: 50.7204335CurrentTrain: epoch  6, batch     2 | loss: 37.9211675CurrentTrain: epoch  6, batch     3 | loss: 30.4896544CurrentTrain: epoch  6, batch     4 | loss: 50.9894517CurrentTrain: epoch  6, batch     5 | loss: 44.8401285CurrentTrain: epoch  7, batch     0 | loss: 48.5797768CurrentTrain: epoch  7, batch     1 | loss: 40.1158797CurrentTrain: epoch  7, batch     2 | loss: 49.0126133CurrentTrain: epoch  7, batch     3 | loss: 66.6912630CurrentTrain: epoch  7, batch     4 | loss: 28.8219532CurrentTrain: epoch  7, batch     5 | loss: 45.4390400CurrentTrain: epoch  8, batch     0 | loss: 38.1853490CurrentTrain: epoch  8, batch     1 | loss: 48.6831454CurrentTrain: epoch  8, batch     2 | loss: 27.9664223CurrentTrain: epoch  8, batch     3 | loss: 106.9695914CurrentTrain: epoch  8, batch     4 | loss: 29.7481500CurrentTrain: epoch  8, batch     5 | loss: 44.6968525CurrentTrain: epoch  9, batch     0 | loss: 37.8376012CurrentTrain: epoch  9, batch     1 | loss: 68.6228329CurrentTrain: epoch  9, batch     2 | loss: 30.4038762CurrentTrain: epoch  9, batch     3 | loss: 49.0640591CurrentTrain: epoch  9, batch     4 | loss: 35.0856872CurrentTrain: epoch  9, batch     5 | loss: 24.7841181
MemoryTrain:  epoch  0, batch     0 | loss: 0.4380649MemoryTrain:  epoch  1, batch     0 | loss: 0.3359826MemoryTrain:  epoch  2, batch     0 | loss: 0.2118864MemoryTrain:  epoch  3, batch     0 | loss: 0.1662096MemoryTrain:  epoch  4, batch     0 | loss: 0.0999017MemoryTrain:  epoch  5, batch     0 | loss: 0.0943824MemoryTrain:  epoch  6, batch     0 | loss: 0.0629644MemoryTrain:  epoch  7, batch     0 | loss: 0.0524853MemoryTrain:  epoch  8, batch     0 | loss: 0.0471375MemoryTrain:  epoch  9, batch     0 | loss: 0.0410820

F1 score per class: {32: 0.0, 33: 0.3728813559322034, 36: 0.0, 6: 0.62, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.8571428571428571, 26: 0.0, 29: 0.35294117647058826, 30: 0.10126582278481013}
Micro-average F1 score: 0.3614457831325301
Weighted-average F1 score: 0.32317934042827884
F1 score per class: {32: 0.0, 33: 0.47533632286995514, 36: 0.0, 6: 0.6013986013986014, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.7906976744186046, 26: 0.0, 29: 0.17142857142857143, 30: 0.48}
Micro-average F1 score: 0.43157894736842106
Weighted-average F1 score: 0.394013730193462
F1 score per class: {32: 0.0, 33: 0.4690265486725664, 36: 0.0, 6: 0.6013986013986014, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.85, 26: 0.0, 29: 0.16666666666666666, 30: 0.4897959183673469}
Micro-average F1 score: 0.43967828418230565
Weighted-average F1 score: 0.4023756526920765

F1 score per class: {32: 0.38578680203045684, 33: 0.2838709677419355, 36: 0.6781115879828327, 6: 0.5254237288135594, 8: 0.1935483870967742, 19: 0.660377358490566, 20: 0.8246445497630331, 24: 0.8571428571428571, 26: 0.7798165137614679, 29: 0.3, 30: 0.09876543209876543}
Micro-average F1 score: 0.5784248841826605
Weighted-average F1 score: 0.6038999806852502
F1 score per class: {32: 0.41702127659574467, 33: 0.263681592039801, 36: 0.5972222222222222, 6: 0.46236559139784944, 8: 0.2191780821917808, 19: 0.6363636363636364, 20: 0.7763713080168776, 24: 0.4927536231884058, 26: 0.712, 29: 0.08571428571428572, 30: 0.43243243243243246}
Micro-average F1 score: 0.4955595026642984
Weighted-average F1 score: 0.47437477231271763
F1 score per class: {32: 0.4107142857142857, 33: 0.2566585956416465, 36: 0.5972222222222222, 6: 0.4648648648648649, 8: 0.25806451612903225, 19: 0.6422018348623854, 20: 0.8070175438596491, 24: 0.6666666666666666, 26: 0.7206477732793523, 29: 0.08695652173913043, 30: 0.4444444444444444}
Micro-average F1 score: 0.5043162199000454
Weighted-average F1 score: 0.4812161859690644
cur_acc:  ['0.6827', '0.3614']
his_acc:  ['0.6827', '0.5784']
cur_acc des:  ['0.6588', '0.4316']
his_acc des:  ['0.6588', '0.4956']
cur_acc rrf:  ['0.6598', '0.4397']
his_acc rrf:  ['0.6598', '0.5043']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 81.6109326CurrentTrain: epoch  0, batch     1 | loss: 63.9746435CurrentTrain: epoch  0, batch     2 | loss: 55.7250236CurrentTrain: epoch  0, batch     3 | loss: 42.7463771CurrentTrain: epoch  0, batch     4 | loss: 75.1211601CurrentTrain: epoch  0, batch     5 | loss: 74.6002989CurrentTrain: epoch  0, batch     6 | loss: 31.8535700CurrentTrain: epoch  1, batch     0 | loss: 37.6648752CurrentTrain: epoch  1, batch     1 | loss: 73.0068284CurrentTrain: epoch  1, batch     2 | loss: 108.6513875CurrentTrain: epoch  1, batch     3 | loss: 74.7095733CurrentTrain: epoch  1, batch     4 | loss: 44.7804745CurrentTrain: epoch  1, batch     5 | loss: 44.2822658CurrentTrain: epoch  1, batch     6 | loss: 19.6818638CurrentTrain: epoch  2, batch     0 | loss: 73.9646699CurrentTrain: epoch  2, batch     1 | loss: 42.4557615CurrentTrain: epoch  2, batch     2 | loss: 48.4747100CurrentTrain: epoch  2, batch     3 | loss: 112.9044844CurrentTrain: epoch  2, batch     4 | loss: 74.0017732CurrentTrain: epoch  2, batch     5 | loss: 42.7995171CurrentTrain: epoch  2, batch     6 | loss: 28.1763588CurrentTrain: epoch  3, batch     0 | loss: 39.7591408CurrentTrain: epoch  3, batch     1 | loss: 67.2185163CurrentTrain: epoch  3, batch     2 | loss: 107.2799349CurrentTrain: epoch  3, batch     3 | loss: 50.4594002CurrentTrain: epoch  3, batch     4 | loss: 109.4199060CurrentTrain: epoch  3, batch     5 | loss: 52.0862830CurrentTrain: epoch  3, batch     6 | loss: 18.2866497CurrentTrain: epoch  4, batch     0 | loss: 108.8657460CurrentTrain: epoch  4, batch     1 | loss: 67.9483338CurrentTrain: epoch  4, batch     2 | loss: 51.3484965CurrentTrain: epoch  4, batch     3 | loss: 66.6087072CurrentTrain: epoch  4, batch     4 | loss: 39.8090043CurrentTrain: epoch  4, batch     5 | loss: 38.9985680CurrentTrain: epoch  4, batch     6 | loss: 26.8188421CurrentTrain: epoch  5, batch     0 | loss: 39.4191088CurrentTrain: epoch  5, batch     1 | loss: 107.9062618CurrentTrain: epoch  5, batch     2 | loss: 109.6860821CurrentTrain: epoch  5, batch     3 | loss: 37.4302888CurrentTrain: epoch  5, batch     4 | loss: 48.4516406CurrentTrain: epoch  5, batch     5 | loss: 38.9055866CurrentTrain: epoch  5, batch     6 | loss: 16.6031825CurrentTrain: epoch  6, batch     0 | loss: 222.4781444CurrentTrain: epoch  6, batch     1 | loss: 30.5742118CurrentTrain: epoch  6, batch     2 | loss: 50.2908350CurrentTrain: epoch  6, batch     3 | loss: 50.2166776CurrentTrain: epoch  6, batch     4 | loss: 49.2578735CurrentTrain: epoch  6, batch     5 | loss: 36.5234524CurrentTrain: epoch  6, batch     6 | loss: 15.6098644CurrentTrain: epoch  7, batch     0 | loss: 68.8537120CurrentTrain: epoch  7, batch     1 | loss: 35.4375454CurrentTrain: epoch  7, batch     2 | loss: 36.8182507CurrentTrain: epoch  7, batch     3 | loss: 49.5730669CurrentTrain: epoch  7, batch     4 | loss: 30.5636121CurrentTrain: epoch  7, batch     5 | loss: 68.9087406CurrentTrain: epoch  7, batch     6 | loss: 59.7935623CurrentTrain: epoch  8, batch     0 | loss: 68.1815457CurrentTrain: epoch  8, batch     1 | loss: 30.0098728CurrentTrain: epoch  8, batch     2 | loss: 49.7017168CurrentTrain: epoch  8, batch     3 | loss: 67.1547243CurrentTrain: epoch  8, batch     4 | loss: 46.9659704CurrentTrain: epoch  8, batch     5 | loss: 49.4433835CurrentTrain: epoch  8, batch     6 | loss: 14.6745659CurrentTrain: epoch  9, batch     0 | loss: 103.6024270CurrentTrain: epoch  9, batch     1 | loss: 36.7825748CurrentTrain: epoch  9, batch     2 | loss: 68.4398770CurrentTrain: epoch  9, batch     3 | loss: 38.0083075CurrentTrain: epoch  9, batch     4 | loss: 37.0555046CurrentTrain: epoch  9, batch     5 | loss: 45.0337252CurrentTrain: epoch  9, batch     6 | loss: 26.6224750
MemoryTrain:  epoch  0, batch     0 | loss: 0.4868423MemoryTrain:  epoch  1, batch     0 | loss: 0.4170041MemoryTrain:  epoch  2, batch     0 | loss: 0.2477094MemoryTrain:  epoch  3, batch     0 | loss: 0.2382233MemoryTrain:  epoch  4, batch     0 | loss: 0.1445862MemoryTrain:  epoch  5, batch     0 | loss: 0.1310281MemoryTrain:  epoch  6, batch     0 | loss: 0.1119120MemoryTrain:  epoch  7, batch     0 | loss: 0.1093073MemoryTrain:  epoch  8, batch     0 | loss: 0.0870871MemoryTrain:  epoch  9, batch     0 | loss: 0.0702971

F1 score per class: {32: 0.41379310344827586, 2: 0.0, 6: 0.0, 39: 0.44274809160305345, 8: 0.24390243902439024, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.0, 24: 0.17647058823529413, 26: 0.0, 28: 0.0, 29: 0.09523809523809523}
Micro-average F1 score: 0.2559241706161137
Weighted-average F1 score: 0.18807235500611874
F1 score per class: {32: 0.2807017543859649, 2: 0.0, 36: 0.0, 6: 0.5194805194805194, 39: 0.551440329218107, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.12987012987012986, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.18181818181818182}
Micro-average F1 score: 0.34793187347931875
Weighted-average F1 score: 0.29690041457713817
F1 score per class: {32: 0.2909090909090909, 2: 0.0, 36: 0.0, 6: 0.5172413793103449, 39: 0.5583333333333333, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.12048192771084337, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.14285714285714285}
Micro-average F1 score: 0.35192069392812886
Weighted-average F1 score: 0.30113501368620366

F1 score per class: {32: 0.3076923076923077, 33: 0.37209302325581395, 2: 0.17094017094017094, 36: 0.27358490566037735, 6: 0.20408163265306123, 39: 0.61, 8: 0.3157894736842105, 11: 0.26666666666666666, 12: 0.6766169154228856, 19: 0.06818181818181818, 20: 0.8137254901960784, 24: 0.8823529411764706, 26: 0.7063492063492064, 28: 0.0, 29: 0.058823529411764705, 30: 0.08}
Micro-average F1 score: 0.4489795918367347
Weighted-average F1 score: 0.4595021761017123
F1 score per class: {32: 0.1523809523809524, 33: 0.4068767908309456, 2: 0.25757575757575757, 36: 0.2863961813842482, 6: 0.25868725868725867, 39: 0.6104417670682731, 8: 0.35233160621761656, 11: 0.17582417582417584, 12: 0.6511627906976745, 19: 0.05263157894736842, 20: 0.8161434977578476, 24: 0.6666666666666666, 26: 0.5806451612903226, 28: 0.2857142857142857, 29: 0.2831858407079646, 30: 0.11320754716981132}
Micro-average F1 score: 0.38893844781445136
Weighted-average F1 score: 0.36303543005473765
F1 score per class: {32: 0.16326530612244897, 33: 0.3834808259587021, 2: 0.21666666666666667, 36: 0.27842227378190254, 6: 0.26120857699805067, 39: 0.6129032258064516, 8: 0.3469387755102041, 11: 0.19047619047619047, 12: 0.6542056074766355, 19: 0.04739336492890995, 20: 0.8186046511627907, 24: 0.7307692307692307, 26: 0.5769230769230769, 28: 0.3076923076923077, 29: 0.2692307692307692, 30: 0.1}
Micro-average F1 score: 0.38308157099697887
Weighted-average F1 score: 0.3564329555745109
cur_acc:  ['0.6827', '0.3614', '0.2559']
his_acc:  ['0.6827', '0.5784', '0.4490']
cur_acc des:  ['0.6588', '0.4316', '0.3479']
his_acc des:  ['0.6588', '0.4956', '0.3889']
cur_acc rrf:  ['0.6598', '0.4397', '0.3519']
his_acc rrf:  ['0.6598', '0.5043', '0.3831']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 52.7674209CurrentTrain: epoch  0, batch     1 | loss: 58.5541794CurrentTrain: epoch  0, batch     2 | loss: 79.5959932CurrentTrain: epoch  0, batch     3 | loss: 77.8214611CurrentTrain: epoch  0, batch     4 | loss: 110.5087774CurrentTrain: epoch  0, batch     5 | loss: 80.7225426CurrentTrain: epoch  0, batch     6 | loss: 60.1666062CurrentTrain: epoch  0, batch     7 | loss: 2.9845044CurrentTrain: epoch  1, batch     0 | loss: 119.3718147CurrentTrain: epoch  1, batch     1 | loss: 40.4643074CurrentTrain: epoch  1, batch     2 | loss: 43.5697846CurrentTrain: epoch  1, batch     3 | loss: 112.0193633CurrentTrain: epoch  1, batch     4 | loss: 109.6798675CurrentTrain: epoch  1, batch     5 | loss: 41.0879667CurrentTrain: epoch  1, batch     6 | loss: 56.1288420CurrentTrain: epoch  1, batch     7 | loss: 2.9224466CurrentTrain: epoch  2, batch     0 | loss: 106.8327361CurrentTrain: epoch  2, batch     1 | loss: 68.3199137CurrentTrain: epoch  2, batch     2 | loss: 50.4394999CurrentTrain: epoch  2, batch     3 | loss: 33.7990641CurrentTrain: epoch  2, batch     4 | loss: 111.1912838CurrentTrain: epoch  2, batch     5 | loss: 42.9724968CurrentTrain: epoch  2, batch     6 | loss: 51.5904090CurrentTrain: epoch  2, batch     7 | loss: 2.8520402CurrentTrain: epoch  3, batch     0 | loss: 223.0570290CurrentTrain: epoch  3, batch     1 | loss: 38.5687556CurrentTrain: epoch  3, batch     2 | loss: 69.2221594CurrentTrain: epoch  3, batch     3 | loss: 50.3215088CurrentTrain: epoch  3, batch     4 | loss: 222.7268860CurrentTrain: epoch  3, batch     5 | loss: 47.6608891CurrentTrain: epoch  3, batch     6 | loss: 37.9924466CurrentTrain: epoch  3, batch     7 | loss: 2.8272973CurrentTrain: epoch  4, batch     0 | loss: 50.7645036CurrentTrain: epoch  4, batch     1 | loss: 108.1442199CurrentTrain: epoch  4, batch     2 | loss: 39.4164338CurrentTrain: epoch  4, batch     3 | loss: 69.7918252CurrentTrain: epoch  4, batch     4 | loss: 47.5358629CurrentTrain: epoch  4, batch     5 | loss: 47.4672987CurrentTrain: epoch  4, batch     6 | loss: 51.0208625CurrentTrain: epoch  4, batch     7 | loss: 2.8920883CurrentTrain: epoch  5, batch     0 | loss: 107.2308542CurrentTrain: epoch  5, batch     1 | loss: 38.7703153CurrentTrain: epoch  5, batch     2 | loss: 48.8027851CurrentTrain: epoch  5, batch     3 | loss: 37.1037681CurrentTrain: epoch  5, batch     4 | loss: 108.2177949CurrentTrain: epoch  5, batch     5 | loss: 47.5059141CurrentTrain: epoch  5, batch     6 | loss: 48.6799429CurrentTrain: epoch  5, batch     7 | loss: 2.8464792CurrentTrain: epoch  6, batch     0 | loss: 50.4358650CurrentTrain: epoch  6, batch     1 | loss: 68.6035773CurrentTrain: epoch  6, batch     2 | loss: 68.5316576CurrentTrain: epoch  6, batch     3 | loss: 46.9951763CurrentTrain: epoch  6, batch     4 | loss: 66.4885053CurrentTrain: epoch  6, batch     5 | loss: 37.3598319CurrentTrain: epoch  6, batch     6 | loss: 66.6111688CurrentTrain: epoch  6, batch     7 | loss: 2.8199542CurrentTrain: epoch  7, batch     0 | loss: 66.6112143CurrentTrain: epoch  7, batch     1 | loss: 107.1549950CurrentTrain: epoch  7, batch     2 | loss: 48.5917710CurrentTrain: epoch  7, batch     3 | loss: 64.8678924CurrentTrain: epoch  7, batch     4 | loss: 49.5647373CurrentTrain: epoch  7, batch     5 | loss: 47.4585932CurrentTrain: epoch  7, batch     6 | loss: 47.2435280CurrentTrain: epoch  7, batch     7 | loss: 2.8573761CurrentTrain: epoch  8, batch     0 | loss: 36.2147309CurrentTrain: epoch  8, batch     1 | loss: 50.3806733CurrentTrain: epoch  8, batch     2 | loss: 68.1707883CurrentTrain: epoch  8, batch     3 | loss: 30.1993943CurrentTrain: epoch  8, batch     4 | loss: 49.0417752CurrentTrain: epoch  8, batch     5 | loss: 48.0802374CurrentTrain: epoch  8, batch     6 | loss: 107.2082904CurrentTrain: epoch  8, batch     7 | loss: 2.8155113CurrentTrain: epoch  9, batch     0 | loss: 48.2826479CurrentTrain: epoch  9, batch     1 | loss: 68.3375309CurrentTrain: epoch  9, batch     2 | loss: 66.4648412CurrentTrain: epoch  9, batch     3 | loss: 46.4711025CurrentTrain: epoch  9, batch     4 | loss: 64.4435700CurrentTrain: epoch  9, batch     5 | loss: 64.7112915CurrentTrain: epoch  9, batch     6 | loss: 68.1663005CurrentTrain: epoch  9, batch     7 | loss: 0.3130203
MemoryTrain:  epoch  0, batch     0 | loss: 0.3697075MemoryTrain:  epoch  1, batch     0 | loss: 0.3107409MemoryTrain:  epoch  2, batch     0 | loss: 0.2098138MemoryTrain:  epoch  3, batch     0 | loss: 0.1593493MemoryTrain:  epoch  4, batch     0 | loss: 0.1252868MemoryTrain:  epoch  5, batch     0 | loss: 0.1054201MemoryTrain:  epoch  6, batch     0 | loss: 0.1018788MemoryTrain:  epoch  7, batch     0 | loss: 0.0737604MemoryTrain:  epoch  8, batch     0 | loss: 0.0690247MemoryTrain:  epoch  9, batch     0 | loss: 0.0639758

F1 score per class: {2: 0.0, 5: 0.93, 6: 0.0, 8: 0.0, 10: 0.2676056338028169, 11: 0.0, 12: 0.0, 16: 0.6086956521739131, 17: 0.6153846153846154, 18: 0.2, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.46579804560260585
Weighted-average F1 score: 0.3995269393965662
F1 score per class: {2: 0.0, 5: 0.6468646864686468, 6: 0.0, 8: 0.0, 10: 0.5164835164835165, 11: 0.0, 12: 0.0, 16: 0.5777777777777777, 17: 0.5882352941176471, 18: 0.2781456953642384, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.3675373134328358
Weighted-average F1 score: 0.30747027386421444
F1 score per class: {2: 0.0, 5: 0.6468646864686468, 6: 0.0, 8: 0.0, 10: 0.5230769230769231, 11: 0.0, 12: 0.0, 16: 0.5454545454545454, 17: 0.625, 18: 0.27631578947368424, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.3882926829268293
Weighted-average F1 score: 0.33216758140357855

F1 score per class: {2: 0.3125, 5: 0.9029126213592233, 6: 0.3475609756097561, 8: 0.109375, 10: 0.1688888888888889, 11: 0.08108108108108109, 12: 0.16551724137931034, 16: 0.4772727272727273, 17: 0.22857142857142856, 18: 0.13186813186813187, 19: 0.5779816513761468, 20: 0.3300970873786408, 24: 0.24, 26: 0.6764705882352942, 28: 0.09523809523809523, 29: 0.7766990291262136, 30: 0.8823529411764706, 32: 0.6075085324232082, 33: 0.0, 36: 0.029850746268656716, 39: 0.06896551724137931}
Micro-average F1 score: 0.4218635363169479
Weighted-average F1 score: 0.43520720081835373
F1 score per class: {2: 0.2608695652173913, 5: 0.5104166666666666, 6: 0.3645320197044335, 8: 0.21212121212121213, 10: 0.33935018050541516, 11: 0.17040358744394618, 12: 0.24050632911392406, 16: 0.43333333333333335, 17: 0.16129032258064516, 18: 0.14334470989761092, 19: 0.5357142857142857, 20: 0.3468208092485549, 24: 0.14545454545454545, 26: 0.639269406392694, 28: 0.0736196319018405, 29: 0.7909090909090909, 30: 0.6440677966101694, 32: 0.48663101604278075, 33: 0.3333333333333333, 36: 0.20618556701030927, 39: 0.10344827586206896}
Micro-average F1 score: 0.3625
Weighted-average F1 score: 0.34723362571585287
F1 score per class: {2: 0.2608695652173913, 5: 0.5117493472584856, 6: 0.3641025641025641, 8: 0.19469026548672566, 10: 0.3167701863354037, 11: 0.16593886462882096, 12: 0.25474254742547425, 16: 0.43243243243243246, 17: 0.2127659574468085, 18: 0.14334470989761092, 19: 0.5319148936170213, 20: 0.3314917127071823, 24: 0.16842105263157894, 26: 0.6481481481481481, 28: 0.05263157894736842, 29: 0.7906976744186046, 30: 0.7169811320754716, 32: 0.5, 33: 0.3333333333333333, 36: 0.21739130434782608, 39: 0.09302325581395349}
Micro-average F1 score: 0.3687908892658105
Weighted-average F1 score: 0.35483275803847336
cur_acc:  ['0.6827', '0.3614', '0.2559', '0.4658']
his_acc:  ['0.6827', '0.5784', '0.4490', '0.4219']
cur_acc des:  ['0.6588', '0.4316', '0.3479', '0.3675']
his_acc des:  ['0.6588', '0.4956', '0.3889', '0.3625']
cur_acc rrf:  ['0.6598', '0.4397', '0.3519', '0.3883']
his_acc rrf:  ['0.6598', '0.5043', '0.3831', '0.3688']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 63.6322818CurrentTrain: epoch  0, batch     1 | loss: 71.4143460CurrentTrain: epoch  0, batch     2 | loss: 62.4148643CurrentTrain: epoch  0, batch     3 | loss: 93.8885556CurrentTrain: epoch  0, batch     4 | loss: 56.1847067CurrentTrain: epoch  0, batch     5 | loss: 54.5381601CurrentTrain: epoch  0, batch     6 | loss: 114.1384018CurrentTrain: epoch  1, batch     0 | loss: 43.2759517CurrentTrain: epoch  1, batch     1 | loss: 73.5963119CurrentTrain: epoch  1, batch     2 | loss: 113.3134425CurrentTrain: epoch  1, batch     3 | loss: 54.6788740CurrentTrain: epoch  1, batch     4 | loss: 42.1245813CurrentTrain: epoch  1, batch     5 | loss: 53.1635060CurrentTrain: epoch  1, batch     6 | loss: 64.6065800CurrentTrain: epoch  2, batch     0 | loss: 223.2430758CurrentTrain: epoch  2, batch     1 | loss: 71.9381392CurrentTrain: epoch  2, batch     2 | loss: 49.4448201CurrentTrain: epoch  2, batch     3 | loss: 32.7391200CurrentTrain: epoch  2, batch     4 | loss: 49.0265559CurrentTrain: epoch  2, batch     5 | loss: 105.5287158CurrentTrain: epoch  2, batch     6 | loss: 67.1971077CurrentTrain: epoch  3, batch     0 | loss: 72.4135145CurrentTrain: epoch  3, batch     1 | loss: 65.9773536CurrentTrain: epoch  3, batch     2 | loss: 39.6232588CurrentTrain: epoch  3, batch     3 | loss: 107.7555207CurrentTrain: epoch  3, batch     4 | loss: 39.9981807CurrentTrain: epoch  3, batch     5 | loss: 69.6692800CurrentTrain: epoch  3, batch     6 | loss: 99.3611151CurrentTrain: epoch  4, batch     0 | loss: 47.9637680CurrentTrain: epoch  4, batch     1 | loss: 39.3842130CurrentTrain: epoch  4, batch     2 | loss: 68.7052466CurrentTrain: epoch  4, batch     3 | loss: 38.4086071CurrentTrain: epoch  4, batch     4 | loss: 68.9064620CurrentTrain: epoch  4, batch     5 | loss: 69.0466927CurrentTrain: epoch  4, batch     6 | loss: 46.9221299CurrentTrain: epoch  5, batch     0 | loss: 69.8027200CurrentTrain: epoch  5, batch     1 | loss: 49.4578326CurrentTrain: epoch  5, batch     2 | loss: 38.5030725CurrentTrain: epoch  5, batch     3 | loss: 66.8114927CurrentTrain: epoch  5, batch     4 | loss: 49.5012118CurrentTrain: epoch  5, batch     5 | loss: 39.3932465CurrentTrain: epoch  5, batch     6 | loss: 44.6316905CurrentTrain: epoch  6, batch     0 | loss: 36.4942913CurrentTrain: epoch  6, batch     1 | loss: 66.8658011CurrentTrain: epoch  6, batch     2 | loss: 66.9445262CurrentTrain: epoch  6, batch     3 | loss: 39.0873778CurrentTrain: epoch  6, batch     4 | loss: 107.1765833CurrentTrain: epoch  6, batch     5 | loss: 68.6620351CurrentTrain: epoch  6, batch     6 | loss: 35.2660205CurrentTrain: epoch  7, batch     0 | loss: 48.8839413CurrentTrain: epoch  7, batch     1 | loss: 103.7334516CurrentTrain: epoch  7, batch     2 | loss: 36.9393223CurrentTrain: epoch  7, batch     3 | loss: 48.5026314CurrentTrain: epoch  7, batch     4 | loss: 64.6340833CurrentTrain: epoch  7, batch     5 | loss: 68.1555155CurrentTrain: epoch  7, batch     6 | loss: 63.9302098CurrentTrain: epoch  8, batch     0 | loss: 29.7657195CurrentTrain: epoch  8, batch     1 | loss: 104.2673634CurrentTrain: epoch  8, batch     2 | loss: 46.8228162CurrentTrain: epoch  8, batch     3 | loss: 47.2271260CurrentTrain: epoch  8, batch     4 | loss: 68.2580766CurrentTrain: epoch  8, batch     5 | loss: 49.5494036CurrentTrain: epoch  8, batch     6 | loss: 103.1234640CurrentTrain: epoch  9, batch     0 | loss: 68.3810739CurrentTrain: epoch  9, batch     1 | loss: 107.2590518CurrentTrain: epoch  9, batch     2 | loss: 35.1224577CurrentTrain: epoch  9, batch     3 | loss: 68.3779672CurrentTrain: epoch  9, batch     4 | loss: 45.6702184CurrentTrain: epoch  9, batch     5 | loss: 107.3088793CurrentTrain: epoch  9, batch     6 | loss: 28.4907301
MemoryTrain:  epoch  0, batch     0 | loss: 0.5176412MemoryTrain:  epoch  1, batch     0 | loss: 0.7887893MemoryTrain:  epoch  2, batch     0 | loss: 0.3591050MemoryTrain:  epoch  3, batch     0 | loss: 0.3336588MemoryTrain:  epoch  4, batch     0 | loss: 0.2860918MemoryTrain:  epoch  5, batch     0 | loss: 0.2673564MemoryTrain:  epoch  6, batch     0 | loss: 0.2053259MemoryTrain:  epoch  7, batch     0 | loss: 0.1783095MemoryTrain:  epoch  8, batch     0 | loss: 0.1481564MemoryTrain:  epoch  9, batch     0 | loss: 0.1249969

F1 score per class: {32: 0.08148148148148149, 1: 0.3815789473684211, 34: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.07909604519774012, 11: 0.0, 14: 0.0, 18: 0.0, 19: 0.2647058823529412, 20: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 29: 0.4854368932038835}
Micro-average F1 score: 0.18588640275387264
Weighted-average F1 score: 0.15024636015486778
F1 score per class: {1: 0.141643059490085, 2: 0.0, 3: 0.41194029850746267, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.038910505836575876, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.3950617283950617, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.5323741007194245}
Micro-average F1 score: 0.22262552934059285
Weighted-average F1 score: 0.1960224436365512
F1 score per class: {1: 0.1400560224089636, 2: 0.0, 3: 0.4041095890410959, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.03937007874015748, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.39669421487603307, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.5323741007194245}
Micro-average F1 score: 0.21900566393958465
Weighted-average F1 score: 0.1904571483174754

F1 score per class: {1: 0.06321839080459771, 2: 0.26666666666666666, 3: 0.29292929292929293, 5: 0.9029126213592233, 6: 0.38345864661654133, 8: 0.08130081300813008, 10: 0.18779342723004694, 11: 0.07741935483870968, 12: 0.17266187050359713, 14: 0.05384615384615385, 16: 0.4883720930232558, 17: 0.24, 18: 0.136986301369863, 19: 0.2929936305732484, 20: 0.2653061224489796, 22: 0.22712933753943218, 24: 0.0425531914893617, 26: 0.6764705882352942, 28: 0.08791208791208792, 29: 0.7342995169082126, 30: 0.8571428571428571, 32: 0.48502994011976047, 33: 0.0, 34: 0.2777777777777778, 36: 0.029411764705882353, 39: 0.08333333333333333}
Micro-average F1 score: 0.31424903722721437
Weighted-average F1 score: 0.3030501405344254
F1 score per class: {1: 0.11185682326621924, 2: 0.27906976744186046, 3: 0.23752151462994836, 5: 0.5283018867924528, 6: 0.3390804597701149, 8: 0.23, 10: 0.33935018050541516, 11: 0.1415929203539823, 12: 0.273972602739726, 14: 0.021551724137931036, 16: 0.45714285714285713, 17: 0.21818181818181817, 18: 0.13756613756613756, 19: 0.3786008230452675, 20: 0.33136094674556216, 22: 0.32653061224489793, 24: 0.045454545454545456, 26: 0.6278026905829597, 28: 0.04819277108433735, 29: 0.7441860465116279, 30: 0.6938775510204082, 32: 0.44155844155844154, 33: 0.4, 34: 0.23417721518987342, 36: 0.33043478260869563, 39: 0.03636363636363636}
Micro-average F1 score: 0.2950439250787336
Weighted-average F1 score: 0.2762210657358929
F1 score per class: {1: 0.1072961373390558, 2: 0.27906976744186046, 3: 0.23694779116465864, 5: 0.536986301369863, 6: 0.3136094674556213, 8: 0.19767441860465115, 10: 0.33548387096774196, 11: 0.12931034482758622, 12: 0.28493150684931506, 14: 0.022026431718061675, 16: 0.46601941747572817, 17: 0.27906976744186046, 18: 0.13978494623655913, 19: 0.39148936170212767, 20: 0.3236994219653179, 22: 0.3333333333333333, 24: 0.044444444444444446, 26: 0.639269406392694, 28: 0.05555555555555555, 29: 0.7393364928909952, 30: 0.8292682926829268, 32: 0.44502617801047123, 33: 0.2857142857142857, 34: 0.22981366459627328, 36: 0.28846153846153844, 39: 0.047619047619047616}
Micro-average F1 score: 0.29577221742881793
Weighted-average F1 score: 0.27685412384142905
cur_acc:  ['0.6827', '0.3614', '0.2559', '0.4658', '0.1859']
his_acc:  ['0.6827', '0.5784', '0.4490', '0.4219', '0.3142']
cur_acc des:  ['0.6588', '0.4316', '0.3479', '0.3675', '0.2226']
his_acc des:  ['0.6588', '0.4956', '0.3889', '0.3625', '0.2950']
cur_acc rrf:  ['0.6598', '0.4397', '0.3519', '0.3883', '0.2190']
his_acc rrf:  ['0.6598', '0.5043', '0.3831', '0.3688', '0.2958']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 52.6423500CurrentTrain: epoch  0, batch     1 | loss: 51.7687639CurrentTrain: epoch  0, batch     2 | loss: 47.4517907CurrentTrain: epoch  0, batch     3 | loss: 110.1064914CurrentTrain: epoch  0, batch     4 | loss: 24.4720071CurrentTrain: epoch  1, batch     0 | loss: 55.2208415CurrentTrain: epoch  1, batch     1 | loss: 41.3821203CurrentTrain: epoch  1, batch     2 | loss: 45.5088434CurrentTrain: epoch  1, batch     3 | loss: 35.9067545CurrentTrain: epoch  1, batch     4 | loss: 47.8620597CurrentTrain: epoch  2, batch     0 | loss: 71.4911396CurrentTrain: epoch  2, batch     1 | loss: 40.0554402CurrentTrain: epoch  2, batch     2 | loss: 53.6686763CurrentTrain: epoch  2, batch     3 | loss: 32.0300868CurrentTrain: epoch  2, batch     4 | loss: 26.2459302CurrentTrain: epoch  3, batch     0 | loss: 30.7044002CurrentTrain: epoch  3, batch     1 | loss: 75.2102248CurrentTrain: epoch  3, batch     2 | loss: 38.1750632CurrentTrain: epoch  3, batch     3 | loss: 49.6129596CurrentTrain: epoch  3, batch     4 | loss: 23.3334828CurrentTrain: epoch  4, batch     0 | loss: 38.4261606CurrentTrain: epoch  4, batch     1 | loss: 66.7526768CurrentTrain: epoch  4, batch     2 | loss: 46.3140690CurrentTrain: epoch  4, batch     3 | loss: 36.8641163CurrentTrain: epoch  4, batch     4 | loss: 66.8518522CurrentTrain: epoch  5, batch     0 | loss: 48.0223438CurrentTrain: epoch  5, batch     1 | loss: 28.9508804CurrentTrain: epoch  5, batch     2 | loss: 48.4735036CurrentTrain: epoch  5, batch     3 | loss: 37.8352273CurrentTrain: epoch  5, batch     4 | loss: 24.0643951CurrentTrain: epoch  6, batch     0 | loss: 34.9161553CurrentTrain: epoch  6, batch     1 | loss: 29.4830396CurrentTrain: epoch  6, batch     2 | loss: 30.7202761CurrentTrain: epoch  6, batch     3 | loss: 67.4246751CurrentTrain: epoch  6, batch     4 | loss: 30.7123976CurrentTrain: epoch  7, batch     0 | loss: 45.6847564CurrentTrain: epoch  7, batch     1 | loss: 37.2381422CurrentTrain: epoch  7, batch     2 | loss: 38.8312780CurrentTrain: epoch  7, batch     3 | loss: 46.6871091CurrentTrain: epoch  7, batch     4 | loss: 37.9947192CurrentTrain: epoch  8, batch     0 | loss: 99.1744731CurrentTrain: epoch  8, batch     1 | loss: 34.9383771CurrentTrain: epoch  8, batch     2 | loss: 47.0783548CurrentTrain: epoch  8, batch     3 | loss: 36.6383985CurrentTrain: epoch  8, batch     4 | loss: 22.9840149CurrentTrain: epoch  9, batch     0 | loss: 64.8281276CurrentTrain: epoch  9, batch     1 | loss: 36.4878593CurrentTrain: epoch  9, batch     2 | loss: 35.1537352CurrentTrain: epoch  9, batch     3 | loss: 28.4535896CurrentTrain: epoch  9, batch     4 | loss: 41.0910363
MemoryTrain:  epoch  0, batch     0 | loss: 0.2943807MemoryTrain:  epoch  1, batch     0 | loss: 0.2591919MemoryTrain:  epoch  2, batch     0 | loss: 0.2293446MemoryTrain:  epoch  3, batch     0 | loss: 0.1872322MemoryTrain:  epoch  4, batch     0 | loss: 0.1458136MemoryTrain:  epoch  5, batch     0 | loss: 0.1221964MemoryTrain:  epoch  6, batch     0 | loss: 0.1041237MemoryTrain:  epoch  7, batch     0 | loss: 0.0923609MemoryTrain:  epoch  8, batch     0 | loss: 0.0778243MemoryTrain:  epoch  9, batch     0 | loss: 0.0743785

F1 score per class: {1: 0.0, 3: 0.0, 6: 0.0, 7: 0.5, 8: 0.0, 9: 0.9433962264150944, 11: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.2222222222222222, 31: 0.0, 32: 0.0, 34: 0.0, 40: 0.1935483870967742}
Micro-average F1 score: 0.2714285714285714
Weighted-average F1 score: 0.19832240302251813
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4, 8: 0.0, 9: 0.78125, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.2962962962962963, 28: 0.0, 30: 0.0, 31: 0.25, 32: 0.0, 34: 0.0, 40: 0.38636363636363635}
Micro-average F1 score: 0.30180180180180183
Weighted-average F1 score: 0.24564521477400264
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4, 8: 0.0, 9: 0.8333333333333334, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.32, 28: 0.0, 31: 0.4, 32: 0.0, 34: 0.0, 40: 0.38596491228070173}
Micro-average F1 score: 0.3170731707317073
Weighted-average F1 score: 0.253809862494073

F1 score per class: {1: 0.10948905109489052, 2: 0.22727272727272727, 3: 0.2702702702702703, 5: 0.8755760368663594, 6: 0.25870646766169153, 7: 0.044444444444444446, 8: 0.12280701754385964, 9: 0.8928571428571429, 10: 0.14193548387096774, 11: 0.11538461538461539, 12: 0.1111111111111111, 14: 0.0653061224489796, 16: 0.5, 17: 0.35294117647058826, 18: 0.14285714285714285, 19: 0.34146341463414637, 20: 0.27972027972027974, 22: 0.24, 24: 0.038461538461538464, 26: 0.6507177033492823, 27: 0.030303030303030304, 28: 0.10256410256410256, 29: 0.7177033492822966, 30: 0.8421052631578947, 31: 0.0, 32: 0.5384615384615384, 33: 0.0, 34: 0.3291139240506329, 36: 0.029850746268656716, 39: 0.05714285714285714, 40: 0.1651376146788991}
Micro-average F1 score: 0.3111760409057706
Weighted-average F1 score: 0.3014322691840979
F1 score per class: {1: 0.11618257261410789, 2: 0.16216216216216217, 3: 0.2857142857142857, 5: 0.4688995215311005, 6: 0.2698412698412698, 7: 0.031496062992125984, 8: 0.17543859649122806, 9: 0.6578947368421053, 10: 0.26337448559670784, 11: 0.09326424870466321, 12: 0.25301204819277107, 14: 0.02544529262086514, 16: 0.4230769230769231, 17: 0.2553191489361702, 18: 0.09701492537313433, 19: 0.3781818181818182, 20: 0.26356589147286824, 22: 0.33064516129032256, 24: 0.046511627906976744, 26: 0.6160714285714286, 27: 0.031746031746031744, 28: 0.05128205128205128, 29: 0.7348837209302326, 30: 0.6538461538461539, 31: 0.020100502512562814, 32: 0.4438356164383562, 33: 0.3157894736842105, 34: 0.19759036144578312, 36: 0.40350877192982454, 39: 0.029411764705882353, 40: 0.17042606516290726}
Micro-average F1 score: 0.2599171107164002
Weighted-average F1 score: 0.23916084953581987
F1 score per class: {1: 0.12075471698113208, 2: 0.17391304347826086, 3: 0.2923076923076923, 5: 0.5038560411311054, 6: 0.2786885245901639, 7: 0.03076923076923077, 8: 0.18072289156626506, 9: 0.7692307692307693, 10: 0.24812030075187969, 11: 0.1073170731707317, 12: 0.24193548387096775, 14: 0.0330188679245283, 16: 0.4444444444444444, 17: 0.2857142857142857, 18: 0.11255411255411256, 19: 0.38405797101449274, 20: 0.24812030075187969, 22: 0.3418803418803419, 24: 0.043478260869565216, 26: 0.6133333333333333, 27: 0.03305785123966942, 28: 0.05847953216374269, 29: 0.719626168224299, 30: 0.7441860465116279, 31: 0.02127659574468085, 32: 0.4576271186440678, 33: 0.26666666666666666, 34: 0.19523809523809524, 36: 0.26262626262626265, 39: 0.043478260869565216, 40: 0.18911174785100288}
Micro-average F1 score: 0.26706795501346425
Weighted-average F1 score: 0.24634719026717553
cur_acc:  ['0.6827', '0.3614', '0.2559', '0.4658', '0.1859', '0.2714']
his_acc:  ['0.6827', '0.5784', '0.4490', '0.4219', '0.3142', '0.3112']
cur_acc des:  ['0.6588', '0.4316', '0.3479', '0.3675', '0.2226', '0.3018']
his_acc des:  ['0.6588', '0.4956', '0.3889', '0.3625', '0.2950', '0.2599']
cur_acc rrf:  ['0.6598', '0.4397', '0.3519', '0.3883', '0.2190', '0.3171']
his_acc rrf:  ['0.6598', '0.5043', '0.3831', '0.3688', '0.2958', '0.2671']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 54.3182780CurrentTrain: epoch  0, batch     1 | loss: 65.1553636CurrentTrain: epoch  0, batch     2 | loss: 68.0113215CurrentTrain: epoch  0, batch     3 | loss: 52.6758891CurrentTrain: epoch  0, batch     4 | loss: 44.0566392CurrentTrain: epoch  0, batch     5 | loss: 120.8758733CurrentTrain: epoch  1, batch     0 | loss: 33.8839712CurrentTrain: epoch  1, batch     1 | loss: 45.7051403CurrentTrain: epoch  1, batch     2 | loss: 55.9713689CurrentTrain: epoch  1, batch     3 | loss: 42.7292684CurrentTrain: epoch  1, batch     4 | loss: 74.0669946CurrentTrain: epoch  1, batch     5 | loss: 57.0601964CurrentTrain: epoch  2, batch     0 | loss: 39.2579407CurrentTrain: epoch  2, batch     1 | loss: 40.2380747CurrentTrain: epoch  2, batch     2 | loss: 68.9104094CurrentTrain: epoch  2, batch     3 | loss: 40.7591505CurrentTrain: epoch  2, batch     4 | loss: 68.0622853CurrentTrain: epoch  2, batch     5 | loss: 20.7213151CurrentTrain: epoch  3, batch     0 | loss: 63.4868191CurrentTrain: epoch  3, batch     1 | loss: 49.6178321CurrentTrain: epoch  3, batch     2 | loss: 47.4332723CurrentTrain: epoch  3, batch     3 | loss: 51.9354887CurrentTrain: epoch  3, batch     4 | loss: 51.6740794CurrentTrain: epoch  3, batch     5 | loss: 37.1038452CurrentTrain: epoch  4, batch     0 | loss: 38.0113905CurrentTrain: epoch  4, batch     1 | loss: 67.0608652CurrentTrain: epoch  4, batch     2 | loss: 47.5839992CurrentTrain: epoch  4, batch     3 | loss: 48.2189023CurrentTrain: epoch  4, batch     4 | loss: 45.9122398CurrentTrain: epoch  4, batch     5 | loss: 57.7562762CurrentTrain: epoch  5, batch     0 | loss: 50.2677000CurrentTrain: epoch  5, batch     1 | loss: 47.6066337CurrentTrain: epoch  5, batch     2 | loss: 36.9782039CurrentTrain: epoch  5, batch     3 | loss: 49.6805571CurrentTrain: epoch  5, batch     4 | loss: 48.3062127CurrentTrain: epoch  5, batch     5 | loss: 22.3910916CurrentTrain: epoch  6, batch     0 | loss: 68.4484746CurrentTrain: epoch  6, batch     1 | loss: 66.5461732CurrentTrain: epoch  6, batch     2 | loss: 27.7598566CurrentTrain: epoch  6, batch     3 | loss: 66.8775948CurrentTrain: epoch  6, batch     4 | loss: 29.3553755CurrentTrain: epoch  6, batch     5 | loss: 57.0775266CurrentTrain: epoch  7, batch     0 | loss: 45.8384661CurrentTrain: epoch  7, batch     1 | loss: 46.2858301CurrentTrain: epoch  7, batch     2 | loss: 49.5311016CurrentTrain: epoch  7, batch     3 | loss: 37.9141485CurrentTrain: epoch  7, batch     4 | loss: 36.8220440CurrentTrain: epoch  7, batch     5 | loss: 34.9107282CurrentTrain: epoch  8, batch     0 | loss: 68.2527884CurrentTrain: epoch  8, batch     1 | loss: 36.5467659CurrentTrain: epoch  8, batch     2 | loss: 28.3741910CurrentTrain: epoch  8, batch     3 | loss: 36.4916501CurrentTrain: epoch  8, batch     4 | loss: 46.7892005CurrentTrain: epoch  8, batch     5 | loss: 120.5818837CurrentTrain: epoch  9, batch     0 | loss: 44.8242656CurrentTrain: epoch  9, batch     1 | loss: 48.5149397CurrentTrain: epoch  9, batch     2 | loss: 47.6337623CurrentTrain: epoch  9, batch     3 | loss: 47.8323216CurrentTrain: epoch  9, batch     4 | loss: 37.7208735CurrentTrain: epoch  9, batch     5 | loss: 34.8169678
MemoryTrain:  epoch  0, batch     0 | loss: 0.2185357MemoryTrain:  epoch  0, batch     1 | loss: 0.1873246MemoryTrain:  epoch  1, batch     0 | loss: 0.2595434MemoryTrain:  epoch  1, batch     1 | loss: 0.1398158MemoryTrain:  epoch  2, batch     0 | loss: 0.1677746MemoryTrain:  epoch  2, batch     1 | loss: 0.1258828MemoryTrain:  epoch  3, batch     0 | loss: 0.1341650MemoryTrain:  epoch  3, batch     1 | loss: 0.0924986MemoryTrain:  epoch  4, batch     0 | loss: 0.1083765MemoryTrain:  epoch  4, batch     1 | loss: 0.0679561MemoryTrain:  epoch  5, batch     0 | loss: 0.0778937MemoryTrain:  epoch  5, batch     1 | loss: 0.0690699MemoryTrain:  epoch  6, batch     0 | loss: 0.0778828MemoryTrain:  epoch  6, batch     1 | loss: 0.0617594MemoryTrain:  epoch  7, batch     0 | loss: 0.0603318MemoryTrain:  epoch  7, batch     1 | loss: 0.0972169MemoryTrain:  epoch  8, batch     0 | loss: 0.0597957MemoryTrain:  epoch  8, batch     1 | loss: 0.0650326MemoryTrain:  epoch  9, batch     0 | loss: 0.0534331MemoryTrain:  epoch  9, batch     1 | loss: 0.0386908

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.8235294117647058, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.36923076923076925, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.6206896551724138, 37: 0.3013698630136986, 38: 0.4}
Micro-average F1 score: 0.2989010989010989
Weighted-average F1 score: 0.18750921924306269
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.5405405405405406, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7678571428571429, 36: 0.0, 37: 0.41025641025641024, 38: 0.5230769230769231, 40: 0.0}
Micro-average F1 score: 0.2692778457772338
Weighted-average F1 score: 0.1733814488278774
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.75, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.5405405405405406, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.8173913043478261, 36: 0.0, 37: 0.3300970873786408, 38: 0.4857142857142857, 40: 0.0}
Micro-average F1 score: 0.2740076824583867
Weighted-average F1 score: 0.1781019140128178

F1 score per class: {1: 0.10569105691056911, 2: 0.27906976744186046, 3: 0.35, 5: 0.6857142857142857, 6: 0.23529411764705882, 7: 0.023529411764705882, 8: 0.07692307692307693, 9: 0.847457627118644, 10: 0.16923076923076924, 11: 0.05970149253731343, 12: 0.06201550387596899, 14: 0.08, 15: 0.5185185185185185, 16: 0.4423076923076923, 17: 0.0, 18: 0.17475728155339806, 19: 0.3310344827586207, 20: 0.2222222222222222, 22: 0.28440366972477066, 24: 0.03636363636363636, 25: 0.36923076923076925, 26: 0.6188340807174888, 27: 0.0446927374301676, 28: 0.13157894736842105, 29: 0.7149321266968326, 30: 0.8648648648648649, 31: 0.0, 32: 0.5102040816326531, 33: 0.4, 34: 0.3652173913043478, 35: 0.2488479262672811, 36: 0.056338028169014086, 37: 0.21153846153846154, 38: 0.10377358490566038, 39: 0.0, 40: 0.16666666666666666}
Micro-average F1 score: 0.30049794327776574
Weighted-average F1 score: 0.29801499646787566
F1 score per class: {1: 0.08979591836734693, 2: 0.19047619047619047, 3: 0.2512396694214876, 5: 0.3889980353634578, 6: 0.23308270676691728, 7: 0.017857142857142856, 8: 0.21153846153846154, 9: 0.5681818181818182, 10: 0.2731707317073171, 11: 0.027210884353741496, 12: 0.24289405684754523, 14: 0.027777777777777776, 15: 0.23529411764705882, 16: 0.4, 17: 0.17391304347826086, 18: 0.06376811594202898, 19: 0.303886925795053, 20: 0.23595505617977527, 22: 0.30357142857142855, 24: 0.04081632653061224, 25: 0.5194805194805194, 26: 0.5882352941176471, 27: 0.030534351145038167, 28: 0.0670391061452514, 29: 0.6666666666666666, 30: 0.6296296296296297, 31: 0.027586206896551724, 32: 0.47230320699708456, 33: 0.25, 34: 0.15953307392996108, 35: 0.2747603833865815, 36: 0.31343283582089554, 37: 0.07881773399014778, 38: 0.09770114942528736, 39: 0.0, 40: 0.19337016574585636}
Micro-average F1 score: 0.2295350743646797
Weighted-average F1 score: 0.2127140223647642
F1 score per class: {1: 0.08856088560885608, 2: 0.19047619047619047, 3: 0.3465909090909091, 5: 0.4008097165991903, 6: 0.23754789272030652, 7: 0.036036036036036036, 8: 0.1606425702811245, 9: 0.6172839506172839, 10: 0.24271844660194175, 11: 0.04678362573099415, 12: 0.22988505747126436, 14: 0.03111111111111111, 15: 0.3333333333333333, 16: 0.3937007874015748, 17: 0.125, 18: 0.07514450867052024, 19: 0.3102310231023102, 20: 0.24444444444444444, 22: 0.2982456140350877, 24: 0.04, 25: 0.5263157894736842, 26: 0.5982905982905983, 27: 0.031746031746031744, 28: 0.06493506493506493, 29: 0.6854838709677419, 30: 0.7619047619047619, 31: 0.041237113402061855, 32: 0.4823529411764706, 33: 0.24, 34: 0.16, 35: 0.2655367231638418, 36: 0.3064516129032258, 37: 0.07657657657657657, 38: 0.08018867924528301, 39: 0.0, 40: 0.20754716981132076}
Micro-average F1 score: 0.23533825184444165
Weighted-average F1 score: 0.216921143999535
cur_acc:  ['0.6827', '0.3614', '0.2559', '0.4658', '0.1859', '0.2714', '0.2989']
his_acc:  ['0.6827', '0.5784', '0.4490', '0.4219', '0.3142', '0.3112', '0.3005']
cur_acc des:  ['0.6588', '0.4316', '0.3479', '0.3675', '0.2226', '0.3018', '0.2693']
his_acc des:  ['0.6588', '0.4956', '0.3889', '0.3625', '0.2950', '0.2599', '0.2295']
cur_acc rrf:  ['0.6598', '0.4397', '0.3519', '0.3883', '0.2190', '0.3171', '0.2740']
his_acc rrf:  ['0.6598', '0.5043', '0.3831', '0.3688', '0.2958', '0.2671', '0.2353']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 53.5302023CurrentTrain: epoch  0, batch     1 | loss: 54.4615167CurrentTrain: epoch  0, batch     2 | loss: 49.7227192CurrentTrain: epoch  0, batch     3 | loss: 110.5690341CurrentTrain: epoch  0, batch     4 | loss: 65.7669866CurrentTrain: epoch  0, batch     5 | loss: 28.9907515CurrentTrain: epoch  1, batch     0 | loss: 58.8292113CurrentTrain: epoch  1, batch     1 | loss: 56.4242690CurrentTrain: epoch  1, batch     2 | loss: 45.8864230CurrentTrain: epoch  1, batch     3 | loss: 35.5295399CurrentTrain: epoch  1, batch     4 | loss: 73.7963078CurrentTrain: epoch  1, batch     5 | loss: 35.4010547CurrentTrain: epoch  2, batch     0 | loss: 68.7718299CurrentTrain: epoch  2, batch     1 | loss: 42.8717645CurrentTrain: epoch  2, batch     2 | loss: 41.0014126CurrentTrain: epoch  2, batch     3 | loss: 39.5418073CurrentTrain: epoch  2, batch     4 | loss: 39.3551880CurrentTrain: epoch  2, batch     5 | loss: 48.5527425CurrentTrain: epoch  3, batch     0 | loss: 50.9802432CurrentTrain: epoch  3, batch     1 | loss: 110.4735344CurrentTrain: epoch  3, batch     2 | loss: 31.5153188CurrentTrain: epoch  3, batch     3 | loss: 48.6504495CurrentTrain: epoch  3, batch     4 | loss: 32.7563033CurrentTrain: epoch  3, batch     5 | loss: 35.3033461CurrentTrain: epoch  4, batch     0 | loss: 64.5270625CurrentTrain: epoch  4, batch     1 | loss: 37.9419986CurrentTrain: epoch  4, batch     2 | loss: 53.7552636CurrentTrain: epoch  4, batch     3 | loss: 102.7443620CurrentTrain: epoch  4, batch     4 | loss: 46.0869593CurrentTrain: epoch  4, batch     5 | loss: 33.6197677CurrentTrain: epoch  5, batch     0 | loss: 103.8901292CurrentTrain: epoch  5, batch     1 | loss: 51.1026160CurrentTrain: epoch  5, batch     2 | loss: 36.0660516CurrentTrain: epoch  5, batch     3 | loss: 64.8343088CurrentTrain: epoch  5, batch     4 | loss: 63.0333373CurrentTrain: epoch  5, batch     5 | loss: 25.6815374CurrentTrain: epoch  6, batch     0 | loss: 107.6189176CurrentTrain: epoch  6, batch     1 | loss: 68.2609765CurrentTrain: epoch  6, batch     2 | loss: 47.9764149CurrentTrain: epoch  6, batch     3 | loss: 36.1666633CurrentTrain: epoch  6, batch     4 | loss: 46.3386606CurrentTrain: epoch  6, batch     5 | loss: 31.4824340CurrentTrain: epoch  7, batch     0 | loss: 64.8663041CurrentTrain: epoch  7, batch     1 | loss: 35.4769853CurrentTrain: epoch  7, batch     2 | loss: 103.6679027CurrentTrain: epoch  7, batch     3 | loss: 36.8148906CurrentTrain: epoch  7, batch     4 | loss: 46.2070819CurrentTrain: epoch  7, batch     5 | loss: 48.1652349CurrentTrain: epoch  8, batch     0 | loss: 69.9783282CurrentTrain: epoch  8, batch     1 | loss: 66.2919122CurrentTrain: epoch  8, batch     2 | loss: 37.6218289CurrentTrain: epoch  8, batch     3 | loss: 28.0574311CurrentTrain: epoch  8, batch     4 | loss: 47.7190009CurrentTrain: epoch  8, batch     5 | loss: 24.7668710CurrentTrain: epoch  9, batch     0 | loss: 34.1976339CurrentTrain: epoch  9, batch     1 | loss: 46.6302763CurrentTrain: epoch  9, batch     2 | loss: 65.1216585CurrentTrain: epoch  9, batch     3 | loss: 68.5129878CurrentTrain: epoch  9, batch     4 | loss: 63.3251894CurrentTrain: epoch  9, batch     5 | loss: 48.2731127
MemoryTrain:  epoch  0, batch     0 | loss: 0.1451812MemoryTrain:  epoch  0, batch     1 | loss: 0.1737867MemoryTrain:  epoch  1, batch     0 | loss: 0.1470123MemoryTrain:  epoch  1, batch     1 | loss: 0.0707487MemoryTrain:  epoch  2, batch     0 | loss: 0.1088381MemoryTrain:  epoch  2, batch     1 | loss: 0.0641653MemoryTrain:  epoch  3, batch     0 | loss: 0.1114755MemoryTrain:  epoch  3, batch     1 | loss: 0.0573331MemoryTrain:  epoch  4, batch     0 | loss: 0.0833744MemoryTrain:  epoch  4, batch     1 | loss: 0.0605188MemoryTrain:  epoch  5, batch     0 | loss: 0.0758819MemoryTrain:  epoch  5, batch     1 | loss: 0.0638816MemoryTrain:  epoch  6, batch     0 | loss: 0.0692163MemoryTrain:  epoch  6, batch     1 | loss: 0.0424573MemoryTrain:  epoch  7, batch     0 | loss: 0.0540149MemoryTrain:  epoch  7, batch     1 | loss: 0.0489928MemoryTrain:  epoch  8, batch     0 | loss: 0.0528145MemoryTrain:  epoch  8, batch     1 | loss: 0.0413315MemoryTrain:  epoch  9, batch     0 | loss: 0.0490707MemoryTrain:  epoch  9, batch     1 | loss: 0.0363177

F1 score per class: {0: 0.868421052631579, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9528795811518325, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 13: 0.058823529411764705, 14: 0.0, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.17647058823529413, 22: 0.0, 23: 0.75, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.56
Weighted-average F1 score: 0.4325795973382969
F1 score per class: {0: 0.7070707070707071, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9441624365482234, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.047619047619047616, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 20: 0.0, 21: 0.3333333333333333, 22: 0.0, 23: 0.6857142857142857, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.35987590486039295
Weighted-average F1 score: 0.24715145941574498
F1 score per class: {0: 0.7216494845360825, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9595959595959596, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.047619047619047616, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 20: 0.0, 21: 0.2564102564102564, 22: 0.0, 23: 0.7058823529411765, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.37269772481040087
Weighted-average F1 score: 0.25601150688125246

F1 score per class: {0: 0.2661290322580645, 1: 0.08, 2: 0.21428571428571427, 3: 0.3300970873786408, 4: 0.9238578680203046, 5: 0.7320754716981132, 6: 0.2275449101796407, 7: 0.022727272727272728, 8: 0.13071895424836602, 9: 0.8064516129032258, 10: 0.2112676056338028, 11: 0.07194244604316546, 12: 0.0625, 13: 0.011049723756906077, 14: 0.03076923076923077, 15: 0.631578947368421, 16: 0.368, 17: 0.0, 18: 0.17307692307692307, 19: 0.31952662721893493, 20: 0.21739130434782608, 21: 0.125, 22: 0.31802120141342755, 23: 0.673469387755102, 24: 0.05714285714285714, 25: 0.36923076923076925, 26: 0.6063348416289592, 27: 0.04291845493562232, 28: 0.2727272727272727, 29: 0.6827309236947792, 30: 0.8421052631578947, 31: 0.015384615384615385, 32: 0.48375451263537905, 33: 0.42857142857142855, 34: 0.3269230769230769, 35: 0.21476510067114093, 36: 0.07792207792207792, 37: 0.13636363636363635, 38: 0.10727969348659004, 39: 0.0, 40: 0.13592233009708737}
Micro-average F1 score: 0.3034697048161574
Weighted-average F1 score: 0.28593478096395386
F1 score per class: {0: 0.19230769230769232, 1: 0.0933852140077821, 2: 0.14432989690721648, 3: 0.21968616262482168, 4: 0.9073170731707317, 5: 0.3597122302158273, 6: 0.23904382470119523, 7: 0.0125, 8: 0.21176470588235294, 9: 0.43859649122807015, 10: 0.22797927461139897, 11: 0.041379310344827586, 12: 0.2100656455142232, 13: 0.008695652173913044, 14: 0.014742014742014743, 15: 0.24489795918367346, 16: 0.33962264150943394, 17: 0.13333333333333333, 18: 0.07526881720430108, 19: 0.28732394366197184, 20: 0.208, 21: 0.060810810810810814, 22: 0.233502538071066, 23: 0.48322147651006714, 24: 0.05555555555555555, 25: 0.5333333333333333, 26: 0.563265306122449, 27: 0.028368794326241134, 28: 0.060240963855421686, 29: 0.6101694915254238, 30: 0.5142857142857142, 31: 0.01606425702811245, 32: 0.4393063583815029, 33: 0.35294117647058826, 34: 0.18932038834951456, 35: 0.2603550295857988, 36: 0.32894736842105265, 37: 0.08550185873605948, 38: 0.07272727272727272, 39: 0.0, 40: 0.20209059233449478}
Micro-average F1 score: 0.22239367344970584
Weighted-average F1 score: 0.20279435502663348
F1 score per class: {0: 0.18617021276595744, 1: 0.08602150537634409, 2: 0.15217391304347827, 3: 0.3333333333333333, 4: 0.926829268292683, 5: 0.36968576709796674, 6: 0.24701195219123506, 7: 0.013157894736842105, 8: 0.17747440273037543, 9: 0.5, 10: 0.20588235294117646, 11: 0.07228915662650602, 12: 0.22330097087378642, 13: 0.008064516129032258, 14: 0.016563146997929608, 15: 0.25, 16: 0.3375, 17: 0.08, 18: 0.08152173913043478, 19: 0.2872340425531915, 20: 0.20717131474103587, 21: 0.06289308176100629, 22: 0.3275862068965517, 23: 0.5217391304347826, 24: 0.05555555555555555, 25: 0.5333333333333333, 26: 0.563265306122449, 27: 0.02877697841726619, 28: 0.08333333333333333, 29: 0.6289752650176679, 30: 0.6181818181818182, 31: 0.018433179723502304, 32: 0.4342857142857143, 33: 0.3333333333333333, 34: 0.1897810218978102, 35: 0.22278481012658227, 36: 0.33557046979865773, 37: 0.0916030534351145, 38: 0.05873261205564142, 39: 0.0, 40: 0.21774193548387097}
Micro-average F1 score: 0.23017388682279627
Weighted-average F1 score: 0.20815260308632483
cur_acc:  ['0.6827', '0.3614', '0.2559', '0.4658', '0.1859', '0.2714', '0.2989', '0.5600']
his_acc:  ['0.6827', '0.5784', '0.4490', '0.4219', '0.3142', '0.3112', '0.3005', '0.3035']
cur_acc des:  ['0.6588', '0.4316', '0.3479', '0.3675', '0.2226', '0.3018', '0.2693', '0.3599']
his_acc des:  ['0.6588', '0.4956', '0.3889', '0.3625', '0.2950', '0.2599', '0.2295', '0.2224']
cur_acc rrf:  ['0.6598', '0.4397', '0.3519', '0.3883', '0.2190', '0.3171', '0.2740', '0.3727']
his_acc rrf:  ['0.6598', '0.5043', '0.3831', '0.3688', '0.2958', '0.2671', '0.2353', '0.2302']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 58.8961066CurrentTrain: epoch  0, batch     1 | loss: 40.4483787CurrentTrain: epoch  0, batch     2 | loss: 58.2174851CurrentTrain: epoch  0, batch     3 | loss: 59.8227266CurrentTrain: epoch  0, batch     4 | loss: 77.7143627CurrentTrain: epoch  0, batch     5 | loss: 59.5871387CurrentTrain: epoch  0, batch     6 | loss: 80.4198984CurrentTrain: epoch  0, batch     7 | loss: 113.5294070CurrentTrain: epoch  0, batch     8 | loss: 58.8268658CurrentTrain: epoch  0, batch     9 | loss: 78.2900485CurrentTrain: epoch  0, batch    10 | loss: 58.4384926CurrentTrain: epoch  0, batch    11 | loss: 76.9599592CurrentTrain: epoch  0, batch    12 | loss: 113.3588997CurrentTrain: epoch  0, batch    13 | loss: 59.7428281CurrentTrain: epoch  0, batch    14 | loss: 58.5428542CurrentTrain: epoch  0, batch    15 | loss: 76.2225670CurrentTrain: epoch  0, batch    16 | loss: 47.7329514CurrentTrain: epoch  0, batch    17 | loss: 77.3940640CurrentTrain: epoch  0, batch    18 | loss: 40.3453534CurrentTrain: epoch  0, batch    19 | loss: 46.4750768CurrentTrain: epoch  0, batch    20 | loss: 39.2119583CurrentTrain: epoch  0, batch    21 | loss: 75.8266653CurrentTrain: epoch  0, batch    22 | loss: 46.9339769CurrentTrain: epoch  0, batch    23 | loss: 76.2753997CurrentTrain: epoch  0, batch    24 | loss: 75.9492900CurrentTrain: epoch  0, batch    25 | loss: 57.5116867CurrentTrain: epoch  0, batch    26 | loss: 76.1094310CurrentTrain: epoch  0, batch    27 | loss: 113.3836616CurrentTrain: epoch  0, batch    28 | loss: 77.0353995CurrentTrain: epoch  0, batch    29 | loss: 56.8553337CurrentTrain: epoch  0, batch    30 | loss: 75.8244719CurrentTrain: epoch  0, batch    31 | loss: 112.9844500CurrentTrain: epoch  0, batch    32 | loss: 40.2356904CurrentTrain: epoch  0, batch    33 | loss: 76.0399833CurrentTrain: epoch  0, batch    34 | loss: 38.3865228CurrentTrain: epoch  0, batch    35 | loss: 33.5824337CurrentTrain: epoch  0, batch    36 | loss: 56.9422718CurrentTrain: epoch  0, batch    37 | loss: 76.7720511CurrentTrain: epoch  0, batch    38 | loss: 46.7401338CurrentTrain: epoch  0, batch    39 | loss: 56.8149317CurrentTrain: epoch  0, batch    40 | loss: 46.3950732CurrentTrain: epoch  0, batch    41 | loss: 75.0904247CurrentTrain: epoch  0, batch    42 | loss: 112.4666916CurrentTrain: epoch  0, batch    43 | loss: 37.6411113CurrentTrain: epoch  0, batch    44 | loss: 74.3469125CurrentTrain: epoch  0, batch    45 | loss: 57.9859376CurrentTrain: epoch  0, batch    46 | loss: 46.0400135CurrentTrain: epoch  0, batch    47 | loss: 39.4690647CurrentTrain: epoch  0, batch    48 | loss: 46.2709453CurrentTrain: epoch  0, batch    49 | loss: 75.8902768CurrentTrain: epoch  0, batch    50 | loss: 75.2131030CurrentTrain: epoch  0, batch    51 | loss: 111.5688790CurrentTrain: epoch  0, batch    52 | loss: 39.0073681CurrentTrain: epoch  0, batch    53 | loss: 38.8757674CurrentTrain: epoch  0, batch    54 | loss: 56.3917129CurrentTrain: epoch  0, batch    55 | loss: 38.2976518CurrentTrain: epoch  0, batch    56 | loss: 76.0373104CurrentTrain: epoch  0, batch    57 | loss: 43.3054980CurrentTrain: epoch  0, batch    58 | loss: 55.7608958CurrentTrain: epoch  0, batch    59 | loss: 75.6072609CurrentTrain: epoch  0, batch    60 | loss: 33.0493482CurrentTrain: epoch  0, batch    61 | loss: 59.2882352CurrentTrain: epoch  0, batch    62 | loss: 74.9210081CurrentTrain: epoch  0, batch    63 | loss: 54.3933530CurrentTrain: epoch  0, batch    64 | loss: 72.6256068CurrentTrain: epoch  0, batch    65 | loss: 39.7436471CurrentTrain: epoch  0, batch    66 | loss: 45.8346087CurrentTrain: epoch  0, batch    67 | loss: 37.5420556CurrentTrain: epoch  0, batch    68 | loss: 46.0026765CurrentTrain: epoch  0, batch    69 | loss: 74.8211196CurrentTrain: epoch  0, batch    70 | loss: 45.9771800CurrentTrain: epoch  0, batch    71 | loss: 113.1879554CurrentTrain: epoch  0, batch    72 | loss: 45.9479728CurrentTrain: epoch  0, batch    73 | loss: 54.8447066CurrentTrain: epoch  0, batch    74 | loss: 57.2942291CurrentTrain: epoch  0, batch    75 | loss: 44.5978552CurrentTrain: epoch  0, batch    76 | loss: 35.0978942CurrentTrain: epoch  0, batch    77 | loss: 37.7815975CurrentTrain: epoch  0, batch    78 | loss: 74.7150780CurrentTrain: epoch  0, batch    79 | loss: 73.4687734CurrentTrain: epoch  0, batch    80 | loss: 112.5853568CurrentTrain: epoch  0, batch    81 | loss: 75.7610321CurrentTrain: epoch  0, batch    82 | loss: 36.4101169CurrentTrain: epoch  0, batch    83 | loss: 55.4191237CurrentTrain: epoch  0, batch    84 | loss: 75.5931529CurrentTrain: epoch  0, batch    85 | loss: 74.3957036CurrentTrain: epoch  0, batch    86 | loss: 44.6171431CurrentTrain: epoch  0, batch    87 | loss: 55.4293884CurrentTrain: epoch  0, batch    88 | loss: 56.2555046CurrentTrain: epoch  0, batch    89 | loss: 108.3522820CurrentTrain: epoch  0, batch    90 | loss: 57.9265100CurrentTrain: epoch  0, batch    91 | loss: 46.9071176CurrentTrain: epoch  0, batch    92 | loss: 111.8956398CurrentTrain: epoch  0, batch    93 | loss: 54.1397222CurrentTrain: epoch  0, batch    94 | loss: 72.1920431CurrentTrain: epoch  0, batch    95 | loss: 74.0546827CurrentTrain: epoch  0, batch    96 | loss: 44.3318678CurrentTrain: epoch  0, batch    97 | loss: 52.5756546CurrentTrain: epoch  0, batch    98 | loss: 56.3188278CurrentTrain: epoch  0, batch    99 | loss: 42.7152915CurrentTrain: epoch  0, batch   100 | loss: 73.0158632CurrentTrain: epoch  0, batch   101 | loss: 44.5279507CurrentTrain: epoch  0, batch   102 | loss: 56.2207714CurrentTrain: epoch  0, batch   103 | loss: 43.6443803CurrentTrain: epoch  0, batch   104 | loss: 54.2680081CurrentTrain: epoch  0, batch   105 | loss: 37.9455206CurrentTrain: epoch  0, batch   106 | loss: 43.1385777CurrentTrain: epoch  0, batch   107 | loss: 113.1921279CurrentTrain: epoch  0, batch   108 | loss: 51.5463224CurrentTrain: epoch  0, batch   109 | loss: 55.0938654CurrentTrain: epoch  0, batch   110 | loss: 41.4200646CurrentTrain: epoch  0, batch   111 | loss: 36.2539563CurrentTrain: epoch  0, batch   112 | loss: 53.0922810CurrentTrain: epoch  0, batch   113 | loss: 55.2450486CurrentTrain: epoch  0, batch   114 | loss: 71.1101707CurrentTrain: epoch  0, batch   115 | loss: 43.7320951CurrentTrain: epoch  0, batch   116 | loss: 52.6880828CurrentTrain: epoch  0, batch   117 | loss: 71.2046058CurrentTrain: epoch  0, batch   118 | loss: 110.4024969CurrentTrain: epoch  0, batch   119 | loss: 108.7318880CurrentTrain: epoch  0, batch   120 | loss: 43.1356727CurrentTrain: epoch  0, batch   121 | loss: 50.5798500CurrentTrain: epoch  0, batch   122 | loss: 40.7420831CurrentTrain: epoch  0, batch   123 | loss: 43.1837930CurrentTrain: epoch  0, batch   124 | loss: 72.1982162CurrentTrain: epoch  0, batch   125 | loss: 53.6119735CurrentTrain: epoch  0, batch   126 | loss: 53.7593969CurrentTrain: epoch  0, batch   127 | loss: 41.8210941CurrentTrain: epoch  0, batch   128 | loss: 51.3245676CurrentTrain: epoch  0, batch   129 | loss: 72.5667786CurrentTrain: epoch  0, batch   130 | loss: 52.6560835CurrentTrain: epoch  0, batch   131 | loss: 70.5137784CurrentTrain: epoch  0, batch   132 | loss: 72.1046815CurrentTrain: epoch  0, batch   133 | loss: 48.4110799CurrentTrain: epoch  0, batch   134 | loss: 51.8940025CurrentTrain: epoch  0, batch   135 | loss: 38.9612886CurrentTrain: epoch  0, batch   136 | loss: 70.0458985CurrentTrain: epoch  0, batch   137 | loss: 40.7065265CurrentTrain: epoch  0, batch   138 | loss: 75.0012645CurrentTrain: epoch  0, batch   139 | loss: 42.5718490CurrentTrain: epoch  0, batch   140 | loss: 74.3967048CurrentTrain: epoch  0, batch   141 | loss: 43.5122292CurrentTrain: epoch  0, batch   142 | loss: 56.9223595CurrentTrain: epoch  0, batch   143 | loss: 40.8852524CurrentTrain: epoch  1, batch     0 | loss: 70.3714940CurrentTrain: epoch  1, batch     1 | loss: 51.3781521CurrentTrain: epoch  1, batch     2 | loss: 41.2564939CurrentTrain: epoch  1, batch     3 | loss: 53.4179775CurrentTrain: epoch  1, batch     4 | loss: 50.8274921CurrentTrain: epoch  1, batch     5 | loss: 108.5938944CurrentTrain: epoch  1, batch     6 | loss: 37.9277734CurrentTrain: epoch  1, batch     7 | loss: 107.5109204CurrentTrain: epoch  1, batch     8 | loss: 55.1016165CurrentTrain: epoch  1, batch     9 | loss: 222.7352957CurrentTrain: epoch  1, batch    10 | loss: 39.5230300CurrentTrain: epoch  1, batch    11 | loss: 43.0255704CurrentTrain: epoch  1, batch    12 | loss: 36.0877579CurrentTrain: epoch  1, batch    13 | loss: 31.9840323CurrentTrain: epoch  1, batch    14 | loss: 70.5080495CurrentTrain: epoch  1, batch    15 | loss: 41.0861350CurrentTrain: epoch  1, batch    16 | loss: 69.7954074CurrentTrain: epoch  1, batch    17 | loss: 54.7032585CurrentTrain: epoch  1, batch    18 | loss: 70.0412246CurrentTrain: epoch  1, batch    19 | loss: 41.0255812CurrentTrain: epoch  1, batch    20 | loss: 37.4152729CurrentTrain: epoch  1, batch    21 | loss: 69.4916759CurrentTrain: epoch  1, batch    22 | loss: 51.2594001CurrentTrain: epoch  1, batch    23 | loss: 34.1699951CurrentTrain: epoch  1, batch    24 | loss: 53.3862357CurrentTrain: epoch  1, batch    25 | loss: 55.2571850CurrentTrain: epoch  1, batch    26 | loss: 51.2386796CurrentTrain: epoch  1, batch    27 | loss: 110.3129318CurrentTrain: epoch  1, batch    28 | loss: 48.0815975CurrentTrain: epoch  1, batch    29 | loss: 41.6618389CurrentTrain: epoch  1, batch    30 | loss: 109.2327924CurrentTrain: epoch  1, batch    31 | loss: 31.2195407CurrentTrain: epoch  1, batch    32 | loss: 68.4082856CurrentTrain: epoch  1, batch    33 | loss: 41.3321960CurrentTrain: epoch  1, batch    34 | loss: 52.4494319CurrentTrain: epoch  1, batch    35 | loss: 105.0130868CurrentTrain: epoch  1, batch    36 | loss: 73.8153635CurrentTrain: epoch  1, batch    37 | loss: 41.4759942CurrentTrain: epoch  1, batch    38 | loss: 40.2002651CurrentTrain: epoch  1, batch    39 | loss: 54.4778568CurrentTrain: epoch  1, batch    40 | loss: 40.2945678CurrentTrain: epoch  1, batch    41 | loss: 41.2532453CurrentTrain: epoch  1, batch    42 | loss: 222.4531796CurrentTrain: epoch  1, batch    43 | loss: 39.4015263CurrentTrain: epoch  1, batch    44 | loss: 109.3302093CurrentTrain: epoch  1, batch    45 | loss: 101.8593816CurrentTrain: epoch  1, batch    46 | loss: 50.9064415CurrentTrain: epoch  1, batch    47 | loss: 40.7240468CurrentTrain: epoch  1, batch    48 | loss: 71.6015630CurrentTrain: epoch  1, batch    49 | loss: 64.7059478CurrentTrain: epoch  1, batch    50 | loss: 70.1483539CurrentTrain: epoch  1, batch    51 | loss: 68.9440355CurrentTrain: epoch  1, batch    52 | loss: 108.5218355CurrentTrain: epoch  1, batch    53 | loss: 71.7023920CurrentTrain: epoch  1, batch    54 | loss: 70.9315107CurrentTrain: epoch  1, batch    55 | loss: 43.2741626CurrentTrain: epoch  1, batch    56 | loss: 37.7874463CurrentTrain: epoch  1, batch    57 | loss: 108.3618147CurrentTrain: epoch  1, batch    58 | loss: 71.5723851CurrentTrain: epoch  1, batch    59 | loss: 54.0829989CurrentTrain: epoch  1, batch    60 | loss: 69.1065254CurrentTrain: epoch  1, batch    61 | loss: 69.8096210CurrentTrain: epoch  1, batch    62 | loss: 71.7388084CurrentTrain: epoch  1, batch    63 | loss: 41.6814537CurrentTrain: epoch  1, batch    64 | loss: 39.3597348CurrentTrain: epoch  1, batch    65 | loss: 52.8424898CurrentTrain: epoch  1, batch    66 | loss: 66.3879707CurrentTrain: epoch  1, batch    67 | loss: 49.1188013CurrentTrain: epoch  1, batch    68 | loss: 49.3955998CurrentTrain: epoch  1, batch    69 | loss: 31.3473442CurrentTrain: epoch  1, batch    70 | loss: 71.2520627CurrentTrain: epoch  1, batch    71 | loss: 43.3058388CurrentTrain: epoch  1, batch    72 | loss: 51.7502133CurrentTrain: epoch  1, batch    73 | loss: 37.8800501CurrentTrain: epoch  1, batch    74 | loss: 52.7710634CurrentTrain: epoch  1, batch    75 | loss: 34.1552479CurrentTrain: epoch  1, batch    76 | loss: 44.9044918CurrentTrain: epoch  1, batch    77 | loss: 31.0452370CurrentTrain: epoch  1, batch    78 | loss: 70.3500110CurrentTrain: epoch  1, batch    79 | loss: 52.2180573CurrentTrain: epoch  1, batch    80 | loss: 54.5812114CurrentTrain: epoch  1, batch    81 | loss: 29.5990798CurrentTrain: epoch  1, batch    82 | loss: 34.4324888CurrentTrain: epoch  1, batch    83 | loss: 50.9015055CurrentTrain: epoch  1, batch    84 | loss: 51.3425226CurrentTrain: epoch  1, batch    85 | loss: 42.9976031CurrentTrain: epoch  1, batch    86 | loss: 70.2350177CurrentTrain: epoch  1, batch    87 | loss: 71.4253956CurrentTrain: epoch  1, batch    88 | loss: 41.5711938CurrentTrain: epoch  1, batch    89 | loss: 51.3962426CurrentTrain: epoch  1, batch    90 | loss: 28.9150282CurrentTrain: epoch  1, batch    91 | loss: 41.3343356CurrentTrain: epoch  1, batch    92 | loss: 49.1062215CurrentTrain: epoch  1, batch    93 | loss: 38.8886473CurrentTrain: epoch  1, batch    94 | loss: 50.8798726CurrentTrain: epoch  1, batch    95 | loss: 40.2120843CurrentTrain: epoch  1, batch    96 | loss: 40.6188473CurrentTrain: epoch  1, batch    97 | loss: 40.2268408CurrentTrain: epoch  1, batch    98 | loss: 70.8929593CurrentTrain: epoch  1, batch    99 | loss: 50.9872993CurrentTrain: epoch  1, batch   100 | loss: 50.0677024CurrentTrain: epoch  1, batch   101 | loss: 107.9245549CurrentTrain: epoch  1, batch   102 | loss: 51.0531268CurrentTrain: epoch  1, batch   103 | loss: 70.6212419CurrentTrain: epoch  1, batch   104 | loss: 69.7474103CurrentTrain: epoch  1, batch   105 | loss: 48.6338073CurrentTrain: epoch  1, batch   106 | loss: 106.3369293CurrentTrain: epoch  1, batch   107 | loss: 55.3862270CurrentTrain: epoch  1, batch   108 | loss: 40.1474546CurrentTrain: epoch  1, batch   109 | loss: 51.2466373CurrentTrain: epoch  1, batch   110 | loss: 48.7474660CurrentTrain: epoch  1, batch   111 | loss: 106.0147875CurrentTrain: epoch  1, batch   112 | loss: 71.7134368CurrentTrain: epoch  1, batch   113 | loss: 51.4096768CurrentTrain: epoch  1, batch   114 | loss: 41.3542679CurrentTrain: epoch  1, batch   115 | loss: 71.7004770CurrentTrain: epoch  1, batch   116 | loss: 39.7878321CurrentTrain: epoch  1, batch   117 | loss: 52.9260075CurrentTrain: epoch  1, batch   118 | loss: 40.9930974CurrentTrain: epoch  1, batch   119 | loss: 70.2178860CurrentTrain: epoch  1, batch   120 | loss: 50.6720534CurrentTrain: epoch  1, batch   121 | loss: 33.9768874CurrentTrain: epoch  1, batch   122 | loss: 56.1206600CurrentTrain: epoch  1, batch   123 | loss: 44.2094349CurrentTrain: epoch  1, batch   124 | loss: 40.7599065CurrentTrain: epoch  1, batch   125 | loss: 41.2461505CurrentTrain: epoch  1, batch   126 | loss: 68.1205176CurrentTrain: epoch  1, batch   127 | loss: 47.8385161CurrentTrain: epoch  1, batch   128 | loss: 50.2712055CurrentTrain: epoch  1, batch   129 | loss: 52.1125520CurrentTrain: epoch  1, batch   130 | loss: 54.3827416CurrentTrain: epoch  1, batch   131 | loss: 222.3570333CurrentTrain: epoch  1, batch   132 | loss: 32.0515037CurrentTrain: epoch  1, batch   133 | loss: 44.9039950CurrentTrain: epoch  1, batch   134 | loss: 52.0371545CurrentTrain: epoch  1, batch   135 | loss: 41.2342966CurrentTrain: epoch  1, batch   136 | loss: 40.2717160CurrentTrain: epoch  1, batch   137 | loss: 41.5908468CurrentTrain: epoch  1, batch   138 | loss: 47.4616273CurrentTrain: epoch  1, batch   139 | loss: 50.3170209CurrentTrain: epoch  1, batch   140 | loss: 39.8575104CurrentTrain: epoch  1, batch   141 | loss: 50.3352885CurrentTrain: epoch  1, batch   142 | loss: 67.6633008CurrentTrain: epoch  1, batch   143 | loss: 29.8084747CurrentTrain: epoch  2, batch     0 | loss: 37.4719159CurrentTrain: epoch  2, batch     1 | loss: 68.0744183CurrentTrain: epoch  2, batch     2 | loss: 41.2278793CurrentTrain: epoch  2, batch     3 | loss: 48.3586728CurrentTrain: epoch  2, batch     4 | loss: 50.7664801CurrentTrain: epoch  2, batch     5 | loss: 67.9185375CurrentTrain: epoch  2, batch     6 | loss: 108.1907338CurrentTrain: epoch  2, batch     7 | loss: 38.2169035CurrentTrain: epoch  2, batch     8 | loss: 38.4429418CurrentTrain: epoch  2, batch     9 | loss: 51.1764210CurrentTrain: epoch  2, batch    10 | loss: 222.3333546CurrentTrain: epoch  2, batch    11 | loss: 50.0605727CurrentTrain: epoch  2, batch    12 | loss: 36.7339887CurrentTrain: epoch  2, batch    13 | loss: 50.0465579CurrentTrain: epoch  2, batch    14 | loss: 69.2966350CurrentTrain: epoch  2, batch    15 | loss: 70.0503946CurrentTrain: epoch  2, batch    16 | loss: 48.8037677CurrentTrain: epoch  2, batch    17 | loss: 32.4983156CurrentTrain: epoch  2, batch    18 | loss: 67.8516057CurrentTrain: epoch  2, batch    19 | loss: 40.3004464CurrentTrain: epoch  2, batch    20 | loss: 40.1089538CurrentTrain: epoch  2, batch    21 | loss: 68.6815150CurrentTrain: epoch  2, batch    22 | loss: 40.0042526CurrentTrain: epoch  2, batch    23 | loss: 67.8300161CurrentTrain: epoch  2, batch    24 | loss: 48.5182195CurrentTrain: epoch  2, batch    25 | loss: 67.8656702CurrentTrain: epoch  2, batch    26 | loss: 40.0917286CurrentTrain: epoch  2, batch    27 | loss: 53.1122590CurrentTrain: epoch  2, batch    28 | loss: 67.1210099CurrentTrain: epoch  2, batch    29 | loss: 47.3773092CurrentTrain: epoch  2, batch    30 | loss: 66.9365976CurrentTrain: epoch  2, batch    31 | loss: 48.8321440CurrentTrain: epoch  2, batch    32 | loss: 68.5583616CurrentTrain: epoch  2, batch    33 | loss: 54.6889758CurrentTrain: epoch  2, batch    34 | loss: 71.6103879CurrentTrain: epoch  2, batch    35 | loss: 71.8385198CurrentTrain: epoch  2, batch    36 | loss: 52.8112445CurrentTrain: epoch  2, batch    37 | loss: 107.5275479CurrentTrain: epoch  2, batch    38 | loss: 68.9807634CurrentTrain: epoch  2, batch    39 | loss: 50.1231790CurrentTrain: epoch  2, batch    40 | loss: 107.3018735CurrentTrain: epoch  2, batch    41 | loss: 48.4217410CurrentTrain: epoch  2, batch    42 | loss: 66.5339237CurrentTrain: epoch  2, batch    43 | loss: 68.7390301CurrentTrain: epoch  2, batch    44 | loss: 29.9770263CurrentTrain: epoch  2, batch    45 | loss: 67.4479663CurrentTrain: epoch  2, batch    46 | loss: 39.2541897CurrentTrain: epoch  2, batch    47 | loss: 50.5082900CurrentTrain: epoch  2, batch    48 | loss: 70.1335516CurrentTrain: epoch  2, batch    49 | loss: 51.0440029CurrentTrain: epoch  2, batch    50 | loss: 30.6732028CurrentTrain: epoch  2, batch    51 | loss: 50.0490943CurrentTrain: epoch  2, batch    52 | loss: 38.7075941CurrentTrain: epoch  2, batch    53 | loss: 40.9615180CurrentTrain: epoch  2, batch    54 | loss: 32.3830411CurrentTrain: epoch  2, batch    55 | loss: 38.4048205CurrentTrain: epoch  2, batch    56 | loss: 68.5712298CurrentTrain: epoch  2, batch    57 | loss: 49.8956809CurrentTrain: epoch  2, batch    58 | loss: 26.7707604CurrentTrain: epoch  2, batch    59 | loss: 50.6762622CurrentTrain: epoch  2, batch    60 | loss: 70.2897652CurrentTrain: epoch  2, batch    61 | loss: 53.9036825CurrentTrain: epoch  2, batch    62 | loss: 40.3609904CurrentTrain: epoch  2, batch    63 | loss: 75.9550996CurrentTrain: epoch  2, batch    64 | loss: 51.8652353CurrentTrain: epoch  2, batch    65 | loss: 66.5037899CurrentTrain: epoch  2, batch    66 | loss: 49.5536924CurrentTrain: epoch  2, batch    67 | loss: 115.3836860CurrentTrain: epoch  2, batch    68 | loss: 48.7541622CurrentTrain: epoch  2, batch    69 | loss: 109.5073051CurrentTrain: epoch  2, batch    70 | loss: 69.4398329CurrentTrain: epoch  2, batch    71 | loss: 66.6104182CurrentTrain: epoch  2, batch    72 | loss: 42.1087187CurrentTrain: epoch  2, batch    73 | loss: 51.1000150CurrentTrain: epoch  2, batch    74 | loss: 36.4536719CurrentTrain: epoch  2, batch    75 | loss: 32.5978807CurrentTrain: epoch  2, batch    76 | loss: 107.0305382CurrentTrain: epoch  2, batch    77 | loss: 65.7029301CurrentTrain: epoch  2, batch    78 | loss: 71.8428465CurrentTrain: epoch  2, batch    79 | loss: 50.9993351CurrentTrain: epoch  2, batch    80 | loss: 36.3902554CurrentTrain: epoch  2, batch    81 | loss: 67.5169766CurrentTrain: epoch  2, batch    82 | loss: 48.1370239CurrentTrain: epoch  2, batch    83 | loss: 35.8555260CurrentTrain: epoch  2, batch    84 | loss: 33.7756550CurrentTrain: epoch  2, batch    85 | loss: 38.7947040CurrentTrain: epoch  2, batch    86 | loss: 40.2605570CurrentTrain: epoch  2, batch    87 | loss: 36.5638221CurrentTrain: epoch  2, batch    88 | loss: 38.0012642CurrentTrain: epoch  2, batch    89 | loss: 50.4204472CurrentTrain: epoch  2, batch    90 | loss: 51.0027985CurrentTrain: epoch  2, batch    91 | loss: 74.1880096CurrentTrain: epoch  2, batch    92 | loss: 48.6106438CurrentTrain: epoch  2, batch    93 | loss: 38.8759963CurrentTrain: epoch  2, batch    94 | loss: 69.5520163CurrentTrain: epoch  2, batch    95 | loss: 51.2144360CurrentTrain: epoch  2, batch    96 | loss: 66.9027171CurrentTrain: epoch  2, batch    97 | loss: 37.4739302CurrentTrain: epoch  2, batch    98 | loss: 67.6875796CurrentTrain: epoch  2, batch    99 | loss: 66.4239613CurrentTrain: epoch  2, batch   100 | loss: 52.6299072CurrentTrain: epoch  2, batch   101 | loss: 101.7843239CurrentTrain: epoch  2, batch   102 | loss: 68.4501395CurrentTrain: epoch  2, batch   103 | loss: 56.6193278CurrentTrain: epoch  2, batch   104 | loss: 39.0870608CurrentTrain: epoch  2, batch   105 | loss: 63.6405625CurrentTrain: epoch  2, batch   106 | loss: 38.2305714CurrentTrain: epoch  2, batch   107 | loss: 39.3395517CurrentTrain: epoch  2, batch   108 | loss: 42.6507011CurrentTrain: epoch  2, batch   109 | loss: 108.1105888CurrentTrain: epoch  2, batch   110 | loss: 29.5862727CurrentTrain: epoch  2, batch   111 | loss: 48.1224321CurrentTrain: epoch  2, batch   112 | loss: 39.9539102CurrentTrain: epoch  2, batch   113 | loss: 66.2553500CurrentTrain: epoch  2, batch   114 | loss: 39.6439638CurrentTrain: epoch  2, batch   115 | loss: 49.5437025CurrentTrain: epoch  2, batch   116 | loss: 50.8080202CurrentTrain: epoch  2, batch   117 | loss: 49.0547879CurrentTrain: epoch  2, batch   118 | loss: 38.3076942CurrentTrain: epoch  2, batch   119 | loss: 41.7388710CurrentTrain: epoch  2, batch   120 | loss: 108.1331637CurrentTrain: epoch  2, batch   121 | loss: 34.2344060CurrentTrain: epoch  2, batch   122 | loss: 32.3198850CurrentTrain: epoch  2, batch   123 | loss: 69.3925130CurrentTrain: epoch  2, batch   124 | loss: 51.6179625CurrentTrain: epoch  2, batch   125 | loss: 41.4352800CurrentTrain: epoch  2, batch   126 | loss: 37.8159733CurrentTrain: epoch  2, batch   127 | loss: 104.5696849CurrentTrain: epoch  2, batch   128 | loss: 29.0222533CurrentTrain: epoch  2, batch   129 | loss: 50.3196310CurrentTrain: epoch  2, batch   130 | loss: 51.7260551CurrentTrain: epoch  2, batch   131 | loss: 40.6328113CurrentTrain: epoch  2, batch   132 | loss: 74.8981710CurrentTrain: epoch  2, batch   133 | loss: 48.9125868CurrentTrain: epoch  2, batch   134 | loss: 38.5410983CurrentTrain: epoch  2, batch   135 | loss: 37.0580395CurrentTrain: epoch  2, batch   136 | loss: 51.1630924CurrentTrain: epoch  2, batch   137 | loss: 43.3548218CurrentTrain: epoch  2, batch   138 | loss: 39.5968357CurrentTrain: epoch  2, batch   139 | loss: 105.9797248CurrentTrain: epoch  2, batch   140 | loss: 31.6128196CurrentTrain: epoch  2, batch   141 | loss: 70.4205029CurrentTrain: epoch  2, batch   142 | loss: 42.9014003CurrentTrain: epoch  2, batch   143 | loss: 32.1470651CurrentTrain: epoch  3, batch     0 | loss: 70.2820913CurrentTrain: epoch  3, batch     1 | loss: 68.9267096CurrentTrain: epoch  3, batch     2 | loss: 49.4935942CurrentTrain: epoch  3, batch     3 | loss: 49.3451162CurrentTrain: epoch  3, batch     4 | loss: 48.3256392CurrentTrain: epoch  3, batch     5 | loss: 50.8505975CurrentTrain: epoch  3, batch     6 | loss: 68.3993446CurrentTrain: epoch  3, batch     7 | loss: 48.0004298CurrentTrain: epoch  3, batch     8 | loss: 70.4216051CurrentTrain: epoch  3, batch     9 | loss: 41.1812518CurrentTrain: epoch  3, batch    10 | loss: 48.9317958CurrentTrain: epoch  3, batch    11 | loss: 108.3563726CurrentTrain: epoch  3, batch    12 | loss: 36.9240812CurrentTrain: epoch  3, batch    13 | loss: 38.7092273CurrentTrain: epoch  3, batch    14 | loss: 39.4428222CurrentTrain: epoch  3, batch    15 | loss: 35.4419702CurrentTrain: epoch  3, batch    16 | loss: 36.4530586CurrentTrain: epoch  3, batch    17 | loss: 49.7714497CurrentTrain: epoch  3, batch    18 | loss: 69.2204427CurrentTrain: epoch  3, batch    19 | loss: 48.6788085CurrentTrain: epoch  3, batch    20 | loss: 46.6357589CurrentTrain: epoch  3, batch    21 | loss: 30.6043744CurrentTrain: epoch  3, batch    22 | loss: 49.7376373CurrentTrain: epoch  3, batch    23 | loss: 67.6096675CurrentTrain: epoch  3, batch    24 | loss: 66.1526626CurrentTrain: epoch  3, batch    25 | loss: 107.1766491CurrentTrain: epoch  3, batch    26 | loss: 68.1233332CurrentTrain: epoch  3, batch    27 | loss: 43.6988797CurrentTrain: epoch  3, batch    28 | loss: 108.6721375CurrentTrain: epoch  3, batch    29 | loss: 69.9593703CurrentTrain: epoch  3, batch    30 | loss: 65.1524774CurrentTrain: epoch  3, batch    31 | loss: 49.9519966CurrentTrain: epoch  3, batch    32 | loss: 49.8006071CurrentTrain: epoch  3, batch    33 | loss: 33.6648571CurrentTrain: epoch  3, batch    34 | loss: 69.4948725CurrentTrain: epoch  3, batch    35 | loss: 38.1062206CurrentTrain: epoch  3, batch    36 | loss: 49.7624178CurrentTrain: epoch  3, batch    37 | loss: 69.5744035CurrentTrain: epoch  3, batch    38 | loss: 51.1368547CurrentTrain: epoch  3, batch    39 | loss: 44.2826139CurrentTrain: epoch  3, batch    40 | loss: 52.7533023CurrentTrain: epoch  3, batch    41 | loss: 70.9877537CurrentTrain: epoch  3, batch    42 | loss: 50.2575632CurrentTrain: epoch  3, batch    43 | loss: 46.9240472CurrentTrain: epoch  3, batch    44 | loss: 37.4298524CurrentTrain: epoch  3, batch    45 | loss: 36.9863374CurrentTrain: epoch  3, batch    46 | loss: 48.6400146CurrentTrain: epoch  3, batch    47 | loss: 72.0793834CurrentTrain: epoch  3, batch    48 | loss: 51.6412192CurrentTrain: epoch  3, batch    49 | loss: 35.1965863CurrentTrain: epoch  3, batch    50 | loss: 37.5256055CurrentTrain: epoch  3, batch    51 | loss: 68.6685551CurrentTrain: epoch  3, batch    52 | loss: 37.5852851CurrentTrain: epoch  3, batch    53 | loss: 30.0284749CurrentTrain: epoch  3, batch    54 | loss: 39.5535066CurrentTrain: epoch  3, batch    55 | loss: 51.3138518CurrentTrain: epoch  3, batch    56 | loss: 41.3540077CurrentTrain: epoch  3, batch    57 | loss: 48.5466089CurrentTrain: epoch  3, batch    58 | loss: 30.1695560CurrentTrain: epoch  3, batch    59 | loss: 50.0338278CurrentTrain: epoch  3, batch    60 | loss: 50.3896398CurrentTrain: epoch  3, batch    61 | loss: 37.5353889CurrentTrain: epoch  3, batch    62 | loss: 69.3935685CurrentTrain: epoch  3, batch    63 | loss: 49.8453364CurrentTrain: epoch  3, batch    64 | loss: 66.9775034CurrentTrain: epoch  3, batch    65 | loss: 67.1314385CurrentTrain: epoch  3, batch    66 | loss: 29.7782252CurrentTrain: epoch  3, batch    67 | loss: 36.4591650CurrentTrain: epoch  3, batch    68 | loss: 38.1165034CurrentTrain: epoch  3, batch    69 | loss: 37.4896439CurrentTrain: epoch  3, batch    70 | loss: 35.5337402CurrentTrain: epoch  3, batch    71 | loss: 49.5769399CurrentTrain: epoch  3, batch    72 | loss: 36.1957848CurrentTrain: epoch  3, batch    73 | loss: 45.7203241CurrentTrain: epoch  3, batch    74 | loss: 67.9833511CurrentTrain: epoch  3, batch    75 | loss: 51.8709678CurrentTrain: epoch  3, batch    76 | loss: 30.5894891CurrentTrain: epoch  3, batch    77 | loss: 46.9955225CurrentTrain: epoch  3, batch    78 | loss: 50.9920964CurrentTrain: epoch  3, batch    79 | loss: 74.7856850CurrentTrain: epoch  3, batch    80 | loss: 107.5801190CurrentTrain: epoch  3, batch    81 | loss: 35.5454982CurrentTrain: epoch  3, batch    82 | loss: 30.4636056CurrentTrain: epoch  3, batch    83 | loss: 51.2272914CurrentTrain: epoch  3, batch    84 | loss: 69.7280407CurrentTrain: epoch  3, batch    85 | loss: 38.2552155CurrentTrain: epoch  3, batch    86 | loss: 49.4086855CurrentTrain: epoch  3, batch    87 | loss: 50.5479552CurrentTrain: epoch  3, batch    88 | loss: 31.5108885CurrentTrain: epoch  3, batch    89 | loss: 36.8913611CurrentTrain: epoch  3, batch    90 | loss: 49.8603588CurrentTrain: epoch  3, batch    91 | loss: 30.3870293CurrentTrain: epoch  3, batch    92 | loss: 70.2995197CurrentTrain: epoch  3, batch    93 | loss: 37.8412310CurrentTrain: epoch  3, batch    94 | loss: 46.6606792CurrentTrain: epoch  3, batch    95 | loss: 53.1257222CurrentTrain: epoch  3, batch    96 | loss: 47.8665411CurrentTrain: epoch  3, batch    97 | loss: 222.4070762CurrentTrain: epoch  3, batch    98 | loss: 39.0659237CurrentTrain: epoch  3, batch    99 | loss: 48.3550745CurrentTrain: epoch  3, batch   100 | loss: 103.6890510CurrentTrain: epoch  3, batch   101 | loss: 37.5627649CurrentTrain: epoch  3, batch   102 | loss: 68.5487189CurrentTrain: epoch  3, batch   103 | loss: 49.9402664CurrentTrain: epoch  3, batch   104 | loss: 39.0385009CurrentTrain: epoch  3, batch   105 | loss: 37.6038730CurrentTrain: epoch  3, batch   106 | loss: 28.1048873CurrentTrain: epoch  3, batch   107 | loss: 37.7125483CurrentTrain: epoch  3, batch   108 | loss: 36.0243507CurrentTrain: epoch  3, batch   109 | loss: 110.1532870CurrentTrain: epoch  3, batch   110 | loss: 38.5591598CurrentTrain: epoch  3, batch   111 | loss: 48.5474366CurrentTrain: epoch  3, batch   112 | loss: 104.0308442CurrentTrain: epoch  3, batch   113 | loss: 107.2115678CurrentTrain: epoch  3, batch   114 | loss: 75.1186880CurrentTrain: epoch  3, batch   115 | loss: 72.3419358CurrentTrain: epoch  3, batch   116 | loss: 66.7535943CurrentTrain: epoch  3, batch   117 | loss: 68.4334626CurrentTrain: epoch  3, batch   118 | loss: 31.9484939CurrentTrain: epoch  3, batch   119 | loss: 39.2492588CurrentTrain: epoch  3, batch   120 | loss: 38.5931922CurrentTrain: epoch  3, batch   121 | loss: 38.1637237CurrentTrain: epoch  3, batch   122 | loss: 38.9960895CurrentTrain: epoch  3, batch   123 | loss: 66.9561383CurrentTrain: epoch  3, batch   124 | loss: 50.3250314CurrentTrain: epoch  3, batch   125 | loss: 67.4818705CurrentTrain: epoch  3, batch   126 | loss: 107.2704338CurrentTrain: epoch  3, batch   127 | loss: 71.9743954CurrentTrain: epoch  3, batch   128 | loss: 37.6485075CurrentTrain: epoch  3, batch   129 | loss: 52.0862454CurrentTrain: epoch  3, batch   130 | loss: 49.5742077CurrentTrain: epoch  3, batch   131 | loss: 65.1846808CurrentTrain: epoch  3, batch   132 | loss: 68.4705534CurrentTrain: epoch  3, batch   133 | loss: 68.2605803CurrentTrain: epoch  3, batch   134 | loss: 36.0555022CurrentTrain: epoch  3, batch   135 | loss: 47.7702686CurrentTrain: epoch  3, batch   136 | loss: 37.5157238CurrentTrain: epoch  3, batch   137 | loss: 40.4299534CurrentTrain: epoch  3, batch   138 | loss: 103.2320904CurrentTrain: epoch  3, batch   139 | loss: 109.1879022CurrentTrain: epoch  3, batch   140 | loss: 108.1279503CurrentTrain: epoch  3, batch   141 | loss: 30.9644360CurrentTrain: epoch  3, batch   142 | loss: 47.8974333CurrentTrain: epoch  3, batch   143 | loss: 36.9633144CurrentTrain: epoch  4, batch     0 | loss: 38.8326542CurrentTrain: epoch  4, batch     1 | loss: 49.4638597CurrentTrain: epoch  4, batch     2 | loss: 71.0003930CurrentTrain: epoch  4, batch     3 | loss: 47.1682985CurrentTrain: epoch  4, batch     4 | loss: 49.0448321CurrentTrain: epoch  4, batch     5 | loss: 38.1731121CurrentTrain: epoch  4, batch     6 | loss: 37.1351724CurrentTrain: epoch  4, batch     7 | loss: 36.7556442CurrentTrain: epoch  4, batch     8 | loss: 50.0758596CurrentTrain: epoch  4, batch     9 | loss: 68.0070174CurrentTrain: epoch  4, batch    10 | loss: 104.0824570CurrentTrain: epoch  4, batch    11 | loss: 47.9930216CurrentTrain: epoch  4, batch    12 | loss: 46.7291500CurrentTrain: epoch  4, batch    13 | loss: 45.7228114CurrentTrain: epoch  4, batch    14 | loss: 64.7635803CurrentTrain: epoch  4, batch    15 | loss: 69.7175089CurrentTrain: epoch  4, batch    16 | loss: 38.1570765CurrentTrain: epoch  4, batch    17 | loss: 68.3651644CurrentTrain: epoch  4, batch    18 | loss: 68.4157661CurrentTrain: epoch  4, batch    19 | loss: 52.1697157CurrentTrain: epoch  4, batch    20 | loss: 49.0080737CurrentTrain: epoch  4, batch    21 | loss: 68.3721113CurrentTrain: epoch  4, batch    22 | loss: 38.8605738CurrentTrain: epoch  4, batch    23 | loss: 65.6533678CurrentTrain: epoch  4, batch    24 | loss: 222.3755874CurrentTrain: epoch  4, batch    25 | loss: 107.3815118CurrentTrain: epoch  4, batch    26 | loss: 36.2647435CurrentTrain: epoch  4, batch    27 | loss: 68.9994164CurrentTrain: epoch  4, batch    28 | loss: 31.2783839CurrentTrain: epoch  4, batch    29 | loss: 38.2002493CurrentTrain: epoch  4, batch    30 | loss: 26.1275279CurrentTrain: epoch  4, batch    31 | loss: 68.4610022CurrentTrain: epoch  4, batch    32 | loss: 46.5571434CurrentTrain: epoch  4, batch    33 | loss: 30.0126493CurrentTrain: epoch  4, batch    34 | loss: 36.2176446CurrentTrain: epoch  4, batch    35 | loss: 63.2293552CurrentTrain: epoch  4, batch    36 | loss: 69.1955323CurrentTrain: epoch  4, batch    37 | loss: 30.7762347CurrentTrain: epoch  4, batch    38 | loss: 49.2377307CurrentTrain: epoch  4, batch    39 | loss: 38.0131902CurrentTrain: epoch  4, batch    40 | loss: 67.1304272CurrentTrain: epoch  4, batch    41 | loss: 48.9962264CurrentTrain: epoch  4, batch    42 | loss: 66.9261487CurrentTrain: epoch  4, batch    43 | loss: 25.5799946CurrentTrain: epoch  4, batch    44 | loss: 50.7584108CurrentTrain: epoch  4, batch    45 | loss: 67.2835003CurrentTrain: epoch  4, batch    46 | loss: 36.9395048CurrentTrain: epoch  4, batch    47 | loss: 68.1838051CurrentTrain: epoch  4, batch    48 | loss: 51.2263164CurrentTrain: epoch  4, batch    49 | loss: 38.6490319CurrentTrain: epoch  4, batch    50 | loss: 103.9928011CurrentTrain: epoch  4, batch    51 | loss: 47.6352579CurrentTrain: epoch  4, batch    52 | loss: 38.7254228CurrentTrain: epoch  4, batch    53 | loss: 68.7279808CurrentTrain: epoch  4, batch    54 | loss: 30.1461050CurrentTrain: epoch  4, batch    55 | loss: 47.9199547CurrentTrain: epoch  4, batch    56 | loss: 107.0390035CurrentTrain: epoch  4, batch    57 | loss: 65.3170801CurrentTrain: epoch  4, batch    58 | loss: 36.2904394CurrentTrain: epoch  4, batch    59 | loss: 39.0094623CurrentTrain: epoch  4, batch    60 | loss: 107.4105066CurrentTrain: epoch  4, batch    61 | loss: 107.4987109CurrentTrain: epoch  4, batch    62 | loss: 48.5683087CurrentTrain: epoch  4, batch    63 | loss: 49.0175369CurrentTrain: epoch  4, batch    64 | loss: 33.6809876CurrentTrain: epoch  4, batch    65 | loss: 39.7897422CurrentTrain: epoch  4, batch    66 | loss: 35.9561074CurrentTrain: epoch  4, batch    67 | loss: 35.5983614CurrentTrain: epoch  4, batch    68 | loss: 103.8522915CurrentTrain: epoch  4, batch    69 | loss: 48.3355979CurrentTrain: epoch  4, batch    70 | loss: 36.4620725CurrentTrain: epoch  4, batch    71 | loss: 66.1826174CurrentTrain: epoch  4, batch    72 | loss: 28.4196207CurrentTrain: epoch  4, batch    73 | loss: 41.5090194CurrentTrain: epoch  4, batch    74 | loss: 103.8335625CurrentTrain: epoch  4, batch    75 | loss: 38.3601142CurrentTrain: epoch  4, batch    76 | loss: 68.4250141CurrentTrain: epoch  4, batch    77 | loss: 47.7368776CurrentTrain: epoch  4, batch    78 | loss: 66.2844093CurrentTrain: epoch  4, batch    79 | loss: 31.3521687CurrentTrain: epoch  4, batch    80 | loss: 47.5851518CurrentTrain: epoch  4, batch    81 | loss: 222.2325051CurrentTrain: epoch  4, batch    82 | loss: 48.0470921CurrentTrain: epoch  4, batch    83 | loss: 47.2379718CurrentTrain: epoch  4, batch    84 | loss: 47.8761412CurrentTrain: epoch  4, batch    85 | loss: 40.8669651CurrentTrain: epoch  4, batch    86 | loss: 68.5492553CurrentTrain: epoch  4, batch    87 | loss: 35.4622986CurrentTrain: epoch  4, batch    88 | loss: 66.5038440CurrentTrain: epoch  4, batch    89 | loss: 49.2757157CurrentTrain: epoch  4, batch    90 | loss: 29.1066378CurrentTrain: epoch  4, batch    91 | loss: 38.4252428CurrentTrain: epoch  4, batch    92 | loss: 38.4199022CurrentTrain: epoch  4, batch    93 | loss: 103.6626500CurrentTrain: epoch  4, batch    94 | loss: 30.8269902CurrentTrain: epoch  4, batch    95 | loss: 69.3715338CurrentTrain: epoch  4, batch    96 | loss: 47.8602185CurrentTrain: epoch  4, batch    97 | loss: 107.0487593CurrentTrain: epoch  4, batch    98 | loss: 50.8032818CurrentTrain: epoch  4, batch    99 | loss: 51.8463128CurrentTrain: epoch  4, batch   100 | loss: 36.6710640CurrentTrain: epoch  4, batch   101 | loss: 33.9342485CurrentTrain: epoch  4, batch   102 | loss: 37.6966175CurrentTrain: epoch  4, batch   103 | loss: 68.1603487CurrentTrain: epoch  4, batch   104 | loss: 68.4012015CurrentTrain: epoch  4, batch   105 | loss: 26.8861711CurrentTrain: epoch  4, batch   106 | loss: 67.5802937CurrentTrain: epoch  4, batch   107 | loss: 39.0172317CurrentTrain: epoch  4, batch   108 | loss: 35.6089386CurrentTrain: epoch  4, batch   109 | loss: 47.4968002CurrentTrain: epoch  4, batch   110 | loss: 68.3290284CurrentTrain: epoch  4, batch   111 | loss: 49.8208456CurrentTrain: epoch  4, batch   112 | loss: 37.8311812CurrentTrain: epoch  4, batch   113 | loss: 66.1820749CurrentTrain: epoch  4, batch   114 | loss: 68.4202137CurrentTrain: epoch  4, batch   115 | loss: 37.9717142CurrentTrain: epoch  4, batch   116 | loss: 69.3432555CurrentTrain: epoch  4, batch   117 | loss: 103.7641594CurrentTrain: epoch  4, batch   118 | loss: 36.6537349CurrentTrain: epoch  4, batch   119 | loss: 34.4031419CurrentTrain: epoch  4, batch   120 | loss: 107.1790407CurrentTrain: epoch  4, batch   121 | loss: 69.4220511CurrentTrain: epoch  4, batch   122 | loss: 45.0897015CurrentTrain: epoch  4, batch   123 | loss: 39.6044354CurrentTrain: epoch  4, batch   124 | loss: 37.5876390CurrentTrain: epoch  4, batch   125 | loss: 47.5005563CurrentTrain: epoch  4, batch   126 | loss: 68.6819789CurrentTrain: epoch  4, batch   127 | loss: 66.4757699CurrentTrain: epoch  4, batch   128 | loss: 47.9826238CurrentTrain: epoch  4, batch   129 | loss: 52.3997599CurrentTrain: epoch  4, batch   130 | loss: 23.5507269CurrentTrain: epoch  4, batch   131 | loss: 51.8767271CurrentTrain: epoch  4, batch   132 | loss: 74.4169417CurrentTrain: epoch  4, batch   133 | loss: 70.4266381CurrentTrain: epoch  4, batch   134 | loss: 49.4820590CurrentTrain: epoch  4, batch   135 | loss: 35.6308837CurrentTrain: epoch  4, batch   136 | loss: 32.8953281CurrentTrain: epoch  4, batch   137 | loss: 66.1657128CurrentTrain: epoch  4, batch   138 | loss: 48.0398949CurrentTrain: epoch  4, batch   139 | loss: 107.1593139CurrentTrain: epoch  4, batch   140 | loss: 31.2759331CurrentTrain: epoch  4, batch   141 | loss: 49.0652950CurrentTrain: epoch  4, batch   142 | loss: 69.7637579CurrentTrain: epoch  4, batch   143 | loss: 37.8166631CurrentTrain: epoch  5, batch     0 | loss: 67.3380021CurrentTrain: epoch  5, batch     1 | loss: 36.1787106CurrentTrain: epoch  5, batch     2 | loss: 47.5638433CurrentTrain: epoch  5, batch     3 | loss: 47.6892021CurrentTrain: epoch  5, batch     4 | loss: 30.2315657CurrentTrain: epoch  5, batch     5 | loss: 47.9557171CurrentTrain: epoch  5, batch     6 | loss: 68.7254367CurrentTrain: epoch  5, batch     7 | loss: 47.3950198CurrentTrain: epoch  5, batch     8 | loss: 68.4976892CurrentTrain: epoch  5, batch     9 | loss: 32.2685045CurrentTrain: epoch  5, batch    10 | loss: 28.8728835CurrentTrain: epoch  5, batch    11 | loss: 107.4048936CurrentTrain: epoch  5, batch    12 | loss: 45.1438598CurrentTrain: epoch  5, batch    13 | loss: 49.0948703CurrentTrain: epoch  5, batch    14 | loss: 36.2120971CurrentTrain: epoch  5, batch    15 | loss: 66.8494134CurrentTrain: epoch  5, batch    16 | loss: 66.1502644CurrentTrain: epoch  5, batch    17 | loss: 30.3277551CurrentTrain: epoch  5, batch    18 | loss: 66.5469569CurrentTrain: epoch  5, batch    19 | loss: 38.3135953CurrentTrain: epoch  5, batch    20 | loss: 33.4639767CurrentTrain: epoch  5, batch    21 | loss: 35.4192626CurrentTrain: epoch  5, batch    22 | loss: 47.3710342CurrentTrain: epoch  5, batch    23 | loss: 67.2337288CurrentTrain: epoch  5, batch    24 | loss: 36.5803959CurrentTrain: epoch  5, batch    25 | loss: 37.8212210CurrentTrain: epoch  5, batch    26 | loss: 47.2779116CurrentTrain: epoch  5, batch    27 | loss: 46.3130692CurrentTrain: epoch  5, batch    28 | loss: 48.2173348CurrentTrain: epoch  5, batch    29 | loss: 38.9044304CurrentTrain: epoch  5, batch    30 | loss: 28.0911942CurrentTrain: epoch  5, batch    31 | loss: 48.9709512CurrentTrain: epoch  5, batch    32 | loss: 35.8043853CurrentTrain: epoch  5, batch    33 | loss: 101.9377561CurrentTrain: epoch  5, batch    34 | loss: 64.6680546CurrentTrain: epoch  5, batch    35 | loss: 46.6284830CurrentTrain: epoch  5, batch    36 | loss: 49.2158031CurrentTrain: epoch  5, batch    37 | loss: 29.6338414CurrentTrain: epoch  5, batch    38 | loss: 49.3675442CurrentTrain: epoch  5, batch    39 | loss: 68.3414042CurrentTrain: epoch  5, batch    40 | loss: 69.8048318CurrentTrain: epoch  5, batch    41 | loss: 46.1190414CurrentTrain: epoch  5, batch    42 | loss: 65.4814012CurrentTrain: epoch  5, batch    43 | loss: 37.2721600CurrentTrain: epoch  5, batch    44 | loss: 69.0026419CurrentTrain: epoch  5, batch    45 | loss: 49.8043189CurrentTrain: epoch  5, batch    46 | loss: 24.1464358CurrentTrain: epoch  5, batch    47 | loss: 47.9193810CurrentTrain: epoch  5, batch    48 | loss: 63.2188624CurrentTrain: epoch  5, batch    49 | loss: 28.4818941CurrentTrain: epoch  5, batch    50 | loss: 68.3516523CurrentTrain: epoch  5, batch    51 | loss: 68.4055761CurrentTrain: epoch  5, batch    52 | loss: 52.4479635CurrentTrain: epoch  5, batch    53 | loss: 68.9415606CurrentTrain: epoch  5, batch    54 | loss: 37.7568170CurrentTrain: epoch  5, batch    55 | loss: 68.3012907CurrentTrain: epoch  5, batch    56 | loss: 69.9975051CurrentTrain: epoch  5, batch    57 | loss: 52.1921799CurrentTrain: epoch  5, batch    58 | loss: 46.7250611CurrentTrain: epoch  5, batch    59 | loss: 46.9231256CurrentTrain: epoch  5, batch    60 | loss: 49.3207708CurrentTrain: epoch  5, batch    61 | loss: 37.7822333CurrentTrain: epoch  5, batch    62 | loss: 47.4684341CurrentTrain: epoch  5, batch    63 | loss: 36.5493283CurrentTrain: epoch  5, batch    64 | loss: 35.2891120CurrentTrain: epoch  5, batch    65 | loss: 35.7024644CurrentTrain: epoch  5, batch    66 | loss: 68.1741012CurrentTrain: epoch  5, batch    67 | loss: 68.2230559CurrentTrain: epoch  5, batch    68 | loss: 36.5963681CurrentTrain: epoch  5, batch    69 | loss: 38.7578557CurrentTrain: epoch  5, batch    70 | loss: 68.2380803CurrentTrain: epoch  5, batch    71 | loss: 38.1335567CurrentTrain: epoch  5, batch    72 | loss: 29.5468707CurrentTrain: epoch  5, batch    73 | loss: 42.5328126CurrentTrain: epoch  5, batch    74 | loss: 47.9299051CurrentTrain: epoch  5, batch    75 | loss: 107.5057088CurrentTrain: epoch  5, batch    76 | loss: 49.0643503CurrentTrain: epoch  5, batch    77 | loss: 49.5318573CurrentTrain: epoch  5, batch    78 | loss: 36.7049907CurrentTrain: epoch  5, batch    79 | loss: 37.7424061CurrentTrain: epoch  5, batch    80 | loss: 38.4155375CurrentTrain: epoch  5, batch    81 | loss: 47.7743818CurrentTrain: epoch  5, batch    82 | loss: 106.9021418CurrentTrain: epoch  5, batch    83 | loss: 68.4411743CurrentTrain: epoch  5, batch    84 | loss: 68.5563151CurrentTrain: epoch  5, batch    85 | loss: 37.6532790CurrentTrain: epoch  5, batch    86 | loss: 39.2671147CurrentTrain: epoch  5, batch    87 | loss: 69.6597218CurrentTrain: epoch  5, batch    88 | loss: 68.2976591CurrentTrain: epoch  5, batch    89 | loss: 37.2166587CurrentTrain: epoch  5, batch    90 | loss: 68.6737763CurrentTrain: epoch  5, batch    91 | loss: 47.6212542CurrentTrain: epoch  5, batch    92 | loss: 70.1048249CurrentTrain: epoch  5, batch    93 | loss: 48.9455550CurrentTrain: epoch  5, batch    94 | loss: 37.6047320CurrentTrain: epoch  5, batch    95 | loss: 68.2586475CurrentTrain: epoch  5, batch    96 | loss: 105.1560334CurrentTrain: epoch  5, batch    97 | loss: 66.1826598CurrentTrain: epoch  5, batch    98 | loss: 51.0912507CurrentTrain: epoch  5, batch    99 | loss: 66.3730866CurrentTrain: epoch  5, batch   100 | loss: 38.8333554CurrentTrain: epoch  5, batch   101 | loss: 36.0964052CurrentTrain: epoch  5, batch   102 | loss: 68.1587765CurrentTrain: epoch  5, batch   103 | loss: 37.6232405CurrentTrain: epoch  5, batch   104 | loss: 44.0073784CurrentTrain: epoch  5, batch   105 | loss: 66.3872814CurrentTrain: epoch  5, batch   106 | loss: 48.1786234CurrentTrain: epoch  5, batch   107 | loss: 68.5169639CurrentTrain: epoch  5, batch   108 | loss: 68.2024853CurrentTrain: epoch  5, batch   109 | loss: 46.6121817CurrentTrain: epoch  5, batch   110 | loss: 36.9748986CurrentTrain: epoch  5, batch   111 | loss: 45.9586079CurrentTrain: epoch  5, batch   112 | loss: 107.0679252CurrentTrain: epoch  5, batch   113 | loss: 66.3291695CurrentTrain: epoch  5, batch   114 | loss: 29.6874742CurrentTrain: epoch  5, batch   115 | loss: 30.7310297CurrentTrain: epoch  5, batch   116 | loss: 48.9804557CurrentTrain: epoch  5, batch   117 | loss: 68.1197301CurrentTrain: epoch  5, batch   118 | loss: 30.4377492CurrentTrain: epoch  5, batch   119 | loss: 47.9541051CurrentTrain: epoch  5, batch   120 | loss: 47.5852780CurrentTrain: epoch  5, batch   121 | loss: 64.9175462CurrentTrain: epoch  5, batch   122 | loss: 47.4785966CurrentTrain: epoch  5, batch   123 | loss: 66.7762415CurrentTrain: epoch  5, batch   124 | loss: 37.5722065CurrentTrain: epoch  5, batch   125 | loss: 46.4352390CurrentTrain: epoch  5, batch   126 | loss: 33.6150784CurrentTrain: epoch  5, batch   127 | loss: 104.6419372CurrentTrain: epoch  5, batch   128 | loss: 50.3079088CurrentTrain: epoch  5, batch   129 | loss: 34.4256418CurrentTrain: epoch  5, batch   130 | loss: 48.9505277CurrentTrain: epoch  5, batch   131 | loss: 49.7291875CurrentTrain: epoch  5, batch   132 | loss: 33.7072358CurrentTrain: epoch  5, batch   133 | loss: 222.2774164CurrentTrain: epoch  5, batch   134 | loss: 63.8086470CurrentTrain: epoch  5, batch   135 | loss: 68.0036114CurrentTrain: epoch  5, batch   136 | loss: 49.1537925CurrentTrain: epoch  5, batch   137 | loss: 49.8018257CurrentTrain: epoch  5, batch   138 | loss: 36.5550379CurrentTrain: epoch  5, batch   139 | loss: 48.6633749CurrentTrain: epoch  5, batch   140 | loss: 38.7610455CurrentTrain: epoch  5, batch   141 | loss: 64.5847655CurrentTrain: epoch  5, batch   142 | loss: 107.1122921CurrentTrain: epoch  5, batch   143 | loss: 34.0162446CurrentTrain: epoch  6, batch     0 | loss: 50.5718714CurrentTrain: epoch  6, batch     1 | loss: 49.1826290CurrentTrain: epoch  6, batch     2 | loss: 32.0221409CurrentTrain: epoch  6, batch     3 | loss: 105.5017817CurrentTrain: epoch  6, batch     4 | loss: 34.2325377CurrentTrain: epoch  6, batch     5 | loss: 46.9332901CurrentTrain: epoch  6, batch     6 | loss: 66.5667646CurrentTrain: epoch  6, batch     7 | loss: 46.1562247CurrentTrain: epoch  6, batch     8 | loss: 68.2535644CurrentTrain: epoch  6, batch     9 | loss: 68.3843845CurrentTrain: epoch  6, batch    10 | loss: 66.6720441CurrentTrain: epoch  6, batch    11 | loss: 38.2970568CurrentTrain: epoch  6, batch    12 | loss: 27.4589026CurrentTrain: epoch  6, batch    13 | loss: 38.8704855CurrentTrain: epoch  6, batch    14 | loss: 30.5865964CurrentTrain: epoch  6, batch    15 | loss: 34.8350030CurrentTrain: epoch  6, batch    16 | loss: 66.2499599CurrentTrain: epoch  6, batch    17 | loss: 46.1110045CurrentTrain: epoch  6, batch    18 | loss: 46.6276910CurrentTrain: epoch  6, batch    19 | loss: 66.6459526CurrentTrain: epoch  6, batch    20 | loss: 36.9106174CurrentTrain: epoch  6, batch    21 | loss: 25.4665357CurrentTrain: epoch  6, batch    22 | loss: 37.7664953CurrentTrain: epoch  6, batch    23 | loss: 47.8282995CurrentTrain: epoch  6, batch    24 | loss: 36.5833191CurrentTrain: epoch  6, batch    25 | loss: 36.7117037CurrentTrain: epoch  6, batch    26 | loss: 37.9664399CurrentTrain: epoch  6, batch    27 | loss: 29.8996620CurrentTrain: epoch  6, batch    28 | loss: 28.3125054CurrentTrain: epoch  6, batch    29 | loss: 66.1656825CurrentTrain: epoch  6, batch    30 | loss: 69.8346788CurrentTrain: epoch  6, batch    31 | loss: 49.3617254CurrentTrain: epoch  6, batch    32 | loss: 68.1789419CurrentTrain: epoch  6, batch    33 | loss: 48.1251657CurrentTrain: epoch  6, batch    34 | loss: 29.6652392CurrentTrain: epoch  6, batch    35 | loss: 49.1671042CurrentTrain: epoch  6, batch    36 | loss: 68.1818963CurrentTrain: epoch  6, batch    37 | loss: 106.8958356CurrentTrain: epoch  6, batch    38 | loss: 68.2455680CurrentTrain: epoch  6, batch    39 | loss: 106.9109123CurrentTrain: epoch  6, batch    40 | loss: 46.3598727CurrentTrain: epoch  6, batch    41 | loss: 47.9956627CurrentTrain: epoch  6, batch    42 | loss: 47.5254345CurrentTrain: epoch  6, batch    43 | loss: 107.2858214CurrentTrain: epoch  6, batch    44 | loss: 49.1603238CurrentTrain: epoch  6, batch    45 | loss: 68.1469812CurrentTrain: epoch  6, batch    46 | loss: 37.3341119CurrentTrain: epoch  6, batch    47 | loss: 45.4712218CurrentTrain: epoch  6, batch    48 | loss: 48.5167453CurrentTrain: epoch  6, batch    49 | loss: 31.0640406CurrentTrain: epoch  6, batch    50 | loss: 35.3909443CurrentTrain: epoch  6, batch    51 | loss: 28.9385492CurrentTrain: epoch  6, batch    52 | loss: 34.8810535CurrentTrain: epoch  6, batch    53 | loss: 49.0128110CurrentTrain: epoch  6, batch    54 | loss: 48.0646460CurrentTrain: epoch  6, batch    55 | loss: 36.9126654CurrentTrain: epoch  6, batch    56 | loss: 106.8155690CurrentTrain: epoch  6, batch    57 | loss: 49.4667130CurrentTrain: epoch  6, batch    58 | loss: 68.1376154CurrentTrain: epoch  6, batch    59 | loss: 47.5348497CurrentTrain: epoch  6, batch    60 | loss: 68.1200362CurrentTrain: epoch  6, batch    61 | loss: 103.8193608CurrentTrain: epoch  6, batch    62 | loss: 48.4550207CurrentTrain: epoch  6, batch    63 | loss: 65.1723043CurrentTrain: epoch  6, batch    64 | loss: 36.5180078CurrentTrain: epoch  6, batch    65 | loss: 68.1048888CurrentTrain: epoch  6, batch    66 | loss: 48.9031109CurrentTrain: epoch  6, batch    67 | loss: 27.9426775CurrentTrain: epoch  6, batch    68 | loss: 68.2653217CurrentTrain: epoch  6, batch    69 | loss: 47.5030947CurrentTrain: epoch  6, batch    70 | loss: 47.9667168CurrentTrain: epoch  6, batch    71 | loss: 50.6094927CurrentTrain: epoch  6, batch    72 | loss: 68.1931395CurrentTrain: epoch  6, batch    73 | loss: 29.0756726CurrentTrain: epoch  6, batch    74 | loss: 29.0923158CurrentTrain: epoch  6, batch    75 | loss: 43.3640355CurrentTrain: epoch  6, batch    76 | loss: 35.2685161CurrentTrain: epoch  6, batch    77 | loss: 36.8283522CurrentTrain: epoch  6, batch    78 | loss: 28.8965460CurrentTrain: epoch  6, batch    79 | loss: 35.5924185CurrentTrain: epoch  6, batch    80 | loss: 53.1826657CurrentTrain: epoch  6, batch    81 | loss: 69.3636793CurrentTrain: epoch  6, batch    82 | loss: 36.3798988CurrentTrain: epoch  6, batch    83 | loss: 70.0744483CurrentTrain: epoch  6, batch    84 | loss: 48.1476376CurrentTrain: epoch  6, batch    85 | loss: 29.3894083CurrentTrain: epoch  6, batch    86 | loss: 66.4947622CurrentTrain: epoch  6, batch    87 | loss: 46.0584928CurrentTrain: epoch  6, batch    88 | loss: 48.2047420CurrentTrain: epoch  6, batch    89 | loss: 47.5144261CurrentTrain: epoch  6, batch    90 | loss: 46.3546789CurrentTrain: epoch  6, batch    91 | loss: 222.1666769CurrentTrain: epoch  6, batch    92 | loss: 47.9229285CurrentTrain: epoch  6, batch    93 | loss: 68.0757224CurrentTrain: epoch  6, batch    94 | loss: 30.6113506CurrentTrain: epoch  6, batch    95 | loss: 68.4153851CurrentTrain: epoch  6, batch    96 | loss: 106.7306998CurrentTrain: epoch  6, batch    97 | loss: 35.3714403CurrentTrain: epoch  6, batch    98 | loss: 106.7721857CurrentTrain: epoch  6, batch    99 | loss: 66.2747662CurrentTrain: epoch  6, batch   100 | loss: 64.5592391CurrentTrain: epoch  6, batch   101 | loss: 29.7394055CurrentTrain: epoch  6, batch   102 | loss: 47.3360756CurrentTrain: epoch  6, batch   103 | loss: 52.8273164CurrentTrain: epoch  6, batch   104 | loss: 27.8819202CurrentTrain: epoch  6, batch   105 | loss: 48.9416182CurrentTrain: epoch  6, batch   106 | loss: 48.5131418CurrentTrain: epoch  6, batch   107 | loss: 69.0035092CurrentTrain: epoch  6, batch   108 | loss: 66.1662897CurrentTrain: epoch  6, batch   109 | loss: 47.9171492CurrentTrain: epoch  6, batch   110 | loss: 27.7827406CurrentTrain: epoch  6, batch   111 | loss: 36.4053048CurrentTrain: epoch  6, batch   112 | loss: 68.2611268CurrentTrain: epoch  6, batch   113 | loss: 48.9608885CurrentTrain: epoch  6, batch   114 | loss: 64.4888391CurrentTrain: epoch  6, batch   115 | loss: 46.3139592CurrentTrain: epoch  6, batch   116 | loss: 29.1534010CurrentTrain: epoch  6, batch   117 | loss: 51.3187279CurrentTrain: epoch  6, batch   118 | loss: 101.8219774CurrentTrain: epoch  6, batch   119 | loss: 51.5625630CurrentTrain: epoch  6, batch   120 | loss: 23.2887961CurrentTrain: epoch  6, batch   121 | loss: 45.3512602CurrentTrain: epoch  6, batch   122 | loss: 47.2174107CurrentTrain: epoch  6, batch   123 | loss: 35.4578409CurrentTrain: epoch  6, batch   124 | loss: 49.7844715CurrentTrain: epoch  6, batch   125 | loss: 49.3804890CurrentTrain: epoch  6, batch   126 | loss: 68.1837678CurrentTrain: epoch  6, batch   127 | loss: 68.3333753CurrentTrain: epoch  6, batch   128 | loss: 49.9685118CurrentTrain: epoch  6, batch   129 | loss: 106.8335463CurrentTrain: epoch  6, batch   130 | loss: 49.2210064CurrentTrain: epoch  6, batch   131 | loss: 28.7924018CurrentTrain: epoch  6, batch   132 | loss: 47.6203881CurrentTrain: epoch  6, batch   133 | loss: 68.1080977CurrentTrain: epoch  6, batch   134 | loss: 66.1262270CurrentTrain: epoch  6, batch   135 | loss: 46.5307293CurrentTrain: epoch  6, batch   136 | loss: 50.0705045CurrentTrain: epoch  6, batch   137 | loss: 107.5453917CurrentTrain: epoch  6, batch   138 | loss: 63.8949344CurrentTrain: epoch  6, batch   139 | loss: 35.5547906CurrentTrain: epoch  6, batch   140 | loss: 49.9022631CurrentTrain: epoch  6, batch   141 | loss: 36.9503407CurrentTrain: epoch  6, batch   142 | loss: 111.7639617CurrentTrain: epoch  6, batch   143 | loss: 26.3431752CurrentTrain: epoch  7, batch     0 | loss: 107.1859654CurrentTrain: epoch  7, batch     1 | loss: 64.3857695CurrentTrain: epoch  7, batch     2 | loss: 28.8422263CurrentTrain: epoch  7, batch     3 | loss: 68.1731515CurrentTrain: epoch  7, batch     4 | loss: 46.2341372CurrentTrain: epoch  7, batch     5 | loss: 48.6754694CurrentTrain: epoch  7, batch     6 | loss: 66.9566285CurrentTrain: epoch  7, batch     7 | loss: 66.8952458CurrentTrain: epoch  7, batch     8 | loss: 35.7065830CurrentTrain: epoch  7, batch     9 | loss: 35.6401173CurrentTrain: epoch  7, batch    10 | loss: 68.2077917CurrentTrain: epoch  7, batch    11 | loss: 66.2084750CurrentTrain: epoch  7, batch    12 | loss: 50.3645470CurrentTrain: epoch  7, batch    13 | loss: 30.3304921CurrentTrain: epoch  7, batch    14 | loss: 50.6131535CurrentTrain: epoch  7, batch    15 | loss: 46.3887467CurrentTrain: epoch  7, batch    16 | loss: 107.3409526CurrentTrain: epoch  7, batch    17 | loss: 65.1686791CurrentTrain: epoch  7, batch    18 | loss: 28.8416858CurrentTrain: epoch  7, batch    19 | loss: 48.5020569CurrentTrain: epoch  7, batch    20 | loss: 31.0893101CurrentTrain: epoch  7, batch    21 | loss: 24.4367386CurrentTrain: epoch  7, batch    22 | loss: 34.7791265CurrentTrain: epoch  7, batch    23 | loss: 34.5337557CurrentTrain: epoch  7, batch    24 | loss: 107.9248217CurrentTrain: epoch  7, batch    25 | loss: 29.0656277CurrentTrain: epoch  7, batch    26 | loss: 62.6963853CurrentTrain: epoch  7, batch    27 | loss: 34.9562969CurrentTrain: epoch  7, batch    28 | loss: 45.8349482CurrentTrain: epoch  7, batch    29 | loss: 36.5868295CurrentTrain: epoch  7, batch    30 | loss: 68.4192410CurrentTrain: epoch  7, batch    31 | loss: 66.2957931CurrentTrain: epoch  7, batch    32 | loss: 106.8899667CurrentTrain: epoch  7, batch    33 | loss: 47.7365887CurrentTrain: epoch  7, batch    34 | loss: 49.2242959CurrentTrain: epoch  7, batch    35 | loss: 46.0828683CurrentTrain: epoch  7, batch    36 | loss: 29.1926373CurrentTrain: epoch  7, batch    37 | loss: 66.5470019CurrentTrain: epoch  7, batch    38 | loss: 64.3372110CurrentTrain: epoch  7, batch    39 | loss: 71.4298140CurrentTrain: epoch  7, batch    40 | loss: 34.4481173CurrentTrain: epoch  7, batch    41 | loss: 66.1544131CurrentTrain: epoch  7, batch    42 | loss: 37.3130541CurrentTrain: epoch  7, batch    43 | loss: 48.5137547CurrentTrain: epoch  7, batch    44 | loss: 222.2991474CurrentTrain: epoch  7, batch    45 | loss: 48.9012371CurrentTrain: epoch  7, batch    46 | loss: 47.2718086CurrentTrain: epoch  7, batch    47 | loss: 68.7807013CurrentTrain: epoch  7, batch    48 | loss: 66.1243430CurrentTrain: epoch  7, batch    49 | loss: 36.4679923CurrentTrain: epoch  7, batch    50 | loss: 29.9589100CurrentTrain: epoch  7, batch    51 | loss: 35.2122315CurrentTrain: epoch  7, batch    52 | loss: 28.7729012CurrentTrain: epoch  7, batch    53 | loss: 36.5410347CurrentTrain: epoch  7, batch    54 | loss: 66.1404514CurrentTrain: epoch  7, batch    55 | loss: 68.1404046CurrentTrain: epoch  7, batch    56 | loss: 68.1653031CurrentTrain: epoch  7, batch    57 | loss: 66.2116277CurrentTrain: epoch  7, batch    58 | loss: 36.6541444CurrentTrain: epoch  7, batch    59 | loss: 106.8023176CurrentTrain: epoch  7, batch    60 | loss: 37.8479839CurrentTrain: epoch  7, batch    61 | loss: 35.3851641CurrentTrain: epoch  7, batch    62 | loss: 68.2003594CurrentTrain: epoch  7, batch    63 | loss: 35.4916319CurrentTrain: epoch  7, batch    64 | loss: 27.9208262CurrentTrain: epoch  7, batch    65 | loss: 66.3704046CurrentTrain: epoch  7, batch    66 | loss: 49.2773108CurrentTrain: epoch  7, batch    67 | loss: 48.9832777CurrentTrain: epoch  7, batch    68 | loss: 222.1345328CurrentTrain: epoch  7, batch    69 | loss: 47.5434281CurrentTrain: epoch  7, batch    70 | loss: 47.6232787CurrentTrain: epoch  7, batch    71 | loss: 48.0711552CurrentTrain: epoch  7, batch    72 | loss: 66.5243217CurrentTrain: epoch  7, batch    73 | loss: 35.5443559CurrentTrain: epoch  7, batch    74 | loss: 27.3293043CurrentTrain: epoch  7, batch    75 | loss: 106.8540612CurrentTrain: epoch  7, batch    76 | loss: 45.2283082CurrentTrain: epoch  7, batch    77 | loss: 49.7139201CurrentTrain: epoch  7, batch    78 | loss: 47.5546752CurrentTrain: epoch  7, batch    79 | loss: 34.4480430CurrentTrain: epoch  7, batch    80 | loss: 99.0450192CurrentTrain: epoch  7, batch    81 | loss: 48.9003534CurrentTrain: epoch  7, batch    82 | loss: 50.9486410CurrentTrain: epoch  7, batch    83 | loss: 68.2022501CurrentTrain: epoch  7, batch    84 | loss: 47.5854562CurrentTrain: epoch  7, batch    85 | loss: 50.2328577CurrentTrain: epoch  7, batch    86 | loss: 46.0640478CurrentTrain: epoch  7, batch    87 | loss: 35.2096794CurrentTrain: epoch  7, batch    88 | loss: 35.4659506CurrentTrain: epoch  7, batch    89 | loss: 49.0805520CurrentTrain: epoch  7, batch    90 | loss: 67.0030025CurrentTrain: epoch  7, batch    91 | loss: 64.5918542CurrentTrain: epoch  7, batch    92 | loss: 106.8185142CurrentTrain: epoch  7, batch    93 | loss: 37.5319424CurrentTrain: epoch  7, batch    94 | loss: 44.9978310CurrentTrain: epoch  7, batch    95 | loss: 28.5873639CurrentTrain: epoch  7, batch    96 | loss: 70.9842892CurrentTrain: epoch  7, batch    97 | loss: 68.0858618CurrentTrain: epoch  7, batch    98 | loss: 35.1212340CurrentTrain: epoch  7, batch    99 | loss: 49.9978788CurrentTrain: epoch  7, batch   100 | loss: 49.2221630CurrentTrain: epoch  7, batch   101 | loss: 45.4503891CurrentTrain: epoch  7, batch   102 | loss: 63.2058979CurrentTrain: epoch  7, batch   103 | loss: 36.4298173CurrentTrain: epoch  7, batch   104 | loss: 66.1867624CurrentTrain: epoch  7, batch   105 | loss: 27.7975545CurrentTrain: epoch  7, batch   106 | loss: 35.2347412CurrentTrain: epoch  7, batch   107 | loss: 48.8902438CurrentTrain: epoch  7, batch   108 | loss: 48.9825487CurrentTrain: epoch  7, batch   109 | loss: 47.8983084CurrentTrain: epoch  7, batch   110 | loss: 61.9971218CurrentTrain: epoch  7, batch   111 | loss: 68.2018415CurrentTrain: epoch  7, batch   112 | loss: 66.1098543CurrentTrain: epoch  7, batch   113 | loss: 37.9399678CurrentTrain: epoch  7, batch   114 | loss: 53.3741102CurrentTrain: epoch  7, batch   115 | loss: 68.1238247CurrentTrain: epoch  7, batch   116 | loss: 48.9717995CurrentTrain: epoch  7, batch   117 | loss: 49.3480379CurrentTrain: epoch  7, batch   118 | loss: 68.0552049CurrentTrain: epoch  7, batch   119 | loss: 36.4408258CurrentTrain: epoch  7, batch   120 | loss: 64.2481267CurrentTrain: epoch  7, batch   121 | loss: 34.5777645CurrentTrain: epoch  7, batch   122 | loss: 46.6448710CurrentTrain: epoch  7, batch   123 | loss: 70.9151135CurrentTrain: epoch  7, batch   124 | loss: 66.5833799CurrentTrain: epoch  7, batch   125 | loss: 68.5666886CurrentTrain: epoch  7, batch   126 | loss: 68.0942466CurrentTrain: epoch  7, batch   127 | loss: 47.4450027CurrentTrain: epoch  7, batch   128 | loss: 66.1663949CurrentTrain: epoch  7, batch   129 | loss: 69.3040774CurrentTrain: epoch  7, batch   130 | loss: 40.0809434CurrentTrain: epoch  7, batch   131 | loss: 36.4308979CurrentTrain: epoch  7, batch   132 | loss: 35.7353453CurrentTrain: epoch  7, batch   133 | loss: 103.6270244CurrentTrain: epoch  7, batch   134 | loss: 64.5771548CurrentTrain: epoch  7, batch   135 | loss: 47.5676541CurrentTrain: epoch  7, batch   136 | loss: 38.9559414CurrentTrain: epoch  7, batch   137 | loss: 68.1266661CurrentTrain: epoch  7, batch   138 | loss: 36.5495849CurrentTrain: epoch  7, batch   139 | loss: 31.2028359CurrentTrain: epoch  7, batch   140 | loss: 48.0611667CurrentTrain: epoch  7, batch   141 | loss: 35.9697964CurrentTrain: epoch  7, batch   142 | loss: 64.6655645CurrentTrain: epoch  7, batch   143 | loss: 36.1763972CurrentTrain: epoch  8, batch     0 | loss: 28.9555516CurrentTrain: epoch  8, batch     1 | loss: 36.7556838CurrentTrain: epoch  8, batch     2 | loss: 37.4969090CurrentTrain: epoch  8, batch     3 | loss: 45.1472261CurrentTrain: epoch  8, batch     4 | loss: 37.4464445CurrentTrain: epoch  8, batch     5 | loss: 66.5067855CurrentTrain: epoch  8, batch     6 | loss: 47.4912676CurrentTrain: epoch  8, batch     7 | loss: 68.0925201CurrentTrain: epoch  8, batch     8 | loss: 47.4457974CurrentTrain: epoch  8, batch     9 | loss: 29.7096423CurrentTrain: epoch  8, batch    10 | loss: 68.0798555CurrentTrain: epoch  8, batch    11 | loss: 36.2614535CurrentTrain: epoch  8, batch    12 | loss: 36.2528776CurrentTrain: epoch  8, batch    13 | loss: 68.2646446CurrentTrain: epoch  8, batch    14 | loss: 37.7358719CurrentTrain: epoch  8, batch    15 | loss: 28.8624029CurrentTrain: epoch  8, batch    16 | loss: 30.7864234CurrentTrain: epoch  8, batch    17 | loss: 34.2639519CurrentTrain: epoch  8, batch    18 | loss: 29.8307602CurrentTrain: epoch  8, batch    19 | loss: 66.0924466CurrentTrain: epoch  8, batch    20 | loss: 50.0838163CurrentTrain: epoch  8, batch    21 | loss: 48.0911430CurrentTrain: epoch  8, batch    22 | loss: 66.3160455CurrentTrain: epoch  8, batch    23 | loss: 48.9328432CurrentTrain: epoch  8, batch    24 | loss: 49.3553934CurrentTrain: epoch  8, batch    25 | loss: 69.2431952CurrentTrain: epoch  8, batch    26 | loss: 33.1135891CurrentTrain: epoch  8, batch    27 | loss: 67.3038437CurrentTrain: epoch  8, batch    28 | loss: 106.8042646CurrentTrain: epoch  8, batch    29 | loss: 46.8099744CurrentTrain: epoch  8, batch    30 | loss: 32.6097693CurrentTrain: epoch  8, batch    31 | loss: 46.4401384CurrentTrain: epoch  8, batch    32 | loss: 35.3926206CurrentTrain: epoch  8, batch    33 | loss: 37.0468286CurrentTrain: epoch  8, batch    34 | loss: 68.4129634CurrentTrain: epoch  8, batch    35 | loss: 68.0918101CurrentTrain: epoch  8, batch    36 | loss: 46.0136146CurrentTrain: epoch  8, batch    37 | loss: 48.8838134CurrentTrain: epoch  8, batch    38 | loss: 30.1253982CurrentTrain: epoch  8, batch    39 | loss: 48.9199176CurrentTrain: epoch  8, batch    40 | loss: 29.9196937CurrentTrain: epoch  8, batch    41 | loss: 67.4874568CurrentTrain: epoch  8, batch    42 | loss: 38.2924351CurrentTrain: epoch  8, batch    43 | loss: 68.1747179CurrentTrain: epoch  8, batch    44 | loss: 26.6039849CurrentTrain: epoch  8, batch    45 | loss: 49.4167990CurrentTrain: epoch  8, batch    46 | loss: 48.8761504CurrentTrain: epoch  8, batch    47 | loss: 28.0982975CurrentTrain: epoch  8, batch    48 | loss: 47.5380638CurrentTrain: epoch  8, batch    49 | loss: 28.0611692CurrentTrain: epoch  8, batch    50 | loss: 48.2730537CurrentTrain: epoch  8, batch    51 | loss: 49.1405729CurrentTrain: epoch  8, batch    52 | loss: 47.6604762CurrentTrain: epoch  8, batch    53 | loss: 38.3073875CurrentTrain: epoch  8, batch    54 | loss: 47.4515228CurrentTrain: epoch  8, batch    55 | loss: 47.5674036CurrentTrain: epoch  8, batch    56 | loss: 29.0246179CurrentTrain: epoch  8, batch    57 | loss: 103.5170922CurrentTrain: epoch  8, batch    58 | loss: 49.0428734CurrentTrain: epoch  8, batch    59 | loss: 33.6597540CurrentTrain: epoch  8, batch    60 | loss: 47.7834303CurrentTrain: epoch  8, batch    61 | loss: 36.3384653CurrentTrain: epoch  8, batch    62 | loss: 48.9512969CurrentTrain: epoch  8, batch    63 | loss: 47.4768221CurrentTrain: epoch  8, batch    64 | loss: 51.7927598CurrentTrain: epoch  8, batch    65 | loss: 47.5004917CurrentTrain: epoch  8, batch    66 | loss: 38.3176808CurrentTrain: epoch  8, batch    67 | loss: 36.2824953CurrentTrain: epoch  8, batch    68 | loss: 64.6573644CurrentTrain: epoch  8, batch    69 | loss: 48.8843927CurrentTrain: epoch  8, batch    70 | loss: 48.9107418CurrentTrain: epoch  8, batch    71 | loss: 66.4976164CurrentTrain: epoch  8, batch    72 | loss: 46.1575170CurrentTrain: epoch  8, batch    73 | loss: 49.1056014CurrentTrain: epoch  8, batch    74 | loss: 106.8786969CurrentTrain: epoch  8, batch    75 | loss: 38.2970048CurrentTrain: epoch  8, batch    76 | loss: 68.0680518CurrentTrain: epoch  8, batch    77 | loss: 68.1037584CurrentTrain: epoch  8, batch    78 | loss: 33.5047180CurrentTrain: epoch  8, batch    79 | loss: 62.6442962CurrentTrain: epoch  8, batch    80 | loss: 103.5241619CurrentTrain: epoch  8, batch    81 | loss: 49.2224043CurrentTrain: epoch  8, batch    82 | loss: 44.9579502CurrentTrain: epoch  8, batch    83 | loss: 32.5282360CurrentTrain: epoch  8, batch    84 | loss: 37.4729098CurrentTrain: epoch  8, batch    85 | loss: 37.4594502CurrentTrain: epoch  8, batch    86 | loss: 49.3219357CurrentTrain: epoch  8, batch    87 | loss: 43.7480317CurrentTrain: epoch  8, batch    88 | loss: 47.8357744CurrentTrain: epoch  8, batch    89 | loss: 29.0406709CurrentTrain: epoch  8, batch    90 | loss: 47.4179265CurrentTrain: epoch  8, batch    91 | loss: 66.2159398CurrentTrain: epoch  8, batch    92 | loss: 106.8099630CurrentTrain: epoch  8, batch    93 | loss: 48.9928468CurrentTrain: epoch  8, batch    94 | loss: 106.8144516CurrentTrain: epoch  8, batch    95 | loss: 37.9932259CurrentTrain: epoch  8, batch    96 | loss: 66.1986486CurrentTrain: epoch  8, batch    97 | loss: 68.2437244CurrentTrain: epoch  8, batch    98 | loss: 64.1091014CurrentTrain: epoch  8, batch    99 | loss: 103.4543057CurrentTrain: epoch  8, batch   100 | loss: 36.4006091CurrentTrain: epoch  8, batch   101 | loss: 67.4471427CurrentTrain: epoch  8, batch   102 | loss: 29.8801559CurrentTrain: epoch  8, batch   103 | loss: 68.0880944CurrentTrain: epoch  8, batch   104 | loss: 68.0889892CurrentTrain: epoch  8, batch   105 | loss: 45.0387928CurrentTrain: epoch  8, batch   106 | loss: 106.8397837CurrentTrain: epoch  8, batch   107 | loss: 35.2722256CurrentTrain: epoch  8, batch   108 | loss: 36.2459920CurrentTrain: epoch  8, batch   109 | loss: 37.1645533CurrentTrain: epoch  8, batch   110 | loss: 35.3786951CurrentTrain: epoch  8, batch   111 | loss: 48.0020956CurrentTrain: epoch  8, batch   112 | loss: 68.3221717CurrentTrain: epoch  8, batch   113 | loss: 37.4371148CurrentTrain: epoch  8, batch   114 | loss: 37.4033367CurrentTrain: epoch  8, batch   115 | loss: 39.3281303CurrentTrain: epoch  8, batch   116 | loss: 37.8786950CurrentTrain: epoch  8, batch   117 | loss: 63.8391477CurrentTrain: epoch  8, batch   118 | loss: 37.6269098CurrentTrain: epoch  8, batch   119 | loss: 65.7407644CurrentTrain: epoch  8, batch   120 | loss: 68.0619251CurrentTrain: epoch  8, batch   121 | loss: 36.3545804CurrentTrain: epoch  8, batch   122 | loss: 106.7904666CurrentTrain: epoch  8, batch   123 | loss: 49.0953617CurrentTrain: epoch  8, batch   124 | loss: 47.6332985CurrentTrain: epoch  8, batch   125 | loss: 68.0803369CurrentTrain: epoch  8, batch   126 | loss: 37.8575679CurrentTrain: epoch  8, batch   127 | loss: 36.3520293CurrentTrain: epoch  8, batch   128 | loss: 33.9313032CurrentTrain: epoch  8, batch   129 | loss: 35.2378827CurrentTrain: epoch  8, batch   130 | loss: 40.9465571CurrentTrain: epoch  8, batch   131 | loss: 103.4870578CurrentTrain: epoch  8, batch   132 | loss: 37.5585949CurrentTrain: epoch  8, batch   133 | loss: 34.6861374CurrentTrain: epoch  8, batch   134 | loss: 66.1105779CurrentTrain: epoch  8, batch   135 | loss: 37.5575196CurrentTrain: epoch  8, batch   136 | loss: 106.9798149CurrentTrain: epoch  8, batch   137 | loss: 66.0812163CurrentTrain: epoch  8, batch   138 | loss: 28.9565202CurrentTrain: epoch  8, batch   139 | loss: 44.4318005CurrentTrain: epoch  8, batch   140 | loss: 35.2860620CurrentTrain: epoch  8, batch   141 | loss: 68.0881747CurrentTrain: epoch  8, batch   142 | loss: 48.0251188CurrentTrain: epoch  8, batch   143 | loss: 50.9832884CurrentTrain: epoch  9, batch     0 | loss: 38.1730660CurrentTrain: epoch  9, batch     1 | loss: 106.7775864CurrentTrain: epoch  9, batch     2 | loss: 106.7972721CurrentTrain: epoch  9, batch     3 | loss: 66.0726577CurrentTrain: epoch  9, batch     4 | loss: 36.2921635CurrentTrain: epoch  9, batch     5 | loss: 35.3172615CurrentTrain: epoch  9, batch     6 | loss: 49.1280714CurrentTrain: epoch  9, batch     7 | loss: 66.2229019CurrentTrain: epoch  9, batch     8 | loss: 48.9195164CurrentTrain: epoch  9, batch     9 | loss: 46.0490985CurrentTrain: epoch  9, batch    10 | loss: 70.9542874CurrentTrain: epoch  9, batch    11 | loss: 62.5325470CurrentTrain: epoch  9, batch    12 | loss: 47.4359076CurrentTrain: epoch  9, batch    13 | loss: 66.0977594CurrentTrain: epoch  9, batch    14 | loss: 106.7316254CurrentTrain: epoch  9, batch    15 | loss: 100.9860752CurrentTrain: epoch  9, batch    16 | loss: 106.9829724CurrentTrain: epoch  9, batch    17 | loss: 66.9019781CurrentTrain: epoch  9, batch    18 | loss: 64.1644148CurrentTrain: epoch  9, batch    19 | loss: 30.0075374CurrentTrain: epoch  9, batch    20 | loss: 66.0894408CurrentTrain: epoch  9, batch    21 | loss: 46.2717980CurrentTrain: epoch  9, batch    22 | loss: 36.2879763CurrentTrain: epoch  9, batch    23 | loss: 37.6514910CurrentTrain: epoch  9, batch    24 | loss: 38.8293618CurrentTrain: epoch  9, batch    25 | loss: 49.0460215CurrentTrain: epoch  9, batch    26 | loss: 27.4701480CurrentTrain: epoch  9, batch    27 | loss: 36.3732014CurrentTrain: epoch  9, batch    28 | loss: 47.5087908CurrentTrain: epoch  9, batch    29 | loss: 62.5265410CurrentTrain: epoch  9, batch    30 | loss: 66.1479404CurrentTrain: epoch  9, batch    31 | loss: 48.9263149CurrentTrain: epoch  9, batch    32 | loss: 47.5187181CurrentTrain: epoch  9, batch    33 | loss: 49.1311974CurrentTrain: epoch  9, batch    34 | loss: 35.2608762CurrentTrain: epoch  9, batch    35 | loss: 46.3360142CurrentTrain: epoch  9, batch    36 | loss: 36.3275169CurrentTrain: epoch  9, batch    37 | loss: 34.2595872CurrentTrain: epoch  9, batch    38 | loss: 66.1272725CurrentTrain: epoch  9, batch    39 | loss: 28.9295300CurrentTrain: epoch  9, batch    40 | loss: 68.1064073CurrentTrain: epoch  9, batch    41 | loss: 34.2273006CurrentTrain: epoch  9, batch    42 | loss: 46.0980243CurrentTrain: epoch  9, batch    43 | loss: 46.1398733CurrentTrain: epoch  9, batch    44 | loss: 106.9474622CurrentTrain: epoch  9, batch    45 | loss: 46.0575334CurrentTrain: epoch  9, batch    46 | loss: 36.2190662CurrentTrain: epoch  9, batch    47 | loss: 66.0797937CurrentTrain: epoch  9, batch    48 | loss: 66.0955231CurrentTrain: epoch  9, batch    49 | loss: 66.1597428CurrentTrain: epoch  9, batch    50 | loss: 46.2314743CurrentTrain: epoch  9, batch    51 | loss: 28.8541481CurrentTrain: epoch  9, batch    52 | loss: 46.4324127CurrentTrain: epoch  9, batch    53 | loss: 66.1321556CurrentTrain: epoch  9, batch    54 | loss: 49.0236237CurrentTrain: epoch  9, batch    55 | loss: 36.2987771CurrentTrain: epoch  9, batch    56 | loss: 49.0626577CurrentTrain: epoch  9, batch    57 | loss: 49.0577795CurrentTrain: epoch  9, batch    58 | loss: 100.9951560CurrentTrain: epoch  9, batch    59 | loss: 68.0685169CurrentTrain: epoch  9, batch    60 | loss: 35.3998861CurrentTrain: epoch  9, batch    61 | loss: 51.9088530CurrentTrain: epoch  9, batch    62 | loss: 28.5881932CurrentTrain: epoch  9, batch    63 | loss: 222.1092705CurrentTrain: epoch  9, batch    64 | loss: 37.1134814CurrentTrain: epoch  9, batch    65 | loss: 28.9135109CurrentTrain: epoch  9, batch    66 | loss: 68.3087610CurrentTrain: epoch  9, batch    67 | loss: 35.0722277CurrentTrain: epoch  9, batch    68 | loss: 106.6995971CurrentTrain: epoch  9, batch    69 | loss: 47.5144559CurrentTrain: epoch  9, batch    70 | loss: 39.5859361CurrentTrain: epoch  9, batch    71 | loss: 37.0452742CurrentTrain: epoch  9, batch    72 | loss: 28.2176680CurrentTrain: epoch  9, batch    73 | loss: 27.4000499CurrentTrain: epoch  9, batch    74 | loss: 33.4286226CurrentTrain: epoch  9, batch    75 | loss: 68.2283097CurrentTrain: epoch  9, batch    76 | loss: 68.0889696CurrentTrain: epoch  9, batch    77 | loss: 45.9370354CurrentTrain: epoch  9, batch    78 | loss: 44.9172374CurrentTrain: epoch  9, batch    79 | loss: 106.8367743CurrentTrain: epoch  9, batch    80 | loss: 48.5728420CurrentTrain: epoch  9, batch    81 | loss: 28.1967333CurrentTrain: epoch  9, batch    82 | loss: 48.2002455CurrentTrain: epoch  9, batch    83 | loss: 46.2746386CurrentTrain: epoch  9, batch    84 | loss: 64.6717935CurrentTrain: epoch  9, batch    85 | loss: 66.2410802CurrentTrain: epoch  9, batch    86 | loss: 68.9438479CurrentTrain: epoch  9, batch    87 | loss: 107.0393918CurrentTrain: epoch  9, batch    88 | loss: 68.1126775CurrentTrain: epoch  9, batch    89 | loss: 46.0376027CurrentTrain: epoch  9, batch    90 | loss: 37.8890952CurrentTrain: epoch  9, batch    91 | loss: 68.3911207CurrentTrain: epoch  9, batch    92 | loss: 66.1074783CurrentTrain: epoch  9, batch    93 | loss: 68.0789598CurrentTrain: epoch  9, batch    94 | loss: 108.5848214CurrentTrain: epoch  9, batch    95 | loss: 68.0507642CurrentTrain: epoch  9, batch    96 | loss: 47.6202924CurrentTrain: epoch  9, batch    97 | loss: 49.0186359CurrentTrain: epoch  9, batch    98 | loss: 64.4460796CurrentTrain: epoch  9, batch    99 | loss: 34.6915690CurrentTrain: epoch  9, batch   100 | loss: 64.1282424CurrentTrain: epoch  9, batch   101 | loss: 101.1494856CurrentTrain: epoch  9, batch   102 | loss: 46.8166649CurrentTrain: epoch  9, batch   103 | loss: 37.3468158CurrentTrain: epoch  9, batch   104 | loss: 27.9646309CurrentTrain: epoch  9, batch   105 | loss: 47.4562164CurrentTrain: epoch  9, batch   106 | loss: 68.4079040CurrentTrain: epoch  9, batch   107 | loss: 37.3960572CurrentTrain: epoch  9, batch   108 | loss: 48.8913286CurrentTrain: epoch  9, batch   109 | loss: 68.0907002CurrentTrain: epoch  9, batch   110 | loss: 47.5693530CurrentTrain: epoch  9, batch   111 | loss: 30.0212276CurrentTrain: epoch  9, batch   112 | loss: 103.4737334CurrentTrain: epoch  9, batch   113 | loss: 36.4872961CurrentTrain: epoch  9, batch   114 | loss: 28.5705942CurrentTrain: epoch  9, batch   115 | loss: 64.6056198CurrentTrain: epoch  9, batch   116 | loss: 68.1329437CurrentTrain: epoch  9, batch   117 | loss: 50.1043457CurrentTrain: epoch  9, batch   118 | loss: 36.4396302CurrentTrain: epoch  9, batch   119 | loss: 36.2568565CurrentTrain: epoch  9, batch   120 | loss: 68.1030557CurrentTrain: epoch  9, batch   121 | loss: 44.6166805CurrentTrain: epoch  9, batch   122 | loss: 29.7383459CurrentTrain: epoch  9, batch   123 | loss: 37.9800472CurrentTrain: epoch  9, batch   124 | loss: 49.7021034CurrentTrain: epoch  9, batch   125 | loss: 37.9701373CurrentTrain: epoch  9, batch   126 | loss: 47.4027442CurrentTrain: epoch  9, batch   127 | loss: 66.3414385CurrentTrain: epoch  9, batch   128 | loss: 35.2768172CurrentTrain: epoch  9, batch   129 | loss: 36.3245774CurrentTrain: epoch  9, batch   130 | loss: 23.6385249CurrentTrain: epoch  9, batch   131 | loss: 43.7796983CurrentTrain: epoch  9, batch   132 | loss: 36.1641588CurrentTrain: epoch  9, batch   133 | loss: 33.5026476CurrentTrain: epoch  9, batch   134 | loss: 37.4648401CurrentTrain: epoch  9, batch   135 | loss: 68.0790650CurrentTrain: epoch  9, batch   136 | loss: 28.1546623CurrentTrain: epoch  9, batch   137 | loss: 66.1547659CurrentTrain: epoch  9, batch   138 | loss: 30.2589679CurrentTrain: epoch  9, batch   139 | loss: 23.6342678CurrentTrain: epoch  9, batch   140 | loss: 47.4416496CurrentTrain: epoch  9, batch   141 | loss: 44.8212429CurrentTrain: epoch  9, batch   142 | loss: 66.2154342CurrentTrain: epoch  9, batch   143 | loss: 33.8069649

F1 score per class: {32: 0.48672566371681414, 6: 0.6551724137931034, 19: 0.13793103448275862, 24: 0.6948356807511737, 26: 0.8888888888888888, 29: 0.7894736842105263}
Micro-average F1 score: 0.6838365896980462
Weighted-average F1 score: 0.6842230398737823
F1 score per class: {32: 0.5, 6: 0.5985401459854015, 19: 0.1686746987951807, 24: 0.6862745098039216, 26: 0.8785046728971962, 29: 0.6973180076628352}
Micro-average F1 score: 0.6268656716417911
Weighted-average F1 score: 0.6093675447527388
F1 score per class: {32: 0.5, 6: 0.5963636363636363, 19: 0.18666666666666668, 24: 0.6796116504854369, 26: 0.8785046728971962, 29: 0.6893939393939394}
Micro-average F1 score: 0.6281859070464768
Weighted-average F1 score: 0.6130546692230135

F1 score per class: {32: 0.48672566371681414, 6: 0.6551724137931034, 19: 0.13793103448275862, 24: 0.6948356807511737, 26: 0.8888888888888888, 29: 0.7894736842105263}
Micro-average F1 score: 0.6838365896980462
Weighted-average F1 score: 0.6842230398737823
F1 score per class: {32: 0.5, 6: 0.5985401459854015, 19: 0.1686746987951807, 24: 0.6862745098039216, 26: 0.8785046728971962, 29: 0.6973180076628352}
Micro-average F1 score: 0.6268656716417911
Weighted-average F1 score: 0.6093675447527388
F1 score per class: {32: 0.5, 6: 0.5963636363636363, 19: 0.18666666666666668, 24: 0.6796116504854369, 26: 0.8785046728971962, 29: 0.6893939393939394}
Micro-average F1 score: 0.6281859070464768
Weighted-average F1 score: 0.6130546692230135
cur_acc:  ['0.6838']
his_acc:  ['0.6838']
cur_acc des:  ['0.6269']
his_acc des:  ['0.6269']
cur_acc rrf:  ['0.6282']
his_acc rrf:  ['0.6282']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 59.9140519CurrentTrain: epoch  0, batch     1 | loss: 55.6829749CurrentTrain: epoch  0, batch     2 | loss: 72.7824903CurrentTrain: epoch  0, batch     3 | loss: 50.9138203CurrentTrain: epoch  0, batch     4 | loss: 116.8781159CurrentTrain: epoch  0, batch     5 | loss: 76.7023203CurrentTrain: epoch  0, batch     6 | loss: 78.0558552CurrentTrain: epoch  0, batch     7 | loss: 1.8547333CurrentTrain: epoch  1, batch     0 | loss: 43.3941645CurrentTrain: epoch  1, batch     1 | loss: 52.1429564CurrentTrain: epoch  1, batch     2 | loss: 57.1341823CurrentTrain: epoch  1, batch     3 | loss: 112.2269823CurrentTrain: epoch  1, batch     4 | loss: 56.0840866CurrentTrain: epoch  1, batch     5 | loss: 73.4505387CurrentTrain: epoch  1, batch     6 | loss: 45.3716172CurrentTrain: epoch  1, batch     7 | loss: 2.8560537CurrentTrain: epoch  2, batch     0 | loss: 44.9203737CurrentTrain: epoch  2, batch     1 | loss: 43.1585205CurrentTrain: epoch  2, batch     2 | loss: 74.3578044CurrentTrain: epoch  2, batch     3 | loss: 53.7179260CurrentTrain: epoch  2, batch     4 | loss: 71.2700528CurrentTrain: epoch  2, batch     5 | loss: 53.8186703CurrentTrain: epoch  2, batch     6 | loss: 109.5065135CurrentTrain: epoch  2, batch     7 | loss: 2.8593187CurrentTrain: epoch  3, batch     0 | loss: 41.9068089CurrentTrain: epoch  3, batch     1 | loss: 34.0398485CurrentTrain: epoch  3, batch     2 | loss: 109.4312906CurrentTrain: epoch  3, batch     3 | loss: 73.6631505CurrentTrain: epoch  3, batch     4 | loss: 49.2700801CurrentTrain: epoch  3, batch     5 | loss: 110.5624017CurrentTrain: epoch  3, batch     6 | loss: 52.5193031CurrentTrain: epoch  3, batch     7 | loss: 2.8707875CurrentTrain: epoch  4, batch     0 | loss: 33.1041675CurrentTrain: epoch  4, batch     1 | loss: 42.2868540CurrentTrain: epoch  4, batch     2 | loss: 69.1660773CurrentTrain: epoch  4, batch     3 | loss: 222.2564770CurrentTrain: epoch  4, batch     4 | loss: 107.9484087CurrentTrain: epoch  4, batch     5 | loss: 69.5059579CurrentTrain: epoch  4, batch     6 | loss: 37.0972115CurrentTrain: epoch  4, batch     7 | loss: 0.6820344CurrentTrain: epoch  5, batch     0 | loss: 30.8666235CurrentTrain: epoch  5, batch     1 | loss: 106.8517924CurrentTrain: epoch  5, batch     2 | loss: 51.0881586CurrentTrain: epoch  5, batch     3 | loss: 222.4116522CurrentTrain: epoch  5, batch     4 | loss: 50.8319637CurrentTrain: epoch  5, batch     5 | loss: 39.3280684CurrentTrain: epoch  5, batch     6 | loss: 47.8568860CurrentTrain: epoch  5, batch     7 | loss: 2.8081366CurrentTrain: epoch  6, batch     0 | loss: 68.1536001CurrentTrain: epoch  6, batch     1 | loss: 50.5571375CurrentTrain: epoch  6, batch     2 | loss: 107.1935675CurrentTrain: epoch  6, batch     3 | loss: 47.8747577CurrentTrain: epoch  6, batch     4 | loss: 38.9206177CurrentTrain: epoch  6, batch     5 | loss: 36.1654113CurrentTrain: epoch  6, batch     6 | loss: 39.3219305CurrentTrain: epoch  6, batch     7 | loss: 2.8218693CurrentTrain: epoch  7, batch     0 | loss: 48.4253806CurrentTrain: epoch  7, batch     1 | loss: 65.2945295CurrentTrain: epoch  7, batch     2 | loss: 38.5445450CurrentTrain: epoch  7, batch     3 | loss: 66.4475765CurrentTrain: epoch  7, batch     4 | loss: 47.3834843CurrentTrain: epoch  7, batch     5 | loss: 107.0209013CurrentTrain: epoch  7, batch     6 | loss: 101.1107088CurrentTrain: epoch  7, batch     7 | loss: 2.7981459CurrentTrain: epoch  8, batch     0 | loss: 69.3992748CurrentTrain: epoch  8, batch     1 | loss: 67.0652450CurrentTrain: epoch  8, batch     2 | loss: 68.2820014CurrentTrain: epoch  8, batch     3 | loss: 49.1479713CurrentTrain: epoch  8, batch     4 | loss: 46.5683446CurrentTrain: epoch  8, batch     5 | loss: 37.6658461CurrentTrain: epoch  8, batch     6 | loss: 35.4208918CurrentTrain: epoch  8, batch     7 | loss: 2.8273556CurrentTrain: epoch  9, batch     0 | loss: 106.9885781CurrentTrain: epoch  9, batch     1 | loss: 49.1035544CurrentTrain: epoch  9, batch     2 | loss: 106.9948893CurrentTrain: epoch  9, batch     3 | loss: 36.6605776CurrentTrain: epoch  9, batch     4 | loss: 28.0453847CurrentTrain: epoch  9, batch     5 | loss: 47.6708609CurrentTrain: epoch  9, batch     6 | loss: 38.0888809CurrentTrain: epoch  9, batch     7 | loss: 2.7992425
MemoryTrain:  epoch  0, batch     0 | loss: 0.5948848MemoryTrain:  epoch  1, batch     0 | loss: 0.4206779MemoryTrain:  epoch  2, batch     0 | loss: 0.3190161MemoryTrain:  epoch  3, batch     0 | loss: 0.3098415MemoryTrain:  epoch  4, batch     0 | loss: 0.1803562MemoryTrain:  epoch  5, batch     0 | loss: 0.1517560MemoryTrain:  epoch  6, batch     0 | loss: 0.1221049MemoryTrain:  epoch  7, batch     0 | loss: 0.0883382MemoryTrain:  epoch  8, batch     0 | loss: 0.0695794MemoryTrain:  epoch  9, batch     0 | loss: 0.0553352

F1 score per class: {32: 0.9175257731958762, 5: 0.0, 6: 0.5145631067961165, 10: 0.5671641791044776, 16: 0.2222222222222222, 17: 0.26666666666666666, 18: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.549520766773163
Weighted-average F1 score: 0.49117005703142774
F1 score per class: {32: 0.7101449275362319, 5: 0.0, 6: 0.5462962962962963, 10: 0.4791666666666667, 16: 0.12903225806451613, 17: 0.3389830508474576, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.455467869222097
Weighted-average F1 score: 0.4096190088589372
F1 score per class: {32: 0.7211895910780669, 5: 0.0, 6: 0.5327510917030568, 10: 0.5476190476190477, 16: 0.12903225806451613, 17: 0.32432432432432434, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.47294117647058825
Weighted-average F1 score: 0.4303415853148704

F1 score per class: {32: 0.9081632653061225, 5: 0.38961038961038963, 6: 0.42231075697211157, 10: 0.5352112676056338, 16: 0.0967741935483871, 17: 0.24615384615384617, 18: 0.6457399103139013, 19: 0.12903225806451613, 24: 0.6893203883495146, 26: 0.8071748878923767, 29: 0.7468879668049793}
Micro-average F1 score: 0.6022222222222222
Weighted-average F1 score: 0.5911746170498036
F1 score per class: {32: 0.6782006920415224, 5: 0.38461538461538464, 6: 0.4169611307420495, 10: 0.39655172413793105, 16: 0.056338028169014086, 17: 0.27586206896551724, 18: 0.5830258302583026, 19: 0.16666666666666666, 24: 0.6570048309178744, 26: 0.746031746031746, 29: 0.6453900709219859}
Micro-average F1 score: 0.516595744680851
Weighted-average F1 score: 0.4995286252684642
F1 score per class: {32: 0.6953405017921147, 5: 0.40764331210191085, 6: 0.4053156146179402, 10: 0.46, 16: 0.057971014492753624, 17: 0.26666666666666666, 18: 0.5855513307984791, 19: 0.17543859649122806, 24: 0.6538461538461539, 26: 0.7642276422764228, 29: 0.6691449814126395}
Micro-average F1 score: 0.5345827755466309
Weighted-average F1 score: 0.5200737704087702
cur_acc:  ['0.6838', '0.5495']
his_acc:  ['0.6838', '0.6022']
cur_acc des:  ['0.6269', '0.4555']
his_acc des:  ['0.6269', '0.5166']
cur_acc rrf:  ['0.6282', '0.4729']
his_acc rrf:  ['0.6282', '0.5346']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 49.1126943CurrentTrain: epoch  0, batch     1 | loss: 75.4214568CurrentTrain: epoch  0, batch     2 | loss: 65.9764105CurrentTrain: epoch  0, batch     3 | loss: 51.7603661CurrentTrain: epoch  0, batch     4 | loss: 45.1407172CurrentTrain: epoch  0, batch     5 | loss: 64.9986589CurrentTrain: epoch  0, batch     6 | loss: 60.1016065CurrentTrain: epoch  1, batch     0 | loss: 106.3975090CurrentTrain: epoch  1, batch     1 | loss: 45.6536388CurrentTrain: epoch  1, batch     2 | loss: 57.7612744CurrentTrain: epoch  1, batch     3 | loss: 41.9330911CurrentTrain: epoch  1, batch     4 | loss: 54.8755962CurrentTrain: epoch  1, batch     5 | loss: 55.3787158CurrentTrain: epoch  1, batch     6 | loss: 59.9743993CurrentTrain: epoch  2, batch     0 | loss: 44.1584269CurrentTrain: epoch  2, batch     1 | loss: 40.9676687CurrentTrain: epoch  2, batch     2 | loss: 73.7220898CurrentTrain: epoch  2, batch     3 | loss: 66.2534905CurrentTrain: epoch  2, batch     4 | loss: 71.6999456CurrentTrain: epoch  2, batch     5 | loss: 75.0792856CurrentTrain: epoch  2, batch     6 | loss: 29.7184640CurrentTrain: epoch  3, batch     0 | loss: 53.1975602CurrentTrain: epoch  3, batch     1 | loss: 43.0264310CurrentTrain: epoch  3, batch     2 | loss: 41.4464069CurrentTrain: epoch  3, batch     3 | loss: 50.1321375CurrentTrain: epoch  3, batch     4 | loss: 50.4347871CurrentTrain: epoch  3, batch     5 | loss: 105.1167610CurrentTrain: epoch  3, batch     6 | loss: 28.4283369CurrentTrain: epoch  4, batch     0 | loss: 111.3699997CurrentTrain: epoch  4, batch     1 | loss: 40.0783288CurrentTrain: epoch  4, batch     2 | loss: 52.2768535CurrentTrain: epoch  4, batch     3 | loss: 50.7171521CurrentTrain: epoch  4, batch     4 | loss: 39.2594286CurrentTrain: epoch  4, batch     5 | loss: 66.6555736CurrentTrain: epoch  4, batch     6 | loss: 16.7143367CurrentTrain: epoch  5, batch     0 | loss: 67.3942979CurrentTrain: epoch  5, batch     1 | loss: 47.5564059CurrentTrain: epoch  5, batch     2 | loss: 69.0028242CurrentTrain: epoch  5, batch     3 | loss: 52.4475646CurrentTrain: epoch  5, batch     4 | loss: 50.3929941CurrentTrain: epoch  5, batch     5 | loss: 49.4107503CurrentTrain: epoch  5, batch     6 | loss: 27.8613054CurrentTrain: epoch  6, batch     0 | loss: 108.0591321CurrentTrain: epoch  6, batch     1 | loss: 38.1271945CurrentTrain: epoch  6, batch     2 | loss: 69.5972482CurrentTrain: epoch  6, batch     3 | loss: 31.3621866CurrentTrain: epoch  6, batch     4 | loss: 108.2232875CurrentTrain: epoch  6, batch     5 | loss: 30.0150659CurrentTrain: epoch  6, batch     6 | loss: 25.8351996CurrentTrain: epoch  7, batch     0 | loss: 67.4463763CurrentTrain: epoch  7, batch     1 | loss: 35.1851717CurrentTrain: epoch  7, batch     2 | loss: 69.3328054CurrentTrain: epoch  7, batch     3 | loss: 67.3293005CurrentTrain: epoch  7, batch     4 | loss: 46.9198014CurrentTrain: epoch  7, batch     5 | loss: 49.3824881CurrentTrain: epoch  7, batch     6 | loss: 59.8802376CurrentTrain: epoch  8, batch     0 | loss: 49.2078953CurrentTrain: epoch  8, batch     1 | loss: 67.0844120CurrentTrain: epoch  8, batch     2 | loss: 36.1077602CurrentTrain: epoch  8, batch     3 | loss: 48.5739291CurrentTrain: epoch  8, batch     4 | loss: 68.6614958CurrentTrain: epoch  8, batch     5 | loss: 37.2513615CurrentTrain: epoch  8, batch     6 | loss: 15.5615917CurrentTrain: epoch  9, batch     0 | loss: 66.8141979CurrentTrain: epoch  9, batch     1 | loss: 66.3424431CurrentTrain: epoch  9, batch     2 | loss: 107.0290024CurrentTrain: epoch  9, batch     3 | loss: 46.5218777CurrentTrain: epoch  9, batch     4 | loss: 32.4788624CurrentTrain: epoch  9, batch     5 | loss: 48.0226973CurrentTrain: epoch  9, batch     6 | loss: 26.7957945
MemoryTrain:  epoch  0, batch     0 | loss: 0.6176519MemoryTrain:  epoch  1, batch     0 | loss: 0.4437140MemoryTrain:  epoch  2, batch     0 | loss: 0.4436004MemoryTrain:  epoch  3, batch     0 | loss: 0.3167640MemoryTrain:  epoch  4, batch     0 | loss: 0.3041707MemoryTrain:  epoch  5, batch     0 | loss: 0.2105667MemoryTrain:  epoch  6, batch     0 | loss: 0.1574408MemoryTrain:  epoch  7, batch     0 | loss: 0.1151696MemoryTrain:  epoch  8, batch     0 | loss: 0.0841483MemoryTrain:  epoch  9, batch     0 | loss: 0.0712651

F1 score per class: {32: 0.42857142857142855, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.2857142857142857, 10: 0.38571428571428573, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.13636363636363635, 26: 0.0, 28: 0.0, 29: 0.21428571428571427}
Micro-average F1 score: 0.24347826086956523
Weighted-average F1 score: 0.16737619227245784
F1 score per class: {32: 0.22857142857142856, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.5443037974683544, 10: 0.4972972972972973, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.09345794392523364, 26: 0.0, 28: 0.0, 29: 0.23728813559322035}
Micro-average F1 score: 0.27455919395465994
Weighted-average F1 score: 0.19868642745427006
F1 score per class: {32: 0.3076923076923077, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.5508982035928144, 10: 0.5054945054945055, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.08196721311475409, 26: 0.0, 28: 0.0, 29: 0.18181818181818182}
Micro-average F1 score: 0.28835978835978837
Weighted-average F1 score: 0.21043032498096542

F1 score per class: {32: 0.35294117647058826, 2: 0.9154228855721394, 5: 0.3368421052631579, 6: 0.18181818181818182, 39: 0.17, 10: 0.2583732057416268, 11: 0.48717948717948717, 12: 0.11764705882352941, 16: 0.14432989690721648, 17: 0.5963302752293578, 18: 0.2222222222222222, 19: 0.6934673366834171, 24: 0.0625, 26: 0.8018433179723502, 28: 0.674074074074074, 29: 0.18181818181818182}
Micro-average F1 score: 0.46849894291754757
Weighted-average F1 score: 0.45561133306251206
F1 score per class: {32: 0.1032258064516129, 2: 0.5268817204301075, 5: 0.35260115606936415, 6: 0.34375, 39: 0.30935251798561153, 10: 0.22439024390243903, 11: 0.4380952380952381, 12: 0.1111111111111111, 16: 0.12749003984063745, 17: 0.5314685314685315, 18: 0.17475728155339806, 19: 0.6540284360189573, 24: 0.044444444444444446, 26: 0.75, 28: 0.5498489425981873, 29: 0.1686746987951807}
Micro-average F1 score: 0.3731506849315068
Weighted-average F1 score: 0.3472993637037668
F1 score per class: {32: 0.15841584158415842, 2: 0.5490196078431373, 5: 0.3614457831325301, 6: 0.3263157894736842, 39: 0.3026315789473684, 10: 0.21800947867298578, 11: 0.4423076923076923, 12: 0.14545454545454545, 16: 0.15458937198067632, 17: 0.5454545454545454, 18: 0.2077922077922078, 19: 0.6764705882352942, 24: 0.03875968992248062, 26: 0.775, 28: 0.5814696485623003, 29: 0.13793103448275862}
Micro-average F1 score: 0.3871890191592794
Weighted-average F1 score: 0.3608472762306447
cur_acc:  ['0.6838', '0.5495', '0.2435']
his_acc:  ['0.6838', '0.6022', '0.4685']
cur_acc des:  ['0.6269', '0.4555', '0.2746']
his_acc des:  ['0.6269', '0.5166', '0.3732']
cur_acc rrf:  ['0.6282', '0.4729', '0.2884']
his_acc rrf:  ['0.6282', '0.5346', '0.3872']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 58.4629333CurrentTrain: epoch  0, batch     1 | loss: 53.8421899CurrentTrain: epoch  0, batch     2 | loss: 64.0902319CurrentTrain: epoch  0, batch     3 | loss: 54.1984090CurrentTrain: epoch  0, batch     4 | loss: 82.8032717CurrentTrain: epoch  0, batch     5 | loss: 53.0310713CurrentTrain: epoch  1, batch     0 | loss: 59.0289286CurrentTrain: epoch  1, batch     1 | loss: 47.2122673CurrentTrain: epoch  1, batch     2 | loss: 105.5139103CurrentTrain: epoch  1, batch     3 | loss: 41.6773773CurrentTrain: epoch  1, batch     4 | loss: 42.8857486CurrentTrain: epoch  1, batch     5 | loss: 57.0136828CurrentTrain: epoch  2, batch     0 | loss: 222.7350326CurrentTrain: epoch  2, batch     1 | loss: 76.9853211CurrentTrain: epoch  2, batch     2 | loss: 50.1875618CurrentTrain: epoch  2, batch     3 | loss: 42.9822759CurrentTrain: epoch  2, batch     4 | loss: 52.1768185CurrentTrain: epoch  2, batch     5 | loss: 54.6704420CurrentTrain: epoch  3, batch     0 | loss: 49.0335001CurrentTrain: epoch  3, batch     1 | loss: 49.1047545CurrentTrain: epoch  3, batch     2 | loss: 69.7104010CurrentTrain: epoch  3, batch     3 | loss: 33.7842755CurrentTrain: epoch  3, batch     4 | loss: 72.6013393CurrentTrain: epoch  3, batch     5 | loss: 40.3899050CurrentTrain: epoch  4, batch     0 | loss: 44.4911422CurrentTrain: epoch  4, batch     1 | loss: 48.4501021CurrentTrain: epoch  4, batch     2 | loss: 71.8832012CurrentTrain: epoch  4, batch     3 | loss: 33.0216640CurrentTrain: epoch  4, batch     4 | loss: 49.7548586CurrentTrain: epoch  4, batch     5 | loss: 47.3331572CurrentTrain: epoch  5, batch     0 | loss: 41.3507809CurrentTrain: epoch  5, batch     1 | loss: 39.8497348CurrentTrain: epoch  5, batch     2 | loss: 39.4482714CurrentTrain: epoch  5, batch     3 | loss: 38.8759523CurrentTrain: epoch  5, batch     4 | loss: 49.4643927CurrentTrain: epoch  5, batch     5 | loss: 53.3711320CurrentTrain: epoch  6, batch     0 | loss: 110.2764222CurrentTrain: epoch  6, batch     1 | loss: 35.1548976CurrentTrain: epoch  6, batch     2 | loss: 37.9632269CurrentTrain: epoch  6, batch     3 | loss: 48.6306995CurrentTrain: epoch  6, batch     4 | loss: 46.1144226CurrentTrain: epoch  6, batch     5 | loss: 35.9091615CurrentTrain: epoch  7, batch     0 | loss: 49.6123093CurrentTrain: epoch  7, batch     1 | loss: 51.2989690CurrentTrain: epoch  7, batch     2 | loss: 48.9532043CurrentTrain: epoch  7, batch     3 | loss: 107.7433016CurrentTrain: epoch  7, batch     4 | loss: 37.4471287CurrentTrain: epoch  7, batch     5 | loss: 23.4223452CurrentTrain: epoch  8, batch     0 | loss: 49.4325095CurrentTrain: epoch  8, batch     1 | loss: 49.3938492CurrentTrain: epoch  8, batch     2 | loss: 47.0338292CurrentTrain: epoch  8, batch     3 | loss: 107.2297550CurrentTrain: epoch  8, batch     4 | loss: 30.0260350CurrentTrain: epoch  8, batch     5 | loss: 33.4443495CurrentTrain: epoch  9, batch     0 | loss: 49.0465002CurrentTrain: epoch  9, batch     1 | loss: 70.0853553CurrentTrain: epoch  9, batch     2 | loss: 36.3269319CurrentTrain: epoch  9, batch     3 | loss: 47.9816614CurrentTrain: epoch  9, batch     4 | loss: 49.2871321CurrentTrain: epoch  9, batch     5 | loss: 26.1169371
MemoryTrain:  epoch  0, batch     0 | loss: 0.4268871MemoryTrain:  epoch  1, batch     0 | loss: 0.3649921MemoryTrain:  epoch  2, batch     0 | loss: 0.3078934MemoryTrain:  epoch  3, batch     0 | loss: 0.2271067MemoryTrain:  epoch  4, batch     0 | loss: 0.1660914MemoryTrain:  epoch  5, batch     0 | loss: 0.1399783MemoryTrain:  epoch  6, batch     0 | loss: 0.1020483MemoryTrain:  epoch  7, batch     0 | loss: 0.0790490MemoryTrain:  epoch  8, batch     0 | loss: 0.0746623MemoryTrain:  epoch  9, batch     0 | loss: 0.0622822

F1 score per class: {0: 0.8571428571428571, 2: 0.0, 4: 0.8457142857142858, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.08333333333333333, 16: 0.0, 19: 0.0, 21: 0.32558139534883723, 23: 0.691358024691358, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.4862236628849271
Weighted-average F1 score: 0.3544351854692979
F1 score per class: {0: 0.6486486486486487, 2: 0.0, 4: 0.8449197860962567, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.05128205128205128, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.23357664233576642, 23: 0.6041666666666666, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3343717549325026
Weighted-average F1 score: 0.23905101196261372
F1 score per class: {0: 0.7446808510638298, 2: 0.0, 4: 0.8494623655913979, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.04878048780487805, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.23357664233576642, 23: 0.5777777777777777, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3435448577680525
Weighted-average F1 score: 0.24043261966067694

F1 score per class: {0: 0.49624060150375937, 2: 0.1694915254237288, 4: 0.8314606741573034, 5: 0.8952380952380953, 6: 0.30144927536231886, 10: 0.2638888888888889, 11: 0.1875, 12: 0.2100456621004566, 13: 0.013422818791946308, 16: 0.4528301886792453, 17: 0.09523809523809523, 18: 0.15789473684210525, 19: 0.5405405405405406, 21: 0.1590909090909091, 23: 0.6363636363636364, 24: 0.13333333333333333, 26: 0.6473429951690821, 28: 0.15789473684210525, 29: 0.7521367521367521, 32: 0.704, 39: 0.0784313725490196}
Micro-average F1 score: 0.44050947499223364
Weighted-average F1 score: 0.4063695929407017
F1 score per class: {0: 0.23920265780730898, 2: 0.08974358974358974, 4: 0.7783251231527094, 5: 0.5169712793733682, 6: 0.2945054945054945, 10: 0.3235294117647059, 11: 0.298932384341637, 12: 0.20045558086560364, 13: 0.0106951871657754, 16: 0.416, 17: 0.125, 18: 0.12949640287769784, 19: 0.4782608695652174, 21: 0.08767123287671233, 23: 0.5272727272727272, 24: 0.10810810810810811, 26: 0.6267281105990783, 28: 0.0365296803652968, 29: 0.6870229007633588, 32: 0.4972067039106145, 39: 0.06593406593406594}
Micro-average F1 score: 0.33104260750049086
Weighted-average F1 score: 0.30067991775287906
F1 score per class: {0: 0.2723735408560311, 2: 0.10687022900763359, 4: 0.8061224489795918, 5: 0.5552407932011332, 6: 0.29977628635346754, 10: 0.33064516129032256, 11: 0.26666666666666666, 12: 0.19953596287703015, 13: 0.009852216748768473, 16: 0.4132231404958678, 17: 0.125, 18: 0.125, 19: 0.49324324324324326, 21: 0.08695652173913043, 23: 0.5098039215686274, 24: 0.10810810810810811, 26: 0.6330275229357798, 28: 0.03686635944700461, 29: 0.7228915662650602, 32: 0.5545171339563862, 39: 0.05128205128205128}
Micro-average F1 score: 0.34134217067108535
Weighted-average F1 score: 0.30770026454093485
cur_acc:  ['0.6838', '0.5495', '0.2435', '0.4862']
his_acc:  ['0.6838', '0.6022', '0.4685', '0.4405']
cur_acc des:  ['0.6269', '0.4555', '0.2746', '0.3344']
his_acc des:  ['0.6269', '0.5166', '0.3732', '0.3310']
cur_acc rrf:  ['0.6282', '0.4729', '0.2884', '0.3435']
his_acc rrf:  ['0.6282', '0.5346', '0.3872', '0.3413']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 113.0820496CurrentTrain: epoch  0, batch     1 | loss: 40.9019061CurrentTrain: epoch  0, batch     2 | loss: 107.0848219CurrentTrain: epoch  0, batch     3 | loss: 55.8590069CurrentTrain: epoch  0, batch     4 | loss: 45.5663607CurrentTrain: epoch  0, batch     5 | loss: 63.0063462CurrentTrain: epoch  1, batch     0 | loss: 51.8559127CurrentTrain: epoch  1, batch     1 | loss: 46.6288334CurrentTrain: epoch  1, batch     2 | loss: 116.7204072CurrentTrain: epoch  1, batch     3 | loss: 44.5539936CurrentTrain: epoch  1, batch     4 | loss: 29.0704266CurrentTrain: epoch  1, batch     5 | loss: 36.3882213CurrentTrain: epoch  2, batch     0 | loss: 49.2636649CurrentTrain: epoch  2, batch     1 | loss: 38.2634862CurrentTrain: epoch  2, batch     2 | loss: 68.3583178CurrentTrain: epoch  2, batch     3 | loss: 39.5665801CurrentTrain: epoch  2, batch     4 | loss: 51.8120695CurrentTrain: epoch  2, batch     5 | loss: 57.3476438CurrentTrain: epoch  3, batch     0 | loss: 31.0306237CurrentTrain: epoch  3, batch     1 | loss: 107.9586992CurrentTrain: epoch  3, batch     2 | loss: 33.3683104CurrentTrain: epoch  3, batch     3 | loss: 110.1097028CurrentTrain: epoch  3, batch     4 | loss: 30.4384265CurrentTrain: epoch  3, batch     5 | loss: 25.9888676CurrentTrain: epoch  4, batch     0 | loss: 26.5498746CurrentTrain: epoch  4, batch     1 | loss: 69.3966613CurrentTrain: epoch  4, batch     2 | loss: 36.9859822CurrentTrain: epoch  4, batch     3 | loss: 66.8120386CurrentTrain: epoch  4, batch     4 | loss: 107.7222174CurrentTrain: epoch  4, batch     5 | loss: 39.4907991CurrentTrain: epoch  5, batch     0 | loss: 68.5544706CurrentTrain: epoch  5, batch     1 | loss: 35.9368343CurrentTrain: epoch  5, batch     2 | loss: 50.3325403CurrentTrain: epoch  5, batch     3 | loss: 48.2065927CurrentTrain: epoch  5, batch     4 | loss: 46.6348418CurrentTrain: epoch  5, batch     5 | loss: 34.9372597CurrentTrain: epoch  6, batch     0 | loss: 66.7407147CurrentTrain: epoch  6, batch     1 | loss: 35.6198363CurrentTrain: epoch  6, batch     2 | loss: 49.4337180CurrentTrain: epoch  6, batch     3 | loss: 48.0148692CurrentTrain: epoch  6, batch     4 | loss: 48.9924161CurrentTrain: epoch  6, batch     5 | loss: 35.2023997CurrentTrain: epoch  7, batch     0 | loss: 69.4150037CurrentTrain: epoch  7, batch     1 | loss: 35.8074041CurrentTrain: epoch  7, batch     2 | loss: 49.0733606CurrentTrain: epoch  7, batch     3 | loss: 36.6815215CurrentTrain: epoch  7, batch     4 | loss: 64.9147716CurrentTrain: epoch  7, batch     5 | loss: 23.3602481CurrentTrain: epoch  8, batch     0 | loss: 37.1083408CurrentTrain: epoch  8, batch     1 | loss: 36.3760225CurrentTrain: epoch  8, batch     2 | loss: 46.3984780CurrentTrain: epoch  8, batch     3 | loss: 48.2391585CurrentTrain: epoch  8, batch     4 | loss: 47.6935228CurrentTrain: epoch  8, batch     5 | loss: 56.5518689CurrentTrain: epoch  9, batch     0 | loss: 45.1299246CurrentTrain: epoch  9, batch     1 | loss: 28.4181574CurrentTrain: epoch  9, batch     2 | loss: 36.6767554CurrentTrain: epoch  9, batch     3 | loss: 103.5007255CurrentTrain: epoch  9, batch     4 | loss: 66.4310660CurrentTrain: epoch  9, batch     5 | loss: 34.8960697
MemoryTrain:  epoch  0, batch     0 | loss: 0.3034671MemoryTrain:  epoch  1, batch     0 | loss: 0.2584284MemoryTrain:  epoch  2, batch     0 | loss: 0.2192842MemoryTrain:  epoch  3, batch     0 | loss: 0.1603116MemoryTrain:  epoch  4, batch     0 | loss: 0.1320951MemoryTrain:  epoch  5, batch     0 | loss: 0.1398924MemoryTrain:  epoch  6, batch     0 | loss: 0.0939584MemoryTrain:  epoch  7, batch     0 | loss: 0.0833320MemoryTrain:  epoch  8, batch     0 | loss: 0.0662395MemoryTrain:  epoch  9, batch     0 | loss: 0.0629686

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.8, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.47058823529411764, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.35294117647058826, 37: 0.4772727272727273, 38: 0.24242424242424243, 39: 0.0}
Micro-average F1 score: 0.2894168466522678
Weighted-average F1 score: 0.18360751656665442
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.7058823529411765, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.4675324675324675, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.5611510791366906, 37: 0.4409448818897638, 38: 0.47761194029850745, 39: 0.0}
Micro-average F1 score: 0.27261146496815286
Weighted-average F1 score: 0.19551533541298746
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.7058823529411765, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.47368421052631576, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.5571428571428572, 37: 0.42424242424242425, 38: 0.45901639344262296, 39: 0.0}
Micro-average F1 score: 0.2734375
Weighted-average F1 score: 0.1968047748465524

F1 score per class: {0: 0.616822429906542, 2: 0.19672131147540983, 4: 0.7738095238095238, 5: 0.7836734693877551, 6: 0.3474903474903475, 10: 0.27979274611398963, 11: 0.11510791366906475, 12: 0.15286624203821655, 13: 0.015503875968992248, 15: 0.64, 16: 0.42201834862385323, 17: 0.041666666666666664, 18: 0.14678899082568808, 19: 0.4077669902912621, 21: 0.10526315789473684, 23: 0.6976744186046512, 24: 0.16, 25: 0.47058823529411764, 26: 0.6448598130841121, 28: 0.2962962962962963, 29: 0.7242798353909465, 32: 0.7123287671232876, 35: 0.19672131147540983, 37: 0.22340425531914893, 38: 0.10256410256410256, 39: 0.07407407407407407}
Micro-average F1 score: 0.41725352112676056
Weighted-average F1 score: 0.39844268370172226
F1 score per class: {0: 0.259927797833935, 2: 0.09722222222222222, 4: 0.8020833333333334, 5: 0.4424778761061947, 6: 0.2898550724637681, 10: 0.2788844621513944, 11: 0.18652849740932642, 12: 0.2, 13: 0.010526315789473684, 15: 0.36363636363636365, 16: 0.38235294117647056, 17: 0.07142857142857142, 18: 0.08670520231213873, 19: 0.3806646525679758, 21: 0.08695652173913043, 23: 0.5614035087719298, 24: 0.12121212121212122, 25: 0.4675324675324675, 26: 0.6052631578947368, 28: 0.0427807486631016, 29: 0.6546762589928058, 32: 0.5695364238410596, 35: 0.21138211382113822, 37: 0.10163339382940109, 38: 0.13008130081300814, 39: 0.10714285714285714}
Micro-average F1 score: 0.2859897172236504
Weighted-average F1 score: 0.25757836523331396
F1 score per class: {0: 0.26277372262773724, 2: 0.13861386138613863, 4: 0.819672131147541, 5: 0.45146726862302483, 6: 0.2961165048543689, 10: 0.26459143968871596, 11: 0.18461538461538463, 12: 0.1937046004842615, 13: 0.010050251256281407, 15: 0.375, 16: 0.3851851851851852, 17: 0.07272727272727272, 18: 0.08118081180811808, 19: 0.39622641509433965, 21: 0.09014084507042254, 23: 0.5333333333333333, 24: 0.125, 25: 0.4675324675324675, 26: 0.6133333333333333, 28: 0.04597701149425287, 29: 0.6973180076628352, 32: 0.5791245791245792, 35: 0.20855614973262032, 37: 0.09120521172638436, 38: 0.1222707423580786, 39: 0.11538461538461539}
Micro-average F1 score: 0.28867335196449123
Weighted-average F1 score: 0.2586956430094025
cur_acc:  ['0.6838', '0.5495', '0.2435', '0.4862', '0.2894']
his_acc:  ['0.6838', '0.6022', '0.4685', '0.4405', '0.4173']
cur_acc des:  ['0.6269', '0.4555', '0.2746', '0.3344', '0.2726']
his_acc des:  ['0.6269', '0.5166', '0.3732', '0.3310', '0.2860']
cur_acc rrf:  ['0.6282', '0.4729', '0.2884', '0.3435', '0.2734']
his_acc rrf:  ['0.6282', '0.5346', '0.3872', '0.3413', '0.2887']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 45.0208775CurrentTrain: epoch  0, batch     1 | loss: 58.2358649CurrentTrain: epoch  0, batch     2 | loss: 61.9992436CurrentTrain: epoch  0, batch     3 | loss: 52.7470587CurrentTrain: epoch  0, batch     4 | loss: 51.8923072CurrentTrain: epoch  0, batch     5 | loss: 33.6578312CurrentTrain: epoch  1, batch     0 | loss: 74.8166850CurrentTrain: epoch  1, batch     1 | loss: 52.6453899CurrentTrain: epoch  1, batch     2 | loss: 70.8356379CurrentTrain: epoch  1, batch     3 | loss: 41.1033305CurrentTrain: epoch  1, batch     4 | loss: 68.9988184CurrentTrain: epoch  1, batch     5 | loss: 23.3281812CurrentTrain: epoch  2, batch     0 | loss: 69.9310684CurrentTrain: epoch  2, batch     1 | loss: 54.3182131CurrentTrain: epoch  2, batch     2 | loss: 49.0855665CurrentTrain: epoch  2, batch     3 | loss: 38.3493399CurrentTrain: epoch  2, batch     4 | loss: 49.1607203CurrentTrain: epoch  2, batch     5 | loss: 30.3330356CurrentTrain: epoch  3, batch     0 | loss: 32.6187271CurrentTrain: epoch  3, batch     1 | loss: 68.8877481CurrentTrain: epoch  3, batch     2 | loss: 39.5563648CurrentTrain: epoch  3, batch     3 | loss: 36.9727386CurrentTrain: epoch  3, batch     4 | loss: 68.1926808CurrentTrain: epoch  3, batch     5 | loss: 28.5360743CurrentTrain: epoch  4, batch     0 | loss: 50.6108711CurrentTrain: epoch  4, batch     1 | loss: 48.6567331CurrentTrain: epoch  4, batch     2 | loss: 70.1968735CurrentTrain: epoch  4, batch     3 | loss: 46.9240892CurrentTrain: epoch  4, batch     4 | loss: 46.0839276CurrentTrain: epoch  4, batch     5 | loss: 28.4775500CurrentTrain: epoch  5, batch     0 | loss: 39.6116697CurrentTrain: epoch  5, batch     1 | loss: 69.0338905CurrentTrain: epoch  5, batch     2 | loss: 67.2469401CurrentTrain: epoch  5, batch     3 | loss: 36.4744237CurrentTrain: epoch  5, batch     4 | loss: 34.7608385CurrentTrain: epoch  5, batch     5 | loss: 42.7949386CurrentTrain: epoch  6, batch     0 | loss: 49.4068066CurrentTrain: epoch  6, batch     1 | loss: 46.8538893CurrentTrain: epoch  6, batch     2 | loss: 46.8352409CurrentTrain: epoch  6, batch     3 | loss: 49.4681891CurrentTrain: epoch  6, batch     4 | loss: 48.2056146CurrentTrain: epoch  6, batch     5 | loss: 17.9334123CurrentTrain: epoch  7, batch     0 | loss: 48.4179297CurrentTrain: epoch  7, batch     1 | loss: 39.0576801CurrentTrain: epoch  7, batch     2 | loss: 36.3738914CurrentTrain: epoch  7, batch     3 | loss: 48.1011104CurrentTrain: epoch  7, batch     4 | loss: 45.6445824CurrentTrain: epoch  7, batch     5 | loss: 45.2896803CurrentTrain: epoch  8, batch     0 | loss: 28.8400230CurrentTrain: epoch  8, batch     1 | loss: 68.3884504CurrentTrain: epoch  8, batch     2 | loss: 66.5004757CurrentTrain: epoch  8, batch     3 | loss: 36.8888556CurrentTrain: epoch  8, batch     4 | loss: 35.4439254CurrentTrain: epoch  8, batch     5 | loss: 27.3247890CurrentTrain: epoch  9, batch     0 | loss: 38.4506597CurrentTrain: epoch  9, batch     1 | loss: 36.6991949CurrentTrain: epoch  9, batch     2 | loss: 47.6417758CurrentTrain: epoch  9, batch     3 | loss: 36.6393560CurrentTrain: epoch  9, batch     4 | loss: 46.2332762CurrentTrain: epoch  9, batch     5 | loss: 27.5869886
MemoryTrain:  epoch  0, batch     0 | loss: 0.2200167MemoryTrain:  epoch  1, batch     0 | loss: 0.2297336MemoryTrain:  epoch  2, batch     0 | loss: 0.1633726MemoryTrain:  epoch  3, batch     0 | loss: 0.1244825MemoryTrain:  epoch  4, batch     0 | loss: 0.1177581MemoryTrain:  epoch  5, batch     0 | loss: 0.0920903MemoryTrain:  epoch  6, batch     0 | loss: 0.0847557MemoryTrain:  epoch  7, batch     0 | loss: 0.0797609MemoryTrain:  epoch  8, batch     0 | loss: 0.0646467MemoryTrain:  epoch  9, batch     0 | loss: 0.0563217

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.432, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.5979381443298969, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9142857142857143, 32: 0.0, 33: 0.3076923076923077, 35: 0.0, 36: 0.3595505617977528}
Micro-average F1 score: 0.375
Weighted-average F1 score: 0.27402794847987916
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5490196078431373, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.656934306569343, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9, 32: 0.0, 33: 0.2857142857142857, 35: 0.0, 36: 0.4639175257731959, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.3297413793103448
Weighted-average F1 score: 0.25135372362420844
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5714285714285714, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6474820143884892, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9230769230769231, 32: 0.0, 33: 0.3, 35: 0.0, 36: 0.47058823529411764, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.3433667781493868
Weighted-average F1 score: 0.26254199980395493

F1 score per class: {0: 0.5871559633027523, 2: 0.16129032258064516, 4: 0.8379888268156425, 5: 0.6906474820143885, 6: 0.32592592592592595, 8: 0.25471698113207547, 10: 0.16666666666666666, 11: 0.1625, 12: 0.06837606837606838, 13: 0.021739130434782608, 15: 0.5833333333333334, 16: 0.3793103448275862, 17: 0.07272727272727272, 18: 0.15384615384615385, 19: 0.41081081081081083, 20: 0.3352601156069364, 21: 0.14634146341463414, 23: 0.5647058823529412, 24: 0.08333333333333333, 25: 0.43478260869565216, 26: 0.6098654708520179, 28: 0.17391304347826086, 29: 0.7272727272727273, 30: 0.9142857142857143, 32: 0.6582278481012658, 33: 0.2222222222222222, 35: 0.23333333333333334, 36: 0.2689075630252101, 37: 0.22857142857142856, 38: 0.12, 39: 0.125}
Micro-average F1 score: 0.4070324986680874
Weighted-average F1 score: 0.408652608833459
F1 score per class: {0: 0.23333333333333334, 2: 0.1414141414141414, 4: 0.8465608465608465, 5: 0.32, 6: 0.2800982800982801, 8: 0.20689655172413793, 10: 0.1696969696969697, 11: 0.21714285714285714, 12: 0.1942257217847769, 13: 0.015267175572519083, 15: 0.2857142857142857, 16: 0.3225806451612903, 17: 0.07692307692307693, 18: 0.14606741573033707, 19: 0.40148698884758366, 20: 0.22842639593908629, 21: 0.09584664536741214, 23: 0.5081967213114754, 24: 0.12121212121212122, 25: 0.45, 26: 0.6079295154185022, 28: 0.05405405405405406, 29: 0.6691449814126395, 30: 0.5806451612903226, 32: 0.5358255451713395, 33: 0.12244897959183673, 35: 0.23780487804878048, 36: 0.1325478645066274, 37: 0.125, 38: 0.11067193675889328, 39: 0.1}
Micro-average F1 score: 0.280520967511092
Weighted-average F1 score: 0.2597824455847903
F1 score per class: {0: 0.2491103202846975, 2: 0.15730337078651685, 4: 0.8540540540540541, 5: 0.3454231433506045, 6: 0.2711864406779661, 8: 0.21153846153846154, 10: 0.2073170731707317, 11: 0.20408163265306123, 12: 0.19220779220779222, 13: 0.014925373134328358, 15: 0.3076923076923077, 16: 0.33112582781456956, 17: 0.07017543859649122, 18: 0.14606741573033707, 19: 0.40601503759398494, 20: 0.22670025188916876, 21: 0.0970873786407767, 23: 0.5344827586206896, 24: 0.125, 25: 0.46153846153846156, 26: 0.6052631578947368, 28: 0.0594059405940594, 29: 0.6867924528301886, 30: 0.6923076923076923, 32: 0.5548387096774193, 33: 0.11538461538461539, 35: 0.24169184290030213, 36: 0.13793103448275862, 37: 0.12727272727272726, 38: 0.1111111111111111, 39: 0.10810810810810811}
Micro-average F1 score: 0.28861131520940486
Weighted-average F1 score: 0.26685426994834344
cur_acc:  ['0.6838', '0.5495', '0.2435', '0.4862', '0.2894', '0.3750']
his_acc:  ['0.6838', '0.6022', '0.4685', '0.4405', '0.4173', '0.4070']
cur_acc des:  ['0.6269', '0.4555', '0.2746', '0.3344', '0.2726', '0.3297']
his_acc des:  ['0.6269', '0.5166', '0.3732', '0.3310', '0.2860', '0.2805']
cur_acc rrf:  ['0.6282', '0.4729', '0.2884', '0.3435', '0.2734', '0.3434']
his_acc rrf:  ['0.6282', '0.5346', '0.3872', '0.3413', '0.2887', '0.2886']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 44.0116877CurrentTrain: epoch  0, batch     1 | loss: 56.0663160CurrentTrain: epoch  0, batch     2 | loss: 44.2799849CurrentTrain: epoch  0, batch     3 | loss: 61.6154030CurrentTrain: epoch  0, batch     4 | loss: 34.5135268CurrentTrain: epoch  1, batch     0 | loss: 53.1033826CurrentTrain: epoch  1, batch     1 | loss: 40.9162184CurrentTrain: epoch  1, batch     2 | loss: 76.2843105CurrentTrain: epoch  1, batch     3 | loss: 44.4252767CurrentTrain: epoch  1, batch     4 | loss: 26.1932369CurrentTrain: epoch  2, batch     0 | loss: 53.8407822CurrentTrain: epoch  2, batch     1 | loss: 50.3270540CurrentTrain: epoch  2, batch     2 | loss: 63.8833115CurrentTrain: epoch  2, batch     3 | loss: 42.2373237CurrentTrain: epoch  2, batch     4 | loss: 23.6836740CurrentTrain: epoch  3, batch     0 | loss: 38.5338525CurrentTrain: epoch  3, batch     1 | loss: 31.9343185CurrentTrain: epoch  3, batch     2 | loss: 50.7789492CurrentTrain: epoch  3, batch     3 | loss: 31.2489942CurrentTrain: epoch  3, batch     4 | loss: 30.8549189CurrentTrain: epoch  4, batch     0 | loss: 47.4291148CurrentTrain: epoch  4, batch     1 | loss: 51.6365317CurrentTrain: epoch  4, batch     2 | loss: 29.8827270CurrentTrain: epoch  4, batch     3 | loss: 35.9117714CurrentTrain: epoch  4, batch     4 | loss: 29.2990984CurrentTrain: epoch  5, batch     0 | loss: 36.0202170CurrentTrain: epoch  5, batch     1 | loss: 67.5575472CurrentTrain: epoch  5, batch     2 | loss: 36.4100215CurrentTrain: epoch  5, batch     3 | loss: 38.1853585CurrentTrain: epoch  5, batch     4 | loss: 29.2690188CurrentTrain: epoch  6, batch     0 | loss: 47.4542175CurrentTrain: epoch  6, batch     1 | loss: 66.3247785CurrentTrain: epoch  6, batch     2 | loss: 63.9952563CurrentTrain: epoch  6, batch     3 | loss: 46.7096619CurrentTrain: epoch  6, batch     4 | loss: 20.0238028CurrentTrain: epoch  7, batch     0 | loss: 46.3307458CurrentTrain: epoch  7, batch     1 | loss: 27.1606242CurrentTrain: epoch  7, batch     2 | loss: 68.2617213CurrentTrain: epoch  7, batch     3 | loss: 45.7229287CurrentTrain: epoch  7, batch     4 | loss: 65.5900501CurrentTrain: epoch  8, batch     0 | loss: 27.8366255CurrentTrain: epoch  8, batch     1 | loss: 29.3665142CurrentTrain: epoch  8, batch     2 | loss: 37.7023319CurrentTrain: epoch  8, batch     3 | loss: 49.8229592CurrentTrain: epoch  8, batch     4 | loss: 22.6471910CurrentTrain: epoch  9, batch     0 | loss: 34.8007400CurrentTrain: epoch  9, batch     1 | loss: 47.5071466CurrentTrain: epoch  9, batch     2 | loss: 27.8812585CurrentTrain: epoch  9, batch     3 | loss: 47.5285757CurrentTrain: epoch  9, batch     4 | loss: 68.5508357
MemoryTrain:  epoch  0, batch     0 | loss: 0.3872615MemoryTrain:  epoch  0, batch     1 | loss: 0.0630725MemoryTrain:  epoch  1, batch     0 | loss: 0.1985083MemoryTrain:  epoch  1, batch     1 | loss: 0.0606520MemoryTrain:  epoch  2, batch     0 | loss: 0.1406292MemoryTrain:  epoch  2, batch     1 | loss: 0.0502813MemoryTrain:  epoch  3, batch     0 | loss: 0.1191898MemoryTrain:  epoch  3, batch     1 | loss: 0.0808018MemoryTrain:  epoch  4, batch     0 | loss: 0.1638884MemoryTrain:  epoch  4, batch     1 | loss: 0.0503074MemoryTrain:  epoch  5, batch     0 | loss: 0.0964951MemoryTrain:  epoch  5, batch     1 | loss: 0.0488977MemoryTrain:  epoch  6, batch     0 | loss: 0.0665502MemoryTrain:  epoch  6, batch     1 | loss: 0.0660308MemoryTrain:  epoch  7, batch     0 | loss: 0.0597703MemoryTrain:  epoch  7, batch     1 | loss: 0.0664891MemoryTrain:  epoch  8, batch     0 | loss: 0.0617390MemoryTrain:  epoch  8, batch     1 | loss: 0.0317590MemoryTrain:  epoch  9, batch     0 | loss: 0.0568592MemoryTrain:  epoch  9, batch     1 | loss: 0.0558677

F1 score per class: {0: 0.0, 6: 0.0, 7: 0.5714285714285714, 8: 0.0, 9: 0.9803921568627451, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 26: 0.0, 27: 0.37037037037037035, 31: 0.0, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.3564356435643564}
Micro-average F1 score: 0.33557046979865773
Weighted-average F1 score: 0.23152294830018869
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.4, 8: 0.0, 9: 0.8771929824561403, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.3829787234042553, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.18181818181818182, 32: 0.0, 33: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.4277456647398844}
Micro-average F1 score: 0.28957528957528955
Weighted-average F1 score: 0.2246631620939023
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.4444444444444444, 8: 0.0, 9: 0.9259259259259259, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.3829787234042553, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.2, 32: 0.0, 33: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.4327485380116959}
Micro-average F1 score: 0.3067484662576687
Weighted-average F1 score: 0.23706154590574147

F1 score per class: {0: 0.34196891191709844, 2: 0.18181818181818182, 4: 0.7953216374269005, 5: 0.735632183908046, 6: 0.24598930481283424, 7: 0.0425531914893617, 8: 0.2871287128712871, 9: 0.9433962264150944, 10: 0.13333333333333333, 11: 0.1910828025477707, 12: 0.034482758620689655, 13: 0.019230769230769232, 15: 0.6363636363636364, 16: 0.36363636363636365, 17: 0.05128205128205128, 18: 0.15126050420168066, 19: 0.35833333333333334, 20: 0.2727272727272727, 21: 0.1702127659574468, 23: 0.5581395348837209, 24: 0.08695652173913043, 25: 0.4411764705882353, 26: 0.6098654708520179, 27: 0.14084507042253522, 28: 0.125, 29: 0.7522123893805309, 30: 0.8823529411764706, 31: 0.0, 32: 0.6915887850467289, 33: 0.11764705882352941, 35: 0.30656934306569344, 36: 0.2711864406779661, 37: 0.1643835616438356, 38: 0.21739130434782608, 39: 0.13333333333333333, 40: 0.2553191489361702}
Micro-average F1 score: 0.3845962732919255
Weighted-average F1 score: 0.37820557345961076
F1 score per class: {0: 0.07472527472527472, 2: 0.14, 4: 0.8216216216216217, 5: 0.3257328990228013, 6: 0.24285714285714285, 7: 0.02857142857142857, 8: 0.24060150375939848, 9: 0.7936507936507936, 10: 0.14388489208633093, 11: 0.2233502538071066, 12: 0.2, 13: 0.010810810810810811, 15: 0.2727272727272727, 16: 0.3125, 17: 0.17142857142857143, 18: 0.13658536585365855, 19: 0.3106060606060606, 20: 0.2113022113022113, 21: 0.11214953271028037, 23: 0.5081967213114754, 24: 0.11428571428571428, 25: 0.525, 26: 0.5903083700440529, 27: 0.11180124223602485, 28: 0.0335195530726257, 29: 0.6875, 30: 0.6792452830188679, 31: 0.05128205128205128, 32: 0.5664335664335665, 33: 0.09836065573770492, 35: 0.28, 36: 0.1554054054054054, 37: 0.13861386138613863, 38: 0.10471204188481675, 39: 0.06779661016949153, 40: 0.21022727272727273}
Micro-average F1 score: 0.25176259453916167
Weighted-average F1 score: 0.2254127041746245
F1 score per class: {0: 0.08553459119496855, 2: 0.15217391304347827, 4: 0.8287292817679558, 5: 0.35398230088495575, 6: 0.26666666666666666, 7: 0.02531645569620253, 8: 0.24836601307189543, 9: 0.8620689655172413, 10: 0.11764705882352941, 11: 0.22009569377990432, 12: 0.18604651162790697, 13: 0.010050251256281407, 15: 0.2608695652173913, 16: 0.31446540880503143, 17: 0.16216216216216217, 18: 0.1368421052631579, 19: 0.3333333333333333, 20: 0.21204819277108433, 21: 0.12371134020618557, 23: 0.5585585585585585, 24: 0.06451612903225806, 25: 0.5128205128205128, 26: 0.5982142857142857, 27: 0.10975609756097561, 28: 0.041666666666666664, 29: 0.7068273092369478, 30: 0.7659574468085106, 31: 0.058823529411764705, 32: 0.5806451612903226, 33: 0.09836065573770492, 35: 0.29152542372881357, 36: 0.15889464594127806, 37: 0.15384615384615385, 38: 0.1232876712328767, 39: 0.1, 40: 0.20903954802259886}
Micro-average F1 score: 0.26437860302989674
Weighted-average F1 score: 0.23751466652465517
cur_acc:  ['0.6838', '0.5495', '0.2435', '0.4862', '0.2894', '0.3750', '0.3356']
his_acc:  ['0.6838', '0.6022', '0.4685', '0.4405', '0.4173', '0.4070', '0.3846']
cur_acc des:  ['0.6269', '0.4555', '0.2746', '0.3344', '0.2726', '0.3297', '0.2896']
his_acc des:  ['0.6269', '0.5166', '0.3732', '0.3310', '0.2860', '0.2805', '0.2518']
cur_acc rrf:  ['0.6282', '0.4729', '0.2884', '0.3435', '0.2734', '0.3434', '0.3067']
his_acc rrf:  ['0.6282', '0.5346', '0.3872', '0.3413', '0.2887', '0.2886', '0.2644']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 118.0920733CurrentTrain: epoch  0, batch     1 | loss: 37.3462307CurrentTrain: epoch  0, batch     2 | loss: 58.3070636CurrentTrain: epoch  0, batch     3 | loss: 66.8543440CurrentTrain: epoch  0, batch     4 | loss: 58.1813545CurrentTrain: epoch  0, batch     5 | loss: 48.8214481CurrentTrain: epoch  0, batch     6 | loss: 117.0111247CurrentTrain: epoch  1, batch     0 | loss: 47.2325646CurrentTrain: epoch  1, batch     1 | loss: 56.6193743CurrentTrain: epoch  1, batch     2 | loss: 73.0012325CurrentTrain: epoch  1, batch     3 | loss: 110.0079434CurrentTrain: epoch  1, batch     4 | loss: 56.6986314CurrentTrain: epoch  1, batch     5 | loss: 70.2533342CurrentTrain: epoch  1, batch     6 | loss: 50.8558413CurrentTrain: epoch  2, batch     0 | loss: 54.0410431CurrentTrain: epoch  2, batch     1 | loss: 43.7463057CurrentTrain: epoch  2, batch     2 | loss: 51.2306835CurrentTrain: epoch  2, batch     3 | loss: 40.8761236CurrentTrain: epoch  2, batch     4 | loss: 70.3474335CurrentTrain: epoch  2, batch     5 | loss: 54.8357231CurrentTrain: epoch  2, batch     6 | loss: 214.4918919CurrentTrain: epoch  3, batch     0 | loss: 52.6963119CurrentTrain: epoch  3, batch     1 | loss: 51.2109021CurrentTrain: epoch  3, batch     2 | loss: 48.4424792CurrentTrain: epoch  3, batch     3 | loss: 109.6427049CurrentTrain: epoch  3, batch     4 | loss: 48.9914304CurrentTrain: epoch  3, batch     5 | loss: 69.0447498CurrentTrain: epoch  3, batch     6 | loss: 40.0648892CurrentTrain: epoch  4, batch     0 | loss: 50.9644212CurrentTrain: epoch  4, batch     1 | loss: 50.7102250CurrentTrain: epoch  4, batch     2 | loss: 48.1939207CurrentTrain: epoch  4, batch     3 | loss: 51.1022429CurrentTrain: epoch  4, batch     4 | loss: 50.2001778CurrentTrain: epoch  4, batch     5 | loss: 49.4089924CurrentTrain: epoch  4, batch     6 | loss: 47.7846588CurrentTrain: epoch  5, batch     0 | loss: 108.1797842CurrentTrain: epoch  5, batch     1 | loss: 51.4604464CurrentTrain: epoch  5, batch     2 | loss: 68.6936961CurrentTrain: epoch  5, batch     3 | loss: 65.9259152CurrentTrain: epoch  5, batch     4 | loss: 46.1242376CurrentTrain: epoch  5, batch     5 | loss: 37.7601189CurrentTrain: epoch  5, batch     6 | loss: 106.0735168CurrentTrain: epoch  6, batch     0 | loss: 37.8762470CurrentTrain: epoch  6, batch     1 | loss: 37.8290408CurrentTrain: epoch  6, batch     2 | loss: 65.1813473CurrentTrain: epoch  6, batch     3 | loss: 67.1208941CurrentTrain: epoch  6, batch     4 | loss: 68.4530165CurrentTrain: epoch  6, batch     5 | loss: 49.6002321CurrentTrain: epoch  6, batch     6 | loss: 65.9442076CurrentTrain: epoch  7, batch     0 | loss: 49.3807407CurrentTrain: epoch  7, batch     1 | loss: 49.7233985CurrentTrain: epoch  7, batch     2 | loss: 36.7604667CurrentTrain: epoch  7, batch     3 | loss: 50.5088343CurrentTrain: epoch  7, batch     4 | loss: 49.9406661CurrentTrain: epoch  7, batch     5 | loss: 68.4158607CurrentTrain: epoch  7, batch     6 | loss: 44.7304133CurrentTrain: epoch  8, batch     0 | loss: 36.0728653CurrentTrain: epoch  8, batch     1 | loss: 50.4147390CurrentTrain: epoch  8, batch     2 | loss: 49.8268299CurrentTrain: epoch  8, batch     3 | loss: 223.1072463CurrentTrain: epoch  8, batch     4 | loss: 47.8430345CurrentTrain: epoch  8, batch     5 | loss: 47.5900344CurrentTrain: epoch  8, batch     6 | loss: 103.0478075CurrentTrain: epoch  9, batch     0 | loss: 68.2542736CurrentTrain: epoch  9, batch     1 | loss: 46.9489790CurrentTrain: epoch  9, batch     2 | loss: 102.6487060CurrentTrain: epoch  9, batch     3 | loss: 66.3684776CurrentTrain: epoch  9, batch     4 | loss: 36.7567235CurrentTrain: epoch  9, batch     5 | loss: 38.2740407CurrentTrain: epoch  9, batch     6 | loss: 63.5713621
MemoryTrain:  epoch  0, batch     0 | loss: 0.3549789MemoryTrain:  epoch  0, batch     1 | loss: 0.2570767MemoryTrain:  epoch  1, batch     0 | loss: 0.3091661MemoryTrain:  epoch  1, batch     1 | loss: 0.1377295MemoryTrain:  epoch  2, batch     0 | loss: 0.2729525MemoryTrain:  epoch  2, batch     1 | loss: 0.0673221MemoryTrain:  epoch  3, batch     0 | loss: 0.1702401MemoryTrain:  epoch  3, batch     1 | loss: 0.0910210MemoryTrain:  epoch  4, batch     0 | loss: 0.1494811MemoryTrain:  epoch  4, batch     1 | loss: 0.1161869MemoryTrain:  epoch  5, batch     0 | loss: 0.1173172MemoryTrain:  epoch  5, batch     1 | loss: 0.0664533MemoryTrain:  epoch  6, batch     0 | loss: 0.0847100MemoryTrain:  epoch  6, batch     1 | loss: 0.0668935MemoryTrain:  epoch  7, batch     0 | loss: 0.0708536MemoryTrain:  epoch  7, batch     1 | loss: 0.1021070MemoryTrain:  epoch  8, batch     0 | loss: 0.0900720MemoryTrain:  epoch  8, batch     1 | loss: 0.0498704MemoryTrain:  epoch  9, batch     0 | loss: 0.0614877MemoryTrain:  epoch  9, batch     1 | loss: 0.0538574

F1 score per class: {0: 0.0, 1: 0.21212121212121213, 2: 0.0, 3: 0.6428571428571429, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 13: 0.0, 14: 0.06521739130434782, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.44036697247706424, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.4536082474226804, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2771929824561403
Weighted-average F1 score: 0.2270059621525189
F1 score per class: {0: 0.0, 1: 0.1951219512195122, 2: 0.0, 3: 0.4774774774774775, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.05517241379310345, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4166666666666667, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.4530386740331492, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.18996062992125984
Weighted-average F1 score: 0.15882593108320073
F1 score per class: {0: 0.0, 1: 0.18085106382978725, 2: 0.0, 3: 0.4782608695652174, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.055944055944055944, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.410958904109589, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.4342857142857143, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.1910055583628095
Weighted-average F1 score: 0.16037593259287095

F1 score per class: {0: 0.3285024154589372, 1: 0.19718309859154928, 2: 0.17543859649122806, 3: 0.4090909090909091, 4: 0.7953216374269005, 5: 0.7384615384615385, 6: 0.24875621890547264, 7: 0.032520325203252036, 8: 0.2658959537572254, 9: 0.847457627118644, 10: 0.20155038759689922, 11: 0.1016949152542373, 12: 0.05217391304347826, 13: 0.015748031496062992, 14: 0.05454545454545454, 15: 0.4444444444444444, 16: 0.39285714285714285, 17: 0.0, 18: 0.14545454545454545, 19: 0.2553191489361702, 20: 0.2603550295857988, 21: 0.06060606060606061, 22: 0.34532374100719426, 23: 0.6136363636363636, 24: 0.07407407407407407, 25: 0.4411764705882353, 26: 0.5897435897435898, 27: 0.044444444444444446, 28: 0.16666666666666666, 29: 0.6831275720164609, 30: 0.8823529411764706, 31: 0.0, 32: 0.5259259259259259, 33: 0.3333333333333333, 34: 0.1647940074906367, 35: 0.23529411764705882, 36: 0.29310344827586204, 37: 0.11764705882352941, 38: 0.1111111111111111, 39: 0.0, 40: 0.2601626016260163}
Micro-average F1 score: 0.33439909723528305
Weighted-average F1 score: 0.32583833286698016
F1 score per class: {0: 0.09716599190283401, 1: 0.11049723756906077, 2: 0.13333333333333333, 3: 0.23296703296703297, 4: 0.8156424581005587, 5: 0.3875968992248062, 6: 0.2459546925566343, 7: 0.023255813953488372, 8: 0.20918367346938777, 9: 0.6097560975609756, 10: 0.15950920245398773, 11: 0.015873015873015872, 12: 0.19014084507042253, 13: 0.009523809523809525, 14: 0.029304029304029304, 15: 0.27906976744186046, 16: 0.35714285714285715, 17: 0.15789473684210525, 18: 0.145985401459854, 19: 0.27615062761506276, 20: 0.22641509433962265, 21: 0.08163265306122448, 22: 0.3024193548387097, 23: 0.49230769230769234, 24: 0.058823529411764705, 25: 0.4772727272727273, 26: 0.5691056910569106, 27: 0.03468208092485549, 28: 0.027906976744186046, 29: 0.6289752650176679, 30: 0.7083333333333334, 31: 0.0, 32: 0.43548387096774194, 33: 0.05454545454545454, 34: 0.12112259970457903, 35: 0.2318840579710145, 36: 0.1811175337186898, 37: 0.07142857142857142, 38: 0.08835341365461848, 39: 0.0784313725490196, 40: 0.3}
Micro-average F1 score: 0.231224305625188
Weighted-average F1 score: 0.2142956311916959
F1 score per class: {0: 0.0967741935483871, 1: 0.09686609686609686, 2: 0.16666666666666666, 3: 0.2277432712215321, 4: 0.8202247191011236, 5: 0.4140786749482402, 6: 0.25084745762711863, 7: 0.020100502512562814, 8: 0.22691292875989447, 9: 0.746268656716418, 10: 0.15584415584415584, 11: 0.03636363636363636, 12: 0.19622641509433963, 13: 0.008695652173913044, 14: 0.0311284046692607, 15: 0.3, 16: 0.352112676056338, 17: 0.05714285714285714, 18: 0.14285714285714285, 19: 0.2510460251046025, 20: 0.22281167108753316, 21: 0.11320754716981132, 22: 0.2982107355864811, 23: 0.5423728813559322, 24: 0.05555555555555555, 25: 0.47619047619047616, 26: 0.5714285714285714, 27: 0.0427807486631016, 28: 0.0273972602739726, 29: 0.6446886446886447, 30: 0.782608695652174, 31: 0.0, 32: 0.46153846153846156, 33: 0.06060606060606061, 34: 0.11377245508982035, 35: 0.24922118380062305, 36: 0.17467248908296942, 37: 0.08450704225352113, 38: 0.10576923076923077, 39: 0.125, 40: 0.2848101265822785}
Micro-average F1 score: 0.23564891846921798
Weighted-average F1 score: 0.21702658185460094
cur_acc:  ['0.6838', '0.5495', '0.2435', '0.4862', '0.2894', '0.3750', '0.3356', '0.2772']
his_acc:  ['0.6838', '0.6022', '0.4685', '0.4405', '0.4173', '0.4070', '0.3846', '0.3344']
cur_acc des:  ['0.6269', '0.4555', '0.2746', '0.3344', '0.2726', '0.3297', '0.2896', '0.1900']
his_acc des:  ['0.6269', '0.5166', '0.3732', '0.3310', '0.2860', '0.2805', '0.2518', '0.2312']
cur_acc rrf:  ['0.6282', '0.4729', '0.2884', '0.3435', '0.2734', '0.3434', '0.3067', '0.1910']
his_acc rrf:  ['0.6282', '0.5346', '0.3872', '0.3413', '0.2887', '0.2886', '0.2644', '0.2356']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 41.2648090CurrentTrain: epoch  0, batch     1 | loss: 47.2454163CurrentTrain: epoch  0, batch     2 | loss: 59.5708414CurrentTrain: epoch  0, batch     3 | loss: 48.0563298CurrentTrain: epoch  0, batch     4 | loss: 78.3816990CurrentTrain: epoch  0, batch     5 | loss: 76.0587869CurrentTrain: epoch  0, batch     6 | loss: 58.8802316CurrentTrain: epoch  0, batch     7 | loss: 57.7385526CurrentTrain: epoch  0, batch     8 | loss: 39.9633230CurrentTrain: epoch  0, batch     9 | loss: 76.8072362CurrentTrain: epoch  0, batch    10 | loss: 77.7441767CurrentTrain: epoch  0, batch    11 | loss: 40.6883327CurrentTrain: epoch  0, batch    12 | loss: 77.1337081CurrentTrain: epoch  0, batch    13 | loss: 77.1524977CurrentTrain: epoch  0, batch    14 | loss: 40.1431406CurrentTrain: epoch  0, batch    15 | loss: 47.6855487CurrentTrain: epoch  0, batch    16 | loss: 77.6060871CurrentTrain: epoch  0, batch    17 | loss: 75.8042962CurrentTrain: epoch  0, batch    18 | loss: 77.8418135CurrentTrain: epoch  0, batch    19 | loss: 76.9926230CurrentTrain: epoch  0, batch    20 | loss: 48.1460646CurrentTrain: epoch  0, batch    21 | loss: 38.8505916CurrentTrain: epoch  0, batch    22 | loss: 57.3931993CurrentTrain: epoch  0, batch    23 | loss: 75.7463887CurrentTrain: epoch  0, batch    24 | loss: 46.5234327CurrentTrain: epoch  0, batch    25 | loss: 60.4059542CurrentTrain: epoch  0, batch    26 | loss: 113.0410430CurrentTrain: epoch  0, batch    27 | loss: 47.6672123CurrentTrain: epoch  0, batch    28 | loss: 39.5379165CurrentTrain: epoch  0, batch    29 | loss: 57.4567853CurrentTrain: epoch  0, batch    30 | loss: 58.0642101CurrentTrain: epoch  0, batch    31 | loss: 39.0690469CurrentTrain: epoch  0, batch    32 | loss: 57.3747864CurrentTrain: epoch  0, batch    33 | loss: 57.6809544CurrentTrain: epoch  0, batch    34 | loss: 46.4356840CurrentTrain: epoch  0, batch    35 | loss: 112.1154969CurrentTrain: epoch  0, batch    36 | loss: 47.1014628CurrentTrain: epoch  0, batch    37 | loss: 57.9879411CurrentTrain: epoch  0, batch    38 | loss: 58.0587210CurrentTrain: epoch  0, batch    39 | loss: 112.3806897CurrentTrain: epoch  0, batch    40 | loss: 45.7550742CurrentTrain: epoch  0, batch    41 | loss: 56.5063997CurrentTrain: epoch  0, batch    42 | loss: 39.8659773CurrentTrain: epoch  0, batch    43 | loss: 58.8174689CurrentTrain: epoch  0, batch    44 | loss: 113.1638603CurrentTrain: epoch  0, batch    45 | loss: 222.2483374CurrentTrain: epoch  0, batch    46 | loss: 46.0530496CurrentTrain: epoch  0, batch    47 | loss: 39.2920312CurrentTrain: epoch  0, batch    48 | loss: 56.5634301CurrentTrain: epoch  0, batch    49 | loss: 45.6421612CurrentTrain: epoch  0, batch    50 | loss: 44.8950840CurrentTrain: epoch  0, batch    51 | loss: 45.9642944CurrentTrain: epoch  0, batch    52 | loss: 75.3585891CurrentTrain: epoch  0, batch    53 | loss: 56.4464614CurrentTrain: epoch  0, batch    54 | loss: 38.8374868CurrentTrain: epoch  0, batch    55 | loss: 57.7488227CurrentTrain: epoch  0, batch    56 | loss: 58.6965375CurrentTrain: epoch  0, batch    57 | loss: 56.6547312CurrentTrain: epoch  0, batch    58 | loss: 56.6277923CurrentTrain: epoch  0, batch    59 | loss: 56.2761621CurrentTrain: epoch  0, batch    60 | loss: 38.7429981CurrentTrain: epoch  0, batch    61 | loss: 57.3427984CurrentTrain: epoch  0, batch    62 | loss: 111.8987657CurrentTrain: epoch  0, batch    63 | loss: 75.0310792CurrentTrain: epoch  0, batch    64 | loss: 39.8211549CurrentTrain: epoch  0, batch    65 | loss: 56.8038732CurrentTrain: epoch  0, batch    66 | loss: 56.3836800CurrentTrain: epoch  0, batch    67 | loss: 57.7443325CurrentTrain: epoch  0, batch    68 | loss: 46.5795687CurrentTrain: epoch  0, batch    69 | loss: 74.0602611CurrentTrain: epoch  0, batch    70 | loss: 74.3411702CurrentTrain: epoch  0, batch    71 | loss: 73.9709427CurrentTrain: epoch  0, batch    72 | loss: 46.3133056CurrentTrain: epoch  0, batch    73 | loss: 46.6904410CurrentTrain: epoch  0, batch    74 | loss: 44.9821223CurrentTrain: epoch  0, batch    75 | loss: 55.6754014CurrentTrain: epoch  0, batch    76 | loss: 56.7369786CurrentTrain: epoch  0, batch    77 | loss: 53.5895548CurrentTrain: epoch  0, batch    78 | loss: 38.3431849CurrentTrain: epoch  0, batch    79 | loss: 37.7617132CurrentTrain: epoch  0, batch    80 | loss: 57.0364999CurrentTrain: epoch  0, batch    81 | loss: 44.4381098CurrentTrain: epoch  0, batch    82 | loss: 55.9386528CurrentTrain: epoch  0, batch    83 | loss: 43.5206965CurrentTrain: epoch  0, batch    84 | loss: 55.5769519CurrentTrain: epoch  0, batch    85 | loss: 74.2645364CurrentTrain: epoch  0, batch    86 | loss: 55.5623264CurrentTrain: epoch  0, batch    87 | loss: 72.1986849CurrentTrain: epoch  0, batch    88 | loss: 53.7718518CurrentTrain: epoch  0, batch    89 | loss: 73.8445307CurrentTrain: epoch  0, batch    90 | loss: 43.9868146CurrentTrain: epoch  0, batch    91 | loss: 43.8473505CurrentTrain: epoch  0, batch    92 | loss: 42.4003015CurrentTrain: epoch  0, batch    93 | loss: 222.5413784CurrentTrain: epoch  0, batch    94 | loss: 55.5335528CurrentTrain: epoch  0, batch    95 | loss: 41.7496722CurrentTrain: epoch  0, batch    96 | loss: 45.8478474CurrentTrain: epoch  0, batch    97 | loss: 105.7315013CurrentTrain: epoch  0, batch    98 | loss: 35.4871063CurrentTrain: epoch  0, batch    99 | loss: 76.3524552CurrentTrain: epoch  0, batch   100 | loss: 43.0100881CurrentTrain: epoch  0, batch   101 | loss: 74.2760321CurrentTrain: epoch  0, batch   102 | loss: 43.6151485CurrentTrain: epoch  0, batch   103 | loss: 53.7002468CurrentTrain: epoch  0, batch   104 | loss: 57.2700968CurrentTrain: epoch  0, batch   105 | loss: 73.9093589CurrentTrain: epoch  0, batch   106 | loss: 108.0308606CurrentTrain: epoch  0, batch   107 | loss: 56.6256013CurrentTrain: epoch  0, batch   108 | loss: 53.3914985CurrentTrain: epoch  0, batch   109 | loss: 72.8214920CurrentTrain: epoch  0, batch   110 | loss: 70.8618107CurrentTrain: epoch  0, batch   111 | loss: 53.8871433CurrentTrain: epoch  0, batch   112 | loss: 58.7920243CurrentTrain: epoch  0, batch   113 | loss: 41.1749799CurrentTrain: epoch  0, batch   114 | loss: 43.7225851CurrentTrain: epoch  0, batch   115 | loss: 71.6035382CurrentTrain: epoch  0, batch   116 | loss: 75.5009981CurrentTrain: epoch  0, batch   117 | loss: 75.2963715CurrentTrain: epoch  0, batch   118 | loss: 51.4885654CurrentTrain: epoch  0, batch   119 | loss: 68.9217058CurrentTrain: epoch  0, batch   120 | loss: 54.0189311CurrentTrain: epoch  0, batch   121 | loss: 70.7007578CurrentTrain: epoch  0, batch   122 | loss: 73.5261873CurrentTrain: epoch  0, batch   123 | loss: 54.8480381CurrentTrain: epoch  0, batch   124 | loss: 72.7339473CurrentTrain: epoch  0, batch   125 | loss: 43.0314631CurrentTrain: epoch  0, batch   126 | loss: 75.7279763CurrentTrain: epoch  0, batch   127 | loss: 42.5415359CurrentTrain: epoch  0, batch   128 | loss: 73.7942424CurrentTrain: epoch  0, batch   129 | loss: 71.0637334CurrentTrain: epoch  0, batch   130 | loss: 44.2773351CurrentTrain: epoch  0, batch   131 | loss: 53.4215703CurrentTrain: epoch  0, batch   132 | loss: 53.7609340CurrentTrain: epoch  0, batch   133 | loss: 29.3379285CurrentTrain: epoch  0, batch   134 | loss: 53.5688233CurrentTrain: epoch  0, batch   135 | loss: 43.9660422CurrentTrain: epoch  0, batch   136 | loss: 42.3114720CurrentTrain: epoch  0, batch   137 | loss: 42.5617283CurrentTrain: epoch  0, batch   138 | loss: 35.1008777CurrentTrain: epoch  0, batch   139 | loss: 53.2396438CurrentTrain: epoch  0, batch   140 | loss: 45.4589948CurrentTrain: epoch  0, batch   141 | loss: 41.6411813CurrentTrain: epoch  0, batch   142 | loss: 51.7602311CurrentTrain: epoch  0, batch   143 | loss: 57.6800335CurrentTrain: epoch  1, batch     0 | loss: 40.2287628CurrentTrain: epoch  1, batch     1 | loss: 71.8344768CurrentTrain: epoch  1, batch     2 | loss: 52.6975548CurrentTrain: epoch  1, batch     3 | loss: 55.0701046CurrentTrain: epoch  1, batch     4 | loss: 72.7613134CurrentTrain: epoch  1, batch     5 | loss: 41.9074936CurrentTrain: epoch  1, batch     6 | loss: 32.8937398CurrentTrain: epoch  1, batch     7 | loss: 53.6351509CurrentTrain: epoch  1, batch     8 | loss: 67.9010795CurrentTrain: epoch  1, batch     9 | loss: 53.4796010CurrentTrain: epoch  1, batch    10 | loss: 46.2562733CurrentTrain: epoch  1, batch    11 | loss: 72.0994208CurrentTrain: epoch  1, batch    12 | loss: 41.3799055CurrentTrain: epoch  1, batch    13 | loss: 49.8809358CurrentTrain: epoch  1, batch    14 | loss: 39.4216855CurrentTrain: epoch  1, batch    15 | loss: 222.6424309CurrentTrain: epoch  1, batch    16 | loss: 110.2036036CurrentTrain: epoch  1, batch    17 | loss: 72.4708019CurrentTrain: epoch  1, batch    18 | loss: 41.9596177CurrentTrain: epoch  1, batch    19 | loss: 72.2599404CurrentTrain: epoch  1, batch    20 | loss: 36.2548772CurrentTrain: epoch  1, batch    21 | loss: 42.3475053CurrentTrain: epoch  1, batch    22 | loss: 40.6304347CurrentTrain: epoch  1, batch    23 | loss: 69.7001340CurrentTrain: epoch  1, batch    24 | loss: 69.7552300CurrentTrain: epoch  1, batch    25 | loss: 105.8952088CurrentTrain: epoch  1, batch    26 | loss: 38.6153607CurrentTrain: epoch  1, batch    27 | loss: 52.6211288CurrentTrain: epoch  1, batch    28 | loss: 70.9118421CurrentTrain: epoch  1, batch    29 | loss: 34.1440485CurrentTrain: epoch  1, batch    30 | loss: 38.6710467CurrentTrain: epoch  1, batch    31 | loss: 50.6224955CurrentTrain: epoch  1, batch    32 | loss: 55.6820652CurrentTrain: epoch  1, batch    33 | loss: 67.9720637CurrentTrain: epoch  1, batch    34 | loss: 41.4494497CurrentTrain: epoch  1, batch    35 | loss: 30.9190484CurrentTrain: epoch  1, batch    36 | loss: 51.7208095CurrentTrain: epoch  1, batch    37 | loss: 34.6461642CurrentTrain: epoch  1, batch    38 | loss: 35.2401938CurrentTrain: epoch  1, batch    39 | loss: 104.8789842CurrentTrain: epoch  1, batch    40 | loss: 71.7057490CurrentTrain: epoch  1, batch    41 | loss: 43.4530947CurrentTrain: epoch  1, batch    42 | loss: 70.3655667CurrentTrain: epoch  1, batch    43 | loss: 56.0200259CurrentTrain: epoch  1, batch    44 | loss: 47.7574365CurrentTrain: epoch  1, batch    45 | loss: 40.1079435CurrentTrain: epoch  1, batch    46 | loss: 70.5949877CurrentTrain: epoch  1, batch    47 | loss: 70.4292501CurrentTrain: epoch  1, batch    48 | loss: 39.3862108CurrentTrain: epoch  1, batch    49 | loss: 68.1117194CurrentTrain: epoch  1, batch    50 | loss: 71.1820427CurrentTrain: epoch  1, batch    51 | loss: 107.8886994CurrentTrain: epoch  1, batch    52 | loss: 71.5364256CurrentTrain: epoch  1, batch    53 | loss: 69.7873299CurrentTrain: epoch  1, batch    54 | loss: 69.6573623CurrentTrain: epoch  1, batch    55 | loss: 69.6231812CurrentTrain: epoch  1, batch    56 | loss: 39.9121291CurrentTrain: epoch  1, batch    57 | loss: 42.4701465CurrentTrain: epoch  1, batch    58 | loss: 41.0365830CurrentTrain: epoch  1, batch    59 | loss: 51.5330979CurrentTrain: epoch  1, batch    60 | loss: 32.2315976CurrentTrain: epoch  1, batch    61 | loss: 51.4285342CurrentTrain: epoch  1, batch    62 | loss: 222.6502317CurrentTrain: epoch  1, batch    63 | loss: 70.6181789CurrentTrain: epoch  1, batch    64 | loss: 50.7472139CurrentTrain: epoch  1, batch    65 | loss: 35.6441332CurrentTrain: epoch  1, batch    66 | loss: 72.1298019CurrentTrain: epoch  1, batch    67 | loss: 69.4641533CurrentTrain: epoch  1, batch    68 | loss: 32.5639520CurrentTrain: epoch  1, batch    69 | loss: 70.0862045CurrentTrain: epoch  1, batch    70 | loss: 42.4210398CurrentTrain: epoch  1, batch    71 | loss: 51.6428344CurrentTrain: epoch  1, batch    72 | loss: 67.2413738CurrentTrain: epoch  1, batch    73 | loss: 52.9310474CurrentTrain: epoch  1, batch    74 | loss: 41.5481835CurrentTrain: epoch  1, batch    75 | loss: 71.4714355CurrentTrain: epoch  1, batch    76 | loss: 76.8582681CurrentTrain: epoch  1, batch    77 | loss: 32.9899706CurrentTrain: epoch  1, batch    78 | loss: 35.5084269CurrentTrain: epoch  1, batch    79 | loss: 68.4923295CurrentTrain: epoch  1, batch    80 | loss: 53.3640324CurrentTrain: epoch  1, batch    81 | loss: 33.2687328CurrentTrain: epoch  1, batch    82 | loss: 52.4198760CurrentTrain: epoch  1, batch    83 | loss: 52.8181130CurrentTrain: epoch  1, batch    84 | loss: 34.6858679CurrentTrain: epoch  1, batch    85 | loss: 70.6539506CurrentTrain: epoch  1, batch    86 | loss: 72.6522414CurrentTrain: epoch  1, batch    87 | loss: 67.9768815CurrentTrain: epoch  1, batch    88 | loss: 53.5214824CurrentTrain: epoch  1, batch    89 | loss: 52.8272556CurrentTrain: epoch  1, batch    90 | loss: 38.6904172CurrentTrain: epoch  1, batch    91 | loss: 52.7010023CurrentTrain: epoch  1, batch    92 | loss: 70.3322217CurrentTrain: epoch  1, batch    93 | loss: 52.5185979CurrentTrain: epoch  1, batch    94 | loss: 40.3800136CurrentTrain: epoch  1, batch    95 | loss: 52.0605301CurrentTrain: epoch  1, batch    96 | loss: 39.0832147CurrentTrain: epoch  1, batch    97 | loss: 41.1786308CurrentTrain: epoch  1, batch    98 | loss: 50.3476743CurrentTrain: epoch  1, batch    99 | loss: 31.5703032CurrentTrain: epoch  1, batch   100 | loss: 70.6702038CurrentTrain: epoch  1, batch   101 | loss: 49.4724329CurrentTrain: epoch  1, batch   102 | loss: 40.7091133CurrentTrain: epoch  1, batch   103 | loss: 51.3514912CurrentTrain: epoch  1, batch   104 | loss: 39.1810600CurrentTrain: epoch  1, batch   105 | loss: 37.6985428CurrentTrain: epoch  1, batch   106 | loss: 69.2445848CurrentTrain: epoch  1, batch   107 | loss: 52.6049186CurrentTrain: epoch  1, batch   108 | loss: 68.6471021CurrentTrain: epoch  1, batch   109 | loss: 38.5801375CurrentTrain: epoch  1, batch   110 | loss: 107.2620094CurrentTrain: epoch  1, batch   111 | loss: 67.7360871CurrentTrain: epoch  1, batch   112 | loss: 55.4622513CurrentTrain: epoch  1, batch   113 | loss: 40.2953348CurrentTrain: epoch  1, batch   114 | loss: 42.5222935CurrentTrain: epoch  1, batch   115 | loss: 41.7351056CurrentTrain: epoch  1, batch   116 | loss: 40.4151021CurrentTrain: epoch  1, batch   117 | loss: 40.2946845CurrentTrain: epoch  1, batch   118 | loss: 107.2375912CurrentTrain: epoch  1, batch   119 | loss: 42.3091848CurrentTrain: epoch  1, batch   120 | loss: 71.8908999CurrentTrain: epoch  1, batch   121 | loss: 52.2743406CurrentTrain: epoch  1, batch   122 | loss: 39.3862728CurrentTrain: epoch  1, batch   123 | loss: 40.7908266CurrentTrain: epoch  1, batch   124 | loss: 39.2934366CurrentTrain: epoch  1, batch   125 | loss: 38.1212704CurrentTrain: epoch  1, batch   126 | loss: 40.1501317CurrentTrain: epoch  1, batch   127 | loss: 53.7332193CurrentTrain: epoch  1, batch   128 | loss: 66.0700192CurrentTrain: epoch  1, batch   129 | loss: 109.4882245CurrentTrain: epoch  1, batch   130 | loss: 47.7629365CurrentTrain: epoch  1, batch   131 | loss: 34.3815427CurrentTrain: epoch  1, batch   132 | loss: 32.3435416CurrentTrain: epoch  1, batch   133 | loss: 70.4939206CurrentTrain: epoch  1, batch   134 | loss: 44.5805604CurrentTrain: epoch  1, batch   135 | loss: 53.2601663CurrentTrain: epoch  1, batch   136 | loss: 110.6249932CurrentTrain: epoch  1, batch   137 | loss: 53.0931114CurrentTrain: epoch  1, batch   138 | loss: 70.7587312CurrentTrain: epoch  1, batch   139 | loss: 70.7834360CurrentTrain: epoch  1, batch   140 | loss: 53.5707189CurrentTrain: epoch  1, batch   141 | loss: 51.6645384CurrentTrain: epoch  1, batch   142 | loss: 40.2370066CurrentTrain: epoch  1, batch   143 | loss: 39.0216640CurrentTrain: epoch  2, batch     0 | loss: 52.1250659CurrentTrain: epoch  2, batch     1 | loss: 50.0969093CurrentTrain: epoch  2, batch     2 | loss: 51.3137136CurrentTrain: epoch  2, batch     3 | loss: 67.7245472CurrentTrain: epoch  2, batch     4 | loss: 53.8547483CurrentTrain: epoch  2, batch     5 | loss: 74.3780480CurrentTrain: epoch  2, batch     6 | loss: 29.3263766CurrentTrain: epoch  2, batch     7 | loss: 32.0390582CurrentTrain: epoch  2, batch     8 | loss: 27.2019600CurrentTrain: epoch  2, batch     9 | loss: 27.5583956CurrentTrain: epoch  2, batch    10 | loss: 33.0602145CurrentTrain: epoch  2, batch    11 | loss: 104.0504365CurrentTrain: epoch  2, batch    12 | loss: 48.9018499CurrentTrain: epoch  2, batch    13 | loss: 50.8982813CurrentTrain: epoch  2, batch    14 | loss: 28.8424143CurrentTrain: epoch  2, batch    15 | loss: 49.2546104CurrentTrain: epoch  2, batch    16 | loss: 66.7265941CurrentTrain: epoch  2, batch    17 | loss: 50.5939316CurrentTrain: epoch  2, batch    18 | loss: 43.3966638CurrentTrain: epoch  2, batch    19 | loss: 71.5617776CurrentTrain: epoch  2, batch    20 | loss: 48.6258385CurrentTrain: epoch  2, batch    21 | loss: 66.8292933CurrentTrain: epoch  2, batch    22 | loss: 35.0181316CurrentTrain: epoch  2, batch    23 | loss: 36.9098784CurrentTrain: epoch  2, batch    24 | loss: 69.3679579CurrentTrain: epoch  2, batch    25 | loss: 50.5868469CurrentTrain: epoch  2, batch    26 | loss: 55.4082154CurrentTrain: epoch  2, batch    27 | loss: 69.8686862CurrentTrain: epoch  2, batch    28 | loss: 69.0717315CurrentTrain: epoch  2, batch    29 | loss: 37.1019478CurrentTrain: epoch  2, batch    30 | loss: 50.8396245CurrentTrain: epoch  2, batch    31 | loss: 70.6936322CurrentTrain: epoch  2, batch    32 | loss: 50.7036887CurrentTrain: epoch  2, batch    33 | loss: 69.5030940CurrentTrain: epoch  2, batch    34 | loss: 67.3472929CurrentTrain: epoch  2, batch    35 | loss: 109.4042256CurrentTrain: epoch  2, batch    36 | loss: 31.9490393CurrentTrain: epoch  2, batch    37 | loss: 52.8094082CurrentTrain: epoch  2, batch    38 | loss: 40.9204005CurrentTrain: epoch  2, batch    39 | loss: 70.1229299CurrentTrain: epoch  2, batch    40 | loss: 38.8554778CurrentTrain: epoch  2, batch    41 | loss: 39.5578762CurrentTrain: epoch  2, batch    42 | loss: 107.6729982CurrentTrain: epoch  2, batch    43 | loss: 108.4187446CurrentTrain: epoch  2, batch    44 | loss: 54.0088360CurrentTrain: epoch  2, batch    45 | loss: 103.7769571CurrentTrain: epoch  2, batch    46 | loss: 39.5838920CurrentTrain: epoch  2, batch    47 | loss: 107.9633700CurrentTrain: epoch  2, batch    48 | loss: 68.6186054CurrentTrain: epoch  2, batch    49 | loss: 47.4076130CurrentTrain: epoch  2, batch    50 | loss: 41.2416029CurrentTrain: epoch  2, batch    51 | loss: 31.5776557CurrentTrain: epoch  2, batch    52 | loss: 29.7989050CurrentTrain: epoch  2, batch    53 | loss: 49.6444394CurrentTrain: epoch  2, batch    54 | loss: 70.1528361CurrentTrain: epoch  2, batch    55 | loss: 67.2430246CurrentTrain: epoch  2, batch    56 | loss: 71.2811692CurrentTrain: epoch  2, batch    57 | loss: 52.3090818CurrentTrain: epoch  2, batch    58 | loss: 66.6808898CurrentTrain: epoch  2, batch    59 | loss: 50.5744904CurrentTrain: epoch  2, batch    60 | loss: 48.8840663CurrentTrain: epoch  2, batch    61 | loss: 73.9987405CurrentTrain: epoch  2, batch    62 | loss: 38.6393403CurrentTrain: epoch  2, batch    63 | loss: 39.0241568CurrentTrain: epoch  2, batch    64 | loss: 52.3736345CurrentTrain: epoch  2, batch    65 | loss: 37.5800040CurrentTrain: epoch  2, batch    66 | loss: 31.8366797CurrentTrain: epoch  2, batch    67 | loss: 66.5136283CurrentTrain: epoch  2, batch    68 | loss: 48.3272732CurrentTrain: epoch  2, batch    69 | loss: 40.8907458CurrentTrain: epoch  2, batch    70 | loss: 41.9324672CurrentTrain: epoch  2, batch    71 | loss: 47.8713384CurrentTrain: epoch  2, batch    72 | loss: 49.2707582CurrentTrain: epoch  2, batch    73 | loss: 105.2222965CurrentTrain: epoch  2, batch    74 | loss: 33.4329010CurrentTrain: epoch  2, batch    75 | loss: 50.1954906CurrentTrain: epoch  2, batch    76 | loss: 50.5029553CurrentTrain: epoch  2, batch    77 | loss: 50.7910770CurrentTrain: epoch  2, batch    78 | loss: 47.0410838CurrentTrain: epoch  2, batch    79 | loss: 50.5933548CurrentTrain: epoch  2, batch    80 | loss: 69.8531700CurrentTrain: epoch  2, batch    81 | loss: 35.0104332CurrentTrain: epoch  2, batch    82 | loss: 49.0772255CurrentTrain: epoch  2, batch    83 | loss: 67.1696863CurrentTrain: epoch  2, batch    84 | loss: 69.0794632CurrentTrain: epoch  2, batch    85 | loss: 108.0090851CurrentTrain: epoch  2, batch    86 | loss: 31.2615106CurrentTrain: epoch  2, batch    87 | loss: 31.2089381CurrentTrain: epoch  2, batch    88 | loss: 49.8409422CurrentTrain: epoch  2, batch    89 | loss: 37.4108275CurrentTrain: epoch  2, batch    90 | loss: 104.0956682CurrentTrain: epoch  2, batch    91 | loss: 39.0736538CurrentTrain: epoch  2, batch    92 | loss: 29.2987104CurrentTrain: epoch  2, batch    93 | loss: 40.5019544CurrentTrain: epoch  2, batch    94 | loss: 68.1113349CurrentTrain: epoch  2, batch    95 | loss: 40.0664838CurrentTrain: epoch  2, batch    96 | loss: 52.8872400CurrentTrain: epoch  2, batch    97 | loss: 104.7890687CurrentTrain: epoch  2, batch    98 | loss: 30.8752872CurrentTrain: epoch  2, batch    99 | loss: 37.1444505CurrentTrain: epoch  2, batch   100 | loss: 55.0623892CurrentTrain: epoch  2, batch   101 | loss: 53.6771921CurrentTrain: epoch  2, batch   102 | loss: 68.4273582CurrentTrain: epoch  2, batch   103 | loss: 69.6476672CurrentTrain: epoch  2, batch   104 | loss: 48.7589778CurrentTrain: epoch  2, batch   105 | loss: 48.4565527CurrentTrain: epoch  2, batch   106 | loss: 33.2334868CurrentTrain: epoch  2, batch   107 | loss: 50.0449794CurrentTrain: epoch  2, batch   108 | loss: 70.1458376CurrentTrain: epoch  2, batch   109 | loss: 104.4889234CurrentTrain: epoch  2, batch   110 | loss: 69.4585089CurrentTrain: epoch  2, batch   111 | loss: 73.5140400CurrentTrain: epoch  2, batch   112 | loss: 38.6031145CurrentTrain: epoch  2, batch   113 | loss: 43.5836186CurrentTrain: epoch  2, batch   114 | loss: 70.3282969CurrentTrain: epoch  2, batch   115 | loss: 68.9355592CurrentTrain: epoch  2, batch   116 | loss: 38.3651643CurrentTrain: epoch  2, batch   117 | loss: 38.5271874CurrentTrain: epoch  2, batch   118 | loss: 104.6665443CurrentTrain: epoch  2, batch   119 | loss: 51.0177332CurrentTrain: epoch  2, batch   120 | loss: 51.8391654CurrentTrain: epoch  2, batch   121 | loss: 26.2124547CurrentTrain: epoch  2, batch   122 | loss: 50.6799622CurrentTrain: epoch  2, batch   123 | loss: 47.2391725CurrentTrain: epoch  2, batch   124 | loss: 68.5164866CurrentTrain: epoch  2, batch   125 | loss: 69.8565304CurrentTrain: epoch  2, batch   126 | loss: 72.1448812CurrentTrain: epoch  2, batch   127 | loss: 40.0395279CurrentTrain: epoch  2, batch   128 | loss: 69.3930870CurrentTrain: epoch  2, batch   129 | loss: 67.9537666CurrentTrain: epoch  2, batch   130 | loss: 37.3993043CurrentTrain: epoch  2, batch   131 | loss: 68.6236408CurrentTrain: epoch  2, batch   132 | loss: 62.7702945CurrentTrain: epoch  2, batch   133 | loss: 68.7126063CurrentTrain: epoch  2, batch   134 | loss: 44.0390631CurrentTrain: epoch  2, batch   135 | loss: 36.2391216CurrentTrain: epoch  2, batch   136 | loss: 108.0729506CurrentTrain: epoch  2, batch   137 | loss: 37.6673997CurrentTrain: epoch  2, batch   138 | loss: 54.9530883CurrentTrain: epoch  2, batch   139 | loss: 31.2522287CurrentTrain: epoch  2, batch   140 | loss: 38.1944816CurrentTrain: epoch  2, batch   141 | loss: 222.6942479CurrentTrain: epoch  2, batch   142 | loss: 55.7068225CurrentTrain: epoch  2, batch   143 | loss: 36.4233928CurrentTrain: epoch  3, batch     0 | loss: 39.1815623CurrentTrain: epoch  3, batch     1 | loss: 49.4705976CurrentTrain: epoch  3, batch     2 | loss: 51.1634481CurrentTrain: epoch  3, batch     3 | loss: 48.4818788CurrentTrain: epoch  3, batch     4 | loss: 37.9992533CurrentTrain: epoch  3, batch     5 | loss: 37.5684199CurrentTrain: epoch  3, batch     6 | loss: 52.0231789CurrentTrain: epoch  3, batch     7 | loss: 68.0117300CurrentTrain: epoch  3, batch     8 | loss: 70.2823424CurrentTrain: epoch  3, batch     9 | loss: 37.1312029CurrentTrain: epoch  3, batch    10 | loss: 36.0154534CurrentTrain: epoch  3, batch    11 | loss: 108.3848667CurrentTrain: epoch  3, batch    12 | loss: 107.2285265CurrentTrain: epoch  3, batch    13 | loss: 38.7943093CurrentTrain: epoch  3, batch    14 | loss: 38.9681425CurrentTrain: epoch  3, batch    15 | loss: 35.8887227CurrentTrain: epoch  3, batch    16 | loss: 69.8524538CurrentTrain: epoch  3, batch    17 | loss: 68.6067162CurrentTrain: epoch  3, batch    18 | loss: 68.8426382CurrentTrain: epoch  3, batch    19 | loss: 24.5752069CurrentTrain: epoch  3, batch    20 | loss: 49.3450111CurrentTrain: epoch  3, batch    21 | loss: 35.3259242CurrentTrain: epoch  3, batch    22 | loss: 49.7407131CurrentTrain: epoch  3, batch    23 | loss: 51.2947471CurrentTrain: epoch  3, batch    24 | loss: 70.0720580CurrentTrain: epoch  3, batch    25 | loss: 48.9938446CurrentTrain: epoch  3, batch    26 | loss: 38.5448748CurrentTrain: epoch  3, batch    27 | loss: 36.3797730CurrentTrain: epoch  3, batch    28 | loss: 48.1073435CurrentTrain: epoch  3, batch    29 | loss: 50.6554136CurrentTrain: epoch  3, batch    30 | loss: 48.7740515CurrentTrain: epoch  3, batch    31 | loss: 38.4618998CurrentTrain: epoch  3, batch    32 | loss: 112.2929812CurrentTrain: epoch  3, batch    33 | loss: 50.3531115CurrentTrain: epoch  3, batch    34 | loss: 47.3805248CurrentTrain: epoch  3, batch    35 | loss: 50.2056415CurrentTrain: epoch  3, batch    36 | loss: 50.4938630CurrentTrain: epoch  3, batch    37 | loss: 49.7533619CurrentTrain: epoch  3, batch    38 | loss: 38.2053968CurrentTrain: epoch  3, batch    39 | loss: 27.2076563CurrentTrain: epoch  3, batch    40 | loss: 69.0342725CurrentTrain: epoch  3, batch    41 | loss: 222.4034070CurrentTrain: epoch  3, batch    42 | loss: 46.6936378CurrentTrain: epoch  3, batch    43 | loss: 53.3212521CurrentTrain: epoch  3, batch    44 | loss: 48.8222516CurrentTrain: epoch  3, batch    45 | loss: 39.4063729CurrentTrain: epoch  3, batch    46 | loss: 38.1866983CurrentTrain: epoch  3, batch    47 | loss: 30.6155302CurrentTrain: epoch  3, batch    48 | loss: 37.4825831CurrentTrain: epoch  3, batch    49 | loss: 49.7791944CurrentTrain: epoch  3, batch    50 | loss: 106.9172806CurrentTrain: epoch  3, batch    51 | loss: 34.8123439CurrentTrain: epoch  3, batch    52 | loss: 49.6068455CurrentTrain: epoch  3, batch    53 | loss: 35.9347809CurrentTrain: epoch  3, batch    54 | loss: 49.1545019CurrentTrain: epoch  3, batch    55 | loss: 66.5424342CurrentTrain: epoch  3, batch    56 | loss: 37.9972924CurrentTrain: epoch  3, batch    57 | loss: 49.6711413CurrentTrain: epoch  3, batch    58 | loss: 68.9339427CurrentTrain: epoch  3, batch    59 | loss: 32.7840018CurrentTrain: epoch  3, batch    60 | loss: 68.8896528CurrentTrain: epoch  3, batch    61 | loss: 50.2536454CurrentTrain: epoch  3, batch    62 | loss: 38.2354234CurrentTrain: epoch  3, batch    63 | loss: 48.4078120CurrentTrain: epoch  3, batch    64 | loss: 69.5032261CurrentTrain: epoch  3, batch    65 | loss: 48.0295615CurrentTrain: epoch  3, batch    66 | loss: 31.7371895CurrentTrain: epoch  3, batch    67 | loss: 56.4575538CurrentTrain: epoch  3, batch    68 | loss: 53.1302141CurrentTrain: epoch  3, batch    69 | loss: 107.9323302CurrentTrain: epoch  3, batch    70 | loss: 24.7000354CurrentTrain: epoch  3, batch    71 | loss: 69.3041993CurrentTrain: epoch  3, batch    72 | loss: 40.0617432CurrentTrain: epoch  3, batch    73 | loss: 47.6837480CurrentTrain: epoch  3, batch    74 | loss: 49.9706182CurrentTrain: epoch  3, batch    75 | loss: 67.2985436CurrentTrain: epoch  3, batch    76 | loss: 50.0439296CurrentTrain: epoch  3, batch    77 | loss: 66.9407922CurrentTrain: epoch  3, batch    78 | loss: 104.0362353CurrentTrain: epoch  3, batch    79 | loss: 50.0745909CurrentTrain: epoch  3, batch    80 | loss: 38.2969414CurrentTrain: epoch  3, batch    81 | loss: 51.8431448CurrentTrain: epoch  3, batch    82 | loss: 36.2885510CurrentTrain: epoch  3, batch    83 | loss: 51.2443883CurrentTrain: epoch  3, batch    84 | loss: 68.2503561CurrentTrain: epoch  3, batch    85 | loss: 50.5397225CurrentTrain: epoch  3, batch    86 | loss: 51.4208270CurrentTrain: epoch  3, batch    87 | loss: 33.0520485CurrentTrain: epoch  3, batch    88 | loss: 103.8684075CurrentTrain: epoch  3, batch    89 | loss: 68.3591616CurrentTrain: epoch  3, batch    90 | loss: 48.6810949CurrentTrain: epoch  3, batch    91 | loss: 70.3829698CurrentTrain: epoch  3, batch    92 | loss: 69.2813495CurrentTrain: epoch  3, batch    93 | loss: 69.2588420CurrentTrain: epoch  3, batch    94 | loss: 49.7724081CurrentTrain: epoch  3, batch    95 | loss: 66.5147200CurrentTrain: epoch  3, batch    96 | loss: 66.7280557CurrentTrain: epoch  3, batch    97 | loss: 27.9129653CurrentTrain: epoch  3, batch    98 | loss: 30.2324478CurrentTrain: epoch  3, batch    99 | loss: 42.7064708CurrentTrain: epoch  3, batch   100 | loss: 37.2228000CurrentTrain: epoch  3, batch   101 | loss: 49.9338429CurrentTrain: epoch  3, batch   102 | loss: 38.0628297CurrentTrain: epoch  3, batch   103 | loss: 50.7061640CurrentTrain: epoch  3, batch   104 | loss: 68.6203728CurrentTrain: epoch  3, batch   105 | loss: 68.1594562CurrentTrain: epoch  3, batch   106 | loss: 31.9303548CurrentTrain: epoch  3, batch   107 | loss: 40.4601562CurrentTrain: epoch  3, batch   108 | loss: 74.1725194CurrentTrain: epoch  3, batch   109 | loss: 44.9894844CurrentTrain: epoch  3, batch   110 | loss: 38.5805434CurrentTrain: epoch  3, batch   111 | loss: 50.0024757CurrentTrain: epoch  3, batch   112 | loss: 49.6244711CurrentTrain: epoch  3, batch   113 | loss: 36.9960673CurrentTrain: epoch  3, batch   114 | loss: 49.0992605CurrentTrain: epoch  3, batch   115 | loss: 51.1911444CurrentTrain: epoch  3, batch   116 | loss: 29.4442379CurrentTrain: epoch  3, batch   117 | loss: 104.5791189CurrentTrain: epoch  3, batch   118 | loss: 67.8484062CurrentTrain: epoch  3, batch   119 | loss: 39.6296357CurrentTrain: epoch  3, batch   120 | loss: 48.4928731CurrentTrain: epoch  3, batch   121 | loss: 41.3660858CurrentTrain: epoch  3, batch   122 | loss: 48.4978727CurrentTrain: epoch  3, batch   123 | loss: 39.8282135CurrentTrain: epoch  3, batch   124 | loss: 47.8290327CurrentTrain: epoch  3, batch   125 | loss: 107.1385928CurrentTrain: epoch  3, batch   126 | loss: 51.1631406CurrentTrain: epoch  3, batch   127 | loss: 35.1913448CurrentTrain: epoch  3, batch   128 | loss: 107.3204159CurrentTrain: epoch  3, batch   129 | loss: 65.5238244CurrentTrain: epoch  3, batch   130 | loss: 49.7056051CurrentTrain: epoch  3, batch   131 | loss: 48.5461117CurrentTrain: epoch  3, batch   132 | loss: 69.9542910CurrentTrain: epoch  3, batch   133 | loss: 30.7593593CurrentTrain: epoch  3, batch   134 | loss: 68.3202885CurrentTrain: epoch  3, batch   135 | loss: 40.0136586CurrentTrain: epoch  3, batch   136 | loss: 40.0887137CurrentTrain: epoch  3, batch   137 | loss: 68.3518025CurrentTrain: epoch  3, batch   138 | loss: 51.9419726CurrentTrain: epoch  3, batch   139 | loss: 39.9094651CurrentTrain: epoch  3, batch   140 | loss: 40.0472403CurrentTrain: epoch  3, batch   141 | loss: 37.3522879CurrentTrain: epoch  3, batch   142 | loss: 37.7370067CurrentTrain: epoch  3, batch   143 | loss: 78.4425124CurrentTrain: epoch  4, batch     0 | loss: 104.1101672CurrentTrain: epoch  4, batch     1 | loss: 31.4325538CurrentTrain: epoch  4, batch     2 | loss: 39.2266825CurrentTrain: epoch  4, batch     3 | loss: 38.1924023CurrentTrain: epoch  4, batch     4 | loss: 29.1656338CurrentTrain: epoch  4, batch     5 | loss: 66.8702764CurrentTrain: epoch  4, batch     6 | loss: 37.5208895CurrentTrain: epoch  4, batch     7 | loss: 30.6594978CurrentTrain: epoch  4, batch     8 | loss: 106.9753051CurrentTrain: epoch  4, batch     9 | loss: 45.7504379CurrentTrain: epoch  4, batch    10 | loss: 65.0109351CurrentTrain: epoch  4, batch    11 | loss: 106.9116796CurrentTrain: epoch  4, batch    12 | loss: 71.0509886CurrentTrain: epoch  4, batch    13 | loss: 68.6356466CurrentTrain: epoch  4, batch    14 | loss: 36.6402694CurrentTrain: epoch  4, batch    15 | loss: 45.8653960CurrentTrain: epoch  4, batch    16 | loss: 50.5260005CurrentTrain: epoch  4, batch    17 | loss: 45.8009140CurrentTrain: epoch  4, batch    18 | loss: 66.1858473CurrentTrain: epoch  4, batch    19 | loss: 222.2081662CurrentTrain: epoch  4, batch    20 | loss: 107.0501821CurrentTrain: epoch  4, batch    21 | loss: 38.2625192CurrentTrain: epoch  4, batch    22 | loss: 46.9953944CurrentTrain: epoch  4, batch    23 | loss: 38.1434312CurrentTrain: epoch  4, batch    24 | loss: 48.9751536CurrentTrain: epoch  4, batch    25 | loss: 30.5372943CurrentTrain: epoch  4, batch    26 | loss: 36.6110681CurrentTrain: epoch  4, batch    27 | loss: 48.6230104CurrentTrain: epoch  4, batch    28 | loss: 107.7647000CurrentTrain: epoch  4, batch    29 | loss: 48.3568026CurrentTrain: epoch  4, batch    30 | loss: 50.1814951CurrentTrain: epoch  4, batch    31 | loss: 33.3531323CurrentTrain: epoch  4, batch    32 | loss: 68.6202551CurrentTrain: epoch  4, batch    33 | loss: 37.8633029CurrentTrain: epoch  4, batch    34 | loss: 49.1156594CurrentTrain: epoch  4, batch    35 | loss: 47.8168053CurrentTrain: epoch  4, batch    36 | loss: 36.0464286CurrentTrain: epoch  4, batch    37 | loss: 36.9418401CurrentTrain: epoch  4, batch    38 | loss: 48.4621893CurrentTrain: epoch  4, batch    39 | loss: 52.8216046CurrentTrain: epoch  4, batch    40 | loss: 68.6896386CurrentTrain: epoch  4, batch    41 | loss: 37.9304644CurrentTrain: epoch  4, batch    42 | loss: 68.1790310CurrentTrain: epoch  4, batch    43 | loss: 40.7774173CurrentTrain: epoch  4, batch    44 | loss: 35.5741350CurrentTrain: epoch  4, batch    45 | loss: 46.9206195CurrentTrain: epoch  4, batch    46 | loss: 67.0504606CurrentTrain: epoch  4, batch    47 | loss: 38.4119811CurrentTrain: epoch  4, batch    48 | loss: 39.1918040CurrentTrain: epoch  4, batch    49 | loss: 68.4230401CurrentTrain: epoch  4, batch    50 | loss: 107.4996181CurrentTrain: epoch  4, batch    51 | loss: 54.8298797CurrentTrain: epoch  4, batch    52 | loss: 36.6599990CurrentTrain: epoch  4, batch    53 | loss: 48.8848494CurrentTrain: epoch  4, batch    54 | loss: 67.3382470CurrentTrain: epoch  4, batch    55 | loss: 47.7005607CurrentTrain: epoch  4, batch    56 | loss: 48.3353548CurrentTrain: epoch  4, batch    57 | loss: 46.6264964CurrentTrain: epoch  4, batch    58 | loss: 49.7985770CurrentTrain: epoch  4, batch    59 | loss: 66.3336903CurrentTrain: epoch  4, batch    60 | loss: 68.7593619CurrentTrain: epoch  4, batch    61 | loss: 67.3393781CurrentTrain: epoch  4, batch    62 | loss: 39.3002438CurrentTrain: epoch  4, batch    63 | loss: 46.8484872CurrentTrain: epoch  4, batch    64 | loss: 35.4498646CurrentTrain: epoch  4, batch    65 | loss: 74.7661962CurrentTrain: epoch  4, batch    66 | loss: 50.4050419CurrentTrain: epoch  4, batch    67 | loss: 49.4143194CurrentTrain: epoch  4, batch    68 | loss: 68.8459238CurrentTrain: epoch  4, batch    69 | loss: 37.0103958CurrentTrain: epoch  4, batch    70 | loss: 39.8671257CurrentTrain: epoch  4, batch    71 | loss: 37.1483529CurrentTrain: epoch  4, batch    72 | loss: 30.6458521CurrentTrain: epoch  4, batch    73 | loss: 38.8431129CurrentTrain: epoch  4, batch    74 | loss: 47.7444623CurrentTrain: epoch  4, batch    75 | loss: 30.6392614CurrentTrain: epoch  4, batch    76 | loss: 68.2415288CurrentTrain: epoch  4, batch    77 | loss: 48.7957270CurrentTrain: epoch  4, batch    78 | loss: 65.9933511CurrentTrain: epoch  4, batch    79 | loss: 36.9127277CurrentTrain: epoch  4, batch    80 | loss: 27.1390220CurrentTrain: epoch  4, batch    81 | loss: 50.6495426CurrentTrain: epoch  4, batch    82 | loss: 50.6047910CurrentTrain: epoch  4, batch    83 | loss: 39.0134670CurrentTrain: epoch  4, batch    84 | loss: 35.8940925CurrentTrain: epoch  4, batch    85 | loss: 47.3764420CurrentTrain: epoch  4, batch    86 | loss: 63.7670303CurrentTrain: epoch  4, batch    87 | loss: 36.4197697CurrentTrain: epoch  4, batch    88 | loss: 68.5206427CurrentTrain: epoch  4, batch    89 | loss: 48.4727786CurrentTrain: epoch  4, batch    90 | loss: 38.1554551CurrentTrain: epoch  4, batch    91 | loss: 47.9876811CurrentTrain: epoch  4, batch    92 | loss: 64.4452408CurrentTrain: epoch  4, batch    93 | loss: 68.7940460CurrentTrain: epoch  4, batch    94 | loss: 48.3991971CurrentTrain: epoch  4, batch    95 | loss: 108.4856837CurrentTrain: epoch  4, batch    96 | loss: 34.2487053CurrentTrain: epoch  4, batch    97 | loss: 68.4996402CurrentTrain: epoch  4, batch    98 | loss: 29.5839245CurrentTrain: epoch  4, batch    99 | loss: 30.2610840CurrentTrain: epoch  4, batch   100 | loss: 103.5969829CurrentTrain: epoch  4, batch   101 | loss: 38.6848608CurrentTrain: epoch  4, batch   102 | loss: 68.1874240CurrentTrain: epoch  4, batch   103 | loss: 48.8056772CurrentTrain: epoch  4, batch   104 | loss: 50.0620180CurrentTrain: epoch  4, batch   105 | loss: 67.0546975CurrentTrain: epoch  4, batch   106 | loss: 69.7340817CurrentTrain: epoch  4, batch   107 | loss: 29.9795937CurrentTrain: epoch  4, batch   108 | loss: 38.3771459CurrentTrain: epoch  4, batch   109 | loss: 46.5850050CurrentTrain: epoch  4, batch   110 | loss: 30.4331826CurrentTrain: epoch  4, batch   111 | loss: 35.2112226CurrentTrain: epoch  4, batch   112 | loss: 38.0029978CurrentTrain: epoch  4, batch   113 | loss: 69.6520412CurrentTrain: epoch  4, batch   114 | loss: 47.7599319CurrentTrain: epoch  4, batch   115 | loss: 106.4258226CurrentTrain: epoch  4, batch   116 | loss: 105.1054287CurrentTrain: epoch  4, batch   117 | loss: 48.7609327CurrentTrain: epoch  4, batch   118 | loss: 36.8000277CurrentTrain: epoch  4, batch   119 | loss: 103.6558513CurrentTrain: epoch  4, batch   120 | loss: 47.6055809CurrentTrain: epoch  4, batch   121 | loss: 37.8491194CurrentTrain: epoch  4, batch   122 | loss: 47.0763084CurrentTrain: epoch  4, batch   123 | loss: 49.3877447CurrentTrain: epoch  4, batch   124 | loss: 107.1414020CurrentTrain: epoch  4, batch   125 | loss: 67.0946249CurrentTrain: epoch  4, batch   126 | loss: 54.4911346CurrentTrain: epoch  4, batch   127 | loss: 70.1189661CurrentTrain: epoch  4, batch   128 | loss: 28.8084470CurrentTrain: epoch  4, batch   129 | loss: 68.2857269CurrentTrain: epoch  4, batch   130 | loss: 36.6450117CurrentTrain: epoch  4, batch   131 | loss: 49.9525907CurrentTrain: epoch  4, batch   132 | loss: 64.4362268CurrentTrain: epoch  4, batch   133 | loss: 43.6211746CurrentTrain: epoch  4, batch   134 | loss: 37.6594190CurrentTrain: epoch  4, batch   135 | loss: 50.0232681CurrentTrain: epoch  4, batch   136 | loss: 29.3863384CurrentTrain: epoch  4, batch   137 | loss: 69.3112489CurrentTrain: epoch  4, batch   138 | loss: 47.7540100CurrentTrain: epoch  4, batch   139 | loss: 106.9231527CurrentTrain: epoch  4, batch   140 | loss: 49.1708363CurrentTrain: epoch  4, batch   141 | loss: 36.1098503CurrentTrain: epoch  4, batch   142 | loss: 46.5222987CurrentTrain: epoch  4, batch   143 | loss: 48.5676932CurrentTrain: epoch  5, batch     0 | loss: 49.0687843CurrentTrain: epoch  5, batch     1 | loss: 36.7741402CurrentTrain: epoch  5, batch     2 | loss: 37.7939183CurrentTrain: epoch  5, batch     3 | loss: 68.2996476CurrentTrain: epoch  5, batch     4 | loss: 44.1746904CurrentTrain: epoch  5, batch     5 | loss: 39.7486560CurrentTrain: epoch  5, batch     6 | loss: 35.1850890CurrentTrain: epoch  5, batch     7 | loss: 46.7025014CurrentTrain: epoch  5, batch     8 | loss: 70.3764520CurrentTrain: epoch  5, batch     9 | loss: 71.9518785CurrentTrain: epoch  5, batch    10 | loss: 36.8976558CurrentTrain: epoch  5, batch    11 | loss: 222.2186605CurrentTrain: epoch  5, batch    12 | loss: 38.2012738CurrentTrain: epoch  5, batch    13 | loss: 37.7304612CurrentTrain: epoch  5, batch    14 | loss: 34.8713152CurrentTrain: epoch  5, batch    15 | loss: 28.0450848CurrentTrain: epoch  5, batch    16 | loss: 39.9718856CurrentTrain: epoch  5, batch    17 | loss: 103.6594912CurrentTrain: epoch  5, batch    18 | loss: 39.1517819CurrentTrain: epoch  5, batch    19 | loss: 70.0678506CurrentTrain: epoch  5, batch    20 | loss: 37.6456087CurrentTrain: epoch  5, batch    21 | loss: 35.2956168CurrentTrain: epoch  5, batch    22 | loss: 68.2362577CurrentTrain: epoch  5, batch    23 | loss: 49.0032727CurrentTrain: epoch  5, batch    24 | loss: 51.7650116CurrentTrain: epoch  5, batch    25 | loss: 30.9203173CurrentTrain: epoch  5, batch    26 | loss: 30.1140599CurrentTrain: epoch  5, batch    27 | loss: 49.1958437CurrentTrain: epoch  5, batch    28 | loss: 46.4142449CurrentTrain: epoch  5, batch    29 | loss: 48.7877138CurrentTrain: epoch  5, batch    30 | loss: 26.0126085CurrentTrain: epoch  5, batch    31 | loss: 68.1009453CurrentTrain: epoch  5, batch    32 | loss: 47.2612189CurrentTrain: epoch  5, batch    33 | loss: 104.1057475CurrentTrain: epoch  5, batch    34 | loss: 70.5030964CurrentTrain: epoch  5, batch    35 | loss: 49.2462820CurrentTrain: epoch  5, batch    36 | loss: 70.9800039CurrentTrain: epoch  5, batch    37 | loss: 48.4049198CurrentTrain: epoch  5, batch    38 | loss: 67.7218946CurrentTrain: epoch  5, batch    39 | loss: 24.6285353CurrentTrain: epoch  5, batch    40 | loss: 49.1391277CurrentTrain: epoch  5, batch    41 | loss: 68.1987438CurrentTrain: epoch  5, batch    42 | loss: 49.5160832CurrentTrain: epoch  5, batch    43 | loss: 38.2946475CurrentTrain: epoch  5, batch    44 | loss: 38.1488331CurrentTrain: epoch  5, batch    45 | loss: 46.4842855CurrentTrain: epoch  5, batch    46 | loss: 69.7689855CurrentTrain: epoch  5, batch    47 | loss: 39.1140393CurrentTrain: epoch  5, batch    48 | loss: 37.7493296CurrentTrain: epoch  5, batch    49 | loss: 31.3889547CurrentTrain: epoch  5, batch    50 | loss: 68.2902537CurrentTrain: epoch  5, batch    51 | loss: 69.4110125CurrentTrain: epoch  5, batch    52 | loss: 34.5535555CurrentTrain: epoch  5, batch    53 | loss: 36.5136191CurrentTrain: epoch  5, batch    54 | loss: 69.4370391CurrentTrain: epoch  5, batch    55 | loss: 106.8590844CurrentTrain: epoch  5, batch    56 | loss: 66.2335623CurrentTrain: epoch  5, batch    57 | loss: 106.8741778CurrentTrain: epoch  5, batch    58 | loss: 107.6090116CurrentTrain: epoch  5, batch    59 | loss: 39.0390510CurrentTrain: epoch  5, batch    60 | loss: 36.5325054CurrentTrain: epoch  5, batch    61 | loss: 39.8122811CurrentTrain: epoch  5, batch    62 | loss: 66.3303571CurrentTrain: epoch  5, batch    63 | loss: 36.9251864CurrentTrain: epoch  5, batch    64 | loss: 37.1221332CurrentTrain: epoch  5, batch    65 | loss: 49.7784038CurrentTrain: epoch  5, batch    66 | loss: 66.5307297CurrentTrain: epoch  5, batch    67 | loss: 35.8133671CurrentTrain: epoch  5, batch    68 | loss: 68.6305848CurrentTrain: epoch  5, batch    69 | loss: 49.0523199CurrentTrain: epoch  5, batch    70 | loss: 29.3497228CurrentTrain: epoch  5, batch    71 | loss: 68.7520770CurrentTrain: epoch  5, batch    72 | loss: 70.9992546CurrentTrain: epoch  5, batch    73 | loss: 45.3696038CurrentTrain: epoch  5, batch    74 | loss: 49.2132348CurrentTrain: epoch  5, batch    75 | loss: 64.7995164CurrentTrain: epoch  5, batch    76 | loss: 49.8167712CurrentTrain: epoch  5, batch    77 | loss: 104.1489509CurrentTrain: epoch  5, batch    78 | loss: 40.2565544CurrentTrain: epoch  5, batch    79 | loss: 63.3324262CurrentTrain: epoch  5, batch    80 | loss: 39.5624081CurrentTrain: epoch  5, batch    81 | loss: 39.1813298CurrentTrain: epoch  5, batch    82 | loss: 47.6121313CurrentTrain: epoch  5, batch    83 | loss: 48.7402864CurrentTrain: epoch  5, batch    84 | loss: 30.2379468CurrentTrain: epoch  5, batch    85 | loss: 26.8738531CurrentTrain: epoch  5, batch    86 | loss: 69.5450944CurrentTrain: epoch  5, batch    87 | loss: 107.0231926CurrentTrain: epoch  5, batch    88 | loss: 68.8801761CurrentTrain: epoch  5, batch    89 | loss: 30.3130342CurrentTrain: epoch  5, batch    90 | loss: 62.6975086CurrentTrain: epoch  5, batch    91 | loss: 37.7963713CurrentTrain: epoch  5, batch    92 | loss: 35.0519827CurrentTrain: epoch  5, batch    93 | loss: 47.7463156CurrentTrain: epoch  5, batch    94 | loss: 36.0081292CurrentTrain: epoch  5, batch    95 | loss: 36.6956600CurrentTrain: epoch  5, batch    96 | loss: 38.7988885CurrentTrain: epoch  5, batch    97 | loss: 37.7325493CurrentTrain: epoch  5, batch    98 | loss: 66.2803442CurrentTrain: epoch  5, batch    99 | loss: 68.1971841CurrentTrain: epoch  5, batch   100 | loss: 74.3479066CurrentTrain: epoch  5, batch   101 | loss: 30.8702863CurrentTrain: epoch  5, batch   102 | loss: 38.0529905CurrentTrain: epoch  5, batch   103 | loss: 48.1836808CurrentTrain: epoch  5, batch   104 | loss: 222.1019195CurrentTrain: epoch  5, batch   105 | loss: 68.7205675CurrentTrain: epoch  5, batch   106 | loss: 47.6129688CurrentTrain: epoch  5, batch   107 | loss: 67.2670845CurrentTrain: epoch  5, batch   108 | loss: 29.3068041CurrentTrain: epoch  5, batch   109 | loss: 49.1026073CurrentTrain: epoch  5, batch   110 | loss: 49.0333554CurrentTrain: epoch  5, batch   111 | loss: 47.8489866CurrentTrain: epoch  5, batch   112 | loss: 38.1544026CurrentTrain: epoch  5, batch   113 | loss: 50.6174817CurrentTrain: epoch  5, batch   114 | loss: 47.5465080CurrentTrain: epoch  5, batch   115 | loss: 36.4234729CurrentTrain: epoch  5, batch   116 | loss: 30.7730701CurrentTrain: epoch  5, batch   117 | loss: 49.3215806CurrentTrain: epoch  5, batch   118 | loss: 68.2086777CurrentTrain: epoch  5, batch   119 | loss: 68.2906449CurrentTrain: epoch  5, batch   120 | loss: 35.3428006CurrentTrain: epoch  5, batch   121 | loss: 49.1498252CurrentTrain: epoch  5, batch   122 | loss: 38.1472790CurrentTrain: epoch  5, batch   123 | loss: 28.7774426CurrentTrain: epoch  5, batch   124 | loss: 101.0587217CurrentTrain: epoch  5, batch   125 | loss: 66.3296693CurrentTrain: epoch  5, batch   126 | loss: 36.6561479CurrentTrain: epoch  5, batch   127 | loss: 46.7520319CurrentTrain: epoch  5, batch   128 | loss: 46.9253565CurrentTrain: epoch  5, batch   129 | loss: 72.2455773CurrentTrain: epoch  5, batch   130 | loss: 66.3451674CurrentTrain: epoch  5, batch   131 | loss: 66.1102715CurrentTrain: epoch  5, batch   132 | loss: 49.5473750CurrentTrain: epoch  5, batch   133 | loss: 35.7080191CurrentTrain: epoch  5, batch   134 | loss: 48.4896853CurrentTrain: epoch  5, batch   135 | loss: 35.5908146CurrentTrain: epoch  5, batch   136 | loss: 29.6140316CurrentTrain: epoch  5, batch   137 | loss: 68.1528350CurrentTrain: epoch  5, batch   138 | loss: 34.4718544CurrentTrain: epoch  5, batch   139 | loss: 49.0948726CurrentTrain: epoch  5, batch   140 | loss: 37.6567808CurrentTrain: epoch  5, batch   141 | loss: 49.5857698CurrentTrain: epoch  5, batch   142 | loss: 35.2184584CurrentTrain: epoch  5, batch   143 | loss: 80.8816700CurrentTrain: epoch  6, batch     0 | loss: 66.3751851CurrentTrain: epoch  6, batch     1 | loss: 64.6020572CurrentTrain: epoch  6, batch     2 | loss: 45.7814860CurrentTrain: epoch  6, batch     3 | loss: 48.0439956CurrentTrain: epoch  6, batch     4 | loss: 106.9725736CurrentTrain: epoch  6, batch     5 | loss: 38.6161607CurrentTrain: epoch  6, batch     6 | loss: 46.4680175CurrentTrain: epoch  6, batch     7 | loss: 46.5085636CurrentTrain: epoch  6, batch     8 | loss: 29.1029676CurrentTrain: epoch  6, batch     9 | loss: 49.7815224CurrentTrain: epoch  6, batch    10 | loss: 49.1375543CurrentTrain: epoch  6, batch    11 | loss: 68.1688520CurrentTrain: epoch  6, batch    12 | loss: 29.7478589CurrentTrain: epoch  6, batch    13 | loss: 101.1132501CurrentTrain: epoch  6, batch    14 | loss: 63.2832342CurrentTrain: epoch  6, batch    15 | loss: 48.7890461CurrentTrain: epoch  6, batch    16 | loss: 46.3861684CurrentTrain: epoch  6, batch    17 | loss: 46.1656337CurrentTrain: epoch  6, batch    18 | loss: 69.6342875CurrentTrain: epoch  6, batch    19 | loss: 107.0630433CurrentTrain: epoch  6, batch    20 | loss: 106.7267266CurrentTrain: epoch  6, batch    21 | loss: 66.1879845CurrentTrain: epoch  6, batch    22 | loss: 35.2610685CurrentTrain: epoch  6, batch    23 | loss: 66.8719362CurrentTrain: epoch  6, batch    24 | loss: 48.6239902CurrentTrain: epoch  6, batch    25 | loss: 106.6730871CurrentTrain: epoch  6, batch    26 | loss: 35.4354683CurrentTrain: epoch  6, batch    27 | loss: 103.7431607CurrentTrain: epoch  6, batch    28 | loss: 34.6198588CurrentTrain: epoch  6, batch    29 | loss: 46.6552168CurrentTrain: epoch  6, batch    30 | loss: 66.1332742CurrentTrain: epoch  6, batch    31 | loss: 36.5557112CurrentTrain: epoch  6, batch    32 | loss: 35.3266648CurrentTrain: epoch  6, batch    33 | loss: 65.5559249CurrentTrain: epoch  6, batch    34 | loss: 35.9149331CurrentTrain: epoch  6, batch    35 | loss: 66.2671827CurrentTrain: epoch  6, batch    36 | loss: 29.1663564CurrentTrain: epoch  6, batch    37 | loss: 66.1787153CurrentTrain: epoch  6, batch    38 | loss: 49.0446556CurrentTrain: epoch  6, batch    39 | loss: 33.6412301CurrentTrain: epoch  6, batch    40 | loss: 37.7614696CurrentTrain: epoch  6, batch    41 | loss: 49.0246021CurrentTrain: epoch  6, batch    42 | loss: 36.5233672CurrentTrain: epoch  6, batch    43 | loss: 68.3429259CurrentTrain: epoch  6, batch    44 | loss: 104.3283832CurrentTrain: epoch  6, batch    45 | loss: 68.1749989CurrentTrain: epoch  6, batch    46 | loss: 33.8291866CurrentTrain: epoch  6, batch    47 | loss: 28.8212873CurrentTrain: epoch  6, batch    48 | loss: 29.1884138CurrentTrain: epoch  6, batch    49 | loss: 27.3180701CurrentTrain: epoch  6, batch    50 | loss: 46.8940844CurrentTrain: epoch  6, batch    51 | loss: 49.0073226CurrentTrain: epoch  6, batch    52 | loss: 107.3030634CurrentTrain: epoch  6, batch    53 | loss: 48.9544064CurrentTrain: epoch  6, batch    54 | loss: 106.8823577CurrentTrain: epoch  6, batch    55 | loss: 28.9123510CurrentTrain: epoch  6, batch    56 | loss: 38.5030281CurrentTrain: epoch  6, batch    57 | loss: 36.3738101CurrentTrain: epoch  6, batch    58 | loss: 103.5188904CurrentTrain: epoch  6, batch    59 | loss: 68.2235660CurrentTrain: epoch  6, batch    60 | loss: 35.7969202CurrentTrain: epoch  6, batch    61 | loss: 49.2166409CurrentTrain: epoch  6, batch    62 | loss: 36.2842971CurrentTrain: epoch  6, batch    63 | loss: 37.3769111CurrentTrain: epoch  6, batch    64 | loss: 38.9031078CurrentTrain: epoch  6, batch    65 | loss: 39.3282750CurrentTrain: epoch  6, batch    66 | loss: 29.2334208CurrentTrain: epoch  6, batch    67 | loss: 36.8911980CurrentTrain: epoch  6, batch    68 | loss: 69.0924342CurrentTrain: epoch  6, batch    69 | loss: 66.0685859CurrentTrain: epoch  6, batch    70 | loss: 46.3623936CurrentTrain: epoch  6, batch    71 | loss: 66.2177608CurrentTrain: epoch  6, batch    72 | loss: 25.9864394CurrentTrain: epoch  6, batch    73 | loss: 37.9845297CurrentTrain: epoch  6, batch    74 | loss: 49.6278389CurrentTrain: epoch  6, batch    75 | loss: 106.9030872CurrentTrain: epoch  6, batch    76 | loss: 68.2146023CurrentTrain: epoch  6, batch    77 | loss: 50.0365922CurrentTrain: epoch  6, batch    78 | loss: 68.2443666CurrentTrain: epoch  6, batch    79 | loss: 36.9202879CurrentTrain: epoch  6, batch    80 | loss: 29.5148622CurrentTrain: epoch  6, batch    81 | loss: 46.9539380CurrentTrain: epoch  6, batch    82 | loss: 34.8948788CurrentTrain: epoch  6, batch    83 | loss: 36.5850265CurrentTrain: epoch  6, batch    84 | loss: 21.8287760CurrentTrain: epoch  6, batch    85 | loss: 68.4672465CurrentTrain: epoch  6, batch    86 | loss: 68.1621844CurrentTrain: epoch  6, batch    87 | loss: 37.1862203CurrentTrain: epoch  6, batch    88 | loss: 29.9754918CurrentTrain: epoch  6, batch    89 | loss: 69.1714059CurrentTrain: epoch  6, batch    90 | loss: 106.8926052CurrentTrain: epoch  6, batch    91 | loss: 66.3192473CurrentTrain: epoch  6, batch    92 | loss: 48.0551104CurrentTrain: epoch  6, batch    93 | loss: 69.4437363CurrentTrain: epoch  6, batch    94 | loss: 47.5533287CurrentTrain: epoch  6, batch    95 | loss: 28.7291654CurrentTrain: epoch  6, batch    96 | loss: 37.6963391CurrentTrain: epoch  6, batch    97 | loss: 49.2036059CurrentTrain: epoch  6, batch    98 | loss: 43.3809876CurrentTrain: epoch  6, batch    99 | loss: 49.1216785CurrentTrain: epoch  6, batch   100 | loss: 51.7865467CurrentTrain: epoch  6, batch   101 | loss: 35.4926004CurrentTrain: epoch  6, batch   102 | loss: 37.4693033CurrentTrain: epoch  6, batch   103 | loss: 25.6748922CurrentTrain: epoch  6, batch   104 | loss: 64.8589256CurrentTrain: epoch  6, batch   105 | loss: 36.6378669CurrentTrain: epoch  6, batch   106 | loss: 36.8722787CurrentTrain: epoch  6, batch   107 | loss: 63.8053165CurrentTrain: epoch  6, batch   108 | loss: 28.3177514CurrentTrain: epoch  6, batch   109 | loss: 37.7298648CurrentTrain: epoch  6, batch   110 | loss: 66.1393316CurrentTrain: epoch  6, batch   111 | loss: 69.0774134CurrentTrain: epoch  6, batch   112 | loss: 108.8913542CurrentTrain: epoch  6, batch   113 | loss: 69.3426020CurrentTrain: epoch  6, batch   114 | loss: 49.0826328CurrentTrain: epoch  6, batch   115 | loss: 46.2249723CurrentTrain: epoch  6, batch   116 | loss: 23.1041461CurrentTrain: epoch  6, batch   117 | loss: 50.2432744CurrentTrain: epoch  6, batch   118 | loss: 68.1247715CurrentTrain: epoch  6, batch   119 | loss: 29.1223278CurrentTrain: epoch  6, batch   120 | loss: 109.8134617CurrentTrain: epoch  6, batch   121 | loss: 107.0661274CurrentTrain: epoch  6, batch   122 | loss: 63.4577348CurrentTrain: epoch  6, batch   123 | loss: 48.2758441CurrentTrain: epoch  6, batch   124 | loss: 108.0443258CurrentTrain: epoch  6, batch   125 | loss: 38.1351600CurrentTrain: epoch  6, batch   126 | loss: 36.6911165CurrentTrain: epoch  6, batch   127 | loss: 49.3007138CurrentTrain: epoch  6, batch   128 | loss: 69.0239740CurrentTrain: epoch  6, batch   129 | loss: 222.1764097CurrentTrain: epoch  6, batch   130 | loss: 64.6696229CurrentTrain: epoch  6, batch   131 | loss: 42.0173423CurrentTrain: epoch  6, batch   132 | loss: 36.0447378CurrentTrain: epoch  6, batch   133 | loss: 106.8173155CurrentTrain: epoch  6, batch   134 | loss: 23.9727508CurrentTrain: epoch  6, batch   135 | loss: 103.5110359CurrentTrain: epoch  6, batch   136 | loss: 64.4859053CurrentTrain: epoch  6, batch   137 | loss: 37.4971348CurrentTrain: epoch  6, batch   138 | loss: 35.9108163CurrentTrain: epoch  6, batch   139 | loss: 68.1008689CurrentTrain: epoch  6, batch   140 | loss: 66.3936665CurrentTrain: epoch  6, batch   141 | loss: 66.3309375CurrentTrain: epoch  6, batch   142 | loss: 46.0081274CurrentTrain: epoch  6, batch   143 | loss: 35.0671608CurrentTrain: epoch  7, batch     0 | loss: 35.6292695CurrentTrain: epoch  7, batch     1 | loss: 48.1118981CurrentTrain: epoch  7, batch     2 | loss: 37.6544789CurrentTrain: epoch  7, batch     3 | loss: 37.3750085CurrentTrain: epoch  7, batch     4 | loss: 106.7077467CurrentTrain: epoch  7, batch     5 | loss: 31.4627976CurrentTrain: epoch  7, batch     6 | loss: 66.3171108CurrentTrain: epoch  7, batch     7 | loss: 63.4853601CurrentTrain: epoch  7, batch     8 | loss: 28.2601403CurrentTrain: epoch  7, batch     9 | loss: 49.0474513CurrentTrain: epoch  7, batch    10 | loss: 48.9845918CurrentTrain: epoch  7, batch    11 | loss: 28.1165388CurrentTrain: epoch  7, batch    12 | loss: 28.3352713CurrentTrain: epoch  7, batch    13 | loss: 49.1241867CurrentTrain: epoch  7, batch    14 | loss: 106.9967318CurrentTrain: epoch  7, batch    15 | loss: 36.6047585CurrentTrain: epoch  7, batch    16 | loss: 65.0845956CurrentTrain: epoch  7, batch    17 | loss: 47.7970706CurrentTrain: epoch  7, batch    18 | loss: 106.8340034CurrentTrain: epoch  7, batch    19 | loss: 62.9254683CurrentTrain: epoch  7, batch    20 | loss: 69.9817151CurrentTrain: epoch  7, batch    21 | loss: 49.8149252CurrentTrain: epoch  7, batch    22 | loss: 65.6420106CurrentTrain: epoch  7, batch    23 | loss: 27.0267410CurrentTrain: epoch  7, batch    24 | loss: 34.2778071CurrentTrain: epoch  7, batch    25 | loss: 64.5491704CurrentTrain: epoch  7, batch    26 | loss: 47.7028375CurrentTrain: epoch  7, batch    27 | loss: 66.8216551CurrentTrain: epoch  7, batch    28 | loss: 46.1067821CurrentTrain: epoch  7, batch    29 | loss: 106.8814376CurrentTrain: epoch  7, batch    30 | loss: 49.3422577CurrentTrain: epoch  7, batch    31 | loss: 30.0266329CurrentTrain: epoch  7, batch    32 | loss: 68.1256667CurrentTrain: epoch  7, batch    33 | loss: 68.7150548CurrentTrain: epoch  7, batch    34 | loss: 33.5161208CurrentTrain: epoch  7, batch    35 | loss: 37.7314494CurrentTrain: epoch  7, batch    36 | loss: 37.7517121CurrentTrain: epoch  7, batch    37 | loss: 48.1485699CurrentTrain: epoch  7, batch    38 | loss: 35.6518323CurrentTrain: epoch  7, batch    39 | loss: 222.2231466CurrentTrain: epoch  7, batch    40 | loss: 36.5441349CurrentTrain: epoch  7, batch    41 | loss: 44.2836466CurrentTrain: epoch  7, batch    42 | loss: 66.2260225CurrentTrain: epoch  7, batch    43 | loss: 64.2212856CurrentTrain: epoch  7, batch    44 | loss: 66.8097302CurrentTrain: epoch  7, batch    45 | loss: 47.9394405CurrentTrain: epoch  7, batch    46 | loss: 68.2306886CurrentTrain: epoch  7, batch    47 | loss: 47.7659594CurrentTrain: epoch  7, batch    48 | loss: 49.1546931CurrentTrain: epoch  7, batch    49 | loss: 29.9724599CurrentTrain: epoch  7, batch    50 | loss: 32.4042307CurrentTrain: epoch  7, batch    51 | loss: 48.0226838CurrentTrain: epoch  7, batch    52 | loss: 64.4296280CurrentTrain: epoch  7, batch    53 | loss: 106.7977897CurrentTrain: epoch  7, batch    54 | loss: 46.5349738CurrentTrain: epoch  7, batch    55 | loss: 47.5385133CurrentTrain: epoch  7, batch    56 | loss: 35.9637820CurrentTrain: epoch  7, batch    57 | loss: 45.8099287CurrentTrain: epoch  7, batch    58 | loss: 106.7610382CurrentTrain: epoch  7, batch    59 | loss: 46.6175333CurrentTrain: epoch  7, batch    60 | loss: 37.4740155CurrentTrain: epoch  7, batch    61 | loss: 47.8142972CurrentTrain: epoch  7, batch    62 | loss: 106.7754963CurrentTrain: epoch  7, batch    63 | loss: 48.9111970CurrentTrain: epoch  7, batch    64 | loss: 52.8888915CurrentTrain: epoch  7, batch    65 | loss: 64.6633557CurrentTrain: epoch  7, batch    66 | loss: 48.9706691CurrentTrain: epoch  7, batch    67 | loss: 27.7500520CurrentTrain: epoch  7, batch    68 | loss: 38.9699577CurrentTrain: epoch  7, batch    69 | loss: 44.2601526CurrentTrain: epoch  7, batch    70 | loss: 222.1000527CurrentTrain: epoch  7, batch    71 | loss: 68.1190106CurrentTrain: epoch  7, batch    72 | loss: 28.2957422CurrentTrain: epoch  7, batch    73 | loss: 29.0571778CurrentTrain: epoch  7, batch    74 | loss: 48.9891697CurrentTrain: epoch  7, batch    75 | loss: 49.5279089CurrentTrain: epoch  7, batch    76 | loss: 106.7049752CurrentTrain: epoch  7, batch    77 | loss: 49.4275482CurrentTrain: epoch  7, batch    78 | loss: 48.9153211CurrentTrain: epoch  7, batch    79 | loss: 68.0732270CurrentTrain: epoch  7, batch    80 | loss: 29.9185926CurrentTrain: epoch  7, batch    81 | loss: 64.1657409CurrentTrain: epoch  7, batch    82 | loss: 68.0704599CurrentTrain: epoch  7, batch    83 | loss: 48.5363536CurrentTrain: epoch  7, batch    84 | loss: 106.7911954CurrentTrain: epoch  7, batch    85 | loss: 35.4748438CurrentTrain: epoch  7, batch    86 | loss: 46.7815269CurrentTrain: epoch  7, batch    87 | loss: 47.5128718CurrentTrain: epoch  7, batch    88 | loss: 34.6620758CurrentTrain: epoch  7, batch    89 | loss: 36.5596863CurrentTrain: epoch  7, batch    90 | loss: 47.9504079CurrentTrain: epoch  7, batch    91 | loss: 103.5267896CurrentTrain: epoch  7, batch    92 | loss: 68.0993102CurrentTrain: epoch  7, batch    93 | loss: 49.2227077CurrentTrain: epoch  7, batch    94 | loss: 29.3337510CurrentTrain: epoch  7, batch    95 | loss: 66.1428923CurrentTrain: epoch  7, batch    96 | loss: 106.9310568CurrentTrain: epoch  7, batch    97 | loss: 66.1104477CurrentTrain: epoch  7, batch    98 | loss: 34.5516331CurrentTrain: epoch  7, batch    99 | loss: 68.0799072CurrentTrain: epoch  7, batch   100 | loss: 106.7208821CurrentTrain: epoch  7, batch   101 | loss: 106.8131287CurrentTrain: epoch  7, batch   102 | loss: 28.2059916CurrentTrain: epoch  7, batch   103 | loss: 34.1480146CurrentTrain: epoch  7, batch   104 | loss: 47.5012064CurrentTrain: epoch  7, batch   105 | loss: 45.2733755CurrentTrain: epoch  7, batch   106 | loss: 68.1016652CurrentTrain: epoch  7, batch   107 | loss: 47.4580839CurrentTrain: epoch  7, batch   108 | loss: 28.3799114CurrentTrain: epoch  7, batch   109 | loss: 50.0678806CurrentTrain: epoch  7, batch   110 | loss: 28.2300243CurrentTrain: epoch  7, batch   111 | loss: 68.4059144CurrentTrain: epoch  7, batch   112 | loss: 44.7710924CurrentTrain: epoch  7, batch   113 | loss: 68.1400487CurrentTrain: epoch  7, batch   114 | loss: 28.9900709CurrentTrain: epoch  7, batch   115 | loss: 34.8113354CurrentTrain: epoch  7, batch   116 | loss: 48.0430764CurrentTrain: epoch  7, batch   117 | loss: 36.4876521CurrentTrain: epoch  7, batch   118 | loss: 48.9480847CurrentTrain: epoch  7, batch   119 | loss: 48.8701343CurrentTrain: epoch  7, batch   120 | loss: 46.0429115CurrentTrain: epoch  7, batch   121 | loss: 46.8565080CurrentTrain: epoch  7, batch   122 | loss: 47.4458227CurrentTrain: epoch  7, batch   123 | loss: 50.3891399CurrentTrain: epoch  7, batch   124 | loss: 37.5455215CurrentTrain: epoch  7, batch   125 | loss: 66.6061463CurrentTrain: epoch  7, batch   126 | loss: 47.3981460CurrentTrain: epoch  7, batch   127 | loss: 47.4503295CurrentTrain: epoch  7, batch   128 | loss: 35.8293937CurrentTrain: epoch  7, batch   129 | loss: 66.1514842CurrentTrain: epoch  7, batch   130 | loss: 36.4879201CurrentTrain: epoch  7, batch   131 | loss: 43.9793096CurrentTrain: epoch  7, batch   132 | loss: 46.0490335CurrentTrain: epoch  7, batch   133 | loss: 68.1164568CurrentTrain: epoch  7, batch   134 | loss: 48.9505421CurrentTrain: epoch  7, batch   135 | loss: 43.9987758CurrentTrain: epoch  7, batch   136 | loss: 37.4516155CurrentTrain: epoch  7, batch   137 | loss: 49.6069502CurrentTrain: epoch  7, batch   138 | loss: 66.2447020CurrentTrain: epoch  7, batch   139 | loss: 30.7554753CurrentTrain: epoch  7, batch   140 | loss: 68.0644044CurrentTrain: epoch  7, batch   141 | loss: 108.1609853CurrentTrain: epoch  7, batch   142 | loss: 49.0281552CurrentTrain: epoch  7, batch   143 | loss: 23.2992499CurrentTrain: epoch  8, batch     0 | loss: 29.4379213CurrentTrain: epoch  8, batch     1 | loss: 48.9066976CurrentTrain: epoch  8, batch     2 | loss: 47.3588937CurrentTrain: epoch  8, batch     3 | loss: 47.6368002CurrentTrain: epoch  8, batch     4 | loss: 35.1623761CurrentTrain: epoch  8, batch     5 | loss: 36.2516487CurrentTrain: epoch  8, batch     6 | loss: 36.5214646CurrentTrain: epoch  8, batch     7 | loss: 34.8577112CurrentTrain: epoch  8, batch     8 | loss: 36.6498870CurrentTrain: epoch  8, batch     9 | loss: 45.1578199CurrentTrain: epoch  8, batch    10 | loss: 47.4252380CurrentTrain: epoch  8, batch    11 | loss: 38.7031918CurrentTrain: epoch  8, batch    12 | loss: 106.6721698CurrentTrain: epoch  8, batch    13 | loss: 48.8755312CurrentTrain: epoch  8, batch    14 | loss: 68.3948439CurrentTrain: epoch  8, batch    15 | loss: 35.4036409CurrentTrain: epoch  8, batch    16 | loss: 68.1145843CurrentTrain: epoch  8, batch    17 | loss: 45.0607671CurrentTrain: epoch  8, batch    18 | loss: 68.0739068CurrentTrain: epoch  8, batch    19 | loss: 66.1025243CurrentTrain: epoch  8, batch    20 | loss: 48.8803771CurrentTrain: epoch  8, batch    21 | loss: 50.8721843CurrentTrain: epoch  8, batch    22 | loss: 32.6126839CurrentTrain: epoch  8, batch    23 | loss: 37.5481893CurrentTrain: epoch  8, batch    24 | loss: 46.2949586CurrentTrain: epoch  8, batch    25 | loss: 106.8217603CurrentTrain: epoch  8, batch    26 | loss: 103.4544617CurrentTrain: epoch  8, batch    27 | loss: 34.8494032CurrentTrain: epoch  8, batch    28 | loss: 49.0590086CurrentTrain: epoch  8, batch    29 | loss: 66.4994326CurrentTrain: epoch  8, batch    30 | loss: 35.1738487CurrentTrain: epoch  8, batch    31 | loss: 36.7975372CurrentTrain: epoch  8, batch    32 | loss: 36.3925183CurrentTrain: epoch  8, batch    33 | loss: 106.6813949CurrentTrain: epoch  8, batch    34 | loss: 30.4821359CurrentTrain: epoch  8, batch    35 | loss: 66.2066279CurrentTrain: epoch  8, batch    36 | loss: 47.4467605CurrentTrain: epoch  8, batch    37 | loss: 47.4541531CurrentTrain: epoch  8, batch    38 | loss: 36.4057173CurrentTrain: epoch  8, batch    39 | loss: 68.2193645CurrentTrain: epoch  8, batch    40 | loss: 45.6379667CurrentTrain: epoch  8, batch    41 | loss: 26.1150496CurrentTrain: epoch  8, batch    42 | loss: 37.4208046CurrentTrain: epoch  8, batch    43 | loss: 26.7262848CurrentTrain: epoch  8, batch    44 | loss: 103.4642139CurrentTrain: epoch  8, batch    45 | loss: 106.7882533CurrentTrain: epoch  8, batch    46 | loss: 42.8487708CurrentTrain: epoch  8, batch    47 | loss: 29.2797693CurrentTrain: epoch  8, batch    48 | loss: 48.8960870CurrentTrain: epoch  8, batch    49 | loss: 48.0006612CurrentTrain: epoch  8, batch    50 | loss: 49.1472067CurrentTrain: epoch  8, batch    51 | loss: 37.5850653CurrentTrain: epoch  8, batch    52 | loss: 66.1108790CurrentTrain: epoch  8, batch    53 | loss: 48.8174346CurrentTrain: epoch  8, batch    54 | loss: 45.3235056CurrentTrain: epoch  8, batch    55 | loss: 47.4660506CurrentTrain: epoch  8, batch    56 | loss: 37.8549945CurrentTrain: epoch  8, batch    57 | loss: 35.6460777CurrentTrain: epoch  8, batch    58 | loss: 39.0609257CurrentTrain: epoch  8, batch    59 | loss: 47.6599740CurrentTrain: epoch  8, batch    60 | loss: 47.4420471CurrentTrain: epoch  8, batch    61 | loss: 64.5026963CurrentTrain: epoch  8, batch    62 | loss: 68.0752641CurrentTrain: epoch  8, batch    63 | loss: 35.2314687CurrentTrain: epoch  8, batch    64 | loss: 47.7463495CurrentTrain: epoch  8, batch    65 | loss: 35.8434505CurrentTrain: epoch  8, batch    66 | loss: 37.4840374CurrentTrain: epoch  8, batch    67 | loss: 44.9413640CurrentTrain: epoch  8, batch    68 | loss: 36.8571055CurrentTrain: epoch  8, batch    69 | loss: 48.5434851CurrentTrain: epoch  8, batch    70 | loss: 35.1240472CurrentTrain: epoch  8, batch    71 | loss: 106.7268959CurrentTrain: epoch  8, batch    72 | loss: 64.5133562CurrentTrain: epoch  8, batch    73 | loss: 68.0741586CurrentTrain: epoch  8, batch    74 | loss: 49.6323215CurrentTrain: epoch  8, batch    75 | loss: 46.3258939CurrentTrain: epoch  8, batch    76 | loss: 35.1416986CurrentTrain: epoch  8, batch    77 | loss: 44.9989829CurrentTrain: epoch  8, batch    78 | loss: 68.0814370CurrentTrain: epoch  8, batch    79 | loss: 46.7819532CurrentTrain: epoch  8, batch    80 | loss: 106.7378298CurrentTrain: epoch  8, batch    81 | loss: 36.3410688CurrentTrain: epoch  8, batch    82 | loss: 66.4596011CurrentTrain: epoch  8, batch    83 | loss: 48.8858862CurrentTrain: epoch  8, batch    84 | loss: 46.6553535CurrentTrain: epoch  8, batch    85 | loss: 103.5149168CurrentTrain: epoch  8, batch    86 | loss: 34.5545170CurrentTrain: epoch  8, batch    87 | loss: 40.1279471CurrentTrain: epoch  8, batch    88 | loss: 47.4611862CurrentTrain: epoch  8, batch    89 | loss: 68.0527625CurrentTrain: epoch  8, batch    90 | loss: 49.0436754CurrentTrain: epoch  8, batch    91 | loss: 64.6424614CurrentTrain: epoch  8, batch    92 | loss: 35.1799484CurrentTrain: epoch  8, batch    93 | loss: 36.3752142CurrentTrain: epoch  8, batch    94 | loss: 47.4491489CurrentTrain: epoch  8, batch    95 | loss: 103.4608502CurrentTrain: epoch  8, batch    96 | loss: 47.4541480CurrentTrain: epoch  8, batch    97 | loss: 46.2349555CurrentTrain: epoch  8, batch    98 | loss: 73.6871371CurrentTrain: epoch  8, batch    99 | loss: 47.5231861CurrentTrain: epoch  8, batch   100 | loss: 222.0810527CurrentTrain: epoch  8, batch   101 | loss: 66.2586764CurrentTrain: epoch  8, batch   102 | loss: 36.4009379CurrentTrain: epoch  8, batch   103 | loss: 35.1930309CurrentTrain: epoch  8, batch   104 | loss: 66.1548480CurrentTrain: epoch  8, batch   105 | loss: 222.1822169CurrentTrain: epoch  8, batch   106 | loss: 38.4176378CurrentTrain: epoch  8, batch   107 | loss: 64.7785602CurrentTrain: epoch  8, batch   108 | loss: 43.4738387CurrentTrain: epoch  8, batch   109 | loss: 45.9804240CurrentTrain: epoch  8, batch   110 | loss: 103.8353071CurrentTrain: epoch  8, batch   111 | loss: 30.1383775CurrentTrain: epoch  8, batch   112 | loss: 48.9391689CurrentTrain: epoch  8, batch   113 | loss: 46.0814601CurrentTrain: epoch  8, batch   114 | loss: 38.3633694CurrentTrain: epoch  8, batch   115 | loss: 106.7784293CurrentTrain: epoch  8, batch   116 | loss: 37.1376068CurrentTrain: epoch  8, batch   117 | loss: 38.5552556CurrentTrain: epoch  8, batch   118 | loss: 68.0754751CurrentTrain: epoch  8, batch   119 | loss: 46.2200613CurrentTrain: epoch  8, batch   120 | loss: 36.6592254CurrentTrain: epoch  8, batch   121 | loss: 48.9810912CurrentTrain: epoch  8, batch   122 | loss: 48.9687661CurrentTrain: epoch  8, batch   123 | loss: 28.8659121CurrentTrain: epoch  8, batch   124 | loss: 33.0009810CurrentTrain: epoch  8, batch   125 | loss: 47.4931065CurrentTrain: epoch  8, batch   126 | loss: 39.8405829CurrentTrain: epoch  8, batch   127 | loss: 46.4236279CurrentTrain: epoch  8, batch   128 | loss: 68.0817069CurrentTrain: epoch  8, batch   129 | loss: 46.6772952CurrentTrain: epoch  8, batch   130 | loss: 36.6847342CurrentTrain: epoch  8, batch   131 | loss: 66.0813211CurrentTrain: epoch  8, batch   132 | loss: 46.5611463CurrentTrain: epoch  8, batch   133 | loss: 66.1193422CurrentTrain: epoch  8, batch   134 | loss: 46.0910804CurrentTrain: epoch  8, batch   135 | loss: 48.9700719CurrentTrain: epoch  8, batch   136 | loss: 106.8136397CurrentTrain: epoch  8, batch   137 | loss: 48.8722194CurrentTrain: epoch  8, batch   138 | loss: 35.1527563CurrentTrain: epoch  8, batch   139 | loss: 68.0651927CurrentTrain: epoch  8, batch   140 | loss: 37.3836281CurrentTrain: epoch  8, batch   141 | loss: 66.1034154CurrentTrain: epoch  8, batch   142 | loss: 106.7657600CurrentTrain: epoch  8, batch   143 | loss: 77.7042476CurrentTrain: epoch  9, batch     0 | loss: 66.1104389CurrentTrain: epoch  9, batch     1 | loss: 28.8454113CurrentTrain: epoch  9, batch     2 | loss: 66.1540692CurrentTrain: epoch  9, batch     3 | loss: 36.4211811CurrentTrain: epoch  9, batch     4 | loss: 27.9562036CurrentTrain: epoch  9, batch     5 | loss: 47.9486661CurrentTrain: epoch  9, batch     6 | loss: 43.6237313CurrentTrain: epoch  9, batch     7 | loss: 28.1379169CurrentTrain: epoch  9, batch     8 | loss: 47.4465656CurrentTrain: epoch  9, batch     9 | loss: 48.9917547CurrentTrain: epoch  9, batch    10 | loss: 34.6317263CurrentTrain: epoch  9, batch    11 | loss: 47.4374578CurrentTrain: epoch  9, batch    12 | loss: 37.0816869CurrentTrain: epoch  9, batch    13 | loss: 36.4475594CurrentTrain: epoch  9, batch    14 | loss: 29.0880393CurrentTrain: epoch  9, batch    15 | loss: 67.5409986CurrentTrain: epoch  9, batch    16 | loss: 63.8128797CurrentTrain: epoch  9, batch    17 | loss: 28.3088516CurrentTrain: epoch  9, batch    18 | loss: 48.8798663CurrentTrain: epoch  9, batch    19 | loss: 36.6490382CurrentTrain: epoch  9, batch    20 | loss: 45.7893707CurrentTrain: epoch  9, batch    21 | loss: 64.2165763CurrentTrain: epoch  9, batch    22 | loss: 30.1480996CurrentTrain: epoch  9, batch    23 | loss: 68.1079593CurrentTrain: epoch  9, batch    24 | loss: 36.3967879CurrentTrain: epoch  9, batch    25 | loss: 47.4787045CurrentTrain: epoch  9, batch    26 | loss: 68.0804660CurrentTrain: epoch  9, batch    27 | loss: 36.0315023CurrentTrain: epoch  9, batch    28 | loss: 68.1006789CurrentTrain: epoch  9, batch    29 | loss: 34.2625547CurrentTrain: epoch  9, batch    30 | loss: 36.1028105CurrentTrain: epoch  9, batch    31 | loss: 36.3761037CurrentTrain: epoch  9, batch    32 | loss: 37.4859253CurrentTrain: epoch  9, batch    33 | loss: 68.0803356CurrentTrain: epoch  9, batch    34 | loss: 45.1038767CurrentTrain: epoch  9, batch    35 | loss: 106.7144118CurrentTrain: epoch  9, batch    36 | loss: 47.6761390CurrentTrain: epoch  9, batch    37 | loss: 36.3728835CurrentTrain: epoch  9, batch    38 | loss: 35.3460527CurrentTrain: epoch  9, batch    39 | loss: 68.1120332CurrentTrain: epoch  9, batch    40 | loss: 34.4748962CurrentTrain: epoch  9, batch    41 | loss: 68.4005639CurrentTrain: epoch  9, batch    42 | loss: 37.5219901CurrentTrain: epoch  9, batch    43 | loss: 64.4810129CurrentTrain: epoch  9, batch    44 | loss: 49.5395405CurrentTrain: epoch  9, batch    45 | loss: 106.6873953CurrentTrain: epoch  9, batch    46 | loss: 47.4042627CurrentTrain: epoch  9, batch    47 | loss: 68.0506664CurrentTrain: epoch  9, batch    48 | loss: 48.8778621CurrentTrain: epoch  9, batch    49 | loss: 68.1067057CurrentTrain: epoch  9, batch    50 | loss: 68.0774510CurrentTrain: epoch  9, batch    51 | loss: 35.1704853CurrentTrain: epoch  9, batch    52 | loss: 68.3585981CurrentTrain: epoch  9, batch    53 | loss: 34.2887509CurrentTrain: epoch  9, batch    54 | loss: 31.5199909CurrentTrain: epoch  9, batch    55 | loss: 66.1943044CurrentTrain: epoch  9, batch    56 | loss: 47.5306625CurrentTrain: epoch  9, batch    57 | loss: 46.0716922CurrentTrain: epoch  9, batch    58 | loss: 45.4919746CurrentTrain: epoch  9, batch    59 | loss: 36.2547160CurrentTrain: epoch  9, batch    60 | loss: 68.5731426CurrentTrain: epoch  9, batch    61 | loss: 50.5387305CurrentTrain: epoch  9, batch    62 | loss: 36.3043491CurrentTrain: epoch  9, batch    63 | loss: 29.9051392CurrentTrain: epoch  9, batch    64 | loss: 48.8848937CurrentTrain: epoch  9, batch    65 | loss: 106.7582725CurrentTrain: epoch  9, batch    66 | loss: 36.2311888CurrentTrain: epoch  9, batch    67 | loss: 68.0586501CurrentTrain: epoch  9, batch    68 | loss: 68.3832561CurrentTrain: epoch  9, batch    69 | loss: 68.1560886CurrentTrain: epoch  9, batch    70 | loss: 48.9693551CurrentTrain: epoch  9, batch    71 | loss: 45.9939256CurrentTrain: epoch  9, batch    72 | loss: 37.4049629CurrentTrain: epoch  9, batch    73 | loss: 47.6694809CurrentTrain: epoch  9, batch    74 | loss: 37.3874474CurrentTrain: epoch  9, batch    75 | loss: 103.4477975CurrentTrain: epoch  9, batch    76 | loss: 66.6286227CurrentTrain: epoch  9, batch    77 | loss: 28.1911159CurrentTrain: epoch  9, batch    78 | loss: 66.8752831CurrentTrain: epoch  9, batch    79 | loss: 46.2608688CurrentTrain: epoch  9, batch    80 | loss: 47.3946113CurrentTrain: epoch  9, batch    81 | loss: 23.1603313CurrentTrain: epoch  9, batch    82 | loss: 46.1870328CurrentTrain: epoch  9, batch    83 | loss: 37.6781814CurrentTrain: epoch  9, batch    84 | loss: 25.9760192CurrentTrain: epoch  9, batch    85 | loss: 106.7244378CurrentTrain: epoch  9, batch    86 | loss: 68.7564233CurrentTrain: epoch  9, batch    87 | loss: 37.1454154CurrentTrain: epoch  9, batch    88 | loss: 47.4253302CurrentTrain: epoch  9, batch    89 | loss: 68.1386459CurrentTrain: epoch  9, batch    90 | loss: 45.9776214CurrentTrain: epoch  9, batch    91 | loss: 50.1223856CurrentTrain: epoch  9, batch    92 | loss: 103.4725224CurrentTrain: epoch  9, batch    93 | loss: 68.1283718CurrentTrain: epoch  9, batch    94 | loss: 66.1516503CurrentTrain: epoch  9, batch    95 | loss: 64.2684938CurrentTrain: epoch  9, batch    96 | loss: 66.1770952CurrentTrain: epoch  9, batch    97 | loss: 24.7733468CurrentTrain: epoch  9, batch    98 | loss: 106.8780987CurrentTrain: epoch  9, batch    99 | loss: 49.6090820CurrentTrain: epoch  9, batch   100 | loss: 29.3304085CurrentTrain: epoch  9, batch   101 | loss: 46.1004244CurrentTrain: epoch  9, batch   102 | loss: 44.2372225CurrentTrain: epoch  9, batch   103 | loss: 28.9108318CurrentTrain: epoch  9, batch   104 | loss: 66.2034850CurrentTrain: epoch  9, batch   105 | loss: 107.6572624CurrentTrain: epoch  9, batch   106 | loss: 28.8449332CurrentTrain: epoch  9, batch   107 | loss: 66.1561839CurrentTrain: epoch  9, batch   108 | loss: 47.9569210CurrentTrain: epoch  9, batch   109 | loss: 35.4017433CurrentTrain: epoch  9, batch   110 | loss: 29.7825590CurrentTrain: epoch  9, batch   111 | loss: 66.3190828CurrentTrain: epoch  9, batch   112 | loss: 37.6251751CurrentTrain: epoch  9, batch   113 | loss: 47.0936845CurrentTrain: epoch  9, batch   114 | loss: 106.7607379CurrentTrain: epoch  9, batch   115 | loss: 48.8736094CurrentTrain: epoch  9, batch   116 | loss: 68.1137359CurrentTrain: epoch  9, batch   117 | loss: 42.1621591CurrentTrain: epoch  9, batch   118 | loss: 68.0607026CurrentTrain: epoch  9, batch   119 | loss: 34.2529188CurrentTrain: epoch  9, batch   120 | loss: 31.0959948CurrentTrain: epoch  9, batch   121 | loss: 99.3630534CurrentTrain: epoch  9, batch   122 | loss: 43.4223647CurrentTrain: epoch  9, batch   123 | loss: 46.1131731CurrentTrain: epoch  9, batch   124 | loss: 68.0828071CurrentTrain: epoch  9, batch   125 | loss: 38.5571156CurrentTrain: epoch  9, batch   126 | loss: 66.3285961CurrentTrain: epoch  9, batch   127 | loss: 68.0722717CurrentTrain: epoch  9, batch   128 | loss: 68.0807653CurrentTrain: epoch  9, batch   129 | loss: 47.4622727CurrentTrain: epoch  9, batch   130 | loss: 48.9904189CurrentTrain: epoch  9, batch   131 | loss: 68.0797723CurrentTrain: epoch  9, batch   132 | loss: 106.7987925CurrentTrain: epoch  9, batch   133 | loss: 48.9741034CurrentTrain: epoch  9, batch   134 | loss: 47.4589928CurrentTrain: epoch  9, batch   135 | loss: 23.0301486CurrentTrain: epoch  9, batch   136 | loss: 47.4119438CurrentTrain: epoch  9, batch   137 | loss: 36.2963250CurrentTrain: epoch  9, batch   138 | loss: 48.8838710CurrentTrain: epoch  9, batch   139 | loss: 29.8179235CurrentTrain: epoch  9, batch   140 | loss: 47.4932131CurrentTrain: epoch  9, batch   141 | loss: 36.3683193CurrentTrain: epoch  9, batch   142 | loss: 49.1115935CurrentTrain: epoch  9, batch   143 | loss: 46.4466504

F1 score per class: {32: 0.5, 6: 0.6818181818181818, 19: 0.13636363636363635, 24: 0.6831683168316832, 26: 0.8955223880597015, 29: 0.776255707762557}
Micro-average F1 score: 0.6837294332723949
Weighted-average F1 score: 0.6803281164900038
F1 score per class: {32: 0.5034965034965035, 6: 0.6612903225806451, 19: 0.13636363636363635, 24: 0.7150259067357513, 26: 0.8981481481481481, 29: 0.639344262295082}
Micro-average F1 score: 0.6171341925701289
Weighted-average F1 score: 0.5865355058924364
F1 score per class: {32: 0.5052631578947369, 6: 0.6612903225806451, 19: 0.1487603305785124, 24: 0.7150259067357513, 26: 0.8981481481481481, 29: 0.642570281124498}
Micro-average F1 score: 0.6234756097560976
Weighted-average F1 score: 0.5958344236791043

F1 score per class: {32: 0.5, 6: 0.6818181818181818, 19: 0.13636363636363635, 24: 0.6831683168316832, 26: 0.8955223880597015, 29: 0.776255707762557}
Micro-average F1 score: 0.6837294332723949
Weighted-average F1 score: 0.6803281164900038
F1 score per class: {32: 0.5034965034965035, 6: 0.6612903225806451, 19: 0.13636363636363635, 24: 0.7150259067357513, 26: 0.8981481481481481, 29: 0.639344262295082}
Micro-average F1 score: 0.6171341925701289
Weighted-average F1 score: 0.5865355058924364
F1 score per class: {32: 0.5052631578947369, 6: 0.6612903225806451, 19: 0.1487603305785124, 24: 0.7150259067357513, 26: 0.8981481481481481, 29: 0.642570281124498}
Micro-average F1 score: 0.6234756097560976
Weighted-average F1 score: 0.5958344236791043
cur_acc:  ['0.6837']
his_acc:  ['0.6837']
cur_acc des:  ['0.6171']
his_acc des:  ['0.6171']
cur_acc rrf:  ['0.6235']
his_acc rrf:  ['0.6235']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 120.5010573CurrentTrain: epoch  0, batch     1 | loss: 62.0554830CurrentTrain: epoch  0, batch     2 | loss: 49.0300305CurrentTrain: epoch  0, batch     3 | loss: 57.7107661CurrentTrain: epoch  0, batch     4 | loss: 37.9863282CurrentTrain: epoch  1, batch     0 | loss: 45.8410183CurrentTrain: epoch  1, batch     1 | loss: 46.5756392CurrentTrain: epoch  1, batch     2 | loss: 34.1933242CurrentTrain: epoch  1, batch     3 | loss: 54.0545472CurrentTrain: epoch  1, batch     4 | loss: 69.7689046CurrentTrain: epoch  2, batch     0 | loss: 42.2256303CurrentTrain: epoch  2, batch     1 | loss: 52.1218196CurrentTrain: epoch  2, batch     2 | loss: 44.2072642CurrentTrain: epoch  2, batch     3 | loss: 34.2993329CurrentTrain: epoch  2, batch     4 | loss: 43.7376983CurrentTrain: epoch  3, batch     0 | loss: 54.8233502CurrentTrain: epoch  3, batch     1 | loss: 31.0980416CurrentTrain: epoch  3, batch     2 | loss: 42.8655413CurrentTrain: epoch  3, batch     3 | loss: 68.0365252CurrentTrain: epoch  3, batch     4 | loss: 24.1524364CurrentTrain: epoch  4, batch     0 | loss: 40.2768620CurrentTrain: epoch  4, batch     1 | loss: 32.2344101CurrentTrain: epoch  4, batch     2 | loss: 50.4428097CurrentTrain: epoch  4, batch     3 | loss: 52.0788396CurrentTrain: epoch  4, batch     4 | loss: 43.0855294CurrentTrain: epoch  5, batch     0 | loss: 38.3213182CurrentTrain: epoch  5, batch     1 | loss: 46.6962825CurrentTrain: epoch  5, batch     2 | loss: 68.3801614CurrentTrain: epoch  5, batch     3 | loss: 67.9811049CurrentTrain: epoch  5, batch     4 | loss: 40.1270200CurrentTrain: epoch  6, batch     0 | loss: 31.5656037CurrentTrain: epoch  6, batch     1 | loss: 29.7685903CurrentTrain: epoch  6, batch     2 | loss: 39.1906941CurrentTrain: epoch  6, batch     3 | loss: 37.5000667CurrentTrain: epoch  6, batch     4 | loss: 43.9908978CurrentTrain: epoch  7, batch     0 | loss: 38.2300989CurrentTrain: epoch  7, batch     1 | loss: 50.9437566CurrentTrain: epoch  7, batch     2 | loss: 45.9324142CurrentTrain: epoch  7, batch     3 | loss: 28.5626268CurrentTrain: epoch  7, batch     4 | loss: 42.9607988CurrentTrain: epoch  8, batch     0 | loss: 28.6919562CurrentTrain: epoch  8, batch     1 | loss: 62.4383643CurrentTrain: epoch  8, batch     2 | loss: 68.4399182CurrentTrain: epoch  8, batch     3 | loss: 35.7929602CurrentTrain: epoch  8, batch     4 | loss: 22.6475087CurrentTrain: epoch  9, batch     0 | loss: 36.5656380CurrentTrain: epoch  9, batch     1 | loss: 37.1645564CurrentTrain: epoch  9, batch     2 | loss: 29.4522175CurrentTrain: epoch  9, batch     3 | loss: 47.6177894CurrentTrain: epoch  9, batch     4 | loss: 41.2194095
MemoryTrain:  epoch  0, batch     0 | loss: 0.7479825MemoryTrain:  epoch  1, batch     0 | loss: 0.5431554MemoryTrain:  epoch  2, batch     0 | loss: 0.4419956MemoryTrain:  epoch  3, batch     0 | loss: 0.3966903MemoryTrain:  epoch  4, batch     0 | loss: 0.4439719MemoryTrain:  epoch  5, batch     0 | loss: 0.2788471MemoryTrain:  epoch  6, batch     0 | loss: 0.2010364MemoryTrain:  epoch  7, batch     0 | loss: 0.2128021MemoryTrain:  epoch  8, batch     0 | loss: 0.1261571MemoryTrain:  epoch  9, batch     0 | loss: 0.1030056

F1 score per class: {32: 0.0, 6: 0.25, 7: 0.9166666666666666, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.46153846153846156, 26: 0.4, 27: 0.0, 31: 0.4594594594594595}
Micro-average F1 score: 0.45390070921985815
Weighted-average F1 score: 0.3861513299013299
F1 score per class: {32: 0.0, 6: 0.36363636363636365, 7: 0.847457627118644, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.34285714285714286, 26: 0.0, 27: 0.23529411764705882, 29: 0.0, 31: 0.3487179487179487}
Micro-average F1 score: 0.35384615384615387
Weighted-average F1 score: 0.3098093053314346
F1 score per class: {32: 0.0, 6: 0.3333333333333333, 7: 0.8620689655172413, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.3783783783783784, 26: 0.0, 27: 0.23529411764705882, 29: 0.0, 31: 0.3487179487179487}
Micro-average F1 score: 0.36175710594315247
Weighted-average F1 score: 0.3180117676294927

F1 score per class: {32: 0.36942675159235666, 6: 0.02040816326530612, 7: 0.9166666666666666, 40: 0.5637065637065637, 9: 0.2, 19: 0.7127659574468085, 24: 0.17391304347826086, 26: 0.8944723618090452, 27: 0.16666666666666666, 29: 0.7884615384615384, 31: 0.35789473684210527}
Micro-average F1 score: 0.5582990397805213
Weighted-average F1 score: 0.5165257427904298
F1 score per class: {32: 0.4200913242009132, 6: 0.03418803418803419, 7: 0.847457627118644, 40: 0.556390977443609, 9: 0.1643835616438356, 19: 0.6766169154228856, 24: 0.1111111111111111, 26: 0.8888888888888888, 27: 0.07407407407407407, 29: 0.6153846153846154, 31: 0.20606060606060606}
Micro-average F1 score: 0.46242171189979125
Weighted-average F1 score: 0.41237871610247745
F1 score per class: {32: 0.39603960396039606, 6: 0.030534351145038167, 7: 0.8620689655172413, 40: 0.556390977443609, 9: 0.1568627450980392, 19: 0.6871794871794872, 24: 0.11864406779661017, 26: 0.8930232558139535, 27: 0.07407407407407407, 29: 0.6181818181818182, 31: 0.2073170731707317}
Micro-average F1 score: 0.4606444796619123
Weighted-average F1 score: 0.4089931875226182
cur_acc:  ['0.6837', '0.4539']
his_acc:  ['0.6837', '0.5583']
cur_acc des:  ['0.6171', '0.3538']
his_acc des:  ['0.6171', '0.4624']
cur_acc rrf:  ['0.6235', '0.3618']
his_acc rrf:  ['0.6235', '0.4606']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 58.2461883CurrentTrain: epoch  0, batch     1 | loss: 48.9935470CurrentTrain: epoch  0, batch     2 | loss: 78.1823918CurrentTrain: epoch  0, batch     3 | loss: 110.8329182CurrentTrain: epoch  0, batch     4 | loss: 45.6202497CurrentTrain: epoch  0, batch     5 | loss: 37.3529545CurrentTrain: epoch  1, batch     0 | loss: 68.8074109CurrentTrain: epoch  1, batch     1 | loss: 57.5498663CurrentTrain: epoch  1, batch     2 | loss: 75.4758694CurrentTrain: epoch  1, batch     3 | loss: 56.2298716CurrentTrain: epoch  1, batch     4 | loss: 40.8815773CurrentTrain: epoch  1, batch     5 | loss: 53.1489542CurrentTrain: epoch  2, batch     0 | loss: 113.9483678CurrentTrain: epoch  2, batch     1 | loss: 49.8081884CurrentTrain: epoch  2, batch     2 | loss: 56.3243909CurrentTrain: epoch  2, batch     3 | loss: 33.8110642CurrentTrain: epoch  2, batch     4 | loss: 52.5697605CurrentTrain: epoch  2, batch     5 | loss: 31.1870112CurrentTrain: epoch  3, batch     0 | loss: 51.4106372CurrentTrain: epoch  3, batch     1 | loss: 50.9126975CurrentTrain: epoch  3, batch     2 | loss: 41.9847303CurrentTrain: epoch  3, batch     3 | loss: 48.6812213CurrentTrain: epoch  3, batch     4 | loss: 53.3383266CurrentTrain: epoch  3, batch     5 | loss: 79.6603266CurrentTrain: epoch  4, batch     0 | loss: 39.9932979CurrentTrain: epoch  4, batch     1 | loss: 38.9486241CurrentTrain: epoch  4, batch     2 | loss: 43.3082392CurrentTrain: epoch  4, batch     3 | loss: 47.7866097CurrentTrain: epoch  4, batch     4 | loss: 68.6254627CurrentTrain: epoch  4, batch     5 | loss: 49.8738724CurrentTrain: epoch  5, batch     0 | loss: 47.2520726CurrentTrain: epoch  5, batch     1 | loss: 39.0174970CurrentTrain: epoch  5, batch     2 | loss: 107.4707025CurrentTrain: epoch  5, batch     3 | loss: 72.1762273CurrentTrain: epoch  5, batch     4 | loss: 38.1996763CurrentTrain: epoch  5, batch     5 | loss: 34.4152831CurrentTrain: epoch  6, batch     0 | loss: 46.7756322CurrentTrain: epoch  6, batch     1 | loss: 48.0904529CurrentTrain: epoch  6, batch     2 | loss: 52.0052828CurrentTrain: epoch  6, batch     3 | loss: 36.5376940CurrentTrain: epoch  6, batch     4 | loss: 107.0130646CurrentTrain: epoch  6, batch     5 | loss: 48.4383530CurrentTrain: epoch  7, batch     0 | loss: 50.9451743CurrentTrain: epoch  7, batch     1 | loss: 66.2336798CurrentTrain: epoch  7, batch     2 | loss: 36.6634621CurrentTrain: epoch  7, batch     3 | loss: 106.9342330CurrentTrain: epoch  7, batch     4 | loss: 35.3490224CurrentTrain: epoch  7, batch     5 | loss: 49.0075988CurrentTrain: epoch  8, batch     0 | loss: 70.3307872CurrentTrain: epoch  8, batch     1 | loss: 69.1185233CurrentTrain: epoch  8, batch     2 | loss: 36.8554685CurrentTrain: epoch  8, batch     3 | loss: 47.6454498CurrentTrain: epoch  8, batch     4 | loss: 45.9095749CurrentTrain: epoch  8, batch     5 | loss: 26.2458164CurrentTrain: epoch  9, batch     0 | loss: 28.4136176CurrentTrain: epoch  9, batch     1 | loss: 106.8942209CurrentTrain: epoch  9, batch     2 | loss: 66.4935359CurrentTrain: epoch  9, batch     3 | loss: 39.8163361CurrentTrain: epoch  9, batch     4 | loss: 36.4020588CurrentTrain: epoch  9, batch     5 | loss: 33.6751653
MemoryTrain:  epoch  0, batch     0 | loss: 0.5988520MemoryTrain:  epoch  1, batch     0 | loss: 0.4734012MemoryTrain:  epoch  2, batch     0 | loss: 0.3077847MemoryTrain:  epoch  3, batch     0 | loss: 0.2662237MemoryTrain:  epoch  4, batch     0 | loss: 0.2021209MemoryTrain:  epoch  5, batch     0 | loss: 0.1640364MemoryTrain:  epoch  6, batch     0 | loss: 0.1626213MemoryTrain:  epoch  7, batch     0 | loss: 0.1096777MemoryTrain:  epoch  8, batch     0 | loss: 0.0842742MemoryTrain:  epoch  9, batch     0 | loss: 0.0673841

F1 score per class: {0: 0.8529411764705882, 32: 0.9637305699481865, 4: 0.0, 6: 0.0, 7: 0.029411764705882353, 40: 0.0, 13: 0.35514018691588783, 19: 0.6944444444444444, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5604026845637584
Weighted-average F1 score: 0.4315766648110605
F1 score per class: {0: 0.64, 32: 0.9705882352941176, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.04878048780487805, 9: 0.0, 13: 0.32558139534883723, 19: 0.6862745098039216, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.4621026894865526
Weighted-average F1 score: 0.35897884417218323
F1 score per class: {0: 0.7654320987654321, 32: 0.9949748743718593, 4: 0.0, 6: 0.0, 7: 0.044444444444444446, 40: 0.0, 13: 0.3157894736842105, 19: 0.6966292134831461, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.48484848484848486
Weighted-average F1 score: 0.36628618918061684

F1 score per class: {32: 0.6236559139784946, 0: 0.9587628865979382, 4: 0.3850267379679144, 6: 0.0, 7: 0.9387755102040817, 40: 0.014705882352941176, 9: 0.5194805194805194, 13: 0.17511520737327188, 19: 0.6578947368421053, 21: 0.07692307692307693, 23: 0.676923076923077, 24: 0.13953488372093023, 26: 0.8476190476190476, 27: 0.04081632653061224, 29: 0.7378640776699029, 31: 0.3902439024390244}
Micro-average F1 score: 0.5199436884091976
Weighted-average F1 score: 0.45481547531495875
F1 score per class: {32: 0.40764331210191085, 0: 0.9473684210526315, 4: 0.3669064748201439, 6: 0.0, 7: 0.7936507936507936, 40: 0.02072538860103627, 9: 0.5421245421245421, 13: 0.16153846153846155, 19: 0.6194690265486725, 21: 0.07407407407407407, 23: 0.6839378238341969, 24: 0.12371134020618557, 26: 0.8296943231441049, 27: 0.036036036036036036, 29: 0.5046728971962616, 31: 0.2357142857142857}
Micro-average F1 score: 0.4359692092372288
Weighted-average F1 score: 0.383620758148045
F1 score per class: {32: 0.512396694214876, 0: 0.9801980198019802, 4: 0.36065573770491804, 6: 0.0, 7: 0.9056603773584906, 40: 0.018867924528301886, 9: 0.5173745173745173, 13: 0.15384615384615385, 19: 0.6138613861386139, 21: 0.08333333333333333, 23: 0.6839378238341969, 24: 0.08823529411764706, 26: 0.8310502283105022, 27: 0.041666666666666664, 29: 0.5625, 31: 0.2591093117408907}
Micro-average F1 score: 0.44854881266490765
Weighted-average F1 score: 0.38859391388366843
cur_acc:  ['0.6837', '0.4539', '0.5604']
his_acc:  ['0.6837', '0.5583', '0.5199']
cur_acc des:  ['0.6171', '0.3538', '0.4621']
his_acc des:  ['0.6171', '0.4624', '0.4360']
cur_acc rrf:  ['0.6235', '0.3618', '0.4848']
his_acc rrf:  ['0.6235', '0.4606', '0.4485']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 59.7137354CurrentTrain: epoch  0, batch     1 | loss: 44.5576110CurrentTrain: epoch  0, batch     2 | loss: 59.5591181CurrentTrain: epoch  0, batch     3 | loss: 58.9535344CurrentTrain: epoch  0, batch     4 | loss: 116.8432515CurrentTrain: epoch  0, batch     5 | loss: 53.4573673CurrentTrain: epoch  0, batch     6 | loss: 73.8029773CurrentTrain: epoch  0, batch     7 | loss: 3.0483476CurrentTrain: epoch  1, batch     0 | loss: 73.5052226CurrentTrain: epoch  1, batch     1 | loss: 73.7397476CurrentTrain: epoch  1, batch     2 | loss: 113.0929645CurrentTrain: epoch  1, batch     3 | loss: 51.6217821CurrentTrain: epoch  1, batch     4 | loss: 73.8368540CurrentTrain: epoch  1, batch     5 | loss: 38.3680950CurrentTrain: epoch  1, batch     6 | loss: 40.9494010CurrentTrain: epoch  1, batch     7 | loss: 2.8467046CurrentTrain: epoch  2, batch     0 | loss: 39.7855804CurrentTrain: epoch  2, batch     1 | loss: 34.1633900CurrentTrain: epoch  2, batch     2 | loss: 71.7273316CurrentTrain: epoch  2, batch     3 | loss: 40.4184903CurrentTrain: epoch  2, batch     4 | loss: 70.0195083CurrentTrain: epoch  2, batch     5 | loss: 50.7211759CurrentTrain: epoch  2, batch     6 | loss: 70.6080144CurrentTrain: epoch  2, batch     7 | loss: 2.8716617CurrentTrain: epoch  3, batch     0 | loss: 50.7996027CurrentTrain: epoch  3, batch     1 | loss: 51.1587338CurrentTrain: epoch  3, batch     2 | loss: 49.8847426CurrentTrain: epoch  3, batch     3 | loss: 68.9897571CurrentTrain: epoch  3, batch     4 | loss: 39.4350751CurrentTrain: epoch  3, batch     5 | loss: 68.4859360CurrentTrain: epoch  3, batch     6 | loss: 104.6357084CurrentTrain: epoch  3, batch     7 | loss: 2.8501538CurrentTrain: epoch  4, batch     0 | loss: 67.1222833CurrentTrain: epoch  4, batch     1 | loss: 66.8739593CurrentTrain: epoch  4, batch     2 | loss: 110.3899387CurrentTrain: epoch  4, batch     3 | loss: 107.6478322CurrentTrain: epoch  4, batch     4 | loss: 37.8347900CurrentTrain: epoch  4, batch     5 | loss: 65.2618628CurrentTrain: epoch  4, batch     6 | loss: 46.9659993CurrentTrain: epoch  4, batch     7 | loss: 1.0508921CurrentTrain: epoch  5, batch     0 | loss: 66.6673467CurrentTrain: epoch  5, batch     1 | loss: 68.3686046CurrentTrain: epoch  5, batch     2 | loss: 29.7189295CurrentTrain: epoch  5, batch     3 | loss: 68.8954810CurrentTrain: epoch  5, batch     4 | loss: 38.3056290CurrentTrain: epoch  5, batch     5 | loss: 104.3070873CurrentTrain: epoch  5, batch     6 | loss: 50.0388757CurrentTrain: epoch  5, batch     7 | loss: 2.8199722CurrentTrain: epoch  6, batch     0 | loss: 49.4159463CurrentTrain: epoch  6, batch     1 | loss: 50.4267624CurrentTrain: epoch  6, batch     2 | loss: 48.1597509CurrentTrain: epoch  6, batch     3 | loss: 47.8731027CurrentTrain: epoch  6, batch     4 | loss: 47.9645719CurrentTrain: epoch  6, batch     5 | loss: 46.6319045CurrentTrain: epoch  6, batch     6 | loss: 107.4515221CurrentTrain: epoch  6, batch     7 | loss: 2.8306081CurrentTrain: epoch  7, batch     0 | loss: 49.3924102CurrentTrain: epoch  7, batch     1 | loss: 68.6382351CurrentTrain: epoch  7, batch     2 | loss: 68.3578743CurrentTrain: epoch  7, batch     3 | loss: 37.6363846CurrentTrain: epoch  7, batch     4 | loss: 46.2383538CurrentTrain: epoch  7, batch     5 | loss: 35.7127098CurrentTrain: epoch  7, batch     6 | loss: 67.8403634CurrentTrain: epoch  7, batch     7 | loss: 2.8171953CurrentTrain: epoch  8, batch     0 | loss: 68.3627101CurrentTrain: epoch  8, batch     1 | loss: 50.2360655CurrentTrain: epoch  8, batch     2 | loss: 29.2906827CurrentTrain: epoch  8, batch     3 | loss: 35.6708009CurrentTrain: epoch  8, batch     4 | loss: 107.0044785CurrentTrain: epoch  8, batch     5 | loss: 50.2134098CurrentTrain: epoch  8, batch     6 | loss: 37.6850155CurrentTrain: epoch  8, batch     7 | loss: 2.8175570CurrentTrain: epoch  9, batch     0 | loss: 37.6656552CurrentTrain: epoch  9, batch     1 | loss: 68.4388610CurrentTrain: epoch  9, batch     2 | loss: 46.6989833CurrentTrain: epoch  9, batch     3 | loss: 48.9802237CurrentTrain: epoch  9, batch     4 | loss: 66.3290913CurrentTrain: epoch  9, batch     5 | loss: 47.6477221CurrentTrain: epoch  9, batch     6 | loss: 37.4913263CurrentTrain: epoch  9, batch     7 | loss: 2.8337169
MemoryTrain:  epoch  0, batch     0 | loss: 0.2482367MemoryTrain:  epoch  1, batch     0 | loss: 0.2076308MemoryTrain:  epoch  2, batch     0 | loss: 0.1555304MemoryTrain:  epoch  3, batch     0 | loss: 0.1113798MemoryTrain:  epoch  4, batch     0 | loss: 0.0957027MemoryTrain:  epoch  5, batch     0 | loss: 0.0804264MemoryTrain:  epoch  6, batch     0 | loss: 0.0661733MemoryTrain:  epoch  7, batch     0 | loss: 0.0516010MemoryTrain:  epoch  8, batch     0 | loss: 0.0467067MemoryTrain:  epoch  9, batch     0 | loss: 0.0464093

F1 score per class: {32: 0.0, 4: 0.934010152284264, 5: 0.0, 6: 0.0, 7: 0.4258064516129032, 40: 0.0, 10: 0.5675675675675675, 13: 0.3157894736842105, 16: 0.3025210084033613, 17: 0.0, 18: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5335463258785943
Weighted-average F1 score: 0.4770169428833542
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.5885885885885885, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.5384615384615384, 13: 0.0, 16: 0.5094339622641509, 17: 0.358974358974359, 18: 0.1978798586572438, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.35791217895608946
Weighted-average F1 score: 0.3152090702749468
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.5903614457831325, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.5488372093023256, 13: 0.0, 16: 0.4954128440366973, 17: 0.3333333333333333, 18: 0.1978798586572438, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.36622073578595316
Weighted-average F1 score: 0.32488716782612526

F1 score per class: {0: 0.6391752577319587, 4: 0.9381443298969072, 5: 0.934010152284264, 6: 0.28402366863905326, 7: 0.0, 9: 0.9230769230769231, 10: 0.36464088397790057, 13: 0.04, 16: 0.4883720930232558, 17: 0.11320754716981132, 18: 0.26865671641791045, 19: 0.5488721804511278, 21: 0.15873015873015872, 23: 0.6493506493506493, 24: 0.06896551724137931, 26: 0.6565656565656566, 27: 0.18181818181818182, 29: 0.7878787878787878, 31: 0.07407407407407407, 32: 0.7184466019417476, 40: 0.3493975903614458}
Micro-average F1 score: 0.5380557648831952
Weighted-average F1 score: 0.5034925587620324
F1 score per class: {0: 0.4, 4: 0.9463414634146341, 5: 0.532608695652174, 6: 0.3217391304347826, 7: 0.0, 9: 0.78125, 10: 0.36012861736334406, 13: 0.025477707006369428, 16: 0.38848920863309355, 17: 0.08974358974358974, 18: 0.13365155131264916, 19: 0.4578313253012048, 21: 0.11229946524064172, 23: 0.5692307692307692, 24: 0.1276595744680851, 26: 0.6536585365853659, 27: 0.11188811188811189, 29: 0.7011070110701108, 31: 0.047619047619047616, 32: 0.430939226519337, 40: 0.20307692307692307}
Micro-average F1 score: 0.36782627963660536
Weighted-average F1 score: 0.3305835210132567
F1 score per class: {0: 0.40993788819875776, 4: 0.9556650246305419, 5: 0.5340599455040872, 6: 0.2843601895734597, 7: 0.0, 9: 0.78125, 10: 0.36875, 13: 0.025974025974025976, 16: 0.3776223776223776, 17: 0.08433734939759036, 18: 0.13333333333333333, 19: 0.47318611987381703, 21: 0.11320754716981132, 23: 0.5806451612903226, 24: 0.13043478260869565, 26: 0.6536585365853659, 27: 0.12121212121212122, 29: 0.7286821705426356, 31: 0.04878048780487805, 32: 0.43333333333333335, 40: 0.2006079027355623}
Micro-average F1 score: 0.36905829596412554
Weighted-average F1 score: 0.33065339634712404
cur_acc:  ['0.6837', '0.4539', '0.5604', '0.5335']
his_acc:  ['0.6837', '0.5583', '0.5199', '0.5381']
cur_acc des:  ['0.6171', '0.3538', '0.4621', '0.3579']
his_acc des:  ['0.6171', '0.4624', '0.4360', '0.3678']
cur_acc rrf:  ['0.6235', '0.3618', '0.4848', '0.3662']
his_acc rrf:  ['0.6235', '0.4606', '0.4485', '0.3691']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 51.7329693CurrentTrain: epoch  0, batch     1 | loss: 47.0993075CurrentTrain: epoch  0, batch     2 | loss: 40.0925727CurrentTrain: epoch  0, batch     3 | loss: 61.0238485CurrentTrain: epoch  0, batch     4 | loss: 53.2666469CurrentTrain: epoch  0, batch     5 | loss: 28.2751592CurrentTrain: epoch  1, batch     0 | loss: 38.8004041CurrentTrain: epoch  1, batch     1 | loss: 51.3402252CurrentTrain: epoch  1, batch     2 | loss: 68.3080258CurrentTrain: epoch  1, batch     3 | loss: 50.0808610CurrentTrain: epoch  1, batch     4 | loss: 34.1765061CurrentTrain: epoch  1, batch     5 | loss: 60.9158167CurrentTrain: epoch  2, batch     0 | loss: 42.5933404CurrentTrain: epoch  2, batch     1 | loss: 70.2369204CurrentTrain: epoch  2, batch     2 | loss: 49.8827386CurrentTrain: epoch  2, batch     3 | loss: 35.6945628CurrentTrain: epoch  2, batch     4 | loss: 71.2805204CurrentTrain: epoch  2, batch     5 | loss: 24.6640672CurrentTrain: epoch  3, batch     0 | loss: 52.1267997CurrentTrain: epoch  3, batch     1 | loss: 108.6587715CurrentTrain: epoch  3, batch     2 | loss: 28.3178819CurrentTrain: epoch  3, batch     3 | loss: 108.7340398CurrentTrain: epoch  3, batch     4 | loss: 64.0034691CurrentTrain: epoch  3, batch     5 | loss: 22.3973061CurrentTrain: epoch  4, batch     0 | loss: 49.6023825CurrentTrain: epoch  4, batch     1 | loss: 37.3561201CurrentTrain: epoch  4, batch     2 | loss: 38.1604730CurrentTrain: epoch  4, batch     3 | loss: 36.0798810CurrentTrain: epoch  4, batch     4 | loss: 69.4319014CurrentTrain: epoch  4, batch     5 | loss: 57.0221435CurrentTrain: epoch  5, batch     0 | loss: 38.1225795CurrentTrain: epoch  5, batch     1 | loss: 67.5075580CurrentTrain: epoch  5, batch     2 | loss: 46.7484875CurrentTrain: epoch  5, batch     3 | loss: 46.9636704CurrentTrain: epoch  5, batch     4 | loss: 36.4078445CurrentTrain: epoch  5, batch     5 | loss: 36.2015453CurrentTrain: epoch  6, batch     0 | loss: 35.5582656CurrentTrain: epoch  6, batch     1 | loss: 37.6592573CurrentTrain: epoch  6, batch     2 | loss: 49.0761103CurrentTrain: epoch  6, batch     3 | loss: 36.8636191CurrentTrain: epoch  6, batch     4 | loss: 66.9610272CurrentTrain: epoch  6, batch     5 | loss: 24.3222050CurrentTrain: epoch  7, batch     0 | loss: 66.5768931CurrentTrain: epoch  7, batch     1 | loss: 103.7610652CurrentTrain: epoch  7, batch     2 | loss: 36.4176187CurrentTrain: epoch  7, batch     3 | loss: 36.8767944CurrentTrain: epoch  7, batch     4 | loss: 45.4533937CurrentTrain: epoch  7, batch     5 | loss: 20.0198363CurrentTrain: epoch  8, batch     0 | loss: 45.3089374CurrentTrain: epoch  8, batch     1 | loss: 44.6935463CurrentTrain: epoch  8, batch     2 | loss: 35.5171467CurrentTrain: epoch  8, batch     3 | loss: 68.0924125CurrentTrain: epoch  8, batch     4 | loss: 66.2875680CurrentTrain: epoch  8, batch     5 | loss: 34.8919404CurrentTrain: epoch  9, batch     0 | loss: 43.8354220CurrentTrain: epoch  9, batch     1 | loss: 66.2743124CurrentTrain: epoch  9, batch     2 | loss: 66.3133944CurrentTrain: epoch  9, batch     3 | loss: 47.5853417CurrentTrain: epoch  9, batch     4 | loss: 49.0543992CurrentTrain: epoch  9, batch     5 | loss: 30.3838329
MemoryTrain:  epoch  0, batch     0 | loss: 0.2896889MemoryTrain:  epoch  1, batch     0 | loss: 0.2060800MemoryTrain:  epoch  2, batch     0 | loss: 0.1447443MemoryTrain:  epoch  3, batch     0 | loss: 0.1254775MemoryTrain:  epoch  4, batch     0 | loss: 0.0831841MemoryTrain:  epoch  5, batch     0 | loss: 0.0727281MemoryTrain:  epoch  6, batch     0 | loss: 0.0637631MemoryTrain:  epoch  7, batch     0 | loss: 0.0568022MemoryTrain:  epoch  8, batch     0 | loss: 0.0554548MemoryTrain:  epoch  9, batch     0 | loss: 0.0447224

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.75, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.5555555555555556, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.3368421052631579, 37: 0.4, 38: 0.4}
Micro-average F1 score: 0.3108108108108108
Weighted-average F1 score: 0.2026014968834982
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.6419753086419753, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5168539325842697, 37: 0.38095238095238093, 38: 0.5263157894736842, 40: 0.0}
Micro-average F1 score: 0.33518005540166207
Weighted-average F1 score: 0.2648696386943318
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.65, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.4444444444444444, 37: 0.3918918918918919, 38: 0.5357142857142857, 40: 0.0}
Micro-average F1 score: 0.31486880466472306
Weighted-average F1 score: 0.2357796972229962

F1 score per class: {0: 0.6041666666666666, 4: 0.9375, 5: 0.8545454545454545, 6: 0.24113475177304963, 7: 0.0, 9: 0.9230769230769231, 10: 0.36607142857142855, 13: 0.029411764705882353, 15: 0.3870967741935484, 16: 0.42592592592592593, 17: 0.11267605633802817, 18: 0.1791044776119403, 19: 0.43349753694581283, 21: 0.12345679012345678, 23: 0.7073170731707317, 24: 0.09090909090909091, 25: 0.5555555555555556, 26: 0.6632653061224489, 27: 0.17142857142857143, 29: 0.7725321888412017, 31: 0.09090909090909091, 32: 0.59, 35: 0.1797752808988764, 37: 0.1623931623931624, 38: 0.18604651162790697, 40: 0.21311475409836064}
Micro-average F1 score: 0.45422869813113714
Weighted-average F1 score: 0.42053308117575
F1 score per class: {0: 0.4697986577181208, 4: 0.9552238805970149, 5: 0.6325878594249201, 6: 0.32684824902723736, 7: 0.05128205128205128, 9: 0.7692307692307693, 10: 0.2878787878787879, 13: 0.018433179723502304, 15: 0.27906976744186046, 16: 0.32335329341317365, 17: 0.14814814814814814, 18: 0.16143497757847533, 19: 0.4236111111111111, 21: 0.13846153846153847, 23: 0.5904761904761905, 24: 0.08695652173913043, 25: 0.6419753086419753, 26: 0.6232558139534884, 27: 0.11666666666666667, 29: 0.6923076923076923, 31: 0.07017543859649122, 32: 0.570446735395189, 35: 0.1936842105263158, 37: 0.11740041928721175, 38: 0.18072289156626506, 40: 0.2510822510822511}
Micro-average F1 score: 0.35477505919494867
Weighted-average F1 score: 0.3160068849956
F1 score per class: {0: 0.4794520547945205, 4: 0.9591836734693877, 5: 0.6804123711340206, 6: 0.29591836734693877, 7: 0.04878048780487805, 9: 0.78125, 10: 0.2825278810408922, 13: 0.025806451612903226, 15: 0.2553191489361702, 16: 0.35036496350364965, 17: 0.12962962962962962, 18: 0.16831683168316833, 19: 0.45185185185185184, 21: 0.14007782101167315, 23: 0.6595744680851063, 24: 0.08695652173913043, 25: 0.65, 26: 0.6346153846153846, 27: 0.12280701754385964, 29: 0.7250996015936255, 31: 0.07017543859649122, 32: 0.535031847133758, 35: 0.17827298050139276, 37: 0.11860940695296524, 38: 0.14084507042253522, 40: 0.2543859649122807}
Micro-average F1 score: 0.3614056976502391
Weighted-average F1 score: 0.3197578579502783
cur_acc:  ['0.6837', '0.4539', '0.5604', '0.5335', '0.3108']
his_acc:  ['0.6837', '0.5583', '0.5199', '0.5381', '0.4542']
cur_acc des:  ['0.6171', '0.3538', '0.4621', '0.3579', '0.3352']
his_acc des:  ['0.6171', '0.4624', '0.4360', '0.3678', '0.3548']
cur_acc rrf:  ['0.6235', '0.3618', '0.4848', '0.3662', '0.3149']
his_acc rrf:  ['0.6235', '0.4606', '0.4485', '0.3691', '0.3614']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 80.4369825CurrentTrain: epoch  0, batch     1 | loss: 62.8582476CurrentTrain: epoch  0, batch     2 | loss: 70.3997476CurrentTrain: epoch  0, batch     3 | loss: 62.4259470CurrentTrain: epoch  0, batch     4 | loss: 59.8174846CurrentTrain: epoch  0, batch     5 | loss: 51.2985365CurrentTrain: epoch  0, batch     6 | loss: 33.6070840CurrentTrain: epoch  1, batch     0 | loss: 40.4581530CurrentTrain: epoch  1, batch     1 | loss: 108.6722514CurrentTrain: epoch  1, batch     2 | loss: 58.2240678CurrentTrain: epoch  1, batch     3 | loss: 110.7896760CurrentTrain: epoch  1, batch     4 | loss: 36.2913257CurrentTrain: epoch  1, batch     5 | loss: 56.6449165CurrentTrain: epoch  1, batch     6 | loss: 60.1268327CurrentTrain: epoch  2, batch     0 | loss: 76.3078807CurrentTrain: epoch  2, batch     1 | loss: 39.8579402CurrentTrain: epoch  2, batch     2 | loss: 111.4025207CurrentTrain: epoch  2, batch     3 | loss: 40.5008823CurrentTrain: epoch  2, batch     4 | loss: 33.8248702CurrentTrain: epoch  2, batch     5 | loss: 113.4735527CurrentTrain: epoch  2, batch     6 | loss: 30.7227240CurrentTrain: epoch  3, batch     0 | loss: 107.8184179CurrentTrain: epoch  3, batch     1 | loss: 55.0679906CurrentTrain: epoch  3, batch     2 | loss: 109.8975900CurrentTrain: epoch  3, batch     3 | loss: 32.8734890CurrentTrain: epoch  3, batch     4 | loss: 31.9556501CurrentTrain: epoch  3, batch     5 | loss: 109.0681154CurrentTrain: epoch  3, batch     6 | loss: 12.2498062CurrentTrain: epoch  4, batch     0 | loss: 39.8132903CurrentTrain: epoch  4, batch     1 | loss: 40.3276615CurrentTrain: epoch  4, batch     2 | loss: 52.2494272CurrentTrain: epoch  4, batch     3 | loss: 70.2278709CurrentTrain: epoch  4, batch     4 | loss: 39.3035543CurrentTrain: epoch  4, batch     5 | loss: 53.4425661CurrentTrain: epoch  4, batch     6 | loss: 60.0626960CurrentTrain: epoch  5, batch     0 | loss: 69.9341884CurrentTrain: epoch  5, batch     1 | loss: 51.4575721CurrentTrain: epoch  5, batch     2 | loss: 47.3290171CurrentTrain: epoch  5, batch     3 | loss: 50.0422972CurrentTrain: epoch  5, batch     4 | loss: 47.1558566CurrentTrain: epoch  5, batch     5 | loss: 51.0959023CurrentTrain: epoch  5, batch     6 | loss: 60.0717821CurrentTrain: epoch  6, batch     0 | loss: 36.9471577CurrentTrain: epoch  6, batch     1 | loss: 51.3339812CurrentTrain: epoch  6, batch     2 | loss: 49.9938472CurrentTrain: epoch  6, batch     3 | loss: 67.4240958CurrentTrain: epoch  6, batch     4 | loss: 39.3586418CurrentTrain: epoch  6, batch     5 | loss: 68.1096428CurrentTrain: epoch  6, batch     6 | loss: 27.0546472CurrentTrain: epoch  7, batch     0 | loss: 48.1522027CurrentTrain: epoch  7, batch     1 | loss: 49.4187912CurrentTrain: epoch  7, batch     2 | loss: 38.5351926CurrentTrain: epoch  7, batch     3 | loss: 107.7468682CurrentTrain: epoch  7, batch     4 | loss: 34.1267442CurrentTrain: epoch  7, batch     5 | loss: 107.7028289CurrentTrain: epoch  7, batch     6 | loss: 60.0212119CurrentTrain: epoch  8, batch     0 | loss: 48.6968931CurrentTrain: epoch  8, batch     1 | loss: 38.5599620CurrentTrain: epoch  8, batch     2 | loss: 48.2133267CurrentTrain: epoch  8, batch     3 | loss: 49.4995871CurrentTrain: epoch  8, batch     4 | loss: 49.4081116CurrentTrain: epoch  8, batch     5 | loss: 36.6532633CurrentTrain: epoch  8, batch     6 | loss: 26.6911919CurrentTrain: epoch  9, batch     0 | loss: 48.4148751CurrentTrain: epoch  9, batch     1 | loss: 29.6090616CurrentTrain: epoch  9, batch     2 | loss: 68.4598047CurrentTrain: epoch  9, batch     3 | loss: 63.8065078CurrentTrain: epoch  9, batch     4 | loss: 48.0940219CurrentTrain: epoch  9, batch     5 | loss: 48.5033491CurrentTrain: epoch  9, batch     6 | loss: 60.0976973
MemoryTrain:  epoch  0, batch     0 | loss: 0.3556989MemoryTrain:  epoch  1, batch     0 | loss: 0.2992555MemoryTrain:  epoch  2, batch     0 | loss: 0.2208404MemoryTrain:  epoch  3, batch     0 | loss: 0.1626139MemoryTrain:  epoch  4, batch     0 | loss: 0.1435267MemoryTrain:  epoch  5, batch     0 | loss: 0.1141639MemoryTrain:  epoch  6, batch     0 | loss: 0.0999156MemoryTrain:  epoch  7, batch     0 | loss: 0.0910876MemoryTrain:  epoch  8, batch     0 | loss: 0.0883558MemoryTrain:  epoch  9, batch     0 | loss: 0.0748513

F1 score per class: {0: 0.0, 2: 0.35, 4: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.45962732919254656, 12: 0.20967741935483872, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 28: 0.2727272727272727, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.1, 40: 0.0}
Micro-average F1 score: 0.23552123552123552
Weighted-average F1 score: 0.18011115355770518
F1 score per class: {0: 0.0, 2: 0.21875, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.45878136200716846, 12: 0.568, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.13793103448275862, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.3333333333333333, 40: 0.0}
Micro-average F1 score: 0.2682926829268293
Weighted-average F1 score: 0.21860966114191652
F1 score per class: {0: 0.0, 2: 0.21875, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.4697508896797153, 12: 0.5447154471544715, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.23255813953488372, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.07142857142857142, 40: 0.0}
Micro-average F1 score: 0.26666666666666666
Weighted-average F1 score: 0.2184488149274644

F1 score per class: {0: 0.5714285714285714, 2: 0.2222222222222222, 4: 0.9405940594059405, 5: 0.8288288288288288, 6: 0.28054298642533937, 7: 0.0, 9: 0.8888888888888888, 10: 0.17777777777777778, 11: 0.23492063492063492, 12: 0.13471502590673576, 13: 0.03571428571428571, 15: 0.2553191489361702, 16: 0.42201834862385323, 17: 0.13559322033898305, 18: 0.12987012987012986, 19: 0.4661016949152542, 21: 0.14678899082568808, 23: 0.6987951807228916, 24: 0.07692307692307693, 25: 0.44776119402985076, 26: 0.6839378238341969, 27: 0.16393442622950818, 28: 0.2, 29: 0.7447698744769874, 31: 0.0, 32: 0.6446280991735537, 35: 0.1896551724137931, 37: 0.24615384615384617, 38: 0.11764705882352941, 39: 0.08, 40: 0.21311475409836064}
Micro-average F1 score: 0.41838097782527384
Weighted-average F1 score: 0.40014797672931895
F1 score per class: {0: 0.4069767441860465, 2: 0.05785123966942149, 4: 0.8990825688073395, 5: 0.5689655172413793, 6: 0.28, 7: 0.0, 9: 0.684931506849315, 10: 0.3008849557522124, 11: 0.19692307692307692, 12: 0.17906683480453972, 13: 0.014492753623188406, 15: 0.21818181818181817, 16: 0.3586206896551724, 17: 0.08860759493670886, 18: 0.11764705882352941, 19: 0.436046511627907, 21: 0.12578616352201258, 23: 0.5223880597014925, 24: 0.06896551724137931, 25: 0.5925925925925926, 26: 0.6567164179104478, 27: 0.13333333333333333, 28: 0.05825242718446602, 29: 0.6872586872586872, 31: 0.056338028169014086, 32: 0.4860335195530726, 35: 0.22740524781341107, 37: 0.11518324607329843, 38: 0.15584415584415584, 39: 0.18181818181818182, 40: 0.28421052631578947}
Micro-average F1 score: 0.29690371836561574
Weighted-average F1 score: 0.26681466678476456
F1 score per class: {0: 0.4473684210526316, 2: 0.058333333333333334, 4: 0.937799043062201, 5: 0.6068111455108359, 6: 0.2789115646258503, 7: 0.0, 9: 0.7352941176470589, 10: 0.3063063063063063, 11: 0.19701492537313434, 12: 0.17585301837270342, 13: 0.024242424242424242, 15: 0.1875, 16: 0.35374149659863946, 17: 0.0958904109589041, 18: 0.15384615384615385, 19: 0.4485981308411215, 21: 0.12307692307692308, 23: 0.6055045871559633, 24: 0.07142857142857142, 25: 0.575, 26: 0.6633165829145728, 27: 0.13592233009708737, 28: 0.08849557522123894, 29: 0.726530612244898, 31: 0.0547945205479452, 32: 0.4496124031007752, 35: 0.18571428571428572, 37: 0.11970074812967581, 38: 0.15510204081632653, 39: 0.044444444444444446, 40: 0.27411167512690354}
Micro-average F1 score: 0.3007938841517201
Weighted-average F1 score: 0.2679153490020713
cur_acc:  ['0.6837', '0.4539', '0.5604', '0.5335', '0.3108', '0.2355']
his_acc:  ['0.6837', '0.5583', '0.5199', '0.5381', '0.4542', '0.4184']
cur_acc des:  ['0.6171', '0.3538', '0.4621', '0.3579', '0.3352', '0.2683']
his_acc des:  ['0.6171', '0.4624', '0.4360', '0.3678', '0.3548', '0.2969']
cur_acc rrf:  ['0.6235', '0.3618', '0.4848', '0.3662', '0.3149', '0.2667']
his_acc rrf:  ['0.6235', '0.4606', '0.4485', '0.3691', '0.3614', '0.3008']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 38.1222783CurrentTrain: epoch  0, batch     1 | loss: 49.4605565CurrentTrain: epoch  0, batch     2 | loss: 48.8195980CurrentTrain: epoch  0, batch     3 | loss: 113.5843161CurrentTrain: epoch  0, batch     4 | loss: 80.8108405CurrentTrain: epoch  0, batch     5 | loss: 45.8772437CurrentTrain: epoch  0, batch     6 | loss: 110.9735352CurrentTrain: epoch  1, batch     0 | loss: 111.8155029CurrentTrain: epoch  1, batch     1 | loss: 109.6241540CurrentTrain: epoch  1, batch     2 | loss: 53.2539251CurrentTrain: epoch  1, batch     3 | loss: 106.6563520CurrentTrain: epoch  1, batch     4 | loss: 44.3487436CurrentTrain: epoch  1, batch     5 | loss: 44.5761629CurrentTrain: epoch  1, batch     6 | loss: 40.7979416CurrentTrain: epoch  2, batch     0 | loss: 42.1914241CurrentTrain: epoch  2, batch     1 | loss: 51.5809357CurrentTrain: epoch  2, batch     2 | loss: 70.6429471CurrentTrain: epoch  2, batch     3 | loss: 50.1317446CurrentTrain: epoch  2, batch     4 | loss: 67.1480142CurrentTrain: epoch  2, batch     5 | loss: 69.5841688CurrentTrain: epoch  2, batch     6 | loss: 67.0993832CurrentTrain: epoch  3, batch     0 | loss: 104.2136515CurrentTrain: epoch  3, batch     1 | loss: 49.9257074CurrentTrain: epoch  3, batch     2 | loss: 32.5426498CurrentTrain: epoch  3, batch     3 | loss: 39.9904752CurrentTrain: epoch  3, batch     4 | loss: 68.8539110CurrentTrain: epoch  3, batch     5 | loss: 107.0743245CurrentTrain: epoch  3, batch     6 | loss: 48.3216262CurrentTrain: epoch  4, batch     0 | loss: 68.9926017CurrentTrain: epoch  4, batch     1 | loss: 37.7070390CurrentTrain: epoch  4, batch     2 | loss: 107.8236746CurrentTrain: epoch  4, batch     3 | loss: 38.2728489CurrentTrain: epoch  4, batch     4 | loss: 109.1954473CurrentTrain: epoch  4, batch     5 | loss: 39.1858851CurrentTrain: epoch  4, batch     6 | loss: 45.8940502CurrentTrain: epoch  5, batch     0 | loss: 48.6782202CurrentTrain: epoch  5, batch     1 | loss: 68.6203690CurrentTrain: epoch  5, batch     2 | loss: 107.4506732CurrentTrain: epoch  5, batch     3 | loss: 66.6593699CurrentTrain: epoch  5, batch     4 | loss: 49.3273345CurrentTrain: epoch  5, batch     5 | loss: 39.3766349CurrentTrain: epoch  5, batch     6 | loss: 29.5224003CurrentTrain: epoch  6, batch     0 | loss: 49.1669288CurrentTrain: epoch  6, batch     1 | loss: 66.5558627CurrentTrain: epoch  6, batch     2 | loss: 48.0232373CurrentTrain: epoch  6, batch     3 | loss: 48.3768388CurrentTrain: epoch  6, batch     4 | loss: 67.2490908CurrentTrain: epoch  6, batch     5 | loss: 64.8798804CurrentTrain: epoch  6, batch     6 | loss: 103.0131420CurrentTrain: epoch  7, batch     0 | loss: 67.4625686CurrentTrain: epoch  7, batch     1 | loss: 34.9353452CurrentTrain: epoch  7, batch     2 | loss: 48.5149247CurrentTrain: epoch  7, batch     3 | loss: 222.6081351CurrentTrain: epoch  7, batch     4 | loss: 64.9275440CurrentTrain: epoch  7, batch     5 | loss: 66.5201649CurrentTrain: epoch  7, batch     6 | loss: 45.6987351CurrentTrain: epoch  8, batch     0 | loss: 38.7269107CurrentTrain: epoch  8, batch     1 | loss: 66.6934397CurrentTrain: epoch  8, batch     2 | loss: 222.6560796CurrentTrain: epoch  8, batch     3 | loss: 47.9595219CurrentTrain: epoch  8, batch     4 | loss: 103.8965363CurrentTrain: epoch  8, batch     5 | loss: 49.1047270CurrentTrain: epoch  8, batch     6 | loss: 31.9276978CurrentTrain: epoch  9, batch     0 | loss: 107.3078954CurrentTrain: epoch  9, batch     1 | loss: 28.5863221CurrentTrain: epoch  9, batch     2 | loss: 35.7286604CurrentTrain: epoch  9, batch     3 | loss: 68.1849540CurrentTrain: epoch  9, batch     4 | loss: 50.1621954CurrentTrain: epoch  9, batch     5 | loss: 49.2761091CurrentTrain: epoch  9, batch     6 | loss: 65.2323700
MemoryTrain:  epoch  0, batch     0 | loss: 0.4249466MemoryTrain:  epoch  0, batch     1 | loss: 0.0833237MemoryTrain:  epoch  1, batch     0 | loss: 0.3316309MemoryTrain:  epoch  1, batch     1 | loss: 0.1608287MemoryTrain:  epoch  2, batch     0 | loss: 0.2064711MemoryTrain:  epoch  2, batch     1 | loss: 0.0935742MemoryTrain:  epoch  3, batch     0 | loss: 0.1346191MemoryTrain:  epoch  3, batch     1 | loss: 0.0752653MemoryTrain:  epoch  4, batch     0 | loss: 0.1593586MemoryTrain:  epoch  4, batch     1 | loss: 0.0574961MemoryTrain:  epoch  5, batch     0 | loss: 0.1031516MemoryTrain:  epoch  5, batch     1 | loss: 0.0553430MemoryTrain:  epoch  6, batch     0 | loss: 0.0949520MemoryTrain:  epoch  6, batch     1 | loss: 0.0889389MemoryTrain:  epoch  7, batch     0 | loss: 0.0865144MemoryTrain:  epoch  7, batch     1 | loss: 0.0397461MemoryTrain:  epoch  8, batch     0 | loss: 0.0642228MemoryTrain:  epoch  8, batch     1 | loss: 0.0489097MemoryTrain:  epoch  9, batch     0 | loss: 0.0535422MemoryTrain:  epoch  9, batch     1 | loss: 0.0608323

F1 score per class: {0: 0.0, 1: 0.13745704467353953, 2: 0.0, 3: 0.4411764705882353, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0851063829787234, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.46017699115044247, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.16129032258064516, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.17936507936507937
Weighted-average F1 score: 0.1347984262207943
F1 score per class: {0: 0.0, 1: 0.1242603550295858, 2: 0.0, 3: 0.3769633507853403, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.061224489795918366, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.45161290322580644, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.38202247191011235, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.16199047114875595
Weighted-average F1 score: 0.12914172378351654
F1 score per class: {0: 0.0, 1: 0.1253731343283582, 2: 0.0, 3: 0.3979591836734694, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.05970149253731343, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.44715447154471544, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.27906976744186046, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.1444866920152091
Weighted-average F1 score: 0.10953614961765049

F1 score per class: {0: 0.49056603773584906, 1: 0.10443864229765012, 2: 0.2857142857142857, 3: 0.3225806451612903, 4: 0.94, 5: 0.8016877637130801, 6: 0.24761904761904763, 7: 0.028985507246376812, 9: 0.7936507936507936, 10: 0.36363636363636365, 11: 0.19693654266958424, 12: 0.11180124223602485, 13: 0.027777777777777776, 14: 0.053811659192825115, 15: 0.26666666666666666, 16: 0.45714285714285713, 17: 0.07547169811320754, 18: 0.13008130081300814, 19: 0.36, 21: 0.12307692307692308, 22: 0.3939393939393939, 23: 0.6788990825688074, 24: 0.047619047619047616, 25: 0.4857142857142857, 26: 0.6376811594202898, 27: 0.08, 28: 0.20689655172413793, 29: 0.6901960784313725, 31: 0.0, 32: 0.4575645756457565, 34: 0.09090909090909091, 35: 0.16560509554140126, 37: 0.1554054054054054, 38: 0.12121212121212122, 39: 0.06666666666666667, 40: 0.23214285714285715}
Micro-average F1 score: 0.3338929304234285
Weighted-average F1 score: 0.30747642653913454
F1 score per class: {0: 0.46808510638297873, 1: 0.08955223880597014, 2: 0.07407407407407407, 3: 0.17391304347826086, 4: 0.9090909090909091, 5: 0.40408163265306124, 6: 0.254071661237785, 7: 0.0, 9: 0.6410256410256411, 10: 0.311787072243346, 11: 0.14798206278026907, 12: 0.19174041297935104, 13: 0.019230769230769232, 14: 0.028985507246376812, 15: 0.21428571428571427, 16: 0.38235294117647056, 17: 0.10810810810810811, 18: 0.11398963730569948, 19: 0.35170603674540685, 21: 0.128, 22: 0.358974358974359, 23: 0.5694444444444444, 24: 0.0547945205479452, 25: 0.5227272727272727, 26: 0.6036036036036037, 27: 0.0, 28: 0.06486486486486487, 29: 0.6379928315412187, 31: 0.03278688524590164, 32: 0.39195979899497485, 34: 0.09066666666666667, 35: 0.14225941422594143, 37: 0.11764705882352941, 38: 0.17346938775510204, 39: 0.04, 40: 0.3106796116504854}
Micro-average F1 score: 0.24624492928406974
Weighted-average F1 score: 0.22304249352682282
F1 score per class: {0: 0.46715328467153283, 1: 0.08860759493670886, 2: 0.07407407407407407, 3: 0.18886198547215496, 4: 0.9320388349514563, 5: 0.4288840262582057, 6: 0.2576271186440678, 7: 0.02531645569620253, 9: 0.6944444444444444, 10: 0.3115942028985507, 11: 0.15782312925170067, 12: 0.1968365553602812, 13: 0.022857142857142857, 14: 0.027842227378190254, 15: 0.1935483870967742, 16: 0.391304347826087, 17: 0.11320754716981132, 18: 0.12222222222222222, 19: 0.35543766578249336, 21: 0.13559322033898305, 22: 0.36789297658862874, 23: 0.6456692913385826, 24: 0.05128205128205128, 25: 0.5287356321839081, 26: 0.6090909090909091, 27: 0.0, 28: 0.06451612903225806, 29: 0.652014652014652, 31: 0.03333333333333333, 32: 0.3816793893129771, 34: 0.09056603773584905, 35: 0.13881748071979436, 37: 0.09012875536480687, 38: 0.17475728155339806, 39: 0.043478260869565216, 40: 0.30097087378640774}
Micro-average F1 score: 0.254337899543379
Weighted-average F1 score: 0.23019950361998992
cur_acc:  ['0.6837', '0.4539', '0.5604', '0.5335', '0.3108', '0.2355', '0.1794']
his_acc:  ['0.6837', '0.5583', '0.5199', '0.5381', '0.4542', '0.4184', '0.3339']
cur_acc des:  ['0.6171', '0.3538', '0.4621', '0.3579', '0.3352', '0.2683', '0.1620']
his_acc des:  ['0.6171', '0.4624', '0.4360', '0.3678', '0.3548', '0.2969', '0.2462']
cur_acc rrf:  ['0.6235', '0.3618', '0.4848', '0.3662', '0.3149', '0.2667', '0.1445']
his_acc rrf:  ['0.6235', '0.4606', '0.4485', '0.3691', '0.3614', '0.3008', '0.2543']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 44.9627420CurrentTrain: epoch  0, batch     1 | loss: 122.3320531CurrentTrain: epoch  0, batch     2 | loss: 74.3627159CurrentTrain: epoch  0, batch     3 | loss: 56.0052122CurrentTrain: epoch  0, batch     4 | loss: 37.8382111CurrentTrain: epoch  0, batch     5 | loss: 33.4974602CurrentTrain: epoch  1, batch     0 | loss: 53.2044118CurrentTrain: epoch  1, batch     1 | loss: 55.5952269CurrentTrain: epoch  1, batch     2 | loss: 42.1469140CurrentTrain: epoch  1, batch     3 | loss: 66.5805182CurrentTrain: epoch  1, batch     4 | loss: 71.1612291CurrentTrain: epoch  1, batch     5 | loss: 22.6161398CurrentTrain: epoch  2, batch     0 | loss: 71.8439699CurrentTrain: epoch  2, batch     1 | loss: 54.3267076CurrentTrain: epoch  2, batch     2 | loss: 51.2654324CurrentTrain: epoch  2, batch     3 | loss: 36.8307750CurrentTrain: epoch  2, batch     4 | loss: 48.7282494CurrentTrain: epoch  2, batch     5 | loss: 19.0135124CurrentTrain: epoch  3, batch     0 | loss: 69.8524293CurrentTrain: epoch  3, batch     1 | loss: 50.3245971CurrentTrain: epoch  3, batch     2 | loss: 68.9675625CurrentTrain: epoch  3, batch     3 | loss: 30.4633961CurrentTrain: epoch  3, batch     4 | loss: 45.9599564CurrentTrain: epoch  3, batch     5 | loss: 14.9037862CurrentTrain: epoch  4, batch     0 | loss: 37.0122985CurrentTrain: epoch  4, batch     1 | loss: 38.9332942CurrentTrain: epoch  4, batch     2 | loss: 31.5169128CurrentTrain: epoch  4, batch     3 | loss: 108.4123616CurrentTrain: epoch  4, batch     4 | loss: 36.7193450CurrentTrain: epoch  4, batch     5 | loss: 28.3872648CurrentTrain: epoch  5, batch     0 | loss: 38.9142808CurrentTrain: epoch  5, batch     1 | loss: 48.3191159CurrentTrain: epoch  5, batch     2 | loss: 66.9635813CurrentTrain: epoch  5, batch     3 | loss: 46.4669626CurrentTrain: epoch  5, batch     4 | loss: 29.7222150CurrentTrain: epoch  5, batch     5 | loss: 45.2845678CurrentTrain: epoch  6, batch     0 | loss: 35.5439576CurrentTrain: epoch  6, batch     1 | loss: 69.3204726CurrentTrain: epoch  6, batch     2 | loss: 48.0034820CurrentTrain: epoch  6, batch     3 | loss: 28.8133557CurrentTrain: epoch  6, batch     4 | loss: 49.4726871CurrentTrain: epoch  6, batch     5 | loss: 27.7745647CurrentTrain: epoch  7, batch     0 | loss: 38.4225992CurrentTrain: epoch  7, batch     1 | loss: 29.6275053CurrentTrain: epoch  7, batch     2 | loss: 48.0653488CurrentTrain: epoch  7, batch     3 | loss: 36.9385121CurrentTrain: epoch  7, batch     4 | loss: 68.3134327CurrentTrain: epoch  7, batch     5 | loss: 17.7746216CurrentTrain: epoch  8, batch     0 | loss: 34.5080072CurrentTrain: epoch  8, batch     1 | loss: 46.8112931CurrentTrain: epoch  8, batch     2 | loss: 222.9631134CurrentTrain: epoch  8, batch     3 | loss: 63.0512271CurrentTrain: epoch  8, batch     4 | loss: 35.8303328CurrentTrain: epoch  8, batch     5 | loss: 27.3518197CurrentTrain: epoch  9, batch     0 | loss: 44.9420997CurrentTrain: epoch  9, batch     1 | loss: 36.4325101CurrentTrain: epoch  9, batch     2 | loss: 66.4114979CurrentTrain: epoch  9, batch     3 | loss: 29.3474990CurrentTrain: epoch  9, batch     4 | loss: 68.2149514CurrentTrain: epoch  9, batch     5 | loss: 26.1136736
MemoryTrain:  epoch  0, batch     0 | loss: 0.1918589MemoryTrain:  epoch  0, batch     1 | loss: 0.0359416MemoryTrain:  epoch  1, batch     0 | loss: 0.1260905MemoryTrain:  epoch  1, batch     1 | loss: 0.0991552MemoryTrain:  epoch  2, batch     0 | loss: 0.1726897MemoryTrain:  epoch  2, batch     1 | loss: 0.0686167MemoryTrain:  epoch  3, batch     0 | loss: 0.1022730MemoryTrain:  epoch  3, batch     1 | loss: 0.0742894MemoryTrain:  epoch  4, batch     0 | loss: 0.0664137MemoryTrain:  epoch  4, batch     1 | loss: 0.1172716MemoryTrain:  epoch  5, batch     0 | loss: 0.0669105MemoryTrain:  epoch  5, batch     1 | loss: 0.0631234MemoryTrain:  epoch  6, batch     0 | loss: 0.0594006MemoryTrain:  epoch  6, batch     1 | loss: 0.0444759MemoryTrain:  epoch  7, batch     0 | loss: 0.0656793MemoryTrain:  epoch  7, batch     1 | loss: 0.0378978MemoryTrain:  epoch  8, batch     0 | loss: 0.0556008MemoryTrain:  epoch  8, batch     1 | loss: 0.0446615MemoryTrain:  epoch  9, batch     0 | loss: 0.0472075MemoryTrain:  epoch  9, batch     1 | loss: 0.0347602

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.2803738317757009, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6837606837606838, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 31: 0.0, 32: 0.0, 33: 0.42857142857142855, 34: 0.0, 35: 0.0, 36: 0.4423076923076923, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.38076923076923075
Weighted-average F1 score: 0.29748637650522974
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.46875, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.5806451612903226, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.9743589743589743, 31: 0.0, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 35: 0.0, 36: 0.4954128440366973, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.30019880715705766
Weighted-average F1 score: 0.2296679469168435
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5109489051094891, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5911949685534591, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.9743589743589743, 31: 0.0, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 35: 0.0, 36: 0.4816753926701571, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.31645569620253167
Weighted-average F1 score: 0.2411597346508611

F1 score per class: {0: 0.4807692307692308, 1: 0.10126582278481013, 2: 0.22641509433962265, 3: 0.26373626373626374, 4: 0.9458128078817734, 5: 0.8, 6: 0.2054794520547945, 7: 0.0, 8: 0.1910828025477707, 9: 0.8064516129032258, 10: 0.16129032258064516, 11: 0.2134387351778656, 12: 0.015873015873015872, 13: 0.03389830508474576, 14: 0.05263157894736842, 15: 0.3157894736842105, 16: 0.423728813559322, 17: 0.08888888888888889, 18: 0.0, 19: 0.38497652582159625, 20: 0.26578073089701, 21: 0.12, 22: 0.4152542372881356, 23: 0.5964912280701754, 24: 0.06153846153846154, 25: 0.3582089552238806, 26: 0.6055045871559633, 27: 0.08, 28: 0.26666666666666666, 29: 0.6796875, 30: 0.9473684210526315, 31: 0.05405405405405406, 32: 0.423963133640553, 33: 0.16666666666666666, 34: 0.12121212121212122, 35: 0.07246376811594203, 36: 0.39655172413793105, 37: 0.26744186046511625, 38: 0.1276595744680851, 39: 0.0, 40: 0.26666666666666666}
Micro-average F1 score: 0.3421309872922776
Weighted-average F1 score: 0.3346914409391131
F1 score per class: {0: 0.43137254901960786, 1: 0.09643605870020965, 2: 0.07650273224043716, 3: 0.18181818181818182, 4: 0.9116279069767442, 5: 0.3407917383820998, 6: 0.27715355805243447, 7: 0.0, 8: 0.18072289156626506, 9: 0.6666666666666666, 10: 0.21052631578947367, 11: 0.20147420147420148, 12: 0.1679160419790105, 13: 0.02040816326530612, 14: 0.035443037974683546, 15: 0.2033898305084746, 16: 0.373134328358209, 17: 0.11650485436893204, 18: 0.11926605504587157, 19: 0.3526170798898072, 20: 0.16885553470919323, 21: 0.12962962962962962, 22: 0.4015748031496063, 23: 0.5416666666666666, 24: 0.03225806451612903, 25: 0.4772727272727273, 26: 0.5726495726495726, 27: 0.04878048780487805, 28: 0.10309278350515463, 29: 0.644927536231884, 30: 0.6031746031746031, 31: 0.02247191011235955, 32: 0.39267015706806285, 33: 0.075, 34: 0.12546125461254612, 35: 0.14249363867684478, 36: 0.2660098522167488, 37: 0.15591397849462366, 38: 0.1641025641025641, 39: 0.07142857142857142, 40: 0.28431372549019607}
Micro-average F1 score: 0.2505876341338784
Weighted-average F1 score: 0.2315636479184054
F1 score per class: {0: 0.43137254901960786, 1: 0.09563409563409564, 2: 0.13725490196078433, 3: 0.2152230971128609, 4: 0.9333333333333333, 5: 0.38735177865612647, 6: 0.2692307692307692, 7: 0.03636363636363636, 8: 0.1891891891891892, 9: 0.6578947368421053, 10: 0.2122905027932961, 11: 0.18837675350701402, 12: 0.16140350877192983, 13: 0.021739130434782608, 14: 0.03580562659846547, 15: 0.18181818181818182, 16: 0.35714285714285715, 17: 0.12121212121212122, 18: 0.10810810810810811, 19: 0.3588235294117647, 20: 0.16666666666666666, 21: 0.125, 22: 0.416, 23: 0.5669291338582677, 24: 0.031746031746031744, 25: 0.4470588235294118, 26: 0.5775862068965517, 27: 0.05, 28: 0.14285714285714285, 29: 0.664179104477612, 30: 0.5846153846153846, 31: 0.025, 32: 0.39473684210526316, 33: 0.07317073170731707, 34: 0.15384615384615385, 35: 0.11538461538461539, 36: 0.2746268656716418, 37: 0.13592233009708737, 38: 0.14925373134328357, 39: 0.0, 40: 0.2871287128712871}
Micro-average F1 score: 0.2572972972972973
Weighted-average F1 score: 0.2374983558933368
cur_acc:  ['0.6837', '0.4539', '0.5604', '0.5335', '0.3108', '0.2355', '0.1794', '0.3808']
his_acc:  ['0.6837', '0.5583', '0.5199', '0.5381', '0.4542', '0.4184', '0.3339', '0.3421']
cur_acc des:  ['0.6171', '0.3538', '0.4621', '0.3579', '0.3352', '0.2683', '0.1620', '0.3002']
his_acc des:  ['0.6171', '0.4624', '0.4360', '0.3678', '0.3548', '0.2969', '0.2462', '0.2506']
cur_acc rrf:  ['0.6235', '0.3618', '0.4848', '0.3662', '0.3149', '0.2667', '0.1445', '0.3165']
his_acc rrf:  ['0.6235', '0.4606', '0.4485', '0.3691', '0.3614', '0.3008', '0.2543', '0.2573']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 60.8956819CurrentTrain: epoch  0, batch     1 | loss: 48.6317699CurrentTrain: epoch  0, batch     2 | loss: 60.1601048CurrentTrain: epoch  0, batch     3 | loss: 40.9494497CurrentTrain: epoch  0, batch     4 | loss: 114.3716511CurrentTrain: epoch  0, batch     5 | loss: 47.3807384CurrentTrain: epoch  0, batch     6 | loss: 77.4513904CurrentTrain: epoch  0, batch     7 | loss: 75.9605174CurrentTrain: epoch  0, batch     8 | loss: 77.7032046CurrentTrain: epoch  0, batch     9 | loss: 47.2404824CurrentTrain: epoch  0, batch    10 | loss: 40.4098371CurrentTrain: epoch  0, batch    11 | loss: 77.1566765CurrentTrain: epoch  0, batch    12 | loss: 112.5611649CurrentTrain: epoch  0, batch    13 | loss: 47.2414557CurrentTrain: epoch  0, batch    14 | loss: 76.7758285CurrentTrain: epoch  0, batch    15 | loss: 39.8973953CurrentTrain: epoch  0, batch    16 | loss: 47.1243401CurrentTrain: epoch  0, batch    17 | loss: 112.0702858CurrentTrain: epoch  0, batch    18 | loss: 57.9302001CurrentTrain: epoch  0, batch    19 | loss: 76.7546965CurrentTrain: epoch  0, batch    20 | loss: 34.3547331CurrentTrain: epoch  0, batch    21 | loss: 57.9844954CurrentTrain: epoch  0, batch    22 | loss: 58.0882371CurrentTrain: epoch  0, batch    23 | loss: 57.0811962CurrentTrain: epoch  0, batch    24 | loss: 76.7410562CurrentTrain: epoch  0, batch    25 | loss: 75.7355547CurrentTrain: epoch  0, batch    26 | loss: 57.7885839CurrentTrain: epoch  0, batch    27 | loss: 57.5494277CurrentTrain: epoch  0, batch    28 | loss: 76.2583186CurrentTrain: epoch  0, batch    29 | loss: 75.8439826CurrentTrain: epoch  0, batch    30 | loss: 76.2643229CurrentTrain: epoch  0, batch    31 | loss: 76.0085875CurrentTrain: epoch  0, batch    32 | loss: 56.3866266CurrentTrain: epoch  0, batch    33 | loss: 112.3661222CurrentTrain: epoch  0, batch    34 | loss: 34.2201197CurrentTrain: epoch  0, batch    35 | loss: 75.3771596CurrentTrain: epoch  0, batch    36 | loss: 47.0540923CurrentTrain: epoch  0, batch    37 | loss: 45.7955304CurrentTrain: epoch  0, batch    38 | loss: 39.3944128CurrentTrain: epoch  0, batch    39 | loss: 39.2784821CurrentTrain: epoch  0, batch    40 | loss: 58.2399968CurrentTrain: epoch  0, batch    41 | loss: 75.6213941CurrentTrain: epoch  0, batch    42 | loss: 76.2453398CurrentTrain: epoch  0, batch    43 | loss: 222.2270327CurrentTrain: epoch  0, batch    44 | loss: 46.0673591CurrentTrain: epoch  0, batch    45 | loss: 57.5673652CurrentTrain: epoch  0, batch    46 | loss: 74.5612298CurrentTrain: epoch  0, batch    47 | loss: 47.0795853CurrentTrain: epoch  0, batch    48 | loss: 38.6482540CurrentTrain: epoch  0, batch    49 | loss: 57.2935585CurrentTrain: epoch  0, batch    50 | loss: 57.2208789CurrentTrain: epoch  0, batch    51 | loss: 57.0637497CurrentTrain: epoch  0, batch    52 | loss: 45.2076460CurrentTrain: epoch  0, batch    53 | loss: 44.7184655CurrentTrain: epoch  0, batch    54 | loss: 57.4853008CurrentTrain: epoch  0, batch    55 | loss: 56.6761946CurrentTrain: epoch  0, batch    56 | loss: 46.0595031CurrentTrain: epoch  0, batch    57 | loss: 46.0749094CurrentTrain: epoch  0, batch    58 | loss: 222.2920967CurrentTrain: epoch  0, batch    59 | loss: 56.7365658CurrentTrain: epoch  0, batch    60 | loss: 222.2510686CurrentTrain: epoch  0, batch    61 | loss: 38.2121750CurrentTrain: epoch  0, batch    62 | loss: 56.3645077CurrentTrain: epoch  0, batch    63 | loss: 45.9874097CurrentTrain: epoch  0, batch    64 | loss: 56.6664965CurrentTrain: epoch  0, batch    65 | loss: 75.3504650CurrentTrain: epoch  0, batch    66 | loss: 45.6135999CurrentTrain: epoch  0, batch    67 | loss: 46.2591932CurrentTrain: epoch  0, batch    68 | loss: 45.0606345CurrentTrain: epoch  0, batch    69 | loss: 44.7973103CurrentTrain: epoch  0, batch    70 | loss: 45.4474890CurrentTrain: epoch  0, batch    71 | loss: 56.0586407CurrentTrain: epoch  0, batch    72 | loss: 55.5957248CurrentTrain: epoch  0, batch    73 | loss: 76.5189553CurrentTrain: epoch  0, batch    74 | loss: 111.4005756CurrentTrain: epoch  0, batch    75 | loss: 45.2449679CurrentTrain: epoch  0, batch    76 | loss: 56.2370878CurrentTrain: epoch  0, batch    77 | loss: 45.8737078CurrentTrain: epoch  0, batch    78 | loss: 73.9756625CurrentTrain: epoch  0, batch    79 | loss: 112.4707231CurrentTrain: epoch  0, batch    80 | loss: 112.1777553CurrentTrain: epoch  0, batch    81 | loss: 46.6148267CurrentTrain: epoch  0, batch    82 | loss: 43.0039298CurrentTrain: epoch  0, batch    83 | loss: 222.3664882CurrentTrain: epoch  0, batch    84 | loss: 74.8748497CurrentTrain: epoch  0, batch    85 | loss: 72.2002777CurrentTrain: epoch  0, batch    86 | loss: 31.4950724CurrentTrain: epoch  0, batch    87 | loss: 43.1491944CurrentTrain: epoch  0, batch    88 | loss: 222.3993305CurrentTrain: epoch  0, batch    89 | loss: 73.1584161CurrentTrain: epoch  0, batch    90 | loss: 44.5850218CurrentTrain: epoch  0, batch    91 | loss: 56.1337285CurrentTrain: epoch  0, batch    92 | loss: 73.0496153CurrentTrain: epoch  0, batch    93 | loss: 43.3275826CurrentTrain: epoch  0, batch    94 | loss: 111.1800618CurrentTrain: epoch  0, batch    95 | loss: 44.6176983CurrentTrain: epoch  0, batch    96 | loss: 55.2365205CurrentTrain: epoch  0, batch    97 | loss: 43.0486575CurrentTrain: epoch  0, batch    98 | loss: 222.5536140CurrentTrain: epoch  0, batch    99 | loss: 41.9852189CurrentTrain: epoch  0, batch   100 | loss: 55.4031689CurrentTrain: epoch  0, batch   101 | loss: 54.6038240CurrentTrain: epoch  0, batch   102 | loss: 72.0610290CurrentTrain: epoch  0, batch   103 | loss: 44.7270765CurrentTrain: epoch  0, batch   104 | loss: 56.7263056CurrentTrain: epoch  0, batch   105 | loss: 76.1751662CurrentTrain: epoch  0, batch   106 | loss: 74.7446579CurrentTrain: epoch  0, batch   107 | loss: 35.1051449CurrentTrain: epoch  0, batch   108 | loss: 44.7746966CurrentTrain: epoch  0, batch   109 | loss: 37.1245857CurrentTrain: epoch  0, batch   110 | loss: 45.3999056CurrentTrain: epoch  0, batch   111 | loss: 45.6396276CurrentTrain: epoch  0, batch   112 | loss: 41.6492827CurrentTrain: epoch  0, batch   113 | loss: 36.6756459CurrentTrain: epoch  0, batch   114 | loss: 42.8727007CurrentTrain: epoch  0, batch   115 | loss: 43.8865289CurrentTrain: epoch  0, batch   116 | loss: 57.2177886CurrentTrain: epoch  0, batch   117 | loss: 53.2179889CurrentTrain: epoch  0, batch   118 | loss: 43.9802270CurrentTrain: epoch  0, batch   119 | loss: 50.4185518CurrentTrain: epoch  0, batch   120 | loss: 53.1881838CurrentTrain: epoch  0, batch   121 | loss: 36.4614053CurrentTrain: epoch  0, batch   122 | loss: 44.3465573CurrentTrain: epoch  0, batch   123 | loss: 42.3932541CurrentTrain: epoch  0, batch   124 | loss: 54.2643165CurrentTrain: epoch  0, batch   125 | loss: 51.6194154CurrentTrain: epoch  0, batch   126 | loss: 112.5003904CurrentTrain: epoch  0, batch   127 | loss: 41.0232968CurrentTrain: epoch  0, batch   128 | loss: 43.7025875CurrentTrain: epoch  0, batch   129 | loss: 35.7635568CurrentTrain: epoch  0, batch   130 | loss: 42.3317959CurrentTrain: epoch  0, batch   131 | loss: 71.4758647CurrentTrain: epoch  0, batch   132 | loss: 68.7259413CurrentTrain: epoch  0, batch   133 | loss: 72.8369075CurrentTrain: epoch  0, batch   134 | loss: 71.7395067CurrentTrain: epoch  0, batch   135 | loss: 52.7579071CurrentTrain: epoch  0, batch   136 | loss: 41.4956408CurrentTrain: epoch  0, batch   137 | loss: 39.9421410CurrentTrain: epoch  0, batch   138 | loss: 53.8095425CurrentTrain: epoch  0, batch   139 | loss: 41.7292285CurrentTrain: epoch  0, batch   140 | loss: 43.1206917CurrentTrain: epoch  0, batch   141 | loss: 40.6319131CurrentTrain: epoch  0, batch   142 | loss: 54.1858048CurrentTrain: epoch  0, batch   143 | loss: 40.9921467CurrentTrain: epoch  1, batch     0 | loss: 51.7940207CurrentTrain: epoch  1, batch     1 | loss: 28.7021991CurrentTrain: epoch  1, batch     2 | loss: 41.9614906CurrentTrain: epoch  1, batch     3 | loss: 49.6738658CurrentTrain: epoch  1, batch     4 | loss: 111.0450512CurrentTrain: epoch  1, batch     5 | loss: 55.2529882CurrentTrain: epoch  1, batch     6 | loss: 106.1577296CurrentTrain: epoch  1, batch     7 | loss: 41.3265286CurrentTrain: epoch  1, batch     8 | loss: 42.4187133CurrentTrain: epoch  1, batch     9 | loss: 41.4722879CurrentTrain: epoch  1, batch    10 | loss: 48.5307875CurrentTrain: epoch  1, batch    11 | loss: 39.8866364CurrentTrain: epoch  1, batch    12 | loss: 39.2433104CurrentTrain: epoch  1, batch    13 | loss: 75.7217334CurrentTrain: epoch  1, batch    14 | loss: 38.8871853CurrentTrain: epoch  1, batch    15 | loss: 54.8053389CurrentTrain: epoch  1, batch    16 | loss: 36.7189934CurrentTrain: epoch  1, batch    17 | loss: 69.9631151CurrentTrain: epoch  1, batch    18 | loss: 39.9004285CurrentTrain: epoch  1, batch    19 | loss: 29.0082268CurrentTrain: epoch  1, batch    20 | loss: 71.4974622CurrentTrain: epoch  1, batch    21 | loss: 41.5415491CurrentTrain: epoch  1, batch    22 | loss: 110.2153716CurrentTrain: epoch  1, batch    23 | loss: 73.4673790CurrentTrain: epoch  1, batch    24 | loss: 52.2784892CurrentTrain: epoch  1, batch    25 | loss: 41.0115826CurrentTrain: epoch  1, batch    26 | loss: 72.2793499CurrentTrain: epoch  1, batch    27 | loss: 51.5238074CurrentTrain: epoch  1, batch    28 | loss: 36.5015184CurrentTrain: epoch  1, batch    29 | loss: 54.2649877CurrentTrain: epoch  1, batch    30 | loss: 53.8742791CurrentTrain: epoch  1, batch    31 | loss: 39.2958011CurrentTrain: epoch  1, batch    32 | loss: 38.3629829CurrentTrain: epoch  1, batch    33 | loss: 36.7952174CurrentTrain: epoch  1, batch    34 | loss: 39.7267805CurrentTrain: epoch  1, batch    35 | loss: 52.4371026CurrentTrain: epoch  1, batch    36 | loss: 56.2506691CurrentTrain: epoch  1, batch    37 | loss: 40.6456811CurrentTrain: epoch  1, batch    38 | loss: 52.0962952CurrentTrain: epoch  1, batch    39 | loss: 71.5418584CurrentTrain: epoch  1, batch    40 | loss: 53.2086544CurrentTrain: epoch  1, batch    41 | loss: 69.7212410CurrentTrain: epoch  1, batch    42 | loss: 107.8927623CurrentTrain: epoch  1, batch    43 | loss: 52.0892863CurrentTrain: epoch  1, batch    44 | loss: 42.6779038CurrentTrain: epoch  1, batch    45 | loss: 41.1893164CurrentTrain: epoch  1, batch    46 | loss: 72.9644487CurrentTrain: epoch  1, batch    47 | loss: 72.8040660CurrentTrain: epoch  1, batch    48 | loss: 39.6182691CurrentTrain: epoch  1, batch    49 | loss: 38.9414155CurrentTrain: epoch  1, batch    50 | loss: 52.5002743CurrentTrain: epoch  1, batch    51 | loss: 69.6612342CurrentTrain: epoch  1, batch    52 | loss: 39.9326817CurrentTrain: epoch  1, batch    53 | loss: 70.4280602CurrentTrain: epoch  1, batch    54 | loss: 52.3772786CurrentTrain: epoch  1, batch    55 | loss: 69.3478403CurrentTrain: epoch  1, batch    56 | loss: 70.4903904CurrentTrain: epoch  1, batch    57 | loss: 110.4490822CurrentTrain: epoch  1, batch    58 | loss: 71.6194945CurrentTrain: epoch  1, batch    59 | loss: 69.7305962CurrentTrain: epoch  1, batch    60 | loss: 48.9911023CurrentTrain: epoch  1, batch    61 | loss: 40.8923839CurrentTrain: epoch  1, batch    62 | loss: 41.2610419CurrentTrain: epoch  1, batch    63 | loss: 42.0971200CurrentTrain: epoch  1, batch    64 | loss: 34.7893133CurrentTrain: epoch  1, batch    65 | loss: 111.1492087CurrentTrain: epoch  1, batch    66 | loss: 41.4364275CurrentTrain: epoch  1, batch    67 | loss: 72.1990568CurrentTrain: epoch  1, batch    68 | loss: 39.5550167CurrentTrain: epoch  1, batch    69 | loss: 72.0690938CurrentTrain: epoch  1, batch    70 | loss: 52.3112425CurrentTrain: epoch  1, batch    71 | loss: 53.4362926CurrentTrain: epoch  1, batch    72 | loss: 51.9397056CurrentTrain: epoch  1, batch    73 | loss: 72.3843114CurrentTrain: epoch  1, batch    74 | loss: 70.7540236CurrentTrain: epoch  1, batch    75 | loss: 73.3386310CurrentTrain: epoch  1, batch    76 | loss: 38.1027871CurrentTrain: epoch  1, batch    77 | loss: 53.1484650CurrentTrain: epoch  1, batch    78 | loss: 32.6413553CurrentTrain: epoch  1, batch    79 | loss: 69.6907634CurrentTrain: epoch  1, batch    80 | loss: 53.0285590CurrentTrain: epoch  1, batch    81 | loss: 50.3749982CurrentTrain: epoch  1, batch    82 | loss: 39.1011392CurrentTrain: epoch  1, batch    83 | loss: 50.6884321CurrentTrain: epoch  1, batch    84 | loss: 43.6856404CurrentTrain: epoch  1, batch    85 | loss: 50.4874176CurrentTrain: epoch  1, batch    86 | loss: 52.6852977CurrentTrain: epoch  1, batch    87 | loss: 70.7010819CurrentTrain: epoch  1, batch    88 | loss: 38.8584333CurrentTrain: epoch  1, batch    89 | loss: 69.4933267CurrentTrain: epoch  1, batch    90 | loss: 35.3903969CurrentTrain: epoch  1, batch    91 | loss: 39.5237759CurrentTrain: epoch  1, batch    92 | loss: 69.4743427CurrentTrain: epoch  1, batch    93 | loss: 54.9052349CurrentTrain: epoch  1, batch    94 | loss: 55.7853203CurrentTrain: epoch  1, batch    95 | loss: 222.5974955CurrentTrain: epoch  1, batch    96 | loss: 48.1506906CurrentTrain: epoch  1, batch    97 | loss: 51.3817108CurrentTrain: epoch  1, batch    98 | loss: 71.2287744CurrentTrain: epoch  1, batch    99 | loss: 71.6203320CurrentTrain: epoch  1, batch   100 | loss: 31.2359115CurrentTrain: epoch  1, batch   101 | loss: 39.2810575CurrentTrain: epoch  1, batch   102 | loss: 51.9410790CurrentTrain: epoch  1, batch   103 | loss: 67.5251280CurrentTrain: epoch  1, batch   104 | loss: 51.8252272CurrentTrain: epoch  1, batch   105 | loss: 48.7746110CurrentTrain: epoch  1, batch   106 | loss: 52.8096208CurrentTrain: epoch  1, batch   107 | loss: 53.2719429CurrentTrain: epoch  1, batch   108 | loss: 72.8096418CurrentTrain: epoch  1, batch   109 | loss: 49.7802102CurrentTrain: epoch  1, batch   110 | loss: 43.5662233CurrentTrain: epoch  1, batch   111 | loss: 110.5465293CurrentTrain: epoch  1, batch   112 | loss: 48.0500363CurrentTrain: epoch  1, batch   113 | loss: 47.8833982CurrentTrain: epoch  1, batch   114 | loss: 69.8645639CurrentTrain: epoch  1, batch   115 | loss: 50.0242471CurrentTrain: epoch  1, batch   116 | loss: 41.5236920CurrentTrain: epoch  1, batch   117 | loss: 50.0895290CurrentTrain: epoch  1, batch   118 | loss: 37.1382110CurrentTrain: epoch  1, batch   119 | loss: 65.7866280CurrentTrain: epoch  1, batch   120 | loss: 68.3548098CurrentTrain: epoch  1, batch   121 | loss: 67.8712071CurrentTrain: epoch  1, batch   122 | loss: 39.0505487CurrentTrain: epoch  1, batch   123 | loss: 50.2394208CurrentTrain: epoch  1, batch   124 | loss: 55.0372877CurrentTrain: epoch  1, batch   125 | loss: 42.7397726CurrentTrain: epoch  1, batch   126 | loss: 78.9515211CurrentTrain: epoch  1, batch   127 | loss: 69.3205536CurrentTrain: epoch  1, batch   128 | loss: 74.8019296CurrentTrain: epoch  1, batch   129 | loss: 71.6291397CurrentTrain: epoch  1, batch   130 | loss: 31.5319411CurrentTrain: epoch  1, batch   131 | loss: 48.8416056CurrentTrain: epoch  1, batch   132 | loss: 49.6576440CurrentTrain: epoch  1, batch   133 | loss: 25.4212409CurrentTrain: epoch  1, batch   134 | loss: 38.6411666CurrentTrain: epoch  1, batch   135 | loss: 50.2726917CurrentTrain: epoch  1, batch   136 | loss: 51.5884342CurrentTrain: epoch  1, batch   137 | loss: 71.0737648CurrentTrain: epoch  1, batch   138 | loss: 52.7194508CurrentTrain: epoch  1, batch   139 | loss: 41.0669051CurrentTrain: epoch  1, batch   140 | loss: 66.9800610CurrentTrain: epoch  1, batch   141 | loss: 50.2943700CurrentTrain: epoch  1, batch   142 | loss: 52.1906423CurrentTrain: epoch  1, batch   143 | loss: 53.5406459CurrentTrain: epoch  2, batch     0 | loss: 48.3265730CurrentTrain: epoch  2, batch     1 | loss: 106.7474666CurrentTrain: epoch  2, batch     2 | loss: 51.6204885CurrentTrain: epoch  2, batch     3 | loss: 70.3888937CurrentTrain: epoch  2, batch     4 | loss: 47.0884785CurrentTrain: epoch  2, batch     5 | loss: 34.2447830CurrentTrain: epoch  2, batch     6 | loss: 50.2540457CurrentTrain: epoch  2, batch     7 | loss: 31.9307547CurrentTrain: epoch  2, batch     8 | loss: 37.6280568CurrentTrain: epoch  2, batch     9 | loss: 49.2769787CurrentTrain: epoch  2, batch    10 | loss: 48.7845869CurrentTrain: epoch  2, batch    11 | loss: 68.3109895CurrentTrain: epoch  2, batch    12 | loss: 66.0600108CurrentTrain: epoch  2, batch    13 | loss: 51.0280607CurrentTrain: epoch  2, batch    14 | loss: 49.1112767CurrentTrain: epoch  2, batch    15 | loss: 68.2349736CurrentTrain: epoch  2, batch    16 | loss: 107.3865794CurrentTrain: epoch  2, batch    17 | loss: 32.8146839CurrentTrain: epoch  2, batch    18 | loss: 40.3570599CurrentTrain: epoch  2, batch    19 | loss: 107.4016838CurrentTrain: epoch  2, batch    20 | loss: 51.3530367CurrentTrain: epoch  2, batch    21 | loss: 41.9648264CurrentTrain: epoch  2, batch    22 | loss: 48.4175357CurrentTrain: epoch  2, batch    23 | loss: 50.0670233CurrentTrain: epoch  2, batch    24 | loss: 69.3076828CurrentTrain: epoch  2, batch    25 | loss: 68.7606302CurrentTrain: epoch  2, batch    26 | loss: 37.6534020CurrentTrain: epoch  2, batch    27 | loss: 52.2112858CurrentTrain: epoch  2, batch    28 | loss: 39.0272356CurrentTrain: epoch  2, batch    29 | loss: 31.4456480CurrentTrain: epoch  2, batch    30 | loss: 46.8580282CurrentTrain: epoch  2, batch    31 | loss: 59.8931546CurrentTrain: epoch  2, batch    32 | loss: 31.6060355CurrentTrain: epoch  2, batch    33 | loss: 70.7842389CurrentTrain: epoch  2, batch    34 | loss: 107.9351910CurrentTrain: epoch  2, batch    35 | loss: 32.7588874CurrentTrain: epoch  2, batch    36 | loss: 50.7132600CurrentTrain: epoch  2, batch    37 | loss: 38.6731769CurrentTrain: epoch  2, batch    38 | loss: 70.2455548CurrentTrain: epoch  2, batch    39 | loss: 50.1734264CurrentTrain: epoch  2, batch    40 | loss: 38.2420147CurrentTrain: epoch  2, batch    41 | loss: 52.0751170CurrentTrain: epoch  2, batch    42 | loss: 67.8259796CurrentTrain: epoch  2, batch    43 | loss: 67.9590755CurrentTrain: epoch  2, batch    44 | loss: 111.9097307CurrentTrain: epoch  2, batch    45 | loss: 31.4517119CurrentTrain: epoch  2, batch    46 | loss: 50.4533164CurrentTrain: epoch  2, batch    47 | loss: 49.3593710CurrentTrain: epoch  2, batch    48 | loss: 67.4720945CurrentTrain: epoch  2, batch    49 | loss: 52.3256506CurrentTrain: epoch  2, batch    50 | loss: 66.9854668CurrentTrain: epoch  2, batch    51 | loss: 39.9488603CurrentTrain: epoch  2, batch    52 | loss: 43.7807589CurrentTrain: epoch  2, batch    53 | loss: 39.1120552CurrentTrain: epoch  2, batch    54 | loss: 50.3070459CurrentTrain: epoch  2, batch    55 | loss: 72.7378838CurrentTrain: epoch  2, batch    56 | loss: 50.2606269CurrentTrain: epoch  2, batch    57 | loss: 36.2993337CurrentTrain: epoch  2, batch    58 | loss: 44.9528953CurrentTrain: epoch  2, batch    59 | loss: 51.6122184CurrentTrain: epoch  2, batch    60 | loss: 49.2022168CurrentTrain: epoch  2, batch    61 | loss: 40.8222862CurrentTrain: epoch  2, batch    62 | loss: 24.4264825CurrentTrain: epoch  2, batch    63 | loss: 49.5484571CurrentTrain: epoch  2, batch    64 | loss: 52.5328644CurrentTrain: epoch  2, batch    65 | loss: 50.0293544CurrentTrain: epoch  2, batch    66 | loss: 107.2736121CurrentTrain: epoch  2, batch    67 | loss: 50.0971658CurrentTrain: epoch  2, batch    68 | loss: 36.1982989CurrentTrain: epoch  2, batch    69 | loss: 71.1972198CurrentTrain: epoch  2, batch    70 | loss: 32.5120966CurrentTrain: epoch  2, batch    71 | loss: 37.6865720CurrentTrain: epoch  2, batch    72 | loss: 40.9415710CurrentTrain: epoch  2, batch    73 | loss: 53.3855489CurrentTrain: epoch  2, batch    74 | loss: 37.6686336CurrentTrain: epoch  2, batch    75 | loss: 49.3825364CurrentTrain: epoch  2, batch    76 | loss: 48.2131411CurrentTrain: epoch  2, batch    77 | loss: 52.4966637CurrentTrain: epoch  2, batch    78 | loss: 40.3774265CurrentTrain: epoch  2, batch    79 | loss: 48.8094736CurrentTrain: epoch  2, batch    80 | loss: 49.1290652CurrentTrain: epoch  2, batch    81 | loss: 51.5329211CurrentTrain: epoch  2, batch    82 | loss: 71.4577305CurrentTrain: epoch  2, batch    83 | loss: 44.5681797CurrentTrain: epoch  2, batch    84 | loss: 44.7473009CurrentTrain: epoch  2, batch    85 | loss: 70.4323902CurrentTrain: epoch  2, batch    86 | loss: 69.2880361CurrentTrain: epoch  2, batch    87 | loss: 49.2927993CurrentTrain: epoch  2, batch    88 | loss: 38.3029129CurrentTrain: epoch  2, batch    89 | loss: 69.7646131CurrentTrain: epoch  2, batch    90 | loss: 36.6858148CurrentTrain: epoch  2, batch    91 | loss: 68.8532991CurrentTrain: epoch  2, batch    92 | loss: 52.8287447CurrentTrain: epoch  2, batch    93 | loss: 32.1928538CurrentTrain: epoch  2, batch    94 | loss: 28.6569336CurrentTrain: epoch  2, batch    95 | loss: 43.1499603CurrentTrain: epoch  2, batch    96 | loss: 49.3361580CurrentTrain: epoch  2, batch    97 | loss: 32.2355521CurrentTrain: epoch  2, batch    98 | loss: 50.1409104CurrentTrain: epoch  2, batch    99 | loss: 68.4167297CurrentTrain: epoch  2, batch   100 | loss: 50.4203366CurrentTrain: epoch  2, batch   101 | loss: 73.2180052CurrentTrain: epoch  2, batch   102 | loss: 37.0870139CurrentTrain: epoch  2, batch   103 | loss: 57.2760604CurrentTrain: epoch  2, batch   104 | loss: 66.6609926CurrentTrain: epoch  2, batch   105 | loss: 40.4001873CurrentTrain: epoch  2, batch   106 | loss: 71.9660064CurrentTrain: epoch  2, batch   107 | loss: 51.1349866CurrentTrain: epoch  2, batch   108 | loss: 104.3415171CurrentTrain: epoch  2, batch   109 | loss: 107.8295738CurrentTrain: epoch  2, batch   110 | loss: 53.2297208CurrentTrain: epoch  2, batch   111 | loss: 39.8793523CurrentTrain: epoch  2, batch   112 | loss: 54.0055985CurrentTrain: epoch  2, batch   113 | loss: 40.0796532CurrentTrain: epoch  2, batch   114 | loss: 50.1522451CurrentTrain: epoch  2, batch   115 | loss: 47.3914066CurrentTrain: epoch  2, batch   116 | loss: 68.1561060CurrentTrain: epoch  2, batch   117 | loss: 66.1680507CurrentTrain: epoch  2, batch   118 | loss: 39.1847192CurrentTrain: epoch  2, batch   119 | loss: 33.1652939CurrentTrain: epoch  2, batch   120 | loss: 39.9446694CurrentTrain: epoch  2, batch   121 | loss: 68.5989081CurrentTrain: epoch  2, batch   122 | loss: 49.1217208CurrentTrain: epoch  2, batch   123 | loss: 52.9081768CurrentTrain: epoch  2, batch   124 | loss: 48.2919162CurrentTrain: epoch  2, batch   125 | loss: 69.6809746CurrentTrain: epoch  2, batch   126 | loss: 52.2751483CurrentTrain: epoch  2, batch   127 | loss: 40.4771976CurrentTrain: epoch  2, batch   128 | loss: 38.6154788CurrentTrain: epoch  2, batch   129 | loss: 52.0323569CurrentTrain: epoch  2, batch   130 | loss: 39.1242889CurrentTrain: epoch  2, batch   131 | loss: 68.3581958CurrentTrain: epoch  2, batch   132 | loss: 42.6407075CurrentTrain: epoch  2, batch   133 | loss: 51.1322567CurrentTrain: epoch  2, batch   134 | loss: 42.1863868CurrentTrain: epoch  2, batch   135 | loss: 31.1965732CurrentTrain: epoch  2, batch   136 | loss: 70.9969203CurrentTrain: epoch  2, batch   137 | loss: 50.0476570CurrentTrain: epoch  2, batch   138 | loss: 110.3316407CurrentTrain: epoch  2, batch   139 | loss: 51.2468013CurrentTrain: epoch  2, batch   140 | loss: 47.5893639CurrentTrain: epoch  2, batch   141 | loss: 49.2547224CurrentTrain: epoch  2, batch   142 | loss: 41.5211336CurrentTrain: epoch  2, batch   143 | loss: 52.5894095CurrentTrain: epoch  3, batch     0 | loss: 68.2504752CurrentTrain: epoch  3, batch     1 | loss: 48.5752004CurrentTrain: epoch  3, batch     2 | loss: 39.4245305CurrentTrain: epoch  3, batch     3 | loss: 31.6980955CurrentTrain: epoch  3, batch     4 | loss: 38.2554373CurrentTrain: epoch  3, batch     5 | loss: 36.9046972CurrentTrain: epoch  3, batch     6 | loss: 73.2480543CurrentTrain: epoch  3, batch     7 | loss: 69.3052112CurrentTrain: epoch  3, batch     8 | loss: 39.0984934CurrentTrain: epoch  3, batch     9 | loss: 47.8187641CurrentTrain: epoch  3, batch    10 | loss: 38.4235298CurrentTrain: epoch  3, batch    11 | loss: 49.5895199CurrentTrain: epoch  3, batch    12 | loss: 70.0064857CurrentTrain: epoch  3, batch    13 | loss: 49.8747047CurrentTrain: epoch  3, batch    14 | loss: 69.6888641CurrentTrain: epoch  3, batch    15 | loss: 47.0099754CurrentTrain: epoch  3, batch    16 | loss: 37.9155369CurrentTrain: epoch  3, batch    17 | loss: 50.9035090CurrentTrain: epoch  3, batch    18 | loss: 36.6556114CurrentTrain: epoch  3, batch    19 | loss: 35.9256243CurrentTrain: epoch  3, batch    20 | loss: 66.4988754CurrentTrain: epoch  3, batch    21 | loss: 46.9537210CurrentTrain: epoch  3, batch    22 | loss: 69.9579079CurrentTrain: epoch  3, batch    23 | loss: 39.5544327CurrentTrain: epoch  3, batch    24 | loss: 49.8658813CurrentTrain: epoch  3, batch    25 | loss: 71.1084359CurrentTrain: epoch  3, batch    26 | loss: 71.6845241CurrentTrain: epoch  3, batch    27 | loss: 25.7975827CurrentTrain: epoch  3, batch    28 | loss: 38.1175379CurrentTrain: epoch  3, batch    29 | loss: 38.4660793CurrentTrain: epoch  3, batch    30 | loss: 48.7571682CurrentTrain: epoch  3, batch    31 | loss: 49.6212066CurrentTrain: epoch  3, batch    32 | loss: 37.6695833CurrentTrain: epoch  3, batch    33 | loss: 39.1396337CurrentTrain: epoch  3, batch    34 | loss: 52.0669440CurrentTrain: epoch  3, batch    35 | loss: 69.2741970CurrentTrain: epoch  3, batch    36 | loss: 48.1953234CurrentTrain: epoch  3, batch    37 | loss: 48.3005144CurrentTrain: epoch  3, batch    38 | loss: 68.5286591CurrentTrain: epoch  3, batch    39 | loss: 68.5832487CurrentTrain: epoch  3, batch    40 | loss: 36.9902033CurrentTrain: epoch  3, batch    41 | loss: 53.4741940CurrentTrain: epoch  3, batch    42 | loss: 36.1078284CurrentTrain: epoch  3, batch    43 | loss: 36.9011126CurrentTrain: epoch  3, batch    44 | loss: 69.1494049CurrentTrain: epoch  3, batch    45 | loss: 38.4628570CurrentTrain: epoch  3, batch    46 | loss: 48.5314733CurrentTrain: epoch  3, batch    47 | loss: 108.0660653CurrentTrain: epoch  3, batch    48 | loss: 107.6573594CurrentTrain: epoch  3, batch    49 | loss: 38.5238893CurrentTrain: epoch  3, batch    50 | loss: 38.9931345CurrentTrain: epoch  3, batch    51 | loss: 47.3489041CurrentTrain: epoch  3, batch    52 | loss: 49.9403978CurrentTrain: epoch  3, batch    53 | loss: 67.5440757CurrentTrain: epoch  3, batch    54 | loss: 37.3072557CurrentTrain: epoch  3, batch    55 | loss: 47.2256877CurrentTrain: epoch  3, batch    56 | loss: 50.3229452CurrentTrain: epoch  3, batch    57 | loss: 68.9227213CurrentTrain: epoch  3, batch    58 | loss: 51.0114220CurrentTrain: epoch  3, batch    59 | loss: 32.7200908CurrentTrain: epoch  3, batch    60 | loss: 47.9435624CurrentTrain: epoch  3, batch    61 | loss: 70.2043285CurrentTrain: epoch  3, batch    62 | loss: 68.3416445CurrentTrain: epoch  3, batch    63 | loss: 51.2725344CurrentTrain: epoch  3, batch    64 | loss: 43.4460451CurrentTrain: epoch  3, batch    65 | loss: 49.0892074CurrentTrain: epoch  3, batch    66 | loss: 69.0203567CurrentTrain: epoch  3, batch    67 | loss: 38.2500126CurrentTrain: epoch  3, batch    68 | loss: 40.8326883CurrentTrain: epoch  3, batch    69 | loss: 68.5111350CurrentTrain: epoch  3, batch    70 | loss: 49.3699684CurrentTrain: epoch  3, batch    71 | loss: 31.9064063CurrentTrain: epoch  3, batch    72 | loss: 68.6293128CurrentTrain: epoch  3, batch    73 | loss: 69.3965183CurrentTrain: epoch  3, batch    74 | loss: 33.5394496CurrentTrain: epoch  3, batch    75 | loss: 40.8431670CurrentTrain: epoch  3, batch    76 | loss: 47.3981995CurrentTrain: epoch  3, batch    77 | loss: 65.7497053CurrentTrain: epoch  3, batch    78 | loss: 51.1337031CurrentTrain: epoch  3, batch    79 | loss: 107.2203136CurrentTrain: epoch  3, batch    80 | loss: 40.4748866CurrentTrain: epoch  3, batch    81 | loss: 36.2687399CurrentTrain: epoch  3, batch    82 | loss: 67.8609948CurrentTrain: epoch  3, batch    83 | loss: 36.5205748CurrentTrain: epoch  3, batch    84 | loss: 66.6211109CurrentTrain: epoch  3, batch    85 | loss: 36.8918379CurrentTrain: epoch  3, batch    86 | loss: 30.4028767CurrentTrain: epoch  3, batch    87 | loss: 47.7825117CurrentTrain: epoch  3, batch    88 | loss: 38.8310529CurrentTrain: epoch  3, batch    89 | loss: 50.8554900CurrentTrain: epoch  3, batch    90 | loss: 29.9847672CurrentTrain: epoch  3, batch    91 | loss: 49.0730048CurrentTrain: epoch  3, batch    92 | loss: 38.8037091CurrentTrain: epoch  3, batch    93 | loss: 50.8141976CurrentTrain: epoch  3, batch    94 | loss: 108.8814247CurrentTrain: epoch  3, batch    95 | loss: 52.2897451CurrentTrain: epoch  3, batch    96 | loss: 64.4381886CurrentTrain: epoch  3, batch    97 | loss: 65.2621671CurrentTrain: epoch  3, batch    98 | loss: 50.7652052CurrentTrain: epoch  3, batch    99 | loss: 37.4651379CurrentTrain: epoch  3, batch   100 | loss: 41.6442340CurrentTrain: epoch  3, batch   101 | loss: 106.9071386CurrentTrain: epoch  3, batch   102 | loss: 50.2513874CurrentTrain: epoch  3, batch   103 | loss: 68.8605654CurrentTrain: epoch  3, batch   104 | loss: 66.6516081CurrentTrain: epoch  3, batch   105 | loss: 33.2389589CurrentTrain: epoch  3, batch   106 | loss: 32.4251500CurrentTrain: epoch  3, batch   107 | loss: 38.4368895CurrentTrain: epoch  3, batch   108 | loss: 49.6347081CurrentTrain: epoch  3, batch   109 | loss: 70.9614670CurrentTrain: epoch  3, batch   110 | loss: 68.3382705CurrentTrain: epoch  3, batch   111 | loss: 38.0401256CurrentTrain: epoch  3, batch   112 | loss: 51.5510021CurrentTrain: epoch  3, batch   113 | loss: 76.4688678CurrentTrain: epoch  3, batch   114 | loss: 38.4313551CurrentTrain: epoch  3, batch   115 | loss: 37.8926403CurrentTrain: epoch  3, batch   116 | loss: 44.9385368CurrentTrain: epoch  3, batch   117 | loss: 37.3872868CurrentTrain: epoch  3, batch   118 | loss: 49.6664065CurrentTrain: epoch  3, batch   119 | loss: 62.8759918CurrentTrain: epoch  3, batch   120 | loss: 70.0135900CurrentTrain: epoch  3, batch   121 | loss: 39.7058897CurrentTrain: epoch  3, batch   122 | loss: 222.3723154CurrentTrain: epoch  3, batch   123 | loss: 107.2540329CurrentTrain: epoch  3, batch   124 | loss: 68.3991504CurrentTrain: epoch  3, batch   125 | loss: 68.8920460CurrentTrain: epoch  3, batch   126 | loss: 49.4834911CurrentTrain: epoch  3, batch   127 | loss: 51.4850418CurrentTrain: epoch  3, batch   128 | loss: 68.5353194CurrentTrain: epoch  3, batch   129 | loss: 36.5290481CurrentTrain: epoch  3, batch   130 | loss: 39.4908137CurrentTrain: epoch  3, batch   131 | loss: 48.4212328CurrentTrain: epoch  3, batch   132 | loss: 36.5884753CurrentTrain: epoch  3, batch   133 | loss: 36.5984806CurrentTrain: epoch  3, batch   134 | loss: 30.2366295CurrentTrain: epoch  3, batch   135 | loss: 40.5915944CurrentTrain: epoch  3, batch   136 | loss: 29.4088091CurrentTrain: epoch  3, batch   137 | loss: 68.5027624CurrentTrain: epoch  3, batch   138 | loss: 45.7427486CurrentTrain: epoch  3, batch   139 | loss: 38.3772606CurrentTrain: epoch  3, batch   140 | loss: 39.0733627CurrentTrain: epoch  3, batch   141 | loss: 36.3583876CurrentTrain: epoch  3, batch   142 | loss: 105.9693486CurrentTrain: epoch  3, batch   143 | loss: 35.7655462CurrentTrain: epoch  4, batch     0 | loss: 49.2874751CurrentTrain: epoch  4, batch     1 | loss: 49.1792797CurrentTrain: epoch  4, batch     2 | loss: 30.8176461CurrentTrain: epoch  4, batch     3 | loss: 40.0466561CurrentTrain: epoch  4, batch     4 | loss: 34.6519211CurrentTrain: epoch  4, batch     5 | loss: 67.0453227CurrentTrain: epoch  4, batch     6 | loss: 38.5980988CurrentTrain: epoch  4, batch     7 | loss: 66.6010411CurrentTrain: epoch  4, batch     8 | loss: 37.2016525CurrentTrain: epoch  4, batch     9 | loss: 68.4452465CurrentTrain: epoch  4, batch    10 | loss: 103.6588817CurrentTrain: epoch  4, batch    11 | loss: 70.8274565CurrentTrain: epoch  4, batch    12 | loss: 69.8393005CurrentTrain: epoch  4, batch    13 | loss: 67.3205737CurrentTrain: epoch  4, batch    14 | loss: 68.3139027CurrentTrain: epoch  4, batch    15 | loss: 29.7852251CurrentTrain: epoch  4, batch    16 | loss: 37.9799953CurrentTrain: epoch  4, batch    17 | loss: 39.7522196CurrentTrain: epoch  4, batch    18 | loss: 44.2786659CurrentTrain: epoch  4, batch    19 | loss: 68.6039238CurrentTrain: epoch  4, batch    20 | loss: 36.9913279CurrentTrain: epoch  4, batch    21 | loss: 68.4733580CurrentTrain: epoch  4, batch    22 | loss: 47.6677027CurrentTrain: epoch  4, batch    23 | loss: 35.8600396CurrentTrain: epoch  4, batch    24 | loss: 68.5919743CurrentTrain: epoch  4, batch    25 | loss: 102.5925857CurrentTrain: epoch  4, batch    26 | loss: 30.5093556CurrentTrain: epoch  4, batch    27 | loss: 47.9095250CurrentTrain: epoch  4, batch    28 | loss: 33.3914780CurrentTrain: epoch  4, batch    29 | loss: 74.4936894CurrentTrain: epoch  4, batch    30 | loss: 38.0456924CurrentTrain: epoch  4, batch    31 | loss: 37.1480869CurrentTrain: epoch  4, batch    32 | loss: 49.2823932CurrentTrain: epoch  4, batch    33 | loss: 36.1763710CurrentTrain: epoch  4, batch    34 | loss: 50.1848360CurrentTrain: epoch  4, batch    35 | loss: 67.9439999CurrentTrain: epoch  4, batch    36 | loss: 68.5447865CurrentTrain: epoch  4, batch    37 | loss: 39.0606604CurrentTrain: epoch  4, batch    38 | loss: 103.6927456CurrentTrain: epoch  4, batch    39 | loss: 49.2846442CurrentTrain: epoch  4, batch    40 | loss: 36.3717537CurrentTrain: epoch  4, batch    41 | loss: 33.7962816CurrentTrain: epoch  4, batch    42 | loss: 30.7521131CurrentTrain: epoch  4, batch    43 | loss: 49.0971857CurrentTrain: epoch  4, batch    44 | loss: 108.5523269CurrentTrain: epoch  4, batch    45 | loss: 37.8636029CurrentTrain: epoch  4, batch    46 | loss: 30.4702836CurrentTrain: epoch  4, batch    47 | loss: 38.2726545CurrentTrain: epoch  4, batch    48 | loss: 25.2081946CurrentTrain: epoch  4, batch    49 | loss: 38.3095373CurrentTrain: epoch  4, batch    50 | loss: 67.1343745CurrentTrain: epoch  4, batch    51 | loss: 24.0462736CurrentTrain: epoch  4, batch    52 | loss: 28.8462114CurrentTrain: epoch  4, batch    53 | loss: 29.7600312CurrentTrain: epoch  4, batch    54 | loss: 222.3211954CurrentTrain: epoch  4, batch    55 | loss: 37.8194196CurrentTrain: epoch  4, batch    56 | loss: 47.8871849CurrentTrain: epoch  4, batch    57 | loss: 68.6825105CurrentTrain: epoch  4, batch    58 | loss: 50.5694090CurrentTrain: epoch  4, batch    59 | loss: 70.0538158CurrentTrain: epoch  4, batch    60 | loss: 38.5606697CurrentTrain: epoch  4, batch    61 | loss: 49.5082991CurrentTrain: epoch  4, batch    62 | loss: 64.7096328CurrentTrain: epoch  4, batch    63 | loss: 37.9266975CurrentTrain: epoch  4, batch    64 | loss: 49.0483657CurrentTrain: epoch  4, batch    65 | loss: 39.3750661CurrentTrain: epoch  4, batch    66 | loss: 48.6305772CurrentTrain: epoch  4, batch    67 | loss: 50.0599241CurrentTrain: epoch  4, batch    68 | loss: 36.9608160CurrentTrain: epoch  4, batch    69 | loss: 47.6096925CurrentTrain: epoch  4, batch    70 | loss: 222.3350486CurrentTrain: epoch  4, batch    71 | loss: 28.4792021CurrentTrain: epoch  4, batch    72 | loss: 36.0122787CurrentTrain: epoch  4, batch    73 | loss: 36.3955279CurrentTrain: epoch  4, batch    74 | loss: 47.9472853CurrentTrain: epoch  4, batch    75 | loss: 49.2581973CurrentTrain: epoch  4, batch    76 | loss: 66.9232379CurrentTrain: epoch  4, batch    77 | loss: 107.1298303CurrentTrain: epoch  4, batch    78 | loss: 48.0901626CurrentTrain: epoch  4, batch    79 | loss: 66.5848508CurrentTrain: epoch  4, batch    80 | loss: 68.7753696CurrentTrain: epoch  4, batch    81 | loss: 47.4260912CurrentTrain: epoch  4, batch    82 | loss: 29.9292735CurrentTrain: epoch  4, batch    83 | loss: 48.2227009CurrentTrain: epoch  4, batch    84 | loss: 36.6987154CurrentTrain: epoch  4, batch    85 | loss: 52.1108387CurrentTrain: epoch  4, batch    86 | loss: 35.0407644CurrentTrain: epoch  4, batch    87 | loss: 36.9094822CurrentTrain: epoch  4, batch    88 | loss: 66.5361016CurrentTrain: epoch  4, batch    89 | loss: 48.2698166CurrentTrain: epoch  4, batch    90 | loss: 29.7631084CurrentTrain: epoch  4, batch    91 | loss: 31.1004588CurrentTrain: epoch  4, batch    92 | loss: 222.3766945CurrentTrain: epoch  4, batch    93 | loss: 38.7352752CurrentTrain: epoch  4, batch    94 | loss: 37.8375514CurrentTrain: epoch  4, batch    95 | loss: 48.8006943CurrentTrain: epoch  4, batch    96 | loss: 51.4163840CurrentTrain: epoch  4, batch    97 | loss: 52.3428593CurrentTrain: epoch  4, batch    98 | loss: 29.3335385CurrentTrain: epoch  4, batch    99 | loss: 49.5793419CurrentTrain: epoch  4, batch   100 | loss: 37.5463962CurrentTrain: epoch  4, batch   101 | loss: 68.2090953CurrentTrain: epoch  4, batch   102 | loss: 23.7667925CurrentTrain: epoch  4, batch   103 | loss: 36.6655710CurrentTrain: epoch  4, batch   104 | loss: 45.4835755CurrentTrain: epoch  4, batch   105 | loss: 36.6026259CurrentTrain: epoch  4, batch   106 | loss: 29.0414516CurrentTrain: epoch  4, batch   107 | loss: 47.6340912CurrentTrain: epoch  4, batch   108 | loss: 48.9676239CurrentTrain: epoch  4, batch   109 | loss: 36.7461904CurrentTrain: epoch  4, batch   110 | loss: 68.9536025CurrentTrain: epoch  4, batch   111 | loss: 37.4776480CurrentTrain: epoch  4, batch   112 | loss: 49.2109013CurrentTrain: epoch  4, batch   113 | loss: 48.1155721CurrentTrain: epoch  4, batch   114 | loss: 109.8296127CurrentTrain: epoch  4, batch   115 | loss: 46.9775264CurrentTrain: epoch  4, batch   116 | loss: 66.2009892CurrentTrain: epoch  4, batch   117 | loss: 38.2585899CurrentTrain: epoch  4, batch   118 | loss: 37.5384061CurrentTrain: epoch  4, batch   119 | loss: 68.8682873CurrentTrain: epoch  4, batch   120 | loss: 49.9291112CurrentTrain: epoch  4, batch   121 | loss: 63.6013482CurrentTrain: epoch  4, batch   122 | loss: 38.9690112CurrentTrain: epoch  4, batch   123 | loss: 73.0151395CurrentTrain: epoch  4, batch   124 | loss: 64.7126486CurrentTrain: epoch  4, batch   125 | loss: 68.2300399CurrentTrain: epoch  4, batch   126 | loss: 66.8770610CurrentTrain: epoch  4, batch   127 | loss: 49.1950963CurrentTrain: epoch  4, batch   128 | loss: 48.6864567CurrentTrain: epoch  4, batch   129 | loss: 68.8169454CurrentTrain: epoch  4, batch   130 | loss: 36.3183298CurrentTrain: epoch  4, batch   131 | loss: 105.0482144CurrentTrain: epoch  4, batch   132 | loss: 45.4067559CurrentTrain: epoch  4, batch   133 | loss: 49.2653645CurrentTrain: epoch  4, batch   134 | loss: 48.7366329CurrentTrain: epoch  4, batch   135 | loss: 66.4860686CurrentTrain: epoch  4, batch   136 | loss: 49.8453745CurrentTrain: epoch  4, batch   137 | loss: 36.9184601CurrentTrain: epoch  4, batch   138 | loss: 73.8514785CurrentTrain: epoch  4, batch   139 | loss: 108.0979741CurrentTrain: epoch  4, batch   140 | loss: 69.8497807CurrentTrain: epoch  4, batch   141 | loss: 42.3027196CurrentTrain: epoch  4, batch   142 | loss: 107.2925116CurrentTrain: epoch  4, batch   143 | loss: 27.7944071CurrentTrain: epoch  5, batch     0 | loss: 27.9957633CurrentTrain: epoch  5, batch     1 | loss: 50.2841186CurrentTrain: epoch  5, batch     2 | loss: 45.3043490CurrentTrain: epoch  5, batch     3 | loss: 48.0061165CurrentTrain: epoch  5, batch     4 | loss: 35.6496576CurrentTrain: epoch  5, batch     5 | loss: 29.3276320CurrentTrain: epoch  5, batch     6 | loss: 35.0196736CurrentTrain: epoch  5, batch     7 | loss: 62.8831104CurrentTrain: epoch  5, batch     8 | loss: 222.2886924CurrentTrain: epoch  5, batch     9 | loss: 37.9983312CurrentTrain: epoch  5, batch    10 | loss: 37.0738249CurrentTrain: epoch  5, batch    11 | loss: 49.5333065CurrentTrain: epoch  5, batch    12 | loss: 49.8445126CurrentTrain: epoch  5, batch    13 | loss: 50.0378262CurrentTrain: epoch  5, batch    14 | loss: 35.8607772CurrentTrain: epoch  5, batch    15 | loss: 49.7632258CurrentTrain: epoch  5, batch    16 | loss: 45.1141535CurrentTrain: epoch  5, batch    17 | loss: 68.2990627CurrentTrain: epoch  5, batch    18 | loss: 103.6401831CurrentTrain: epoch  5, batch    19 | loss: 46.5227544CurrentTrain: epoch  5, batch    20 | loss: 68.9612129CurrentTrain: epoch  5, batch    21 | loss: 30.6890166CurrentTrain: epoch  5, batch    22 | loss: 47.6841388CurrentTrain: epoch  5, batch    23 | loss: 68.1807266CurrentTrain: epoch  5, batch    24 | loss: 39.7769744CurrentTrain: epoch  5, batch    25 | loss: 36.0657901CurrentTrain: epoch  5, batch    26 | loss: 70.2039019CurrentTrain: epoch  5, batch    27 | loss: 47.9914120CurrentTrain: epoch  5, batch    28 | loss: 47.7055125CurrentTrain: epoch  5, batch    29 | loss: 103.5896603CurrentTrain: epoch  5, batch    30 | loss: 47.6096744CurrentTrain: epoch  5, batch    31 | loss: 46.7695660CurrentTrain: epoch  5, batch    32 | loss: 47.6906969CurrentTrain: epoch  5, batch    33 | loss: 37.9323451CurrentTrain: epoch  5, batch    34 | loss: 36.5148228CurrentTrain: epoch  5, batch    35 | loss: 36.0753912CurrentTrain: epoch  5, batch    36 | loss: 45.4501201CurrentTrain: epoch  5, batch    37 | loss: 107.0098126CurrentTrain: epoch  5, batch    38 | loss: 107.3607017CurrentTrain: epoch  5, batch    39 | loss: 66.3237252CurrentTrain: epoch  5, batch    40 | loss: 48.2621865CurrentTrain: epoch  5, batch    41 | loss: 28.5137848CurrentTrain: epoch  5, batch    42 | loss: 66.5946561CurrentTrain: epoch  5, batch    43 | loss: 66.4294190CurrentTrain: epoch  5, batch    44 | loss: 47.8532340CurrentTrain: epoch  5, batch    45 | loss: 39.4954877CurrentTrain: epoch  5, batch    46 | loss: 106.8634098CurrentTrain: epoch  5, batch    47 | loss: 37.5861615CurrentTrain: epoch  5, batch    48 | loss: 108.8587590CurrentTrain: epoch  5, batch    49 | loss: 36.6587624CurrentTrain: epoch  5, batch    50 | loss: 29.9300014CurrentTrain: epoch  5, batch    51 | loss: 37.5339927CurrentTrain: epoch  5, batch    52 | loss: 35.4739564CurrentTrain: epoch  5, batch    53 | loss: 36.7607200CurrentTrain: epoch  5, batch    54 | loss: 29.0123260CurrentTrain: epoch  5, batch    55 | loss: 37.1016600CurrentTrain: epoch  5, batch    56 | loss: 65.2226721CurrentTrain: epoch  5, batch    57 | loss: 47.3491951CurrentTrain: epoch  5, batch    58 | loss: 67.0514033CurrentTrain: epoch  5, batch    59 | loss: 38.2572774CurrentTrain: epoch  5, batch    60 | loss: 68.7537607CurrentTrain: epoch  5, batch    61 | loss: 30.7173135CurrentTrain: epoch  5, batch    62 | loss: 36.2090963CurrentTrain: epoch  5, batch    63 | loss: 48.8344244CurrentTrain: epoch  5, batch    64 | loss: 66.1810436CurrentTrain: epoch  5, batch    65 | loss: 101.8449967CurrentTrain: epoch  5, batch    66 | loss: 52.4525152CurrentTrain: epoch  5, batch    67 | loss: 47.9914796CurrentTrain: epoch  5, batch    68 | loss: 35.2818459CurrentTrain: epoch  5, batch    69 | loss: 39.6119550CurrentTrain: epoch  5, batch    70 | loss: 39.8823219CurrentTrain: epoch  5, batch    71 | loss: 69.3402507CurrentTrain: epoch  5, batch    72 | loss: 36.5718108CurrentTrain: epoch  5, batch    73 | loss: 48.4221573CurrentTrain: epoch  5, batch    74 | loss: 39.9910472CurrentTrain: epoch  5, batch    75 | loss: 50.3982412CurrentTrain: epoch  5, batch    76 | loss: 25.7045510CurrentTrain: epoch  5, batch    77 | loss: 107.1539537CurrentTrain: epoch  5, batch    78 | loss: 111.4512856CurrentTrain: epoch  5, batch    79 | loss: 36.7988214CurrentTrain: epoch  5, batch    80 | loss: 46.2224837CurrentTrain: epoch  5, batch    81 | loss: 36.7782422CurrentTrain: epoch  5, batch    82 | loss: 48.1378229CurrentTrain: epoch  5, batch    83 | loss: 49.0110915CurrentTrain: epoch  5, batch    84 | loss: 47.5748454CurrentTrain: epoch  5, batch    85 | loss: 38.2997677CurrentTrain: epoch  5, batch    86 | loss: 48.0338001CurrentTrain: epoch  5, batch    87 | loss: 66.3515923CurrentTrain: epoch  5, batch    88 | loss: 222.2746290CurrentTrain: epoch  5, batch    89 | loss: 47.2912294CurrentTrain: epoch  5, batch    90 | loss: 47.6374134CurrentTrain: epoch  5, batch    91 | loss: 66.3198594CurrentTrain: epoch  5, batch    92 | loss: 35.4516310CurrentTrain: epoch  5, batch    93 | loss: 34.1171350CurrentTrain: epoch  5, batch    94 | loss: 68.3117413CurrentTrain: epoch  5, batch    95 | loss: 38.7032149CurrentTrain: epoch  5, batch    96 | loss: 68.1529858CurrentTrain: epoch  5, batch    97 | loss: 67.8781162CurrentTrain: epoch  5, batch    98 | loss: 36.6876014CurrentTrain: epoch  5, batch    99 | loss: 29.2025867CurrentTrain: epoch  5, batch   100 | loss: 40.2321048CurrentTrain: epoch  5, batch   101 | loss: 49.1228691CurrentTrain: epoch  5, batch   102 | loss: 47.8700653CurrentTrain: epoch  5, batch   103 | loss: 47.9281967CurrentTrain: epoch  5, batch   104 | loss: 49.4353560CurrentTrain: epoch  5, batch   105 | loss: 36.9650988CurrentTrain: epoch  5, batch   106 | loss: 222.1851370CurrentTrain: epoch  5, batch   107 | loss: 49.2069345CurrentTrain: epoch  5, batch   108 | loss: 65.1763898CurrentTrain: epoch  5, batch   109 | loss: 62.9935177CurrentTrain: epoch  5, batch   110 | loss: 68.9642323CurrentTrain: epoch  5, batch   111 | loss: 24.2303354CurrentTrain: epoch  5, batch   112 | loss: 51.7990103CurrentTrain: epoch  5, batch   113 | loss: 47.6281419CurrentTrain: epoch  5, batch   114 | loss: 55.4009425CurrentTrain: epoch  5, batch   115 | loss: 30.6295164CurrentTrain: epoch  5, batch   116 | loss: 31.2192338CurrentTrain: epoch  5, batch   117 | loss: 66.4105291CurrentTrain: epoch  5, batch   118 | loss: 63.8260753CurrentTrain: epoch  5, batch   119 | loss: 41.1329615CurrentTrain: epoch  5, batch   120 | loss: 30.0163479CurrentTrain: epoch  5, batch   121 | loss: 69.8311604CurrentTrain: epoch  5, batch   122 | loss: 68.2060568CurrentTrain: epoch  5, batch   123 | loss: 66.0751946CurrentTrain: epoch  5, batch   124 | loss: 34.5408873CurrentTrain: epoch  5, batch   125 | loss: 34.8637348CurrentTrain: epoch  5, batch   126 | loss: 46.4747031CurrentTrain: epoch  5, batch   127 | loss: 37.5039494CurrentTrain: epoch  5, batch   128 | loss: 37.7428430CurrentTrain: epoch  5, batch   129 | loss: 50.2011903CurrentTrain: epoch  5, batch   130 | loss: 47.7101430CurrentTrain: epoch  5, batch   131 | loss: 47.6248366CurrentTrain: epoch  5, batch   132 | loss: 107.1447368CurrentTrain: epoch  5, batch   133 | loss: 49.4247962CurrentTrain: epoch  5, batch   134 | loss: 71.8251072CurrentTrain: epoch  5, batch   135 | loss: 25.8704098CurrentTrain: epoch  5, batch   136 | loss: 49.6327400CurrentTrain: epoch  5, batch   137 | loss: 106.9130660CurrentTrain: epoch  5, batch   138 | loss: 33.3985222CurrentTrain: epoch  5, batch   139 | loss: 68.2428778CurrentTrain: epoch  5, batch   140 | loss: 66.0236066CurrentTrain: epoch  5, batch   141 | loss: 222.4280703CurrentTrain: epoch  5, batch   142 | loss: 43.7526936CurrentTrain: epoch  5, batch   143 | loss: 52.5196375CurrentTrain: epoch  6, batch     0 | loss: 49.1094342CurrentTrain: epoch  6, batch     1 | loss: 47.9770027CurrentTrain: epoch  6, batch     2 | loss: 48.4017974CurrentTrain: epoch  6, batch     3 | loss: 68.1192331CurrentTrain: epoch  6, batch     4 | loss: 68.0963109CurrentTrain: epoch  6, batch     5 | loss: 47.8944655CurrentTrain: epoch  6, batch     6 | loss: 68.8648464CurrentTrain: epoch  6, batch     7 | loss: 33.9416701CurrentTrain: epoch  6, batch     8 | loss: 68.3217356CurrentTrain: epoch  6, batch     9 | loss: 66.1317293CurrentTrain: epoch  6, batch    10 | loss: 49.5299961CurrentTrain: epoch  6, batch    11 | loss: 66.1786392CurrentTrain: epoch  6, batch    12 | loss: 66.2626816CurrentTrain: epoch  6, batch    13 | loss: 52.4452950CurrentTrain: epoch  6, batch    14 | loss: 28.9812262CurrentTrain: epoch  6, batch    15 | loss: 66.1947327CurrentTrain: epoch  6, batch    16 | loss: 64.2496303CurrentTrain: epoch  6, batch    17 | loss: 37.8747682CurrentTrain: epoch  6, batch    18 | loss: 68.1207418CurrentTrain: epoch  6, batch    19 | loss: 103.4782437CurrentTrain: epoch  6, batch    20 | loss: 68.1125002CurrentTrain: epoch  6, batch    21 | loss: 105.1529725CurrentTrain: epoch  6, batch    22 | loss: 36.7473123CurrentTrain: epoch  6, batch    23 | loss: 66.1749021CurrentTrain: epoch  6, batch    24 | loss: 27.5302915CurrentTrain: epoch  6, batch    25 | loss: 47.8226326CurrentTrain: epoch  6, batch    26 | loss: 103.5220125CurrentTrain: epoch  6, batch    27 | loss: 35.7073025CurrentTrain: epoch  6, batch    28 | loss: 34.3466049CurrentTrain: epoch  6, batch    29 | loss: 34.9471666CurrentTrain: epoch  6, batch    30 | loss: 33.7123804CurrentTrain: epoch  6, batch    31 | loss: 107.2700188CurrentTrain: epoch  6, batch    32 | loss: 35.8604980CurrentTrain: epoch  6, batch    33 | loss: 29.0742786CurrentTrain: epoch  6, batch    34 | loss: 106.7905586CurrentTrain: epoch  6, batch    35 | loss: 222.2030292CurrentTrain: epoch  6, batch    36 | loss: 64.6096927CurrentTrain: epoch  6, batch    37 | loss: 35.7905440CurrentTrain: epoch  6, batch    38 | loss: 38.1278722CurrentTrain: epoch  6, batch    39 | loss: 66.1852419CurrentTrain: epoch  6, batch    40 | loss: 106.8104954CurrentTrain: epoch  6, batch    41 | loss: 106.8215797CurrentTrain: epoch  6, batch    42 | loss: 45.1177957CurrentTrain: epoch  6, batch    43 | loss: 104.2356139CurrentTrain: epoch  6, batch    44 | loss: 41.2296545CurrentTrain: epoch  6, batch    45 | loss: 35.6297853CurrentTrain: epoch  6, batch    46 | loss: 66.2200026CurrentTrain: epoch  6, batch    47 | loss: 36.4066322CurrentTrain: epoch  6, batch    48 | loss: 66.1981039CurrentTrain: epoch  6, batch    49 | loss: 69.2732633CurrentTrain: epoch  6, batch    50 | loss: 48.9953043CurrentTrain: epoch  6, batch    51 | loss: 46.3097265CurrentTrain: epoch  6, batch    52 | loss: 106.9735530CurrentTrain: epoch  6, batch    53 | loss: 106.8294641CurrentTrain: epoch  6, batch    54 | loss: 34.7406853CurrentTrain: epoch  6, batch    55 | loss: 48.8958818CurrentTrain: epoch  6, batch    56 | loss: 39.3099847CurrentTrain: epoch  6, batch    57 | loss: 48.9534764CurrentTrain: epoch  6, batch    58 | loss: 69.4860393CurrentTrain: epoch  6, batch    59 | loss: 37.8176756CurrentTrain: epoch  6, batch    60 | loss: 36.8530644CurrentTrain: epoch  6, batch    61 | loss: 30.8293430CurrentTrain: epoch  6, batch    62 | loss: 27.4821101CurrentTrain: epoch  6, batch    63 | loss: 103.8518942CurrentTrain: epoch  6, batch    64 | loss: 35.5327704CurrentTrain: epoch  6, batch    65 | loss: 49.2941394CurrentTrain: epoch  6, batch    66 | loss: 66.5923323CurrentTrain: epoch  6, batch    67 | loss: 45.2254724CurrentTrain: epoch  6, batch    68 | loss: 106.8539584CurrentTrain: epoch  6, batch    69 | loss: 37.8573237CurrentTrain: epoch  6, batch    70 | loss: 33.8530579CurrentTrain: epoch  6, batch    71 | loss: 49.0234358CurrentTrain: epoch  6, batch    72 | loss: 38.1142927CurrentTrain: epoch  6, batch    73 | loss: 47.5818005CurrentTrain: epoch  6, batch    74 | loss: 49.0129163CurrentTrain: epoch  6, batch    75 | loss: 62.8198377CurrentTrain: epoch  6, batch    76 | loss: 29.1215258CurrentTrain: epoch  6, batch    77 | loss: 66.4020657CurrentTrain: epoch  6, batch    78 | loss: 64.7027500CurrentTrain: epoch  6, batch    79 | loss: 69.6780166CurrentTrain: epoch  6, batch    80 | loss: 24.6394236CurrentTrain: epoch  6, batch    81 | loss: 48.9016885CurrentTrain: epoch  6, batch    82 | loss: 106.7392533CurrentTrain: epoch  6, batch    83 | loss: 35.5592626CurrentTrain: epoch  6, batch    84 | loss: 106.9262709CurrentTrain: epoch  6, batch    85 | loss: 21.5596552CurrentTrain: epoch  6, batch    86 | loss: 65.1960654CurrentTrain: epoch  6, batch    87 | loss: 64.3476371CurrentTrain: epoch  6, batch    88 | loss: 66.3179590CurrentTrain: epoch  6, batch    89 | loss: 48.6053233CurrentTrain: epoch  6, batch    90 | loss: 48.9736763CurrentTrain: epoch  6, batch    91 | loss: 48.6911456CurrentTrain: epoch  6, batch    92 | loss: 47.6235489CurrentTrain: epoch  6, batch    93 | loss: 47.5417377CurrentTrain: epoch  6, batch    94 | loss: 29.8889875CurrentTrain: epoch  6, batch    95 | loss: 66.1392622CurrentTrain: epoch  6, batch    96 | loss: 48.4444232CurrentTrain: epoch  6, batch    97 | loss: 68.1455713CurrentTrain: epoch  6, batch    98 | loss: 32.0097870CurrentTrain: epoch  6, batch    99 | loss: 106.8423154CurrentTrain: epoch  6, batch   100 | loss: 38.0837086CurrentTrain: epoch  6, batch   101 | loss: 40.1548169CurrentTrain: epoch  6, batch   102 | loss: 35.4663805CurrentTrain: epoch  6, batch   103 | loss: 47.4208291CurrentTrain: epoch  6, batch   104 | loss: 36.3977497CurrentTrain: epoch  6, batch   105 | loss: 31.7856993CurrentTrain: epoch  6, batch   106 | loss: 44.2317028CurrentTrain: epoch  6, batch   107 | loss: 47.9732694CurrentTrain: epoch  6, batch   108 | loss: 106.8370798CurrentTrain: epoch  6, batch   109 | loss: 48.8973344CurrentTrain: epoch  6, batch   110 | loss: 64.8336415CurrentTrain: epoch  6, batch   111 | loss: 48.1220231CurrentTrain: epoch  6, batch   112 | loss: 33.8306473CurrentTrain: epoch  6, batch   113 | loss: 68.1194045CurrentTrain: epoch  6, batch   114 | loss: 37.0275375CurrentTrain: epoch  6, batch   115 | loss: 68.0160400CurrentTrain: epoch  6, batch   116 | loss: 66.2264159CurrentTrain: epoch  6, batch   117 | loss: 68.1112387CurrentTrain: epoch  6, batch   118 | loss: 103.5460619CurrentTrain: epoch  6, batch   119 | loss: 49.0210580CurrentTrain: epoch  6, batch   120 | loss: 45.4499917CurrentTrain: epoch  6, batch   121 | loss: 47.7975793CurrentTrain: epoch  6, batch   122 | loss: 77.2990799CurrentTrain: epoch  6, batch   123 | loss: 29.9558040CurrentTrain: epoch  6, batch   124 | loss: 36.0208212CurrentTrain: epoch  6, batch   125 | loss: 50.9204577CurrentTrain: epoch  6, batch   126 | loss: 47.8615291CurrentTrain: epoch  6, batch   127 | loss: 47.6766549CurrentTrain: epoch  6, batch   128 | loss: 37.1487080CurrentTrain: epoch  6, batch   129 | loss: 35.5525529CurrentTrain: epoch  6, batch   130 | loss: 29.7123184CurrentTrain: epoch  6, batch   131 | loss: 28.1994986CurrentTrain: epoch  6, batch   132 | loss: 38.6321910CurrentTrain: epoch  6, batch   133 | loss: 46.0653362CurrentTrain: epoch  6, batch   134 | loss: 65.9810305CurrentTrain: epoch  6, batch   135 | loss: 103.5292925CurrentTrain: epoch  6, batch   136 | loss: 66.6386065CurrentTrain: epoch  6, batch   137 | loss: 24.0442203CurrentTrain: epoch  6, batch   138 | loss: 69.8516172CurrentTrain: epoch  6, batch   139 | loss: 28.7666731CurrentTrain: epoch  6, batch   140 | loss: 49.6764729CurrentTrain: epoch  6, batch   141 | loss: 29.9834588CurrentTrain: epoch  6, batch   142 | loss: 47.8688595CurrentTrain: epoch  6, batch   143 | loss: 20.3722578CurrentTrain: epoch  7, batch     0 | loss: 35.8915522CurrentTrain: epoch  7, batch     1 | loss: 35.6648031CurrentTrain: epoch  7, batch     2 | loss: 106.7860158CurrentTrain: epoch  7, batch     3 | loss: 28.5615116CurrentTrain: epoch  7, batch     4 | loss: 69.1503258CurrentTrain: epoch  7, batch     5 | loss: 34.7085035CurrentTrain: epoch  7, batch     6 | loss: 29.5517535CurrentTrain: epoch  7, batch     7 | loss: 103.5441711CurrentTrain: epoch  7, batch     8 | loss: 222.1982640CurrentTrain: epoch  7, batch     9 | loss: 66.1602752CurrentTrain: epoch  7, batch    10 | loss: 106.7737030CurrentTrain: epoch  7, batch    11 | loss: 37.5783554CurrentTrain: epoch  7, batch    12 | loss: 49.2779781CurrentTrain: epoch  7, batch    13 | loss: 66.4721849CurrentTrain: epoch  7, batch    14 | loss: 37.4810490CurrentTrain: epoch  7, batch    15 | loss: 47.7024005CurrentTrain: epoch  7, batch    16 | loss: 47.4943688CurrentTrain: epoch  7, batch    17 | loss: 47.8193637CurrentTrain: epoch  7, batch    18 | loss: 26.7769707CurrentTrain: epoch  7, batch    19 | loss: 107.0009274CurrentTrain: epoch  7, batch    20 | loss: 66.4218578CurrentTrain: epoch  7, batch    21 | loss: 47.5115706CurrentTrain: epoch  7, batch    22 | loss: 68.3295222CurrentTrain: epoch  7, batch    23 | loss: 66.2813779CurrentTrain: epoch  7, batch    24 | loss: 68.5262037CurrentTrain: epoch  7, batch    25 | loss: 37.5373634CurrentTrain: epoch  7, batch    26 | loss: 49.2118476CurrentTrain: epoch  7, batch    27 | loss: 36.6825441CurrentTrain: epoch  7, batch    28 | loss: 48.2009025CurrentTrain: epoch  7, batch    29 | loss: 38.0238998CurrentTrain: epoch  7, batch    30 | loss: 106.8050891CurrentTrain: epoch  7, batch    31 | loss: 47.5387081CurrentTrain: epoch  7, batch    32 | loss: 49.1319168CurrentTrain: epoch  7, batch    33 | loss: 44.4568730CurrentTrain: epoch  7, batch    34 | loss: 36.3434264CurrentTrain: epoch  7, batch    35 | loss: 68.2426778CurrentTrain: epoch  7, batch    36 | loss: 46.0912126CurrentTrain: epoch  7, batch    37 | loss: 46.0293805CurrentTrain: epoch  7, batch    38 | loss: 27.9773077CurrentTrain: epoch  7, batch    39 | loss: 45.8042886CurrentTrain: epoch  7, batch    40 | loss: 38.0848863CurrentTrain: epoch  7, batch    41 | loss: 46.0300749CurrentTrain: epoch  7, batch    42 | loss: 49.0204331CurrentTrain: epoch  7, batch    43 | loss: 48.9684501CurrentTrain: epoch  7, batch    44 | loss: 64.7261083CurrentTrain: epoch  7, batch    45 | loss: 68.0928888CurrentTrain: epoch  7, batch    46 | loss: 36.7385756CurrentTrain: epoch  7, batch    47 | loss: 72.6033866CurrentTrain: epoch  7, batch    48 | loss: 35.2867474CurrentTrain: epoch  7, batch    49 | loss: 35.3842068CurrentTrain: epoch  7, batch    50 | loss: 64.5624619CurrentTrain: epoch  7, batch    51 | loss: 50.5389630CurrentTrain: epoch  7, batch    52 | loss: 35.4674973CurrentTrain: epoch  7, batch    53 | loss: 34.5818672CurrentTrain: epoch  7, batch    54 | loss: 32.1629896CurrentTrain: epoch  7, batch    55 | loss: 22.9912934CurrentTrain: epoch  7, batch    56 | loss: 36.8522624CurrentTrain: epoch  7, batch    57 | loss: 48.7509122CurrentTrain: epoch  7, batch    58 | loss: 64.1573901CurrentTrain: epoch  7, batch    59 | loss: 45.2584736CurrentTrain: epoch  7, batch    60 | loss: 46.9556320CurrentTrain: epoch  7, batch    61 | loss: 66.1645260CurrentTrain: epoch  7, batch    62 | loss: 49.8693982CurrentTrain: epoch  7, batch    63 | loss: 46.2324566CurrentTrain: epoch  7, batch    64 | loss: 27.6549148CurrentTrain: epoch  7, batch    65 | loss: 101.3354111CurrentTrain: epoch  7, batch    66 | loss: 68.3082022CurrentTrain: epoch  7, batch    67 | loss: 35.9612644CurrentTrain: epoch  7, batch    68 | loss: 103.4895686CurrentTrain: epoch  7, batch    69 | loss: 68.2017914CurrentTrain: epoch  7, batch    70 | loss: 35.0894559CurrentTrain: epoch  7, batch    71 | loss: 49.2080250CurrentTrain: epoch  7, batch    72 | loss: 50.2828363CurrentTrain: epoch  7, batch    73 | loss: 69.2770376CurrentTrain: epoch  7, batch    74 | loss: 38.3402368CurrentTrain: epoch  7, batch    75 | loss: 50.1815608CurrentTrain: epoch  7, batch    76 | loss: 28.0291936CurrentTrain: epoch  7, batch    77 | loss: 48.9711752CurrentTrain: epoch  7, batch    78 | loss: 48.9586386CurrentTrain: epoch  7, batch    79 | loss: 66.1084234CurrentTrain: epoch  7, batch    80 | loss: 49.9977195CurrentTrain: epoch  7, batch    81 | loss: 49.6129592CurrentTrain: epoch  7, batch    82 | loss: 49.0288870CurrentTrain: epoch  7, batch    83 | loss: 47.6520515CurrentTrain: epoch  7, batch    84 | loss: 48.9364279CurrentTrain: epoch  7, batch    85 | loss: 36.2633999CurrentTrain: epoch  7, batch    86 | loss: 106.8739524CurrentTrain: epoch  7, batch    87 | loss: 67.3772575CurrentTrain: epoch  7, batch    88 | loss: 37.8562366CurrentTrain: epoch  7, batch    89 | loss: 29.9551685CurrentTrain: epoch  7, batch    90 | loss: 48.9381090CurrentTrain: epoch  7, batch    91 | loss: 26.5760042CurrentTrain: epoch  7, batch    92 | loss: 66.2703379CurrentTrain: epoch  7, batch    93 | loss: 46.1587547CurrentTrain: epoch  7, batch    94 | loss: 35.7448453CurrentTrain: epoch  7, batch    95 | loss: 66.1589912CurrentTrain: epoch  7, batch    96 | loss: 30.7862582CurrentTrain: epoch  7, batch    97 | loss: 70.5161775CurrentTrain: epoch  7, batch    98 | loss: 51.2870285CurrentTrain: epoch  7, batch    99 | loss: 47.5955737CurrentTrain: epoch  7, batch   100 | loss: 35.6192693CurrentTrain: epoch  7, batch   101 | loss: 49.1813077CurrentTrain: epoch  7, batch   102 | loss: 49.9181473CurrentTrain: epoch  7, batch   103 | loss: 36.5381427CurrentTrain: epoch  7, batch   104 | loss: 29.9150893CurrentTrain: epoch  7, batch   105 | loss: 66.0866409CurrentTrain: epoch  7, batch   106 | loss: 47.4777538CurrentTrain: epoch  7, batch   107 | loss: 48.7266524CurrentTrain: epoch  7, batch   108 | loss: 68.1435006CurrentTrain: epoch  7, batch   109 | loss: 69.0925012CurrentTrain: epoch  7, batch   110 | loss: 47.4825680CurrentTrain: epoch  7, batch   111 | loss: 28.6411992CurrentTrain: epoch  7, batch   112 | loss: 49.0149409CurrentTrain: epoch  7, batch   113 | loss: 48.2175413CurrentTrain: epoch  7, batch   114 | loss: 22.9493844CurrentTrain: epoch  7, batch   115 | loss: 33.1073829CurrentTrain: epoch  7, batch   116 | loss: 35.4509392CurrentTrain: epoch  7, batch   117 | loss: 47.4674513CurrentTrain: epoch  7, batch   118 | loss: 47.8760516CurrentTrain: epoch  7, batch   119 | loss: 66.3645486CurrentTrain: epoch  7, batch   120 | loss: 30.4211056CurrentTrain: epoch  7, batch   121 | loss: 48.0765354CurrentTrain: epoch  7, batch   122 | loss: 46.3597367CurrentTrain: epoch  7, batch   123 | loss: 43.0657179CurrentTrain: epoch  7, batch   124 | loss: 68.2792379CurrentTrain: epoch  7, batch   125 | loss: 107.1059121CurrentTrain: epoch  7, batch   126 | loss: 69.0923847CurrentTrain: epoch  7, batch   127 | loss: 49.0161206CurrentTrain: epoch  7, batch   128 | loss: 35.8783930CurrentTrain: epoch  7, batch   129 | loss: 69.7018684CurrentTrain: epoch  7, batch   130 | loss: 103.5249569CurrentTrain: epoch  7, batch   131 | loss: 30.7802924CurrentTrain: epoch  7, batch   132 | loss: 47.4528769CurrentTrain: epoch  7, batch   133 | loss: 66.1956740CurrentTrain: epoch  7, batch   134 | loss: 48.0518125CurrentTrain: epoch  7, batch   135 | loss: 65.2694682CurrentTrain: epoch  7, batch   136 | loss: 47.8616296CurrentTrain: epoch  7, batch   137 | loss: 68.0643615CurrentTrain: epoch  7, batch   138 | loss: 49.9157165CurrentTrain: epoch  7, batch   139 | loss: 36.7294491CurrentTrain: epoch  7, batch   140 | loss: 34.6323817CurrentTrain: epoch  7, batch   141 | loss: 47.7151819CurrentTrain: epoch  7, batch   142 | loss: 48.9401855CurrentTrain: epoch  7, batch   143 | loss: 50.9164286CurrentTrain: epoch  8, batch     0 | loss: 29.5806300CurrentTrain: epoch  8, batch     1 | loss: 101.1071618CurrentTrain: epoch  8, batch     2 | loss: 67.4125834CurrentTrain: epoch  8, batch     3 | loss: 106.8019855CurrentTrain: epoch  8, batch     4 | loss: 35.4036614CurrentTrain: epoch  8, batch     5 | loss: 47.4542274CurrentTrain: epoch  8, batch     6 | loss: 52.0830028CurrentTrain: epoch  8, batch     7 | loss: 47.8898094CurrentTrain: epoch  8, batch     8 | loss: 36.3474899CurrentTrain: epoch  8, batch     9 | loss: 47.0746932CurrentTrain: epoch  8, batch    10 | loss: 29.1749607CurrentTrain: epoch  8, batch    11 | loss: 222.2008611CurrentTrain: epoch  8, batch    12 | loss: 47.4471262CurrentTrain: epoch  8, batch    13 | loss: 67.6309096CurrentTrain: epoch  8, batch    14 | loss: 36.3894420CurrentTrain: epoch  8, batch    15 | loss: 36.6822562CurrentTrain: epoch  8, batch    16 | loss: 27.1795381CurrentTrain: epoch  8, batch    17 | loss: 34.5820184CurrentTrain: epoch  8, batch    18 | loss: 47.6849288CurrentTrain: epoch  8, batch    19 | loss: 46.2936135CurrentTrain: epoch  8, batch    20 | loss: 69.2534646CurrentTrain: epoch  8, batch    21 | loss: 29.8223538CurrentTrain: epoch  8, batch    22 | loss: 48.9176885CurrentTrain: epoch  8, batch    23 | loss: 37.8902024CurrentTrain: epoch  8, batch    24 | loss: 112.7741854CurrentTrain: epoch  8, batch    25 | loss: 107.4107404CurrentTrain: epoch  8, batch    26 | loss: 48.3835730CurrentTrain: epoch  8, batch    27 | loss: 64.5369306CurrentTrain: epoch  8, batch    28 | loss: 49.2891172CurrentTrain: epoch  8, batch    29 | loss: 69.0503755CurrentTrain: epoch  8, batch    30 | loss: 66.1195021CurrentTrain: epoch  8, batch    31 | loss: 68.1006458CurrentTrain: epoch  8, batch    32 | loss: 49.4162652CurrentTrain: epoch  8, batch    33 | loss: 47.6881645CurrentTrain: epoch  8, batch    34 | loss: 68.2160621CurrentTrain: epoch  8, batch    35 | loss: 36.5116939CurrentTrain: epoch  8, batch    36 | loss: 28.9929732CurrentTrain: epoch  8, batch    37 | loss: 37.4848934CurrentTrain: epoch  8, batch    38 | loss: 103.6552411CurrentTrain: epoch  8, batch    39 | loss: 49.7762821CurrentTrain: epoch  8, batch    40 | loss: 48.1557419CurrentTrain: epoch  8, batch    41 | loss: 37.1669014CurrentTrain: epoch  8, batch    42 | loss: 35.6818682CurrentTrain: epoch  8, batch    43 | loss: 68.2209470CurrentTrain: epoch  8, batch    44 | loss: 28.8821407CurrentTrain: epoch  8, batch    45 | loss: 48.9454599CurrentTrain: epoch  8, batch    46 | loss: 29.0962835CurrentTrain: epoch  8, batch    47 | loss: 28.8278357CurrentTrain: epoch  8, batch    48 | loss: 47.5971706CurrentTrain: epoch  8, batch    49 | loss: 46.2149477CurrentTrain: epoch  8, batch    50 | loss: 48.0229408CurrentTrain: epoch  8, batch    51 | loss: 35.4079947CurrentTrain: epoch  8, batch    52 | loss: 49.0010668CurrentTrain: epoch  8, batch    53 | loss: 38.2000065CurrentTrain: epoch  8, batch    54 | loss: 49.0221921CurrentTrain: epoch  8, batch    55 | loss: 28.6094822CurrentTrain: epoch  8, batch    56 | loss: 34.9844093CurrentTrain: epoch  8, batch    57 | loss: 39.8875853CurrentTrain: epoch  8, batch    58 | loss: 46.0144614CurrentTrain: epoch  8, batch    59 | loss: 49.0081165CurrentTrain: epoch  8, batch    60 | loss: 47.8781738CurrentTrain: epoch  8, batch    61 | loss: 106.7736810CurrentTrain: epoch  8, batch    62 | loss: 37.4356467CurrentTrain: epoch  8, batch    63 | loss: 108.0015505CurrentTrain: epoch  8, batch    64 | loss: 36.7940459CurrentTrain: epoch  8, batch    65 | loss: 36.5961123CurrentTrain: epoch  8, batch    66 | loss: 36.7366333CurrentTrain: epoch  8, batch    67 | loss: 37.5743131CurrentTrain: epoch  8, batch    68 | loss: 68.7950862CurrentTrain: epoch  8, batch    69 | loss: 68.7147981CurrentTrain: epoch  8, batch    70 | loss: 46.3246532CurrentTrain: epoch  8, batch    71 | loss: 47.5481209CurrentTrain: epoch  8, batch    72 | loss: 38.4353884CurrentTrain: epoch  8, batch    73 | loss: 68.0826159CurrentTrain: epoch  8, batch    74 | loss: 36.2285055CurrentTrain: epoch  8, batch    75 | loss: 68.1787489CurrentTrain: epoch  8, batch    76 | loss: 46.3188886CurrentTrain: epoch  8, batch    77 | loss: 49.3369166CurrentTrain: epoch  8, batch    78 | loss: 64.8221141CurrentTrain: epoch  8, batch    79 | loss: 48.9012378CurrentTrain: epoch  8, batch    80 | loss: 37.4824826CurrentTrain: epoch  8, batch    81 | loss: 37.4455559CurrentTrain: epoch  8, batch    82 | loss: 68.0980729CurrentTrain: epoch  8, batch    83 | loss: 39.4695301CurrentTrain: epoch  8, batch    84 | loss: 46.2887573CurrentTrain: epoch  8, batch    85 | loss: 35.9804552CurrentTrain: epoch  8, batch    86 | loss: 46.2846150CurrentTrain: epoch  8, batch    87 | loss: 47.8402746CurrentTrain: epoch  8, batch    88 | loss: 36.3278272CurrentTrain: epoch  8, batch    89 | loss: 66.1103585CurrentTrain: epoch  8, batch    90 | loss: 29.9324542CurrentTrain: epoch  8, batch    91 | loss: 37.7925616CurrentTrain: epoch  8, batch    92 | loss: 48.3738955CurrentTrain: epoch  8, batch    93 | loss: 38.9919590CurrentTrain: epoch  8, batch    94 | loss: 47.4446276CurrentTrain: epoch  8, batch    95 | loss: 36.6250670CurrentTrain: epoch  8, batch    96 | loss: 32.2430523CurrentTrain: epoch  8, batch    97 | loss: 68.1173390CurrentTrain: epoch  8, batch    98 | loss: 38.3610819CurrentTrain: epoch  8, batch    99 | loss: 37.9132742CurrentTrain: epoch  8, batch   100 | loss: 106.7082430CurrentTrain: epoch  8, batch   101 | loss: 66.5207572CurrentTrain: epoch  8, batch   102 | loss: 37.5229818CurrentTrain: epoch  8, batch   103 | loss: 66.6017082CurrentTrain: epoch  8, batch   104 | loss: 34.7038523CurrentTrain: epoch  8, batch   105 | loss: 47.6965102CurrentTrain: epoch  8, batch   106 | loss: 107.2341872CurrentTrain: epoch  8, batch   107 | loss: 28.9707498CurrentTrain: epoch  8, batch   108 | loss: 30.3491880CurrentTrain: epoch  8, batch   109 | loss: 71.8306185CurrentTrain: epoch  8, batch   110 | loss: 27.9817173CurrentTrain: epoch  8, batch   111 | loss: 42.5016064CurrentTrain: epoch  8, batch   112 | loss: 34.8908494CurrentTrain: epoch  8, batch   113 | loss: 106.7847307CurrentTrain: epoch  8, batch   114 | loss: 36.4226944CurrentTrain: epoch  8, batch   115 | loss: 33.7781085CurrentTrain: epoch  8, batch   116 | loss: 68.1032961CurrentTrain: epoch  8, batch   117 | loss: 101.0494440CurrentTrain: epoch  8, batch   118 | loss: 38.1829667CurrentTrain: epoch  8, batch   119 | loss: 35.8547357CurrentTrain: epoch  8, batch   120 | loss: 46.8746897CurrentTrain: epoch  8, batch   121 | loss: 48.9089057CurrentTrain: epoch  8, batch   122 | loss: 106.6883305CurrentTrain: epoch  8, batch   123 | loss: 23.3402960CurrentTrain: epoch  8, batch   124 | loss: 45.6119083CurrentTrain: epoch  8, batch   125 | loss: 27.4504812CurrentTrain: epoch  8, batch   126 | loss: 106.7077453CurrentTrain: epoch  8, batch   127 | loss: 36.2621755CurrentTrain: epoch  8, batch   128 | loss: 68.1923512CurrentTrain: epoch  8, batch   129 | loss: 66.6605144CurrentTrain: epoch  8, batch   130 | loss: 62.5929862CurrentTrain: epoch  8, batch   131 | loss: 103.6825001CurrentTrain: epoch  8, batch   132 | loss: 47.6338696CurrentTrain: epoch  8, batch   133 | loss: 37.9523857CurrentTrain: epoch  8, batch   134 | loss: 48.9228614CurrentTrain: epoch  8, batch   135 | loss: 64.5667661CurrentTrain: epoch  8, batch   136 | loss: 37.4473248CurrentTrain: epoch  8, batch   137 | loss: 46.1599074CurrentTrain: epoch  8, batch   138 | loss: 47.6746286CurrentTrain: epoch  8, batch   139 | loss: 49.5885083CurrentTrain: epoch  8, batch   140 | loss: 49.4055752CurrentTrain: epoch  8, batch   141 | loss: 68.1975365CurrentTrain: epoch  8, batch   142 | loss: 47.9034832CurrentTrain: epoch  8, batch   143 | loss: 49.4805498CurrentTrain: epoch  9, batch     0 | loss: 34.3595954CurrentTrain: epoch  9, batch     1 | loss: 37.4392071CurrentTrain: epoch  9, batch     2 | loss: 28.9854096CurrentTrain: epoch  9, batch     3 | loss: 48.8895679CurrentTrain: epoch  9, batch     4 | loss: 47.7724462CurrentTrain: epoch  9, batch     5 | loss: 47.9532452CurrentTrain: epoch  9, batch     6 | loss: 103.5511783CurrentTrain: epoch  9, batch     7 | loss: 66.1091607CurrentTrain: epoch  9, batch     8 | loss: 28.2264142CurrentTrain: epoch  9, batch     9 | loss: 68.1175701CurrentTrain: epoch  9, batch    10 | loss: 45.3407597CurrentTrain: epoch  9, batch    11 | loss: 47.4641150CurrentTrain: epoch  9, batch    12 | loss: 68.1418981CurrentTrain: epoch  9, batch    13 | loss: 47.6480723CurrentTrain: epoch  9, batch    14 | loss: 47.5486546CurrentTrain: epoch  9, batch    15 | loss: 64.2404120CurrentTrain: epoch  9, batch    16 | loss: 66.1755174CurrentTrain: epoch  9, batch    17 | loss: 34.4182750CurrentTrain: epoch  9, batch    18 | loss: 106.8337379CurrentTrain: epoch  9, batch    19 | loss: 222.2098119CurrentTrain: epoch  9, batch    20 | loss: 106.7955786CurrentTrain: epoch  9, batch    21 | loss: 34.4107724CurrentTrain: epoch  9, batch    22 | loss: 50.8692607CurrentTrain: epoch  9, batch    23 | loss: 103.4978259CurrentTrain: epoch  9, batch    24 | loss: 68.1297810CurrentTrain: epoch  9, batch    25 | loss: 68.0610445CurrentTrain: epoch  9, batch    26 | loss: 28.1914749CurrentTrain: epoch  9, batch    27 | loss: 36.1085826CurrentTrain: epoch  9, batch    28 | loss: 98.9830889CurrentTrain: epoch  9, batch    29 | loss: 66.0904272CurrentTrain: epoch  9, batch    30 | loss: 106.7859067CurrentTrain: epoch  9, batch    31 | loss: 23.9187599CurrentTrain: epoch  9, batch    32 | loss: 68.1033267CurrentTrain: epoch  9, batch    33 | loss: 68.0884113CurrentTrain: epoch  9, batch    34 | loss: 48.9412614CurrentTrain: epoch  9, batch    35 | loss: 36.2465865CurrentTrain: epoch  9, batch    36 | loss: 68.3459476CurrentTrain: epoch  9, batch    37 | loss: 46.1726360CurrentTrain: epoch  9, batch    38 | loss: 106.8306371CurrentTrain: epoch  9, batch    39 | loss: 101.0499630CurrentTrain: epoch  9, batch    40 | loss: 47.5355703CurrentTrain: epoch  9, batch    41 | loss: 35.8879873CurrentTrain: epoch  9, batch    42 | loss: 62.9924515CurrentTrain: epoch  9, batch    43 | loss: 36.2951570CurrentTrain: epoch  9, batch    44 | loss: 106.7926536CurrentTrain: epoch  9, batch    45 | loss: 44.7852438CurrentTrain: epoch  9, batch    46 | loss: 107.0831016CurrentTrain: epoch  9, batch    47 | loss: 47.4199992CurrentTrain: epoch  9, batch    48 | loss: 49.1740052CurrentTrain: epoch  9, batch    49 | loss: 68.6394579CurrentTrain: epoch  9, batch    50 | loss: 46.1894214CurrentTrain: epoch  9, batch    51 | loss: 48.9288908CurrentTrain: epoch  9, batch    52 | loss: 36.2991403CurrentTrain: epoch  9, batch    53 | loss: 36.2262463CurrentTrain: epoch  9, batch    54 | loss: 48.8807317CurrentTrain: epoch  9, batch    55 | loss: 46.6824539CurrentTrain: epoch  9, batch    56 | loss: 47.5842529CurrentTrain: epoch  9, batch    57 | loss: 47.3991641CurrentTrain: epoch  9, batch    58 | loss: 64.1261423CurrentTrain: epoch  9, batch    59 | loss: 37.3806391CurrentTrain: epoch  9, batch    60 | loss: 34.7103228CurrentTrain: epoch  9, batch    61 | loss: 36.3185982CurrentTrain: epoch  9, batch    62 | loss: 68.2296277CurrentTrain: epoch  9, batch    63 | loss: 47.5365228CurrentTrain: epoch  9, batch    64 | loss: 66.2285588CurrentTrain: epoch  9, batch    65 | loss: 36.2892676CurrentTrain: epoch  9, batch    66 | loss: 36.0492113CurrentTrain: epoch  9, batch    67 | loss: 36.3085797CurrentTrain: epoch  9, batch    68 | loss: 34.6136555CurrentTrain: epoch  9, batch    69 | loss: 68.1923728CurrentTrain: epoch  9, batch    70 | loss: 66.1569920CurrentTrain: epoch  9, batch    71 | loss: 50.2506891CurrentTrain: epoch  9, batch    72 | loss: 36.7812306CurrentTrain: epoch  9, batch    73 | loss: 66.1650501CurrentTrain: epoch  9, batch    74 | loss: 36.2332222CurrentTrain: epoch  9, batch    75 | loss: 47.5024087CurrentTrain: epoch  9, batch    76 | loss: 28.5114280CurrentTrain: epoch  9, batch    77 | loss: 48.9126526CurrentTrain: epoch  9, batch    78 | loss: 66.0981432CurrentTrain: epoch  9, batch    79 | loss: 48.0544940CurrentTrain: epoch  9, batch    80 | loss: 36.8311431CurrentTrain: epoch  9, batch    81 | loss: 47.4639410CurrentTrain: epoch  9, batch    82 | loss: 48.9298900CurrentTrain: epoch  9, batch    83 | loss: 66.0697739CurrentTrain: epoch  9, batch    84 | loss: 36.3410523CurrentTrain: epoch  9, batch    85 | loss: 28.6244549CurrentTrain: epoch  9, batch    86 | loss: 64.2276134CurrentTrain: epoch  9, batch    87 | loss: 68.3364001CurrentTrain: epoch  9, batch    88 | loss: 107.0281662CurrentTrain: epoch  9, batch    89 | loss: 47.5907278CurrentTrain: epoch  9, batch    90 | loss: 24.8176776CurrentTrain: epoch  9, batch    91 | loss: 36.2972080CurrentTrain: epoch  9, batch    92 | loss: 31.0939578CurrentTrain: epoch  9, batch    93 | loss: 50.1557147CurrentTrain: epoch  9, batch    94 | loss: 27.5641171CurrentTrain: epoch  9, batch    95 | loss: 36.3093028CurrentTrain: epoch  9, batch    96 | loss: 35.3488434CurrentTrain: epoch  9, batch    97 | loss: 66.1119943CurrentTrain: epoch  9, batch    98 | loss: 34.7139641CurrentTrain: epoch  9, batch    99 | loss: 37.0517960CurrentTrain: epoch  9, batch   100 | loss: 36.8517888CurrentTrain: epoch  9, batch   101 | loss: 46.2449258CurrentTrain: epoch  9, batch   102 | loss: 36.5355872CurrentTrain: epoch  9, batch   103 | loss: 28.9844903CurrentTrain: epoch  9, batch   104 | loss: 44.8027200CurrentTrain: epoch  9, batch   105 | loss: 47.4560293CurrentTrain: epoch  9, batch   106 | loss: 49.1088462CurrentTrain: epoch  9, batch   107 | loss: 53.1115437CurrentTrain: epoch  9, batch   108 | loss: 47.6147415CurrentTrain: epoch  9, batch   109 | loss: 28.2579520CurrentTrain: epoch  9, batch   110 | loss: 36.9253205CurrentTrain: epoch  9, batch   111 | loss: 66.2121262CurrentTrain: epoch  9, batch   112 | loss: 64.3691416CurrentTrain: epoch  9, batch   113 | loss: 68.3901513CurrentTrain: epoch  9, batch   114 | loss: 44.8947707CurrentTrain: epoch  9, batch   115 | loss: 106.7563237CurrentTrain: epoch  9, batch   116 | loss: 29.0096836CurrentTrain: epoch  9, batch   117 | loss: 46.7332304CurrentTrain: epoch  9, batch   118 | loss: 46.2062626CurrentTrain: epoch  9, batch   119 | loss: 51.0874446CurrentTrain: epoch  9, batch   120 | loss: 37.1661700CurrentTrain: epoch  9, batch   121 | loss: 70.7870656CurrentTrain: epoch  9, batch   122 | loss: 34.2373728CurrentTrain: epoch  9, batch   123 | loss: 36.4682063CurrentTrain: epoch  9, batch   124 | loss: 66.0634643CurrentTrain: epoch  9, batch   125 | loss: 47.4324375CurrentTrain: epoch  9, batch   126 | loss: 48.9760411CurrentTrain: epoch  9, batch   127 | loss: 48.8911264CurrentTrain: epoch  9, batch   128 | loss: 222.0728601CurrentTrain: epoch  9, batch   129 | loss: 36.6857525CurrentTrain: epoch  9, batch   130 | loss: 106.6878590CurrentTrain: epoch  9, batch   131 | loss: 29.2641288CurrentTrain: epoch  9, batch   132 | loss: 49.4847943CurrentTrain: epoch  9, batch   133 | loss: 31.5642765CurrentTrain: epoch  9, batch   134 | loss: 222.0320297CurrentTrain: epoch  9, batch   135 | loss: 46.2391112CurrentTrain: epoch  9, batch   136 | loss: 47.5382535CurrentTrain: epoch  9, batch   137 | loss: 104.1864979CurrentTrain: epoch  9, batch   138 | loss: 53.7152443CurrentTrain: epoch  9, batch   139 | loss: 47.5729238CurrentTrain: epoch  9, batch   140 | loss: 46.0622136CurrentTrain: epoch  9, batch   141 | loss: 47.5794722CurrentTrain: epoch  9, batch   142 | loss: 48.8735884CurrentTrain: epoch  9, batch   143 | loss: 19.7032764

F1 score per class: {32: 0.44878048780487806, 6: 0.649746192893401, 19: 0.23076923076923078, 24: 0.7128712871287128, 26: 0.8795811518324608, 29: 0.8115942028985508}
Micro-average F1 score: 0.6867704280155642
Weighted-average F1 score: 0.6893898462555952
F1 score per class: {32: 0.5124555160142349, 6: 0.6419753086419753, 19: 0.13636363636363635, 24: 0.6965174129353234, 26: 0.8985507246376812, 29: 0.8055555555555556}
Micro-average F1 score: 0.6761744966442953
Weighted-average F1 score: 0.6647477785860201
F1 score per class: {32: 0.5093632958801498, 6: 0.6419753086419753, 19: 0.13636363636363635, 24: 0.6965174129353234, 26: 0.8985507246376812, 29: 0.8093023255813954}
Micro-average F1 score: 0.6779949022939677
Weighted-average F1 score: 0.6676396779632808

F1 score per class: {32: 0.44878048780487806, 6: 0.649746192893401, 19: 0.23076923076923078, 24: 0.7128712871287128, 26: 0.8795811518324608, 29: 0.8115942028985508}
Micro-average F1 score: 0.6867704280155642
Weighted-average F1 score: 0.6893898462555952
F1 score per class: {32: 0.5124555160142349, 6: 0.6419753086419753, 19: 0.13636363636363635, 24: 0.6965174129353234, 26: 0.8985507246376812, 29: 0.8055555555555556}
Micro-average F1 score: 0.6761744966442953
Weighted-average F1 score: 0.6647477785860201
F1 score per class: {32: 0.5093632958801498, 6: 0.6419753086419753, 19: 0.13636363636363635, 24: 0.6965174129353234, 26: 0.8985507246376812, 29: 0.8093023255813954}
Micro-average F1 score: 0.6779949022939677
Weighted-average F1 score: 0.6676396779632808
cur_acc:  ['0.6868']
his_acc:  ['0.6868']
cur_acc des:  ['0.6762']
his_acc des:  ['0.6762']
cur_acc rrf:  ['0.6780']
his_acc rrf:  ['0.6780']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 66.0515652CurrentTrain: epoch  0, batch     1 | loss: 60.4157058CurrentTrain: epoch  0, batch     2 | loss: 60.1864837CurrentTrain: epoch  0, batch     3 | loss: 79.4557570CurrentTrain: epoch  0, batch     4 | loss: 57.7673849CurrentTrain: epoch  0, batch     5 | loss: 27.1874272CurrentTrain: epoch  1, batch     0 | loss: 112.1967084CurrentTrain: epoch  1, batch     1 | loss: 55.6278252CurrentTrain: epoch  1, batch     2 | loss: 56.0716176CurrentTrain: epoch  1, batch     3 | loss: 74.6259838CurrentTrain: epoch  1, batch     4 | loss: 34.6259401CurrentTrain: epoch  1, batch     5 | loss: 39.2707350CurrentTrain: epoch  2, batch     0 | loss: 43.1722914CurrentTrain: epoch  2, batch     1 | loss: 54.8534811CurrentTrain: epoch  2, batch     2 | loss: 42.2096869CurrentTrain: epoch  2, batch     3 | loss: 72.1841552CurrentTrain: epoch  2, batch     4 | loss: 54.0810631CurrentTrain: epoch  2, batch     5 | loss: 26.8667884CurrentTrain: epoch  3, batch     0 | loss: 52.6259868CurrentTrain: epoch  3, batch     1 | loss: 43.5223965CurrentTrain: epoch  3, batch     2 | loss: 38.6752147CurrentTrain: epoch  3, batch     3 | loss: 51.7654881CurrentTrain: epoch  3, batch     4 | loss: 71.4339410CurrentTrain: epoch  3, batch     5 | loss: 38.1954242CurrentTrain: epoch  4, batch     0 | loss: 33.1786282CurrentTrain: epoch  4, batch     1 | loss: 50.7208535CurrentTrain: epoch  4, batch     2 | loss: 68.2470641CurrentTrain: epoch  4, batch     3 | loss: 40.2604723CurrentTrain: epoch  4, batch     4 | loss: 50.7554847CurrentTrain: epoch  4, batch     5 | loss: 40.1418779CurrentTrain: epoch  5, batch     0 | loss: 66.9058603CurrentTrain: epoch  5, batch     1 | loss: 49.1288092CurrentTrain: epoch  5, batch     2 | loss: 41.2506807CurrentTrain: epoch  5, batch     3 | loss: 51.4917509CurrentTrain: epoch  5, batch     4 | loss: 49.9774327CurrentTrain: epoch  5, batch     5 | loss: 56.3619204CurrentTrain: epoch  6, batch     0 | loss: 33.1369664CurrentTrain: epoch  6, batch     1 | loss: 71.5735372CurrentTrain: epoch  6, batch     2 | loss: 68.5574965CurrentTrain: epoch  6, batch     3 | loss: 39.5853754CurrentTrain: epoch  6, batch     4 | loss: 36.1039165CurrentTrain: epoch  6, batch     5 | loss: 25.4402890CurrentTrain: epoch  7, batch     0 | loss: 46.2107264CurrentTrain: epoch  7, batch     1 | loss: 37.8326123CurrentTrain: epoch  7, batch     2 | loss: 66.5162693CurrentTrain: epoch  7, batch     3 | loss: 30.8307359CurrentTrain: epoch  7, batch     4 | loss: 40.5398549CurrentTrain: epoch  7, batch     5 | loss: 56.4885139CurrentTrain: epoch  8, batch     0 | loss: 47.0432837CurrentTrain: epoch  8, batch     1 | loss: 38.5047123CurrentTrain: epoch  8, batch     2 | loss: 47.7831656CurrentTrain: epoch  8, batch     3 | loss: 67.1663935CurrentTrain: epoch  8, batch     4 | loss: 36.8221762CurrentTrain: epoch  8, batch     5 | loss: 34.7542911CurrentTrain: epoch  9, batch     0 | loss: 38.3533325CurrentTrain: epoch  9, batch     1 | loss: 64.5985597CurrentTrain: epoch  9, batch     2 | loss: 107.3005616CurrentTrain: epoch  9, batch     3 | loss: 49.2510460CurrentTrain: epoch  9, batch     4 | loss: 47.6093796CurrentTrain: epoch  9, batch     5 | loss: 12.3247728
MemoryTrain:  epoch  0, batch     0 | loss: 0.8195789MemoryTrain:  epoch  1, batch     0 | loss: 0.5268452MemoryTrain:  epoch  2, batch     0 | loss: 0.3454198MemoryTrain:  epoch  3, batch     0 | loss: 0.2755981MemoryTrain:  epoch  4, batch     0 | loss: 0.2020854MemoryTrain:  epoch  5, batch     0 | loss: 0.1433765MemoryTrain:  epoch  6, batch     0 | loss: 0.1476167MemoryTrain:  epoch  7, batch     0 | loss: 0.1109721MemoryTrain:  epoch  8, batch     0 | loss: 0.1126502MemoryTrain:  epoch  9, batch     0 | loss: 0.0744700

F1 score per class: {32: 0.0, 35: 0.9473684210526315, 37: 0.0, 38: 0.0, 6: 0.42424242424242425, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6949152542372882, 26: 0.35714285714285715, 29: 0.3917525773195876}
Micro-average F1 score: 0.4307692307692308
Weighted-average F1 score: 0.3804227880678978
F1 score per class: {32: 0.0, 35: 0.6666666666666666, 37: 0.0, 38: 0.0, 6: 0.88, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6622516556291391, 26: 0.40437158469945356, 29: 0.36363636363636365}
Micro-average F1 score: 0.4423076923076923
Weighted-average F1 score: 0.3820766330786753
F1 score per class: {32: 0.0, 35: 0.7368421052631579, 37: 0.0, 38: 0.0, 6: 0.7529411764705882, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6578947368421053, 26: 0.41530054644808745, 29: 0.3609022556390977}
Micro-average F1 score: 0.4332855093256815
Weighted-average F1 score: 0.37681998033703723

F1 score per class: {32: 0.5038759689922481, 35: 0.782608695652174, 37: 0.5727699530516432, 6: 0.21428571428571427, 38: 0.42424242424242425, 15: 0.7106598984771574, 19: 0.832579185520362, 24: 0.7671232876712328, 25: 0.5061728395061729, 26: 0.3125, 29: 0.2}
Micro-average F1 score: 0.5654512851165571
Weighted-average F1 score: 0.5486273793916668
F1 score per class: {32: 0.4899135446685879, 35: 0.48, 37: 0.4702702702702703, 6: 0.21621621621621623, 38: 0.88, 15: 0.6731707317073171, 19: 0.8083333333333333, 24: 0.7333333333333333, 25: 0.3597122302158273, 26: 0.28793774319066145, 29: 0.1605351170568562}
Micro-average F1 score: 0.4887063655030801
Weighted-average F1 score: 0.45333882002726406
F1 score per class: {32: 0.495114006514658, 35: 0.4827586206896552, 37: 0.536741214057508, 38: 0.208955223880597, 6: 0.7529411764705882, 15: 0.6764705882352942, 19: 0.8185654008438819, 24: 0.7364016736401674, 25: 0.36101083032490977, 26: 0.2846441947565543, 29: 0.14953271028037382}
Micro-average F1 score: 0.4876385336743393
Weighted-average F1 score: 0.4488333560773307
cur_acc:  ['0.6868', '0.4308']
his_acc:  ['0.6868', '0.5655']
cur_acc des:  ['0.6762', '0.4423']
his_acc des:  ['0.6762', '0.4887']
cur_acc rrf:  ['0.6780', '0.4333']
his_acc rrf:  ['0.6780', '0.4876']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 66.3266428CurrentTrain: epoch  0, batch     1 | loss: 76.2211431CurrentTrain: epoch  0, batch     2 | loss: 57.8174501CurrentTrain: epoch  0, batch     3 | loss: 56.5784192CurrentTrain: epoch  0, batch     4 | loss: 46.5163648CurrentTrain: epoch  0, batch     5 | loss: 25.8218988CurrentTrain: epoch  1, batch     0 | loss: 75.3339394CurrentTrain: epoch  1, batch     1 | loss: 56.9887804CurrentTrain: epoch  1, batch     2 | loss: 42.6831350CurrentTrain: epoch  1, batch     3 | loss: 42.6806491CurrentTrain: epoch  1, batch     4 | loss: 55.1232671CurrentTrain: epoch  1, batch     5 | loss: 45.7363084CurrentTrain: epoch  2, batch     0 | loss: 75.5832717CurrentTrain: epoch  2, batch     1 | loss: 39.0980217CurrentTrain: epoch  2, batch     2 | loss: 51.6596013CurrentTrain: epoch  2, batch     3 | loss: 70.6680724CurrentTrain: epoch  2, batch     4 | loss: 40.3406277CurrentTrain: epoch  2, batch     5 | loss: 20.0744472CurrentTrain: epoch  3, batch     0 | loss: 52.3018407CurrentTrain: epoch  3, batch     1 | loss: 41.1696532CurrentTrain: epoch  3, batch     2 | loss: 50.7490331CurrentTrain: epoch  3, batch     3 | loss: 69.8849658CurrentTrain: epoch  3, batch     4 | loss: 63.8742314CurrentTrain: epoch  3, batch     5 | loss: 21.4695589CurrentTrain: epoch  4, batch     0 | loss: 38.2429092CurrentTrain: epoch  4, batch     1 | loss: 49.4825972CurrentTrain: epoch  4, batch     2 | loss: 50.1041863CurrentTrain: epoch  4, batch     3 | loss: 50.5997819CurrentTrain: epoch  4, batch     4 | loss: 68.1917216CurrentTrain: epoch  4, batch     5 | loss: 30.1667338CurrentTrain: epoch  5, batch     0 | loss: 69.2669171CurrentTrain: epoch  5, batch     1 | loss: 48.6515329CurrentTrain: epoch  5, batch     2 | loss: 30.9707598CurrentTrain: epoch  5, batch     3 | loss: 29.2305883CurrentTrain: epoch  5, batch     4 | loss: 49.8733161CurrentTrain: epoch  5, batch     5 | loss: 44.8752511CurrentTrain: epoch  6, batch     0 | loss: 37.8452971CurrentTrain: epoch  6, batch     1 | loss: 36.5419594CurrentTrain: epoch  6, batch     2 | loss: 68.8087292CurrentTrain: epoch  6, batch     3 | loss: 64.0223206CurrentTrain: epoch  6, batch     4 | loss: 49.3474015CurrentTrain: epoch  6, batch     5 | loss: 27.8595104CurrentTrain: epoch  7, batch     0 | loss: 37.6048717CurrentTrain: epoch  7, batch     1 | loss: 48.4736374CurrentTrain: epoch  7, batch     2 | loss: 65.0155037CurrentTrain: epoch  7, batch     3 | loss: 46.3247458CurrentTrain: epoch  7, batch     4 | loss: 47.1343665CurrentTrain: epoch  7, batch     5 | loss: 44.7979348CurrentTrain: epoch  8, batch     0 | loss: 34.5639842CurrentTrain: epoch  8, batch     1 | loss: 63.7363109CurrentTrain: epoch  8, batch     2 | loss: 49.1840427CurrentTrain: epoch  8, batch     3 | loss: 46.9435023CurrentTrain: epoch  8, batch     4 | loss: 68.1976558CurrentTrain: epoch  8, batch     5 | loss: 26.1933460CurrentTrain: epoch  9, batch     0 | loss: 49.7758236CurrentTrain: epoch  9, batch     1 | loss: 44.9338703CurrentTrain: epoch  9, batch     2 | loss: 35.9152716CurrentTrain: epoch  9, batch     3 | loss: 34.6843702CurrentTrain: epoch  9, batch     4 | loss: 49.1194846CurrentTrain: epoch  9, batch     5 | loss: 96.8827015
MemoryTrain:  epoch  0, batch     0 | loss: 0.2887439MemoryTrain:  epoch  1, batch     0 | loss: 0.2631460MemoryTrain:  epoch  2, batch     0 | loss: 0.1924731MemoryTrain:  epoch  3, batch     0 | loss: 0.1425736MemoryTrain:  epoch  4, batch     0 | loss: 0.1139571MemoryTrain:  epoch  5, batch     0 | loss: 0.0969790MemoryTrain:  epoch  6, batch     0 | loss: 0.0770117MemoryTrain:  epoch  7, batch     0 | loss: 0.0685170MemoryTrain:  epoch  8, batch     0 | loss: 0.0563814MemoryTrain:  epoch  9, batch     0 | loss: 0.0586209

F1 score per class: {32: 0.0, 33: 0.4852941176470588, 35: 0.0, 36: 0.0, 37: 0.7, 38: 0.0, 6: 0.0, 8: 0.9444444444444444, 15: 0.0, 19: 0.3076923076923077, 20: 0.0, 26: 0.1111111111111111, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.43847874720357943
Weighted-average F1 score: 0.41772584864905127
F1 score per class: {32: 0.0, 33: 0.5514018691588785, 35: 0.0, 36: 0.0, 37: 0.6619718309859155, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.0, 19: 0.8571428571428571, 20: 0.0, 24: 0.42105263157894735, 25: 0.0, 26: 0.5116279069767442, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.42627013630731103
Weighted-average F1 score: 0.3626826990127492
F1 score per class: {32: 0.0, 33: 0.5566037735849056, 35: 0.0, 36: 0.0, 37: 0.6527777777777778, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.7659574468085106, 19: 0.0, 20: 0.4, 24: 0.0, 26: 0.49411764705882355, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.43645699614890887
Weighted-average F1 score: 0.3797447776357274

F1 score per class: {32: 0.39111111111111113, 33: 0.3473684210526316, 35: 0.6956521739130435, 36: 0.5358851674641149, 37: 0.3574468085106383, 6: 0.2608695652173913, 38: 0.375, 8: 0.6731707317073171, 15: 0.8317757009345794, 19: 0.9444444444444444, 20: 0.7605633802816901, 24: 0.21052631578947367, 25: 0.5390070921985816, 26: 0.1111111111111111, 29: 0.3157894736842105, 30: 0.21052631578947367}
Micro-average F1 score: 0.518406454866364
Weighted-average F1 score: 0.5326555788533303
F1 score per class: {32: 0.42524916943521596, 33: 0.3172043010752688, 35: 0.5, 36: 0.45714285714285713, 37: 0.27485380116959063, 6: 0.2033898305084746, 38: 0.4722222222222222, 8: 0.6363636363636364, 15: 0.744, 19: 0.6428571428571429, 20: 0.7035573122529645, 24: 0.16, 25: 0.40160642570281124, 26: 0.41706161137440756, 29: 0.3563218390804598, 30: 0.2}
Micro-average F1 score: 0.4463185209211807
Weighted-average F1 score: 0.42974912620951816
F1 score per class: {32: 0.41496598639455784, 33: 0.32065217391304346, 35: 0.56, 36: 0.5205479452054794, 37: 0.26857142857142857, 6: 0.23529411764705882, 38: 0.3939393939393939, 8: 0.639269406392694, 15: 0.7622950819672131, 19: 0.5142857142857142, 20: 0.7035573122529645, 24: 0.14545454545454545, 25: 0.39357429718875503, 26: 0.39622641509433965, 29: 0.4094488188976378, 30: 0.2}
Micro-average F1 score: 0.4504201680672269
Weighted-average F1 score: 0.4332735855250668
cur_acc:  ['0.6868', '0.4308', '0.4385']
his_acc:  ['0.6868', '0.5655', '0.5184']
cur_acc des:  ['0.6762', '0.4423', '0.4263']
his_acc des:  ['0.6762', '0.4887', '0.4463']
cur_acc rrf:  ['0.6780', '0.4333', '0.4365']
his_acc rrf:  ['0.6780', '0.4876', '0.4504']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 115.8849393CurrentTrain: epoch  0, batch     1 | loss: 62.0490002CurrentTrain: epoch  0, batch     2 | loss: 37.8369519CurrentTrain: epoch  0, batch     3 | loss: 81.7939424CurrentTrain: epoch  0, batch     4 | loss: 82.6755684CurrentTrain: epoch  0, batch     5 | loss: 46.9794869CurrentTrain: epoch  0, batch     6 | loss: 46.5766482CurrentTrain: epoch  1, batch     0 | loss: 70.9115859CurrentTrain: epoch  1, batch     1 | loss: 46.9740438CurrentTrain: epoch  1, batch     2 | loss: 72.6777948CurrentTrain: epoch  1, batch     3 | loss: 110.6675429CurrentTrain: epoch  1, batch     4 | loss: 70.2711410CurrentTrain: epoch  1, batch     5 | loss: 53.0642988CurrentTrain: epoch  1, batch     6 | loss: 67.6400348CurrentTrain: epoch  2, batch     0 | loss: 54.2691946CurrentTrain: epoch  2, batch     1 | loss: 70.6005417CurrentTrain: epoch  2, batch     2 | loss: 74.3026466CurrentTrain: epoch  2, batch     3 | loss: 43.1197082CurrentTrain: epoch  2, batch     4 | loss: 40.2630198CurrentTrain: epoch  2, batch     5 | loss: 50.9768286CurrentTrain: epoch  2, batch     6 | loss: 106.6261910CurrentTrain: epoch  3, batch     0 | loss: 69.6318301CurrentTrain: epoch  3, batch     1 | loss: 52.1198227CurrentTrain: epoch  3, batch     2 | loss: 51.7835338CurrentTrain: epoch  3, batch     3 | loss: 48.0075592CurrentTrain: epoch  3, batch     4 | loss: 50.4578033CurrentTrain: epoch  3, batch     5 | loss: 53.0949833CurrentTrain: epoch  3, batch     6 | loss: 39.7701722CurrentTrain: epoch  4, batch     0 | loss: 106.3871282CurrentTrain: epoch  4, batch     1 | loss: 68.7636662CurrentTrain: epoch  4, batch     2 | loss: 222.8769642CurrentTrain: epoch  4, batch     3 | loss: 50.4805446CurrentTrain: epoch  4, batch     4 | loss: 34.6438329CurrentTrain: epoch  4, batch     5 | loss: 66.8026330CurrentTrain: epoch  4, batch     6 | loss: 47.8187291CurrentTrain: epoch  5, batch     0 | loss: 103.9453701CurrentTrain: epoch  5, batch     1 | loss: 69.2994403CurrentTrain: epoch  5, batch     2 | loss: 32.9595462CurrentTrain: epoch  5, batch     3 | loss: 39.0882903CurrentTrain: epoch  5, batch     4 | loss: 68.7091730CurrentTrain: epoch  5, batch     5 | loss: 47.8718528CurrentTrain: epoch  5, batch     6 | loss: 65.9676469CurrentTrain: epoch  6, batch     0 | loss: 38.1506732CurrentTrain: epoch  6, batch     1 | loss: 37.5553425CurrentTrain: epoch  6, batch     2 | loss: 68.5638262CurrentTrain: epoch  6, batch     3 | loss: 49.5703795CurrentTrain: epoch  6, batch     4 | loss: 49.4149824CurrentTrain: epoch  6, batch     5 | loss: 39.7444573CurrentTrain: epoch  6, batch     6 | loss: 64.8383021CurrentTrain: epoch  7, batch     0 | loss: 48.2506844CurrentTrain: epoch  7, batch     1 | loss: 50.1168253CurrentTrain: epoch  7, batch     2 | loss: 68.3481381CurrentTrain: epoch  7, batch     3 | loss: 50.2461221CurrentTrain: epoch  7, batch     4 | loss: 68.9543516CurrentTrain: epoch  7, batch     5 | loss: 37.5458061CurrentTrain: epoch  7, batch     6 | loss: 46.2789940CurrentTrain: epoch  8, batch     0 | loss: 107.2818746CurrentTrain: epoch  8, batch     1 | loss: 48.2554065CurrentTrain: epoch  8, batch     2 | loss: 107.6364039CurrentTrain: epoch  8, batch     3 | loss: 47.8752261CurrentTrain: epoch  8, batch     4 | loss: 47.5341559CurrentTrain: epoch  8, batch     5 | loss: 30.7210841CurrentTrain: epoch  8, batch     6 | loss: 46.2295023CurrentTrain: epoch  9, batch     0 | loss: 33.9386126CurrentTrain: epoch  9, batch     1 | loss: 48.6341338CurrentTrain: epoch  9, batch     2 | loss: 107.7363586CurrentTrain: epoch  9, batch     3 | loss: 49.4780569CurrentTrain: epoch  9, batch     4 | loss: 68.2210526CurrentTrain: epoch  9, batch     5 | loss: 47.8811409CurrentTrain: epoch  9, batch     6 | loss: 47.6964588
MemoryTrain:  epoch  0, batch     0 | loss: 0.6370810MemoryTrain:  epoch  1, batch     0 | loss: 0.5620016MemoryTrain:  epoch  2, batch     0 | loss: 0.3935025MemoryTrain:  epoch  3, batch     0 | loss: 0.3601234MemoryTrain:  epoch  4, batch     0 | loss: 0.3288002MemoryTrain:  epoch  5, batch     0 | loss: 0.2853048MemoryTrain:  epoch  6, batch     0 | loss: 0.2770909MemoryTrain:  epoch  7, batch     0 | loss: 0.1690111MemoryTrain:  epoch  8, batch     0 | loss: 0.1282140MemoryTrain:  epoch  9, batch     0 | loss: 0.1113298

F1 score per class: {32: 0.15444015444015444, 1: 0.59375, 34: 0.0, 3: 0.0, 35: 0.0, 37: 0.0, 6: 0.0, 38: 0.459214501510574, 8: 0.0, 14: 0.0, 19: 0.0, 20: 0.0, 22: 0.391304347826087, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.29947460595446584
Weighted-average F1 score: 0.2741851613556208
F1 score per class: {1: 0.14465408805031446, 3: 0.5849056603773585, 6: 0.0, 8: 0.0, 14: 0.029411764705882353, 19: 0.0, 20: 0.0, 22: 0.4430769230769231, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.4195121951219512, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.24094955489614242
Weighted-average F1 score: 0.21048328131421642
F1 score per class: {1: 0.14743589743589744, 3: 0.5753424657534246, 6: 0.0, 8: 0.0, 14: 0.028985507246376812, 19: 0.0, 20: 0.0, 22: 0.4444444444444444, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.3888888888888889, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.24550526968381897
Weighted-average F1 score: 0.21630260121136297

F1 score per class: {1: 0.12861736334405144, 3: 0.4634146341463415, 6: 0.3448275862068966, 8: 0.313953488372093, 14: 0.0, 15: 0.6666666666666666, 19: 0.19672131147540983, 20: 0.32673267326732675, 22: 0.3848101265822785, 24: 0.1, 25: 0.3225806451612903, 26: 0.6698564593301436, 29: 0.75, 30: 0.9444444444444444, 32: 0.5363984674329502, 33: 0.4, 34: 0.23225806451612904, 35: 0.29213483146067415, 36: 0.16216216216216217, 37: 0.056338028169014086, 38: 0.15789473684210525}
Micro-average F1 score: 0.3630613535736875
Weighted-average F1 score: 0.3634260205236579
F1 score per class: {1: 0.11219512195121951, 3: 0.40129449838187703, 6: 0.38323353293413176, 8: 0.2969187675070028, 14: 0.018461538461538463, 15: 0.42857142857142855, 19: 0.31654676258992803, 20: 0.27564102564102566, 22: 0.35121951219512193, 24: 0.0273972602739726, 25: 0.4722222222222222, 26: 0.6278026905829597, 29: 0.6838235294117647, 30: 0.6792452830188679, 32: 0.519298245614035, 33: 0.2, 34: 0.172, 35: 0.3181818181818182, 36: 0.3373493975903614, 37: 0.136986301369863, 38: 0.1267605633802817}
Micro-average F1 score: 0.31421548545678
Weighted-average F1 score: 0.29638459174369025
F1 score per class: {1: 0.11616161616161616, 3: 0.38414634146341464, 6: 0.38283828382838286, 8: 0.3219814241486068, 14: 0.01729106628242075, 15: 0.46153846153846156, 19: 0.30097087378640774, 20: 0.26380368098159507, 22: 0.357487922705314, 24: 0.03125, 25: 0.3225806451612903, 26: 0.639269406392694, 29: 0.7165354330708661, 30: 0.6545454545454545, 32: 0.5278810408921933, 33: 0.23529411764705882, 34: 0.171990171990172, 35: 0.31336405529953915, 36: 0.3473053892215569, 37: 0.11267605633802817, 38: 0.11428571428571428}
Micro-average F1 score: 0.3146067415730337
Weighted-average F1 score: 0.2969092144347395
cur_acc:  ['0.6868', '0.4308', '0.4385', '0.2995']
his_acc:  ['0.6868', '0.5655', '0.5184', '0.3631']
cur_acc des:  ['0.6762', '0.4423', '0.4263', '0.2409']
his_acc des:  ['0.6762', '0.4887', '0.4463', '0.3142']
cur_acc rrf:  ['0.6780', '0.4333', '0.4365', '0.2455']
his_acc rrf:  ['0.6780', '0.4876', '0.4504', '0.3146']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 58.1538459CurrentTrain: epoch  0, batch     1 | loss: 80.1217562CurrentTrain: epoch  0, batch     2 | loss: 52.8259854CurrentTrain: epoch  0, batch     3 | loss: 54.9770313CurrentTrain: epoch  0, batch     4 | loss: 45.8341715CurrentTrain: epoch  0, batch     5 | loss: 58.8534936CurrentTrain: epoch  0, batch     6 | loss: 55.2059726CurrentTrain: epoch  0, batch     7 | loss: 2.9463306CurrentTrain: epoch  1, batch     0 | loss: 74.9315957CurrentTrain: epoch  1, batch     1 | loss: 52.7003517CurrentTrain: epoch  1, batch     2 | loss: 50.8431801CurrentTrain: epoch  1, batch     3 | loss: 41.2311619CurrentTrain: epoch  1, batch     4 | loss: 68.9932106CurrentTrain: epoch  1, batch     5 | loss: 42.6083591CurrentTrain: epoch  1, batch     6 | loss: 41.5880220CurrentTrain: epoch  1, batch     7 | loss: 2.8796212CurrentTrain: epoch  2, batch     0 | loss: 107.1266091CurrentTrain: epoch  2, batch     1 | loss: 73.7897782CurrentTrain: epoch  2, batch     2 | loss: 51.9551072CurrentTrain: epoch  2, batch     3 | loss: 67.7412839CurrentTrain: epoch  2, batch     4 | loss: 40.3085018CurrentTrain: epoch  2, batch     5 | loss: 49.5917349CurrentTrain: epoch  2, batch     6 | loss: 39.3770322CurrentTrain: epoch  2, batch     7 | loss: 2.8712063CurrentTrain: epoch  3, batch     0 | loss: 104.1404881CurrentTrain: epoch  3, batch     1 | loss: 68.2365424CurrentTrain: epoch  3, batch     2 | loss: 30.6776526CurrentTrain: epoch  3, batch     3 | loss: 68.8672970CurrentTrain: epoch  3, batch     4 | loss: 41.3193486CurrentTrain: epoch  3, batch     5 | loss: 69.6350924CurrentTrain: epoch  3, batch     6 | loss: 39.5601719CurrentTrain: epoch  3, batch     7 | loss: 2.8684790CurrentTrain: epoch  4, batch     0 | loss: 66.0439397CurrentTrain: epoch  4, batch     1 | loss: 49.2940735CurrentTrain: epoch  4, batch     2 | loss: 38.4957599CurrentTrain: epoch  4, batch     3 | loss: 67.0422139CurrentTrain: epoch  4, batch     4 | loss: 69.3129139CurrentTrain: epoch  4, batch     5 | loss: 48.7673850CurrentTrain: epoch  4, batch     6 | loss: 108.2495248CurrentTrain: epoch  4, batch     7 | loss: 2.8653348CurrentTrain: epoch  5, batch     0 | loss: 49.8352575CurrentTrain: epoch  5, batch     1 | loss: 108.0353141CurrentTrain: epoch  5, batch     2 | loss: 66.9828143CurrentTrain: epoch  5, batch     3 | loss: 37.8534770CurrentTrain: epoch  5, batch     4 | loss: 36.4296856CurrentTrain: epoch  5, batch     5 | loss: 68.6437700CurrentTrain: epoch  5, batch     6 | loss: 48.0854517CurrentTrain: epoch  5, batch     7 | loss: 2.8338358CurrentTrain: epoch  6, batch     0 | loss: 67.7575335CurrentTrain: epoch  6, batch     1 | loss: 102.5707744CurrentTrain: epoch  6, batch     2 | loss: 37.8045671CurrentTrain: epoch  6, batch     3 | loss: 68.4831436CurrentTrain: epoch  6, batch     4 | loss: 48.5781089CurrentTrain: epoch  6, batch     5 | loss: 62.9841411CurrentTrain: epoch  6, batch     6 | loss: 47.7851944CurrentTrain: epoch  6, batch     7 | loss: 2.8618202CurrentTrain: epoch  7, batch     0 | loss: 68.7543089CurrentTrain: epoch  7, batch     1 | loss: 47.9961051CurrentTrain: epoch  7, batch     2 | loss: 47.0649981CurrentTrain: epoch  7, batch     3 | loss: 49.2002899CurrentTrain: epoch  7, batch     4 | loss: 68.1835097CurrentTrain: epoch  7, batch     5 | loss: 47.7239433CurrentTrain: epoch  7, batch     6 | loss: 64.5091796CurrentTrain: epoch  7, batch     7 | loss: 2.8115514CurrentTrain: epoch  8, batch     0 | loss: 66.2750172CurrentTrain: epoch  8, batch     1 | loss: 34.5051751CurrentTrain: epoch  8, batch     2 | loss: 49.0197494CurrentTrain: epoch  8, batch     3 | loss: 49.3607084CurrentTrain: epoch  8, batch     4 | loss: 107.3482196CurrentTrain: epoch  8, batch     5 | loss: 49.0879262CurrentTrain: epoch  8, batch     6 | loss: 47.9633606CurrentTrain: epoch  8, batch     7 | loss: 0.4353424CurrentTrain: epoch  9, batch     0 | loss: 46.3457666CurrentTrain: epoch  9, batch     1 | loss: 46.2826341CurrentTrain: epoch  9, batch     2 | loss: 49.1396780CurrentTrain: epoch  9, batch     3 | loss: 107.1167410CurrentTrain: epoch  9, batch     4 | loss: 34.7439332CurrentTrain: epoch  9, batch     5 | loss: 68.1626204CurrentTrain: epoch  9, batch     6 | loss: 68.1782974CurrentTrain: epoch  9, batch     7 | loss: 2.8200815
MemoryTrain:  epoch  0, batch     0 | loss: 0.4543863MemoryTrain:  epoch  1, batch     0 | loss: 0.3659635MemoryTrain:  epoch  2, batch     0 | loss: 0.2901812MemoryTrain:  epoch  3, batch     0 | loss: 0.2352372MemoryTrain:  epoch  4, batch     0 | loss: 0.1500132MemoryTrain:  epoch  5, batch     0 | loss: 0.1368853MemoryTrain:  epoch  6, batch     0 | loss: 0.1115851MemoryTrain:  epoch  7, batch     0 | loss: 0.0793385MemoryTrain:  epoch  8, batch     0 | loss: 0.0674918MemoryTrain:  epoch  9, batch     0 | loss: 0.0609991

F1 score per class: {3: 0.0, 5: 0.8826291079812206, 6: 0.0, 8: 0.0, 10: 0.5228758169934641, 14: 0.0, 15: 0.0, 16: 0.5428571428571428, 17: 0.0, 18: 0.32142857142857145, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 36: 0.0, 38: 0.0}
Micro-average F1 score: 0.5311475409836065
Weighted-average F1 score: 0.4648830421376051
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6430868167202572, 6: 0.0, 8: 0.0, 10: 0.5786802030456852, 14: 0.0, 15: 0.0, 16: 0.5208333333333334, 17: 0.35294117647058826, 18: 0.1990521327014218, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.34361968306922436
Weighted-average F1 score: 0.2856521110782425
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6734006734006734, 6: 0.0, 8: 0.0, 10: 0.6048780487804878, 14: 0.0, 15: 0.0, 16: 0.5263157894736842, 17: 0.0, 18: 0.20952380952380953, 20: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3644289450741064
Weighted-average F1 score: 0.30611971658472154

F1 score per class: {1: 0.11688311688311688, 3: 0.4228571428571429, 5: 0.7736625514403292, 6: 0.2949640287769784, 8: 0.18497109826589594, 10: 0.3418803418803419, 14: 0.0, 15: 0.6153846153846154, 16: 0.48717948717948717, 17: 0.0, 18: 0.16071428571428573, 19: 0.2647058823529412, 20: 0.3103448275862069, 22: 0.3756345177664975, 24: 0.0975609756097561, 25: 0.3225806451612903, 26: 0.6542056074766355, 29: 0.748898678414097, 30: 0.9444444444444444, 32: 0.5512367491166078, 33: 0.42857142857142855, 34: 0.044444444444444446, 35: 0.1791044776119403, 36: 0.057971014492753624, 37: 0.14285714285714285, 38: 0.125}
Micro-average F1 score: 0.3708467309753483
Weighted-average F1 score: 0.3775042870235163
F1 score per class: {1: 0.1111111111111111, 3: 0.37770897832817335, 5: 0.45977011494252873, 6: 0.311614730878187, 8: 0.3202846975088968, 10: 0.3048128342245989, 14: 0.01904761904761905, 15: 0.3870967741935484, 16: 0.3472222222222222, 17: 0.09375, 18: 0.11764705882352941, 19: 0.3046875, 20: 0.28771929824561404, 22: 0.3421686746987952, 24: 0.02666666666666667, 25: 0.5205479452054794, 26: 0.611353711790393, 29: 0.7003891050583657, 30: 0.8, 32: 0.459214501510574, 33: 0.16326530612244897, 34: 0.16835016835016836, 35: 0.3130434782608696, 36: 0.26737967914438504, 37: 0.175, 38: 0.15384615384615385}
Micro-average F1 score: 0.31013716962194715
Weighted-average F1 score: 0.2958423764365004
F1 score per class: {1: 0.11442786069651742, 3: 0.38961038961038963, 5: 0.5128205128205128, 6: 0.3137254901960784, 8: 0.3283582089552239, 10: 0.3123425692695214, 14: 0.018018018018018018, 15: 0.375, 16: 0.373134328358209, 17: 0.0, 18: 0.12188365650969529, 19: 0.330188679245283, 20: 0.2724252491694352, 22: 0.34299516908212563, 24: 0.028169014084507043, 25: 0.4927536231884058, 26: 0.6222222222222222, 29: 0.72, 30: 0.7659574468085106, 32: 0.48253968253968255, 33: 0.1276595744680851, 34: 0.16974169741697417, 35: 0.32673267326732675, 36: 0.3194444444444444, 37: 0.1794871794871795, 38: 0.14084507042253522}
Micro-average F1 score: 0.3186334321073732
Weighted-average F1 score: 0.30268037566404093
cur_acc:  ['0.6868', '0.4308', '0.4385', '0.2995', '0.5311']
his_acc:  ['0.6868', '0.5655', '0.5184', '0.3631', '0.3708']
cur_acc des:  ['0.6762', '0.4423', '0.4263', '0.2409', '0.3436']
his_acc des:  ['0.6762', '0.4887', '0.4463', '0.3142', '0.3101']
cur_acc rrf:  ['0.6780', '0.4333', '0.4365', '0.2455', '0.3644']
his_acc rrf:  ['0.6780', '0.4876', '0.4504', '0.3146', '0.3186']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 46.5099262CurrentTrain: epoch  0, batch     1 | loss: 94.0741820CurrentTrain: epoch  0, batch     2 | loss: 49.2578948CurrentTrain: epoch  0, batch     3 | loss: 59.9420399CurrentTrain: epoch  0, batch     4 | loss: 40.5698793CurrentTrain: epoch  0, batch     5 | loss: 77.6491471CurrentTrain: epoch  1, batch     0 | loss: 59.5426697CurrentTrain: epoch  1, batch     1 | loss: 59.6571577CurrentTrain: epoch  1, batch     2 | loss: 71.1498333CurrentTrain: epoch  1, batch     3 | loss: 42.3046723CurrentTrain: epoch  1, batch     4 | loss: 57.5452002CurrentTrain: epoch  1, batch     5 | loss: 50.9718221CurrentTrain: epoch  2, batch     0 | loss: 41.2877792CurrentTrain: epoch  2, batch     1 | loss: 111.7369256CurrentTrain: epoch  2, batch     2 | loss: 50.5248758CurrentTrain: epoch  2, batch     3 | loss: 33.6376284CurrentTrain: epoch  2, batch     4 | loss: 53.7242677CurrentTrain: epoch  2, batch     5 | loss: 50.3798055CurrentTrain: epoch  3, batch     0 | loss: 56.8098149CurrentTrain: epoch  3, batch     1 | loss: 34.2263568CurrentTrain: epoch  3, batch     2 | loss: 47.7597744CurrentTrain: epoch  3, batch     3 | loss: 39.4707453CurrentTrain: epoch  3, batch     4 | loss: 32.7613448CurrentTrain: epoch  3, batch     5 | loss: 50.0262092CurrentTrain: epoch  4, batch     0 | loss: 71.0282297CurrentTrain: epoch  4, batch     1 | loss: 107.5935677CurrentTrain: epoch  4, batch     2 | loss: 41.2025070CurrentTrain: epoch  4, batch     3 | loss: 48.0228139CurrentTrain: epoch  4, batch     4 | loss: 33.8740866CurrentTrain: epoch  4, batch     5 | loss: 49.1520987CurrentTrain: epoch  5, batch     0 | loss: 38.1475023CurrentTrain: epoch  5, batch     1 | loss: 35.6346564CurrentTrain: epoch  5, batch     2 | loss: 36.9770253CurrentTrain: epoch  5, batch     3 | loss: 67.5919052CurrentTrain: epoch  5, batch     4 | loss: 68.8975701CurrentTrain: epoch  5, batch     5 | loss: 77.6136443CurrentTrain: epoch  6, batch     0 | loss: 106.9404150CurrentTrain: epoch  6, batch     1 | loss: 68.6178832CurrentTrain: epoch  6, batch     2 | loss: 36.6439863CurrentTrain: epoch  6, batch     3 | loss: 37.0768932CurrentTrain: epoch  6, batch     4 | loss: 30.4580643CurrentTrain: epoch  6, batch     5 | loss: 26.5288988CurrentTrain: epoch  7, batch     0 | loss: 47.1563748CurrentTrain: epoch  7, batch     1 | loss: 49.3755913CurrentTrain: epoch  7, batch     2 | loss: 107.1890959CurrentTrain: epoch  7, batch     3 | loss: 36.0384278CurrentTrain: epoch  7, batch     4 | loss: 47.1446490CurrentTrain: epoch  7, batch     5 | loss: 33.8190822CurrentTrain: epoch  8, batch     0 | loss: 49.1999452CurrentTrain: epoch  8, batch     1 | loss: 27.8627001CurrentTrain: epoch  8, batch     2 | loss: 69.6626370CurrentTrain: epoch  8, batch     3 | loss: 29.9234496CurrentTrain: epoch  8, batch     4 | loss: 47.7212762CurrentTrain: epoch  8, batch     5 | loss: 48.4057722CurrentTrain: epoch  9, batch     0 | loss: 36.1677282CurrentTrain: epoch  9, batch     1 | loss: 68.3383766CurrentTrain: epoch  9, batch     2 | loss: 37.7598311CurrentTrain: epoch  9, batch     3 | loss: 64.3889766CurrentTrain: epoch  9, batch     4 | loss: 34.7013109CurrentTrain: epoch  9, batch     5 | loss: 48.2494791
MemoryTrain:  epoch  0, batch     0 | loss: 0.3250667MemoryTrain:  epoch  1, batch     0 | loss: 0.3239418MemoryTrain:  epoch  2, batch     0 | loss: 0.1986462MemoryTrain:  epoch  3, batch     0 | loss: 0.1805724MemoryTrain:  epoch  4, batch     0 | loss: 0.1462960MemoryTrain:  epoch  5, batch     0 | loss: 0.1073144MemoryTrain:  epoch  6, batch     0 | loss: 0.0933664MemoryTrain:  epoch  7, batch     0 | loss: 0.0977451MemoryTrain:  epoch  8, batch     0 | loss: 0.0819998MemoryTrain:  epoch  9, batch     0 | loss: 0.0757415

F1 score per class: {0: 0.8918918918918919, 1: 0.0, 3: 0.0, 4: 0.9417989417989417, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.058823529411764705, 14: 0.0, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.17142857142857143, 22: 0.0, 23: 0.7272727272727273, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 36: 0.0, 38: 0.0}
Micro-average F1 score: 0.5980582524271845
Weighted-average F1 score: 0.4780674043286207
F1 score per class: {0: 0.6486486486486487, 1: 0.0, 3: 0.0, 4: 0.9696969696969697, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.1568627450980392, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.26, 22: 0.0, 23: 0.7333333333333333, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.416
Weighted-average F1 score: 0.3049149963941163
F1 score per class: {0: 0.6486486486486487, 1: 0.0, 3: 0.0, 4: 0.9795918367346939, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.14545454545454545, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.26, 22: 0.0, 23: 0.7272727272727273, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 38: 0.0}
Micro-average F1 score: 0.43146603098927294
Weighted-average F1 score: 0.3183711234530528

F1 score per class: {0: 0.4024390243902439, 1: 0.10982658959537572, 3: 0.47058823529411764, 4: 0.9319371727748691, 5: 0.8260869565217391, 6: 0.24644549763033174, 8: 0.17647058823529413, 10: 0.3288888888888889, 13: 0.010526315789473684, 14: 0.0, 15: 0.7058823529411765, 16: 0.4166666666666667, 17: 0.29411764705882354, 18: 0.13861386138613863, 19: 0.26573426573426573, 20: 0.2945205479452055, 21: 0.08695652173913043, 22: 0.4797297297297297, 23: 0.6666666666666666, 24: 0.1, 25: 0.3225806451612903, 26: 0.6534653465346535, 29: 0.6978723404255319, 30: 0.972972972972973, 32: 0.5327868852459017, 33: 0.3076923076923077, 34: 0.07142857142857142, 35: 0.1, 36: 0.0547945205479452, 37: 0.0625, 38: 0.21621621621621623}
Micro-average F1 score: 0.3863103609939053
Weighted-average F1 score: 0.37904143871936
F1 score per class: {0: 0.1575492341356674, 1: 0.10933940774487472, 3: 0.319634703196347, 4: 0.9504950495049505, 5: 0.449438202247191, 6: 0.3209169054441261, 8: 0.33076923076923076, 10: 0.297029702970297, 13: 0.028469750889679714, 14: 0.03319502074688797, 15: 0.7058823529411765, 16: 0.3333333333333333, 17: 0.15789473684210525, 18: 0.12716763005780346, 19: 0.3013698630136986, 20: 0.23017902813299232, 21: 0.06770833333333333, 22: 0.5063291139240507, 23: 0.6407766990291263, 24: 0.0, 25: 0.5405405405405406, 26: 0.6188340807174888, 29: 0.654275092936803, 30: 0.8085106382978723, 32: 0.49169435215946844, 33: 0.17142857142857143, 34: 0.18461538461538463, 35: 0.30638297872340425, 36: 0.2641509433962264, 37: 0.19753086419753085, 38: 0.1724137931034483}
Micro-average F1 score: 0.30471984543196245
Weighted-average F1 score: 0.27671618153126265
F1 score per class: {0: 0.15789473684210525, 1: 0.11009174311926606, 3: 0.3482587064676617, 4: 0.96, 5: 0.5305039787798409, 6: 0.31645569620253167, 8: 0.32432432432432434, 10: 0.304635761589404, 13: 0.025806451612903226, 14: 0.031007751937984496, 15: 0.7058823529411765, 16: 0.32051282051282054, 17: 0.1791044776119403, 18: 0.12574850299401197, 19: 0.312, 20: 0.22332506203473945, 21: 0.07854984894259819, 22: 0.483271375464684, 23: 0.6336633663366337, 24: 0.0, 25: 0.5352112676056338, 26: 0.6388888888888888, 29: 0.6901960784313725, 30: 0.8085106382978723, 32: 0.5033112582781457, 33: 0.17142857142857143, 34: 0.18867924528301888, 35: 0.3076923076923077, 36: 0.2896551724137931, 37: 0.18181818181818182, 38: 0.18181818181818182}
Micro-average F1 score: 0.3132599884192241
Weighted-average F1 score: 0.28306849528564526
cur_acc:  ['0.6868', '0.4308', '0.4385', '0.2995', '0.5311', '0.5981']
his_acc:  ['0.6868', '0.5655', '0.5184', '0.3631', '0.3708', '0.3863']
cur_acc des:  ['0.6762', '0.4423', '0.4263', '0.2409', '0.3436', '0.4160']
his_acc des:  ['0.6762', '0.4887', '0.4463', '0.3142', '0.3101', '0.3047']
cur_acc rrf:  ['0.6780', '0.4333', '0.4365', '0.2455', '0.3644', '0.4315']
his_acc rrf:  ['0.6780', '0.4876', '0.4504', '0.3146', '0.3186', '0.3133']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 118.7916603CurrentTrain: epoch  0, batch     1 | loss: 66.8830084CurrentTrain: epoch  0, batch     2 | loss: 46.2352236CurrentTrain: epoch  0, batch     3 | loss: 76.0251987CurrentTrain: epoch  0, batch     4 | loss: 53.1499102CurrentTrain: epoch  0, batch     5 | loss: 43.2619115CurrentTrain: epoch  0, batch     6 | loss: 60.3233578CurrentTrain: epoch  1, batch     0 | loss: 111.9030361CurrentTrain: epoch  1, batch     1 | loss: 45.7281244CurrentTrain: epoch  1, batch     2 | loss: 44.1851438CurrentTrain: epoch  1, batch     3 | loss: 75.0471612CurrentTrain: epoch  1, batch     4 | loss: 57.7838900CurrentTrain: epoch  1, batch     5 | loss: 42.0346788CurrentTrain: epoch  1, batch     6 | loss: 60.1802902CurrentTrain: epoch  2, batch     0 | loss: 70.8945221CurrentTrain: epoch  2, batch     1 | loss: 42.7738001CurrentTrain: epoch  2, batch     2 | loss: 109.6269518CurrentTrain: epoch  2, batch     3 | loss: 108.6333036CurrentTrain: epoch  2, batch     4 | loss: 38.5052080CurrentTrain: epoch  2, batch     5 | loss: 34.2327409CurrentTrain: epoch  2, batch     6 | loss: 16.8549992CurrentTrain: epoch  3, batch     0 | loss: 51.5903397CurrentTrain: epoch  3, batch     1 | loss: 109.6144949CurrentTrain: epoch  3, batch     2 | loss: 31.6241735CurrentTrain: epoch  3, batch     3 | loss: 51.6166761CurrentTrain: epoch  3, batch     4 | loss: 71.4416051CurrentTrain: epoch  3, batch     5 | loss: 74.4755161CurrentTrain: epoch  3, batch     6 | loss: 27.5023397CurrentTrain: epoch  4, batch     0 | loss: 39.4907800CurrentTrain: epoch  4, batch     1 | loss: 70.6958390CurrentTrain: epoch  4, batch     2 | loss: 107.6655082CurrentTrain: epoch  4, batch     3 | loss: 53.1183226CurrentTrain: epoch  4, batch     4 | loss: 39.0862542CurrentTrain: epoch  4, batch     5 | loss: 37.1699928CurrentTrain: epoch  4, batch     6 | loss: 27.9829579CurrentTrain: epoch  5, batch     0 | loss: 105.0656075CurrentTrain: epoch  5, batch     1 | loss: 38.9060885CurrentTrain: epoch  5, batch     2 | loss: 104.1001602CurrentTrain: epoch  5, batch     3 | loss: 71.8912137CurrentTrain: epoch  5, batch     4 | loss: 39.3837768CurrentTrain: epoch  5, batch     5 | loss: 50.4433069CurrentTrain: epoch  5, batch     6 | loss: 9.4199824CurrentTrain: epoch  6, batch     0 | loss: 50.0502933CurrentTrain: epoch  6, batch     1 | loss: 49.7730131CurrentTrain: epoch  6, batch     2 | loss: 39.1259709CurrentTrain: epoch  6, batch     3 | loss: 38.6458495CurrentTrain: epoch  6, batch     4 | loss: 46.9313900CurrentTrain: epoch  6, batch     5 | loss: 49.3931118CurrentTrain: epoch  6, batch     6 | loss: 59.9118363CurrentTrain: epoch  7, batch     0 | loss: 68.4738748CurrentTrain: epoch  7, batch     1 | loss: 66.9983155CurrentTrain: epoch  7, batch     2 | loss: 37.0180789CurrentTrain: epoch  7, batch     3 | loss: 35.1528473CurrentTrain: epoch  7, batch     4 | loss: 71.6397642CurrentTrain: epoch  7, batch     5 | loss: 49.3009294CurrentTrain: epoch  7, batch     6 | loss: 26.8395148CurrentTrain: epoch  8, batch     0 | loss: 67.7930784CurrentTrain: epoch  8, batch     1 | loss: 46.8278890CurrentTrain: epoch  8, batch     2 | loss: 66.4527627CurrentTrain: epoch  8, batch     3 | loss: 66.8218810CurrentTrain: epoch  8, batch     4 | loss: 68.3432346CurrentTrain: epoch  8, batch     5 | loss: 35.4555742CurrentTrain: epoch  8, batch     6 | loss: 16.2709741CurrentTrain: epoch  9, batch     0 | loss: 34.8506983CurrentTrain: epoch  9, batch     1 | loss: 49.5569573CurrentTrain: epoch  9, batch     2 | loss: 48.4562716CurrentTrain: epoch  9, batch     3 | loss: 108.0149872CurrentTrain: epoch  9, batch     4 | loss: 63.9493581CurrentTrain: epoch  9, batch     5 | loss: 68.4445926CurrentTrain: epoch  9, batch     6 | loss: 15.9138421
MemoryTrain:  epoch  0, batch     0 | loss: 0.1501200MemoryTrain:  epoch  0, batch     1 | loss: 0.1946058MemoryTrain:  epoch  1, batch     0 | loss: 0.2879474MemoryTrain:  epoch  1, batch     1 | loss: 0.1333312MemoryTrain:  epoch  2, batch     0 | loss: 0.2901695MemoryTrain:  epoch  2, batch     1 | loss: 0.0937368MemoryTrain:  epoch  3, batch     0 | loss: 0.1550251MemoryTrain:  epoch  3, batch     1 | loss: 0.0892678MemoryTrain:  epoch  4, batch     0 | loss: 0.1385674MemoryTrain:  epoch  4, batch     1 | loss: 0.0700564MemoryTrain:  epoch  5, batch     0 | loss: 0.0915831MemoryTrain:  epoch  5, batch     1 | loss: 0.0911650MemoryTrain:  epoch  6, batch     0 | loss: 0.0915788MemoryTrain:  epoch  6, batch     1 | loss: 0.0578796MemoryTrain:  epoch  7, batch     0 | loss: 0.0748483MemoryTrain:  epoch  7, batch     1 | loss: 0.0501581MemoryTrain:  epoch  8, batch     0 | loss: 0.0710163MemoryTrain:  epoch  8, batch     1 | loss: 0.0571494MemoryTrain:  epoch  9, batch     0 | loss: 0.0681010MemoryTrain:  epoch  9, batch     1 | loss: 0.0326838

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.41379310344827586, 3: 0.0, 4: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.3418803418803419, 12: 0.3684210526315789, 13: 0.0, 14: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 28: 0.24, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.11764705882352941}
Micro-average F1 score: 0.23625254582484725
Weighted-average F1 score: 0.15360745550297106
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.3111111111111111, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.5130434782608696, 12: 0.5617021276595745, 13: 0.0, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 28: 0.09900990099009901, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.15384615384615385}
Micro-average F1 score: 0.2458001768346596
Weighted-average F1 score: 0.18742986379969198
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.32558139534883723, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.5104602510460251, 12: 0.5679012345679012, 13: 0.0, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 28: 0.13725490196078433, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.2222222222222222}
Micro-average F1 score: 0.26838235294117646
Weighted-average F1 score: 0.21075948396573388

F1 score per class: {0: 0.48484848484848486, 1: 0.11634349030470914, 2: 0.22641509433962265, 3: 0.39344262295081966, 4: 0.8900523560209425, 5: 0.8558558558558559, 6: 0.3028391167192429, 8: 0.1323529411764706, 10: 0.23376623376623376, 11: 0.1694915254237288, 12: 0.17391304347826086, 13: 0.021739130434782608, 14: 0.020618556701030927, 15: 0.7058823529411765, 16: 0.4788732394366197, 17: 0.20408163265306123, 18: 0.0851063829787234, 19: 0.3448275862068966, 20: 0.2796610169491525, 21: 0.07058823529411765, 22: 0.4610169491525424, 23: 0.631578947368421, 24: 0.16666666666666666, 25: 0.3225806451612903, 26: 0.6565656565656566, 28: 0.061224489795918366, 29: 0.6771653543307087, 30: 0.972972972972973, 32: 0.5214285714285715, 33: 0.2857142857142857, 34: 0.0, 35: 0.12698412698412698, 36: 0.028985507246376812, 37: 0.12987012987012986, 38: 0.1917808219178082, 39: 0.1}
Micro-average F1 score: 0.3514811849479584
Weighted-average F1 score: 0.3419163421669076
F1 score per class: {0: 0.16783216783216784, 1: 0.0975609756097561, 2: 0.10687022900763359, 3: 0.3130841121495327, 4: 0.8910891089108911, 5: 0.4728132387706856, 6: 0.26666666666666666, 8: 0.25120772946859904, 10: 0.2796610169491525, 11: 0.18153846153846154, 12: 0.1573301549463647, 13: 0.015384615384615385, 14: 0.02631578947368421, 15: 0.5454545454545454, 16: 0.3851851851851852, 17: 0.12631578947368421, 18: 0.06593406593406594, 19: 0.32116788321167883, 20: 0.18803418803418803, 21: 0.07446808510638298, 22: 0.46875, 23: 0.6017699115044248, 24: 0.1, 25: 0.5263157894736842, 26: 0.6232558139534884, 28: 0.02849002849002849, 29: 0.6075085324232082, 30: 0.7169811320754716, 32: 0.4329896907216495, 33: 0.2608695652173913, 34: 0.1407035175879397, 35: 0.28688524590163933, 36: 0.14545454545454545, 37: 0.15789473684210525, 38: 0.11904761904761904, 39: 0.07692307692307693}
Micro-average F1 score: 0.25327510917030566
Weighted-average F1 score: 0.23070085392901218
F1 score per class: {0: 0.17777777777777778, 1: 0.10126582278481013, 2: 0.11666666666666667, 3: 0.32075471698113206, 4: 0.9081632653061225, 5: 0.5181347150259067, 6: 0.2757009345794392, 8: 0.18064516129032257, 10: 0.3089430894308943, 11: 0.1804733727810651, 12: 0.1504907306434024, 13: 0.011764705882352941, 14: 0.024691358024691357, 15: 0.5714285714285714, 16: 0.45098039215686275, 17: 0.15789473684210525, 18: 0.058394160583941604, 19: 0.34444444444444444, 20: 0.19008264462809918, 21: 0.07692307692307693, 22: 0.4674329501915709, 23: 0.6285714285714286, 24: 0.10526315789473684, 25: 0.45714285714285713, 26: 0.6320754716981132, 28: 0.040697674418604654, 29: 0.6472727272727272, 30: 0.7450980392156863, 32: 0.4450402144772118, 33: 0.3157894736842105, 34: 0.11904761904761904, 35: 0.3027027027027027, 36: 0.18604651162790697, 37: 0.15584415584415584, 38: 0.10676156583629894, 39: 0.16666666666666666}
Micro-average F1 score: 0.256957928802589
Weighted-average F1 score: 0.23191459460061845
cur_acc:  ['0.6868', '0.4308', '0.4385', '0.2995', '0.5311', '0.5981', '0.2363']
his_acc:  ['0.6868', '0.5655', '0.5184', '0.3631', '0.3708', '0.3863', '0.3515']
cur_acc des:  ['0.6762', '0.4423', '0.4263', '0.2409', '0.3436', '0.4160', '0.2458']
his_acc des:  ['0.6762', '0.4887', '0.4463', '0.3142', '0.3101', '0.3047', '0.2533']
cur_acc rrf:  ['0.6780', '0.4333', '0.4365', '0.2455', '0.3644', '0.4315', '0.2684']
his_acc rrf:  ['0.6780', '0.4876', '0.4504', '0.3146', '0.3186', '0.3133', '0.2570']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 53.2042818CurrentTrain: epoch  0, batch     1 | loss: 51.8669911CurrentTrain: epoch  0, batch     2 | loss: 62.0378015CurrentTrain: epoch  0, batch     3 | loss: 47.5475136CurrentTrain: epoch  0, batch     4 | loss: 31.3570903CurrentTrain: epoch  1, batch     0 | loss: 77.6397906CurrentTrain: epoch  1, batch     1 | loss: 48.6440337CurrentTrain: epoch  1, batch     2 | loss: 36.0793508CurrentTrain: epoch  1, batch     3 | loss: 44.6336084CurrentTrain: epoch  1, batch     4 | loss: 36.8272187CurrentTrain: epoch  2, batch     0 | loss: 53.5213940CurrentTrain: epoch  2, batch     1 | loss: 49.6914819CurrentTrain: epoch  2, batch     2 | loss: 40.4300809CurrentTrain: epoch  2, batch     3 | loss: 51.0376551CurrentTrain: epoch  2, batch     4 | loss: 24.5384770CurrentTrain: epoch  3, batch     0 | loss: 65.1062477CurrentTrain: epoch  3, batch     1 | loss: 49.6937131CurrentTrain: epoch  3, batch     2 | loss: 40.2965319CurrentTrain: epoch  3, batch     3 | loss: 36.1524254CurrentTrain: epoch  3, batch     4 | loss: 31.3274554CurrentTrain: epoch  4, batch     0 | loss: 50.3440397CurrentTrain: epoch  4, batch     1 | loss: 38.5512937CurrentTrain: epoch  4, batch     2 | loss: 109.9606464CurrentTrain: epoch  4, batch     3 | loss: 29.8321838CurrentTrain: epoch  4, batch     4 | loss: 16.9360760CurrentTrain: epoch  5, batch     0 | loss: 35.5389466CurrentTrain: epoch  5, batch     1 | loss: 68.6243071CurrentTrain: epoch  5, batch     2 | loss: 47.0298402CurrentTrain: epoch  5, batch     3 | loss: 44.6889210CurrentTrain: epoch  5, batch     4 | loss: 43.8216986CurrentTrain: epoch  6, batch     0 | loss: 69.2705457CurrentTrain: epoch  6, batch     1 | loss: 37.6995094CurrentTrain: epoch  6, batch     2 | loss: 37.1778595CurrentTrain: epoch  6, batch     3 | loss: 29.2740505CurrentTrain: epoch  6, batch     4 | loss: 27.6880677CurrentTrain: epoch  7, batch     0 | loss: 45.7519143CurrentTrain: epoch  7, batch     1 | loss: 34.8240464CurrentTrain: epoch  7, batch     2 | loss: 66.7216000CurrentTrain: epoch  7, batch     3 | loss: 46.6889521CurrentTrain: epoch  7, batch     4 | loss: 29.2641091CurrentTrain: epoch  8, batch     0 | loss: 27.0518313CurrentTrain: epoch  8, batch     1 | loss: 46.1422609CurrentTrain: epoch  8, batch     2 | loss: 107.8888013CurrentTrain: epoch  8, batch     3 | loss: 29.1550972CurrentTrain: epoch  8, batch     4 | loss: 28.8932525CurrentTrain: epoch  9, batch     0 | loss: 47.4611468CurrentTrain: epoch  9, batch     1 | loss: 34.7992037CurrentTrain: epoch  9, batch     2 | loss: 33.1967432CurrentTrain: epoch  9, batch     3 | loss: 36.5372596CurrentTrain: epoch  9, batch     4 | loss: 145.1021403
MemoryTrain:  epoch  0, batch     0 | loss: 0.1740380MemoryTrain:  epoch  0, batch     1 | loss: 0.0941452MemoryTrain:  epoch  1, batch     0 | loss: 0.2221466MemoryTrain:  epoch  1, batch     1 | loss: 0.0857421MemoryTrain:  epoch  2, batch     0 | loss: 0.1752162MemoryTrain:  epoch  2, batch     1 | loss: 0.0845471MemoryTrain:  epoch  3, batch     0 | loss: 0.2353344MemoryTrain:  epoch  3, batch     1 | loss: 0.0634876MemoryTrain:  epoch  4, batch     0 | loss: 0.1293738MemoryTrain:  epoch  4, batch     1 | loss: 0.0528471MemoryTrain:  epoch  5, batch     0 | loss: 0.1233634MemoryTrain:  epoch  5, batch     1 | loss: 0.0423801MemoryTrain:  epoch  6, batch     0 | loss: 0.0854608MemoryTrain:  epoch  6, batch     1 | loss: 0.0588034MemoryTrain:  epoch  7, batch     0 | loss: 0.0634518MemoryTrain:  epoch  7, batch     1 | loss: 0.0709566MemoryTrain:  epoch  8, batch     0 | loss: 0.0636085MemoryTrain:  epoch  8, batch     1 | loss: 0.0505596MemoryTrain:  epoch  9, batch     0 | loss: 0.0553919MemoryTrain:  epoch  9, batch     1 | loss: 0.0440942

F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 7: 0.25, 9: 0.9230769230769231, 14: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.5217391304347826, 31: 0.25, 32: 0.0, 40: 0.4696969696969697}
Micro-average F1 score: 0.45161290322580644
Weighted-average F1 score: 0.3728969537269933
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.2222222222222222, 8: 0.0, 9: 0.8064516129032258, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.5161290322580645, 28: 0.0, 30: 0.0, 31: 0.17391304347826086, 32: 0.0, 35: 0.0, 40: 0.5876288659793815}
Micro-average F1 score: 0.37349397590361444
Weighted-average F1 score: 0.30443979312629094
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.2222222222222222, 8: 0.0, 9: 0.847457627118644, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.4444444444444444, 28: 0.0, 31: 0.1111111111111111, 32: 0.0, 35: 0.0, 38: 0.0, 40: 0.5757575757575758}
Micro-average F1 score: 0.379746835443038
Weighted-average F1 score: 0.3102874317563583

F1 score per class: {0: 0.4383561643835616, 1: 0.1935483870967742, 2: 0.18181818181818182, 3: 0.3181818181818182, 4: 0.8633879781420765, 5: 0.8847926267281107, 6: 0.018867924528301886, 7: 0.011299435028248588, 8: 0.024390243902439025, 9: 0.8888888888888888, 10: 0.0196078431372549, 11: 0.15706806282722513, 12: 0.13414634146341464, 13: 0.029411764705882353, 14: 0.018691588785046728, 15: 0.75, 16: 0.45161290322580644, 17: 0.0, 18: 0.09523809523809523, 19: 0.3333333333333333, 20: 0.25, 21: 0.08955223880597014, 22: 0.47232472324723246, 23: 0.6382978723404256, 24: 0.05, 25: 0.3225806451612903, 26: 0.6243386243386243, 27: 0.04938271604938271, 28: 0.16326530612244897, 29: 0.6831275720164609, 30: 0.972972972972973, 31: 0.031746031746031744, 32: 0.44171779141104295, 33: 0.15384615384615385, 34: 0.0, 35: 0.1016949152542373, 36: 0.05555555555555555, 37: 0.22727272727272727, 38: 0.14285714285714285, 39: 0.1, 40: 0.2831050228310502}
Micro-average F1 score: 0.32426778242677823
Weighted-average F1 score: 0.32048468522158496
F1 score per class: {0: 0.3575418994413408, 1: 0.1348314606741573, 2: 0.11650485436893204, 3: 0.3404255319148936, 4: 0.9119170984455959, 5: 0.4434589800443459, 6: 0.031746031746031744, 7: 0.006802721088435374, 8: 0.24390243902439024, 9: 0.6756756756756757, 10: 0.17142857142857143, 11: 0.16265060240963855, 12: 0.16534181240063592, 13: 0.01092896174863388, 14: 0.03292181069958848, 15: 0.5454545454545454, 16: 0.36231884057971014, 17: 0.17647058823529413, 18: 0.08527131782945736, 19: 0.3150684931506849, 20: 0.20465116279069767, 21: 0.06741573033707865, 22: 0.4674329501915709, 23: 0.5862068965517241, 24: 0.0392156862745098, 25: 0.4444444444444444, 26: 0.6063348416289592, 27: 0.04892966360856269, 28: 0.03943661971830986, 29: 0.6312056737588653, 30: 0.6181818181818182, 31: 0.013605442176870748, 32: 0.4483985765124555, 33: 0.35294117647058826, 34: 0.15217391304347827, 35: 0.2642857142857143, 36: 0.2937062937062937, 37: 0.31683168316831684, 38: 0.11158798283261803, 39: 0.08695652173913043, 40: 0.26146788990825687}
Micro-average F1 score: 0.24737582512715073
Weighted-average F1 score: 0.22198719234767886
F1 score per class: {0: 0.29493087557603687, 1: 0.13131313131313133, 2: 0.14285714285714285, 3: 0.3767313019390582, 4: 0.9157894736842105, 5: 0.547945205479452, 6: 0.017241379310344827, 7: 0.006389776357827476, 8: 0.19047619047619047, 9: 0.704225352112676, 10: 0.16923076923076924, 11: 0.17252396166134185, 12: 0.15895953757225434, 13: 0.010362694300518135, 14: 0.027972027972027972, 15: 0.5454545454545454, 16: 0.41509433962264153, 17: 0.0, 18: 0.08835341365461848, 19: 0.3435114503816794, 20: 0.2012847965738758, 21: 0.07317073170731707, 22: 0.4659498207885305, 23: 0.6, 24: 0.04, 25: 0.3939393939393939, 26: 0.6037735849056604, 27: 0.039473684210526314, 28: 0.04590163934426229, 29: 0.6666666666666666, 30: 0.72, 31: 0.011764705882352941, 32: 0.4548736462093863, 33: 0.35294117647058826, 34: 0.1111111111111111, 35: 0.29508196721311475, 36: 0.2833333333333333, 37: 0.28865979381443296, 38: 0.1, 39: 0.06451612903225806, 40: 0.25277161862527714}
Micro-average F1 score: 0.254449420140085
Weighted-average F1 score: 0.22798581715903768
cur_acc:  ['0.6868', '0.4308', '0.4385', '0.2995', '0.5311', '0.5981', '0.2363', '0.4516']
his_acc:  ['0.6868', '0.5655', '0.5184', '0.3631', '0.3708', '0.3863', '0.3515', '0.3243']
cur_acc des:  ['0.6762', '0.4423', '0.4263', '0.2409', '0.3436', '0.4160', '0.2458', '0.3735']
his_acc des:  ['0.6762', '0.4887', '0.4463', '0.3142', '0.3101', '0.3047', '0.2533', '0.2474']
cur_acc rrf:  ['0.6780', '0.4333', '0.4365', '0.2455', '0.3644', '0.4315', '0.2684', '0.3797']
his_acc rrf:  ['0.6780', '0.4876', '0.4504', '0.3146', '0.3186', '0.3133', '0.2570', '0.2544']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 60.9869647CurrentTrain: epoch  0, batch     1 | loss: 78.8886886CurrentTrain: epoch  0, batch     2 | loss: 49.1591113CurrentTrain: epoch  0, batch     3 | loss: 77.4976466CurrentTrain: epoch  0, batch     4 | loss: 58.7234846CurrentTrain: epoch  0, batch     5 | loss: 77.2264863CurrentTrain: epoch  0, batch     6 | loss: 40.5213998CurrentTrain: epoch  0, batch     7 | loss: 113.5101320CurrentTrain: epoch  0, batch     8 | loss: 77.2350063CurrentTrain: epoch  0, batch     9 | loss: 47.7125003CurrentTrain: epoch  0, batch    10 | loss: 58.0743366CurrentTrain: epoch  0, batch    11 | loss: 47.2280885CurrentTrain: epoch  0, batch    12 | loss: 58.1650945CurrentTrain: epoch  0, batch    13 | loss: 47.7401338CurrentTrain: epoch  0, batch    14 | loss: 46.8601189CurrentTrain: epoch  0, batch    15 | loss: 112.5439664CurrentTrain: epoch  0, batch    16 | loss: 39.6637686CurrentTrain: epoch  0, batch    17 | loss: 76.4548586CurrentTrain: epoch  0, batch    18 | loss: 58.4611583CurrentTrain: epoch  0, batch    19 | loss: 76.4526925CurrentTrain: epoch  0, batch    20 | loss: 113.2456559CurrentTrain: epoch  0, batch    21 | loss: 78.0090944CurrentTrain: epoch  0, batch    22 | loss: 57.9785029CurrentTrain: epoch  0, batch    23 | loss: 39.3050708CurrentTrain: epoch  0, batch    24 | loss: 34.1999788CurrentTrain: epoch  0, batch    25 | loss: 39.3665797CurrentTrain: epoch  0, batch    26 | loss: 47.1020055CurrentTrain: epoch  0, batch    27 | loss: 46.8360256CurrentTrain: epoch  0, batch    28 | loss: 58.1555238CurrentTrain: epoch  0, batch    29 | loss: 76.6553832CurrentTrain: epoch  0, batch    30 | loss: 75.6782056CurrentTrain: epoch  0, batch    31 | loss: 113.7351848CurrentTrain: epoch  0, batch    32 | loss: 39.0124595CurrentTrain: epoch  0, batch    33 | loss: 76.0551686CurrentTrain: epoch  0, batch    34 | loss: 47.3823593CurrentTrain: epoch  0, batch    35 | loss: 113.0053781CurrentTrain: epoch  0, batch    36 | loss: 46.5483926CurrentTrain: epoch  0, batch    37 | loss: 75.9194751CurrentTrain: epoch  0, batch    38 | loss: 57.3906049CurrentTrain: epoch  0, batch    39 | loss: 56.9340469CurrentTrain: epoch  0, batch    40 | loss: 57.7350438CurrentTrain: epoch  0, batch    41 | loss: 39.3136698CurrentTrain: epoch  0, batch    42 | loss: 57.5299402CurrentTrain: epoch  0, batch    43 | loss: 57.4628702CurrentTrain: epoch  0, batch    44 | loss: 46.5391563CurrentTrain: epoch  0, batch    45 | loss: 75.1035518CurrentTrain: epoch  0, batch    46 | loss: 39.2621875CurrentTrain: epoch  0, batch    47 | loss: 57.3362044CurrentTrain: epoch  0, batch    48 | loss: 112.5752149CurrentTrain: epoch  0, batch    49 | loss: 75.2983118CurrentTrain: epoch  0, batch    50 | loss: 46.1020002CurrentTrain: epoch  0, batch    51 | loss: 46.5674977CurrentTrain: epoch  0, batch    52 | loss: 56.9141280CurrentTrain: epoch  0, batch    53 | loss: 57.7410253CurrentTrain: epoch  0, batch    54 | loss: 57.4698501CurrentTrain: epoch  0, batch    55 | loss: 57.7582603CurrentTrain: epoch  0, batch    56 | loss: 46.4142451CurrentTrain: epoch  0, batch    57 | loss: 57.9652236CurrentTrain: epoch  0, batch    58 | loss: 57.4683650CurrentTrain: epoch  0, batch    59 | loss: 46.1862021CurrentTrain: epoch  0, batch    60 | loss: 46.1539164CurrentTrain: epoch  0, batch    61 | loss: 46.2835408CurrentTrain: epoch  0, batch    62 | loss: 45.9995647CurrentTrain: epoch  0, batch    63 | loss: 74.6569790CurrentTrain: epoch  0, batch    64 | loss: 75.4843117CurrentTrain: epoch  0, batch    65 | loss: 75.4900642CurrentTrain: epoch  0, batch    66 | loss: 112.3026377CurrentTrain: epoch  0, batch    67 | loss: 45.6522238CurrentTrain: epoch  0, batch    68 | loss: 46.3688582CurrentTrain: epoch  0, batch    69 | loss: 111.2949964CurrentTrain: epoch  0, batch    70 | loss: 74.8567915CurrentTrain: epoch  0, batch    71 | loss: 38.4700069CurrentTrain: epoch  0, batch    72 | loss: 57.8990590CurrentTrain: epoch  0, batch    73 | loss: 57.2937817CurrentTrain: epoch  0, batch    74 | loss: 46.5444287CurrentTrain: epoch  0, batch    75 | loss: 75.1322115CurrentTrain: epoch  0, batch    76 | loss: 46.2082034CurrentTrain: epoch  0, batch    77 | loss: 58.3410081CurrentTrain: epoch  0, batch    78 | loss: 45.4720030CurrentTrain: epoch  0, batch    79 | loss: 74.7780314CurrentTrain: epoch  0, batch    80 | loss: 37.7712437CurrentTrain: epoch  0, batch    81 | loss: 45.7065213CurrentTrain: epoch  0, batch    82 | loss: 44.5369560CurrentTrain: epoch  0, batch    83 | loss: 45.4756649CurrentTrain: epoch  0, batch    84 | loss: 74.5248358CurrentTrain: epoch  0, batch    85 | loss: 56.9790030CurrentTrain: epoch  0, batch    86 | loss: 45.4283050CurrentTrain: epoch  0, batch    87 | loss: 44.1765929CurrentTrain: epoch  0, batch    88 | loss: 73.4579848CurrentTrain: epoch  0, batch    89 | loss: 44.5580779CurrentTrain: epoch  0, batch    90 | loss: 222.3605202CurrentTrain: epoch  0, batch    91 | loss: 55.7935443CurrentTrain: epoch  0, batch    92 | loss: 55.6415413CurrentTrain: epoch  0, batch    93 | loss: 55.0310172CurrentTrain: epoch  0, batch    94 | loss: 55.1177187CurrentTrain: epoch  0, batch    95 | loss: 75.4595899CurrentTrain: epoch  0, batch    96 | loss: 45.4764956CurrentTrain: epoch  0, batch    97 | loss: 75.4784322CurrentTrain: epoch  0, batch    98 | loss: 55.8445035CurrentTrain: epoch  0, batch    99 | loss: 55.9987120CurrentTrain: epoch  0, batch   100 | loss: 58.3836712CurrentTrain: epoch  0, batch   101 | loss: 54.6519933CurrentTrain: epoch  0, batch   102 | loss: 55.4969379CurrentTrain: epoch  0, batch   103 | loss: 73.8686918CurrentTrain: epoch  0, batch   104 | loss: 109.9541329CurrentTrain: epoch  0, batch   105 | loss: 110.5933201CurrentTrain: epoch  0, batch   106 | loss: 35.2782167CurrentTrain: epoch  0, batch   107 | loss: 56.5167675CurrentTrain: epoch  0, batch   108 | loss: 75.4396665CurrentTrain: epoch  0, batch   109 | loss: 73.1447885CurrentTrain: epoch  0, batch   110 | loss: 110.9063471CurrentTrain: epoch  0, batch   111 | loss: 53.9469729CurrentTrain: epoch  0, batch   112 | loss: 37.0423575CurrentTrain: epoch  0, batch   113 | loss: 110.8949408CurrentTrain: epoch  0, batch   114 | loss: 72.5130056CurrentTrain: epoch  0, batch   115 | loss: 42.5832375CurrentTrain: epoch  0, batch   116 | loss: 55.2442213CurrentTrain: epoch  0, batch   117 | loss: 71.1425699CurrentTrain: epoch  0, batch   118 | loss: 55.5600464CurrentTrain: epoch  0, batch   119 | loss: 45.3183615CurrentTrain: epoch  0, batch   120 | loss: 72.5013054CurrentTrain: epoch  0, batch   121 | loss: 55.3428918CurrentTrain: epoch  0, batch   122 | loss: 72.9240193CurrentTrain: epoch  0, batch   123 | loss: 55.4665054CurrentTrain: epoch  0, batch   124 | loss: 72.5782735CurrentTrain: epoch  0, batch   125 | loss: 70.3663659CurrentTrain: epoch  0, batch   126 | loss: 37.2361127CurrentTrain: epoch  0, batch   127 | loss: 54.1298333CurrentTrain: epoch  0, batch   128 | loss: 52.6552048CurrentTrain: epoch  0, batch   129 | loss: 43.7020636CurrentTrain: epoch  0, batch   130 | loss: 67.5569321CurrentTrain: epoch  0, batch   131 | loss: 36.8478779CurrentTrain: epoch  0, batch   132 | loss: 34.1791175CurrentTrain: epoch  0, batch   133 | loss: 54.1776388CurrentTrain: epoch  0, batch   134 | loss: 55.4001446CurrentTrain: epoch  0, batch   135 | loss: 74.6038958CurrentTrain: epoch  0, batch   136 | loss: 72.8737320CurrentTrain: epoch  0, batch   137 | loss: 43.4974829CurrentTrain: epoch  0, batch   138 | loss: 53.9221799CurrentTrain: epoch  0, batch   139 | loss: 41.6276634CurrentTrain: epoch  0, batch   140 | loss: 70.0773014CurrentTrain: epoch  0, batch   141 | loss: 54.5697879CurrentTrain: epoch  0, batch   142 | loss: 43.3379610CurrentTrain: epoch  0, batch   143 | loss: 43.2567927CurrentTrain: epoch  1, batch     0 | loss: 72.0380760CurrentTrain: epoch  1, batch     1 | loss: 74.0166923CurrentTrain: epoch  1, batch     2 | loss: 40.1956127CurrentTrain: epoch  1, batch     3 | loss: 34.3264025CurrentTrain: epoch  1, batch     4 | loss: 51.4084486CurrentTrain: epoch  1, batch     5 | loss: 222.5367353CurrentTrain: epoch  1, batch     6 | loss: 42.2515378CurrentTrain: epoch  1, batch     7 | loss: 72.7716568CurrentTrain: epoch  1, batch     8 | loss: 50.7281296CurrentTrain: epoch  1, batch     9 | loss: 41.4560522CurrentTrain: epoch  1, batch    10 | loss: 52.6795446CurrentTrain: epoch  1, batch    11 | loss: 109.2036813CurrentTrain: epoch  1, batch    12 | loss: 51.7413938CurrentTrain: epoch  1, batch    13 | loss: 55.0938685CurrentTrain: epoch  1, batch    14 | loss: 39.7581905CurrentTrain: epoch  1, batch    15 | loss: 45.3095960CurrentTrain: epoch  1, batch    16 | loss: 108.9274740CurrentTrain: epoch  1, batch    17 | loss: 51.8216386CurrentTrain: epoch  1, batch    18 | loss: 70.5670287CurrentTrain: epoch  1, batch    19 | loss: 54.1308085CurrentTrain: epoch  1, batch    20 | loss: 72.9562812CurrentTrain: epoch  1, batch    21 | loss: 72.2976818CurrentTrain: epoch  1, batch    22 | loss: 74.0666309CurrentTrain: epoch  1, batch    23 | loss: 31.3778384CurrentTrain: epoch  1, batch    24 | loss: 67.9342863CurrentTrain: epoch  1, batch    25 | loss: 40.5793129CurrentTrain: epoch  1, batch    26 | loss: 70.5019325CurrentTrain: epoch  1, batch    27 | loss: 52.0016599CurrentTrain: epoch  1, batch    28 | loss: 73.6098901CurrentTrain: epoch  1, batch    29 | loss: 112.1045921CurrentTrain: epoch  1, batch    30 | loss: 110.2715683CurrentTrain: epoch  1, batch    31 | loss: 72.7966019CurrentTrain: epoch  1, batch    32 | loss: 43.6681066CurrentTrain: epoch  1, batch    33 | loss: 69.7417770CurrentTrain: epoch  1, batch    34 | loss: 41.0014100CurrentTrain: epoch  1, batch    35 | loss: 51.7057758CurrentTrain: epoch  1, batch    36 | loss: 105.8873694CurrentTrain: epoch  1, batch    37 | loss: 40.9819011CurrentTrain: epoch  1, batch    38 | loss: 74.2619384CurrentTrain: epoch  1, batch    39 | loss: 33.5429521CurrentTrain: epoch  1, batch    40 | loss: 73.6944292CurrentTrain: epoch  1, batch    41 | loss: 40.5602354CurrentTrain: epoch  1, batch    42 | loss: 53.6799660CurrentTrain: epoch  1, batch    43 | loss: 52.2605973CurrentTrain: epoch  1, batch    44 | loss: 33.8988422CurrentTrain: epoch  1, batch    45 | loss: 39.1952201CurrentTrain: epoch  1, batch    46 | loss: 53.3092221CurrentTrain: epoch  1, batch    47 | loss: 35.4032614CurrentTrain: epoch  1, batch    48 | loss: 52.7425520CurrentTrain: epoch  1, batch    49 | loss: 34.2196666CurrentTrain: epoch  1, batch    50 | loss: 53.6635492CurrentTrain: epoch  1, batch    51 | loss: 33.2999964CurrentTrain: epoch  1, batch    52 | loss: 52.2174547CurrentTrain: epoch  1, batch    53 | loss: 38.8016558CurrentTrain: epoch  1, batch    54 | loss: 70.6649465CurrentTrain: epoch  1, batch    55 | loss: 71.4802245CurrentTrain: epoch  1, batch    56 | loss: 40.5980659CurrentTrain: epoch  1, batch    57 | loss: 55.4712907CurrentTrain: epoch  1, batch    58 | loss: 50.5469833CurrentTrain: epoch  1, batch    59 | loss: 71.5638212CurrentTrain: epoch  1, batch    60 | loss: 55.3018653CurrentTrain: epoch  1, batch    61 | loss: 73.9453203CurrentTrain: epoch  1, batch    62 | loss: 222.3532756CurrentTrain: epoch  1, batch    63 | loss: 41.4804686CurrentTrain: epoch  1, batch    64 | loss: 42.5879603CurrentTrain: epoch  1, batch    65 | loss: 106.6071057CurrentTrain: epoch  1, batch    66 | loss: 33.4529622CurrentTrain: epoch  1, batch    67 | loss: 42.5785384CurrentTrain: epoch  1, batch    68 | loss: 53.7989267CurrentTrain: epoch  1, batch    69 | loss: 49.8449910CurrentTrain: epoch  1, batch    70 | loss: 70.3380025CurrentTrain: epoch  1, batch    71 | loss: 113.8510539CurrentTrain: epoch  1, batch    72 | loss: 65.8500511CurrentTrain: epoch  1, batch    73 | loss: 51.3510695CurrentTrain: epoch  1, batch    74 | loss: 66.5575484CurrentTrain: epoch  1, batch    75 | loss: 68.7941028CurrentTrain: epoch  1, batch    76 | loss: 105.1682773CurrentTrain: epoch  1, batch    77 | loss: 48.4191453CurrentTrain: epoch  1, batch    78 | loss: 47.5244758CurrentTrain: epoch  1, batch    79 | loss: 68.6553852CurrentTrain: epoch  1, batch    80 | loss: 70.2416330CurrentTrain: epoch  1, batch    81 | loss: 37.6357969CurrentTrain: epoch  1, batch    82 | loss: 71.4140243CurrentTrain: epoch  1, batch    83 | loss: 70.5334466CurrentTrain: epoch  1, batch    84 | loss: 32.5211054CurrentTrain: epoch  1, batch    85 | loss: 51.2698417CurrentTrain: epoch  1, batch    86 | loss: 69.5457679CurrentTrain: epoch  1, batch    87 | loss: 110.1420798CurrentTrain: epoch  1, batch    88 | loss: 43.0804859CurrentTrain: epoch  1, batch    89 | loss: 48.9971056CurrentTrain: epoch  1, batch    90 | loss: 43.3305771CurrentTrain: epoch  1, batch    91 | loss: 32.0072720CurrentTrain: epoch  1, batch    92 | loss: 54.3099280CurrentTrain: epoch  1, batch    93 | loss: 42.6267193CurrentTrain: epoch  1, batch    94 | loss: 40.9057563CurrentTrain: epoch  1, batch    95 | loss: 40.2073993CurrentTrain: epoch  1, batch    96 | loss: 39.8468387CurrentTrain: epoch  1, batch    97 | loss: 41.1414957CurrentTrain: epoch  1, batch    98 | loss: 41.8622459CurrentTrain: epoch  1, batch    99 | loss: 72.9124600CurrentTrain: epoch  1, batch   100 | loss: 71.4382586CurrentTrain: epoch  1, batch   101 | loss: 66.4770649CurrentTrain: epoch  1, batch   102 | loss: 38.2386581CurrentTrain: epoch  1, batch   103 | loss: 52.1770004CurrentTrain: epoch  1, batch   104 | loss: 50.3629331CurrentTrain: epoch  1, batch   105 | loss: 70.6257524CurrentTrain: epoch  1, batch   106 | loss: 49.2040792CurrentTrain: epoch  1, batch   107 | loss: 28.0548190CurrentTrain: epoch  1, batch   108 | loss: 75.6193875CurrentTrain: epoch  1, batch   109 | loss: 39.5050427CurrentTrain: epoch  1, batch   110 | loss: 49.0339273CurrentTrain: epoch  1, batch   111 | loss: 55.8412270CurrentTrain: epoch  1, batch   112 | loss: 47.7356092CurrentTrain: epoch  1, batch   113 | loss: 31.4885784CurrentTrain: epoch  1, batch   114 | loss: 56.9652863CurrentTrain: epoch  1, batch   115 | loss: 34.2861395CurrentTrain: epoch  1, batch   116 | loss: 51.6934601CurrentTrain: epoch  1, batch   117 | loss: 69.9081194CurrentTrain: epoch  1, batch   118 | loss: 64.8576484CurrentTrain: epoch  1, batch   119 | loss: 53.6336839CurrentTrain: epoch  1, batch   120 | loss: 49.3742825CurrentTrain: epoch  1, batch   121 | loss: 69.0605941CurrentTrain: epoch  1, batch   122 | loss: 35.4299094CurrentTrain: epoch  1, batch   123 | loss: 110.5523147CurrentTrain: epoch  1, batch   124 | loss: 31.8008754CurrentTrain: epoch  1, batch   125 | loss: 49.7069294CurrentTrain: epoch  1, batch   126 | loss: 71.0923957CurrentTrain: epoch  1, batch   127 | loss: 108.0668692CurrentTrain: epoch  1, batch   128 | loss: 71.5495821CurrentTrain: epoch  1, batch   129 | loss: 50.1257389CurrentTrain: epoch  1, batch   130 | loss: 38.0458395CurrentTrain: epoch  1, batch   131 | loss: 39.5773630CurrentTrain: epoch  1, batch   132 | loss: 38.6179650CurrentTrain: epoch  1, batch   133 | loss: 53.6602868CurrentTrain: epoch  1, batch   134 | loss: 39.7309207CurrentTrain: epoch  1, batch   135 | loss: 39.3940869CurrentTrain: epoch  1, batch   136 | loss: 41.5290006CurrentTrain: epoch  1, batch   137 | loss: 104.4142980CurrentTrain: epoch  1, batch   138 | loss: 32.0378134CurrentTrain: epoch  1, batch   139 | loss: 41.5541368CurrentTrain: epoch  1, batch   140 | loss: 57.0498815CurrentTrain: epoch  1, batch   141 | loss: 54.7311332CurrentTrain: epoch  1, batch   142 | loss: 50.2296285CurrentTrain: epoch  1, batch   143 | loss: 36.8461320CurrentTrain: epoch  2, batch     0 | loss: 108.9836718CurrentTrain: epoch  2, batch     1 | loss: 69.5673187CurrentTrain: epoch  2, batch     2 | loss: 49.1252161CurrentTrain: epoch  2, batch     3 | loss: 50.2740206CurrentTrain: epoch  2, batch     4 | loss: 51.5714968CurrentTrain: epoch  2, batch     5 | loss: 70.2528119CurrentTrain: epoch  2, batch     6 | loss: 38.4627018CurrentTrain: epoch  2, batch     7 | loss: 67.9610573CurrentTrain: epoch  2, batch     8 | loss: 32.2453567CurrentTrain: epoch  2, batch     9 | loss: 38.9580685CurrentTrain: epoch  2, batch    10 | loss: 35.1625756CurrentTrain: epoch  2, batch    11 | loss: 39.8643544CurrentTrain: epoch  2, batch    12 | loss: 53.0888048CurrentTrain: epoch  2, batch    13 | loss: 111.7301466CurrentTrain: epoch  2, batch    14 | loss: 49.2234754CurrentTrain: epoch  2, batch    15 | loss: 105.1693518CurrentTrain: epoch  2, batch    16 | loss: 27.0471008CurrentTrain: epoch  2, batch    17 | loss: 40.8985159CurrentTrain: epoch  2, batch    18 | loss: 32.4901728CurrentTrain: epoch  2, batch    19 | loss: 67.7176267CurrentTrain: epoch  2, batch    20 | loss: 66.7949314CurrentTrain: epoch  2, batch    21 | loss: 39.8998967CurrentTrain: epoch  2, batch    22 | loss: 69.8695579CurrentTrain: epoch  2, batch    23 | loss: 51.7651330CurrentTrain: epoch  2, batch    24 | loss: 38.0500928CurrentTrain: epoch  2, batch    25 | loss: 30.2755211CurrentTrain: epoch  2, batch    26 | loss: 39.4163571CurrentTrain: epoch  2, batch    27 | loss: 38.9332917CurrentTrain: epoch  2, batch    28 | loss: 69.3364148CurrentTrain: epoch  2, batch    29 | loss: 69.3881002CurrentTrain: epoch  2, batch    30 | loss: 27.3135975CurrentTrain: epoch  2, batch    31 | loss: 31.7705706CurrentTrain: epoch  2, batch    32 | loss: 39.7655445CurrentTrain: epoch  2, batch    33 | loss: 48.3998944CurrentTrain: epoch  2, batch    34 | loss: 34.5318983CurrentTrain: epoch  2, batch    35 | loss: 108.0694643CurrentTrain: epoch  2, batch    36 | loss: 69.3528314CurrentTrain: epoch  2, batch    37 | loss: 52.4084681CurrentTrain: epoch  2, batch    38 | loss: 65.9024276CurrentTrain: epoch  2, batch    39 | loss: 38.8240973CurrentTrain: epoch  2, batch    40 | loss: 50.7572669CurrentTrain: epoch  2, batch    41 | loss: 72.1850377CurrentTrain: epoch  2, batch    42 | loss: 49.0919926CurrentTrain: epoch  2, batch    43 | loss: 48.2876344CurrentTrain: epoch  2, batch    44 | loss: 40.1909764CurrentTrain: epoch  2, batch    45 | loss: 34.9017192CurrentTrain: epoch  2, batch    46 | loss: 68.6207667CurrentTrain: epoch  2, batch    47 | loss: 73.7026364CurrentTrain: epoch  2, batch    48 | loss: 68.8622043CurrentTrain: epoch  2, batch    49 | loss: 108.7760037CurrentTrain: epoch  2, batch    50 | loss: 69.1040502CurrentTrain: epoch  2, batch    51 | loss: 47.6778303CurrentTrain: epoch  2, batch    52 | loss: 110.0231706CurrentTrain: epoch  2, batch    53 | loss: 69.9725816CurrentTrain: epoch  2, batch    54 | loss: 38.2289105CurrentTrain: epoch  2, batch    55 | loss: 49.6275297CurrentTrain: epoch  2, batch    56 | loss: 48.1195449CurrentTrain: epoch  2, batch    57 | loss: 51.0121004CurrentTrain: epoch  2, batch    58 | loss: 65.5286086CurrentTrain: epoch  2, batch    59 | loss: 69.4548648CurrentTrain: epoch  2, batch    60 | loss: 49.2398471CurrentTrain: epoch  2, batch    61 | loss: 70.6495898CurrentTrain: epoch  2, batch    62 | loss: 39.2136729CurrentTrain: epoch  2, batch    63 | loss: 52.3868680CurrentTrain: epoch  2, batch    64 | loss: 51.2549410CurrentTrain: epoch  2, batch    65 | loss: 50.3416058CurrentTrain: epoch  2, batch    66 | loss: 40.4884700CurrentTrain: epoch  2, batch    67 | loss: 70.0292220CurrentTrain: epoch  2, batch    68 | loss: 49.0562546CurrentTrain: epoch  2, batch    69 | loss: 114.1022908CurrentTrain: epoch  2, batch    70 | loss: 31.7090852CurrentTrain: epoch  2, batch    71 | loss: 71.1054980CurrentTrain: epoch  2, batch    72 | loss: 34.2736064CurrentTrain: epoch  2, batch    73 | loss: 42.0767350CurrentTrain: epoch  2, batch    74 | loss: 55.4687625CurrentTrain: epoch  2, batch    75 | loss: 37.0116452CurrentTrain: epoch  2, batch    76 | loss: 51.1094287CurrentTrain: epoch  2, batch    77 | loss: 68.9600118CurrentTrain: epoch  2, batch    78 | loss: 71.8963343CurrentTrain: epoch  2, batch    79 | loss: 68.1923052CurrentTrain: epoch  2, batch    80 | loss: 107.0011099CurrentTrain: epoch  2, batch    81 | loss: 30.6507079CurrentTrain: epoch  2, batch    82 | loss: 47.1106480CurrentTrain: epoch  2, batch    83 | loss: 28.6075005CurrentTrain: epoch  2, batch    84 | loss: 222.2807830CurrentTrain: epoch  2, batch    85 | loss: 111.0542626CurrentTrain: epoch  2, batch    86 | loss: 50.1751040CurrentTrain: epoch  2, batch    87 | loss: 41.1937044CurrentTrain: epoch  2, batch    88 | loss: 50.6418474CurrentTrain: epoch  2, batch    89 | loss: 51.2436634CurrentTrain: epoch  2, batch    90 | loss: 70.2431843CurrentTrain: epoch  2, batch    91 | loss: 48.5961330CurrentTrain: epoch  2, batch    92 | loss: 37.0527171CurrentTrain: epoch  2, batch    93 | loss: 222.3842490CurrentTrain: epoch  2, batch    94 | loss: 69.3326126CurrentTrain: epoch  2, batch    95 | loss: 65.4713752CurrentTrain: epoch  2, batch    96 | loss: 48.5522666CurrentTrain: epoch  2, batch    97 | loss: 108.1777140CurrentTrain: epoch  2, batch    98 | loss: 108.3747817CurrentTrain: epoch  2, batch    99 | loss: 108.1598398CurrentTrain: epoch  2, batch   100 | loss: 107.1442889CurrentTrain: epoch  2, batch   101 | loss: 39.5498699CurrentTrain: epoch  2, batch   102 | loss: 39.5809027CurrentTrain: epoch  2, batch   103 | loss: 35.7745319CurrentTrain: epoch  2, batch   104 | loss: 69.8151856CurrentTrain: epoch  2, batch   105 | loss: 50.3927804CurrentTrain: epoch  2, batch   106 | loss: 107.2104361CurrentTrain: epoch  2, batch   107 | loss: 39.9833049CurrentTrain: epoch  2, batch   108 | loss: 38.9585134CurrentTrain: epoch  2, batch   109 | loss: 52.0631313CurrentTrain: epoch  2, batch   110 | loss: 40.5345732CurrentTrain: epoch  2, batch   111 | loss: 48.6208420CurrentTrain: epoch  2, batch   112 | loss: 51.7886780CurrentTrain: epoch  2, batch   113 | loss: 48.6995701CurrentTrain: epoch  2, batch   114 | loss: 52.4202419CurrentTrain: epoch  2, batch   115 | loss: 108.8809010CurrentTrain: epoch  2, batch   116 | loss: 63.3425273CurrentTrain: epoch  2, batch   117 | loss: 40.5937499CurrentTrain: epoch  2, batch   118 | loss: 40.1950025CurrentTrain: epoch  2, batch   119 | loss: 38.2233158CurrentTrain: epoch  2, batch   120 | loss: 73.0808216CurrentTrain: epoch  2, batch   121 | loss: 39.0439790CurrentTrain: epoch  2, batch   122 | loss: 39.4922705CurrentTrain: epoch  2, batch   123 | loss: 51.2473162CurrentTrain: epoch  2, batch   124 | loss: 49.6280522CurrentTrain: epoch  2, batch   125 | loss: 69.2639795CurrentTrain: epoch  2, batch   126 | loss: 28.6126389CurrentTrain: epoch  2, batch   127 | loss: 50.7561301CurrentTrain: epoch  2, batch   128 | loss: 68.7295428CurrentTrain: epoch  2, batch   129 | loss: 32.0742324CurrentTrain: epoch  2, batch   130 | loss: 47.9868216CurrentTrain: epoch  2, batch   131 | loss: 37.8810868CurrentTrain: epoch  2, batch   132 | loss: 31.0992415CurrentTrain: epoch  2, batch   133 | loss: 68.2628044CurrentTrain: epoch  2, batch   134 | loss: 38.2309532CurrentTrain: epoch  2, batch   135 | loss: 49.7154622CurrentTrain: epoch  2, batch   136 | loss: 49.5033294CurrentTrain: epoch  2, batch   137 | loss: 108.5238129CurrentTrain: epoch  2, batch   138 | loss: 48.4434152CurrentTrain: epoch  2, batch   139 | loss: 49.8095442CurrentTrain: epoch  2, batch   140 | loss: 43.2630953CurrentTrain: epoch  2, batch   141 | loss: 66.8716240CurrentTrain: epoch  2, batch   142 | loss: 40.1928524CurrentTrain: epoch  2, batch   143 | loss: 36.6470801CurrentTrain: epoch  3, batch     0 | loss: 37.4363000CurrentTrain: epoch  3, batch     1 | loss: 104.2375689CurrentTrain: epoch  3, batch     2 | loss: 38.3090426CurrentTrain: epoch  3, batch     3 | loss: 51.8811584CurrentTrain: epoch  3, batch     4 | loss: 52.1894030CurrentTrain: epoch  3, batch     5 | loss: 29.4766879CurrentTrain: epoch  3, batch     6 | loss: 69.1746236CurrentTrain: epoch  3, batch     7 | loss: 45.9525512CurrentTrain: epoch  3, batch     8 | loss: 48.7067443CurrentTrain: epoch  3, batch     9 | loss: 36.9631409CurrentTrain: epoch  3, batch    10 | loss: 50.3029621CurrentTrain: epoch  3, batch    11 | loss: 69.6863234CurrentTrain: epoch  3, batch    12 | loss: 37.2123312CurrentTrain: epoch  3, batch    13 | loss: 51.2034712CurrentTrain: epoch  3, batch    14 | loss: 52.4483966CurrentTrain: epoch  3, batch    15 | loss: 50.3714982CurrentTrain: epoch  3, batch    16 | loss: 26.6527743CurrentTrain: epoch  3, batch    17 | loss: 31.4993217CurrentTrain: epoch  3, batch    18 | loss: 37.0895341CurrentTrain: epoch  3, batch    19 | loss: 48.8431308CurrentTrain: epoch  3, batch    20 | loss: 69.1806017CurrentTrain: epoch  3, batch    21 | loss: 49.6680083CurrentTrain: epoch  3, batch    22 | loss: 72.3218247CurrentTrain: epoch  3, batch    23 | loss: 52.0412488CurrentTrain: epoch  3, batch    24 | loss: 49.7569434CurrentTrain: epoch  3, batch    25 | loss: 50.4097164CurrentTrain: epoch  3, batch    26 | loss: 49.2714330CurrentTrain: epoch  3, batch    27 | loss: 49.0941329CurrentTrain: epoch  3, batch    28 | loss: 50.1708511CurrentTrain: epoch  3, batch    29 | loss: 68.3170848CurrentTrain: epoch  3, batch    30 | loss: 68.3951031CurrentTrain: epoch  3, batch    31 | loss: 50.3390740CurrentTrain: epoch  3, batch    32 | loss: 33.1784656CurrentTrain: epoch  3, batch    33 | loss: 68.3277967CurrentTrain: epoch  3, batch    34 | loss: 29.7971818CurrentTrain: epoch  3, batch    35 | loss: 49.9915999CurrentTrain: epoch  3, batch    36 | loss: 27.9936137CurrentTrain: epoch  3, batch    37 | loss: 37.0530221CurrentTrain: epoch  3, batch    38 | loss: 49.1834116CurrentTrain: epoch  3, batch    39 | loss: 48.4776237CurrentTrain: epoch  3, batch    40 | loss: 48.2833666CurrentTrain: epoch  3, batch    41 | loss: 44.6083316CurrentTrain: epoch  3, batch    42 | loss: 30.2170698CurrentTrain: epoch  3, batch    43 | loss: 38.8806818CurrentTrain: epoch  3, batch    44 | loss: 48.1661775CurrentTrain: epoch  3, batch    45 | loss: 39.0392608CurrentTrain: epoch  3, batch    46 | loss: 37.4524332CurrentTrain: epoch  3, batch    47 | loss: 48.0776624CurrentTrain: epoch  3, batch    48 | loss: 37.7017251CurrentTrain: epoch  3, batch    49 | loss: 49.6590408CurrentTrain: epoch  3, batch    50 | loss: 37.8940527CurrentTrain: epoch  3, batch    51 | loss: 48.9019784CurrentTrain: epoch  3, batch    52 | loss: 41.2474159CurrentTrain: epoch  3, batch    53 | loss: 24.7152010CurrentTrain: epoch  3, batch    54 | loss: 26.7532179CurrentTrain: epoch  3, batch    55 | loss: 32.7129321CurrentTrain: epoch  3, batch    56 | loss: 49.9734551CurrentTrain: epoch  3, batch    57 | loss: 35.5170842CurrentTrain: epoch  3, batch    58 | loss: 67.2291322CurrentTrain: epoch  3, batch    59 | loss: 29.2431739CurrentTrain: epoch  3, batch    60 | loss: 38.4476658CurrentTrain: epoch  3, batch    61 | loss: 46.7729024CurrentTrain: epoch  3, batch    62 | loss: 102.9331509CurrentTrain: epoch  3, batch    63 | loss: 48.0775973CurrentTrain: epoch  3, batch    64 | loss: 69.0155476CurrentTrain: epoch  3, batch    65 | loss: 66.7659396CurrentTrain: epoch  3, batch    66 | loss: 64.5622191CurrentTrain: epoch  3, batch    67 | loss: 51.1734336CurrentTrain: epoch  3, batch    68 | loss: 48.8314926CurrentTrain: epoch  3, batch    69 | loss: 66.2987440CurrentTrain: epoch  3, batch    70 | loss: 35.0627029CurrentTrain: epoch  3, batch    71 | loss: 71.1925948CurrentTrain: epoch  3, batch    72 | loss: 40.5437237CurrentTrain: epoch  3, batch    73 | loss: 222.2509694CurrentTrain: epoch  3, batch    74 | loss: 48.3098713CurrentTrain: epoch  3, batch    75 | loss: 72.0046521CurrentTrain: epoch  3, batch    76 | loss: 108.1964277CurrentTrain: epoch  3, batch    77 | loss: 49.8721130CurrentTrain: epoch  3, batch    78 | loss: 35.9817572CurrentTrain: epoch  3, batch    79 | loss: 47.2072893CurrentTrain: epoch  3, batch    80 | loss: 47.8985452CurrentTrain: epoch  3, batch    81 | loss: 69.1579288CurrentTrain: epoch  3, batch    82 | loss: 37.5120101CurrentTrain: epoch  3, batch    83 | loss: 36.6954951CurrentTrain: epoch  3, batch    84 | loss: 107.4298353CurrentTrain: epoch  3, batch    85 | loss: 26.9829653CurrentTrain: epoch  3, batch    86 | loss: 28.8763551CurrentTrain: epoch  3, batch    87 | loss: 67.7588201CurrentTrain: epoch  3, batch    88 | loss: 49.3830848CurrentTrain: epoch  3, batch    89 | loss: 31.4422573CurrentTrain: epoch  3, batch    90 | loss: 67.4068850CurrentTrain: epoch  3, batch    91 | loss: 46.5371375CurrentTrain: epoch  3, batch    92 | loss: 222.2366789CurrentTrain: epoch  3, batch    93 | loss: 66.2295623CurrentTrain: epoch  3, batch    94 | loss: 39.6054596CurrentTrain: epoch  3, batch    95 | loss: 48.1383643CurrentTrain: epoch  3, batch    96 | loss: 66.3063831CurrentTrain: epoch  3, batch    97 | loss: 107.0884247CurrentTrain: epoch  3, batch    98 | loss: 49.6748212CurrentTrain: epoch  3, batch    99 | loss: 48.4620971CurrentTrain: epoch  3, batch   100 | loss: 104.5417631CurrentTrain: epoch  3, batch   101 | loss: 37.1496362CurrentTrain: epoch  3, batch   102 | loss: 31.0079694CurrentTrain: epoch  3, batch   103 | loss: 47.0041472CurrentTrain: epoch  3, batch   104 | loss: 70.9646209CurrentTrain: epoch  3, batch   105 | loss: 50.3689514CurrentTrain: epoch  3, batch   106 | loss: 67.4974395CurrentTrain: epoch  3, batch   107 | loss: 66.7709133CurrentTrain: epoch  3, batch   108 | loss: 72.5121177CurrentTrain: epoch  3, batch   109 | loss: 37.3395824CurrentTrain: epoch  3, batch   110 | loss: 106.8480225CurrentTrain: epoch  3, batch   111 | loss: 33.7417992CurrentTrain: epoch  3, batch   112 | loss: 54.8735862CurrentTrain: epoch  3, batch   113 | loss: 65.4641463CurrentTrain: epoch  3, batch   114 | loss: 70.5220172CurrentTrain: epoch  3, batch   115 | loss: 38.8984795CurrentTrain: epoch  3, batch   116 | loss: 71.1847094CurrentTrain: epoch  3, batch   117 | loss: 70.3651138CurrentTrain: epoch  3, batch   118 | loss: 42.6427522CurrentTrain: epoch  3, batch   119 | loss: 69.3951844CurrentTrain: epoch  3, batch   120 | loss: 39.7264292CurrentTrain: epoch  3, batch   121 | loss: 107.1761346CurrentTrain: epoch  3, batch   122 | loss: 69.8456325CurrentTrain: epoch  3, batch   123 | loss: 68.9385663CurrentTrain: epoch  3, batch   124 | loss: 49.9957634CurrentTrain: epoch  3, batch   125 | loss: 103.5906451CurrentTrain: epoch  3, batch   126 | loss: 48.6059611CurrentTrain: epoch  3, batch   127 | loss: 39.0033448CurrentTrain: epoch  3, batch   128 | loss: 51.4643929CurrentTrain: epoch  3, batch   129 | loss: 49.1363952CurrentTrain: epoch  3, batch   130 | loss: 69.9824057CurrentTrain: epoch  3, batch   131 | loss: 51.4319122CurrentTrain: epoch  3, batch   132 | loss: 49.5044923CurrentTrain: epoch  3, batch   133 | loss: 39.8223719CurrentTrain: epoch  3, batch   134 | loss: 222.3619322CurrentTrain: epoch  3, batch   135 | loss: 41.3800414CurrentTrain: epoch  3, batch   136 | loss: 42.2077962CurrentTrain: epoch  3, batch   137 | loss: 39.4286308CurrentTrain: epoch  3, batch   138 | loss: 50.9135239CurrentTrain: epoch  3, batch   139 | loss: 39.6454624CurrentTrain: epoch  3, batch   140 | loss: 70.0850495CurrentTrain: epoch  3, batch   141 | loss: 38.5937602CurrentTrain: epoch  3, batch   142 | loss: 71.9217852CurrentTrain: epoch  3, batch   143 | loss: 53.2919196CurrentTrain: epoch  4, batch     0 | loss: 222.1715154CurrentTrain: epoch  4, batch     1 | loss: 36.6955238CurrentTrain: epoch  4, batch     2 | loss: 50.2828513CurrentTrain: epoch  4, batch     3 | loss: 38.6289490CurrentTrain: epoch  4, batch     4 | loss: 49.2893686CurrentTrain: epoch  4, batch     5 | loss: 104.8731864CurrentTrain: epoch  4, batch     6 | loss: 38.5853137CurrentTrain: epoch  4, batch     7 | loss: 66.2088308CurrentTrain: epoch  4, batch     8 | loss: 49.6805417CurrentTrain: epoch  4, batch     9 | loss: 47.9109133CurrentTrain: epoch  4, batch    10 | loss: 107.2688019CurrentTrain: epoch  4, batch    11 | loss: 107.4831111CurrentTrain: epoch  4, batch    12 | loss: 29.6847784CurrentTrain: epoch  4, batch    13 | loss: 65.1700839CurrentTrain: epoch  4, batch    14 | loss: 65.4805292CurrentTrain: epoch  4, batch    15 | loss: 46.5859998CurrentTrain: epoch  4, batch    16 | loss: 67.2798687CurrentTrain: epoch  4, batch    17 | loss: 29.6753787CurrentTrain: epoch  4, batch    18 | loss: 31.8964808CurrentTrain: epoch  4, batch    19 | loss: 47.9163150CurrentTrain: epoch  4, batch    20 | loss: 48.4621358CurrentTrain: epoch  4, batch    21 | loss: 40.2278738CurrentTrain: epoch  4, batch    22 | loss: 66.5862041CurrentTrain: epoch  4, batch    23 | loss: 27.3712777CurrentTrain: epoch  4, batch    24 | loss: 29.7756327CurrentTrain: epoch  4, batch    25 | loss: 30.6916840CurrentTrain: epoch  4, batch    26 | loss: 37.5747078CurrentTrain: epoch  4, batch    27 | loss: 35.8063353CurrentTrain: epoch  4, batch    28 | loss: 46.7931644CurrentTrain: epoch  4, batch    29 | loss: 49.2908134CurrentTrain: epoch  4, batch    30 | loss: 49.9598918CurrentTrain: epoch  4, batch    31 | loss: 50.3851064CurrentTrain: epoch  4, batch    32 | loss: 32.1833283CurrentTrain: epoch  4, batch    33 | loss: 49.1438784CurrentTrain: epoch  4, batch    34 | loss: 37.9742659CurrentTrain: epoch  4, batch    35 | loss: 107.8254602CurrentTrain: epoch  4, batch    36 | loss: 107.9615508CurrentTrain: epoch  4, batch    37 | loss: 68.4532074CurrentTrain: epoch  4, batch    38 | loss: 45.7557675CurrentTrain: epoch  4, batch    39 | loss: 62.8359023CurrentTrain: epoch  4, batch    40 | loss: 34.2740988CurrentTrain: epoch  4, batch    41 | loss: 48.0756635CurrentTrain: epoch  4, batch    42 | loss: 108.6531560CurrentTrain: epoch  4, batch    43 | loss: 38.6913658CurrentTrain: epoch  4, batch    44 | loss: 107.2455391CurrentTrain: epoch  4, batch    45 | loss: 47.8012601CurrentTrain: epoch  4, batch    46 | loss: 47.3984401CurrentTrain: epoch  4, batch    47 | loss: 109.0262993CurrentTrain: epoch  4, batch    48 | loss: 38.1531670CurrentTrain: epoch  4, batch    49 | loss: 36.5135340CurrentTrain: epoch  4, batch    50 | loss: 50.4013599CurrentTrain: epoch  4, batch    51 | loss: 38.8399921CurrentTrain: epoch  4, batch    52 | loss: 104.1457974CurrentTrain: epoch  4, batch    53 | loss: 49.2907645CurrentTrain: epoch  4, batch    54 | loss: 48.6491983CurrentTrain: epoch  4, batch    55 | loss: 30.3770397CurrentTrain: epoch  4, batch    56 | loss: 36.0027199CurrentTrain: epoch  4, batch    57 | loss: 37.5010222CurrentTrain: epoch  4, batch    58 | loss: 48.1333339CurrentTrain: epoch  4, batch    59 | loss: 36.3716535CurrentTrain: epoch  4, batch    60 | loss: 31.4317578CurrentTrain: epoch  4, batch    61 | loss: 37.9499160CurrentTrain: epoch  4, batch    62 | loss: 106.8001175CurrentTrain: epoch  4, batch    63 | loss: 51.2949736CurrentTrain: epoch  4, batch    64 | loss: 47.1114704CurrentTrain: epoch  4, batch    65 | loss: 48.3359102CurrentTrain: epoch  4, batch    66 | loss: 107.2392961CurrentTrain: epoch  4, batch    67 | loss: 69.3207440CurrentTrain: epoch  4, batch    68 | loss: 47.6497818CurrentTrain: epoch  4, batch    69 | loss: 36.3724030CurrentTrain: epoch  4, batch    70 | loss: 35.6125730CurrentTrain: epoch  4, batch    71 | loss: 107.0856994CurrentTrain: epoch  4, batch    72 | loss: 48.4912733CurrentTrain: epoch  4, batch    73 | loss: 108.2017849CurrentTrain: epoch  4, batch    74 | loss: 30.6319745CurrentTrain: epoch  4, batch    75 | loss: 49.6896832CurrentTrain: epoch  4, batch    76 | loss: 110.1227490CurrentTrain: epoch  4, batch    77 | loss: 39.4176814CurrentTrain: epoch  4, batch    78 | loss: 70.9472448CurrentTrain: epoch  4, batch    79 | loss: 68.6672337CurrentTrain: epoch  4, batch    80 | loss: 69.1894927CurrentTrain: epoch  4, batch    81 | loss: 68.4410301CurrentTrain: epoch  4, batch    82 | loss: 46.0281481CurrentTrain: epoch  4, batch    83 | loss: 50.6183217CurrentTrain: epoch  4, batch    84 | loss: 49.7812249CurrentTrain: epoch  4, batch    85 | loss: 38.7569349CurrentTrain: epoch  4, batch    86 | loss: 34.6398327CurrentTrain: epoch  4, batch    87 | loss: 48.3774131CurrentTrain: epoch  4, batch    88 | loss: 51.4756966CurrentTrain: epoch  4, batch    89 | loss: 71.1490045CurrentTrain: epoch  4, batch    90 | loss: 32.1981686CurrentTrain: epoch  4, batch    91 | loss: 48.1768860CurrentTrain: epoch  4, batch    92 | loss: 66.7871189CurrentTrain: epoch  4, batch    93 | loss: 66.4970316CurrentTrain: epoch  4, batch    94 | loss: 49.9795679CurrentTrain: epoch  4, batch    95 | loss: 48.0681455CurrentTrain: epoch  4, batch    96 | loss: 51.4052861CurrentTrain: epoch  4, batch    97 | loss: 49.6896366CurrentTrain: epoch  4, batch    98 | loss: 47.7269479CurrentTrain: epoch  4, batch    99 | loss: 67.4994517CurrentTrain: epoch  4, batch   100 | loss: 47.7798272CurrentTrain: epoch  4, batch   101 | loss: 30.6971377CurrentTrain: epoch  4, batch   102 | loss: 51.4036216CurrentTrain: epoch  4, batch   103 | loss: 25.7368970CurrentTrain: epoch  4, batch   104 | loss: 29.8451750CurrentTrain: epoch  4, batch   105 | loss: 107.1687333CurrentTrain: epoch  4, batch   106 | loss: 40.2337232CurrentTrain: epoch  4, batch   107 | loss: 38.7771472CurrentTrain: epoch  4, batch   108 | loss: 63.6827570CurrentTrain: epoch  4, batch   109 | loss: 33.6628795CurrentTrain: epoch  4, batch   110 | loss: 107.0096601CurrentTrain: epoch  4, batch   111 | loss: 66.4985513CurrentTrain: epoch  4, batch   112 | loss: 41.7929317CurrentTrain: epoch  4, batch   113 | loss: 50.9227022CurrentTrain: epoch  4, batch   114 | loss: 38.1239284CurrentTrain: epoch  4, batch   115 | loss: 103.5748009CurrentTrain: epoch  4, batch   116 | loss: 65.5593415CurrentTrain: epoch  4, batch   117 | loss: 46.1155865CurrentTrain: epoch  4, batch   118 | loss: 29.1387436CurrentTrain: epoch  4, batch   119 | loss: 107.0871379CurrentTrain: epoch  4, batch   120 | loss: 38.2874818CurrentTrain: epoch  4, batch   121 | loss: 37.7056177CurrentTrain: epoch  4, batch   122 | loss: 28.0448693CurrentTrain: epoch  4, batch   123 | loss: 64.4586184CurrentTrain: epoch  4, batch   124 | loss: 67.8101591CurrentTrain: epoch  4, batch   125 | loss: 46.3130375CurrentTrain: epoch  4, batch   126 | loss: 67.7820514CurrentTrain: epoch  4, batch   127 | loss: 49.1039002CurrentTrain: epoch  4, batch   128 | loss: 47.8988845CurrentTrain: epoch  4, batch   129 | loss: 38.1670377CurrentTrain: epoch  4, batch   130 | loss: 222.3981956CurrentTrain: epoch  4, batch   131 | loss: 39.3067165CurrentTrain: epoch  4, batch   132 | loss: 108.6574638CurrentTrain: epoch  4, batch   133 | loss: 35.6248575CurrentTrain: epoch  4, batch   134 | loss: 46.7024770CurrentTrain: epoch  4, batch   135 | loss: 49.2747817CurrentTrain: epoch  4, batch   136 | loss: 37.3783685CurrentTrain: epoch  4, batch   137 | loss: 49.3358614CurrentTrain: epoch  4, batch   138 | loss: 108.8487139CurrentTrain: epoch  4, batch   139 | loss: 36.3402327CurrentTrain: epoch  4, batch   140 | loss: 108.6405669CurrentTrain: epoch  4, batch   141 | loss: 66.4536974CurrentTrain: epoch  4, batch   142 | loss: 63.2653462CurrentTrain: epoch  4, batch   143 | loss: 26.4177011CurrentTrain: epoch  5, batch     0 | loss: 49.0487883CurrentTrain: epoch  5, batch     1 | loss: 29.9180009CurrentTrain: epoch  5, batch     2 | loss: 67.3767539CurrentTrain: epoch  5, batch     3 | loss: 36.6243526CurrentTrain: epoch  5, batch     4 | loss: 103.5761679CurrentTrain: epoch  5, batch     5 | loss: 49.7902081CurrentTrain: epoch  5, batch     6 | loss: 64.7327447CurrentTrain: epoch  5, batch     7 | loss: 49.3233542CurrentTrain: epoch  5, batch     8 | loss: 36.7223057CurrentTrain: epoch  5, batch     9 | loss: 69.0079289CurrentTrain: epoch  5, batch    10 | loss: 48.1144054CurrentTrain: epoch  5, batch    11 | loss: 49.9536139CurrentTrain: epoch  5, batch    12 | loss: 68.1434339CurrentTrain: epoch  5, batch    13 | loss: 49.6637136CurrentTrain: epoch  5, batch    14 | loss: 38.7446244CurrentTrain: epoch  5, batch    15 | loss: 37.9443627CurrentTrain: epoch  5, batch    16 | loss: 70.7546215CurrentTrain: epoch  5, batch    17 | loss: 62.5358037CurrentTrain: epoch  5, batch    18 | loss: 34.7208364CurrentTrain: epoch  5, batch    19 | loss: 66.3122598CurrentTrain: epoch  5, batch    20 | loss: 35.0870134CurrentTrain: epoch  5, batch    21 | loss: 46.3981392CurrentTrain: epoch  5, batch    22 | loss: 106.8870966CurrentTrain: epoch  5, batch    23 | loss: 46.8342679CurrentTrain: epoch  5, batch    24 | loss: 49.2572792CurrentTrain: epoch  5, batch    25 | loss: 28.8905781CurrentTrain: epoch  5, batch    26 | loss: 34.8330295CurrentTrain: epoch  5, batch    27 | loss: 48.3318719CurrentTrain: epoch  5, batch    28 | loss: 106.9530418CurrentTrain: epoch  5, batch    29 | loss: 222.2551086CurrentTrain: epoch  5, batch    30 | loss: 107.5041113CurrentTrain: epoch  5, batch    31 | loss: 31.3252055CurrentTrain: epoch  5, batch    32 | loss: 222.2158012CurrentTrain: epoch  5, batch    33 | loss: 107.6963992CurrentTrain: epoch  5, batch    34 | loss: 39.4148510CurrentTrain: epoch  5, batch    35 | loss: 66.5041789CurrentTrain: epoch  5, batch    36 | loss: 38.3882974CurrentTrain: epoch  5, batch    37 | loss: 47.8775668CurrentTrain: epoch  5, batch    38 | loss: 49.0362651CurrentTrain: epoch  5, batch    39 | loss: 29.0839196CurrentTrain: epoch  5, batch    40 | loss: 50.2917637CurrentTrain: epoch  5, batch    41 | loss: 47.8088022CurrentTrain: epoch  5, batch    42 | loss: 37.4765417CurrentTrain: epoch  5, batch    43 | loss: 222.2079800CurrentTrain: epoch  5, batch    44 | loss: 65.1255333CurrentTrain: epoch  5, batch    45 | loss: 30.2875143CurrentTrain: epoch  5, batch    46 | loss: 103.4817344CurrentTrain: epoch  5, batch    47 | loss: 106.9580179CurrentTrain: epoch  5, batch    48 | loss: 49.5352766CurrentTrain: epoch  5, batch    49 | loss: 107.1491231CurrentTrain: epoch  5, batch    50 | loss: 47.6552839CurrentTrain: epoch  5, batch    51 | loss: 68.3692241CurrentTrain: epoch  5, batch    52 | loss: 47.7083675CurrentTrain: epoch  5, batch    53 | loss: 47.8955701CurrentTrain: epoch  5, batch    54 | loss: 36.5617078CurrentTrain: epoch  5, batch    55 | loss: 103.4857655CurrentTrain: epoch  5, batch    56 | loss: 108.9893667CurrentTrain: epoch  5, batch    57 | loss: 68.4406575CurrentTrain: epoch  5, batch    58 | loss: 106.9457199CurrentTrain: epoch  5, batch    59 | loss: 106.8739470CurrentTrain: epoch  5, batch    60 | loss: 103.8528011CurrentTrain: epoch  5, batch    61 | loss: 46.5043465CurrentTrain: epoch  5, batch    62 | loss: 33.7930309CurrentTrain: epoch  5, batch    63 | loss: 66.1521825CurrentTrain: epoch  5, batch    64 | loss: 29.4480018CurrentTrain: epoch  5, batch    65 | loss: 66.8662194CurrentTrain: epoch  5, batch    66 | loss: 34.3497056CurrentTrain: epoch  5, batch    67 | loss: 38.3348579CurrentTrain: epoch  5, batch    68 | loss: 71.7084019CurrentTrain: epoch  5, batch    69 | loss: 66.0185489CurrentTrain: epoch  5, batch    70 | loss: 36.4072883CurrentTrain: epoch  5, batch    71 | loss: 49.5417724CurrentTrain: epoch  5, batch    72 | loss: 34.8656284CurrentTrain: epoch  5, batch    73 | loss: 40.1539935CurrentTrain: epoch  5, batch    74 | loss: 50.3475378CurrentTrain: epoch  5, batch    75 | loss: 38.0290944CurrentTrain: epoch  5, batch    76 | loss: 103.9999261CurrentTrain: epoch  5, batch    77 | loss: 44.9900631CurrentTrain: epoch  5, batch    78 | loss: 37.8116150CurrentTrain: epoch  5, batch    79 | loss: 107.6424546CurrentTrain: epoch  5, batch    80 | loss: 46.1565604CurrentTrain: epoch  5, batch    81 | loss: 66.4980646CurrentTrain: epoch  5, batch    82 | loss: 37.7014778CurrentTrain: epoch  5, batch    83 | loss: 36.9817397CurrentTrain: epoch  5, batch    84 | loss: 35.7571939CurrentTrain: epoch  5, batch    85 | loss: 30.5286298CurrentTrain: epoch  5, batch    86 | loss: 50.7486325CurrentTrain: epoch  5, batch    87 | loss: 35.2810258CurrentTrain: epoch  5, batch    88 | loss: 66.5256895CurrentTrain: epoch  5, batch    89 | loss: 36.5878391CurrentTrain: epoch  5, batch    90 | loss: 47.4834067CurrentTrain: epoch  5, batch    91 | loss: 46.5347020CurrentTrain: epoch  5, batch    92 | loss: 69.8102208CurrentTrain: epoch  5, batch    93 | loss: 49.3345320CurrentTrain: epoch  5, batch    94 | loss: 37.6216953CurrentTrain: epoch  5, batch    95 | loss: 100.0329385CurrentTrain: epoch  5, batch    96 | loss: 36.7195711CurrentTrain: epoch  5, batch    97 | loss: 44.1921900CurrentTrain: epoch  5, batch    98 | loss: 46.9167037CurrentTrain: epoch  5, batch    99 | loss: 48.9982955CurrentTrain: epoch  5, batch   100 | loss: 34.4061181CurrentTrain: epoch  5, batch   101 | loss: 46.3894531CurrentTrain: epoch  5, batch   102 | loss: 50.8183967CurrentTrain: epoch  5, batch   103 | loss: 66.0184748CurrentTrain: epoch  5, batch   104 | loss: 46.1816372CurrentTrain: epoch  5, batch   105 | loss: 37.1316940CurrentTrain: epoch  5, batch   106 | loss: 46.9590453CurrentTrain: epoch  5, batch   107 | loss: 47.7170219CurrentTrain: epoch  5, batch   108 | loss: 66.2588102CurrentTrain: epoch  5, batch   109 | loss: 69.0872785CurrentTrain: epoch  5, batch   110 | loss: 49.3274995CurrentTrain: epoch  5, batch   111 | loss: 36.8330934CurrentTrain: epoch  5, batch   112 | loss: 49.6114115CurrentTrain: epoch  5, batch   113 | loss: 47.6260910CurrentTrain: epoch  5, batch   114 | loss: 67.4557820CurrentTrain: epoch  5, batch   115 | loss: 65.3756427CurrentTrain: epoch  5, batch   116 | loss: 106.9833482CurrentTrain: epoch  5, batch   117 | loss: 34.3643126CurrentTrain: epoch  5, batch   118 | loss: 48.4616391CurrentTrain: epoch  5, batch   119 | loss: 38.3684914CurrentTrain: epoch  5, batch   120 | loss: 37.3172815CurrentTrain: epoch  5, batch   121 | loss: 36.5162661CurrentTrain: epoch  5, batch   122 | loss: 47.6082765CurrentTrain: epoch  5, batch   123 | loss: 37.0351643CurrentTrain: epoch  5, batch   124 | loss: 222.2879725CurrentTrain: epoch  5, batch   125 | loss: 37.2487584CurrentTrain: epoch  5, batch   126 | loss: 35.9505243CurrentTrain: epoch  5, batch   127 | loss: 36.9428922CurrentTrain: epoch  5, batch   128 | loss: 38.6632090CurrentTrain: epoch  5, batch   129 | loss: 47.2226333CurrentTrain: epoch  5, batch   130 | loss: 222.1792918CurrentTrain: epoch  5, batch   131 | loss: 47.5996204CurrentTrain: epoch  5, batch   132 | loss: 49.0021353CurrentTrain: epoch  5, batch   133 | loss: 47.4749004CurrentTrain: epoch  5, batch   134 | loss: 35.0189210CurrentTrain: epoch  5, batch   135 | loss: 48.1999456CurrentTrain: epoch  5, batch   136 | loss: 68.5439662CurrentTrain: epoch  5, batch   137 | loss: 29.6719594CurrentTrain: epoch  5, batch   138 | loss: 37.2788726CurrentTrain: epoch  5, batch   139 | loss: 62.2640303CurrentTrain: epoch  5, batch   140 | loss: 51.6379884CurrentTrain: epoch  5, batch   141 | loss: 50.6443594CurrentTrain: epoch  5, batch   142 | loss: 36.9669390CurrentTrain: epoch  5, batch   143 | loss: 35.9315913CurrentTrain: epoch  6, batch     0 | loss: 36.7485868CurrentTrain: epoch  6, batch     1 | loss: 67.1438693CurrentTrain: epoch  6, batch     2 | loss: 51.3386083CurrentTrain: epoch  6, batch     3 | loss: 68.6138199CurrentTrain: epoch  6, batch     4 | loss: 36.4971680CurrentTrain: epoch  6, batch     5 | loss: 64.8236943CurrentTrain: epoch  6, batch     6 | loss: 48.9601493CurrentTrain: epoch  6, batch     7 | loss: 45.7053319CurrentTrain: epoch  6, batch     8 | loss: 26.4709306CurrentTrain: epoch  6, batch     9 | loss: 29.3342703CurrentTrain: epoch  6, batch    10 | loss: 37.4531717CurrentTrain: epoch  6, batch    11 | loss: 68.7627613CurrentTrain: epoch  6, batch    12 | loss: 49.9205539CurrentTrain: epoch  6, batch    13 | loss: 48.1095518CurrentTrain: epoch  6, batch    14 | loss: 29.3993596CurrentTrain: epoch  6, batch    15 | loss: 66.2168769CurrentTrain: epoch  6, batch    16 | loss: 107.2307620CurrentTrain: epoch  6, batch    17 | loss: 50.0125655CurrentTrain: epoch  6, batch    18 | loss: 69.4725384CurrentTrain: epoch  6, batch    19 | loss: 28.7987610CurrentTrain: epoch  6, batch    20 | loss: 34.6185316CurrentTrain: epoch  6, batch    21 | loss: 49.1280260CurrentTrain: epoch  6, batch    22 | loss: 66.1462912CurrentTrain: epoch  6, batch    23 | loss: 47.5927704CurrentTrain: epoch  6, batch    24 | loss: 36.6091808CurrentTrain: epoch  6, batch    25 | loss: 35.6547059CurrentTrain: epoch  6, batch    26 | loss: 32.9226229CurrentTrain: epoch  6, batch    27 | loss: 33.7374768CurrentTrain: epoch  6, batch    28 | loss: 36.5562706CurrentTrain: epoch  6, batch    29 | loss: 28.1903228CurrentTrain: epoch  6, batch    30 | loss: 38.0293879CurrentTrain: epoch  6, batch    31 | loss: 47.8339857CurrentTrain: epoch  6, batch    32 | loss: 38.5420870CurrentTrain: epoch  6, batch    33 | loss: 36.3849796CurrentTrain: epoch  6, batch    34 | loss: 68.2647923CurrentTrain: epoch  6, batch    35 | loss: 68.4199581CurrentTrain: epoch  6, batch    36 | loss: 28.3288837CurrentTrain: epoch  6, batch    37 | loss: 48.9376476CurrentTrain: epoch  6, batch    38 | loss: 103.5681986CurrentTrain: epoch  6, batch    39 | loss: 48.9816771CurrentTrain: epoch  6, batch    40 | loss: 68.1957134CurrentTrain: epoch  6, batch    41 | loss: 49.2114035CurrentTrain: epoch  6, batch    42 | loss: 46.6339931CurrentTrain: epoch  6, batch    43 | loss: 34.6187832CurrentTrain: epoch  6, batch    44 | loss: 68.2195532CurrentTrain: epoch  6, batch    45 | loss: 37.9047830CurrentTrain: epoch  6, batch    46 | loss: 37.5740555CurrentTrain: epoch  6, batch    47 | loss: 46.1095453CurrentTrain: epoch  6, batch    48 | loss: 28.1095719CurrentTrain: epoch  6, batch    49 | loss: 36.7347415CurrentTrain: epoch  6, batch    50 | loss: 66.1423954CurrentTrain: epoch  6, batch    51 | loss: 34.6550437CurrentTrain: epoch  6, batch    52 | loss: 50.9510794CurrentTrain: epoch  6, batch    53 | loss: 49.2263073CurrentTrain: epoch  6, batch    54 | loss: 34.2269305CurrentTrain: epoch  6, batch    55 | loss: 108.7661427CurrentTrain: epoch  6, batch    56 | loss: 38.3338147CurrentTrain: epoch  6, batch    57 | loss: 47.5267915CurrentTrain: epoch  6, batch    58 | loss: 66.1306111CurrentTrain: epoch  6, batch    59 | loss: 50.1707207CurrentTrain: epoch  6, batch    60 | loss: 36.2623603CurrentTrain: epoch  6, batch    61 | loss: 37.1445243CurrentTrain: epoch  6, batch    62 | loss: 64.6828246CurrentTrain: epoch  6, batch    63 | loss: 36.3271710CurrentTrain: epoch  6, batch    64 | loss: 28.8672724CurrentTrain: epoch  6, batch    65 | loss: 49.0406858CurrentTrain: epoch  6, batch    66 | loss: 38.8285282CurrentTrain: epoch  6, batch    67 | loss: 66.7407083CurrentTrain: epoch  6, batch    68 | loss: 46.5486729CurrentTrain: epoch  6, batch    69 | loss: 47.8081649CurrentTrain: epoch  6, batch    70 | loss: 34.8547761CurrentTrain: epoch  6, batch    71 | loss: 68.1238920CurrentTrain: epoch  6, batch    72 | loss: 68.0918430CurrentTrain: epoch  6, batch    73 | loss: 46.2737463CurrentTrain: epoch  6, batch    74 | loss: 48.9248959CurrentTrain: epoch  6, batch    75 | loss: 35.4081528CurrentTrain: epoch  6, batch    76 | loss: 23.2558509CurrentTrain: epoch  6, batch    77 | loss: 46.6315677CurrentTrain: epoch  6, batch    78 | loss: 222.3263867CurrentTrain: epoch  6, batch    79 | loss: 49.1202684CurrentTrain: epoch  6, batch    80 | loss: 68.1474558CurrentTrain: epoch  6, batch    81 | loss: 35.3704132CurrentTrain: epoch  6, batch    82 | loss: 68.1184812CurrentTrain: epoch  6, batch    83 | loss: 36.5437514CurrentTrain: epoch  6, batch    84 | loss: 35.5838724CurrentTrain: epoch  6, batch    85 | loss: 51.2587754CurrentTrain: epoch  6, batch    86 | loss: 49.3004408CurrentTrain: epoch  6, batch    87 | loss: 48.8974936CurrentTrain: epoch  6, batch    88 | loss: 106.7918098CurrentTrain: epoch  6, batch    89 | loss: 37.4766859CurrentTrain: epoch  6, batch    90 | loss: 49.2174454CurrentTrain: epoch  6, batch    91 | loss: 68.1899727CurrentTrain: epoch  6, batch    92 | loss: 66.0927677CurrentTrain: epoch  6, batch    93 | loss: 45.6032957CurrentTrain: epoch  6, batch    94 | loss: 68.1190526CurrentTrain: epoch  6, batch    95 | loss: 37.4132087CurrentTrain: epoch  6, batch    96 | loss: 48.9756179CurrentTrain: epoch  6, batch    97 | loss: 49.2171682CurrentTrain: epoch  6, batch    98 | loss: 47.6972212CurrentTrain: epoch  6, batch    99 | loss: 34.1973639CurrentTrain: epoch  6, batch   100 | loss: 66.2647360CurrentTrain: epoch  6, batch   101 | loss: 46.4451508CurrentTrain: epoch  6, batch   102 | loss: 68.2870140CurrentTrain: epoch  6, batch   103 | loss: 66.2030635CurrentTrain: epoch  6, batch   104 | loss: 64.5394332CurrentTrain: epoch  6, batch   105 | loss: 103.5980310CurrentTrain: epoch  6, batch   106 | loss: 33.4541707CurrentTrain: epoch  6, batch   107 | loss: 47.1695316CurrentTrain: epoch  6, batch   108 | loss: 68.1553242CurrentTrain: epoch  6, batch   109 | loss: 48.9885597CurrentTrain: epoch  6, batch   110 | loss: 28.3332217CurrentTrain: epoch  6, batch   111 | loss: 68.4948033CurrentTrain: epoch  6, batch   112 | loss: 68.0741140CurrentTrain: epoch  6, batch   113 | loss: 47.4807076CurrentTrain: epoch  6, batch   114 | loss: 37.4707948CurrentTrain: epoch  6, batch   115 | loss: 35.2572693CurrentTrain: epoch  6, batch   116 | loss: 37.5907060CurrentTrain: epoch  6, batch   117 | loss: 66.2586814CurrentTrain: epoch  6, batch   118 | loss: 66.1677671CurrentTrain: epoch  6, batch   119 | loss: 49.0878815CurrentTrain: epoch  6, batch   120 | loss: 68.1735069CurrentTrain: epoch  6, batch   121 | loss: 49.4552911CurrentTrain: epoch  6, batch   122 | loss: 52.1823266CurrentTrain: epoch  6, batch   123 | loss: 115.0685207CurrentTrain: epoch  6, batch   124 | loss: 47.6829968CurrentTrain: epoch  6, batch   125 | loss: 36.8576039CurrentTrain: epoch  6, batch   126 | loss: 49.1735592CurrentTrain: epoch  6, batch   127 | loss: 37.4309580CurrentTrain: epoch  6, batch   128 | loss: 69.8616059CurrentTrain: epoch  6, batch   129 | loss: 68.5903145CurrentTrain: epoch  6, batch   130 | loss: 103.4806897CurrentTrain: epoch  6, batch   131 | loss: 37.3243986CurrentTrain: epoch  6, batch   132 | loss: 44.9441240CurrentTrain: epoch  6, batch   133 | loss: 48.6071370CurrentTrain: epoch  6, batch   134 | loss: 106.9075189CurrentTrain: epoch  6, batch   135 | loss: 106.8095000CurrentTrain: epoch  6, batch   136 | loss: 68.4294538CurrentTrain: epoch  6, batch   137 | loss: 66.2380424CurrentTrain: epoch  6, batch   138 | loss: 26.3923736CurrentTrain: epoch  6, batch   139 | loss: 37.6787703CurrentTrain: epoch  6, batch   140 | loss: 47.5596230CurrentTrain: epoch  6, batch   141 | loss: 30.9190597CurrentTrain: epoch  6, batch   142 | loss: 62.6516025CurrentTrain: epoch  6, batch   143 | loss: 50.9213297CurrentTrain: epoch  7, batch     0 | loss: 68.1215535CurrentTrain: epoch  7, batch     1 | loss: 47.3238055CurrentTrain: epoch  7, batch     2 | loss: 103.6204881CurrentTrain: epoch  7, batch     3 | loss: 47.4977447CurrentTrain: epoch  7, batch     4 | loss: 68.0999365CurrentTrain: epoch  7, batch     5 | loss: 66.7954456CurrentTrain: epoch  7, batch     6 | loss: 49.2217574CurrentTrain: epoch  7, batch     7 | loss: 68.1535293CurrentTrain: epoch  7, batch     8 | loss: 37.4716150CurrentTrain: epoch  7, batch     9 | loss: 103.5861455CurrentTrain: epoch  7, batch    10 | loss: 30.0862593CurrentTrain: epoch  7, batch    11 | loss: 46.2800882CurrentTrain: epoch  7, batch    12 | loss: 25.9701880CurrentTrain: epoch  7, batch    13 | loss: 47.5715348CurrentTrain: epoch  7, batch    14 | loss: 64.9284913CurrentTrain: epoch  7, batch    15 | loss: 66.3571828CurrentTrain: epoch  7, batch    16 | loss: 29.3465373CurrentTrain: epoch  7, batch    17 | loss: 36.5498735CurrentTrain: epoch  7, batch    18 | loss: 37.4823271CurrentTrain: epoch  7, batch    19 | loss: 68.2786432CurrentTrain: epoch  7, batch    20 | loss: 106.7013210CurrentTrain: epoch  7, batch    21 | loss: 68.8109662CurrentTrain: epoch  7, batch    22 | loss: 34.2792407CurrentTrain: epoch  7, batch    23 | loss: 23.3657022CurrentTrain: epoch  7, batch    24 | loss: 46.2578342CurrentTrain: epoch  7, batch    25 | loss: 36.2528461CurrentTrain: epoch  7, batch    26 | loss: 29.7950389CurrentTrain: epoch  7, batch    27 | loss: 46.1356009CurrentTrain: epoch  7, batch    28 | loss: 68.1087328CurrentTrain: epoch  7, batch    29 | loss: 48.9766935CurrentTrain: epoch  7, batch    30 | loss: 66.0850301CurrentTrain: epoch  7, batch    31 | loss: 26.7447768CurrentTrain: epoch  7, batch    32 | loss: 68.1021188CurrentTrain: epoch  7, batch    33 | loss: 47.5537896CurrentTrain: epoch  7, batch    34 | loss: 68.1703023CurrentTrain: epoch  7, batch    35 | loss: 49.1155555CurrentTrain: epoch  7, batch    36 | loss: 47.4536887CurrentTrain: epoch  7, batch    37 | loss: 64.7550685CurrentTrain: epoch  7, batch    38 | loss: 37.9328273CurrentTrain: epoch  7, batch    39 | loss: 46.2603667CurrentTrain: epoch  7, batch    40 | loss: 37.9405138CurrentTrain: epoch  7, batch    41 | loss: 35.9416964CurrentTrain: epoch  7, batch    42 | loss: 35.3572140CurrentTrain: epoch  7, batch    43 | loss: 36.5229743CurrentTrain: epoch  7, batch    44 | loss: 35.1282981CurrentTrain: epoch  7, batch    45 | loss: 108.2859760CurrentTrain: epoch  7, batch    46 | loss: 66.1638775CurrentTrain: epoch  7, batch    47 | loss: 36.0445025CurrentTrain: epoch  7, batch    48 | loss: 47.7251959CurrentTrain: epoch  7, batch    49 | loss: 38.2776330CurrentTrain: epoch  7, batch    50 | loss: 103.7532129CurrentTrain: epoch  7, batch    51 | loss: 68.0698562CurrentTrain: epoch  7, batch    52 | loss: 106.8337946CurrentTrain: epoch  7, batch    53 | loss: 49.1787511CurrentTrain: epoch  7, batch    54 | loss: 27.9583424CurrentTrain: epoch  7, batch    55 | loss: 38.8965645CurrentTrain: epoch  7, batch    56 | loss: 68.1071334CurrentTrain: epoch  7, batch    57 | loss: 33.2394638CurrentTrain: epoch  7, batch    58 | loss: 37.9790905CurrentTrain: epoch  7, batch    59 | loss: 48.9254446CurrentTrain: epoch  7, batch    60 | loss: 37.5039035CurrentTrain: epoch  7, batch    61 | loss: 68.0984401CurrentTrain: epoch  7, batch    62 | loss: 36.2570745CurrentTrain: epoch  7, batch    63 | loss: 37.4305356CurrentTrain: epoch  7, batch    64 | loss: 65.3762868CurrentTrain: epoch  7, batch    65 | loss: 50.3554017CurrentTrain: epoch  7, batch    66 | loss: 47.0204323CurrentTrain: epoch  7, batch    67 | loss: 46.7914671CurrentTrain: epoch  7, batch    68 | loss: 47.6032817CurrentTrain: epoch  7, batch    69 | loss: 36.8870767CurrentTrain: epoch  7, batch    70 | loss: 64.4204747CurrentTrain: epoch  7, batch    71 | loss: 35.5519329CurrentTrain: epoch  7, batch    72 | loss: 30.6766358CurrentTrain: epoch  7, batch    73 | loss: 68.8596610CurrentTrain: epoch  7, batch    74 | loss: 103.5309413CurrentTrain: epoch  7, batch    75 | loss: 29.0587624CurrentTrain: epoch  7, batch    76 | loss: 47.6660425CurrentTrain: epoch  7, batch    77 | loss: 68.1002414CurrentTrain: epoch  7, batch    78 | loss: 30.7908412CurrentTrain: epoch  7, batch    79 | loss: 66.2885852CurrentTrain: epoch  7, batch    80 | loss: 21.4017927CurrentTrain: epoch  7, batch    81 | loss: 47.8004899CurrentTrain: epoch  7, batch    82 | loss: 36.8609419CurrentTrain: epoch  7, batch    83 | loss: 47.4881286CurrentTrain: epoch  7, batch    84 | loss: 106.8523276CurrentTrain: epoch  7, batch    85 | loss: 31.0027089CurrentTrain: epoch  7, batch    86 | loss: 50.0559512CurrentTrain: epoch  7, batch    87 | loss: 35.1451746CurrentTrain: epoch  7, batch    88 | loss: 103.6791214CurrentTrain: epoch  7, batch    89 | loss: 64.5807241CurrentTrain: epoch  7, batch    90 | loss: 66.9181636CurrentTrain: epoch  7, batch    91 | loss: 47.3500630CurrentTrain: epoch  7, batch    92 | loss: 37.7891146CurrentTrain: epoch  7, batch    93 | loss: 49.2779042CurrentTrain: epoch  7, batch    94 | loss: 29.9732193CurrentTrain: epoch  7, batch    95 | loss: 66.1006397CurrentTrain: epoch  7, batch    96 | loss: 66.0924805CurrentTrain: epoch  7, batch    97 | loss: 31.4291049CurrentTrain: epoch  7, batch    98 | loss: 30.7410577CurrentTrain: epoch  7, batch    99 | loss: 64.2008332CurrentTrain: epoch  7, batch   100 | loss: 68.5659557CurrentTrain: epoch  7, batch   101 | loss: 222.2647289CurrentTrain: epoch  7, batch   102 | loss: 103.5226771CurrentTrain: epoch  7, batch   103 | loss: 28.9371576CurrentTrain: epoch  7, batch   104 | loss: 36.7760235CurrentTrain: epoch  7, batch   105 | loss: 46.1021725CurrentTrain: epoch  7, batch   106 | loss: 37.8340538CurrentTrain: epoch  7, batch   107 | loss: 38.0274542CurrentTrain: epoch  7, batch   108 | loss: 48.9529810CurrentTrain: epoch  7, batch   109 | loss: 39.1988560CurrentTrain: epoch  7, batch   110 | loss: 64.5513581CurrentTrain: epoch  7, batch   111 | loss: 68.1922760CurrentTrain: epoch  7, batch   112 | loss: 68.6951299CurrentTrain: epoch  7, batch   113 | loss: 36.5187238CurrentTrain: epoch  7, batch   114 | loss: 66.5706495CurrentTrain: epoch  7, batch   115 | loss: 44.9170383CurrentTrain: epoch  7, batch   116 | loss: 35.5313007CurrentTrain: epoch  7, batch   117 | loss: 37.8354435CurrentTrain: epoch  7, batch   118 | loss: 64.6820709CurrentTrain: epoch  7, batch   119 | loss: 61.4084459CurrentTrain: epoch  7, batch   120 | loss: 68.0857783CurrentTrain: epoch  7, batch   121 | loss: 46.9499056CurrentTrain: epoch  7, batch   122 | loss: 49.0063981CurrentTrain: epoch  7, batch   123 | loss: 68.0874358CurrentTrain: epoch  7, batch   124 | loss: 68.5077979CurrentTrain: epoch  7, batch   125 | loss: 48.8998269CurrentTrain: epoch  7, batch   126 | loss: 39.5418187CurrentTrain: epoch  7, batch   127 | loss: 47.4920361CurrentTrain: epoch  7, batch   128 | loss: 36.4301608CurrentTrain: epoch  7, batch   129 | loss: 36.3068516CurrentTrain: epoch  7, batch   130 | loss: 68.9669474CurrentTrain: epoch  7, batch   131 | loss: 48.9126529CurrentTrain: epoch  7, batch   132 | loss: 30.1326602CurrentTrain: epoch  7, batch   133 | loss: 48.9122247CurrentTrain: epoch  7, batch   134 | loss: 36.6960172CurrentTrain: epoch  7, batch   135 | loss: 30.5135014CurrentTrain: epoch  7, batch   136 | loss: 49.1781103CurrentTrain: epoch  7, batch   137 | loss: 46.2905448CurrentTrain: epoch  7, batch   138 | loss: 48.1780362CurrentTrain: epoch  7, batch   139 | loss: 27.9471966CurrentTrain: epoch  7, batch   140 | loss: 68.0653431CurrentTrain: epoch  7, batch   141 | loss: 36.8985953CurrentTrain: epoch  7, batch   142 | loss: 30.7490314CurrentTrain: epoch  7, batch   143 | loss: 80.8518504CurrentTrain: epoch  8, batch     0 | loss: 47.4997820CurrentTrain: epoch  8, batch     1 | loss: 29.6323784CurrentTrain: epoch  8, batch     2 | loss: 47.6196646CurrentTrain: epoch  8, batch     3 | loss: 35.2147513CurrentTrain: epoch  8, batch     4 | loss: 106.7715012CurrentTrain: epoch  8, batch     5 | loss: 66.1684925CurrentTrain: epoch  8, batch     6 | loss: 101.2235144CurrentTrain: epoch  8, batch     7 | loss: 48.0954393CurrentTrain: epoch  8, batch     8 | loss: 66.0694546CurrentTrain: epoch  8, batch     9 | loss: 35.1255322CurrentTrain: epoch  8, batch    10 | loss: 68.1047727CurrentTrain: epoch  8, batch    11 | loss: 47.4192555CurrentTrain: epoch  8, batch    12 | loss: 68.0900899CurrentTrain: epoch  8, batch    13 | loss: 28.4384727CurrentTrain: epoch  8, batch    14 | loss: 106.8059161CurrentTrain: epoch  8, batch    15 | loss: 68.1564077CurrentTrain: epoch  8, batch    16 | loss: 46.7472533CurrentTrain: epoch  8, batch    17 | loss: 46.6431132CurrentTrain: epoch  8, batch    18 | loss: 35.2604770CurrentTrain: epoch  8, batch    19 | loss: 37.4744248CurrentTrain: epoch  8, batch    20 | loss: 37.5189810CurrentTrain: epoch  8, batch    21 | loss: 29.1295074CurrentTrain: epoch  8, batch    22 | loss: 49.4467761CurrentTrain: epoch  8, batch    23 | loss: 49.2113841CurrentTrain: epoch  8, batch    24 | loss: 68.0467690CurrentTrain: epoch  8, batch    25 | loss: 35.5759371CurrentTrain: epoch  8, batch    26 | loss: 49.0432755CurrentTrain: epoch  8, batch    27 | loss: 37.5275304CurrentTrain: epoch  8, batch    28 | loss: 66.5475204CurrentTrain: epoch  8, batch    29 | loss: 64.6601769CurrentTrain: epoch  8, batch    30 | loss: 36.5306326CurrentTrain: epoch  8, batch    31 | loss: 62.6607374CurrentTrain: epoch  8, batch    32 | loss: 48.8916266CurrentTrain: epoch  8, batch    33 | loss: 68.0791441CurrentTrain: epoch  8, batch    34 | loss: 44.8251295CurrentTrain: epoch  8, batch    35 | loss: 66.1199699CurrentTrain: epoch  8, batch    36 | loss: 35.6483333CurrentTrain: epoch  8, batch    37 | loss: 34.7309753CurrentTrain: epoch  8, batch    38 | loss: 37.5516553CurrentTrain: epoch  8, batch    39 | loss: 66.1300923CurrentTrain: epoch  8, batch    40 | loss: 35.3619711CurrentTrain: epoch  8, batch    41 | loss: 46.0953472CurrentTrain: epoch  8, batch    42 | loss: 35.9783809CurrentTrain: epoch  8, batch    43 | loss: 106.7327732CurrentTrain: epoch  8, batch    44 | loss: 68.1432548CurrentTrain: epoch  8, batch    45 | loss: 37.4187590CurrentTrain: epoch  8, batch    46 | loss: 35.2787527CurrentTrain: epoch  8, batch    47 | loss: 66.1024327CurrentTrain: epoch  8, batch    48 | loss: 33.2278329CurrentTrain: epoch  8, batch    49 | loss: 34.3945532CurrentTrain: epoch  8, batch    50 | loss: 28.0339396CurrentTrain: epoch  8, batch    51 | loss: 34.2760635CurrentTrain: epoch  8, batch    52 | loss: 27.9875661CurrentTrain: epoch  8, batch    53 | loss: 66.1167357CurrentTrain: epoch  8, batch    54 | loss: 31.1015682CurrentTrain: epoch  8, batch    55 | loss: 35.4379443CurrentTrain: epoch  8, batch    56 | loss: 36.2071776CurrentTrain: epoch  8, batch    57 | loss: 49.5807511CurrentTrain: epoch  8, batch    58 | loss: 37.4366963CurrentTrain: epoch  8, batch    59 | loss: 36.0642591CurrentTrain: epoch  8, batch    60 | loss: 45.9813991CurrentTrain: epoch  8, batch    61 | loss: 66.0942406CurrentTrain: epoch  8, batch    62 | loss: 29.1081564CurrentTrain: epoch  8, batch    63 | loss: 47.3966916CurrentTrain: epoch  8, batch    64 | loss: 48.2146745CurrentTrain: epoch  8, batch    65 | loss: 28.1591595CurrentTrain: epoch  8, batch    66 | loss: 68.1008701CurrentTrain: epoch  8, batch    67 | loss: 66.0732568CurrentTrain: epoch  8, batch    68 | loss: 68.1100574CurrentTrain: epoch  8, batch    69 | loss: 48.9795491CurrentTrain: epoch  8, batch    70 | loss: 46.1302092CurrentTrain: epoch  8, batch    71 | loss: 106.7492013CurrentTrain: epoch  8, batch    72 | loss: 66.3149590CurrentTrain: epoch  8, batch    73 | loss: 48.9628743CurrentTrain: epoch  8, batch    74 | loss: 49.9694960CurrentTrain: epoch  8, batch    75 | loss: 36.2950986CurrentTrain: epoch  8, batch    76 | loss: 103.5080836CurrentTrain: epoch  8, batch    77 | loss: 34.6027208CurrentTrain: epoch  8, batch    78 | loss: 28.2478243CurrentTrain: epoch  8, batch    79 | loss: 26.9627421CurrentTrain: epoch  8, batch    80 | loss: 29.2275052CurrentTrain: epoch  8, batch    81 | loss: 47.4386464CurrentTrain: epoch  8, batch    82 | loss: 49.5452399CurrentTrain: epoch  8, batch    83 | loss: 64.5858723CurrentTrain: epoch  8, batch    84 | loss: 66.0963260CurrentTrain: epoch  8, batch    85 | loss: 106.6771898CurrentTrain: epoch  8, batch    86 | loss: 106.7997883CurrentTrain: epoch  8, batch    87 | loss: 49.0204109CurrentTrain: epoch  8, batch    88 | loss: 34.2478571CurrentTrain: epoch  8, batch    89 | loss: 106.7790869CurrentTrain: epoch  8, batch    90 | loss: 28.4296383CurrentTrain: epoch  8, batch    91 | loss: 47.7428912CurrentTrain: epoch  8, batch    92 | loss: 103.5304112CurrentTrain: epoch  8, batch    93 | loss: 106.7563227CurrentTrain: epoch  8, batch    94 | loss: 66.2029503CurrentTrain: epoch  8, batch    95 | loss: 35.4076240CurrentTrain: epoch  8, batch    96 | loss: 66.0874911CurrentTrain: epoch  8, batch    97 | loss: 67.0609121CurrentTrain: epoch  8, batch    98 | loss: 35.6351720CurrentTrain: epoch  8, batch    99 | loss: 36.5101145CurrentTrain: epoch  8, batch   100 | loss: 36.3363354CurrentTrain: epoch  8, batch   101 | loss: 68.1705836CurrentTrain: epoch  8, batch   102 | loss: 30.8491641CurrentTrain: epoch  8, batch   103 | loss: 38.9567333CurrentTrain: epoch  8, batch   104 | loss: 68.1264774CurrentTrain: epoch  8, batch   105 | loss: 66.6333349CurrentTrain: epoch  8, batch   106 | loss: 37.5724386CurrentTrain: epoch  8, batch   107 | loss: 47.6198128CurrentTrain: epoch  8, batch   108 | loss: 37.7808427CurrentTrain: epoch  8, batch   109 | loss: 37.6453673CurrentTrain: epoch  8, batch   110 | loss: 47.5514286CurrentTrain: epoch  8, batch   111 | loss: 45.9189298CurrentTrain: epoch  8, batch   112 | loss: 36.6261448CurrentTrain: epoch  8, batch   113 | loss: 47.7340499CurrentTrain: epoch  8, batch   114 | loss: 68.0611303CurrentTrain: epoch  8, batch   115 | loss: 103.5428930CurrentTrain: epoch  8, batch   116 | loss: 68.9943912CurrentTrain: epoch  8, batch   117 | loss: 47.4102642CurrentTrain: epoch  8, batch   118 | loss: 68.0692557CurrentTrain: epoch  8, batch   119 | loss: 35.0794202CurrentTrain: epoch  8, batch   120 | loss: 48.9596359CurrentTrain: epoch  8, batch   121 | loss: 68.0578808CurrentTrain: epoch  8, batch   122 | loss: 47.4359141CurrentTrain: epoch  8, batch   123 | loss: 45.9948926CurrentTrain: epoch  8, batch   124 | loss: 67.4417190CurrentTrain: epoch  8, batch   125 | loss: 48.5704985CurrentTrain: epoch  8, batch   126 | loss: 49.5539443CurrentTrain: epoch  8, batch   127 | loss: 68.9284053CurrentTrain: epoch  8, batch   128 | loss: 38.9711872CurrentTrain: epoch  8, batch   129 | loss: 64.5221184CurrentTrain: epoch  8, batch   130 | loss: 68.0983295CurrentTrain: epoch  8, batch   131 | loss: 108.9750311CurrentTrain: epoch  8, batch   132 | loss: 32.9937154CurrentTrain: epoch  8, batch   133 | loss: 36.8012444CurrentTrain: epoch  8, batch   134 | loss: 64.8821208CurrentTrain: epoch  8, batch   135 | loss: 29.3405187CurrentTrain: epoch  8, batch   136 | loss: 47.5215797CurrentTrain: epoch  8, batch   137 | loss: 106.6934092CurrentTrain: epoch  8, batch   138 | loss: 62.7468134CurrentTrain: epoch  8, batch   139 | loss: 68.2641628CurrentTrain: epoch  8, batch   140 | loss: 36.3814604CurrentTrain: epoch  8, batch   141 | loss: 25.6778202CurrentTrain: epoch  8, batch   142 | loss: 68.1254391CurrentTrain: epoch  8, batch   143 | loss: 48.0894967CurrentTrain: epoch  9, batch     0 | loss: 101.2820753CurrentTrain: epoch  9, batch     1 | loss: 47.5312867CurrentTrain: epoch  9, batch     2 | loss: 66.2756878CurrentTrain: epoch  9, batch     3 | loss: 36.0782151CurrentTrain: epoch  9, batch     4 | loss: 222.0615184CurrentTrain: epoch  9, batch     5 | loss: 36.1157244CurrentTrain: epoch  9, batch     6 | loss: 37.6433177CurrentTrain: epoch  9, batch     7 | loss: 49.0066222CurrentTrain: epoch  9, batch     8 | loss: 64.5427091CurrentTrain: epoch  9, batch     9 | loss: 36.9741283CurrentTrain: epoch  9, batch    10 | loss: 30.9869573CurrentTrain: epoch  9, batch    11 | loss: 68.0597968CurrentTrain: epoch  9, batch    12 | loss: 49.2836854CurrentTrain: epoch  9, batch    13 | loss: 34.3157095CurrentTrain: epoch  9, batch    14 | loss: 36.2667452CurrentTrain: epoch  9, batch    15 | loss: 48.9159754CurrentTrain: epoch  9, batch    16 | loss: 66.1499747CurrentTrain: epoch  9, batch    17 | loss: 66.1706962CurrentTrain: epoch  9, batch    18 | loss: 50.1977618CurrentTrain: epoch  9, batch    19 | loss: 49.5766579CurrentTrain: epoch  9, batch    20 | loss: 38.1491556CurrentTrain: epoch  9, batch    21 | loss: 48.9114250CurrentTrain: epoch  9, batch    22 | loss: 47.4894621CurrentTrain: epoch  9, batch    23 | loss: 47.5291082CurrentTrain: epoch  9, batch    24 | loss: 68.0988313CurrentTrain: epoch  9, batch    25 | loss: 33.7680371CurrentTrain: epoch  9, batch    26 | loss: 48.9715483CurrentTrain: epoch  9, batch    27 | loss: 66.0739682CurrentTrain: epoch  9, batch    28 | loss: 38.8680818CurrentTrain: epoch  9, batch    29 | loss: 46.4809975CurrentTrain: epoch  9, batch    30 | loss: 68.4956313CurrentTrain: epoch  9, batch    31 | loss: 68.0718870CurrentTrain: epoch  9, batch    32 | loss: 27.3440462CurrentTrain: epoch  9, batch    33 | loss: 34.2461127CurrentTrain: epoch  9, batch    34 | loss: 36.6203591CurrentTrain: epoch  9, batch    35 | loss: 46.3214120CurrentTrain: epoch  9, batch    36 | loss: 64.1611592CurrentTrain: epoch  9, batch    37 | loss: 66.0719206CurrentTrain: epoch  9, batch    38 | loss: 46.0579995CurrentTrain: epoch  9, batch    39 | loss: 46.8193678CurrentTrain: epoch  9, batch    40 | loss: 43.8355211CurrentTrain: epoch  9, batch    41 | loss: 35.6607988CurrentTrain: epoch  9, batch    42 | loss: 47.4479559CurrentTrain: epoch  9, batch    43 | loss: 66.7288683CurrentTrain: epoch  9, batch    44 | loss: 23.5167969CurrentTrain: epoch  9, batch    45 | loss: 36.5092405CurrentTrain: epoch  9, batch    46 | loss: 47.4722146CurrentTrain: epoch  9, batch    47 | loss: 37.4074096CurrentTrain: epoch  9, batch    48 | loss: 48.8982297CurrentTrain: epoch  9, batch    49 | loss: 106.7211097CurrentTrain: epoch  9, batch    50 | loss: 48.8864071CurrentTrain: epoch  9, batch    51 | loss: 62.6103145CurrentTrain: epoch  9, batch    52 | loss: 48.8673838CurrentTrain: epoch  9, batch    53 | loss: 36.7358292CurrentTrain: epoch  9, batch    54 | loss: 47.4715185CurrentTrain: epoch  9, batch    55 | loss: 48.9258181CurrentTrain: epoch  9, batch    56 | loss: 48.9218469CurrentTrain: epoch  9, batch    57 | loss: 36.1263353CurrentTrain: epoch  9, batch    58 | loss: 35.0489639CurrentTrain: epoch  9, batch    59 | loss: 66.6827983CurrentTrain: epoch  9, batch    60 | loss: 37.8269988CurrentTrain: epoch  9, batch    61 | loss: 44.5611571CurrentTrain: epoch  9, batch    62 | loss: 68.1645338CurrentTrain: epoch  9, batch    63 | loss: 45.9489588CurrentTrain: epoch  9, batch    64 | loss: 103.4938457CurrentTrain: epoch  9, batch    65 | loss: 46.3002131CurrentTrain: epoch  9, batch    66 | loss: 66.3191089CurrentTrain: epoch  9, batch    67 | loss: 36.2632256CurrentTrain: epoch  9, batch    68 | loss: 47.9158092CurrentTrain: epoch  9, batch    69 | loss: 106.9176238CurrentTrain: epoch  9, batch    70 | loss: 48.9319968CurrentTrain: epoch  9, batch    71 | loss: 47.4366556CurrentTrain: epoch  9, batch    72 | loss: 48.9487214CurrentTrain: epoch  9, batch    73 | loss: 48.9784558CurrentTrain: epoch  9, batch    74 | loss: 46.0816109CurrentTrain: epoch  9, batch    75 | loss: 47.4556929CurrentTrain: epoch  9, batch    76 | loss: 46.2882270CurrentTrain: epoch  9, batch    77 | loss: 29.9333864CurrentTrain: epoch  9, batch    78 | loss: 47.5412208CurrentTrain: epoch  9, batch    79 | loss: 36.2414142CurrentTrain: epoch  9, batch    80 | loss: 47.6274578CurrentTrain: epoch  9, batch    81 | loss: 68.0827041CurrentTrain: epoch  9, batch    82 | loss: 68.2186967CurrentTrain: epoch  9, batch    83 | loss: 103.4717729CurrentTrain: epoch  9, batch    84 | loss: 48.9089682CurrentTrain: epoch  9, batch    85 | loss: 64.4927051CurrentTrain: epoch  9, batch    86 | loss: 37.6008667CurrentTrain: epoch  9, batch    87 | loss: 42.7587193CurrentTrain: epoch  9, batch    88 | loss: 48.0127688CurrentTrain: epoch  9, batch    89 | loss: 47.3914903CurrentTrain: epoch  9, batch    90 | loss: 68.5281285CurrentTrain: epoch  9, batch    91 | loss: 35.4396617CurrentTrain: epoch  9, batch    92 | loss: 28.8621104CurrentTrain: epoch  9, batch    93 | loss: 48.8790205CurrentTrain: epoch  9, batch    94 | loss: 66.1736889CurrentTrain: epoch  9, batch    95 | loss: 48.9017288CurrentTrain: epoch  9, batch    96 | loss: 34.4625452CurrentTrain: epoch  9, batch    97 | loss: 31.3780276CurrentTrain: epoch  9, batch    98 | loss: 64.5866507CurrentTrain: epoch  9, batch    99 | loss: 103.4868394CurrentTrain: epoch  9, batch   100 | loss: 47.4454479CurrentTrain: epoch  9, batch   101 | loss: 47.8581714CurrentTrain: epoch  9, batch   102 | loss: 48.9856994CurrentTrain: epoch  9, batch   103 | loss: 61.2080980CurrentTrain: epoch  9, batch   104 | loss: 34.1897749CurrentTrain: epoch  9, batch   105 | loss: 36.2308964CurrentTrain: epoch  9, batch   106 | loss: 50.6180613CurrentTrain: epoch  9, batch   107 | loss: 103.5288166CurrentTrain: epoch  9, batch   108 | loss: 106.8057751CurrentTrain: epoch  9, batch   109 | loss: 48.8913944CurrentTrain: epoch  9, batch   110 | loss: 65.3837841CurrentTrain: epoch  9, batch   111 | loss: 47.5101297CurrentTrain: epoch  9, batch   112 | loss: 49.2728557CurrentTrain: epoch  9, batch   113 | loss: 37.2579252CurrentTrain: epoch  9, batch   114 | loss: 48.8850477CurrentTrain: epoch  9, batch   115 | loss: 33.7753402CurrentTrain: epoch  9, batch   116 | loss: 64.2977304CurrentTrain: epoch  9, batch   117 | loss: 49.7132719CurrentTrain: epoch  9, batch   118 | loss: 29.8004206CurrentTrain: epoch  9, batch   119 | loss: 47.6734238CurrentTrain: epoch  9, batch   120 | loss: 36.4870865CurrentTrain: epoch  9, batch   121 | loss: 28.0742883CurrentTrain: epoch  9, batch   122 | loss: 37.4501618CurrentTrain: epoch  9, batch   123 | loss: 35.3400935CurrentTrain: epoch  9, batch   124 | loss: 48.8759622CurrentTrain: epoch  9, batch   125 | loss: 48.1502389CurrentTrain: epoch  9, batch   126 | loss: 109.1582838CurrentTrain: epoch  9, batch   127 | loss: 68.1453656CurrentTrain: epoch  9, batch   128 | loss: 36.0687683CurrentTrain: epoch  9, batch   129 | loss: 47.4045180CurrentTrain: epoch  9, batch   130 | loss: 66.0971347CurrentTrain: epoch  9, batch   131 | loss: 29.1873191CurrentTrain: epoch  9, batch   132 | loss: 66.1818981CurrentTrain: epoch  9, batch   133 | loss: 36.3268023CurrentTrain: epoch  9, batch   134 | loss: 44.9052231CurrentTrain: epoch  9, batch   135 | loss: 48.7565056CurrentTrain: epoch  9, batch   136 | loss: 64.1855432CurrentTrain: epoch  9, batch   137 | loss: 47.4638154CurrentTrain: epoch  9, batch   138 | loss: 68.1479777CurrentTrain: epoch  9, batch   139 | loss: 47.7638225CurrentTrain: epoch  9, batch   140 | loss: 45.1040501CurrentTrain: epoch  9, batch   141 | loss: 47.4531395CurrentTrain: epoch  9, batch   142 | loss: 68.0961855CurrentTrain: epoch  9, batch   143 | loss: 36.1141046

F1 score per class: {32: 0.5182186234817814, 6: 0.6986899563318777, 19: 0.24242424242424243, 24: 0.7309644670050761, 26: 0.8783068783068783, 29: 0.8054298642533937}
Micro-average F1 score: 0.7025089605734767
Weighted-average F1 score: 0.6964471316027807
F1 score per class: {32: 0.49240121580547114, 6: 0.672, 19: 0.1388888888888889, 24: 0.711340206185567, 26: 0.9, 29: 0.7054263565891473}
Micro-average F1 score: 0.6446661550268611
Weighted-average F1 score: 0.6234181173783204
F1 score per class: {32: 0.49390243902439024, 6: 0.672, 19: 0.1388888888888889, 24: 0.711340206185567, 26: 0.9054726368159204, 29: 0.7193675889328063}
Micro-average F1 score: 0.6486902927580893
Weighted-average F1 score: 0.6272970678258097

F1 score per class: {32: 0.5182186234817814, 6: 0.6986899563318777, 19: 0.24242424242424243, 24: 0.7309644670050761, 26: 0.8783068783068783, 29: 0.8054298642533937}
Micro-average F1 score: 0.7025089605734767
Weighted-average F1 score: 0.6964471316027807
F1 score per class: {32: 0.49240121580547114, 6: 0.672, 19: 0.1388888888888889, 24: 0.711340206185567, 26: 0.9, 29: 0.7054263565891473}
Micro-average F1 score: 0.6446661550268611
Weighted-average F1 score: 0.6234181173783204
F1 score per class: {32: 0.49390243902439024, 6: 0.672, 19: 0.1388888888888889, 24: 0.711340206185567, 26: 0.9054726368159204, 29: 0.7193675889328063}
Micro-average F1 score: 0.6486902927580893
Weighted-average F1 score: 0.6272970678258097
cur_acc:  ['0.7025']
his_acc:  ['0.7025']
cur_acc des:  ['0.6447']
his_acc des:  ['0.6447']
cur_acc rrf:  ['0.6487']
his_acc rrf:  ['0.6487']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 54.9754299CurrentTrain: epoch  0, batch     1 | loss: 77.5019209CurrentTrain: epoch  0, batch     2 | loss: 113.7182213CurrentTrain: epoch  0, batch     3 | loss: 75.3157992CurrentTrain: epoch  0, batch     4 | loss: 79.7612769CurrentTrain: epoch  0, batch     5 | loss: 39.0437821CurrentTrain: epoch  0, batch     6 | loss: 75.8862532CurrentTrain: epoch  0, batch     7 | loss: 2.8662043CurrentTrain: epoch  1, batch     0 | loss: 54.6756007CurrentTrain: epoch  1, batch     1 | loss: 55.4993148CurrentTrain: epoch  1, batch     2 | loss: 55.1395324CurrentTrain: epoch  1, batch     3 | loss: 75.3059253CurrentTrain: epoch  1, batch     4 | loss: 53.7442099CurrentTrain: epoch  1, batch     5 | loss: 54.1235956CurrentTrain: epoch  1, batch     6 | loss: 54.5859592CurrentTrain: epoch  1, batch     7 | loss: 2.8549712CurrentTrain: epoch  2, batch     0 | loss: 71.4930772CurrentTrain: epoch  2, batch     1 | loss: 54.4114495CurrentTrain: epoch  2, batch     2 | loss: 52.3958394CurrentTrain: epoch  2, batch     3 | loss: 109.7150906CurrentTrain: epoch  2, batch     4 | loss: 53.7954519CurrentTrain: epoch  2, batch     5 | loss: 43.2714682CurrentTrain: epoch  2, batch     6 | loss: 53.7209398CurrentTrain: epoch  2, batch     7 | loss: 2.8457785CurrentTrain: epoch  3, batch     0 | loss: 49.6618650CurrentTrain: epoch  3, batch     1 | loss: 51.7868236CurrentTrain: epoch  3, batch     2 | loss: 54.5926609CurrentTrain: epoch  3, batch     3 | loss: 71.7567534CurrentTrain: epoch  3, batch     4 | loss: 32.7865407CurrentTrain: epoch  3, batch     5 | loss: 106.7540924CurrentTrain: epoch  3, batch     6 | loss: 108.0523323CurrentTrain: epoch  3, batch     7 | loss: 1.0500354CurrentTrain: epoch  4, batch     0 | loss: 38.3458753CurrentTrain: epoch  4, batch     1 | loss: 32.0908310CurrentTrain: epoch  4, batch     2 | loss: 71.0636680CurrentTrain: epoch  4, batch     3 | loss: 49.7389605CurrentTrain: epoch  4, batch     4 | loss: 108.6352462CurrentTrain: epoch  4, batch     5 | loss: 68.9992642CurrentTrain: epoch  4, batch     6 | loss: 50.5038415CurrentTrain: epoch  4, batch     7 | loss: 2.8355767CurrentTrain: epoch  5, batch     0 | loss: 70.3970951CurrentTrain: epoch  5, batch     1 | loss: 68.0371811CurrentTrain: epoch  5, batch     2 | loss: 66.9027949CurrentTrain: epoch  5, batch     3 | loss: 69.8379445CurrentTrain: epoch  5, batch     4 | loss: 30.9067433CurrentTrain: epoch  5, batch     5 | loss: 31.6796807CurrentTrain: epoch  5, batch     6 | loss: 37.9800341CurrentTrain: epoch  5, batch     7 | loss: 2.8274209CurrentTrain: epoch  6, batch     0 | loss: 66.6369711CurrentTrain: epoch  6, batch     1 | loss: 37.4639968CurrentTrain: epoch  6, batch     2 | loss: 35.6929381CurrentTrain: epoch  6, batch     3 | loss: 68.5903799CurrentTrain: epoch  6, batch     4 | loss: 49.3344094CurrentTrain: epoch  6, batch     5 | loss: 68.7068520CurrentTrain: epoch  6, batch     6 | loss: 103.9605257CurrentTrain: epoch  6, batch     7 | loss: 0.4978829CurrentTrain: epoch  7, batch     0 | loss: 65.1965409CurrentTrain: epoch  7, batch     1 | loss: 64.4677798CurrentTrain: epoch  7, batch     2 | loss: 68.1870106CurrentTrain: epoch  7, batch     3 | loss: 46.5459910CurrentTrain: epoch  7, batch     4 | loss: 68.2704646CurrentTrain: epoch  7, batch     5 | loss: 49.3336206CurrentTrain: epoch  7, batch     6 | loss: 37.7707182CurrentTrain: epoch  7, batch     7 | loss: 2.8116215CurrentTrain: epoch  8, batch     0 | loss: 47.7245132CurrentTrain: epoch  8, batch     1 | loss: 48.1090894CurrentTrain: epoch  8, batch     2 | loss: 64.9684209CurrentTrain: epoch  8, batch     3 | loss: 67.5053692CurrentTrain: epoch  8, batch     4 | loss: 106.8999646CurrentTrain: epoch  8, batch     5 | loss: 66.2065890CurrentTrain: epoch  8, batch     6 | loss: 62.7236583CurrentTrain: epoch  8, batch     7 | loss: 0.7240292CurrentTrain: epoch  9, batch     0 | loss: 49.0205729CurrentTrain: epoch  9, batch     1 | loss: 36.6716485CurrentTrain: epoch  9, batch     2 | loss: 68.5705396CurrentTrain: epoch  9, batch     3 | loss: 46.1260846CurrentTrain: epoch  9, batch     4 | loss: 49.0458309CurrentTrain: epoch  9, batch     5 | loss: 38.0868448CurrentTrain: epoch  9, batch     6 | loss: 103.5307897CurrentTrain: epoch  9, batch     7 | loss: 2.8276455
MemoryTrain:  epoch  0, batch     0 | loss: 0.3862532MemoryTrain:  epoch  1, batch     0 | loss: 0.3342105MemoryTrain:  epoch  2, batch     0 | loss: 0.2800745MemoryTrain:  epoch  3, batch     0 | loss: 0.2076510MemoryTrain:  epoch  4, batch     0 | loss: 0.1236391MemoryTrain:  epoch  5, batch     0 | loss: 0.1094217MemoryTrain:  epoch  6, batch     0 | loss: 0.1062446MemoryTrain:  epoch  7, batch     0 | loss: 0.0689397MemoryTrain:  epoch  8, batch     0 | loss: 0.0613191MemoryTrain:  epoch  9, batch     0 | loss: 0.0462457

F1 score per class: {32: 0.9591836734693877, 5: 0.0, 6: 0.4972375690607735, 10: 0.5205479452054794, 16: 0.0, 17: 0.23529411764705882, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.529886914378029
Weighted-average F1 score: 0.4607450584194703
F1 score per class: {32: 0.7348484848484849, 5: 0.0, 6: 0.5411764705882353, 10: 0.45714285714285713, 16: 0.0, 17: 0.3076923076923077, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.43343653250773995
Weighted-average F1 score: 0.38571227035703737
F1 score per class: {32: 0.7320754716981132, 5: 0.0, 6: 0.5525291828793775, 10: 0.46601941747572817, 16: 0.0, 17: 0.3076923076923077, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.4463157894736842
Weighted-average F1 score: 0.40085213150350696

F1 score per class: {32: 0.9591836734693877, 5: 0.38571428571428573, 6: 0.4368932038834951, 10: 0.48717948717948717, 16: 0.0, 17: 0.22641509433962265, 18: 0.6497890295358649, 19: 0.16, 24: 0.6951871657754011, 26: 0.847926267281106, 29: 0.712}
Micro-average F1 score: 0.6034197462768891
Weighted-average F1 score: 0.5882805636193281
F1 score per class: {32: 0.7185185185185186, 5: 0.3597560975609756, 6: 0.41818181818181815, 10: 0.3902439024390244, 16: 0.0, 17: 0.26666666666666666, 18: 0.607773851590106, 19: 0.09900990099009901, 24: 0.676923076923077, 26: 0.8173913043478261, 29: 0.5808580858085809}
Micro-average F1 score: 0.4945542557482856
Weighted-average F1 score: 0.4656994437681012
F1 score per class: {32: 0.7158671586715867, 5: 0.3582089552238806, 6: 0.4264264264264264, 10: 0.39344262295081966, 16: 0.0, 17: 0.26666666666666666, 18: 0.607773851590106, 19: 0.14285714285714285, 24: 0.676923076923077, 26: 0.8225108225108225, 29: 0.5808580858085809}
Micro-average F1 score: 0.5092860090796533
Weighted-average F1 score: 0.48551368223323027
cur_acc:  ['0.7025', '0.5299']
his_acc:  ['0.7025', '0.6034']
cur_acc des:  ['0.6447', '0.4334']
his_acc des:  ['0.6447', '0.4946']
cur_acc rrf:  ['0.6487', '0.4463']
his_acc rrf:  ['0.6487', '0.5093']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 61.6756619CurrentTrain: epoch  0, batch     1 | loss: 44.9770171CurrentTrain: epoch  0, batch     2 | loss: 52.9540457CurrentTrain: epoch  0, batch     3 | loss: 49.8515960CurrentTrain: epoch  0, batch     4 | loss: 38.6495588CurrentTrain: epoch  1, batch     0 | loss: 48.8261132CurrentTrain: epoch  1, batch     1 | loss: 41.5463509CurrentTrain: epoch  1, batch     2 | loss: 45.6066666CurrentTrain: epoch  1, batch     3 | loss: 54.5692708CurrentTrain: epoch  1, batch     4 | loss: 36.3025682CurrentTrain: epoch  2, batch     0 | loss: 39.9059958CurrentTrain: epoch  2, batch     1 | loss: 50.3658266CurrentTrain: epoch  2, batch     2 | loss: 44.3833262CurrentTrain: epoch  2, batch     3 | loss: 68.2007156CurrentTrain: epoch  2, batch     4 | loss: 26.3134892CurrentTrain: epoch  3, batch     0 | loss: 73.6352796CurrentTrain: epoch  3, batch     1 | loss: 39.2439311CurrentTrain: epoch  3, batch     2 | loss: 69.7288626CurrentTrain: epoch  3, batch     3 | loss: 30.7096238CurrentTrain: epoch  3, batch     4 | loss: 29.3311523CurrentTrain: epoch  4, batch     0 | loss: 29.6187092CurrentTrain: epoch  4, batch     1 | loss: 40.8411955CurrentTrain: epoch  4, batch     2 | loss: 39.2257092CurrentTrain: epoch  4, batch     3 | loss: 68.3347073CurrentTrain: epoch  4, batch     4 | loss: 69.7771011CurrentTrain: epoch  5, batch     0 | loss: 34.9915375CurrentTrain: epoch  5, batch     1 | loss: 47.8151390CurrentTrain: epoch  5, batch     2 | loss: 49.3166733CurrentTrain: epoch  5, batch     3 | loss: 107.1664990CurrentTrain: epoch  5, batch     4 | loss: 23.2255528CurrentTrain: epoch  6, batch     0 | loss: 38.0861999CurrentTrain: epoch  6, batch     1 | loss: 34.1363098CurrentTrain: epoch  6, batch     2 | loss: 37.5297407CurrentTrain: epoch  6, batch     3 | loss: 48.3538143CurrentTrain: epoch  6, batch     4 | loss: 43.7782483CurrentTrain: epoch  7, batch     0 | loss: 222.5522309CurrentTrain: epoch  7, batch     1 | loss: 28.6706711CurrentTrain: epoch  7, batch     2 | loss: 45.1505049CurrentTrain: epoch  7, batch     3 | loss: 48.9241262CurrentTrain: epoch  7, batch     4 | loss: 20.5882147CurrentTrain: epoch  8, batch     0 | loss: 104.1067599CurrentTrain: epoch  8, batch     1 | loss: 46.8155578CurrentTrain: epoch  8, batch     2 | loss: 35.6497276CurrentTrain: epoch  8, batch     3 | loss: 34.5278848CurrentTrain: epoch  8, batch     4 | loss: 29.1665806CurrentTrain: epoch  9, batch     0 | loss: 36.1874086CurrentTrain: epoch  9, batch     1 | loss: 27.1116644CurrentTrain: epoch  9, batch     2 | loss: 49.4248641CurrentTrain: epoch  9, batch     3 | loss: 35.5499023CurrentTrain: epoch  9, batch     4 | loss: 68.5155988
MemoryTrain:  epoch  0, batch     0 | loss: 0.4378854MemoryTrain:  epoch  1, batch     0 | loss: 0.3030584MemoryTrain:  epoch  2, batch     0 | loss: 0.2340796MemoryTrain:  epoch  3, batch     0 | loss: 0.2015719MemoryTrain:  epoch  4, batch     0 | loss: 0.1514239MemoryTrain:  epoch  5, batch     0 | loss: 0.1396773MemoryTrain:  epoch  6, batch     0 | loss: 0.1031057MemoryTrain:  epoch  7, batch     0 | loss: 0.0958139MemoryTrain:  epoch  8, batch     0 | loss: 0.0738371MemoryTrain:  epoch  9, batch     0 | loss: 0.0592583

F1 score per class: {32: 0.0, 6: 0.3333333333333333, 7: 0.92, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 19: 0.0, 24: 0.27586206896551724, 26: 0.0, 27: 0.0, 31: 0.2631578947368421}
Micro-average F1 score: 0.3018867924528302
Weighted-average F1 score: 0.23986465487586442
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.4444444444444444, 7: 0.847457627118644, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.45714285714285713, 26: 0.18181818181818182, 27: 0.0, 31: 0.34951456310679613}
Micro-average F1 score: 0.3501199040767386
Weighted-average F1 score: 0.3025425599832917
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.4444444444444444, 7: 0.8928571428571429, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.4, 26: 0.2222222222222222, 27: 0.0, 31: 0.35294117647058826}
Micro-average F1 score: 0.3640897755610973
Weighted-average F1 score: 0.31679110668657706

F1 score per class: {32: 0.9447236180904522, 5: 0.35294117647058826, 6: 0.029411764705882353, 7: 0.92, 40: 0.4195121951219512, 10: 0.5, 9: 0.0, 16: 0.2535211267605634, 17: 0.5475285171102662, 18: 0.125, 19: 0.6885245901639344, 24: 0.13559322033898305, 26: 0.8557692307692307, 27: 0.0, 29: 0.7713004484304933, 31: 0.16877637130801687}
Micro-average F1 score: 0.532449076267172
Weighted-average F1 score: 0.5004432059495313
F1 score per class: {32: 0.5351351351351351, 5: 0.30708661417322836, 6: 0.02702702702702703, 7: 0.819672131147541, 40: 0.4067796610169492, 10: 0.3898305084745763, 9: 0.0, 16: 0.24175824175824176, 17: 0.5867768595041323, 18: 0.14457831325301204, 19: 0.6868686868686869, 24: 0.16161616161616163, 26: 0.8310502283105022, 27: 0.05, 29: 0.5894736842105263, 31: 0.20512820512820512}
Micro-average F1 score: 0.42414138046015337
Weighted-average F1 score: 0.39064763015090304
F1 score per class: {32: 0.616822429906542, 5: 0.3333333333333333, 6: 0.03333333333333333, 7: 0.8771929824561403, 40: 0.3973509933774834, 10: 0.45454545454545453, 9: 0.0, 16: 0.2222222222222222, 17: 0.592274678111588, 18: 0.19047619047619047, 19: 0.6938775510204082, 24: 0.12903225806451613, 26: 0.8272727272727273, 27: 0.07272727272727272, 29: 0.6036363636363636, 31: 0.1935483870967742}
Micro-average F1 score: 0.4478903714388749
Weighted-average F1 score: 0.4141009043504546
cur_acc:  ['0.7025', '0.5299', '0.3019']
his_acc:  ['0.7025', '0.6034', '0.5324']
cur_acc des:  ['0.6447', '0.4334', '0.3501']
his_acc des:  ['0.6447', '0.4946', '0.4241']
cur_acc rrf:  ['0.6487', '0.4463', '0.3641']
his_acc rrf:  ['0.6487', '0.5093', '0.4479']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 86.5603121CurrentTrain: epoch  0, batch     1 | loss: 81.6445285CurrentTrain: epoch  0, batch     2 | loss: 54.8432078CurrentTrain: epoch  0, batch     3 | loss: 63.1154481CurrentTrain: epoch  0, batch     4 | loss: 52.9877091CurrentTrain: epoch  0, batch     5 | loss: 54.1224611CurrentTrain: epoch  1, batch     0 | loss: 55.2136062CurrentTrain: epoch  1, batch     1 | loss: 71.5130575CurrentTrain: epoch  1, batch     2 | loss: 52.2605028CurrentTrain: epoch  1, batch     3 | loss: 38.4912383CurrentTrain: epoch  1, batch     4 | loss: 77.0578410CurrentTrain: epoch  1, batch     5 | loss: 25.8634315CurrentTrain: epoch  2, batch     0 | loss: 30.6464916CurrentTrain: epoch  2, batch     1 | loss: 222.8252787CurrentTrain: epoch  2, batch     2 | loss: 76.1758680CurrentTrain: epoch  2, batch     3 | loss: 55.3361541CurrentTrain: epoch  2, batch     4 | loss: 38.9983131CurrentTrain: epoch  2, batch     5 | loss: 47.3048062CurrentTrain: epoch  3, batch     0 | loss: 52.4271877CurrentTrain: epoch  3, batch     1 | loss: 38.6877322CurrentTrain: epoch  3, batch     2 | loss: 51.4846741CurrentTrain: epoch  3, batch     3 | loss: 32.9057694CurrentTrain: epoch  3, batch     4 | loss: 50.0860544CurrentTrain: epoch  3, batch     5 | loss: 52.5946491CurrentTrain: epoch  4, batch     0 | loss: 104.1604034CurrentTrain: epoch  4, batch     1 | loss: 68.9107322CurrentTrain: epoch  4, batch     2 | loss: 30.1614097CurrentTrain: epoch  4, batch     3 | loss: 37.0124909CurrentTrain: epoch  4, batch     4 | loss: 48.6897054CurrentTrain: epoch  4, batch     5 | loss: 84.7044492CurrentTrain: epoch  5, batch     0 | loss: 49.2731329CurrentTrain: epoch  5, batch     1 | loss: 29.2690677CurrentTrain: epoch  5, batch     2 | loss: 37.9691006CurrentTrain: epoch  5, batch     3 | loss: 70.1792745CurrentTrain: epoch  5, batch     4 | loss: 52.9444025CurrentTrain: epoch  5, batch     5 | loss: 34.8107704CurrentTrain: epoch  6, batch     0 | loss: 48.3389110CurrentTrain: epoch  6, batch     1 | loss: 49.2954051CurrentTrain: epoch  6, batch     2 | loss: 48.0166912CurrentTrain: epoch  6, batch     3 | loss: 48.0831442CurrentTrain: epoch  6, batch     4 | loss: 36.2756246CurrentTrain: epoch  6, batch     5 | loss: 79.9193698CurrentTrain: epoch  7, batch     0 | loss: 50.0452371CurrentTrain: epoch  7, batch     1 | loss: 38.3255136CurrentTrain: epoch  7, batch     2 | loss: 35.7965622CurrentTrain: epoch  7, batch     3 | loss: 38.8299187CurrentTrain: epoch  7, batch     4 | loss: 66.3695120CurrentTrain: epoch  7, batch     5 | loss: 46.4647395CurrentTrain: epoch  8, batch     0 | loss: 47.7354151CurrentTrain: epoch  8, batch     1 | loss: 49.3340564CurrentTrain: epoch  8, batch     2 | loss: 37.3608950CurrentTrain: epoch  8, batch     3 | loss: 103.6268081CurrentTrain: epoch  8, batch     4 | loss: 29.8527595CurrentTrain: epoch  8, batch     5 | loss: 34.1943843CurrentTrain: epoch  9, batch     0 | loss: 36.6262644CurrentTrain: epoch  9, batch     1 | loss: 68.1474113CurrentTrain: epoch  9, batch     2 | loss: 48.4162141CurrentTrain: epoch  9, batch     3 | loss: 36.6400449CurrentTrain: epoch  9, batch     4 | loss: 37.5361575CurrentTrain: epoch  9, batch     5 | loss: 26.5493041
MemoryTrain:  epoch  0, batch     0 | loss: 0.4217943MemoryTrain:  epoch  1, batch     0 | loss: 0.2912089MemoryTrain:  epoch  2, batch     0 | loss: 0.2319572MemoryTrain:  epoch  3, batch     0 | loss: 0.1763321MemoryTrain:  epoch  4, batch     0 | loss: 0.1546893MemoryTrain:  epoch  5, batch     0 | loss: 0.1080182MemoryTrain:  epoch  6, batch     0 | loss: 0.0927628MemoryTrain:  epoch  7, batch     0 | loss: 0.0792737MemoryTrain:  epoch  8, batch     0 | loss: 0.0583839MemoryTrain:  epoch  9, batch     0 | loss: 0.0509504

F1 score per class: {0: 0.8918918918918919, 4: 0.9361702127659575, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.11428571428571428, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.3076923076923077, 23: 0.7317073170731707, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.5913978494623656
Weighted-average F1 score: 0.4700675879025892
F1 score per class: {0: 0.5789473684210527, 4: 0.9365853658536586, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.10666666666666667, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.32558139534883723, 23: 0.7047619047619048, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.40380549682875266
Weighted-average F1 score: 0.30739703122603085
F1 score per class: {0: 0.6055045871559633, 4: 0.941747572815534, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.09876543209876543, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.3230769230769231, 23: 0.6981132075471698, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4164859002169197
Weighted-average F1 score: 0.31923225137190964

F1 score per class: {0: 0.4748201438848921, 4: 0.9214659685863874, 5: 0.852017937219731, 6: 0.3333333333333333, 7: 0.03571428571428571, 9: 0.92, 10: 0.26582278481012656, 13: 0.03278688524590164, 16: 0.4367816091954023, 17: 0.0, 18: 0.25, 19: 0.5278810408921933, 21: 0.1951219512195122, 23: 0.6818181818181818, 24: 0.0, 26: 0.6666666666666666, 27: 0.1346153846153846, 29: 0.7946428571428571, 31: 0.04, 32: 0.7123287671232876, 40: 0.21839080459770116}
Micro-average F1 score: 0.495960660344222
Weighted-average F1 score: 0.4570244067610439
F1 score per class: {0: 0.25287356321839083, 4: 0.8888888888888888, 5: 0.3256578947368421, 6: 0.25219941348973607, 7: 0.04838709677419355, 9: 0.7246376811594203, 10: 0.2898550724637681, 13: 0.02877697841726619, 16: 0.3582089552238806, 17: 0.0, 18: 0.20833333333333334, 19: 0.5066666666666667, 21: 0.14046822742474915, 23: 0.6115702479338843, 24: 0.05128205128205128, 26: 0.6598984771573604, 27: 0.13043478260869565, 29: 0.774468085106383, 31: 0.037037037037037035, 32: 0.4772727272727273, 40: 0.14395886889460155}
Micro-average F1 score: 0.34111849153631885
Weighted-average F1 score: 0.30570546214963934
F1 score per class: {0: 0.24444444444444444, 4: 0.9107981220657277, 5: 0.33389544688026984, 6: 0.2617801047120419, 7: 0.047058823529411764, 9: 0.8333333333333334, 10: 0.3, 13: 0.028368794326241134, 16: 0.375, 17: 0.0, 18: 0.23148148148148148, 19: 0.5263157894736842, 21: 0.13953488372093023, 23: 0.6115702479338843, 24: 0.05405405405405406, 26: 0.6598984771573604, 27: 0.14184397163120568, 29: 0.7829787234042553, 31: 0.044444444444444446, 32: 0.509090909090909, 40: 0.15041782729805014}
Micro-average F1 score: 0.35413450937155455
Weighted-average F1 score: 0.31711697972080855
cur_acc:  ['0.7025', '0.5299', '0.3019', '0.5914']
his_acc:  ['0.7025', '0.6034', '0.5324', '0.4960']
cur_acc des:  ['0.6447', '0.4334', '0.3501', '0.4038']
his_acc des:  ['0.6447', '0.4946', '0.4241', '0.3411']
cur_acc rrf:  ['0.6487', '0.4463', '0.3641', '0.4165']
his_acc rrf:  ['0.6487', '0.5093', '0.4479', '0.3541']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 64.1324247CurrentTrain: epoch  0, batch     1 | loss: 127.3780953CurrentTrain: epoch  0, batch     2 | loss: 46.6120171CurrentTrain: epoch  0, batch     3 | loss: 58.9437971CurrentTrain: epoch  0, batch     4 | loss: 46.7146867CurrentTrain: epoch  0, batch     5 | loss: 45.2509839CurrentTrain: epoch  1, batch     0 | loss: 57.6688788CurrentTrain: epoch  1, batch     1 | loss: 48.5025000CurrentTrain: epoch  1, batch     2 | loss: 56.2296649CurrentTrain: epoch  1, batch     3 | loss: 71.8080602CurrentTrain: epoch  1, batch     4 | loss: 73.4869676CurrentTrain: epoch  1, batch     5 | loss: 97.1246972CurrentTrain: epoch  2, batch     0 | loss: 43.3900736CurrentTrain: epoch  2, batch     1 | loss: 31.5951800CurrentTrain: epoch  2, batch     2 | loss: 51.9754351CurrentTrain: epoch  2, batch     3 | loss: 41.0808072CurrentTrain: epoch  2, batch     4 | loss: 109.2451658CurrentTrain: epoch  2, batch     5 | loss: 20.7002382CurrentTrain: epoch  3, batch     0 | loss: 31.4112113CurrentTrain: epoch  3, batch     1 | loss: 68.7097107CurrentTrain: epoch  3, batch     2 | loss: 51.7068952CurrentTrain: epoch  3, batch     3 | loss: 32.8372191CurrentTrain: epoch  3, batch     4 | loss: 51.1016606CurrentTrain: epoch  3, batch     5 | loss: 30.3283902CurrentTrain: epoch  4, batch     0 | loss: 68.8905092CurrentTrain: epoch  4, batch     1 | loss: 49.8022501CurrentTrain: epoch  4, batch     2 | loss: 39.0527637CurrentTrain: epoch  4, batch     3 | loss: 38.2011662CurrentTrain: epoch  4, batch     4 | loss: 46.4974116CurrentTrain: epoch  4, batch     5 | loss: 28.9468477CurrentTrain: epoch  5, batch     0 | loss: 67.2954281CurrentTrain: epoch  5, batch     1 | loss: 37.3882868CurrentTrain: epoch  5, batch     2 | loss: 38.2203686CurrentTrain: epoch  5, batch     3 | loss: 39.0605694CurrentTrain: epoch  5, batch     4 | loss: 48.8828539CurrentTrain: epoch  5, batch     5 | loss: 25.5716551CurrentTrain: epoch  6, batch     0 | loss: 64.5748479CurrentTrain: epoch  6, batch     1 | loss: 38.2295882CurrentTrain: epoch  6, batch     2 | loss: 46.5856191CurrentTrain: epoch  6, batch     3 | loss: 68.9493260CurrentTrain: epoch  6, batch     4 | loss: 68.8144890CurrentTrain: epoch  6, batch     5 | loss: 26.5666658CurrentTrain: epoch  7, batch     0 | loss: 45.5947056CurrentTrain: epoch  7, batch     1 | loss: 47.3105625CurrentTrain: epoch  7, batch     2 | loss: 36.2354205CurrentTrain: epoch  7, batch     3 | loss: 66.4540242CurrentTrain: epoch  7, batch     4 | loss: 36.8233751CurrentTrain: epoch  7, batch     5 | loss: 96.9362927CurrentTrain: epoch  8, batch     0 | loss: 46.2687730CurrentTrain: epoch  8, batch     1 | loss: 46.4724334CurrentTrain: epoch  8, batch     2 | loss: 49.2291805CurrentTrain: epoch  8, batch     3 | loss: 50.0083599CurrentTrain: epoch  8, batch     4 | loss: 34.2262396CurrentTrain: epoch  8, batch     5 | loss: 44.8689181CurrentTrain: epoch  9, batch     0 | loss: 38.9598407CurrentTrain: epoch  9, batch     1 | loss: 49.1586575CurrentTrain: epoch  9, batch     2 | loss: 45.4164237CurrentTrain: epoch  9, batch     3 | loss: 66.4226646CurrentTrain: epoch  9, batch     4 | loss: 34.8780583CurrentTrain: epoch  9, batch     5 | loss: 27.2198049
MemoryTrain:  epoch  0, batch     0 | loss: 0.1574775MemoryTrain:  epoch  1, batch     0 | loss: 0.1465302MemoryTrain:  epoch  2, batch     0 | loss: 0.1135360MemoryTrain:  epoch  3, batch     0 | loss: 0.0876187MemoryTrain:  epoch  4, batch     0 | loss: 0.0676121MemoryTrain:  epoch  5, batch     0 | loss: 0.0610220MemoryTrain:  epoch  6, batch     0 | loss: 0.0522290MemoryTrain:  epoch  7, batch     0 | loss: 0.0455129MemoryTrain:  epoch  8, batch     0 | loss: 0.0387566MemoryTrain:  epoch  9, batch     0 | loss: 0.0345625

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.41025641025641024, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.6764705882352942, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.9142857142857143, 31: 0.0, 32: 0.0, 33: 0.375, 36: 0.43564356435643564, 40: 0.0}
Micro-average F1 score: 0.4277456647398844
Weighted-average F1 score: 0.3541351221721235
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.4796380090497738, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5949367088607594, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8947368421052632, 31: 0.0, 32: 0.0, 33: 0.2727272727272727, 36: 0.4864864864864865, 40: 0.0}
Micro-average F1 score: 0.336231884057971
Weighted-average F1 score: 0.27974456577161594
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.4796380090497738, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5875, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8717948717948718, 31: 0.0, 32: 0.0, 33: 0.2222222222222222, 36: 0.48, 40: 0.0}
Micro-average F1 score: 0.3466135458167331
Weighted-average F1 score: 0.2933192422489279

F1 score per class: {0: 0.594059405940594, 4: 0.9042553191489362, 5: 0.8558558558558559, 6: 0.2719298245614035, 7: 0.03636363636363636, 8: 0.2681564245810056, 9: 0.8518518518518519, 10: 0.17886178861788618, 13: 0.02857142857142857, 16: 0.41379310344827586, 17: 0.0, 18: 0.22018348623853212, 19: 0.4897959183673469, 20: 0.3786008230452675, 21: 0.16129032258064516, 23: 0.7032967032967034, 24: 0.08, 26: 0.6532663316582915, 27: 0.10526315789473684, 29: 0.74235807860262, 30: 0.9142857142857143, 31: 0.046511627906976744, 32: 0.6820276497695853, 33: 0.2608695652173913, 36: 0.38596491228070173, 40: 0.2692307692307692}
Micro-average F1 score: 0.48115577889447236
Weighted-average F1 score: 0.46137394260217707
F1 score per class: {0: 0.2905982905982906, 4: 0.926829268292683, 5: 0.3183279742765273, 6: 0.27602905569007263, 7: 0.0, 8: 0.18466898954703834, 9: 0.684931506849315, 10: 0.24705882352941178, 13: 0.05405405405405406, 16: 0.32432432432432434, 17: 0.0, 18: 0.18309859154929578, 19: 0.5119453924914675, 20: 0.3032258064516129, 21: 0.1532567049808429, 23: 0.5736434108527132, 24: 0.041666666666666664, 26: 0.6350710900473934, 27: 0.12598425196850394, 29: 0.7355371900826446, 30: 0.6666666666666666, 31: 0.027586206896551724, 32: 0.46153846153846156, 33: 0.09230769230769231, 36: 0.29427792915531337, 40: 0.17191977077363896}
Micro-average F1 score: 0.3324790705621049
Weighted-average F1 score: 0.3068866898115524
F1 score per class: {0: 0.288135593220339, 4: 0.96, 5: 0.37786259541984735, 6: 0.2720763723150358, 7: 0.0, 8: 0.18661971830985916, 9: 0.78125, 10: 0.24691358024691357, 13: 0.05063291139240506, 16: 0.35294117647058826, 17: 0.0, 18: 0.19318181818181818, 19: 0.5236363636363637, 20: 0.2822822822822823, 21: 0.15503875968992248, 23: 0.5606060606060606, 24: 0.05263157894736842, 26: 0.6442307692307693, 27: 0.12030075187969924, 29: 0.7346938775510204, 30: 0.5666666666666667, 31: 0.03418803418803419, 32: 0.46648793565683644, 33: 0.06976744186046512, 36: 0.2727272727272727, 40: 0.17391304347826086}
Micro-average F1 score: 0.34107427761035275
Weighted-average F1 score: 0.31457104856343515
cur_acc:  ['0.7025', '0.5299', '0.3019', '0.5914', '0.4277']
his_acc:  ['0.7025', '0.6034', '0.5324', '0.4960', '0.4812']
cur_acc des:  ['0.6447', '0.4334', '0.3501', '0.4038', '0.3362']
his_acc des:  ['0.6447', '0.4946', '0.4241', '0.3411', '0.3325']
cur_acc rrf:  ['0.6487', '0.4463', '0.3641', '0.4165', '0.3466']
his_acc rrf:  ['0.6487', '0.5093', '0.4479', '0.3541', '0.3411']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 70.9044846CurrentTrain: epoch  0, batch     1 | loss: 61.1331069CurrentTrain: epoch  0, batch     2 | loss: 64.2161368CurrentTrain: epoch  0, batch     3 | loss: 44.8229962CurrentTrain: epoch  0, batch     4 | loss: 49.6231505CurrentTrain: epoch  0, batch     5 | loss: 46.4931081CurrentTrain: epoch  0, batch     6 | loss: 60.5366709CurrentTrain: epoch  1, batch     0 | loss: 55.2532398CurrentTrain: epoch  1, batch     1 | loss: 58.5034381CurrentTrain: epoch  1, batch     2 | loss: 113.3923369CurrentTrain: epoch  1, batch     3 | loss: 53.6528858CurrentTrain: epoch  1, batch     4 | loss: 38.1681068CurrentTrain: epoch  1, batch     5 | loss: 47.7725367CurrentTrain: epoch  1, batch     6 | loss: 27.7003614CurrentTrain: epoch  2, batch     0 | loss: 49.8447232CurrentTrain: epoch  2, batch     1 | loss: 40.2039119CurrentTrain: epoch  2, batch     2 | loss: 73.8121248CurrentTrain: epoch  2, batch     3 | loss: 109.5278785CurrentTrain: epoch  2, batch     4 | loss: 112.8355536CurrentTrain: epoch  2, batch     5 | loss: 47.9452888CurrentTrain: epoch  2, batch     6 | loss: 28.0574834CurrentTrain: epoch  3, batch     0 | loss: 65.5243441CurrentTrain: epoch  3, batch     1 | loss: 68.9760656CurrentTrain: epoch  3, batch     2 | loss: 109.6591312CurrentTrain: epoch  3, batch     3 | loss: 70.5600295CurrentTrain: epoch  3, batch     4 | loss: 51.0997483CurrentTrain: epoch  3, batch     5 | loss: 48.7955317CurrentTrain: epoch  3, batch     6 | loss: 17.5452345CurrentTrain: epoch  4, batch     0 | loss: 69.5513868CurrentTrain: epoch  4, batch     1 | loss: 68.6751511CurrentTrain: epoch  4, batch     2 | loss: 48.5591133CurrentTrain: epoch  4, batch     3 | loss: 50.6363833CurrentTrain: epoch  4, batch     4 | loss: 38.0859917CurrentTrain: epoch  4, batch     5 | loss: 66.6431162CurrentTrain: epoch  4, batch     6 | loss: 60.1412197CurrentTrain: epoch  5, batch     0 | loss: 48.7248917CurrentTrain: epoch  5, batch     1 | loss: 69.8923233CurrentTrain: epoch  5, batch     2 | loss: 28.9065351CurrentTrain: epoch  5, batch     3 | loss: 68.6106436CurrentTrain: epoch  5, batch     4 | loss: 68.0251221CurrentTrain: epoch  5, batch     5 | loss: 50.4509911CurrentTrain: epoch  5, batch     6 | loss: 27.1302233CurrentTrain: epoch  6, batch     0 | loss: 50.0316080CurrentTrain: epoch  6, batch     1 | loss: 48.7298679CurrentTrain: epoch  6, batch     2 | loss: 50.6823860CurrentTrain: epoch  6, batch     3 | loss: 29.3296137CurrentTrain: epoch  6, batch     4 | loss: 104.5269872CurrentTrain: epoch  6, batch     5 | loss: 49.4508663CurrentTrain: epoch  6, batch     6 | loss: 27.0017650CurrentTrain: epoch  7, batch     0 | loss: 49.0078388CurrentTrain: epoch  7, batch     1 | loss: 68.4790457CurrentTrain: epoch  7, batch     2 | loss: 107.5560731CurrentTrain: epoch  7, batch     3 | loss: 33.2714522CurrentTrain: epoch  7, batch     4 | loss: 49.0834056CurrentTrain: epoch  7, batch     5 | loss: 63.8460683CurrentTrain: epoch  7, batch     6 | loss: 26.8438800CurrentTrain: epoch  8, batch     0 | loss: 29.3902759CurrentTrain: epoch  8, batch     1 | loss: 66.3650469CurrentTrain: epoch  8, batch     2 | loss: 64.8166791CurrentTrain: epoch  8, batch     3 | loss: 47.0613462CurrentTrain: epoch  8, batch     4 | loss: 48.4358985CurrentTrain: epoch  8, batch     5 | loss: 107.4716714CurrentTrain: epoch  8, batch     6 | loss: 25.4439786CurrentTrain: epoch  9, batch     0 | loss: 47.9090333CurrentTrain: epoch  9, batch     1 | loss: 49.2836292CurrentTrain: epoch  9, batch     2 | loss: 66.6194553CurrentTrain: epoch  9, batch     3 | loss: 48.0444605CurrentTrain: epoch  9, batch     4 | loss: 66.8404980CurrentTrain: epoch  9, batch     5 | loss: 47.7211089CurrentTrain: epoch  9, batch     6 | loss: 13.5497496
MemoryTrain:  epoch  0, batch     0 | loss: 0.3306029MemoryTrain:  epoch  1, batch     0 | loss: 0.2823510MemoryTrain:  epoch  2, batch     0 | loss: 0.2101277MemoryTrain:  epoch  3, batch     0 | loss: 0.1651170MemoryTrain:  epoch  4, batch     0 | loss: 0.1189590MemoryTrain:  epoch  5, batch     0 | loss: 0.1130673MemoryTrain:  epoch  6, batch     0 | loss: 0.0969620MemoryTrain:  epoch  7, batch     0 | loss: 0.0885779MemoryTrain:  epoch  8, batch     0 | loss: 0.0770753MemoryTrain:  epoch  9, batch     0 | loss: 0.0696388

F1 score per class: {0: 0.0, 2: 0.42424242424242425, 4: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.22857142857142856, 12: 0.1864406779661017, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 28: 0.23529411764705882, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 39: 0.08333333333333333, 40: 0.0}
Micro-average F1 score: 0.15566037735849056
Weighted-average F1 score: 0.09938753689306908
F1 score per class: {0: 0.0, 2: 0.3333333333333333, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.467005076142132, 12: 0.5607476635514018, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 28: 0.19230769230769232, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.19047619047619047, 40: 0.0}
Micro-average F1 score: 0.29221556886227545
Weighted-average F1 score: 0.22666178366757142
F1 score per class: {0: 0.0, 2: 0.34146341463414637, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.45555555555555555, 12: 0.583732057416268, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.16, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.17142857142857143, 40: 0.0}
Micro-average F1 score: 0.29292929292929293
Weighted-average F1 score: 0.22126480739577137

F1 score per class: {0: 0.5614035087719298, 2: 0.23728813559322035, 4: 0.8842105263157894, 5: 0.9178743961352657, 6: 0.2641509433962264, 7: 0.0, 8: 0.12389380530973451, 9: 0.92, 10: 0.0196078431372549, 11: 0.14814814814814814, 12: 0.1456953642384106, 13: 0.03508771929824561, 16: 0.41379310344827586, 17: 0.0, 18: 0.11764705882352941, 19: 0.4864864864864865, 20: 0.3463203463203463, 21: 0.24615384615384617, 23: 0.7708333333333334, 24: 0.13333333333333333, 26: 0.6421052631578947, 27: 0.1568627450980392, 28: 0.07272727272727272, 29: 0.7467811158798283, 30: 0.9142857142857143, 31: 0.05263157894736842, 32: 0.7085201793721974, 33: 0.2727272727272727, 36: 0.14814814814814814, 39: 0.0625, 40: 0.29411764705882354}
Micro-average F1 score: 0.43898156277436345
Weighted-average F1 score: 0.4420353012646548
F1 score per class: {0: 0.32142857142857145, 2: 0.1346153846153846, 4: 0.8627450980392157, 5: 0.528, 6: 0.275, 7: 0.022222222222222223, 8: 0.21153846153846154, 9: 0.704225352112676, 10: 0.2302158273381295, 11: 0.21345707656612528, 12: 0.1935483870967742, 13: 0.037037037037037035, 16: 0.3709677419354839, 17: 0.0, 18: 0.13636363636363635, 19: 0.5238095238095238, 20: 0.27710843373493976, 21: 0.14728682170542637, 23: 0.5755395683453237, 24: 0.06896551724137931, 26: 0.6376811594202898, 27: 0.1414141414141414, 28: 0.08130081300813008, 29: 0.7407407407407407, 30: 0.8095238095238095, 31: 0.043010752688172046, 32: 0.49162011173184356, 33: 0.1, 36: 0.35467980295566504, 39: 0.08163265306122448, 40: 0.2737642585551331}
Micro-average F1 score: 0.336402499177902
Weighted-average F1 score: 0.31168185953267896
F1 score per class: {0: 0.35714285714285715, 2: 0.1414141414141414, 4: 0.9025641025641026, 5: 0.5892857142857143, 6: 0.271356783919598, 7: 0.023809523809523808, 8: 0.20714285714285716, 9: 0.8333333333333334, 10: 0.2361111111111111, 11: 0.20603015075376885, 12: 0.2026578073089701, 13: 0.030303030303030304, 16: 0.40350877192982454, 17: 0.0, 18: 0.13333333333333333, 19: 0.5128205128205128, 20: 0.27058823529411763, 21: 0.1386861313868613, 23: 0.6106870229007634, 24: 0.10256410256410256, 26: 0.6439024390243903, 27: 0.14, 28: 0.06201550387596899, 29: 0.7459016393442623, 30: 0.7555555555555555, 31: 0.024390243902439025, 32: 0.4971751412429379, 33: 0.09090909090909091, 36: 0.3417085427135678, 39: 0.0759493670886076, 40: 0.2608695652173913}
Micro-average F1 score: 0.3426131511528608
Weighted-average F1 score: 0.31538818963259646
cur_acc:  ['0.7025', '0.5299', '0.3019', '0.5914', '0.4277', '0.1557']
his_acc:  ['0.7025', '0.6034', '0.5324', '0.4960', '0.4812', '0.4390']
cur_acc des:  ['0.6447', '0.4334', '0.3501', '0.4038', '0.3362', '0.2922']
his_acc des:  ['0.6447', '0.4946', '0.4241', '0.3411', '0.3325', '0.3364']
cur_acc rrf:  ['0.6487', '0.4463', '0.3641', '0.4165', '0.3466', '0.2929']
his_acc rrf:  ['0.6487', '0.5093', '0.4479', '0.3541', '0.3411', '0.3426']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 52.2958304CurrentTrain: epoch  0, batch     1 | loss: 80.8813637CurrentTrain: epoch  0, batch     2 | loss: 79.3973107CurrentTrain: epoch  0, batch     3 | loss: 81.0524898CurrentTrain: epoch  0, batch     4 | loss: 110.7457209CurrentTrain: epoch  0, batch     5 | loss: 62.6166967CurrentTrain: epoch  0, batch     6 | loss: 71.6267370CurrentTrain: epoch  1, batch     0 | loss: 224.1973773CurrentTrain: epoch  1, batch     1 | loss: 54.8483566CurrentTrain: epoch  1, batch     2 | loss: 73.0762715CurrentTrain: epoch  1, batch     3 | loss: 52.4836901CurrentTrain: epoch  1, batch     4 | loss: 51.5045457CurrentTrain: epoch  1, batch     5 | loss: 43.1951725CurrentTrain: epoch  1, batch     6 | loss: 107.5624140CurrentTrain: epoch  2, batch     0 | loss: 41.9427883CurrentTrain: epoch  2, batch     1 | loss: 52.1225495CurrentTrain: epoch  2, batch     2 | loss: 67.8487169CurrentTrain: epoch  2, batch     3 | loss: 54.6541516CurrentTrain: epoch  2, batch     4 | loss: 223.3821295CurrentTrain: epoch  2, batch     5 | loss: 31.9190261CurrentTrain: epoch  2, batch     6 | loss: 42.9790826CurrentTrain: epoch  3, batch     0 | loss: 71.1015863CurrentTrain: epoch  3, batch     1 | loss: 51.8691003CurrentTrain: epoch  3, batch     2 | loss: 39.1705864CurrentTrain: epoch  3, batch     3 | loss: 39.7209757CurrentTrain: epoch  3, batch     4 | loss: 39.3574205CurrentTrain: epoch  3, batch     5 | loss: 68.2487424CurrentTrain: epoch  3, batch     6 | loss: 103.8049453CurrentTrain: epoch  4, batch     0 | loss: 40.5685437CurrentTrain: epoch  4, batch     1 | loss: 69.1310806CurrentTrain: epoch  4, batch     2 | loss: 68.1900178CurrentTrain: epoch  4, batch     3 | loss: 39.0758216CurrentTrain: epoch  4, batch     4 | loss: 38.1630919CurrentTrain: epoch  4, batch     5 | loss: 50.2593345CurrentTrain: epoch  4, batch     6 | loss: 66.9839240CurrentTrain: epoch  5, batch     0 | loss: 108.4307806CurrentTrain: epoch  5, batch     1 | loss: 48.4705548CurrentTrain: epoch  5, batch     2 | loss: 47.6195803CurrentTrain: epoch  5, batch     3 | loss: 48.4737840CurrentTrain: epoch  5, batch     4 | loss: 68.9029630CurrentTrain: epoch  5, batch     5 | loss: 48.1153422CurrentTrain: epoch  5, batch     6 | loss: 65.4641441CurrentTrain: epoch  6, batch     0 | loss: 66.9762637CurrentTrain: epoch  6, batch     1 | loss: 67.2641791CurrentTrain: epoch  6, batch     2 | loss: 38.9078379CurrentTrain: epoch  6, batch     3 | loss: 30.8351263CurrentTrain: epoch  6, batch     4 | loss: 46.2968545CurrentTrain: epoch  6, batch     5 | loss: 222.8529387CurrentTrain: epoch  6, batch     6 | loss: 47.3356225CurrentTrain: epoch  7, batch     0 | loss: 103.7103137CurrentTrain: epoch  7, batch     1 | loss: 222.5847184CurrentTrain: epoch  7, batch     2 | loss: 28.9584292CurrentTrain: epoch  7, batch     3 | loss: 37.0058480CurrentTrain: epoch  7, batch     4 | loss: 65.0434312CurrentTrain: epoch  7, batch     5 | loss: 66.7640531CurrentTrain: epoch  7, batch     6 | loss: 46.2507932CurrentTrain: epoch  8, batch     0 | loss: 49.4794520CurrentTrain: epoch  8, batch     1 | loss: 37.1607542CurrentTrain: epoch  8, batch     2 | loss: 27.5520775CurrentTrain: epoch  8, batch     3 | loss: 66.6839392CurrentTrain: epoch  8, batch     4 | loss: 107.6569081CurrentTrain: epoch  8, batch     5 | loss: 66.9457667CurrentTrain: epoch  8, batch     6 | loss: 102.9466336CurrentTrain: epoch  9, batch     0 | loss: 67.2572460CurrentTrain: epoch  9, batch     1 | loss: 66.4563651CurrentTrain: epoch  9, batch     2 | loss: 101.0746998CurrentTrain: epoch  9, batch     3 | loss: 45.5251869CurrentTrain: epoch  9, batch     4 | loss: 68.6188943CurrentTrain: epoch  9, batch     5 | loss: 66.4734346CurrentTrain: epoch  9, batch     6 | loss: 44.6965923
MemoryTrain:  epoch  0, batch     0 | loss: 0.4600526MemoryTrain:  epoch  0, batch     1 | loss: 0.0406498MemoryTrain:  epoch  1, batch     0 | loss: 0.3684315MemoryTrain:  epoch  1, batch     1 | loss: 0.1000127MemoryTrain:  epoch  2, batch     0 | loss: 0.1730035MemoryTrain:  epoch  2, batch     1 | loss: 0.2258518MemoryTrain:  epoch  3, batch     0 | loss: 0.1348593MemoryTrain:  epoch  3, batch     1 | loss: 0.1020501MemoryTrain:  epoch  4, batch     0 | loss: 0.1239263MemoryTrain:  epoch  4, batch     1 | loss: 0.0657507MemoryTrain:  epoch  5, batch     0 | loss: 0.1033239MemoryTrain:  epoch  5, batch     1 | loss: 0.0526937MemoryTrain:  epoch  6, batch     0 | loss: 0.0902912MemoryTrain:  epoch  6, batch     1 | loss: 0.0530761MemoryTrain:  epoch  7, batch     0 | loss: 0.0765130MemoryTrain:  epoch  7, batch     1 | loss: 0.0347083MemoryTrain:  epoch  8, batch     0 | loss: 0.0638354MemoryTrain:  epoch  8, batch     1 | loss: 0.0458078MemoryTrain:  epoch  9, batch     0 | loss: 0.0521159MemoryTrain:  epoch  9, batch     1 | loss: 0.0594213

F1 score per class: {0: 0.0, 1: 0.1732283464566929, 2: 0.0, 3: 0.4563758389261745, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.07476635514018691, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.43727598566308246, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.2153846153846154, 40: 0.0}
Micro-average F1 score: 0.2333637192342753
Weighted-average F1 score: 0.19992820627693042
F1 score per class: {0: 0.0, 1: 0.15584415584415584, 2: 0.0, 3: 0.4792332268370607, 5: 0.0, 6: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.1641025641025641, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.47876447876447875, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.49523809523809526, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.23081296191017622
Weighted-average F1 score: 0.19389606670431928
F1 score per class: {0: 0.0, 1: 0.15337423312883436, 2: 0.0, 3: 0.46645367412140576, 5: 0.0, 6: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.12244897959183673, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4555160142348754, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.3684210526315789, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.21906453522794553
Weighted-average F1 score: 0.18692105231163278

F1 score per class: {0: 0.5454545454545454, 1: 0.13253012048192772, 2: 0.23076923076923078, 3: 0.3148148148148148, 4: 0.8900523560209425, 5: 0.8962264150943396, 6: 0.22142857142857142, 7: 0.02531645569620253, 8: 0.09090909090909091, 9: 0.9433962264150944, 10: 0.11009174311926606, 11: 0.1588785046728972, 12: 0.10606060606060606, 13: 0.03225806451612903, 14: 0.05228758169934641, 16: 0.4186046511627907, 17: 0.0, 18: 0.1282051282051282, 19: 0.33519553072625696, 20: 0.3657142857142857, 21: 0.09523809523809523, 22: 0.3556851311953353, 23: 0.6833333333333333, 24: 0.02631578947368421, 26: 0.6439024390243903, 27: 0.04081632653061224, 28: 0.125, 29: 0.7091633466135459, 30: 0.9142857142857143, 31: 0.0, 32: 0.5627376425855514, 33: 0.15, 34: 0.09523809523809523, 36: 0.2641509433962264, 39: 0.03636363636363636, 40: 0.3063063063063063}
Micro-average F1 score: 0.357504215851602
Weighted-average F1 score: 0.34448556164264343
F1 score per class: {0: 0.4383561643835616, 1: 0.10933940774487472, 2: 0.11864406779661017, 3: 0.24077046548956663, 4: 0.8704663212435233, 5: 0.4716981132075472, 6: 0.23448275862068965, 7: 0.022222222222222223, 8: 0.1935483870967742, 9: 0.6756756756756757, 10: 0.2608695652173913, 11: 0.16705336426914152, 12: 0.2064516129032258, 13: 0.02040816326530612, 14: 0.10847457627118644, 16: 0.3779527559055118, 17: 0.125, 18: 0.11224489795918367, 19: 0.3868312757201646, 20: 0.3269961977186312, 21: 0.11627906976744186, 22: 0.3502824858757062, 23: 0.45614035087719296, 24: 0.047619047619047616, 26: 0.6181818181818182, 27: 0.07692307692307693, 28: 0.03980099502487562, 29: 0.6870229007633588, 30: 0.7346938775510204, 31: 0.046511627906976744, 32: 0.4143222506393862, 33: 0.05714285714285714, 34: 0.12351543942992874, 36: 0.20353982300884957, 39: 0.06451612903225806, 40: 0.30662020905923343}
Micro-average F1 score: 0.2791641791044776
Weighted-average F1 score: 0.2592282850027441
F1 score per class: {0: 0.4411764705882353, 1: 0.10775862068965517, 2: 0.11475409836065574, 3: 0.2225609756097561, 4: 0.8842105263157894, 5: 0.5689655172413793, 6: 0.23943661971830985, 7: 0.02197802197802198, 8: 0.171875, 9: 0.746268656716418, 10: 0.27710843373493976, 11: 0.13688212927756654, 12: 0.24390243902439024, 13: 0.021739130434782608, 14: 0.0782608695652174, 16: 0.3898305084745763, 17: 0.0, 18: 0.12857142857142856, 19: 0.4, 20: 0.31970260223048325, 21: 0.11764705882352941, 22: 0.33773087071240104, 23: 0.5064935064935064, 24: 0.04938271604938271, 26: 0.6238532110091743, 27: 0.07407407407407407, 28: 0.03, 29: 0.7045454545454546, 30: 0.75, 31: 0.05405405405405406, 32: 0.43010752688172044, 33: 0.047619047619047616, 34: 0.11764705882352941, 36: 0.23140495867768596, 39: 0.06976744186046512, 40: 0.3173431734317343}
Micro-average F1 score: 0.2884468247895945
Weighted-average F1 score: 0.26647248439729476
cur_acc:  ['0.7025', '0.5299', '0.3019', '0.5914', '0.4277', '0.1557', '0.2334']
his_acc:  ['0.7025', '0.6034', '0.5324', '0.4960', '0.4812', '0.4390', '0.3575']
cur_acc des:  ['0.6447', '0.4334', '0.3501', '0.4038', '0.3362', '0.2922', '0.2308']
his_acc des:  ['0.6447', '0.4946', '0.4241', '0.3411', '0.3325', '0.3364', '0.2792']
cur_acc rrf:  ['0.6487', '0.4463', '0.3641', '0.4165', '0.3466', '0.2929', '0.2191']
his_acc rrf:  ['0.6487', '0.5093', '0.4479', '0.3541', '0.3411', '0.3426', '0.2884']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 53.2969770CurrentTrain: epoch  0, batch     1 | loss: 79.1090086CurrentTrain: epoch  0, batch     2 | loss: 61.0479221CurrentTrain: epoch  0, batch     3 | loss: 56.9673925CurrentTrain: epoch  0, batch     4 | loss: 62.2052019CurrentTrain: epoch  0, batch     5 | loss: 41.4185368CurrentTrain: epoch  1, batch     0 | loss: 46.7149408CurrentTrain: epoch  1, batch     1 | loss: 52.3257400CurrentTrain: epoch  1, batch     2 | loss: 43.6412172CurrentTrain: epoch  1, batch     3 | loss: 69.0618995CurrentTrain: epoch  1, batch     4 | loss: 49.9335693CurrentTrain: epoch  1, batch     5 | loss: 29.0484416CurrentTrain: epoch  2, batch     0 | loss: 111.6919743CurrentTrain: epoch  2, batch     1 | loss: 47.3864051CurrentTrain: epoch  2, batch     2 | loss: 39.9769801CurrentTrain: epoch  2, batch     3 | loss: 39.3538529CurrentTrain: epoch  2, batch     4 | loss: 50.6063956CurrentTrain: epoch  2, batch     5 | loss: 35.6790903CurrentTrain: epoch  3, batch     0 | loss: 38.3220270CurrentTrain: epoch  3, batch     1 | loss: 37.4385524CurrentTrain: epoch  3, batch     2 | loss: 70.6239313CurrentTrain: epoch  3, batch     3 | loss: 50.1262353CurrentTrain: epoch  3, batch     4 | loss: 66.8591297CurrentTrain: epoch  3, batch     5 | loss: 55.9966701CurrentTrain: epoch  4, batch     0 | loss: 50.1505511CurrentTrain: epoch  4, batch     1 | loss: 39.7048587CurrentTrain: epoch  4, batch     2 | loss: 69.1849443CurrentTrain: epoch  4, batch     3 | loss: 28.3917460CurrentTrain: epoch  4, batch     4 | loss: 37.8479030CurrentTrain: epoch  4, batch     5 | loss: 56.8431641CurrentTrain: epoch  5, batch     0 | loss: 35.4502301CurrentTrain: epoch  5, batch     1 | loss: 38.9274365CurrentTrain: epoch  5, batch     2 | loss: 37.5814805CurrentTrain: epoch  5, batch     3 | loss: 37.2174138CurrentTrain: epoch  5, batch     4 | loss: 107.8917094CurrentTrain: epoch  5, batch     5 | loss: 38.2089621CurrentTrain: epoch  6, batch     0 | loss: 36.8899176CurrentTrain: epoch  6, batch     1 | loss: 68.5327661CurrentTrain: epoch  6, batch     2 | loss: 35.7170600CurrentTrain: epoch  6, batch     3 | loss: 35.2482839CurrentTrain: epoch  6, batch     4 | loss: 49.7685093CurrentTrain: epoch  6, batch     5 | loss: 56.9021480CurrentTrain: epoch  7, batch     0 | loss: 38.1635770CurrentTrain: epoch  7, batch     1 | loss: 47.1187046CurrentTrain: epoch  7, batch     2 | loss: 47.8976973CurrentTrain: epoch  7, batch     3 | loss: 36.9829422CurrentTrain: epoch  7, batch     4 | loss: 46.8727277CurrentTrain: epoch  7, batch     5 | loss: 56.6312244CurrentTrain: epoch  8, batch     0 | loss: 45.5039783CurrentTrain: epoch  8, batch     1 | loss: 34.8267619CurrentTrain: epoch  8, batch     2 | loss: 107.4972583CurrentTrain: epoch  8, batch     3 | loss: 47.9918836CurrentTrain: epoch  8, batch     4 | loss: 47.9061860CurrentTrain: epoch  8, batch     5 | loss: 33.2426870CurrentTrain: epoch  9, batch     0 | loss: 34.1356556CurrentTrain: epoch  9, batch     1 | loss: 107.4505507CurrentTrain: epoch  9, batch     2 | loss: 66.5812017CurrentTrain: epoch  9, batch     3 | loss: 35.3939469CurrentTrain: epoch  9, batch     4 | loss: 66.4186987CurrentTrain: epoch  9, batch     5 | loss: 23.1518578
MemoryTrain:  epoch  0, batch     0 | loss: 0.2919602MemoryTrain:  epoch  0, batch     1 | loss: 0.0854932MemoryTrain:  epoch  1, batch     0 | loss: 0.1483950MemoryTrain:  epoch  1, batch     1 | loss: 0.3095938MemoryTrain:  epoch  2, batch     0 | loss: 0.1840300MemoryTrain:  epoch  2, batch     1 | loss: 0.0920511MemoryTrain:  epoch  3, batch     0 | loss: 0.1300637MemoryTrain:  epoch  3, batch     1 | loss: 0.0663436MemoryTrain:  epoch  4, batch     0 | loss: 0.0808184MemoryTrain:  epoch  4, batch     1 | loss: 0.0849570MemoryTrain:  epoch  5, batch     0 | loss: 0.1210038MemoryTrain:  epoch  5, batch     1 | loss: 0.0630930MemoryTrain:  epoch  6, batch     0 | loss: 0.0788117MemoryTrain:  epoch  6, batch     1 | loss: 0.0657736MemoryTrain:  epoch  7, batch     0 | loss: 0.0708865MemoryTrain:  epoch  7, batch     1 | loss: 0.0351968MemoryTrain:  epoch  8, batch     0 | loss: 0.0615982MemoryTrain:  epoch  8, batch     1 | loss: 0.0441447MemoryTrain:  epoch  9, batch     0 | loss: 0.0487880MemoryTrain:  epoch  9, batch     1 | loss: 0.0661514

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.7058823529411765, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 25: 0.3492063492063492, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.43478260869565216, 36: 0.0, 37: 0.3614457831325301, 38: 0.3582089552238806, 40: 0.0}
Micro-average F1 score: 0.2260536398467433
Weighted-average F1 score: 0.12115867767650888
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.5714285714285714, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 25: 0.48717948717948717, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7777777777777778, 36: 0.0, 37: 0.35036496350364965, 38: 0.3880597014925373, 40: 0.0}
Micro-average F1 score: 0.23582766439909297
Weighted-average F1 score: 0.15476966935933079
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.5714285714285714, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 25: 0.4507042253521127, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.8108108108108109, 36: 0.0, 37: 0.32558139534883723, 38: 0.37333333333333335, 40: 0.0}
Micro-average F1 score: 0.24938875305623473
Weighted-average F1 score: 0.1677626031795616

F1 score per class: {0: 0.47540983606557374, 1: 0.10133333333333333, 2: 0.16470588235294117, 3: 0.34594594594594597, 4: 0.8478260869565217, 5: 0.7348484848484849, 6: 0.22119815668202766, 7: 0.02666666666666667, 8: 0.0821917808219178, 9: 0.847457627118644, 10: 0.10256410256410256, 11: 0.10714285714285714, 12: 0.07407407407407407, 13: 0.02857142857142857, 14: 0.06896551724137931, 15: 0.375, 16: 0.38333333333333336, 17: 0.0, 18: 0.11764705882352941, 19: 0.3181818181818182, 20: 0.3161764705882353, 21: 0.16216216216216217, 22: 0.4268774703557312, 23: 0.7422680412371134, 24: 0.043478260869565216, 25: 0.3492063492063492, 26: 0.6368159203980099, 27: 0.08695652173913043, 28: 0.2727272727272727, 29: 0.7250996015936255, 30: 0.9142857142857143, 31: 0.0, 32: 0.5423728813559322, 33: 0.15789473684210525, 34: 0.04918032786885246, 35: 0.2158273381294964, 36: 0.22857142857142856, 37: 0.15228426395939088, 38: 0.05139186295503212, 39: 0.038461538461538464, 40: 0.2711864406779661}
Micro-average F1 score: 0.31054131054131057
Weighted-average F1 score: 0.2891065771456388
F1 score per class: {0: 0.375, 1: 0.10300429184549356, 2: 0.08484848484848485, 3: 0.20168067226890757, 4: 0.8775510204081632, 5: 0.3992015968063872, 6: 0.24878048780487805, 7: 0.02631578947368421, 8: 0.21333333333333335, 9: 0.5434782608695652, 10: 0.22748815165876776, 11: 0.18487394957983194, 12: 0.1761827079934747, 13: 0.06557377049180328, 14: 0.10646387832699619, 15: 0.24489795918367346, 16: 0.36231884057971014, 17: 0.0, 18: 0.10970464135021098, 19: 0.3443223443223443, 20: 0.2679425837320574, 21: 0.06, 22: 0.3782051282051282, 23: 0.5777777777777777, 24: 0.031746031746031744, 25: 0.4810126582278481, 26: 0.6071428571428571, 27: 0.05128205128205128, 28: 0.07058823529411765, 29: 0.6618181818181819, 30: 0.6666666666666666, 31: 0.029411764705882353, 32: 0.45325779036827196, 33: 0.06315789473684211, 34: 0.05390835579514825, 35: 0.23796033994334279, 36: 0.14864864864864866, 37: 0.07079646017699115, 38: 0.06132075471698113, 39: 0.045454545454545456, 40: 0.3181818181818182}
Micro-average F1 score: 0.24588674674472596
Weighted-average F1 score: 0.22529348499418134
F1 score per class: {0: 0.3950617283950617, 1: 0.1006036217303823, 2: 0.09032258064516129, 3: 0.2814498933901919, 4: 0.9042553191489362, 5: 0.45662100456621, 6: 0.2469733656174334, 7: 0.025, 8: 0.21739130434782608, 9: 0.6172839506172839, 10: 0.20994475138121546, 11: 0.16541353383458646, 12: 0.18439716312056736, 13: 0.05555555555555555, 14: 0.08490566037735849, 15: 0.2608695652173913, 16: 0.36496350364963503, 17: 0.0, 18: 0.12154696132596685, 19: 0.36363636363636365, 20: 0.2909090909090909, 21: 0.06741573033707865, 22: 0.389937106918239, 23: 0.6, 24: 0.031746031746031744, 25: 0.4444444444444444, 26: 0.6126126126126126, 27: 0.05405405405405406, 28: 0.0625, 29: 0.674074074074074, 30: 0.6792452830188679, 31: 0.03278688524590164, 32: 0.46153846153846156, 33: 0.07058823529411765, 34: 0.06818181818181818, 35: 0.24064171122994651, 36: 0.17834394904458598, 37: 0.058823529411764705, 38: 0.052930056710775046, 39: 0.04081632653061224, 40: 0.3166023166023166}
Micro-average F1 score: 0.255278310940499
Weighted-average F1 score: 0.23279932614621496
cur_acc:  ['0.7025', '0.5299', '0.3019', '0.5914', '0.4277', '0.1557', '0.2334', '0.2261']
his_acc:  ['0.7025', '0.6034', '0.5324', '0.4960', '0.4812', '0.4390', '0.3575', '0.3105']
cur_acc des:  ['0.6447', '0.4334', '0.3501', '0.4038', '0.3362', '0.2922', '0.2308', '0.2358']
his_acc des:  ['0.6447', '0.4946', '0.4241', '0.3411', '0.3325', '0.3364', '0.2792', '0.2459']
cur_acc rrf:  ['0.6487', '0.4463', '0.3641', '0.4165', '0.3466', '0.2929', '0.2191', '0.2494']
his_acc rrf:  ['0.6487', '0.5093', '0.4479', '0.3541', '0.3411', '0.3426', '0.2884', '0.2553']
----------END
his_acc mean:  [0.6852 0.576  0.4918 0.4459 0.3888 0.3805 0.3413 0.3236]
his_acc des mean:  [0.6409 0.4901 0.4112 0.3422 0.3056 0.2881 0.2529 0.238 ]
his_acc rrf mean:  [0.6441 0.4981 0.4198 0.3472 0.3105 0.2947 0.2608 0.2446]
