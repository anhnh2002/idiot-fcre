#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 100.8243714CurrentTrain: epoch  0, batch     1 | loss: 68.2585837CurrentTrain: epoch  0, batch     2 | loss: 60.5998465CurrentTrain: epoch  0, batch     3 | loss: 67.2289609CurrentTrain: epoch  0, batch     4 | loss: 68.6189985CurrentTrain: epoch  0, batch     5 | loss: 67.2111274CurrentTrain: epoch  0, batch     6 | loss: 78.9731789CurrentTrain: epoch  0, batch     7 | loss: 77.8290985CurrentTrain: epoch  0, batch     8 | loss: 65.4571616CurrentTrain: epoch  0, batch     9 | loss: 66.6849613CurrentTrain: epoch  0, batch    10 | loss: 57.3477642CurrentTrain: epoch  0, batch    11 | loss: 77.1123547CurrentTrain: epoch  0, batch    12 | loss: 78.6625773CurrentTrain: epoch  0, batch    13 | loss: 189.0160833CurrentTrain: epoch  0, batch    14 | loss: 77.8515421CurrentTrain: epoch  0, batch    15 | loss: 65.3458501CurrentTrain: epoch  0, batch    16 | loss: 64.8383082CurrentTrain: epoch  0, batch    17 | loss: 94.8061343CurrentTrain: epoch  0, batch    18 | loss: 77.7527585CurrentTrain: epoch  0, batch    19 | loss: 65.4449149CurrentTrain: epoch  0, batch    20 | loss: 126.4741223CurrentTrain: epoch  0, batch    21 | loss: 127.4349443CurrentTrain: epoch  0, batch    22 | loss: 187.7734874CurrentTrain: epoch  0, batch    23 | loss: 126.8929096CurrentTrain: epoch  0, batch    24 | loss: 77.0173803CurrentTrain: epoch  0, batch    25 | loss: 188.5437438CurrentTrain: epoch  0, batch    26 | loss: 65.0120151CurrentTrain: epoch  0, batch    27 | loss: 97.8200831CurrentTrain: epoch  0, batch    28 | loss: 126.2277205CurrentTrain: epoch  0, batch    29 | loss: 77.3054146CurrentTrain: epoch  0, batch    30 | loss: 126.0609989CurrentTrain: epoch  0, batch    31 | loss: 126.2377471CurrentTrain: epoch  0, batch    32 | loss: 95.1731922CurrentTrain: epoch  0, batch    33 | loss: 64.6130681CurrentTrain: epoch  0, batch    34 | loss: 126.4637709CurrentTrain: epoch  0, batch    35 | loss: 64.6571466CurrentTrain: epoch  0, batch    36 | loss: 63.9916070CurrentTrain: epoch  0, batch    37 | loss: 77.3226743CurrentTrain: epoch  0, batch    38 | loss: 76.6948850CurrentTrain: epoch  0, batch    39 | loss: 64.2252211CurrentTrain: epoch  0, batch    40 | loss: 94.6911351CurrentTrain: epoch  0, batch    41 | loss: 64.5064800CurrentTrain: epoch  0, batch    42 | loss: 64.2999992CurrentTrain: epoch  0, batch    43 | loss: 56.2926810CurrentTrain: epoch  0, batch    44 | loss: 76.6568864CurrentTrain: epoch  0, batch    45 | loss: 64.3253210CurrentTrain: epoch  0, batch    46 | loss: 94.6032756CurrentTrain: epoch  0, batch    47 | loss: 64.7412100CurrentTrain: epoch  0, batch    48 | loss: 76.4637516CurrentTrain: epoch  0, batch    49 | loss: 76.8092422CurrentTrain: epoch  0, batch    50 | loss: 76.1451825CurrentTrain: epoch  0, batch    51 | loss: 76.4205819CurrentTrain: epoch  0, batch    52 | loss: 76.3374298CurrentTrain: epoch  0, batch    53 | loss: 125.8150154CurrentTrain: epoch  0, batch    54 | loss: 94.9620607CurrentTrain: epoch  0, batch    55 | loss: 63.9928525CurrentTrain: epoch  0, batch    56 | loss: 64.2298634CurrentTrain: epoch  0, batch    57 | loss: 76.7457371CurrentTrain: epoch  0, batch    58 | loss: 76.7493719CurrentTrain: epoch  0, batch    59 | loss: 75.8409422CurrentTrain: epoch  0, batch    60 | loss: 76.0552787CurrentTrain: epoch  0, batch    61 | loss: 76.0779458CurrentTrain: epoch  0, batch    62 | loss: 63.4148370CurrentTrain: epoch  0, batch    63 | loss: 94.9409372CurrentTrain: epoch  0, batch    64 | loss: 76.7320787CurrentTrain: epoch  0, batch    65 | loss: 56.1441977CurrentTrain: epoch  0, batch    66 | loss: 75.5283494CurrentTrain: epoch  0, batch    67 | loss: 76.2975461CurrentTrain: epoch  0, batch    68 | loss: 63.8545961CurrentTrain: epoch  0, batch    69 | loss: 125.4987201CurrentTrain: epoch  0, batch    70 | loss: 94.9042210CurrentTrain: epoch  0, batch    71 | loss: 64.1282498CurrentTrain: epoch  0, batch    72 | loss: 75.9777229CurrentTrain: epoch  0, batch    73 | loss: 93.3324271CurrentTrain: epoch  0, batch    74 | loss: 125.5898086CurrentTrain: epoch  0, batch    75 | loss: 76.4682093CurrentTrain: epoch  0, batch    76 | loss: 92.9088124CurrentTrain: epoch  0, batch    77 | loss: 75.5780603CurrentTrain: epoch  0, batch    78 | loss: 93.3913893CurrentTrain: epoch  0, batch    79 | loss: 93.5560021CurrentTrain: epoch  0, batch    80 | loss: 62.2329525CurrentTrain: epoch  0, batch    81 | loss: 74.8067461CurrentTrain: epoch  0, batch    82 | loss: 94.1339849CurrentTrain: epoch  0, batch    83 | loss: 75.4143451CurrentTrain: epoch  0, batch    84 | loss: 91.0392207CurrentTrain: epoch  0, batch    85 | loss: 63.2680696CurrentTrain: epoch  0, batch    86 | loss: 122.9531087CurrentTrain: epoch  0, batch    87 | loss: 62.5062162CurrentTrain: epoch  0, batch    88 | loss: 62.6824451CurrentTrain: epoch  0, batch    89 | loss: 55.0444579CurrentTrain: epoch  0, batch    90 | loss: 74.9244121CurrentTrain: epoch  0, batch    91 | loss: 62.8143552CurrentTrain: epoch  0, batch    92 | loss: 74.5001060CurrentTrain: epoch  0, batch    93 | loss: 121.6045714CurrentTrain: epoch  0, batch    94 | loss: 63.1629504CurrentTrain: epoch  0, batch    95 | loss: 78.0026200CurrentTrain: epoch  1, batch     0 | loss: 61.2917911CurrentTrain: epoch  1, batch     1 | loss: 74.8971400CurrentTrain: epoch  1, batch     2 | loss: 63.3987389CurrentTrain: epoch  1, batch     3 | loss: 61.6176010CurrentTrain: epoch  1, batch     4 | loss: 72.5769243CurrentTrain: epoch  1, batch     5 | loss: 58.2449698CurrentTrain: epoch  1, batch     6 | loss: 74.1237762CurrentTrain: epoch  1, batch     7 | loss: 119.7613416CurrentTrain: epoch  1, batch     8 | loss: 51.8832868CurrentTrain: epoch  1, batch     9 | loss: 91.6064931CurrentTrain: epoch  1, batch    10 | loss: 72.4753516CurrentTrain: epoch  1, batch    11 | loss: 60.9928886CurrentTrain: epoch  1, batch    12 | loss: 59.1062964CurrentTrain: epoch  1, batch    13 | loss: 92.4202412CurrentTrain: epoch  1, batch    14 | loss: 87.1332843CurrentTrain: epoch  1, batch    15 | loss: 73.6956641CurrentTrain: epoch  1, batch    16 | loss: 121.6551542CurrentTrain: epoch  1, batch    17 | loss: 59.7217706CurrentTrain: epoch  1, batch    18 | loss: 69.2867790CurrentTrain: epoch  1, batch    19 | loss: 91.3661410CurrentTrain: epoch  1, batch    20 | loss: 124.5145194CurrentTrain: epoch  1, batch    21 | loss: 59.1541309CurrentTrain: epoch  1, batch    22 | loss: 59.1776037CurrentTrain: epoch  1, batch    23 | loss: 72.1997395CurrentTrain: epoch  1, batch    24 | loss: 122.4053806CurrentTrain: epoch  1, batch    25 | loss: 73.1400687CurrentTrain: epoch  1, batch    26 | loss: 50.8107495CurrentTrain: epoch  1, batch    27 | loss: 60.0702335CurrentTrain: epoch  1, batch    28 | loss: 50.6700627CurrentTrain: epoch  1, batch    29 | loss: 52.3505059CurrentTrain: epoch  1, batch    30 | loss: 70.6139700CurrentTrain: epoch  1, batch    31 | loss: 72.8893546CurrentTrain: epoch  1, batch    32 | loss: 92.2251072CurrentTrain: epoch  1, batch    33 | loss: 71.5699571CurrentTrain: epoch  1, batch    34 | loss: 82.1539047CurrentTrain: epoch  1, batch    35 | loss: 123.8787940CurrentTrain: epoch  1, batch    36 | loss: 59.2013468CurrentTrain: epoch  1, batch    37 | loss: 69.6722830CurrentTrain: epoch  1, batch    38 | loss: 68.9783270CurrentTrain: epoch  1, batch    39 | loss: 49.9060646CurrentTrain: epoch  1, batch    40 | loss: 57.1538416CurrentTrain: epoch  1, batch    41 | loss: 59.2513284CurrentTrain: epoch  1, batch    42 | loss: 60.0757896CurrentTrain: epoch  1, batch    43 | loss: 71.8659561CurrentTrain: epoch  1, batch    44 | loss: 124.4573764CurrentTrain: epoch  1, batch    45 | loss: 72.8087454CurrentTrain: epoch  1, batch    46 | loss: 88.0827597CurrentTrain: epoch  1, batch    47 | loss: 56.8795865CurrentTrain: epoch  1, batch    48 | loss: 70.6904617CurrentTrain: epoch  1, batch    49 | loss: 71.4459933CurrentTrain: epoch  1, batch    50 | loss: 71.4750907CurrentTrain: epoch  1, batch    51 | loss: 93.1058378CurrentTrain: epoch  1, batch    52 | loss: 93.4557488CurrentTrain: epoch  1, batch    53 | loss: 121.1809413CurrentTrain: epoch  1, batch    54 | loss: 61.6801988CurrentTrain: epoch  1, batch    55 | loss: 70.6638521CurrentTrain: epoch  1, batch    56 | loss: 69.8935155CurrentTrain: epoch  1, batch    57 | loss: 87.9788815CurrentTrain: epoch  1, batch    58 | loss: 72.2174292CurrentTrain: epoch  1, batch    59 | loss: 58.5840095CurrentTrain: epoch  1, batch    60 | loss: 69.8567389CurrentTrain: epoch  1, batch    61 | loss: 119.1431922CurrentTrain: epoch  1, batch    62 | loss: 120.2000942CurrentTrain: epoch  1, batch    63 | loss: 57.5932130CurrentTrain: epoch  1, batch    64 | loss: 70.5281763CurrentTrain: epoch  1, batch    65 | loss: 88.9715303CurrentTrain: epoch  1, batch    66 | loss: 50.1243376CurrentTrain: epoch  1, batch    67 | loss: 119.6114477CurrentTrain: epoch  1, batch    68 | loss: 54.8802394CurrentTrain: epoch  1, batch    69 | loss: 60.3958112CurrentTrain: epoch  1, batch    70 | loss: 57.9535390CurrentTrain: epoch  1, batch    71 | loss: 72.4111857CurrentTrain: epoch  1, batch    72 | loss: 88.5564982CurrentTrain: epoch  1, batch    73 | loss: 87.2684313CurrentTrain: epoch  1, batch    74 | loss: 87.8902491CurrentTrain: epoch  1, batch    75 | loss: 68.7877376CurrentTrain: epoch  1, batch    76 | loss: 70.7939075CurrentTrain: epoch  1, batch    77 | loss: 74.1091830CurrentTrain: epoch  1, batch    78 | loss: 71.9832009CurrentTrain: epoch  1, batch    79 | loss: 68.1449960CurrentTrain: epoch  1, batch    80 | loss: 47.5152295CurrentTrain: epoch  1, batch    81 | loss: 87.8789704CurrentTrain: epoch  1, batch    82 | loss: 57.2423213CurrentTrain: epoch  1, batch    83 | loss: 186.7405796CurrentTrain: epoch  1, batch    84 | loss: 90.0826454CurrentTrain: epoch  1, batch    85 | loss: 58.1745123CurrentTrain: epoch  1, batch    86 | loss: 47.3419652CurrentTrain: epoch  1, batch    87 | loss: 59.4538478CurrentTrain: epoch  1, batch    88 | loss: 89.2205754CurrentTrain: epoch  1, batch    89 | loss: 73.3233372CurrentTrain: epoch  1, batch    90 | loss: 58.5550917CurrentTrain: epoch  1, batch    91 | loss: 70.2831194CurrentTrain: epoch  1, batch    92 | loss: 67.7703796CurrentTrain: epoch  1, batch    93 | loss: 122.0222649CurrentTrain: epoch  1, batch    94 | loss: 72.3328938CurrentTrain: epoch  1, batch    95 | loss: 56.6619056CurrentTrain: epoch  2, batch     0 | loss: 47.6887884CurrentTrain: epoch  2, batch     1 | loss: 91.2988022CurrentTrain: epoch  2, batch     2 | loss: 68.2433309CurrentTrain: epoch  2, batch     3 | loss: 70.6172093CurrentTrain: epoch  2, batch     4 | loss: 117.6422093CurrentTrain: epoch  2, batch     5 | loss: 56.6391502CurrentTrain: epoch  2, batch     6 | loss: 121.2018282CurrentTrain: epoch  2, batch     7 | loss: 67.8174891CurrentTrain: epoch  2, batch     8 | loss: 86.2063358CurrentTrain: epoch  2, batch     9 | loss: 46.5884701CurrentTrain: epoch  2, batch    10 | loss: 84.5244223CurrentTrain: epoch  2, batch    11 | loss: 44.8664728CurrentTrain: epoch  2, batch    12 | loss: 49.3955014CurrentTrain: epoch  2, batch    13 | loss: 88.2986078CurrentTrain: epoch  2, batch    14 | loss: 89.2184605CurrentTrain: epoch  2, batch    15 | loss: 90.5187308CurrentTrain: epoch  2, batch    16 | loss: 67.0235918CurrentTrain: epoch  2, batch    17 | loss: 68.0097489CurrentTrain: epoch  2, batch    18 | loss: 68.2192479CurrentTrain: epoch  2, batch    19 | loss: 87.4915784CurrentTrain: epoch  2, batch    20 | loss: 55.2448329CurrentTrain: epoch  2, batch    21 | loss: 84.6674761CurrentTrain: epoch  2, batch    22 | loss: 56.7938160CurrentTrain: epoch  2, batch    23 | loss: 67.7892878CurrentTrain: epoch  2, batch    24 | loss: 65.1402737CurrentTrain: epoch  2, batch    25 | loss: 67.3451097CurrentTrain: epoch  2, batch    26 | loss: 72.6641684CurrentTrain: epoch  2, batch    27 | loss: 69.5852420CurrentTrain: epoch  2, batch    28 | loss: 86.6142632CurrentTrain: epoch  2, batch    29 | loss: 87.5124650CurrentTrain: epoch  2, batch    30 | loss: 70.9164050CurrentTrain: epoch  2, batch    31 | loss: 65.5952212CurrentTrain: epoch  2, batch    32 | loss: 58.1199210CurrentTrain: epoch  2, batch    33 | loss: 96.5706770CurrentTrain: epoch  2, batch    34 | loss: 87.7137416CurrentTrain: epoch  2, batch    35 | loss: 88.2572830CurrentTrain: epoch  2, batch    36 | loss: 69.0201837CurrentTrain: epoch  2, batch    37 | loss: 84.3669257CurrentTrain: epoch  2, batch    38 | loss: 56.6839415CurrentTrain: epoch  2, batch    39 | loss: 85.8770858CurrentTrain: epoch  2, batch    40 | loss: 45.0312407CurrentTrain: epoch  2, batch    41 | loss: 87.5929719CurrentTrain: epoch  2, batch    42 | loss: 85.0144979CurrentTrain: epoch  2, batch    43 | loss: 56.3492501CurrentTrain: epoch  2, batch    44 | loss: 68.6045297CurrentTrain: epoch  2, batch    45 | loss: 58.4924082CurrentTrain: epoch  2, batch    46 | loss: 68.5650715CurrentTrain: epoch  2, batch    47 | loss: 57.1608694CurrentTrain: epoch  2, batch    48 | loss: 119.1874002CurrentTrain: epoch  2, batch    49 | loss: 120.4566341CurrentTrain: epoch  2, batch    50 | loss: 55.2447083CurrentTrain: epoch  2, batch    51 | loss: 120.4085162CurrentTrain: epoch  2, batch    52 | loss: 56.9954765CurrentTrain: epoch  2, batch    53 | loss: 68.9513695CurrentTrain: epoch  2, batch    54 | loss: 87.0766102CurrentTrain: epoch  2, batch    55 | loss: 68.7528431CurrentTrain: epoch  2, batch    56 | loss: 65.1753568CurrentTrain: epoch  2, batch    57 | loss: 54.1580167CurrentTrain: epoch  2, batch    58 | loss: 57.1368957CurrentTrain: epoch  2, batch    59 | loss: 56.7479781CurrentTrain: epoch  2, batch    60 | loss: 67.3406896CurrentTrain: epoch  2, batch    61 | loss: 53.2333186CurrentTrain: epoch  2, batch    62 | loss: 67.0743320CurrentTrain: epoch  2, batch    63 | loss: 56.1434450CurrentTrain: epoch  2, batch    64 | loss: 54.4157178CurrentTrain: epoch  2, batch    65 | loss: 69.0351540CurrentTrain: epoch  2, batch    66 | loss: 119.0292244CurrentTrain: epoch  2, batch    67 | loss: 91.6149938CurrentTrain: epoch  2, batch    68 | loss: 58.3544638CurrentTrain: epoch  2, batch    69 | loss: 54.0890362CurrentTrain: epoch  2, batch    70 | loss: 68.2587267CurrentTrain: epoch  2, batch    71 | loss: 54.9455482CurrentTrain: epoch  2, batch    72 | loss: 182.0121052CurrentTrain: epoch  2, batch    73 | loss: 72.3577775CurrentTrain: epoch  2, batch    74 | loss: 67.7000480CurrentTrain: epoch  2, batch    75 | loss: 70.5250735CurrentTrain: epoch  2, batch    76 | loss: 84.9834360CurrentTrain: epoch  2, batch    77 | loss: 47.9894576CurrentTrain: epoch  2, batch    78 | loss: 68.4158313CurrentTrain: epoch  2, batch    79 | loss: 69.3800936CurrentTrain: epoch  2, batch    80 | loss: 55.4520601CurrentTrain: epoch  2, batch    81 | loss: 68.6717202CurrentTrain: epoch  2, batch    82 | loss: 88.0789629CurrentTrain: epoch  2, batch    83 | loss: 90.5863654CurrentTrain: epoch  2, batch    84 | loss: 57.4935744CurrentTrain: epoch  2, batch    85 | loss: 117.0313861CurrentTrain: epoch  2, batch    86 | loss: 47.6353914CurrentTrain: epoch  2, batch    87 | loss: 47.7255908CurrentTrain: epoch  2, batch    88 | loss: 48.5462709CurrentTrain: epoch  2, batch    89 | loss: 83.8791875CurrentTrain: epoch  2, batch    90 | loss: 69.4348187CurrentTrain: epoch  2, batch    91 | loss: 59.2624226CurrentTrain: epoch  2, batch    92 | loss: 58.4699840CurrentTrain: epoch  2, batch    93 | loss: 73.4876613CurrentTrain: epoch  2, batch    94 | loss: 68.4390291CurrentTrain: epoch  2, batch    95 | loss: 55.0134494CurrentTrain: epoch  3, batch     0 | loss: 68.6177047CurrentTrain: epoch  3, batch     1 | loss: 86.3344601CurrentTrain: epoch  3, batch     2 | loss: 71.2898342CurrentTrain: epoch  3, batch     3 | loss: 68.1409154CurrentTrain: epoch  3, batch     4 | loss: 67.8525476CurrentTrain: epoch  3, batch     5 | loss: 85.0034362CurrentTrain: epoch  3, batch     6 | loss: 65.1817985CurrentTrain: epoch  3, batch     7 | loss: 47.4918557CurrentTrain: epoch  3, batch     8 | loss: 55.6977057CurrentTrain: epoch  3, batch     9 | loss: 115.8302963CurrentTrain: epoch  3, batch    10 | loss: 55.9002677CurrentTrain: epoch  3, batch    11 | loss: 64.1696014CurrentTrain: epoch  3, batch    12 | loss: 84.1733763CurrentTrain: epoch  3, batch    13 | loss: 84.7187579CurrentTrain: epoch  3, batch    14 | loss: 67.9616157CurrentTrain: epoch  3, batch    15 | loss: 87.0515828CurrentTrain: epoch  3, batch    16 | loss: 85.3093978CurrentTrain: epoch  3, batch    17 | loss: 52.9925682CurrentTrain: epoch  3, batch    18 | loss: 56.8240670CurrentTrain: epoch  3, batch    19 | loss: 56.1270055CurrentTrain: epoch  3, batch    20 | loss: 86.6798827CurrentTrain: epoch  3, batch    21 | loss: 67.0473811CurrentTrain: epoch  3, batch    22 | loss: 85.0918780CurrentTrain: epoch  3, batch    23 | loss: 66.6608314CurrentTrain: epoch  3, batch    24 | loss: 56.3044469CurrentTrain: epoch  3, batch    25 | loss: 69.5857490CurrentTrain: epoch  3, batch    26 | loss: 86.2540693CurrentTrain: epoch  3, batch    27 | loss: 90.8860530CurrentTrain: epoch  3, batch    28 | loss: 84.2320281CurrentTrain: epoch  3, batch    29 | loss: 55.4107840CurrentTrain: epoch  3, batch    30 | loss: 70.0393844CurrentTrain: epoch  3, batch    31 | loss: 63.7165676CurrentTrain: epoch  3, batch    32 | loss: 66.1325290CurrentTrain: epoch  3, batch    33 | loss: 115.8755625CurrentTrain: epoch  3, batch    34 | loss: 68.3016018CurrentTrain: epoch  3, batch    35 | loss: 115.5025553CurrentTrain: epoch  3, batch    36 | loss: 43.9369090CurrentTrain: epoch  3, batch    37 | loss: 118.5699918CurrentTrain: epoch  3, batch    38 | loss: 50.5261103CurrentTrain: epoch  3, batch    39 | loss: 66.5389782CurrentTrain: epoch  3, batch    40 | loss: 67.6171527CurrentTrain: epoch  3, batch    41 | loss: 64.8640862CurrentTrain: epoch  3, batch    42 | loss: 65.8346844CurrentTrain: epoch  3, batch    43 | loss: 52.2310214CurrentTrain: epoch  3, batch    44 | loss: 49.4655410CurrentTrain: epoch  3, batch    45 | loss: 56.5908407CurrentTrain: epoch  3, batch    46 | loss: 86.9499748CurrentTrain: epoch  3, batch    47 | loss: 69.7468299CurrentTrain: epoch  3, batch    48 | loss: 119.1434615CurrentTrain: epoch  3, batch    49 | loss: 49.0504097CurrentTrain: epoch  3, batch    50 | loss: 55.1606779CurrentTrain: epoch  3, batch    51 | loss: 55.7226623CurrentTrain: epoch  3, batch    52 | loss: 120.8880387CurrentTrain: epoch  3, batch    53 | loss: 57.2358708CurrentTrain: epoch  3, batch    54 | loss: 84.4243283CurrentTrain: epoch  3, batch    55 | loss: 67.0892356CurrentTrain: epoch  3, batch    56 | loss: 57.7437406CurrentTrain: epoch  3, batch    57 | loss: 56.4630122CurrentTrain: epoch  3, batch    58 | loss: 71.3398589CurrentTrain: epoch  3, batch    59 | loss: 113.6056057CurrentTrain: epoch  3, batch    60 | loss: 68.3842224CurrentTrain: epoch  3, batch    61 | loss: 54.5126323CurrentTrain: epoch  3, batch    62 | loss: 67.4356206CurrentTrain: epoch  3, batch    63 | loss: 83.7972196CurrentTrain: epoch  3, batch    64 | loss: 64.9551323CurrentTrain: epoch  3, batch    65 | loss: 87.2183084CurrentTrain: epoch  3, batch    66 | loss: 89.9950469CurrentTrain: epoch  3, batch    67 | loss: 55.0545834CurrentTrain: epoch  3, batch    68 | loss: 67.1039425CurrentTrain: epoch  3, batch    69 | loss: 54.8840907CurrentTrain: epoch  3, batch    70 | loss: 69.1469865CurrentTrain: epoch  3, batch    71 | loss: 44.4772250CurrentTrain: epoch  3, batch    72 | loss: 55.5506351CurrentTrain: epoch  3, batch    73 | loss: 42.6187192CurrentTrain: epoch  3, batch    74 | loss: 69.7829478CurrentTrain: epoch  3, batch    75 | loss: 88.4854860CurrentTrain: epoch  3, batch    76 | loss: 81.1837791CurrentTrain: epoch  3, batch    77 | loss: 67.8749741CurrentTrain: epoch  3, batch    78 | loss: 112.4383668CurrentTrain: epoch  3, batch    79 | loss: 65.2063886CurrentTrain: epoch  3, batch    80 | loss: 86.1705152CurrentTrain: epoch  3, batch    81 | loss: 43.4703017CurrentTrain: epoch  3, batch    82 | loss: 92.7390578CurrentTrain: epoch  3, batch    83 | loss: 67.1619283CurrentTrain: epoch  3, batch    84 | loss: 55.7002662CurrentTrain: epoch  3, batch    85 | loss: 86.8361379CurrentTrain: epoch  3, batch    86 | loss: 56.0827517CurrentTrain: epoch  3, batch    87 | loss: 68.0896457CurrentTrain: epoch  3, batch    88 | loss: 52.3349306CurrentTrain: epoch  3, batch    89 | loss: 53.6146954CurrentTrain: epoch  3, batch    90 | loss: 84.4196071CurrentTrain: epoch  3, batch    91 | loss: 87.1369711CurrentTrain: epoch  3, batch    92 | loss: 56.6785067CurrentTrain: epoch  3, batch    93 | loss: 67.6750705CurrentTrain: epoch  3, batch    94 | loss: 88.2603610CurrentTrain: epoch  3, batch    95 | loss: 42.6756176CurrentTrain: epoch  4, batch     0 | loss: 66.0932990CurrentTrain: epoch  4, batch     1 | loss: 86.7578387CurrentTrain: epoch  4, batch     2 | loss: 82.6693957CurrentTrain: epoch  4, batch     3 | loss: 63.4497713CurrentTrain: epoch  4, batch     4 | loss: 119.8551909CurrentTrain: epoch  4, batch     5 | loss: 82.3076581CurrentTrain: epoch  4, batch     6 | loss: 54.2788327CurrentTrain: epoch  4, batch     7 | loss: 49.7729750CurrentTrain: epoch  4, batch     8 | loss: 87.8973852CurrentTrain: epoch  4, batch     9 | loss: 119.5547604CurrentTrain: epoch  4, batch    10 | loss: 87.6824909CurrentTrain: epoch  4, batch    11 | loss: 54.8986950CurrentTrain: epoch  4, batch    12 | loss: 66.0310513CurrentTrain: epoch  4, batch    13 | loss: 53.8163112CurrentTrain: epoch  4, batch    14 | loss: 63.5756347CurrentTrain: epoch  4, batch    15 | loss: 84.9436812CurrentTrain: epoch  4, batch    16 | loss: 82.2385312CurrentTrain: epoch  4, batch    17 | loss: 117.9808707CurrentTrain: epoch  4, batch    18 | loss: 64.3627621CurrentTrain: epoch  4, batch    19 | loss: 65.5658874CurrentTrain: epoch  4, batch    20 | loss: 65.2282239CurrentTrain: epoch  4, batch    21 | loss: 56.0606061CurrentTrain: epoch  4, batch    22 | loss: 85.9666311CurrentTrain: epoch  4, batch    23 | loss: 61.7217351CurrentTrain: epoch  4, batch    24 | loss: 81.5468735CurrentTrain: epoch  4, batch    25 | loss: 84.4970468CurrentTrain: epoch  4, batch    26 | loss: 66.7653382CurrentTrain: epoch  4, batch    27 | loss: 46.1851204CurrentTrain: epoch  4, batch    28 | loss: 85.3332623CurrentTrain: epoch  4, batch    29 | loss: 65.4559979CurrentTrain: epoch  4, batch    30 | loss: 68.1823083CurrentTrain: epoch  4, batch    31 | loss: 56.9155094CurrentTrain: epoch  4, batch    32 | loss: 79.9775502CurrentTrain: epoch  4, batch    33 | loss: 43.3212607CurrentTrain: epoch  4, batch    34 | loss: 83.9464607CurrentTrain: epoch  4, batch    35 | loss: 68.0729171CurrentTrain: epoch  4, batch    36 | loss: 53.8301524CurrentTrain: epoch  4, batch    37 | loss: 45.4527836CurrentTrain: epoch  4, batch    38 | loss: 64.5818131CurrentTrain: epoch  4, batch    39 | loss: 56.2528717CurrentTrain: epoch  4, batch    40 | loss: 66.5815219CurrentTrain: epoch  4, batch    41 | loss: 55.9222808CurrentTrain: epoch  4, batch    42 | loss: 68.9096347CurrentTrain: epoch  4, batch    43 | loss: 43.7458412CurrentTrain: epoch  4, batch    44 | loss: 51.7578716CurrentTrain: epoch  4, batch    45 | loss: 70.3964628CurrentTrain: epoch  4, batch    46 | loss: 59.3867801CurrentTrain: epoch  4, batch    47 | loss: 83.4125061CurrentTrain: epoch  4, batch    48 | loss: 55.6436240CurrentTrain: epoch  4, batch    49 | loss: 71.7256688CurrentTrain: epoch  4, batch    50 | loss: 67.4465185CurrentTrain: epoch  4, batch    51 | loss: 57.3369179CurrentTrain: epoch  4, batch    52 | loss: 68.7808195CurrentTrain: epoch  4, batch    53 | loss: 113.9558274CurrentTrain: epoch  4, batch    54 | loss: 66.2420321CurrentTrain: epoch  4, batch    55 | loss: 85.2837581CurrentTrain: epoch  4, batch    56 | loss: 119.6090146CurrentTrain: epoch  4, batch    57 | loss: 54.2719280CurrentTrain: epoch  4, batch    58 | loss: 114.1024057CurrentTrain: epoch  4, batch    59 | loss: 86.1756902CurrentTrain: epoch  4, batch    60 | loss: 68.4171279CurrentTrain: epoch  4, batch    61 | loss: 44.8439141CurrentTrain: epoch  4, batch    62 | loss: 46.3203382CurrentTrain: epoch  4, batch    63 | loss: 53.7884761CurrentTrain: epoch  4, batch    64 | loss: 66.8165487CurrentTrain: epoch  4, batch    65 | loss: 116.3476694CurrentTrain: epoch  4, batch    66 | loss: 57.4395176CurrentTrain: epoch  4, batch    67 | loss: 52.2589073CurrentTrain: epoch  4, batch    68 | loss: 86.2903308CurrentTrain: epoch  4, batch    69 | loss: 52.8833104CurrentTrain: epoch  4, batch    70 | loss: 78.3476771CurrentTrain: epoch  4, batch    71 | loss: 87.1002409CurrentTrain: epoch  4, batch    72 | loss: 49.3583471CurrentTrain: epoch  4, batch    73 | loss: 55.3268193CurrentTrain: epoch  4, batch    74 | loss: 118.9646330CurrentTrain: epoch  4, batch    75 | loss: 65.0851482CurrentTrain: epoch  4, batch    76 | loss: 121.8813045CurrentTrain: epoch  4, batch    77 | loss: 82.7884562CurrentTrain: epoch  4, batch    78 | loss: 53.9272757CurrentTrain: epoch  4, batch    79 | loss: 83.8008486CurrentTrain: epoch  4, batch    80 | loss: 53.1778178CurrentTrain: epoch  4, batch    81 | loss: 119.9076847CurrentTrain: epoch  4, batch    82 | loss: 63.4940254CurrentTrain: epoch  4, batch    83 | loss: 52.7800022CurrentTrain: epoch  4, batch    84 | loss: 60.6382002CurrentTrain: epoch  4, batch    85 | loss: 65.3330565CurrentTrain: epoch  4, batch    86 | loss: 88.2795858CurrentTrain: epoch  4, batch    87 | loss: 114.9194679CurrentTrain: epoch  4, batch    88 | loss: 85.0805341CurrentTrain: epoch  4, batch    89 | loss: 55.5169151CurrentTrain: epoch  4, batch    90 | loss: 112.1934035CurrentTrain: epoch  4, batch    91 | loss: 80.7570620CurrentTrain: epoch  4, batch    92 | loss: 66.2775744CurrentTrain: epoch  4, batch    93 | loss: 63.0011471CurrentTrain: epoch  4, batch    94 | loss: 70.0877233CurrentTrain: epoch  4, batch    95 | loss: 45.7394281CurrentTrain: epoch  5, batch     0 | loss: 64.2925869CurrentTrain: epoch  5, batch     1 | loss: 62.8402632CurrentTrain: epoch  5, batch     2 | loss: 83.9913617CurrentTrain: epoch  5, batch     3 | loss: 66.0403718CurrentTrain: epoch  5, batch     4 | loss: 62.5680350CurrentTrain: epoch  5, batch     5 | loss: 52.5392861CurrentTrain: epoch  5, batch     6 | loss: 118.6976191CurrentTrain: epoch  5, batch     7 | loss: 63.5392986CurrentTrain: epoch  5, batch     8 | loss: 64.5217328CurrentTrain: epoch  5, batch     9 | loss: 63.7350301CurrentTrain: epoch  5, batch    10 | loss: 55.0189486CurrentTrain: epoch  5, batch    11 | loss: 84.8155681CurrentTrain: epoch  5, batch    12 | loss: 117.6025443CurrentTrain: epoch  5, batch    13 | loss: 53.0159577CurrentTrain: epoch  5, batch    14 | loss: 82.7819285CurrentTrain: epoch  5, batch    15 | loss: 51.4309208CurrentTrain: epoch  5, batch    16 | loss: 64.8724396CurrentTrain: epoch  5, batch    17 | loss: 50.9613774CurrentTrain: epoch  5, batch    18 | loss: 68.5881631CurrentTrain: epoch  5, batch    19 | loss: 82.8371804CurrentTrain: epoch  5, batch    20 | loss: 64.5422574CurrentTrain: epoch  5, batch    21 | loss: 52.9546032CurrentTrain: epoch  5, batch    22 | loss: 66.1305254CurrentTrain: epoch  5, batch    23 | loss: 83.2582313CurrentTrain: epoch  5, batch    24 | loss: 44.7371306CurrentTrain: epoch  5, batch    25 | loss: 66.4610608CurrentTrain: epoch  5, batch    26 | loss: 85.9820034CurrentTrain: epoch  5, batch    27 | loss: 81.3504301CurrentTrain: epoch  5, batch    28 | loss: 118.4746677CurrentTrain: epoch  5, batch    29 | loss: 372.1895496CurrentTrain: epoch  5, batch    30 | loss: 56.3378805CurrentTrain: epoch  5, batch    31 | loss: 177.6049333CurrentTrain: epoch  5, batch    32 | loss: 52.9757495CurrentTrain: epoch  5, batch    33 | loss: 59.3856833CurrentTrain: epoch  5, batch    34 | loss: 66.0427252CurrentTrain: epoch  5, batch    35 | loss: 54.9918079CurrentTrain: epoch  5, batch    36 | loss: 118.0528564CurrentTrain: epoch  5, batch    37 | loss: 67.3465453CurrentTrain: epoch  5, batch    38 | loss: 80.0413658CurrentTrain: epoch  5, batch    39 | loss: 86.3920377CurrentTrain: epoch  5, batch    40 | loss: 63.4138431CurrentTrain: epoch  5, batch    41 | loss: 87.3905699CurrentTrain: epoch  5, batch    42 | loss: 123.3384169CurrentTrain: epoch  5, batch    43 | loss: 54.1842841CurrentTrain: epoch  5, batch    44 | loss: 52.6689122CurrentTrain: epoch  5, batch    45 | loss: 66.2617356CurrentTrain: epoch  5, batch    46 | loss: 67.4653252CurrentTrain: epoch  5, batch    47 | loss: 52.0734584CurrentTrain: epoch  5, batch    48 | loss: 50.1309584CurrentTrain: epoch  5, batch    49 | loss: 54.4621894CurrentTrain: epoch  5, batch    50 | loss: 45.2181378CurrentTrain: epoch  5, batch    51 | loss: 63.2504854CurrentTrain: epoch  5, batch    52 | loss: 68.4507152CurrentTrain: epoch  5, batch    53 | loss: 62.3517233CurrentTrain: epoch  5, batch    54 | loss: 63.2682169CurrentTrain: epoch  5, batch    55 | loss: 85.9424845CurrentTrain: epoch  5, batch    56 | loss: 118.0505967CurrentTrain: epoch  5, batch    57 | loss: 69.9034033CurrentTrain: epoch  5, batch    58 | loss: 66.3852842CurrentTrain: epoch  5, batch    59 | loss: 115.6130810CurrentTrain: epoch  5, batch    60 | loss: 85.5577145CurrentTrain: epoch  5, batch    61 | loss: 68.5605601CurrentTrain: epoch  5, batch    62 | loss: 88.6547584CurrentTrain: epoch  5, batch    63 | loss: 67.3273811CurrentTrain: epoch  5, batch    64 | loss: 45.7436502CurrentTrain: epoch  5, batch    65 | loss: 65.3485644CurrentTrain: epoch  5, batch    66 | loss: 64.5379386CurrentTrain: epoch  5, batch    67 | loss: 65.8270015CurrentTrain: epoch  5, batch    68 | loss: 52.6909020CurrentTrain: epoch  5, batch    69 | loss: 66.0862592CurrentTrain: epoch  5, batch    70 | loss: 86.0142176CurrentTrain: epoch  5, batch    71 | loss: 86.5454322CurrentTrain: epoch  5, batch    72 | loss: 85.4858404CurrentTrain: epoch  5, batch    73 | loss: 63.6129161CurrentTrain: epoch  5, batch    74 | loss: 50.3470965CurrentTrain: epoch  5, batch    75 | loss: 52.1574649CurrentTrain: epoch  5, batch    76 | loss: 115.0684404CurrentTrain: epoch  5, batch    77 | loss: 67.0527170CurrentTrain: epoch  5, batch    78 | loss: 63.5397578CurrentTrain: epoch  5, batch    79 | loss: 67.1130630CurrentTrain: epoch  5, batch    80 | loss: 84.1311094CurrentTrain: epoch  5, batch    81 | loss: 85.5529150CurrentTrain: epoch  5, batch    82 | loss: 66.1058437CurrentTrain: epoch  5, batch    83 | loss: 84.4862804CurrentTrain: epoch  5, batch    84 | loss: 83.2173252CurrentTrain: epoch  5, batch    85 | loss: 64.9989252CurrentTrain: epoch  5, batch    86 | loss: 85.1595385CurrentTrain: epoch  5, batch    87 | loss: 53.0272218CurrentTrain: epoch  5, batch    88 | loss: 84.2473320CurrentTrain: epoch  5, batch    89 | loss: 64.0740300CurrentTrain: epoch  5, batch    90 | loss: 65.3895327CurrentTrain: epoch  5, batch    91 | loss: 69.1977845CurrentTrain: epoch  5, batch    92 | loss: 64.3314100CurrentTrain: epoch  5, batch    93 | loss: 52.5245382CurrentTrain: epoch  5, batch    94 | loss: 116.4722685CurrentTrain: epoch  5, batch    95 | loss: 45.4485179CurrentTrain: epoch  6, batch     0 | loss: 43.7046334CurrentTrain: epoch  6, batch     1 | loss: 65.6789648CurrentTrain: epoch  6, batch     2 | loss: 62.4701886CurrentTrain: epoch  6, batch     3 | loss: 86.3657573CurrentTrain: epoch  6, batch     4 | loss: 118.2944925CurrentTrain: epoch  6, batch     5 | loss: 181.6979184CurrentTrain: epoch  6, batch     6 | loss: 65.9409289CurrentTrain: epoch  6, batch     7 | loss: 66.8754425CurrentTrain: epoch  6, batch     8 | loss: 84.5734391CurrentTrain: epoch  6, batch     9 | loss: 68.6513319CurrentTrain: epoch  6, batch    10 | loss: 112.1434443CurrentTrain: epoch  6, batch    11 | loss: 66.7890732CurrentTrain: epoch  6, batch    12 | loss: 85.8557092CurrentTrain: epoch  6, batch    13 | loss: 85.2361287CurrentTrain: epoch  6, batch    14 | loss: 43.9950767CurrentTrain: epoch  6, batch    15 | loss: 62.3576591CurrentTrain: epoch  6, batch    16 | loss: 51.4757302CurrentTrain: epoch  6, batch    17 | loss: 65.6215289CurrentTrain: epoch  6, batch    18 | loss: 52.1013458CurrentTrain: epoch  6, batch    19 | loss: 51.2903886CurrentTrain: epoch  6, batch    20 | loss: 51.6885540CurrentTrain: epoch  6, batch    21 | loss: 66.5483704CurrentTrain: epoch  6, batch    22 | loss: 68.0094951CurrentTrain: epoch  6, batch    23 | loss: 84.9563465CurrentTrain: epoch  6, batch    24 | loss: 84.5119811CurrentTrain: epoch  6, batch    25 | loss: 51.8475934CurrentTrain: epoch  6, batch    26 | loss: 53.3823168CurrentTrain: epoch  6, batch    27 | loss: 51.1392645CurrentTrain: epoch  6, batch    28 | loss: 55.0787122CurrentTrain: epoch  6, batch    29 | loss: 81.3630271CurrentTrain: epoch  6, batch    30 | loss: 84.5256502CurrentTrain: epoch  6, batch    31 | loss: 42.5126759CurrentTrain: epoch  6, batch    32 | loss: 82.9270370CurrentTrain: epoch  6, batch    33 | loss: 68.0213648CurrentTrain: epoch  6, batch    34 | loss: 83.7985216CurrentTrain: epoch  6, batch    35 | loss: 52.7574749CurrentTrain: epoch  6, batch    36 | loss: 52.5827689CurrentTrain: epoch  6, batch    37 | loss: 55.1359928CurrentTrain: epoch  6, batch    38 | loss: 83.2976279CurrentTrain: epoch  6, batch    39 | loss: 63.8615893CurrentTrain: epoch  6, batch    40 | loss: 51.0325219CurrentTrain: epoch  6, batch    41 | loss: 80.9842418CurrentTrain: epoch  6, batch    42 | loss: 63.6504581CurrentTrain: epoch  6, batch    43 | loss: 44.8323800CurrentTrain: epoch  6, batch    44 | loss: 41.1303353CurrentTrain: epoch  6, batch    45 | loss: 66.3662306CurrentTrain: epoch  6, batch    46 | loss: 63.9202658CurrentTrain: epoch  6, batch    47 | loss: 117.6103764CurrentTrain: epoch  6, batch    48 | loss: 53.2033605CurrentTrain: epoch  6, batch    49 | loss: 51.8152617CurrentTrain: epoch  6, batch    50 | loss: 86.6188172CurrentTrain: epoch  6, batch    51 | loss: 82.0289656CurrentTrain: epoch  6, batch    52 | loss: 118.0480598CurrentTrain: epoch  6, batch    53 | loss: 88.9668589CurrentTrain: epoch  6, batch    54 | loss: 65.5615798CurrentTrain: epoch  6, batch    55 | loss: 65.4472777CurrentTrain: epoch  6, batch    56 | loss: 83.5897233CurrentTrain: epoch  6, batch    57 | loss: 49.4965495CurrentTrain: epoch  6, batch    58 | loss: 85.8634458CurrentTrain: epoch  6, batch    59 | loss: 43.8004348CurrentTrain: epoch  6, batch    60 | loss: 63.1533974CurrentTrain: epoch  6, batch    61 | loss: 69.0520636CurrentTrain: epoch  6, batch    62 | loss: 81.6160533CurrentTrain: epoch  6, batch    63 | loss: 64.0192407CurrentTrain: epoch  6, batch    64 | loss: 62.9048359CurrentTrain: epoch  6, batch    65 | loss: 85.1708941CurrentTrain: epoch  6, batch    66 | loss: 71.5410449CurrentTrain: epoch  6, batch    67 | loss: 65.1431281CurrentTrain: epoch  6, batch    68 | loss: 83.6105789CurrentTrain: epoch  6, batch    69 | loss: 63.4084226CurrentTrain: epoch  6, batch    70 | loss: 86.4831796CurrentTrain: epoch  6, batch    71 | loss: 43.4550099CurrentTrain: epoch  6, batch    72 | loss: 56.1236268CurrentTrain: epoch  6, batch    73 | loss: 50.3032768CurrentTrain: epoch  6, batch    74 | loss: 85.8425047CurrentTrain: epoch  6, batch    75 | loss: 82.9454704CurrentTrain: epoch  6, batch    76 | loss: 41.9581083CurrentTrain: epoch  6, batch    77 | loss: 44.6048003CurrentTrain: epoch  6, batch    78 | loss: 84.5488221CurrentTrain: epoch  6, batch    79 | loss: 85.0555380CurrentTrain: epoch  6, batch    80 | loss: 64.4743775CurrentTrain: epoch  6, batch    81 | loss: 45.1353391CurrentTrain: epoch  6, batch    82 | loss: 54.8490491CurrentTrain: epoch  6, batch    83 | loss: 79.8085059CurrentTrain: epoch  6, batch    84 | loss: 68.3002257CurrentTrain: epoch  6, batch    85 | loss: 43.5851061CurrentTrain: epoch  6, batch    86 | loss: 63.3593092CurrentTrain: epoch  6, batch    87 | loss: 84.3610303CurrentTrain: epoch  6, batch    88 | loss: 67.1540338CurrentTrain: epoch  6, batch    89 | loss: 86.4858291CurrentTrain: epoch  6, batch    90 | loss: 83.0756411CurrentTrain: epoch  6, batch    91 | loss: 43.6084114CurrentTrain: epoch  6, batch    92 | loss: 66.1295089CurrentTrain: epoch  6, batch    93 | loss: 64.5767631CurrentTrain: epoch  6, batch    94 | loss: 55.7821553CurrentTrain: epoch  6, batch    95 | loss: 92.6883933CurrentTrain: epoch  7, batch     0 | loss: 65.4271895CurrentTrain: epoch  7, batch     1 | loss: 82.6610869CurrentTrain: epoch  7, batch     2 | loss: 65.9863298CurrentTrain: epoch  7, batch     3 | loss: 64.6199142CurrentTrain: epoch  7, batch     4 | loss: 66.1442576CurrentTrain: epoch  7, batch     5 | loss: 83.4006384CurrentTrain: epoch  7, batch     6 | loss: 67.4013493CurrentTrain: epoch  7, batch     7 | loss: 63.8716097CurrentTrain: epoch  7, batch     8 | loss: 51.1236117CurrentTrain: epoch  7, batch     9 | loss: 51.9923951CurrentTrain: epoch  7, batch    10 | loss: 52.8369958CurrentTrain: epoch  7, batch    11 | loss: 52.5532551CurrentTrain: epoch  7, batch    12 | loss: 84.8835667CurrentTrain: epoch  7, batch    13 | loss: 65.7251194CurrentTrain: epoch  7, batch    14 | loss: 66.4643986CurrentTrain: epoch  7, batch    15 | loss: 54.3728699CurrentTrain: epoch  7, batch    16 | loss: 85.5404202CurrentTrain: epoch  7, batch    17 | loss: 58.5328788CurrentTrain: epoch  7, batch    18 | loss: 52.1632745CurrentTrain: epoch  7, batch    19 | loss: 50.3755136CurrentTrain: epoch  7, batch    20 | loss: 63.2767957CurrentTrain: epoch  7, batch    21 | loss: 62.2135361CurrentTrain: epoch  7, batch    22 | loss: 84.1077420CurrentTrain: epoch  7, batch    23 | loss: 82.9259757CurrentTrain: epoch  7, batch    24 | loss: 54.4140340CurrentTrain: epoch  7, batch    25 | loss: 43.0171231CurrentTrain: epoch  7, batch    26 | loss: 65.9451700CurrentTrain: epoch  7, batch    27 | loss: 85.8982257CurrentTrain: epoch  7, batch    28 | loss: 52.0640145CurrentTrain: epoch  7, batch    29 | loss: 86.3922139CurrentTrain: epoch  7, batch    30 | loss: 63.3770752CurrentTrain: epoch  7, batch    31 | loss: 66.8713705CurrentTrain: epoch  7, batch    32 | loss: 111.5838670CurrentTrain: epoch  7, batch    33 | loss: 65.9499980CurrentTrain: epoch  7, batch    34 | loss: 66.9770263CurrentTrain: epoch  7, batch    35 | loss: 86.0447172CurrentTrain: epoch  7, batch    36 | loss: 67.4921939CurrentTrain: epoch  7, batch    37 | loss: 68.0403511CurrentTrain: epoch  7, batch    38 | loss: 65.9107386CurrentTrain: epoch  7, batch    39 | loss: 84.4690444CurrentTrain: epoch  7, batch    40 | loss: 67.0351274CurrentTrain: epoch  7, batch    41 | loss: 49.4075922CurrentTrain: epoch  7, batch    42 | loss: 81.5899273CurrentTrain: epoch  7, batch    43 | loss: 51.9005267CurrentTrain: epoch  7, batch    44 | loss: 62.8510088CurrentTrain: epoch  7, batch    45 | loss: 84.2945312CurrentTrain: epoch  7, batch    46 | loss: 53.1977719CurrentTrain: epoch  7, batch    47 | loss: 61.5101202CurrentTrain: epoch  7, batch    48 | loss: 111.8206316CurrentTrain: epoch  7, batch    49 | loss: 44.9229770CurrentTrain: epoch  7, batch    50 | loss: 81.1222168CurrentTrain: epoch  7, batch    51 | loss: 61.7190979CurrentTrain: epoch  7, batch    52 | loss: 51.3291061CurrentTrain: epoch  7, batch    53 | loss: 83.2809770CurrentTrain: epoch  7, batch    54 | loss: 51.5704152CurrentTrain: epoch  7, batch    55 | loss: 85.8176212CurrentTrain: epoch  7, batch    56 | loss: 82.6656914CurrentTrain: epoch  7, batch    57 | loss: 65.0434956CurrentTrain: epoch  7, batch    58 | loss: 63.3886625CurrentTrain: epoch  7, batch    59 | loss: 117.6074643CurrentTrain: epoch  7, batch    60 | loss: 51.4070725CurrentTrain: epoch  7, batch    61 | loss: 53.3836433CurrentTrain: epoch  7, batch    62 | loss: 113.2848445CurrentTrain: epoch  7, batch    63 | loss: 66.5508168CurrentTrain: epoch  7, batch    64 | loss: 48.4068130CurrentTrain: epoch  7, batch    65 | loss: 61.3890325CurrentTrain: epoch  7, batch    66 | loss: 64.1318476CurrentTrain: epoch  7, batch    67 | loss: 66.7170031CurrentTrain: epoch  7, batch    68 | loss: 50.5339770CurrentTrain: epoch  7, batch    69 | loss: 64.6543155CurrentTrain: epoch  7, batch    70 | loss: 53.0165899CurrentTrain: epoch  7, batch    71 | loss: 85.7935210CurrentTrain: epoch  7, batch    72 | loss: 49.8878223CurrentTrain: epoch  7, batch    73 | loss: 64.5493387CurrentTrain: epoch  7, batch    74 | loss: 41.4733513CurrentTrain: epoch  7, batch    75 | loss: 49.7730169CurrentTrain: epoch  7, batch    76 | loss: 52.9023438CurrentTrain: epoch  7, batch    77 | loss: 84.3117808CurrentTrain: epoch  7, batch    78 | loss: 66.2019232CurrentTrain: epoch  7, batch    79 | loss: 117.4820785CurrentTrain: epoch  7, batch    80 | loss: 53.3696827CurrentTrain: epoch  7, batch    81 | loss: 51.0488514CurrentTrain: epoch  7, batch    82 | loss: 64.6859392CurrentTrain: epoch  7, batch    83 | loss: 49.2885731CurrentTrain: epoch  7, batch    84 | loss: 63.1002205CurrentTrain: epoch  7, batch    85 | loss: 65.4735522CurrentTrain: epoch  7, batch    86 | loss: 55.8674652CurrentTrain: epoch  7, batch    87 | loss: 82.7480529CurrentTrain: epoch  7, batch    88 | loss: 82.7041050CurrentTrain: epoch  7, batch    89 | loss: 118.8787990CurrentTrain: epoch  7, batch    90 | loss: 66.6186057CurrentTrain: epoch  7, batch    91 | loss: 62.8636698CurrentTrain: epoch  7, batch    92 | loss: 82.7204359CurrentTrain: epoch  7, batch    93 | loss: 44.5438953CurrentTrain: epoch  7, batch    94 | loss: 48.1616965CurrentTrain: epoch  7, batch    95 | loss: 70.1096343CurrentTrain: epoch  8, batch     0 | loss: 52.2067908CurrentTrain: epoch  8, batch     1 | loss: 51.3126565CurrentTrain: epoch  8, batch     2 | loss: 84.0707163CurrentTrain: epoch  8, batch     3 | loss: 117.4885187CurrentTrain: epoch  8, batch     4 | loss: 65.2094239CurrentTrain: epoch  8, batch     5 | loss: 54.5487289CurrentTrain: epoch  8, batch     6 | loss: 66.9126640CurrentTrain: epoch  8, batch     7 | loss: 63.3664128CurrentTrain: epoch  8, batch     8 | loss: 53.7475738CurrentTrain: epoch  8, batch     9 | loss: 84.0424438CurrentTrain: epoch  8, batch    10 | loss: 60.4705312CurrentTrain: epoch  8, batch    11 | loss: 63.3449778CurrentTrain: epoch  8, batch    12 | loss: 64.4837955CurrentTrain: epoch  8, batch    13 | loss: 80.3710443CurrentTrain: epoch  8, batch    14 | loss: 63.4081244CurrentTrain: epoch  8, batch    15 | loss: 65.6637342CurrentTrain: epoch  8, batch    16 | loss: 82.4375181CurrentTrain: epoch  8, batch    17 | loss: 64.1479529CurrentTrain: epoch  8, batch    18 | loss: 84.5279344CurrentTrain: epoch  8, batch    19 | loss: 52.1549773CurrentTrain: epoch  8, batch    20 | loss: 62.6967704CurrentTrain: epoch  8, batch    21 | loss: 62.4898962CurrentTrain: epoch  8, batch    22 | loss: 80.4561071CurrentTrain: epoch  8, batch    23 | loss: 52.9544482CurrentTrain: epoch  8, batch    24 | loss: 53.0653290CurrentTrain: epoch  8, batch    25 | loss: 83.3334889CurrentTrain: epoch  8, batch    26 | loss: 49.2927966CurrentTrain: epoch  8, batch    27 | loss: 65.0537189CurrentTrain: epoch  8, batch    28 | loss: 51.3938315CurrentTrain: epoch  8, batch    29 | loss: 51.9608501CurrentTrain: epoch  8, batch    30 | loss: 86.2207526CurrentTrain: epoch  8, batch    31 | loss: 41.7817396CurrentTrain: epoch  8, batch    32 | loss: 177.6251925CurrentTrain: epoch  8, batch    33 | loss: 84.0899943CurrentTrain: epoch  8, batch    34 | loss: 82.7096894CurrentTrain: epoch  8, batch    35 | loss: 50.6572257CurrentTrain: epoch  8, batch    36 | loss: 80.8117532CurrentTrain: epoch  8, batch    37 | loss: 53.0909016CurrentTrain: epoch  8, batch    38 | loss: 63.2969562CurrentTrain: epoch  8, batch    39 | loss: 66.8171919CurrentTrain: epoch  8, batch    40 | loss: 66.0265175CurrentTrain: epoch  8, batch    41 | loss: 86.0632592CurrentTrain: epoch  8, batch    42 | loss: 65.4050650CurrentTrain: epoch  8, batch    43 | loss: 85.7392034CurrentTrain: epoch  8, batch    44 | loss: 115.3725656CurrentTrain: epoch  8, batch    45 | loss: 65.6355937CurrentTrain: epoch  8, batch    46 | loss: 51.5373137CurrentTrain: epoch  8, batch    47 | loss: 65.5404949CurrentTrain: epoch  8, batch    48 | loss: 50.8250331CurrentTrain: epoch  8, batch    49 | loss: 63.0965898CurrentTrain: epoch  8, batch    50 | loss: 82.4511194CurrentTrain: epoch  8, batch    51 | loss: 52.4223009CurrentTrain: epoch  8, batch    52 | loss: 65.4291760CurrentTrain: epoch  8, batch    53 | loss: 66.7245030CurrentTrain: epoch  8, batch    54 | loss: 82.6030267CurrentTrain: epoch  8, batch    55 | loss: 80.9317828CurrentTrain: epoch  8, batch    56 | loss: 52.1988291CurrentTrain: epoch  8, batch    57 | loss: 50.8332330CurrentTrain: epoch  8, batch    58 | loss: 66.5517320CurrentTrain: epoch  8, batch    59 | loss: 42.3181252CurrentTrain: epoch  8, batch    60 | loss: 115.8454214CurrentTrain: epoch  8, batch    61 | loss: 84.1647846CurrentTrain: epoch  8, batch    62 | loss: 42.1550164CurrentTrain: epoch  8, batch    63 | loss: 70.6577084CurrentTrain: epoch  8, batch    64 | loss: 66.6691538CurrentTrain: epoch  8, batch    65 | loss: 63.2059089CurrentTrain: epoch  8, batch    66 | loss: 59.9137135CurrentTrain: epoch  8, batch    67 | loss: 81.6720517CurrentTrain: epoch  8, batch    68 | loss: 79.7019420CurrentTrain: epoch  8, batch    69 | loss: 65.6564889CurrentTrain: epoch  8, batch    70 | loss: 82.4414038CurrentTrain: epoch  8, batch    71 | loss: 112.5668560CurrentTrain: epoch  8, batch    72 | loss: 65.9444379CurrentTrain: epoch  8, batch    73 | loss: 63.1736834CurrentTrain: epoch  8, batch    74 | loss: 84.1535080CurrentTrain: epoch  8, batch    75 | loss: 64.4687272CurrentTrain: epoch  8, batch    76 | loss: 43.8606963CurrentTrain: epoch  8, batch    77 | loss: 113.4290387CurrentTrain: epoch  8, batch    78 | loss: 49.0214879CurrentTrain: epoch  8, batch    79 | loss: 54.0175989CurrentTrain: epoch  8, batch    80 | loss: 81.4773107CurrentTrain: epoch  8, batch    81 | loss: 65.6078149CurrentTrain: epoch  8, batch    82 | loss: 54.5331807CurrentTrain: epoch  8, batch    83 | loss: 52.0722705CurrentTrain: epoch  8, batch    84 | loss: 115.4008393CurrentTrain: epoch  8, batch    85 | loss: 111.2072944CurrentTrain: epoch  8, batch    86 | loss: 84.2446460CurrentTrain: epoch  8, batch    87 | loss: 64.5876473CurrentTrain: epoch  8, batch    88 | loss: 38.7645241CurrentTrain: epoch  8, batch    89 | loss: 62.9769151CurrentTrain: epoch  8, batch    90 | loss: 63.9314482CurrentTrain: epoch  8, batch    91 | loss: 67.4183122CurrentTrain: epoch  8, batch    92 | loss: 64.2422717CurrentTrain: epoch  8, batch    93 | loss: 64.1899247CurrentTrain: epoch  8, batch    94 | loss: 82.0117823CurrentTrain: epoch  8, batch    95 | loss: 96.3323903CurrentTrain: epoch  9, batch     0 | loss: 53.1661122CurrentTrain: epoch  9, batch     1 | loss: 111.2092878CurrentTrain: epoch  9, batch     2 | loss: 62.0497025CurrentTrain: epoch  9, batch     3 | loss: 52.6944597CurrentTrain: epoch  9, batch     4 | loss: 64.8160405CurrentTrain: epoch  9, batch     5 | loss: 40.8866854CurrentTrain: epoch  9, batch     6 | loss: 82.6556334CurrentTrain: epoch  9, batch     7 | loss: 63.4427387CurrentTrain: epoch  9, batch     8 | loss: 82.5093164CurrentTrain: epoch  9, batch     9 | loss: 51.6143436CurrentTrain: epoch  9, batch    10 | loss: 86.6913033CurrentTrain: epoch  9, batch    11 | loss: 52.9495641CurrentTrain: epoch  9, batch    12 | loss: 82.5256289CurrentTrain: epoch  9, batch    13 | loss: 84.0658135CurrentTrain: epoch  9, batch    14 | loss: 83.2233963CurrentTrain: epoch  9, batch    15 | loss: 62.0791286CurrentTrain: epoch  9, batch    16 | loss: 117.4878113CurrentTrain: epoch  9, batch    17 | loss: 65.5437330CurrentTrain: epoch  9, batch    18 | loss: 63.4563857CurrentTrain: epoch  9, batch    19 | loss: 66.6850274CurrentTrain: epoch  9, batch    20 | loss: 49.9001506CurrentTrain: epoch  9, batch    21 | loss: 84.4329830CurrentTrain: epoch  9, batch    22 | loss: 53.0032890CurrentTrain: epoch  9, batch    23 | loss: 52.7558121CurrentTrain: epoch  9, batch    24 | loss: 115.1792026CurrentTrain: epoch  9, batch    25 | loss: 82.6806994CurrentTrain: epoch  9, batch    26 | loss: 47.3850894CurrentTrain: epoch  9, batch    27 | loss: 111.0258331CurrentTrain: epoch  9, batch    28 | loss: 51.3785508CurrentTrain: epoch  9, batch    29 | loss: 53.6076697CurrentTrain: epoch  9, batch    30 | loss: 51.7853897CurrentTrain: epoch  9, batch    31 | loss: 63.2969252CurrentTrain: epoch  9, batch    32 | loss: 84.3656543CurrentTrain: epoch  9, batch    33 | loss: 55.9267110CurrentTrain: epoch  9, batch    34 | loss: 45.5368858CurrentTrain: epoch  9, batch    35 | loss: 113.0271326CurrentTrain: epoch  9, batch    36 | loss: 63.0896538CurrentTrain: epoch  9, batch    37 | loss: 69.7534760CurrentTrain: epoch  9, batch    38 | loss: 64.0310087CurrentTrain: epoch  9, batch    39 | loss: 52.9557648CurrentTrain: epoch  9, batch    40 | loss: 49.3731634CurrentTrain: epoch  9, batch    41 | loss: 66.7027647CurrentTrain: epoch  9, batch    42 | loss: 49.8218776CurrentTrain: epoch  9, batch    43 | loss: 50.9001187CurrentTrain: epoch  9, batch    44 | loss: 113.3905435CurrentTrain: epoch  9, batch    45 | loss: 54.4894678CurrentTrain: epoch  9, batch    46 | loss: 65.5077170CurrentTrain: epoch  9, batch    47 | loss: 84.1464108CurrentTrain: epoch  9, batch    48 | loss: 63.0766391CurrentTrain: epoch  9, batch    49 | loss: 53.5123306CurrentTrain: epoch  9, batch    50 | loss: 61.9160256CurrentTrain: epoch  9, batch    51 | loss: 67.4002269CurrentTrain: epoch  9, batch    52 | loss: 57.5174511CurrentTrain: epoch  9, batch    53 | loss: 65.4978614CurrentTrain: epoch  9, batch    54 | loss: 52.1314351CurrentTrain: epoch  9, batch    55 | loss: 51.9687608CurrentTrain: epoch  9, batch    56 | loss: 65.4448734CurrentTrain: epoch  9, batch    57 | loss: 40.5073330CurrentTrain: epoch  9, batch    58 | loss: 79.7582742CurrentTrain: epoch  9, batch    59 | loss: 84.9395025CurrentTrain: epoch  9, batch    60 | loss: 60.9973118CurrentTrain: epoch  9, batch    61 | loss: 43.7406799CurrentTrain: epoch  9, batch    62 | loss: 82.9135155CurrentTrain: epoch  9, batch    63 | loss: 48.2108209CurrentTrain: epoch  9, batch    64 | loss: 43.4916412CurrentTrain: epoch  9, batch    65 | loss: 84.6494223CurrentTrain: epoch  9, batch    66 | loss: 83.0577498CurrentTrain: epoch  9, batch    67 | loss: 63.6198840CurrentTrain: epoch  9, batch    68 | loss: 117.5201649CurrentTrain: epoch  9, batch    69 | loss: 84.3899270CurrentTrain: epoch  9, batch    70 | loss: 65.9937149CurrentTrain: epoch  9, batch    71 | loss: 82.7389121CurrentTrain: epoch  9, batch    72 | loss: 50.3391050CurrentTrain: epoch  9, batch    73 | loss: 84.0394608CurrentTrain: epoch  9, batch    74 | loss: 117.4842344CurrentTrain: epoch  9, batch    75 | loss: 90.7293808CurrentTrain: epoch  9, batch    76 | loss: 82.3408594CurrentTrain: epoch  9, batch    77 | loss: 51.8558953CurrentTrain: epoch  9, batch    78 | loss: 111.6926843CurrentTrain: epoch  9, batch    79 | loss: 64.2470636CurrentTrain: epoch  9, batch    80 | loss: 43.6037411CurrentTrain: epoch  9, batch    81 | loss: 41.4440015CurrentTrain: epoch  9, batch    82 | loss: 85.7652434CurrentTrain: epoch  9, batch    83 | loss: 118.0316406CurrentTrain: epoch  9, batch    84 | loss: 117.6387723CurrentTrain: epoch  9, batch    85 | loss: 52.0091024CurrentTrain: epoch  9, batch    86 | loss: 61.9457418CurrentTrain: epoch  9, batch    87 | loss: 52.8669887CurrentTrain: epoch  9, batch    88 | loss: 117.5303036CurrentTrain: epoch  9, batch    89 | loss: 44.1084410CurrentTrain: epoch  9, batch    90 | loss: 65.0352414CurrentTrain: epoch  9, batch    91 | loss: 62.9485690CurrentTrain: epoch  9, batch    92 | loss: 40.5655160CurrentTrain: epoch  9, batch    93 | loss: 53.6077249CurrentTrain: epoch  9, batch    94 | loss: 79.4253629CurrentTrain: epoch  9, batch    95 | loss: 57.4035342

F1 score per class: {32: 0.4507042253521127, 6: 0.6530612244897959, 19: 0.16216216216216217, 24: 0.71, 26: 0.8540540540540541, 29: 0.8246445497630331}
Micro-average F1 score: 0.6756238003838771
Weighted-average F1 score: 0.671440220830499
F1 score per class: {32: 0.47692307692307695, 6: 0.6728971962616822, 19: 0.14432989690721648, 24: 0.6460176991150443, 26: 0.896551724137931, 29: 0.7373271889400922}
Micro-average F1 score: 0.6327033689400164
Weighted-average F1 score: 0.6071965283517703
F1 score per class: {32: 0.47692307692307695, 6: 0.6728971962616822, 19: 0.15053763440860216, 24: 0.6488888888888888, 26: 0.896551724137931, 29: 0.7397260273972602}
Micro-average F1 score: 0.6359143327841845
Weighted-average F1 score: 0.6117695850858639

F1 score per class: {32: 0.4507042253521127, 6: 0.6530612244897959, 19: 0.16216216216216217, 24: 0.71, 26: 0.8540540540540541, 29: 0.8246445497630331}
Micro-average F1 score: 0.6756238003838771
Weighted-average F1 score: 0.671440220830499
F1 score per class: {32: 0.47692307692307695, 6: 0.6728971962616822, 19: 0.14432989690721648, 24: 0.6460176991150443, 26: 0.896551724137931, 29: 0.7373271889400922}
Micro-average F1 score: 0.6327033689400164
Weighted-average F1 score: 0.6071965283517703
F1 score per class: {32: 0.47692307692307695, 6: 0.6728971962616822, 19: 0.15053763440860216, 24: 0.6488888888888888, 26: 0.896551724137931, 29: 0.7397260273972602}
Micro-average F1 score: 0.6359143327841845
Weighted-average F1 score: 0.6117695850858639
cur_acc:  ['0.6756']
his_acc:  ['0.6756']
cur_acc des:  ['0.6327']
his_acc des:  ['0.6327']
cur_acc rrf:  ['0.6359']
his_acc rrf:  ['0.6359']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 78.8456479CurrentTrain: epoch  0, batch     1 | loss: 80.5312641CurrentTrain: epoch  0, batch     2 | loss: 99.1241136CurrentTrain: epoch  0, batch     3 | loss: 79.3437978CurrentTrain: epoch  0, batch     4 | loss: 20.1452211CurrentTrain: epoch  1, batch     0 | loss: 93.9477164CurrentTrain: epoch  1, batch     1 | loss: 75.0197703CurrentTrain: epoch  1, batch     2 | loss: 93.1658747CurrentTrain: epoch  1, batch     3 | loss: 71.3890555CurrentTrain: epoch  1, batch     4 | loss: 21.0032236CurrentTrain: epoch  2, batch     0 | loss: 59.5673152CurrentTrain: epoch  2, batch     1 | loss: 124.4726632CurrentTrain: epoch  2, batch     2 | loss: 92.4875755CurrentTrain: epoch  2, batch     3 | loss: 90.0077902CurrentTrain: epoch  2, batch     4 | loss: 18.4938359CurrentTrain: epoch  3, batch     0 | loss: 92.4313932CurrentTrain: epoch  3, batch     1 | loss: 70.3737258CurrentTrain: epoch  3, batch     2 | loss: 90.8909543CurrentTrain: epoch  3, batch     3 | loss: 70.2660175CurrentTrain: epoch  3, batch     4 | loss: 30.0893203CurrentTrain: epoch  4, batch     0 | loss: 115.8977250CurrentTrain: epoch  4, batch     1 | loss: 60.4933514CurrentTrain: epoch  4, batch     2 | loss: 71.0052571CurrentTrain: epoch  4, batch     3 | loss: 69.6619233CurrentTrain: epoch  4, batch     4 | loss: 17.4104141CurrentTrain: epoch  5, batch     0 | loss: 67.5633771CurrentTrain: epoch  5, batch     1 | loss: 68.2111531CurrentTrain: epoch  5, batch     2 | loss: 68.7072844CurrentTrain: epoch  5, batch     3 | loss: 90.3357171CurrentTrain: epoch  5, batch     4 | loss: 28.6835427CurrentTrain: epoch  6, batch     0 | loss: 69.4511202CurrentTrain: epoch  6, batch     1 | loss: 88.3566254CurrentTrain: epoch  6, batch     2 | loss: 55.5898758CurrentTrain: epoch  6, batch     3 | loss: 85.6855618CurrentTrain: epoch  6, batch     4 | loss: 10.3953471CurrentTrain: epoch  7, batch     0 | loss: 67.1065151CurrentTrain: epoch  7, batch     1 | loss: 66.4198396CurrentTrain: epoch  7, batch     2 | loss: 66.0852503CurrentTrain: epoch  7, batch     3 | loss: 119.3288831CurrentTrain: epoch  7, batch     4 | loss: 15.0979197CurrentTrain: epoch  8, batch     0 | loss: 65.6485572CurrentTrain: epoch  8, batch     1 | loss: 84.9388097CurrentTrain: epoch  8, batch     2 | loss: 66.5986795CurrentTrain: epoch  8, batch     3 | loss: 82.3617819CurrentTrain: epoch  8, batch     4 | loss: 26.6499052CurrentTrain: epoch  9, batch     0 | loss: 113.4797202CurrentTrain: epoch  9, batch     1 | loss: 79.9506238CurrentTrain: epoch  9, batch     2 | loss: 85.0660052CurrentTrain: epoch  9, batch     3 | loss: 66.4648529CurrentTrain: epoch  9, batch     4 | loss: 14.9084234
MemoryTrain:  epoch  0, batch     0 | loss: 0.6728208MemoryTrain:  epoch  1, batch     0 | loss: 0.6269651MemoryTrain:  epoch  2, batch     0 | loss: 0.5106787MemoryTrain:  epoch  3, batch     0 | loss: 0.3786903MemoryTrain:  epoch  4, batch     0 | loss: 0.2776138MemoryTrain:  epoch  5, batch     0 | loss: 0.2449145MemoryTrain:  epoch  6, batch     0 | loss: 0.1538477MemoryTrain:  epoch  7, batch     0 | loss: 0.1487623MemoryTrain:  epoch  8, batch     0 | loss: 0.0835678MemoryTrain:  epoch  9, batch     0 | loss: 0.0758314

F1 score per class: {32: 0.34285714285714286, 2: 0.0, 6: 0.2926829268292683, 39: 0.5, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.06666666666666667, 26: 0.0, 28: 0.0, 29: 0.19047619047619047}
Micro-average F1 score: 0.3274336283185841
Weighted-average F1 score: 0.28308235258931375
F1 score per class: {32: 0.1927710843373494, 2: 0.0, 6: 0.47474747474747475, 39: 0.44651162790697674, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.16666666666666666, 26: 0.0, 28: 0.0, 29: 0.22727272727272727}
Micro-average F1 score: 0.3183098591549296
Weighted-average F1 score: 0.2715294667155538
F1 score per class: {32: 0.1951219512195122, 2: 0.0, 6: 0.47959183673469385, 39: 0.47161572052401746, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.1724137931034483, 26: 0.0, 28: 0.0, 29: 0.3}
Micro-average F1 score: 0.3463203463203463
Weighted-average F1 score: 0.30231843546232484

F1 score per class: {32: 0.25, 2: 0.42105263157894735, 6: 0.2553191489361702, 39: 0.3164983164983165, 11: 0.6542056074766355, 12: 0.16393442622950818, 19: 0.6990291262135923, 24: 0.05405405405405406, 26: 0.8844221105527639, 28: 0.7889908256880734, 29: 0.13793103448275862}
Micro-average F1 score: 0.5244956772334294
Weighted-average F1 score: 0.5045568802659939
F1 score per class: {32: 0.09696969696969697, 2: 0.38323353293413176, 6: 0.37751004016064255, 39: 0.25806451612903225, 11: 0.56, 12: 0.13008130081300814, 19: 0.6515837104072398, 24: 0.12345679012345678, 26: 0.8611111111111112, 28: 0.7413793103448276, 29: 0.11494252873563218}
Micro-average F1 score: 0.4369747899159664
Weighted-average F1 score: 0.3989232245663916
F1 score per class: {32: 0.09815950920245399, 2: 0.39274924471299094, 6: 0.373015873015873, 39: 0.25653206650831356, 11: 0.6147859922178989, 12: 0.15730337078651685, 19: 0.676056338028169, 24: 0.13333333333333333, 26: 0.8708133971291866, 28: 0.7413793103448276, 29: 0.1643835616438356}
Micro-average F1 score: 0.44924406047516197
Weighted-average F1 score: 0.4086565497425885
cur_acc:  ['0.6756', '0.3274']
his_acc:  ['0.6756', '0.5245']
cur_acc des:  ['0.6327', '0.3183']
his_acc des:  ['0.6327', '0.4370']
cur_acc rrf:  ['0.6359', '0.3463']
his_acc rrf:  ['0.6359', '0.4492']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 71.6581246CurrentTrain: epoch  0, batch     1 | loss: 70.5486750CurrentTrain: epoch  0, batch     2 | loss: 67.4753541CurrentTrain: epoch  0, batch     3 | loss: 16.8762333CurrentTrain: epoch  1, batch     0 | loss: 78.4691019CurrentTrain: epoch  1, batch     1 | loss: 62.0324389CurrentTrain: epoch  1, batch     2 | loss: 71.6566654CurrentTrain: epoch  1, batch     3 | loss: 11.0851913CurrentTrain: epoch  2, batch     0 | loss: 90.5660151CurrentTrain: epoch  2, batch     1 | loss: 88.6525176CurrentTrain: epoch  2, batch     2 | loss: 53.2005580CurrentTrain: epoch  2, batch     3 | loss: 27.6734669CurrentTrain: epoch  3, batch     0 | loss: 69.1069758CurrentTrain: epoch  3, batch     1 | loss: 56.3217784CurrentTrain: epoch  3, batch     2 | loss: 54.4114432CurrentTrain: epoch  3, batch     3 | loss: 27.4845602CurrentTrain: epoch  4, batch     0 | loss: 85.1557177CurrentTrain: epoch  4, batch     1 | loss: 53.4844683CurrentTrain: epoch  4, batch     2 | loss: 86.3564516CurrentTrain: epoch  4, batch     3 | loss: 5.6741681CurrentTrain: epoch  5, batch     0 | loss: 53.4863223CurrentTrain: epoch  5, batch     1 | loss: 53.9157762CurrentTrain: epoch  5, batch     2 | loss: 66.0170986CurrentTrain: epoch  5, batch     3 | loss: 11.0331676CurrentTrain: epoch  6, batch     0 | loss: 51.6172428CurrentTrain: epoch  6, batch     1 | loss: 66.6940418CurrentTrain: epoch  6, batch     2 | loss: 54.3518136CurrentTrain: epoch  6, batch     3 | loss: 11.7203530CurrentTrain: epoch  7, batch     0 | loss: 115.2393212CurrentTrain: epoch  7, batch     1 | loss: 65.2060215CurrentTrain: epoch  7, batch     2 | loss: 48.1308704CurrentTrain: epoch  7, batch     3 | loss: 27.4646223CurrentTrain: epoch  8, batch     0 | loss: 114.5727827CurrentTrain: epoch  8, batch     1 | loss: 49.0636965CurrentTrain: epoch  8, batch     2 | loss: 52.3945205CurrentTrain: epoch  8, batch     3 | loss: 6.0903361CurrentTrain: epoch  9, batch     0 | loss: 50.0083175CurrentTrain: epoch  9, batch     1 | loss: 80.7391186CurrentTrain: epoch  9, batch     2 | loss: 63.3902277CurrentTrain: epoch  9, batch     3 | loss: 11.0724710
MemoryTrain:  epoch  0, batch     0 | loss: 0.6027766MemoryTrain:  epoch  1, batch     0 | loss: 0.3763436MemoryTrain:  epoch  2, batch     0 | loss: 0.3448809MemoryTrain:  epoch  3, batch     0 | loss: 0.3276588MemoryTrain:  epoch  4, batch     0 | loss: 0.2734467MemoryTrain:  epoch  5, batch     0 | loss: 0.2070983MemoryTrain:  epoch  6, batch     0 | loss: 0.1711061MemoryTrain:  epoch  7, batch     0 | loss: 0.1796985MemoryTrain:  epoch  8, batch     0 | loss: 0.1239440MemoryTrain:  epoch  9, batch     0 | loss: 0.1115540

F1 score per class: {32: 0.0, 6: 0.5714285714285714, 7: 0.8888888888888888, 40: 0.0, 9: 0.0, 12: 0.0, 19: 0.30303030303030304, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.3333333333333333}
Micro-average F1 score: 0.36879432624113473
Weighted-average F1 score: 0.3061052703909847
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.5714285714285714, 7: 0.8064516129032258, 40: 0.0, 9: 0.0, 11: 0.0, 39: 0.0, 12: 0.0, 19: 0.34146341463414637, 24: 0.0, 26: 0.0, 27: 0.36363636363636365, 28: 0.0, 29: 0.0, 31: 0.26373626373626374}
Micro-average F1 score: 0.31007751937984496
Weighted-average F1 score: 0.26767308481974617
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.5714285714285714, 7: 0.8064516129032258, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 39: 0.32558139534883723, 19: 0.0, 26: 0.2857142857142857, 27: 0.0, 29: 0.0, 31: 0.2742857142857143}
Micro-average F1 score: 0.31466666666666665
Weighted-average F1 score: 0.27095670469341476

F1 score per class: {32: 0.24390243902439024, 2: 0.23333333333333334, 6: 0.04, 7: 0.8888888888888888, 40: 0.3246753246753247, 39: 0.29239766081871343, 11: 0.5403225806451613, 12: 0.14285714285714285, 9: 0.7021276595744681, 19: 0.11904761904761904, 24: 0.08333333333333333, 26: 0.8691099476439791, 27: 0.0, 28: 0.7839195979899497, 29: 0.0, 31: 0.2441860465116279}
Micro-average F1 score: 0.4548852108916177
Weighted-average F1 score: 0.42505958406768624
F1 score per class: {32: 0.09523809523809523, 2: 0.26136363636363635, 6: 0.038834951456310676, 39: 0.7936507936507936, 7: 0.35526315789473684, 40: 0.2727272727272727, 11: 0.5032258064516129, 12: 0.08, 9: 0.663594470046083, 19: 0.13861386138613863, 24: 0.13513513513513514, 26: 0.8544600938967136, 27: 0.0547945205479452, 28: 0.6896551724137931, 29: 0.12987012987012986, 31: 0.16326530612244897}
Micro-average F1 score: 0.3788546255506608
Weighted-average F1 score: 0.3402002593226384
F1 score per class: {32: 0.09230769230769231, 2: 0.27807486631016043, 6: 0.04, 7: 0.7936507936507936, 39: 0.36363636363636365, 40: 0.2987012987012987, 11: 0.5387453874538746, 12: 0.12244897959183673, 9: 0.6728971962616822, 19: 0.1111111111111111, 24: 0.16129032258064516, 26: 0.8640776699029126, 27: 0.045454545454545456, 28: 0.7022222222222222, 29: 0.11538461538461539, 31: 0.15894039735099338}
Micro-average F1 score: 0.3907435508345979
Weighted-average F1 score: 0.3492489393082303
cur_acc:  ['0.6756', '0.3274', '0.3688']
his_acc:  ['0.6756', '0.5245', '0.4549']
cur_acc des:  ['0.6327', '0.3183', '0.3101']
his_acc des:  ['0.6327', '0.4370', '0.3789']
cur_acc rrf:  ['0.6359', '0.3463', '0.3147']
his_acc rrf:  ['0.6359', '0.4492', '0.3907']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 67.7637248CurrentTrain: epoch  0, batch     1 | loss: 81.5072731CurrentTrain: epoch  0, batch     2 | loss: 70.6089080CurrentTrain: epoch  0, batch     3 | loss: 54.3376324CurrentTrain: epoch  1, batch     0 | loss: 62.8022442CurrentTrain: epoch  1, batch     1 | loss: 131.4888352CurrentTrain: epoch  1, batch     2 | loss: 58.8182626CurrentTrain: epoch  1, batch     3 | loss: 51.4683642CurrentTrain: epoch  2, batch     0 | loss: 74.7388418CurrentTrain: epoch  2, batch     1 | loss: 69.0536400CurrentTrain: epoch  2, batch     2 | loss: 74.3393974CurrentTrain: epoch  2, batch     3 | loss: 47.5592500CurrentTrain: epoch  3, batch     0 | loss: 52.4624003CurrentTrain: epoch  3, batch     1 | loss: 115.0569782CurrentTrain: epoch  3, batch     2 | loss: 71.2814046CurrentTrain: epoch  3, batch     3 | loss: 80.4973207CurrentTrain: epoch  4, batch     0 | loss: 70.7779669CurrentTrain: epoch  4, batch     1 | loss: 53.8035007CurrentTrain: epoch  4, batch     2 | loss: 58.0933152CurrentTrain: epoch  4, batch     3 | loss: 45.1377484CurrentTrain: epoch  5, batch     0 | loss: 56.8338244CurrentTrain: epoch  5, batch     1 | loss: 114.6552340CurrentTrain: epoch  5, batch     2 | loss: 55.6566796CurrentTrain: epoch  5, batch     3 | loss: 56.5285191CurrentTrain: epoch  6, batch     0 | loss: 53.1870664CurrentTrain: epoch  6, batch     1 | loss: 63.8266922CurrentTrain: epoch  6, batch     2 | loss: 86.1659863CurrentTrain: epoch  6, batch     3 | loss: 122.4501457CurrentTrain: epoch  7, batch     0 | loss: 52.4445296CurrentTrain: epoch  7, batch     1 | loss: 84.2134925CurrentTrain: epoch  7, batch     2 | loss: 65.5682388CurrentTrain: epoch  7, batch     3 | loss: 59.0978418CurrentTrain: epoch  8, batch     0 | loss: 64.5386023CurrentTrain: epoch  8, batch     1 | loss: 53.4685507CurrentTrain: epoch  8, batch     2 | loss: 54.3269861CurrentTrain: epoch  8, batch     3 | loss: 77.1633197CurrentTrain: epoch  9, batch     0 | loss: 82.3589321CurrentTrain: epoch  9, batch     1 | loss: 64.3499617CurrentTrain: epoch  9, batch     2 | loss: 65.6123548CurrentTrain: epoch  9, batch     3 | loss: 42.9808003
MemoryTrain:  epoch  0, batch     0 | loss: 0.5040861MemoryTrain:  epoch  1, batch     0 | loss: 0.4469034MemoryTrain:  epoch  2, batch     0 | loss: 0.3470737MemoryTrain:  epoch  3, batch     0 | loss: 0.2880122MemoryTrain:  epoch  4, batch     0 | loss: 0.1920123MemoryTrain:  epoch  5, batch     0 | loss: 0.1492785MemoryTrain:  epoch  6, batch     0 | loss: 0.1414464MemoryTrain:  epoch  7, batch     0 | loss: 0.1038267MemoryTrain:  epoch  8, batch     0 | loss: 0.0916076MemoryTrain:  epoch  9, batch     0 | loss: 0.0774613

F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.7777777777777778, 19: 0.0, 24: 0.0, 25: 0.3939393939393939, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.2828282828282828, 37: 0.37267080745341613, 38: 0.34782608695652173, 40: 0.0}
Micro-average F1 score: 0.30062630480167013
Weighted-average F1 score: 0.24897033248612854
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.5217391304347826, 19: 0.0, 24: 0.0, 25: 0.7294117647058823, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5806451612903226, 37: 0.3804878048780488, 38: 0.46153846153846156, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3480392156862745
Weighted-average F1 score: 0.28534625770630895
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.6, 19: 0.0, 24: 0.0, 25: 0.7294117647058823, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5098039215686274, 37: 0.3696682464454976, 38: 0.4788732394366197, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3487450462351387
Weighted-average F1 score: 0.2850396452236311

F1 score per class: {2: 0.24489795918367346, 6: 0.2736842105263158, 7: 0.03361344537815126, 9: 0.8571428571428571, 11: 0.0847457627118644, 12: 0.1276595744680851, 15: 0.5185185185185185, 19: 0.4126984126984127, 24: 0.15384615384615385, 25: 0.3939393939393939, 26: 0.6731707317073171, 27: 0.16901408450704225, 28: 0.20689655172413793, 29: 0.8631578947368421, 31: 0.0, 32: 0.7384615384615385, 35: 0.18543046357615894, 37: 0.12072434607645875, 38: 0.13333333333333333, 39: 0.0, 40: 0.3586206896551724}
Micro-average F1 score: 0.33867276887871856
Weighted-average F1 score: 0.29929280183932544
F1 score per class: {2: 0.0718562874251497, 6: 0.28085106382978725, 7: 0.028169014084507043, 9: 0.7246376811594203, 11: 0.27932960893854747, 12: 0.26380368098159507, 15: 0.34285714285714286, 19: 0.4083044982698962, 24: 0.07692307692307693, 25: 0.7294117647058823, 26: 0.6486486486486487, 27: 0.10526315789473684, 28: 0.06779661016949153, 29: 0.827906976744186, 31: 0.05405405405405406, 32: 0.6307692307692307, 35: 0.28753993610223644, 37: 0.11048158640226628, 38: 0.14482758620689656, 39: 0.1891891891891892, 40: 0.2833333333333333}
Micro-average F1 score: 0.2996927440321437
Weighted-average F1 score: 0.25974741517685834
F1 score per class: {2: 0.07272727272727272, 6: 0.26720647773279355, 7: 0.032520325203252036, 9: 0.7936507936507936, 11: 0.22929936305732485, 12: 0.28662420382165604, 15: 0.34285714285714286, 19: 0.462882096069869, 24: 0.1111111111111111, 25: 0.7294117647058823, 26: 0.6486486486486487, 27: 0.09836065573770492, 28: 0.09523809523809523, 29: 0.8446601941747572, 31: 0.08, 32: 0.6694214876033058, 35: 0.2653061224489796, 37: 0.10512129380053908, 38: 0.12781954887218044, 39: 0.16666666666666666, 40: 0.27615062761506276}
Micro-average F1 score: 0.30440251572327043
Weighted-average F1 score: 0.260122575367119
cur_acc:  ['0.6756', '0.3274', '0.3688', '0.3006']
his_acc:  ['0.6756', '0.5245', '0.4549', '0.3387']
cur_acc des:  ['0.6327', '0.3183', '0.3101', '0.3480']
his_acc des:  ['0.6327', '0.4370', '0.3789', '0.2997']
cur_acc rrf:  ['0.6359', '0.3463', '0.3147', '0.3487']
his_acc rrf:  ['0.6359', '0.4492', '0.3907', '0.3044']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 105.2891212CurrentTrain: epoch  0, batch     1 | loss: 107.5871837CurrentTrain: epoch  0, batch     2 | loss: 76.0080280CurrentTrain: epoch  0, batch     3 | loss: 77.4700756CurrentTrain: epoch  0, batch     4 | loss: 44.2574777CurrentTrain: epoch  1, batch     0 | loss: 67.8279570CurrentTrain: epoch  1, batch     1 | loss: 74.5887770CurrentTrain: epoch  1, batch     2 | loss: 69.2214804CurrentTrain: epoch  1, batch     3 | loss: 120.8907581CurrentTrain: epoch  1, batch     4 | loss: 69.5671993CurrentTrain: epoch  2, batch     0 | loss: 117.1939296CurrentTrain: epoch  2, batch     1 | loss: 68.6864623CurrentTrain: epoch  2, batch     2 | loss: 123.3552261CurrentTrain: epoch  2, batch     3 | loss: 56.6714591CurrentTrain: epoch  2, batch     4 | loss: 52.4793292CurrentTrain: epoch  3, batch     0 | loss: 70.1885272CurrentTrain: epoch  3, batch     1 | loss: 69.9490030CurrentTrain: epoch  3, batch     2 | loss: 85.6846968CurrentTrain: epoch  3, batch     3 | loss: 69.2771446CurrentTrain: epoch  3, batch     4 | loss: 51.3102215CurrentTrain: epoch  4, batch     0 | loss: 64.6790935CurrentTrain: epoch  4, batch     1 | loss: 118.2928154CurrentTrain: epoch  4, batch     2 | loss: 69.5268081CurrentTrain: epoch  4, batch     3 | loss: 55.7514952CurrentTrain: epoch  4, batch     4 | loss: 213.9781176CurrentTrain: epoch  5, batch     0 | loss: 83.6653554CurrentTrain: epoch  5, batch     1 | loss: 66.4292298CurrentTrain: epoch  5, batch     2 | loss: 87.6149269CurrentTrain: epoch  5, batch     3 | loss: 66.4295415CurrentTrain: epoch  5, batch     4 | loss: 106.0094781CurrentTrain: epoch  6, batch     0 | loss: 113.9760232CurrentTrain: epoch  6, batch     1 | loss: 68.7636451CurrentTrain: epoch  6, batch     2 | loss: 84.7125367CurrentTrain: epoch  6, batch     3 | loss: 83.5924270CurrentTrain: epoch  6, batch     4 | loss: 63.2784454CurrentTrain: epoch  7, batch     0 | loss: 51.8841435CurrentTrain: epoch  7, batch     1 | loss: 66.7554887CurrentTrain: epoch  7, batch     2 | loss: 115.8648437CurrentTrain: epoch  7, batch     3 | loss: 85.9071523CurrentTrain: epoch  7, batch     4 | loss: 103.2644294CurrentTrain: epoch  8, batch     0 | loss: 113.9178670CurrentTrain: epoch  8, batch     1 | loss: 65.6450877CurrentTrain: epoch  8, batch     2 | loss: 66.0419597CurrentTrain: epoch  8, batch     3 | loss: 82.0896653CurrentTrain: epoch  8, batch     4 | loss: 63.8689507CurrentTrain: epoch  9, batch     0 | loss: 63.9881251CurrentTrain: epoch  9, batch     1 | loss: 83.0043228CurrentTrain: epoch  9, batch     2 | loss: 52.8249445CurrentTrain: epoch  9, batch     3 | loss: 84.8164150CurrentTrain: epoch  9, batch     4 | loss: 103.3637638
MemoryTrain:  epoch  0, batch     0 | loss: 0.7061911MemoryTrain:  epoch  1, batch     0 | loss: 0.5411884MemoryTrain:  epoch  2, batch     0 | loss: 0.4995067MemoryTrain:  epoch  3, batch     0 | loss: 0.3861649MemoryTrain:  epoch  4, batch     0 | loss: 0.3386872MemoryTrain:  epoch  5, batch     0 | loss: 0.2724362MemoryTrain:  epoch  6, batch     0 | loss: 0.2380556MemoryTrain:  epoch  7, batch     0 | loss: 0.1706292MemoryTrain:  epoch  8, batch     0 | loss: 0.1455685MemoryTrain:  epoch  9, batch     0 | loss: 0.1298658

F1 score per class: {1: 0.12264150943396226, 2: 0.0, 3: 0.39316239316239315, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 14: 0.09174311926605505, 19: 0.0, 22: 0.5306122448979592, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.18181818181818182, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.20216606498194944
Weighted-average F1 score: 0.15374530648535698
F1 score per class: {1: 0.21052631578947367, 2: 0.0, 3: 0.3333333333333333, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.11920529801324503, 19: 0.0, 22: 0.4580152671755725, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.2682926829268293, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.15878787878787878
Weighted-average F1 score: 0.1204706625987573
F1 score per class: {1: 0.20618556701030927, 2: 0.0, 3: 0.32592592592592595, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.10526315789473684, 19: 0.0, 22: 0.46387832699619774, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.19718309859154928, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.1620253164556962
Weighted-average F1 score: 0.12646943428778643

F1 score per class: {1: 0.09420289855072464, 2: 0.22641509433962265, 3: 0.30666666666666664, 6: 0.25609756097560976, 7: 0.018691588785046728, 9: 0.9259259259259259, 11: 0.13138686131386862, 12: 0.24161073825503357, 14: 0.06369426751592357, 15: 0.5833333333333334, 19: 0.2158273381294964, 22: 0.47619047619047616, 24: 0.043478260869565216, 25: 0.3939393939393939, 26: 0.6728971962616822, 27: 0.024390243902439025, 28: 0.0, 29: 0.7884615384615384, 31: 0.0, 32: 0.5, 34: 0.11320754716981132, 35: 0.10810810810810811, 37: 0.11877394636015326, 38: 0.07272727272727272, 39: 0.0, 40: 0.31932773109243695}
Micro-average F1 score: 0.27212020033388984
Weighted-average F1 score: 0.23731181732440967
F1 score per class: {1: 0.13761467889908258, 2: 0.075, 3: 0.2048780487804878, 6: 0.26804123711340205, 7: 0.028985507246376812, 9: 0.7352941176470589, 11: 0.2314540059347181, 12: 0.3022508038585209, 14: 0.08, 15: 0.36363636363636365, 19: 0.2631578947368421, 22: 0.410958904109589, 24: 0.07228915662650602, 25: 0.7228915662650602, 26: 0.6320346320346321, 27: 0.03076923076923077, 28: 0.05263157894736842, 29: 0.7457627118644068, 31: 0.04, 32: 0.5152838427947598, 34: 0.1375, 35: 0.17515274949083504, 37: 0.07190412782956059, 38: 0.13452914798206278, 39: 0.17391304347826086, 40: 0.35051546391752575}
Micro-average F1 score: 0.2465215840171245
Weighted-average F1 score: 0.21511642214070775
F1 score per class: {1: 0.13452914798206278, 2: 0.07547169811320754, 3: 0.21052631578947367, 6: 0.2966507177033493, 7: 0.032, 9: 0.8064516129032258, 11: 0.23107569721115537, 12: 0.2827586206896552, 14: 0.07017543859649122, 15: 0.46153846153846156, 19: 0.28402366863905326, 22: 0.4107744107744108, 24: 0.057971014492753624, 25: 0.7228915662650602, 26: 0.6431718061674009, 27: 0.03076923076923077, 28: 0.19047619047619047, 29: 0.7543859649122807, 31: 0.0, 32: 0.5395348837209303, 34: 0.11023622047244094, 35: 0.18571428571428572, 37: 0.07239819004524888, 38: 0.10344827586206896, 39: 0.15384615384615385, 40: 0.37433155080213903}
Micro-average F1 score: 0.24944154877140728
Weighted-average F1 score: 0.214643886670549
cur_acc:  ['0.6756', '0.3274', '0.3688', '0.3006', '0.2022']
his_acc:  ['0.6756', '0.5245', '0.4549', '0.3387', '0.2721']
cur_acc des:  ['0.6327', '0.3183', '0.3101', '0.3480', '0.1588']
his_acc des:  ['0.6327', '0.4370', '0.3789', '0.2997', '0.2465']
cur_acc rrf:  ['0.6359', '0.3463', '0.3147', '0.3487', '0.1620']
his_acc rrf:  ['0.6359', '0.4492', '0.3907', '0.3044', '0.2494']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 98.8377085CurrentTrain: epoch  0, batch     1 | loss: 79.9880429CurrentTrain: epoch  0, batch     2 | loss: 75.8469467CurrentTrain: epoch  0, batch     3 | loss: 108.1692541CurrentTrain: epoch  1, batch     0 | loss: 100.2922975CurrentTrain: epoch  1, batch     1 | loss: 69.4439769CurrentTrain: epoch  1, batch     2 | loss: 58.2839466CurrentTrain: epoch  1, batch     3 | loss: 48.9713570CurrentTrain: epoch  2, batch     0 | loss: 66.5195809CurrentTrain: epoch  2, batch     1 | loss: 59.0097343CurrentTrain: epoch  2, batch     2 | loss: 72.5598052CurrentTrain: epoch  2, batch     3 | loss: 70.8170926CurrentTrain: epoch  3, batch     0 | loss: 74.0073739CurrentTrain: epoch  3, batch     1 | loss: 54.4657262CurrentTrain: epoch  3, batch     2 | loss: 70.4914155CurrentTrain: epoch  3, batch     3 | loss: 52.1032770CurrentTrain: epoch  4, batch     0 | loss: 83.7876101CurrentTrain: epoch  4, batch     1 | loss: 70.4258511CurrentTrain: epoch  4, batch     2 | loss: 68.1684099CurrentTrain: epoch  4, batch     3 | loss: 53.4932793CurrentTrain: epoch  5, batch     0 | loss: 86.9762497CurrentTrain: epoch  5, batch     1 | loss: 82.3635254CurrentTrain: epoch  5, batch     2 | loss: 63.3771202CurrentTrain: epoch  5, batch     3 | loss: 149.2803533CurrentTrain: epoch  6, batch     0 | loss: 84.6322708CurrentTrain: epoch  6, batch     1 | loss: 54.0094506CurrentTrain: epoch  6, batch     2 | loss: 115.1512653CurrentTrain: epoch  6, batch     3 | loss: 67.4186876CurrentTrain: epoch  7, batch     0 | loss: 64.9265253CurrentTrain: epoch  7, batch     1 | loss: 62.7335058CurrentTrain: epoch  7, batch     2 | loss: 87.7485533CurrentTrain: epoch  7, batch     3 | loss: 53.2135711CurrentTrain: epoch  8, batch     0 | loss: 67.5050570CurrentTrain: epoch  8, batch     1 | loss: 66.9353295CurrentTrain: epoch  8, batch     2 | loss: 51.6507039CurrentTrain: epoch  8, batch     3 | loss: 66.4170088CurrentTrain: epoch  9, batch     0 | loss: 52.6409982CurrentTrain: epoch  9, batch     1 | loss: 66.2516948CurrentTrain: epoch  9, batch     2 | loss: 64.8238075CurrentTrain: epoch  9, batch     3 | loss: 66.4305458
MemoryTrain:  epoch  0, batch     0 | loss: 0.4914033MemoryTrain:  epoch  1, batch     0 | loss: 0.3932204MemoryTrain:  epoch  2, batch     0 | loss: 0.3054005MemoryTrain:  epoch  3, batch     0 | loss: 0.2373377MemoryTrain:  epoch  4, batch     0 | loss: 0.1958104MemoryTrain:  epoch  5, batch     0 | loss: 0.1590518MemoryTrain:  epoch  6, batch     0 | loss: 0.1399979MemoryTrain:  epoch  7, batch     0 | loss: 0.1164373MemoryTrain:  epoch  8, batch     0 | loss: 0.1189185MemoryTrain:  epoch  9, batch     0 | loss: 0.1014505

F1 score per class: {0: 0.8717948717948718, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9795918367346939, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 13: 0.03773584905660377, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.23880597014925373, 22: 0.0, 23: 0.7422680412371134, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.5072463768115942
Weighted-average F1 score: 0.3772471144937577
F1 score per class: {0: 0.6491228070175439, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.964824120603015, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.09375, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.34615384615384615, 22: 0.0, 23: 0.6470588235294118, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3899895724713243
Weighted-average F1 score: 0.2867236267163705
F1 score per class: {0: 0.6486486486486487, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9847715736040609, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.08108108108108109, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.34615384615384615, 22: 0.0, 23: 0.6868686868686869, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.4078091106290672
Weighted-average F1 score: 0.29929321897520084

F1 score per class: {0: 0.3300970873786408, 1: 0.08021390374331551, 2: 0.08536585365853659, 3: 0.3523809523809524, 4: 0.96, 6: 0.271356783919598, 7: 0.0, 9: 0.8333333333333334, 11: 0.07692307692307693, 12: 0.16091954022988506, 13: 0.006688963210702341, 14: 0.057971014492753624, 15: 0.48, 19: 0.3482142857142857, 21: 0.08247422680412371, 22: 0.550561797752809, 23: 0.6728971962616822, 24: 0.05714285714285714, 25: 0.5866666666666667, 26: 0.6540284360189573, 27: 0.05454545454545454, 28: 0.0, 29: 0.7609756097560976, 31: 0.03508771929824561, 32: 0.4444444444444444, 34: 0.10714285714285714, 35: 0.11560693641618497, 37: 0.10365853658536585, 38: 0.23728813559322035, 39: 0.0, 40: 0.34408602150537637}
Micro-average F1 score: 0.2799285288862418
Weighted-average F1 score: 0.23323559796542298
F1 score per class: {0: 0.19680851063829788, 1: 0.09633911368015415, 2: 0.06086956521739131, 3: 0.17827298050139276, 4: 0.9365853658536586, 6: 0.2727272727272727, 7: 0.03508771929824561, 9: 0.704225352112676, 11: 0.2689655172413793, 12: 0.26700251889168763, 13: 0.01948051948051948, 14: 0.07650273224043716, 15: 0.36363636363636365, 19: 0.32132963988919666, 21: 0.10227272727272728, 22: 0.39378238341968913, 23: 0.5196850393700787, 24: 0.06666666666666667, 25: 0.7126436781609196, 26: 0.6278026905829597, 27: 0.07017543859649122, 28: 0.10256410256410256, 29: 0.710204081632653, 31: 0.02877697841726619, 32: 0.4641350210970464, 34: 0.1044776119402985, 35: 0.1634980988593156, 37: 0.08967391304347826, 38: 0.17687074829931973, 39: 0.21052631578947367, 40: 0.3142857142857143}
Micro-average F1 score: 0.2484094052558783
Weighted-average F1 score: 0.21423777430815732
F1 score per class: {0: 0.192, 1: 0.10384615384615385, 2: 0.058091286307053944, 3: 0.20454545454545456, 4: 0.9509803921568627, 6: 0.2696629213483146, 7: 0.03508771929824561, 9: 0.7575757575757576, 11: 0.2823529411764706, 12: 0.2751322751322751, 13: 0.017341040462427744, 14: 0.0718562874251497, 15: 0.36363636363636365, 19: 0.3561643835616438, 21: 0.0975609756097561, 22: 0.4342857142857143, 23: 0.5573770491803278, 24: 0.12121212121212122, 25: 0.7126436781609196, 26: 0.6278026905829597, 27: 0.06349206349206349, 28: 0.2, 29: 0.7345132743362832, 31: 0.0425531914893617, 32: 0.4827586206896552, 34: 0.11347517730496454, 35: 0.1477832512315271, 37: 0.0800942285041225, 38: 0.20967741935483872, 39: 0.22857142857142856, 40: 0.2972972972972973}
Micro-average F1 score: 0.2502159516268356
Weighted-average F1 score: 0.21087258393415173
cur_acc:  ['0.6756', '0.3274', '0.3688', '0.3006', '0.2022', '0.5072']
his_acc:  ['0.6756', '0.5245', '0.4549', '0.3387', '0.2721', '0.2799']
cur_acc des:  ['0.6327', '0.3183', '0.3101', '0.3480', '0.1588', '0.3900']
his_acc des:  ['0.6327', '0.4370', '0.3789', '0.2997', '0.2465', '0.2484']
cur_acc rrf:  ['0.6359', '0.3463', '0.3147', '0.3487', '0.1620', '0.4078']
his_acc rrf:  ['0.6359', '0.4492', '0.3907', '0.3044', '0.2494', '0.2502']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 76.0054191CurrentTrain: epoch  0, batch     1 | loss: 80.0294390CurrentTrain: epoch  0, batch     2 | loss: 74.8264636CurrentTrain: epoch  0, batch     3 | loss: 61.3784612CurrentTrain: epoch  1, batch     0 | loss: 74.2887653CurrentTrain: epoch  1, batch     1 | loss: 60.3247062CurrentTrain: epoch  1, batch     2 | loss: 57.2393109CurrentTrain: epoch  1, batch     3 | loss: 54.5507068CurrentTrain: epoch  2, batch     0 | loss: 90.6639755CurrentTrain: epoch  2, batch     1 | loss: 66.1942705CurrentTrain: epoch  2, batch     2 | loss: 68.8882338CurrentTrain: epoch  2, batch     3 | loss: 34.0079074CurrentTrain: epoch  3, batch     0 | loss: 68.1545576CurrentTrain: epoch  3, batch     1 | loss: 70.4836816CurrentTrain: epoch  3, batch     2 | loss: 66.4800335CurrentTrain: epoch  3, batch     3 | loss: 39.2299091CurrentTrain: epoch  4, batch     0 | loss: 52.8997924CurrentTrain: epoch  4, batch     1 | loss: 82.7741926CurrentTrain: epoch  4, batch     2 | loss: 65.8459059CurrentTrain: epoch  4, batch     3 | loss: 72.6855435CurrentTrain: epoch  5, batch     0 | loss: 52.0462507CurrentTrain: epoch  5, batch     1 | loss: 85.9267200CurrentTrain: epoch  5, batch     2 | loss: 114.1696281CurrentTrain: epoch  5, batch     3 | loss: 48.7951248CurrentTrain: epoch  6, batch     0 | loss: 53.0993811CurrentTrain: epoch  6, batch     1 | loss: 84.2724592CurrentTrain: epoch  6, batch     2 | loss: 83.9886396CurrentTrain: epoch  6, batch     3 | loss: 37.2030808CurrentTrain: epoch  7, batch     0 | loss: 49.1625224CurrentTrain: epoch  7, batch     1 | loss: 64.2764165CurrentTrain: epoch  7, batch     2 | loss: 64.8949874CurrentTrain: epoch  7, batch     3 | loss: 231.9194878CurrentTrain: epoch  8, batch     0 | loss: 53.9371608CurrentTrain: epoch  8, batch     1 | loss: 114.1474098CurrentTrain: epoch  8, batch     2 | loss: 51.1129471CurrentTrain: epoch  8, batch     3 | loss: 37.7074756CurrentTrain: epoch  9, batch     0 | loss: 50.0357977CurrentTrain: epoch  9, batch     1 | loss: 64.7296615CurrentTrain: epoch  9, batch     2 | loss: 108.5101364CurrentTrain: epoch  9, batch     3 | loss: 108.0862912
MemoryTrain:  epoch  0, batch     0 | loss: 0.2400341MemoryTrain:  epoch  1, batch     0 | loss: 0.2228941MemoryTrain:  epoch  2, batch     0 | loss: 0.1757689MemoryTrain:  epoch  3, batch     0 | loss: 0.1470633MemoryTrain:  epoch  4, batch     0 | loss: 0.1233639MemoryTrain:  epoch  5, batch     0 | loss: 0.1029939MemoryTrain:  epoch  6, batch     0 | loss: 0.0896127MemoryTrain:  epoch  7, batch     0 | loss: 0.0767372MemoryTrain:  epoch  8, batch     0 | loss: 0.0758581MemoryTrain:  epoch  9, batch     0 | loss: 0.0681527

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.2909090909090909, 12: 0.0, 13: 0.0, 15: 0.0, 19: 0.0, 20: 0.6055045871559633, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.9473684210526315, 31: 0.0, 32: 0.0, 33: 0.2857142857142857, 34: 0.0, 35: 0.0, 36: 0.42105263157894735, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3508771929824561
Weighted-average F1 score: 0.2652722875988438
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 6: 0.0, 7: 0.0, 8: 0.41228070175438597, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.5170068027210885, 21: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.95, 31: 0.0, 32: 0.0, 33: 0.32, 34: 0.0, 35: 0.0, 36: 0.6035502958579881, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3261538461538461
Weighted-average F1 score: 0.2624765810273236
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.43636363636363634, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.5205479452054794, 21: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.9047619047619048, 31: 0.0, 32: 0.0, 33: 0.2962962962962963, 34: 0.0, 35: 0.0, 36: 0.5774647887323944, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3225806451612903
Weighted-average F1 score: 0.2546245277644865

F1 score per class: {0: 0.4166666666666667, 1: 0.10410958904109589, 2: 0.23076923076923078, 3: 0.25316455696202533, 4: 0.9547738693467337, 6: 0.30097087378640774, 7: 0.0, 8: 0.18181818181818182, 9: 0.847457627118644, 11: 0.043478260869565216, 12: 0.15757575757575756, 13: 0.011904761904761904, 14: 0.07547169811320754, 15: 0.6086956521739131, 19: 0.3018867924528302, 20: 0.3251231527093596, 21: 0.08121827411167512, 22: 0.5093167701863354, 23: 0.6464646464646465, 24: 0.0625, 25: 0.4383561643835616, 26: 0.638095238095238, 27: 0.05194805194805195, 28: 0.0, 29: 0.7317073170731707, 30: 0.72, 31: 0.03225806451612903, 32: 0.4093567251461988, 33: 0.14634146341463414, 34: 0.09859154929577464, 35: 0.1085972850678733, 36: 0.36363636363636365, 37: 0.23668639053254437, 38: 0.18604651162790697, 39: 0.0, 40: 0.28346456692913385}
Micro-average F1 score: 0.3093138309313831
Weighted-average F1 score: 0.2780343671289242
F1 score per class: {0: 0.18508997429305912, 1: 0.10300429184549356, 2: 0.07142857142857142, 3: 0.20863309352517986, 4: 0.912621359223301, 6: 0.3018867924528302, 7: 0.041237113402061855, 8: 0.17184643510054845, 9: 0.6756756756756757, 11: 0.2541436464088398, 12: 0.24598930481283424, 13: 0.012195121951219513, 14: 0.08695652173913043, 15: 0.46153846153846156, 19: 0.3136094674556213, 20: 0.20708446866485014, 21: 0.07901234567901234, 22: 0.4279475982532751, 23: 0.45528455284552843, 24: 0.0784313725490196, 25: 0.6326530612244898, 26: 0.5948275862068966, 27: 0.05555555555555555, 28: 0.09302325581395349, 29: 0.7136929460580913, 30: 0.5277777777777778, 31: 0.023255813953488372, 32: 0.484375, 33: 0.11764705882352941, 34: 0.10434782608695652, 35: 0.17463617463617465, 36: 0.3322475570032573, 37: 0.07758620689655173, 38: 0.24719101123595505, 39: 0.1875, 40: 0.3584905660377358}
Micro-average F1 score: 0.2585599194360524
Weighted-average F1 score: 0.23022637511904703
F1 score per class: {0: 0.18045112781954886, 1: 0.10224948875255624, 2: 0.07777777777777778, 3: 0.20224719101123595, 4: 0.9411764705882353, 6: 0.3052959501557632, 7: 0.04040404040404041, 8: 0.18147448015122875, 9: 0.7142857142857143, 11: 0.07339449541284404, 12: 0.23940149625935161, 13: 0.020202020202020204, 14: 0.09655172413793103, 15: 0.375, 19: 0.35251798561151076, 20: 0.21529745042492918, 21: 0.07281553398058252, 22: 0.46875, 23: 0.49557522123893805, 24: 0.08888888888888889, 25: 0.6262626262626263, 26: 0.6026200873362445, 27: 0.05555555555555555, 28: 0.1111111111111111, 29: 0.7174887892376681, 30: 0.41304347826086957, 31: 0.03225806451612903, 32: 0.4901185770750988, 33: 0.08695652173913043, 34: 0.10071942446043165, 35: 0.14795918367346939, 36: 0.35344827586206895, 37: 0.076, 38: 0.22727272727272727, 39: 0.2222222222222222, 40: 0.336283185840708}
Micro-average F1 score: 0.2554305155718398
Weighted-average F1 score: 0.22548278524477594
cur_acc:  ['0.6756', '0.3274', '0.3688', '0.3006', '0.2022', '0.5072', '0.3509']
his_acc:  ['0.6756', '0.5245', '0.4549', '0.3387', '0.2721', '0.2799', '0.3093']
cur_acc des:  ['0.6327', '0.3183', '0.3101', '0.3480', '0.1588', '0.3900', '0.3262']
his_acc des:  ['0.6327', '0.4370', '0.3789', '0.2997', '0.2465', '0.2484', '0.2586']
cur_acc rrf:  ['0.6359', '0.3463', '0.3147', '0.3487', '0.1620', '0.4078', '0.3226']
his_acc rrf:  ['0.6359', '0.4492', '0.3907', '0.3044', '0.2494', '0.2502', '0.2554']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 82.9859963CurrentTrain: epoch  0, batch     1 | loss: 95.4980288CurrentTrain: epoch  0, batch     2 | loss: 99.3134754CurrentTrain: epoch  0, batch     3 | loss: 91.5602242CurrentTrain: epoch  0, batch     4 | loss: 36.6717804CurrentTrain: epoch  1, batch     0 | loss: 71.2457139CurrentTrain: epoch  1, batch     1 | loss: 76.7824126CurrentTrain: epoch  1, batch     2 | loss: 73.2085807CurrentTrain: epoch  1, batch     3 | loss: 120.7569348CurrentTrain: epoch  1, batch     4 | loss: 74.6189080CurrentTrain: epoch  2, batch     0 | loss: 122.0872934CurrentTrain: epoch  2, batch     1 | loss: 64.7561313CurrentTrain: epoch  2, batch     2 | loss: 84.9062528CurrentTrain: epoch  2, batch     3 | loss: 118.9944386CurrentTrain: epoch  2, batch     4 | loss: 56.8415551CurrentTrain: epoch  3, batch     0 | loss: 184.6979371CurrentTrain: epoch  3, batch     1 | loss: 81.6275957CurrentTrain: epoch  3, batch     2 | loss: 55.7702753CurrentTrain: epoch  3, batch     3 | loss: 57.0421616CurrentTrain: epoch  3, batch     4 | loss: 112.2476581CurrentTrain: epoch  4, batch     0 | loss: 85.0240247CurrentTrain: epoch  4, batch     1 | loss: 86.5138158CurrentTrain: epoch  4, batch     2 | loss: 66.3873754CurrentTrain: epoch  4, batch     3 | loss: 69.7853638CurrentTrain: epoch  4, batch     4 | loss: 54.0186985CurrentTrain: epoch  5, batch     0 | loss: 83.8228559CurrentTrain: epoch  5, batch     1 | loss: 66.0363584CurrentTrain: epoch  5, batch     2 | loss: 64.7252117CurrentTrain: epoch  5, batch     3 | loss: 119.2133756CurrentTrain: epoch  5, batch     4 | loss: 55.0058369CurrentTrain: epoch  6, batch     0 | loss: 86.5698399CurrentTrain: epoch  6, batch     1 | loss: 65.6327565CurrentTrain: epoch  6, batch     2 | loss: 64.6771598CurrentTrain: epoch  6, batch     3 | loss: 54.1938036CurrentTrain: epoch  6, batch     4 | loss: 55.0012352CurrentTrain: epoch  7, batch     0 | loss: 115.8018565CurrentTrain: epoch  7, batch     1 | loss: 64.8835423CurrentTrain: epoch  7, batch     2 | loss: 85.8803802CurrentTrain: epoch  7, batch     3 | loss: 52.8151537CurrentTrain: epoch  7, batch     4 | loss: 40.8120247CurrentTrain: epoch  8, batch     0 | loss: 62.7345391CurrentTrain: epoch  8, batch     1 | loss: 86.5157147CurrentTrain: epoch  8, batch     2 | loss: 83.7249451CurrentTrain: epoch  8, batch     3 | loss: 85.9669130CurrentTrain: epoch  8, batch     4 | loss: 39.4225070CurrentTrain: epoch  9, batch     0 | loss: 82.7460819CurrentTrain: epoch  9, batch     1 | loss: 80.0363834CurrentTrain: epoch  9, batch     2 | loss: 118.1198581CurrentTrain: epoch  9, batch     3 | loss: 64.7232743CurrentTrain: epoch  9, batch     4 | loss: 40.3135479
MemoryTrain:  epoch  0, batch     0 | loss: 0.2080926MemoryTrain:  epoch  1, batch     0 | loss: 0.2370428MemoryTrain:  epoch  2, batch     0 | loss: 0.1776898MemoryTrain:  epoch  3, batch     0 | loss: 0.1454581MemoryTrain:  epoch  4, batch     0 | loss: 0.1156833MemoryTrain:  epoch  5, batch     0 | loss: 0.1056638MemoryTrain:  epoch  6, batch     0 | loss: 0.0866560MemoryTrain:  epoch  7, batch     0 | loss: 0.0902658MemoryTrain:  epoch  8, batch     0 | loss: 0.0781941MemoryTrain:  epoch  9, batch     0 | loss: 0.0715831

F1 score per class: {2: 0.0, 3: 0.0, 4: 0.0, 5: 0.8755760368663594, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.373134328358209, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.5494505494505495, 17: 0.3333333333333333, 18: 0.23952095808383234, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 29: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.45566166439290584
Weighted-average F1 score: 0.39558680863222073
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.6055045871559633, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.5192307692307693, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.5272727272727272, 17: 0.46153846153846156, 18: 0.16260162601626016, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.30639494026704145
Weighted-average F1 score: 0.2594055590468153
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.6055045871559633, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.5073170731707317, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.5132743362831859, 17: 0.5217391304347826, 18: 0.16393442622950818, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.30857142857142855
Weighted-average F1 score: 0.2620626138206673

F1 score per class: {0: 0.4918032786885246, 1: 0.1059190031152648, 2: 0.20408163265306123, 3: 0.24836601307189543, 4: 0.8842105263157894, 5: 0.7364341085271318, 6: 0.21978021978021978, 7: 0.0, 8: 0.18, 9: 0.819672131147541, 10: 0.22123893805309736, 11: 0.0, 12: 0.08695652173913043, 13: 0.016, 14: 0.03636363636363636, 15: 0.7, 16: 0.36231884057971014, 17: 0.14814814814814814, 18: 0.11235955056179775, 19: 0.3236514522821577, 20: 0.3050847457627119, 21: 0.09395973154362416, 22: 0.5176470588235295, 23: 0.6741573033707865, 24: 0.05263157894736842, 25: 0.5135135135135135, 26: 0.6057692307692307, 27: 0.07079646017699115, 28: 0.0, 29: 0.729064039408867, 30: 0.8372093023255814, 31: 0.05714285714285714, 32: 0.379746835443038, 33: 0.16216216216216217, 34: 0.06779661016949153, 35: 0.08484848484848485, 36: 0.13953488372093023, 37: 0.2360248447204969, 38: 0.10256410256410256, 39: 0.0, 40: 0.30864197530864196}
Micro-average F1 score: 0.3100038037276531
Weighted-average F1 score: 0.28704383221989865
F1 score per class: {0: 0.2696629213483146, 1: 0.09312638580931264, 2: 0.08955223880597014, 3: 0.22142857142857142, 4: 0.87, 5: 0.3844660194174757, 6: 0.23972602739726026, 7: 0.03333333333333333, 8: 0.2535211267605634, 9: 0.684931506849315, 10: 0.20889748549323017, 11: 0.144, 12: 0.20588235294117646, 13: 0.011363636363636364, 14: 0.07734806629834254, 15: 0.4, 16: 0.3372093023255814, 17: 0.08, 18: 0.05439709882139619, 19: 0.32432432432432434, 20: 0.22418879056047197, 21: 0.08022922636103152, 22: 0.42857142857142855, 23: 0.48, 24: 0.09302325581395349, 25: 0.5, 26: 0.5851528384279476, 27: 0.05, 28: 0.07692307692307693, 29: 0.6875, 30: 0.5901639344262295, 31: 0.03669724770642202, 32: 0.4982456140350877, 33: 0.08080808080808081, 34: 0.08130081300813008, 35: 0.15711252653927812, 36: 0.4375, 37: 0.15267175572519084, 38: 0.14893617021276595, 39: 0.1935483870967742, 40: 0.2890625}
Micro-average F1 score: 0.24525968672712284
Weighted-average F1 score: 0.22098094046067224
F1 score per class: {0: 0.2706766917293233, 1: 0.10638297872340426, 2: 0.09090909090909091, 3: 0.23293172690763053, 4: 0.9, 5: 0.38747553816046965, 6: 0.22591362126245848, 7: 0.032, 8: 0.25825825825825827, 9: 0.6944444444444444, 10: 0.20717131474103587, 11: 0.06060606060606061, 12: 0.20603015075376885, 13: 0.02127659574468085, 14: 0.075, 15: 0.3333333333333333, 16: 0.327683615819209, 17: 0.08450704225352113, 18: 0.05429864253393665, 19: 0.33540372670807456, 20: 0.22485207100591717, 21: 0.0748663101604278, 22: 0.4444444444444444, 23: 0.49586776859504134, 24: 0.08888888888888889, 25: 0.5, 26: 0.5851528384279476, 27: 0.04819277108433735, 28: 0.11764705882352941, 29: 0.7083333333333334, 30: 0.4444444444444444, 31: 0.043478260869565216, 32: 0.5052631578947369, 33: 0.0784313725490196, 34: 0.07692307692307693, 35: 0.12690355329949238, 36: 0.38666666666666666, 37: 0.14084507042253522, 38: 0.18666666666666668, 39: 0.1509433962264151, 40: 0.2835249042145594}
Micro-average F1 score: 0.24318037640093043
Weighted-average F1 score: 0.21838678654436797
cur_acc:  ['0.6756', '0.3274', '0.3688', '0.3006', '0.2022', '0.5072', '0.3509', '0.4557']
his_acc:  ['0.6756', '0.5245', '0.4549', '0.3387', '0.2721', '0.2799', '0.3093', '0.3100']
cur_acc des:  ['0.6327', '0.3183', '0.3101', '0.3480', '0.1588', '0.3900', '0.3262', '0.3064']
his_acc des:  ['0.6327', '0.4370', '0.3789', '0.2997', '0.2465', '0.2484', '0.2586', '0.2453']
cur_acc rrf:  ['0.6359', '0.3463', '0.3147', '0.3487', '0.1620', '0.4078', '0.3226', '0.3086']
his_acc rrf:  ['0.6359', '0.4492', '0.3907', '0.3044', '0.2494', '0.2502', '0.2554', '0.2432']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 70.9368587CurrentTrain: epoch  0, batch     1 | loss: 78.5053726CurrentTrain: epoch  0, batch     2 | loss: 58.4708270CurrentTrain: epoch  0, batch     3 | loss: 80.6341189CurrentTrain: epoch  0, batch     4 | loss: 79.6868851CurrentTrain: epoch  0, batch     5 | loss: 78.3725697CurrentTrain: epoch  0, batch     6 | loss: 78.2294605CurrentTrain: epoch  0, batch     7 | loss: 78.0926560CurrentTrain: epoch  0, batch     8 | loss: 78.5953841CurrentTrain: epoch  0, batch     9 | loss: 77.7801361CurrentTrain: epoch  0, batch    10 | loss: 97.5120023CurrentTrain: epoch  0, batch    11 | loss: 127.4118484CurrentTrain: epoch  0, batch    12 | loss: 65.0872506CurrentTrain: epoch  0, batch    13 | loss: 76.9493583CurrentTrain: epoch  0, batch    14 | loss: 80.0691938CurrentTrain: epoch  0, batch    15 | loss: 95.5916038CurrentTrain: epoch  0, batch    16 | loss: 77.3050402CurrentTrain: epoch  0, batch    17 | loss: 78.2643469CurrentTrain: epoch  0, batch    18 | loss: 55.6451045CurrentTrain: epoch  0, batch    19 | loss: 95.8509057CurrentTrain: epoch  0, batch    20 | loss: 95.8883227CurrentTrain: epoch  0, batch    21 | loss: 96.4843040CurrentTrain: epoch  0, batch    22 | loss: 64.6943584CurrentTrain: epoch  0, batch    23 | loss: 65.0389292CurrentTrain: epoch  0, batch    24 | loss: 55.8667923CurrentTrain: epoch  0, batch    25 | loss: 76.8488117CurrentTrain: epoch  0, batch    26 | loss: 64.5141106CurrentTrain: epoch  0, batch    27 | loss: 56.0146551CurrentTrain: epoch  0, batch    28 | loss: 77.1012466CurrentTrain: epoch  0, batch    29 | loss: 76.8224879CurrentTrain: epoch  0, batch    30 | loss: 125.9729440CurrentTrain: epoch  0, batch    31 | loss: 77.6370008CurrentTrain: epoch  0, batch    32 | loss: 126.0553817CurrentTrain: epoch  0, batch    33 | loss: 76.6047281CurrentTrain: epoch  0, batch    34 | loss: 94.8194863CurrentTrain: epoch  0, batch    35 | loss: 95.1052506CurrentTrain: epoch  0, batch    36 | loss: 76.6174007CurrentTrain: epoch  0, batch    37 | loss: 77.4410217CurrentTrain: epoch  0, batch    38 | loss: 76.6247348CurrentTrain: epoch  0, batch    39 | loss: 125.3669597CurrentTrain: epoch  0, batch    40 | loss: 76.2025972CurrentTrain: epoch  0, batch    41 | loss: 76.2225656CurrentTrain: epoch  0, batch    42 | loss: 64.1410740CurrentTrain: epoch  0, batch    43 | loss: 77.3473678CurrentTrain: epoch  0, batch    44 | loss: 76.0301537CurrentTrain: epoch  0, batch    45 | loss: 76.5006284CurrentTrain: epoch  0, batch    46 | loss: 76.7532588CurrentTrain: epoch  0, batch    47 | loss: 76.1467948CurrentTrain: epoch  0, batch    48 | loss: 125.4552694CurrentTrain: epoch  0, batch    49 | loss: 94.8421541CurrentTrain: epoch  0, batch    50 | loss: 63.7359757CurrentTrain: epoch  0, batch    51 | loss: 63.6730668CurrentTrain: epoch  0, batch    52 | loss: 76.8757826CurrentTrain: epoch  0, batch    53 | loss: 62.8318695CurrentTrain: epoch  0, batch    54 | loss: 64.1097102CurrentTrain: epoch  0, batch    55 | loss: 94.6635459CurrentTrain: epoch  0, batch    56 | loss: 76.1891443CurrentTrain: epoch  0, batch    57 | loss: 63.0729710CurrentTrain: epoch  0, batch    58 | loss: 93.8123354CurrentTrain: epoch  0, batch    59 | loss: 93.0302961CurrentTrain: epoch  0, batch    60 | loss: 75.5194859CurrentTrain: epoch  0, batch    61 | loss: 76.4690205CurrentTrain: epoch  0, batch    62 | loss: 54.0465134CurrentTrain: epoch  0, batch    63 | loss: 62.5749051CurrentTrain: epoch  0, batch    64 | loss: 125.7603501CurrentTrain: epoch  0, batch    65 | loss: 64.2303281CurrentTrain: epoch  0, batch    66 | loss: 73.8910449CurrentTrain: epoch  0, batch    67 | loss: 54.0611344CurrentTrain: epoch  0, batch    68 | loss: 76.5114875CurrentTrain: epoch  0, batch    69 | loss: 125.1962434CurrentTrain: epoch  0, batch    70 | loss: 63.0830281CurrentTrain: epoch  0, batch    71 | loss: 74.2985453CurrentTrain: epoch  0, batch    72 | loss: 125.3805044CurrentTrain: epoch  0, batch    73 | loss: 62.6980315CurrentTrain: epoch  0, batch    74 | loss: 124.8300988CurrentTrain: epoch  0, batch    75 | loss: 62.3960865CurrentTrain: epoch  0, batch    76 | loss: 73.2700896CurrentTrain: epoch  0, batch    77 | loss: 72.7449519CurrentTrain: epoch  0, batch    78 | loss: 184.8383303CurrentTrain: epoch  0, batch    79 | loss: 75.1573450CurrentTrain: epoch  0, batch    80 | loss: 93.4301516CurrentTrain: epoch  0, batch    81 | loss: 72.7481188CurrentTrain: epoch  0, batch    82 | loss: 62.6693025CurrentTrain: epoch  0, batch    83 | loss: 124.6993391CurrentTrain: epoch  0, batch    84 | loss: 93.9697969CurrentTrain: epoch  0, batch    85 | loss: 75.2502886CurrentTrain: epoch  0, batch    86 | loss: 94.6232222CurrentTrain: epoch  0, batch    87 | loss: 93.7444375CurrentTrain: epoch  0, batch    88 | loss: 73.7463319CurrentTrain: epoch  0, batch    89 | loss: 93.2515843CurrentTrain: epoch  0, batch    90 | loss: 90.9492573CurrentTrain: epoch  0, batch    91 | loss: 91.8785866CurrentTrain: epoch  0, batch    92 | loss: 72.3027164CurrentTrain: epoch  0, batch    93 | loss: 60.3045428CurrentTrain: epoch  0, batch    94 | loss: 74.7194504CurrentTrain: epoch  0, batch    95 | loss: 50.8350170CurrentTrain: epoch  1, batch     0 | loss: 60.5537157CurrentTrain: epoch  1, batch     1 | loss: 121.7518816CurrentTrain: epoch  1, batch     2 | loss: 70.6720052CurrentTrain: epoch  1, batch     3 | loss: 73.5810534CurrentTrain: epoch  1, batch     4 | loss: 60.7635717CurrentTrain: epoch  1, batch     5 | loss: 71.3440091CurrentTrain: epoch  1, batch     6 | loss: 90.8009694CurrentTrain: epoch  1, batch     7 | loss: 120.5127816CurrentTrain: epoch  1, batch     8 | loss: 89.5120312CurrentTrain: epoch  1, batch     9 | loss: 71.6330520CurrentTrain: epoch  1, batch    10 | loss: 51.8511910CurrentTrain: epoch  1, batch    11 | loss: 73.0821295CurrentTrain: epoch  1, batch    12 | loss: 120.6973975CurrentTrain: epoch  1, batch    13 | loss: 121.2547852CurrentTrain: epoch  1, batch    14 | loss: 60.0856289CurrentTrain: epoch  1, batch    15 | loss: 48.9508428CurrentTrain: epoch  1, batch    16 | loss: 72.0264563CurrentTrain: epoch  1, batch    17 | loss: 73.4810475CurrentTrain: epoch  1, batch    18 | loss: 93.3021014CurrentTrain: epoch  1, batch    19 | loss: 119.7652231CurrentTrain: epoch  1, batch    20 | loss: 68.5890960CurrentTrain: epoch  1, batch    21 | loss: 71.0766172CurrentTrain: epoch  1, batch    22 | loss: 59.2341341CurrentTrain: epoch  1, batch    23 | loss: 58.6039768CurrentTrain: epoch  1, batch    24 | loss: 89.1169948CurrentTrain: epoch  1, batch    25 | loss: 90.5892200CurrentTrain: epoch  1, batch    26 | loss: 88.8002573CurrentTrain: epoch  1, batch    27 | loss: 51.1215403CurrentTrain: epoch  1, batch    28 | loss: 87.9768398CurrentTrain: epoch  1, batch    29 | loss: 73.5772829CurrentTrain: epoch  1, batch    30 | loss: 69.7515504CurrentTrain: epoch  1, batch    31 | loss: 74.6514787CurrentTrain: epoch  1, batch    32 | loss: 89.2923715CurrentTrain: epoch  1, batch    33 | loss: 50.6594804CurrentTrain: epoch  1, batch    34 | loss: 88.6996971CurrentTrain: epoch  1, batch    35 | loss: 59.4553714CurrentTrain: epoch  1, batch    36 | loss: 71.3267223CurrentTrain: epoch  1, batch    37 | loss: 70.0018688CurrentTrain: epoch  1, batch    38 | loss: 51.3269244CurrentTrain: epoch  1, batch    39 | loss: 90.5348033CurrentTrain: epoch  1, batch    40 | loss: 57.7583875CurrentTrain: epoch  1, batch    41 | loss: 74.0360897CurrentTrain: epoch  1, batch    42 | loss: 48.5903185CurrentTrain: epoch  1, batch    43 | loss: 72.9531104CurrentTrain: epoch  1, batch    44 | loss: 67.7489090CurrentTrain: epoch  1, batch    45 | loss: 57.8750109CurrentTrain: epoch  1, batch    46 | loss: 54.1896345CurrentTrain: epoch  1, batch    47 | loss: 57.4413614CurrentTrain: epoch  1, batch    48 | loss: 64.7347071CurrentTrain: epoch  1, batch    49 | loss: 58.9233246CurrentTrain: epoch  1, batch    50 | loss: 183.0781149CurrentTrain: epoch  1, batch    51 | loss: 85.2900857CurrentTrain: epoch  1, batch    52 | loss: 89.4304229CurrentTrain: epoch  1, batch    53 | loss: 58.0913699CurrentTrain: epoch  1, batch    54 | loss: 57.8639903CurrentTrain: epoch  1, batch    55 | loss: 56.6856016CurrentTrain: epoch  1, batch    56 | loss: 89.1907380CurrentTrain: epoch  1, batch    57 | loss: 120.6422878CurrentTrain: epoch  1, batch    58 | loss: 123.8564225CurrentTrain: epoch  1, batch    59 | loss: 117.7618040CurrentTrain: epoch  1, batch    60 | loss: 72.3611450CurrentTrain: epoch  1, batch    61 | loss: 60.3188784CurrentTrain: epoch  1, batch    62 | loss: 65.5719668CurrentTrain: epoch  1, batch    63 | loss: 75.5985878CurrentTrain: epoch  1, batch    64 | loss: 70.6687129CurrentTrain: epoch  1, batch    65 | loss: 68.3448235CurrentTrain: epoch  1, batch    66 | loss: 56.3173394CurrentTrain: epoch  1, batch    67 | loss: 58.0377046CurrentTrain: epoch  1, batch    68 | loss: 66.4739731CurrentTrain: epoch  1, batch    69 | loss: 122.4527226CurrentTrain: epoch  1, batch    70 | loss: 79.7900490CurrentTrain: epoch  1, batch    71 | loss: 68.2958281CurrentTrain: epoch  1, batch    72 | loss: 59.8121120CurrentTrain: epoch  1, batch    73 | loss: 92.5159244CurrentTrain: epoch  1, batch    74 | loss: 70.6909970CurrentTrain: epoch  1, batch    75 | loss: 54.5881044CurrentTrain: epoch  1, batch    76 | loss: 57.3640755CurrentTrain: epoch  1, batch    77 | loss: 89.8529677CurrentTrain: epoch  1, batch    78 | loss: 92.7321277CurrentTrain: epoch  1, batch    79 | loss: 90.9920723CurrentTrain: epoch  1, batch    80 | loss: 48.4478853CurrentTrain: epoch  1, batch    81 | loss: 57.3471628CurrentTrain: epoch  1, batch    82 | loss: 86.0088875CurrentTrain: epoch  1, batch    83 | loss: 72.9515317CurrentTrain: epoch  1, batch    84 | loss: 119.2247311CurrentTrain: epoch  1, batch    85 | loss: 59.9126967CurrentTrain: epoch  1, batch    86 | loss: 58.3241015CurrentTrain: epoch  1, batch    87 | loss: 86.0349809CurrentTrain: epoch  1, batch    88 | loss: 91.7761000CurrentTrain: epoch  1, batch    89 | loss: 52.3023428CurrentTrain: epoch  1, batch    90 | loss: 122.0378541CurrentTrain: epoch  1, batch    91 | loss: 57.7795107CurrentTrain: epoch  1, batch    92 | loss: 53.0665883CurrentTrain: epoch  1, batch    93 | loss: 69.5045700CurrentTrain: epoch  1, batch    94 | loss: 67.9779310CurrentTrain: epoch  1, batch    95 | loss: 59.1853790CurrentTrain: epoch  2, batch     0 | loss: 84.0871511CurrentTrain: epoch  2, batch     1 | loss: 69.1296151CurrentTrain: epoch  2, batch     2 | loss: 57.3300671CurrentTrain: epoch  2, batch     3 | loss: 90.8800189CurrentTrain: epoch  2, batch     4 | loss: 86.8250158CurrentTrain: epoch  2, batch     5 | loss: 65.0020893CurrentTrain: epoch  2, batch     6 | loss: 86.3042289CurrentTrain: epoch  2, batch     7 | loss: 45.8493812CurrentTrain: epoch  2, batch     8 | loss: 85.5093320CurrentTrain: epoch  2, batch     9 | loss: 65.2257237CurrentTrain: epoch  2, batch    10 | loss: 86.7051288CurrentTrain: epoch  2, batch    11 | loss: 69.0242396CurrentTrain: epoch  2, batch    12 | loss: 59.4431076CurrentTrain: epoch  2, batch    13 | loss: 70.1255634CurrentTrain: epoch  2, batch    14 | loss: 188.3987838CurrentTrain: epoch  2, batch    15 | loss: 65.1502645CurrentTrain: epoch  2, batch    16 | loss: 70.4095903CurrentTrain: epoch  2, batch    17 | loss: 69.0226492CurrentTrain: epoch  2, batch    18 | loss: 87.6151287CurrentTrain: epoch  2, batch    19 | loss: 46.8549738CurrentTrain: epoch  2, batch    20 | loss: 57.4053601CurrentTrain: epoch  2, batch    21 | loss: 70.4456487CurrentTrain: epoch  2, batch    22 | loss: 57.1835046CurrentTrain: epoch  2, batch    23 | loss: 117.0600364CurrentTrain: epoch  2, batch    24 | loss: 115.8828930CurrentTrain: epoch  2, batch    25 | loss: 117.8746304CurrentTrain: epoch  2, batch    26 | loss: 83.4580183CurrentTrain: epoch  2, batch    27 | loss: 84.0612292CurrentTrain: epoch  2, batch    28 | loss: 85.3689951CurrentTrain: epoch  2, batch    29 | loss: 70.9840173CurrentTrain: epoch  2, batch    30 | loss: 58.0763145CurrentTrain: epoch  2, batch    31 | loss: 117.5362168CurrentTrain: epoch  2, batch    32 | loss: 58.9131163CurrentTrain: epoch  2, batch    33 | loss: 69.3279792CurrentTrain: epoch  2, batch    34 | loss: 58.9316075CurrentTrain: epoch  2, batch    35 | loss: 47.3997584CurrentTrain: epoch  2, batch    36 | loss: 86.1153337CurrentTrain: epoch  2, batch    37 | loss: 56.3495419CurrentTrain: epoch  2, batch    38 | loss: 72.3903182CurrentTrain: epoch  2, batch    39 | loss: 89.3443286CurrentTrain: epoch  2, batch    40 | loss: 67.5098280CurrentTrain: epoch  2, batch    41 | loss: 66.1544971CurrentTrain: epoch  2, batch    42 | loss: 115.0618308CurrentTrain: epoch  2, batch    43 | loss: 68.1119587CurrentTrain: epoch  2, batch    44 | loss: 70.1530779CurrentTrain: epoch  2, batch    45 | loss: 122.7230418CurrentTrain: epoch  2, batch    46 | loss: 65.2272321CurrentTrain: epoch  2, batch    47 | loss: 46.8463894CurrentTrain: epoch  2, batch    48 | loss: 65.7825715CurrentTrain: epoch  2, batch    49 | loss: 89.6534383CurrentTrain: epoch  2, batch    50 | loss: 69.5674115CurrentTrain: epoch  2, batch    51 | loss: 56.3658192CurrentTrain: epoch  2, batch    52 | loss: 67.9491261CurrentTrain: epoch  2, batch    53 | loss: 68.5646140CurrentTrain: epoch  2, batch    54 | loss: 87.6345533CurrentTrain: epoch  2, batch    55 | loss: 72.2844984CurrentTrain: epoch  2, batch    56 | loss: 56.7068512CurrentTrain: epoch  2, batch    57 | loss: 68.9376879CurrentTrain: epoch  2, batch    58 | loss: 57.6490874CurrentTrain: epoch  2, batch    59 | loss: 57.0422732CurrentTrain: epoch  2, batch    60 | loss: 66.4644787CurrentTrain: epoch  2, batch    61 | loss: 56.2244012CurrentTrain: epoch  2, batch    62 | loss: 44.5954130CurrentTrain: epoch  2, batch    63 | loss: 46.5497475CurrentTrain: epoch  2, batch    64 | loss: 70.1660950CurrentTrain: epoch  2, batch    65 | loss: 68.1671082CurrentTrain: epoch  2, batch    66 | loss: 54.9654768CurrentTrain: epoch  2, batch    67 | loss: 84.6706624CurrentTrain: epoch  2, batch    68 | loss: 117.1445904CurrentTrain: epoch  2, batch    69 | loss: 66.6313459CurrentTrain: epoch  2, batch    70 | loss: 55.9385443CurrentTrain: epoch  2, batch    71 | loss: 65.9741081CurrentTrain: epoch  2, batch    72 | loss: 57.0167540CurrentTrain: epoch  2, batch    73 | loss: 66.3714646CurrentTrain: epoch  2, batch    74 | loss: 65.9223165CurrentTrain: epoch  2, batch    75 | loss: 87.8160662CurrentTrain: epoch  2, batch    76 | loss: 53.4215774CurrentTrain: epoch  2, batch    77 | loss: 79.4958655CurrentTrain: epoch  2, batch    78 | loss: 54.0209742CurrentTrain: epoch  2, batch    79 | loss: 55.1275748CurrentTrain: epoch  2, batch    80 | loss: 85.2967735CurrentTrain: epoch  2, batch    81 | loss: 92.3434543CurrentTrain: epoch  2, batch    82 | loss: 45.7280393CurrentTrain: epoch  2, batch    83 | loss: 52.6825351CurrentTrain: epoch  2, batch    84 | loss: 65.5320432CurrentTrain: epoch  2, batch    85 | loss: 67.1738728CurrentTrain: epoch  2, batch    86 | loss: 88.0869152CurrentTrain: epoch  2, batch    87 | loss: 65.3960991CurrentTrain: epoch  2, batch    88 | loss: 91.6939338CurrentTrain: epoch  2, batch    89 | loss: 121.7475961CurrentTrain: epoch  2, batch    90 | loss: 70.1908353CurrentTrain: epoch  2, batch    91 | loss: 118.8945561CurrentTrain: epoch  2, batch    92 | loss: 119.3686436CurrentTrain: epoch  2, batch    93 | loss: 65.8240739CurrentTrain: epoch  2, batch    94 | loss: 67.9456245CurrentTrain: epoch  2, batch    95 | loss: 58.9998429CurrentTrain: epoch  3, batch     0 | loss: 67.3234950CurrentTrain: epoch  3, batch     1 | loss: 55.0653840CurrentTrain: epoch  3, batch     2 | loss: 89.3264177CurrentTrain: epoch  3, batch     3 | loss: 64.7600678CurrentTrain: epoch  3, batch     4 | loss: 54.8084454CurrentTrain: epoch  3, batch     5 | loss: 67.2923471CurrentTrain: epoch  3, batch     6 | loss: 85.8474872CurrentTrain: epoch  3, batch     7 | loss: 64.8171806CurrentTrain: epoch  3, batch     8 | loss: 61.9524480CurrentTrain: epoch  3, batch     9 | loss: 65.1186071CurrentTrain: epoch  3, batch    10 | loss: 64.0741589CurrentTrain: epoch  3, batch    11 | loss: 85.8500188CurrentTrain: epoch  3, batch    12 | loss: 66.7328858CurrentTrain: epoch  3, batch    13 | loss: 69.8964378CurrentTrain: epoch  3, batch    14 | loss: 85.9040762CurrentTrain: epoch  3, batch    15 | loss: 53.6219827CurrentTrain: epoch  3, batch    16 | loss: 69.6333559CurrentTrain: epoch  3, batch    17 | loss: 65.8098716CurrentTrain: epoch  3, batch    18 | loss: 67.9661697CurrentTrain: epoch  3, batch    19 | loss: 88.1152719CurrentTrain: epoch  3, batch    20 | loss: 53.1479292CurrentTrain: epoch  3, batch    21 | loss: 114.9126526CurrentTrain: epoch  3, batch    22 | loss: 54.4564392CurrentTrain: epoch  3, batch    23 | loss: 52.8205203CurrentTrain: epoch  3, batch    24 | loss: 72.9202965CurrentTrain: epoch  3, batch    25 | loss: 43.2575449CurrentTrain: epoch  3, batch    26 | loss: 85.3980956CurrentTrain: epoch  3, batch    27 | loss: 120.7889971CurrentTrain: epoch  3, batch    28 | loss: 66.4807743CurrentTrain: epoch  3, batch    29 | loss: 116.6640118CurrentTrain: epoch  3, batch    30 | loss: 68.7429318CurrentTrain: epoch  3, batch    31 | loss: 66.6549627CurrentTrain: epoch  3, batch    32 | loss: 69.0615822CurrentTrain: epoch  3, batch    33 | loss: 48.4381277CurrentTrain: epoch  3, batch    34 | loss: 68.5520806CurrentTrain: epoch  3, batch    35 | loss: 89.5424762CurrentTrain: epoch  3, batch    36 | loss: 55.5668190CurrentTrain: epoch  3, batch    37 | loss: 87.7200222CurrentTrain: epoch  3, batch    38 | loss: 71.2683695CurrentTrain: epoch  3, batch    39 | loss: 85.2543674CurrentTrain: epoch  3, batch    40 | loss: 66.2182039CurrentTrain: epoch  3, batch    41 | loss: 86.2432219CurrentTrain: epoch  3, batch    42 | loss: 86.6276743CurrentTrain: epoch  3, batch    43 | loss: 88.6721401CurrentTrain: epoch  3, batch    44 | loss: 55.4384754CurrentTrain: epoch  3, batch    45 | loss: 54.9495419CurrentTrain: epoch  3, batch    46 | loss: 117.4407403CurrentTrain: epoch  3, batch    47 | loss: 68.3234725CurrentTrain: epoch  3, batch    48 | loss: 67.9817139CurrentTrain: epoch  3, batch    49 | loss: 55.0817537CurrentTrain: epoch  3, batch    50 | loss: 90.9161903CurrentTrain: epoch  3, batch    51 | loss: 46.0920305CurrentTrain: epoch  3, batch    52 | loss: 87.7367047CurrentTrain: epoch  3, batch    53 | loss: 84.7336455CurrentTrain: epoch  3, batch    54 | loss: 84.1335834CurrentTrain: epoch  3, batch    55 | loss: 54.0763172CurrentTrain: epoch  3, batch    56 | loss: 113.5525825CurrentTrain: epoch  3, batch    57 | loss: 66.3245405CurrentTrain: epoch  3, batch    58 | loss: 51.3546316CurrentTrain: epoch  3, batch    59 | loss: 66.8255369CurrentTrain: epoch  3, batch    60 | loss: 55.1141866CurrentTrain: epoch  3, batch    61 | loss: 66.6488956CurrentTrain: epoch  3, batch    62 | loss: 86.0577955CurrentTrain: epoch  3, batch    63 | loss: 84.7035935CurrentTrain: epoch  3, batch    64 | loss: 84.5861938CurrentTrain: epoch  3, batch    65 | loss: 65.5054878CurrentTrain: epoch  3, batch    66 | loss: 54.3103009CurrentTrain: epoch  3, batch    67 | loss: 52.4207879CurrentTrain: epoch  3, batch    68 | loss: 85.2843498CurrentTrain: epoch  3, batch    69 | loss: 87.0796401CurrentTrain: epoch  3, batch    70 | loss: 84.1106099CurrentTrain: epoch  3, batch    71 | loss: 81.0403958CurrentTrain: epoch  3, batch    72 | loss: 54.4583666CurrentTrain: epoch  3, batch    73 | loss: 85.2215300CurrentTrain: epoch  3, batch    74 | loss: 68.2100039CurrentTrain: epoch  3, batch    75 | loss: 53.6892920CurrentTrain: epoch  3, batch    76 | loss: 54.3611961CurrentTrain: epoch  3, batch    77 | loss: 86.7982663CurrentTrain: epoch  3, batch    78 | loss: 83.7123869CurrentTrain: epoch  3, batch    79 | loss: 85.0940822CurrentTrain: epoch  3, batch    80 | loss: 49.8653130CurrentTrain: epoch  3, batch    81 | loss: 63.6608515CurrentTrain: epoch  3, batch    82 | loss: 86.4229678CurrentTrain: epoch  3, batch    83 | loss: 52.5349956CurrentTrain: epoch  3, batch    84 | loss: 85.4011042CurrentTrain: epoch  3, batch    85 | loss: 47.9528584CurrentTrain: epoch  3, batch    86 | loss: 86.0451003CurrentTrain: epoch  3, batch    87 | loss: 44.7447590CurrentTrain: epoch  3, batch    88 | loss: 56.1783636CurrentTrain: epoch  3, batch    89 | loss: 71.3755938CurrentTrain: epoch  3, batch    90 | loss: 66.3066079CurrentTrain: epoch  3, batch    91 | loss: 84.5734493CurrentTrain: epoch  3, batch    92 | loss: 85.2628394CurrentTrain: epoch  3, batch    93 | loss: 55.3336246CurrentTrain: epoch  3, batch    94 | loss: 48.3483550CurrentTrain: epoch  3, batch    95 | loss: 52.5069604CurrentTrain: epoch  4, batch     0 | loss: 117.4578053CurrentTrain: epoch  4, batch     1 | loss: 68.4354253CurrentTrain: epoch  4, batch     2 | loss: 52.9593295CurrentTrain: epoch  4, batch     3 | loss: 56.8206818CurrentTrain: epoch  4, batch     4 | loss: 65.0751897CurrentTrain: epoch  4, batch     5 | loss: 86.7361824CurrentTrain: epoch  4, batch     6 | loss: 66.8083450CurrentTrain: epoch  4, batch     7 | loss: 50.8328128CurrentTrain: epoch  4, batch     8 | loss: 86.9506914CurrentTrain: epoch  4, batch     9 | loss: 43.9446986CurrentTrain: epoch  4, batch    10 | loss: 84.1505666CurrentTrain: epoch  4, batch    11 | loss: 85.3475791CurrentTrain: epoch  4, batch    12 | loss: 46.7648416CurrentTrain: epoch  4, batch    13 | loss: 54.8414357CurrentTrain: epoch  4, batch    14 | loss: 67.9963018CurrentTrain: epoch  4, batch    15 | loss: 53.5864715CurrentTrain: epoch  4, batch    16 | loss: 56.3993407CurrentTrain: epoch  4, batch    17 | loss: 64.1792348CurrentTrain: epoch  4, batch    18 | loss: 62.2295731CurrentTrain: epoch  4, batch    19 | loss: 85.1810943CurrentTrain: epoch  4, batch    20 | loss: 67.8223284CurrentTrain: epoch  4, batch    21 | loss: 88.0995156CurrentTrain: epoch  4, batch    22 | loss: 67.9972721CurrentTrain: epoch  4, batch    23 | loss: 181.8611503CurrentTrain: epoch  4, batch    24 | loss: 182.1820623CurrentTrain: epoch  4, batch    25 | loss: 45.4921821CurrentTrain: epoch  4, batch    26 | loss: 83.7023292CurrentTrain: epoch  4, batch    27 | loss: 44.2954041CurrentTrain: epoch  4, batch    28 | loss: 66.3765970CurrentTrain: epoch  4, batch    29 | loss: 87.5471433CurrentTrain: epoch  4, batch    30 | loss: 48.7421710CurrentTrain: epoch  4, batch    31 | loss: 54.7891653CurrentTrain: epoch  4, batch    32 | loss: 47.1339376CurrentTrain: epoch  4, batch    33 | loss: 85.4198548CurrentTrain: epoch  4, batch    34 | loss: 56.1843847CurrentTrain: epoch  4, batch    35 | loss: 45.5534325CurrentTrain: epoch  4, batch    36 | loss: 67.7318359CurrentTrain: epoch  4, batch    37 | loss: 67.5153107CurrentTrain: epoch  4, batch    38 | loss: 81.1133799CurrentTrain: epoch  4, batch    39 | loss: 67.2113792CurrentTrain: epoch  4, batch    40 | loss: 84.7044858CurrentTrain: epoch  4, batch    41 | loss: 65.8771664CurrentTrain: epoch  4, batch    42 | loss: 84.6514318CurrentTrain: epoch  4, batch    43 | loss: 42.4001027CurrentTrain: epoch  4, batch    44 | loss: 56.3994652CurrentTrain: epoch  4, batch    45 | loss: 85.4153432CurrentTrain: epoch  4, batch    46 | loss: 52.7120385CurrentTrain: epoch  4, batch    47 | loss: 52.7560918CurrentTrain: epoch  4, batch    48 | loss: 67.8414088CurrentTrain: epoch  4, batch    49 | loss: 119.7842501CurrentTrain: epoch  4, batch    50 | loss: 57.5311383CurrentTrain: epoch  4, batch    51 | loss: 65.0605554CurrentTrain: epoch  4, batch    52 | loss: 67.5965343CurrentTrain: epoch  4, batch    53 | loss: 118.9554244CurrentTrain: epoch  4, batch    54 | loss: 63.7043146CurrentTrain: epoch  4, batch    55 | loss: 86.5837280CurrentTrain: epoch  4, batch    56 | loss: 63.9042642CurrentTrain: epoch  4, batch    57 | loss: 117.2914644CurrentTrain: epoch  4, batch    58 | loss: 69.1844699CurrentTrain: epoch  4, batch    59 | loss: 67.6465792CurrentTrain: epoch  4, batch    60 | loss: 120.1270885CurrentTrain: epoch  4, batch    61 | loss: 55.0786173CurrentTrain: epoch  4, batch    62 | loss: 54.2229177CurrentTrain: epoch  4, batch    63 | loss: 53.3537603CurrentTrain: epoch  4, batch    64 | loss: 51.4087962CurrentTrain: epoch  4, batch    65 | loss: 81.7336101CurrentTrain: epoch  4, batch    66 | loss: 112.8033644CurrentTrain: epoch  4, batch    67 | loss: 114.7039395CurrentTrain: epoch  4, batch    68 | loss: 50.2657512CurrentTrain: epoch  4, batch    69 | loss: 67.2246699CurrentTrain: epoch  4, batch    70 | loss: 116.9742003CurrentTrain: epoch  4, batch    71 | loss: 64.8246986CurrentTrain: epoch  4, batch    72 | loss: 55.3382614CurrentTrain: epoch  4, batch    73 | loss: 55.3381907CurrentTrain: epoch  4, batch    74 | loss: 65.5040271CurrentTrain: epoch  4, batch    75 | loss: 54.9481960CurrentTrain: epoch  4, batch    76 | loss: 78.9947488CurrentTrain: epoch  4, batch    77 | loss: 89.0199937CurrentTrain: epoch  4, batch    78 | loss: 64.5882975CurrentTrain: epoch  4, batch    79 | loss: 116.2626211CurrentTrain: epoch  4, batch    80 | loss: 66.4956428CurrentTrain: epoch  4, batch    81 | loss: 86.5593161CurrentTrain: epoch  4, batch    82 | loss: 83.0894992CurrentTrain: epoch  4, batch    83 | loss: 60.3169695CurrentTrain: epoch  4, batch    84 | loss: 181.9059203CurrentTrain: epoch  4, batch    85 | loss: 67.3352705CurrentTrain: epoch  4, batch    86 | loss: 69.8565960CurrentTrain: epoch  4, batch    87 | loss: 52.2496878CurrentTrain: epoch  4, batch    88 | loss: 54.0458568CurrentTrain: epoch  4, batch    89 | loss: 46.6237825CurrentTrain: epoch  4, batch    90 | loss: 62.9261908CurrentTrain: epoch  4, batch    91 | loss: 52.2833927CurrentTrain: epoch  4, batch    92 | loss: 54.0640686CurrentTrain: epoch  4, batch    93 | loss: 52.1111020CurrentTrain: epoch  4, batch    94 | loss: 53.8068513CurrentTrain: epoch  4, batch    95 | loss: 52.4759120CurrentTrain: epoch  5, batch     0 | loss: 53.2463332CurrentTrain: epoch  5, batch     1 | loss: 84.7080370CurrentTrain: epoch  5, batch     2 | loss: 66.4338353CurrentTrain: epoch  5, batch     3 | loss: 64.9608974CurrentTrain: epoch  5, batch     4 | loss: 83.8286683CurrentTrain: epoch  5, batch     5 | loss: 54.1934950CurrentTrain: epoch  5, batch     6 | loss: 54.2382449CurrentTrain: epoch  5, batch     7 | loss: 64.7920163CurrentTrain: epoch  5, batch     8 | loss: 43.4509749CurrentTrain: epoch  5, batch     9 | loss: 65.0742549CurrentTrain: epoch  5, batch    10 | loss: 116.4191750CurrentTrain: epoch  5, batch    11 | loss: 65.8829851CurrentTrain: epoch  5, batch    12 | loss: 113.9538058CurrentTrain: epoch  5, batch    13 | loss: 66.7496454CurrentTrain: epoch  5, batch    14 | loss: 86.5483628CurrentTrain: epoch  5, batch    15 | loss: 88.8669279CurrentTrain: epoch  5, batch    16 | loss: 66.3089096CurrentTrain: epoch  5, batch    17 | loss: 65.7687177CurrentTrain: epoch  5, batch    18 | loss: 49.9031179CurrentTrain: epoch  5, batch    19 | loss: 80.7472063CurrentTrain: epoch  5, batch    20 | loss: 55.2785487CurrentTrain: epoch  5, batch    21 | loss: 70.2283339CurrentTrain: epoch  5, batch    22 | loss: 115.6464275CurrentTrain: epoch  5, batch    23 | loss: 51.7142540CurrentTrain: epoch  5, batch    24 | loss: 52.5840493CurrentTrain: epoch  5, batch    25 | loss: 45.8337472CurrentTrain: epoch  5, batch    26 | loss: 65.5380717CurrentTrain: epoch  5, batch    27 | loss: 66.9921983CurrentTrain: epoch  5, batch    28 | loss: 65.7679944CurrentTrain: epoch  5, batch    29 | loss: 79.6325394CurrentTrain: epoch  5, batch    30 | loss: 50.0953484CurrentTrain: epoch  5, batch    31 | loss: 67.6056486CurrentTrain: epoch  5, batch    32 | loss: 51.3761337CurrentTrain: epoch  5, batch    33 | loss: 119.2607725CurrentTrain: epoch  5, batch    34 | loss: 46.4544498CurrentTrain: epoch  5, batch    35 | loss: 86.5791763CurrentTrain: epoch  5, batch    36 | loss: 83.7906935CurrentTrain: epoch  5, batch    37 | loss: 85.2052074CurrentTrain: epoch  5, batch    38 | loss: 53.6755504CurrentTrain: epoch  5, batch    39 | loss: 84.4005151CurrentTrain: epoch  5, batch    40 | loss: 115.5185482CurrentTrain: epoch  5, batch    41 | loss: 53.6350538CurrentTrain: epoch  5, batch    42 | loss: 63.9023760CurrentTrain: epoch  5, batch    43 | loss: 51.3477229CurrentTrain: epoch  5, batch    44 | loss: 115.7524597CurrentTrain: epoch  5, batch    45 | loss: 62.1686771CurrentTrain: epoch  5, batch    46 | loss: 51.3247873CurrentTrain: epoch  5, batch    47 | loss: 61.4272004CurrentTrain: epoch  5, batch    48 | loss: 51.8645644CurrentTrain: epoch  5, batch    49 | loss: 54.2251178CurrentTrain: epoch  5, batch    50 | loss: 64.9634132CurrentTrain: epoch  5, batch    51 | loss: 54.0525467CurrentTrain: epoch  5, batch    52 | loss: 79.8405348CurrentTrain: epoch  5, batch    53 | loss: 174.9207111CurrentTrain: epoch  5, batch    54 | loss: 113.1087639CurrentTrain: epoch  5, batch    55 | loss: 63.1579782CurrentTrain: epoch  5, batch    56 | loss: 116.8362985CurrentTrain: epoch  5, batch    57 | loss: 85.0582978CurrentTrain: epoch  5, batch    58 | loss: 54.7981078CurrentTrain: epoch  5, batch    59 | loss: 183.3015795CurrentTrain: epoch  5, batch    60 | loss: 81.5123580CurrentTrain: epoch  5, batch    61 | loss: 113.9491844CurrentTrain: epoch  5, batch    62 | loss: 67.2249745CurrentTrain: epoch  5, batch    63 | loss: 64.7154578CurrentTrain: epoch  5, batch    64 | loss: 66.5599493CurrentTrain: epoch  5, batch    65 | loss: 64.4612263CurrentTrain: epoch  5, batch    66 | loss: 84.7744793CurrentTrain: epoch  5, batch    67 | loss: 64.0529230CurrentTrain: epoch  5, batch    68 | loss: 66.7246787CurrentTrain: epoch  5, batch    69 | loss: 64.1335538CurrentTrain: epoch  5, batch    70 | loss: 87.1996513CurrentTrain: epoch  5, batch    71 | loss: 177.9295881CurrentTrain: epoch  5, batch    72 | loss: 62.1051192CurrentTrain: epoch  5, batch    73 | loss: 83.8663378CurrentTrain: epoch  5, batch    74 | loss: 66.3838698CurrentTrain: epoch  5, batch    75 | loss: 53.5534003CurrentTrain: epoch  5, batch    76 | loss: 65.2135356CurrentTrain: epoch  5, batch    77 | loss: 51.1247861CurrentTrain: epoch  5, batch    78 | loss: 88.8390133CurrentTrain: epoch  5, batch    79 | loss: 50.3958799CurrentTrain: epoch  5, batch    80 | loss: 111.9935324CurrentTrain: epoch  5, batch    81 | loss: 64.8063077CurrentTrain: epoch  5, batch    82 | loss: 82.8376318CurrentTrain: epoch  5, batch    83 | loss: 117.2342017CurrentTrain: epoch  5, batch    84 | loss: 63.5749605CurrentTrain: epoch  5, batch    85 | loss: 84.4180981CurrentTrain: epoch  5, batch    86 | loss: 87.6557505CurrentTrain: epoch  5, batch    87 | loss: 86.1880871CurrentTrain: epoch  5, batch    88 | loss: 61.5967415CurrentTrain: epoch  5, batch    89 | loss: 84.7572837CurrentTrain: epoch  5, batch    90 | loss: 82.0460919CurrentTrain: epoch  5, batch    91 | loss: 113.5486735CurrentTrain: epoch  5, batch    92 | loss: 62.7089097CurrentTrain: epoch  5, batch    93 | loss: 81.9895331CurrentTrain: epoch  5, batch    94 | loss: 75.7490065CurrentTrain: epoch  5, batch    95 | loss: 50.5628166CurrentTrain: epoch  6, batch     0 | loss: 83.6042694CurrentTrain: epoch  6, batch     1 | loss: 78.8522820CurrentTrain: epoch  6, batch     2 | loss: 115.3493571CurrentTrain: epoch  6, batch     3 | loss: 65.6302765CurrentTrain: epoch  6, batch     4 | loss: 63.4890605CurrentTrain: epoch  6, batch     5 | loss: 65.2053322CurrentTrain: epoch  6, batch     6 | loss: 115.4224724CurrentTrain: epoch  6, batch     7 | loss: 114.3078064CurrentTrain: epoch  6, batch     8 | loss: 51.6913503CurrentTrain: epoch  6, batch     9 | loss: 50.0217201CurrentTrain: epoch  6, batch    10 | loss: 67.0164049CurrentTrain: epoch  6, batch    11 | loss: 65.6409027CurrentTrain: epoch  6, batch    12 | loss: 64.8864217CurrentTrain: epoch  6, batch    13 | loss: 65.9949115CurrentTrain: epoch  6, batch    14 | loss: 85.9086486CurrentTrain: epoch  6, batch    15 | loss: 42.5461422CurrentTrain: epoch  6, batch    16 | loss: 67.1099506CurrentTrain: epoch  6, batch    17 | loss: 85.2726927CurrentTrain: epoch  6, batch    18 | loss: 62.2602918CurrentTrain: epoch  6, batch    19 | loss: 82.9803054CurrentTrain: epoch  6, batch    20 | loss: 82.1205155CurrentTrain: epoch  6, batch    21 | loss: 63.5383894CurrentTrain: epoch  6, batch    22 | loss: 59.8317659CurrentTrain: epoch  6, batch    23 | loss: 83.7956868CurrentTrain: epoch  6, batch    24 | loss: 63.1509565CurrentTrain: epoch  6, batch    25 | loss: 53.1471758CurrentTrain: epoch  6, batch    26 | loss: 82.5975651CurrentTrain: epoch  6, batch    27 | loss: 63.4200508CurrentTrain: epoch  6, batch    28 | loss: 67.4468823CurrentTrain: epoch  6, batch    29 | loss: 85.9314925CurrentTrain: epoch  6, batch    30 | loss: 43.8506170CurrentTrain: epoch  6, batch    31 | loss: 40.9621434CurrentTrain: epoch  6, batch    32 | loss: 54.1908567CurrentTrain: epoch  6, batch    33 | loss: 53.5522019CurrentTrain: epoch  6, batch    34 | loss: 115.3325093CurrentTrain: epoch  6, batch    35 | loss: 79.8647348CurrentTrain: epoch  6, batch    36 | loss: 86.0707837CurrentTrain: epoch  6, batch    37 | loss: 65.2039448CurrentTrain: epoch  6, batch    38 | loss: 113.5070834CurrentTrain: epoch  6, batch    39 | loss: 87.7989294CurrentTrain: epoch  6, batch    40 | loss: 52.3989106CurrentTrain: epoch  6, batch    41 | loss: 64.7485404CurrentTrain: epoch  6, batch    42 | loss: 67.0781405CurrentTrain: epoch  6, batch    43 | loss: 86.6037812CurrentTrain: epoch  6, batch    44 | loss: 87.9547163CurrentTrain: epoch  6, batch    45 | loss: 65.6446980CurrentTrain: epoch  6, batch    46 | loss: 177.7022772CurrentTrain: epoch  6, batch    47 | loss: 68.3646342CurrentTrain: epoch  6, batch    48 | loss: 117.1218487CurrentTrain: epoch  6, batch    49 | loss: 63.2114222CurrentTrain: epoch  6, batch    50 | loss: 118.6325428CurrentTrain: epoch  6, batch    51 | loss: 62.8466657CurrentTrain: epoch  6, batch    52 | loss: 65.8148649CurrentTrain: epoch  6, batch    53 | loss: 81.2021898CurrentTrain: epoch  6, batch    54 | loss: 49.4974037CurrentTrain: epoch  6, batch    55 | loss: 85.7384364CurrentTrain: epoch  6, batch    56 | loss: 66.6181056CurrentTrain: epoch  6, batch    57 | loss: 56.0643285CurrentTrain: epoch  6, batch    58 | loss: 88.0729267CurrentTrain: epoch  6, batch    59 | loss: 83.4217572CurrentTrain: epoch  6, batch    60 | loss: 49.5026532CurrentTrain: epoch  6, batch    61 | loss: 43.6040343CurrentTrain: epoch  6, batch    62 | loss: 44.7918726CurrentTrain: epoch  6, batch    63 | loss: 57.7732351CurrentTrain: epoch  6, batch    64 | loss: 66.1753832CurrentTrain: epoch  6, batch    65 | loss: 64.3558576CurrentTrain: epoch  6, batch    66 | loss: 51.8855354CurrentTrain: epoch  6, batch    67 | loss: 84.1515216CurrentTrain: epoch  6, batch    68 | loss: 65.3499738CurrentTrain: epoch  6, batch    69 | loss: 63.3443307CurrentTrain: epoch  6, batch    70 | loss: 42.9966123CurrentTrain: epoch  6, batch    71 | loss: 47.1735281CurrentTrain: epoch  6, batch    72 | loss: 54.5798274CurrentTrain: epoch  6, batch    73 | loss: 53.3735498CurrentTrain: epoch  6, batch    74 | loss: 53.1207090CurrentTrain: epoch  6, batch    75 | loss: 52.0421008CurrentTrain: epoch  6, batch    76 | loss: 52.4040647CurrentTrain: epoch  6, batch    77 | loss: 64.4201581CurrentTrain: epoch  6, batch    78 | loss: 85.3666444CurrentTrain: epoch  6, batch    79 | loss: 62.1179699CurrentTrain: epoch  6, batch    80 | loss: 117.8102058CurrentTrain: epoch  6, batch    81 | loss: 44.7516652CurrentTrain: epoch  6, batch    82 | loss: 65.4621614CurrentTrain: epoch  6, batch    83 | loss: 84.0514822CurrentTrain: epoch  6, batch    84 | loss: 64.6254043CurrentTrain: epoch  6, batch    85 | loss: 86.1403674CurrentTrain: epoch  6, batch    86 | loss: 85.4428221CurrentTrain: epoch  6, batch    87 | loss: 54.5519549CurrentTrain: epoch  6, batch    88 | loss: 61.1496796CurrentTrain: epoch  6, batch    89 | loss: 71.0560066CurrentTrain: epoch  6, batch    90 | loss: 52.6806082CurrentTrain: epoch  6, batch    91 | loss: 81.1956479CurrentTrain: epoch  6, batch    92 | loss: 63.0231350CurrentTrain: epoch  6, batch    93 | loss: 49.3108729CurrentTrain: epoch  6, batch    94 | loss: 82.8010604CurrentTrain: epoch  6, batch    95 | loss: 54.9264653CurrentTrain: epoch  7, batch     0 | loss: 64.3564681CurrentTrain: epoch  7, batch     1 | loss: 86.0979415CurrentTrain: epoch  7, batch     2 | loss: 45.2485661CurrentTrain: epoch  7, batch     3 | loss: 52.6530054CurrentTrain: epoch  7, batch     4 | loss: 41.1987430CurrentTrain: epoch  7, batch     5 | loss: 65.2379394CurrentTrain: epoch  7, batch     6 | loss: 82.8854360CurrentTrain: epoch  7, batch     7 | loss: 43.6079854CurrentTrain: epoch  7, batch     8 | loss: 52.2572448CurrentTrain: epoch  7, batch     9 | loss: 52.9979854CurrentTrain: epoch  7, batch    10 | loss: 65.7039153CurrentTrain: epoch  7, batch    11 | loss: 50.2333825CurrentTrain: epoch  7, batch    12 | loss: 44.3737973CurrentTrain: epoch  7, batch    13 | loss: 84.3196855CurrentTrain: epoch  7, batch    14 | loss: 64.2365646CurrentTrain: epoch  7, batch    15 | loss: 46.6789415CurrentTrain: epoch  7, batch    16 | loss: 63.1203615CurrentTrain: epoch  7, batch    17 | loss: 65.2007463CurrentTrain: epoch  7, batch    18 | loss: 65.7323397CurrentTrain: epoch  7, batch    19 | loss: 84.5019554CurrentTrain: epoch  7, batch    20 | loss: 51.8499789CurrentTrain: epoch  7, batch    21 | loss: 116.8598075CurrentTrain: epoch  7, batch    22 | loss: 71.8151350CurrentTrain: epoch  7, batch    23 | loss: 119.0067166CurrentTrain: epoch  7, batch    24 | loss: 63.9364345CurrentTrain: epoch  7, batch    25 | loss: 115.8666871CurrentTrain: epoch  7, batch    26 | loss: 87.9478001CurrentTrain: epoch  7, batch    27 | loss: 81.0116355CurrentTrain: epoch  7, batch    28 | loss: 52.3585518CurrentTrain: epoch  7, batch    29 | loss: 52.0100509CurrentTrain: epoch  7, batch    30 | loss: 42.2282376CurrentTrain: epoch  7, batch    31 | loss: 65.6111542CurrentTrain: epoch  7, batch    32 | loss: 65.6380332CurrentTrain: epoch  7, batch    33 | loss: 54.6657027CurrentTrain: epoch  7, batch    34 | loss: 67.7735208CurrentTrain: epoch  7, batch    35 | loss: 84.2629875CurrentTrain: epoch  7, batch    36 | loss: 53.6694740CurrentTrain: epoch  7, batch    37 | loss: 67.6630021CurrentTrain: epoch  7, batch    38 | loss: 55.8628391CurrentTrain: epoch  7, batch    39 | loss: 62.1908840CurrentTrain: epoch  7, batch    40 | loss: 52.0261634CurrentTrain: epoch  7, batch    41 | loss: 80.3897582CurrentTrain: epoch  7, batch    42 | loss: 81.0203710CurrentTrain: epoch  7, batch    43 | loss: 67.1811731CurrentTrain: epoch  7, batch    44 | loss: 51.2597140CurrentTrain: epoch  7, batch    45 | loss: 43.4888718CurrentTrain: epoch  7, batch    46 | loss: 53.3474235CurrentTrain: epoch  7, batch    47 | loss: 62.1672504CurrentTrain: epoch  7, batch    48 | loss: 82.9564117CurrentTrain: epoch  7, batch    49 | loss: 64.2775461CurrentTrain: epoch  7, batch    50 | loss: 67.8014579CurrentTrain: epoch  7, batch    51 | loss: 65.8999536CurrentTrain: epoch  7, batch    52 | loss: 68.1674348CurrentTrain: epoch  7, batch    53 | loss: 64.3715444CurrentTrain: epoch  7, batch    54 | loss: 64.3314348CurrentTrain: epoch  7, batch    55 | loss: 64.6195113CurrentTrain: epoch  7, batch    56 | loss: 78.4228170CurrentTrain: epoch  7, batch    57 | loss: 68.7153403CurrentTrain: epoch  7, batch    58 | loss: 113.3221575CurrentTrain: epoch  7, batch    59 | loss: 53.1359604CurrentTrain: epoch  7, batch    60 | loss: 86.9119250CurrentTrain: epoch  7, batch    61 | loss: 51.4470288CurrentTrain: epoch  7, batch    62 | loss: 63.8418430CurrentTrain: epoch  7, batch    63 | loss: 84.9540439CurrentTrain: epoch  7, batch    64 | loss: 84.6607821CurrentTrain: epoch  7, batch    65 | loss: 53.5941134CurrentTrain: epoch  7, batch    66 | loss: 52.8225008CurrentTrain: epoch  7, batch    67 | loss: 43.2815827CurrentTrain: epoch  7, batch    68 | loss: 115.2377890CurrentTrain: epoch  7, batch    69 | loss: 62.9935949CurrentTrain: epoch  7, batch    70 | loss: 84.9931099CurrentTrain: epoch  7, batch    71 | loss: 60.8608714CurrentTrain: epoch  7, batch    72 | loss: 86.5995067CurrentTrain: epoch  7, batch    73 | loss: 85.2287775CurrentTrain: epoch  7, batch    74 | loss: 85.7245574CurrentTrain: epoch  7, batch    75 | loss: 116.0970854CurrentTrain: epoch  7, batch    76 | loss: 61.3950132CurrentTrain: epoch  7, batch    77 | loss: 115.7075047CurrentTrain: epoch  7, batch    78 | loss: 65.6255923CurrentTrain: epoch  7, batch    79 | loss: 53.5611585CurrentTrain: epoch  7, batch    80 | loss: 61.6431425CurrentTrain: epoch  7, batch    81 | loss: 67.3638384CurrentTrain: epoch  7, batch    82 | loss: 84.2353340CurrentTrain: epoch  7, batch    83 | loss: 66.3570324CurrentTrain: epoch  7, batch    84 | loss: 66.1963664CurrentTrain: epoch  7, batch    85 | loss: 66.8424838CurrentTrain: epoch  7, batch    86 | loss: 49.7249937CurrentTrain: epoch  7, batch    87 | loss: 84.1124971CurrentTrain: epoch  7, batch    88 | loss: 51.5365563CurrentTrain: epoch  7, batch    89 | loss: 53.4556224CurrentTrain: epoch  7, batch    90 | loss: 66.8477569CurrentTrain: epoch  7, batch    91 | loss: 113.4997518CurrentTrain: epoch  7, batch    92 | loss: 60.2610356CurrentTrain: epoch  7, batch    93 | loss: 44.8997865CurrentTrain: epoch  7, batch    94 | loss: 43.8002278CurrentTrain: epoch  7, batch    95 | loss: 53.2744208CurrentTrain: epoch  8, batch     0 | loss: 82.4111573CurrentTrain: epoch  8, batch     1 | loss: 64.6618914CurrentTrain: epoch  8, batch     2 | loss: 52.1823140CurrentTrain: epoch  8, batch     3 | loss: 51.4941000CurrentTrain: epoch  8, batch     4 | loss: 60.1767054CurrentTrain: epoch  8, batch     5 | loss: 81.1605384CurrentTrain: epoch  8, batch     6 | loss: 40.4426690CurrentTrain: epoch  8, batch     7 | loss: 84.2686054CurrentTrain: epoch  8, batch     8 | loss: 41.7162196CurrentTrain: epoch  8, batch     9 | loss: 44.3216518CurrentTrain: epoch  8, batch    10 | loss: 117.7723725CurrentTrain: epoch  8, batch    11 | loss: 55.9184133CurrentTrain: epoch  8, batch    12 | loss: 62.9960258CurrentTrain: epoch  8, batch    13 | loss: 81.3902922CurrentTrain: epoch  8, batch    14 | loss: 82.6405622CurrentTrain: epoch  8, batch    15 | loss: 117.6623922CurrentTrain: epoch  8, batch    16 | loss: 60.8182521CurrentTrain: epoch  8, batch    17 | loss: 82.3208152CurrentTrain: epoch  8, batch    18 | loss: 64.6942878CurrentTrain: epoch  8, batch    19 | loss: 51.6119429CurrentTrain: epoch  8, batch    20 | loss: 51.9740161CurrentTrain: epoch  8, batch    21 | loss: 78.4197388CurrentTrain: epoch  8, batch    22 | loss: 78.7382389CurrentTrain: epoch  8, batch    23 | loss: 42.3084775CurrentTrain: epoch  8, batch    24 | loss: 178.1012192CurrentTrain: epoch  8, batch    25 | loss: 117.5492503CurrentTrain: epoch  8, batch    26 | loss: 63.2274984CurrentTrain: epoch  8, batch    27 | loss: 65.6489325CurrentTrain: epoch  8, batch    28 | loss: 67.0167501CurrentTrain: epoch  8, batch    29 | loss: 65.4185503CurrentTrain: epoch  8, batch    30 | loss: 82.6459256CurrentTrain: epoch  8, batch    31 | loss: 84.0395103CurrentTrain: epoch  8, batch    32 | loss: 84.2206236CurrentTrain: epoch  8, batch    33 | loss: 50.6352279CurrentTrain: epoch  8, batch    34 | loss: 52.3394617CurrentTrain: epoch  8, batch    35 | loss: 65.6700285CurrentTrain: epoch  8, batch    36 | loss: 43.2809397CurrentTrain: epoch  8, batch    37 | loss: 66.5259506CurrentTrain: epoch  8, batch    38 | loss: 181.5590150CurrentTrain: epoch  8, batch    39 | loss: 60.3498105CurrentTrain: epoch  8, batch    40 | loss: 48.3743704CurrentTrain: epoch  8, batch    41 | loss: 65.7727411CurrentTrain: epoch  8, batch    42 | loss: 64.1016755CurrentTrain: epoch  8, batch    43 | loss: 82.5808329CurrentTrain: epoch  8, batch    44 | loss: 65.6025077CurrentTrain: epoch  8, batch    45 | loss: 64.3648672CurrentTrain: epoch  8, batch    46 | loss: 85.8821885CurrentTrain: epoch  8, batch    47 | loss: 84.0532600CurrentTrain: epoch  8, batch    48 | loss: 67.2442116CurrentTrain: epoch  8, batch    49 | loss: 62.9975309CurrentTrain: epoch  8, batch    50 | loss: 64.4120874CurrentTrain: epoch  8, batch    51 | loss: 63.1327458CurrentTrain: epoch  8, batch    52 | loss: 65.8786182CurrentTrain: epoch  8, batch    53 | loss: 61.6325077CurrentTrain: epoch  8, batch    54 | loss: 51.3018014CurrentTrain: epoch  8, batch    55 | loss: 85.8049765CurrentTrain: epoch  8, batch    56 | loss: 64.9726171CurrentTrain: epoch  8, batch    57 | loss: 82.3892896CurrentTrain: epoch  8, batch    58 | loss: 63.6927992CurrentTrain: epoch  8, batch    59 | loss: 115.1910566CurrentTrain: epoch  8, batch    60 | loss: 63.9633589CurrentTrain: epoch  8, batch    61 | loss: 42.0094444CurrentTrain: epoch  8, batch    62 | loss: 66.8305260CurrentTrain: epoch  8, batch    63 | loss: 65.4785187CurrentTrain: epoch  8, batch    64 | loss: 52.4093097CurrentTrain: epoch  8, batch    65 | loss: 53.7278434CurrentTrain: epoch  8, batch    66 | loss: 64.1060423CurrentTrain: epoch  8, batch    67 | loss: 67.0904995CurrentTrain: epoch  8, batch    68 | loss: 81.0973370CurrentTrain: epoch  8, batch    69 | loss: 50.8731548CurrentTrain: epoch  8, batch    70 | loss: 66.8062516CurrentTrain: epoch  8, batch    71 | loss: 62.9063133CurrentTrain: epoch  8, batch    72 | loss: 51.0530993CurrentTrain: epoch  8, batch    73 | loss: 82.8023979CurrentTrain: epoch  8, batch    74 | loss: 54.5196569CurrentTrain: epoch  8, batch    75 | loss: 46.5190302CurrentTrain: epoch  8, batch    76 | loss: 113.3252075CurrentTrain: epoch  8, batch    77 | loss: 42.2115042CurrentTrain: epoch  8, batch    78 | loss: 52.0963955CurrentTrain: epoch  8, batch    79 | loss: 94.7119360CurrentTrain: epoch  8, batch    80 | loss: 66.2939381CurrentTrain: epoch  8, batch    81 | loss: 113.1379362CurrentTrain: epoch  8, batch    82 | loss: 118.3906370CurrentTrain: epoch  8, batch    83 | loss: 49.0571057CurrentTrain: epoch  8, batch    84 | loss: 50.9861227CurrentTrain: epoch  8, batch    85 | loss: 85.3906712CurrentTrain: epoch  8, batch    86 | loss: 64.4820363CurrentTrain: epoch  8, batch    87 | loss: 81.6656719CurrentTrain: epoch  8, batch    88 | loss: 64.0829712CurrentTrain: epoch  8, batch    89 | loss: 62.7200416CurrentTrain: epoch  8, batch    90 | loss: 69.1438046CurrentTrain: epoch  8, batch    91 | loss: 48.0765208CurrentTrain: epoch  8, batch    92 | loss: 48.4830514CurrentTrain: epoch  8, batch    93 | loss: 115.2292868CurrentTrain: epoch  8, batch    94 | loss: 64.9476999CurrentTrain: epoch  8, batch    95 | loss: 55.5412927CurrentTrain: epoch  9, batch     0 | loss: 51.4373904CurrentTrain: epoch  9, batch     1 | loss: 81.2260739CurrentTrain: epoch  9, batch     2 | loss: 51.8275798CurrentTrain: epoch  9, batch     3 | loss: 84.0905999CurrentTrain: epoch  9, batch     4 | loss: 48.1960138CurrentTrain: epoch  9, batch     5 | loss: 81.5462718CurrentTrain: epoch  9, batch     6 | loss: 65.7006417CurrentTrain: epoch  9, batch     7 | loss: 117.5069915CurrentTrain: epoch  9, batch     8 | loss: 82.8915371CurrentTrain: epoch  9, batch     9 | loss: 63.3736368CurrentTrain: epoch  9, batch    10 | loss: 82.3180202CurrentTrain: epoch  9, batch    11 | loss: 51.3037092CurrentTrain: epoch  9, batch    12 | loss: 64.1450712CurrentTrain: epoch  9, batch    13 | loss: 85.0082164CurrentTrain: epoch  9, batch    14 | loss: 47.3613348CurrentTrain: epoch  9, batch    15 | loss: 45.6383147CurrentTrain: epoch  9, batch    16 | loss: 115.4331249CurrentTrain: epoch  9, batch    17 | loss: 84.1094642CurrentTrain: epoch  9, batch    18 | loss: 66.7744191CurrentTrain: epoch  9, batch    19 | loss: 85.8342009CurrentTrain: epoch  9, batch    20 | loss: 65.4474235CurrentTrain: epoch  9, batch    21 | loss: 60.8777524CurrentTrain: epoch  9, batch    22 | loss: 65.4211766CurrentTrain: epoch  9, batch    23 | loss: 63.4751152CurrentTrain: epoch  9, batch    24 | loss: 63.1673239CurrentTrain: epoch  9, batch    25 | loss: 65.7635879CurrentTrain: epoch  9, batch    26 | loss: 64.6287600CurrentTrain: epoch  9, batch    27 | loss: 51.9393374CurrentTrain: epoch  9, batch    28 | loss: 79.8115023CurrentTrain: epoch  9, batch    29 | loss: 82.5918082CurrentTrain: epoch  9, batch    30 | loss: 79.5813737CurrentTrain: epoch  9, batch    31 | loss: 66.7801171CurrentTrain: epoch  9, batch    32 | loss: 85.7433178CurrentTrain: epoch  9, batch    33 | loss: 62.3542417CurrentTrain: epoch  9, batch    34 | loss: 52.6366068CurrentTrain: epoch  9, batch    35 | loss: 117.4617861CurrentTrain: epoch  9, batch    36 | loss: 55.1399719CurrentTrain: epoch  9, batch    37 | loss: 64.2874778CurrentTrain: epoch  9, batch    38 | loss: 54.0392512CurrentTrain: epoch  9, batch    39 | loss: 64.4021081CurrentTrain: epoch  9, batch    40 | loss: 48.6754624CurrentTrain: epoch  9, batch    41 | loss: 52.9053775CurrentTrain: epoch  9, batch    42 | loss: 81.7084731CurrentTrain: epoch  9, batch    43 | loss: 84.0057169CurrentTrain: epoch  9, batch    44 | loss: 48.8551043CurrentTrain: epoch  9, batch    45 | loss: 82.6292119CurrentTrain: epoch  9, batch    46 | loss: 64.9534204CurrentTrain: epoch  9, batch    47 | loss: 59.4620364CurrentTrain: epoch  9, batch    48 | loss: 84.4033497CurrentTrain: epoch  9, batch    49 | loss: 44.9337325CurrentTrain: epoch  9, batch    50 | loss: 81.0746112CurrentTrain: epoch  9, batch    51 | loss: 110.1143729CurrentTrain: epoch  9, batch    52 | loss: 84.7421532CurrentTrain: epoch  9, batch    53 | loss: 62.8498627CurrentTrain: epoch  9, batch    54 | loss: 64.2246664CurrentTrain: epoch  9, batch    55 | loss: 61.9172923CurrentTrain: epoch  9, batch    56 | loss: 81.9904668CurrentTrain: epoch  9, batch    57 | loss: 61.7387546CurrentTrain: epoch  9, batch    58 | loss: 111.2752055CurrentTrain: epoch  9, batch    59 | loss: 64.0219329CurrentTrain: epoch  9, batch    60 | loss: 66.0080444CurrentTrain: epoch  9, batch    61 | loss: 115.2180749CurrentTrain: epoch  9, batch    62 | loss: 64.6186340CurrentTrain: epoch  9, batch    63 | loss: 61.9289742CurrentTrain: epoch  9, batch    64 | loss: 52.6734708CurrentTrain: epoch  9, batch    65 | loss: 62.9024969CurrentTrain: epoch  9, batch    66 | loss: 42.3486180CurrentTrain: epoch  9, batch    67 | loss: 52.9644663CurrentTrain: epoch  9, batch    68 | loss: 65.6272255CurrentTrain: epoch  9, batch    69 | loss: 81.7171054CurrentTrain: epoch  9, batch    70 | loss: 82.3626559CurrentTrain: epoch  9, batch    71 | loss: 79.7529640CurrentTrain: epoch  9, batch    72 | loss: 63.0945950CurrentTrain: epoch  9, batch    73 | loss: 62.9500891CurrentTrain: epoch  9, batch    74 | loss: 68.3548465CurrentTrain: epoch  9, batch    75 | loss: 53.6484270CurrentTrain: epoch  9, batch    76 | loss: 53.1056198CurrentTrain: epoch  9, batch    77 | loss: 79.5741199CurrentTrain: epoch  9, batch    78 | loss: 63.4444767CurrentTrain: epoch  9, batch    79 | loss: 54.2526630CurrentTrain: epoch  9, batch    80 | loss: 66.9646322CurrentTrain: epoch  9, batch    81 | loss: 76.8461664CurrentTrain: epoch  9, batch    82 | loss: 84.0462962CurrentTrain: epoch  9, batch    83 | loss: 84.3357309CurrentTrain: epoch  9, batch    84 | loss: 48.7849677CurrentTrain: epoch  9, batch    85 | loss: 115.1734992CurrentTrain: epoch  9, batch    86 | loss: 79.5613414CurrentTrain: epoch  9, batch    87 | loss: 51.9270576CurrentTrain: epoch  9, batch    88 | loss: 82.7429423CurrentTrain: epoch  9, batch    89 | loss: 85.7468181CurrentTrain: epoch  9, batch    90 | loss: 64.1397162CurrentTrain: epoch  9, batch    91 | loss: 84.0235057CurrentTrain: epoch  9, batch    92 | loss: 53.8097842CurrentTrain: epoch  9, batch    93 | loss: 64.0653678CurrentTrain: epoch  9, batch    94 | loss: 66.3924352CurrentTrain: epoch  9, batch    95 | loss: 65.5935304

F1 score per class: {32: 0.5225225225225225, 6: 0.6872246696035242, 19: 0.2631578947368421, 24: 0.743455497382199, 26: 0.8877551020408163, 29: 0.7945205479452054}
Micro-average F1 score: 0.7063129002744739
Weighted-average F1 score: 0.7021136068614089
F1 score per class: {32: 0.5146579804560261, 6: 0.6459143968871596, 19: 0.12121212121212122, 24: 0.6995073891625616, 26: 0.9029126213592233, 29: 0.680672268907563}
Micro-average F1 score: 0.6305343511450382
Weighted-average F1 score: 0.6053683284607682
F1 score per class: {32: 0.5148514851485149, 6: 0.6459143968871596, 19: 0.13043478260869565, 24: 0.6995073891625616, 26: 0.8899521531100478, 29: 0.680672268907563}
Micro-average F1 score: 0.6328725038402457
Weighted-average F1 score: 0.6104112303462486

F1 score per class: {32: 0.5225225225225225, 6: 0.6872246696035242, 19: 0.2631578947368421, 24: 0.743455497382199, 26: 0.8877551020408163, 29: 0.7945205479452054}
Micro-average F1 score: 0.7063129002744739
Weighted-average F1 score: 0.7021136068614089
F1 score per class: {32: 0.5146579804560261, 6: 0.6459143968871596, 19: 0.12121212121212122, 24: 0.6995073891625616, 26: 0.9029126213592233, 29: 0.680672268907563}
Micro-average F1 score: 0.6305343511450382
Weighted-average F1 score: 0.6053683284607682
F1 score per class: {32: 0.5148514851485149, 6: 0.6459143968871596, 19: 0.13043478260869565, 24: 0.6995073891625616, 26: 0.8899521531100478, 29: 0.680672268907563}
Micro-average F1 score: 0.6328725038402457
Weighted-average F1 score: 0.6104112303462486
cur_acc:  ['0.7063']
his_acc:  ['0.7063']
cur_acc des:  ['0.6305']
his_acc des:  ['0.6305']
cur_acc rrf:  ['0.6329']
his_acc rrf:  ['0.6329']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 80.7082631CurrentTrain: epoch  0, batch     1 | loss: 68.4601247CurrentTrain: epoch  0, batch     2 | loss: 80.3146822CurrentTrain: epoch  0, batch     3 | loss: 46.5388368CurrentTrain: epoch  1, batch     0 | loss: 94.4162342CurrentTrain: epoch  1, batch     1 | loss: 72.4023399CurrentTrain: epoch  1, batch     2 | loss: 75.1199973CurrentTrain: epoch  1, batch     3 | loss: 57.5466224CurrentTrain: epoch  2, batch     0 | loss: 121.6556469CurrentTrain: epoch  2, batch     1 | loss: 72.7499449CurrentTrain: epoch  2, batch     2 | loss: 70.6856732CurrentTrain: epoch  2, batch     3 | loss: 36.9608467CurrentTrain: epoch  3, batch     0 | loss: 58.4041815CurrentTrain: epoch  3, batch     1 | loss: 70.4807661CurrentTrain: epoch  3, batch     2 | loss: 70.8478824CurrentTrain: epoch  3, batch     3 | loss: 42.0610928CurrentTrain: epoch  4, batch     0 | loss: 71.1491879CurrentTrain: epoch  4, batch     1 | loss: 68.3793072CurrentTrain: epoch  4, batch     2 | loss: 55.1884504CurrentTrain: epoch  4, batch     3 | loss: 42.8631136CurrentTrain: epoch  5, batch     0 | loss: 120.1963806CurrentTrain: epoch  5, batch     1 | loss: 54.9915535CurrentTrain: epoch  5, batch     2 | loss: 52.4884344CurrentTrain: epoch  5, batch     3 | loss: 70.8010375CurrentTrain: epoch  6, batch     0 | loss: 66.5555141CurrentTrain: epoch  6, batch     1 | loss: 66.1076292CurrentTrain: epoch  6, batch     2 | loss: 52.6155100CurrentTrain: epoch  6, batch     3 | loss: 71.9685925CurrentTrain: epoch  7, batch     0 | loss: 65.8120348CurrentTrain: epoch  7, batch     1 | loss: 65.7673823CurrentTrain: epoch  7, batch     2 | loss: 62.5826736CurrentTrain: epoch  7, batch     3 | loss: 52.2476219CurrentTrain: epoch  8, batch     0 | loss: 53.3657782CurrentTrain: epoch  8, batch     1 | loss: 50.0599798CurrentTrain: epoch  8, batch     2 | loss: 86.1012563CurrentTrain: epoch  8, batch     3 | loss: 38.9654660CurrentTrain: epoch  9, batch     0 | loss: 66.8216530CurrentTrain: epoch  9, batch     1 | loss: 53.9330562CurrentTrain: epoch  9, batch     2 | loss: 67.0612461CurrentTrain: epoch  9, batch     3 | loss: 28.0645555
MemoryTrain:  epoch  0, batch     0 | loss: 0.5293184MemoryTrain:  epoch  1, batch     0 | loss: 0.3318328MemoryTrain:  epoch  2, batch     0 | loss: 0.2245448MemoryTrain:  epoch  3, batch     0 | loss: 0.1504932MemoryTrain:  epoch  4, batch     0 | loss: 0.1481951MemoryTrain:  epoch  5, batch     0 | loss: 0.1098637MemoryTrain:  epoch  6, batch     0 | loss: 0.1010537MemoryTrain:  epoch  7, batch     0 | loss: 0.0864768MemoryTrain:  epoch  8, batch     0 | loss: 0.0519785MemoryTrain:  epoch  9, batch     0 | loss: 0.0536202

F1 score per class: {32: 0.0, 33: 0.5714285714285714, 36: 0.0, 6: 0.5862068965517241, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.9444444444444444, 26: 0.0, 29: 0.24, 30: 0.5233644859813084}
Micro-average F1 score: 0.49586776859504134
Weighted-average F1 score: 0.4226585485426158
F1 score per class: {32: 0.0, 33: 0.5, 36: 0.0, 6: 0.5679012345679012, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.926829268292683, 26: 0.0, 29: 0.24242424242424243, 30: 0.5527638190954773}
Micro-average F1 score: 0.4739583333333333
Weighted-average F1 score: 0.43961731565885315
F1 score per class: {32: 0.0, 33: 0.5064377682403434, 36: 0.0, 6: 0.5679012345679012, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.9047619047619048, 26: 0.0, 29: 0.21052631578947367, 30: 0.5641025641025641}
Micro-average F1 score: 0.48348745046235136
Weighted-average F1 score: 0.4516631924162279

F1 score per class: {32: 0.4251968503937008, 33: 0.4634146341463415, 36: 0.6639676113360324, 6: 0.48226950354609927, 8: 0.08, 19: 0.6893203883495146, 20: 0.8279569892473119, 24: 0.9444444444444444, 26: 0.8056872037914692, 29: 0.14285714285714285, 30: 0.5137614678899083}
Micro-average F1 score: 0.6045650832819247
Weighted-average F1 score: 0.5967245725163384
F1 score per class: {32: 0.4578313253012048, 33: 0.33819241982507287, 36: 0.5838926174496645, 6: 0.431924882629108, 8: 0.17647058823529413, 19: 0.6513761467889908, 20: 0.837696335078534, 24: 0.7450980392156863, 26: 0.7372881355932204, 29: 0.10256410256410256, 30: 0.47413793103448276}
Micro-average F1 score: 0.5212389380530974
Weighted-average F1 score: 0.4968361167446866
F1 score per class: {32: 0.4459016393442623, 33: 0.3224043715846995, 36: 0.6, 6: 0.431924882629108, 8: 0.25, 19: 0.6826923076923077, 20: 0.837696335078534, 24: 0.7916666666666666, 26: 0.759825327510917, 29: 0.0963855421686747, 30: 0.49107142857142855}
Micro-average F1 score: 0.5289030496131087
Weighted-average F1 score: 0.5020283973486388
cur_acc:  ['0.7063', '0.4959']
his_acc:  ['0.7063', '0.6046']
cur_acc des:  ['0.6305', '0.4740']
his_acc des:  ['0.6305', '0.5212']
cur_acc rrf:  ['0.6329', '0.4835']
his_acc rrf:  ['0.6329', '0.5289']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 81.4521972CurrentTrain: epoch  0, batch     1 | loss: 75.7625507CurrentTrain: epoch  0, batch     2 | loss: 72.7627837CurrentTrain: epoch  0, batch     3 | loss: 96.4225012CurrentTrain: epoch  0, batch     4 | loss: 29.5627153CurrentTrain: epoch  1, batch     0 | loss: 60.6693349CurrentTrain: epoch  1, batch     1 | loss: 180.5544028CurrentTrain: epoch  1, batch     2 | loss: 97.2396292CurrentTrain: epoch  1, batch     3 | loss: 58.4048029CurrentTrain: epoch  1, batch     4 | loss: 20.7836727CurrentTrain: epoch  2, batch     0 | loss: 74.0694919CurrentTrain: epoch  2, batch     1 | loss: 66.1076054CurrentTrain: epoch  2, batch     2 | loss: 121.2846859CurrentTrain: epoch  2, batch     3 | loss: 74.0583841CurrentTrain: epoch  2, batch     4 | loss: 27.8967875CurrentTrain: epoch  3, batch     0 | loss: 66.8929830CurrentTrain: epoch  3, batch     1 | loss: 178.4253033CurrentTrain: epoch  3, batch     2 | loss: 86.6614627CurrentTrain: epoch  3, batch     3 | loss: 70.0341373CurrentTrain: epoch  3, batch     4 | loss: 17.7813121CurrentTrain: epoch  4, batch     0 | loss: 89.6758527CurrentTrain: epoch  4, batch     1 | loss: 68.1876633CurrentTrain: epoch  4, batch     2 | loss: 65.3382696CurrentTrain: epoch  4, batch     3 | loss: 67.7359645CurrentTrain: epoch  4, batch     4 | loss: 26.8560162CurrentTrain: epoch  5, batch     0 | loss: 68.4388420CurrentTrain: epoch  5, batch     1 | loss: 183.9567275CurrentTrain: epoch  5, batch     2 | loss: 53.9915947CurrentTrain: epoch  5, batch     3 | loss: 67.1388592CurrentTrain: epoch  5, batch     4 | loss: 15.7572940CurrentTrain: epoch  6, batch     0 | loss: 87.4783364CurrentTrain: epoch  6, batch     1 | loss: 67.2575561CurrentTrain: epoch  6, batch     2 | loss: 67.8000946CurrentTrain: epoch  6, batch     3 | loss: 53.6672131CurrentTrain: epoch  6, batch     4 | loss: 17.2859804CurrentTrain: epoch  7, batch     0 | loss: 53.5785005CurrentTrain: epoch  7, batch     1 | loss: 64.7830662CurrentTrain: epoch  7, batch     2 | loss: 54.6265439CurrentTrain: epoch  7, batch     3 | loss: 85.8755112CurrentTrain: epoch  7, batch     4 | loss: 59.8753883CurrentTrain: epoch  8, batch     0 | loss: 67.1994287CurrentTrain: epoch  8, batch     1 | loss: 82.7387314CurrentTrain: epoch  8, batch     2 | loss: 85.3568757CurrentTrain: epoch  8, batch     3 | loss: 64.3429576CurrentTrain: epoch  8, batch     4 | loss: 15.6008897CurrentTrain: epoch  9, batch     0 | loss: 114.7906724CurrentTrain: epoch  9, batch     1 | loss: 83.9838211CurrentTrain: epoch  9, batch     2 | loss: 65.6523882CurrentTrain: epoch  9, batch     3 | loss: 61.9026726CurrentTrain: epoch  9, batch     4 | loss: 26.7862939
MemoryTrain:  epoch  0, batch     0 | loss: 0.7349554MemoryTrain:  epoch  1, batch     0 | loss: 0.7024849MemoryTrain:  epoch  2, batch     0 | loss: 0.5072754MemoryTrain:  epoch  3, batch     0 | loss: 0.3417275MemoryTrain:  epoch  4, batch     0 | loss: 0.2826395MemoryTrain:  epoch  5, batch     0 | loss: 0.2074879MemoryTrain:  epoch  6, batch     0 | loss: 0.1734661MemoryTrain:  epoch  7, batch     0 | loss: 0.1434135MemoryTrain:  epoch  8, batch     0 | loss: 0.1209482MemoryTrain:  epoch  9, batch     0 | loss: 0.1046350

F1 score per class: {32: 0.4, 33: 0.0, 2: 0.25225225225225223, 36: 0.6276595744680851, 6: 0.0, 39: 0.0, 11: 0.0, 12: 0.0, 19: 0.21621621621621623, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.11764705882352941}
Micro-average F1 score: 0.36909871244635195
Weighted-average F1 score: 0.3241257319908265
F1 score per class: {32: 0.23880597014925373, 33: 0.0, 2: 0.0, 36: 0.5615763546798029, 6: 0.5970149253731343, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.18518518518518517, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.34285714285714286}
Micro-average F1 score: 0.38095238095238093
Weighted-average F1 score: 0.32408175506339854
F1 score per class: {32: 0.25, 33: 0.0, 2: 0.0, 36: 0.3969465648854962, 6: 0.5942028985507246, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.18518518518518517, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.2222222222222222}
Micro-average F1 score: 0.32503276539973786
Weighted-average F1 score: 0.2698636394161972

F1 score per class: {32: 0.27586206896551724, 33: 0.3927272727272727, 2: 0.11494252873563218, 36: 0.18543046357615894, 6: 0.37579617834394907, 39: 0.6521739130434783, 8: 0.4161849710982659, 11: 0.12903225806451613, 12: 0.6731707317073171, 19: 0.08888888888888889, 20: 0.84375, 24: 0.9473684210526315, 26: 0.7678571428571429, 28: 0.13043478260869565, 29: 0.4423076923076923, 30: 0.1}
Micro-average F1 score: 0.48078641644325293
Weighted-average F1 score: 0.47386356015159375
F1 score per class: {32: 0.125, 33: 0.3865979381443299, 2: 0.33980582524271846, 36: 0.3573667711598746, 6: 0.25356576862123614, 39: 0.5618729096989966, 8: 0.3684210526315789, 11: 0.14583333333333334, 12: 0.6729857819905213, 19: 0.0847457627118644, 20: 0.7887323943661971, 24: 0.8260869565217391, 26: 0.6496350364963503, 28: 0.08955223880597014, 29: 0.41830065359477125, 30: 0.13333333333333333}
Micro-average F1 score: 0.4020767233919815
Weighted-average F1 score: 0.3734335853593466
F1 score per class: {32: 0.14285714285714285, 33: 0.3916913946587537, 2: 0.2603550295857988, 36: 0.2639593908629442, 6: 0.2411764705882353, 39: 0.5793103448275863, 8: 0.3598326359832636, 11: 0.19607843137254902, 12: 0.6761904761904762, 19: 0.07751937984496124, 20: 0.82, 24: 0.8085106382978723, 26: 0.6848249027237354, 28: 0.05309734513274336, 29: 0.39361702127659576, 30: 0.10344827586206896}
Micro-average F1 score: 0.393042416844675
Weighted-average F1 score: 0.3621153926747564
cur_acc:  ['0.7063', '0.4959', '0.3691']
his_acc:  ['0.7063', '0.6046', '0.4808']
cur_acc des:  ['0.6305', '0.4740', '0.3810']
his_acc des:  ['0.6305', '0.5212', '0.4021']
cur_acc rrf:  ['0.6329', '0.4835', '0.3250']
his_acc rrf:  ['0.6329', '0.5289', '0.3930']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 66.2271468CurrentTrain: epoch  0, batch     1 | loss: 133.0315384CurrentTrain: epoch  0, batch     2 | loss: 93.6914150CurrentTrain: epoch  0, batch     3 | loss: 130.8958165CurrentTrain: epoch  0, batch     4 | loss: 62.0548098CurrentTrain: epoch  1, batch     0 | loss: 95.9485983CurrentTrain: epoch  1, batch     1 | loss: 69.4469744CurrentTrain: epoch  1, batch     2 | loss: 124.6973288CurrentTrain: epoch  1, batch     3 | loss: 72.6974425CurrentTrain: epoch  1, batch     4 | loss: 58.7872594CurrentTrain: epoch  2, batch     0 | loss: 87.9245534CurrentTrain: epoch  2, batch     1 | loss: 86.5619441CurrentTrain: epoch  2, batch     2 | loss: 58.6318992CurrentTrain: epoch  2, batch     3 | loss: 75.7863912CurrentTrain: epoch  2, batch     4 | loss: 57.0170905CurrentTrain: epoch  3, batch     0 | loss: 90.9853391CurrentTrain: epoch  3, batch     1 | loss: 67.6540550CurrentTrain: epoch  3, batch     2 | loss: 90.5445929CurrentTrain: epoch  3, batch     3 | loss: 86.9438790CurrentTrain: epoch  3, batch     4 | loss: 42.7675779CurrentTrain: epoch  4, batch     0 | loss: 69.5199723CurrentTrain: epoch  4, batch     1 | loss: 69.0783337CurrentTrain: epoch  4, batch     2 | loss: 117.9928027CurrentTrain: epoch  4, batch     3 | loss: 62.5104121CurrentTrain: epoch  4, batch     4 | loss: 55.5842339CurrentTrain: epoch  5, batch     0 | loss: 88.6238327CurrentTrain: epoch  5, batch     1 | loss: 66.0718852CurrentTrain: epoch  5, batch     2 | loss: 54.6250500CurrentTrain: epoch  5, batch     3 | loss: 84.5724411CurrentTrain: epoch  5, batch     4 | loss: 56.1869950CurrentTrain: epoch  6, batch     0 | loss: 85.9444724CurrentTrain: epoch  6, batch     1 | loss: 116.3360164CurrentTrain: epoch  6, batch     2 | loss: 83.9081806CurrentTrain: epoch  6, batch     3 | loss: 52.5397547CurrentTrain: epoch  6, batch     4 | loss: 72.9418955CurrentTrain: epoch  7, batch     0 | loss: 85.0758472CurrentTrain: epoch  7, batch     1 | loss: 84.5327449CurrentTrain: epoch  7, batch     2 | loss: 83.7359643CurrentTrain: epoch  7, batch     3 | loss: 82.1813594CurrentTrain: epoch  7, batch     4 | loss: 51.9706238CurrentTrain: epoch  8, batch     0 | loss: 52.6277212CurrentTrain: epoch  8, batch     1 | loss: 115.5701278CurrentTrain: epoch  8, batch     2 | loss: 53.9512793CurrentTrain: epoch  8, batch     3 | loss: 64.0598295CurrentTrain: epoch  8, batch     4 | loss: 116.5375464CurrentTrain: epoch  9, batch     0 | loss: 66.0948363CurrentTrain: epoch  9, batch     1 | loss: 84.6390223CurrentTrain: epoch  9, batch     2 | loss: 61.4725253CurrentTrain: epoch  9, batch     3 | loss: 113.3144371CurrentTrain: epoch  9, batch     4 | loss: 54.0304166
MemoryTrain:  epoch  0, batch     0 | loss: 0.6223959MemoryTrain:  epoch  1, batch     0 | loss: 0.5670562MemoryTrain:  epoch  2, batch     0 | loss: 0.4109948MemoryTrain:  epoch  3, batch     0 | loss: 0.3528984MemoryTrain:  epoch  4, batch     0 | loss: 0.2775143MemoryTrain:  epoch  5, batch     0 | loss: 0.2258359MemoryTrain:  epoch  6, batch     0 | loss: 0.1966530MemoryTrain:  epoch  7, batch     0 | loss: 0.1438613MemoryTrain:  epoch  8, batch     0 | loss: 0.1316866MemoryTrain:  epoch  9, batch     0 | loss: 0.1240569

F1 score per class: {33: 0.0, 2: 0.9405940594059405, 5: 0.0, 6: 0.4166666666666667, 10: 0.0, 11: 0.0, 12: 0.6176470588235294, 16: 0.3333333333333333, 17: 0.3516483516483517, 18: 0.0, 20: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.5359477124183006
Weighted-average F1 score: 0.4694450196935845
F1 score per class: {2: 0.0, 5: 0.5806451612903226, 6: 0.0, 8: 0.0, 10: 0.49142857142857144, 11: 0.0, 12: 0.0, 16: 0.5483870967741935, 17: 0.3333333333333333, 18: 0.391304347826087, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.37166324435318276
Weighted-average F1 score: 0.316422282170945
F1 score per class: {2: 0.0, 5: 0.5963855421686747, 6: 0.0, 8: 0.0, 10: 0.5052631578947369, 11: 0.0, 12: 0.0, 16: 0.5714285714285714, 17: 0.5, 18: 0.42718446601941745, 19: 0.0, 20: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.410752688172043
Weighted-average F1 score: 0.3583890636209224

F1 score per class: {2: 0.23255813953488372, 5: 0.8837209302325582, 6: 0.33210332103321033, 8: 0.04819277108433735, 10: 0.2777777777777778, 11: 0.021052631578947368, 12: 0.2616033755274262, 16: 0.56, 17: 0.10810810810810811, 18: 0.15841584158415842, 19: 0.6388888888888888, 20: 0.4094488188976378, 24: 0.07142857142857142, 26: 0.67, 28: 0.10526315789473684, 29: 0.7888888888888889, 30: 0.9142857142857143, 32: 0.7981220657276995, 33: 0.15789473684210525, 36: 0.0, 39: 0.2222222222222222}
Micro-average F1 score: 0.44553335846211833
Weighted-average F1 score: 0.45347651471329176
F1 score per class: {2: 0.16666666666666666, 5: 0.44097995545657015, 6: 0.33879781420765026, 8: 0.35772357723577236, 10: 0.2792207792207792, 11: 0.18461538461538463, 12: 0.2571428571428571, 16: 0.5230769230769231, 17: 0.07476635514018691, 18: 0.1875, 19: 0.5693430656934306, 20: 0.375, 24: 0.125, 26: 0.6507177033492823, 28: 0.10752688172043011, 29: 0.7920792079207921, 30: 0.8260869565217391, 32: 0.6170212765957447, 33: 0.05714285714285714, 36: 0.39285714285714285, 39: 0.13636363636363635}
Micro-average F1 score: 0.3795620437956204
Weighted-average F1 score: 0.3576120060668957
F1 score per class: {2: 0.15625, 5: 0.47255369928400953, 6: 0.345821325648415, 8: 0.18556701030927836, 10: 0.2719546742209632, 11: 0.07079646017699115, 12: 0.2743362831858407, 16: 0.5217391304347826, 17: 0.11940298507462686, 18: 0.18723404255319148, 19: 0.608, 20: 0.37185929648241206, 24: 0.2564102564102564, 26: 0.68, 28: 0.10752688172043011, 29: 0.8043478260869565, 30: 0.7916666666666666, 32: 0.6825396825396826, 33: 0.05263157894736842, 36: 0.12987012987012986, 39: 0.24}
Micro-average F1 score: 0.38420204191295004
Weighted-average F1 score: 0.3699010814906734
cur_acc:  ['0.7063', '0.4959', '0.3691', '0.5359']
his_acc:  ['0.7063', '0.6046', '0.4808', '0.4455']
cur_acc des:  ['0.6305', '0.4740', '0.3810', '0.3717']
his_acc des:  ['0.6305', '0.5212', '0.4021', '0.3796']
cur_acc rrf:  ['0.6329', '0.4835', '0.3250', '0.4108']
his_acc rrf:  ['0.6329', '0.5289', '0.3930', '0.3842']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 103.7218895CurrentTrain: epoch  0, batch     1 | loss: 83.3401463CurrentTrain: epoch  0, batch     2 | loss: 86.7500116CurrentTrain: epoch  0, batch     3 | loss: 92.7454378CurrentTrain: epoch  0, batch     4 | loss: 109.2368754CurrentTrain: epoch  1, batch     0 | loss: 75.3890865CurrentTrain: epoch  1, batch     1 | loss: 129.1566493CurrentTrain: epoch  1, batch     2 | loss: 75.7440180CurrentTrain: epoch  1, batch     3 | loss: 61.0144093CurrentTrain: epoch  1, batch     4 | loss: 68.8412661CurrentTrain: epoch  2, batch     0 | loss: 185.4880260CurrentTrain: epoch  2, batch     1 | loss: 69.6377155CurrentTrain: epoch  2, batch     2 | loss: 58.2730788CurrentTrain: epoch  2, batch     3 | loss: 71.1957384CurrentTrain: epoch  2, batch     4 | loss: 68.4068488CurrentTrain: epoch  3, batch     0 | loss: 88.2364169CurrentTrain: epoch  3, batch     1 | loss: 68.7250297CurrentTrain: epoch  3, batch     2 | loss: 90.5803452CurrentTrain: epoch  3, batch     3 | loss: 88.7378224CurrentTrain: epoch  3, batch     4 | loss: 101.2555485CurrentTrain: epoch  4, batch     0 | loss: 67.9777630CurrentTrain: epoch  4, batch     1 | loss: 57.6212526CurrentTrain: epoch  4, batch     2 | loss: 57.9786220CurrentTrain: epoch  4, batch     3 | loss: 88.6541569CurrentTrain: epoch  4, batch     4 | loss: 48.5437490CurrentTrain: epoch  5, batch     0 | loss: 118.3946149CurrentTrain: epoch  5, batch     1 | loss: 55.7697859CurrentTrain: epoch  5, batch     2 | loss: 84.8741040CurrentTrain: epoch  5, batch     3 | loss: 68.5957185CurrentTrain: epoch  5, batch     4 | loss: 45.2468539CurrentTrain: epoch  6, batch     0 | loss: 53.9590955CurrentTrain: epoch  6, batch     1 | loss: 86.2112438CurrentTrain: epoch  6, batch     2 | loss: 67.8628127CurrentTrain: epoch  6, batch     3 | loss: 118.5768549CurrentTrain: epoch  6, batch     4 | loss: 36.3887177CurrentTrain: epoch  7, batch     0 | loss: 67.8484546CurrentTrain: epoch  7, batch     1 | loss: 54.1611829CurrentTrain: epoch  7, batch     2 | loss: 65.9173359CurrentTrain: epoch  7, batch     3 | loss: 67.3422536CurrentTrain: epoch  7, batch     4 | loss: 64.2478816CurrentTrain: epoch  8, batch     0 | loss: 53.2961843CurrentTrain: epoch  8, batch     1 | loss: 83.7082165CurrentTrain: epoch  8, batch     2 | loss: 64.9838999CurrentTrain: epoch  8, batch     3 | loss: 85.6734170CurrentTrain: epoch  8, batch     4 | loss: 104.4029444CurrentTrain: epoch  9, batch     0 | loss: 85.8982759CurrentTrain: epoch  9, batch     1 | loss: 64.3930755CurrentTrain: epoch  9, batch     2 | loss: 116.0728311CurrentTrain: epoch  9, batch     3 | loss: 81.9524782CurrentTrain: epoch  9, batch     4 | loss: 28.2269607
MemoryTrain:  epoch  0, batch     0 | loss: 0.5759336MemoryTrain:  epoch  1, batch     0 | loss: 0.4993184MemoryTrain:  epoch  2, batch     0 | loss: 0.4026195MemoryTrain:  epoch  3, batch     0 | loss: 0.3717315MemoryTrain:  epoch  4, batch     0 | loss: 0.2906263MemoryTrain:  epoch  5, batch     0 | loss: 0.2426363MemoryTrain:  epoch  6, batch     0 | loss: 0.1978185MemoryTrain:  epoch  7, batch     0 | loss: 0.1482308MemoryTrain:  epoch  8, batch     0 | loss: 0.1457206MemoryTrain:  epoch  9, batch     0 | loss: 0.1411758

F1 score per class: {1: 0.1830065359477124, 2: 0.0, 3: 0.4675324675324675, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 12: 0.0, 14: 0.13008130081300814, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.4075471698113208, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.3023255813953488, 39: 0.0}
Micro-average F1 score: 0.23539373412362405
Weighted-average F1 score: 0.19828446953741716
F1 score per class: {1: 0.14414414414414414, 2: 0.0, 3: 0.49169435215946844, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.08888888888888889, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.3482849604221636, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.5827814569536424, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.23742227247032222
Weighted-average F1 score: 0.21104216935119458
F1 score per class: {1: 0.16140350877192983, 2: 0.0, 3: 0.5701357466063348, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 12: 0.0, 14: 0.11290322580645161, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.36683417085427134, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.5906040268456376, 39: 0.0}
Micro-average F1 score: 0.2491103202846975
Weighted-average F1 score: 0.21591993732716186

F1 score per class: {1: 0.1462140992167102, 2: 0.2222222222222222, 3: 0.3829787234042553, 5: 0.8878504672897196, 6: 0.32919254658385094, 8: 0.07058823529411765, 10: 0.2958904109589041, 11: 0.0, 12: 0.25757575757575757, 14: 0.10126582278481013, 16: 0.6086956521739131, 17: 0.0, 18: 0.1407035175879397, 19: 0.35406698564593303, 20: 0.41025641025641024, 22: 0.3312883435582822, 24: 0.03125, 26: 0.6666666666666666, 28: 0.10869565217391304, 29: 0.7157894736842105, 30: 0.972972972972973, 32: 0.5472972972972973, 33: 0.13953488372093023, 34: 0.16049382716049382, 36: 0.05714285714285714, 39: 0.1111111111111111}
Micro-average F1 score: 0.3431673353788058
Weighted-average F1 score: 0.3329721039254095
F1 score per class: {1: 0.0935672514619883, 2: 0.14457831325301204, 3: 0.3064182194616977, 5: 0.42283298097251587, 6: 0.2879581151832461, 8: 0.3386243386243386, 10: 0.23236514522821577, 11: 0.12389380530973451, 12: 0.24083769633507854, 14: 0.05319148936170213, 16: 0.5432098765432098, 17: 0.07142857142857142, 18: 0.11049723756906077, 19: 0.35911602209944754, 20: 0.3474178403755869, 22: 0.27906976744186046, 24: 0.024691358024691357, 26: 0.6330275229357798, 28: 0.07352941176470588, 29: 0.7289719626168224, 30: 0.7755102040816326, 32: 0.45, 33: 0.06779661016949153, 34: 0.23466666666666666, 36: 0.42424242424242425, 39: 0.12844036697247707}
Micro-average F1 score: 0.28365106874638935
Weighted-average F1 score: 0.26782882508173816
F1 score per class: {1: 0.1038374717832957, 2: 0.14634146341463414, 3: 0.3761194029850746, 5: 0.4520547945205479, 6: 0.30025445292620867, 8: 0.19230769230769232, 10: 0.21371610845295055, 11: 0.021052631578947368, 12: 0.228099173553719, 14: 0.0707070707070707, 16: 0.5569620253164557, 17: 0.10344827586206896, 18: 0.10194174757281553, 19: 0.375, 20: 0.33183856502242154, 22: 0.292, 24: 0.02564102564102564, 26: 0.6634615384615384, 28: 0.07246376811594203, 29: 0.7352941176470589, 30: 0.7450980392156863, 32: 0.4668587896253602, 33: 0.07017543859649122, 34: 0.23655913978494625, 36: 0.2708333333333333, 39: 0.17777777777777778}
Micro-average F1 score: 0.28506306032517853
Weighted-average F1 score: 0.2715477155187698
cur_acc:  ['0.7063', '0.4959', '0.3691', '0.5359', '0.2354']
his_acc:  ['0.7063', '0.6046', '0.4808', '0.4455', '0.3432']
cur_acc des:  ['0.6305', '0.4740', '0.3810', '0.3717', '0.2374']
his_acc des:  ['0.6305', '0.5212', '0.4021', '0.3796', '0.2837']
cur_acc rrf:  ['0.6329', '0.4835', '0.3250', '0.4108', '0.2491']
his_acc rrf:  ['0.6329', '0.5289', '0.3930', '0.3842', '0.2851']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 83.4546192CurrentTrain: epoch  0, batch     1 | loss: 69.4921814CurrentTrain: epoch  0, batch     2 | loss: 79.3163031CurrentTrain: epoch  0, batch     3 | loss: 11.0655527CurrentTrain: epoch  1, batch     0 | loss: 66.8452840CurrentTrain: epoch  1, batch     1 | loss: 62.1673469CurrentTrain: epoch  1, batch     2 | loss: 60.7792799CurrentTrain: epoch  1, batch     3 | loss: 27.7981790CurrentTrain: epoch  2, batch     0 | loss: 91.5334395CurrentTrain: epoch  2, batch     1 | loss: 57.3415979CurrentTrain: epoch  2, batch     2 | loss: 54.9296305CurrentTrain: epoch  2, batch     3 | loss: 6.6149097CurrentTrain: epoch  3, batch     0 | loss: 56.8040729CurrentTrain: epoch  3, batch     1 | loss: 67.3621239CurrentTrain: epoch  3, batch     2 | loss: 67.9418064CurrentTrain: epoch  3, batch     3 | loss: 3.3198648CurrentTrain: epoch  4, batch     0 | loss: 68.6372606CurrentTrain: epoch  4, batch     1 | loss: 51.7929242CurrentTrain: epoch  4, batch     2 | loss: 64.7307579CurrentTrain: epoch  4, batch     3 | loss: 27.4739011CurrentTrain: epoch  5, batch     0 | loss: 81.1810536CurrentTrain: epoch  5, batch     1 | loss: 53.1985862CurrentTrain: epoch  5, batch     2 | loss: 65.1709552CurrentTrain: epoch  5, batch     3 | loss: 12.0569254CurrentTrain: epoch  6, batch     0 | loss: 49.4255363CurrentTrain: epoch  6, batch     1 | loss: 54.9813176CurrentTrain: epoch  6, batch     2 | loss: 114.3699297CurrentTrain: epoch  6, batch     3 | loss: 5.8599819CurrentTrain: epoch  7, batch     0 | loss: 61.3011089CurrentTrain: epoch  7, batch     1 | loss: 67.4688942CurrentTrain: epoch  7, batch     2 | loss: 60.6734718CurrentTrain: epoch  7, batch     3 | loss: 27.4603207CurrentTrain: epoch  8, batch     0 | loss: 52.8794864CurrentTrain: epoch  8, batch     1 | loss: 62.7846779CurrentTrain: epoch  8, batch     2 | loss: 51.9629921CurrentTrain: epoch  8, batch     3 | loss: 12.5433356CurrentTrain: epoch  9, batch     0 | loss: 65.2538689CurrentTrain: epoch  9, batch     1 | loss: 50.6943351CurrentTrain: epoch  9, batch     2 | loss: 51.9113608CurrentTrain: epoch  9, batch     3 | loss: 9.8564195
MemoryTrain:  epoch  0, batch     0 | loss: 0.4090919MemoryTrain:  epoch  1, batch     0 | loss: 0.3656988MemoryTrain:  epoch  2, batch     0 | loss: 0.2830735MemoryTrain:  epoch  3, batch     0 | loss: 0.2660534MemoryTrain:  epoch  4, batch     0 | loss: 0.2013532MemoryTrain:  epoch  5, batch     0 | loss: 0.1764587MemoryTrain:  epoch  6, batch     0 | loss: 0.1485683MemoryTrain:  epoch  7, batch     0 | loss: 0.1185777MemoryTrain:  epoch  8, batch     0 | loss: 0.1387851MemoryTrain:  epoch  9, batch     0 | loss: 0.1011802

F1 score per class: {32: 0.0, 1: 0.0, 34: 0.2222222222222222, 3: 0.9433962264150944, 7: 0.0, 40: 0.0, 9: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.3076923076923077, 22: 0.0, 26: 0.4, 27: 0.0, 28: 0.0, 31: 0.4745762711864407}
Micro-average F1 score: 0.4041095890410959
Weighted-average F1 score: 0.3106146098956094
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.6153846153846154, 9: 0.819672131147541, 10: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 26: 0.0, 27: 0.4864864864864865, 28: 0.0, 31: 0.3333333333333333, 32: 0.0, 34: 0.0, 36: 0.0, 40: 0.5376344086021505}
Micro-average F1 score: 0.375
Weighted-average F1 score: 0.3061788817959116
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.6153846153846154, 9: 0.8620689655172413, 10: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 26: 0.0, 27: 0.42105263157894735, 28: 0.0, 31: 0.36363636363636365, 32: 0.0, 34: 0.0, 40: 0.5494505494505495}
Micro-average F1 score: 0.38611713665943603
Weighted-average F1 score: 0.3139645231100641

F1 score per class: {1: 0.11023622047244094, 2: 0.23255813953488372, 3: 0.3333333333333333, 5: 0.897196261682243, 6: 0.035398230088495575, 7: 0.010638297872340425, 8: 0.047619047619047616, 9: 0.9259259259259259, 10: 0.17721518987341772, 11: 0.0, 12: 0.21495327102803738, 14: 0.10050251256281408, 16: 0.5797101449275363, 17: 0.0, 18: 0.18181818181818182, 19: 0.4, 20: 0.391304347826087, 22: 0.42857142857142855, 24: 0.04819277108433735, 26: 0.6467661691542289, 27: 0.039603960396039604, 28: 0.08955223880597014, 29: 0.6739130434782609, 30: 0.972972972972973, 31: 0.0625, 32: 0.576271186440678, 33: 0.18181818181818182, 34: 0.30120481927710846, 36: 0.057971014492753624, 39: 0.13333333333333333, 40: 0.2814070351758794}
Micro-average F1 score: 0.3189305323466221
Weighted-average F1 score: 0.30270308391648537
F1 score per class: {1: 0.07756232686980609, 2: 0.0847457627118644, 3: 0.3458646616541353, 5: 0.36429872495446264, 6: 0.09022556390977443, 7: 0.028985507246376812, 8: 0.26229508196721313, 9: 0.6756756756756757, 10: 0.2408026755852843, 11: 0.23333333333333334, 12: 0.24686192468619247, 14: 0.047619047619047616, 16: 0.5238095238095238, 17: 0.058823529411764705, 18: 0.0958904109589041, 19: 0.3861671469740634, 20: 0.25, 22: 0.45714285714285713, 24: 0.049586776859504134, 26: 0.6339285714285714, 27: 0.04556962025316456, 28: 0.05319148936170213, 29: 0.7403846153846154, 30: 0.7727272727272727, 31: 0.024844720496894408, 32: 0.5161290322580645, 33: 0.1282051282051282, 34: 0.2026431718061674, 36: 0.45714285714285713, 39: 0.13333333333333333, 40: 0.25906735751295334}
Micro-average F1 score: 0.25522612378740267
Weighted-average F1 score: 0.23020318005676854
F1 score per class: {1: 0.0797872340425532, 2: 0.09090909090909091, 3: 0.38596491228070173, 5: 0.38461538461538464, 6: 0.078125, 7: 0.02909090909090909, 8: 0.20618556701030927, 9: 0.7692307692307693, 10: 0.21787709497206703, 11: 0.061224489795918366, 12: 0.24701195219123506, 14: 0.044444444444444446, 16: 0.5432098765432098, 17: 0.14285714285714285, 18: 0.09462365591397849, 19: 0.4166666666666667, 20: 0.2482758620689655, 22: 0.4574468085106383, 24: 0.035398230088495575, 26: 0.647887323943662, 27: 0.041666666666666664, 28: 0.06666666666666667, 29: 0.7319587628865979, 30: 0.782608695652174, 31: 0.041237113402061855, 32: 0.5277777777777778, 33: 0.10989010989010989, 34: 0.19327731092436976, 36: 0.29213483146067415, 39: 0.2222222222222222, 40: 0.2544529262086514}
Micro-average F1 score: 0.2533238027162259
Weighted-average F1 score: 0.22995986806232788
cur_acc:  ['0.7063', '0.4959', '0.3691', '0.5359', '0.2354', '0.4041']
his_acc:  ['0.7063', '0.6046', '0.4808', '0.4455', '0.3432', '0.3189']
cur_acc des:  ['0.6305', '0.4740', '0.3810', '0.3717', '0.2374', '0.3750']
his_acc des:  ['0.6305', '0.5212', '0.4021', '0.3796', '0.2837', '0.2552']
cur_acc rrf:  ['0.6329', '0.4835', '0.3250', '0.4108', '0.2491', '0.3861']
his_acc rrf:  ['0.6329', '0.5289', '0.3930', '0.3842', '0.2851', '0.2533']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 77.8159126CurrentTrain: epoch  0, batch     1 | loss: 76.4941155CurrentTrain: epoch  0, batch     2 | loss: 66.0665826CurrentTrain: epoch  0, batch     3 | loss: 127.5975119CurrentTrain: epoch  1, batch     0 | loss: 61.1377361CurrentTrain: epoch  1, batch     1 | loss: 73.2111633CurrentTrain: epoch  1, batch     2 | loss: 59.8040177CurrentTrain: epoch  1, batch     3 | loss: 127.0609975CurrentTrain: epoch  2, batch     0 | loss: 68.6117224CurrentTrain: epoch  2, batch     1 | loss: 69.8750640CurrentTrain: epoch  2, batch     2 | loss: 69.2558984CurrentTrain: epoch  2, batch     3 | loss: 47.9456480CurrentTrain: epoch  3, batch     0 | loss: 111.8688109CurrentTrain: epoch  3, batch     1 | loss: 55.0391989CurrentTrain: epoch  3, batch     2 | loss: 88.6819616CurrentTrain: epoch  3, batch     3 | loss: 60.1327180CurrentTrain: epoch  4, batch     0 | loss: 56.6535169CurrentTrain: epoch  4, batch     1 | loss: 68.7132619CurrentTrain: epoch  4, batch     2 | loss: 63.8391743CurrentTrain: epoch  4, batch     3 | loss: 81.7875784CurrentTrain: epoch  5, batch     0 | loss: 86.0943254CurrentTrain: epoch  5, batch     1 | loss: 64.2752681CurrentTrain: epoch  5, batch     2 | loss: 85.3654623CurrentTrain: epoch  5, batch     3 | loss: 42.7200541CurrentTrain: epoch  6, batch     0 | loss: 86.1564191CurrentTrain: epoch  6, batch     1 | loss: 51.0305123CurrentTrain: epoch  6, batch     2 | loss: 81.8796228CurrentTrain: epoch  6, batch     3 | loss: 45.3630344CurrentTrain: epoch  7, batch     0 | loss: 62.3243967CurrentTrain: epoch  7, batch     1 | loss: 84.7309248CurrentTrain: epoch  7, batch     2 | loss: 53.6510854CurrentTrain: epoch  7, batch     3 | loss: 45.4216150CurrentTrain: epoch  8, batch     0 | loss: 66.3702027CurrentTrain: epoch  8, batch     1 | loss: 51.1538804CurrentTrain: epoch  8, batch     2 | loss: 64.2819561CurrentTrain: epoch  8, batch     3 | loss: 78.5472277CurrentTrain: epoch  9, batch     0 | loss: 62.1142389CurrentTrain: epoch  9, batch     1 | loss: 65.8931150CurrentTrain: epoch  9, batch     2 | loss: 83.7950413CurrentTrain: epoch  9, batch     3 | loss: 55.1289678
MemoryTrain:  epoch  0, batch     0 | loss: 0.4245728MemoryTrain:  epoch  1, batch     0 | loss: 0.3873878MemoryTrain:  epoch  2, batch     0 | loss: 0.2950570MemoryTrain:  epoch  3, batch     0 | loss: 0.2256097MemoryTrain:  epoch  4, batch     0 | loss: 0.1753316MemoryTrain:  epoch  5, batch     0 | loss: 0.1525865MemoryTrain:  epoch  6, batch     0 | loss: 0.1239207MemoryTrain:  epoch  7, batch     0 | loss: 0.0998075MemoryTrain:  epoch  8, batch     0 | loss: 0.0859567MemoryTrain:  epoch  9, batch     0 | loss: 0.0829962

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.7777777777777778, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.5352112676056338, 26: 0.0, 27: 0.0, 28: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.41975308641975306, 37: 0.5137614678899083, 38: 0.34615384615384615, 40: 0.0}
Micro-average F1 score: 0.32
Weighted-average F1 score: 0.20891582987832663
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.6666666666666666, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.5121951219512195, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7401574803149606, 37: 0.43529411764705883, 38: 0.4523809523809524, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2885682574916759
Weighted-average F1 score: 0.21452405967188107
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.6666666666666666, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.5121951219512195, 26: 0.0, 27: 0.0, 28: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7049180327868853, 37: 0.4260355029585799, 38: 0.42857142857142855, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.287369640787949
Weighted-average F1 score: 0.21327226154414503

F1 score per class: {1: 0.09389671361502347, 2: 0.15151515151515152, 3: 0.2127659574468085, 5: 0.6906474820143885, 6: 0.05217391304347826, 7: 0.010309278350515464, 8: 0.04819277108433735, 9: 0.8928571428571429, 10: 0.2389937106918239, 11: 0.06382978723404255, 12: 0.2032520325203252, 14: 0.0375, 15: 0.5384615384615384, 16: 0.4943820224719101, 17: 0.0, 18: 0.14218009478672985, 19: 0.37623762376237624, 20: 0.26744186046511625, 22: 0.4808743169398907, 24: 0.037037037037037035, 25: 0.5352112676056338, 26: 0.6415094339622641, 27: 0.043010752688172046, 28: 0.10638297872340426, 29: 0.7225130890052356, 30: 0.9473684210526315, 31: 0.05714285714285714, 32: 0.5388127853881278, 33: 0.13953488372093023, 34: 0.23841059602649006, 35: 0.14225941422594143, 36: 0.029411764705882353, 37: 0.14395886889460155, 38: 0.11612903225806452, 39: 0.13333333333333333, 40: 0.2692307692307692}
Micro-average F1 score: 0.2759407069555302
Weighted-average F1 score: 0.2546419863397238
F1 score per class: {1: 0.0924092409240924, 2: 0.15384615384615385, 3: 0.18556701030927836, 5: 0.3454231433506045, 6: 0.15730337078651685, 7: 0.024793388429752067, 8: 0.3448275862068966, 9: 0.6024096385542169, 10: 0.21818181818181817, 11: 0.23255813953488372, 12: 0.22306238185255198, 14: 0.05235602094240838, 15: 0.2857142857142857, 16: 0.4854368932038835, 17: 0.08333333333333333, 18: 0.11063829787234042, 19: 0.35858585858585856, 20: 0.24427480916030533, 22: 0.42857142857142855, 24: 0.07792207792207792, 25: 0.4827586206896552, 26: 0.6094420600858369, 27: 0.04411764705882353, 28: 0.04926108374384237, 29: 0.6949152542372882, 30: 0.72, 31: 0.034782608695652174, 32: 0.5365853658536586, 33: 0.1111111111111111, 34: 0.1978021978021978, 35: 0.19789473684210526, 36: 0.35051546391752575, 37: 0.06948356807511737, 38: 0.0979381443298969, 39: 0.12307692307692308, 40: 0.21798365122615804}
Micro-average F1 score: 0.2256398742703188
Weighted-average F1 score: 0.20257405016217325
F1 score per class: {1: 0.09009009009009009, 2: 0.14736842105263157, 3: 0.25396825396825395, 5: 0.3838771593090211, 6: 0.12, 7: 0.022900763358778626, 8: 0.18556701030927836, 9: 0.7692307692307693, 10: 0.21203438395415472, 11: 0.11320754716981132, 12: 0.21554770318021202, 14: 0.05732484076433121, 15: 0.2727272727272727, 16: 0.49019607843137253, 17: 0.10526315789473684, 18: 0.11428571428571428, 19: 0.3901098901098901, 20: 0.23880597014925373, 22: 0.43386243386243384, 24: 0.05555555555555555, 25: 0.47191011235955055, 26: 0.6200873362445415, 27: 0.03821656050955414, 28: 0.06172839506172839, 29: 0.7414634146341463, 30: 0.75, 31: 0.04878048780487805, 32: 0.5202702702702703, 33: 0.08888888888888889, 34: 0.22105263157894736, 35: 0.18376068376068377, 36: 0.1095890410958904, 37: 0.06557377049180328, 38: 0.0975609756097561, 39: 0.21428571428571427, 40: 0.23668639053254437}
Micro-average F1 score: 0.22373999057936883
Weighted-average F1 score: 0.20176235653257377
cur_acc:  ['0.7063', '0.4959', '0.3691', '0.5359', '0.2354', '0.4041', '0.3200']
his_acc:  ['0.7063', '0.6046', '0.4808', '0.4455', '0.3432', '0.3189', '0.2759']
cur_acc des:  ['0.6305', '0.4740', '0.3810', '0.3717', '0.2374', '0.3750', '0.2886']
his_acc des:  ['0.6305', '0.5212', '0.4021', '0.3796', '0.2837', '0.2552', '0.2256']
cur_acc rrf:  ['0.6329', '0.4835', '0.3250', '0.4108', '0.2491', '0.3861', '0.2874']
his_acc rrf:  ['0.6329', '0.5289', '0.3930', '0.3842', '0.2851', '0.2533', '0.2237']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 87.6595865CurrentTrain: epoch  0, batch     1 | loss: 81.4844767CurrentTrain: epoch  0, batch     2 | loss: 129.9945874CurrentTrain: epoch  0, batch     3 | loss: 57.7351231CurrentTrain: epoch  1, batch     0 | loss: 78.4574382CurrentTrain: epoch  1, batch     1 | loss: 64.9726198CurrentTrain: epoch  1, batch     2 | loss: 60.1282295CurrentTrain: epoch  1, batch     3 | loss: 70.0181568CurrentTrain: epoch  2, batch     0 | loss: 86.6939479CurrentTrain: epoch  2, batch     1 | loss: 75.7274338CurrentTrain: epoch  2, batch     2 | loss: 54.6387569CurrentTrain: epoch  2, batch     3 | loss: 72.8746719CurrentTrain: epoch  3, batch     0 | loss: 70.5541527CurrentTrain: epoch  3, batch     1 | loss: 56.6772344CurrentTrain: epoch  3, batch     2 | loss: 84.1937967CurrentTrain: epoch  3, batch     3 | loss: 46.4091132CurrentTrain: epoch  4, batch     0 | loss: 113.4555937CurrentTrain: epoch  4, batch     1 | loss: 59.4260858CurrentTrain: epoch  4, batch     2 | loss: 86.1667420CurrentTrain: epoch  4, batch     3 | loss: 53.6604632CurrentTrain: epoch  5, batch     0 | loss: 123.9391927CurrentTrain: epoch  5, batch     1 | loss: 53.8007020CurrentTrain: epoch  5, batch     2 | loss: 81.9747540CurrentTrain: epoch  5, batch     3 | loss: 52.7835147CurrentTrain: epoch  6, batch     0 | loss: 118.9664165CurrentTrain: epoch  6, batch     1 | loss: 83.9608075CurrentTrain: epoch  6, batch     2 | loss: 51.9084754CurrentTrain: epoch  6, batch     3 | loss: 51.3350903CurrentTrain: epoch  7, batch     0 | loss: 64.9832809CurrentTrain: epoch  7, batch     1 | loss: 83.8688928CurrentTrain: epoch  7, batch     2 | loss: 65.9985013CurrentTrain: epoch  7, batch     3 | loss: 53.6213399CurrentTrain: epoch  8, batch     0 | loss: 89.0817392CurrentTrain: epoch  8, batch     1 | loss: 65.3601794CurrentTrain: epoch  8, batch     2 | loss: 48.8670318CurrentTrain: epoch  8, batch     3 | loss: 52.7487776CurrentTrain: epoch  9, batch     0 | loss: 62.2061498CurrentTrain: epoch  9, batch     1 | loss: 63.6094458CurrentTrain: epoch  9, batch     2 | loss: 113.0826139CurrentTrain: epoch  9, batch     3 | loss: 93.5809239
MemoryTrain:  epoch  0, batch     0 | loss: 0.2840756MemoryTrain:  epoch  1, batch     0 | loss: 0.2568489MemoryTrain:  epoch  2, batch     0 | loss: 0.1877022MemoryTrain:  epoch  3, batch     0 | loss: 0.1754873MemoryTrain:  epoch  4, batch     0 | loss: 0.1501062MemoryTrain:  epoch  5, batch     0 | loss: 0.1165795MemoryTrain:  epoch  6, batch     0 | loss: 0.1070924MemoryTrain:  epoch  7, batch     0 | loss: 0.0901093MemoryTrain:  epoch  8, batch     0 | loss: 0.0908581MemoryTrain:  epoch  9, batch     0 | loss: 0.0775192

F1 score per class: {0: 0.7865168539325843, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9847715736040609, 5: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 13: 0.045454545454545456, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.23423423423423423, 22: 0.0, 23: 0.6341463414634146, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.47058823529411764
Weighted-average F1 score: 0.35054463210718945
F1 score per class: {0: 0.625, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9702970297029703, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.07547169811320754, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3090909090909091, 23: 0.5591397849462365, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3284132841328413
Weighted-average F1 score: 0.23102610007505278
F1 score per class: {0: 0.6086956521739131, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9748743718592965, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.07142857142857142, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.2956521739130435, 23: 0.5684210526315789, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.34462729912875123
Weighted-average F1 score: 0.2456982459551097

F1 score per class: {0: 0.20348837209302326, 1: 0.102803738317757, 2: 0.1686746987951807, 3: 0.25333333333333335, 4: 0.9463414634146341, 5: 0.6081504702194357, 6: 0.11347517730496454, 7: 0.0, 8: 0.07058823529411765, 9: 0.847457627118644, 10: 0.08333333333333333, 11: 0.02127659574468085, 12: 0.2085889570552147, 13: 0.005633802816901409, 14: 0.023255813953488372, 15: 0.631578947368421, 16: 0.4485981308411215, 17: 0.0, 18: 0.14232209737827714, 19: 0.3791208791208791, 20: 0.19373219373219372, 21: 0.049523809523809526, 22: 0.3971631205673759, 23: 0.5416666666666666, 24: 0.0625, 25: 0.4931506849315068, 26: 0.600896860986547, 27: 0.04697986577181208, 28: 0.13333333333333333, 29: 0.6597938144329897, 30: 0.9230769230769231, 31: 0.04395604395604396, 32: 0.5112107623318386, 33: 0.15384615384615385, 34: 0.2159090909090909, 35: 0.1415929203539823, 36: 0.05714285714285714, 37: 0.21374045801526717, 38: 0.07777777777777778, 39: 0.13333333333333333, 40: 0.29473684210526313}
Micro-average F1 score: 0.25414981168921746
Weighted-average F1 score: 0.22502802328747384
F1 score per class: {0: 0.15086206896551724, 1: 0.09205020920502092, 2: 0.1044776119402985, 3: 0.17158176943699732, 4: 0.8868778280542986, 5: 0.3215434083601286, 6: 0.16666666666666666, 7: 0.021052631578947368, 8: 0.3225806451612903, 9: 0.5494505494505495, 10: 0.13513513513513514, 11: 0.27692307692307694, 12: 0.18538324420677363, 13: 0.014035087719298246, 14: 0.0, 15: 0.4444444444444444, 16: 0.4032258064516129, 17: 0.0, 18: 0.10891089108910891, 19: 0.3182711198428291, 20: 0.19047619047619047, 21: 0.06759443339960239, 22: 0.3111111111111111, 23: 0.416, 24: 0.0, 25: 0.4888888888888889, 26: 0.5714285714285714, 27: 0.04294478527607362, 28: 0.08064516129032258, 29: 0.70042194092827, 30: 0.6923076923076923, 31: 0.019417475728155338, 32: 0.47384615384615386, 33: 0.13793103448275862, 34: 0.26148409893992935, 35: 0.1435114503816794, 36: 0.4327485380116959, 37: 0.09411764705882353, 38: 0.08126410835214447, 39: 0.08888888888888889, 40: 0.23648648648648649}
Micro-average F1 score: 0.21766561514195584
Weighted-average F1 score: 0.19408274179025842
F1 score per class: {0: 0.14112903225806453, 1: 0.07228915662650602, 2: 0.1044776119402985, 3: 0.1935483870967742, 4: 0.9107981220657277, 5: 0.35587188612099646, 6: 0.19148936170212766, 7: 0.020618556701030927, 8: 0.2564102564102564, 9: 0.704225352112676, 10: 0.1485148514851485, 11: 0.12844036697247707, 12: 0.18374558303886926, 13: 0.011019283746556474, 14: 0.0, 15: 0.48, 16: 0.4166666666666667, 17: 0.0, 18: 0.10161662817551963, 19: 0.34606741573033706, 20: 0.1791044776119403, 21: 0.06093189964157706, 22: 0.3053435114503817, 23: 0.43548387096774194, 24: 0.0, 25: 0.4883720930232558, 26: 0.5882352941176471, 27: 0.040697674418604654, 28: 0.1111111111111111, 29: 0.7109004739336493, 30: 0.76, 31: 0.029850746268656716, 32: 0.459214501510574, 33: 0.0975609756097561, 34: 0.23367697594501718, 35: 0.13782051282051283, 36: 0.1686746987951807, 37: 0.09401709401709402, 38: 0.07906976744186046, 39: 0.15384615384615385, 40: 0.24305555555555555}
Micro-average F1 score: 0.21207114394013024
Weighted-average F1 score: 0.18773155681914322
cur_acc:  ['0.7063', '0.4959', '0.3691', '0.5359', '0.2354', '0.4041', '0.3200', '0.4706']
his_acc:  ['0.7063', '0.6046', '0.4808', '0.4455', '0.3432', '0.3189', '0.2759', '0.2541']
cur_acc des:  ['0.6305', '0.4740', '0.3810', '0.3717', '0.2374', '0.3750', '0.2886', '0.3284']
his_acc des:  ['0.6305', '0.5212', '0.4021', '0.3796', '0.2837', '0.2552', '0.2256', '0.2177']
cur_acc rrf:  ['0.6329', '0.4835', '0.3250', '0.4108', '0.2491', '0.3861', '0.2874', '0.3446']
his_acc rrf:  ['0.6329', '0.5289', '0.3930', '0.3842', '0.2851', '0.2533', '0.2237', '0.2121']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 84.9553347CurrentTrain: epoch  0, batch     1 | loss: 64.9994974CurrentTrain: epoch  0, batch     2 | loss: 98.5789041CurrentTrain: epoch  0, batch     3 | loss: 98.2947492CurrentTrain: epoch  0, batch     4 | loss: 128.2202127CurrentTrain: epoch  0, batch     5 | loss: 96.8347912CurrentTrain: epoch  0, batch     6 | loss: 96.2613252CurrentTrain: epoch  0, batch     7 | loss: 98.0957989CurrentTrain: epoch  0, batch     8 | loss: 97.1858398CurrentTrain: epoch  0, batch     9 | loss: 77.8312719CurrentTrain: epoch  0, batch    10 | loss: 125.7981735CurrentTrain: epoch  0, batch    11 | loss: 66.2831235CurrentTrain: epoch  0, batch    12 | loss: 65.1833553CurrentTrain: epoch  0, batch    13 | loss: 56.7128158CurrentTrain: epoch  0, batch    14 | loss: 78.6121897CurrentTrain: epoch  0, batch    15 | loss: 77.8987285CurrentTrain: epoch  0, batch    16 | loss: 77.8145619CurrentTrain: epoch  0, batch    17 | loss: 126.0286603CurrentTrain: epoch  0, batch    18 | loss: 187.7173149CurrentTrain: epoch  0, batch    19 | loss: 76.9840282CurrentTrain: epoch  0, batch    20 | loss: 126.8276760CurrentTrain: epoch  0, batch    21 | loss: 65.5746654CurrentTrain: epoch  0, batch    22 | loss: 77.2756575CurrentTrain: epoch  0, batch    23 | loss: 56.3004140CurrentTrain: epoch  0, batch    24 | loss: 95.7649657CurrentTrain: epoch  0, batch    25 | loss: 77.0221183CurrentTrain: epoch  0, batch    26 | loss: 65.3992985CurrentTrain: epoch  0, batch    27 | loss: 126.1888454CurrentTrain: epoch  0, batch    28 | loss: 76.7116755CurrentTrain: epoch  0, batch    29 | loss: 76.3530531CurrentTrain: epoch  0, batch    30 | loss: 77.1599755CurrentTrain: epoch  0, batch    31 | loss: 55.7718761CurrentTrain: epoch  0, batch    32 | loss: 76.9499235CurrentTrain: epoch  0, batch    33 | loss: 125.4344624CurrentTrain: epoch  0, batch    34 | loss: 125.6312552CurrentTrain: epoch  0, batch    35 | loss: 55.7879400CurrentTrain: epoch  0, batch    36 | loss: 64.4558684CurrentTrain: epoch  0, batch    37 | loss: 76.8191893CurrentTrain: epoch  0, batch    38 | loss: 76.1218693CurrentTrain: epoch  0, batch    39 | loss: 93.9099522CurrentTrain: epoch  0, batch    40 | loss: 56.6192185CurrentTrain: epoch  0, batch    41 | loss: 96.0198522CurrentTrain: epoch  0, batch    42 | loss: 74.9341764CurrentTrain: epoch  0, batch    43 | loss: 63.8635470CurrentTrain: epoch  0, batch    44 | loss: 64.2954987CurrentTrain: epoch  0, batch    45 | loss: 64.2993564CurrentTrain: epoch  0, batch    46 | loss: 76.0427507CurrentTrain: epoch  0, batch    47 | loss: 126.7700986CurrentTrain: epoch  0, batch    48 | loss: 63.7368907CurrentTrain: epoch  0, batch    49 | loss: 76.8689192CurrentTrain: epoch  0, batch    50 | loss: 62.4188922CurrentTrain: epoch  0, batch    51 | loss: 62.6240727CurrentTrain: epoch  0, batch    52 | loss: 94.8166709CurrentTrain: epoch  0, batch    53 | loss: 94.2504309CurrentTrain: epoch  0, batch    54 | loss: 75.7800102CurrentTrain: epoch  0, batch    55 | loss: 76.2103049CurrentTrain: epoch  0, batch    56 | loss: 125.4776888CurrentTrain: epoch  0, batch    57 | loss: 75.3075164CurrentTrain: epoch  0, batch    58 | loss: 94.3621450CurrentTrain: epoch  0, batch    59 | loss: 76.1028367CurrentTrain: epoch  0, batch    60 | loss: 75.9614255CurrentTrain: epoch  0, batch    61 | loss: 94.1434882CurrentTrain: epoch  0, batch    62 | loss: 73.6009042CurrentTrain: epoch  0, batch    63 | loss: 75.4494160CurrentTrain: epoch  0, batch    64 | loss: 54.5093318CurrentTrain: epoch  0, batch    65 | loss: 93.6718233CurrentTrain: epoch  0, batch    66 | loss: 74.6352440CurrentTrain: epoch  0, batch    67 | loss: 75.7263080CurrentTrain: epoch  0, batch    68 | loss: 63.5902664CurrentTrain: epoch  0, batch    69 | loss: 72.9335675CurrentTrain: epoch  0, batch    70 | loss: 63.0297946CurrentTrain: epoch  0, batch    71 | loss: 91.0801427CurrentTrain: epoch  0, batch    72 | loss: 72.2114640CurrentTrain: epoch  0, batch    73 | loss: 72.9560650CurrentTrain: epoch  0, batch    74 | loss: 61.5819845CurrentTrain: epoch  0, batch    75 | loss: 90.2924804CurrentTrain: epoch  0, batch    76 | loss: 73.4005968CurrentTrain: epoch  0, batch    77 | loss: 92.1262323CurrentTrain: epoch  0, batch    78 | loss: 93.3483145CurrentTrain: epoch  0, batch    79 | loss: 185.7989337CurrentTrain: epoch  0, batch    80 | loss: 61.3781721CurrentTrain: epoch  0, batch    81 | loss: 60.4853970CurrentTrain: epoch  0, batch    82 | loss: 61.7909316CurrentTrain: epoch  0, batch    83 | loss: 75.0373980CurrentTrain: epoch  0, batch    84 | loss: 62.0246589CurrentTrain: epoch  0, batch    85 | loss: 70.6410038CurrentTrain: epoch  0, batch    86 | loss: 91.3500447CurrentTrain: epoch  0, batch    87 | loss: 90.7498020CurrentTrain: epoch  0, batch    88 | loss: 72.1509340CurrentTrain: epoch  0, batch    89 | loss: 69.7579203CurrentTrain: epoch  0, batch    90 | loss: 67.4668884CurrentTrain: epoch  0, batch    91 | loss: 71.7899321CurrentTrain: epoch  0, batch    92 | loss: 71.3359922CurrentTrain: epoch  0, batch    93 | loss: 95.1856463CurrentTrain: epoch  0, batch    94 | loss: 60.6075400CurrentTrain: epoch  0, batch    95 | loss: 78.3544969CurrentTrain: epoch  1, batch     0 | loss: 120.2503398CurrentTrain: epoch  1, batch     1 | loss: 59.4341735CurrentTrain: epoch  1, batch     2 | loss: 71.8885982CurrentTrain: epoch  1, batch     3 | loss: 183.9927302CurrentTrain: epoch  1, batch     4 | loss: 68.3302450CurrentTrain: epoch  1, batch     5 | loss: 90.6246505CurrentTrain: epoch  1, batch     6 | loss: 90.1498343CurrentTrain: epoch  1, batch     7 | loss: 73.3877948CurrentTrain: epoch  1, batch     8 | loss: 59.1068720CurrentTrain: epoch  1, batch     9 | loss: 70.9245648CurrentTrain: epoch  1, batch    10 | loss: 70.9391797CurrentTrain: epoch  1, batch    11 | loss: 73.4417729CurrentTrain: epoch  1, batch    12 | loss: 88.6041792CurrentTrain: epoch  1, batch    13 | loss: 67.4200082CurrentTrain: epoch  1, batch    14 | loss: 59.9274490CurrentTrain: epoch  1, batch    15 | loss: 59.3082433CurrentTrain: epoch  1, batch    16 | loss: 90.8167217CurrentTrain: epoch  1, batch    17 | loss: 69.0985186CurrentTrain: epoch  1, batch    18 | loss: 88.6506289CurrentTrain: epoch  1, batch    19 | loss: 57.2359243CurrentTrain: epoch  1, batch    20 | loss: 70.5141829CurrentTrain: epoch  1, batch    21 | loss: 66.8053415CurrentTrain: epoch  1, batch    22 | loss: 67.2984482CurrentTrain: epoch  1, batch    23 | loss: 118.8948192CurrentTrain: epoch  1, batch    24 | loss: 73.1601974CurrentTrain: epoch  1, batch    25 | loss: 60.1381081CurrentTrain: epoch  1, batch    26 | loss: 72.4333952CurrentTrain: epoch  1, batch    27 | loss: 58.1968774CurrentTrain: epoch  1, batch    28 | loss: 119.2505927CurrentTrain: epoch  1, batch    29 | loss: 88.9719732CurrentTrain: epoch  1, batch    30 | loss: 69.3751016CurrentTrain: epoch  1, batch    31 | loss: 69.8672330CurrentTrain: epoch  1, batch    32 | loss: 88.6834321CurrentTrain: epoch  1, batch    33 | loss: 68.4672021CurrentTrain: epoch  1, batch    34 | loss: 87.5677890CurrentTrain: epoch  1, batch    35 | loss: 122.9707064CurrentTrain: epoch  1, batch    36 | loss: 88.9157733CurrentTrain: epoch  1, batch    37 | loss: 56.4974404CurrentTrain: epoch  1, batch    38 | loss: 125.3659076CurrentTrain: epoch  1, batch    39 | loss: 90.3781632CurrentTrain: epoch  1, batch    40 | loss: 118.6120635CurrentTrain: epoch  1, batch    41 | loss: 119.7681860CurrentTrain: epoch  1, batch    42 | loss: 49.1003020CurrentTrain: epoch  1, batch    43 | loss: 58.7819789CurrentTrain: epoch  1, batch    44 | loss: 84.8227710CurrentTrain: epoch  1, batch    45 | loss: 86.0426636CurrentTrain: epoch  1, batch    46 | loss: 53.7062238CurrentTrain: epoch  1, batch    47 | loss: 58.5678731CurrentTrain: epoch  1, batch    48 | loss: 72.1311667CurrentTrain: epoch  1, batch    49 | loss: 57.3851622CurrentTrain: epoch  1, batch    50 | loss: 47.6097548CurrentTrain: epoch  1, batch    51 | loss: 57.1344852CurrentTrain: epoch  1, batch    52 | loss: 90.6423433CurrentTrain: epoch  1, batch    53 | loss: 72.6361675CurrentTrain: epoch  1, batch    54 | loss: 49.5973087CurrentTrain: epoch  1, batch    55 | loss: 71.1080761CurrentTrain: epoch  1, batch    56 | loss: 58.9795443CurrentTrain: epoch  1, batch    57 | loss: 120.7882206CurrentTrain: epoch  1, batch    58 | loss: 120.2691850CurrentTrain: epoch  1, batch    59 | loss: 57.3555695CurrentTrain: epoch  1, batch    60 | loss: 47.8469298CurrentTrain: epoch  1, batch    61 | loss: 70.4803586CurrentTrain: epoch  1, batch    62 | loss: 57.9754718CurrentTrain: epoch  1, batch    63 | loss: 57.1522319CurrentTrain: epoch  1, batch    64 | loss: 67.2053521CurrentTrain: epoch  1, batch    65 | loss: 120.1613518CurrentTrain: epoch  1, batch    66 | loss: 69.8537198CurrentTrain: epoch  1, batch    67 | loss: 117.5018468CurrentTrain: epoch  1, batch    68 | loss: 87.6564157CurrentTrain: epoch  1, batch    69 | loss: 88.8532209CurrentTrain: epoch  1, batch    70 | loss: 86.2728322CurrentTrain: epoch  1, batch    71 | loss: 71.1395645CurrentTrain: epoch  1, batch    72 | loss: 69.4524599CurrentTrain: epoch  1, batch    73 | loss: 57.4523068CurrentTrain: epoch  1, batch    74 | loss: 119.6053382CurrentTrain: epoch  1, batch    75 | loss: 71.8196995CurrentTrain: epoch  1, batch    76 | loss: 59.3137760CurrentTrain: epoch  1, batch    77 | loss: 60.1991615CurrentTrain: epoch  1, batch    78 | loss: 87.7511404CurrentTrain: epoch  1, batch    79 | loss: 85.5361853CurrentTrain: epoch  1, batch    80 | loss: 87.0902796CurrentTrain: epoch  1, batch    81 | loss: 59.3568629CurrentTrain: epoch  1, batch    82 | loss: 56.3229505CurrentTrain: epoch  1, batch    83 | loss: 59.9052935CurrentTrain: epoch  1, batch    84 | loss: 67.5582911CurrentTrain: epoch  1, batch    85 | loss: 86.3213150CurrentTrain: epoch  1, batch    86 | loss: 89.3510621CurrentTrain: epoch  1, batch    87 | loss: 91.6121850CurrentTrain: epoch  1, batch    88 | loss: 57.6122705CurrentTrain: epoch  1, batch    89 | loss: 59.7452084CurrentTrain: epoch  1, batch    90 | loss: 68.6700180CurrentTrain: epoch  1, batch    91 | loss: 58.1327092CurrentTrain: epoch  1, batch    92 | loss: 56.1491534CurrentTrain: epoch  1, batch    93 | loss: 69.5828367CurrentTrain: epoch  1, batch    94 | loss: 71.2640517CurrentTrain: epoch  1, batch    95 | loss: 56.5703847CurrentTrain: epoch  2, batch     0 | loss: 65.9361983CurrentTrain: epoch  2, batch     1 | loss: 59.3446807CurrentTrain: epoch  2, batch     2 | loss: 65.2905086CurrentTrain: epoch  2, batch     3 | loss: 115.1026947CurrentTrain: epoch  2, batch     4 | loss: 87.1925658CurrentTrain: epoch  2, batch     5 | loss: 55.8617166CurrentTrain: epoch  2, batch     6 | loss: 88.1886960CurrentTrain: epoch  2, batch     7 | loss: 87.1340164CurrentTrain: epoch  2, batch     8 | loss: 63.8939996CurrentTrain: epoch  2, batch     9 | loss: 84.8921299CurrentTrain: epoch  2, batch    10 | loss: 87.4874339CurrentTrain: epoch  2, batch    11 | loss: 57.2302843CurrentTrain: epoch  2, batch    12 | loss: 56.3914859CurrentTrain: epoch  2, batch    13 | loss: 69.8989542CurrentTrain: epoch  2, batch    14 | loss: 67.4149456CurrentTrain: epoch  2, batch    15 | loss: 86.8999793CurrentTrain: epoch  2, batch    16 | loss: 85.6185819CurrentTrain: epoch  2, batch    17 | loss: 66.7241300CurrentTrain: epoch  2, batch    18 | loss: 70.9951172CurrentTrain: epoch  2, batch    19 | loss: 65.9909061CurrentTrain: epoch  2, batch    20 | loss: 83.9679089CurrentTrain: epoch  2, batch    21 | loss: 85.8985044CurrentTrain: epoch  2, batch    22 | loss: 69.6158098CurrentTrain: epoch  2, batch    23 | loss: 121.8894386CurrentTrain: epoch  2, batch    24 | loss: 72.3187881CurrentTrain: epoch  2, batch    25 | loss: 120.2904214CurrentTrain: epoch  2, batch    26 | loss: 85.7361851CurrentTrain: epoch  2, batch    27 | loss: 85.8975965CurrentTrain: epoch  2, batch    28 | loss: 119.0939175CurrentTrain: epoch  2, batch    29 | loss: 56.8749360CurrentTrain: epoch  2, batch    30 | loss: 117.2746692CurrentTrain: epoch  2, batch    31 | loss: 55.2847951CurrentTrain: epoch  2, batch    32 | loss: 86.8297559CurrentTrain: epoch  2, batch    33 | loss: 46.7190019CurrentTrain: epoch  2, batch    34 | loss: 66.7297080CurrentTrain: epoch  2, batch    35 | loss: 57.6998043CurrentTrain: epoch  2, batch    36 | loss: 44.2174979CurrentTrain: epoch  2, batch    37 | loss: 89.1397621CurrentTrain: epoch  2, batch    38 | loss: 66.7717522CurrentTrain: epoch  2, batch    39 | loss: 53.7128557CurrentTrain: epoch  2, batch    40 | loss: 86.1091208CurrentTrain: epoch  2, batch    41 | loss: 58.3493169CurrentTrain: epoch  2, batch    42 | loss: 120.4212388CurrentTrain: epoch  2, batch    43 | loss: 84.5040946CurrentTrain: epoch  2, batch    44 | loss: 72.1841428CurrentTrain: epoch  2, batch    45 | loss: 86.1023814CurrentTrain: epoch  2, batch    46 | loss: 117.2653416CurrentTrain: epoch  2, batch    47 | loss: 114.9810989CurrentTrain: epoch  2, batch    48 | loss: 68.4842397CurrentTrain: epoch  2, batch    49 | loss: 65.5750426CurrentTrain: epoch  2, batch    50 | loss: 56.8908913CurrentTrain: epoch  2, batch    51 | loss: 114.4089180CurrentTrain: epoch  2, batch    52 | loss: 120.7299034CurrentTrain: epoch  2, batch    53 | loss: 52.0464752CurrentTrain: epoch  2, batch    54 | loss: 112.6672934CurrentTrain: epoch  2, batch    55 | loss: 52.9485144CurrentTrain: epoch  2, batch    56 | loss: 47.4462622CurrentTrain: epoch  2, batch    57 | loss: 68.6901281CurrentTrain: epoch  2, batch    58 | loss: 64.6452409CurrentTrain: epoch  2, batch    59 | loss: 54.8199856CurrentTrain: epoch  2, batch    60 | loss: 69.5641431CurrentTrain: epoch  2, batch    61 | loss: 87.0914744CurrentTrain: epoch  2, batch    62 | loss: 70.6585059CurrentTrain: epoch  2, batch    63 | loss: 70.3501417CurrentTrain: epoch  2, batch    64 | loss: 83.0189506CurrentTrain: epoch  2, batch    65 | loss: 67.9032136CurrentTrain: epoch  2, batch    66 | loss: 86.8364056CurrentTrain: epoch  2, batch    67 | loss: 86.5446301CurrentTrain: epoch  2, batch    68 | loss: 89.9891786CurrentTrain: epoch  2, batch    69 | loss: 57.3520109CurrentTrain: epoch  2, batch    70 | loss: 113.0225929CurrentTrain: epoch  2, batch    71 | loss: 66.8297223CurrentTrain: epoch  2, batch    72 | loss: 69.4071257CurrentTrain: epoch  2, batch    73 | loss: 52.4320931CurrentTrain: epoch  2, batch    74 | loss: 67.5642956CurrentTrain: epoch  2, batch    75 | loss: 53.1856870CurrentTrain: epoch  2, batch    76 | loss: 47.1373899CurrentTrain: epoch  2, batch    77 | loss: 69.6118305CurrentTrain: epoch  2, batch    78 | loss: 57.0934541CurrentTrain: epoch  2, batch    79 | loss: 71.2826378CurrentTrain: epoch  2, batch    80 | loss: 118.1669826CurrentTrain: epoch  2, batch    81 | loss: 52.9256048CurrentTrain: epoch  2, batch    82 | loss: 87.1258231CurrentTrain: epoch  2, batch    83 | loss: 58.6296865CurrentTrain: epoch  2, batch    84 | loss: 67.8062955CurrentTrain: epoch  2, batch    85 | loss: 54.3958731CurrentTrain: epoch  2, batch    86 | loss: 58.2548913CurrentTrain: epoch  2, batch    87 | loss: 59.7439562CurrentTrain: epoch  2, batch    88 | loss: 91.1657221CurrentTrain: epoch  2, batch    89 | loss: 53.2553319CurrentTrain: epoch  2, batch    90 | loss: 53.8224058CurrentTrain: epoch  2, batch    91 | loss: 70.8758816CurrentTrain: epoch  2, batch    92 | loss: 70.4892372CurrentTrain: epoch  2, batch    93 | loss: 55.4748377CurrentTrain: epoch  2, batch    94 | loss: 60.4281357CurrentTrain: epoch  2, batch    95 | loss: 58.7484100CurrentTrain: epoch  3, batch     0 | loss: 117.9655499CurrentTrain: epoch  3, batch     1 | loss: 68.8293638CurrentTrain: epoch  3, batch     2 | loss: 87.1234098CurrentTrain: epoch  3, batch     3 | loss: 55.3462095CurrentTrain: epoch  3, batch     4 | loss: 88.4261067CurrentTrain: epoch  3, batch     5 | loss: 69.2868276CurrentTrain: epoch  3, batch     6 | loss: 57.6540600CurrentTrain: epoch  3, batch     7 | loss: 86.1664430CurrentTrain: epoch  3, batch     8 | loss: 54.9060952CurrentTrain: epoch  3, batch     9 | loss: 55.2330164CurrentTrain: epoch  3, batch    10 | loss: 50.5311195CurrentTrain: epoch  3, batch    11 | loss: 56.7825598CurrentTrain: epoch  3, batch    12 | loss: 85.4327920CurrentTrain: epoch  3, batch    13 | loss: 80.6228031CurrentTrain: epoch  3, batch    14 | loss: 54.8422618CurrentTrain: epoch  3, batch    15 | loss: 85.3108580CurrentTrain: epoch  3, batch    16 | loss: 85.4197252CurrentTrain: epoch  3, batch    17 | loss: 116.8110177CurrentTrain: epoch  3, batch    18 | loss: 62.1985389CurrentTrain: epoch  3, batch    19 | loss: 120.8475298CurrentTrain: epoch  3, batch    20 | loss: 114.7316776CurrentTrain: epoch  3, batch    21 | loss: 66.5225967CurrentTrain: epoch  3, batch    22 | loss: 57.6097122CurrentTrain: epoch  3, batch    23 | loss: 55.5450034CurrentTrain: epoch  3, batch    24 | loss: 84.2188478CurrentTrain: epoch  3, batch    25 | loss: 86.0946544CurrentTrain: epoch  3, batch    26 | loss: 53.8406162CurrentTrain: epoch  3, batch    27 | loss: 118.4571726CurrentTrain: epoch  3, batch    28 | loss: 67.1250929CurrentTrain: epoch  3, batch    29 | loss: 53.9242317CurrentTrain: epoch  3, batch    30 | loss: 66.7293479CurrentTrain: epoch  3, batch    31 | loss: 85.1193633CurrentTrain: epoch  3, batch    32 | loss: 66.0238527CurrentTrain: epoch  3, batch    33 | loss: 44.7993408CurrentTrain: epoch  3, batch    34 | loss: 121.9225970CurrentTrain: epoch  3, batch    35 | loss: 50.8492858CurrentTrain: epoch  3, batch    36 | loss: 68.8495349CurrentTrain: epoch  3, batch    37 | loss: 69.9198026CurrentTrain: epoch  3, batch    38 | loss: 45.8420326CurrentTrain: epoch  3, batch    39 | loss: 70.6771859CurrentTrain: epoch  3, batch    40 | loss: 66.6537618CurrentTrain: epoch  3, batch    41 | loss: 86.4627726CurrentTrain: epoch  3, batch    42 | loss: 68.1188401CurrentTrain: epoch  3, batch    43 | loss: 117.2220346CurrentTrain: epoch  3, batch    44 | loss: 53.4230937CurrentTrain: epoch  3, batch    45 | loss: 53.5834149CurrentTrain: epoch  3, batch    46 | loss: 52.2150917CurrentTrain: epoch  3, batch    47 | loss: 66.5341816CurrentTrain: epoch  3, batch    48 | loss: 64.6291946CurrentTrain: epoch  3, batch    49 | loss: 81.8350798CurrentTrain: epoch  3, batch    50 | loss: 54.7453877CurrentTrain: epoch  3, batch    51 | loss: 64.8091856CurrentTrain: epoch  3, batch    52 | loss: 71.3663721CurrentTrain: epoch  3, batch    53 | loss: 118.7347489CurrentTrain: epoch  3, batch    54 | loss: 52.2233295CurrentTrain: epoch  3, batch    55 | loss: 54.0714959CurrentTrain: epoch  3, batch    56 | loss: 87.4527522CurrentTrain: epoch  3, batch    57 | loss: 46.0291259CurrentTrain: epoch  3, batch    58 | loss: 66.8537881CurrentTrain: epoch  3, batch    59 | loss: 64.6823714CurrentTrain: epoch  3, batch    60 | loss: 53.3772386CurrentTrain: epoch  3, batch    61 | loss: 86.7670081CurrentTrain: epoch  3, batch    62 | loss: 54.4616708CurrentTrain: epoch  3, batch    63 | loss: 58.2290750CurrentTrain: epoch  3, batch    64 | loss: 85.8594119CurrentTrain: epoch  3, batch    65 | loss: 70.3703604CurrentTrain: epoch  3, batch    66 | loss: 86.1754178CurrentTrain: epoch  3, batch    67 | loss: 63.9615456CurrentTrain: epoch  3, batch    68 | loss: 68.3520154CurrentTrain: epoch  3, batch    69 | loss: 67.5806934CurrentTrain: epoch  3, batch    70 | loss: 43.5416399CurrentTrain: epoch  3, batch    71 | loss: 53.0284396CurrentTrain: epoch  3, batch    72 | loss: 66.0442317CurrentTrain: epoch  3, batch    73 | loss: 70.0008576CurrentTrain: epoch  3, batch    74 | loss: 87.3453866CurrentTrain: epoch  3, batch    75 | loss: 176.3559701CurrentTrain: epoch  3, batch    76 | loss: 91.9673835CurrentTrain: epoch  3, batch    77 | loss: 85.8673301CurrentTrain: epoch  3, batch    78 | loss: 84.1516263CurrentTrain: epoch  3, batch    79 | loss: 67.2972375CurrentTrain: epoch  3, batch    80 | loss: 51.1080935CurrentTrain: epoch  3, batch    81 | loss: 56.3869491CurrentTrain: epoch  3, batch    82 | loss: 65.6078944CurrentTrain: epoch  3, batch    83 | loss: 117.0044231CurrentTrain: epoch  3, batch    84 | loss: 182.7861603CurrentTrain: epoch  3, batch    85 | loss: 68.7634083CurrentTrain: epoch  3, batch    86 | loss: 88.7763496CurrentTrain: epoch  3, batch    87 | loss: 83.6364318CurrentTrain: epoch  3, batch    88 | loss: 85.5645234CurrentTrain: epoch  3, batch    89 | loss: 64.9281243CurrentTrain: epoch  3, batch    90 | loss: 87.2271183CurrentTrain: epoch  3, batch    91 | loss: 53.9510721CurrentTrain: epoch  3, batch    92 | loss: 117.6165046CurrentTrain: epoch  3, batch    93 | loss: 119.6093491CurrentTrain: epoch  3, batch    94 | loss: 46.6132053CurrentTrain: epoch  3, batch    95 | loss: 68.3587223CurrentTrain: epoch  4, batch     0 | loss: 55.6401451CurrentTrain: epoch  4, batch     1 | loss: 89.2310673CurrentTrain: epoch  4, batch     2 | loss: 53.4112409CurrentTrain: epoch  4, batch     3 | loss: 67.1270663CurrentTrain: epoch  4, batch     4 | loss: 45.5733748CurrentTrain: epoch  4, batch     5 | loss: 68.0277301CurrentTrain: epoch  4, batch     6 | loss: 115.9849396CurrentTrain: epoch  4, batch     7 | loss: 67.7538086CurrentTrain: epoch  4, batch     8 | loss: 65.6326508CurrentTrain: epoch  4, batch     9 | loss: 62.9980880CurrentTrain: epoch  4, batch    10 | loss: 113.6369274CurrentTrain: epoch  4, batch    11 | loss: 69.1577656CurrentTrain: epoch  4, batch    12 | loss: 116.2508358CurrentTrain: epoch  4, batch    13 | loss: 68.5089623CurrentTrain: epoch  4, batch    14 | loss: 58.3678784CurrentTrain: epoch  4, batch    15 | loss: 113.1031531CurrentTrain: epoch  4, batch    16 | loss: 372.5710544CurrentTrain: epoch  4, batch    17 | loss: 64.5155725CurrentTrain: epoch  4, batch    18 | loss: 68.4303622CurrentTrain: epoch  4, batch    19 | loss: 50.9921682CurrentTrain: epoch  4, batch    20 | loss: 45.4443579CurrentTrain: epoch  4, batch    21 | loss: 82.3339544CurrentTrain: epoch  4, batch    22 | loss: 50.8954732CurrentTrain: epoch  4, batch    23 | loss: 81.8647375CurrentTrain: epoch  4, batch    24 | loss: 82.6778872CurrentTrain: epoch  4, batch    25 | loss: 66.9264559CurrentTrain: epoch  4, batch    26 | loss: 69.6820431CurrentTrain: epoch  4, batch    27 | loss: 64.5088473CurrentTrain: epoch  4, batch    28 | loss: 65.6750556CurrentTrain: epoch  4, batch    29 | loss: 56.3316991CurrentTrain: epoch  4, batch    30 | loss: 66.6351751CurrentTrain: epoch  4, batch    31 | loss: 67.8536463CurrentTrain: epoch  4, batch    32 | loss: 45.1518253CurrentTrain: epoch  4, batch    33 | loss: 178.7794961CurrentTrain: epoch  4, batch    34 | loss: 56.0527376CurrentTrain: epoch  4, batch    35 | loss: 85.5084026CurrentTrain: epoch  4, batch    36 | loss: 51.7371383CurrentTrain: epoch  4, batch    37 | loss: 118.7173123CurrentTrain: epoch  4, batch    38 | loss: 82.6922554CurrentTrain: epoch  4, batch    39 | loss: 54.6176155CurrentTrain: epoch  4, batch    40 | loss: 183.5276623CurrentTrain: epoch  4, batch    41 | loss: 65.7671142CurrentTrain: epoch  4, batch    42 | loss: 63.4105896CurrentTrain: epoch  4, batch    43 | loss: 53.7424119CurrentTrain: epoch  4, batch    44 | loss: 62.0785296CurrentTrain: epoch  4, batch    45 | loss: 112.4392520CurrentTrain: epoch  4, batch    46 | loss: 51.6855602CurrentTrain: epoch  4, batch    47 | loss: 113.6135237CurrentTrain: epoch  4, batch    48 | loss: 52.5487446CurrentTrain: epoch  4, batch    49 | loss: 67.2768603CurrentTrain: epoch  4, batch    50 | loss: 68.0637970CurrentTrain: epoch  4, batch    51 | loss: 67.4359162CurrentTrain: epoch  4, batch    52 | loss: 64.5171098CurrentTrain: epoch  4, batch    53 | loss: 53.7713233CurrentTrain: epoch  4, batch    54 | loss: 116.4397444CurrentTrain: epoch  4, batch    55 | loss: 82.4279866CurrentTrain: epoch  4, batch    56 | loss: 82.5860075CurrentTrain: epoch  4, batch    57 | loss: 91.7904410CurrentTrain: epoch  4, batch    58 | loss: 65.9641391CurrentTrain: epoch  4, batch    59 | loss: 66.3403595CurrentTrain: epoch  4, batch    60 | loss: 55.0575347CurrentTrain: epoch  4, batch    61 | loss: 68.2749657CurrentTrain: epoch  4, batch    62 | loss: 64.7222607CurrentTrain: epoch  4, batch    63 | loss: 87.9212609CurrentTrain: epoch  4, batch    64 | loss: 84.3233866CurrentTrain: epoch  4, batch    65 | loss: 87.1005023CurrentTrain: epoch  4, batch    66 | loss: 68.2113312CurrentTrain: epoch  4, batch    67 | loss: 51.8853242CurrentTrain: epoch  4, batch    68 | loss: 66.2500014CurrentTrain: epoch  4, batch    69 | loss: 86.4329775CurrentTrain: epoch  4, batch    70 | loss: 50.7169386CurrentTrain: epoch  4, batch    71 | loss: 68.8782851CurrentTrain: epoch  4, batch    72 | loss: 51.6089502CurrentTrain: epoch  4, batch    73 | loss: 68.1963132CurrentTrain: epoch  4, batch    74 | loss: 68.6569515CurrentTrain: epoch  4, batch    75 | loss: 87.4440960CurrentTrain: epoch  4, batch    76 | loss: 67.2165310CurrentTrain: epoch  4, batch    77 | loss: 66.4686040CurrentTrain: epoch  4, batch    78 | loss: 65.7738744CurrentTrain: epoch  4, batch    79 | loss: 63.7651432CurrentTrain: epoch  4, batch    80 | loss: 118.5085418CurrentTrain: epoch  4, batch    81 | loss: 62.9710531CurrentTrain: epoch  4, batch    82 | loss: 55.2912957CurrentTrain: epoch  4, batch    83 | loss: 54.5765334CurrentTrain: epoch  4, batch    84 | loss: 87.4566726CurrentTrain: epoch  4, batch    85 | loss: 84.7310363CurrentTrain: epoch  4, batch    86 | loss: 52.0193158CurrentTrain: epoch  4, batch    87 | loss: 68.0080875CurrentTrain: epoch  4, batch    88 | loss: 86.7941151CurrentTrain: epoch  4, batch    89 | loss: 67.9813259CurrentTrain: epoch  4, batch    90 | loss: 51.7359729CurrentTrain: epoch  4, batch    91 | loss: 65.0459951CurrentTrain: epoch  4, batch    92 | loss: 67.2314414CurrentTrain: epoch  4, batch    93 | loss: 55.2941846CurrentTrain: epoch  4, batch    94 | loss: 86.5351627CurrentTrain: epoch  4, batch    95 | loss: 71.9511141CurrentTrain: epoch  5, batch     0 | loss: 65.1412692CurrentTrain: epoch  5, batch     1 | loss: 64.0160127CurrentTrain: epoch  5, batch     2 | loss: 63.8508680CurrentTrain: epoch  5, batch     3 | loss: 53.7326949CurrentTrain: epoch  5, batch     4 | loss: 67.1610118CurrentTrain: epoch  5, batch     5 | loss: 67.7824812CurrentTrain: epoch  5, batch     6 | loss: 53.8039905CurrentTrain: epoch  5, batch     7 | loss: 47.3539813CurrentTrain: epoch  5, batch     8 | loss: 81.4249614CurrentTrain: epoch  5, batch     9 | loss: 63.6697536CurrentTrain: epoch  5, batch    10 | loss: 66.4197662CurrentTrain: epoch  5, batch    11 | loss: 53.7688080CurrentTrain: epoch  5, batch    12 | loss: 81.9475637CurrentTrain: epoch  5, batch    13 | loss: 51.8425457CurrentTrain: epoch  5, batch    14 | loss: 51.2813742CurrentTrain: epoch  5, batch    15 | loss: 83.3510289CurrentTrain: epoch  5, batch    16 | loss: 50.7635809CurrentTrain: epoch  5, batch    17 | loss: 85.8984131CurrentTrain: epoch  5, batch    18 | loss: 63.9176224CurrentTrain: epoch  5, batch    19 | loss: 65.3354945CurrentTrain: epoch  5, batch    20 | loss: 44.6534724CurrentTrain: epoch  5, batch    21 | loss: 51.1123089CurrentTrain: epoch  5, batch    22 | loss: 83.3468904CurrentTrain: epoch  5, batch    23 | loss: 67.1015366CurrentTrain: epoch  5, batch    24 | loss: 54.4446144CurrentTrain: epoch  5, batch    25 | loss: 53.9762811CurrentTrain: epoch  5, batch    26 | loss: 115.9985608CurrentTrain: epoch  5, batch    27 | loss: 81.0434408CurrentTrain: epoch  5, batch    28 | loss: 64.7194934CurrentTrain: epoch  5, batch    29 | loss: 72.8544912CurrentTrain: epoch  5, batch    30 | loss: 54.8058538CurrentTrain: epoch  5, batch    31 | loss: 79.7674018CurrentTrain: epoch  5, batch    32 | loss: 63.4021710CurrentTrain: epoch  5, batch    33 | loss: 66.7532560CurrentTrain: epoch  5, batch    34 | loss: 67.9530873CurrentTrain: epoch  5, batch    35 | loss: 86.3785766CurrentTrain: epoch  5, batch    36 | loss: 67.9281526CurrentTrain: epoch  5, batch    37 | loss: 86.2136420CurrentTrain: epoch  5, batch    38 | loss: 65.9812564CurrentTrain: epoch  5, batch    39 | loss: 63.7233041CurrentTrain: epoch  5, batch    40 | loss: 66.2346317CurrentTrain: epoch  5, batch    41 | loss: 64.9798084CurrentTrain: epoch  5, batch    42 | loss: 62.9832079CurrentTrain: epoch  5, batch    43 | loss: 50.9170832CurrentTrain: epoch  5, batch    44 | loss: 117.9078217CurrentTrain: epoch  5, batch    45 | loss: 64.7346975CurrentTrain: epoch  5, batch    46 | loss: 65.6141265CurrentTrain: epoch  5, batch    47 | loss: 67.5573782CurrentTrain: epoch  5, batch    48 | loss: 50.0309218CurrentTrain: epoch  5, batch    49 | loss: 51.0619767CurrentTrain: epoch  5, batch    50 | loss: 118.6282142CurrentTrain: epoch  5, batch    51 | loss: 54.4921463CurrentTrain: epoch  5, batch    52 | loss: 65.1788784CurrentTrain: epoch  5, batch    53 | loss: 44.2387303CurrentTrain: epoch  5, batch    54 | loss: 65.9594468CurrentTrain: epoch  5, batch    55 | loss: 119.1468520CurrentTrain: epoch  5, batch    56 | loss: 83.7994768CurrentTrain: epoch  5, batch    57 | loss: 52.6144566CurrentTrain: epoch  5, batch    58 | loss: 86.1087618CurrentTrain: epoch  5, batch    59 | loss: 66.7250317CurrentTrain: epoch  5, batch    60 | loss: 115.4130725CurrentTrain: epoch  5, batch    61 | loss: 67.0917661CurrentTrain: epoch  5, batch    62 | loss: 67.2217201CurrentTrain: epoch  5, batch    63 | loss: 83.6528685CurrentTrain: epoch  5, batch    64 | loss: 117.0534471CurrentTrain: epoch  5, batch    65 | loss: 54.6165987CurrentTrain: epoch  5, batch    66 | loss: 82.2751832CurrentTrain: epoch  5, batch    67 | loss: 52.6061218CurrentTrain: epoch  5, batch    68 | loss: 86.1726579CurrentTrain: epoch  5, batch    69 | loss: 60.9439255CurrentTrain: epoch  5, batch    70 | loss: 84.7305305CurrentTrain: epoch  5, batch    71 | loss: 116.5476340CurrentTrain: epoch  5, batch    72 | loss: 67.9111275CurrentTrain: epoch  5, batch    73 | loss: 64.0716699CurrentTrain: epoch  5, batch    74 | loss: 64.4527501CurrentTrain: epoch  5, batch    75 | loss: 115.4625262CurrentTrain: epoch  5, batch    76 | loss: 42.6777995CurrentTrain: epoch  5, batch    77 | loss: 52.3212820CurrentTrain: epoch  5, batch    78 | loss: 54.0498184CurrentTrain: epoch  5, batch    79 | loss: 64.9266502CurrentTrain: epoch  5, batch    80 | loss: 82.8412862CurrentTrain: epoch  5, batch    81 | loss: 52.1197222CurrentTrain: epoch  5, batch    82 | loss: 113.5974048CurrentTrain: epoch  5, batch    83 | loss: 44.2915524CurrentTrain: epoch  5, batch    84 | loss: 62.2298792CurrentTrain: epoch  5, batch    85 | loss: 88.1333278CurrentTrain: epoch  5, batch    86 | loss: 43.4284493CurrentTrain: epoch  5, batch    87 | loss: 86.9890267CurrentTrain: epoch  5, batch    88 | loss: 54.3403896CurrentTrain: epoch  5, batch    89 | loss: 113.5764356CurrentTrain: epoch  5, batch    90 | loss: 119.5656533CurrentTrain: epoch  5, batch    91 | loss: 53.8639916CurrentTrain: epoch  5, batch    92 | loss: 55.0007042CurrentTrain: epoch  5, batch    93 | loss: 66.5571495CurrentTrain: epoch  5, batch    94 | loss: 83.2796534CurrentTrain: epoch  5, batch    95 | loss: 69.8545055CurrentTrain: epoch  6, batch     0 | loss: 71.0994939CurrentTrain: epoch  6, batch     1 | loss: 47.7206127CurrentTrain: epoch  6, batch     2 | loss: 59.6196890CurrentTrain: epoch  6, batch     3 | loss: 63.5583629CurrentTrain: epoch  6, batch     4 | loss: 109.9357582CurrentTrain: epoch  6, batch     5 | loss: 67.5880407CurrentTrain: epoch  6, batch     6 | loss: 68.4890497CurrentTrain: epoch  6, batch     7 | loss: 55.1765170CurrentTrain: epoch  6, batch     8 | loss: 47.4299539CurrentTrain: epoch  6, batch     9 | loss: 54.8065780CurrentTrain: epoch  6, batch    10 | loss: 50.2958469CurrentTrain: epoch  6, batch    11 | loss: 82.7739038CurrentTrain: epoch  6, batch    12 | loss: 64.8526266CurrentTrain: epoch  6, batch    13 | loss: 63.5721101CurrentTrain: epoch  6, batch    14 | loss: 43.0643413CurrentTrain: epoch  6, batch    15 | loss: 82.9908409CurrentTrain: epoch  6, batch    16 | loss: 51.4887314CurrentTrain: epoch  6, batch    17 | loss: 55.0458578CurrentTrain: epoch  6, batch    18 | loss: 50.4906875CurrentTrain: epoch  6, batch    19 | loss: 83.7868391CurrentTrain: epoch  6, batch    20 | loss: 121.2186768CurrentTrain: epoch  6, batch    21 | loss: 114.6625348CurrentTrain: epoch  6, batch    22 | loss: 62.0353635CurrentTrain: epoch  6, batch    23 | loss: 82.9951567CurrentTrain: epoch  6, batch    24 | loss: 85.8405567CurrentTrain: epoch  6, batch    25 | loss: 117.9436542CurrentTrain: epoch  6, batch    26 | loss: 82.7753544CurrentTrain: epoch  6, batch    27 | loss: 82.8460851CurrentTrain: epoch  6, batch    28 | loss: 84.1058923CurrentTrain: epoch  6, batch    29 | loss: 65.3142462CurrentTrain: epoch  6, batch    30 | loss: 66.3188887CurrentTrain: epoch  6, batch    31 | loss: 63.7882660CurrentTrain: epoch  6, batch    32 | loss: 52.4278381CurrentTrain: epoch  6, batch    33 | loss: 52.8888955CurrentTrain: epoch  6, batch    34 | loss: 51.5867292CurrentTrain: epoch  6, batch    35 | loss: 44.4392966CurrentTrain: epoch  6, batch    36 | loss: 82.0551821CurrentTrain: epoch  6, batch    37 | loss: 84.5048338CurrentTrain: epoch  6, batch    38 | loss: 66.9743097CurrentTrain: epoch  6, batch    39 | loss: 65.5546954CurrentTrain: epoch  6, batch    40 | loss: 113.7127254CurrentTrain: epoch  6, batch    41 | loss: 85.7891943CurrentTrain: epoch  6, batch    42 | loss: 65.3728365CurrentTrain: epoch  6, batch    43 | loss: 67.5702688CurrentTrain: epoch  6, batch    44 | loss: 63.0696040CurrentTrain: epoch  6, batch    45 | loss: 81.8220511CurrentTrain: epoch  6, batch    46 | loss: 52.7982088CurrentTrain: epoch  6, batch    47 | loss: 84.4693460CurrentTrain: epoch  6, batch    48 | loss: 84.2677400CurrentTrain: epoch  6, batch    49 | loss: 43.1245376CurrentTrain: epoch  6, batch    50 | loss: 76.2666046CurrentTrain: epoch  6, batch    51 | loss: 62.0566270CurrentTrain: epoch  6, batch    52 | loss: 51.6984563CurrentTrain: epoch  6, batch    53 | loss: 53.8257939CurrentTrain: epoch  6, batch    54 | loss: 82.6222170CurrentTrain: epoch  6, batch    55 | loss: 86.8676524CurrentTrain: epoch  6, batch    56 | loss: 83.7906264CurrentTrain: epoch  6, batch    57 | loss: 63.0843754CurrentTrain: epoch  6, batch    58 | loss: 79.3985443CurrentTrain: epoch  6, batch    59 | loss: 53.1528258CurrentTrain: epoch  6, batch    60 | loss: 83.1597636CurrentTrain: epoch  6, batch    61 | loss: 85.2422053CurrentTrain: epoch  6, batch    62 | loss: 113.0995570CurrentTrain: epoch  6, batch    63 | loss: 83.9053615CurrentTrain: epoch  6, batch    64 | loss: 84.4536537CurrentTrain: epoch  6, batch    65 | loss: 113.2905374CurrentTrain: epoch  6, batch    66 | loss: 81.4628228CurrentTrain: epoch  6, batch    67 | loss: 44.7753432CurrentTrain: epoch  6, batch    68 | loss: 66.4086011CurrentTrain: epoch  6, batch    69 | loss: 50.3297763CurrentTrain: epoch  6, batch    70 | loss: 70.3958321CurrentTrain: epoch  6, batch    71 | loss: 56.0587968CurrentTrain: epoch  6, batch    72 | loss: 55.9237616CurrentTrain: epoch  6, batch    73 | loss: 42.8777968CurrentTrain: epoch  6, batch    74 | loss: 66.3071309CurrentTrain: epoch  6, batch    75 | loss: 69.3001817CurrentTrain: epoch  6, batch    76 | loss: 78.8013377CurrentTrain: epoch  6, batch    77 | loss: 45.7491515CurrentTrain: epoch  6, batch    78 | loss: 88.0992071CurrentTrain: epoch  6, batch    79 | loss: 88.7080442CurrentTrain: epoch  6, batch    80 | loss: 41.4491210CurrentTrain: epoch  6, batch    81 | loss: 62.7125050CurrentTrain: epoch  6, batch    82 | loss: 51.0230766CurrentTrain: epoch  6, batch    83 | loss: 70.9405934CurrentTrain: epoch  6, batch    84 | loss: 116.1429974CurrentTrain: epoch  6, batch    85 | loss: 66.1692611CurrentTrain: epoch  6, batch    86 | loss: 117.6705962CurrentTrain: epoch  6, batch    87 | loss: 44.9356057CurrentTrain: epoch  6, batch    88 | loss: 65.7883920CurrentTrain: epoch  6, batch    89 | loss: 115.5006321CurrentTrain: epoch  6, batch    90 | loss: 63.5468447CurrentTrain: epoch  6, batch    91 | loss: 117.3440181CurrentTrain: epoch  6, batch    92 | loss: 63.8213287CurrentTrain: epoch  6, batch    93 | loss: 65.7080562CurrentTrain: epoch  6, batch    94 | loss: 53.6682028CurrentTrain: epoch  6, batch    95 | loss: 54.3859994CurrentTrain: epoch  7, batch     0 | loss: 115.7936350CurrentTrain: epoch  7, batch     1 | loss: 43.4730859CurrentTrain: epoch  7, batch     2 | loss: 65.6420255CurrentTrain: epoch  7, batch     3 | loss: 82.3749349CurrentTrain: epoch  7, batch     4 | loss: 84.5185856CurrentTrain: epoch  7, batch     5 | loss: 62.6125716CurrentTrain: epoch  7, batch     6 | loss: 52.4608929CurrentTrain: epoch  7, batch     7 | loss: 84.1457472CurrentTrain: epoch  7, batch     8 | loss: 66.9806921CurrentTrain: epoch  7, batch     9 | loss: 63.2440207CurrentTrain: epoch  7, batch    10 | loss: 81.4266811CurrentTrain: epoch  7, batch    11 | loss: 113.6855941CurrentTrain: epoch  7, batch    12 | loss: 51.6806133CurrentTrain: epoch  7, batch    13 | loss: 50.1174327CurrentTrain: epoch  7, batch    14 | loss: 43.2813939CurrentTrain: epoch  7, batch    15 | loss: 49.4454534CurrentTrain: epoch  7, batch    16 | loss: 84.8775233CurrentTrain: epoch  7, batch    17 | loss: 62.8099006CurrentTrain: epoch  7, batch    18 | loss: 61.6291503CurrentTrain: epoch  7, batch    19 | loss: 63.5847975CurrentTrain: epoch  7, batch    20 | loss: 113.4130330CurrentTrain: epoch  7, batch    21 | loss: 117.6831350CurrentTrain: epoch  7, batch    22 | loss: 65.1744867CurrentTrain: epoch  7, batch    23 | loss: 81.8282051CurrentTrain: epoch  7, batch    24 | loss: 54.7953350CurrentTrain: epoch  7, batch    25 | loss: 82.8288318CurrentTrain: epoch  7, batch    26 | loss: 69.4943592CurrentTrain: epoch  7, batch    27 | loss: 52.5729503CurrentTrain: epoch  7, batch    28 | loss: 65.8209903CurrentTrain: epoch  7, batch    29 | loss: 86.2051717CurrentTrain: epoch  7, batch    30 | loss: 65.9767288CurrentTrain: epoch  7, batch    31 | loss: 111.7409700CurrentTrain: epoch  7, batch    32 | loss: 82.6011374CurrentTrain: epoch  7, batch    33 | loss: 42.7136031CurrentTrain: epoch  7, batch    34 | loss: 42.4474341CurrentTrain: epoch  7, batch    35 | loss: 61.0842958CurrentTrain: epoch  7, batch    36 | loss: 115.4061357CurrentTrain: epoch  7, batch    37 | loss: 67.2081940CurrentTrain: epoch  7, batch    38 | loss: 64.8766774CurrentTrain: epoch  7, batch    39 | loss: 67.3287894CurrentTrain: epoch  7, batch    40 | loss: 65.1860197CurrentTrain: epoch  7, batch    41 | loss: 52.1608975CurrentTrain: epoch  7, batch    42 | loss: 61.6084001CurrentTrain: epoch  7, batch    43 | loss: 51.2286447CurrentTrain: epoch  7, batch    44 | loss: 84.3629072CurrentTrain: epoch  7, batch    45 | loss: 117.5863100CurrentTrain: epoch  7, batch    46 | loss: 83.0322695CurrentTrain: epoch  7, batch    47 | loss: 63.4881700CurrentTrain: epoch  7, batch    48 | loss: 64.3595001CurrentTrain: epoch  7, batch    49 | loss: 50.9871502CurrentTrain: epoch  7, batch    50 | loss: 65.8899314CurrentTrain: epoch  7, batch    51 | loss: 65.8994047CurrentTrain: epoch  7, batch    52 | loss: 62.1631178CurrentTrain: epoch  7, batch    53 | loss: 110.4381174CurrentTrain: epoch  7, batch    54 | loss: 65.6792564CurrentTrain: epoch  7, batch    55 | loss: 86.0659504CurrentTrain: epoch  7, batch    56 | loss: 64.4465938CurrentTrain: epoch  7, batch    57 | loss: 63.1264844CurrentTrain: epoch  7, batch    58 | loss: 49.2614908CurrentTrain: epoch  7, batch    59 | loss: 65.4403149CurrentTrain: epoch  7, batch    60 | loss: 85.0592526CurrentTrain: epoch  7, batch    61 | loss: 114.1027022CurrentTrain: epoch  7, batch    62 | loss: 51.1940512CurrentTrain: epoch  7, batch    63 | loss: 41.5990815CurrentTrain: epoch  7, batch    64 | loss: 118.4861236CurrentTrain: epoch  7, batch    65 | loss: 51.0248171CurrentTrain: epoch  7, batch    66 | loss: 85.6426944CurrentTrain: epoch  7, batch    67 | loss: 62.6651664CurrentTrain: epoch  7, batch    68 | loss: 78.3235014CurrentTrain: epoch  7, batch    69 | loss: 65.8491273CurrentTrain: epoch  7, batch    70 | loss: 50.5982010CurrentTrain: epoch  7, batch    71 | loss: 44.5484356CurrentTrain: epoch  7, batch    72 | loss: 85.9587925CurrentTrain: epoch  7, batch    73 | loss: 61.2495395CurrentTrain: epoch  7, batch    74 | loss: 66.7963945CurrentTrain: epoch  7, batch    75 | loss: 65.8266849CurrentTrain: epoch  7, batch    76 | loss: 84.3621894CurrentTrain: epoch  7, batch    77 | loss: 87.0636772CurrentTrain: epoch  7, batch    78 | loss: 67.0246311CurrentTrain: epoch  7, batch    79 | loss: 64.7265597CurrentTrain: epoch  7, batch    80 | loss: 78.9726855CurrentTrain: epoch  7, batch    81 | loss: 64.3119086CurrentTrain: epoch  7, batch    82 | loss: 71.1369118CurrentTrain: epoch  7, batch    83 | loss: 86.2619622CurrentTrain: epoch  7, batch    84 | loss: 64.4120697CurrentTrain: epoch  7, batch    85 | loss: 115.4000126CurrentTrain: epoch  7, batch    86 | loss: 83.8441145CurrentTrain: epoch  7, batch    87 | loss: 53.4267924CurrentTrain: epoch  7, batch    88 | loss: 63.2475230CurrentTrain: epoch  7, batch    89 | loss: 84.4326642CurrentTrain: epoch  7, batch    90 | loss: 64.6582014CurrentTrain: epoch  7, batch    91 | loss: 67.7312781CurrentTrain: epoch  7, batch    92 | loss: 42.5692035CurrentTrain: epoch  7, batch    93 | loss: 65.1835243CurrentTrain: epoch  7, batch    94 | loss: 50.9087143CurrentTrain: epoch  7, batch    95 | loss: 68.9498377CurrentTrain: epoch  8, batch     0 | loss: 50.1514628CurrentTrain: epoch  8, batch     1 | loss: 64.9779985CurrentTrain: epoch  8, batch     2 | loss: 49.5737381CurrentTrain: epoch  8, batch     3 | loss: 115.9068791CurrentTrain: epoch  8, batch     4 | loss: 64.2949365CurrentTrain: epoch  8, batch     5 | loss: 84.2149631CurrentTrain: epoch  8, batch     6 | loss: 45.4059332CurrentTrain: epoch  8, batch     7 | loss: 65.3961971CurrentTrain: epoch  8, batch     8 | loss: 65.4862870CurrentTrain: epoch  8, batch     9 | loss: 53.2979046CurrentTrain: epoch  8, batch    10 | loss: 50.5451421CurrentTrain: epoch  8, batch    11 | loss: 51.9741547CurrentTrain: epoch  8, batch    12 | loss: 52.9787277CurrentTrain: epoch  8, batch    13 | loss: 82.6263118CurrentTrain: epoch  8, batch    14 | loss: 81.7181402CurrentTrain: epoch  8, batch    15 | loss: 84.4707205CurrentTrain: epoch  8, batch    16 | loss: 67.5259466CurrentTrain: epoch  8, batch    17 | loss: 61.6522278CurrentTrain: epoch  8, batch    18 | loss: 84.0556515CurrentTrain: epoch  8, batch    19 | loss: 83.1104079CurrentTrain: epoch  8, batch    20 | loss: 56.8880932CurrentTrain: epoch  8, batch    21 | loss: 63.2191035CurrentTrain: epoch  8, batch    22 | loss: 64.3561805CurrentTrain: epoch  8, batch    23 | loss: 86.2279979CurrentTrain: epoch  8, batch    24 | loss: 82.4811497CurrentTrain: epoch  8, batch    25 | loss: 42.2752064CurrentTrain: epoch  8, batch    26 | loss: 65.4064843CurrentTrain: epoch  8, batch    27 | loss: 64.3953366CurrentTrain: epoch  8, batch    28 | loss: 54.4076694CurrentTrain: epoch  8, batch    29 | loss: 49.3911280CurrentTrain: epoch  8, batch    30 | loss: 66.7542991CurrentTrain: epoch  8, batch    31 | loss: 42.7518201CurrentTrain: epoch  8, batch    32 | loss: 50.2025600CurrentTrain: epoch  8, batch    33 | loss: 83.1411995CurrentTrain: epoch  8, batch    34 | loss: 84.1236411CurrentTrain: epoch  8, batch    35 | loss: 51.9911069CurrentTrain: epoch  8, batch    36 | loss: 66.2119476CurrentTrain: epoch  8, batch    37 | loss: 43.4913943CurrentTrain: epoch  8, batch    38 | loss: 116.4854545CurrentTrain: epoch  8, batch    39 | loss: 59.3751757CurrentTrain: epoch  8, batch    40 | loss: 53.0056837CurrentTrain: epoch  8, batch    41 | loss: 64.5356226CurrentTrain: epoch  8, batch    42 | loss: 64.8316007CurrentTrain: epoch  8, batch    43 | loss: 65.1850007CurrentTrain: epoch  8, batch    44 | loss: 66.2583991CurrentTrain: epoch  8, batch    45 | loss: 51.0102443CurrentTrain: epoch  8, batch    46 | loss: 65.7823728CurrentTrain: epoch  8, batch    47 | loss: 116.5528154CurrentTrain: epoch  8, batch    48 | loss: 63.1182905CurrentTrain: epoch  8, batch    49 | loss: 118.0289788CurrentTrain: epoch  8, batch    50 | loss: 43.9291187CurrentTrain: epoch  8, batch    51 | loss: 117.5071069CurrentTrain: epoch  8, batch    52 | loss: 49.1790840CurrentTrain: epoch  8, batch    53 | loss: 83.6829085CurrentTrain: epoch  8, batch    54 | loss: 54.0591847CurrentTrain: epoch  8, batch    55 | loss: 46.5969207CurrentTrain: epoch  8, batch    56 | loss: 65.4566628CurrentTrain: epoch  8, batch    57 | loss: 54.1861800CurrentTrain: epoch  8, batch    58 | loss: 49.3043544CurrentTrain: epoch  8, batch    59 | loss: 53.2103363CurrentTrain: epoch  8, batch    60 | loss: 64.4894003CurrentTrain: epoch  8, batch    61 | loss: 117.5336509CurrentTrain: epoch  8, batch    62 | loss: 85.7495529CurrentTrain: epoch  8, batch    63 | loss: 54.1646447CurrentTrain: epoch  8, batch    64 | loss: 84.2091609CurrentTrain: epoch  8, batch    65 | loss: 82.9787403CurrentTrain: epoch  8, batch    66 | loss: 82.6182668CurrentTrain: epoch  8, batch    67 | loss: 115.0400778CurrentTrain: epoch  8, batch    68 | loss: 45.1047496CurrentTrain: epoch  8, batch    69 | loss: 85.9985191CurrentTrain: epoch  8, batch    70 | loss: 82.0511638CurrentTrain: epoch  8, batch    71 | loss: 62.8562754CurrentTrain: epoch  8, batch    72 | loss: 53.9561720CurrentTrain: epoch  8, batch    73 | loss: 50.1748773CurrentTrain: epoch  8, batch    74 | loss: 65.2920862CurrentTrain: epoch  8, batch    75 | loss: 69.9694076CurrentTrain: epoch  8, batch    76 | loss: 44.0058516CurrentTrain: epoch  8, batch    77 | loss: 65.7468087CurrentTrain: epoch  8, batch    78 | loss: 81.9142624CurrentTrain: epoch  8, batch    79 | loss: 64.9451410CurrentTrain: epoch  8, batch    80 | loss: 87.1230881CurrentTrain: epoch  8, batch    81 | loss: 54.4530253CurrentTrain: epoch  8, batch    82 | loss: 67.0018206CurrentTrain: epoch  8, batch    83 | loss: 67.4232636CurrentTrain: epoch  8, batch    84 | loss: 44.3741366CurrentTrain: epoch  8, batch    85 | loss: 60.5042279CurrentTrain: epoch  8, batch    86 | loss: 56.8229634CurrentTrain: epoch  8, batch    87 | loss: 64.2566008CurrentTrain: epoch  8, batch    88 | loss: 64.4382446CurrentTrain: epoch  8, batch    89 | loss: 62.0814745CurrentTrain: epoch  8, batch    90 | loss: 70.3329926CurrentTrain: epoch  8, batch    91 | loss: 115.4388832CurrentTrain: epoch  8, batch    92 | loss: 42.7889466CurrentTrain: epoch  8, batch    93 | loss: 49.2662039CurrentTrain: epoch  8, batch    94 | loss: 83.1631559CurrentTrain: epoch  8, batch    95 | loss: 71.6190052CurrentTrain: epoch  9, batch     0 | loss: 67.6612589CurrentTrain: epoch  9, batch     1 | loss: 117.4657426CurrentTrain: epoch  9, batch     2 | loss: 63.3194930CurrentTrain: epoch  9, batch     3 | loss: 51.8961651CurrentTrain: epoch  9, batch     4 | loss: 66.4154082CurrentTrain: epoch  9, batch     5 | loss: 84.3431464CurrentTrain: epoch  9, batch     6 | loss: 64.3341532CurrentTrain: epoch  9, batch     7 | loss: 81.1908536CurrentTrain: epoch  9, batch     8 | loss: 82.4568020CurrentTrain: epoch  9, batch     9 | loss: 85.7369558CurrentTrain: epoch  9, batch    10 | loss: 174.6774630CurrentTrain: epoch  9, batch    11 | loss: 85.4474588CurrentTrain: epoch  9, batch    12 | loss: 51.2862549CurrentTrain: epoch  9, batch    13 | loss: 113.3165938CurrentTrain: epoch  9, batch    14 | loss: 63.1884908CurrentTrain: epoch  9, batch    15 | loss: 52.1164422CurrentTrain: epoch  9, batch    16 | loss: 53.2776373CurrentTrain: epoch  9, batch    17 | loss: 50.9252991CurrentTrain: epoch  9, batch    18 | loss: 51.0356902CurrentTrain: epoch  9, batch    19 | loss: 80.9366795CurrentTrain: epoch  9, batch    20 | loss: 84.1529389CurrentTrain: epoch  9, batch    21 | loss: 64.6634881CurrentTrain: epoch  9, batch    22 | loss: 65.5309374CurrentTrain: epoch  9, batch    23 | loss: 50.7300770CurrentTrain: epoch  9, batch    24 | loss: 59.5383666CurrentTrain: epoch  9, batch    25 | loss: 82.4651635CurrentTrain: epoch  9, batch    26 | loss: 44.6172198CurrentTrain: epoch  9, batch    27 | loss: 61.5898123CurrentTrain: epoch  9, batch    28 | loss: 60.5755634CurrentTrain: epoch  9, batch    29 | loss: 117.6780155CurrentTrain: epoch  9, batch    30 | loss: 63.0977419CurrentTrain: epoch  9, batch    31 | loss: 81.0512088CurrentTrain: epoch  9, batch    32 | loss: 82.1416182CurrentTrain: epoch  9, batch    33 | loss: 82.6631298CurrentTrain: epoch  9, batch    34 | loss: 50.9060160CurrentTrain: epoch  9, batch    35 | loss: 81.5971801CurrentTrain: epoch  9, batch    36 | loss: 65.0613423CurrentTrain: epoch  9, batch    37 | loss: 82.5940496CurrentTrain: epoch  9, batch    38 | loss: 82.3889843CurrentTrain: epoch  9, batch    39 | loss: 82.6783819CurrentTrain: epoch  9, batch    40 | loss: 61.8790843CurrentTrain: epoch  9, batch    41 | loss: 54.2344458CurrentTrain: epoch  9, batch    42 | loss: 115.8978515CurrentTrain: epoch  9, batch    43 | loss: 52.4869809CurrentTrain: epoch  9, batch    44 | loss: 82.4592258CurrentTrain: epoch  9, batch    45 | loss: 67.4919785CurrentTrain: epoch  9, batch    46 | loss: 65.0700291CurrentTrain: epoch  9, batch    47 | loss: 50.7533901CurrentTrain: epoch  9, batch    48 | loss: 40.3168837CurrentTrain: epoch  9, batch    49 | loss: 50.7239068CurrentTrain: epoch  9, batch    50 | loss: 86.0388671CurrentTrain: epoch  9, batch    51 | loss: 64.2951950CurrentTrain: epoch  9, batch    52 | loss: 62.9981661CurrentTrain: epoch  9, batch    53 | loss: 86.5851359CurrentTrain: epoch  9, batch    54 | loss: 41.9990681CurrentTrain: epoch  9, batch    55 | loss: 62.9555781CurrentTrain: epoch  9, batch    56 | loss: 82.6286771CurrentTrain: epoch  9, batch    57 | loss: 84.1020261CurrentTrain: epoch  9, batch    58 | loss: 117.8123306CurrentTrain: epoch  9, batch    59 | loss: 64.2029908CurrentTrain: epoch  9, batch    60 | loss: 65.6410957CurrentTrain: epoch  9, batch    61 | loss: 84.0686033CurrentTrain: epoch  9, batch    62 | loss: 117.5411926CurrentTrain: epoch  9, batch    63 | loss: 115.2123311CurrentTrain: epoch  9, batch    64 | loss: 53.0611374CurrentTrain: epoch  9, batch    65 | loss: 82.5957767CurrentTrain: epoch  9, batch    66 | loss: 51.7650390CurrentTrain: epoch  9, batch    67 | loss: 169.9789656CurrentTrain: epoch  9, batch    68 | loss: 63.1459416CurrentTrain: epoch  9, batch    69 | loss: 52.0110111CurrentTrain: epoch  9, batch    70 | loss: 64.9404785CurrentTrain: epoch  9, batch    71 | loss: 54.9287127CurrentTrain: epoch  9, batch    72 | loss: 83.1714404CurrentTrain: epoch  9, batch    73 | loss: 85.1899100CurrentTrain: epoch  9, batch    74 | loss: 51.4160454CurrentTrain: epoch  9, batch    75 | loss: 65.5929710CurrentTrain: epoch  9, batch    76 | loss: 49.4772107CurrentTrain: epoch  9, batch    77 | loss: 85.8656883CurrentTrain: epoch  9, batch    78 | loss: 65.7565377CurrentTrain: epoch  9, batch    79 | loss: 50.9199706CurrentTrain: epoch  9, batch    80 | loss: 67.0092834CurrentTrain: epoch  9, batch    81 | loss: 42.3910774CurrentTrain: epoch  9, batch    82 | loss: 65.9259323CurrentTrain: epoch  9, batch    83 | loss: 53.6138479CurrentTrain: epoch  9, batch    84 | loss: 82.6211916CurrentTrain: epoch  9, batch    85 | loss: 62.7492119CurrentTrain: epoch  9, batch    86 | loss: 42.4963246CurrentTrain: epoch  9, batch    87 | loss: 59.8431362CurrentTrain: epoch  9, batch    88 | loss: 50.9319032CurrentTrain: epoch  9, batch    89 | loss: 49.9760440CurrentTrain: epoch  9, batch    90 | loss: 51.8346683CurrentTrain: epoch  9, batch    91 | loss: 64.2890399CurrentTrain: epoch  9, batch    92 | loss: 52.0063306CurrentTrain: epoch  9, batch    93 | loss: 65.5875235CurrentTrain: epoch  9, batch    94 | loss: 62.4634152CurrentTrain: epoch  9, batch    95 | loss: 69.0062488

F1 score per class: {32: 0.49327354260089684, 6: 0.6912442396313364, 19: 0.18181818181818182, 24: 0.7225130890052356, 26: 0.9081632653061225, 29: 0.7873303167420814}
Micro-average F1 score: 0.6941391941391941
Weighted-average F1 score: 0.6872383752590687
F1 score per class: {32: 0.509090909090909, 6: 0.642570281124498, 19: 0.12403100775193798, 24: 0.7120418848167539, 26: 0.9047619047619048, 29: 0.7258064516129032}
Micro-average F1 score: 0.631336405529954
Weighted-average F1 score: 0.5999238504078676
F1 score per class: {32: 0.5128205128205128, 6: 0.642570281124498, 19: 0.13445378151260504, 24: 0.7150259067357513, 26: 0.9134615384615384, 29: 0.7171314741035857}
Micro-average F1 score: 0.637277648878577
Weighted-average F1 score: 0.6083598201395929

F1 score per class: {32: 0.49327354260089684, 6: 0.6912442396313364, 19: 0.18181818181818182, 24: 0.7225130890052356, 26: 0.9081632653061225, 29: 0.7873303167420814}
Micro-average F1 score: 0.6941391941391941
Weighted-average F1 score: 0.6872383752590687
F1 score per class: {32: 0.509090909090909, 6: 0.642570281124498, 19: 0.12403100775193798, 24: 0.7120418848167539, 26: 0.9047619047619048, 29: 0.7258064516129032}
Micro-average F1 score: 0.631336405529954
Weighted-average F1 score: 0.5999238504078676
F1 score per class: {32: 0.5128205128205128, 6: 0.642570281124498, 19: 0.13445378151260504, 24: 0.7150259067357513, 26: 0.9134615384615384, 29: 0.7171314741035857}
Micro-average F1 score: 0.637277648878577
Weighted-average F1 score: 0.6083598201395929
cur_acc:  ['0.6941']
his_acc:  ['0.6941']
cur_acc des:  ['0.6313']
his_acc des:  ['0.6313']
cur_acc rrf:  ['0.6373']
his_acc rrf:  ['0.6373']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 92.7457690CurrentTrain: epoch  0, batch     1 | loss: 92.8390328CurrentTrain: epoch  0, batch     2 | loss: 80.3717184CurrentTrain: epoch  0, batch     3 | loss: 97.2282032CurrentTrain: epoch  0, batch     4 | loss: 62.0886684CurrentTrain: epoch  1, batch     0 | loss: 70.6564462CurrentTrain: epoch  1, batch     1 | loss: 63.0837039CurrentTrain: epoch  1, batch     2 | loss: 186.7988251CurrentTrain: epoch  1, batch     3 | loss: 74.1950716CurrentTrain: epoch  1, batch     4 | loss: 48.1539186CurrentTrain: epoch  2, batch     0 | loss: 72.5240833CurrentTrain: epoch  2, batch     1 | loss: 90.7202847CurrentTrain: epoch  2, batch     2 | loss: 71.5843375CurrentTrain: epoch  2, batch     3 | loss: 88.6333674CurrentTrain: epoch  2, batch     4 | loss: 118.2034591CurrentTrain: epoch  3, batch     0 | loss: 58.9112985CurrentTrain: epoch  3, batch     1 | loss: 89.5961665CurrentTrain: epoch  3, batch     2 | loss: 90.1159657CurrentTrain: epoch  3, batch     3 | loss: 70.4663095CurrentTrain: epoch  3, batch     4 | loss: 55.8762448CurrentTrain: epoch  4, batch     0 | loss: 55.1472911CurrentTrain: epoch  4, batch     1 | loss: 71.8341055CurrentTrain: epoch  4, batch     2 | loss: 182.2783462CurrentTrain: epoch  4, batch     3 | loss: 120.1923363CurrentTrain: epoch  4, batch     4 | loss: 40.5269075CurrentTrain: epoch  5, batch     0 | loss: 56.0153958CurrentTrain: epoch  5, batch     1 | loss: 86.4151175CurrentTrain: epoch  5, batch     2 | loss: 119.8333619CurrentTrain: epoch  5, batch     3 | loss: 55.8019579CurrentTrain: epoch  5, batch     4 | loss: 52.5106897CurrentTrain: epoch  6, batch     0 | loss: 118.8445626CurrentTrain: epoch  6, batch     1 | loss: 70.0582794CurrentTrain: epoch  6, batch     2 | loss: 66.4923327CurrentTrain: epoch  6, batch     3 | loss: 62.4052701CurrentTrain: epoch  6, batch     4 | loss: 43.7843384CurrentTrain: epoch  7, batch     0 | loss: 82.2394928CurrentTrain: epoch  7, batch     1 | loss: 66.0564588CurrentTrain: epoch  7, batch     2 | loss: 66.8725092CurrentTrain: epoch  7, batch     3 | loss: 86.7252217CurrentTrain: epoch  7, batch     4 | loss: 110.3794083CurrentTrain: epoch  8, batch     0 | loss: 70.0097618CurrentTrain: epoch  8, batch     1 | loss: 115.7267996CurrentTrain: epoch  8, batch     2 | loss: 85.1477362CurrentTrain: epoch  8, batch     3 | loss: 52.7027954CurrentTrain: epoch  8, batch     4 | loss: 39.1393956CurrentTrain: epoch  9, batch     0 | loss: 182.9147743CurrentTrain: epoch  9, batch     1 | loss: 67.5097757CurrentTrain: epoch  9, batch     2 | loss: 50.8886614CurrentTrain: epoch  9, batch     3 | loss: 52.1913009CurrentTrain: epoch  9, batch     4 | loss: 41.9075722
MemoryTrain:  epoch  0, batch     0 | loss: 0.4070866MemoryTrain:  epoch  1, batch     0 | loss: 0.3746382MemoryTrain:  epoch  2, batch     0 | loss: 0.3124890MemoryTrain:  epoch  3, batch     0 | loss: 0.2310977MemoryTrain:  epoch  4, batch     0 | loss: 0.1467973MemoryTrain:  epoch  5, batch     0 | loss: 0.1341420MemoryTrain:  epoch  6, batch     0 | loss: 0.1345826MemoryTrain:  epoch  7, batch     0 | loss: 0.0755557MemoryTrain:  epoch  8, batch     0 | loss: 0.0478200MemoryTrain:  epoch  9, batch     0 | loss: 0.0408806

F1 score per class: {32: 0.9543147208121827, 5: 0.0, 6: 0.49514563106796117, 10: 0.547945205479452, 16: 0.0, 17: 0.24347826086956523, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5173410404624278
Weighted-average F1 score: 0.45097815623901
F1 score per class: {32: 0.7747035573122529, 5: 0.0, 6: 0.5519713261648745, 10: 0.5306122448979592, 16: 0.0, 17: 0.3103448275862069, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.4662576687116564
Weighted-average F1 score: 0.42155468046247696
F1 score per class: {32: 0.7716535433070866, 5: 0.0, 6: 0.5403508771929825, 10: 0.49523809523809526, 16: 0.0, 17: 0.3068181818181818, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.47010309278350515
Weighted-average F1 score: 0.430245161910054

F1 score per class: {32: 0.9543147208121827, 5: 0.41818181818181815, 6: 0.3923076923076923, 10: 0.49382716049382713, 16: 0.0, 17: 0.22580645161290322, 18: 0.6755555555555556, 19: 0.20408163265306123, 24: 0.711340206185567, 26: 0.8544600938967136, 29: 0.7652173913043478}
Micro-average F1 score: 0.5915643352909771
Weighted-average F1 score: 0.5597953005221176
F1 score per class: {32: 0.7567567567567568, 5: 0.44660194174757284, 6: 0.38213399503722084, 10: 0.42276422764227645, 16: 0.0, 17: 0.23076923076923078, 18: 0.5963636363636363, 19: 0.11851851851851852, 24: 0.6536585365853659, 26: 0.7901234567901234, 29: 0.6593406593406593}
Micro-average F1 score: 0.5061289047054172
Weighted-average F1 score: 0.4773183746609817
F1 score per class: {32: 0.7538461538461538, 5: 0.4627450980392157, 6: 0.3684210526315789, 10: 0.3880597014925373, 16: 0.0, 17: 0.22594142259414227, 18: 0.6029411764705882, 19: 0.1518987341772152, 24: 0.6666666666666666, 26: 0.7916666666666666, 29: 0.6846153846153846}
Micro-average F1 score: 0.5062676910634857
Weighted-average F1 score: 0.47419534199548863
cur_acc:  ['0.6941', '0.5173']
his_acc:  ['0.6941', '0.5916']
cur_acc des:  ['0.6313', '0.4663']
his_acc des:  ['0.6313', '0.5061']
cur_acc rrf:  ['0.6373', '0.4701']
his_acc rrf:  ['0.6373', '0.5063']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 80.9545216CurrentTrain: epoch  0, batch     1 | loss: 99.8027974CurrentTrain: epoch  0, batch     2 | loss: 65.2609153CurrentTrain: epoch  0, batch     3 | loss: 78.9404282CurrentTrain: epoch  0, batch     4 | loss: 60.1816292CurrentTrain: epoch  1, batch     0 | loss: 94.6249691CurrentTrain: epoch  1, batch     1 | loss: 63.5626334CurrentTrain: epoch  1, batch     2 | loss: 58.9454249CurrentTrain: epoch  1, batch     3 | loss: 93.1341487CurrentTrain: epoch  1, batch     4 | loss: 59.9853379CurrentTrain: epoch  2, batch     0 | loss: 72.4310578CurrentTrain: epoch  2, batch     1 | loss: 72.4631178CurrentTrain: epoch  2, batch     2 | loss: 70.8666118CurrentTrain: epoch  2, batch     3 | loss: 122.2185372CurrentTrain: epoch  2, batch     4 | loss: 30.4370056CurrentTrain: epoch  3, batch     0 | loss: 90.7230992CurrentTrain: epoch  3, batch     1 | loss: 59.2222450CurrentTrain: epoch  3, batch     2 | loss: 69.7156353CurrentTrain: epoch  3, batch     3 | loss: 86.3505426CurrentTrain: epoch  3, batch     4 | loss: 27.7777236CurrentTrain: epoch  4, batch     0 | loss: 88.8506868CurrentTrain: epoch  4, batch     1 | loss: 69.1834442CurrentTrain: epoch  4, batch     2 | loss: 68.4690020CurrentTrain: epoch  4, batch     3 | loss: 85.7392314CurrentTrain: epoch  4, batch     4 | loss: 17.4950725CurrentTrain: epoch  5, batch     0 | loss: 67.6242149CurrentTrain: epoch  5, batch     1 | loss: 67.6505330CurrentTrain: epoch  5, batch     2 | loss: 67.8180051CurrentTrain: epoch  5, batch     3 | loss: 68.3165158CurrentTrain: epoch  5, batch     4 | loss: 28.1502386CurrentTrain: epoch  6, batch     0 | loss: 69.1180454CurrentTrain: epoch  6, batch     1 | loss: 85.5183196CurrentTrain: epoch  6, batch     2 | loss: 55.9986407CurrentTrain: epoch  6, batch     3 | loss: 54.8090841CurrentTrain: epoch  6, batch     4 | loss: 26.3809316CurrentTrain: epoch  7, batch     0 | loss: 67.2922601CurrentTrain: epoch  7, batch     1 | loss: 83.2174961CurrentTrain: epoch  7, batch     2 | loss: 83.9303701CurrentTrain: epoch  7, batch     3 | loss: 66.7548398CurrentTrain: epoch  7, batch     4 | loss: 59.9617672CurrentTrain: epoch  8, batch     0 | loss: 69.9054737CurrentTrain: epoch  8, batch     1 | loss: 52.7082477CurrentTrain: epoch  8, batch     2 | loss: 86.1743984CurrentTrain: epoch  8, batch     3 | loss: 66.1882688CurrentTrain: epoch  8, batch     4 | loss: 15.6964628CurrentTrain: epoch  9, batch     0 | loss: 85.6807748CurrentTrain: epoch  9, batch     1 | loss: 86.0229786CurrentTrain: epoch  9, batch     2 | loss: 53.1398672CurrentTrain: epoch  9, batch     3 | loss: 51.8445784CurrentTrain: epoch  9, batch     4 | loss: 26.7760722
MemoryTrain:  epoch  0, batch     0 | loss: 0.5443609MemoryTrain:  epoch  1, batch     0 | loss: 0.4895858MemoryTrain:  epoch  2, batch     0 | loss: 0.3858621MemoryTrain:  epoch  3, batch     0 | loss: 0.3181133MemoryTrain:  epoch  4, batch     0 | loss: 0.2576458MemoryTrain:  epoch  5, batch     0 | loss: 0.2078804MemoryTrain:  epoch  6, batch     0 | loss: 0.1530778MemoryTrain:  epoch  7, batch     0 | loss: 0.1403932MemoryTrain:  epoch  8, batch     0 | loss: 0.1032580MemoryTrain:  epoch  9, batch     0 | loss: 0.0972369

F1 score per class: {32: 0.35294117647058826, 2: 0.0, 6: 0.0, 39: 0.16, 10: 0.4246575342465753, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.1935483870967742, 26: 0.0, 28: 0.0, 29: 0.11764705882352941}
Micro-average F1 score: 0.2182628062360802
Weighted-average F1 score: 0.1525318698556677
F1 score per class: {32: 0.17391304347826086, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.5, 10: 0.5470085470085471, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.136986301369863, 26: 0.0, 28: 0.0, 29: 0.3448275862068966}
Micro-average F1 score: 0.2993710691823899
Weighted-average F1 score: 0.22883758904511373
F1 score per class: {32: 0.17391304347826086, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.3305785123966942, 10: 0.5394190871369294, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.12658227848101267, 26: 0.0, 28: 0.0, 29: 0.2222222222222222}
Micro-average F1 score: 0.2626788036410923
Weighted-average F1 score: 0.20596035539115343

F1 score per class: {32: 0.27906976744186046, 2: 0.9595959595959596, 5: 0.411522633744856, 6: 0.2, 39: 0.13675213675213677, 10: 0.2683982683982684, 11: 0.4752475247524752, 12: 0.0392156862745098, 16: 0.17647058823529413, 17: 0.6604651162790698, 18: 0.15873015873015872, 19: 0.7046632124352331, 24: 0.07407407407407407, 26: 0.8193832599118943, 28: 0.7333333333333333, 29: 0.10526315789473684}
Micro-average F1 score: 0.48650927487352447
Weighted-average F1 score: 0.46920883542675584
F1 score per class: {32: 0.06374501992031872, 2: 0.6925795053003534, 5: 0.40816326530612246, 6: 0.2952029520295203, 39: 0.3070539419087137, 10: 0.2544731610337972, 11: 0.4778761061946903, 12: 0.046511627906976744, 16: 0.1693121693121693, 17: 0.6230769230769231, 18: 0.10975609756097561, 19: 0.6601941747572816, 24: 0.06711409395973154, 26: 0.7470817120622568, 28: 0.5901639344262295, 29: 0.22727272727272727}
Micro-average F1 score: 0.39480949751518496
Weighted-average F1 score: 0.3635868939602739
F1 score per class: {32: 0.06349206349206349, 2: 0.697508896797153, 5: 0.3904109589041096, 6: 0.2939068100358423, 39: 0.23255813953488372, 10: 0.23985239852398524, 11: 0.46956521739130436, 12: 0.03389830508474576, 16: 0.17094017094017094, 17: 0.6230769230769231, 18: 0.14736842105263157, 19: 0.6766169154228856, 24: 0.05747126436781609, 26: 0.7710843373493976, 28: 0.6330935251798561, 29: 0.15}
Micro-average F1 score: 0.3888731195004258
Weighted-average F1 score: 0.3547725694558998
cur_acc:  ['0.6941', '0.5173', '0.2183']
his_acc:  ['0.6941', '0.5916', '0.4865']
cur_acc des:  ['0.6313', '0.4663', '0.2994']
his_acc des:  ['0.6313', '0.5061', '0.3948']
cur_acc rrf:  ['0.6373', '0.4701', '0.2627']
his_acc rrf:  ['0.6373', '0.5063', '0.3889']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 102.5208372CurrentTrain: epoch  0, batch     1 | loss: 83.5636547CurrentTrain: epoch  0, batch     2 | loss: 69.8164135CurrentTrain: epoch  0, batch     3 | loss: 96.4257113CurrentTrain: epoch  1, batch     0 | loss: 78.0450508CurrentTrain: epoch  1, batch     1 | loss: 99.2495877CurrentTrain: epoch  1, batch     2 | loss: 68.0122366CurrentTrain: epoch  1, batch     3 | loss: 77.8449661CurrentTrain: epoch  2, batch     0 | loss: 184.5358575CurrentTrain: epoch  2, batch     1 | loss: 73.4435400CurrentTrain: epoch  2, batch     2 | loss: 58.5232119CurrentTrain: epoch  2, batch     3 | loss: 73.4554870CurrentTrain: epoch  3, batch     0 | loss: 87.9917483CurrentTrain: epoch  3, batch     1 | loss: 83.7126142CurrentTrain: epoch  3, batch     2 | loss: 56.8921415CurrentTrain: epoch  3, batch     3 | loss: 77.1054492CurrentTrain: epoch  4, batch     0 | loss: 61.2867487CurrentTrain: epoch  4, batch     1 | loss: 70.1233153CurrentTrain: epoch  4, batch     2 | loss: 56.5267111CurrentTrain: epoch  4, batch     3 | loss: 67.6826000CurrentTrain: epoch  5, batch     0 | loss: 67.9297643CurrentTrain: epoch  5, batch     1 | loss: 56.5230626CurrentTrain: epoch  5, batch     2 | loss: 65.6111463CurrentTrain: epoch  5, batch     3 | loss: 74.2581993CurrentTrain: epoch  6, batch     0 | loss: 88.7848608CurrentTrain: epoch  6, batch     1 | loss: 57.4471094CurrentTrain: epoch  6, batch     2 | loss: 63.7377634CurrentTrain: epoch  6, batch     3 | loss: 55.1246866CurrentTrain: epoch  7, batch     0 | loss: 70.4727757CurrentTrain: epoch  7, batch     1 | loss: 85.6891101CurrentTrain: epoch  7, batch     2 | loss: 86.6967796CurrentTrain: epoch  7, batch     3 | loss: 49.8019057CurrentTrain: epoch  8, batch     0 | loss: 85.3566431CurrentTrain: epoch  8, batch     1 | loss: 55.1694985CurrentTrain: epoch  8, batch     2 | loss: 85.2152585CurrentTrain: epoch  8, batch     3 | loss: 45.1280693CurrentTrain: epoch  9, batch     0 | loss: 65.9831777CurrentTrain: epoch  9, batch     1 | loss: 55.8659976CurrentTrain: epoch  9, batch     2 | loss: 82.7189111CurrentTrain: epoch  9, batch     3 | loss: 54.9131931
MemoryTrain:  epoch  0, batch     0 | loss: 0.4691068MemoryTrain:  epoch  1, batch     0 | loss: 0.3972972MemoryTrain:  epoch  2, batch     0 | loss: 0.2727444MemoryTrain:  epoch  3, batch     0 | loss: 0.2074490MemoryTrain:  epoch  4, batch     0 | loss: 0.1684921MemoryTrain:  epoch  5, batch     0 | loss: 0.1547889MemoryTrain:  epoch  6, batch     0 | loss: 0.1216737MemoryTrain:  epoch  7, batch     0 | loss: 0.0850483MemoryTrain:  epoch  8, batch     0 | loss: 0.0789023MemoryTrain:  epoch  9, batch     0 | loss: 0.0736063

F1 score per class: {0: 0.868421052631579, 2: 0.0, 4: 0.9743589743589743, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.14814814814814814, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3055555555555556, 23: 0.7209302325581395, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0}
Micro-average F1 score: 0.6164874551971327
Weighted-average F1 score: 0.5039521885311793
F1 score per class: {0: 0.8089887640449438, 2: 0.0, 4: 0.9949748743718593, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0425531914893617, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3308270676691729, 23: 0.5869565217391305, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3965702036441586
Weighted-average F1 score: 0.2840316503733813
F1 score per class: {0: 0.8181818181818182, 2: 0.0, 4: 0.98989898989899, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0784313725490196, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3384615384615385, 23: 0.5869565217391305, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.41855203619909503
Weighted-average F1 score: 0.3046173985852136

F1 score per class: {0: 0.6055045871559633, 2: 0.15730337078651685, 4: 0.945273631840796, 5: 0.9134615384615384, 6: 0.3788546255506608, 10: 0.208955223880597, 11: 0.07339449541284404, 12: 0.2549019607843137, 13: 0.03125, 16: 0.45045045045045046, 17: 0.17777777777777778, 18: 0.16666666666666666, 19: 0.5945945945945946, 21: 0.22448979591836735, 23: 0.6391752577319587, 24: 0.07407407407407407, 26: 0.65, 28: 0.17391304347826086, 29: 0.7586206896551724, 32: 0.7562189054726368, 39: 0.18181818181818182}
Micro-average F1 score: 0.486652977412731
Weighted-average F1 score: 0.4629525305649046
F1 score per class: {0: 0.26181818181818184, 2: 0.03910614525139665, 4: 0.9519230769230769, 5: 0.5739130434782609, 6: 0.35751295336787564, 10: 0.2964824120603015, 11: 0.31088082901554404, 12: 0.2553191489361702, 13: 0.012048192771084338, 16: 0.4444444444444444, 17: 0.047619047619047616, 18: 0.14232209737827714, 19: 0.5734265734265734, 21: 0.11924119241192412, 23: 0.5046728971962616, 24: 0.14705882352941177, 26: 0.6355140186915887, 28: 0.08, 29: 0.7116104868913857, 32: 0.6, 39: 0.20512820512820512}
Micro-average F1 score: 0.36268596702854844
Weighted-average F1 score: 0.3256803174224803
F1 score per class: {0: 0.2647058823529412, 2: 0.03899721448467967, 4: 0.9514563106796117, 5: 0.632258064516129, 6: 0.3630952380952381, 10: 0.2918918918918919, 11: 0.2064516129032258, 12: 0.25957446808510637, 13: 0.021052631578947368, 16: 0.4444444444444444, 17: 0.041666666666666664, 18: 0.14788732394366197, 19: 0.5890909090909091, 21: 0.11458333333333333, 23: 0.4954128440366973, 24: 0.09090909090909091, 26: 0.6415094339622641, 28: 0.07692307692307693, 29: 0.7286821705426356, 32: 0.7131147540983607, 39: 0.16666666666666666}
Micro-average F1 score: 0.36508602601762485
Weighted-average F1 score: 0.3237794637208031
cur_acc:  ['0.6941', '0.5173', '0.2183', '0.6165']
his_acc:  ['0.6941', '0.5916', '0.4865', '0.4867']
cur_acc des:  ['0.6313', '0.4663', '0.2994', '0.3966']
his_acc des:  ['0.6313', '0.5061', '0.3948', '0.3627']
cur_acc rrf:  ['0.6373', '0.4701', '0.2627', '0.4186']
his_acc rrf:  ['0.6373', '0.5063', '0.3889', '0.3651']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 79.2031597CurrentTrain: epoch  0, batch     1 | loss: 79.8984252CurrentTrain: epoch  0, batch     2 | loss: 74.5230360CurrentTrain: epoch  0, batch     3 | loss: 87.4358067CurrentTrain: epoch  1, batch     0 | loss: 73.7677310CurrentTrain: epoch  1, batch     1 | loss: 75.6087163CurrentTrain: epoch  1, batch     2 | loss: 58.7419977CurrentTrain: epoch  1, batch     3 | loss: 39.5768495CurrentTrain: epoch  2, batch     0 | loss: 63.3870608CurrentTrain: epoch  2, batch     1 | loss: 72.2814949CurrentTrain: epoch  2, batch     2 | loss: 68.6560793CurrentTrain: epoch  2, batch     3 | loss: 82.8531720CurrentTrain: epoch  3, batch     0 | loss: 56.0202705CurrentTrain: epoch  3, batch     1 | loss: 55.4340199CurrentTrain: epoch  3, batch     2 | loss: 88.2126600CurrentTrain: epoch  3, batch     3 | loss: 57.7769839CurrentTrain: epoch  4, batch     0 | loss: 51.3459742CurrentTrain: epoch  4, batch     1 | loss: 63.9359674CurrentTrain: epoch  4, batch     2 | loss: 85.3360646CurrentTrain: epoch  4, batch     3 | loss: 83.8415057CurrentTrain: epoch  5, batch     0 | loss: 82.8818263CurrentTrain: epoch  5, batch     1 | loss: 53.5198343CurrentTrain: epoch  5, batch     2 | loss: 84.9768030CurrentTrain: epoch  5, batch     3 | loss: 43.6305628CurrentTrain: epoch  6, batch     0 | loss: 85.4245124CurrentTrain: epoch  6, batch     1 | loss: 51.8328432CurrentTrain: epoch  6, batch     2 | loss: 84.8736451CurrentTrain: epoch  6, batch     3 | loss: 56.7911088CurrentTrain: epoch  7, batch     0 | loss: 67.9420606CurrentTrain: epoch  7, batch     1 | loss: 53.3532031CurrentTrain: epoch  7, batch     2 | loss: 61.9091885CurrentTrain: epoch  7, batch     3 | loss: 56.6811448CurrentTrain: epoch  8, batch     0 | loss: 52.7704937CurrentTrain: epoch  8, batch     1 | loss: 62.9146195CurrentTrain: epoch  8, batch     2 | loss: 81.7127180CurrentTrain: epoch  8, batch     3 | loss: 78.1055848CurrentTrain: epoch  9, batch     0 | loss: 61.0838886CurrentTrain: epoch  9, batch     1 | loss: 50.8743671CurrentTrain: epoch  9, batch     2 | loss: 86.0169608CurrentTrain: epoch  9, batch     3 | loss: 58.1007413
MemoryTrain:  epoch  0, batch     0 | loss: 0.3396611MemoryTrain:  epoch  1, batch     0 | loss: 0.3121138MemoryTrain:  epoch  2, batch     0 | loss: 0.2440791MemoryTrain:  epoch  3, batch     0 | loss: 0.2042331MemoryTrain:  epoch  4, batch     0 | loss: 0.1656447MemoryTrain:  epoch  5, batch     0 | loss: 0.1303593MemoryTrain:  epoch  6, batch     0 | loss: 0.1100439MemoryTrain:  epoch  7, batch     0 | loss: 0.0882131MemoryTrain:  epoch  8, batch     0 | loss: 0.0804230MemoryTrain:  epoch  9, batch     0 | loss: 0.0702453

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.7777777777777778, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4927536231884058, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.4523809523809524, 37: 0.40707964601769914, 38: 0.4383561643835616}
Micro-average F1 score: 0.3374485596707819
Weighted-average F1 score: 0.24852229431998202
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.4722222222222222, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.7121212121212122, 37: 0.3815028901734104, 38: 0.41509433962264153}
Micro-average F1 score: 0.3075030750307503
Weighted-average F1 score: 0.24174310627411333
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.7368421052631579, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.4788732394366197, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.6666666666666666, 37: 0.3815028901734104, 38: 0.42990654205607476}
Micro-average F1 score: 0.3084702907711757
Weighted-average F1 score: 0.2420383286587957

F1 score per class: {0: 0.5945945945945946, 2: 0.26666666666666666, 4: 0.9128205128205128, 5: 0.8, 6: 0.30434782608695654, 10: 0.08695652173913043, 11: 0.0, 12: 0.18181818181818182, 13: 0.013986013986013986, 15: 0.3333333333333333, 16: 0.3851851851851852, 17: 0.08955223880597014, 18: 0.07017543859649122, 19: 0.43636363636363634, 21: 0.21568627450980393, 23: 0.6419753086419753, 24: 0.07692307692307693, 25: 0.4927536231884058, 26: 0.6598984771573604, 28: 0.3, 29: 0.7213114754098361, 32: 0.7309644670050761, 35: 0.2753623188405797, 37: 0.09603340292275574, 38: 0.11469534050179211, 39: 0.0}
Micro-average F1 score: 0.3756786102062975
Weighted-average F1 score: 0.3415585511320505
F1 score per class: {0: 0.2681992337164751, 2: 0.06930693069306931, 4: 0.9359605911330049, 5: 0.47393364928909953, 6: 0.3517915309446254, 10: 0.2374429223744292, 11: 0.01652892561983471, 12: 0.2103386809269162, 13: 0.009523809523809525, 15: 0.34285714285714286, 16: 0.34838709677419355, 17: 0.136986301369863, 18: 0.0, 19: 0.484375, 21: 0.11382113821138211, 23: 0.4943820224719101, 24: 0.15730337078651685, 25: 0.4722222222222222, 26: 0.6330275229357798, 28: 0.13559322033898305, 29: 0.6505190311418685, 32: 0.6021505376344086, 35: 0.29936305732484075, 37: 0.06818181818181818, 38: 0.10208816705336426, 39: 0.2631578947368421}
Micro-average F1 score: 0.2852179406190777
Weighted-average F1 score: 0.2531127842467345
F1 score per class: {0: 0.2702702702702703, 2: 0.0707070707070707, 4: 0.9353233830845771, 5: 0.5340599455040872, 6: 0.34576271186440677, 10: 0.18633540372670807, 11: 0.0, 12: 0.20959147424511546, 13: 0.010256410256410256, 15: 0.3181818181818182, 16: 0.34394904458598724, 17: 0.1282051282051282, 18: 0.0, 19: 0.49166666666666664, 21: 0.11282051282051282, 23: 0.4819277108433735, 24: 0.18181818181818182, 25: 0.4788732394366197, 26: 0.641860465116279, 28: 0.16, 29: 0.6666666666666666, 32: 0.6332046332046332, 35: 0.2896551724137931, 37: 0.06707317073170732, 38: 0.09406952965235174, 39: 0.13333333333333333}
Micro-average F1 score: 0.2831109664822649
Weighted-average F1 score: 0.24858053736560953
cur_acc:  ['0.6941', '0.5173', '0.2183', '0.6165', '0.3374']
his_acc:  ['0.6941', '0.5916', '0.4865', '0.4867', '0.3757']
cur_acc des:  ['0.6313', '0.4663', '0.2994', '0.3966', '0.3075']
his_acc des:  ['0.6313', '0.5061', '0.3948', '0.3627', '0.2852']
cur_acc rrf:  ['0.6373', '0.4701', '0.2627', '0.4186', '0.3085']
his_acc rrf:  ['0.6373', '0.5063', '0.3889', '0.3651', '0.2831']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 76.5820429CurrentTrain: epoch  0, batch     1 | loss: 96.9232346CurrentTrain: epoch  0, batch     2 | loss: 76.7032263CurrentTrain: epoch  0, batch     3 | loss: 49.9845804CurrentTrain: epoch  1, batch     0 | loss: 91.5422784CurrentTrain: epoch  1, batch     1 | loss: 122.0842875CurrentTrain: epoch  1, batch     2 | loss: 60.1763749CurrentTrain: epoch  1, batch     3 | loss: 44.2699935CurrentTrain: epoch  2, batch     0 | loss: 71.6347168CurrentTrain: epoch  2, batch     1 | loss: 69.6206976CurrentTrain: epoch  2, batch     2 | loss: 68.3473644CurrentTrain: epoch  2, batch     3 | loss: 42.3173291CurrentTrain: epoch  3, batch     0 | loss: 56.6441407CurrentTrain: epoch  3, batch     1 | loss: 67.6419237CurrentTrain: epoch  3, batch     2 | loss: 55.4395068CurrentTrain: epoch  3, batch     3 | loss: 55.0847862CurrentTrain: epoch  4, batch     0 | loss: 68.3373259CurrentTrain: epoch  4, batch     1 | loss: 85.1469412CurrentTrain: epoch  4, batch     2 | loss: 63.5040322CurrentTrain: epoch  4, batch     3 | loss: 40.3402963CurrentTrain: epoch  5, batch     0 | loss: 67.9448928CurrentTrain: epoch  5, batch     1 | loss: 86.7440678CurrentTrain: epoch  5, batch     2 | loss: 48.8682421CurrentTrain: epoch  5, batch     3 | loss: 52.0653691CurrentTrain: epoch  6, batch     0 | loss: 66.0942817CurrentTrain: epoch  6, batch     1 | loss: 81.4454907CurrentTrain: epoch  6, batch     2 | loss: 80.8389851CurrentTrain: epoch  6, batch     3 | loss: 49.6647849CurrentTrain: epoch  7, batch     0 | loss: 66.0658967CurrentTrain: epoch  7, batch     1 | loss: 52.1540607CurrentTrain: epoch  7, batch     2 | loss: 65.1168743CurrentTrain: epoch  7, batch     3 | loss: 39.3589502CurrentTrain: epoch  8, batch     0 | loss: 50.5429871CurrentTrain: epoch  8, batch     1 | loss: 115.6815160CurrentTrain: epoch  8, batch     2 | loss: 52.4843993CurrentTrain: epoch  8, batch     3 | loss: 38.1044845CurrentTrain: epoch  9, batch     0 | loss: 63.9348556CurrentTrain: epoch  9, batch     1 | loss: 81.4340926CurrentTrain: epoch  9, batch     2 | loss: 61.6852402CurrentTrain: epoch  9, batch     3 | loss: 69.6554081
MemoryTrain:  epoch  0, batch     0 | loss: 0.2039071MemoryTrain:  epoch  1, batch     0 | loss: 0.2587895MemoryTrain:  epoch  2, batch     0 | loss: 0.1709382MemoryTrain:  epoch  3, batch     0 | loss: 0.1480036MemoryTrain:  epoch  4, batch     0 | loss: 0.1154237MemoryTrain:  epoch  5, batch     0 | loss: 0.0994138MemoryTrain:  epoch  6, batch     0 | loss: 0.0808568MemoryTrain:  epoch  7, batch     0 | loss: 0.0737956MemoryTrain:  epoch  8, batch     0 | loss: 0.0726561MemoryTrain:  epoch  9, batch     0 | loss: 0.0676686

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.28865979381443296, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.7017543859649122, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 32: 0.0, 33: 0.1, 35: 0.0, 36: 0.11267605633802817, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3162217659137577
Weighted-average F1 score: 0.25664341271144264
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.56, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5897435897435898, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9743589743589743, 32: 0.0, 33: 0.2222222222222222, 35: 0.0, 36: 0.5166666666666667, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.30751708428246016
Weighted-average F1 score: 0.21288497866550418
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5555555555555556, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6013071895424836, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9743589743589743, 32: 0.0, 33: 0.15384615384615385, 35: 0.0, 36: 0.49056603773584906, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.30677764565992865
Weighted-average F1 score: 0.209749057490183

F1 score per class: {0: 0.6153846153846154, 2: 0.20833333333333334, 4: 0.8478260869565217, 5: 0.7084870848708487, 6: 0.3048780487804878, 8: 0.22764227642276422, 10: 0.07017543859649122, 11: 0.0, 12: 0.20765027322404372, 13: 0.02040816326530612, 15: 0.4, 16: 0.3816793893129771, 17: 0.13559322033898305, 18: 0.12048192771084337, 19: 0.38961038961038963, 20: 0.2952029520295203, 21: 0.20689655172413793, 23: 0.525, 24: 0.07142857142857142, 25: 0.4722222222222222, 26: 0.624390243902439, 28: 0.21052631578947367, 29: 0.7264957264957265, 30: 0.9473684210526315, 32: 0.6842105263157895, 33: 0.06896551724137931, 35: 0.23255813953488372, 36: 0.10526315789473684, 37: 0.19607843137254902, 38: 0.12359550561797752, 39: 0.0}
Micro-average F1 score: 0.3901639344262295
Weighted-average F1 score: 0.39096075339043235
F1 score per class: {0: 0.35602094240837695, 2: 0.15, 4: 0.9411764705882353, 5: 0.3412969283276451, 6: 0.32989690721649484, 8: 0.3286384976525822, 10: 0.2698412698412698, 11: 0.02040816326530612, 12: 0.2, 13: 0.014492753623188406, 15: 0.26666666666666666, 16: 0.30857142857142855, 17: 0.19444444444444445, 18: 0.07792207792207792, 19: 0.4163265306122449, 20: 0.17228464419475656, 21: 0.11299435028248588, 23: 0.4470588235294118, 24: 0.17142857142857143, 25: 0.47619047619047616, 26: 0.6, 28: 0.13333333333333333, 29: 0.6793893129770993, 30: 0.8085106382978723, 32: 0.611764705882353, 33: 0.07547169811320754, 35: 0.2572347266881029, 36: 0.37575757575757573, 37: 0.11737089201877934, 38: 0.10764872521246459, 39: 0.10256410256410256}
Micro-average F1 score: 0.30061162079510706
Weighted-average F1 score: 0.2746928333410655
F1 score per class: {0: 0.3487179487179487, 2: 0.19047619047619047, 4: 0.9292929292929293, 5: 0.35842293906810035, 6: 0.33210332103321033, 8: 0.32558139534883723, 10: 0.2125, 11: 0.0, 12: 0.2021978021978022, 13: 0.025974025974025976, 15: 0.2916666666666667, 16: 0.30337078651685395, 17: 0.15584415584415584, 18: 0.0784313725490196, 19: 0.44144144144144143, 20: 0.1726078799249531, 21: 0.12903225806451613, 23: 0.42857142857142855, 24: 0.13636363636363635, 25: 0.47619047619047616, 26: 0.6074766355140186, 28: 0.13953488372093023, 29: 0.694980694980695, 30: 0.7450980392156863, 32: 0.6582278481012658, 33: 0.05405405405405406, 35: 0.226890756302521, 36: 0.36879432624113473, 37: 0.1187214611872146, 38: 0.09813084112149532, 39: 0.125}
Micro-average F1 score: 0.29827066476281133
Weighted-average F1 score: 0.27055062587479545
cur_acc:  ['0.6941', '0.5173', '0.2183', '0.6165', '0.3374', '0.3162']
his_acc:  ['0.6941', '0.5916', '0.4865', '0.4867', '0.3757', '0.3902']
cur_acc des:  ['0.6313', '0.4663', '0.2994', '0.3966', '0.3075', '0.3075']
his_acc des:  ['0.6313', '0.5061', '0.3948', '0.3627', '0.2852', '0.3006']
cur_acc rrf:  ['0.6373', '0.4701', '0.2627', '0.4186', '0.3085', '0.3068']
his_acc rrf:  ['0.6373', '0.5063', '0.3889', '0.3651', '0.2831', '0.2983']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 67.3410297CurrentTrain: epoch  0, batch     1 | loss: 66.9856713CurrentTrain: epoch  0, batch     2 | loss: 61.5116228CurrentTrain: epoch  0, batch     3 | loss: 11.5798584CurrentTrain: epoch  1, batch     0 | loss: 57.8887148CurrentTrain: epoch  1, batch     1 | loss: 91.4043570CurrentTrain: epoch  1, batch     2 | loss: 59.8489840CurrentTrain: epoch  1, batch     3 | loss: 8.4101255CurrentTrain: epoch  2, batch     0 | loss: 88.3843777CurrentTrain: epoch  2, batch     1 | loss: 82.7353825CurrentTrain: epoch  2, batch     2 | loss: 55.5431360CurrentTrain: epoch  2, batch     3 | loss: 11.9189762CurrentTrain: epoch  3, batch     0 | loss: 64.0949411CurrentTrain: epoch  3, batch     1 | loss: 67.2798962CurrentTrain: epoch  3, batch     2 | loss: 52.8586009CurrentTrain: epoch  3, batch     3 | loss: 6.9369027CurrentTrain: epoch  4, batch     0 | loss: 82.7059164CurrentTrain: epoch  4, batch     1 | loss: 53.0556566CurrentTrain: epoch  4, batch     2 | loss: 52.9030408CurrentTrain: epoch  4, batch     3 | loss: 11.1457918CurrentTrain: epoch  5, batch     0 | loss: 63.5152795CurrentTrain: epoch  5, batch     1 | loss: 63.7523118CurrentTrain: epoch  5, batch     2 | loss: 65.4403428CurrentTrain: epoch  5, batch     3 | loss: 3.3440489CurrentTrain: epoch  6, batch     0 | loss: 53.3441960CurrentTrain: epoch  6, batch     1 | loss: 83.2756425CurrentTrain: epoch  6, batch     2 | loss: 60.3936597CurrentTrain: epoch  6, batch     3 | loss: 10.9759313CurrentTrain: epoch  7, batch     0 | loss: 49.6637485CurrentTrain: epoch  7, batch     1 | loss: 65.3463986CurrentTrain: epoch  7, batch     2 | loss: 63.4276315CurrentTrain: epoch  7, batch     3 | loss: 10.9837901CurrentTrain: epoch  8, batch     0 | loss: 47.7776667CurrentTrain: epoch  8, batch     1 | loss: 64.5967566CurrentTrain: epoch  8, batch     2 | loss: 52.5538627CurrentTrain: epoch  8, batch     3 | loss: 27.6253325CurrentTrain: epoch  9, batch     0 | loss: 62.6818931CurrentTrain: epoch  9, batch     1 | loss: 47.8296195CurrentTrain: epoch  9, batch     2 | loss: 65.5721774CurrentTrain: epoch  9, batch     3 | loss: 27.4335410
MemoryTrain:  epoch  0, batch     0 | loss: 0.2763377MemoryTrain:  epoch  1, batch     0 | loss: 0.3003414MemoryTrain:  epoch  2, batch     0 | loss: 0.1860666MemoryTrain:  epoch  3, batch     0 | loss: 0.1496226MemoryTrain:  epoch  4, batch     0 | loss: 0.1312946MemoryTrain:  epoch  5, batch     0 | loss: 0.1157680MemoryTrain:  epoch  6, batch     0 | loss: 0.0972647MemoryTrain:  epoch  7, batch     0 | loss: 0.0906644MemoryTrain:  epoch  8, batch     0 | loss: 0.0822922MemoryTrain:  epoch  9, batch     0 | loss: 0.0751824

F1 score per class: {0: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 6: 0.9259259259259259, 7: 0.0, 40: 0.0, 9: 0.0, 8: 0.0, 12: 0.0, 13: 0.0, 16: 0.4, 17: 0.2, 19: 0.0, 26: 0.0, 27: 0.0, 31: 0.4878048780487805}
Micro-average F1 score: 0.4230769230769231
Weighted-average F1 score: 0.34926865766974163
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.8333333333333334, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.43902439024390244, 30: 0.0, 31: 0.16, 32: 0.0, 33: 0.0, 35: 0.0, 37: 0.0, 40: 0.5566037735849056}
Micro-average F1 score: 0.3877551020408163
Weighted-average F1 score: 0.33100255227763037
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.847457627118644, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.40816326530612246, 31: 0.18181818181818182, 32: 0.0, 33: 0.0, 35: 0.0, 37: 0.0, 40: 0.5742574257425742}
Micro-average F1 score: 0.40860215053763443
Weighted-average F1 score: 0.3511305923983027

F1 score per class: {0: 0.40963855421686746, 2: 0.19230769230769232, 4: 0.8602150537634409, 5: 0.6879432624113475, 6: 0.25316455696202533, 7: 0.0, 8: 0.23300970873786409, 9: 0.9259259259259259, 10: 0.05825242718446602, 11: 0.0, 12: 0.14285714285714285, 13: 0.024096385542168676, 15: 0.4, 16: 0.3968253968253968, 17: 0.3333333333333333, 18: 0.1016949152542373, 19: 0.4253393665158371, 20: 0.24773413897280966, 21: 0.15555555555555556, 23: 0.5641025641025641, 24: 0.08695652173913043, 25: 0.5066666666666667, 26: 0.6, 27: 0.11494252873563218, 28: 0.25, 29: 0.7312775330396476, 30: 0.9473684210526315, 31: 0.034482758620689655, 32: 0.6, 33: 0.1935483870967742, 35: 0.25287356321839083, 36: 0.15584415584415584, 37: 0.16740088105726872, 38: 0.07272727272727272, 39: 0.0, 40: 0.3488372093023256}
Micro-average F1 score: 0.3599439775910364
Weighted-average F1 score: 0.3465754172145592
F1 score per class: {0: 0.17391304347826086, 2: 0.14736842105263157, 4: 0.9306930693069307, 5: 0.30864197530864196, 6: 0.2857142857142857, 7: 0.0, 8: 0.3448275862068966, 9: 0.7142857142857143, 10: 0.26540284360189575, 11: 0.01904761904761905, 12: 0.21348314606741572, 13: 0.010471204188481676, 15: 0.24, 16: 0.3181818181818182, 17: 0.13793103448275862, 18: 0.08588957055214724, 19: 0.41818181818181815, 20: 0.16576576576576577, 21: 0.12612612612612611, 23: 0.4835164835164835, 24: 0.12903225806451613, 25: 0.45454545454545453, 26: 0.5851528384279476, 27: 0.11392405063291139, 28: 0.16666666666666666, 29: 0.6907630522088354, 30: 0.8444444444444444, 31: 0.021505376344086023, 32: 0.6031746031746031, 33: 0.05309734513274336, 35: 0.28363636363636363, 36: 0.42105263157894735, 37: 0.0963855421686747, 38: 0.12048192771084337, 39: 0.15384615384615385, 40: 0.2964824120603015}
Micro-average F1 score: 0.2745200470158025
Weighted-average F1 score: 0.24579190604324494
F1 score per class: {0: 0.17326732673267325, 2: 0.1891891891891892, 4: 0.9183673469387755, 5: 0.33557046979865773, 6: 0.2692307692307692, 7: 0.0, 8: 0.3448275862068966, 9: 0.7692307692307693, 10: 0.19875776397515527, 11: 0.020618556701030927, 12: 0.2193211488250653, 13: 0.01092896174863388, 15: 0.24489795918367346, 16: 0.3236994219653179, 17: 0.1518987341772152, 18: 0.08235294117647059, 19: 0.45, 20: 0.16399286987522282, 21: 0.15767634854771784, 23: 0.449438202247191, 24: 0.07692307692307693, 25: 0.449438202247191, 26: 0.5972850678733032, 27: 0.10204081632653061, 28: 0.2, 29: 0.7073170731707317, 30: 0.8260869565217391, 31: 0.028985507246376812, 32: 0.646551724137931, 33: 0.04878048780487805, 35: 0.2564102564102564, 36: 0.3776223776223776, 37: 0.10385259631490787, 38: 0.10830324909747292, 39: 0.09090909090909091, 40: 0.3258426966292135}
Micro-average F1 score: 0.278722521775197
Weighted-average F1 score: 0.24894372059058786
cur_acc:  ['0.6941', '0.5173', '0.2183', '0.6165', '0.3374', '0.3162', '0.4231']
his_acc:  ['0.6941', '0.5916', '0.4865', '0.4867', '0.3757', '0.3902', '0.3599']
cur_acc des:  ['0.6313', '0.4663', '0.2994', '0.3966', '0.3075', '0.3075', '0.3878']
his_acc des:  ['0.6313', '0.5061', '0.3948', '0.3627', '0.2852', '0.3006', '0.2745']
cur_acc rrf:  ['0.6373', '0.4701', '0.2627', '0.4186', '0.3085', '0.3068', '0.4086']
his_acc rrf:  ['0.6373', '0.5063', '0.3889', '0.3651', '0.2831', '0.2983', '0.2787']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 96.3221012CurrentTrain: epoch  0, batch     1 | loss: 80.6325039CurrentTrain: epoch  0, batch     2 | loss: 83.7344676CurrentTrain: epoch  0, batch     3 | loss: 66.9034455CurrentTrain: epoch  0, batch     4 | loss: 112.4264407CurrentTrain: epoch  1, batch     0 | loss: 77.0040909CurrentTrain: epoch  1, batch     1 | loss: 93.3980697CurrentTrain: epoch  1, batch     2 | loss: 92.1720907CurrentTrain: epoch  1, batch     3 | loss: 91.5456884CurrentTrain: epoch  1, batch     4 | loss: 51.3634592CurrentTrain: epoch  2, batch     0 | loss: 75.9736985CurrentTrain: epoch  2, batch     1 | loss: 57.4439733CurrentTrain: epoch  2, batch     2 | loss: 68.7714806CurrentTrain: epoch  2, batch     3 | loss: 74.3335292CurrentTrain: epoch  2, batch     4 | loss: 214.3038519CurrentTrain: epoch  3, batch     0 | loss: 89.5154159CurrentTrain: epoch  3, batch     1 | loss: 64.5928064CurrentTrain: epoch  3, batch     2 | loss: 120.0786587CurrentTrain: epoch  3, batch     3 | loss: 85.9130593CurrentTrain: epoch  3, batch     4 | loss: 38.4827137CurrentTrain: epoch  4, batch     0 | loss: 68.3546795CurrentTrain: epoch  4, batch     1 | loss: 68.5012894CurrentTrain: epoch  4, batch     2 | loss: 68.3819703CurrentTrain: epoch  4, batch     3 | loss: 85.9235002CurrentTrain: epoch  4, batch     4 | loss: 48.7722960CurrentTrain: epoch  5, batch     0 | loss: 119.9668986CurrentTrain: epoch  5, batch     1 | loss: 83.9749538CurrentTrain: epoch  5, batch     2 | loss: 64.2323712CurrentTrain: epoch  5, batch     3 | loss: 64.3729831CurrentTrain: epoch  5, batch     4 | loss: 103.8150224CurrentTrain: epoch  6, batch     0 | loss: 53.1716056CurrentTrain: epoch  6, batch     1 | loss: 65.3102162CurrentTrain: epoch  6, batch     2 | loss: 66.4982111CurrentTrain: epoch  6, batch     3 | loss: 86.4091468CurrentTrain: epoch  6, batch     4 | loss: 66.0982350CurrentTrain: epoch  7, batch     0 | loss: 86.4767456CurrentTrain: epoch  7, batch     1 | loss: 63.2284107CurrentTrain: epoch  7, batch     2 | loss: 67.9151007CurrentTrain: epoch  7, batch     3 | loss: 67.9307491CurrentTrain: epoch  7, batch     4 | loss: 45.0682782CurrentTrain: epoch  8, batch     0 | loss: 65.7876036CurrentTrain: epoch  8, batch     1 | loss: 53.9891881CurrentTrain: epoch  8, batch     2 | loss: 118.6558610CurrentTrain: epoch  8, batch     3 | loss: 62.0675687CurrentTrain: epoch  8, batch     4 | loss: 102.9693203CurrentTrain: epoch  9, batch     0 | loss: 84.3960508CurrentTrain: epoch  9, batch     1 | loss: 65.5554758CurrentTrain: epoch  9, batch     2 | loss: 66.0446518CurrentTrain: epoch  9, batch     3 | loss: 52.7392081CurrentTrain: epoch  9, batch     4 | loss: 63.6070169
MemoryTrain:  epoch  0, batch     0 | loss: 0.4119202MemoryTrain:  epoch  1, batch     0 | loss: 0.3672983MemoryTrain:  epoch  2, batch     0 | loss: 0.3198273MemoryTrain:  epoch  3, batch     0 | loss: 0.2366795MemoryTrain:  epoch  4, batch     0 | loss: 0.2102016MemoryTrain:  epoch  5, batch     0 | loss: 0.1743869MemoryTrain:  epoch  6, batch     0 | loss: 0.1561856MemoryTrain:  epoch  7, batch     0 | loss: 0.1356363MemoryTrain:  epoch  8, batch     0 | loss: 0.1070497MemoryTrain:  epoch  9, batch     0 | loss: 0.0969086

F1 score per class: {0: 0.0, 1: 0.24096385542168675, 2: 0.0, 3: 0.7169811320754716, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 12: 0.0, 13: 0.0, 14: 0.12716763005780346, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.3392226148409894, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 34: 0.2222222222222222, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.23539823008849559
Weighted-average F1 score: 0.18116424493183245
F1 score per class: {0: 0.0, 1: 0.22695035460992907, 2: 0.0, 3: 0.5046728971962616, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.17391304347826086, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.3864406779661017, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.25316455696202533, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.17435897435897435
Weighted-average F1 score: 0.13546762018547867
F1 score per class: {0: 0.0, 1: 0.2222222222222222, 2: 0.0, 3: 0.5414847161572053, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 12: 0.0, 13: 0.0, 14: 0.15217391304347827, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.3918918918918919, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.2608695652173913, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.18683901292596944
Weighted-average F1 score: 0.14779152142199414

F1 score per class: {0: 0.4, 1: 0.1724137931034483, 2: 0.20833333333333334, 3: 0.47107438016528924, 4: 0.7745664739884393, 5: 0.8154506437768241, 6: 0.21951219512195122, 7: 0.0, 8: 0.18518518518518517, 9: 0.9803921568627451, 10: 0.09090909090909091, 11: 0.0, 12: 0.14814814814814814, 13: 0.01904761904761905, 14: 0.08029197080291971, 15: 0.4444444444444444, 16: 0.39669421487603307, 17: 0.06666666666666667, 18: 0.13793103448275862, 19: 0.11666666666666667, 20: 0.3162393162393162, 21: 0.07142857142857142, 22: 0.2644628099173554, 23: 0.5, 24: 0.05405405405405406, 25: 0.5070422535211268, 26: 0.5933014354066986, 27: 0.05714285714285714, 28: 0.2727272727272727, 29: 0.691358024691358, 30: 0.9230769230769231, 31: 0.0, 32: 0.5688073394495413, 33: 0.15, 34: 0.09395973154362416, 35: 0.24347826086956523, 36: 0.02666666666666667, 37: 0.17733990147783252, 38: 0.07207207207207207, 39: 0.0, 40: 0.36904761904761907}
Micro-average F1 score: 0.3228943785255787
Weighted-average F1 score: 0.31528248196113245
F1 score per class: {0: 0.22591362126245848, 1: 0.12403100775193798, 2: 0.1794871794871795, 3: 0.24269662921348314, 4: 0.9207920792079208, 5: 0.5012531328320802, 6: 0.24691358024691357, 7: 0.0, 8: 0.3142857142857143, 9: 0.78125, 10: 0.3106796116504854, 11: 0.0, 12: 0.23094688221709006, 13: 0.00847457627118644, 14: 0.10355987055016182, 15: 0.3, 16: 0.35526315789473684, 17: 0.13953488372093023, 18: 0.08163265306122448, 19: 0.23129251700680273, 20: 0.19909502262443438, 21: 0.13658536585365855, 22: 0.28287841191067, 23: 0.4842105263157895, 24: 0.05128205128205128, 25: 0.4819277108433735, 26: 0.5714285714285714, 27: 0.049079754601226995, 28: 0.20512820512820512, 29: 0.6470588235294118, 30: 0.631578947368421, 31: 0.04395604395604396, 32: 0.4678362573099415, 33: 0.04054054054054054, 34: 0.06944444444444445, 35: 0.2254335260115607, 36: 0.4105960264900662, 37: 0.06814814814814815, 38: 0.0912863070539419, 39: 0.12903225806451613, 40: 0.3111111111111111}
Micro-average F1 score: 0.25980780101752404
Weighted-average F1 score: 0.23811249097355747
F1 score per class: {0: 0.22006472491909385, 1: 0.11552346570397112, 2: 0.17391304347826086, 3: 0.2605042016806723, 4: 0.9183673469387755, 5: 0.554016620498615, 6: 0.24369747899159663, 7: 0.0, 8: 0.3076923076923077, 9: 0.8928571428571429, 10: 0.2564102564102564, 11: 0.0, 12: 0.21670428893905191, 13: 0.0091324200913242, 14: 0.08408408408408409, 15: 0.3, 16: 0.34838709677419355, 17: 0.14084507042253522, 18: 0.08823529411764706, 19: 0.2446043165467626, 20: 0.1925601750547046, 21: 0.15254237288135594, 22: 0.2885572139303483, 23: 0.4782608695652174, 24: 0.05128205128205128, 25: 0.5, 26: 0.5726872246696035, 27: 0.04878048780487805, 28: 0.22857142857142856, 29: 0.654275092936803, 30: 0.6666666666666666, 31: 0.07142857142857142, 32: 0.5205479452054794, 33: 0.03550295857988166, 34: 0.07659574468085106, 35: 0.24444444444444444, 36: 0.34782608695652173, 37: 0.07564296520423601, 38: 0.08208955223880597, 39: 0.0, 40: 0.30409356725146197}
Micro-average F1 score: 0.2611570247933884
Weighted-average F1 score: 0.237892012073191
cur_acc:  ['0.6941', '0.5173', '0.2183', '0.6165', '0.3374', '0.3162', '0.4231', '0.2354']
his_acc:  ['0.6941', '0.5916', '0.4865', '0.4867', '0.3757', '0.3902', '0.3599', '0.3229']
cur_acc des:  ['0.6313', '0.4663', '0.2994', '0.3966', '0.3075', '0.3075', '0.3878', '0.1744']
his_acc des:  ['0.6313', '0.5061', '0.3948', '0.3627', '0.2852', '0.3006', '0.2745', '0.2598']
cur_acc rrf:  ['0.6373', '0.4701', '0.2627', '0.4186', '0.3085', '0.3068', '0.4086', '0.1868']
his_acc rrf:  ['0.6373', '0.5063', '0.3889', '0.3651', '0.2831', '0.2983', '0.2787', '0.2612']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 58.0685880CurrentTrain: epoch  0, batch     1 | loss: 67.5810251CurrentTrain: epoch  0, batch     2 | loss: 79.2723345CurrentTrain: epoch  0, batch     3 | loss: 97.6317699CurrentTrain: epoch  0, batch     4 | loss: 78.9570841CurrentTrain: epoch  0, batch     5 | loss: 66.4709226CurrentTrain: epoch  0, batch     6 | loss: 97.1530693CurrentTrain: epoch  0, batch     7 | loss: 66.9599726CurrentTrain: epoch  0, batch     8 | loss: 96.9225120CurrentTrain: epoch  0, batch     9 | loss: 65.0905252CurrentTrain: epoch  0, batch    10 | loss: 77.9336969CurrentTrain: epoch  0, batch    11 | loss: 96.2419718CurrentTrain: epoch  0, batch    12 | loss: 128.4005710CurrentTrain: epoch  0, batch    13 | loss: 66.1584016CurrentTrain: epoch  0, batch    14 | loss: 65.2128942CurrentTrain: epoch  0, batch    15 | loss: 77.7972904CurrentTrain: epoch  0, batch    16 | loss: 65.1922923CurrentTrain: epoch  0, batch    17 | loss: 126.6566413CurrentTrain: epoch  0, batch    18 | loss: 77.6651250CurrentTrain: epoch  0, batch    19 | loss: 77.6186108CurrentTrain: epoch  0, batch    20 | loss: 56.1822434CurrentTrain: epoch  0, batch    21 | loss: 64.6252288CurrentTrain: epoch  0, batch    22 | loss: 55.6868907CurrentTrain: epoch  0, batch    23 | loss: 95.0319364CurrentTrain: epoch  0, batch    24 | loss: 79.1719294CurrentTrain: epoch  0, batch    25 | loss: 77.1579647CurrentTrain: epoch  0, batch    26 | loss: 125.5651378CurrentTrain: epoch  0, batch    27 | loss: 76.7407720CurrentTrain: epoch  0, batch    28 | loss: 64.9089004CurrentTrain: epoch  0, batch    29 | loss: 95.6345798CurrentTrain: epoch  0, batch    30 | loss: 188.0956238CurrentTrain: epoch  0, batch    31 | loss: 56.6637847CurrentTrain: epoch  0, batch    32 | loss: 64.2482148CurrentTrain: epoch  0, batch    33 | loss: 76.3062744CurrentTrain: epoch  0, batch    34 | loss: 77.1464641CurrentTrain: epoch  0, batch    35 | loss: 95.1773577CurrentTrain: epoch  0, batch    36 | loss: 64.9671674CurrentTrain: epoch  0, batch    37 | loss: 77.4787396CurrentTrain: epoch  0, batch    38 | loss: 64.5767667CurrentTrain: epoch  0, batch    39 | loss: 75.9340945CurrentTrain: epoch  0, batch    40 | loss: 55.9180871CurrentTrain: epoch  0, batch    41 | loss: 126.0742290CurrentTrain: epoch  0, batch    42 | loss: 76.6385717CurrentTrain: epoch  0, batch    43 | loss: 64.7048504CurrentTrain: epoch  0, batch    44 | loss: 76.2390955CurrentTrain: epoch  0, batch    45 | loss: 64.5646791CurrentTrain: epoch  0, batch    46 | loss: 126.4639319CurrentTrain: epoch  0, batch    47 | loss: 76.0673073CurrentTrain: epoch  0, batch    48 | loss: 55.8129761CurrentTrain: epoch  0, batch    49 | loss: 76.4780680CurrentTrain: epoch  0, batch    50 | loss: 76.6246417CurrentTrain: epoch  0, batch    51 | loss: 63.7822695CurrentTrain: epoch  0, batch    52 | loss: 64.2887485CurrentTrain: epoch  0, batch    53 | loss: 64.1976655CurrentTrain: epoch  0, batch    54 | loss: 63.6340222CurrentTrain: epoch  0, batch    55 | loss: 75.5511166CurrentTrain: epoch  0, batch    56 | loss: 76.5730975CurrentTrain: epoch  0, batch    57 | loss: 95.4287842CurrentTrain: epoch  0, batch    58 | loss: 93.4476652CurrentTrain: epoch  0, batch    59 | loss: 93.4808067CurrentTrain: epoch  0, batch    60 | loss: 63.1437453CurrentTrain: epoch  0, batch    61 | loss: 63.6102272CurrentTrain: epoch  0, batch    62 | loss: 372.1616374CurrentTrain: epoch  0, batch    63 | loss: 63.0789549CurrentTrain: epoch  0, batch    64 | loss: 63.1551919CurrentTrain: epoch  0, batch    65 | loss: 63.2320366CurrentTrain: epoch  0, batch    66 | loss: 74.0322904CurrentTrain: epoch  0, batch    67 | loss: 125.2307499CurrentTrain: epoch  0, batch    68 | loss: 74.7068329CurrentTrain: epoch  0, batch    69 | loss: 75.7179242CurrentTrain: epoch  0, batch    70 | loss: 122.9533900CurrentTrain: epoch  0, batch    71 | loss: 94.3087673CurrentTrain: epoch  0, batch    72 | loss: 74.0636646CurrentTrain: epoch  0, batch    73 | loss: 92.8999473CurrentTrain: epoch  0, batch    74 | loss: 76.5806761CurrentTrain: epoch  0, batch    75 | loss: 73.1875720CurrentTrain: epoch  0, batch    76 | loss: 74.5031208CurrentTrain: epoch  0, batch    77 | loss: 94.6808211CurrentTrain: epoch  0, batch    78 | loss: 74.5135730CurrentTrain: epoch  0, batch    79 | loss: 121.6998777CurrentTrain: epoch  0, batch    80 | loss: 73.3027843CurrentTrain: epoch  0, batch    81 | loss: 91.9246106CurrentTrain: epoch  0, batch    82 | loss: 92.7848602CurrentTrain: epoch  0, batch    83 | loss: 72.8973603CurrentTrain: epoch  0, batch    84 | loss: 93.1605923CurrentTrain: epoch  0, batch    85 | loss: 74.2236890CurrentTrain: epoch  0, batch    86 | loss: 92.9158319CurrentTrain: epoch  0, batch    87 | loss: 91.5788088CurrentTrain: epoch  0, batch    88 | loss: 72.0031579CurrentTrain: epoch  0, batch    89 | loss: 59.1702077CurrentTrain: epoch  0, batch    90 | loss: 62.4618032CurrentTrain: epoch  0, batch    91 | loss: 60.1342677CurrentTrain: epoch  0, batch    92 | loss: 61.0376169CurrentTrain: epoch  0, batch    93 | loss: 62.0811054CurrentTrain: epoch  0, batch    94 | loss: 59.2619950CurrentTrain: epoch  0, batch    95 | loss: 102.7317146CurrentTrain: epoch  1, batch     0 | loss: 71.7902165CurrentTrain: epoch  1, batch     1 | loss: 73.1661173CurrentTrain: epoch  1, batch     2 | loss: 73.7542614CurrentTrain: epoch  1, batch     3 | loss: 73.5310797CurrentTrain: epoch  1, batch     4 | loss: 58.5819914CurrentTrain: epoch  1, batch     5 | loss: 88.2594853CurrentTrain: epoch  1, batch     6 | loss: 58.0632060CurrentTrain: epoch  1, batch     7 | loss: 57.1424494CurrentTrain: epoch  1, batch     8 | loss: 69.7363713CurrentTrain: epoch  1, batch     9 | loss: 58.3667836CurrentTrain: epoch  1, batch    10 | loss: 185.6879863CurrentTrain: epoch  1, batch    11 | loss: 120.9234326CurrentTrain: epoch  1, batch    12 | loss: 60.1264960CurrentTrain: epoch  1, batch    13 | loss: 61.8764519CurrentTrain: epoch  1, batch    14 | loss: 59.8313679CurrentTrain: epoch  1, batch    15 | loss: 72.5663470CurrentTrain: epoch  1, batch    16 | loss: 89.0899545CurrentTrain: epoch  1, batch    17 | loss: 67.4874570CurrentTrain: epoch  1, batch    18 | loss: 70.2647115CurrentTrain: epoch  1, batch    19 | loss: 50.7844486CurrentTrain: epoch  1, batch    20 | loss: 57.1691811CurrentTrain: epoch  1, batch    21 | loss: 60.2455361CurrentTrain: epoch  1, batch    22 | loss: 70.6595664CurrentTrain: epoch  1, batch    23 | loss: 52.0801474CurrentTrain: epoch  1, batch    24 | loss: 86.7012993CurrentTrain: epoch  1, batch    25 | loss: 60.2014067CurrentTrain: epoch  1, batch    26 | loss: 182.7939588CurrentTrain: epoch  1, batch    27 | loss: 71.8573573CurrentTrain: epoch  1, batch    28 | loss: 73.0881547CurrentTrain: epoch  1, batch    29 | loss: 70.6466213CurrentTrain: epoch  1, batch    30 | loss: 58.8913368CurrentTrain: epoch  1, batch    31 | loss: 73.0837607CurrentTrain: epoch  1, batch    32 | loss: 56.3728460CurrentTrain: epoch  1, batch    33 | loss: 74.0226250CurrentTrain: epoch  1, batch    34 | loss: 180.0929105CurrentTrain: epoch  1, batch    35 | loss: 88.7926485CurrentTrain: epoch  1, batch    36 | loss: 89.0031813CurrentTrain: epoch  1, batch    37 | loss: 68.4420212CurrentTrain: epoch  1, batch    38 | loss: 57.0350254CurrentTrain: epoch  1, batch    39 | loss: 87.5897402CurrentTrain: epoch  1, batch    40 | loss: 49.6094753CurrentTrain: epoch  1, batch    41 | loss: 91.4473389CurrentTrain: epoch  1, batch    42 | loss: 72.6025094CurrentTrain: epoch  1, batch    43 | loss: 58.7161797CurrentTrain: epoch  1, batch    44 | loss: 72.0168736CurrentTrain: epoch  1, batch    45 | loss: 52.7186324CurrentTrain: epoch  1, batch    46 | loss: 74.2476248CurrentTrain: epoch  1, batch    47 | loss: 87.5565054CurrentTrain: epoch  1, batch    48 | loss: 117.1799894CurrentTrain: epoch  1, batch    49 | loss: 49.2537375CurrentTrain: epoch  1, batch    50 | loss: 121.1341539CurrentTrain: epoch  1, batch    51 | loss: 53.4874134CurrentTrain: epoch  1, batch    52 | loss: 49.0362123CurrentTrain: epoch  1, batch    53 | loss: 89.5280424CurrentTrain: epoch  1, batch    54 | loss: 53.5274643CurrentTrain: epoch  1, batch    55 | loss: 91.2081194CurrentTrain: epoch  1, batch    56 | loss: 57.5208922CurrentTrain: epoch  1, batch    57 | loss: 89.0874725CurrentTrain: epoch  1, batch    58 | loss: 87.3178923CurrentTrain: epoch  1, batch    59 | loss: 71.1978563CurrentTrain: epoch  1, batch    60 | loss: 68.9925802CurrentTrain: epoch  1, batch    61 | loss: 59.5515531CurrentTrain: epoch  1, batch    62 | loss: 71.6704584CurrentTrain: epoch  1, batch    63 | loss: 85.8745399CurrentTrain: epoch  1, batch    64 | loss: 56.6970442CurrentTrain: epoch  1, batch    65 | loss: 69.0094363CurrentTrain: epoch  1, batch    66 | loss: 55.7622659CurrentTrain: epoch  1, batch    67 | loss: 68.7470458CurrentTrain: epoch  1, batch    68 | loss: 71.7873522CurrentTrain: epoch  1, batch    69 | loss: 54.2531363CurrentTrain: epoch  1, batch    70 | loss: 68.0413493CurrentTrain: epoch  1, batch    71 | loss: 70.7475251CurrentTrain: epoch  1, batch    72 | loss: 70.7445485CurrentTrain: epoch  1, batch    73 | loss: 83.4255609CurrentTrain: epoch  1, batch    74 | loss: 71.4853066CurrentTrain: epoch  1, batch    75 | loss: 70.8625923CurrentTrain: epoch  1, batch    76 | loss: 71.5351295CurrentTrain: epoch  1, batch    77 | loss: 66.9181766CurrentTrain: epoch  1, batch    78 | loss: 57.5599185CurrentTrain: epoch  1, batch    79 | loss: 71.8422934CurrentTrain: epoch  1, batch    80 | loss: 119.4314702CurrentTrain: epoch  1, batch    81 | loss: 58.4353230CurrentTrain: epoch  1, batch    82 | loss: 67.3970386CurrentTrain: epoch  1, batch    83 | loss: 68.8728840CurrentTrain: epoch  1, batch    84 | loss: 69.2187375CurrentTrain: epoch  1, batch    85 | loss: 86.6681271CurrentTrain: epoch  1, batch    86 | loss: 87.2726997CurrentTrain: epoch  1, batch    87 | loss: 50.1846895CurrentTrain: epoch  1, batch    88 | loss: 57.4682088CurrentTrain: epoch  1, batch    89 | loss: 71.3819255CurrentTrain: epoch  1, batch    90 | loss: 92.0050106CurrentTrain: epoch  1, batch    91 | loss: 88.1517037CurrentTrain: epoch  1, batch    92 | loss: 119.2529042CurrentTrain: epoch  1, batch    93 | loss: 88.9959062CurrentTrain: epoch  1, batch    94 | loss: 69.3102594CurrentTrain: epoch  1, batch    95 | loss: 72.8632284CurrentTrain: epoch  2, batch     0 | loss: 87.5507291CurrentTrain: epoch  2, batch     1 | loss: 59.2079755CurrentTrain: epoch  2, batch     2 | loss: 90.1738725CurrentTrain: epoch  2, batch     3 | loss: 74.3766999CurrentTrain: epoch  2, batch     4 | loss: 51.6695281CurrentTrain: epoch  2, batch     5 | loss: 47.3923570CurrentTrain: epoch  2, batch     6 | loss: 47.3493646CurrentTrain: epoch  2, batch     7 | loss: 68.2215697CurrentTrain: epoch  2, batch     8 | loss: 84.1671704CurrentTrain: epoch  2, batch     9 | loss: 52.6801552CurrentTrain: epoch  2, batch    10 | loss: 55.2832933CurrentTrain: epoch  2, batch    11 | loss: 69.4925336CurrentTrain: epoch  2, batch    12 | loss: 70.0575588CurrentTrain: epoch  2, batch    13 | loss: 84.8962535CurrentTrain: epoch  2, batch    14 | loss: 113.2635015CurrentTrain: epoch  2, batch    15 | loss: 45.8889175CurrentTrain: epoch  2, batch    16 | loss: 120.6201918CurrentTrain: epoch  2, batch    17 | loss: 67.5799085CurrentTrain: epoch  2, batch    18 | loss: 87.8781420CurrentTrain: epoch  2, batch    19 | loss: 53.8735555CurrentTrain: epoch  2, batch    20 | loss: 88.9149822CurrentTrain: epoch  2, batch    21 | loss: 57.5169072CurrentTrain: epoch  2, batch    22 | loss: 116.9504059CurrentTrain: epoch  2, batch    23 | loss: 119.1488783CurrentTrain: epoch  2, batch    24 | loss: 55.9230376CurrentTrain: epoch  2, batch    25 | loss: 58.8503583CurrentTrain: epoch  2, batch    26 | loss: 87.4894197CurrentTrain: epoch  2, batch    27 | loss: 54.9654761CurrentTrain: epoch  2, batch    28 | loss: 183.2953240CurrentTrain: epoch  2, batch    29 | loss: 88.3310687CurrentTrain: epoch  2, batch    30 | loss: 114.1234243CurrentTrain: epoch  2, batch    31 | loss: 88.2129459CurrentTrain: epoch  2, batch    32 | loss: 83.7458297CurrentTrain: epoch  2, batch    33 | loss: 49.4652515CurrentTrain: epoch  2, batch    34 | loss: 43.8948164CurrentTrain: epoch  2, batch    35 | loss: 85.0998116CurrentTrain: epoch  2, batch    36 | loss: 117.0849580CurrentTrain: epoch  2, batch    37 | loss: 70.6280469CurrentTrain: epoch  2, batch    38 | loss: 55.8054625CurrentTrain: epoch  2, batch    39 | loss: 68.1985686CurrentTrain: epoch  2, batch    40 | loss: 67.3022493CurrentTrain: epoch  2, batch    41 | loss: 58.3153417CurrentTrain: epoch  2, batch    42 | loss: 55.1838263CurrentTrain: epoch  2, batch    43 | loss: 56.2472995CurrentTrain: epoch  2, batch    44 | loss: 54.2376426CurrentTrain: epoch  2, batch    45 | loss: 84.6895928CurrentTrain: epoch  2, batch    46 | loss: 67.4045301CurrentTrain: epoch  2, batch    47 | loss: 71.0192023CurrentTrain: epoch  2, batch    48 | loss: 68.3301252CurrentTrain: epoch  2, batch    49 | loss: 58.1472923CurrentTrain: epoch  2, batch    50 | loss: 68.6674441CurrentTrain: epoch  2, batch    51 | loss: 54.5631479CurrentTrain: epoch  2, batch    52 | loss: 64.8153248CurrentTrain: epoch  2, batch    53 | loss: 118.3606340CurrentTrain: epoch  2, batch    54 | loss: 51.5336181CurrentTrain: epoch  2, batch    55 | loss: 84.1469991CurrentTrain: epoch  2, batch    56 | loss: 115.2829447CurrentTrain: epoch  2, batch    57 | loss: 55.2780090CurrentTrain: epoch  2, batch    58 | loss: 56.8374525CurrentTrain: epoch  2, batch    59 | loss: 44.7668314CurrentTrain: epoch  2, batch    60 | loss: 87.3974800CurrentTrain: epoch  2, batch    61 | loss: 51.8276380CurrentTrain: epoch  2, batch    62 | loss: 67.4291600CurrentTrain: epoch  2, batch    63 | loss: 72.8540499CurrentTrain: epoch  2, batch    64 | loss: 88.6547706CurrentTrain: epoch  2, batch    65 | loss: 54.4992356CurrentTrain: epoch  2, batch    66 | loss: 68.3502699CurrentTrain: epoch  2, batch    67 | loss: 71.1851085CurrentTrain: epoch  2, batch    68 | loss: 91.3800210CurrentTrain: epoch  2, batch    69 | loss: 55.3813658CurrentTrain: epoch  2, batch    70 | loss: 64.6003469CurrentTrain: epoch  2, batch    71 | loss: 68.6635672CurrentTrain: epoch  2, batch    72 | loss: 91.5802927CurrentTrain: epoch  2, batch    73 | loss: 87.4737456CurrentTrain: epoch  2, batch    74 | loss: 89.5209560CurrentTrain: epoch  2, batch    75 | loss: 56.4364445CurrentTrain: epoch  2, batch    76 | loss: 87.0734995CurrentTrain: epoch  2, batch    77 | loss: 67.2535952CurrentTrain: epoch  2, batch    78 | loss: 55.0859271CurrentTrain: epoch  2, batch    79 | loss: 89.9137728CurrentTrain: epoch  2, batch    80 | loss: 68.6506448CurrentTrain: epoch  2, batch    81 | loss: 54.5356124CurrentTrain: epoch  2, batch    82 | loss: 65.5200031CurrentTrain: epoch  2, batch    83 | loss: 119.0299667CurrentTrain: epoch  2, batch    84 | loss: 116.8799875CurrentTrain: epoch  2, batch    85 | loss: 70.1556217CurrentTrain: epoch  2, batch    86 | loss: 68.2814784CurrentTrain: epoch  2, batch    87 | loss: 69.2799497CurrentTrain: epoch  2, batch    88 | loss: 81.0057229CurrentTrain: epoch  2, batch    89 | loss: 70.4276710CurrentTrain: epoch  2, batch    90 | loss: 55.5684749CurrentTrain: epoch  2, batch    91 | loss: 67.0523450CurrentTrain: epoch  2, batch    92 | loss: 59.2906484CurrentTrain: epoch  2, batch    93 | loss: 55.9288969CurrentTrain: epoch  2, batch    94 | loss: 123.2441308CurrentTrain: epoch  2, batch    95 | loss: 59.9771036CurrentTrain: epoch  3, batch     0 | loss: 47.8393967CurrentTrain: epoch  3, batch     1 | loss: 87.4250404CurrentTrain: epoch  3, batch     2 | loss: 65.0319606CurrentTrain: epoch  3, batch     3 | loss: 45.4001129CurrentTrain: epoch  3, batch     4 | loss: 68.3839891CurrentTrain: epoch  3, batch     5 | loss: 87.7377502CurrentTrain: epoch  3, batch     6 | loss: 54.4531795CurrentTrain: epoch  3, batch     7 | loss: 68.2701754CurrentTrain: epoch  3, batch     8 | loss: 119.9121683CurrentTrain: epoch  3, batch     9 | loss: 55.4546093CurrentTrain: epoch  3, batch    10 | loss: 67.3282695CurrentTrain: epoch  3, batch    11 | loss: 69.4274777CurrentTrain: epoch  3, batch    12 | loss: 85.3999247CurrentTrain: epoch  3, batch    13 | loss: 46.7746786CurrentTrain: epoch  3, batch    14 | loss: 61.9991438CurrentTrain: epoch  3, batch    15 | loss: 85.6292676CurrentTrain: epoch  3, batch    16 | loss: 89.8685441CurrentTrain: epoch  3, batch    17 | loss: 46.5104665CurrentTrain: epoch  3, batch    18 | loss: 46.6877689CurrentTrain: epoch  3, batch    19 | loss: 82.3150996CurrentTrain: epoch  3, batch    20 | loss: 55.3642304CurrentTrain: epoch  3, batch    21 | loss: 118.5120459CurrentTrain: epoch  3, batch    22 | loss: 82.9013811CurrentTrain: epoch  3, batch    23 | loss: 86.7307705CurrentTrain: epoch  3, batch    24 | loss: 67.4630572CurrentTrain: epoch  3, batch    25 | loss: 67.6587956CurrentTrain: epoch  3, batch    26 | loss: 49.3142083CurrentTrain: epoch  3, batch    27 | loss: 182.7972827CurrentTrain: epoch  3, batch    28 | loss: 55.1998652CurrentTrain: epoch  3, batch    29 | loss: 68.7188731CurrentTrain: epoch  3, batch    30 | loss: 55.2963530CurrentTrain: epoch  3, batch    31 | loss: 45.3849716CurrentTrain: epoch  3, batch    32 | loss: 66.0959641CurrentTrain: epoch  3, batch    33 | loss: 116.0438272CurrentTrain: epoch  3, batch    34 | loss: 56.2001509CurrentTrain: epoch  3, batch    35 | loss: 53.4311394CurrentTrain: epoch  3, batch    36 | loss: 83.1325508CurrentTrain: epoch  3, batch    37 | loss: 65.8722950CurrentTrain: epoch  3, batch    38 | loss: 84.0535266CurrentTrain: epoch  3, batch    39 | loss: 54.1511329CurrentTrain: epoch  3, batch    40 | loss: 88.2072410CurrentTrain: epoch  3, batch    41 | loss: 55.4495422CurrentTrain: epoch  3, batch    42 | loss: 84.8250209CurrentTrain: epoch  3, batch    43 | loss: 54.1545689CurrentTrain: epoch  3, batch    44 | loss: 46.5440898CurrentTrain: epoch  3, batch    45 | loss: 69.1036344CurrentTrain: epoch  3, batch    46 | loss: 70.5965039CurrentTrain: epoch  3, batch    47 | loss: 46.0000716CurrentTrain: epoch  3, batch    48 | loss: 56.4923408CurrentTrain: epoch  3, batch    49 | loss: 85.3113943CurrentTrain: epoch  3, batch    50 | loss: 88.8271635CurrentTrain: epoch  3, batch    51 | loss: 66.4357751CurrentTrain: epoch  3, batch    52 | loss: 113.6041077CurrentTrain: epoch  3, batch    53 | loss: 55.3707734CurrentTrain: epoch  3, batch    54 | loss: 54.1126284CurrentTrain: epoch  3, batch    55 | loss: 66.4721409CurrentTrain: epoch  3, batch    56 | loss: 70.8857579CurrentTrain: epoch  3, batch    57 | loss: 68.3758675CurrentTrain: epoch  3, batch    58 | loss: 56.2161690CurrentTrain: epoch  3, batch    59 | loss: 115.5460583CurrentTrain: epoch  3, batch    60 | loss: 68.5103305CurrentTrain: epoch  3, batch    61 | loss: 85.3999700CurrentTrain: epoch  3, batch    62 | loss: 68.7173866CurrentTrain: epoch  3, batch    63 | loss: 84.2720493CurrentTrain: epoch  3, batch    64 | loss: 51.1327291CurrentTrain: epoch  3, batch    65 | loss: 46.9416812CurrentTrain: epoch  3, batch    66 | loss: 71.4067985CurrentTrain: epoch  3, batch    67 | loss: 64.6180970CurrentTrain: epoch  3, batch    68 | loss: 45.9560559CurrentTrain: epoch  3, batch    69 | loss: 91.6318394CurrentTrain: epoch  3, batch    70 | loss: 54.3555097CurrentTrain: epoch  3, batch    71 | loss: 65.8014718CurrentTrain: epoch  3, batch    72 | loss: 87.6062377CurrentTrain: epoch  3, batch    73 | loss: 50.1911195CurrentTrain: epoch  3, batch    74 | loss: 83.7068207CurrentTrain: epoch  3, batch    75 | loss: 52.8683258CurrentTrain: epoch  3, batch    76 | loss: 67.8639356CurrentTrain: epoch  3, batch    77 | loss: 54.1051292CurrentTrain: epoch  3, batch    78 | loss: 118.7350478CurrentTrain: epoch  3, batch    79 | loss: 66.8719197CurrentTrain: epoch  3, batch    80 | loss: 56.6589326CurrentTrain: epoch  3, batch    81 | loss: 65.4548909CurrentTrain: epoch  3, batch    82 | loss: 66.9162745CurrentTrain: epoch  3, batch    83 | loss: 118.5395872CurrentTrain: epoch  3, batch    84 | loss: 69.7849565CurrentTrain: epoch  3, batch    85 | loss: 82.5668050CurrentTrain: epoch  3, batch    86 | loss: 64.6607039CurrentTrain: epoch  3, batch    87 | loss: 65.3979418CurrentTrain: epoch  3, batch    88 | loss: 86.2183879CurrentTrain: epoch  3, batch    89 | loss: 66.6106417CurrentTrain: epoch  3, batch    90 | loss: 54.0971960CurrentTrain: epoch  3, batch    91 | loss: 87.0945022CurrentTrain: epoch  3, batch    92 | loss: 58.0779976CurrentTrain: epoch  3, batch    93 | loss: 57.8207344CurrentTrain: epoch  3, batch    94 | loss: 45.8003837CurrentTrain: epoch  3, batch    95 | loss: 56.1025938CurrentTrain: epoch  4, batch     0 | loss: 86.7457086CurrentTrain: epoch  4, batch     1 | loss: 67.1947905CurrentTrain: epoch  4, batch     2 | loss: 52.6133529CurrentTrain: epoch  4, batch     3 | loss: 55.0202719CurrentTrain: epoch  4, batch     4 | loss: 53.6437706CurrentTrain: epoch  4, batch     5 | loss: 116.0041446CurrentTrain: epoch  4, batch     6 | loss: 63.2510259CurrentTrain: epoch  4, batch     7 | loss: 86.7442807CurrentTrain: epoch  4, batch     8 | loss: 121.2534650CurrentTrain: epoch  4, batch     9 | loss: 54.1155037CurrentTrain: epoch  4, batch    10 | loss: 81.7772169CurrentTrain: epoch  4, batch    11 | loss: 80.5543654CurrentTrain: epoch  4, batch    12 | loss: 115.5008832CurrentTrain: epoch  4, batch    13 | loss: 182.8204583CurrentTrain: epoch  4, batch    14 | loss: 53.8401230CurrentTrain: epoch  4, batch    15 | loss: 54.6686023CurrentTrain: epoch  4, batch    16 | loss: 53.8707460CurrentTrain: epoch  4, batch    17 | loss: 47.0356808CurrentTrain: epoch  4, batch    18 | loss: 88.7035809CurrentTrain: epoch  4, batch    19 | loss: 86.5885737CurrentTrain: epoch  4, batch    20 | loss: 69.3404045CurrentTrain: epoch  4, batch    21 | loss: 67.3262336CurrentTrain: epoch  4, batch    22 | loss: 66.0442202CurrentTrain: epoch  4, batch    23 | loss: 54.8191188CurrentTrain: epoch  4, batch    24 | loss: 61.9164309CurrentTrain: epoch  4, batch    25 | loss: 67.6614184CurrentTrain: epoch  4, batch    26 | loss: 88.0632494CurrentTrain: epoch  4, batch    27 | loss: 66.4918008CurrentTrain: epoch  4, batch    28 | loss: 55.0763790CurrentTrain: epoch  4, batch    29 | loss: 54.0547018CurrentTrain: epoch  4, batch    30 | loss: 64.9269948CurrentTrain: epoch  4, batch    31 | loss: 66.7307501CurrentTrain: epoch  4, batch    32 | loss: 64.8819066CurrentTrain: epoch  4, batch    33 | loss: 183.4358358CurrentTrain: epoch  4, batch    34 | loss: 67.9013693CurrentTrain: epoch  4, batch    35 | loss: 66.2757551CurrentTrain: epoch  4, batch    36 | loss: 66.9178223CurrentTrain: epoch  4, batch    37 | loss: 84.4322084CurrentTrain: epoch  4, batch    38 | loss: 81.8610612CurrentTrain: epoch  4, batch    39 | loss: 88.0881628CurrentTrain: epoch  4, batch    40 | loss: 85.7236528CurrentTrain: epoch  4, batch    41 | loss: 57.2134602CurrentTrain: epoch  4, batch    42 | loss: 63.3235477CurrentTrain: epoch  4, batch    43 | loss: 69.3741639CurrentTrain: epoch  4, batch    44 | loss: 67.7059453CurrentTrain: epoch  4, batch    45 | loss: 68.2192065CurrentTrain: epoch  4, batch    46 | loss: 63.9556390CurrentTrain: epoch  4, batch    47 | loss: 45.1052430CurrentTrain: epoch  4, batch    48 | loss: 52.9498094CurrentTrain: epoch  4, batch    49 | loss: 56.1642257CurrentTrain: epoch  4, batch    50 | loss: 53.8857883CurrentTrain: epoch  4, batch    51 | loss: 66.7486548CurrentTrain: epoch  4, batch    52 | loss: 54.9332739CurrentTrain: epoch  4, batch    53 | loss: 49.1595053CurrentTrain: epoch  4, batch    54 | loss: 86.2701500CurrentTrain: epoch  4, batch    55 | loss: 54.1651459CurrentTrain: epoch  4, batch    56 | loss: 64.1269467CurrentTrain: epoch  4, batch    57 | loss: 79.9189980CurrentTrain: epoch  4, batch    58 | loss: 65.9515782CurrentTrain: epoch  4, batch    59 | loss: 82.3713973CurrentTrain: epoch  4, batch    60 | loss: 65.8162012CurrentTrain: epoch  4, batch    61 | loss: 81.9820593CurrentTrain: epoch  4, batch    62 | loss: 85.6828011CurrentTrain: epoch  4, batch    63 | loss: 121.2464512CurrentTrain: epoch  4, batch    64 | loss: 62.6398901CurrentTrain: epoch  4, batch    65 | loss: 53.5265372CurrentTrain: epoch  4, batch    66 | loss: 53.9794914CurrentTrain: epoch  4, batch    67 | loss: 68.8296997CurrentTrain: epoch  4, batch    68 | loss: 87.2822887CurrentTrain: epoch  4, batch    69 | loss: 83.8956065CurrentTrain: epoch  4, batch    70 | loss: 84.7967260CurrentTrain: epoch  4, batch    71 | loss: 54.4868729CurrentTrain: epoch  4, batch    72 | loss: 54.1088485CurrentTrain: epoch  4, batch    73 | loss: 53.9819306CurrentTrain: epoch  4, batch    74 | loss: 62.2795684CurrentTrain: epoch  4, batch    75 | loss: 68.9631636CurrentTrain: epoch  4, batch    76 | loss: 65.8801219CurrentTrain: epoch  4, batch    77 | loss: 118.8600196CurrentTrain: epoch  4, batch    78 | loss: 66.5320611CurrentTrain: epoch  4, batch    79 | loss: 67.4318157CurrentTrain: epoch  4, batch    80 | loss: 54.4319759CurrentTrain: epoch  4, batch    81 | loss: 65.0713381CurrentTrain: epoch  4, batch    82 | loss: 88.0610950CurrentTrain: epoch  4, batch    83 | loss: 83.7103588CurrentTrain: epoch  4, batch    84 | loss: 67.4906720CurrentTrain: epoch  4, batch    85 | loss: 53.0832386CurrentTrain: epoch  4, batch    86 | loss: 83.6086383CurrentTrain: epoch  4, batch    87 | loss: 67.0314832CurrentTrain: epoch  4, batch    88 | loss: 82.1612643CurrentTrain: epoch  4, batch    89 | loss: 51.4194004CurrentTrain: epoch  4, batch    90 | loss: 54.7701413CurrentTrain: epoch  4, batch    91 | loss: 65.3302310CurrentTrain: epoch  4, batch    92 | loss: 86.5382528CurrentTrain: epoch  4, batch    93 | loss: 84.4294393CurrentTrain: epoch  4, batch    94 | loss: 54.5306020CurrentTrain: epoch  4, batch    95 | loss: 57.0208275CurrentTrain: epoch  5, batch     0 | loss: 55.2501590CurrentTrain: epoch  5, batch     1 | loss: 53.1984723CurrentTrain: epoch  5, batch     2 | loss: 81.1976033CurrentTrain: epoch  5, batch     3 | loss: 65.3955016CurrentTrain: epoch  5, batch     4 | loss: 51.0725674CurrentTrain: epoch  5, batch     5 | loss: 68.0947717CurrentTrain: epoch  5, batch     6 | loss: 84.3709670CurrentTrain: epoch  5, batch     7 | loss: 68.5630596CurrentTrain: epoch  5, batch     8 | loss: 54.3926811CurrentTrain: epoch  5, batch     9 | loss: 63.1589762CurrentTrain: epoch  5, batch    10 | loss: 50.9333185CurrentTrain: epoch  5, batch    11 | loss: 83.5601338CurrentTrain: epoch  5, batch    12 | loss: 55.1662100CurrentTrain: epoch  5, batch    13 | loss: 65.0360896CurrentTrain: epoch  5, batch    14 | loss: 51.6127071CurrentTrain: epoch  5, batch    15 | loss: 86.2290832CurrentTrain: epoch  5, batch    16 | loss: 67.0405854CurrentTrain: epoch  5, batch    17 | loss: 45.8108119CurrentTrain: epoch  5, batch    18 | loss: 83.9699256CurrentTrain: epoch  5, batch    19 | loss: 53.9040825CurrentTrain: epoch  5, batch    20 | loss: 47.0259142CurrentTrain: epoch  5, batch    21 | loss: 80.5442502CurrentTrain: epoch  5, batch    22 | loss: 115.8660976CurrentTrain: epoch  5, batch    23 | loss: 87.3383834CurrentTrain: epoch  5, batch    24 | loss: 85.4440104CurrentTrain: epoch  5, batch    25 | loss: 66.9445350CurrentTrain: epoch  5, batch    26 | loss: 46.8612701CurrentTrain: epoch  5, batch    27 | loss: 113.8464301CurrentTrain: epoch  5, batch    28 | loss: 84.9548143CurrentTrain: epoch  5, batch    29 | loss: 52.5942324CurrentTrain: epoch  5, batch    30 | loss: 65.6398761CurrentTrain: epoch  5, batch    31 | loss: 67.3348713CurrentTrain: epoch  5, batch    32 | loss: 54.3446913CurrentTrain: epoch  5, batch    33 | loss: 65.7612064CurrentTrain: epoch  5, batch    34 | loss: 85.5076994CurrentTrain: epoch  5, batch    35 | loss: 50.2173979CurrentTrain: epoch  5, batch    36 | loss: 116.9173549CurrentTrain: epoch  5, batch    37 | loss: 116.2769747CurrentTrain: epoch  5, batch    38 | loss: 117.7508725CurrentTrain: epoch  5, batch    39 | loss: 64.7834811CurrentTrain: epoch  5, batch    40 | loss: 52.9745460CurrentTrain: epoch  5, batch    41 | loss: 84.8693202CurrentTrain: epoch  5, batch    42 | loss: 52.1237431CurrentTrain: epoch  5, batch    43 | loss: 66.7020564CurrentTrain: epoch  5, batch    44 | loss: 64.7226731CurrentTrain: epoch  5, batch    45 | loss: 68.3606976CurrentTrain: epoch  5, batch    46 | loss: 44.1657453CurrentTrain: epoch  5, batch    47 | loss: 86.6458116CurrentTrain: epoch  5, batch    48 | loss: 64.1851153CurrentTrain: epoch  5, batch    49 | loss: 54.7001563CurrentTrain: epoch  5, batch    50 | loss: 83.0516179CurrentTrain: epoch  5, batch    51 | loss: 177.7399154CurrentTrain: epoch  5, batch    52 | loss: 62.7233993CurrentTrain: epoch  5, batch    53 | loss: 65.2278526CurrentTrain: epoch  5, batch    54 | loss: 64.2711872CurrentTrain: epoch  5, batch    55 | loss: 84.0479906CurrentTrain: epoch  5, batch    56 | loss: 52.2201063CurrentTrain: epoch  5, batch    57 | loss: 51.4399730CurrentTrain: epoch  5, batch    58 | loss: 182.2232083CurrentTrain: epoch  5, batch    59 | loss: 45.3080138CurrentTrain: epoch  5, batch    60 | loss: 63.7213573CurrentTrain: epoch  5, batch    61 | loss: 61.2506903CurrentTrain: epoch  5, batch    62 | loss: 53.8154421CurrentTrain: epoch  5, batch    63 | loss: 51.0318450CurrentTrain: epoch  5, batch    64 | loss: 44.2111698CurrentTrain: epoch  5, batch    65 | loss: 115.6177203CurrentTrain: epoch  5, batch    66 | loss: 67.0809368CurrentTrain: epoch  5, batch    67 | loss: 47.7609284CurrentTrain: epoch  5, batch    68 | loss: 62.0500876CurrentTrain: epoch  5, batch    69 | loss: 372.5402950CurrentTrain: epoch  5, batch    70 | loss: 86.1963068CurrentTrain: epoch  5, batch    71 | loss: 83.6691247CurrentTrain: epoch  5, batch    72 | loss: 44.1016226CurrentTrain: epoch  5, batch    73 | loss: 68.1576933CurrentTrain: epoch  5, batch    74 | loss: 82.8550114CurrentTrain: epoch  5, batch    75 | loss: 64.3300005CurrentTrain: epoch  5, batch    76 | loss: 52.6226657CurrentTrain: epoch  5, batch    77 | loss: 45.2568808CurrentTrain: epoch  5, batch    78 | loss: 55.1621120CurrentTrain: epoch  5, batch    79 | loss: 117.6519597CurrentTrain: epoch  5, batch    80 | loss: 51.7319363CurrentTrain: epoch  5, batch    81 | loss: 64.7035469CurrentTrain: epoch  5, batch    82 | loss: 44.5054572CurrentTrain: epoch  5, batch    83 | loss: 84.7162436CurrentTrain: epoch  5, batch    84 | loss: 53.2662112CurrentTrain: epoch  5, batch    85 | loss: 52.1477763CurrentTrain: epoch  5, batch    86 | loss: 88.7886463CurrentTrain: epoch  5, batch    87 | loss: 111.2772266CurrentTrain: epoch  5, batch    88 | loss: 55.5460345CurrentTrain: epoch  5, batch    89 | loss: 81.6060466CurrentTrain: epoch  5, batch    90 | loss: 50.6146652CurrentTrain: epoch  5, batch    91 | loss: 83.9436280CurrentTrain: epoch  5, batch    92 | loss: 64.2241808CurrentTrain: epoch  5, batch    93 | loss: 52.4695662CurrentTrain: epoch  5, batch    94 | loss: 65.6138516CurrentTrain: epoch  5, batch    95 | loss: 54.3793054CurrentTrain: epoch  6, batch     0 | loss: 83.1490735CurrentTrain: epoch  6, batch     1 | loss: 63.0388714CurrentTrain: epoch  6, batch     2 | loss: 84.4368987CurrentTrain: epoch  6, batch     3 | loss: 54.4615000CurrentTrain: epoch  6, batch     4 | loss: 81.2144447CurrentTrain: epoch  6, batch     5 | loss: 50.0050346CurrentTrain: epoch  6, batch     6 | loss: 84.2046522CurrentTrain: epoch  6, batch     7 | loss: 68.3286173CurrentTrain: epoch  6, batch     8 | loss: 52.0860452CurrentTrain: epoch  6, batch     9 | loss: 82.5642595CurrentTrain: epoch  6, batch    10 | loss: 62.3820663CurrentTrain: epoch  6, batch    11 | loss: 64.4999982CurrentTrain: epoch  6, batch    12 | loss: 118.5758067CurrentTrain: epoch  6, batch    13 | loss: 120.1009167CurrentTrain: epoch  6, batch    14 | loss: 64.3669465CurrentTrain: epoch  6, batch    15 | loss: 54.9737456CurrentTrain: epoch  6, batch    16 | loss: 86.4696615CurrentTrain: epoch  6, batch    17 | loss: 55.7882225CurrentTrain: epoch  6, batch    18 | loss: 81.8652130CurrentTrain: epoch  6, batch    19 | loss: 61.4060050CurrentTrain: epoch  6, batch    20 | loss: 84.2920429CurrentTrain: epoch  6, batch    21 | loss: 43.2280856CurrentTrain: epoch  6, batch    22 | loss: 63.0635146CurrentTrain: epoch  6, batch    23 | loss: 83.3128265CurrentTrain: epoch  6, batch    24 | loss: 53.3804997CurrentTrain: epoch  6, batch    25 | loss: 64.3596076CurrentTrain: epoch  6, batch    26 | loss: 60.8448514CurrentTrain: epoch  6, batch    27 | loss: 53.3220562CurrentTrain: epoch  6, batch    28 | loss: 64.7568730CurrentTrain: epoch  6, batch    29 | loss: 178.2034743CurrentTrain: epoch  6, batch    30 | loss: 67.4572056CurrentTrain: epoch  6, batch    31 | loss: 49.7924197CurrentTrain: epoch  6, batch    32 | loss: 51.5613287CurrentTrain: epoch  6, batch    33 | loss: 48.8427989CurrentTrain: epoch  6, batch    34 | loss: 85.8089094CurrentTrain: epoch  6, batch    35 | loss: 84.4525981CurrentTrain: epoch  6, batch    36 | loss: 82.5488644CurrentTrain: epoch  6, batch    37 | loss: 53.2827794CurrentTrain: epoch  6, batch    38 | loss: 64.1093294CurrentTrain: epoch  6, batch    39 | loss: 118.2283629CurrentTrain: epoch  6, batch    40 | loss: 45.4613739CurrentTrain: epoch  6, batch    41 | loss: 64.4240843CurrentTrain: epoch  6, batch    42 | loss: 65.5699956CurrentTrain: epoch  6, batch    43 | loss: 53.3640643CurrentTrain: epoch  6, batch    44 | loss: 42.8441037CurrentTrain: epoch  6, batch    45 | loss: 71.7888753CurrentTrain: epoch  6, batch    46 | loss: 80.3767845CurrentTrain: epoch  6, batch    47 | loss: 112.1291568CurrentTrain: epoch  6, batch    48 | loss: 43.9670725CurrentTrain: epoch  6, batch    49 | loss: 81.3966196CurrentTrain: epoch  6, batch    50 | loss: 181.6275385CurrentTrain: epoch  6, batch    51 | loss: 54.2734932CurrentTrain: epoch  6, batch    52 | loss: 70.9664041CurrentTrain: epoch  6, batch    53 | loss: 49.5133508CurrentTrain: epoch  6, batch    54 | loss: 53.4984445CurrentTrain: epoch  6, batch    55 | loss: 52.0563944CurrentTrain: epoch  6, batch    56 | loss: 42.2439118CurrentTrain: epoch  6, batch    57 | loss: 85.9912863CurrentTrain: epoch  6, batch    58 | loss: 53.1580996CurrentTrain: epoch  6, batch    59 | loss: 68.6291311CurrentTrain: epoch  6, batch    60 | loss: 181.6615334CurrentTrain: epoch  6, batch    61 | loss: 81.9452270CurrentTrain: epoch  6, batch    62 | loss: 87.2661695CurrentTrain: epoch  6, batch    63 | loss: 48.7737737CurrentTrain: epoch  6, batch    64 | loss: 68.5940712CurrentTrain: epoch  6, batch    65 | loss: 66.0718009CurrentTrain: epoch  6, batch    66 | loss: 67.9830274CurrentTrain: epoch  6, batch    67 | loss: 63.0900290CurrentTrain: epoch  6, batch    68 | loss: 44.6548504CurrentTrain: epoch  6, batch    69 | loss: 82.9098116CurrentTrain: epoch  6, batch    70 | loss: 43.7444629CurrentTrain: epoch  6, batch    71 | loss: 83.8302280CurrentTrain: epoch  6, batch    72 | loss: 50.8814484CurrentTrain: epoch  6, batch    73 | loss: 82.7529662CurrentTrain: epoch  6, batch    74 | loss: 116.3403128CurrentTrain: epoch  6, batch    75 | loss: 88.1017531CurrentTrain: epoch  6, batch    76 | loss: 52.1803282CurrentTrain: epoch  6, batch    77 | loss: 42.7450340CurrentTrain: epoch  6, batch    78 | loss: 66.2715796CurrentTrain: epoch  6, batch    79 | loss: 53.2710954CurrentTrain: epoch  6, batch    80 | loss: 181.8439303CurrentTrain: epoch  6, batch    81 | loss: 81.6596092CurrentTrain: epoch  6, batch    82 | loss: 85.9138892CurrentTrain: epoch  6, batch    83 | loss: 55.2209684CurrentTrain: epoch  6, batch    84 | loss: 65.0950318CurrentTrain: epoch  6, batch    85 | loss: 72.3709450CurrentTrain: epoch  6, batch    86 | loss: 118.7056861CurrentTrain: epoch  6, batch    87 | loss: 66.0128385CurrentTrain: epoch  6, batch    88 | loss: 65.5205182CurrentTrain: epoch  6, batch    89 | loss: 44.6215826CurrentTrain: epoch  6, batch    90 | loss: 114.8706328CurrentTrain: epoch  6, batch    91 | loss: 64.9850523CurrentTrain: epoch  6, batch    92 | loss: 65.0989590CurrentTrain: epoch  6, batch    93 | loss: 82.7879787CurrentTrain: epoch  6, batch    94 | loss: 64.0006844CurrentTrain: epoch  6, batch    95 | loss: 54.3047835CurrentTrain: epoch  7, batch     0 | loss: 51.8340110CurrentTrain: epoch  7, batch     1 | loss: 52.0449660CurrentTrain: epoch  7, batch     2 | loss: 65.4373230CurrentTrain: epoch  7, batch     3 | loss: 52.0926812CurrentTrain: epoch  7, batch     4 | loss: 82.5312264CurrentTrain: epoch  7, batch     5 | loss: 49.1667002CurrentTrain: epoch  7, batch     6 | loss: 66.5346977CurrentTrain: epoch  7, batch     7 | loss: 50.5351047CurrentTrain: epoch  7, batch     8 | loss: 50.2638209CurrentTrain: epoch  7, batch     9 | loss: 117.7535400CurrentTrain: epoch  7, batch    10 | loss: 44.4884690CurrentTrain: epoch  7, batch    11 | loss: 64.9607443CurrentTrain: epoch  7, batch    12 | loss: 109.3921687CurrentTrain: epoch  7, batch    13 | loss: 118.8822247CurrentTrain: epoch  7, batch    14 | loss: 53.0840680CurrentTrain: epoch  7, batch    15 | loss: 51.0974928CurrentTrain: epoch  7, batch    16 | loss: 62.0258104CurrentTrain: epoch  7, batch    17 | loss: 63.0536990CurrentTrain: epoch  7, batch    18 | loss: 82.9254558CurrentTrain: epoch  7, batch    19 | loss: 112.9727751CurrentTrain: epoch  7, batch    20 | loss: 63.8436778CurrentTrain: epoch  7, batch    21 | loss: 85.7920740CurrentTrain: epoch  7, batch    22 | loss: 53.6061150CurrentTrain: epoch  7, batch    23 | loss: 52.8667333CurrentTrain: epoch  7, batch    24 | loss: 64.3350426CurrentTrain: epoch  7, batch    25 | loss: 50.5948851CurrentTrain: epoch  7, batch    26 | loss: 177.6894110CurrentTrain: epoch  7, batch    27 | loss: 42.2763787CurrentTrain: epoch  7, batch    28 | loss: 82.7374756CurrentTrain: epoch  7, batch    29 | loss: 83.3193593CurrentTrain: epoch  7, batch    30 | loss: 82.7162104CurrentTrain: epoch  7, batch    31 | loss: 65.5027684CurrentTrain: epoch  7, batch    32 | loss: 52.6690580CurrentTrain: epoch  7, batch    33 | loss: 59.9868107CurrentTrain: epoch  7, batch    34 | loss: 64.6546353CurrentTrain: epoch  7, batch    35 | loss: 114.6482211CurrentTrain: epoch  7, batch    36 | loss: 51.3541932CurrentTrain: epoch  7, batch    37 | loss: 54.5343255CurrentTrain: epoch  7, batch    38 | loss: 64.6170349CurrentTrain: epoch  7, batch    39 | loss: 82.8873964CurrentTrain: epoch  7, batch    40 | loss: 52.2720748CurrentTrain: epoch  7, batch    41 | loss: 117.8213477CurrentTrain: epoch  7, batch    42 | loss: 65.5361114CurrentTrain: epoch  7, batch    43 | loss: 83.1807202CurrentTrain: epoch  7, batch    44 | loss: 52.2208974CurrentTrain: epoch  7, batch    45 | loss: 68.4894642CurrentTrain: epoch  7, batch    46 | loss: 80.3450807CurrentTrain: epoch  7, batch    47 | loss: 117.9351417CurrentTrain: epoch  7, batch    48 | loss: 40.6595756CurrentTrain: epoch  7, batch    49 | loss: 64.2353303CurrentTrain: epoch  7, batch    50 | loss: 86.5574498CurrentTrain: epoch  7, batch    51 | loss: 67.6818849CurrentTrain: epoch  7, batch    52 | loss: 65.9572344CurrentTrain: epoch  7, batch    53 | loss: 53.5952611CurrentTrain: epoch  7, batch    54 | loss: 113.4863473CurrentTrain: epoch  7, batch    55 | loss: 53.5078033CurrentTrain: epoch  7, batch    56 | loss: 67.0844798CurrentTrain: epoch  7, batch    57 | loss: 60.8358936CurrentTrain: epoch  7, batch    58 | loss: 53.5697891CurrentTrain: epoch  7, batch    59 | loss: 61.4335624CurrentTrain: epoch  7, batch    60 | loss: 82.9249469CurrentTrain: epoch  7, batch    61 | loss: 85.9438378CurrentTrain: epoch  7, batch    62 | loss: 66.2208143CurrentTrain: epoch  7, batch    63 | loss: 63.5663904CurrentTrain: epoch  7, batch    64 | loss: 117.6306992CurrentTrain: epoch  7, batch    65 | loss: 61.5769371CurrentTrain: epoch  7, batch    66 | loss: 85.7783012CurrentTrain: epoch  7, batch    67 | loss: 181.7609677CurrentTrain: epoch  7, batch    68 | loss: 49.7175956CurrentTrain: epoch  7, batch    69 | loss: 51.0729944CurrentTrain: epoch  7, batch    70 | loss: 63.3706050CurrentTrain: epoch  7, batch    71 | loss: 84.9244348CurrentTrain: epoch  7, batch    72 | loss: 44.2629555CurrentTrain: epoch  7, batch    73 | loss: 51.3616198CurrentTrain: epoch  7, batch    74 | loss: 80.0035814CurrentTrain: epoch  7, batch    75 | loss: 86.7074336CurrentTrain: epoch  7, batch    76 | loss: 43.4611336CurrentTrain: epoch  7, batch    77 | loss: 62.4226782CurrentTrain: epoch  7, batch    78 | loss: 63.0070322CurrentTrain: epoch  7, batch    79 | loss: 66.7270089CurrentTrain: epoch  7, batch    80 | loss: 62.7826228CurrentTrain: epoch  7, batch    81 | loss: 64.4147604CurrentTrain: epoch  7, batch    82 | loss: 68.1018471CurrentTrain: epoch  7, batch    83 | loss: 81.9308892CurrentTrain: epoch  7, batch    84 | loss: 82.6454445CurrentTrain: epoch  7, batch    85 | loss: 51.2736821CurrentTrain: epoch  7, batch    86 | loss: 83.5631544CurrentTrain: epoch  7, batch    87 | loss: 54.9780598CurrentTrain: epoch  7, batch    88 | loss: 64.2984943CurrentTrain: epoch  7, batch    89 | loss: 70.0523453CurrentTrain: epoch  7, batch    90 | loss: 79.1690788CurrentTrain: epoch  7, batch    91 | loss: 52.2039301CurrentTrain: epoch  7, batch    92 | loss: 65.7968113CurrentTrain: epoch  7, batch    93 | loss: 54.0880479CurrentTrain: epoch  7, batch    94 | loss: 90.4785658CurrentTrain: epoch  7, batch    95 | loss: 43.7113364CurrentTrain: epoch  8, batch     0 | loss: 50.8168691CurrentTrain: epoch  8, batch     1 | loss: 82.2446704CurrentTrain: epoch  8, batch     2 | loss: 43.0356683CurrentTrain: epoch  8, batch     3 | loss: 52.1680344CurrentTrain: epoch  8, batch     4 | loss: 62.5804460CurrentTrain: epoch  8, batch     5 | loss: 41.9040410CurrentTrain: epoch  8, batch     6 | loss: 60.8022068CurrentTrain: epoch  8, batch     7 | loss: 66.7398603CurrentTrain: epoch  8, batch     8 | loss: 177.5719206CurrentTrain: epoch  8, batch     9 | loss: 84.3841207CurrentTrain: epoch  8, batch    10 | loss: 62.8454689CurrentTrain: epoch  8, batch    11 | loss: 63.4760113CurrentTrain: epoch  8, batch    12 | loss: 115.2342959CurrentTrain: epoch  8, batch    13 | loss: 82.3596004CurrentTrain: epoch  8, batch    14 | loss: 51.5674183CurrentTrain: epoch  8, batch    15 | loss: 52.0102138CurrentTrain: epoch  8, batch    16 | loss: 64.9416640CurrentTrain: epoch  8, batch    17 | loss: 177.6074866CurrentTrain: epoch  8, batch    18 | loss: 51.2593554CurrentTrain: epoch  8, batch    19 | loss: 86.4853861CurrentTrain: epoch  8, batch    20 | loss: 45.3965610CurrentTrain: epoch  8, batch    21 | loss: 51.2049111CurrentTrain: epoch  8, batch    22 | loss: 117.5173542CurrentTrain: epoch  8, batch    23 | loss: 63.7284253CurrentTrain: epoch  8, batch    24 | loss: 83.0560444CurrentTrain: epoch  8, batch    25 | loss: 53.0864878CurrentTrain: epoch  8, batch    26 | loss: 52.4506646CurrentTrain: epoch  8, batch    27 | loss: 43.1789376CurrentTrain: epoch  8, batch    28 | loss: 51.2323502CurrentTrain: epoch  8, batch    29 | loss: 52.2382682CurrentTrain: epoch  8, batch    30 | loss: 82.8808315CurrentTrain: epoch  8, batch    31 | loss: 42.4756100CurrentTrain: epoch  8, batch    32 | loss: 53.4843930CurrentTrain: epoch  8, batch    33 | loss: 67.4178439CurrentTrain: epoch  8, batch    34 | loss: 62.9770955CurrentTrain: epoch  8, batch    35 | loss: 84.1666328CurrentTrain: epoch  8, batch    36 | loss: 78.9024017CurrentTrain: epoch  8, batch    37 | loss: 63.1773975CurrentTrain: epoch  8, batch    38 | loss: 53.6950090CurrentTrain: epoch  8, batch    39 | loss: 52.6737679CurrentTrain: epoch  8, batch    40 | loss: 64.1000218CurrentTrain: epoch  8, batch    41 | loss: 66.7737790CurrentTrain: epoch  8, batch    42 | loss: 50.8270493CurrentTrain: epoch  8, batch    43 | loss: 52.4285856CurrentTrain: epoch  8, batch    44 | loss: 63.0416349CurrentTrain: epoch  8, batch    45 | loss: 64.9691356CurrentTrain: epoch  8, batch    46 | loss: 64.4102180CurrentTrain: epoch  8, batch    47 | loss: 54.5681415CurrentTrain: epoch  8, batch    48 | loss: 82.9157650CurrentTrain: epoch  8, batch    49 | loss: 85.3067298CurrentTrain: epoch  8, batch    50 | loss: 64.3374833CurrentTrain: epoch  8, batch    51 | loss: 48.3869767CurrentTrain: epoch  8, batch    52 | loss: 117.4721684CurrentTrain: epoch  8, batch    53 | loss: 64.6809305CurrentTrain: epoch  8, batch    54 | loss: 64.3489937CurrentTrain: epoch  8, batch    55 | loss: 66.9833858CurrentTrain: epoch  8, batch    56 | loss: 80.2426750CurrentTrain: epoch  8, batch    57 | loss: 61.3833594CurrentTrain: epoch  8, batch    58 | loss: 55.7816277CurrentTrain: epoch  8, batch    59 | loss: 65.7530414CurrentTrain: epoch  8, batch    60 | loss: 84.1693089CurrentTrain: epoch  8, batch    61 | loss: 42.4232372CurrentTrain: epoch  8, batch    62 | loss: 62.4233380CurrentTrain: epoch  8, batch    63 | loss: 117.2905851CurrentTrain: epoch  8, batch    64 | loss: 82.4201839CurrentTrain: epoch  8, batch    65 | loss: 56.3800386CurrentTrain: epoch  8, batch    66 | loss: 84.2784102CurrentTrain: epoch  8, batch    67 | loss: 120.1966590CurrentTrain: epoch  8, batch    68 | loss: 63.4301343CurrentTrain: epoch  8, batch    69 | loss: 63.0558009CurrentTrain: epoch  8, batch    70 | loss: 87.1936390CurrentTrain: epoch  8, batch    71 | loss: 113.1744678CurrentTrain: epoch  8, batch    72 | loss: 76.8775349CurrentTrain: epoch  8, batch    73 | loss: 66.0956698CurrentTrain: epoch  8, batch    74 | loss: 53.5286896CurrentTrain: epoch  8, batch    75 | loss: 63.6229037CurrentTrain: epoch  8, batch    76 | loss: 66.0916175CurrentTrain: epoch  8, batch    77 | loss: 64.8837728CurrentTrain: epoch  8, batch    78 | loss: 54.5346223CurrentTrain: epoch  8, batch    79 | loss: 61.0498394CurrentTrain: epoch  8, batch    80 | loss: 51.1046069CurrentTrain: epoch  8, batch    81 | loss: 86.7091360CurrentTrain: epoch  8, batch    82 | loss: 48.4120676CurrentTrain: epoch  8, batch    83 | loss: 52.3226558CurrentTrain: epoch  8, batch    84 | loss: 51.1129375CurrentTrain: epoch  8, batch    85 | loss: 85.8044103CurrentTrain: epoch  8, batch    86 | loss: 53.6994394CurrentTrain: epoch  8, batch    87 | loss: 65.4377033CurrentTrain: epoch  8, batch    88 | loss: 64.5323296CurrentTrain: epoch  8, batch    89 | loss: 79.8029848CurrentTrain: epoch  8, batch    90 | loss: 70.7020469CurrentTrain: epoch  8, batch    91 | loss: 89.0937428CurrentTrain: epoch  8, batch    92 | loss: 62.9696326CurrentTrain: epoch  8, batch    93 | loss: 65.7703633CurrentTrain: epoch  8, batch    94 | loss: 84.1832998CurrentTrain: epoch  8, batch    95 | loss: 149.0079954CurrentTrain: epoch  9, batch     0 | loss: 64.5867367CurrentTrain: epoch  9, batch     1 | loss: 62.1388807CurrentTrain: epoch  9, batch     2 | loss: 60.8531308CurrentTrain: epoch  9, batch     3 | loss: 52.1150138CurrentTrain: epoch  9, batch     4 | loss: 76.6260548CurrentTrain: epoch  9, batch     5 | loss: 61.1213101CurrentTrain: epoch  9, batch     6 | loss: 52.1534193CurrentTrain: epoch  9, batch     7 | loss: 63.6990589CurrentTrain: epoch  9, batch     8 | loss: 51.3029919CurrentTrain: epoch  9, batch     9 | loss: 51.5614647CurrentTrain: epoch  9, batch    10 | loss: 115.7342697CurrentTrain: epoch  9, batch    11 | loss: 50.2878741CurrentTrain: epoch  9, batch    12 | loss: 81.0235067CurrentTrain: epoch  9, batch    13 | loss: 61.8077070CurrentTrain: epoch  9, batch    14 | loss: 81.0787477CurrentTrain: epoch  9, batch    15 | loss: 65.5603233CurrentTrain: epoch  9, batch    16 | loss: 53.2160002CurrentTrain: epoch  9, batch    17 | loss: 54.2044260CurrentTrain: epoch  9, batch    18 | loss: 66.8255286CurrentTrain: epoch  9, batch    19 | loss: 50.0489851CurrentTrain: epoch  9, batch    20 | loss: 42.5039370CurrentTrain: epoch  9, batch    21 | loss: 63.4956427CurrentTrain: epoch  9, batch    22 | loss: 82.5121301CurrentTrain: epoch  9, batch    23 | loss: 66.7246998CurrentTrain: epoch  9, batch    24 | loss: 63.1326548CurrentTrain: epoch  9, batch    25 | loss: 62.0146343CurrentTrain: epoch  9, batch    26 | loss: 63.3291920CurrentTrain: epoch  9, batch    27 | loss: 66.8908218CurrentTrain: epoch  9, batch    28 | loss: 54.0951443CurrentTrain: epoch  9, batch    29 | loss: 81.1700143CurrentTrain: epoch  9, batch    30 | loss: 117.4932967CurrentTrain: epoch  9, batch    31 | loss: 83.0098721CurrentTrain: epoch  9, batch    32 | loss: 85.7355558CurrentTrain: epoch  9, batch    33 | loss: 82.6741877CurrentTrain: epoch  9, batch    34 | loss: 64.1834496CurrentTrain: epoch  9, batch    35 | loss: 49.9523832CurrentTrain: epoch  9, batch    36 | loss: 52.4316520CurrentTrain: epoch  9, batch    37 | loss: 53.0237857CurrentTrain: epoch  9, batch    38 | loss: 62.4345162CurrentTrain: epoch  9, batch    39 | loss: 62.6909928CurrentTrain: epoch  9, batch    40 | loss: 115.4891766CurrentTrain: epoch  9, batch    41 | loss: 50.9557343CurrentTrain: epoch  9, batch    42 | loss: 53.1600112CurrentTrain: epoch  9, batch    43 | loss: 115.2003228CurrentTrain: epoch  9, batch    44 | loss: 64.3196288CurrentTrain: epoch  9, batch    45 | loss: 115.2663118CurrentTrain: epoch  9, batch    46 | loss: 65.6043781CurrentTrain: epoch  9, batch    47 | loss: 82.6012213CurrentTrain: epoch  9, batch    48 | loss: 52.0143501CurrentTrain: epoch  9, batch    49 | loss: 66.0899593CurrentTrain: epoch  9, batch    50 | loss: 111.0462925CurrentTrain: epoch  9, batch    51 | loss: 52.2195133CurrentTrain: epoch  9, batch    52 | loss: 83.4631975CurrentTrain: epoch  9, batch    53 | loss: 62.9038308CurrentTrain: epoch  9, batch    54 | loss: 40.6557237CurrentTrain: epoch  9, batch    55 | loss: 61.5969282CurrentTrain: epoch  9, batch    56 | loss: 48.7303132CurrentTrain: epoch  9, batch    57 | loss: 117.5178013CurrentTrain: epoch  9, batch    58 | loss: 64.8732315CurrentTrain: epoch  9, batch    59 | loss: 82.8046445CurrentTrain: epoch  9, batch    60 | loss: 62.9346852CurrentTrain: epoch  9, batch    61 | loss: 119.4937639CurrentTrain: epoch  9, batch    62 | loss: 86.8464699CurrentTrain: epoch  9, batch    63 | loss: 81.0656315CurrentTrain: epoch  9, batch    64 | loss: 41.9048263CurrentTrain: epoch  9, batch    65 | loss: 182.0629426CurrentTrain: epoch  9, batch    66 | loss: 53.2883790CurrentTrain: epoch  9, batch    67 | loss: 51.0266429CurrentTrain: epoch  9, batch    68 | loss: 41.1408458CurrentTrain: epoch  9, batch    69 | loss: 82.6413827CurrentTrain: epoch  9, batch    70 | loss: 82.4792507CurrentTrain: epoch  9, batch    71 | loss: 64.0148626CurrentTrain: epoch  9, batch    72 | loss: 52.0142661CurrentTrain: epoch  9, batch    73 | loss: 50.9783300CurrentTrain: epoch  9, batch    74 | loss: 81.2354227CurrentTrain: epoch  9, batch    75 | loss: 81.7857352CurrentTrain: epoch  9, batch    76 | loss: 85.8647822CurrentTrain: epoch  9, batch    77 | loss: 117.5109351CurrentTrain: epoch  9, batch    78 | loss: 54.2042641CurrentTrain: epoch  9, batch    79 | loss: 62.9650311CurrentTrain: epoch  9, batch    80 | loss: 53.4612312CurrentTrain: epoch  9, batch    81 | loss: 60.7149964CurrentTrain: epoch  9, batch    82 | loss: 64.2464920CurrentTrain: epoch  9, batch    83 | loss: 64.5110779CurrentTrain: epoch  9, batch    84 | loss: 84.3369934CurrentTrain: epoch  9, batch    85 | loss: 115.2091233CurrentTrain: epoch  9, batch    86 | loss: 62.7447845CurrentTrain: epoch  9, batch    87 | loss: 117.5435086CurrentTrain: epoch  9, batch    88 | loss: 181.7306270CurrentTrain: epoch  9, batch    89 | loss: 79.2773930CurrentTrain: epoch  9, batch    90 | loss: 41.5566523CurrentTrain: epoch  9, batch    91 | loss: 44.4708398CurrentTrain: epoch  9, batch    92 | loss: 82.3290145CurrentTrain: epoch  9, batch    93 | loss: 63.1600768CurrentTrain: epoch  9, batch    94 | loss: 62.9973122CurrentTrain: epoch  9, batch    95 | loss: 90.7454866

F1 score per class: {32: 0.4956521739130435, 6: 0.6837606837606838, 19: 0.2222222222222222, 24: 0.7142857142857143, 26: 0.8629441624365483, 29: 0.8056872037914692}
Micro-average F1 score: 0.6790450928381963
Weighted-average F1 score: 0.6651990312907897
F1 score per class: {32: 0.5159010600706714, 6: 0.6563706563706564, 19: 0.13559322033898305, 24: 0.7106598984771574, 26: 0.8826291079812206, 29: 0.702928870292887}
Micro-average F1 score: 0.6325439266615738
Weighted-average F1 score: 0.6051964130399987
F1 score per class: {32: 0.5159010600706714, 6: 0.6563706563706564, 19: 0.13559322033898305, 24: 0.7106598984771574, 26: 0.8826291079812206, 29: 0.702928870292887}
Micro-average F1 score: 0.6325439266615738
Weighted-average F1 score: 0.6051964130399987

F1 score per class: {32: 0.4956521739130435, 6: 0.6837606837606838, 19: 0.2222222222222222, 24: 0.7142857142857143, 26: 0.8629441624365483, 29: 0.8056872037914692}
Micro-average F1 score: 0.6790450928381963
Weighted-average F1 score: 0.6651990312907897
F1 score per class: {32: 0.5159010600706714, 6: 0.6563706563706564, 19: 0.13559322033898305, 24: 0.7106598984771574, 26: 0.8826291079812206, 29: 0.702928870292887}
Micro-average F1 score: 0.6325439266615738
Weighted-average F1 score: 0.6051964130399987
F1 score per class: {32: 0.5159010600706714, 6: 0.6563706563706564, 19: 0.13559322033898305, 24: 0.7106598984771574, 26: 0.8826291079812206, 29: 0.702928870292887}
Micro-average F1 score: 0.6325439266615738
Weighted-average F1 score: 0.6051964130399987
cur_acc:  ['0.6790']
his_acc:  ['0.6790']
cur_acc des:  ['0.6325']
his_acc des:  ['0.6325']
cur_acc rrf:  ['0.6325']
his_acc rrf:  ['0.6325']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 102.7295307CurrentTrain: epoch  0, batch     1 | loss: 75.5348367CurrentTrain: epoch  0, batch     2 | loss: 92.9218878CurrentTrain: epoch  0, batch     3 | loss: 14.1896718CurrentTrain: epoch  1, batch     0 | loss: 74.3689224CurrentTrain: epoch  1, batch     1 | loss: 56.9726888CurrentTrain: epoch  1, batch     2 | loss: 92.6169839CurrentTrain: epoch  1, batch     3 | loss: 11.0514311CurrentTrain: epoch  2, batch     0 | loss: 57.9025837CurrentTrain: epoch  2, batch     1 | loss: 58.7870303CurrentTrain: epoch  2, batch     2 | loss: 57.6962724CurrentTrain: epoch  2, batch     3 | loss: 11.8657066CurrentTrain: epoch  3, batch     0 | loss: 58.0248193CurrentTrain: epoch  3, batch     1 | loss: 56.7768544CurrentTrain: epoch  3, batch     2 | loss: 66.1266813CurrentTrain: epoch  3, batch     3 | loss: 7.5936022CurrentTrain: epoch  4, batch     0 | loss: 69.9020937CurrentTrain: epoch  4, batch     1 | loss: 64.1490120CurrentTrain: epoch  4, batch     2 | loss: 85.9111051CurrentTrain: epoch  4, batch     3 | loss: 11.4001607CurrentTrain: epoch  5, batch     0 | loss: 66.5116973CurrentTrain: epoch  5, batch     1 | loss: 81.0680321CurrentTrain: epoch  5, batch     2 | loss: 83.7584795CurrentTrain: epoch  5, batch     3 | loss: 10.9188304CurrentTrain: epoch  6, batch     0 | loss: 51.9049994CurrentTrain: epoch  6, batch     1 | loss: 54.2463728CurrentTrain: epoch  6, batch     2 | loss: 54.2160766CurrentTrain: epoch  6, batch     3 | loss: 27.4051689CurrentTrain: epoch  7, batch     0 | loss: 65.3186427CurrentTrain: epoch  7, batch     1 | loss: 65.6199227CurrentTrain: epoch  7, batch     2 | loss: 51.0281059CurrentTrain: epoch  7, batch     3 | loss: 27.3725289CurrentTrain: epoch  8, batch     0 | loss: 50.3729685CurrentTrain: epoch  8, batch     1 | loss: 68.1159661CurrentTrain: epoch  8, batch     2 | loss: 51.9893970CurrentTrain: epoch  8, batch     3 | loss: 6.8246400CurrentTrain: epoch  9, batch     0 | loss: 51.7682754CurrentTrain: epoch  9, batch     1 | loss: 51.5836528CurrentTrain: epoch  9, batch     2 | loss: 52.9606033CurrentTrain: epoch  9, batch     3 | loss: 27.3829146
MemoryTrain:  epoch  0, batch     0 | loss: 0.8164073MemoryTrain:  epoch  1, batch     0 | loss: 0.6771229MemoryTrain:  epoch  2, batch     0 | loss: 0.6768326MemoryTrain:  epoch  3, batch     0 | loss: 0.5303242MemoryTrain:  epoch  4, batch     0 | loss: 0.4398622MemoryTrain:  epoch  5, batch     0 | loss: 0.5317255MemoryTrain:  epoch  6, batch     0 | loss: 0.3301435MemoryTrain:  epoch  7, batch     0 | loss: 0.2834871MemoryTrain:  epoch  8, batch     0 | loss: 0.2220211MemoryTrain:  epoch  9, batch     0 | loss: 0.1904357

F1 score per class: {32: 0.0, 6: 0.5714285714285714, 7: 0.9411764705882353, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.4375, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.359375}
Micro-average F1 score: 0.38095238095238093
Weighted-average F1 score: 0.30304038281979456
F1 score per class: {32: 0.0, 6: 0.75, 7: 0.847457627118644, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.34615384615384615, 26: 0.0, 27: 0.16, 29: 0.0, 31: 0.33519553072625696}
Micro-average F1 score: 0.34074074074074073
Weighted-average F1 score: 0.2907312907406522
F1 score per class: {32: 0.0, 6: 0.75, 7: 0.847457627118644, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.32142857142857145, 26: 0.0, 27: 0.16666666666666666, 29: 0.0, 31: 0.33519553072625696}
Micro-average F1 score: 0.345
Weighted-average F1 score: 0.29689126169099656

F1 score per class: {32: 0.391304347826087, 6: 0.058823529411764705, 7: 0.9411764705882353, 40: 0.5435540069686411, 9: 0.2222222222222222, 19: 0.7083333333333334, 24: 0.1590909090909091, 26: 0.86, 27: 0.0, 29: 0.7798165137614679, 31: 0.3006535947712418}
Micro-average F1 score: 0.5498371335504886
Weighted-average F1 score: 0.5143099088217806
F1 score per class: {32: 0.39166666666666666, 6: 0.08108108108108109, 7: 0.847457627118644, 40: 0.5337837837837838, 9: 0.12903225806451613, 19: 0.6965174129353234, 24: 0.10909090909090909, 26: 0.8623853211009175, 27: 0.047619047619047616, 29: 0.6694560669456067, 31: 0.21052631578947367}
Micro-average F1 score: 0.4554759467758444
Weighted-average F1 score: 0.40395459993495814
F1 score per class: {32: 0.3900414937759336, 6: 0.08333333333333333, 7: 0.847457627118644, 40: 0.5392491467576792, 9: 0.16666666666666666, 19: 0.6965174129353234, 24: 0.10344827586206896, 26: 0.8623853211009175, 27: 0.05063291139240506, 29: 0.6978723404255319, 31: 0.20833333333333334}
Micro-average F1 score: 0.46273291925465837
Weighted-average F1 score: 0.4110231565023474
cur_acc:  ['0.6790', '0.3810']
his_acc:  ['0.6790', '0.5498']
cur_acc des:  ['0.6325', '0.3407']
his_acc des:  ['0.6325', '0.4555']
cur_acc rrf:  ['0.6325', '0.3450']
his_acc rrf:  ['0.6325', '0.4627']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 90.7701575CurrentTrain: epoch  0, batch     1 | loss: 108.4763980CurrentTrain: epoch  0, batch     2 | loss: 79.7297446CurrentTrain: epoch  0, batch     3 | loss: 67.8374071CurrentTrain: epoch  1, batch     0 | loss: 93.8918023CurrentTrain: epoch  1, batch     1 | loss: 96.5816532CurrentTrain: epoch  1, batch     2 | loss: 74.0150459CurrentTrain: epoch  1, batch     3 | loss: 59.0174681CurrentTrain: epoch  2, batch     0 | loss: 93.2949234CurrentTrain: epoch  2, batch     1 | loss: 61.4687876CurrentTrain: epoch  2, batch     2 | loss: 57.8428735CurrentTrain: epoch  2, batch     3 | loss: 49.9422465CurrentTrain: epoch  3, batch     0 | loss: 86.0835598CurrentTrain: epoch  3, batch     1 | loss: 74.1568142CurrentTrain: epoch  3, batch     2 | loss: 84.2229181CurrentTrain: epoch  3, batch     3 | loss: 99.9378427CurrentTrain: epoch  4, batch     0 | loss: 60.3103445CurrentTrain: epoch  4, batch     1 | loss: 58.8347660CurrentTrain: epoch  4, batch     2 | loss: 84.5287511CurrentTrain: epoch  4, batch     3 | loss: 71.5964227CurrentTrain: epoch  5, batch     0 | loss: 66.0878516CurrentTrain: epoch  5, batch     1 | loss: 179.0164278CurrentTrain: epoch  5, batch     2 | loss: 58.8472148CurrentTrain: epoch  5, batch     3 | loss: 54.6600007CurrentTrain: epoch  6, batch     0 | loss: 53.9592702CurrentTrain: epoch  6, batch     1 | loss: 89.2223016CurrentTrain: epoch  6, batch     2 | loss: 65.1188456CurrentTrain: epoch  6, batch     3 | loss: 96.6501255CurrentTrain: epoch  7, batch     0 | loss: 67.6635138CurrentTrain: epoch  7, batch     1 | loss: 67.1962522CurrentTrain: epoch  7, batch     2 | loss: 68.9462927CurrentTrain: epoch  7, batch     3 | loss: 55.1778719CurrentTrain: epoch  8, batch     0 | loss: 88.9355440CurrentTrain: epoch  8, batch     1 | loss: 68.2283092CurrentTrain: epoch  8, batch     2 | loss: 81.1625314CurrentTrain: epoch  8, batch     3 | loss: 44.3998907CurrentTrain: epoch  9, batch     0 | loss: 54.4688305CurrentTrain: epoch  9, batch     1 | loss: 116.1744423CurrentTrain: epoch  9, batch     2 | loss: 66.6512829CurrentTrain: epoch  9, batch     3 | loss: 53.8723955
MemoryTrain:  epoch  0, batch     0 | loss: 0.7066445MemoryTrain:  epoch  1, batch     0 | loss: 0.5629173MemoryTrain:  epoch  2, batch     0 | loss: 0.3625921MemoryTrain:  epoch  3, batch     0 | loss: 0.3161719MemoryTrain:  epoch  4, batch     0 | loss: 0.2537597MemoryTrain:  epoch  5, batch     0 | loss: 0.2093991MemoryTrain:  epoch  6, batch     0 | loss: 0.1623398MemoryTrain:  epoch  7, batch     0 | loss: 0.1333683MemoryTrain:  epoch  8, batch     0 | loss: 0.1101459MemoryTrain:  epoch  9, batch     0 | loss: 0.0846600

F1 score per class: {0: 0.8947368421052632, 32: 0.98989898989899, 4: 0.0, 6: 0.0, 7: 0.047619047619047616, 40: 0.0, 13: 0.16216216216216217, 19: 0.5405405405405406, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5304054054054054
Weighted-average F1 score: 0.4065438445744048
F1 score per class: {0: 0.660377358490566, 32: 0.9801980198019802, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.032520325203252036, 9: 0.0, 13: 0.3063063063063063, 19: 0.6153846153846154, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.4389086595492289
Weighted-average F1 score: 0.33541743452818995
F1 score per class: {0: 0.6542056074766355, 32: 0.9850746268656716, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.031746031746031744, 9: 0.0, 13: 0.32432432432432434, 19: 0.6153846153846154, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.45145631067961167
Weighted-average F1 score: 0.34793223294856984

F1 score per class: {32: 0.5483870967741935, 0: 0.9849246231155779, 4: 0.3643724696356275, 6: 0.08695652173913043, 7: 0.9230769230769231, 40: 0.024096385542168676, 9: 0.546875, 13: 0.12244897959183673, 19: 0.5128205128205128, 21: 0.14285714285714285, 23: 0.660377358490566, 24: 0.15555555555555556, 26: 0.8363636363636363, 27: 0.1, 29: 0.7352941176470589, 31: 0.3502824858757062}
Micro-average F1 score: 0.5228403437358661
Weighted-average F1 score: 0.4722754599371537
F1 score per class: {32: 0.35714285714285715, 0: 0.9473684210526315, 4: 0.3536231884057971, 6: 0.05555555555555555, 7: 0.7936507936507936, 40: 0.01694915254237288, 9: 0.501628664495114, 13: 0.15384615384615385, 19: 0.5565217391304348, 21: 0.10344827586206896, 23: 0.6829268292682927, 24: 0.11678832116788321, 26: 0.7966101694915254, 27: 0.0425531914893617, 29: 0.6277372262773723, 31: 0.24166666666666667}
Micro-average F1 score: 0.42686170212765956
Weighted-average F1 score: 0.3747561738617151
F1 score per class: {32: 0.3448275862068966, 0: 0.9611650485436893, 4: 0.36645962732919257, 6: 0.05714285714285714, 7: 0.8333333333333334, 40: 0.01639344262295082, 9: 0.5238095238095238, 13: 0.15384615384615385, 19: 0.5663716814159292, 21: 0.11428571428571428, 23: 0.6796116504854369, 24: 0.125, 26: 0.7932489451476793, 27: 0.05714285714285714, 29: 0.6441947565543071, 31: 0.23293172690763053}
Micro-average F1 score: 0.43567052416609936
Weighted-average F1 score: 0.3822308677665098
cur_acc:  ['0.6790', '0.3810', '0.5304']
his_acc:  ['0.6790', '0.5498', '0.5228']
cur_acc des:  ['0.6325', '0.3407', '0.4389']
his_acc des:  ['0.6325', '0.4555', '0.4269']
cur_acc rrf:  ['0.6325', '0.3450', '0.4515']
his_acc rrf:  ['0.6325', '0.4627', '0.4357']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 77.8131894CurrentTrain: epoch  0, batch     1 | loss: 64.0531863CurrentTrain: epoch  0, batch     2 | loss: 98.1721109CurrentTrain: epoch  0, batch     3 | loss: 92.7637883CurrentTrain: epoch  0, batch     4 | loss: 78.8718233CurrentTrain: epoch  1, batch     0 | loss: 122.1389121CurrentTrain: epoch  1, batch     1 | loss: 124.1654608CurrentTrain: epoch  1, batch     2 | loss: 71.0531019CurrentTrain: epoch  1, batch     3 | loss: 56.5561634CurrentTrain: epoch  1, batch     4 | loss: 46.3480788CurrentTrain: epoch  2, batch     0 | loss: 67.7525531CurrentTrain: epoch  2, batch     1 | loss: 91.2884992CurrentTrain: epoch  2, batch     2 | loss: 58.0300177CurrentTrain: epoch  2, batch     3 | loss: 69.6585085CurrentTrain: epoch  2, batch     4 | loss: 77.7574097CurrentTrain: epoch  3, batch     0 | loss: 67.6008308CurrentTrain: epoch  3, batch     1 | loss: 86.9885734CurrentTrain: epoch  3, batch     2 | loss: 69.3531870CurrentTrain: epoch  3, batch     3 | loss: 69.1686168CurrentTrain: epoch  3, batch     4 | loss: 114.1296916CurrentTrain: epoch  4, batch     0 | loss: 118.2001815CurrentTrain: epoch  4, batch     1 | loss: 116.3273045CurrentTrain: epoch  4, batch     2 | loss: 118.5988636CurrentTrain: epoch  4, batch     3 | loss: 53.9470131CurrentTrain: epoch  4, batch     4 | loss: 39.7092145CurrentTrain: epoch  5, batch     0 | loss: 84.5014204CurrentTrain: epoch  5, batch     1 | loss: 52.9931669CurrentTrain: epoch  5, batch     2 | loss: 68.6619256CurrentTrain: epoch  5, batch     3 | loss: 84.6311769CurrentTrain: epoch  5, batch     4 | loss: 54.9273619CurrentTrain: epoch  6, batch     0 | loss: 67.5447494CurrentTrain: epoch  6, batch     1 | loss: 66.0387224CurrentTrain: epoch  6, batch     2 | loss: 81.1394823CurrentTrain: epoch  6, batch     3 | loss: 64.3043440CurrentTrain: epoch  6, batch     4 | loss: 116.5120366CurrentTrain: epoch  7, batch     0 | loss: 86.4922330CurrentTrain: epoch  7, batch     1 | loss: 67.4405048CurrentTrain: epoch  7, batch     2 | loss: 53.2225258CurrentTrain: epoch  7, batch     3 | loss: 52.1876709CurrentTrain: epoch  7, batch     4 | loss: 73.3749645CurrentTrain: epoch  8, batch     0 | loss: 67.2830671CurrentTrain: epoch  8, batch     1 | loss: 52.8873377CurrentTrain: epoch  8, batch     2 | loss: 64.9675648CurrentTrain: epoch  8, batch     3 | loss: 67.5717032CurrentTrain: epoch  8, batch     4 | loss: 41.6398902CurrentTrain: epoch  9, batch     0 | loss: 54.4761568CurrentTrain: epoch  9, batch     1 | loss: 64.5101223CurrentTrain: epoch  9, batch     2 | loss: 66.1084966CurrentTrain: epoch  9, batch     3 | loss: 65.8287455CurrentTrain: epoch  9, batch     4 | loss: 41.1470635
MemoryTrain:  epoch  0, batch     0 | loss: 0.3518595MemoryTrain:  epoch  1, batch     0 | loss: 0.2966299MemoryTrain:  epoch  2, batch     0 | loss: 0.2152432MemoryTrain:  epoch  3, batch     0 | loss: 0.1666499MemoryTrain:  epoch  4, batch     0 | loss: 0.1408037MemoryTrain:  epoch  5, batch     0 | loss: 0.1295792MemoryTrain:  epoch  6, batch     0 | loss: 0.0896588MemoryTrain:  epoch  7, batch     0 | loss: 0.0789657MemoryTrain:  epoch  8, batch     0 | loss: 0.0617677MemoryTrain:  epoch  9, batch     0 | loss: 0.0584607

F1 score per class: {0: 0.0, 4: 0.0, 5: 0.945273631840796, 6: 0.0, 7: 0.0, 40: 0.4861878453038674, 10: 0.0, 13: 0.5609756097560976, 16: 0.0, 17: 0.32558139534883723, 18: 0.0, 19: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5614035087719298
Weighted-average F1 score: 0.5127580409257658
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.6265822784810127, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.5414847161572053, 13: 0.0, 16: 0.49122807017543857, 17: 0.3157894736842105, 18: 0.26976744186046514, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.40513290559120074
Weighted-average F1 score: 0.3651938132747484
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.6265822784810127, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.5384615384615384, 13: 0.0, 16: 0.48214285714285715, 17: 0.42857142857142855, 18: 0.26976744186046514, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4130841121495327
Weighted-average F1 score: 0.3742982297046336

F1 score per class: {0: 0.6346153846153846, 4: 0.9435897435897436, 5: 0.9313725490196079, 6: 0.351931330472103, 7: 0.0, 9: 0.9230769230769231, 10: 0.3826086956521739, 13: 0.05128205128205128, 16: 0.4842105263157895, 17: 0.0, 18: 0.3076923076923077, 19: 0.5159010600706714, 21: 0.0, 23: 0.625, 24: 0.14285714285714285, 26: 0.6432160804020101, 27: 0.13861386138613863, 29: 0.8070175438596491, 31: 0.0, 32: 0.6703910614525139, 40: 0.2937062937062937}
Micro-average F1 score: 0.5467128027681661
Weighted-average F1 score: 0.5227621494537731
F1 score per class: {0: 0.4268292682926829, 4: 0.9655172413793104, 5: 0.5439560439560439, 6: 0.36721311475409835, 7: 0.0, 9: 0.7692307692307693, 10: 0.36795252225519287, 13: 0.05, 16: 0.3888888888888889, 17: 0.15789473684210525, 18: 0.19463087248322147, 19: 0.4540540540540541, 21: 0.14847161572052403, 23: 0.5148514851485149, 24: 0.08571428571428572, 26: 0.6359447004608295, 27: 0.12345679012345678, 29: 0.7279693486590039, 31: 0.05970149253731343, 32: 0.6694214876033058, 40: 0.19771863117870722}
Micro-average F1 score: 0.42027194066749074
Weighted-average F1 score: 0.3856539934768106
F1 score per class: {0: 0.4166666666666667, 4: 0.9655172413793104, 5: 0.5395095367847411, 6: 0.3564356435643564, 7: 0.0, 9: 0.78125, 10: 0.36103151862464183, 13: 0.05263157894736842, 16: 0.38028169014084506, 17: 0.2608695652173913, 18: 0.19931271477663232, 19: 0.4839650145772595, 21: 0.16101694915254236, 23: 0.5242718446601942, 24: 0.09090909090909091, 26: 0.6359447004608295, 27: 0.125, 29: 0.7224334600760456, 31: 0.07407407407407407, 32: 0.7130434782608696, 40: 0.21311475409836064}
Micro-average F1 score: 0.4327217125382263
Weighted-average F1 score: 0.3989825291995406
cur_acc:  ['0.6790', '0.3810', '0.5304', '0.5614']
his_acc:  ['0.6790', '0.5498', '0.5228', '0.5467']
cur_acc des:  ['0.6325', '0.3407', '0.4389', '0.4051']
his_acc des:  ['0.6325', '0.4555', '0.4269', '0.4203']
cur_acc rrf:  ['0.6325', '0.3450', '0.4515', '0.4131']
his_acc rrf:  ['0.6325', '0.4627', '0.4357', '0.4327']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 63.0768127CurrentTrain: epoch  0, batch     1 | loss: 68.9135022CurrentTrain: epoch  0, batch     2 | loss: 94.5504374CurrentTrain: epoch  0, batch     3 | loss: 60.8898799CurrentTrain: epoch  1, batch     0 | loss: 58.9863748CurrentTrain: epoch  1, batch     1 | loss: 72.0220168CurrentTrain: epoch  1, batch     2 | loss: 90.1824172CurrentTrain: epoch  1, batch     3 | loss: 49.9791319CurrentTrain: epoch  2, batch     0 | loss: 72.6784885CurrentTrain: epoch  2, batch     1 | loss: 69.1614851CurrentTrain: epoch  2, batch     2 | loss: 53.4702875CurrentTrain: epoch  2, batch     3 | loss: 58.5624400CurrentTrain: epoch  3, batch     0 | loss: 89.7808269CurrentTrain: epoch  3, batch     1 | loss: 55.2383618CurrentTrain: epoch  3, batch     2 | loss: 116.0357339CurrentTrain: epoch  3, batch     3 | loss: 41.2677838CurrentTrain: epoch  4, batch     0 | loss: 66.3443038CurrentTrain: epoch  4, batch     1 | loss: 53.7256121CurrentTrain: epoch  4, batch     2 | loss: 65.7634434CurrentTrain: epoch  4, batch     3 | loss: 59.0197411CurrentTrain: epoch  5, batch     0 | loss: 53.4224474CurrentTrain: epoch  5, batch     1 | loss: 66.4077584CurrentTrain: epoch  5, batch     2 | loss: 53.6247859CurrentTrain: epoch  5, batch     3 | loss: 44.7776429CurrentTrain: epoch  6, batch     0 | loss: 61.9599682CurrentTrain: epoch  6, batch     1 | loss: 67.6896296CurrentTrain: epoch  6, batch     2 | loss: 52.3321612CurrentTrain: epoch  6, batch     3 | loss: 58.0904037CurrentTrain: epoch  7, batch     0 | loss: 85.0362202CurrentTrain: epoch  7, batch     1 | loss: 64.4655906CurrentTrain: epoch  7, batch     2 | loss: 51.5207925CurrentTrain: epoch  7, batch     3 | loss: 42.5961447CurrentTrain: epoch  8, batch     0 | loss: 58.9394424CurrentTrain: epoch  8, batch     1 | loss: 52.7468377CurrentTrain: epoch  8, batch     2 | loss: 115.7218520CurrentTrain: epoch  8, batch     3 | loss: 56.9608380CurrentTrain: epoch  9, batch     0 | loss: 62.7561005CurrentTrain: epoch  9, batch     1 | loss: 82.8924239CurrentTrain: epoch  9, batch     2 | loss: 84.3880290CurrentTrain: epoch  9, batch     3 | loss: 51.5217983
MemoryTrain:  epoch  0, batch     0 | loss: 0.3445663MemoryTrain:  epoch  1, batch     0 | loss: 0.2603641MemoryTrain:  epoch  2, batch     0 | loss: 0.2221968MemoryTrain:  epoch  3, batch     0 | loss: 0.1683094MemoryTrain:  epoch  4, batch     0 | loss: 0.1182197MemoryTrain:  epoch  5, batch     0 | loss: 0.1010790MemoryTrain:  epoch  6, batch     0 | loss: 0.0898357MemoryTrain:  epoch  7, batch     0 | loss: 0.0838342MemoryTrain:  epoch  8, batch     0 | loss: 0.0614291MemoryTrain:  epoch  9, batch     0 | loss: 0.0606977

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.8421052631578947, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.417910447761194, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.3855421686746988, 37: 0.41304347826086957, 38: 0.47619047619047616, 40: 0.0}
Micro-average F1 score: 0.319047619047619
Weighted-average F1 score: 0.2103607828924375
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.631578947368421, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5333333333333333, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6052631578947368, 37: 0.4563758389261745, 38: 0.5070422535211268, 40: 0.0}
Micro-average F1 score: 0.3463687150837989
Weighted-average F1 score: 0.27476256836153173
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5333333333333333, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5736434108527132, 37: 0.4563758389261745, 38: 0.5352112676056338, 40: 0.0}
Micro-average F1 score: 0.3366906474820144
Weighted-average F1 score: 0.2590601623845818

F1 score per class: {0: 0.5945945945945946, 4: 0.9381443298969072, 5: 0.7901234567901234, 6: 0.325, 7: 0.0, 9: 0.9056603773584906, 10: 0.373134328358209, 13: 0.0547945205479452, 15: 0.42105263157894735, 16: 0.40336134453781514, 17: 0.4, 18: 0.21951219512195122, 19: 0.3877551020408163, 21: 0.0, 23: 0.7073170731707317, 24: 0.15384615384615385, 25: 0.417910447761194, 26: 0.6699507389162561, 27: 0.16091954022988506, 29: 0.7644444444444445, 31: 0.0, 32: 0.6236559139784946, 35: 0.24242424242424243, 37: 0.19, 38: 0.2857142857142857, 40: 0.29310344827586204}
Micro-average F1 score: 0.47539936102236424
Weighted-average F1 score: 0.4527449233263507
F1 score per class: {0: 0.41975308641975306, 4: 0.9547738693467337, 5: 0.4750593824228028, 6: 0.3188405797101449, 7: 0.0, 9: 0.7352941176470589, 10: 0.3157894736842105, 13: 0.03361344537815126, 15: 0.25, 16: 0.29473684210526313, 17: 0.2727272727272727, 18: 0.1507537688442211, 19: 0.45774647887323944, 21: 0.16042780748663102, 23: 0.5057471264367817, 24: 0.0967741935483871, 25: 0.5333333333333333, 26: 0.6052631578947368, 27: 0.0979020979020979, 29: 0.7351778656126482, 31: 0.07272727272727272, 32: 0.6967213114754098, 35: 0.25, 37: 0.12386156648451731, 38: 0.21951219512195122, 40: 0.34579439252336447}
Micro-average F1 score: 0.366296079578701
Weighted-average F1 score: 0.33133038117318264
F1 score per class: {0: 0.41975308641975306, 4: 0.9547738693467337, 5: 0.4914004914004914, 6: 0.31486880466472306, 7: 0.0, 9: 0.746268656716418, 10: 0.3248730964467005, 13: 0.04, 15: 0.23333333333333334, 16: 0.30939226519337015, 17: 0.2857142857142857, 18: 0.14358974358974358, 19: 0.47232472324723246, 21: 0.16, 23: 0.5121951219512195, 24: 0.1111111111111111, 25: 0.5333333333333333, 26: 0.6160714285714286, 27: 0.1044776119402985, 29: 0.7250996015936255, 31: 0.07272727272727272, 32: 0.6666666666666666, 35: 0.2624113475177305, 37: 0.12186379928315412, 38: 0.2235294117647059, 40: 0.3417085427135678}
Micro-average F1 score: 0.372289156626506
Weighted-average F1 score: 0.3375236793976038
cur_acc:  ['0.6790', '0.3810', '0.5304', '0.5614', '0.3190']
his_acc:  ['0.6790', '0.5498', '0.5228', '0.5467', '0.4754']
cur_acc des:  ['0.6325', '0.3407', '0.4389', '0.4051', '0.3464']
his_acc des:  ['0.6325', '0.4555', '0.4269', '0.4203', '0.3663']
cur_acc rrf:  ['0.6325', '0.3450', '0.4515', '0.4131', '0.3367']
his_acc rrf:  ['0.6325', '0.4627', '0.4357', '0.4327', '0.3723']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 106.9224790CurrentTrain: epoch  0, batch     1 | loss: 83.0107869CurrentTrain: epoch  0, batch     2 | loss: 80.8312666CurrentTrain: epoch  0, batch     3 | loss: 66.1762813CurrentTrain: epoch  0, batch     4 | loss: 34.9765202CurrentTrain: epoch  1, batch     0 | loss: 64.6559626CurrentTrain: epoch  1, batch     1 | loss: 95.1471041CurrentTrain: epoch  1, batch     2 | loss: 61.7265256CurrentTrain: epoch  1, batch     3 | loss: 80.6413973CurrentTrain: epoch  1, batch     4 | loss: 60.4473479CurrentTrain: epoch  2, batch     0 | loss: 72.9955603CurrentTrain: epoch  2, batch     1 | loss: 73.2300937CurrentTrain: epoch  2, batch     2 | loss: 55.7952399CurrentTrain: epoch  2, batch     3 | loss: 124.9071357CurrentTrain: epoch  2, batch     4 | loss: 30.6000966CurrentTrain: epoch  3, batch     0 | loss: 88.4565082CurrentTrain: epoch  3, batch     1 | loss: 125.9567109CurrentTrain: epoch  3, batch     2 | loss: 56.8010025CurrentTrain: epoch  3, batch     3 | loss: 72.9841009CurrentTrain: epoch  3, batch     4 | loss: 11.4529824CurrentTrain: epoch  4, batch     0 | loss: 67.1450395CurrentTrain: epoch  4, batch     1 | loss: 71.1864908CurrentTrain: epoch  4, batch     2 | loss: 87.1704495CurrentTrain: epoch  4, batch     3 | loss: 68.9435822CurrentTrain: epoch  4, batch     4 | loss: 60.1715138CurrentTrain: epoch  5, batch     0 | loss: 87.7115464CurrentTrain: epoch  5, batch     1 | loss: 64.7965898CurrentTrain: epoch  5, batch     2 | loss: 54.2907282CurrentTrain: epoch  5, batch     3 | loss: 68.0026162CurrentTrain: epoch  5, batch     4 | loss: 60.1637247CurrentTrain: epoch  6, batch     0 | loss: 53.8699656CurrentTrain: epoch  6, batch     1 | loss: 85.4055540CurrentTrain: epoch  6, batch     2 | loss: 68.4108220CurrentTrain: epoch  6, batch     3 | loss: 85.3482157CurrentTrain: epoch  6, batch     4 | loss: 27.2246063CurrentTrain: epoch  7, batch     0 | loss: 64.6311335CurrentTrain: epoch  7, batch     1 | loss: 56.9121931CurrentTrain: epoch  7, batch     2 | loss: 66.4869710CurrentTrain: epoch  7, batch     3 | loss: 82.3142089CurrentTrain: epoch  7, batch     4 | loss: 60.0599902CurrentTrain: epoch  8, batch     0 | loss: 52.9001784CurrentTrain: epoch  8, batch     1 | loss: 86.3680654CurrentTrain: epoch  8, batch     2 | loss: 86.5106468CurrentTrain: epoch  8, batch     3 | loss: 62.6761133CurrentTrain: epoch  8, batch     4 | loss: 26.9365263CurrentTrain: epoch  9, batch     0 | loss: 53.2143485CurrentTrain: epoch  9, batch     1 | loss: 83.8923900CurrentTrain: epoch  9, batch     2 | loss: 83.4474897CurrentTrain: epoch  9, batch     3 | loss: 81.9775535CurrentTrain: epoch  9, batch     4 | loss: 60.1151001
MemoryTrain:  epoch  0, batch     0 | loss: 0.3640229MemoryTrain:  epoch  1, batch     0 | loss: 0.3444238MemoryTrain:  epoch  2, batch     0 | loss: 0.2366933MemoryTrain:  epoch  3, batch     0 | loss: 0.1950090MemoryTrain:  epoch  4, batch     0 | loss: 0.1691486MemoryTrain:  epoch  5, batch     0 | loss: 0.1376653MemoryTrain:  epoch  6, batch     0 | loss: 0.1223266MemoryTrain:  epoch  7, batch     0 | loss: 0.0941533MemoryTrain:  epoch  8, batch     0 | loss: 0.0963730MemoryTrain:  epoch  9, batch     0 | loss: 0.0816129

F1 score per class: {0: 0.0, 2: 0.3333333333333333, 4: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.25806451612903225, 12: 0.26277372262773724, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 26: 0.0, 28: 0.35294117647058826, 29: 0.0, 31: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.17813765182186234
Weighted-average F1 score: 0.12068880329493357
F1 score per class: {0: 0.0, 2: 0.20588235294117646, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.5771144278606966, 12: 0.5680933852140078, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.25806451612903225, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.14814814814814814, 40: 0.0}
Micro-average F1 score: 0.26205641492265697
Weighted-average F1 score: 0.19819665959918295
F1 score per class: {0: 0.0, 2: 0.2028985507246377, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.4444444444444444, 12: 0.568, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 28: 0.3076923076923077, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.22801932367149758
Weighted-average F1 score: 0.16642311699856455

F1 score per class: {0: 0.5714285714285714, 2: 0.2153846153846154, 4: 0.9346733668341709, 5: 0.803347280334728, 6: 0.303886925795053, 7: 0.0, 9: 0.9230769230769231, 10: 0.39195979899497485, 11: 0.15311004784688995, 12: 0.16289592760180996, 13: 0.02857142857142857, 15: 0.35555555555555557, 16: 0.37593984962406013, 17: 0.2, 18: 0.11428571428571428, 19: 0.4224137931034483, 21: 0.0, 23: 0.6829268292682927, 24: 0.14814814814814814, 25: 0.42424242424242425, 26: 0.66, 27: 0.18181818181818182, 28: 0.3333333333333333, 29: 0.7295081967213115, 31: 0.0, 32: 0.6557377049180327, 35: 0.2608695652173913, 37: 0.22772277227722773, 38: 0.15789473684210525, 39: 0.0, 40: 0.3333333333333333}
Micro-average F1 score: 0.427063599458728
Weighted-average F1 score: 0.4071201433944781
F1 score per class: {0: 0.37433155080213903, 2: 0.051470588235294115, 4: 0.9514563106796117, 5: 0.5420054200542005, 6: 0.2849740932642487, 7: 0.061224489795918366, 9: 0.6944444444444444, 10: 0.31095406360424027, 11: 0.2678983833718245, 12: 0.1822721598002497, 13: 0.025974025974025976, 15: 0.2553191489361702, 16: 0.36024844720496896, 17: 0.2962962962962963, 18: 0.0821917808219178, 19: 0.4578313253012048, 21: 0.12099644128113879, 23: 0.5060240963855421, 24: 0.09375, 25: 0.5526315789473685, 26: 0.6052631578947368, 27: 0.11475409836065574, 28: 0.14285714285714285, 29: 0.6992481203007519, 31: 0.03773584905660377, 32: 0.6590909090909091, 35: 0.2440318302387268, 37: 0.10443037974683544, 38: 0.16071428571428573, 39: 0.05714285714285714, 40: 0.3707865168539326}
Micro-average F1 score: 0.3076703328095986
Weighted-average F1 score: 0.2749584636504059
F1 score per class: {0: 0.3888888888888889, 2: 0.05054151624548736, 4: 0.9463414634146341, 5: 0.5617977528089888, 6: 0.28940568475452194, 7: 0.06521739130434782, 9: 0.746268656716418, 10: 0.3215434083601286, 11: 0.21364985163204747, 12: 0.1834625322997416, 13: 0.018018018018018018, 15: 0.24615384615384617, 16: 0.3558282208588957, 17: 0.32, 18: 0.09090909090909091, 19: 0.4781144781144781, 21: 0.12080536912751678, 23: 0.5205479452054794, 24: 0.14285714285714285, 25: 0.5454545454545454, 26: 0.6133333333333333, 27: 0.125, 28: 0.22857142857142856, 29: 0.7121212121212122, 31: 0.0425531914893617, 32: 0.6541353383458647, 35: 0.26058631921824105, 37: 0.0995850622406639, 38: 0.13043478260869565, 39: 0.0, 40: 0.3905325443786982}
Micro-average F1 score: 0.31099911058404983
Weighted-average F1 score: 0.27644718950254327
cur_acc:  ['0.6790', '0.3810', '0.5304', '0.5614', '0.3190', '0.1781']
his_acc:  ['0.6790', '0.5498', '0.5228', '0.5467', '0.4754', '0.4271']
cur_acc des:  ['0.6325', '0.3407', '0.4389', '0.4051', '0.3464', '0.2621']
his_acc des:  ['0.6325', '0.4555', '0.4269', '0.4203', '0.3663', '0.3077']
cur_acc rrf:  ['0.6325', '0.3450', '0.4515', '0.4131', '0.3367', '0.2280']
his_acc rrf:  ['0.6325', '0.4627', '0.4357', '0.4327', '0.3723', '0.3110']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 71.7659419CurrentTrain: epoch  0, batch     1 | loss: 68.2839812CurrentTrain: epoch  0, batch     2 | loss: 193.5965770CurrentTrain: epoch  0, batch     3 | loss: 63.6026525CurrentTrain: epoch  0, batch     4 | loss: 112.3830362CurrentTrain: epoch  1, batch     0 | loss: 186.4164399CurrentTrain: epoch  1, batch     1 | loss: 91.2597059CurrentTrain: epoch  1, batch     2 | loss: 93.5309273CurrentTrain: epoch  1, batch     3 | loss: 62.7948208CurrentTrain: epoch  1, batch     4 | loss: 42.0199201CurrentTrain: epoch  2, batch     0 | loss: 71.4283013CurrentTrain: epoch  2, batch     1 | loss: 71.0130523CurrentTrain: epoch  2, batch     2 | loss: 92.2447657CurrentTrain: epoch  2, batch     3 | loss: 87.2878583CurrentTrain: epoch  2, batch     4 | loss: 70.0354578CurrentTrain: epoch  3, batch     0 | loss: 68.7046906CurrentTrain: epoch  3, batch     1 | loss: 58.9458392CurrentTrain: epoch  3, batch     2 | loss: 69.6896127CurrentTrain: epoch  3, batch     3 | loss: 88.3154929CurrentTrain: epoch  3, batch     4 | loss: 49.2590651CurrentTrain: epoch  4, batch     0 | loss: 69.0118515CurrentTrain: epoch  4, batch     1 | loss: 70.8372905CurrentTrain: epoch  4, batch     2 | loss: 67.3262697CurrentTrain: epoch  4, batch     3 | loss: 71.4556104CurrentTrain: epoch  4, batch     4 | loss: 46.3844580CurrentTrain: epoch  5, batch     0 | loss: 87.0350046CurrentTrain: epoch  5, batch     1 | loss: 116.6192602CurrentTrain: epoch  5, batch     2 | loss: 84.7968786CurrentTrain: epoch  5, batch     3 | loss: 67.8672447CurrentTrain: epoch  5, batch     4 | loss: 28.8902087CurrentTrain: epoch  6, batch     0 | loss: 54.6798615CurrentTrain: epoch  6, batch     1 | loss: 67.6653128CurrentTrain: epoch  6, batch     2 | loss: 67.4504825CurrentTrain: epoch  6, batch     3 | loss: 65.6660664CurrentTrain: epoch  6, batch     4 | loss: 103.1048048CurrentTrain: epoch  7, batch     0 | loss: 65.0149049CurrentTrain: epoch  7, batch     1 | loss: 80.3302447CurrentTrain: epoch  7, batch     2 | loss: 182.7985870CurrentTrain: epoch  7, batch     3 | loss: 66.1615239CurrentTrain: epoch  7, batch     4 | loss: 46.7222171CurrentTrain: epoch  8, batch     0 | loss: 55.6408558CurrentTrain: epoch  8, batch     1 | loss: 179.5621905CurrentTrain: epoch  8, batch     2 | loss: 83.4335629CurrentTrain: epoch  8, batch     3 | loss: 85.0430221CurrentTrain: epoch  8, batch     4 | loss: 33.4222171CurrentTrain: epoch  9, batch     0 | loss: 67.8965425CurrentTrain: epoch  9, batch     1 | loss: 49.8829093CurrentTrain: epoch  9, batch     2 | loss: 67.6543469CurrentTrain: epoch  9, batch     3 | loss: 84.5668634CurrentTrain: epoch  9, batch     4 | loss: 65.4755601
MemoryTrain:  epoch  0, batch     0 | loss: 0.4809363MemoryTrain:  epoch  1, batch     0 | loss: 0.4471704MemoryTrain:  epoch  2, batch     0 | loss: 0.3565683MemoryTrain:  epoch  3, batch     0 | loss: 0.2863556MemoryTrain:  epoch  4, batch     0 | loss: 0.2660219MemoryTrain:  epoch  5, batch     0 | loss: 0.2052078MemoryTrain:  epoch  6, batch     0 | loss: 0.1909043MemoryTrain:  epoch  7, batch     0 | loss: 0.1474340MemoryTrain:  epoch  8, batch     0 | loss: 0.1234441MemoryTrain:  epoch  9, batch     0 | loss: 0.0996174

F1 score per class: {0: 0.0, 1: 0.17964071856287425, 2: 0.0, 3: 0.4090909090909091, 5: 0.0, 6: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.11764705882352941, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.4608695652173913, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5631067961165048, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.24134871339840283
Weighted-average F1 score: 0.18647755852369036
F1 score per class: {0: 0.0, 1: 0.1568627450980392, 2: 0.0, 3: 0.34065934065934067, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.05319148936170213, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.43425076452599387, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5454545454545454, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.17462039045553146
Weighted-average F1 score: 0.14020054929286838
F1 score per class: {0: 0.0, 1: 0.15384615384615385, 2: 0.0, 3: 0.35071090047393366, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.058823529411764705, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.44936708860759494, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5605095541401274, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.19387755102040816
Weighted-average F1 score: 0.1597893611751659

F1 score per class: {0: 0.5333333333333333, 1: 0.125, 2: 0.25925925925925924, 3: 0.2967032967032967, 4: 0.945273631840796, 5: 0.8260869565217391, 6: 0.28, 7: 0.0, 9: 0.9230769230769231, 10: 0.34146341463414637, 11: 0.09433962264150944, 12: 0.1784037558685446, 13: 0.031746031746031744, 14: 0.07947019867549669, 15: 0.48484848484848486, 16: 0.4528301886792453, 17: 0.0, 18: 0.13114754098360656, 19: 0.3263157894736842, 21: 0.0, 22: 0.3897058823529412, 23: 0.5974025974025974, 24: 0.0, 25: 0.4927536231884058, 26: 0.6407766990291263, 27: 0.028985507246376812, 28: 0.25, 29: 0.7044534412955465, 31: 0.0, 32: 0.44086021505376344, 34: 0.18354430379746836, 35: 0.15384615384615385, 37: 0.23076923076923078, 38: 0.13157894736842105, 39: 0.13333333333333333, 40: 0.4036697247706422}
Micro-average F1 score: 0.35628227194492257
Weighted-average F1 score: 0.33196607930570843
F1 score per class: {0: 0.4429530201342282, 1: 0.08759124087591241, 2: 0.06930693069306931, 3: 0.1751412429378531, 4: 0.9423076923076923, 5: 0.5847953216374269, 6: 0.26900584795321636, 7: 0.04040404040404041, 9: 0.7936507936507936, 10: 0.36678200692041524, 11: 0.26905829596412556, 12: 0.20118343195266272, 13: 0.012738853503184714, 14: 0.02570694087403599, 15: 0.35294117647058826, 16: 0.4057971014492754, 17: 0.24242424242424243, 18: 0.0979020979020979, 19: 0.36627906976744184, 21: 0.049689440993788817, 22: 0.3279445727482679, 23: 0.5454545454545454, 24: 0.10526315789473684, 25: 0.56, 26: 0.6, 27: 0.05454545454545454, 28: 0.18181818181818182, 29: 0.657243816254417, 31: 0.0, 32: 0.532051282051282, 34: 0.15272727272727274, 35: 0.16990291262135923, 37: 0.04792332268370607, 38: 0.1935483870967742, 39: 0.1111111111111111, 40: 0.4120603015075377}
Micro-average F1 score: 0.2726949092203631
Weighted-average F1 score: 0.2440395334745712
F1 score per class: {0: 0.4533333333333333, 1: 0.0858085808580858, 2: 0.06829268292682927, 3: 0.16856492027334852, 4: 0.9468599033816425, 5: 0.5850746268656717, 6: 0.2752808988764045, 7: 0.02040816326530612, 9: 0.819672131147541, 10: 0.36482084690553745, 11: 0.2057142857142857, 12: 0.2057142857142857, 13: 0.0196078431372549, 14: 0.02962962962962963, 15: 0.3111111111111111, 16: 0.4142857142857143, 17: 0.21052631578947367, 18: 0.1, 19: 0.38, 21: 0.0, 22: 0.3480392156862745, 23: 0.4675324675324675, 24: 0.13333333333333333, 25: 0.5316455696202531, 26: 0.6052631578947368, 27: 0.0594059405940594, 28: 0.2857142857142857, 29: 0.6456140350877193, 31: 0.0, 32: 0.5454545454545454, 34: 0.1474036850921273, 35: 0.13043478260869565, 37: 0.05628517823639775, 38: 0.1875, 39: 0.10526315789473684, 40: 0.43243243243243246}
Micro-average F1 score: 0.2768312398955354
Weighted-average F1 score: 0.24859722713804877
cur_acc:  ['0.6790', '0.3810', '0.5304', '0.5614', '0.3190', '0.1781', '0.2413']
his_acc:  ['0.6790', '0.5498', '0.5228', '0.5467', '0.4754', '0.4271', '0.3563']
cur_acc des:  ['0.6325', '0.3407', '0.4389', '0.4051', '0.3464', '0.2621', '0.1746']
his_acc des:  ['0.6325', '0.4555', '0.4269', '0.4203', '0.3663', '0.3077', '0.2727']
cur_acc rrf:  ['0.6325', '0.3450', '0.4515', '0.4131', '0.3367', '0.2280', '0.1939']
his_acc rrf:  ['0.6325', '0.4627', '0.4357', '0.4327', '0.3723', '0.3110', '0.2768']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 61.0557137CurrentTrain: epoch  0, batch     1 | loss: 122.7589011CurrentTrain: epoch  0, batch     2 | loss: 74.8515712CurrentTrain: epoch  0, batch     3 | loss: 44.6588731CurrentTrain: epoch  1, batch     0 | loss: 73.0749200CurrentTrain: epoch  1, batch     1 | loss: 71.7702441CurrentTrain: epoch  1, batch     2 | loss: 88.6128650CurrentTrain: epoch  1, batch     3 | loss: 45.6883639CurrentTrain: epoch  2, batch     0 | loss: 126.7202792CurrentTrain: epoch  2, batch     1 | loss: 66.9579647CurrentTrain: epoch  2, batch     2 | loss: 54.8967870CurrentTrain: epoch  2, batch     3 | loss: 38.3420135CurrentTrain: epoch  3, batch     0 | loss: 88.5813916CurrentTrain: epoch  3, batch     1 | loss: 85.9493164CurrentTrain: epoch  3, batch     2 | loss: 51.9248335CurrentTrain: epoch  3, batch     3 | loss: 40.2679385CurrentTrain: epoch  4, batch     0 | loss: 63.5457373CurrentTrain: epoch  4, batch     1 | loss: 54.1198769CurrentTrain: epoch  4, batch     2 | loss: 118.5979533CurrentTrain: epoch  4, batch     3 | loss: 47.9602111CurrentTrain: epoch  5, batch     0 | loss: 63.5436063CurrentTrain: epoch  5, batch     1 | loss: 114.5799030CurrentTrain: epoch  5, batch     2 | loss: 61.7138657CurrentTrain: epoch  5, batch     3 | loss: 53.0250968CurrentTrain: epoch  6, batch     0 | loss: 51.9536148CurrentTrain: epoch  6, batch     1 | loss: 84.9979031CurrentTrain: epoch  6, batch     2 | loss: 51.4837451CurrentTrain: epoch  6, batch     3 | loss: 51.7349914CurrentTrain: epoch  7, batch     0 | loss: 51.4852518CurrentTrain: epoch  7, batch     1 | loss: 65.0485436CurrentTrain: epoch  7, batch     2 | loss: 64.8512093CurrentTrain: epoch  7, batch     3 | loss: 49.2035682CurrentTrain: epoch  8, batch     0 | loss: 59.4117486CurrentTrain: epoch  8, batch     1 | loss: 182.9028021CurrentTrain: epoch  8, batch     2 | loss: 62.7341807CurrentTrain: epoch  8, batch     3 | loss: 38.2343219CurrentTrain: epoch  9, batch     0 | loss: 59.8367803CurrentTrain: epoch  9, batch     1 | loss: 113.6884004CurrentTrain: epoch  9, batch     2 | loss: 52.2786541CurrentTrain: epoch  9, batch     3 | loss: 50.2211588
MemoryTrain:  epoch  0, batch     0 | loss: 0.1811029MemoryTrain:  epoch  1, batch     0 | loss: 0.2073689MemoryTrain:  epoch  2, batch     0 | loss: 0.1497230MemoryTrain:  epoch  3, batch     0 | loss: 0.1293478MemoryTrain:  epoch  4, batch     0 | loss: 0.1085484MemoryTrain:  epoch  5, batch     0 | loss: 0.0990022MemoryTrain:  epoch  6, batch     0 | loss: 0.0895206MemoryTrain:  epoch  7, batch     0 | loss: 0.0744662MemoryTrain:  epoch  8, batch     0 | loss: 0.0645285MemoryTrain:  epoch  9, batch     0 | loss: 0.0605884

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.13793103448275862, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.6666666666666666, 22: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 31: 0.0, 32: 0.0, 33: 0.16666666666666666, 34: 0.0, 35: 0.0, 36: 0.4631578947368421, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.32421875
Weighted-average F1 score: 0.24177702965994716
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5826771653543307, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5641025641025641, 21: 0.0, 22: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 31: 0.0, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 35: 0.0, 36: 0.6666666666666666, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.310107948969578
Weighted-average F1 score: 0.21810650626171885
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.42201834862385323, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5660377358490566, 21: 0.0, 22: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 31: 0.0, 32: 0.0, 33: 0.35294117647058826, 34: 0.0, 35: 0.0, 36: 0.6335403726708074, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.29012345679012347
Weighted-average F1 score: 0.20587646219725922

F1 score per class: {0: 0.5, 1: 0.11255411255411256, 2: 0.23333333333333334, 3: 0.22105263157894736, 4: 0.9556650246305419, 5: 0.7300380228136882, 6: 0.24107142857142858, 7: 0.0, 8: 0.1276595744680851, 9: 0.8275862068965517, 10: 0.2787878787878788, 11: 0.28125, 12: 0.14953271028037382, 13: 0.025, 14: 0.0650887573964497, 15: 0.5161290322580645, 16: 0.3401360544217687, 17: 0.0, 18: 0.12903225806451613, 19: 0.35714285714285715, 20: 0.2975206611570248, 21: 0.0, 22: 0.4496124031007752, 23: 0.5647058823529412, 24: 0.0, 25: 0.463768115942029, 26: 0.5919282511210763, 27: 0.041666666666666664, 28: 0.09090909090909091, 29: 0.6564885496183206, 30: 0.972972972972973, 31: 0.05714285714285714, 32: 0.33136094674556216, 33: 0.15384615384615385, 34: 0.24342105263157895, 35: 0.16483516483516483, 36: 0.41509433962264153, 37: 0.21359223300970873, 38: 0.2127659574468085, 39: 0.13333333333333333, 40: 0.42748091603053434}
Micro-average F1 score: 0.33896437752478886
Weighted-average F1 score: 0.31891280976297715
F1 score per class: {0: 0.40764331210191085, 1: 0.10625, 2: 0.07909604519774012, 3: 0.17419354838709677, 4: 0.9142857142857143, 5: 0.3883495145631068, 6: 0.2620320855614973, 7: 0.0, 8: 0.31759656652360513, 9: 0.7142857142857143, 10: 0.30120481927710846, 11: 0.26262626262626265, 12: 0.18072289156626506, 13: 0.013245033112582781, 14: 0.047619047619047616, 15: 0.3076923076923077, 16: 0.3373493975903614, 17: 0.2222222222222222, 18: 0.12162162162162163, 19: 0.3469387755102041, 20: 0.18526315789473685, 21: 0.056179775280898875, 22: 0.4013377926421405, 23: 0.48, 24: 0.0, 25: 0.5411764705882353, 26: 0.5702479338842975, 27: 0.04048582995951417, 28: 0.16666666666666666, 29: 0.5844155844155844, 30: 0.7450980392156863, 31: 0.024844720496894408, 32: 0.5104895104895105, 33: 0.10526315789473684, 34: 0.18281535648994515, 35: 0.152, 36: 0.4230769230769231, 37: 0.07926829268292683, 38: 0.22388059701492538, 39: 0.1111111111111111, 40: 0.3665338645418327}
Micro-average F1 score: 0.2699792960662526
Weighted-average F1 score: 0.24863551829881464
F1 score per class: {0: 0.41025641025641024, 1: 0.10397553516819572, 2: 0.08433734939759036, 3: 0.15748031496062992, 4: 0.9230769230769231, 5: 0.3992015968063872, 6: 0.2552083333333333, 7: 0.0, 8: 0.27710843373493976, 9: 0.7575757575757576, 10: 0.3107344632768362, 11: 0.3064516129032258, 12: 0.1804733727810651, 13: 0.029197080291970802, 14: 0.05673758865248227, 15: 0.27906976744186046, 16: 0.33532934131736525, 17: 0.16666666666666666, 18: 0.10526315789473684, 19: 0.3693181818181818, 20: 0.18218623481781376, 21: 0.08130081300813008, 22: 0.425531914893617, 23: 0.46153846153846156, 24: 0.0, 25: 0.4943820224719101, 26: 0.5726141078838174, 27: 0.04219409282700422, 28: 0.18181818181818182, 29: 0.5705329153605015, 30: 0.7755102040816326, 31: 0.03361344537815126, 32: 0.5126353790613718, 33: 0.10909090909090909, 34: 0.17555938037865748, 35: 0.1242603550295858, 36: 0.4163265306122449, 37: 0.09774436090225563, 38: 0.20270270270270271, 39: 0.10526315789473684, 40: 0.37446808510638296}
Micro-average F1 score: 0.274025974025974
Weighted-average F1 score: 0.2523052729003143
cur_acc:  ['0.6790', '0.3810', '0.5304', '0.5614', '0.3190', '0.1781', '0.2413', '0.3242']
his_acc:  ['0.6790', '0.5498', '0.5228', '0.5467', '0.4754', '0.4271', '0.3563', '0.3390']
cur_acc des:  ['0.6325', '0.3407', '0.4389', '0.4051', '0.3464', '0.2621', '0.1746', '0.3101']
his_acc des:  ['0.6325', '0.4555', '0.4269', '0.4203', '0.3663', '0.3077', '0.2727', '0.2700']
cur_acc rrf:  ['0.6325', '0.3450', '0.4515', '0.4131', '0.3367', '0.2280', '0.1939', '0.2901']
his_acc rrf:  ['0.6325', '0.4627', '0.4357', '0.4327', '0.3723', '0.3110', '0.2768', '0.2740']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 69.1642505CurrentTrain: epoch  0, batch     1 | loss: 97.4475552CurrentTrain: epoch  0, batch     2 | loss: 66.6252826CurrentTrain: epoch  0, batch     3 | loss: 66.8197700CurrentTrain: epoch  0, batch     4 | loss: 129.2105250CurrentTrain: epoch  0, batch     5 | loss: 80.9635231CurrentTrain: epoch  0, batch     6 | loss: 78.7940723CurrentTrain: epoch  0, batch     7 | loss: 96.7864467CurrentTrain: epoch  0, batch     8 | loss: 127.6713631CurrentTrain: epoch  0, batch     9 | loss: 78.9543774CurrentTrain: epoch  0, batch    10 | loss: 57.4096621CurrentTrain: epoch  0, batch    11 | loss: 127.3485724CurrentTrain: epoch  0, batch    12 | loss: 64.8400455CurrentTrain: epoch  0, batch    13 | loss: 56.4352568CurrentTrain: epoch  0, batch    14 | loss: 77.5580165CurrentTrain: epoch  0, batch    15 | loss: 65.7866470CurrentTrain: epoch  0, batch    16 | loss: 96.3525212CurrentTrain: epoch  0, batch    17 | loss: 95.8923427CurrentTrain: epoch  0, batch    18 | loss: 78.8644803CurrentTrain: epoch  0, batch    19 | loss: 96.4610349CurrentTrain: epoch  0, batch    20 | loss: 77.8870804CurrentTrain: epoch  0, batch    21 | loss: 94.8036965CurrentTrain: epoch  0, batch    22 | loss: 64.8673331CurrentTrain: epoch  0, batch    23 | loss: 77.6655799CurrentTrain: epoch  0, batch    24 | loss: 65.4802900CurrentTrain: epoch  0, batch    25 | loss: 64.4180219CurrentTrain: epoch  0, batch    26 | loss: 55.7043253CurrentTrain: epoch  0, batch    27 | loss: 125.4447646CurrentTrain: epoch  0, batch    28 | loss: 126.4021778CurrentTrain: epoch  0, batch    29 | loss: 77.3941216CurrentTrain: epoch  0, batch    30 | loss: 76.7350602CurrentTrain: epoch  0, batch    31 | loss: 55.8355711CurrentTrain: epoch  0, batch    32 | loss: 55.7120816CurrentTrain: epoch  0, batch    33 | loss: 95.9956796CurrentTrain: epoch  0, batch    34 | loss: 77.3427548CurrentTrain: epoch  0, batch    35 | loss: 64.4611023CurrentTrain: epoch  0, batch    36 | loss: 95.6142340CurrentTrain: epoch  0, batch    37 | loss: 76.4505072CurrentTrain: epoch  0, batch    38 | loss: 76.8417280CurrentTrain: epoch  0, batch    39 | loss: 94.6168512CurrentTrain: epoch  0, batch    40 | loss: 95.1656341CurrentTrain: epoch  0, batch    41 | loss: 76.4977588CurrentTrain: epoch  0, batch    42 | loss: 76.1001664CurrentTrain: epoch  0, batch    43 | loss: 94.7595753CurrentTrain: epoch  0, batch    44 | loss: 64.6408654CurrentTrain: epoch  0, batch    45 | loss: 64.7727036CurrentTrain: epoch  0, batch    46 | loss: 64.0640044CurrentTrain: epoch  0, batch    47 | loss: 76.2321831CurrentTrain: epoch  0, batch    48 | loss: 95.3900641CurrentTrain: epoch  0, batch    49 | loss: 124.7523175CurrentTrain: epoch  0, batch    50 | loss: 76.7130681CurrentTrain: epoch  0, batch    51 | loss: 77.6670466CurrentTrain: epoch  0, batch    52 | loss: 125.4279224CurrentTrain: epoch  0, batch    53 | loss: 186.9159896CurrentTrain: epoch  0, batch    54 | loss: 63.7312264CurrentTrain: epoch  0, batch    55 | loss: 124.9478964CurrentTrain: epoch  0, batch    56 | loss: 76.3098859CurrentTrain: epoch  0, batch    57 | loss: 54.8231814CurrentTrain: epoch  0, batch    58 | loss: 75.1914281CurrentTrain: epoch  0, batch    59 | loss: 126.2414933CurrentTrain: epoch  0, batch    60 | loss: 75.6217428CurrentTrain: epoch  0, batch    61 | loss: 93.9781414CurrentTrain: epoch  0, batch    62 | loss: 76.1529201CurrentTrain: epoch  0, batch    63 | loss: 76.4421708CurrentTrain: epoch  0, batch    64 | loss: 63.3766948CurrentTrain: epoch  0, batch    65 | loss: 94.8729481CurrentTrain: epoch  0, batch    66 | loss: 55.1888148CurrentTrain: epoch  0, batch    67 | loss: 94.2148727CurrentTrain: epoch  0, batch    68 | loss: 93.7929774CurrentTrain: epoch  0, batch    69 | loss: 96.3823945CurrentTrain: epoch  0, batch    70 | loss: 127.1622632CurrentTrain: epoch  0, batch    71 | loss: 63.2251296CurrentTrain: epoch  0, batch    72 | loss: 63.6941890CurrentTrain: epoch  0, batch    73 | loss: 63.9401959CurrentTrain: epoch  0, batch    74 | loss: 63.9460976CurrentTrain: epoch  0, batch    75 | loss: 54.0221095CurrentTrain: epoch  0, batch    76 | loss: 75.0593877CurrentTrain: epoch  0, batch    77 | loss: 75.6291784CurrentTrain: epoch  0, batch    78 | loss: 74.7475577CurrentTrain: epoch  0, batch    79 | loss: 61.5150736CurrentTrain: epoch  0, batch    80 | loss: 75.3901229CurrentTrain: epoch  0, batch    81 | loss: 64.8553811CurrentTrain: epoch  0, batch    82 | loss: 61.4218611CurrentTrain: epoch  0, batch    83 | loss: 91.9288088CurrentTrain: epoch  0, batch    84 | loss: 93.2227809CurrentTrain: epoch  0, batch    85 | loss: 61.5549798CurrentTrain: epoch  0, batch    86 | loss: 62.6576731CurrentTrain: epoch  0, batch    87 | loss: 72.2083680CurrentTrain: epoch  0, batch    88 | loss: 121.6673853CurrentTrain: epoch  0, batch    89 | loss: 91.2002057CurrentTrain: epoch  0, batch    90 | loss: 62.6921710CurrentTrain: epoch  0, batch    91 | loss: 60.3287089CurrentTrain: epoch  0, batch    92 | loss: 74.8597791CurrentTrain: epoch  0, batch    93 | loss: 61.4738517CurrentTrain: epoch  0, batch    94 | loss: 73.2063796CurrentTrain: epoch  0, batch    95 | loss: 62.3891893CurrentTrain: epoch  1, batch     0 | loss: 69.6707953CurrentTrain: epoch  1, batch     1 | loss: 61.4670052CurrentTrain: epoch  1, batch     2 | loss: 91.4098001CurrentTrain: epoch  1, batch     3 | loss: 74.5941059CurrentTrain: epoch  1, batch     4 | loss: 118.8828038CurrentTrain: epoch  1, batch     5 | loss: 51.5879055CurrentTrain: epoch  1, batch     6 | loss: 60.4982999CurrentTrain: epoch  1, batch     7 | loss: 57.6527083CurrentTrain: epoch  1, batch     8 | loss: 74.4789089CurrentTrain: epoch  1, batch     9 | loss: 72.6085720CurrentTrain: epoch  1, batch    10 | loss: 70.5286659CurrentTrain: epoch  1, batch    11 | loss: 88.8417309CurrentTrain: epoch  1, batch    12 | loss: 50.9048968CurrentTrain: epoch  1, batch    13 | loss: 72.0416955CurrentTrain: epoch  1, batch    14 | loss: 71.5391931CurrentTrain: epoch  1, batch    15 | loss: 92.1153370CurrentTrain: epoch  1, batch    16 | loss: 92.3615422CurrentTrain: epoch  1, batch    17 | loss: 58.5788321CurrentTrain: epoch  1, batch    18 | loss: 58.1183790CurrentTrain: epoch  1, batch    19 | loss: 53.7663789CurrentTrain: epoch  1, batch    20 | loss: 71.8730022CurrentTrain: epoch  1, batch    21 | loss: 56.6939993CurrentTrain: epoch  1, batch    22 | loss: 57.0326793CurrentTrain: epoch  1, batch    23 | loss: 60.4001300CurrentTrain: epoch  1, batch    24 | loss: 92.8386571CurrentTrain: epoch  1, batch    25 | loss: 68.9285635CurrentTrain: epoch  1, batch    26 | loss: 92.5928380CurrentTrain: epoch  1, batch    27 | loss: 90.2141833CurrentTrain: epoch  1, batch    28 | loss: 71.5080712CurrentTrain: epoch  1, batch    29 | loss: 73.1888051CurrentTrain: epoch  1, batch    30 | loss: 70.1041782CurrentTrain: epoch  1, batch    31 | loss: 124.3399857CurrentTrain: epoch  1, batch    32 | loss: 68.6237754CurrentTrain: epoch  1, batch    33 | loss: 88.4454718CurrentTrain: epoch  1, batch    34 | loss: 86.6874636CurrentTrain: epoch  1, batch    35 | loss: 89.2258465CurrentTrain: epoch  1, batch    36 | loss: 91.3417609CurrentTrain: epoch  1, batch    37 | loss: 91.5413373CurrentTrain: epoch  1, batch    38 | loss: 94.7927306CurrentTrain: epoch  1, batch    39 | loss: 71.5846010CurrentTrain: epoch  1, batch    40 | loss: 57.7546069CurrentTrain: epoch  1, batch    41 | loss: 59.8289249CurrentTrain: epoch  1, batch    42 | loss: 56.5142790CurrentTrain: epoch  1, batch    43 | loss: 187.4190874CurrentTrain: epoch  1, batch    44 | loss: 60.3245891CurrentTrain: epoch  1, batch    45 | loss: 58.5442749CurrentTrain: epoch  1, batch    46 | loss: 90.0969412CurrentTrain: epoch  1, batch    47 | loss: 71.8602727CurrentTrain: epoch  1, batch    48 | loss: 70.0054807CurrentTrain: epoch  1, batch    49 | loss: 91.5915389CurrentTrain: epoch  1, batch    50 | loss: 60.5916792CurrentTrain: epoch  1, batch    51 | loss: 58.3831332CurrentTrain: epoch  1, batch    52 | loss: 57.8499104CurrentTrain: epoch  1, batch    53 | loss: 72.1506607CurrentTrain: epoch  1, batch    54 | loss: 70.6840495CurrentTrain: epoch  1, batch    55 | loss: 68.0644231CurrentTrain: epoch  1, batch    56 | loss: 60.9049976CurrentTrain: epoch  1, batch    57 | loss: 60.3440176CurrentTrain: epoch  1, batch    58 | loss: 89.3125937CurrentTrain: epoch  1, batch    59 | loss: 57.9658367CurrentTrain: epoch  1, batch    60 | loss: 58.8512965CurrentTrain: epoch  1, batch    61 | loss: 87.4427720CurrentTrain: epoch  1, batch    62 | loss: 61.5585841CurrentTrain: epoch  1, batch    63 | loss: 188.3407908CurrentTrain: epoch  1, batch    64 | loss: 85.9956754CurrentTrain: epoch  1, batch    65 | loss: 88.8694397CurrentTrain: epoch  1, batch    66 | loss: 70.2271285CurrentTrain: epoch  1, batch    67 | loss: 68.1290646CurrentTrain: epoch  1, batch    68 | loss: 86.7755167CurrentTrain: epoch  1, batch    69 | loss: 69.7283318CurrentTrain: epoch  1, batch    70 | loss: 58.1671018CurrentTrain: epoch  1, batch    71 | loss: 59.6604477CurrentTrain: epoch  1, batch    72 | loss: 70.6719314CurrentTrain: epoch  1, batch    73 | loss: 62.2163141CurrentTrain: epoch  1, batch    74 | loss: 87.8945146CurrentTrain: epoch  1, batch    75 | loss: 55.3602180CurrentTrain: epoch  1, batch    76 | loss: 87.4222928CurrentTrain: epoch  1, batch    77 | loss: 69.8088482CurrentTrain: epoch  1, batch    78 | loss: 67.8478290CurrentTrain: epoch  1, batch    79 | loss: 64.8406609CurrentTrain: epoch  1, batch    80 | loss: 87.5295617CurrentTrain: epoch  1, batch    81 | loss: 55.9583135CurrentTrain: epoch  1, batch    82 | loss: 57.6394279CurrentTrain: epoch  1, batch    83 | loss: 72.0935147CurrentTrain: epoch  1, batch    84 | loss: 126.2895006CurrentTrain: epoch  1, batch    85 | loss: 73.5251723CurrentTrain: epoch  1, batch    86 | loss: 69.4865260CurrentTrain: epoch  1, batch    87 | loss: 68.8220966CurrentTrain: epoch  1, batch    88 | loss: 55.5086518CurrentTrain: epoch  1, batch    89 | loss: 54.6012675CurrentTrain: epoch  1, batch    90 | loss: 58.0788103CurrentTrain: epoch  1, batch    91 | loss: 122.9128336CurrentTrain: epoch  1, batch    92 | loss: 59.3078053CurrentTrain: epoch  1, batch    93 | loss: 66.4847439CurrentTrain: epoch  1, batch    94 | loss: 89.1503543CurrentTrain: epoch  1, batch    95 | loss: 59.7547463CurrentTrain: epoch  2, batch     0 | loss: 67.6724866CurrentTrain: epoch  2, batch     1 | loss: 89.1578654CurrentTrain: epoch  2, batch     2 | loss: 88.4576811CurrentTrain: epoch  2, batch     3 | loss: 57.0371998CurrentTrain: epoch  2, batch     4 | loss: 69.4333167CurrentTrain: epoch  2, batch     5 | loss: 54.1027529CurrentTrain: epoch  2, batch     6 | loss: 67.1717044CurrentTrain: epoch  2, batch     7 | loss: 84.8192405CurrentTrain: epoch  2, batch     8 | loss: 85.6816162CurrentTrain: epoch  2, batch     9 | loss: 68.0030305CurrentTrain: epoch  2, batch    10 | loss: 117.5231876CurrentTrain: epoch  2, batch    11 | loss: 56.1422608CurrentTrain: epoch  2, batch    12 | loss: 72.8723571CurrentTrain: epoch  2, batch    13 | loss: 87.1984598CurrentTrain: epoch  2, batch    14 | loss: 70.8994675CurrentTrain: epoch  2, batch    15 | loss: 65.9256670CurrentTrain: epoch  2, batch    16 | loss: 119.0229208CurrentTrain: epoch  2, batch    17 | loss: 54.9270563CurrentTrain: epoch  2, batch    18 | loss: 70.7488815CurrentTrain: epoch  2, batch    19 | loss: 54.3441651CurrentTrain: epoch  2, batch    20 | loss: 82.0718299CurrentTrain: epoch  2, batch    21 | loss: 51.2057831CurrentTrain: epoch  2, batch    22 | loss: 117.9353422CurrentTrain: epoch  2, batch    23 | loss: 59.9002493CurrentTrain: epoch  2, batch    24 | loss: 55.6548141CurrentTrain: epoch  2, batch    25 | loss: 89.0329102CurrentTrain: epoch  2, batch    26 | loss: 54.7134873CurrentTrain: epoch  2, batch    27 | loss: 72.9222194CurrentTrain: epoch  2, batch    28 | loss: 84.8415336CurrentTrain: epoch  2, batch    29 | loss: 126.2210341CurrentTrain: epoch  2, batch    30 | loss: 54.2457082CurrentTrain: epoch  2, batch    31 | loss: 54.3750809CurrentTrain: epoch  2, batch    32 | loss: 69.5985362CurrentTrain: epoch  2, batch    33 | loss: 115.9827498CurrentTrain: epoch  2, batch    34 | loss: 53.9987834CurrentTrain: epoch  2, batch    35 | loss: 54.2516519CurrentTrain: epoch  2, batch    36 | loss: 86.3590237CurrentTrain: epoch  2, batch    37 | loss: 70.6505223CurrentTrain: epoch  2, batch    38 | loss: 60.6278203CurrentTrain: epoch  2, batch    39 | loss: 69.9830870CurrentTrain: epoch  2, batch    40 | loss: 56.0798995CurrentTrain: epoch  2, batch    41 | loss: 46.0117665CurrentTrain: epoch  2, batch    42 | loss: 88.0458995CurrentTrain: epoch  2, batch    43 | loss: 56.3729394CurrentTrain: epoch  2, batch    44 | loss: 86.5975704CurrentTrain: epoch  2, batch    45 | loss: 54.4363356CurrentTrain: epoch  2, batch    46 | loss: 69.3760974CurrentTrain: epoch  2, batch    47 | loss: 53.4534359CurrentTrain: epoch  2, batch    48 | loss: 60.0567202CurrentTrain: epoch  2, batch    49 | loss: 59.5626254CurrentTrain: epoch  2, batch    50 | loss: 84.9048056CurrentTrain: epoch  2, batch    51 | loss: 55.7633926CurrentTrain: epoch  2, batch    52 | loss: 56.7479962CurrentTrain: epoch  2, batch    53 | loss: 66.5793817CurrentTrain: epoch  2, batch    54 | loss: 70.2904187CurrentTrain: epoch  2, batch    55 | loss: 72.6630200CurrentTrain: epoch  2, batch    56 | loss: 70.9803381CurrentTrain: epoch  2, batch    57 | loss: 89.3227473CurrentTrain: epoch  2, batch    58 | loss: 65.4813652CurrentTrain: epoch  2, batch    59 | loss: 85.3887107CurrentTrain: epoch  2, batch    60 | loss: 46.6370598CurrentTrain: epoch  2, batch    61 | loss: 71.5839530CurrentTrain: epoch  2, batch    62 | loss: 46.0066994CurrentTrain: epoch  2, batch    63 | loss: 56.9519609CurrentTrain: epoch  2, batch    64 | loss: 47.8399440CurrentTrain: epoch  2, batch    65 | loss: 89.6807524CurrentTrain: epoch  2, batch    66 | loss: 87.2194216CurrentTrain: epoch  2, batch    67 | loss: 90.5363583CurrentTrain: epoch  2, batch    68 | loss: 66.2155274CurrentTrain: epoch  2, batch    69 | loss: 88.5991332CurrentTrain: epoch  2, batch    70 | loss: 55.0212744CurrentTrain: epoch  2, batch    71 | loss: 87.4729417CurrentTrain: epoch  2, batch    72 | loss: 116.4908052CurrentTrain: epoch  2, batch    73 | loss: 88.4523515CurrentTrain: epoch  2, batch    74 | loss: 70.3895599CurrentTrain: epoch  2, batch    75 | loss: 71.4086224CurrentTrain: epoch  2, batch    76 | loss: 84.2346285CurrentTrain: epoch  2, batch    77 | loss: 83.2666978CurrentTrain: epoch  2, batch    78 | loss: 67.1922435CurrentTrain: epoch  2, batch    79 | loss: 55.3089327CurrentTrain: epoch  2, batch    80 | loss: 68.2040238CurrentTrain: epoch  2, batch    81 | loss: 67.1274605CurrentTrain: epoch  2, batch    82 | loss: 68.6027760CurrentTrain: epoch  2, batch    83 | loss: 87.1276006CurrentTrain: epoch  2, batch    84 | loss: 71.6189730CurrentTrain: epoch  2, batch    85 | loss: 66.6883172CurrentTrain: epoch  2, batch    86 | loss: 67.5917662CurrentTrain: epoch  2, batch    87 | loss: 86.4330339CurrentTrain: epoch  2, batch    88 | loss: 70.8759945CurrentTrain: epoch  2, batch    89 | loss: 60.0845859CurrentTrain: epoch  2, batch    90 | loss: 55.8699550CurrentTrain: epoch  2, batch    91 | loss: 69.2377824CurrentTrain: epoch  2, batch    92 | loss: 70.6585356CurrentTrain: epoch  2, batch    93 | loss: 83.9041885CurrentTrain: epoch  2, batch    94 | loss: 54.9349014CurrentTrain: epoch  2, batch    95 | loss: 99.8062253CurrentTrain: epoch  3, batch     0 | loss: 55.3776977CurrentTrain: epoch  3, batch     1 | loss: 68.4317270CurrentTrain: epoch  3, batch     2 | loss: 55.1994460CurrentTrain: epoch  3, batch     3 | loss: 64.4775134CurrentTrain: epoch  3, batch     4 | loss: 90.5053004CurrentTrain: epoch  3, batch     5 | loss: 56.9318978CurrentTrain: epoch  3, batch     6 | loss: 53.2003012CurrentTrain: epoch  3, batch     7 | loss: 69.1024755CurrentTrain: epoch  3, batch     8 | loss: 87.6732980CurrentTrain: epoch  3, batch     9 | loss: 68.0124564CurrentTrain: epoch  3, batch    10 | loss: 53.4531934CurrentTrain: epoch  3, batch    11 | loss: 69.3301384CurrentTrain: epoch  3, batch    12 | loss: 45.6044075CurrentTrain: epoch  3, batch    13 | loss: 79.9504134CurrentTrain: epoch  3, batch    14 | loss: 83.3349290CurrentTrain: epoch  3, batch    15 | loss: 69.1975123CurrentTrain: epoch  3, batch    16 | loss: 56.4285252CurrentTrain: epoch  3, batch    17 | loss: 121.2572821CurrentTrain: epoch  3, batch    18 | loss: 46.0887794CurrentTrain: epoch  3, batch    19 | loss: 55.5139013CurrentTrain: epoch  3, batch    20 | loss: 82.9919157CurrentTrain: epoch  3, batch    21 | loss: 47.3590105CurrentTrain: epoch  3, batch    22 | loss: 68.7673204CurrentTrain: epoch  3, batch    23 | loss: 68.8704507CurrentTrain: epoch  3, batch    24 | loss: 65.8509596CurrentTrain: epoch  3, batch    25 | loss: 112.6224366CurrentTrain: epoch  3, batch    26 | loss: 66.7801784CurrentTrain: epoch  3, batch    27 | loss: 87.2206775CurrentTrain: epoch  3, batch    28 | loss: 64.0276786CurrentTrain: epoch  3, batch    29 | loss: 84.1762054CurrentTrain: epoch  3, batch    30 | loss: 52.9968021CurrentTrain: epoch  3, batch    31 | loss: 118.5852537CurrentTrain: epoch  3, batch    32 | loss: 68.7168344CurrentTrain: epoch  3, batch    33 | loss: 56.7777692CurrentTrain: epoch  3, batch    34 | loss: 86.7181893CurrentTrain: epoch  3, batch    35 | loss: 55.9211377CurrentTrain: epoch  3, batch    36 | loss: 53.5392695CurrentTrain: epoch  3, batch    37 | loss: 69.2413014CurrentTrain: epoch  3, batch    38 | loss: 68.2717873CurrentTrain: epoch  3, batch    39 | loss: 57.5013250CurrentTrain: epoch  3, batch    40 | loss: 68.5861899CurrentTrain: epoch  3, batch    41 | loss: 67.4283246CurrentTrain: epoch  3, batch    42 | loss: 90.0874495CurrentTrain: epoch  3, batch    43 | loss: 85.3064326CurrentTrain: epoch  3, batch    44 | loss: 87.7262812CurrentTrain: epoch  3, batch    45 | loss: 67.8899010CurrentTrain: epoch  3, batch    46 | loss: 115.8543387CurrentTrain: epoch  3, batch    47 | loss: 55.6461022CurrentTrain: epoch  3, batch    48 | loss: 67.6806062CurrentTrain: epoch  3, batch    49 | loss: 62.0226334CurrentTrain: epoch  3, batch    50 | loss: 48.7733608CurrentTrain: epoch  3, batch    51 | loss: 65.2377418CurrentTrain: epoch  3, batch    52 | loss: 87.1636400CurrentTrain: epoch  3, batch    53 | loss: 68.1797147CurrentTrain: epoch  3, batch    54 | loss: 53.7063819CurrentTrain: epoch  3, batch    55 | loss: 65.1957464CurrentTrain: epoch  3, batch    56 | loss: 81.6391281CurrentTrain: epoch  3, batch    57 | loss: 52.7079317CurrentTrain: epoch  3, batch    58 | loss: 66.0868050CurrentTrain: epoch  3, batch    59 | loss: 67.2819465CurrentTrain: epoch  3, batch    60 | loss: 52.1828131CurrentTrain: epoch  3, batch    61 | loss: 67.4131570CurrentTrain: epoch  3, batch    62 | loss: 89.0159601CurrentTrain: epoch  3, batch    63 | loss: 70.0880166CurrentTrain: epoch  3, batch    64 | loss: 82.8684642CurrentTrain: epoch  3, batch    65 | loss: 54.8777769CurrentTrain: epoch  3, batch    66 | loss: 52.4569760CurrentTrain: epoch  3, batch    67 | loss: 89.2340838CurrentTrain: epoch  3, batch    68 | loss: 88.3686821CurrentTrain: epoch  3, batch    69 | loss: 67.1246151CurrentTrain: epoch  3, batch    70 | loss: 51.4752154CurrentTrain: epoch  3, batch    71 | loss: 49.8565210CurrentTrain: epoch  3, batch    72 | loss: 86.8568143CurrentTrain: epoch  3, batch    73 | loss: 70.3798148CurrentTrain: epoch  3, batch    74 | loss: 54.6439910CurrentTrain: epoch  3, batch    75 | loss: 121.7271743CurrentTrain: epoch  3, batch    76 | loss: 54.0553825CurrentTrain: epoch  3, batch    77 | loss: 63.5022599CurrentTrain: epoch  3, batch    78 | loss: 55.0768948CurrentTrain: epoch  3, batch    79 | loss: 83.1301652CurrentTrain: epoch  3, batch    80 | loss: 88.0890535CurrentTrain: epoch  3, batch    81 | loss: 86.1828321CurrentTrain: epoch  3, batch    82 | loss: 86.9477883CurrentTrain: epoch  3, batch    83 | loss: 118.1110166CurrentTrain: epoch  3, batch    84 | loss: 86.4881301CurrentTrain: epoch  3, batch    85 | loss: 69.6539007CurrentTrain: epoch  3, batch    86 | loss: 54.1906023CurrentTrain: epoch  3, batch    87 | loss: 65.5331543CurrentTrain: epoch  3, batch    88 | loss: 53.8936812CurrentTrain: epoch  3, batch    89 | loss: 53.1553769CurrentTrain: epoch  3, batch    90 | loss: 55.2726660CurrentTrain: epoch  3, batch    91 | loss: 84.7690069CurrentTrain: epoch  3, batch    92 | loss: 63.1935026CurrentTrain: epoch  3, batch    93 | loss: 53.9270457CurrentTrain: epoch  3, batch    94 | loss: 53.7123637CurrentTrain: epoch  3, batch    95 | loss: 57.6052328CurrentTrain: epoch  4, batch     0 | loss: 48.7169984CurrentTrain: epoch  4, batch     1 | loss: 55.4225430CurrentTrain: epoch  4, batch     2 | loss: 43.2452063CurrentTrain: epoch  4, batch     3 | loss: 86.9716662CurrentTrain: epoch  4, batch     4 | loss: 68.5261803CurrentTrain: epoch  4, batch     5 | loss: 65.6918310CurrentTrain: epoch  4, batch     6 | loss: 119.3487915CurrentTrain: epoch  4, batch     7 | loss: 87.3221047CurrentTrain: epoch  4, batch     8 | loss: 86.2606817CurrentTrain: epoch  4, batch     9 | loss: 118.0927158CurrentTrain: epoch  4, batch    10 | loss: 44.2841299CurrentTrain: epoch  4, batch    11 | loss: 57.9085676CurrentTrain: epoch  4, batch    12 | loss: 65.2220280CurrentTrain: epoch  4, batch    13 | loss: 65.9349652CurrentTrain: epoch  4, batch    14 | loss: 66.7956161CurrentTrain: epoch  4, batch    15 | loss: 64.5276052CurrentTrain: epoch  4, batch    16 | loss: 116.8773946CurrentTrain: epoch  4, batch    17 | loss: 54.9036771CurrentTrain: epoch  4, batch    18 | loss: 83.4155014CurrentTrain: epoch  4, batch    19 | loss: 56.0703851CurrentTrain: epoch  4, batch    20 | loss: 54.6046983CurrentTrain: epoch  4, batch    21 | loss: 58.8729700CurrentTrain: epoch  4, batch    22 | loss: 53.5943474CurrentTrain: epoch  4, batch    23 | loss: 116.3722841CurrentTrain: epoch  4, batch    24 | loss: 67.2492688CurrentTrain: epoch  4, batch    25 | loss: 116.8013525CurrentTrain: epoch  4, batch    26 | loss: 64.1267208CurrentTrain: epoch  4, batch    27 | loss: 52.0960892CurrentTrain: epoch  4, batch    28 | loss: 45.8566513CurrentTrain: epoch  4, batch    29 | loss: 180.0864060CurrentTrain: epoch  4, batch    30 | loss: 54.7460500CurrentTrain: epoch  4, batch    31 | loss: 53.9230090CurrentTrain: epoch  4, batch    32 | loss: 44.7935825CurrentTrain: epoch  4, batch    33 | loss: 85.1974057CurrentTrain: epoch  4, batch    34 | loss: 42.8283034CurrentTrain: epoch  4, batch    35 | loss: 43.2558377CurrentTrain: epoch  4, batch    36 | loss: 117.6131741CurrentTrain: epoch  4, batch    37 | loss: 83.6034324CurrentTrain: epoch  4, batch    38 | loss: 85.6500366CurrentTrain: epoch  4, batch    39 | loss: 85.5914448CurrentTrain: epoch  4, batch    40 | loss: 67.6678460CurrentTrain: epoch  4, batch    41 | loss: 65.0382298CurrentTrain: epoch  4, batch    42 | loss: 44.6991368CurrentTrain: epoch  4, batch    43 | loss: 69.4495337CurrentTrain: epoch  4, batch    44 | loss: 66.9299915CurrentTrain: epoch  4, batch    45 | loss: 54.0493917CurrentTrain: epoch  4, batch    46 | loss: 84.4737700CurrentTrain: epoch  4, batch    47 | loss: 53.7182953CurrentTrain: epoch  4, batch    48 | loss: 45.1819897CurrentTrain: epoch  4, batch    49 | loss: 65.9659165CurrentTrain: epoch  4, batch    50 | loss: 68.0863239CurrentTrain: epoch  4, batch    51 | loss: 88.4117715CurrentTrain: epoch  4, batch    52 | loss: 84.4224779CurrentTrain: epoch  4, batch    53 | loss: 65.4755899CurrentTrain: epoch  4, batch    54 | loss: 64.2498184CurrentTrain: epoch  4, batch    55 | loss: 55.0462917CurrentTrain: epoch  4, batch    56 | loss: 51.6505036CurrentTrain: epoch  4, batch    57 | loss: 64.6575480CurrentTrain: epoch  4, batch    58 | loss: 54.8246505CurrentTrain: epoch  4, batch    59 | loss: 66.5394847CurrentTrain: epoch  4, batch    60 | loss: 53.5052764CurrentTrain: epoch  4, batch    61 | loss: 118.0863679CurrentTrain: epoch  4, batch    62 | loss: 44.2409030CurrentTrain: epoch  4, batch    63 | loss: 85.0062518CurrentTrain: epoch  4, batch    64 | loss: 85.6109833CurrentTrain: epoch  4, batch    65 | loss: 53.9429669CurrentTrain: epoch  4, batch    66 | loss: 67.2451568CurrentTrain: epoch  4, batch    67 | loss: 68.2169314CurrentTrain: epoch  4, batch    68 | loss: 41.0389811CurrentTrain: epoch  4, batch    69 | loss: 80.3189066CurrentTrain: epoch  4, batch    70 | loss: 50.3015469CurrentTrain: epoch  4, batch    71 | loss: 81.7890971CurrentTrain: epoch  4, batch    72 | loss: 53.2146812CurrentTrain: epoch  4, batch    73 | loss: 86.7345034CurrentTrain: epoch  4, batch    74 | loss: 54.0176332CurrentTrain: epoch  4, batch    75 | loss: 65.9568028CurrentTrain: epoch  4, batch    76 | loss: 87.7882522CurrentTrain: epoch  4, batch    77 | loss: 113.6882443CurrentTrain: epoch  4, batch    78 | loss: 68.1229769CurrentTrain: epoch  4, batch    79 | loss: 53.4935316CurrentTrain: epoch  4, batch    80 | loss: 85.0564626CurrentTrain: epoch  4, batch    81 | loss: 52.8477067CurrentTrain: epoch  4, batch    82 | loss: 88.9511862CurrentTrain: epoch  4, batch    83 | loss: 84.1606160CurrentTrain: epoch  4, batch    84 | loss: 67.4836154CurrentTrain: epoch  4, batch    85 | loss: 65.7242162CurrentTrain: epoch  4, batch    86 | loss: 68.1185083CurrentTrain: epoch  4, batch    87 | loss: 64.5440182CurrentTrain: epoch  4, batch    88 | loss: 63.1947107CurrentTrain: epoch  4, batch    89 | loss: 66.8808773CurrentTrain: epoch  4, batch    90 | loss: 86.3693567CurrentTrain: epoch  4, batch    91 | loss: 54.4765880CurrentTrain: epoch  4, batch    92 | loss: 118.7387099CurrentTrain: epoch  4, batch    93 | loss: 87.5738293CurrentTrain: epoch  4, batch    94 | loss: 68.4366941CurrentTrain: epoch  4, batch    95 | loss: 46.6820235CurrentTrain: epoch  5, batch     0 | loss: 52.3838577CurrentTrain: epoch  5, batch     1 | loss: 51.9348970CurrentTrain: epoch  5, batch     2 | loss: 64.6967298CurrentTrain: epoch  5, batch     3 | loss: 45.9303937CurrentTrain: epoch  5, batch     4 | loss: 63.0374963CurrentTrain: epoch  5, batch     5 | loss: 113.6441137CurrentTrain: epoch  5, batch     6 | loss: 67.1525751CurrentTrain: epoch  5, batch     7 | loss: 64.4714530CurrentTrain: epoch  5, batch     8 | loss: 67.9645914CurrentTrain: epoch  5, batch     9 | loss: 63.5743703CurrentTrain: epoch  5, batch    10 | loss: 69.1920162CurrentTrain: epoch  5, batch    11 | loss: 66.0377644CurrentTrain: epoch  5, batch    12 | loss: 66.2520705CurrentTrain: epoch  5, batch    13 | loss: 67.5268880CurrentTrain: epoch  5, batch    14 | loss: 54.4594123CurrentTrain: epoch  5, batch    15 | loss: 82.9389110CurrentTrain: epoch  5, batch    16 | loss: 55.1691370CurrentTrain: epoch  5, batch    17 | loss: 53.5335382CurrentTrain: epoch  5, batch    18 | loss: 68.1960489CurrentTrain: epoch  5, batch    19 | loss: 66.1520357CurrentTrain: epoch  5, batch    20 | loss: 62.2723424CurrentTrain: epoch  5, batch    21 | loss: 66.0900628CurrentTrain: epoch  5, batch    22 | loss: 52.4252552CurrentTrain: epoch  5, batch    23 | loss: 44.2275521CurrentTrain: epoch  5, batch    24 | loss: 83.5240382CurrentTrain: epoch  5, batch    25 | loss: 117.8002063CurrentTrain: epoch  5, batch    26 | loss: 66.3310502CurrentTrain: epoch  5, batch    27 | loss: 53.2238334CurrentTrain: epoch  5, batch    28 | loss: 85.7578849CurrentTrain: epoch  5, batch    29 | loss: 66.7118523CurrentTrain: epoch  5, batch    30 | loss: 67.2456227CurrentTrain: epoch  5, batch    31 | loss: 65.8272998CurrentTrain: epoch  5, batch    32 | loss: 83.7265012CurrentTrain: epoch  5, batch    33 | loss: 52.6770939CurrentTrain: epoch  5, batch    34 | loss: 63.4163686CurrentTrain: epoch  5, batch    35 | loss: 62.4804970CurrentTrain: epoch  5, batch    36 | loss: 44.9317611CurrentTrain: epoch  5, batch    37 | loss: 82.1229453CurrentTrain: epoch  5, batch    38 | loss: 82.0905962CurrentTrain: epoch  5, batch    39 | loss: 70.0320788CurrentTrain: epoch  5, batch    40 | loss: 87.2283782CurrentTrain: epoch  5, batch    41 | loss: 63.2976186CurrentTrain: epoch  5, batch    42 | loss: 66.5170239CurrentTrain: epoch  5, batch    43 | loss: 113.7229793CurrentTrain: epoch  5, batch    44 | loss: 85.1314048CurrentTrain: epoch  5, batch    45 | loss: 62.4724263CurrentTrain: epoch  5, batch    46 | loss: 54.4452980CurrentTrain: epoch  5, batch    47 | loss: 66.3755005CurrentTrain: epoch  5, batch    48 | loss: 65.7726339CurrentTrain: epoch  5, batch    49 | loss: 53.3701281CurrentTrain: epoch  5, batch    50 | loss: 42.0191223CurrentTrain: epoch  5, batch    51 | loss: 84.5676932CurrentTrain: epoch  5, batch    52 | loss: 87.3455764CurrentTrain: epoch  5, batch    53 | loss: 63.7112106CurrentTrain: epoch  5, batch    54 | loss: 66.9329478CurrentTrain: epoch  5, batch    55 | loss: 66.1438092CurrentTrain: epoch  5, batch    56 | loss: 83.3655290CurrentTrain: epoch  5, batch    57 | loss: 64.1091722CurrentTrain: epoch  5, batch    58 | loss: 116.2146505CurrentTrain: epoch  5, batch    59 | loss: 85.0453896CurrentTrain: epoch  5, batch    60 | loss: 83.9503978CurrentTrain: epoch  5, batch    61 | loss: 63.3267683CurrentTrain: epoch  5, batch    62 | loss: 49.6555063CurrentTrain: epoch  5, batch    63 | loss: 67.3883519CurrentTrain: epoch  5, batch    64 | loss: 67.2130205CurrentTrain: epoch  5, batch    65 | loss: 52.4356599CurrentTrain: epoch  5, batch    66 | loss: 44.3894408CurrentTrain: epoch  5, batch    67 | loss: 67.4072933CurrentTrain: epoch  5, batch    68 | loss: 65.3534426CurrentTrain: epoch  5, batch    69 | loss: 66.3641629CurrentTrain: epoch  5, batch    70 | loss: 65.8401235CurrentTrain: epoch  5, batch    71 | loss: 87.8667668CurrentTrain: epoch  5, batch    72 | loss: 81.5241108CurrentTrain: epoch  5, batch    73 | loss: 85.2257560CurrentTrain: epoch  5, batch    74 | loss: 45.7727681CurrentTrain: epoch  5, batch    75 | loss: 84.3472246CurrentTrain: epoch  5, batch    76 | loss: 69.5806635CurrentTrain: epoch  5, batch    77 | loss: 52.0767077CurrentTrain: epoch  5, batch    78 | loss: 65.8982546CurrentTrain: epoch  5, batch    79 | loss: 54.7579029CurrentTrain: epoch  5, batch    80 | loss: 55.2420744CurrentTrain: epoch  5, batch    81 | loss: 88.2618309CurrentTrain: epoch  5, batch    82 | loss: 53.8302777CurrentTrain: epoch  5, batch    83 | loss: 50.9902627CurrentTrain: epoch  5, batch    84 | loss: 65.2367794CurrentTrain: epoch  5, batch    85 | loss: 52.9142614CurrentTrain: epoch  5, batch    86 | loss: 88.6556626CurrentTrain: epoch  5, batch    87 | loss: 51.6107610CurrentTrain: epoch  5, batch    88 | loss: 118.6431395CurrentTrain: epoch  5, batch    89 | loss: 86.0605737CurrentTrain: epoch  5, batch    90 | loss: 49.7972517CurrentTrain: epoch  5, batch    91 | loss: 68.6859886CurrentTrain: epoch  5, batch    92 | loss: 61.3444299CurrentTrain: epoch  5, batch    93 | loss: 115.2093026CurrentTrain: epoch  5, batch    94 | loss: 85.1976430CurrentTrain: epoch  5, batch    95 | loss: 46.3730719CurrentTrain: epoch  6, batch     0 | loss: 82.9374608CurrentTrain: epoch  6, batch     1 | loss: 81.6233607CurrentTrain: epoch  6, batch     2 | loss: 85.9184496CurrentTrain: epoch  6, batch     3 | loss: 83.9710205CurrentTrain: epoch  6, batch     4 | loss: 66.1350700CurrentTrain: epoch  6, batch     5 | loss: 55.0190544CurrentTrain: epoch  6, batch     6 | loss: 84.4573226CurrentTrain: epoch  6, batch     7 | loss: 81.2524838CurrentTrain: epoch  6, batch     8 | loss: 64.7352371CurrentTrain: epoch  6, batch     9 | loss: 53.4995610CurrentTrain: epoch  6, batch    10 | loss: 83.0443164CurrentTrain: epoch  6, batch    11 | loss: 63.3593828CurrentTrain: epoch  6, batch    12 | loss: 117.7873766CurrentTrain: epoch  6, batch    13 | loss: 84.0889649CurrentTrain: epoch  6, batch    14 | loss: 65.7084620CurrentTrain: epoch  6, batch    15 | loss: 115.4130812CurrentTrain: epoch  6, batch    16 | loss: 44.3121732CurrentTrain: epoch  6, batch    17 | loss: 84.8740510CurrentTrain: epoch  6, batch    18 | loss: 50.7295533CurrentTrain: epoch  6, batch    19 | loss: 42.5227736CurrentTrain: epoch  6, batch    20 | loss: 63.4526955CurrentTrain: epoch  6, batch    21 | loss: 56.9419359CurrentTrain: epoch  6, batch    22 | loss: 54.1558787CurrentTrain: epoch  6, batch    23 | loss: 182.2185125CurrentTrain: epoch  6, batch    24 | loss: 61.2784841CurrentTrain: epoch  6, batch    25 | loss: 66.3372155CurrentTrain: epoch  6, batch    26 | loss: 87.6650945CurrentTrain: epoch  6, batch    27 | loss: 181.6700329CurrentTrain: epoch  6, batch    28 | loss: 64.3496017CurrentTrain: epoch  6, batch    29 | loss: 69.5618100CurrentTrain: epoch  6, batch    30 | loss: 65.0423541CurrentTrain: epoch  6, batch    31 | loss: 51.8695558CurrentTrain: epoch  6, batch    32 | loss: 113.5410688CurrentTrain: epoch  6, batch    33 | loss: 66.0328443CurrentTrain: epoch  6, batch    34 | loss: 81.8944986CurrentTrain: epoch  6, batch    35 | loss: 181.6073301CurrentTrain: epoch  6, batch    36 | loss: 50.3787218CurrentTrain: epoch  6, batch    37 | loss: 69.4704131CurrentTrain: epoch  6, batch    38 | loss: 85.5568252CurrentTrain: epoch  6, batch    39 | loss: 55.3671243CurrentTrain: epoch  6, batch    40 | loss: 52.5483706CurrentTrain: epoch  6, batch    41 | loss: 50.2795253CurrentTrain: epoch  6, batch    42 | loss: 80.8934224CurrentTrain: epoch  6, batch    43 | loss: 65.4939834CurrentTrain: epoch  6, batch    44 | loss: 83.1608556CurrentTrain: epoch  6, batch    45 | loss: 83.5877012CurrentTrain: epoch  6, batch    46 | loss: 63.5125653CurrentTrain: epoch  6, batch    47 | loss: 62.3364076CurrentTrain: epoch  6, batch    48 | loss: 53.9225321CurrentTrain: epoch  6, batch    49 | loss: 54.3159290CurrentTrain: epoch  6, batch    50 | loss: 50.3252171CurrentTrain: epoch  6, batch    51 | loss: 117.3095873CurrentTrain: epoch  6, batch    52 | loss: 81.7706515CurrentTrain: epoch  6, batch    53 | loss: 46.0787363CurrentTrain: epoch  6, batch    54 | loss: 69.1212353CurrentTrain: epoch  6, batch    55 | loss: 64.5686228CurrentTrain: epoch  6, batch    56 | loss: 53.7571364CurrentTrain: epoch  6, batch    57 | loss: 53.7356202CurrentTrain: epoch  6, batch    58 | loss: 113.0905375CurrentTrain: epoch  6, batch    59 | loss: 64.6283230CurrentTrain: epoch  6, batch    60 | loss: 67.2187211CurrentTrain: epoch  6, batch    61 | loss: 57.5362786CurrentTrain: epoch  6, batch    62 | loss: 63.9384153CurrentTrain: epoch  6, batch    63 | loss: 83.6579923CurrentTrain: epoch  6, batch    64 | loss: 84.9635137CurrentTrain: epoch  6, batch    65 | loss: 55.2748124CurrentTrain: epoch  6, batch    66 | loss: 119.0420458CurrentTrain: epoch  6, batch    67 | loss: 45.4904220CurrentTrain: epoch  6, batch    68 | loss: 51.5891914CurrentTrain: epoch  6, batch    69 | loss: 52.7956361CurrentTrain: epoch  6, batch    70 | loss: 51.5572069CurrentTrain: epoch  6, batch    71 | loss: 65.9077091CurrentTrain: epoch  6, batch    72 | loss: 85.9420647CurrentTrain: epoch  6, batch    73 | loss: 112.2942466CurrentTrain: epoch  6, batch    74 | loss: 52.8034951CurrentTrain: epoch  6, batch    75 | loss: 52.5403301CurrentTrain: epoch  6, batch    76 | loss: 67.4802735CurrentTrain: epoch  6, batch    77 | loss: 112.0807419CurrentTrain: epoch  6, batch    78 | loss: 118.0078307CurrentTrain: epoch  6, batch    79 | loss: 65.9731004CurrentTrain: epoch  6, batch    80 | loss: 51.6086050CurrentTrain: epoch  6, batch    81 | loss: 72.5524182CurrentTrain: epoch  6, batch    82 | loss: 50.1363202CurrentTrain: epoch  6, batch    83 | loss: 66.5574741CurrentTrain: epoch  6, batch    84 | loss: 81.2117836CurrentTrain: epoch  6, batch    85 | loss: 52.5411633CurrentTrain: epoch  6, batch    86 | loss: 61.0532025CurrentTrain: epoch  6, batch    87 | loss: 50.1294023CurrentTrain: epoch  6, batch    88 | loss: 42.6556637CurrentTrain: epoch  6, batch    89 | loss: 114.7694584CurrentTrain: epoch  6, batch    90 | loss: 177.6524981CurrentTrain: epoch  6, batch    91 | loss: 42.3688981CurrentTrain: epoch  6, batch    92 | loss: 81.3866456CurrentTrain: epoch  6, batch    93 | loss: 65.0601004CurrentTrain: epoch  6, batch    94 | loss: 43.4052889CurrentTrain: epoch  6, batch    95 | loss: 42.7623784CurrentTrain: epoch  7, batch     0 | loss: 42.2943731CurrentTrain: epoch  7, batch     1 | loss: 175.3713440CurrentTrain: epoch  7, batch     2 | loss: 51.1222800CurrentTrain: epoch  7, batch     3 | loss: 64.6489470CurrentTrain: epoch  7, batch     4 | loss: 53.6546851CurrentTrain: epoch  7, batch     5 | loss: 180.7842179CurrentTrain: epoch  7, batch     6 | loss: 115.2325731CurrentTrain: epoch  7, batch     7 | loss: 65.5906333CurrentTrain: epoch  7, batch     8 | loss: 81.5679395CurrentTrain: epoch  7, batch     9 | loss: 65.4260750CurrentTrain: epoch  7, batch    10 | loss: 51.5374391CurrentTrain: epoch  7, batch    11 | loss: 84.7117695CurrentTrain: epoch  7, batch    12 | loss: 49.8440007CurrentTrain: epoch  7, batch    13 | loss: 115.9390029CurrentTrain: epoch  7, batch    14 | loss: 82.6774887CurrentTrain: epoch  7, batch    15 | loss: 114.1745500CurrentTrain: epoch  7, batch    16 | loss: 66.9957672CurrentTrain: epoch  7, batch    17 | loss: 66.2492916CurrentTrain: epoch  7, batch    18 | loss: 53.4299674CurrentTrain: epoch  7, batch    19 | loss: 53.6569196CurrentTrain: epoch  7, batch    20 | loss: 114.0155372CurrentTrain: epoch  7, batch    21 | loss: 86.8837644CurrentTrain: epoch  7, batch    22 | loss: 78.4415037CurrentTrain: epoch  7, batch    23 | loss: 54.2246748CurrentTrain: epoch  7, batch    24 | loss: 62.0005391CurrentTrain: epoch  7, batch    25 | loss: 50.5589692CurrentTrain: epoch  7, batch    26 | loss: 81.6151552CurrentTrain: epoch  7, batch    27 | loss: 52.3912523CurrentTrain: epoch  7, batch    28 | loss: 86.3198629CurrentTrain: epoch  7, batch    29 | loss: 66.7407907CurrentTrain: epoch  7, batch    30 | loss: 111.6611391CurrentTrain: epoch  7, batch    31 | loss: 65.4132612CurrentTrain: epoch  7, batch    32 | loss: 50.2375172CurrentTrain: epoch  7, batch    33 | loss: 79.8049500CurrentTrain: epoch  7, batch    34 | loss: 52.4265901CurrentTrain: epoch  7, batch    35 | loss: 63.8005366CurrentTrain: epoch  7, batch    36 | loss: 40.3116663CurrentTrain: epoch  7, batch    37 | loss: 53.5276876CurrentTrain: epoch  7, batch    38 | loss: 53.7123693CurrentTrain: epoch  7, batch    39 | loss: 51.3788123CurrentTrain: epoch  7, batch    40 | loss: 82.7820400CurrentTrain: epoch  7, batch    41 | loss: 64.1756208CurrentTrain: epoch  7, batch    42 | loss: 48.5253526CurrentTrain: epoch  7, batch    43 | loss: 170.2163173CurrentTrain: epoch  7, batch    44 | loss: 66.4935392CurrentTrain: epoch  7, batch    45 | loss: 84.0070646CurrentTrain: epoch  7, batch    46 | loss: 117.8407857CurrentTrain: epoch  7, batch    47 | loss: 42.5405730CurrentTrain: epoch  7, batch    48 | loss: 67.7429212CurrentTrain: epoch  7, batch    49 | loss: 67.5743673CurrentTrain: epoch  7, batch    50 | loss: 54.0898521CurrentTrain: epoch  7, batch    51 | loss: 62.9623763CurrentTrain: epoch  7, batch    52 | loss: 66.8766450CurrentTrain: epoch  7, batch    53 | loss: 66.3928760CurrentTrain: epoch  7, batch    54 | loss: 53.5549000CurrentTrain: epoch  7, batch    55 | loss: 84.5277314CurrentTrain: epoch  7, batch    56 | loss: 66.7532149CurrentTrain: epoch  7, batch    57 | loss: 67.4130556CurrentTrain: epoch  7, batch    58 | loss: 63.6744120CurrentTrain: epoch  7, batch    59 | loss: 58.7332022CurrentTrain: epoch  7, batch    60 | loss: 61.5153538CurrentTrain: epoch  7, batch    61 | loss: 62.7372289CurrentTrain: epoch  7, batch    62 | loss: 60.4606439CurrentTrain: epoch  7, batch    63 | loss: 113.8803026CurrentTrain: epoch  7, batch    64 | loss: 54.3740837CurrentTrain: epoch  7, batch    65 | loss: 55.0955265CurrentTrain: epoch  7, batch    66 | loss: 82.2113932CurrentTrain: epoch  7, batch    67 | loss: 51.1535367CurrentTrain: epoch  7, batch    68 | loss: 66.4486100CurrentTrain: epoch  7, batch    69 | loss: 43.2410700CurrentTrain: epoch  7, batch    70 | loss: 64.7405094CurrentTrain: epoch  7, batch    71 | loss: 82.7939055CurrentTrain: epoch  7, batch    72 | loss: 85.7336747CurrentTrain: epoch  7, batch    73 | loss: 82.8319346CurrentTrain: epoch  7, batch    74 | loss: 51.7905872CurrentTrain: epoch  7, batch    75 | loss: 64.7391081CurrentTrain: epoch  7, batch    76 | loss: 40.0138566CurrentTrain: epoch  7, batch    77 | loss: 60.6165258CurrentTrain: epoch  7, batch    78 | loss: 66.9319013CurrentTrain: epoch  7, batch    79 | loss: 84.7784754CurrentTrain: epoch  7, batch    80 | loss: 54.2169090CurrentTrain: epoch  7, batch    81 | loss: 51.1015326CurrentTrain: epoch  7, batch    82 | loss: 63.3631673CurrentTrain: epoch  7, batch    83 | loss: 118.0030022CurrentTrain: epoch  7, batch    84 | loss: 85.3007287CurrentTrain: epoch  7, batch    85 | loss: 52.4229733CurrentTrain: epoch  7, batch    86 | loss: 117.7652390CurrentTrain: epoch  7, batch    87 | loss: 52.8319197CurrentTrain: epoch  7, batch    88 | loss: 81.5024708CurrentTrain: epoch  7, batch    89 | loss: 85.0695957CurrentTrain: epoch  7, batch    90 | loss: 80.4458193CurrentTrain: epoch  7, batch    91 | loss: 85.7817836CurrentTrain: epoch  7, batch    92 | loss: 55.0503086CurrentTrain: epoch  7, batch    93 | loss: 49.4352494CurrentTrain: epoch  7, batch    94 | loss: 81.7039107CurrentTrain: epoch  7, batch    95 | loss: 71.8342337CurrentTrain: epoch  8, batch     0 | loss: 56.4273062CurrentTrain: epoch  8, batch     1 | loss: 86.1862892CurrentTrain: epoch  8, batch     2 | loss: 86.0287126CurrentTrain: epoch  8, batch     3 | loss: 61.9182091CurrentTrain: epoch  8, batch     4 | loss: 64.9497548CurrentTrain: epoch  8, batch     5 | loss: 52.5547181CurrentTrain: epoch  8, batch     6 | loss: 60.8506373CurrentTrain: epoch  8, batch     7 | loss: 117.4819941CurrentTrain: epoch  8, batch     8 | loss: 64.1878914CurrentTrain: epoch  8, batch     9 | loss: 69.9424732CurrentTrain: epoch  8, batch    10 | loss: 49.0105119CurrentTrain: epoch  8, batch    11 | loss: 48.9472801CurrentTrain: epoch  8, batch    12 | loss: 64.3160289CurrentTrain: epoch  8, batch    13 | loss: 82.7826295CurrentTrain: epoch  8, batch    14 | loss: 51.8668378CurrentTrain: epoch  8, batch    15 | loss: 65.7669108CurrentTrain: epoch  8, batch    16 | loss: 117.9235437CurrentTrain: epoch  8, batch    17 | loss: 84.1122074CurrentTrain: epoch  8, batch    18 | loss: 61.9681466CurrentTrain: epoch  8, batch    19 | loss: 87.2500528CurrentTrain: epoch  8, batch    20 | loss: 84.1269875CurrentTrain: epoch  8, batch    21 | loss: 67.0354895CurrentTrain: epoch  8, batch    22 | loss: 82.8910239CurrentTrain: epoch  8, batch    23 | loss: 64.5511957CurrentTrain: epoch  8, batch    24 | loss: 51.2693592CurrentTrain: epoch  8, batch    25 | loss: 84.7645477CurrentTrain: epoch  8, batch    26 | loss: 64.6562417CurrentTrain: epoch  8, batch    27 | loss: 62.8290919CurrentTrain: epoch  8, batch    28 | loss: 64.3286932CurrentTrain: epoch  8, batch    29 | loss: 52.1991064CurrentTrain: epoch  8, batch    30 | loss: 52.1927137CurrentTrain: epoch  8, batch    31 | loss: 50.7662590CurrentTrain: epoch  8, batch    32 | loss: 52.3912715CurrentTrain: epoch  8, batch    33 | loss: 65.2896617CurrentTrain: epoch  8, batch    34 | loss: 51.2067381CurrentTrain: epoch  8, batch    35 | loss: 67.2832058CurrentTrain: epoch  8, batch    36 | loss: 51.3593115CurrentTrain: epoch  8, batch    37 | loss: 60.5007808CurrentTrain: epoch  8, batch    38 | loss: 53.0870833CurrentTrain: epoch  8, batch    39 | loss: 52.9979020CurrentTrain: epoch  8, batch    40 | loss: 85.1292646CurrentTrain: epoch  8, batch    41 | loss: 65.3646514CurrentTrain: epoch  8, batch    42 | loss: 117.0072694CurrentTrain: epoch  8, batch    43 | loss: 51.4513182CurrentTrain: epoch  8, batch    44 | loss: 63.5027751CurrentTrain: epoch  8, batch    45 | loss: 66.5361749CurrentTrain: epoch  8, batch    46 | loss: 84.6949186CurrentTrain: epoch  8, batch    47 | loss: 52.3517532CurrentTrain: epoch  8, batch    48 | loss: 54.8321789CurrentTrain: epoch  8, batch    49 | loss: 64.0391656CurrentTrain: epoch  8, batch    50 | loss: 66.7515561CurrentTrain: epoch  8, batch    51 | loss: 65.8403094CurrentTrain: epoch  8, batch    52 | loss: 82.8136437CurrentTrain: epoch  8, batch    53 | loss: 52.9336657CurrentTrain: epoch  8, batch    54 | loss: 52.9589055CurrentTrain: epoch  8, batch    55 | loss: 67.2424535CurrentTrain: epoch  8, batch    56 | loss: 62.5272306CurrentTrain: epoch  8, batch    57 | loss: 61.8251735CurrentTrain: epoch  8, batch    58 | loss: 52.4465534CurrentTrain: epoch  8, batch    59 | loss: 82.4598082CurrentTrain: epoch  8, batch    60 | loss: 44.0522669CurrentTrain: epoch  8, batch    61 | loss: 53.6856165CurrentTrain: epoch  8, batch    62 | loss: 69.6271109CurrentTrain: epoch  8, batch    63 | loss: 51.3385602CurrentTrain: epoch  8, batch    64 | loss: 59.5148147CurrentTrain: epoch  8, batch    65 | loss: 65.3405288CurrentTrain: epoch  8, batch    66 | loss: 65.5984939CurrentTrain: epoch  8, batch    67 | loss: 115.3226339CurrentTrain: epoch  8, batch    68 | loss: 64.6917637CurrentTrain: epoch  8, batch    69 | loss: 51.3982092CurrentTrain: epoch  8, batch    70 | loss: 85.6672259CurrentTrain: epoch  8, batch    71 | loss: 44.0782188CurrentTrain: epoch  8, batch    72 | loss: 55.7318812CurrentTrain: epoch  8, batch    73 | loss: 43.9915392CurrentTrain: epoch  8, batch    74 | loss: 65.8421611CurrentTrain: epoch  8, batch    75 | loss: 84.1862742CurrentTrain: epoch  8, batch    76 | loss: 63.0406435CurrentTrain: epoch  8, batch    77 | loss: 66.1996054CurrentTrain: epoch  8, batch    78 | loss: 64.3634485CurrentTrain: epoch  8, batch    79 | loss: 52.5075798CurrentTrain: epoch  8, batch    80 | loss: 82.7952349CurrentTrain: epoch  8, batch    81 | loss: 115.2511074CurrentTrain: epoch  8, batch    82 | loss: 40.2040344CurrentTrain: epoch  8, batch    83 | loss: 43.7392400CurrentTrain: epoch  8, batch    84 | loss: 84.1413724CurrentTrain: epoch  8, batch    85 | loss: 84.1030353CurrentTrain: epoch  8, batch    86 | loss: 81.8156061CurrentTrain: epoch  8, batch    87 | loss: 88.1262957CurrentTrain: epoch  8, batch    88 | loss: 53.0978749CurrentTrain: epoch  8, batch    89 | loss: 54.6522933CurrentTrain: epoch  8, batch    90 | loss: 81.0158702CurrentTrain: epoch  8, batch    91 | loss: 63.7767616CurrentTrain: epoch  8, batch    92 | loss: 65.0037062CurrentTrain: epoch  8, batch    93 | loss: 66.8172466CurrentTrain: epoch  8, batch    94 | loss: 117.6461571CurrentTrain: epoch  8, batch    95 | loss: 52.8326095CurrentTrain: epoch  9, batch     0 | loss: 62.0335474CurrentTrain: epoch  9, batch     1 | loss: 42.8588784CurrentTrain: epoch  9, batch     2 | loss: 82.3515636CurrentTrain: epoch  9, batch     3 | loss: 64.7730946CurrentTrain: epoch  9, batch     4 | loss: 175.3583813CurrentTrain: epoch  9, batch     5 | loss: 42.5375319CurrentTrain: epoch  9, batch     6 | loss: 83.2571550CurrentTrain: epoch  9, batch     7 | loss: 63.0474709CurrentTrain: epoch  9, batch     8 | loss: 113.1077233CurrentTrain: epoch  9, batch     9 | loss: 64.1911113CurrentTrain: epoch  9, batch    10 | loss: 81.1328826CurrentTrain: epoch  9, batch    11 | loss: 62.1809487CurrentTrain: epoch  9, batch    12 | loss: 181.7253055CurrentTrain: epoch  9, batch    13 | loss: 182.2057288CurrentTrain: epoch  9, batch    14 | loss: 50.4509684CurrentTrain: epoch  9, batch    15 | loss: 115.4782308CurrentTrain: epoch  9, batch    16 | loss: 117.5756780CurrentTrain: epoch  9, batch    17 | loss: 49.8799912CurrentTrain: epoch  9, batch    18 | loss: 50.7444006CurrentTrain: epoch  9, batch    19 | loss: 115.1947514CurrentTrain: epoch  9, batch    20 | loss: 65.7807269CurrentTrain: epoch  9, batch    21 | loss: 65.0303940CurrentTrain: epoch  9, batch    22 | loss: 84.1884085CurrentTrain: epoch  9, batch    23 | loss: 53.5489183CurrentTrain: epoch  9, batch    24 | loss: 84.6613094CurrentTrain: epoch  9, batch    25 | loss: 115.5032363CurrentTrain: epoch  9, batch    26 | loss: 111.6337876CurrentTrain: epoch  9, batch    27 | loss: 51.3692006CurrentTrain: epoch  9, batch    28 | loss: 77.4966240CurrentTrain: epoch  9, batch    29 | loss: 117.5017348CurrentTrain: epoch  9, batch    30 | loss: 79.5222285CurrentTrain: epoch  9, batch    31 | loss: 84.1785773CurrentTrain: epoch  9, batch    32 | loss: 66.1968555CurrentTrain: epoch  9, batch    33 | loss: 82.5053232CurrentTrain: epoch  9, batch    34 | loss: 65.6436736CurrentTrain: epoch  9, batch    35 | loss: 51.2134080CurrentTrain: epoch  9, batch    36 | loss: 52.3671634CurrentTrain: epoch  9, batch    37 | loss: 82.6644017CurrentTrain: epoch  9, batch    38 | loss: 65.3238637CurrentTrain: epoch  9, batch    39 | loss: 51.8408074CurrentTrain: epoch  9, batch    40 | loss: 41.9502420CurrentTrain: epoch  9, batch    41 | loss: 84.6664738CurrentTrain: epoch  9, batch    42 | loss: 82.4861989CurrentTrain: epoch  9, batch    43 | loss: 63.0212114CurrentTrain: epoch  9, batch    44 | loss: 51.6053325CurrentTrain: epoch  9, batch    45 | loss: 51.2423088CurrentTrain: epoch  9, batch    46 | loss: 66.8052445CurrentTrain: epoch  9, batch    47 | loss: 83.7791880CurrentTrain: epoch  9, batch    48 | loss: 64.5285453CurrentTrain: epoch  9, batch    49 | loss: 63.3035648CurrentTrain: epoch  9, batch    50 | loss: 53.1229269CurrentTrain: epoch  9, batch    51 | loss: 51.0434237CurrentTrain: epoch  9, batch    52 | loss: 64.0714156CurrentTrain: epoch  9, batch    53 | loss: 53.2732461CurrentTrain: epoch  9, batch    54 | loss: 84.1247989CurrentTrain: epoch  9, batch    55 | loss: 53.0261928CurrentTrain: epoch  9, batch    56 | loss: 53.1877770CurrentTrain: epoch  9, batch    57 | loss: 43.0364935CurrentTrain: epoch  9, batch    58 | loss: 117.5980586CurrentTrain: epoch  9, batch    59 | loss: 84.0417343CurrentTrain: epoch  9, batch    60 | loss: 42.5965346CurrentTrain: epoch  9, batch    61 | loss: 44.1298111CurrentTrain: epoch  9, batch    62 | loss: 54.6620228CurrentTrain: epoch  9, batch    63 | loss: 50.4309802CurrentTrain: epoch  9, batch    64 | loss: 63.5493331CurrentTrain: epoch  9, batch    65 | loss: 50.4254420CurrentTrain: epoch  9, batch    66 | loss: 50.5565162CurrentTrain: epoch  9, batch    67 | loss: 52.5953086CurrentTrain: epoch  9, batch    68 | loss: 52.9771105CurrentTrain: epoch  9, batch    69 | loss: 51.7597504CurrentTrain: epoch  9, batch    70 | loss: 62.8904375CurrentTrain: epoch  9, batch    71 | loss: 67.5629669CurrentTrain: epoch  9, batch    72 | loss: 62.2337071CurrentTrain: epoch  9, batch    73 | loss: 52.7492640CurrentTrain: epoch  9, batch    74 | loss: 65.4704676CurrentTrain: epoch  9, batch    75 | loss: 84.7954965CurrentTrain: epoch  9, batch    76 | loss: 81.1707876CurrentTrain: epoch  9, batch    77 | loss: 51.7453052CurrentTrain: epoch  9, batch    78 | loss: 60.8219216CurrentTrain: epoch  9, batch    79 | loss: 81.7681254CurrentTrain: epoch  9, batch    80 | loss: 65.5178289CurrentTrain: epoch  9, batch    81 | loss: 49.1709025CurrentTrain: epoch  9, batch    82 | loss: 52.1671282CurrentTrain: epoch  9, batch    83 | loss: 84.8287886CurrentTrain: epoch  9, batch    84 | loss: 64.2422317CurrentTrain: epoch  9, batch    85 | loss: 372.3407432CurrentTrain: epoch  9, batch    86 | loss: 54.0878884CurrentTrain: epoch  9, batch    87 | loss: 53.5729117CurrentTrain: epoch  9, batch    88 | loss: 63.1211137CurrentTrain: epoch  9, batch    89 | loss: 86.4794136CurrentTrain: epoch  9, batch    90 | loss: 80.9464838CurrentTrain: epoch  9, batch    91 | loss: 65.5243528CurrentTrain: epoch  9, batch    92 | loss: 52.4634806CurrentTrain: epoch  9, batch    93 | loss: 80.7616787CurrentTrain: epoch  9, batch    94 | loss: 44.1637709CurrentTrain: epoch  9, batch    95 | loss: 41.8011114

F1 score per class: {32: 0.47619047619047616, 6: 0.6798418972332015, 19: 0.07692307692307693, 24: 0.7227722772277227, 26: 0.8736842105263158, 29: 0.8078817733990148}
Micro-average F1 score: 0.6918819188191881
Weighted-average F1 score: 0.6936012741302323
F1 score per class: {32: 0.5017667844522968, 6: 0.6798418972332015, 19: 0.1791044776119403, 24: 0.6923076923076923, 26: 0.883495145631068, 29: 0.7727272727272727}
Micro-average F1 score: 0.6645109135004043
Weighted-average F1 score: 0.6477917109427426
F1 score per class: {32: 0.5017667844522968, 6: 0.6798418972332015, 19: 0.13333333333333333, 24: 0.6923076923076923, 26: 0.883495145631068, 29: 0.7727272727272727}
Micro-average F1 score: 0.6650406504065041
Weighted-average F1 score: 0.6496996078157962

F1 score per class: {32: 0.47619047619047616, 6: 0.6798418972332015, 19: 0.07692307692307693, 24: 0.7227722772277227, 26: 0.8736842105263158, 29: 0.8078817733990148}
Micro-average F1 score: 0.6918819188191881
Weighted-average F1 score: 0.6936012741302323
F1 score per class: {32: 0.5017667844522968, 6: 0.6798418972332015, 19: 0.1791044776119403, 24: 0.6923076923076923, 26: 0.883495145631068, 29: 0.7727272727272727}
Micro-average F1 score: 0.6645109135004043
Weighted-average F1 score: 0.6477917109427426
F1 score per class: {32: 0.5017667844522968, 6: 0.6798418972332015, 19: 0.13333333333333333, 24: 0.6923076923076923, 26: 0.883495145631068, 29: 0.7727272727272727}
Micro-average F1 score: 0.6650406504065041
Weighted-average F1 score: 0.6496996078157962
cur_acc:  ['0.6919']
his_acc:  ['0.6919']
cur_acc des:  ['0.6645']
his_acc des:  ['0.6645']
cur_acc rrf:  ['0.6650']
his_acc rrf:  ['0.6650']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 88.8589893CurrentTrain: epoch  0, batch     1 | loss: 76.3281793CurrentTrain: epoch  0, batch     2 | loss: 96.1479294CurrentTrain: epoch  0, batch     3 | loss: 53.4783101CurrentTrain: epoch  1, batch     0 | loss: 121.1928376CurrentTrain: epoch  1, batch     1 | loss: 61.6303019CurrentTrain: epoch  1, batch     2 | loss: 62.3131609CurrentTrain: epoch  1, batch     3 | loss: 41.5959680CurrentTrain: epoch  2, batch     0 | loss: 73.3176081CurrentTrain: epoch  2, batch     1 | loss: 69.2575339CurrentTrain: epoch  2, batch     2 | loss: 71.4973929CurrentTrain: epoch  2, batch     3 | loss: 61.1848111CurrentTrain: epoch  3, batch     0 | loss: 71.7489334CurrentTrain: epoch  3, batch     1 | loss: 55.8062447CurrentTrain: epoch  3, batch     2 | loss: 87.6826569CurrentTrain: epoch  3, batch     3 | loss: 82.6119783CurrentTrain: epoch  4, batch     0 | loss: 56.1201783CurrentTrain: epoch  4, batch     1 | loss: 86.7472216CurrentTrain: epoch  4, batch     2 | loss: 56.8865278CurrentTrain: epoch  4, batch     3 | loss: 86.3703791CurrentTrain: epoch  5, batch     0 | loss: 86.0429819CurrentTrain: epoch  5, batch     1 | loss: 55.4187289CurrentTrain: epoch  5, batch     2 | loss: 56.2416944CurrentTrain: epoch  5, batch     3 | loss: 81.9162482CurrentTrain: epoch  6, batch     0 | loss: 57.6966771CurrentTrain: epoch  6, batch     1 | loss: 116.7470946CurrentTrain: epoch  6, batch     2 | loss: 55.2470342CurrentTrain: epoch  6, batch     3 | loss: 43.6240344CurrentTrain: epoch  7, batch     0 | loss: 63.9695272CurrentTrain: epoch  7, batch     1 | loss: 65.9757060CurrentTrain: epoch  7, batch     2 | loss: 55.4603725CurrentTrain: epoch  7, batch     3 | loss: 79.5719158CurrentTrain: epoch  8, batch     0 | loss: 65.0751965CurrentTrain: epoch  8, batch     1 | loss: 64.4800590CurrentTrain: epoch  8, batch     2 | loss: 86.2572515CurrentTrain: epoch  8, batch     3 | loss: 43.6351305CurrentTrain: epoch  9, batch     0 | loss: 53.7839898CurrentTrain: epoch  9, batch     1 | loss: 117.6596230CurrentTrain: epoch  9, batch     2 | loss: 85.0898245CurrentTrain: epoch  9, batch     3 | loss: 31.9473153
MemoryTrain:  epoch  0, batch     0 | loss: 0.5351725MemoryTrain:  epoch  1, batch     0 | loss: 0.3788887MemoryTrain:  epoch  2, batch     0 | loss: 0.2955900MemoryTrain:  epoch  3, batch     0 | loss: 0.2294838MemoryTrain:  epoch  4, batch     0 | loss: 0.1690099MemoryTrain:  epoch  5, batch     0 | loss: 0.1155804MemoryTrain:  epoch  6, batch     0 | loss: 0.0958605MemoryTrain:  epoch  7, batch     0 | loss: 0.0859303MemoryTrain:  epoch  8, batch     0 | loss: 0.0708930MemoryTrain:  epoch  9, batch     0 | loss: 0.0632966

F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.0, 38: 0.0, 6: 0.42424242424242425, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6885245901639344, 26: 0.47244094488188976, 29: 0.4888888888888889}
Micro-average F1 score: 0.4536489151873767
Weighted-average F1 score: 0.4005032684365489
F1 score per class: {32: 0.0, 35: 0.6666666666666666, 37: 0.0, 38: 0.0, 6: 0.8089887640449438, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.588957055214724, 26: 0.4126984126984127, 29: 0.38095238095238093}
Micro-average F1 score: 0.42382271468144045
Weighted-average F1 score: 0.3681392252561879
F1 score per class: {32: 0.0, 35: 0.7777777777777778, 37: 0.0, 38: 0.0, 6: 0.7954545454545454, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.588957055214724, 26: 0.40625, 29: 0.368}
Micro-average F1 score: 0.428169014084507
Weighted-average F1 score: 0.3735614460562572

F1 score per class: {32: 0.4734982332155477, 35: 0.7, 37: 0.59375, 6: 0.1951219512195122, 38: 0.42424242424242425, 15: 0.6635514018691588, 19: 0.8317757009345794, 24: 0.7685185185185185, 25: 0.5217391304347826, 26: 0.3508771929824561, 29: 0.25142857142857145}
Micro-average F1 score: 0.5558613098514034
Weighted-average F1 score: 0.5369913352755381
F1 score per class: {32: 0.398989898989899, 35: 0.46153846153846156, 37: 0.5573770491803278, 6: 0.1509433962264151, 38: 0.8089887640449438, 15: 0.5957446808510638, 19: 0.7666666666666667, 24: 0.7385892116182573, 25: 0.34532374100719426, 26: 0.2775800711743772, 29: 0.17647058823529413}
Micro-average F1 score: 0.4665856622114216
Weighted-average F1 score: 0.4329462721480935
F1 score per class: {32: 0.4230769230769231, 35: 0.5384615384615384, 37: 0.5676567656765676, 6: 0.1917808219178082, 38: 0.7954545454545454, 15: 0.6060606060606061, 19: 0.7929515418502202, 24: 0.7606837606837606, 25: 0.3392226148409894, 26: 0.2608695652173913, 29: 0.16727272727272727}
Micro-average F1 score: 0.4752392842280483
Weighted-average F1 score: 0.4389077019529985
cur_acc:  ['0.6919', '0.4536']
his_acc:  ['0.6919', '0.5559']
cur_acc des:  ['0.6645', '0.4238']
his_acc des:  ['0.6645', '0.4666']
cur_acc rrf:  ['0.6650', '0.4282']
his_acc rrf:  ['0.6650', '0.4752']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 80.7908685CurrentTrain: epoch  0, batch     1 | loss: 97.4407735CurrentTrain: epoch  0, batch     2 | loss: 75.5945923CurrentTrain: epoch  0, batch     3 | loss: 42.0261000CurrentTrain: epoch  1, batch     0 | loss: 75.9682770CurrentTrain: epoch  1, batch     1 | loss: 71.3839566CurrentTrain: epoch  1, batch     2 | loss: 59.8944340CurrentTrain: epoch  1, batch     3 | loss: 75.9248436CurrentTrain: epoch  2, batch     0 | loss: 74.1670564CurrentTrain: epoch  2, batch     1 | loss: 85.3049031CurrentTrain: epoch  2, batch     2 | loss: 86.9920898CurrentTrain: epoch  2, batch     3 | loss: 36.9574740CurrentTrain: epoch  3, batch     0 | loss: 70.3257949CurrentTrain: epoch  3, batch     1 | loss: 86.3809559CurrentTrain: epoch  3, batch     2 | loss: 87.9926969CurrentTrain: epoch  3, batch     3 | loss: 38.9960965CurrentTrain: epoch  4, batch     0 | loss: 54.2386104CurrentTrain: epoch  4, batch     1 | loss: 68.1574996CurrentTrain: epoch  4, batch     2 | loss: 67.6688651CurrentTrain: epoch  4, batch     3 | loss: 73.4566225CurrentTrain: epoch  5, batch     0 | loss: 69.3152476CurrentTrain: epoch  5, batch     1 | loss: 55.0717821CurrentTrain: epoch  5, batch     2 | loss: 52.6844652CurrentTrain: epoch  5, batch     3 | loss: 71.7852003CurrentTrain: epoch  6, batch     0 | loss: 65.8920943CurrentTrain: epoch  6, batch     1 | loss: 65.9965810CurrentTrain: epoch  6, batch     2 | loss: 81.5757558CurrentTrain: epoch  6, batch     3 | loss: 53.2052587CurrentTrain: epoch  7, batch     0 | loss: 65.6216180CurrentTrain: epoch  7, batch     1 | loss: 65.9057102CurrentTrain: epoch  7, batch     2 | loss: 63.5309326CurrentTrain: epoch  7, batch     3 | loss: 52.6660202CurrentTrain: epoch  8, batch     0 | loss: 60.4270987CurrentTrain: epoch  8, batch     1 | loss: 84.6607942CurrentTrain: epoch  8, batch     2 | loss: 66.1374908CurrentTrain: epoch  8, batch     3 | loss: 50.2950120CurrentTrain: epoch  9, batch     0 | loss: 53.4523144CurrentTrain: epoch  9, batch     1 | loss: 63.3367070CurrentTrain: epoch  9, batch     2 | loss: 62.7250063CurrentTrain: epoch  9, batch     3 | loss: 71.3949157
MemoryTrain:  epoch  0, batch     0 | loss: 0.3516443MemoryTrain:  epoch  1, batch     0 | loss: 0.3198451MemoryTrain:  epoch  2, batch     0 | loss: 0.2436720MemoryTrain:  epoch  3, batch     0 | loss: 0.1866495MemoryTrain:  epoch  4, batch     0 | loss: 0.1321083MemoryTrain:  epoch  5, batch     0 | loss: 0.1270194MemoryTrain:  epoch  6, batch     0 | loss: 0.0988170MemoryTrain:  epoch  7, batch     0 | loss: 0.0788107MemoryTrain:  epoch  8, batch     0 | loss: 0.0669093MemoryTrain:  epoch  9, batch     0 | loss: 0.0543029

F1 score per class: {32: 0.0, 33: 0.5035971223021583, 35: 0.0, 36: 0.0, 37: 0.6206896551724138, 6: 0.0, 38: 0.0, 8: 0.8888888888888888, 15: 0.0, 19: 0.42857142857142855, 20: 0.0, 26: 0.375, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.4431818181818182
Weighted-average F1 score: 0.3816101586230311
F1 score per class: {32: 0.0, 33: 0.54, 35: 0.0, 36: 0.0, 37: 0.4712041884816754, 38: 0.0, 6: 0.0, 8: 0.0, 15: 0.0, 19: 0.9, 20: 0.0, 24: 0.4444444444444444, 25: 0.0, 26: 0.5605095541401274, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.3548387096774194
Weighted-average F1 score: 0.2887445698996456
F1 score per class: {32: 0.0, 33: 0.542713567839196, 35: 0.0, 36: 0.0, 37: 0.46153846153846156, 38: 0.0, 6: 0.0, 8: 0.0, 15: 0.0, 19: 0.9, 20: 0.0, 24: 0.4444444444444444, 25: 0.0, 26: 0.567741935483871, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.36026200873362446
Weighted-average F1 score: 0.2943901714614947

F1 score per class: {32: 0.3829787234042553, 33: 0.40229885057471265, 35: 0.6956521739130435, 36: 0.5228215767634855, 37: 0.30303030303030304, 6: 0.17391304347826086, 38: 0.375, 8: 0.6278026905829597, 15: 0.8127853881278538, 19: 0.8888888888888888, 20: 0.7669902912621359, 24: 0.35294117647058826, 25: 0.41935483870967744, 26: 0.3564356435643564, 29: 0.5081967213114754, 30: 0.35294117647058826}
Micro-average F1 score: 0.5072072072072072
Weighted-average F1 score: 0.4961166030745136
F1 score per class: {32: 0.398876404494382, 33: 0.35526315789473684, 35: 0.3888888888888889, 36: 0.4797507788161994, 37: 0.2158273381294964, 6: 0.18823529411764706, 38: 0.5, 8: 0.5907172995780591, 15: 0.7250996015936255, 19: 0.6428571428571429, 20: 0.7068273092369478, 24: 0.25, 25: 0.3560606060606061, 26: 0.4756756756756757, 29: 0.2846715328467153, 30: 0.20994475138121546}
Micro-average F1 score: 0.421968787515006
Weighted-average F1 score: 0.4007926230768923
F1 score per class: {32: 0.39436619718309857, 33: 0.3588039867109635, 35: 0.3888888888888889, 36: 0.48427672955974843, 37: 0.21176470588235294, 6: 0.19607843137254902, 38: 0.4810126582278481, 8: 0.5957446808510638, 15: 0.7377049180327869, 19: 0.6, 20: 0.7096774193548387, 24: 0.25806451612903225, 25: 0.35471698113207545, 26: 0.4835164835164835, 29: 0.2846715328467153, 30: 0.2331288343558282}
Micro-average F1 score: 0.426078971533517
Weighted-average F1 score: 0.40528457773809196
cur_acc:  ['0.6919', '0.4536', '0.4432']
his_acc:  ['0.6919', '0.5559', '0.5072']
cur_acc des:  ['0.6645', '0.4238', '0.3548']
his_acc des:  ['0.6645', '0.4666', '0.4220']
cur_acc rrf:  ['0.6650', '0.4282', '0.3603']
his_acc rrf:  ['0.6650', '0.4752', '0.4261']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 98.7252207CurrentTrain: epoch  0, batch     1 | loss: 68.0617481CurrentTrain: epoch  0, batch     2 | loss: 99.9637388CurrentTrain: epoch  0, batch     3 | loss: 78.6575461CurrentTrain: epoch  0, batch     4 | loss: 46.3073206CurrentTrain: epoch  1, batch     0 | loss: 94.0385919CurrentTrain: epoch  1, batch     1 | loss: 93.7829298CurrentTrain: epoch  1, batch     2 | loss: 119.7265069CurrentTrain: epoch  1, batch     3 | loss: 72.4939624CurrentTrain: epoch  1, batch     4 | loss: 70.1798337CurrentTrain: epoch  2, batch     0 | loss: 91.7549955CurrentTrain: epoch  2, batch     1 | loss: 87.6785208CurrentTrain: epoch  2, batch     2 | loss: 56.8815798CurrentTrain: epoch  2, batch     3 | loss: 69.9524312CurrentTrain: epoch  2, batch     4 | loss: 106.7346729CurrentTrain: epoch  3, batch     0 | loss: 86.3667013CurrentTrain: epoch  3, batch     1 | loss: 87.5647575CurrentTrain: epoch  3, batch     2 | loss: 54.6493456CurrentTrain: epoch  3, batch     3 | loss: 87.7094881CurrentTrain: epoch  3, batch     4 | loss: 40.2083298CurrentTrain: epoch  4, batch     0 | loss: 179.0314141CurrentTrain: epoch  4, batch     1 | loss: 185.1841173CurrentTrain: epoch  4, batch     2 | loss: 66.0853133CurrentTrain: epoch  4, batch     3 | loss: 63.3388481CurrentTrain: epoch  4, batch     4 | loss: 48.2284194CurrentTrain: epoch  5, batch     0 | loss: 116.9602597CurrentTrain: epoch  5, batch     1 | loss: 55.3411057CurrentTrain: epoch  5, batch     2 | loss: 67.7393173CurrentTrain: epoch  5, batch     3 | loss: 54.6076023CurrentTrain: epoch  5, batch     4 | loss: 67.1003048CurrentTrain: epoch  6, batch     0 | loss: 65.7540395CurrentTrain: epoch  6, batch     1 | loss: 84.4060623CurrentTrain: epoch  6, batch     2 | loss: 67.3069083CurrentTrain: epoch  6, batch     3 | loss: 67.4613808CurrentTrain: epoch  6, batch     4 | loss: 64.1243815CurrentTrain: epoch  7, batch     0 | loss: 65.3070491CurrentTrain: epoch  7, batch     1 | loss: 67.7335715CurrentTrain: epoch  7, batch     2 | loss: 86.2591767CurrentTrain: epoch  7, batch     3 | loss: 64.8791295CurrentTrain: epoch  7, batch     4 | loss: 46.0526279CurrentTrain: epoch  8, batch     0 | loss: 84.8463591CurrentTrain: epoch  8, batch     1 | loss: 118.4014039CurrentTrain: epoch  8, batch     2 | loss: 63.9079748CurrentTrain: epoch  8, batch     3 | loss: 53.1206848CurrentTrain: epoch  8, batch     4 | loss: 45.8223424CurrentTrain: epoch  9, batch     0 | loss: 51.0637112CurrentTrain: epoch  9, batch     1 | loss: 67.9159929CurrentTrain: epoch  9, batch     2 | loss: 54.5886959CurrentTrain: epoch  9, batch     3 | loss: 84.6204786CurrentTrain: epoch  9, batch     4 | loss: 47.8314992
MemoryTrain:  epoch  0, batch     0 | loss: 0.7109694MemoryTrain:  epoch  1, batch     0 | loss: 0.5530858MemoryTrain:  epoch  2, batch     0 | loss: 0.4221447MemoryTrain:  epoch  3, batch     0 | loss: 0.4109071MemoryTrain:  epoch  4, batch     0 | loss: 0.2904655MemoryTrain:  epoch  5, batch     0 | loss: 0.2378199MemoryTrain:  epoch  6, batch     0 | loss: 0.2212237MemoryTrain:  epoch  7, batch     0 | loss: 0.1478349MemoryTrain:  epoch  8, batch     0 | loss: 0.1370671MemoryTrain:  epoch  9, batch     0 | loss: 0.1288046

F1 score per class: {32: 0.18518518518518517, 1: 0.13861386138613863, 34: 0.0, 35: 0.0, 3: 0.12807881773399016, 37: 0.0, 36: 0.0, 6: 0.5029940119760479, 38: 0.0, 8: 0.0, 14: 0.0, 19: 0.0, 20: 0.1935483870967742, 22: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.1821561338289963
Weighted-average F1 score: 0.1467940873543707
F1 score per class: {1: 0.15498154981549817, 3: 0.3384615384615385, 6: 0.0, 8: 0.0, 14: 0.10408921933085502, 19: 0.0, 20: 0.0, 22: 0.49710982658959535, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.38636363636363635, 35: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.1627260083449235
Weighted-average F1 score: 0.11142689625278482
F1 score per class: {1: 0.18120805369127516, 3: 0.32558139534883723, 6: 0.0, 8: 0.0, 14: 0.09917355371900827, 19: 0.0, 20: 0.0, 22: 0.5142857142857142, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.16393442622950818, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.1588447653429603
Weighted-average F1 score: 0.11394956431593699

F1 score per class: {1: 0.14563106796116504, 3: 0.12389380530973451, 6: 0.325, 8: 0.2677165354330709, 14: 0.09352517985611511, 15: 0.5714285714285714, 19: 0.21476510067114093, 20: 0.31223628691983124, 22: 0.4970414201183432, 24: 0.07692307692307693, 25: 0.375, 26: 0.6101694915254238, 29: 0.7413793103448276, 30: 0.9142857142857143, 32: 0.5523012552301255, 33: 0.4, 34: 0.10909090909090909, 35: 0.12195121951219512, 36: 0.4329896907216495, 37: 0.3333333333333333, 38: 0.18461538461538463}
Micro-average F1 score: 0.33407430930454113
Weighted-average F1 score: 0.3139474843470821
F1 score per class: {1: 0.12883435582822086, 3: 0.24444444444444444, 6: 0.3475409836065574, 8: 0.3157894736842105, 14: 0.07124681933842239, 15: 0.3076923076923077, 19: 0.3380281690140845, 20: 0.2733333333333333, 22: 0.4673913043478261, 24: 0.07692307692307693, 25: 0.5, 26: 0.5658914728682171, 29: 0.6545454545454545, 30: 0.6428571428571429, 32: 0.5289855072463768, 33: 0.3157894736842105, 34: 0.18579234972677597, 35: 0.23460410557184752, 36: 0.384, 37: 0.18548387096774194, 38: 0.1951219512195122}
Micro-average F1 score: 0.3143518518518518
Weighted-average F1 score: 0.293036987337681
F1 score per class: {1: 0.14634146341463414, 3: 0.24561403508771928, 6: 0.3367003367003367, 8: 0.3614457831325301, 14: 0.06896551724137931, 15: 0.41379310344827586, 19: 0.3434343434343434, 20: 0.2662337662337662, 22: 0.48128342245989303, 24: 0.07407407407407407, 25: 0.4411764705882353, 26: 0.584, 29: 0.7206477732793523, 30: 0.7555555555555555, 32: 0.5353159851301115, 33: 0.35294117647058826, 34: 0.08064516129032258, 35: 0.2324159021406728, 36: 0.37681159420289856, 37: 0.20422535211267606, 38: 0.15204678362573099}
Micro-average F1 score: 0.31951807228915663
Weighted-average F1 score: 0.29647651531392544
cur_acc:  ['0.6919', '0.4536', '0.4432', '0.1822']
his_acc:  ['0.6919', '0.5559', '0.5072', '0.3341']
cur_acc des:  ['0.6645', '0.4238', '0.3548', '0.1627']
his_acc des:  ['0.6645', '0.4666', '0.4220', '0.3144']
cur_acc rrf:  ['0.6650', '0.4282', '0.3603', '0.1588']
his_acc rrf:  ['0.6650', '0.4752', '0.4261', '0.3195']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 82.0071361CurrentTrain: epoch  0, batch     1 | loss: 90.2176285CurrentTrain: epoch  0, batch     2 | loss: 73.4388323CurrentTrain: epoch  0, batch     3 | loss: 92.3114812CurrentTrain: epoch  0, batch     4 | loss: 60.8813421CurrentTrain: epoch  1, batch     0 | loss: 89.8421061CurrentTrain: epoch  1, batch     1 | loss: 90.1672981CurrentTrain: epoch  1, batch     2 | loss: 68.6872469CurrentTrain: epoch  1, batch     3 | loss: 73.8054813CurrentTrain: epoch  1, batch     4 | loss: 48.2221563CurrentTrain: epoch  2, batch     0 | loss: 120.1394620CurrentTrain: epoch  2, batch     1 | loss: 72.8566645CurrentTrain: epoch  2, batch     2 | loss: 85.7350011CurrentTrain: epoch  2, batch     3 | loss: 68.5587501CurrentTrain: epoch  2, batch     4 | loss: 42.8338630CurrentTrain: epoch  3, batch     0 | loss: 85.8325669CurrentTrain: epoch  3, batch     1 | loss: 57.4784929CurrentTrain: epoch  3, batch     2 | loss: 121.2789587CurrentTrain: epoch  3, batch     3 | loss: 56.5664244CurrentTrain: epoch  3, batch     4 | loss: 42.1754140CurrentTrain: epoch  4, batch     0 | loss: 65.4550870CurrentTrain: epoch  4, batch     1 | loss: 55.5991364CurrentTrain: epoch  4, batch     2 | loss: 84.8655292CurrentTrain: epoch  4, batch     3 | loss: 87.4006225CurrentTrain: epoch  4, batch     4 | loss: 117.1465505CurrentTrain: epoch  5, batch     0 | loss: 67.8750820CurrentTrain: epoch  5, batch     1 | loss: 118.1524783CurrentTrain: epoch  5, batch     2 | loss: 63.5840653CurrentTrain: epoch  5, batch     3 | loss: 68.9450898CurrentTrain: epoch  5, batch     4 | loss: 52.6323236CurrentTrain: epoch  6, batch     0 | loss: 85.3132185CurrentTrain: epoch  6, batch     1 | loss: 64.9277842CurrentTrain: epoch  6, batch     2 | loss: 85.0846540CurrentTrain: epoch  6, batch     3 | loss: 80.9657815CurrentTrain: epoch  6, batch     4 | loss: 52.8116458CurrentTrain: epoch  7, batch     0 | loss: 85.3420478CurrentTrain: epoch  7, batch     1 | loss: 65.4434211CurrentTrain: epoch  7, batch     2 | loss: 67.2060876CurrentTrain: epoch  7, batch     3 | loss: 82.1338700CurrentTrain: epoch  7, batch     4 | loss: 70.3974228CurrentTrain: epoch  8, batch     0 | loss: 83.7843556CurrentTrain: epoch  8, batch     1 | loss: 52.1712828CurrentTrain: epoch  8, batch     2 | loss: 67.4237152CurrentTrain: epoch  8, batch     3 | loss: 86.0667750CurrentTrain: epoch  8, batch     4 | loss: 51.1515800CurrentTrain: epoch  9, batch     0 | loss: 64.9719606CurrentTrain: epoch  9, batch     1 | loss: 63.4576313CurrentTrain: epoch  9, batch     2 | loss: 84.8203839CurrentTrain: epoch  9, batch     3 | loss: 53.3935898CurrentTrain: epoch  9, batch     4 | loss: 74.1594983
MemoryTrain:  epoch  0, batch     0 | loss: 0.3619479MemoryTrain:  epoch  1, batch     0 | loss: 0.3184320MemoryTrain:  epoch  2, batch     0 | loss: 0.3152249MemoryTrain:  epoch  3, batch     0 | loss: 0.2346306MemoryTrain:  epoch  4, batch     0 | loss: 0.1923841MemoryTrain:  epoch  5, batch     0 | loss: 0.1567640MemoryTrain:  epoch  6, batch     0 | loss: 0.1236885MemoryTrain:  epoch  7, batch     0 | loss: 0.0963762MemoryTrain:  epoch  8, batch     0 | loss: 0.0917396MemoryTrain:  epoch  9, batch     0 | loss: 0.0752911

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.8755760368663594, 6: 0.0, 8: 0.0, 10: 0.45, 14: 0.0, 15: 0.0, 16: 0.55, 17: 0.5714285714285714, 18: 0.27350427350427353, 19: 0.0, 20: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.4707482993197279
Weighted-average F1 score: 0.39581673431557474
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6349206349206349, 6: 0.0, 8: 0.0, 10: 0.5560975609756098, 14: 0.0, 15: 0.0, 16: 0.5094339622641509, 17: 0.2727272727272727, 18: 0.1875, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3290371493555724
Weighted-average F1 score: 0.27825802117728243
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6968641114982579, 6: 0.0, 8: 0.0, 10: 0.5767441860465117, 14: 0.0, 15: 0.0, 16: 0.5142857142857142, 17: 0.2727272727272727, 18: 0.17747440273037543, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3466666666666667
Weighted-average F1 score: 0.2904799804418241

F1 score per class: {1: 0.13953488372093023, 3: 0.171875, 5: 0.7949790794979079, 6: 0.30039525691699603, 8: 0.18487394957983194, 10: 0.2589928057553957, 14: 0.08856088560885608, 15: 0.5217391304347826, 16: 0.4782608695652174, 17: 0.19047619047619047, 18: 0.1306122448979592, 19: 0.30939226519337015, 20: 0.2422360248447205, 22: 0.46706586826347307, 24: 0.06451612903225806, 25: 0.375, 26: 0.6186440677966102, 29: 0.7107438016528925, 30: 0.9142857142857143, 32: 0.5357142857142857, 33: 0.375, 34: 0.05825242718446602, 35: 0.12244897959183673, 36: 0.2826086956521739, 37: 0.2823529411764706, 38: 0.2608695652173913}
Micro-average F1 score: 0.3335729690869878
Weighted-average F1 score: 0.3141364466770291
F1 score per class: {1: 0.11834319526627218, 3: 0.22727272727272727, 5: 0.4694835680751174, 6: 0.28289473684210525, 8: 0.3416370106761566, 10: 0.2651162790697674, 14: 0.07565011820330969, 15: 0.2926829268292683, 16: 0.35526315789473684, 17: 0.06217616580310881, 18: 0.09152542372881356, 19: 0.3269961977186312, 20: 0.19027484143763213, 22: 0.4265402843601896, 24: 0.08333333333333333, 25: 0.4225352112676056, 26: 0.5511811023622047, 29: 0.624561403508772, 30: 0.6545454545454545, 32: 0.4727272727272727, 33: 0.15384615384615385, 34: 0.09032258064516129, 35: 0.1793372319688109, 36: 0.2153846153846154, 37: 0.23728813559322035, 38: 0.14545454545454545}
Micro-average F1 score: 0.26985854189336234
Weighted-average F1 score: 0.2520144143492999
F1 score per class: {1: 0.12121212121212122, 3: 0.23529411764705882, 5: 0.5509641873278237, 6: 0.28104575163398693, 8: 0.3574468085106383, 10: 0.2672413793103448, 14: 0.07425742574257425, 15: 0.3157894736842105, 16: 0.3624161073825503, 17: 0.061855670103092786, 18: 0.0858085808580858, 19: 0.33201581027667987, 20: 0.17681728880157171, 22: 0.45, 24: 0.09523809523809523, 25: 0.4225352112676056, 26: 0.56, 29: 0.6716981132075471, 30: 0.7346938775510204, 32: 0.47706422018348627, 33: 0.17142857142857143, 34: 0.07936507936507936, 35: 0.19298245614035087, 36: 0.24793388429752067, 37: 0.21875, 38: 0.19753086419753085}
Micro-average F1 score: 0.2772245422422101
Weighted-average F1 score: 0.2561186645941915
cur_acc:  ['0.6919', '0.4536', '0.4432', '0.1822', '0.4707']
his_acc:  ['0.6919', '0.5559', '0.5072', '0.3341', '0.3336']
cur_acc des:  ['0.6645', '0.4238', '0.3548', '0.1627', '0.3290']
his_acc des:  ['0.6645', '0.4666', '0.4220', '0.3144', '0.2699']
cur_acc rrf:  ['0.6650', '0.4282', '0.3603', '0.1588', '0.3467']
his_acc rrf:  ['0.6650', '0.4752', '0.4261', '0.3195', '0.2772']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 88.2363219CurrentTrain: epoch  0, batch     1 | loss: 70.9876656CurrentTrain: epoch  0, batch     2 | loss: 85.7373306CurrentTrain: epoch  0, batch     3 | loss: 78.0420272CurrentTrain: epoch  1, batch     0 | loss: 95.7688810CurrentTrain: epoch  1, batch     1 | loss: 73.7746035CurrentTrain: epoch  1, batch     2 | loss: 73.1550550CurrentTrain: epoch  1, batch     3 | loss: 69.6622566CurrentTrain: epoch  2, batch     0 | loss: 71.7029742CurrentTrain: epoch  2, batch     1 | loss: 90.5682852CurrentTrain: epoch  2, batch     2 | loss: 59.5972340CurrentTrain: epoch  2, batch     3 | loss: 70.7414208CurrentTrain: epoch  3, batch     0 | loss: 93.2757094CurrentTrain: epoch  3, batch     1 | loss: 55.5088545CurrentTrain: epoch  3, batch     2 | loss: 59.2369577CurrentTrain: epoch  3, batch     3 | loss: 46.3747615CurrentTrain: epoch  4, batch     0 | loss: 124.2359634CurrentTrain: epoch  4, batch     1 | loss: 71.3033958CurrentTrain: epoch  4, batch     2 | loss: 81.7954695CurrentTrain: epoch  4, batch     3 | loss: 53.8583565CurrentTrain: epoch  5, batch     0 | loss: 64.3262449CurrentTrain: epoch  5, batch     1 | loss: 52.7867190CurrentTrain: epoch  5, batch     2 | loss: 87.9458817CurrentTrain: epoch  5, batch     3 | loss: 97.7190576CurrentTrain: epoch  6, batch     0 | loss: 88.4819983CurrentTrain: epoch  6, batch     1 | loss: 66.2785208CurrentTrain: epoch  6, batch     2 | loss: 63.1856759CurrentTrain: epoch  6, batch     3 | loss: 43.9103465CurrentTrain: epoch  7, batch     0 | loss: 64.8581328CurrentTrain: epoch  7, batch     1 | loss: 117.8757439CurrentTrain: epoch  7, batch     2 | loss: 52.4320825CurrentTrain: epoch  7, batch     3 | loss: 51.9663273CurrentTrain: epoch  8, batch     0 | loss: 52.7935425CurrentTrain: epoch  8, batch     1 | loss: 67.0435285CurrentTrain: epoch  8, batch     2 | loss: 53.7370055CurrentTrain: epoch  8, batch     3 | loss: 66.7944395CurrentTrain: epoch  9, batch     0 | loss: 53.8022100CurrentTrain: epoch  9, batch     1 | loss: 66.4311047CurrentTrain: epoch  9, batch     2 | loss: 64.3665601CurrentTrain: epoch  9, batch     3 | loss: 43.0408918
MemoryTrain:  epoch  0, batch     0 | loss: 0.3521785MemoryTrain:  epoch  1, batch     0 | loss: 0.3148775MemoryTrain:  epoch  2, batch     0 | loss: 0.2172515MemoryTrain:  epoch  3, batch     0 | loss: 0.2013708MemoryTrain:  epoch  4, batch     0 | loss: 0.1579381MemoryTrain:  epoch  5, batch     0 | loss: 0.1146118MemoryTrain:  epoch  6, batch     0 | loss: 0.1134394MemoryTrain:  epoch  7, batch     0 | loss: 0.0915421MemoryTrain:  epoch  8, batch     0 | loss: 0.0816992MemoryTrain:  epoch  9, batch     0 | loss: 0.0721991

F1 score per class: {0: 0.8571428571428571, 1: 0.0, 3: 0.0, 4: 0.9247311827956989, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.07142857142857142, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.19672131147540983, 22: 0.0, 23: 0.6, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5093378607809848
Weighted-average F1 score: 0.3825395460522054
F1 score per class: {0: 0.4539877300613497, 1: 0.0, 3: 0.0, 4: 0.9603960396039604, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.1388888888888889, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.22916666666666666, 22: 0.0, 23: 0.6363636363636364, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.34132310642377756
Weighted-average F1 score: 0.24964173512409718
F1 score per class: {0: 0.5138888888888888, 1: 0.0, 3: 0.0, 4: 0.96, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.11267605633802817, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.23157894736842105, 22: 0.0, 23: 0.6511627906976745, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3566362715298885
Weighted-average F1 score: 0.25898191955437966

F1 score per class: {0: 0.3793103448275862, 1: 0.12177985948477751, 3: 0.18666666666666668, 4: 0.91005291005291, 5: 0.7661290322580645, 6: 0.27312775330396477, 8: 0.1553398058252427, 10: 0.28169014084507044, 13: 0.013513513513513514, 14: 0.06808510638297872, 15: 0.6666666666666666, 16: 0.42857142857142855, 17: 0.10526315789473684, 18: 0.15023474178403756, 19: 0.36936936936936937, 20: 0.21345707656612528, 21: 0.08275862068965517, 22: 0.5, 23: 0.5393258426966292, 24: 0.15384615384615385, 25: 0.375, 26: 0.6036036036036037, 29: 0.6771653543307087, 30: 0.9473684210526315, 32: 0.5354330708661418, 33: 0.42857142857142855, 34: 0.04878048780487805, 35: 0.15384615384615385, 36: 0.3838383838383838, 37: 0.30952380952380953, 38: 0.18181818181818182}
Micro-average F1 score: 0.3439153439153439
Weighted-average F1 score: 0.3125738877859422
F1 score per class: {0: 0.09181141439205956, 1: 0.10218978102189781, 3: 0.20967741935483872, 4: 0.8778280542986425, 5: 0.40404040404040403, 6: 0.2565789473684211, 8: 0.35807860262008734, 10: 0.24624624624624625, 13: 0.028328611898016998, 14: 0.054945054945054944, 15: 0.4444444444444444, 16: 0.36486486486486486, 17: 0.058823529411764705, 18: 0.08710217755443886, 19: 0.3340961098398169, 20: 0.15698587127158556, 21: 0.056847545219638244, 22: 0.4697986577181208, 23: 0.5384615384615384, 24: 0.1016949152542373, 25: 0.4594594594594595, 26: 0.5528455284552846, 29: 0.6267605633802817, 30: 0.6440677966101694, 32: 0.4878048780487805, 33: 0.3157894736842105, 34: 0.1323529411764706, 35: 0.14473684210526316, 36: 0.32748538011695905, 37: 0.2809917355371901, 38: 0.13986013986013987}
Micro-average F1 score: 0.24500114916111237
Weighted-average F1 score: 0.21634410012354255
F1 score per class: {0: 0.11616954474097331, 1: 0.11441647597254005, 3: 0.21645021645021645, 4: 0.927536231884058, 5: 0.5167958656330749, 6: 0.26, 8: 0.37373737373737376, 10: 0.24773413897280966, 13: 0.021447721179624665, 14: 0.04747774480712166, 15: 0.4, 16: 0.3611111111111111, 17: 0.0594059405940594, 18: 0.08291873963515754, 19: 0.3471882640586797, 20: 0.14925373134328357, 21: 0.05432098765432099, 22: 0.47297297297297297, 23: 0.5490196078431373, 24: 0.08888888888888889, 25: 0.4594594594594595, 26: 0.5643153526970954, 29: 0.6472727272727272, 30: 0.7169811320754716, 32: 0.4952978056426332, 33: 0.25, 34: 0.05504587155963303, 35: 0.14859437751004015, 36: 0.33540372670807456, 37: 0.30357142857142855, 38: 0.20454545454545456}
Micro-average F1 score: 0.2552147239263804
Weighted-average F1 score: 0.22310193905750098
cur_acc:  ['0.6919', '0.4536', '0.4432', '0.1822', '0.4707', '0.5093']
his_acc:  ['0.6919', '0.5559', '0.5072', '0.3341', '0.3336', '0.3439']
cur_acc des:  ['0.6645', '0.4238', '0.3548', '0.1627', '0.3290', '0.3413']
his_acc des:  ['0.6645', '0.4666', '0.4220', '0.3144', '0.2699', '0.2450']
cur_acc rrf:  ['0.6650', '0.4282', '0.3603', '0.1588', '0.3467', '0.3566']
his_acc rrf:  ['0.6650', '0.4752', '0.4261', '0.3195', '0.2772', '0.2552']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 109.4345516CurrentTrain: epoch  0, batch     1 | loss: 75.8034410CurrentTrain: epoch  0, batch     2 | loss: 96.5307559CurrentTrain: epoch  0, batch     3 | loss: 73.9274153CurrentTrain: epoch  0, batch     4 | loss: 60.4101861CurrentTrain: epoch  1, batch     0 | loss: 74.8206978CurrentTrain: epoch  1, batch     1 | loss: 64.2855443CurrentTrain: epoch  1, batch     2 | loss: 73.0874720CurrentTrain: epoch  1, batch     3 | loss: 75.3161286CurrentTrain: epoch  1, batch     4 | loss: 60.1323436CurrentTrain: epoch  2, batch     0 | loss: 93.4413246CurrentTrain: epoch  2, batch     1 | loss: 121.5177881CurrentTrain: epoch  2, batch     2 | loss: 73.2650024CurrentTrain: epoch  2, batch     3 | loss: 60.4335524CurrentTrain: epoch  2, batch     4 | loss: 17.9222853CurrentTrain: epoch  3, batch     0 | loss: 89.5095002CurrentTrain: epoch  3, batch     1 | loss: 56.7114308CurrentTrain: epoch  3, batch     2 | loss: 70.3388561CurrentTrain: epoch  3, batch     3 | loss: 118.8726060CurrentTrain: epoch  3, batch     4 | loss: 29.3486296CurrentTrain: epoch  4, batch     0 | loss: 57.3204695CurrentTrain: epoch  4, batch     1 | loss: 184.5528099CurrentTrain: epoch  4, batch     2 | loss: 70.6429978CurrentTrain: epoch  4, batch     3 | loss: 63.9366182CurrentTrain: epoch  4, batch     4 | loss: 27.7463714CurrentTrain: epoch  5, batch     0 | loss: 85.3450011CurrentTrain: epoch  5, batch     1 | loss: 118.1807286CurrentTrain: epoch  5, batch     2 | loss: 69.6163164CurrentTrain: epoch  5, batch     3 | loss: 67.7625046CurrentTrain: epoch  5, batch     4 | loss: 9.9760229CurrentTrain: epoch  6, batch     0 | loss: 86.3045548CurrentTrain: epoch  6, batch     1 | loss: 67.2277255CurrentTrain: epoch  6, batch     2 | loss: 55.2030675CurrentTrain: epoch  6, batch     3 | loss: 65.5187852CurrentTrain: epoch  6, batch     4 | loss: 60.0171534CurrentTrain: epoch  7, batch     0 | loss: 86.7295407CurrentTrain: epoch  7, batch     1 | loss: 63.7363072CurrentTrain: epoch  7, batch     2 | loss: 64.9629269CurrentTrain: epoch  7, batch     3 | loss: 68.0224745CurrentTrain: epoch  7, batch     4 | loss: 27.0533929CurrentTrain: epoch  8, batch     0 | loss: 66.5814605CurrentTrain: epoch  8, batch     1 | loss: 66.0574732CurrentTrain: epoch  8, batch     2 | loss: 117.6911330CurrentTrain: epoch  8, batch     3 | loss: 62.5588182CurrentTrain: epoch  8, batch     4 | loss: 15.9684820CurrentTrain: epoch  9, batch     0 | loss: 62.6169481CurrentTrain: epoch  9, batch     1 | loss: 65.4349748CurrentTrain: epoch  9, batch     2 | loss: 83.3154910CurrentTrain: epoch  9, batch     3 | loss: 86.6784563CurrentTrain: epoch  9, batch     4 | loss: 15.9984048
MemoryTrain:  epoch  0, batch     0 | loss: 0.3501347MemoryTrain:  epoch  1, batch     0 | loss: 0.3147406MemoryTrain:  epoch  2, batch     0 | loss: 0.2292529MemoryTrain:  epoch  3, batch     0 | loss: 0.1908451MemoryTrain:  epoch  4, batch     0 | loss: 0.1504958MemoryTrain:  epoch  5, batch     0 | loss: 0.1363033MemoryTrain:  epoch  6, batch     0 | loss: 0.1233238MemoryTrain:  epoch  7, batch     0 | loss: 0.1007011MemoryTrain:  epoch  8, batch     0 | loss: 0.0914953MemoryTrain:  epoch  9, batch     0 | loss: 0.0810993

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.3684210526315789, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.3130434782608696, 12: 0.4691358024691358, 13: 0.0, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 28: 0.2962962962962963, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.22521008403361345
Weighted-average F1 score: 0.14322455803743434
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.32558139534883723, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.5235602094240838, 12: 0.5597014925373134, 13: 0.0, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 28: 0.19607843137254902, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.41379310344827586}
Micro-average F1 score: 0.23754152823920266
Weighted-average F1 score: 0.1763092272685822
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.30434782608695654, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.49122807017543857, 12: 0.5535055350553506, 13: 0.0, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 28: 0.18181818181818182, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.3076923076923077}
Micro-average F1 score: 0.23539823008849559
Weighted-average F1 score: 0.17444567302086278

F1 score per class: {0: 0.44594594594594594, 1: 0.11940298507462686, 2: 0.1686746987951807, 3: 0.11666666666666667, 4: 0.900523560209424, 5: 0.7710843373493976, 6: 0.32362459546925565, 8: 0.13445378151260504, 10: 0.16541353383458646, 11: 0.2647058823529412, 12: 0.1994750656167979, 13: 0.024096385542168676, 14: 0.11666666666666667, 15: 0.6, 16: 0.38095238095238093, 17: 0.16666666666666666, 18: 0.10847457627118644, 19: 0.35379061371841153, 20: 0.1991701244813278, 21: 0.08928571428571429, 22: 0.4413793103448276, 23: 0.5111111111111111, 24: 0.13793103448275862, 25: 0.375, 26: 0.5929203539823009, 28: 0.09876543209876543, 29: 0.6518518518518519, 30: 1.0, 32: 0.5298507462686567, 33: 0.4, 34: 0.04838709677419355, 35: 0.14857142857142858, 36: 0.2962962962962963, 37: 0.36036036036036034, 38: 0.24, 39: 0.0}
Micro-average F1 score: 0.3204018011776931
Weighted-average F1 score: 0.29682303620649525
F1 score per class: {0: 0.11635220125786164, 1: 0.12060301507537688, 2: 0.14, 3: 0.19672131147540983, 4: 0.9320388349514563, 5: 0.4329004329004329, 6: 0.2918918918918919, 8: 0.2684563758389262, 10: 0.21479713603818615, 11: 0.2331002331002331, 12: 0.12335526315789473, 13: 0.015384615384615385, 14: 0.08333333333333333, 15: 0.375, 16: 0.36, 17: 0.048, 18: 0.06722689075630252, 19: 0.3690773067331671, 20: 0.17375231053604437, 21: 0.05504587155963303, 22: 0.41333333333333333, 23: 0.5490196078431373, 24: 0.08333333333333333, 25: 0.4266666666666667, 26: 0.5761316872427984, 28: 0.07407407407407407, 29: 0.5534591194968553, 30: 0.59375, 32: 0.42455242966751916, 33: 0.2222222222222222, 34: 0.041379310344827586, 35: 0.15867158671586715, 36: 0.30697674418604654, 37: 0.3434343434343434, 38: 0.20408163265306123, 39: 0.125}
Micro-average F1 score: 0.23308993082244428
Weighted-average F1 score: 0.21038999126937297
F1 score per class: {0: 0.1978021978021978, 1: 0.12177985948477751, 2: 0.13592233009708737, 3: 0.1827956989247312, 4: 0.9458128078817734, 5: 0.546448087431694, 6: 0.29591836734693877, 8: 0.26785714285714285, 10: 0.24022346368715083, 11: 0.2616822429906542, 12: 0.11737089201877934, 13: 0.013422818791946308, 14: 0.07222222222222222, 15: 0.41379310344827586, 16: 0.36486486486486486, 17: 0.06382978723404255, 18: 0.06412213740458016, 19: 0.37055837563451777, 20: 0.16013071895424835, 21: 0.06030150753768844, 22: 0.4161073825503356, 23: 0.5544554455445545, 24: 0.0851063829787234, 25: 0.47058823529411764, 26: 0.5882352941176471, 28: 0.06493506493506493, 29: 0.5906040268456376, 30: 0.6440677966101694, 32: 0.43116883116883115, 33: 0.2857142857142857, 34: 0.04411764705882353, 35: 0.16170212765957448, 36: 0.32222222222222224, 37: 0.3652173913043478, 38: 0.2037037037037037, 39: 0.1}
Micro-average F1 score: 0.24354772634166325
Weighted-average F1 score: 0.21765482830904712
cur_acc:  ['0.6919', '0.4536', '0.4432', '0.1822', '0.4707', '0.5093', '0.2252']
his_acc:  ['0.6919', '0.5559', '0.5072', '0.3341', '0.3336', '0.3439', '0.3204']
cur_acc des:  ['0.6645', '0.4238', '0.3548', '0.1627', '0.3290', '0.3413', '0.2375']
his_acc des:  ['0.6645', '0.4666', '0.4220', '0.3144', '0.2699', '0.2450', '0.2331']
cur_acc rrf:  ['0.6650', '0.4282', '0.3603', '0.1588', '0.3467', '0.3566', '0.2354']
his_acc rrf:  ['0.6650', '0.4752', '0.4261', '0.3195', '0.2772', '0.2552', '0.2435']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 71.7622852CurrentTrain: epoch  0, batch     1 | loss: 83.8352525CurrentTrain: epoch  0, batch     2 | loss: 64.4897470CurrentTrain: epoch  0, batch     3 | loss: 12.6934398CurrentTrain: epoch  1, batch     0 | loss: 79.9656146CurrentTrain: epoch  1, batch     1 | loss: 59.9658203CurrentTrain: epoch  1, batch     2 | loss: 60.3025907CurrentTrain: epoch  1, batch     3 | loss: 9.4869966CurrentTrain: epoch  2, batch     0 | loss: 92.2250832CurrentTrain: epoch  2, batch     1 | loss: 67.4293938CurrentTrain: epoch  2, batch     2 | loss: 67.4116650CurrentTrain: epoch  2, batch     3 | loss: 27.6923389CurrentTrain: epoch  3, batch     0 | loss: 67.9759580CurrentTrain: epoch  3, batch     1 | loss: 67.7030018CurrentTrain: epoch  3, batch     2 | loss: 64.4004197CurrentTrain: epoch  3, batch     3 | loss: 27.6589563CurrentTrain: epoch  4, batch     0 | loss: 66.2323215CurrentTrain: epoch  4, batch     1 | loss: 119.1042124CurrentTrain: epoch  4, batch     2 | loss: 50.5410544CurrentTrain: epoch  4, batch     3 | loss: 10.1521345CurrentTrain: epoch  5, batch     0 | loss: 53.0504226CurrentTrain: epoch  5, batch     1 | loss: 84.6623561CurrentTrain: epoch  5, batch     2 | loss: 78.5957768CurrentTrain: epoch  5, batch     3 | loss: 11.1793233CurrentTrain: epoch  6, batch     0 | loss: 85.0393729CurrentTrain: epoch  6, batch     1 | loss: 64.2402930CurrentTrain: epoch  6, batch     2 | loss: 49.7848516CurrentTrain: epoch  6, batch     3 | loss: 5.6776179CurrentTrain: epoch  7, batch     0 | loss: 62.7358045CurrentTrain: epoch  7, batch     1 | loss: 63.5649330CurrentTrain: epoch  7, batch     2 | loss: 82.2044691CurrentTrain: epoch  7, batch     3 | loss: 5.1364863CurrentTrain: epoch  8, batch     0 | loss: 48.5978863CurrentTrain: epoch  8, batch     1 | loss: 86.0101934CurrentTrain: epoch  8, batch     2 | loss: 51.9472294CurrentTrain: epoch  8, batch     3 | loss: 5.6229048CurrentTrain: epoch  9, batch     0 | loss: 63.8548281CurrentTrain: epoch  9, batch     1 | loss: 47.7829574CurrentTrain: epoch  9, batch     2 | loss: 65.8081169CurrentTrain: epoch  9, batch     3 | loss: 27.5201092
MemoryTrain:  epoch  0, batch     0 | loss: 0.3896029MemoryTrain:  epoch  1, batch     0 | loss: 0.2675818MemoryTrain:  epoch  2, batch     0 | loss: 0.2416146MemoryTrain:  epoch  3, batch     0 | loss: 0.2036466MemoryTrain:  epoch  4, batch     0 | loss: 0.1717250MemoryTrain:  epoch  5, batch     0 | loss: 0.1410719MemoryTrain:  epoch  6, batch     0 | loss: 0.1317846MemoryTrain:  epoch  7, batch     0 | loss: 0.1084278MemoryTrain:  epoch  8, batch     0 | loss: 0.0971803MemoryTrain:  epoch  9, batch     0 | loss: 0.0874104

F1 score per class: {0: 0.0, 1: 0.0, 34: 0.0, 3: 0.4444444444444444, 35: 0.847457627118644, 37: 0.0, 32: 0.0, 7: 0.0, 40: 0.0, 9: 0.0, 14: 0.4, 16: 0.0, 17: 0.0, 19: 0.0, 26: 0.0, 27: 0.0, 31: 0.38461538461538464}
Micro-average F1 score: 0.35494880546075086
Weighted-average F1 score: 0.27033538166953563
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 7: 0.5714285714285714, 9: 0.78125, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 27: 0.4375, 28: 0.0, 29: 0.0, 31: 0.19047619047619047, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.5465116279069767}
Micro-average F1 score: 0.3406813627254509
Weighted-average F1 score: 0.2690886079590398
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 7: 0.5714285714285714, 9: 0.7692307692307693, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 27: 0.4, 28: 0.0, 29: 0.0, 31: 0.23529411764705882, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.5476190476190477}
Micro-average F1 score: 0.34297520661157027
Weighted-average F1 score: 0.2709031720796427

F1 score per class: {0: 0.3, 1: 0.10612244897959183, 2: 0.22950819672131148, 3: 0.09917355371900827, 4: 0.9015544041450777, 5: 0.7800829875518672, 6: 0.0, 7: 0.012698412698412698, 8: 0.10526315789473684, 9: 0.78125, 10: 0.09345794392523364, 11: 0.29411764705882354, 12: 0.21602787456445993, 13: 0.02702702702702703, 14: 0.09230769230769231, 15: 0.5454545454545454, 16: 0.40707964601769914, 17: 0.0, 18: 0.12345679012345678, 19: 0.3715170278637771, 20: 0.16415410385259632, 21: 0.030303030303030304, 22: 0.4507042253521127, 23: 0.4883720930232558, 24: 0.06666666666666667, 25: 0.42424242424242425, 26: 0.5814977973568282, 27: 0.062111801242236024, 28: 0.10526315789473684, 29: 0.6910569105691057, 30: 0.9473684210526315, 31: 0.0, 32: 0.504424778761062, 33: 0.4, 34: 0.05454545454545454, 35: 0.13559322033898305, 36: 0.35294117647058826, 37: 0.30927835051546393, 38: 0.24615384615384617, 39: 0.23529411764705882, 40: 0.29411764705882354}
Micro-average F1 score: 0.2958579881656805
Weighted-average F1 score: 0.2689435371010849
F1 score per class: {0: 0.16252821670428894, 1: 0.059113300492610835, 2: 0.1590909090909091, 3: 0.15950920245398773, 4: 0.9230769230769231, 5: 0.46403712296983757, 6: 0.056074766355140186, 7: 0.020671834625323, 8: 0.2222222222222222, 9: 0.6329113924050633, 10: 0.21621621621621623, 11: 0.22172949002217296, 12: 0.15777262180974477, 13: 0.012195121951219513, 14: 0.03215434083601286, 15: 0.4, 16: 0.3783783783783784, 17: 0.18181818181818182, 18: 0.06821705426356589, 19: 0.35924932975871315, 20: 0.16428571428571428, 21: 0.06040268456375839, 22: 0.45517241379310347, 23: 0.5544554455445545, 24: 0.06818181818181818, 25: 0.40963855421686746, 26: 0.525096525096525, 27: 0.04501607717041801, 28: 0.05063291139240506, 29: 0.6263345195729537, 30: 0.7037037037037037, 31: 0.0196078431372549, 32: 0.46984126984126984, 33: 0.3, 34: 0.061068702290076333, 35: 0.19811320754716982, 36: 0.35384615384615387, 37: 0.3364485981308411, 38: 0.2, 39: 0.136986301369863, 40: 0.28313253012048195}
Micro-average F1 score: 0.23219973286756396
Weighted-average F1 score: 0.2069561684065875
F1 score per class: {0: 0.15611814345991562, 1: 0.07174887892376682, 2: 0.1891891891891892, 3: 0.14606741573033707, 4: 0.9306930693069307, 5: 0.625, 6: 0.056074766355140186, 7: 0.018561484918793503, 8: 0.2, 9: 0.684931506849315, 10: 0.21468926553672316, 11: 0.28013029315960913, 12: 0.15207373271889402, 13: 0.0125, 14: 0.05673758865248227, 15: 0.41379310344827586, 16: 0.391304347826087, 17: 0.3333333333333333, 18: 0.06741573033707865, 19: 0.39285714285714285, 20: 0.1590909090909091, 21: 0.07468879668049792, 22: 0.4594594594594595, 23: 0.5346534653465347, 24: 0.08450704225352113, 25: 0.48, 26: 0.5447154471544715, 27: 0.03636363636363636, 28: 0.05309734513274336, 29: 0.651685393258427, 30: 0.7450980392156863, 31: 0.031007751937984496, 32: 0.4807692307692308, 33: 0.35294117647058826, 34: 0.06349206349206349, 35: 0.17066666666666666, 36: 0.3548387096774194, 37: 0.38333333333333336, 38: 0.21818181818181817, 39: 0.1509433962264151, 40: 0.2875}
Micro-average F1 score: 0.2399655654793931
Weighted-average F1 score: 0.20987002854070053
cur_acc:  ['0.6919', '0.4536', '0.4432', '0.1822', '0.4707', '0.5093', '0.2252', '0.3549']
his_acc:  ['0.6919', '0.5559', '0.5072', '0.3341', '0.3336', '0.3439', '0.3204', '0.2959']
cur_acc des:  ['0.6645', '0.4238', '0.3548', '0.1627', '0.3290', '0.3413', '0.2375', '0.3407']
his_acc des:  ['0.6645', '0.4666', '0.4220', '0.3144', '0.2699', '0.2450', '0.2331', '0.2322']
cur_acc rrf:  ['0.6650', '0.4282', '0.3603', '0.1588', '0.3467', '0.3566', '0.2354', '0.3430']
his_acc rrf:  ['0.6650', '0.4752', '0.4261', '0.3195', '0.2772', '0.2552', '0.2435', '0.2400']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 82.4746391CurrentTrain: epoch  0, batch     1 | loss: 67.7711373CurrentTrain: epoch  0, batch     2 | loss: 79.3446819CurrentTrain: epoch  0, batch     3 | loss: 128.4703679CurrentTrain: epoch  0, batch     4 | loss: 66.7047399CurrentTrain: epoch  0, batch     5 | loss: 97.6224210CurrentTrain: epoch  0, batch     6 | loss: 81.8770208CurrentTrain: epoch  0, batch     7 | loss: 78.1691420CurrentTrain: epoch  0, batch     8 | loss: 65.4937502CurrentTrain: epoch  0, batch     9 | loss: 78.1792106CurrentTrain: epoch  0, batch    10 | loss: 97.4343733CurrentTrain: epoch  0, batch    11 | loss: 79.2617367CurrentTrain: epoch  0, batch    12 | loss: 96.5468646CurrentTrain: epoch  0, batch    13 | loss: 97.6399688CurrentTrain: epoch  0, batch    14 | loss: 79.1886709CurrentTrain: epoch  0, batch    15 | loss: 56.5977494CurrentTrain: epoch  0, batch    16 | loss: 56.6516746CurrentTrain: epoch  0, batch    17 | loss: 65.1023196CurrentTrain: epoch  0, batch    18 | loss: 77.8242875CurrentTrain: epoch  0, batch    19 | loss: 78.7039779CurrentTrain: epoch  0, batch    20 | loss: 125.8436553CurrentTrain: epoch  0, batch    21 | loss: 65.1917087CurrentTrain: epoch  0, batch    22 | loss: 77.4335509CurrentTrain: epoch  0, batch    23 | loss: 77.1371882CurrentTrain: epoch  0, batch    24 | loss: 77.9213668CurrentTrain: epoch  0, batch    25 | loss: 95.7993024CurrentTrain: epoch  0, batch    26 | loss: 77.2726289CurrentTrain: epoch  0, batch    27 | loss: 64.7955076CurrentTrain: epoch  0, batch    28 | loss: 77.3109936CurrentTrain: epoch  0, batch    29 | loss: 64.4073556CurrentTrain: epoch  0, batch    30 | loss: 76.8440676CurrentTrain: epoch  0, batch    31 | loss: 65.1758026CurrentTrain: epoch  0, batch    32 | loss: 126.4262430CurrentTrain: epoch  0, batch    33 | loss: 77.4732669CurrentTrain: epoch  0, batch    34 | loss: 55.8291464CurrentTrain: epoch  0, batch    35 | loss: 95.6127126CurrentTrain: epoch  0, batch    36 | loss: 64.7408992CurrentTrain: epoch  0, batch    37 | loss: 66.2043306CurrentTrain: epoch  0, batch    38 | loss: 76.8306181CurrentTrain: epoch  0, batch    39 | loss: 64.1253432CurrentTrain: epoch  0, batch    40 | loss: 77.1914316CurrentTrain: epoch  0, batch    41 | loss: 55.9019068CurrentTrain: epoch  0, batch    42 | loss: 94.6491265CurrentTrain: epoch  0, batch    43 | loss: 95.7982374CurrentTrain: epoch  0, batch    44 | loss: 125.8601644CurrentTrain: epoch  0, batch    45 | loss: 64.3700311CurrentTrain: epoch  0, batch    46 | loss: 187.5654098CurrentTrain: epoch  0, batch    47 | loss: 64.6144354CurrentTrain: epoch  0, batch    48 | loss: 77.4506313CurrentTrain: epoch  0, batch    49 | loss: 76.4993020CurrentTrain: epoch  0, batch    50 | loss: 77.3907682CurrentTrain: epoch  0, batch    51 | loss: 96.8163073CurrentTrain: epoch  0, batch    52 | loss: 65.7871913CurrentTrain: epoch  0, batch    53 | loss: 64.1813373CurrentTrain: epoch  0, batch    54 | loss: 64.0439255CurrentTrain: epoch  0, batch    55 | loss: 63.4638486CurrentTrain: epoch  0, batch    56 | loss: 94.5082026CurrentTrain: epoch  0, batch    57 | loss: 63.8967704CurrentTrain: epoch  0, batch    58 | loss: 54.6323335CurrentTrain: epoch  0, batch    59 | loss: 76.2772126CurrentTrain: epoch  0, batch    60 | loss: 124.8774128CurrentTrain: epoch  0, batch    61 | loss: 76.5100194CurrentTrain: epoch  0, batch    62 | loss: 63.6784717CurrentTrain: epoch  0, batch    63 | loss: 94.0786979CurrentTrain: epoch  0, batch    64 | loss: 76.0919077CurrentTrain: epoch  0, batch    65 | loss: 76.2201463CurrentTrain: epoch  0, batch    66 | loss: 76.1788000CurrentTrain: epoch  0, batch    67 | loss: 76.6565607CurrentTrain: epoch  0, batch    68 | loss: 94.6861932CurrentTrain: epoch  0, batch    69 | loss: 124.6727511CurrentTrain: epoch  0, batch    70 | loss: 76.2552379CurrentTrain: epoch  0, batch    71 | loss: 63.8483057CurrentTrain: epoch  0, batch    72 | loss: 95.3248715CurrentTrain: epoch  0, batch    73 | loss: 123.9483465CurrentTrain: epoch  0, batch    74 | loss: 54.2579993CurrentTrain: epoch  0, batch    75 | loss: 75.6885567CurrentTrain: epoch  0, batch    76 | loss: 93.2094658CurrentTrain: epoch  0, batch    77 | loss: 75.5129268CurrentTrain: epoch  0, batch    78 | loss: 75.4144313CurrentTrain: epoch  0, batch    79 | loss: 75.8993509CurrentTrain: epoch  0, batch    80 | loss: 124.4626584CurrentTrain: epoch  0, batch    81 | loss: 93.8766055CurrentTrain: epoch  0, batch    82 | loss: 93.0419687CurrentTrain: epoch  0, batch    83 | loss: 92.9048513CurrentTrain: epoch  0, batch    84 | loss: 62.5665046CurrentTrain: epoch  0, batch    85 | loss: 61.7381172CurrentTrain: epoch  0, batch    86 | loss: 53.9685642CurrentTrain: epoch  0, batch    87 | loss: 63.0973267CurrentTrain: epoch  0, batch    88 | loss: 53.4195986CurrentTrain: epoch  0, batch    89 | loss: 76.0236514CurrentTrain: epoch  0, batch    90 | loss: 94.2723615CurrentTrain: epoch  0, batch    91 | loss: 63.6773786CurrentTrain: epoch  0, batch    92 | loss: 61.2859068CurrentTrain: epoch  0, batch    93 | loss: 91.3905341CurrentTrain: epoch  0, batch    94 | loss: 75.3816065CurrentTrain: epoch  0, batch    95 | loss: 53.1434778CurrentTrain: epoch  1, batch     0 | loss: 123.9257710CurrentTrain: epoch  1, batch     1 | loss: 61.1525267CurrentTrain: epoch  1, batch     2 | loss: 59.9479661CurrentTrain: epoch  1, batch     3 | loss: 122.7949559CurrentTrain: epoch  1, batch     4 | loss: 61.9487624CurrentTrain: epoch  1, batch     5 | loss: 72.8557595CurrentTrain: epoch  1, batch     6 | loss: 60.3710146CurrentTrain: epoch  1, batch     7 | loss: 124.5455774CurrentTrain: epoch  1, batch     8 | loss: 90.6827878CurrentTrain: epoch  1, batch     9 | loss: 52.0652616CurrentTrain: epoch  1, batch    10 | loss: 63.5757484CurrentTrain: epoch  1, batch    11 | loss: 71.9529385CurrentTrain: epoch  1, batch    12 | loss: 91.4629469CurrentTrain: epoch  1, batch    13 | loss: 92.6481785CurrentTrain: epoch  1, batch    14 | loss: 94.5098520CurrentTrain: epoch  1, batch    15 | loss: 58.9539542CurrentTrain: epoch  1, batch    16 | loss: 118.7774749CurrentTrain: epoch  1, batch    17 | loss: 59.8085687CurrentTrain: epoch  1, batch    18 | loss: 72.0044520CurrentTrain: epoch  1, batch    19 | loss: 122.3944937CurrentTrain: epoch  1, batch    20 | loss: 185.8130270CurrentTrain: epoch  1, batch    21 | loss: 72.5462054CurrentTrain: epoch  1, batch    22 | loss: 59.3785574CurrentTrain: epoch  1, batch    23 | loss: 72.0542975CurrentTrain: epoch  1, batch    24 | loss: 87.4745304CurrentTrain: epoch  1, batch    25 | loss: 92.3558129CurrentTrain: epoch  1, batch    26 | loss: 58.9897344CurrentTrain: epoch  1, batch    27 | loss: 73.7938262CurrentTrain: epoch  1, batch    28 | loss: 72.3105391CurrentTrain: epoch  1, batch    29 | loss: 58.0028364CurrentTrain: epoch  1, batch    30 | loss: 69.9172818CurrentTrain: epoch  1, batch    31 | loss: 59.4215088CurrentTrain: epoch  1, batch    32 | loss: 58.8725087CurrentTrain: epoch  1, batch    33 | loss: 87.0938259CurrentTrain: epoch  1, batch    34 | loss: 58.8125244CurrentTrain: epoch  1, batch    35 | loss: 56.8320994CurrentTrain: epoch  1, batch    36 | loss: 121.2470969CurrentTrain: epoch  1, batch    37 | loss: 58.6485452CurrentTrain: epoch  1, batch    38 | loss: 89.6775603CurrentTrain: epoch  1, batch    39 | loss: 72.5117735CurrentTrain: epoch  1, batch    40 | loss: 60.4262089CurrentTrain: epoch  1, batch    41 | loss: 184.4787185CurrentTrain: epoch  1, batch    42 | loss: 59.9416419CurrentTrain: epoch  1, batch    43 | loss: 121.4306784CurrentTrain: epoch  1, batch    44 | loss: 57.2088836CurrentTrain: epoch  1, batch    45 | loss: 70.1917572CurrentTrain: epoch  1, batch    46 | loss: 67.6738195CurrentTrain: epoch  1, batch    47 | loss: 188.1813636CurrentTrain: epoch  1, batch    48 | loss: 83.3880107CurrentTrain: epoch  1, batch    49 | loss: 69.5917812CurrentTrain: epoch  1, batch    50 | loss: 117.0283588CurrentTrain: epoch  1, batch    51 | loss: 86.0092908CurrentTrain: epoch  1, batch    52 | loss: 67.4342096CurrentTrain: epoch  1, batch    53 | loss: 72.8549754CurrentTrain: epoch  1, batch    54 | loss: 60.1872709CurrentTrain: epoch  1, batch    55 | loss: 120.8185370CurrentTrain: epoch  1, batch    56 | loss: 56.6324093CurrentTrain: epoch  1, batch    57 | loss: 86.1265541CurrentTrain: epoch  1, batch    58 | loss: 73.6203593CurrentTrain: epoch  1, batch    59 | loss: 67.5692215CurrentTrain: epoch  1, batch    60 | loss: 62.6822470CurrentTrain: epoch  1, batch    61 | loss: 58.0484596CurrentTrain: epoch  1, batch    62 | loss: 69.3885044CurrentTrain: epoch  1, batch    63 | loss: 70.4984573CurrentTrain: epoch  1, batch    64 | loss: 56.8424751CurrentTrain: epoch  1, batch    65 | loss: 70.6252361CurrentTrain: epoch  1, batch    66 | loss: 92.1961033CurrentTrain: epoch  1, batch    67 | loss: 87.3514678CurrentTrain: epoch  1, batch    68 | loss: 56.0329895CurrentTrain: epoch  1, batch    69 | loss: 69.6674552CurrentTrain: epoch  1, batch    70 | loss: 70.4787017CurrentTrain: epoch  1, batch    71 | loss: 45.7156596CurrentTrain: epoch  1, batch    72 | loss: 90.7995985CurrentTrain: epoch  1, batch    73 | loss: 56.0353478CurrentTrain: epoch  1, batch    74 | loss: 72.0901042CurrentTrain: epoch  1, batch    75 | loss: 56.4195600CurrentTrain: epoch  1, batch    76 | loss: 62.9098558CurrentTrain: epoch  1, batch    77 | loss: 68.4103406CurrentTrain: epoch  1, batch    78 | loss: 88.7851701CurrentTrain: epoch  1, batch    79 | loss: 57.5500271CurrentTrain: epoch  1, batch    80 | loss: 66.2024515CurrentTrain: epoch  1, batch    81 | loss: 60.0406705CurrentTrain: epoch  1, batch    82 | loss: 119.0371546CurrentTrain: epoch  1, batch    83 | loss: 55.9716380CurrentTrain: epoch  1, batch    84 | loss: 122.8128379CurrentTrain: epoch  1, batch    85 | loss: 92.1104971CurrentTrain: epoch  1, batch    86 | loss: 87.2286195CurrentTrain: epoch  1, batch    87 | loss: 46.8384689CurrentTrain: epoch  1, batch    88 | loss: 68.6530595CurrentTrain: epoch  1, batch    89 | loss: 69.4683181CurrentTrain: epoch  1, batch    90 | loss: 56.3374657CurrentTrain: epoch  1, batch    91 | loss: 88.3989594CurrentTrain: epoch  1, batch    92 | loss: 55.9180041CurrentTrain: epoch  1, batch    93 | loss: 61.6028448CurrentTrain: epoch  1, batch    94 | loss: 89.8998884CurrentTrain: epoch  1, batch    95 | loss: 56.9495787CurrentTrain: epoch  2, batch     0 | loss: 117.7081714CurrentTrain: epoch  2, batch     1 | loss: 86.4314662CurrentTrain: epoch  2, batch     2 | loss: 86.0179797CurrentTrain: epoch  2, batch     3 | loss: 71.8634997CurrentTrain: epoch  2, batch     4 | loss: 70.0300858CurrentTrain: epoch  2, batch     5 | loss: 52.8293727CurrentTrain: epoch  2, batch     6 | loss: 56.7905829CurrentTrain: epoch  2, batch     7 | loss: 69.5529608CurrentTrain: epoch  2, batch     8 | loss: 91.5288075CurrentTrain: epoch  2, batch     9 | loss: 70.3349790CurrentTrain: epoch  2, batch    10 | loss: 49.3792804CurrentTrain: epoch  2, batch    11 | loss: 48.9810534CurrentTrain: epoch  2, batch    12 | loss: 54.7633933CurrentTrain: epoch  2, batch    13 | loss: 113.1070509CurrentTrain: epoch  2, batch    14 | loss: 67.3084546CurrentTrain: epoch  2, batch    15 | loss: 70.3084056CurrentTrain: epoch  2, batch    16 | loss: 66.0977721CurrentTrain: epoch  2, batch    17 | loss: 53.0959620CurrentTrain: epoch  2, batch    18 | loss: 68.0529057CurrentTrain: epoch  2, batch    19 | loss: 118.3598801CurrentTrain: epoch  2, batch    20 | loss: 46.7171107CurrentTrain: epoch  2, batch    21 | loss: 57.4650553CurrentTrain: epoch  2, batch    22 | loss: 63.5787876CurrentTrain: epoch  2, batch    23 | loss: 117.7356309CurrentTrain: epoch  2, batch    24 | loss: 88.9202141CurrentTrain: epoch  2, batch    25 | loss: 68.3448446CurrentTrain: epoch  2, batch    26 | loss: 55.9885015CurrentTrain: epoch  2, batch    27 | loss: 88.2597446CurrentTrain: epoch  2, batch    28 | loss: 56.1294570CurrentTrain: epoch  2, batch    29 | loss: 68.3723785CurrentTrain: epoch  2, batch    30 | loss: 63.5968726CurrentTrain: epoch  2, batch    31 | loss: 88.9329683CurrentTrain: epoch  2, batch    32 | loss: 119.3018685CurrentTrain: epoch  2, batch    33 | loss: 89.2241474CurrentTrain: epoch  2, batch    34 | loss: 87.7211733CurrentTrain: epoch  2, batch    35 | loss: 119.9836910CurrentTrain: epoch  2, batch    36 | loss: 64.6013256CurrentTrain: epoch  2, batch    37 | loss: 67.3806025CurrentTrain: epoch  2, batch    38 | loss: 58.1217997CurrentTrain: epoch  2, batch    39 | loss: 68.0417574CurrentTrain: epoch  2, batch    40 | loss: 87.9137181CurrentTrain: epoch  2, batch    41 | loss: 58.3780996CurrentTrain: epoch  2, batch    42 | loss: 88.1890140CurrentTrain: epoch  2, batch    43 | loss: 87.2876919CurrentTrain: epoch  2, batch    44 | loss: 69.6867141CurrentTrain: epoch  2, batch    45 | loss: 68.4232021CurrentTrain: epoch  2, batch    46 | loss: 57.8748690CurrentTrain: epoch  2, batch    47 | loss: 91.8841312CurrentTrain: epoch  2, batch    48 | loss: 45.6729444CurrentTrain: epoch  2, batch    49 | loss: 91.6493523CurrentTrain: epoch  2, batch    50 | loss: 66.2564998CurrentTrain: epoch  2, batch    51 | loss: 66.7530389CurrentTrain: epoch  2, batch    52 | loss: 88.4998707CurrentTrain: epoch  2, batch    53 | loss: 86.7877770CurrentTrain: epoch  2, batch    54 | loss: 52.6021788CurrentTrain: epoch  2, batch    55 | loss: 45.2841523CurrentTrain: epoch  2, batch    56 | loss: 184.6137528CurrentTrain: epoch  2, batch    57 | loss: 87.6999891CurrentTrain: epoch  2, batch    58 | loss: 58.8269121CurrentTrain: epoch  2, batch    59 | loss: 87.7032342CurrentTrain: epoch  2, batch    60 | loss: 68.8994987CurrentTrain: epoch  2, batch    61 | loss: 66.5362037CurrentTrain: epoch  2, batch    62 | loss: 179.9566919CurrentTrain: epoch  2, batch    63 | loss: 85.4392729CurrentTrain: epoch  2, batch    64 | loss: 87.2459848CurrentTrain: epoch  2, batch    65 | loss: 118.3189036CurrentTrain: epoch  2, batch    66 | loss: 118.4389655CurrentTrain: epoch  2, batch    67 | loss: 69.4484121CurrentTrain: epoch  2, batch    68 | loss: 51.1305512CurrentTrain: epoch  2, batch    69 | loss: 89.1591450CurrentTrain: epoch  2, batch    70 | loss: 86.0796673CurrentTrain: epoch  2, batch    71 | loss: 67.2721335CurrentTrain: epoch  2, batch    72 | loss: 66.2131632CurrentTrain: epoch  2, batch    73 | loss: 68.4416824CurrentTrain: epoch  2, batch    74 | loss: 89.1625811CurrentTrain: epoch  2, batch    75 | loss: 66.8530855CurrentTrain: epoch  2, batch    76 | loss: 90.2156842CurrentTrain: epoch  2, batch    77 | loss: 112.2978085CurrentTrain: epoch  2, batch    78 | loss: 57.0636605CurrentTrain: epoch  2, batch    79 | loss: 65.9284743CurrentTrain: epoch  2, batch    80 | loss: 49.4845735CurrentTrain: epoch  2, batch    81 | loss: 48.7422682CurrentTrain: epoch  2, batch    82 | loss: 71.2023161CurrentTrain: epoch  2, batch    83 | loss: 87.0461755CurrentTrain: epoch  2, batch    84 | loss: 51.6647946CurrentTrain: epoch  2, batch    85 | loss: 88.6791217CurrentTrain: epoch  2, batch    86 | loss: 44.8134813CurrentTrain: epoch  2, batch    87 | loss: 67.2715612CurrentTrain: epoch  2, batch    88 | loss: 48.5342950CurrentTrain: epoch  2, batch    89 | loss: 68.6523632CurrentTrain: epoch  2, batch    90 | loss: 69.3856477CurrentTrain: epoch  2, batch    91 | loss: 91.0112207CurrentTrain: epoch  2, batch    92 | loss: 55.8042748CurrentTrain: epoch  2, batch    93 | loss: 69.2393752CurrentTrain: epoch  2, batch    94 | loss: 86.2915668CurrentTrain: epoch  2, batch    95 | loss: 54.6621099CurrentTrain: epoch  3, batch     0 | loss: 65.6981280CurrentTrain: epoch  3, batch     1 | loss: 56.8621255CurrentTrain: epoch  3, batch     2 | loss: 69.0317719CurrentTrain: epoch  3, batch     3 | loss: 55.5507714CurrentTrain: epoch  3, batch     4 | loss: 117.4053885CurrentTrain: epoch  3, batch     5 | loss: 79.1002308CurrentTrain: epoch  3, batch     6 | loss: 66.3158374CurrentTrain: epoch  3, batch     7 | loss: 70.7248664CurrentTrain: epoch  3, batch     8 | loss: 54.8793646CurrentTrain: epoch  3, batch     9 | loss: 88.0493842CurrentTrain: epoch  3, batch    10 | loss: 68.6063964CurrentTrain: epoch  3, batch    11 | loss: 52.8690446CurrentTrain: epoch  3, batch    12 | loss: 66.1299045CurrentTrain: epoch  3, batch    13 | loss: 68.3700039CurrentTrain: epoch  3, batch    14 | loss: 68.1079802CurrentTrain: epoch  3, batch    15 | loss: 89.8444438CurrentTrain: epoch  3, batch    16 | loss: 85.3039518CurrentTrain: epoch  3, batch    17 | loss: 55.5095644CurrentTrain: epoch  3, batch    18 | loss: 67.6672785CurrentTrain: epoch  3, batch    19 | loss: 69.1681320CurrentTrain: epoch  3, batch    20 | loss: 117.9800707CurrentTrain: epoch  3, batch    21 | loss: 53.1999111CurrentTrain: epoch  3, batch    22 | loss: 54.8489772CurrentTrain: epoch  3, batch    23 | loss: 67.8911009CurrentTrain: epoch  3, batch    24 | loss: 49.9207777CurrentTrain: epoch  3, batch    25 | loss: 68.7724868CurrentTrain: epoch  3, batch    26 | loss: 85.9659937CurrentTrain: epoch  3, batch    27 | loss: 61.0099449CurrentTrain: epoch  3, batch    28 | loss: 51.9876891CurrentTrain: epoch  3, batch    29 | loss: 65.0117695CurrentTrain: epoch  3, batch    30 | loss: 52.5403418CurrentTrain: epoch  3, batch    31 | loss: 69.6202592CurrentTrain: epoch  3, batch    32 | loss: 65.0115349CurrentTrain: epoch  3, batch    33 | loss: 55.1519098CurrentTrain: epoch  3, batch    34 | loss: 55.2959945CurrentTrain: epoch  3, batch    35 | loss: 46.0491180CurrentTrain: epoch  3, batch    36 | loss: 47.9599232CurrentTrain: epoch  3, batch    37 | loss: 57.3546050CurrentTrain: epoch  3, batch    38 | loss: 62.4216992CurrentTrain: epoch  3, batch    39 | loss: 53.2541396CurrentTrain: epoch  3, batch    40 | loss: 66.2389184CurrentTrain: epoch  3, batch    41 | loss: 81.8211219CurrentTrain: epoch  3, batch    42 | loss: 83.9904383CurrentTrain: epoch  3, batch    43 | loss: 86.0340793CurrentTrain: epoch  3, batch    44 | loss: 83.3698734CurrentTrain: epoch  3, batch    45 | loss: 67.3588546CurrentTrain: epoch  3, batch    46 | loss: 85.1259236CurrentTrain: epoch  3, batch    47 | loss: 47.2581303CurrentTrain: epoch  3, batch    48 | loss: 69.4646426CurrentTrain: epoch  3, batch    49 | loss: 86.6106207CurrentTrain: epoch  3, batch    50 | loss: 119.2747842CurrentTrain: epoch  3, batch    51 | loss: 87.5094280CurrentTrain: epoch  3, batch    52 | loss: 45.3252083CurrentTrain: epoch  3, batch    53 | loss: 65.8495311CurrentTrain: epoch  3, batch    54 | loss: 86.3839888CurrentTrain: epoch  3, batch    55 | loss: 51.1650135CurrentTrain: epoch  3, batch    56 | loss: 68.8455129CurrentTrain: epoch  3, batch    57 | loss: 43.1032402CurrentTrain: epoch  3, batch    58 | loss: 112.5296956CurrentTrain: epoch  3, batch    59 | loss: 52.9348679CurrentTrain: epoch  3, batch    60 | loss: 82.8729977CurrentTrain: epoch  3, batch    61 | loss: 117.1942723CurrentTrain: epoch  3, batch    62 | loss: 116.6219475CurrentTrain: epoch  3, batch    63 | loss: 54.5453984CurrentTrain: epoch  3, batch    64 | loss: 85.3012232CurrentTrain: epoch  3, batch    65 | loss: 87.5346174CurrentTrain: epoch  3, batch    66 | loss: 83.8018420CurrentTrain: epoch  3, batch    67 | loss: 57.3339200CurrentTrain: epoch  3, batch    68 | loss: 52.9130063CurrentTrain: epoch  3, batch    69 | loss: 69.2233593CurrentTrain: epoch  3, batch    70 | loss: 87.4829864CurrentTrain: epoch  3, batch    71 | loss: 84.9130432CurrentTrain: epoch  3, batch    72 | loss: 86.4157267CurrentTrain: epoch  3, batch    73 | loss: 86.3207073CurrentTrain: epoch  3, batch    74 | loss: 49.0847874CurrentTrain: epoch  3, batch    75 | loss: 66.7064062CurrentTrain: epoch  3, batch    76 | loss: 86.2496206CurrentTrain: epoch  3, batch    77 | loss: 85.2899226CurrentTrain: epoch  3, batch    78 | loss: 86.9977760CurrentTrain: epoch  3, batch    79 | loss: 57.9917238CurrentTrain: epoch  3, batch    80 | loss: 66.9805285CurrentTrain: epoch  3, batch    81 | loss: 119.8768082CurrentTrain: epoch  3, batch    82 | loss: 119.0785714CurrentTrain: epoch  3, batch    83 | loss: 85.0459879CurrentTrain: epoch  3, batch    84 | loss: 64.8160330CurrentTrain: epoch  3, batch    85 | loss: 67.0106293CurrentTrain: epoch  3, batch    86 | loss: 66.8241518CurrentTrain: epoch  3, batch    87 | loss: 90.2005453CurrentTrain: epoch  3, batch    88 | loss: 56.8450054CurrentTrain: epoch  3, batch    89 | loss: 86.4341250CurrentTrain: epoch  3, batch    90 | loss: 57.9199579CurrentTrain: epoch  3, batch    91 | loss: 54.6769673CurrentTrain: epoch  3, batch    92 | loss: 67.8016601CurrentTrain: epoch  3, batch    93 | loss: 87.6789009CurrentTrain: epoch  3, batch    94 | loss: 69.6650415CurrentTrain: epoch  3, batch    95 | loss: 72.1384893CurrentTrain: epoch  4, batch     0 | loss: 85.5832568CurrentTrain: epoch  4, batch     1 | loss: 65.8297274CurrentTrain: epoch  4, batch     2 | loss: 68.7975768CurrentTrain: epoch  4, batch     3 | loss: 84.5321645CurrentTrain: epoch  4, batch     4 | loss: 45.7185518CurrentTrain: epoch  4, batch     5 | loss: 85.4063101CurrentTrain: epoch  4, batch     6 | loss: 83.4435216CurrentTrain: epoch  4, batch     7 | loss: 181.9154002CurrentTrain: epoch  4, batch     8 | loss: 51.1897349CurrentTrain: epoch  4, batch     9 | loss: 83.4994644CurrentTrain: epoch  4, batch    10 | loss: 65.6671218CurrentTrain: epoch  4, batch    11 | loss: 53.5846436CurrentTrain: epoch  4, batch    12 | loss: 53.9817473CurrentTrain: epoch  4, batch    13 | loss: 54.3370260CurrentTrain: epoch  4, batch    14 | loss: 58.8889169CurrentTrain: epoch  4, batch    15 | loss: 50.3113544CurrentTrain: epoch  4, batch    16 | loss: 44.3531226CurrentTrain: epoch  4, batch    17 | loss: 55.1498511CurrentTrain: epoch  4, batch    18 | loss: 51.1244197CurrentTrain: epoch  4, batch    19 | loss: 66.0400386CurrentTrain: epoch  4, batch    20 | loss: 85.7825239CurrentTrain: epoch  4, batch    21 | loss: 54.0051034CurrentTrain: epoch  4, batch    22 | loss: 65.8204172CurrentTrain: epoch  4, batch    23 | loss: 117.3691499CurrentTrain: epoch  4, batch    24 | loss: 121.4632454CurrentTrain: epoch  4, batch    25 | loss: 65.8525306CurrentTrain: epoch  4, batch    26 | loss: 78.0389770CurrentTrain: epoch  4, batch    27 | loss: 53.2945096CurrentTrain: epoch  4, batch    28 | loss: 118.7415984CurrentTrain: epoch  4, batch    29 | loss: 67.3920227CurrentTrain: epoch  4, batch    30 | loss: 82.5656200CurrentTrain: epoch  4, batch    31 | loss: 118.7297202CurrentTrain: epoch  4, batch    32 | loss: 67.4068677CurrentTrain: epoch  4, batch    33 | loss: 52.9390573CurrentTrain: epoch  4, batch    34 | loss: 66.3108237CurrentTrain: epoch  4, batch    35 | loss: 87.7505345CurrentTrain: epoch  4, batch    36 | loss: 65.9527169CurrentTrain: epoch  4, batch    37 | loss: 62.4033600CurrentTrain: epoch  4, batch    38 | loss: 62.7325413CurrentTrain: epoch  4, batch    39 | loss: 64.9199930CurrentTrain: epoch  4, batch    40 | loss: 45.9317749CurrentTrain: epoch  4, batch    41 | loss: 177.9167526CurrentTrain: epoch  4, batch    42 | loss: 85.9863559CurrentTrain: epoch  4, batch    43 | loss: 51.4232795CurrentTrain: epoch  4, batch    44 | loss: 114.4035252CurrentTrain: epoch  4, batch    45 | loss: 57.7857127CurrentTrain: epoch  4, batch    46 | loss: 50.7878612CurrentTrain: epoch  4, batch    47 | loss: 116.7848221CurrentTrain: epoch  4, batch    48 | loss: 85.7421332CurrentTrain: epoch  4, batch    49 | loss: 54.1523543CurrentTrain: epoch  4, batch    50 | loss: 88.0149141CurrentTrain: epoch  4, batch    51 | loss: 55.8576177CurrentTrain: epoch  4, batch    52 | loss: 121.2586612CurrentTrain: epoch  4, batch    53 | loss: 87.0595441CurrentTrain: epoch  4, batch    54 | loss: 86.3741082CurrentTrain: epoch  4, batch    55 | loss: 53.6638803CurrentTrain: epoch  4, batch    56 | loss: 83.7547364CurrentTrain: epoch  4, batch    57 | loss: 49.2035271CurrentTrain: epoch  4, batch    58 | loss: 85.3733756CurrentTrain: epoch  4, batch    59 | loss: 90.2571110CurrentTrain: epoch  4, batch    60 | loss: 54.0368303CurrentTrain: epoch  4, batch    61 | loss: 84.0216577CurrentTrain: epoch  4, batch    62 | loss: 82.2248364CurrentTrain: epoch  4, batch    63 | loss: 84.1220282CurrentTrain: epoch  4, batch    64 | loss: 84.3957945CurrentTrain: epoch  4, batch    65 | loss: 83.1931363CurrentTrain: epoch  4, batch    66 | loss: 81.3091895CurrentTrain: epoch  4, batch    67 | loss: 47.9379263CurrentTrain: epoch  4, batch    68 | loss: 52.2335083CurrentTrain: epoch  4, batch    69 | loss: 44.3646791CurrentTrain: epoch  4, batch    70 | loss: 67.8137088CurrentTrain: epoch  4, batch    71 | loss: 70.1017882CurrentTrain: epoch  4, batch    72 | loss: 65.6421296CurrentTrain: epoch  4, batch    73 | loss: 68.8823048CurrentTrain: epoch  4, batch    74 | loss: 84.0343116CurrentTrain: epoch  4, batch    75 | loss: 86.1735727CurrentTrain: epoch  4, batch    76 | loss: 68.9961391CurrentTrain: epoch  4, batch    77 | loss: 115.9110221CurrentTrain: epoch  4, batch    78 | loss: 62.7666222CurrentTrain: epoch  4, batch    79 | loss: 82.6800878CurrentTrain: epoch  4, batch    80 | loss: 61.8256596CurrentTrain: epoch  4, batch    81 | loss: 51.4058880CurrentTrain: epoch  4, batch    82 | loss: 84.8252974CurrentTrain: epoch  4, batch    83 | loss: 65.6162082CurrentTrain: epoch  4, batch    84 | loss: 117.8484163CurrentTrain: epoch  4, batch    85 | loss: 63.4726666CurrentTrain: epoch  4, batch    86 | loss: 68.4600706CurrentTrain: epoch  4, batch    87 | loss: 68.6977026CurrentTrain: epoch  4, batch    88 | loss: 86.8550772CurrentTrain: epoch  4, batch    89 | loss: 62.5271470CurrentTrain: epoch  4, batch    90 | loss: 65.4486994CurrentTrain: epoch  4, batch    91 | loss: 66.2622449CurrentTrain: epoch  4, batch    92 | loss: 67.6906378CurrentTrain: epoch  4, batch    93 | loss: 115.7862584CurrentTrain: epoch  4, batch    94 | loss: 82.5817061CurrentTrain: epoch  4, batch    95 | loss: 53.6477379CurrentTrain: epoch  5, batch     0 | loss: 64.9165722CurrentTrain: epoch  5, batch     1 | loss: 54.7053319CurrentTrain: epoch  5, batch     2 | loss: 63.6197076CurrentTrain: epoch  5, batch     3 | loss: 68.6383056CurrentTrain: epoch  5, batch     4 | loss: 65.0887290CurrentTrain: epoch  5, batch     5 | loss: 64.4502807CurrentTrain: epoch  5, batch     6 | loss: 116.9422486CurrentTrain: epoch  5, batch     7 | loss: 66.6136517CurrentTrain: epoch  5, batch     8 | loss: 55.1207772CurrentTrain: epoch  5, batch     9 | loss: 67.5869478CurrentTrain: epoch  5, batch    10 | loss: 56.4922431CurrentTrain: epoch  5, batch    11 | loss: 81.8187630CurrentTrain: epoch  5, batch    12 | loss: 52.4362656CurrentTrain: epoch  5, batch    13 | loss: 50.5261005CurrentTrain: epoch  5, batch    14 | loss: 81.9784524CurrentTrain: epoch  5, batch    15 | loss: 64.5862997CurrentTrain: epoch  5, batch    16 | loss: 67.4661770CurrentTrain: epoch  5, batch    17 | loss: 40.9016097CurrentTrain: epoch  5, batch    18 | loss: 87.4679721CurrentTrain: epoch  5, batch    19 | loss: 181.9772915CurrentTrain: epoch  5, batch    20 | loss: 67.1595286CurrentTrain: epoch  5, batch    21 | loss: 68.9245886CurrentTrain: epoch  5, batch    22 | loss: 67.1900527CurrentTrain: epoch  5, batch    23 | loss: 81.9064010CurrentTrain: epoch  5, batch    24 | loss: 65.1773718CurrentTrain: epoch  5, batch    25 | loss: 64.7469592CurrentTrain: epoch  5, batch    26 | loss: 50.7240100CurrentTrain: epoch  5, batch    27 | loss: 66.8118732CurrentTrain: epoch  5, batch    28 | loss: 66.4470217CurrentTrain: epoch  5, batch    29 | loss: 113.5223399CurrentTrain: epoch  5, batch    30 | loss: 54.7013738CurrentTrain: epoch  5, batch    31 | loss: 115.5386715CurrentTrain: epoch  5, batch    32 | loss: 87.9796349CurrentTrain: epoch  5, batch    33 | loss: 85.3114122CurrentTrain: epoch  5, batch    34 | loss: 115.7637554CurrentTrain: epoch  5, batch    35 | loss: 63.9868631CurrentTrain: epoch  5, batch    36 | loss: 65.8720502CurrentTrain: epoch  5, batch    37 | loss: 116.2220408CurrentTrain: epoch  5, batch    38 | loss: 119.0239046CurrentTrain: epoch  5, batch    39 | loss: 117.7370847CurrentTrain: epoch  5, batch    40 | loss: 66.7186857CurrentTrain: epoch  5, batch    41 | loss: 49.3847817CurrentTrain: epoch  5, batch    42 | loss: 63.5696561CurrentTrain: epoch  5, batch    43 | loss: 54.6431646CurrentTrain: epoch  5, batch    44 | loss: 62.1000431CurrentTrain: epoch  5, batch    45 | loss: 68.1959843CurrentTrain: epoch  5, batch    46 | loss: 83.4193475CurrentTrain: epoch  5, batch    47 | loss: 63.3780890CurrentTrain: epoch  5, batch    48 | loss: 62.2672961CurrentTrain: epoch  5, batch    49 | loss: 65.3135998CurrentTrain: epoch  5, batch    50 | loss: 73.2075357CurrentTrain: epoch  5, batch    51 | loss: 62.7844475CurrentTrain: epoch  5, batch    52 | loss: 65.6011145CurrentTrain: epoch  5, batch    53 | loss: 82.8143678CurrentTrain: epoch  5, batch    54 | loss: 115.9262910CurrentTrain: epoch  5, batch    55 | loss: 43.5695591CurrentTrain: epoch  5, batch    56 | loss: 49.6073624CurrentTrain: epoch  5, batch    57 | loss: 70.4747855CurrentTrain: epoch  5, batch    58 | loss: 66.3562270CurrentTrain: epoch  5, batch    59 | loss: 51.8299738CurrentTrain: epoch  5, batch    60 | loss: 66.1150780CurrentTrain: epoch  5, batch    61 | loss: 84.7411133CurrentTrain: epoch  5, batch    62 | loss: 64.2921080CurrentTrain: epoch  5, batch    63 | loss: 171.1089695CurrentTrain: epoch  5, batch    64 | loss: 51.2611220CurrentTrain: epoch  5, batch    65 | loss: 51.1897323CurrentTrain: epoch  5, batch    66 | loss: 63.9871878CurrentTrain: epoch  5, batch    67 | loss: 50.1844954CurrentTrain: epoch  5, batch    68 | loss: 85.1954435CurrentTrain: epoch  5, batch    69 | loss: 62.1150710CurrentTrain: epoch  5, batch    70 | loss: 54.1821803CurrentTrain: epoch  5, batch    71 | loss: 66.0849066CurrentTrain: epoch  5, batch    72 | loss: 84.5519016CurrentTrain: epoch  5, batch    73 | loss: 68.2585914CurrentTrain: epoch  5, batch    74 | loss: 66.3563319CurrentTrain: epoch  5, batch    75 | loss: 52.7046138CurrentTrain: epoch  5, batch    76 | loss: 67.8545577CurrentTrain: epoch  5, batch    77 | loss: 119.7798844CurrentTrain: epoch  5, batch    78 | loss: 50.6661302CurrentTrain: epoch  5, batch    79 | loss: 56.9972847CurrentTrain: epoch  5, batch    80 | loss: 62.9885544CurrentTrain: epoch  5, batch    81 | loss: 64.4742480CurrentTrain: epoch  5, batch    82 | loss: 66.7223186CurrentTrain: epoch  5, batch    83 | loss: 66.3467447CurrentTrain: epoch  5, batch    84 | loss: 42.6785747CurrentTrain: epoch  5, batch    85 | loss: 67.3156159CurrentTrain: epoch  5, batch    86 | loss: 84.9604242CurrentTrain: epoch  5, batch    87 | loss: 85.8389498CurrentTrain: epoch  5, batch    88 | loss: 66.2360189CurrentTrain: epoch  5, batch    89 | loss: 51.7104501CurrentTrain: epoch  5, batch    90 | loss: 83.6223812CurrentTrain: epoch  5, batch    91 | loss: 52.6191591CurrentTrain: epoch  5, batch    92 | loss: 61.6602845CurrentTrain: epoch  5, batch    93 | loss: 90.3413358CurrentTrain: epoch  5, batch    94 | loss: 64.2767858CurrentTrain: epoch  5, batch    95 | loss: 67.1316360CurrentTrain: epoch  6, batch     0 | loss: 54.4088967CurrentTrain: epoch  6, batch     1 | loss: 80.6927686CurrentTrain: epoch  6, batch     2 | loss: 45.6282548CurrentTrain: epoch  6, batch     3 | loss: 83.4854965CurrentTrain: epoch  6, batch     4 | loss: 83.0651061CurrentTrain: epoch  6, batch     5 | loss: 40.4335903CurrentTrain: epoch  6, batch     6 | loss: 44.4459654CurrentTrain: epoch  6, batch     7 | loss: 85.1673200CurrentTrain: epoch  6, batch     8 | loss: 68.3659991CurrentTrain: epoch  6, batch     9 | loss: 43.1773941CurrentTrain: epoch  6, batch    10 | loss: 115.9189965CurrentTrain: epoch  6, batch    11 | loss: 85.3516915CurrentTrain: epoch  6, batch    12 | loss: 55.5915266CurrentTrain: epoch  6, batch    13 | loss: 60.3136864CurrentTrain: epoch  6, batch    14 | loss: 66.9320977CurrentTrain: epoch  6, batch    15 | loss: 65.1309573CurrentTrain: epoch  6, batch    16 | loss: 47.8885924CurrentTrain: epoch  6, batch    17 | loss: 48.6524973CurrentTrain: epoch  6, batch    18 | loss: 42.6593210CurrentTrain: epoch  6, batch    19 | loss: 48.6228900CurrentTrain: epoch  6, batch    20 | loss: 54.9923073CurrentTrain: epoch  6, batch    21 | loss: 66.0555081CurrentTrain: epoch  6, batch    22 | loss: 64.8786780CurrentTrain: epoch  6, batch    23 | loss: 86.1095251CurrentTrain: epoch  6, batch    24 | loss: 42.8138344CurrentTrain: epoch  6, batch    25 | loss: 116.6653676CurrentTrain: epoch  6, batch    26 | loss: 67.1990100CurrentTrain: epoch  6, batch    27 | loss: 82.7640210CurrentTrain: epoch  6, batch    28 | loss: 62.9099345CurrentTrain: epoch  6, batch    29 | loss: 84.0388918CurrentTrain: epoch  6, batch    30 | loss: 64.4945820CurrentTrain: epoch  6, batch    31 | loss: 63.4103095CurrentTrain: epoch  6, batch    32 | loss: 50.2272113CurrentTrain: epoch  6, batch    33 | loss: 82.0989343CurrentTrain: epoch  6, batch    34 | loss: 53.4969095CurrentTrain: epoch  6, batch    35 | loss: 55.1143246CurrentTrain: epoch  6, batch    36 | loss: 64.7178322CurrentTrain: epoch  6, batch    37 | loss: 67.4185941CurrentTrain: epoch  6, batch    38 | loss: 83.4432816CurrentTrain: epoch  6, batch    39 | loss: 69.2243223CurrentTrain: epoch  6, batch    40 | loss: 52.0151682CurrentTrain: epoch  6, batch    41 | loss: 65.4967186CurrentTrain: epoch  6, batch    42 | loss: 44.4186383CurrentTrain: epoch  6, batch    43 | loss: 63.9473150CurrentTrain: epoch  6, batch    44 | loss: 55.7318405CurrentTrain: epoch  6, batch    45 | loss: 64.7440815CurrentTrain: epoch  6, batch    46 | loss: 62.0346064CurrentTrain: epoch  6, batch    47 | loss: 67.4006593CurrentTrain: epoch  6, batch    48 | loss: 83.5503318CurrentTrain: epoch  6, batch    49 | loss: 65.6208322CurrentTrain: epoch  6, batch    50 | loss: 43.3829718CurrentTrain: epoch  6, batch    51 | loss: 64.7254206CurrentTrain: epoch  6, batch    52 | loss: 118.2610564CurrentTrain: epoch  6, batch    53 | loss: 116.9664972CurrentTrain: epoch  6, batch    54 | loss: 64.6548161CurrentTrain: epoch  6, batch    55 | loss: 44.7001989CurrentTrain: epoch  6, batch    56 | loss: 52.9121285CurrentTrain: epoch  6, batch    57 | loss: 56.6727324CurrentTrain: epoch  6, batch    58 | loss: 85.8490685CurrentTrain: epoch  6, batch    59 | loss: 65.7817154CurrentTrain: epoch  6, batch    60 | loss: 65.6294752CurrentTrain: epoch  6, batch    61 | loss: 84.4729588CurrentTrain: epoch  6, batch    62 | loss: 80.5226925CurrentTrain: epoch  6, batch    63 | loss: 66.1425397CurrentTrain: epoch  6, batch    64 | loss: 67.8747532CurrentTrain: epoch  6, batch    65 | loss: 64.3539421CurrentTrain: epoch  6, batch    66 | loss: 62.3275146CurrentTrain: epoch  6, batch    67 | loss: 79.1584805CurrentTrain: epoch  6, batch    68 | loss: 84.4141965CurrentTrain: epoch  6, batch    69 | loss: 82.7682089CurrentTrain: epoch  6, batch    70 | loss: 80.5612615CurrentTrain: epoch  6, batch    71 | loss: 68.3390720CurrentTrain: epoch  6, batch    72 | loss: 115.2200518CurrentTrain: epoch  6, batch    73 | loss: 51.2504471CurrentTrain: epoch  6, batch    74 | loss: 118.0104944CurrentTrain: epoch  6, batch    75 | loss: 64.8633968CurrentTrain: epoch  6, batch    76 | loss: 50.8494684CurrentTrain: epoch  6, batch    77 | loss: 64.1913639CurrentTrain: epoch  6, batch    78 | loss: 111.7303169CurrentTrain: epoch  6, batch    79 | loss: 85.2028044CurrentTrain: epoch  6, batch    80 | loss: 117.9268874CurrentTrain: epoch  6, batch    81 | loss: 49.5335253CurrentTrain: epoch  6, batch    82 | loss: 118.7526175CurrentTrain: epoch  6, batch    83 | loss: 43.3800285CurrentTrain: epoch  6, batch    84 | loss: 66.9611504CurrentTrain: epoch  6, batch    85 | loss: 65.5787714CurrentTrain: epoch  6, batch    86 | loss: 89.3208985CurrentTrain: epoch  6, batch    87 | loss: 68.0852522CurrentTrain: epoch  6, batch    88 | loss: 64.1181575CurrentTrain: epoch  6, batch    89 | loss: 84.7458298CurrentTrain: epoch  6, batch    90 | loss: 118.3472223CurrentTrain: epoch  6, batch    91 | loss: 113.5407039CurrentTrain: epoch  6, batch    92 | loss: 48.6405918CurrentTrain: epoch  6, batch    93 | loss: 81.7443164CurrentTrain: epoch  6, batch    94 | loss: 53.2337135CurrentTrain: epoch  6, batch    95 | loss: 53.9650323CurrentTrain: epoch  7, batch     0 | loss: 65.5094322CurrentTrain: epoch  7, batch     1 | loss: 113.6966130CurrentTrain: epoch  7, batch     2 | loss: 84.0938578CurrentTrain: epoch  7, batch     3 | loss: 66.4679927CurrentTrain: epoch  7, batch     4 | loss: 84.5207203CurrentTrain: epoch  7, batch     5 | loss: 65.5092389CurrentTrain: epoch  7, batch     6 | loss: 84.2917650CurrentTrain: epoch  7, batch     7 | loss: 52.1739402CurrentTrain: epoch  7, batch     8 | loss: 47.7196734CurrentTrain: epoch  7, batch     9 | loss: 81.5763197CurrentTrain: epoch  7, batch    10 | loss: 53.3386346CurrentTrain: epoch  7, batch    11 | loss: 41.5658849CurrentTrain: epoch  7, batch    12 | loss: 65.8085724CurrentTrain: epoch  7, batch    13 | loss: 117.6685332CurrentTrain: epoch  7, batch    14 | loss: 83.8965005CurrentTrain: epoch  7, batch    15 | loss: 40.5547235CurrentTrain: epoch  7, batch    16 | loss: 62.1654069CurrentTrain: epoch  7, batch    17 | loss: 52.5468930CurrentTrain: epoch  7, batch    18 | loss: 64.4811597CurrentTrain: epoch  7, batch    19 | loss: 66.9434816CurrentTrain: epoch  7, batch    20 | loss: 42.7575286CurrentTrain: epoch  7, batch    21 | loss: 83.7515086CurrentTrain: epoch  7, batch    22 | loss: 66.0528533CurrentTrain: epoch  7, batch    23 | loss: 85.7592587CurrentTrain: epoch  7, batch    24 | loss: 53.5494572CurrentTrain: epoch  7, batch    25 | loss: 65.0461330CurrentTrain: epoch  7, batch    26 | loss: 62.1133461CurrentTrain: epoch  7, batch    27 | loss: 63.7494818CurrentTrain: epoch  7, batch    28 | loss: 50.2395184CurrentTrain: epoch  7, batch    29 | loss: 61.8227997CurrentTrain: epoch  7, batch    30 | loss: 115.7659785CurrentTrain: epoch  7, batch    31 | loss: 52.4318521CurrentTrain: epoch  7, batch    32 | loss: 64.8602522CurrentTrain: epoch  7, batch    33 | loss: 67.5278370CurrentTrain: epoch  7, batch    34 | loss: 115.4206742CurrentTrain: epoch  7, batch    35 | loss: 85.4309619CurrentTrain: epoch  7, batch    36 | loss: 50.3307429CurrentTrain: epoch  7, batch    37 | loss: 67.4354409CurrentTrain: epoch  7, batch    38 | loss: 42.8295384CurrentTrain: epoch  7, batch    39 | loss: 64.2833614CurrentTrain: epoch  7, batch    40 | loss: 65.3995256CurrentTrain: epoch  7, batch    41 | loss: 53.2083501CurrentTrain: epoch  7, batch    42 | loss: 65.5125028CurrentTrain: epoch  7, batch    43 | loss: 53.5922791CurrentTrain: epoch  7, batch    44 | loss: 81.4347258CurrentTrain: epoch  7, batch    45 | loss: 51.9365236CurrentTrain: epoch  7, batch    46 | loss: 65.4268144CurrentTrain: epoch  7, batch    47 | loss: 55.9356071CurrentTrain: epoch  7, batch    48 | loss: 53.6833275CurrentTrain: epoch  7, batch    49 | loss: 115.5716511CurrentTrain: epoch  7, batch    50 | loss: 52.3477736CurrentTrain: epoch  7, batch    51 | loss: 67.0598409CurrentTrain: epoch  7, batch    52 | loss: 53.0084209CurrentTrain: epoch  7, batch    53 | loss: 42.2713972CurrentTrain: epoch  7, batch    54 | loss: 53.3268857CurrentTrain: epoch  7, batch    55 | loss: 65.1524499CurrentTrain: epoch  7, batch    56 | loss: 95.7748393CurrentTrain: epoch  7, batch    57 | loss: 52.7268981CurrentTrain: epoch  7, batch    58 | loss: 63.1208463CurrentTrain: epoch  7, batch    59 | loss: 82.9223529CurrentTrain: epoch  7, batch    60 | loss: 82.9581086CurrentTrain: epoch  7, batch    61 | loss: 63.4552622CurrentTrain: epoch  7, batch    62 | loss: 64.4661296CurrentTrain: epoch  7, batch    63 | loss: 61.6448933CurrentTrain: epoch  7, batch    64 | loss: 84.1965969CurrentTrain: epoch  7, batch    65 | loss: 50.5346363CurrentTrain: epoch  7, batch    66 | loss: 113.1276782CurrentTrain: epoch  7, batch    67 | loss: 119.7288887CurrentTrain: epoch  7, batch    68 | loss: 66.2964063CurrentTrain: epoch  7, batch    69 | loss: 53.6951628CurrentTrain: epoch  7, batch    70 | loss: 63.1724895CurrentTrain: epoch  7, batch    71 | loss: 64.0671443CurrentTrain: epoch  7, batch    72 | loss: 83.2888984CurrentTrain: epoch  7, batch    73 | loss: 82.7168086CurrentTrain: epoch  7, batch    74 | loss: 115.6795483CurrentTrain: epoch  7, batch    75 | loss: 64.7390467CurrentTrain: epoch  7, batch    76 | loss: 66.5923981CurrentTrain: epoch  7, batch    77 | loss: 51.4060326CurrentTrain: epoch  7, batch    78 | loss: 51.3116416CurrentTrain: epoch  7, batch    79 | loss: 78.3583108CurrentTrain: epoch  7, batch    80 | loss: 85.8361286CurrentTrain: epoch  7, batch    81 | loss: 53.0243095CurrentTrain: epoch  7, batch    82 | loss: 67.6244756CurrentTrain: epoch  7, batch    83 | loss: 85.8055609CurrentTrain: epoch  7, batch    84 | loss: 51.3242802CurrentTrain: epoch  7, batch    85 | loss: 64.1046987CurrentTrain: epoch  7, batch    86 | loss: 62.8753657CurrentTrain: epoch  7, batch    87 | loss: 86.3632812CurrentTrain: epoch  7, batch    88 | loss: 53.1132863CurrentTrain: epoch  7, batch    89 | loss: 53.7991760CurrentTrain: epoch  7, batch    90 | loss: 53.1718016CurrentTrain: epoch  7, batch    91 | loss: 54.1239811CurrentTrain: epoch  7, batch    92 | loss: 66.2690976CurrentTrain: epoch  7, batch    93 | loss: 51.9144883CurrentTrain: epoch  7, batch    94 | loss: 63.0078505CurrentTrain: epoch  7, batch    95 | loss: 56.7930869CurrentTrain: epoch  8, batch     0 | loss: 52.9061783CurrentTrain: epoch  8, batch     1 | loss: 67.8621336CurrentTrain: epoch  8, batch     2 | loss: 53.5862071CurrentTrain: epoch  8, batch     3 | loss: 115.3760121CurrentTrain: epoch  8, batch     4 | loss: 83.5588234CurrentTrain: epoch  8, batch     5 | loss: 66.1556385CurrentTrain: epoch  8, batch     6 | loss: 50.9664173CurrentTrain: epoch  8, batch     7 | loss: 85.1418978CurrentTrain: epoch  8, batch     8 | loss: 67.5220231CurrentTrain: epoch  8, batch     9 | loss: 53.4402806CurrentTrain: epoch  8, batch    10 | loss: 115.2521150CurrentTrain: epoch  8, batch    11 | loss: 61.6330932CurrentTrain: epoch  8, batch    12 | loss: 51.7888440CurrentTrain: epoch  8, batch    13 | loss: 53.8549408CurrentTrain: epoch  8, batch    14 | loss: 52.3621663CurrentTrain: epoch  8, batch    15 | loss: 67.1939742CurrentTrain: epoch  8, batch    16 | loss: 85.8186904CurrentTrain: epoch  8, batch    17 | loss: 51.4662694CurrentTrain: epoch  8, batch    18 | loss: 53.4571928CurrentTrain: epoch  8, batch    19 | loss: 82.7013166CurrentTrain: epoch  8, batch    20 | loss: 51.2775462CurrentTrain: epoch  8, batch    21 | loss: 65.4187652CurrentTrain: epoch  8, batch    22 | loss: 82.7427595CurrentTrain: epoch  8, batch    23 | loss: 64.2002161CurrentTrain: epoch  8, batch    24 | loss: 43.9175350CurrentTrain: epoch  8, batch    25 | loss: 59.6536534CurrentTrain: epoch  8, batch    26 | loss: 64.4776966CurrentTrain: epoch  8, batch    27 | loss: 62.2341093CurrentTrain: epoch  8, batch    28 | loss: 64.3631499CurrentTrain: epoch  8, batch    29 | loss: 86.0214988CurrentTrain: epoch  8, batch    30 | loss: 51.6454824CurrentTrain: epoch  8, batch    31 | loss: 82.4642033CurrentTrain: epoch  8, batch    32 | loss: 47.9599843CurrentTrain: epoch  8, batch    33 | loss: 41.1196994CurrentTrain: epoch  8, batch    34 | loss: 60.0247609CurrentTrain: epoch  8, batch    35 | loss: 52.1422096CurrentTrain: epoch  8, batch    36 | loss: 42.8597901CurrentTrain: epoch  8, batch    37 | loss: 51.5913092CurrentTrain: epoch  8, batch    38 | loss: 53.1633736CurrentTrain: epoch  8, batch    39 | loss: 63.2236799CurrentTrain: epoch  8, batch    40 | loss: 80.7712967CurrentTrain: epoch  8, batch    41 | loss: 51.8561193CurrentTrain: epoch  8, batch    42 | loss: 66.2474957CurrentTrain: epoch  8, batch    43 | loss: 49.5235202CurrentTrain: epoch  8, batch    44 | loss: 84.0318932CurrentTrain: epoch  8, batch    45 | loss: 85.7818680CurrentTrain: epoch  8, batch    46 | loss: 51.9617289CurrentTrain: epoch  8, batch    47 | loss: 117.4875034CurrentTrain: epoch  8, batch    48 | loss: 82.4675224CurrentTrain: epoch  8, batch    49 | loss: 85.0613451CurrentTrain: epoch  8, batch    50 | loss: 52.8257489CurrentTrain: epoch  8, batch    51 | loss: 64.3452637CurrentTrain: epoch  8, batch    52 | loss: 50.0679430CurrentTrain: epoch  8, batch    53 | loss: 40.9008448CurrentTrain: epoch  8, batch    54 | loss: 64.2681803CurrentTrain: epoch  8, batch    55 | loss: 79.8516090CurrentTrain: epoch  8, batch    56 | loss: 84.0759617CurrentTrain: epoch  8, batch    57 | loss: 181.6385712CurrentTrain: epoch  8, batch    58 | loss: 62.8075018CurrentTrain: epoch  8, batch    59 | loss: 84.0300323CurrentTrain: epoch  8, batch    60 | loss: 49.9230116CurrentTrain: epoch  8, batch    61 | loss: 115.2525016CurrentTrain: epoch  8, batch    62 | loss: 85.9486898CurrentTrain: epoch  8, batch    63 | loss: 53.3425075CurrentTrain: epoch  8, batch    64 | loss: 84.1556046CurrentTrain: epoch  8, batch    65 | loss: 44.0293664CurrentTrain: epoch  8, batch    66 | loss: 62.9440963CurrentTrain: epoch  8, batch    67 | loss: 66.1280581CurrentTrain: epoch  8, batch    68 | loss: 51.5658466CurrentTrain: epoch  8, batch    69 | loss: 65.4134481CurrentTrain: epoch  8, batch    70 | loss: 82.5053523CurrentTrain: epoch  8, batch    71 | loss: 53.3197237CurrentTrain: epoch  8, batch    72 | loss: 63.1213333CurrentTrain: epoch  8, batch    73 | loss: 64.6799433CurrentTrain: epoch  8, batch    74 | loss: 50.0385772CurrentTrain: epoch  8, batch    75 | loss: 55.4703084CurrentTrain: epoch  8, batch    76 | loss: 85.1842200CurrentTrain: epoch  8, batch    77 | loss: 117.7933376CurrentTrain: epoch  8, batch    78 | loss: 82.4429993CurrentTrain: epoch  8, batch    79 | loss: 52.7812449CurrentTrain: epoch  8, batch    80 | loss: 84.0687529CurrentTrain: epoch  8, batch    81 | loss: 82.7101728CurrentTrain: epoch  8, batch    82 | loss: 63.2181230CurrentTrain: epoch  8, batch    83 | loss: 83.1273273CurrentTrain: epoch  8, batch    84 | loss: 84.7557267CurrentTrain: epoch  8, batch    85 | loss: 62.6588868CurrentTrain: epoch  8, batch    86 | loss: 113.3685191CurrentTrain: epoch  8, batch    87 | loss: 87.5993510CurrentTrain: epoch  8, batch    88 | loss: 39.9996871CurrentTrain: epoch  8, batch    89 | loss: 83.0930057CurrentTrain: epoch  8, batch    90 | loss: 44.7752084CurrentTrain: epoch  8, batch    91 | loss: 119.8075509CurrentTrain: epoch  8, batch    92 | loss: 81.2658141CurrentTrain: epoch  8, batch    93 | loss: 64.8245842CurrentTrain: epoch  8, batch    94 | loss: 42.4767266CurrentTrain: epoch  8, batch    95 | loss: 94.8020980CurrentTrain: epoch  9, batch     0 | loss: 83.0470953CurrentTrain: epoch  9, batch     1 | loss: 64.7752381CurrentTrain: epoch  9, batch     2 | loss: 64.7309283CurrentTrain: epoch  9, batch     3 | loss: 64.4536179CurrentTrain: epoch  9, batch     4 | loss: 64.3214205CurrentTrain: epoch  9, batch     5 | loss: 82.6384150CurrentTrain: epoch  9, batch     6 | loss: 45.0678957CurrentTrain: epoch  9, batch     7 | loss: 66.7214529CurrentTrain: epoch  9, batch     8 | loss: 79.9645448CurrentTrain: epoch  9, batch     9 | loss: 52.3283229CurrentTrain: epoch  9, batch    10 | loss: 67.1166568CurrentTrain: epoch  9, batch    11 | loss: 111.6996310CurrentTrain: epoch  9, batch    12 | loss: 68.3129516CurrentTrain: epoch  9, batch    13 | loss: 64.0824263CurrentTrain: epoch  9, batch    14 | loss: 64.1864847CurrentTrain: epoch  9, batch    15 | loss: 65.9563804CurrentTrain: epoch  9, batch    16 | loss: 81.4750172CurrentTrain: epoch  9, batch    17 | loss: 55.8437975CurrentTrain: epoch  9, batch    18 | loss: 68.5485527CurrentTrain: epoch  9, batch    19 | loss: 78.7787813CurrentTrain: epoch  9, batch    20 | loss: 116.4735269CurrentTrain: epoch  9, batch    21 | loss: 50.3741733CurrentTrain: epoch  9, batch    22 | loss: 50.1912558CurrentTrain: epoch  9, batch    23 | loss: 49.6224766CurrentTrain: epoch  9, batch    24 | loss: 113.0098476CurrentTrain: epoch  9, batch    25 | loss: 62.4639602CurrentTrain: epoch  9, batch    26 | loss: 51.5721867CurrentTrain: epoch  9, batch    27 | loss: 49.7004810CurrentTrain: epoch  9, batch    28 | loss: 84.3438527CurrentTrain: epoch  9, batch    29 | loss: 41.7181612CurrentTrain: epoch  9, batch    30 | loss: 52.0323170CurrentTrain: epoch  9, batch    31 | loss: 54.1263787CurrentTrain: epoch  9, batch    32 | loss: 85.8564401CurrentTrain: epoch  9, batch    33 | loss: 66.7742479CurrentTrain: epoch  9, batch    34 | loss: 79.7647628CurrentTrain: epoch  9, batch    35 | loss: 53.1810614CurrentTrain: epoch  9, batch    36 | loss: 84.6251591CurrentTrain: epoch  9, batch    37 | loss: 65.5031023CurrentTrain: epoch  9, batch    38 | loss: 49.8680677CurrentTrain: epoch  9, batch    39 | loss: 81.5795713CurrentTrain: epoch  9, batch    40 | loss: 64.5270897CurrentTrain: epoch  9, batch    41 | loss: 65.4789223CurrentTrain: epoch  9, batch    42 | loss: 82.4343997CurrentTrain: epoch  9, batch    43 | loss: 80.3501240CurrentTrain: epoch  9, batch    44 | loss: 113.0956574CurrentTrain: epoch  9, batch    45 | loss: 61.9035819CurrentTrain: epoch  9, batch    46 | loss: 181.7801842CurrentTrain: epoch  9, batch    47 | loss: 63.2126358CurrentTrain: epoch  9, batch    48 | loss: 84.3053231CurrentTrain: epoch  9, batch    49 | loss: 64.2421605CurrentTrain: epoch  9, batch    50 | loss: 53.2231660CurrentTrain: epoch  9, batch    51 | loss: 52.2140092CurrentTrain: epoch  9, batch    52 | loss: 64.6409489CurrentTrain: epoch  9, batch    53 | loss: 53.2172861CurrentTrain: epoch  9, batch    54 | loss: 84.1307330CurrentTrain: epoch  9, batch    55 | loss: 177.5677163CurrentTrain: epoch  9, batch    56 | loss: 66.8577580CurrentTrain: epoch  9, batch    57 | loss: 64.6600216CurrentTrain: epoch  9, batch    58 | loss: 48.6037716CurrentTrain: epoch  9, batch    59 | loss: 65.3561628CurrentTrain: epoch  9, batch    60 | loss: 116.7877611CurrentTrain: epoch  9, batch    61 | loss: 49.4303066CurrentTrain: epoch  9, batch    62 | loss: 54.4963441CurrentTrain: epoch  9, batch    63 | loss: 84.1957728CurrentTrain: epoch  9, batch    64 | loss: 48.1622314CurrentTrain: epoch  9, batch    65 | loss: 80.9992992CurrentTrain: epoch  9, batch    66 | loss: 84.2204848CurrentTrain: epoch  9, batch    67 | loss: 64.4192967CurrentTrain: epoch  9, batch    68 | loss: 62.6429498CurrentTrain: epoch  9, batch    69 | loss: 51.0470858CurrentTrain: epoch  9, batch    70 | loss: 52.1259831CurrentTrain: epoch  9, batch    71 | loss: 115.2173319CurrentTrain: epoch  9, batch    72 | loss: 115.6334970CurrentTrain: epoch  9, batch    73 | loss: 83.6598548CurrentTrain: epoch  9, batch    74 | loss: 64.2142888CurrentTrain: epoch  9, batch    75 | loss: 53.4118483CurrentTrain: epoch  9, batch    76 | loss: 62.8786856CurrentTrain: epoch  9, batch    77 | loss: 62.0320012CurrentTrain: epoch  9, batch    78 | loss: 44.3518632CurrentTrain: epoch  9, batch    79 | loss: 65.8895623CurrentTrain: epoch  9, batch    80 | loss: 52.4677149CurrentTrain: epoch  9, batch    81 | loss: 40.0703667CurrentTrain: epoch  9, batch    82 | loss: 62.9915814CurrentTrain: epoch  9, batch    83 | loss: 54.8398472CurrentTrain: epoch  9, batch    84 | loss: 83.0558193CurrentTrain: epoch  9, batch    85 | loss: 64.2255633CurrentTrain: epoch  9, batch    86 | loss: 81.0395451CurrentTrain: epoch  9, batch    87 | loss: 52.8611120CurrentTrain: epoch  9, batch    88 | loss: 115.2439254CurrentTrain: epoch  9, batch    89 | loss: 48.0896055CurrentTrain: epoch  9, batch    90 | loss: 63.3867379CurrentTrain: epoch  9, batch    91 | loss: 62.9064847CurrentTrain: epoch  9, batch    92 | loss: 117.4528860CurrentTrain: epoch  9, batch    93 | loss: 59.5854601CurrentTrain: epoch  9, batch    94 | loss: 65.0400053CurrentTrain: epoch  9, batch    95 | loss: 71.5311997

F1 score per class: {32: 0.5232067510548524, 6: 0.6145251396648045, 19: 0.15384615384615385, 24: 0.723404255319149, 26: 0.8677248677248677, 29: 0.8186046511627907}
Micro-average F1 score: 0.690522243713733
Weighted-average F1 score: 0.6908608580196178
F1 score per class: {32: 0.5421245421245421, 6: 0.6009852216748769, 19: 0.2222222222222222, 24: 0.7253886010362695, 26: 0.9054726368159204, 29: 0.7841409691629956}
Micro-average F1 score: 0.6794092093831451
Weighted-average F1 score: 0.6669649335205996
F1 score per class: {32: 0.5421245421245421, 6: 0.6009852216748769, 19: 0.2222222222222222, 24: 0.7291666666666666, 26: 0.9054726368159204, 29: 0.7841409691629956}
Micro-average F1 score: 0.68
Weighted-average F1 score: 0.6674224625878161

F1 score per class: {32: 0.5232067510548524, 6: 0.6145251396648045, 19: 0.15384615384615385, 24: 0.723404255319149, 26: 0.8677248677248677, 29: 0.8186046511627907}
Micro-average F1 score: 0.690522243713733
Weighted-average F1 score: 0.6908608580196178
F1 score per class: {32: 0.5421245421245421, 6: 0.6009852216748769, 19: 0.2222222222222222, 24: 0.7253886010362695, 26: 0.9054726368159204, 29: 0.7841409691629956}
Micro-average F1 score: 0.6794092093831451
Weighted-average F1 score: 0.6669649335205996
F1 score per class: {32: 0.5421245421245421, 6: 0.6009852216748769, 19: 0.2222222222222222, 24: 0.7291666666666666, 26: 0.9054726368159204, 29: 0.7841409691629956}
Micro-average F1 score: 0.68
Weighted-average F1 score: 0.6674224625878161
cur_acc:  ['0.6905']
his_acc:  ['0.6905']
cur_acc des:  ['0.6794']
his_acc des:  ['0.6794']
cur_acc rrf:  ['0.6800']
his_acc rrf:  ['0.6800']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 73.7709520CurrentTrain: epoch  0, batch     1 | loss: 128.5640181CurrentTrain: epoch  0, batch     2 | loss: 97.8268927CurrentTrain: epoch  0, batch     3 | loss: 65.3259606CurrentTrain: epoch  0, batch     4 | loss: 80.0104465CurrentTrain: epoch  1, batch     0 | loss: 72.6933370CurrentTrain: epoch  1, batch     1 | loss: 74.6628513CurrentTrain: epoch  1, batch     2 | loss: 94.2422340CurrentTrain: epoch  1, batch     3 | loss: 72.2548673CurrentTrain: epoch  1, batch     4 | loss: 58.9954891CurrentTrain: epoch  2, batch     0 | loss: 90.1572513CurrentTrain: epoch  2, batch     1 | loss: 60.3667992CurrentTrain: epoch  2, batch     2 | loss: 71.0324581CurrentTrain: epoch  2, batch     3 | loss: 73.7803299CurrentTrain: epoch  2, batch     4 | loss: 58.1791266CurrentTrain: epoch  3, batch     0 | loss: 88.7662130CurrentTrain: epoch  3, batch     1 | loss: 89.7207581CurrentTrain: epoch  3, batch     2 | loss: 70.7276142CurrentTrain: epoch  3, batch     3 | loss: 118.6209656CurrentTrain: epoch  3, batch     4 | loss: 76.3218492CurrentTrain: epoch  4, batch     0 | loss: 69.0720972CurrentTrain: epoch  4, batch     1 | loss: 57.3396868CurrentTrain: epoch  4, batch     2 | loss: 86.7924103CurrentTrain: epoch  4, batch     3 | loss: 119.0710546CurrentTrain: epoch  4, batch     4 | loss: 55.2271570CurrentTrain: epoch  5, batch     0 | loss: 69.9320626CurrentTrain: epoch  5, batch     1 | loss: 86.0837807CurrentTrain: epoch  5, batch     2 | loss: 57.1799834CurrentTrain: epoch  5, batch     3 | loss: 55.9705640CurrentTrain: epoch  5, batch     4 | loss: 43.6666293CurrentTrain: epoch  6, batch     0 | loss: 67.4385477CurrentTrain: epoch  6, batch     1 | loss: 53.1728821CurrentTrain: epoch  6, batch     2 | loss: 68.1621182CurrentTrain: epoch  6, batch     3 | loss: 118.9315205CurrentTrain: epoch  6, batch     4 | loss: 72.9849720CurrentTrain: epoch  7, batch     0 | loss: 65.2170178CurrentTrain: epoch  7, batch     1 | loss: 84.8100876CurrentTrain: epoch  7, batch     2 | loss: 80.7159860CurrentTrain: epoch  7, batch     3 | loss: 87.0361615CurrentTrain: epoch  7, batch     4 | loss: 42.0404225CurrentTrain: epoch  8, batch     0 | loss: 66.6480167CurrentTrain: epoch  8, batch     1 | loss: 65.2876164CurrentTrain: epoch  8, batch     2 | loss: 115.3678910CurrentTrain: epoch  8, batch     3 | loss: 115.3824587CurrentTrain: epoch  8, batch     4 | loss: 67.4990170CurrentTrain: epoch  9, batch     0 | loss: 53.0480243CurrentTrain: epoch  9, batch     1 | loss: 86.4478773CurrentTrain: epoch  9, batch     2 | loss: 82.9093091CurrentTrain: epoch  9, batch     3 | loss: 62.1946588CurrentTrain: epoch  9, batch     4 | loss: 112.5583146
MemoryTrain:  epoch  0, batch     0 | loss: 0.4514070MemoryTrain:  epoch  1, batch     0 | loss: 0.4673219MemoryTrain:  epoch  2, batch     0 | loss: 0.4153029MemoryTrain:  epoch  3, batch     0 | loss: 0.2712874MemoryTrain:  epoch  4, batch     0 | loss: 0.2414523MemoryTrain:  epoch  5, batch     0 | loss: 0.1344479MemoryTrain:  epoch  6, batch     0 | loss: 0.1991158MemoryTrain:  epoch  7, batch     0 | loss: 0.0965977MemoryTrain:  epoch  8, batch     0 | loss: 0.1013162MemoryTrain:  epoch  9, batch     0 | loss: 0.0652484

F1 score per class: {32: 0.9538461538461539, 5: 0.0, 6: 0.5081081081081081, 10: 0.4444444444444444, 16: 0.23255813953488372, 17: 0.3076923076923077, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5351270553064275
Weighted-average F1 score: 0.4707216331572803
F1 score per class: {32: 0.7716535433070866, 5: 0.0, 6: 0.5153846153846153, 10: 0.46956521739130436, 16: 0.2, 17: 0.2988505747126437, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.45962732919254656
Weighted-average F1 score: 0.41790109615177223
F1 score per class: {32: 0.7686274509803922, 5: 0.0, 6: 0.5114503816793893, 10: 0.416, 16: 0.18181818181818182, 17: 0.2937853107344633, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.4613778705636743
Weighted-average F1 score: 0.4237621651033476

F1 score per class: {32: 0.9538461538461539, 5: 0.29591836734693877, 6: 0.4351851851851852, 10: 0.40404040404040403, 16: 0.07194244604316546, 17: 0.27450980392156865, 18: 0.5953488372093023, 19: 0.16326530612244897, 24: 0.7052631578947368, 26: 0.8262910798122066, 29: 0.7606837606837606}
Micro-average F1 score: 0.5627705627705628
Weighted-average F1 score: 0.5283510563674277
F1 score per class: {32: 0.7396226415094339, 5: 0.40298507462686567, 6: 0.37960339943342775, 10: 0.391304347826087, 16: 0.06557377049180328, 17: 0.23853211009174313, 18: 0.5170068027210885, 19: 0.1111111111111111, 24: 0.6729857819905213, 26: 0.8103448275862069, 29: 0.6388888888888888}
Micro-average F1 score: 0.48986083499005967
Weighted-average F1 score: 0.46060504118637613
F1 score per class: {32: 0.7313432835820896, 5: 0.39846743295019155, 6: 0.3743016759776536, 10: 0.34210526315789475, 16: 0.06201550387596899, 17: 0.2311111111111111, 18: 0.5660377358490566, 19: 0.1728395061728395, 24: 0.6965174129353234, 26: 0.8138528138528138, 29: 0.6570397111913358}
Micro-average F1 score: 0.49836601307189543
Weighted-average F1 score: 0.4678116527897888
cur_acc:  ['0.6905', '0.5351']
his_acc:  ['0.6905', '0.5628']
cur_acc des:  ['0.6794', '0.4596']
his_acc des:  ['0.6794', '0.4899']
cur_acc rrf:  ['0.6800', '0.4614']
his_acc rrf:  ['0.6800', '0.4984']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 75.7267882CurrentTrain: epoch  0, batch     1 | loss: 83.3747800CurrentTrain: epoch  0, batch     2 | loss: 63.7492093CurrentTrain: epoch  0, batch     3 | loss: 27.6736218CurrentTrain: epoch  1, batch     0 | loss: 60.9443102CurrentTrain: epoch  1, batch     1 | loss: 62.1757184CurrentTrain: epoch  1, batch     2 | loss: 94.7017584CurrentTrain: epoch  1, batch     3 | loss: 11.0786502CurrentTrain: epoch  2, batch     0 | loss: 68.1765734CurrentTrain: epoch  2, batch     1 | loss: 57.8855333CurrentTrain: epoch  2, batch     2 | loss: 67.5748216CurrentTrain: epoch  2, batch     3 | loss: 14.0569550CurrentTrain: epoch  3, batch     0 | loss: 89.7199079CurrentTrain: epoch  3, batch     1 | loss: 68.7130783CurrentTrain: epoch  3, batch     2 | loss: 51.9395958CurrentTrain: epoch  3, batch     3 | loss: 11.4064024CurrentTrain: epoch  4, batch     0 | loss: 52.4400230CurrentTrain: epoch  4, batch     1 | loss: 55.9842556CurrentTrain: epoch  4, batch     2 | loss: 87.8489058CurrentTrain: epoch  4, batch     3 | loss: 27.4453617CurrentTrain: epoch  5, batch     0 | loss: 50.9857170CurrentTrain: epoch  5, batch     1 | loss: 83.1146113CurrentTrain: epoch  5, batch     2 | loss: 69.2938709CurrentTrain: epoch  5, batch     3 | loss: 3.6123364CurrentTrain: epoch  6, batch     0 | loss: 66.3336120CurrentTrain: epoch  6, batch     1 | loss: 49.0036038CurrentTrain: epoch  6, batch     2 | loss: 83.4727323CurrentTrain: epoch  6, batch     3 | loss: 10.9733016CurrentTrain: epoch  7, batch     0 | loss: 182.1544092CurrentTrain: epoch  7, batch     1 | loss: 48.1417567CurrentTrain: epoch  7, batch     2 | loss: 51.4109602CurrentTrain: epoch  7, batch     3 | loss: 11.0965390CurrentTrain: epoch  8, batch     0 | loss: 84.8046032CurrentTrain: epoch  8, batch     1 | loss: 51.7722517CurrentTrain: epoch  8, batch     2 | loss: 49.0360844CurrentTrain: epoch  8, batch     3 | loss: 10.9354198CurrentTrain: epoch  9, batch     0 | loss: 62.7769372CurrentTrain: epoch  9, batch     1 | loss: 60.8711051CurrentTrain: epoch  9, batch     2 | loss: 64.2547772CurrentTrain: epoch  9, batch     3 | loss: 11.2219052
MemoryTrain:  epoch  0, batch     0 | loss: 0.6444479MemoryTrain:  epoch  1, batch     0 | loss: 0.4375877MemoryTrain:  epoch  2, batch     0 | loss: 0.2911659MemoryTrain:  epoch  3, batch     0 | loss: 0.2426039MemoryTrain:  epoch  4, batch     0 | loss: 0.2132660MemoryTrain:  epoch  5, batch     0 | loss: 0.1683857MemoryTrain:  epoch  6, batch     0 | loss: 0.1860895MemoryTrain:  epoch  7, batch     0 | loss: 0.2154250MemoryTrain:  epoch  8, batch     0 | loss: 0.1306780MemoryTrain:  epoch  9, batch     0 | loss: 0.0933675

F1 score per class: {32: 0.0, 6: 0.3333333333333333, 7: 0.96, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 19: 0.4117647058823529, 26: 0.0, 27: 0.0, 31: 0.3875}
Micro-average F1 score: 0.4012738853503185
Weighted-average F1 score: 0.33835196078431373
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.25, 7: 0.746268656716418, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.2857142857142857, 26: 0.0, 27: 0.15384615384615385, 29: 0.0, 31: 0.3981042654028436}
Micro-average F1 score: 0.3369330453563715
Weighted-average F1 score: 0.29872191076218263
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.2857142857142857, 7: 0.847457627118644, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.27586206896551724, 26: 0.0, 27: 0.17391304347826086, 29: 0.0, 31: 0.3925233644859813}
Micro-average F1 score: 0.3482142857142857
Weighted-average F1 score: 0.305125474544278

F1 score per class: {32: 0.9494949494949495, 5: 0.2375, 6: 0.019417475728155338, 7: 0.96, 40: 0.3068181818181818, 10: 0.42, 9: 0.0, 16: 0.3225806451612903, 17: 0.4803921568627451, 18: 0.125, 19: 0.6808510638297872, 24: 0.1308411214953271, 26: 0.8472906403940886, 27: 0.0, 29: 0.7733333333333333, 31: 0.25833333333333336}
Micro-average F1 score: 0.49714285714285716
Weighted-average F1 score: 0.457484727291088
F1 score per class: {32: 0.5981873111782477, 5: 0.09655172413793103, 6: 0.011764705882352941, 7: 0.7142857142857143, 40: 0.3890784982935154, 10: 0.34532374100719426, 9: 0.0, 16: 0.21645021645021645, 17: 0.4722222222222222, 18: 0.07894736842105263, 19: 0.6859903381642513, 24: 0.08376963350785341, 26: 0.8266666666666667, 27: 0.04395604395604396, 29: 0.6717557251908397, 31: 0.2204724409448819}
Micro-average F1 score: 0.38464020651823166
Weighted-average F1 score: 0.34840987363558806
F1 score per class: {32: 0.5875370919881305, 5: 0.23255813953488372, 6: 0.015267175572519083, 7: 0.847457627118644, 40: 0.397212543554007, 10: 0.3287671232876712, 9: 0.0, 16: 0.22123893805309736, 17: 0.4830917874396135, 18: 0.08163265306122448, 19: 0.6896551724137931, 24: 0.07655502392344497, 26: 0.8340807174887892, 27: 0.04938271604938271, 29: 0.6984126984126984, 31: 0.21428571428571427}
Micro-average F1 score: 0.39698657058630854
Weighted-average F1 score: 0.3554551511690593
cur_acc:  ['0.6905', '0.5351', '0.4013']
his_acc:  ['0.6905', '0.5628', '0.4971']
cur_acc des:  ['0.6794', '0.4596', '0.3369']
his_acc des:  ['0.6794', '0.4899', '0.3846']
cur_acc rrf:  ['0.6800', '0.4614', '0.3482']
his_acc rrf:  ['0.6800', '0.4984', '0.3970']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 109.6399755CurrentTrain: epoch  0, batch     1 | loss: 92.5828280CurrentTrain: epoch  0, batch     2 | loss: 72.2510110CurrentTrain: epoch  0, batch     3 | loss: 61.3158986CurrentTrain: epoch  1, batch     0 | loss: 75.6529096CurrentTrain: epoch  1, batch     1 | loss: 88.9115190CurrentTrain: epoch  1, batch     2 | loss: 65.8883517CurrentTrain: epoch  1, batch     3 | loss: 50.3495288CurrentTrain: epoch  2, batch     0 | loss: 58.3032366CurrentTrain: epoch  2, batch     1 | loss: 127.7560867CurrentTrain: epoch  2, batch     2 | loss: 73.0273908CurrentTrain: epoch  2, batch     3 | loss: 64.9589975CurrentTrain: epoch  3, batch     0 | loss: 66.4413747CurrentTrain: epoch  3, batch     1 | loss: 58.3980903CurrentTrain: epoch  3, batch     2 | loss: 56.9813030CurrentTrain: epoch  3, batch     3 | loss: 75.1154695CurrentTrain: epoch  4, batch     0 | loss: 175.3425827CurrentTrain: epoch  4, batch     1 | loss: 56.1535179CurrentTrain: epoch  4, batch     2 | loss: 52.6958743CurrentTrain: epoch  4, batch     3 | loss: 101.1081664CurrentTrain: epoch  5, batch     0 | loss: 80.9240321CurrentTrain: epoch  5, batch     1 | loss: 65.0055318CurrentTrain: epoch  5, batch     2 | loss: 90.0639640CurrentTrain: epoch  5, batch     3 | loss: 48.8589383CurrentTrain: epoch  6, batch     0 | loss: 81.7074353CurrentTrain: epoch  6, batch     1 | loss: 84.2339632CurrentTrain: epoch  6, batch     2 | loss: 66.3348585CurrentTrain: epoch  6, batch     3 | loss: 57.2468296CurrentTrain: epoch  7, batch     0 | loss: 68.2719904CurrentTrain: epoch  7, batch     1 | loss: 64.3166997CurrentTrain: epoch  7, batch     2 | loss: 68.0685760CurrentTrain: epoch  7, batch     3 | loss: 67.9451496CurrentTrain: epoch  8, batch     0 | loss: 53.9834078CurrentTrain: epoch  8, batch     1 | loss: 67.5777032CurrentTrain: epoch  8, batch     2 | loss: 83.4966004CurrentTrain: epoch  8, batch     3 | loss: 64.6614893CurrentTrain: epoch  9, batch     0 | loss: 54.5170310CurrentTrain: epoch  9, batch     1 | loss: 66.6556142CurrentTrain: epoch  9, batch     2 | loss: 52.6439751CurrentTrain: epoch  9, batch     3 | loss: 54.2594113
MemoryTrain:  epoch  0, batch     0 | loss: 0.5798472MemoryTrain:  epoch  1, batch     0 | loss: 0.5860861MemoryTrain:  epoch  2, batch     0 | loss: 0.3903311MemoryTrain:  epoch  3, batch     0 | loss: 0.3069696MemoryTrain:  epoch  4, batch     0 | loss: 0.2197087MemoryTrain:  epoch  5, batch     0 | loss: 0.1821898MemoryTrain:  epoch  6, batch     0 | loss: 0.1445866MemoryTrain:  epoch  7, batch     0 | loss: 0.1164326MemoryTrain:  epoch  8, batch     0 | loss: 0.1032167MemoryTrain:  epoch  9, batch     0 | loss: 0.0928375

F1 score per class: {0: 0.7727272727272727, 4: 0.98989898989899, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.044444444444444446, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.4158415841584158, 23: 0.7954545454545454, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.6136363636363636
Weighted-average F1 score: 0.5048472650295333
F1 score per class: {0: 0.6238532110091743, 4: 0.9753694581280788, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.06666666666666667, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.32116788321167883, 23: 0.625, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.38477366255144035
Weighted-average F1 score: 0.2841967389188262
F1 score per class: {0: 0.6017699115044248, 4: 0.9850746268656716, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.06349206349206349, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.32167832167832167, 23: 0.631578947368421, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4043010752688172
Weighted-average F1 score: 0.3031737348900916

F1 score per class: {0: 0.3487179487179487, 4: 0.9751243781094527, 5: 0.8715596330275229, 6: 0.20382165605095542, 7: 0.017241379310344827, 9: 0.9411764705882353, 10: 0.31213872832369943, 13: 0.014814814814814815, 16: 0.40707964601769914, 17: 0.0, 18: 0.24761904761904763, 19: 0.35668789808917195, 21: 0.22105263157894736, 23: 0.660377358490566, 24: 0.0, 26: 0.6735751295336787, 27: 0.12389380530973451, 29: 0.8, 31: 0.08, 32: 0.7632850241545893, 40: 0.4075829383886256}
Micro-average F1 score: 0.47203274215552526
Weighted-average F1 score: 0.42244811591613973
F1 score per class: {0: 0.25660377358490566, 4: 0.9252336448598131, 5: 0.35587188612099646, 6: 0.17616580310880828, 7: 0.012738853503184714, 9: 0.704225352112676, 10: 0.28860759493670884, 13: 0.021505376344086023, 16: 0.35294117647058826, 17: 0.0, 18: 0.16524216524216523, 19: 0.44155844155844154, 21: 0.13134328358208955, 23: 0.5128205128205128, 24: 0.1111111111111111, 26: 0.6082949308755761, 27: 0.10362694300518134, 29: 0.7965367965367965, 31: 0.027586206896551724, 32: 0.5602605863192183, 40: 0.22631578947368422}
Micro-average F1 score: 0.32939714108141704
Weighted-average F1 score: 0.2928097902089211
F1 score per class: {0: 0.22442244224422442, 4: 0.9473684210526315, 5: 0.41841004184100417, 6: 0.1958762886597938, 7: 0.014084507042253521, 9: 0.8620689655172413, 10: 0.29295774647887324, 13: 0.020100502512562814, 16: 0.3561643835616438, 17: 0.0, 18: 0.1638418079096045, 19: 0.4549763033175355, 21: 0.12777777777777777, 23: 0.4878048780487805, 24: 0.15384615384615385, 26: 0.6285714285714286, 27: 0.12290502793296089, 29: 0.8, 31: 0.04938271604938271, 32: 0.5827814569536424, 40: 0.25949367088607594}
Micro-average F1 score: 0.3463393248575186
Weighted-average F1 score: 0.30613603169325293
cur_acc:  ['0.6905', '0.5351', '0.4013', '0.6136']
his_acc:  ['0.6905', '0.5628', '0.4971', '0.4720']
cur_acc des:  ['0.6794', '0.4596', '0.3369', '0.3848']
his_acc des:  ['0.6794', '0.4899', '0.3846', '0.3294']
cur_acc rrf:  ['0.6800', '0.4614', '0.3482', '0.4043']
his_acc rrf:  ['0.6800', '0.4984', '0.3970', '0.3463']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 77.6797139CurrentTrain: epoch  0, batch     1 | loss: 77.7941618CurrentTrain: epoch  0, batch     2 | loss: 64.9915349CurrentTrain: epoch  0, batch     3 | loss: 76.8252990CurrentTrain: epoch  1, batch     0 | loss: 61.1759577CurrentTrain: epoch  1, batch     1 | loss: 90.9762253CurrentTrain: epoch  1, batch     2 | loss: 88.7194449CurrentTrain: epoch  1, batch     3 | loss: 114.0029943CurrentTrain: epoch  2, batch     0 | loss: 56.7142756CurrentTrain: epoch  2, batch     1 | loss: 87.4951354CurrentTrain: epoch  2, batch     2 | loss: 70.8037909CurrentTrain: epoch  2, batch     3 | loss: 41.9573864CurrentTrain: epoch  3, batch     0 | loss: 54.8621025CurrentTrain: epoch  3, batch     1 | loss: 70.1944266CurrentTrain: epoch  3, batch     2 | loss: 56.2515497CurrentTrain: epoch  3, batch     3 | loss: 53.1753869CurrentTrain: epoch  4, batch     0 | loss: 87.5096499CurrentTrain: epoch  4, batch     1 | loss: 66.1535992CurrentTrain: epoch  4, batch     2 | loss: 53.0149434CurrentTrain: epoch  4, batch     3 | loss: 40.4581423CurrentTrain: epoch  5, batch     0 | loss: 86.9167623CurrentTrain: epoch  5, batch     1 | loss: 63.3462875CurrentTrain: epoch  5, batch     2 | loss: 65.2960991CurrentTrain: epoch  5, batch     3 | loss: 50.0480010CurrentTrain: epoch  6, batch     0 | loss: 62.8370208CurrentTrain: epoch  6, batch     1 | loss: 63.2559100CurrentTrain: epoch  6, batch     2 | loss: 118.3389559CurrentTrain: epoch  6, batch     3 | loss: 39.0017603CurrentTrain: epoch  7, batch     0 | loss: 63.3496702CurrentTrain: epoch  7, batch     1 | loss: 50.7400054CurrentTrain: epoch  7, batch     2 | loss: 82.9375670CurrentTrain: epoch  7, batch     3 | loss: 51.5168019CurrentTrain: epoch  8, batch     0 | loss: 81.8445354CurrentTrain: epoch  8, batch     1 | loss: 63.6820606CurrentTrain: epoch  8, batch     2 | loss: 63.7929912CurrentTrain: epoch  8, batch     3 | loss: 48.8389990CurrentTrain: epoch  9, batch     0 | loss: 64.6857734CurrentTrain: epoch  9, batch     1 | loss: 80.2728658CurrentTrain: epoch  9, batch     2 | loss: 79.8618931CurrentTrain: epoch  9, batch     3 | loss: 49.1214501
MemoryTrain:  epoch  0, batch     0 | loss: 0.2725137MemoryTrain:  epoch  1, batch     0 | loss: 0.2518624MemoryTrain:  epoch  2, batch     0 | loss: 0.2538682MemoryTrain:  epoch  3, batch     0 | loss: 0.1569678MemoryTrain:  epoch  4, batch     0 | loss: 0.1384168MemoryTrain:  epoch  5, batch     0 | loss: 0.1125645MemoryTrain:  epoch  6, batch     0 | loss: 0.0912231MemoryTrain:  epoch  7, batch     0 | loss: 0.1034584MemoryTrain:  epoch  8, batch     0 | loss: 0.0793683MemoryTrain:  epoch  9, batch     0 | loss: 0.0638301

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.41025641025641024, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.6068965517241379, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.918918918918919, 31: 0.0, 32: 0.0, 33: 0.375, 36: 0.21333333333333335, 40: 0.0}
Micro-average F1 score: 0.3878787878787879
Weighted-average F1 score: 0.3421680208964692
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5027322404371585, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.48205128205128206, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 1.0, 31: 0.0, 32: 0.0, 33: 0.3783783783783784, 36: 0.5373134328358209, 40: 0.0}
Micro-average F1 score: 0.341034103410341
Weighted-average F1 score: 0.27230541381193896
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5108695652173914, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.47959183673469385, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.9743589743589743, 31: 0.0, 32: 0.0, 33: 0.35, 36: 0.5263157894736842, 40: 0.0}
Micro-average F1 score: 0.3510758776896942
Weighted-average F1 score: 0.28469623223256774

F1 score per class: {0: 0.4861111111111111, 4: 0.964824120603015, 5: 0.7384615384615385, 6: 0.112, 7: 0.0, 8: 0.26666666666666666, 9: 0.9411764705882353, 10: 0.10434782608695652, 13: 0.05, 16: 0.34074074074074073, 17: 0.0, 18: 0.20689655172413793, 19: 0.46808510638297873, 20: 0.34108527131782945, 21: 0.14705882352941177, 23: 0.47706422018348627, 24: 0.0, 26: 0.6197183098591549, 27: 0.14457831325301204, 29: 0.7465437788018433, 30: 0.918918918918919, 31: 0.08695652173913043, 32: 0.6555555555555556, 33: 0.2727272727272727, 36: 0.17204301075268819, 40: 0.43820224719101125}
Micro-average F1 score: 0.45952
Weighted-average F1 score: 0.4551513894786997
F1 score per class: {0: 0.3090909090909091, 4: 0.9339622641509434, 5: 0.29027576197387517, 6: 0.18823529411764706, 7: 0.0125, 8: 0.2358974358974359, 9: 0.6410256410256411, 10: 0.19767441860465115, 13: 0.06779661016949153, 16: 0.29743589743589743, 17: 0.0, 18: 0.1597222222222222, 19: 0.4318181818181818, 20: 0.235, 21: 0.10319410319410319, 23: 0.4117647058823529, 24: 0.10256410256410256, 26: 0.5774058577405857, 27: 0.10126582278481013, 29: 0.7647058823529411, 30: 0.9743589743589743, 31: 0.03225806451612903, 32: 0.6292134831460674, 33: 0.12389380530973451, 36: 0.39344262295081966, 40: 0.23052959501557632}
Micro-average F1 score: 0.32011381824648766
Weighted-average F1 score: 0.2899244650114269
F1 score per class: {0: 0.29955947136563876, 4: 0.9565217391304348, 5: 0.3105590062111801, 6: 0.20125786163522014, 7: 0.016666666666666666, 8: 0.23325062034739455, 9: 0.8064516129032258, 10: 0.21686746987951808, 13: 0.07272727272727272, 16: 0.2983425414364641, 17: 0.023255813953488372, 18: 0.16356877323420074, 19: 0.4462809917355372, 20: 0.23209876543209876, 21: 0.0954653937947494, 23: 0.39436619718309857, 24: 0.12121212121212122, 26: 0.5938864628820961, 27: 0.10810810810810811, 29: 0.7679324894514767, 30: 0.8837209302325582, 31: 0.038461538461538464, 32: 0.6412213740458015, 33: 0.11570247933884298, 36: 0.37433155080213903, 40: 0.2482758620689655}
Micro-average F1 score: 0.32824848373460763
Weighted-average F1 score: 0.29608028922339835
cur_acc:  ['0.6905', '0.5351', '0.4013', '0.6136', '0.3879']
his_acc:  ['0.6905', '0.5628', '0.4971', '0.4720', '0.4595']
cur_acc des:  ['0.6794', '0.4596', '0.3369', '0.3848', '0.3410']
his_acc des:  ['0.6794', '0.4899', '0.3846', '0.3294', '0.3201']
cur_acc rrf:  ['0.6800', '0.4614', '0.3482', '0.4043', '0.3511']
his_acc rrf:  ['0.6800', '0.4984', '0.3970', '0.3463', '0.3282']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 92.7636551CurrentTrain: epoch  0, batch     1 | loss: 101.2838539CurrentTrain: epoch  0, batch     2 | loss: 69.8426080CurrentTrain: epoch  0, batch     3 | loss: 78.9731736CurrentTrain: epoch  0, batch     4 | loss: 60.3626797CurrentTrain: epoch  1, batch     0 | loss: 64.1676144CurrentTrain: epoch  1, batch     1 | loss: 127.7383209CurrentTrain: epoch  1, batch     2 | loss: 64.0363491CurrentTrain: epoch  1, batch     3 | loss: 79.0323526CurrentTrain: epoch  1, batch     4 | loss: 28.8171095CurrentTrain: epoch  2, batch     0 | loss: 72.9354765CurrentTrain: epoch  2, batch     1 | loss: 86.9739165CurrentTrain: epoch  2, batch     2 | loss: 125.1754342CurrentTrain: epoch  2, batch     3 | loss: 90.1806583CurrentTrain: epoch  2, batch     4 | loss: 30.5035499CurrentTrain: epoch  3, batch     0 | loss: 114.8970285CurrentTrain: epoch  3, batch     1 | loss: 121.1711082CurrentTrain: epoch  3, batch     2 | loss: 91.5612342CurrentTrain: epoch  3, batch     3 | loss: 69.2542207CurrentTrain: epoch  3, batch     4 | loss: 17.5448262CurrentTrain: epoch  4, batch     0 | loss: 89.6485157CurrentTrain: epoch  4, batch     1 | loss: 68.2371951CurrentTrain: epoch  4, batch     2 | loss: 69.4710413CurrentTrain: epoch  4, batch     3 | loss: 86.4237638CurrentTrain: epoch  4, batch     4 | loss: 59.9885428CurrentTrain: epoch  5, batch     0 | loss: 68.3620094CurrentTrain: epoch  5, batch     1 | loss: 54.5548028CurrentTrain: epoch  5, batch     2 | loss: 68.9513239CurrentTrain: epoch  5, batch     3 | loss: 88.3667652CurrentTrain: epoch  5, batch     4 | loss: 28.1282268CurrentTrain: epoch  6, batch     0 | loss: 56.3959024CurrentTrain: epoch  6, batch     1 | loss: 86.8681055CurrentTrain: epoch  6, batch     2 | loss: 54.1126296CurrentTrain: epoch  6, batch     3 | loss: 68.7387374CurrentTrain: epoch  6, batch     4 | loss: 27.1022842CurrentTrain: epoch  7, batch     0 | loss: 86.1515692CurrentTrain: epoch  7, batch     1 | loss: 119.4009228CurrentTrain: epoch  7, batch     2 | loss: 60.7774810CurrentTrain: epoch  7, batch     3 | loss: 83.3981040CurrentTrain: epoch  7, batch     4 | loss: 27.0876805CurrentTrain: epoch  8, batch     0 | loss: 53.8862633CurrentTrain: epoch  8, batch     1 | loss: 84.2160319CurrentTrain: epoch  8, batch     2 | loss: 62.8274711CurrentTrain: epoch  8, batch     3 | loss: 182.6151295CurrentTrain: epoch  8, batch     4 | loss: 25.4464928CurrentTrain: epoch  9, batch     0 | loss: 84.6577902CurrentTrain: epoch  9, batch     1 | loss: 84.2334958CurrentTrain: epoch  9, batch     2 | loss: 66.4931075CurrentTrain: epoch  9, batch     3 | loss: 54.5137822CurrentTrain: epoch  9, batch     4 | loss: 14.0530634
MemoryTrain:  epoch  0, batch     0 | loss: 0.3882538MemoryTrain:  epoch  1, batch     0 | loss: 0.3586276MemoryTrain:  epoch  2, batch     0 | loss: 0.2592815MemoryTrain:  epoch  3, batch     0 | loss: 0.1977527MemoryTrain:  epoch  4, batch     0 | loss: 0.1458285MemoryTrain:  epoch  5, batch     0 | loss: 0.1348835MemoryTrain:  epoch  6, batch     0 | loss: 0.1097187MemoryTrain:  epoch  7, batch     0 | loss: 0.0958088MemoryTrain:  epoch  8, batch     0 | loss: 0.0856792MemoryTrain:  epoch  9, batch     0 | loss: 0.0740922

F1 score per class: {0: 0.0, 2: 0.3888888888888889, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.5679012345679012, 12: 0.31343283582089554, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 28: 0.2727272727272727, 29: 0.0, 31: 0.0, 32: 0.0, 36: 0.0, 39: 0.08333333333333333, 40: 0.0}
Micro-average F1 score: 0.31388329979879276
Weighted-average F1 score: 0.24400331988291468
F1 score per class: {0: 0.0, 2: 0.3111111111111111, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.4852941176470588, 12: 0.5205479452054794, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.2222222222222222, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.2916666666666667, 40: 0.0}
Micro-average F1 score: 0.28629856850715746
Weighted-average F1 score: 0.23264586563293776
F1 score per class: {0: 0.0, 2: 0.3111111111111111, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.4731182795698925, 12: 0.5357142857142857, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 28: 0.23076923076923078, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.358974358974359, 40: 0.0}
Micro-average F1 score: 0.3029661016949153
Weighted-average F1 score: 0.2498239557994842

F1 score per class: {0: 0.5081967213114754, 2: 0.21875, 4: 0.945273631840796, 5: 0.8636363636363636, 6: 0.16058394160583941, 7: 0.011363636363636364, 8: 0.23728813559322035, 9: 0.9230769230769231, 10: 0.037383177570093455, 11: 0.2948717948717949, 12: 0.2131979695431472, 13: 0.037037037037037035, 16: 0.3898305084745763, 17: 0.0, 18: 0.06349206349206349, 19: 0.43157894736842106, 20: 0.2222222222222222, 21: 0.2, 23: 0.631578947368421, 24: 0.08695652173913043, 26: 0.6256410256410256, 27: 0.125, 28: 0.1111111111111111, 29: 0.7586206896551724, 30: 0.9444444444444444, 31: 0.045454545454545456, 32: 0.7428571428571429, 33: 0.15384615384615385, 36: 0.08333333333333333, 39: 0.07142857142857142, 40: 0.43243243243243246}
Micro-average F1 score: 0.4208219178082192
Weighted-average F1 score: 0.4084455197866234
F1 score per class: {0: 0.3076923076923077, 2: 0.14893617021276595, 4: 0.9389671361502347, 5: 0.37453183520599254, 6: 0.2283464566929134, 7: 0.028985507246376812, 8: 0.2517482517482518, 9: 0.6493506493506493, 10: 0.18848167539267016, 11: 0.2222222222222222, 12: 0.15426251691474965, 13: 0.023529411764705882, 16: 0.3888888888888889, 17: 0.0, 18: 0.09271523178807947, 19: 0.45, 20: 0.23255813953488372, 21: 0.10232558139534884, 23: 0.4, 24: 0.08888888888888889, 26: 0.5887445887445888, 27: 0.11688311688311688, 28: 0.09375, 29: 0.7114624505928854, 30: 0.7755102040816326, 31: 0.03508771929824561, 32: 0.5602605863192183, 33: 0.16901408450704225, 36: 0.4260355029585799, 39: 0.125, 40: 0.29304029304029305}
Micro-average F1 score: 0.3008486976880304
Weighted-average F1 score: 0.2752512230533926
F1 score per class: {0: 0.3105022831050228, 2: 0.15384615384615385, 4: 0.9565217391304348, 5: 0.44742729306487694, 6: 0.21367521367521367, 7: 0.019704433497536946, 8: 0.26506024096385544, 9: 0.78125, 10: 0.15384615384615385, 11: 0.1981981981981982, 12: 0.15625, 13: 0.02247191011235955, 16: 0.38095238095238093, 17: 0.0, 18: 0.09259259259259259, 19: 0.4444444444444444, 20: 0.22508038585209003, 21: 0.10574712643678161, 23: 0.4027777777777778, 24: 0.125, 26: 0.5903083700440529, 27: 0.12413793103448276, 28: 0.08571428571428572, 29: 0.7250996015936255, 30: 0.76, 31: 0.038834951456310676, 32: 0.5657894736842105, 33: 0.1702127659574468, 36: 0.375, 39: 0.14583333333333334, 40: 0.2899628252788104}
Micro-average F1 score: 0.30205811138014527
Weighted-average F1 score: 0.27468979271288174
cur_acc:  ['0.6905', '0.5351', '0.4013', '0.6136', '0.3879', '0.3139']
his_acc:  ['0.6905', '0.5628', '0.4971', '0.4720', '0.4595', '0.4208']
cur_acc des:  ['0.6794', '0.4596', '0.3369', '0.3848', '0.3410', '0.2863']
his_acc des:  ['0.6794', '0.4899', '0.3846', '0.3294', '0.3201', '0.3008']
cur_acc rrf:  ['0.6800', '0.4614', '0.3482', '0.4043', '0.3511', '0.3030']
his_acc rrf:  ['0.6800', '0.4984', '0.3970', '0.3463', '0.3282', '0.3021']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 83.5048635CurrentTrain: epoch  0, batch     1 | loss: 101.7513101CurrentTrain: epoch  0, batch     2 | loss: 130.9656428CurrentTrain: epoch  0, batch     3 | loss: 94.6903708CurrentTrain: epoch  0, batch     4 | loss: 68.9775618CurrentTrain: epoch  1, batch     0 | loss: 94.8460461CurrentTrain: epoch  1, batch     1 | loss: 91.0014616CurrentTrain: epoch  1, batch     2 | loss: 71.6939245CurrentTrain: epoch  1, batch     3 | loss: 61.7046714CurrentTrain: epoch  1, batch     4 | loss: 106.8023556CurrentTrain: epoch  2, batch     0 | loss: 75.2234323CurrentTrain: epoch  2, batch     1 | loss: 114.9779523CurrentTrain: epoch  2, batch     2 | loss: 89.2511059CurrentTrain: epoch  2, batch     3 | loss: 59.1133397CurrentTrain: epoch  2, batch     4 | loss: 44.3415343CurrentTrain: epoch  3, batch     0 | loss: 69.6384721CurrentTrain: epoch  3, batch     1 | loss: 69.3103703CurrentTrain: epoch  3, batch     2 | loss: 68.9964103CurrentTrain: epoch  3, batch     3 | loss: 68.3740380CurrentTrain: epoch  3, batch     4 | loss: 104.7096295CurrentTrain: epoch  4, batch     0 | loss: 69.3834122CurrentTrain: epoch  4, batch     1 | loss: 70.0533604CurrentTrain: epoch  4, batch     2 | loss: 53.9586890CurrentTrain: epoch  4, batch     3 | loss: 87.6240330CurrentTrain: epoch  4, batch     4 | loss: 66.6079614CurrentTrain: epoch  5, batch     0 | loss: 118.4242314CurrentTrain: epoch  5, batch     1 | loss: 64.6510305CurrentTrain: epoch  5, batch     2 | loss: 85.1945003CurrentTrain: epoch  5, batch     3 | loss: 66.3077121CurrentTrain: epoch  5, batch     4 | loss: 65.8395766CurrentTrain: epoch  6, batch     0 | loss: 85.1761143CurrentTrain: epoch  6, batch     1 | loss: 67.2013999CurrentTrain: epoch  6, batch     2 | loss: 53.3779660CurrentTrain: epoch  6, batch     3 | loss: 86.6541868CurrentTrain: epoch  6, batch     4 | loss: 47.7156024CurrentTrain: epoch  7, batch     0 | loss: 177.8098636CurrentTrain: epoch  7, batch     1 | loss: 54.2261258CurrentTrain: epoch  7, batch     2 | loss: 64.8601459CurrentTrain: epoch  7, batch     3 | loss: 85.1382978CurrentTrain: epoch  7, batch     4 | loss: 46.7372619CurrentTrain: epoch  8, batch     0 | loss: 66.4433780CurrentTrain: epoch  8, batch     1 | loss: 49.4281124CurrentTrain: epoch  8, batch     2 | loss: 114.2520915CurrentTrain: epoch  8, batch     3 | loss: 115.5937132CurrentTrain: epoch  8, batch     4 | loss: 102.9614496CurrentTrain: epoch  9, batch     0 | loss: 84.8966402CurrentTrain: epoch  9, batch     1 | loss: 111.3182985CurrentTrain: epoch  9, batch     2 | loss: 62.5132274CurrentTrain: epoch  9, batch     3 | loss: 115.7635390CurrentTrain: epoch  9, batch     4 | loss: 44.4002939
MemoryTrain:  epoch  0, batch     0 | loss: 0.4103255MemoryTrain:  epoch  1, batch     0 | loss: 0.3925553MemoryTrain:  epoch  2, batch     0 | loss: 0.2756796MemoryTrain:  epoch  3, batch     0 | loss: 0.2578603MemoryTrain:  epoch  4, batch     0 | loss: 0.2168900MemoryTrain:  epoch  5, batch     0 | loss: 0.1556901MemoryTrain:  epoch  6, batch     0 | loss: 0.1216706MemoryTrain:  epoch  7, batch     0 | loss: 0.0989435MemoryTrain:  epoch  8, batch     0 | loss: 0.0944088MemoryTrain:  epoch  9, batch     0 | loss: 0.0831399

F1 score per class: {0: 0.0, 1: 0.1660377358490566, 2: 0.0, 3: 0.5540540540540541, 5: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.125, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5277777777777778, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.16129032258064516, 40: 0.0}
Micro-average F1 score: 0.2533081285444234
Weighted-average F1 score: 0.20400442191677734
F1 score per class: {0: 0.0, 1: 0.1444866920152091, 2: 0.0, 3: 0.550185873605948, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.11173184357541899, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5909090909090909, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.208955223880597, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.20685579196217493
Weighted-average F1 score: 0.16383730408422917
F1 score per class: {0: 0.0, 1: 0.1724137931034483, 2: 0.0, 3: 0.5916666666666667, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.10843373493975904, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5603448275862069, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 34: 0.15873015873015872, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.22580645161290322
Weighted-average F1 score: 0.18426644024998934

F1 score per class: {0: 0.5333333333333333, 1: 0.12607449856733524, 2: 0.25925925925925924, 3: 0.3744292237442922, 4: 0.94, 5: 0.9313725490196079, 6: 0.11940298507462686, 7: 0.011494252873563218, 8: 0.18181818181818182, 9: 0.9411764705882353, 10: 0.10084033613445378, 11: 0.1834862385321101, 12: 0.14184397163120568, 13: 0.034482758620689655, 14: 0.09045226130653267, 16: 0.4, 17: 0.0, 18: 0.0, 19: 0.12121212121212122, 20: 0.2857142857142857, 21: 0.10526315789473684, 22: 0.4453125, 23: 0.6, 24: 0.0, 26: 0.6060606060606061, 27: 0.06896551724137931, 28: 0.14285714285714285, 29: 0.6970954356846473, 30: 0.9444444444444444, 31: 0.0, 32: 0.5643153526970954, 33: 0.15384615384615385, 34: 0.08695652173913043, 36: 0.10810810810810811, 39: 0.13333333333333333, 40: 0.3076923076923077}
Micro-average F1 score: 0.34754382168361825
Weighted-average F1 score: 0.33499234086609153
F1 score per class: {0: 0.43037974683544306, 1: 0.09429280397022333, 2: 0.1728395061728395, 3: 0.2813688212927757, 4: 0.9383886255924171, 5: 0.5154639175257731, 6: 0.20967741935483872, 7: 0.010582010582010581, 8: 0.20385674931129477, 9: 0.6944444444444444, 10: 0.19298245614035087, 11: 0.11943539630836048, 12: 0.21031746031746032, 13: 0.020618556701030927, 14: 0.07246376811594203, 16: 0.43333333333333335, 17: 0.0, 18: 0.08163265306122448, 19: 0.2513089005235602, 20: 0.26717557251908397, 21: 0.12260536398467432, 22: 0.4693140794223827, 23: 0.4444444444444444, 24: 0.08163265306122448, 26: 0.6008583690987125, 27: 0.07547169811320754, 28: 0.08108108108108109, 29: 0.674074074074074, 30: 0.7916666666666666, 31: 0.06060606060606061, 32: 0.46112600536193027, 33: 0.1935483870967742, 34: 0.09859154929577464, 36: 0.39344262295081966, 39: 0.08045977011494253, 40: 0.30434782608695654}
Micro-average F1 score: 0.27991026919242273
Weighted-average F1 score: 0.2573292891539828
F1 score per class: {0: 0.43037974683544306, 1: 0.111358574610245, 2: 0.19444444444444445, 3: 0.3008474576271186, 4: 0.9607843137254902, 5: 0.7424242424242424, 6: 0.19626168224299065, 7: 0.010416666666666666, 8: 0.2188449848024316, 9: 0.9090909090909091, 10: 0.2127659574468085, 11: 0.12512413108242304, 12: 0.25925925925925924, 13: 0.023809523809523808, 14: 0.06818181818181818, 16: 0.41935483870967744, 17: 0.0, 18: 0.0784313725490196, 19: 0.21656050955414013, 20: 0.2656826568265683, 21: 0.14444444444444443, 22: 0.44982698961937717, 23: 0.46875, 24: 0.05405405405405406, 26: 0.6106194690265486, 27: 0.06741573033707865, 28: 0.08450704225352113, 29: 0.6642066420664207, 30: 0.8372093023255814, 31: 0.05128205128205128, 32: 0.5, 33: 0.11764705882352941, 34: 0.08264462809917356, 36: 0.29411764705882354, 39: 0.11214953271028037, 40: 0.3315508021390374}
Micro-average F1 score: 0.29730103806228375
Weighted-average F1 score: 0.2712053700331966
cur_acc:  ['0.6905', '0.5351', '0.4013', '0.6136', '0.3879', '0.3139', '0.2533']
his_acc:  ['0.6905', '0.5628', '0.4971', '0.4720', '0.4595', '0.4208', '0.3475']
cur_acc des:  ['0.6794', '0.4596', '0.3369', '0.3848', '0.3410', '0.2863', '0.2069']
his_acc des:  ['0.6794', '0.4899', '0.3846', '0.3294', '0.3201', '0.3008', '0.2799']
cur_acc rrf:  ['0.6800', '0.4614', '0.3482', '0.4043', '0.3511', '0.3030', '0.2258']
his_acc rrf:  ['0.6800', '0.4984', '0.3970', '0.3463', '0.3282', '0.3021', '0.2973']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 66.2716825CurrentTrain: epoch  0, batch     1 | loss: 62.8071548CurrentTrain: epoch  0, batch     2 | loss: 79.7204698CurrentTrain: epoch  0, batch     3 | loss: 56.5158937CurrentTrain: epoch  1, batch     0 | loss: 62.9934991CurrentTrain: epoch  1, batch     1 | loss: 72.5555548CurrentTrain: epoch  1, batch     2 | loss: 91.7077294CurrentTrain: epoch  1, batch     3 | loss: 48.6904241CurrentTrain: epoch  2, batch     0 | loss: 94.7282941CurrentTrain: epoch  2, batch     1 | loss: 55.4246834CurrentTrain: epoch  2, batch     2 | loss: 66.3231388CurrentTrain: epoch  2, batch     3 | loss: 79.5183048CurrentTrain: epoch  3, batch     0 | loss: 65.3857560CurrentTrain: epoch  3, batch     1 | loss: 68.5257745CurrentTrain: epoch  3, batch     2 | loss: 69.7545370CurrentTrain: epoch  3, batch     3 | loss: 57.1574219CurrentTrain: epoch  4, batch     0 | loss: 88.4314458CurrentTrain: epoch  4, batch     1 | loss: 56.2802578CurrentTrain: epoch  4, batch     2 | loss: 48.9356447CurrentTrain: epoch  4, batch     3 | loss: 59.9197410CurrentTrain: epoch  5, batch     0 | loss: 61.0433419CurrentTrain: epoch  5, batch     1 | loss: 65.5581834CurrentTrain: epoch  5, batch     2 | loss: 65.3390824CurrentTrain: epoch  5, batch     3 | loss: 82.2878306CurrentTrain: epoch  6, batch     0 | loss: 65.1651593CurrentTrain: epoch  6, batch     1 | loss: 64.0370060CurrentTrain: epoch  6, batch     2 | loss: 51.7616290CurrentTrain: epoch  6, batch     3 | loss: 122.0385582CurrentTrain: epoch  7, batch     0 | loss: 53.7368989CurrentTrain: epoch  7, batch     1 | loss: 64.3672703CurrentTrain: epoch  7, batch     2 | loss: 62.5607151CurrentTrain: epoch  7, batch     3 | loss: 57.9627402CurrentTrain: epoch  8, batch     0 | loss: 60.3091564CurrentTrain: epoch  8, batch     1 | loss: 117.7434440CurrentTrain: epoch  8, batch     2 | loss: 64.7806493CurrentTrain: epoch  8, batch     3 | loss: 55.4371719CurrentTrain: epoch  9, batch     0 | loss: 62.0731347CurrentTrain: epoch  9, batch     1 | loss: 115.5346060CurrentTrain: epoch  9, batch     2 | loss: 51.4321642CurrentTrain: epoch  9, batch     3 | loss: 55.3066671
MemoryTrain:  epoch  0, batch     0 | loss: 0.3431703MemoryTrain:  epoch  1, batch     0 | loss: 0.3338923MemoryTrain:  epoch  2, batch     0 | loss: 0.2366093MemoryTrain:  epoch  3, batch     0 | loss: 0.1887117MemoryTrain:  epoch  4, batch     0 | loss: 0.1429781MemoryTrain:  epoch  5, batch     0 | loss: 0.1264050MemoryTrain:  epoch  6, batch     0 | loss: 0.1018018MemoryTrain:  epoch  7, batch     0 | loss: 0.0912376MemoryTrain:  epoch  8, batch     0 | loss: 0.0814222MemoryTrain:  epoch  9, batch     0 | loss: 0.0794294

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.7368421052631579, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.547945205479452, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.41025641025641024, 37: 0.5287356321839081, 38: 0.4186046511627907, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2777777777777778
Weighted-average F1 score: 0.1468149946187828
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.6, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.48, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7387387387387387, 36: 0.0, 37: 0.44776119402985076, 38: 0.5633802816901409, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2564102564102564
Weighted-average F1 score: 0.17058046665978352
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.6, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5128205128205128, 26: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6972477064220184, 36: 0.0, 37: 0.4566929133858268, 38: 0.5217391304347826, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.26555023923444976
Weighted-average F1 score: 0.17600540133934942

F1 score per class: {0: 0.43243243243243246, 1: 0.11825192802056556, 2: 0.2028985507246377, 3: 0.46153846153846156, 4: 0.9183673469387755, 5: 0.7142857142857143, 6: 0.15492957746478872, 7: 0.0111731843575419, 8: 0.17777777777777778, 9: 0.9411764705882353, 10: 0.16541353383458646, 11: 0.1686046511627907, 12: 0.07352941176470588, 13: 0.016129032258064516, 14: 0.08465608465608465, 15: 0.45161290322580644, 16: 0.3492063492063492, 17: 0.0, 18: 0.056338028169014086, 19: 0.1910828025477707, 20: 0.27722772277227725, 21: 0.15789473684210525, 22: 0.48412698412698413, 23: 0.7216494845360825, 24: 0.0, 25: 0.547945205479452, 26: 0.6060606060606061, 27: 0.0761904761904762, 28: 0.11764705882352941, 29: 0.6641221374045801, 30: 0.972972972972973, 31: 0.07407407407407407, 32: 0.5658914728682171, 33: 0.2857142857142857, 34: 0.06896551724137931, 35: 0.17582417582417584, 36: 0.1095890410958904, 37: 0.22660098522167488, 38: 0.09836065573770492, 39: 0.17391304347826086, 40: 0.4217687074829932}
Micro-average F1 score: 0.32023770221195114
Weighted-average F1 score: 0.2987120993717085
F1 score per class: {0: 0.38202247191011235, 1: 0.082687338501292, 2: 0.1414141414141414, 3: 0.24358974358974358, 4: 0.9365853658536586, 5: 0.390625, 6: 0.18032786885245902, 7: 0.02197802197802198, 8: 0.2, 9: 0.6666666666666666, 10: 0.2648401826484018, 11: 0.11682242990654206, 12: 0.18461538461538463, 13: 0.009009009009009009, 14: 0.06734006734006734, 15: 0.2222222222222222, 16: 0.3724137931034483, 17: 0.0821917808219178, 18: 0.109375, 19: 0.26609442060085836, 20: 0.2246376811594203, 21: 0.09701492537313433, 22: 0.425531914893617, 23: 0.43548387096774194, 24: 0.09090909090909091, 25: 0.45569620253164556, 26: 0.5702479338842975, 27: 0.046242774566473986, 28: 0.09876543209876543, 29: 0.6388888888888888, 30: 0.76, 31: 0.056338028169014086, 32: 0.4857142857142857, 33: 0.19047619047619047, 34: 0.058394160583941604, 35: 0.21354166666666666, 36: 0.35036496350364965, 37: 0.0907715582450832, 38: 0.11428571428571428, 39: 0.10606060606060606, 40: 0.2915451895043732}
Micro-average F1 score: 0.24390243902439024
Weighted-average F1 score: 0.2230923390287546
F1 score per class: {0: 0.37158469945355194, 1: 0.08430913348946135, 2: 0.1590909090909091, 3: 0.274582560296846, 4: 0.945273631840796, 5: 0.5763688760806917, 6: 0.1724137931034483, 7: 0.010471204188481676, 8: 0.20317460317460317, 9: 0.7246376811594203, 10: 0.2011173184357542, 11: 0.11887382690302398, 12: 0.2112676056338028, 13: 0.010416666666666666, 14: 0.06920415224913495, 15: 0.2222222222222222, 16: 0.3586206896551724, 17: 0.0625, 18: 0.10619469026548672, 19: 0.2857142857142857, 20: 0.2323943661971831, 21: 0.1111111111111111, 22: 0.445141065830721, 23: 0.4576271186440678, 24: 0.125, 25: 0.47058823529411764, 26: 0.6126126126126126, 27: 0.04878048780487805, 28: 0.07407407407407407, 29: 0.6341463414634146, 30: 0.8444444444444444, 31: 0.06896551724137931, 32: 0.48863636363636365, 33: 0.25, 34: 0.04838709677419355, 35: 0.19791666666666666, 36: 0.2831858407079646, 37: 0.09797297297297297, 38: 0.09448818897637795, 39: 0.13592233009708737, 40: 0.3254237288135593}
Micro-average F1 score: 0.25429625724828675
Weighted-average F1 score: 0.2305195880757588
cur_acc:  ['0.6905', '0.5351', '0.4013', '0.6136', '0.3879', '0.3139', '0.2533', '0.2778']
his_acc:  ['0.6905', '0.5628', '0.4971', '0.4720', '0.4595', '0.4208', '0.3475', '0.3202']
cur_acc des:  ['0.6794', '0.4596', '0.3369', '0.3848', '0.3410', '0.2863', '0.2069', '0.2564']
his_acc des:  ['0.6794', '0.4899', '0.3846', '0.3294', '0.3201', '0.3008', '0.2799', '0.2439']
cur_acc rrf:  ['0.6800', '0.4614', '0.3482', '0.4043', '0.3511', '0.3030', '0.2258', '0.2656']
his_acc rrf:  ['0.6800', '0.4984', '0.3970', '0.3463', '0.3282', '0.3021', '0.2973', '0.2543']
----------END
his_acc mean:  [0.6896 0.5648 0.4916 0.4373 0.3766 0.3635 0.3282 0.307 ]
his_acc des mean:  [0.6452 0.4794 0.4015 0.351  0.2953 0.2763 0.2574 0.2448]
his_acc rrf mean:  [0.6473 0.4868 0.4052 0.3587 0.2992 0.2783 0.2626 0.2474]
