#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 99.0717339CurrentTrain: epoch  0, batch     1 | loss: 68.3246431CurrentTrain: epoch  0, batch     2 | loss: 60.3929436CurrentTrain: epoch  0, batch     3 | loss: 66.6118940CurrentTrain: epoch  0, batch     4 | loss: 68.3647400CurrentTrain: epoch  0, batch     5 | loss: 66.8946286CurrentTrain: epoch  0, batch     6 | loss: 78.4852241CurrentTrain: epoch  0, batch     7 | loss: 77.6272132CurrentTrain: epoch  0, batch     8 | loss: 65.1001775CurrentTrain: epoch  0, batch     9 | loss: 66.5996897CurrentTrain: epoch  0, batch    10 | loss: 56.9361311CurrentTrain: epoch  0, batch    11 | loss: 77.5070448CurrentTrain: epoch  0, batch    12 | loss: 78.4222475CurrentTrain: epoch  0, batch    13 | loss: 189.0484587CurrentTrain: epoch  0, batch    14 | loss: 77.7160093CurrentTrain: epoch  0, batch    15 | loss: 64.9459832CurrentTrain: epoch  0, batch    16 | loss: 64.9088054CurrentTrain: epoch  0, batch    17 | loss: 94.0387614CurrentTrain: epoch  0, batch    18 | loss: 77.4481291CurrentTrain: epoch  0, batch    19 | loss: 65.1917801CurrentTrain: epoch  0, batch    20 | loss: 126.0952841CurrentTrain: epoch  0, batch    21 | loss: 127.1528922CurrentTrain: epoch  0, batch    22 | loss: 187.6291775CurrentTrain: epoch  0, batch    23 | loss: 126.5281543CurrentTrain: epoch  0, batch    24 | loss: 76.8004156CurrentTrain: epoch  0, batch    25 | loss: 188.7662329CurrentTrain: epoch  0, batch    26 | loss: 64.5336170CurrentTrain: epoch  0, batch    27 | loss: 97.5983148CurrentTrain: epoch  0, batch    28 | loss: 126.3324073CurrentTrain: epoch  0, batch    29 | loss: 77.2288157CurrentTrain: epoch  0, batch    30 | loss: 125.7219041CurrentTrain: epoch  0, batch    31 | loss: 125.8927488CurrentTrain: epoch  0, batch    32 | loss: 94.8769166CurrentTrain: epoch  0, batch    33 | loss: 64.4749408CurrentTrain: epoch  0, batch    34 | loss: 126.7004462CurrentTrain: epoch  0, batch    35 | loss: 64.3845999CurrentTrain: epoch  0, batch    36 | loss: 63.8105785CurrentTrain: epoch  0, batch    37 | loss: 77.0620044CurrentTrain: epoch  0, batch    38 | loss: 76.0290141CurrentTrain: epoch  0, batch    39 | loss: 63.7510002CurrentTrain: epoch  0, batch    40 | loss: 93.9211060CurrentTrain: epoch  0, batch    41 | loss: 64.4754545CurrentTrain: epoch  0, batch    42 | loss: 64.3790766CurrentTrain: epoch  0, batch    43 | loss: 56.2905030CurrentTrain: epoch  0, batch    44 | loss: 76.5568232CurrentTrain: epoch  0, batch    45 | loss: 64.2227827CurrentTrain: epoch  0, batch    46 | loss: 94.2749491CurrentTrain: epoch  0, batch    47 | loss: 64.7728257CurrentTrain: epoch  0, batch    48 | loss: 76.3033766CurrentTrain: epoch  0, batch    49 | loss: 76.4822318CurrentTrain: epoch  0, batch    50 | loss: 75.9524157CurrentTrain: epoch  0, batch    51 | loss: 76.3414176CurrentTrain: epoch  0, batch    52 | loss: 76.1653165CurrentTrain: epoch  0, batch    53 | loss: 125.4117957CurrentTrain: epoch  0, batch    54 | loss: 94.7404126CurrentTrain: epoch  0, batch    55 | loss: 63.6122928CurrentTrain: epoch  0, batch    56 | loss: 64.0233506CurrentTrain: epoch  0, batch    57 | loss: 75.9528338CurrentTrain: epoch  0, batch    58 | loss: 76.3963200CurrentTrain: epoch  0, batch    59 | loss: 75.2072835CurrentTrain: epoch  0, batch    60 | loss: 75.5007516CurrentTrain: epoch  0, batch    61 | loss: 75.7821294CurrentTrain: epoch  0, batch    62 | loss: 62.9361601CurrentTrain: epoch  0, batch    63 | loss: 94.4887790CurrentTrain: epoch  0, batch    64 | loss: 76.7753936CurrentTrain: epoch  0, batch    65 | loss: 55.7058319CurrentTrain: epoch  0, batch    66 | loss: 74.4570540CurrentTrain: epoch  0, batch    67 | loss: 75.9638666CurrentTrain: epoch  0, batch    68 | loss: 63.1779019CurrentTrain: epoch  0, batch    69 | loss: 125.2016352CurrentTrain: epoch  0, batch    70 | loss: 94.6588768CurrentTrain: epoch  0, batch    71 | loss: 63.5497674CurrentTrain: epoch  0, batch    72 | loss: 75.9080570CurrentTrain: epoch  0, batch    73 | loss: 92.5703476CurrentTrain: epoch  0, batch    74 | loss: 125.1402452CurrentTrain: epoch  0, batch    75 | loss: 75.9308202CurrentTrain: epoch  0, batch    76 | loss: 91.0465729CurrentTrain: epoch  0, batch    77 | loss: 74.2127374CurrentTrain: epoch  0, batch    78 | loss: 92.2035862CurrentTrain: epoch  0, batch    79 | loss: 92.4732564CurrentTrain: epoch  0, batch    80 | loss: 61.0365240CurrentTrain: epoch  0, batch    81 | loss: 73.7346678CurrentTrain: epoch  0, batch    82 | loss: 93.9142809CurrentTrain: epoch  0, batch    83 | loss: 75.5147144CurrentTrain: epoch  0, batch    84 | loss: 89.9728495CurrentTrain: epoch  0, batch    85 | loss: 63.3118555CurrentTrain: epoch  0, batch    86 | loss: 122.3811490CurrentTrain: epoch  0, batch    87 | loss: 62.1899614CurrentTrain: epoch  0, batch    88 | loss: 62.3969128CurrentTrain: epoch  0, batch    89 | loss: 54.1166502CurrentTrain: epoch  0, batch    90 | loss: 73.5797262CurrentTrain: epoch  0, batch    91 | loss: 62.5559627CurrentTrain: epoch  0, batch    92 | loss: 73.3436749CurrentTrain: epoch  0, batch    93 | loss: 120.3566015CurrentTrain: epoch  0, batch    94 | loss: 62.7520737CurrentTrain: epoch  0, batch    95 | loss: 77.9444012CurrentTrain: epoch  1, batch     0 | loss: 60.6085949CurrentTrain: epoch  1, batch     1 | loss: 74.1182280CurrentTrain: epoch  1, batch     2 | loss: 62.9817913CurrentTrain: epoch  1, batch     3 | loss: 60.9403573CurrentTrain: epoch  1, batch     4 | loss: 70.6212517CurrentTrain: epoch  1, batch     5 | loss: 57.0725123CurrentTrain: epoch  1, batch     6 | loss: 75.4064244CurrentTrain: epoch  1, batch     7 | loss: 120.5444956CurrentTrain: epoch  1, batch     8 | loss: 51.6317314CurrentTrain: epoch  1, batch     9 | loss: 90.7609994CurrentTrain: epoch  1, batch    10 | loss: 72.1517449CurrentTrain: epoch  1, batch    11 | loss: 59.7808496CurrentTrain: epoch  1, batch    12 | loss: 58.2920550CurrentTrain: epoch  1, batch    13 | loss: 91.6090990CurrentTrain: epoch  1, batch    14 | loss: 87.0204285CurrentTrain: epoch  1, batch    15 | loss: 73.7465399CurrentTrain: epoch  1, batch    16 | loss: 118.6273847CurrentTrain: epoch  1, batch    17 | loss: 58.3796485CurrentTrain: epoch  1, batch    18 | loss: 67.9504642CurrentTrain: epoch  1, batch    19 | loss: 89.5399388CurrentTrain: epoch  1, batch    20 | loss: 123.5050438CurrentTrain: epoch  1, batch    21 | loss: 58.9292464CurrentTrain: epoch  1, batch    22 | loss: 58.9138349CurrentTrain: epoch  1, batch    23 | loss: 72.2397404CurrentTrain: epoch  1, batch    24 | loss: 122.4573415CurrentTrain: epoch  1, batch    25 | loss: 72.9968481CurrentTrain: epoch  1, batch    26 | loss: 50.8678595CurrentTrain: epoch  1, batch    27 | loss: 59.8642532CurrentTrain: epoch  1, batch    28 | loss: 50.8893924CurrentTrain: epoch  1, batch    29 | loss: 51.8987983CurrentTrain: epoch  1, batch    30 | loss: 71.0878238CurrentTrain: epoch  1, batch    31 | loss: 73.1530718CurrentTrain: epoch  1, batch    32 | loss: 92.4319705CurrentTrain: epoch  1, batch    33 | loss: 71.3523258CurrentTrain: epoch  1, batch    34 | loss: 82.1718610CurrentTrain: epoch  1, batch    35 | loss: 123.7680147CurrentTrain: epoch  1, batch    36 | loss: 58.9572062CurrentTrain: epoch  1, batch    37 | loss: 69.5220183CurrentTrain: epoch  1, batch    38 | loss: 69.2196251CurrentTrain: epoch  1, batch    39 | loss: 49.4583564CurrentTrain: epoch  1, batch    40 | loss: 57.1381410CurrentTrain: epoch  1, batch    41 | loss: 58.9419098CurrentTrain: epoch  1, batch    42 | loss: 59.7964636CurrentTrain: epoch  1, batch    43 | loss: 71.4821518CurrentTrain: epoch  1, batch    44 | loss: 124.2379321CurrentTrain: epoch  1, batch    45 | loss: 71.6895405CurrentTrain: epoch  1, batch    46 | loss: 88.3006517CurrentTrain: epoch  1, batch    47 | loss: 57.1863142CurrentTrain: epoch  1, batch    48 | loss: 69.9447049CurrentTrain: epoch  1, batch    49 | loss: 71.1404754CurrentTrain: epoch  1, batch    50 | loss: 71.0278182CurrentTrain: epoch  1, batch    51 | loss: 92.4311155CurrentTrain: epoch  1, batch    52 | loss: 94.0573151CurrentTrain: epoch  1, batch    53 | loss: 121.5341958CurrentTrain: epoch  1, batch    54 | loss: 61.1518037CurrentTrain: epoch  1, batch    55 | loss: 70.4165386CurrentTrain: epoch  1, batch    56 | loss: 69.4655149CurrentTrain: epoch  1, batch    57 | loss: 87.8286952CurrentTrain: epoch  1, batch    58 | loss: 71.8915611CurrentTrain: epoch  1, batch    59 | loss: 58.4786665CurrentTrain: epoch  1, batch    60 | loss: 69.7513193CurrentTrain: epoch  1, batch    61 | loss: 119.2385813CurrentTrain: epoch  1, batch    62 | loss: 120.1940583CurrentTrain: epoch  1, batch    63 | loss: 57.8432755CurrentTrain: epoch  1, batch    64 | loss: 70.9992671CurrentTrain: epoch  1, batch    65 | loss: 88.6526368CurrentTrain: epoch  1, batch    66 | loss: 49.3668035CurrentTrain: epoch  1, batch    67 | loss: 119.2215815CurrentTrain: epoch  1, batch    68 | loss: 54.5134397CurrentTrain: epoch  1, batch    69 | loss: 60.8286780CurrentTrain: epoch  1, batch    70 | loss: 57.6588724CurrentTrain: epoch  1, batch    71 | loss: 73.3092572CurrentTrain: epoch  1, batch    72 | loss: 89.0278288CurrentTrain: epoch  1, batch    73 | loss: 87.4387432CurrentTrain: epoch  1, batch    74 | loss: 87.6979476CurrentTrain: epoch  1, batch    75 | loss: 68.2408530CurrentTrain: epoch  1, batch    76 | loss: 71.1347640CurrentTrain: epoch  1, batch    77 | loss: 73.8378633CurrentTrain: epoch  1, batch    78 | loss: 71.0645687CurrentTrain: epoch  1, batch    79 | loss: 70.1291449CurrentTrain: epoch  1, batch    80 | loss: 47.8011943CurrentTrain: epoch  1, batch    81 | loss: 87.2829294CurrentTrain: epoch  1, batch    82 | loss: 56.7925313CurrentTrain: epoch  1, batch    83 | loss: 186.6166692CurrentTrain: epoch  1, batch    84 | loss: 89.6674943CurrentTrain: epoch  1, batch    85 | loss: 57.8122076CurrentTrain: epoch  1, batch    86 | loss: 47.2682924CurrentTrain: epoch  1, batch    87 | loss: 59.5640132CurrentTrain: epoch  1, batch    88 | loss: 89.1172481CurrentTrain: epoch  1, batch    89 | loss: 73.5067488CurrentTrain: epoch  1, batch    90 | loss: 58.5993265CurrentTrain: epoch  1, batch    91 | loss: 70.2104749CurrentTrain: epoch  1, batch    92 | loss: 67.8102399CurrentTrain: epoch  1, batch    93 | loss: 121.6532883CurrentTrain: epoch  1, batch    94 | loss: 73.3584983CurrentTrain: epoch  1, batch    95 | loss: 57.0777778CurrentTrain: epoch  2, batch     0 | loss: 47.6499845CurrentTrain: epoch  2, batch     1 | loss: 91.3912510CurrentTrain: epoch  2, batch     2 | loss: 68.4412635CurrentTrain: epoch  2, batch     3 | loss: 70.6235947CurrentTrain: epoch  2, batch     4 | loss: 117.8835029CurrentTrain: epoch  2, batch     5 | loss: 55.9866090CurrentTrain: epoch  2, batch     6 | loss: 121.5455932CurrentTrain: epoch  2, batch     7 | loss: 68.4528017CurrentTrain: epoch  2, batch     8 | loss: 86.0712572CurrentTrain: epoch  2, batch     9 | loss: 46.3866328CurrentTrain: epoch  2, batch    10 | loss: 84.1937001CurrentTrain: epoch  2, batch    11 | loss: 45.2949405CurrentTrain: epoch  2, batch    12 | loss: 48.4362682CurrentTrain: epoch  2, batch    13 | loss: 87.8565417CurrentTrain: epoch  2, batch    14 | loss: 89.4270902CurrentTrain: epoch  2, batch    15 | loss: 90.6178946CurrentTrain: epoch  2, batch    16 | loss: 67.0320108CurrentTrain: epoch  2, batch    17 | loss: 68.2007284CurrentTrain: epoch  2, batch    18 | loss: 68.4375315CurrentTrain: epoch  2, batch    19 | loss: 87.1142342CurrentTrain: epoch  2, batch    20 | loss: 54.9122109CurrentTrain: epoch  2, batch    21 | loss: 84.8237995CurrentTrain: epoch  2, batch    22 | loss: 56.4823966CurrentTrain: epoch  2, batch    23 | loss: 68.3941837CurrentTrain: epoch  2, batch    24 | loss: 65.8865449CurrentTrain: epoch  2, batch    25 | loss: 66.5335383CurrentTrain: epoch  2, batch    26 | loss: 73.1383254CurrentTrain: epoch  2, batch    27 | loss: 68.7894133CurrentTrain: epoch  2, batch    28 | loss: 86.3944566CurrentTrain: epoch  2, batch    29 | loss: 87.7587180CurrentTrain: epoch  2, batch    30 | loss: 71.4355790CurrentTrain: epoch  2, batch    31 | loss: 65.9994365CurrentTrain: epoch  2, batch    32 | loss: 58.0370853CurrentTrain: epoch  2, batch    33 | loss: 96.3634099CurrentTrain: epoch  2, batch    34 | loss: 87.4897484CurrentTrain: epoch  2, batch    35 | loss: 88.7428935CurrentTrain: epoch  2, batch    36 | loss: 68.9517810CurrentTrain: epoch  2, batch    37 | loss: 84.2905928CurrentTrain: epoch  2, batch    38 | loss: 56.5480184CurrentTrain: epoch  2, batch    39 | loss: 86.0870466CurrentTrain: epoch  2, batch    40 | loss: 44.6098166CurrentTrain: epoch  2, batch    41 | loss: 87.6160891CurrentTrain: epoch  2, batch    42 | loss: 84.7539535CurrentTrain: epoch  2, batch    43 | loss: 56.5507275CurrentTrain: epoch  2, batch    44 | loss: 68.3816928CurrentTrain: epoch  2, batch    45 | loss: 57.8059797CurrentTrain: epoch  2, batch    46 | loss: 68.3352783CurrentTrain: epoch  2, batch    47 | loss: 56.9501980CurrentTrain: epoch  2, batch    48 | loss: 118.9625530CurrentTrain: epoch  2, batch    49 | loss: 120.3266827CurrentTrain: epoch  2, batch    50 | loss: 55.6054650CurrentTrain: epoch  2, batch    51 | loss: 120.4062697CurrentTrain: epoch  2, batch    52 | loss: 56.8554752CurrentTrain: epoch  2, batch    53 | loss: 69.3135657CurrentTrain: epoch  2, batch    54 | loss: 88.2410826CurrentTrain: epoch  2, batch    55 | loss: 68.2451787CurrentTrain: epoch  2, batch    56 | loss: 66.4625025CurrentTrain: epoch  2, batch    57 | loss: 54.6444471CurrentTrain: epoch  2, batch    58 | loss: 57.2495472CurrentTrain: epoch  2, batch    59 | loss: 56.7177393CurrentTrain: epoch  2, batch    60 | loss: 67.5390789CurrentTrain: epoch  2, batch    61 | loss: 53.5299837CurrentTrain: epoch  2, batch    62 | loss: 67.1342746CurrentTrain: epoch  2, batch    63 | loss: 56.1415878CurrentTrain: epoch  2, batch    64 | loss: 54.3702301CurrentTrain: epoch  2, batch    65 | loss: 68.3782128CurrentTrain: epoch  2, batch    66 | loss: 118.8963823CurrentTrain: epoch  2, batch    67 | loss: 91.5614778CurrentTrain: epoch  2, batch    68 | loss: 57.9854235CurrentTrain: epoch  2, batch    69 | loss: 55.2063941CurrentTrain: epoch  2, batch    70 | loss: 68.7345001CurrentTrain: epoch  2, batch    71 | loss: 55.3021781CurrentTrain: epoch  2, batch    72 | loss: 181.9403751CurrentTrain: epoch  2, batch    73 | loss: 72.4071640CurrentTrain: epoch  2, batch    74 | loss: 68.3508191CurrentTrain: epoch  2, batch    75 | loss: 70.5971244CurrentTrain: epoch  2, batch    76 | loss: 84.9015786CurrentTrain: epoch  2, batch    77 | loss: 47.6768139CurrentTrain: epoch  2, batch    78 | loss: 68.5282250CurrentTrain: epoch  2, batch    79 | loss: 68.8289900CurrentTrain: epoch  2, batch    80 | loss: 55.4856078CurrentTrain: epoch  2, batch    81 | loss: 68.3196844CurrentTrain: epoch  2, batch    82 | loss: 88.3445182CurrentTrain: epoch  2, batch    83 | loss: 89.9389832CurrentTrain: epoch  2, batch    84 | loss: 57.1707645CurrentTrain: epoch  2, batch    85 | loss: 117.0922009CurrentTrain: epoch  2, batch    86 | loss: 47.1579626CurrentTrain: epoch  2, batch    87 | loss: 47.5556565CurrentTrain: epoch  2, batch    88 | loss: 49.1028266CurrentTrain: epoch  2, batch    89 | loss: 83.7029468CurrentTrain: epoch  2, batch    90 | loss: 69.1200564CurrentTrain: epoch  2, batch    91 | loss: 58.5928701CurrentTrain: epoch  2, batch    92 | loss: 58.2231378CurrentTrain: epoch  2, batch    93 | loss: 72.9600350CurrentTrain: epoch  2, batch    94 | loss: 66.9226015CurrentTrain: epoch  2, batch    95 | loss: 55.1497785CurrentTrain: epoch  3, batch     0 | loss: 69.3767250CurrentTrain: epoch  3, batch     1 | loss: 86.4252921CurrentTrain: epoch  3, batch     2 | loss: 70.9276027CurrentTrain: epoch  3, batch     3 | loss: 68.3362585CurrentTrain: epoch  3, batch     4 | loss: 67.3928791CurrentTrain: epoch  3, batch     5 | loss: 84.6240836CurrentTrain: epoch  3, batch     6 | loss: 64.9414230CurrentTrain: epoch  3, batch     7 | loss: 47.4306290CurrentTrain: epoch  3, batch     8 | loss: 55.3297692CurrentTrain: epoch  3, batch     9 | loss: 116.3698759CurrentTrain: epoch  3, batch    10 | loss: 55.8993740CurrentTrain: epoch  3, batch    11 | loss: 63.8387826CurrentTrain: epoch  3, batch    12 | loss: 84.3196990CurrentTrain: epoch  3, batch    13 | loss: 84.6213154CurrentTrain: epoch  3, batch    14 | loss: 69.0482950CurrentTrain: epoch  3, batch    15 | loss: 87.3097451CurrentTrain: epoch  3, batch    16 | loss: 85.6409863CurrentTrain: epoch  3, batch    17 | loss: 53.1003196CurrentTrain: epoch  3, batch    18 | loss: 56.6232579CurrentTrain: epoch  3, batch    19 | loss: 56.2307879CurrentTrain: epoch  3, batch    20 | loss: 86.6302774CurrentTrain: epoch  3, batch    21 | loss: 67.2116752CurrentTrain: epoch  3, batch    22 | loss: 85.4299820CurrentTrain: epoch  3, batch    23 | loss: 67.2256775CurrentTrain: epoch  3, batch    24 | loss: 56.0430732CurrentTrain: epoch  3, batch    25 | loss: 69.4389063CurrentTrain: epoch  3, batch    26 | loss: 86.2039645CurrentTrain: epoch  3, batch    27 | loss: 91.7241202CurrentTrain: epoch  3, batch    28 | loss: 83.9644305CurrentTrain: epoch  3, batch    29 | loss: 55.6766720CurrentTrain: epoch  3, batch    30 | loss: 70.7107876CurrentTrain: epoch  3, batch    31 | loss: 63.7034002CurrentTrain: epoch  3, batch    32 | loss: 66.1950083CurrentTrain: epoch  3, batch    33 | loss: 115.8916624CurrentTrain: epoch  3, batch    34 | loss: 68.0200608CurrentTrain: epoch  3, batch    35 | loss: 116.0737340CurrentTrain: epoch  3, batch    36 | loss: 43.8423872CurrentTrain: epoch  3, batch    37 | loss: 118.1280241CurrentTrain: epoch  3, batch    38 | loss: 50.1090966CurrentTrain: epoch  3, batch    39 | loss: 66.6123010CurrentTrain: epoch  3, batch    40 | loss: 67.5283313CurrentTrain: epoch  3, batch    41 | loss: 65.2928807CurrentTrain: epoch  3, batch    42 | loss: 66.6704224CurrentTrain: epoch  3, batch    43 | loss: 52.6286473CurrentTrain: epoch  3, batch    44 | loss: 50.0438127CurrentTrain: epoch  3, batch    45 | loss: 56.5877960CurrentTrain: epoch  3, batch    46 | loss: 87.5279448CurrentTrain: epoch  3, batch    47 | loss: 70.2654516CurrentTrain: epoch  3, batch    48 | loss: 119.2085481CurrentTrain: epoch  3, batch    49 | loss: 48.4600526CurrentTrain: epoch  3, batch    50 | loss: 55.3240344CurrentTrain: epoch  3, batch    51 | loss: 56.1274586CurrentTrain: epoch  3, batch    52 | loss: 122.2891352CurrentTrain: epoch  3, batch    53 | loss: 57.7248736CurrentTrain: epoch  3, batch    54 | loss: 84.4522418CurrentTrain: epoch  3, batch    55 | loss: 65.6570069CurrentTrain: epoch  3, batch    56 | loss: 58.2691558CurrentTrain: epoch  3, batch    57 | loss: 56.0031373CurrentTrain: epoch  3, batch    58 | loss: 71.4566969CurrentTrain: epoch  3, batch    59 | loss: 113.8094059CurrentTrain: epoch  3, batch    60 | loss: 68.5329040CurrentTrain: epoch  3, batch    61 | loss: 55.1658388CurrentTrain: epoch  3, batch    62 | loss: 66.8915759CurrentTrain: epoch  3, batch    63 | loss: 83.3430008CurrentTrain: epoch  3, batch    64 | loss: 64.7205595CurrentTrain: epoch  3, batch    65 | loss: 87.0780708CurrentTrain: epoch  3, batch    66 | loss: 88.7176435CurrentTrain: epoch  3, batch    67 | loss: 55.0461234CurrentTrain: epoch  3, batch    68 | loss: 67.7717572CurrentTrain: epoch  3, batch    69 | loss: 54.2791560CurrentTrain: epoch  3, batch    70 | loss: 69.3463483CurrentTrain: epoch  3, batch    71 | loss: 44.4676419CurrentTrain: epoch  3, batch    72 | loss: 55.6038988CurrentTrain: epoch  3, batch    73 | loss: 42.9732797CurrentTrain: epoch  3, batch    74 | loss: 69.2377130CurrentTrain: epoch  3, batch    75 | loss: 88.2978885CurrentTrain: epoch  3, batch    76 | loss: 81.6281837CurrentTrain: epoch  3, batch    77 | loss: 68.3465423CurrentTrain: epoch  3, batch    78 | loss: 113.4137492CurrentTrain: epoch  3, batch    79 | loss: 64.9567918CurrentTrain: epoch  3, batch    80 | loss: 85.7737395CurrentTrain: epoch  3, batch    81 | loss: 43.4359706CurrentTrain: epoch  3, batch    82 | loss: 93.3055235CurrentTrain: epoch  3, batch    83 | loss: 67.1178191CurrentTrain: epoch  3, batch    84 | loss: 55.0310179CurrentTrain: epoch  3, batch    85 | loss: 86.8257490CurrentTrain: epoch  3, batch    86 | loss: 55.6135053CurrentTrain: epoch  3, batch    87 | loss: 68.2662294CurrentTrain: epoch  3, batch    88 | loss: 52.7767659CurrentTrain: epoch  3, batch    89 | loss: 53.9853590CurrentTrain: epoch  3, batch    90 | loss: 84.2259441CurrentTrain: epoch  3, batch    91 | loss: 85.8730845CurrentTrain: epoch  3, batch    92 | loss: 56.8215129CurrentTrain: epoch  3, batch    93 | loss: 67.5406998CurrentTrain: epoch  3, batch    94 | loss: 88.8910523CurrentTrain: epoch  3, batch    95 | loss: 42.6188665CurrentTrain: epoch  4, batch     0 | loss: 66.4263704CurrentTrain: epoch  4, batch     1 | loss: 86.7932369CurrentTrain: epoch  4, batch     2 | loss: 82.3738327CurrentTrain: epoch  4, batch     3 | loss: 62.5186491CurrentTrain: epoch  4, batch     4 | loss: 120.0414410CurrentTrain: epoch  4, batch     5 | loss: 82.3014557CurrentTrain: epoch  4, batch     6 | loss: 53.9083132CurrentTrain: epoch  4, batch     7 | loss: 49.8212222CurrentTrain: epoch  4, batch     8 | loss: 87.3739976CurrentTrain: epoch  4, batch     9 | loss: 119.4775512CurrentTrain: epoch  4, batch    10 | loss: 87.1275459CurrentTrain: epoch  4, batch    11 | loss: 54.9337052CurrentTrain: epoch  4, batch    12 | loss: 66.4964969CurrentTrain: epoch  4, batch    13 | loss: 53.5931353CurrentTrain: epoch  4, batch    14 | loss: 64.2319867CurrentTrain: epoch  4, batch    15 | loss: 84.9008532CurrentTrain: epoch  4, batch    16 | loss: 82.0153020CurrentTrain: epoch  4, batch    17 | loss: 117.8624895CurrentTrain: epoch  4, batch    18 | loss: 63.8814448CurrentTrain: epoch  4, batch    19 | loss: 65.2084872CurrentTrain: epoch  4, batch    20 | loss: 64.9578154CurrentTrain: epoch  4, batch    21 | loss: 56.0938370CurrentTrain: epoch  4, batch    22 | loss: 86.2777789CurrentTrain: epoch  4, batch    23 | loss: 62.0991842CurrentTrain: epoch  4, batch    24 | loss: 82.0526033CurrentTrain: epoch  4, batch    25 | loss: 83.4797792CurrentTrain: epoch  4, batch    26 | loss: 66.6190853CurrentTrain: epoch  4, batch    27 | loss: 46.5482721CurrentTrain: epoch  4, batch    28 | loss: 84.4521460CurrentTrain: epoch  4, batch    29 | loss: 65.0367918CurrentTrain: epoch  4, batch    30 | loss: 68.0576346CurrentTrain: epoch  4, batch    31 | loss: 56.8640783CurrentTrain: epoch  4, batch    32 | loss: 80.7782992CurrentTrain: epoch  4, batch    33 | loss: 43.1692329CurrentTrain: epoch  4, batch    34 | loss: 84.1972328CurrentTrain: epoch  4, batch    35 | loss: 68.0142180CurrentTrain: epoch  4, batch    36 | loss: 53.5634766CurrentTrain: epoch  4, batch    37 | loss: 45.7447464CurrentTrain: epoch  4, batch    38 | loss: 64.6970455CurrentTrain: epoch  4, batch    39 | loss: 55.9004997CurrentTrain: epoch  4, batch    40 | loss: 66.8045242CurrentTrain: epoch  4, batch    41 | loss: 56.2656139CurrentTrain: epoch  4, batch    42 | loss: 68.1734868CurrentTrain: epoch  4, batch    43 | loss: 43.7196841CurrentTrain: epoch  4, batch    44 | loss: 51.9958525CurrentTrain: epoch  4, batch    45 | loss: 70.4332744CurrentTrain: epoch  4, batch    46 | loss: 58.8834311CurrentTrain: epoch  4, batch    47 | loss: 84.1104580CurrentTrain: epoch  4, batch    48 | loss: 55.9372523CurrentTrain: epoch  4, batch    49 | loss: 71.2692090CurrentTrain: epoch  4, batch    50 | loss: 66.4559009CurrentTrain: epoch  4, batch    51 | loss: 57.7525272CurrentTrain: epoch  4, batch    52 | loss: 67.5694726CurrentTrain: epoch  4, batch    53 | loss: 113.7947093CurrentTrain: epoch  4, batch    54 | loss: 66.3796167CurrentTrain: epoch  4, batch    55 | loss: 85.2808205CurrentTrain: epoch  4, batch    56 | loss: 119.0861984CurrentTrain: epoch  4, batch    57 | loss: 54.1315448CurrentTrain: epoch  4, batch    58 | loss: 113.8951271CurrentTrain: epoch  4, batch    59 | loss: 86.1538116CurrentTrain: epoch  4, batch    60 | loss: 68.7032721CurrentTrain: epoch  4, batch    61 | loss: 45.3344640CurrentTrain: epoch  4, batch    62 | loss: 46.6226011CurrentTrain: epoch  4, batch    63 | loss: 53.9778637CurrentTrain: epoch  4, batch    64 | loss: 67.1445594CurrentTrain: epoch  4, batch    65 | loss: 116.1665349CurrentTrain: epoch  4, batch    66 | loss: 57.4994139CurrentTrain: epoch  4, batch    67 | loss: 52.7426028CurrentTrain: epoch  4, batch    68 | loss: 86.0771344CurrentTrain: epoch  4, batch    69 | loss: 52.7393135CurrentTrain: epoch  4, batch    70 | loss: 78.6170581CurrentTrain: epoch  4, batch    71 | loss: 87.2229443CurrentTrain: epoch  4, batch    72 | loss: 49.2805316CurrentTrain: epoch  4, batch    73 | loss: 55.6545570CurrentTrain: epoch  4, batch    74 | loss: 118.4015348CurrentTrain: epoch  4, batch    75 | loss: 65.1391819CurrentTrain: epoch  4, batch    76 | loss: 121.9684759CurrentTrain: epoch  4, batch    77 | loss: 81.9815355CurrentTrain: epoch  4, batch    78 | loss: 54.2645518CurrentTrain: epoch  4, batch    79 | loss: 82.9241217CurrentTrain: epoch  4, batch    80 | loss: 53.6604928CurrentTrain: epoch  4, batch    81 | loss: 120.2245189CurrentTrain: epoch  4, batch    82 | loss: 63.2203339CurrentTrain: epoch  4, batch    83 | loss: 52.7988823CurrentTrain: epoch  4, batch    84 | loss: 61.1182555CurrentTrain: epoch  4, batch    85 | loss: 65.2594996CurrentTrain: epoch  4, batch    86 | loss: 87.7918768CurrentTrain: epoch  4, batch    87 | loss: 115.2492312CurrentTrain: epoch  4, batch    88 | loss: 84.9054812CurrentTrain: epoch  4, batch    89 | loss: 54.9804429CurrentTrain: epoch  4, batch    90 | loss: 111.9847271CurrentTrain: epoch  4, batch    91 | loss: 80.3692781CurrentTrain: epoch  4, batch    92 | loss: 66.8192463CurrentTrain: epoch  4, batch    93 | loss: 62.8071468CurrentTrain: epoch  4, batch    94 | loss: 71.1637022CurrentTrain: epoch  4, batch    95 | loss: 45.5346922CurrentTrain: epoch  5, batch     0 | loss: 64.1273581CurrentTrain: epoch  5, batch     1 | loss: 62.7392642CurrentTrain: epoch  5, batch     2 | loss: 83.1211048CurrentTrain: epoch  5, batch     3 | loss: 65.6435607CurrentTrain: epoch  5, batch     4 | loss: 62.9806333CurrentTrain: epoch  5, batch     5 | loss: 53.1091566CurrentTrain: epoch  5, batch     6 | loss: 120.0358974CurrentTrain: epoch  5, batch     7 | loss: 63.2133320CurrentTrain: epoch  5, batch     8 | loss: 64.1300992CurrentTrain: epoch  5, batch     9 | loss: 64.0002587CurrentTrain: epoch  5, batch    10 | loss: 55.3977523CurrentTrain: epoch  5, batch    11 | loss: 84.6823098CurrentTrain: epoch  5, batch    12 | loss: 117.6587736CurrentTrain: epoch  5, batch    13 | loss: 52.7723168CurrentTrain: epoch  5, batch    14 | loss: 82.8580137CurrentTrain: epoch  5, batch    15 | loss: 52.1769073CurrentTrain: epoch  5, batch    16 | loss: 65.1371451CurrentTrain: epoch  5, batch    17 | loss: 51.3380383CurrentTrain: epoch  5, batch    18 | loss: 67.3368114CurrentTrain: epoch  5, batch    19 | loss: 82.7970986CurrentTrain: epoch  5, batch    20 | loss: 64.4395099CurrentTrain: epoch  5, batch    21 | loss: 52.4169021CurrentTrain: epoch  5, batch    22 | loss: 66.4662391CurrentTrain: epoch  5, batch    23 | loss: 82.3662884CurrentTrain: epoch  5, batch    24 | loss: 44.7748404CurrentTrain: epoch  5, batch    25 | loss: 66.2994478CurrentTrain: epoch  5, batch    26 | loss: 85.9291335CurrentTrain: epoch  5, batch    27 | loss: 81.6566768CurrentTrain: epoch  5, batch    28 | loss: 118.7227964CurrentTrain: epoch  5, batch    29 | loss: 372.1657818CurrentTrain: epoch  5, batch    30 | loss: 55.9915202CurrentTrain: epoch  5, batch    31 | loss: 177.6028256CurrentTrain: epoch  5, batch    32 | loss: 52.6913353CurrentTrain: epoch  5, batch    33 | loss: 59.1418592CurrentTrain: epoch  5, batch    34 | loss: 65.4699952CurrentTrain: epoch  5, batch    35 | loss: 54.2468002CurrentTrain: epoch  5, batch    36 | loss: 117.5352568CurrentTrain: epoch  5, batch    37 | loss: 66.7024163CurrentTrain: epoch  5, batch    38 | loss: 79.5862064CurrentTrain: epoch  5, batch    39 | loss: 86.1314827CurrentTrain: epoch  5, batch    40 | loss: 63.6048872CurrentTrain: epoch  5, batch    41 | loss: 86.7395766CurrentTrain: epoch  5, batch    42 | loss: 120.2538136CurrentTrain: epoch  5, batch    43 | loss: 54.2410437CurrentTrain: epoch  5, batch    44 | loss: 52.4322648CurrentTrain: epoch  5, batch    45 | loss: 65.9873062CurrentTrain: epoch  5, batch    46 | loss: 68.0390992CurrentTrain: epoch  5, batch    47 | loss: 51.6659836CurrentTrain: epoch  5, batch    48 | loss: 50.2734661CurrentTrain: epoch  5, batch    49 | loss: 54.4772512CurrentTrain: epoch  5, batch    50 | loss: 45.5616670CurrentTrain: epoch  5, batch    51 | loss: 63.3991159CurrentTrain: epoch  5, batch    52 | loss: 68.5237399CurrentTrain: epoch  5, batch    53 | loss: 61.4924376CurrentTrain: epoch  5, batch    54 | loss: 62.8833455CurrentTrain: epoch  5, batch    55 | loss: 85.8532907CurrentTrain: epoch  5, batch    56 | loss: 117.9540140CurrentTrain: epoch  5, batch    57 | loss: 69.6319916CurrentTrain: epoch  5, batch    58 | loss: 66.2152461CurrentTrain: epoch  5, batch    59 | loss: 115.7449779CurrentTrain: epoch  5, batch    60 | loss: 85.5687927CurrentTrain: epoch  5, batch    61 | loss: 67.4467428CurrentTrain: epoch  5, batch    62 | loss: 88.5661265CurrentTrain: epoch  5, batch    63 | loss: 67.5370242CurrentTrain: epoch  5, batch    64 | loss: 44.4520171CurrentTrain: epoch  5, batch    65 | loss: 64.4871271CurrentTrain: epoch  5, batch    66 | loss: 64.0472503CurrentTrain: epoch  5, batch    67 | loss: 65.6329126CurrentTrain: epoch  5, batch    68 | loss: 51.3360764CurrentTrain: epoch  5, batch    69 | loss: 65.6348293CurrentTrain: epoch  5, batch    70 | loss: 85.5022399CurrentTrain: epoch  5, batch    71 | loss: 86.3626245CurrentTrain: epoch  5, batch    72 | loss: 84.1450449CurrentTrain: epoch  5, batch    73 | loss: 62.8380415CurrentTrain: epoch  5, batch    74 | loss: 50.3685067CurrentTrain: epoch  5, batch    75 | loss: 52.0936402CurrentTrain: epoch  5, batch    76 | loss: 115.3604489CurrentTrain: epoch  5, batch    77 | loss: 66.9101407CurrentTrain: epoch  5, batch    78 | loss: 62.7947815CurrentTrain: epoch  5, batch    79 | loss: 67.2942522CurrentTrain: epoch  5, batch    80 | loss: 83.8047616CurrentTrain: epoch  5, batch    81 | loss: 85.0164439CurrentTrain: epoch  5, batch    82 | loss: 66.0204160CurrentTrain: epoch  5, batch    83 | loss: 84.3273676CurrentTrain: epoch  5, batch    84 | loss: 83.2668746CurrentTrain: epoch  5, batch    85 | loss: 65.0407669CurrentTrain: epoch  5, batch    86 | loss: 84.6853724CurrentTrain: epoch  5, batch    87 | loss: 54.7876511CurrentTrain: epoch  5, batch    88 | loss: 83.9973423CurrentTrain: epoch  5, batch    89 | loss: 63.8194854CurrentTrain: epoch  5, batch    90 | loss: 65.1117438CurrentTrain: epoch  5, batch    91 | loss: 69.2545372CurrentTrain: epoch  5, batch    92 | loss: 65.2076133CurrentTrain: epoch  5, batch    93 | loss: 51.9221112CurrentTrain: epoch  5, batch    94 | loss: 116.4641887CurrentTrain: epoch  5, batch    95 | loss: 45.2029579CurrentTrain: epoch  6, batch     0 | loss: 43.3191995CurrentTrain: epoch  6, batch     1 | loss: 65.4210458CurrentTrain: epoch  6, batch     2 | loss: 62.4095841CurrentTrain: epoch  6, batch     3 | loss: 85.8757327CurrentTrain: epoch  6, batch     4 | loss: 118.0133932CurrentTrain: epoch  6, batch     5 | loss: 181.7720686CurrentTrain: epoch  6, batch     6 | loss: 66.4688983CurrentTrain: epoch  6, batch     7 | loss: 66.0415203CurrentTrain: epoch  6, batch     8 | loss: 84.4068889CurrentTrain: epoch  6, batch     9 | loss: 68.9224896CurrentTrain: epoch  6, batch    10 | loss: 112.1933307CurrentTrain: epoch  6, batch    11 | loss: 66.6617065CurrentTrain: epoch  6, batch    12 | loss: 85.8622154CurrentTrain: epoch  6, batch    13 | loss: 86.6244664CurrentTrain: epoch  6, batch    14 | loss: 43.5150516CurrentTrain: epoch  6, batch    15 | loss: 61.9048555CurrentTrain: epoch  6, batch    16 | loss: 51.2189541CurrentTrain: epoch  6, batch    17 | loss: 65.5398984CurrentTrain: epoch  6, batch    18 | loss: 52.1607426CurrentTrain: epoch  6, batch    19 | loss: 51.3125066CurrentTrain: epoch  6, batch    20 | loss: 51.0048965CurrentTrain: epoch  6, batch    21 | loss: 66.8099062CurrentTrain: epoch  6, batch    22 | loss: 67.5978857CurrentTrain: epoch  6, batch    23 | loss: 85.0303451CurrentTrain: epoch  6, batch    24 | loss: 84.4966482CurrentTrain: epoch  6, batch    25 | loss: 50.7618396CurrentTrain: epoch  6, batch    26 | loss: 52.6479831CurrentTrain: epoch  6, batch    27 | loss: 50.6692096CurrentTrain: epoch  6, batch    28 | loss: 55.8011191CurrentTrain: epoch  6, batch    29 | loss: 81.0877837CurrentTrain: epoch  6, batch    30 | loss: 84.6871530CurrentTrain: epoch  6, batch    31 | loss: 42.4240338CurrentTrain: epoch  6, batch    32 | loss: 81.9868925CurrentTrain: epoch  6, batch    33 | loss: 67.5839620CurrentTrain: epoch  6, batch    34 | loss: 84.9521937CurrentTrain: epoch  6, batch    35 | loss: 52.9287099CurrentTrain: epoch  6, batch    36 | loss: 52.5316844CurrentTrain: epoch  6, batch    37 | loss: 55.4035269CurrentTrain: epoch  6, batch    38 | loss: 83.0329042CurrentTrain: epoch  6, batch    39 | loss: 63.7650881CurrentTrain: epoch  6, batch    40 | loss: 50.9657597CurrentTrain: epoch  6, batch    41 | loss: 81.0437116CurrentTrain: epoch  6, batch    42 | loss: 63.7216994CurrentTrain: epoch  6, batch    43 | loss: 44.6524251CurrentTrain: epoch  6, batch    44 | loss: 41.0303921CurrentTrain: epoch  6, batch    45 | loss: 66.4707291CurrentTrain: epoch  6, batch    46 | loss: 64.1148023CurrentTrain: epoch  6, batch    47 | loss: 117.6602852CurrentTrain: epoch  6, batch    48 | loss: 53.2503699CurrentTrain: epoch  6, batch    49 | loss: 52.0395204CurrentTrain: epoch  6, batch    50 | loss: 86.8103779CurrentTrain: epoch  6, batch    51 | loss: 82.3980992CurrentTrain: epoch  6, batch    52 | loss: 117.8981867CurrentTrain: epoch  6, batch    53 | loss: 88.6889097CurrentTrain: epoch  6, batch    54 | loss: 65.5860173CurrentTrain: epoch  6, batch    55 | loss: 65.3866616CurrentTrain: epoch  6, batch    56 | loss: 83.4477101CurrentTrain: epoch  6, batch    57 | loss: 49.5682446CurrentTrain: epoch  6, batch    58 | loss: 86.7072685CurrentTrain: epoch  6, batch    59 | loss: 43.9485553CurrentTrain: epoch  6, batch    60 | loss: 63.2190792CurrentTrain: epoch  6, batch    61 | loss: 67.4314631CurrentTrain: epoch  6, batch    62 | loss: 81.4584836CurrentTrain: epoch  6, batch    63 | loss: 64.3499798CurrentTrain: epoch  6, batch    64 | loss: 63.3215998CurrentTrain: epoch  6, batch    65 | loss: 85.0656162CurrentTrain: epoch  6, batch    66 | loss: 70.3883906CurrentTrain: epoch  6, batch    67 | loss: 66.2295311CurrentTrain: epoch  6, batch    68 | loss: 83.6727961CurrentTrain: epoch  6, batch    69 | loss: 63.8453946CurrentTrain: epoch  6, batch    70 | loss: 87.4461451CurrentTrain: epoch  6, batch    71 | loss: 43.4373248CurrentTrain: epoch  6, batch    72 | loss: 55.4791338CurrentTrain: epoch  6, batch    73 | loss: 51.7482317CurrentTrain: epoch  6, batch    74 | loss: 85.8495972CurrentTrain: epoch  6, batch    75 | loss: 83.0683758CurrentTrain: epoch  6, batch    76 | loss: 42.1395235CurrentTrain: epoch  6, batch    77 | loss: 43.6620930CurrentTrain: epoch  6, batch    78 | loss: 84.7446696CurrentTrain: epoch  6, batch    79 | loss: 84.3734653CurrentTrain: epoch  6, batch    80 | loss: 64.3252513CurrentTrain: epoch  6, batch    81 | loss: 44.7524706CurrentTrain: epoch  6, batch    82 | loss: 55.3955942CurrentTrain: epoch  6, batch    83 | loss: 79.6013008CurrentTrain: epoch  6, batch    84 | loss: 66.4287457CurrentTrain: epoch  6, batch    85 | loss: 43.9002992CurrentTrain: epoch  6, batch    86 | loss: 62.7328898CurrentTrain: epoch  6, batch    87 | loss: 83.5751028CurrentTrain: epoch  6, batch    88 | loss: 67.2196879CurrentTrain: epoch  6, batch    89 | loss: 86.4930733CurrentTrain: epoch  6, batch    90 | loss: 83.1363171CurrentTrain: epoch  6, batch    91 | loss: 43.2063477CurrentTrain: epoch  6, batch    92 | loss: 66.3086655CurrentTrain: epoch  6, batch    93 | loss: 64.8287228CurrentTrain: epoch  6, batch    94 | loss: 55.8772649CurrentTrain: epoch  6, batch    95 | loss: 93.2750794CurrentTrain: epoch  7, batch     0 | loss: 65.4516871CurrentTrain: epoch  7, batch     1 | loss: 82.5097756CurrentTrain: epoch  7, batch     2 | loss: 65.9575260CurrentTrain: epoch  7, batch     3 | loss: 64.8513280CurrentTrain: epoch  7, batch     4 | loss: 67.0650511CurrentTrain: epoch  7, batch     5 | loss: 82.9486349CurrentTrain: epoch  7, batch     6 | loss: 66.0713491CurrentTrain: epoch  7, batch     7 | loss: 63.8147370CurrentTrain: epoch  7, batch     8 | loss: 51.0824370CurrentTrain: epoch  7, batch     9 | loss: 51.0854446CurrentTrain: epoch  7, batch    10 | loss: 52.6095027CurrentTrain: epoch  7, batch    11 | loss: 52.3823926CurrentTrain: epoch  7, batch    12 | loss: 84.7052003CurrentTrain: epoch  7, batch    13 | loss: 65.6020051CurrentTrain: epoch  7, batch    14 | loss: 67.6181319CurrentTrain: epoch  7, batch    15 | loss: 54.1311091CurrentTrain: epoch  7, batch    16 | loss: 85.4343759CurrentTrain: epoch  7, batch    17 | loss: 59.0563619CurrentTrain: epoch  7, batch    18 | loss: 52.6248119CurrentTrain: epoch  7, batch    19 | loss: 50.2722930CurrentTrain: epoch  7, batch    20 | loss: 61.7568798CurrentTrain: epoch  7, batch    21 | loss: 62.0554364CurrentTrain: epoch  7, batch    22 | loss: 84.0822349CurrentTrain: epoch  7, batch    23 | loss: 82.7908850CurrentTrain: epoch  7, batch    24 | loss: 53.0313753CurrentTrain: epoch  7, batch    25 | loss: 42.8384700CurrentTrain: epoch  7, batch    26 | loss: 65.3705272CurrentTrain: epoch  7, batch    27 | loss: 85.7926737CurrentTrain: epoch  7, batch    28 | loss: 51.9577622CurrentTrain: epoch  7, batch    29 | loss: 85.9648029CurrentTrain: epoch  7, batch    30 | loss: 63.4019340CurrentTrain: epoch  7, batch    31 | loss: 66.9128074CurrentTrain: epoch  7, batch    32 | loss: 111.0791887CurrentTrain: epoch  7, batch    33 | loss: 65.6369286CurrentTrain: epoch  7, batch    34 | loss: 66.7959249CurrentTrain: epoch  7, batch    35 | loss: 86.1222205CurrentTrain: epoch  7, batch    36 | loss: 70.2294124CurrentTrain: epoch  7, batch    37 | loss: 67.6460698CurrentTrain: epoch  7, batch    38 | loss: 65.6065479CurrentTrain: epoch  7, batch    39 | loss: 84.4895307CurrentTrain: epoch  7, batch    40 | loss: 66.3701530CurrentTrain: epoch  7, batch    41 | loss: 49.2451623CurrentTrain: epoch  7, batch    42 | loss: 81.6366962CurrentTrain: epoch  7, batch    43 | loss: 51.8803752CurrentTrain: epoch  7, batch    44 | loss: 62.5080277CurrentTrain: epoch  7, batch    45 | loss: 84.2007369CurrentTrain: epoch  7, batch    46 | loss: 53.1644866CurrentTrain: epoch  7, batch    47 | loss: 61.9072259CurrentTrain: epoch  7, batch    48 | loss: 112.0431560CurrentTrain: epoch  7, batch    49 | loss: 44.9431348CurrentTrain: epoch  7, batch    50 | loss: 81.0049106CurrentTrain: epoch  7, batch    51 | loss: 61.6201380CurrentTrain: epoch  7, batch    52 | loss: 51.4188530CurrentTrain: epoch  7, batch    53 | loss: 84.0241138CurrentTrain: epoch  7, batch    54 | loss: 51.6742588CurrentTrain: epoch  7, batch    55 | loss: 85.8933409CurrentTrain: epoch  7, batch    56 | loss: 82.4840275CurrentTrain: epoch  7, batch    57 | loss: 65.6855378CurrentTrain: epoch  7, batch    58 | loss: 63.6756617CurrentTrain: epoch  7, batch    59 | loss: 117.9078347CurrentTrain: epoch  7, batch    60 | loss: 51.9258359CurrentTrain: epoch  7, batch    61 | loss: 53.1549961CurrentTrain: epoch  7, batch    62 | loss: 113.1870471CurrentTrain: epoch  7, batch    63 | loss: 66.1471439CurrentTrain: epoch  7, batch    64 | loss: 48.2026263CurrentTrain: epoch  7, batch    65 | loss: 61.3590199CurrentTrain: epoch  7, batch    66 | loss: 64.4919444CurrentTrain: epoch  7, batch    67 | loss: 66.7450530CurrentTrain: epoch  7, batch    68 | loss: 50.8146924CurrentTrain: epoch  7, batch    69 | loss: 64.9934227CurrentTrain: epoch  7, batch    70 | loss: 53.4684449CurrentTrain: epoch  7, batch    71 | loss: 85.8073668CurrentTrain: epoch  7, batch    72 | loss: 49.9161229CurrentTrain: epoch  7, batch    73 | loss: 64.4353390CurrentTrain: epoch  7, batch    74 | loss: 41.4990466CurrentTrain: epoch  7, batch    75 | loss: 49.7287947CurrentTrain: epoch  7, batch    76 | loss: 53.0612787CurrentTrain: epoch  7, batch    77 | loss: 84.1889177CurrentTrain: epoch  7, batch    78 | loss: 68.2399913CurrentTrain: epoch  7, batch    79 | loss: 117.4833236CurrentTrain: epoch  7, batch    80 | loss: 53.6393217CurrentTrain: epoch  7, batch    81 | loss: 51.0546422CurrentTrain: epoch  7, batch    82 | loss: 64.6087147CurrentTrain: epoch  7, batch    83 | loss: 51.1069993CurrentTrain: epoch  7, batch    84 | loss: 62.9525380CurrentTrain: epoch  7, batch    85 | loss: 65.6561782CurrentTrain: epoch  7, batch    86 | loss: 55.6889968CurrentTrain: epoch  7, batch    87 | loss: 83.1952436CurrentTrain: epoch  7, batch    88 | loss: 83.6431710CurrentTrain: epoch  7, batch    89 | loss: 118.5693010CurrentTrain: epoch  7, batch    90 | loss: 66.8870698CurrentTrain: epoch  7, batch    91 | loss: 63.1351727CurrentTrain: epoch  7, batch    92 | loss: 82.8685037CurrentTrain: epoch  7, batch    93 | loss: 44.3852138CurrentTrain: epoch  7, batch    94 | loss: 48.4675083CurrentTrain: epoch  7, batch    95 | loss: 70.4823805CurrentTrain: epoch  8, batch     0 | loss: 53.1739015CurrentTrain: epoch  8, batch     1 | loss: 51.4088458CurrentTrain: epoch  8, batch     2 | loss: 84.0686236CurrentTrain: epoch  8, batch     3 | loss: 117.5121174CurrentTrain: epoch  8, batch     4 | loss: 66.7355789CurrentTrain: epoch  8, batch     5 | loss: 56.2745542CurrentTrain: epoch  8, batch     6 | loss: 67.1572015CurrentTrain: epoch  8, batch     7 | loss: 63.2935317CurrentTrain: epoch  8, batch     8 | loss: 53.6680079CurrentTrain: epoch  8, batch     9 | loss: 84.0407961CurrentTrain: epoch  8, batch    10 | loss: 60.2394242CurrentTrain: epoch  8, batch    11 | loss: 63.5666242CurrentTrain: epoch  8, batch    12 | loss: 64.4492554CurrentTrain: epoch  8, batch    13 | loss: 80.5947222CurrentTrain: epoch  8, batch    14 | loss: 63.6340737CurrentTrain: epoch  8, batch    15 | loss: 65.8197942CurrentTrain: epoch  8, batch    16 | loss: 82.7584267CurrentTrain: epoch  8, batch    17 | loss: 64.1133936CurrentTrain: epoch  8, batch    18 | loss: 85.0126647CurrentTrain: epoch  8, batch    19 | loss: 52.2211074CurrentTrain: epoch  8, batch    20 | loss: 62.3685887CurrentTrain: epoch  8, batch    21 | loss: 65.2412168CurrentTrain: epoch  8, batch    22 | loss: 80.0746376CurrentTrain: epoch  8, batch    23 | loss: 53.0745546CurrentTrain: epoch  8, batch    24 | loss: 52.3305690CurrentTrain: epoch  8, batch    25 | loss: 82.6661832CurrentTrain: epoch  8, batch    26 | loss: 49.2539020CurrentTrain: epoch  8, batch    27 | loss: 63.3786035CurrentTrain: epoch  8, batch    28 | loss: 51.3432844CurrentTrain: epoch  8, batch    29 | loss: 51.9707111CurrentTrain: epoch  8, batch    30 | loss: 86.0973465CurrentTrain: epoch  8, batch    31 | loss: 41.8971636CurrentTrain: epoch  8, batch    32 | loss: 177.5980128CurrentTrain: epoch  8, batch    33 | loss: 84.0282261CurrentTrain: epoch  8, batch    34 | loss: 83.8929041CurrentTrain: epoch  8, batch    35 | loss: 50.6001291CurrentTrain: epoch  8, batch    36 | loss: 80.7457232CurrentTrain: epoch  8, batch    37 | loss: 53.1519275CurrentTrain: epoch  8, batch    38 | loss: 63.4526042CurrentTrain: epoch  8, batch    39 | loss: 66.8731311CurrentTrain: epoch  8, batch    40 | loss: 65.5273928CurrentTrain: epoch  8, batch    41 | loss: 85.9438870CurrentTrain: epoch  8, batch    42 | loss: 65.4615908CurrentTrain: epoch  8, batch    43 | loss: 85.7219735CurrentTrain: epoch  8, batch    44 | loss: 116.3845290CurrentTrain: epoch  8, batch    45 | loss: 65.6332494CurrentTrain: epoch  8, batch    46 | loss: 52.3213200CurrentTrain: epoch  8, batch    47 | loss: 65.4408343CurrentTrain: epoch  8, batch    48 | loss: 50.7565895CurrentTrain: epoch  8, batch    49 | loss: 63.1661595CurrentTrain: epoch  8, batch    50 | loss: 82.4165721CurrentTrain: epoch  8, batch    51 | loss: 52.2755395CurrentTrain: epoch  8, batch    52 | loss: 65.5063706CurrentTrain: epoch  8, batch    53 | loss: 66.9023349CurrentTrain: epoch  8, batch    54 | loss: 82.5753539CurrentTrain: epoch  8, batch    55 | loss: 80.9529003CurrentTrain: epoch  8, batch    56 | loss: 52.1270973CurrentTrain: epoch  8, batch    57 | loss: 51.7718792CurrentTrain: epoch  8, batch    58 | loss: 65.4411414CurrentTrain: epoch  8, batch    59 | loss: 42.2129514CurrentTrain: epoch  8, batch    60 | loss: 116.5167820CurrentTrain: epoch  8, batch    61 | loss: 84.4988059CurrentTrain: epoch  8, batch    62 | loss: 42.4651172CurrentTrain: epoch  8, batch    63 | loss: 69.6429609CurrentTrain: epoch  8, batch    64 | loss: 65.4458087CurrentTrain: epoch  8, batch    65 | loss: 63.4362942CurrentTrain: epoch  8, batch    66 | loss: 60.8627487CurrentTrain: epoch  8, batch    67 | loss: 84.4891367CurrentTrain: epoch  8, batch    68 | loss: 79.6399433CurrentTrain: epoch  8, batch    69 | loss: 65.6808835CurrentTrain: epoch  8, batch    70 | loss: 82.4982001CurrentTrain: epoch  8, batch    71 | loss: 112.5264227CurrentTrain: epoch  8, batch    72 | loss: 66.3945445CurrentTrain: epoch  8, batch    73 | loss: 63.4057241CurrentTrain: epoch  8, batch    74 | loss: 84.1116334CurrentTrain: epoch  8, batch    75 | loss: 64.3470773CurrentTrain: epoch  8, batch    76 | loss: 43.3648839CurrentTrain: epoch  8, batch    77 | loss: 113.3099849CurrentTrain: epoch  8, batch    78 | loss: 49.9973527CurrentTrain: epoch  8, batch    79 | loss: 53.9636958CurrentTrain: epoch  8, batch    80 | loss: 81.4744681CurrentTrain: epoch  8, batch    81 | loss: 65.9147444CurrentTrain: epoch  8, batch    82 | loss: 54.5924031CurrentTrain: epoch  8, batch    83 | loss: 52.2635439CurrentTrain: epoch  8, batch    84 | loss: 115.2514190CurrentTrain: epoch  8, batch    85 | loss: 111.1183206CurrentTrain: epoch  8, batch    86 | loss: 84.9497754CurrentTrain: epoch  8, batch    87 | loss: 64.4704911CurrentTrain: epoch  8, batch    88 | loss: 39.1466246CurrentTrain: epoch  8, batch    89 | loss: 63.0154453CurrentTrain: epoch  8, batch    90 | loss: 63.0398519CurrentTrain: epoch  8, batch    91 | loss: 67.1012921CurrentTrain: epoch  8, batch    92 | loss: 64.2378367CurrentTrain: epoch  8, batch    93 | loss: 64.4700818CurrentTrain: epoch  8, batch    94 | loss: 82.4696602CurrentTrain: epoch  8, batch    95 | loss: 96.2576741CurrentTrain: epoch  9, batch     0 | loss: 53.1000996CurrentTrain: epoch  9, batch     1 | loss: 112.6509847CurrentTrain: epoch  9, batch     2 | loss: 62.2862827CurrentTrain: epoch  9, batch     3 | loss: 52.7031924CurrentTrain: epoch  9, batch     4 | loss: 64.8539862CurrentTrain: epoch  9, batch     5 | loss: 41.0942506CurrentTrain: epoch  9, batch     6 | loss: 82.5546046CurrentTrain: epoch  9, batch     7 | loss: 63.2171109CurrentTrain: epoch  9, batch     8 | loss: 82.5241404CurrentTrain: epoch  9, batch     9 | loss: 51.8487561CurrentTrain: epoch  9, batch    10 | loss: 87.2271252CurrentTrain: epoch  9, batch    11 | loss: 52.9810542CurrentTrain: epoch  9, batch    12 | loss: 82.9649921CurrentTrain: epoch  9, batch    13 | loss: 84.0803502CurrentTrain: epoch  9, batch    14 | loss: 82.7041851CurrentTrain: epoch  9, batch    15 | loss: 62.9284174CurrentTrain: epoch  9, batch    16 | loss: 117.5050895CurrentTrain: epoch  9, batch    17 | loss: 67.9368835CurrentTrain: epoch  9, batch    18 | loss: 63.5188444CurrentTrain: epoch  9, batch    19 | loss: 66.8552506CurrentTrain: epoch  9, batch    20 | loss: 50.1893281CurrentTrain: epoch  9, batch    21 | loss: 84.4621182CurrentTrain: epoch  9, batch    22 | loss: 52.9841001CurrentTrain: epoch  9, batch    23 | loss: 52.7399681CurrentTrain: epoch  9, batch    24 | loss: 115.1819231CurrentTrain: epoch  9, batch    25 | loss: 82.3566456CurrentTrain: epoch  9, batch    26 | loss: 48.3526237CurrentTrain: epoch  9, batch    27 | loss: 111.0215368CurrentTrain: epoch  9, batch    28 | loss: 51.3470146CurrentTrain: epoch  9, batch    29 | loss: 53.2543714CurrentTrain: epoch  9, batch    30 | loss: 51.2040242CurrentTrain: epoch  9, batch    31 | loss: 63.9901303CurrentTrain: epoch  9, batch    32 | loss: 84.9469928CurrentTrain: epoch  9, batch    33 | loss: 55.7707511CurrentTrain: epoch  9, batch    34 | loss: 45.7699438CurrentTrain: epoch  9, batch    35 | loss: 113.1844554CurrentTrain: epoch  9, batch    36 | loss: 63.2058582CurrentTrain: epoch  9, batch    37 | loss: 69.5625389CurrentTrain: epoch  9, batch    38 | loss: 64.0401492CurrentTrain: epoch  9, batch    39 | loss: 53.2858126CurrentTrain: epoch  9, batch    40 | loss: 49.4451977CurrentTrain: epoch  9, batch    41 | loss: 66.8576856CurrentTrain: epoch  9, batch    42 | loss: 49.8229200CurrentTrain: epoch  9, batch    43 | loss: 50.3980270CurrentTrain: epoch  9, batch    44 | loss: 113.3611874CurrentTrain: epoch  9, batch    45 | loss: 54.7868810CurrentTrain: epoch  9, batch    46 | loss: 65.4974395CurrentTrain: epoch  9, batch    47 | loss: 84.3043725CurrentTrain: epoch  9, batch    48 | loss: 63.6247286CurrentTrain: epoch  9, batch    49 | loss: 53.3353528CurrentTrain: epoch  9, batch    50 | loss: 62.6439992CurrentTrain: epoch  9, batch    51 | loss: 66.7714723CurrentTrain: epoch  9, batch    52 | loss: 54.5533914CurrentTrain: epoch  9, batch    53 | loss: 65.4955363CurrentTrain: epoch  9, batch    54 | loss: 52.3505940CurrentTrain: epoch  9, batch    55 | loss: 52.1602432CurrentTrain: epoch  9, batch    56 | loss: 65.4475521CurrentTrain: epoch  9, batch    57 | loss: 40.7283545CurrentTrain: epoch  9, batch    58 | loss: 79.8109992CurrentTrain: epoch  9, batch    59 | loss: 84.1645735CurrentTrain: epoch  9, batch    60 | loss: 61.0149101CurrentTrain: epoch  9, batch    61 | loss: 43.7267471CurrentTrain: epoch  9, batch    62 | loss: 82.8384094CurrentTrain: epoch  9, batch    63 | loss: 49.5245095CurrentTrain: epoch  9, batch    64 | loss: 43.2205768CurrentTrain: epoch  9, batch    65 | loss: 84.1429080CurrentTrain: epoch  9, batch    66 | loss: 83.7078757CurrentTrain: epoch  9, batch    67 | loss: 64.2032938CurrentTrain: epoch  9, batch    68 | loss: 117.5857986CurrentTrain: epoch  9, batch    69 | loss: 84.4231911CurrentTrain: epoch  9, batch    70 | loss: 64.4459969CurrentTrain: epoch  9, batch    71 | loss: 82.6782810CurrentTrain: epoch  9, batch    72 | loss: 51.0965661CurrentTrain: epoch  9, batch    73 | loss: 84.5759497CurrentTrain: epoch  9, batch    74 | loss: 117.4843682CurrentTrain: epoch  9, batch    75 | loss: 90.5783300CurrentTrain: epoch  9, batch    76 | loss: 82.3285379CurrentTrain: epoch  9, batch    77 | loss: 52.1383830CurrentTrain: epoch  9, batch    78 | loss: 112.0461382CurrentTrain: epoch  9, batch    79 | loss: 63.8568919CurrentTrain: epoch  9, batch    80 | loss: 43.8443796CurrentTrain: epoch  9, batch    81 | loss: 41.4833936CurrentTrain: epoch  9, batch    82 | loss: 85.8920950CurrentTrain: epoch  9, batch    83 | loss: 118.6115200CurrentTrain: epoch  9, batch    84 | loss: 117.8225954CurrentTrain: epoch  9, batch    85 | loss: 51.9965326CurrentTrain: epoch  9, batch    86 | loss: 62.2775895CurrentTrain: epoch  9, batch    87 | loss: 52.7669818CurrentTrain: epoch  9, batch    88 | loss: 117.4748202CurrentTrain: epoch  9, batch    89 | loss: 43.7767953CurrentTrain: epoch  9, batch    90 | loss: 64.2861242CurrentTrain: epoch  9, batch    91 | loss: 63.1407458CurrentTrain: epoch  9, batch    92 | loss: 40.6026973CurrentTrain: epoch  9, batch    93 | loss: 53.0793514CurrentTrain: epoch  9, batch    94 | loss: 79.6368156CurrentTrain: epoch  9, batch    95 | loss: 53.6810831

F1 score per class: {32: 0.6395348837209303, 6: 0.8700564971751412, 19: 0.2727272727272727, 24: 0.7724867724867724, 26: 0.9247311827956989, 29: 0.9015544041450777}
Micro-average F1 score: 0.8115015974440895
Weighted-average F1 score: 0.8224573279300403
F1 score per class: {32: 0.7076923076923077, 6: 0.907103825136612, 19: 0.42424242424242425, 24: 0.7634408602150538, 26: 0.9690721649484536, 29: 0.8829787234042553}
Micro-average F1 score: 0.8314606741573034
Weighted-average F1 score: 0.8315289549326857
F1 score per class: {32: 0.7150259067357513, 6: 0.907103825136612, 19: 0.4375, 24: 0.7634408602150538, 26: 0.9690721649484536, 29: 0.8888888888888888}
Micro-average F1 score: 0.8352098259979529
Weighted-average F1 score: 0.8361042165741264

F1 score per class: {32: 0.6395348837209303, 6: 0.8700564971751412, 19: 0.2727272727272727, 24: 0.7724867724867724, 26: 0.9247311827956989, 29: 0.9015544041450777}
Micro-average F1 score: 0.8115015974440895
Weighted-average F1 score: 0.8224573279300403
F1 score per class: {32: 0.7076923076923077, 6: 0.907103825136612, 19: 0.42424242424242425, 24: 0.7634408602150538, 26: 0.9690721649484536, 29: 0.8829787234042553}
Micro-average F1 score: 0.8314606741573034
Weighted-average F1 score: 0.8315289549326857
F1 score per class: {32: 0.7150259067357513, 6: 0.907103825136612, 19: 0.4375, 24: 0.7634408602150538, 26: 0.9690721649484536, 29: 0.8888888888888888}
Micro-average F1 score: 0.8352098259979529
Weighted-average F1 score: 0.8361042165741264

F1 score per class: {32: 0.4782608695652174, 6: 0.6754385964912281, 19: 0.17142857142857143, 24: 0.7156862745098039, 26: 0.8911917098445595, 29: 0.7981651376146789}
Micro-average F1 score: 0.6877256317689531
Weighted-average F1 score: 0.68311868968091
F1 score per class: {32: 0.45695364238410596, 6: 0.6775510204081633, 19: 0.16470588235294117, 24: 0.6729857819905213, 26: 0.9038461538461539, 29: 0.7579908675799086}
Micro-average F1 score: 0.6409448818897637
Weighted-average F1 score: 0.6171576429478215
F1 score per class: {32: 0.46464646464646464, 6: 0.6775510204081633, 19: 0.1728395061728395, 24: 0.6729857819905213, 26: 0.9038461538461539, 29: 0.7601809954751131}
Micro-average F1 score: 0.6460807600950119
Weighted-average F1 score: 0.6241179157748287

F1 score per class: {32: 0.4782608695652174, 6: 0.6754385964912281, 19: 0.17142857142857143, 24: 0.7156862745098039, 26: 0.8911917098445595, 29: 0.7981651376146789}
Micro-average F1 score: 0.6877256317689531
Weighted-average F1 score: 0.68311868968091
F1 score per class: {32: 0.45695364238410596, 6: 0.6775510204081633, 19: 0.16470588235294117, 24: 0.6729857819905213, 26: 0.9038461538461539, 29: 0.7579908675799086}
Micro-average F1 score: 0.6409448818897637
Weighted-average F1 score: 0.6171576429478215
F1 score per class: {32: 0.46464646464646464, 6: 0.6775510204081633, 19: 0.1728395061728395, 24: 0.6729857819905213, 26: 0.9038461538461539, 29: 0.7601809954751131}
Micro-average F1 score: 0.6460807600950119
Weighted-average F1 score: 0.6241179157748287
cur_acc_wo_na:  ['0.8115']
his_acc_wo_na:  ['0.8115']
cur_acc des_wo_na:  ['0.8315']
his_acc des_wo_na:  ['0.8315']
cur_acc rrf_wo_na:  ['0.8352']
his_acc rrf_wo_na:  ['0.8352']
cur_acc_w_na:  ['0.6877']
his_acc_w_na:  ['0.6877']
cur_acc des_w_na:  ['0.6409']
his_acc des_w_na:  ['0.6409']
cur_acc rrf_w_na:  ['0.6461']
his_acc rrf_w_na:  ['0.6461']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 97.5765320CurrentTrain: epoch  0, batch     1 | loss: 79.7600295CurrentTrain: epoch  0, batch     2 | loss: 96.1482445CurrentTrain: epoch  0, batch     3 | loss: 71.4250975CurrentTrain: epoch  0, batch     4 | loss: 23.8036930CurrentTrain: epoch  1, batch     0 | loss: 60.9187396CurrentTrain: epoch  1, batch     1 | loss: 124.6477720CurrentTrain: epoch  1, batch     2 | loss: 93.5219349CurrentTrain: epoch  1, batch     3 | loss: 90.8955700CurrentTrain: epoch  1, batch     4 | loss: 20.2030712CurrentTrain: epoch  2, batch     0 | loss: 92.8308702CurrentTrain: epoch  2, batch     1 | loss: 71.1319069CurrentTrain: epoch  2, batch     2 | loss: 92.0398094CurrentTrain: epoch  2, batch     3 | loss: 71.9403788CurrentTrain: epoch  2, batch     4 | loss: 29.8405568CurrentTrain: epoch  3, batch     0 | loss: 117.7065465CurrentTrain: epoch  3, batch     1 | loss: 59.8801385CurrentTrain: epoch  3, batch     2 | loss: 72.4734076CurrentTrain: epoch  3, batch     3 | loss: 70.6000622CurrentTrain: epoch  3, batch     4 | loss: 17.1476217CurrentTrain: epoch  4, batch     0 | loss: 68.6142981CurrentTrain: epoch  4, batch     1 | loss: 70.3428249CurrentTrain: epoch  4, batch     2 | loss: 68.9516695CurrentTrain: epoch  4, batch     3 | loss: 90.3880378CurrentTrain: epoch  4, batch     4 | loss: 28.8382811CurrentTrain: epoch  5, batch     0 | loss: 70.3925814CurrentTrain: epoch  5, batch     1 | loss: 88.8308194CurrentTrain: epoch  5, batch     2 | loss: 56.3616752CurrentTrain: epoch  5, batch     3 | loss: 86.0397493CurrentTrain: epoch  5, batch     4 | loss: 10.5502205CurrentTrain: epoch  6, batch     0 | loss: 68.0764822CurrentTrain: epoch  6, batch     1 | loss: 67.4470683CurrentTrain: epoch  6, batch     2 | loss: 65.4281882CurrentTrain: epoch  6, batch     3 | loss: 120.9519725CurrentTrain: epoch  6, batch     4 | loss: 14.9287938CurrentTrain: epoch  7, batch     0 | loss: 65.7951265CurrentTrain: epoch  7, batch     1 | loss: 86.4640185CurrentTrain: epoch  7, batch     2 | loss: 66.5550572CurrentTrain: epoch  7, batch     3 | loss: 81.6735272CurrentTrain: epoch  7, batch     4 | loss: 26.7169432CurrentTrain: epoch  8, batch     0 | loss: 113.9363533CurrentTrain: epoch  8, batch     1 | loss: 80.1532662CurrentTrain: epoch  8, batch     2 | loss: 85.9799639CurrentTrain: epoch  8, batch     3 | loss: 66.7390225CurrentTrain: epoch  8, batch     4 | loss: 15.2173331CurrentTrain: epoch  9, batch     0 | loss: 82.6448269CurrentTrain: epoch  9, batch     1 | loss: 53.3644661CurrentTrain: epoch  9, batch     2 | loss: 67.6264073CurrentTrain: epoch  9, batch     3 | loss: 115.6064543CurrentTrain: epoch  9, batch     4 | loss: 12.9388905
MemoryTrain:  epoch  0, batch     0 | loss: 0.7504173MemoryTrain:  epoch  1, batch     0 | loss: 0.5347374MemoryTrain:  epoch  2, batch     0 | loss: 0.4272743MemoryTrain:  epoch  3, batch     0 | loss: 0.3811382MemoryTrain:  epoch  4, batch     0 | loss: 0.2860259MemoryTrain:  epoch  5, batch     0 | loss: 0.2446886MemoryTrain:  epoch  6, batch     0 | loss: 0.1497171MemoryTrain:  epoch  7, batch     0 | loss: 0.1249127MemoryTrain:  epoch  8, batch     0 | loss: 0.1097394MemoryTrain:  epoch  9, batch     0 | loss: 0.0958026

F1 score per class: {2: 0.7142857142857143, 39: 0.33962264150943394, 11: 0.48484848484848486, 12: 0.0, 19: 0.36363636363636365, 28: 0.56}
Micro-average F1 score: 0.4413793103448276
Weighted-average F1 score: 0.4570520665444928
F1 score per class: {2: 0.9411764705882353, 39: 0.7101449275362319, 11: 0.75, 12: 0.0, 19: 0.0, 24: 0.42857142857142855, 28: 0.6153846153846154}
Micro-average F1 score: 0.7111111111111111
Weighted-average F1 score: 0.6949837028195847
F1 score per class: {2: 0.9411764705882353, 39: 0.7101449275362319, 11: 0.7577639751552795, 12: 0.0, 19: 0.0, 24: 0.42857142857142855, 28: 0.6153846153846154}
Micro-average F1 score: 0.7166666666666667
Weighted-average F1 score: 0.703661734624351

F1 score per class: {32: 0.7142857142857143, 2: 0.6666666666666666, 6: 0.33962264150943394, 39: 0.48120300751879697, 11: 0.8603351955307262, 12: 0.4166666666666667, 19: 0.7619047619047619, 24: 0.2857142857142857, 26: 0.9473684210526315, 28: 0.9183673469387755, 29: 0.4666666666666667}
Micro-average F1 score: 0.7300319488817891
Weighted-average F1 score: 0.772390385511291
F1 score per class: {32: 0.9411764705882353, 2: 0.7525773195876289, 6: 0.6901408450704225, 39: 0.7228915662650602, 11: 0.91005291005291, 12: 0.4666666666666667, 19: 0.7741935483870968, 24: 0.3157894736842105, 26: 0.9583333333333334, 28: 0.9238578680203046, 29: 0.5}
Micro-average F1 score: 0.8049853372434017
Weighted-average F1 score: 0.8081021483961748
F1 score per class: {32: 0.9411764705882353, 2: 0.6956521739130435, 6: 0.6901408450704225, 39: 0.7261904761904762, 11: 0.8983957219251337, 12: 0.42857142857142855, 19: 0.7741935483870968, 24: 0.2608695652173913, 26: 0.9583333333333334, 28: 0.9183673469387755, 29: 0.42105263157894735}
Micro-average F1 score: 0.7891256429096253
Weighted-average F1 score: 0.7884419523262702

F1 score per class: {32: 0.35714285714285715, 2: 0.0, 6: 0.3050847457627119, 39: 0.42953020134228187, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.15384615384615385, 26: 0.0, 28: 0.0, 29: 0.24561403508771928}
Micro-average F1 score: 0.29157175398633256
Weighted-average F1 score: 0.22870850364713705
F1 score per class: {32: 0.22535211267605634, 2: 0.0, 6: 0.4666666666666667, 39: 0.5194805194805194, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.10909090909090909, 26: 0.0, 28: 0.0, 29: 0.2}
Micro-average F1 score: 0.3413333333333333
Weighted-average F1 score: 0.2962041150630313
F1 score per class: {32: 0.23880597014925373, 2: 0.0, 6: 0.48756218905472637, 39: 0.5147679324894515, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.10909090909090909, 26: 0.0, 28: 0.0, 29: 0.18604651162790697}
Micro-average F1 score: 0.35390946502057613
Weighted-average F1 score: 0.3097864744066229

F1 score per class: {32: 0.29411764705882354, 2: 0.4354243542435424, 6: 0.2727272727272727, 39: 0.3282051282051282, 11: 0.652542372881356, 12: 0.1724137931034483, 19: 0.6857142857142857, 24: 0.10526315789473684, 26: 0.8910891089108911, 28: 0.7531380753138075, 29: 0.1414141414141414}
Micro-average F1 score: 0.5332555425904317
Weighted-average F1 score: 0.5187474484449862
F1 score per class: {32: 0.125, 2: 0.4319526627218935, 6: 0.37404580152671757, 39: 0.2857142857142857, 11: 0.6187050359712231, 12: 0.13333333333333333, 19: 0.6857142857142857, 24: 0.06593406593406594, 26: 0.8803827751196173, 28: 0.629757785467128, 29: 0.11764705882352941}
Micro-average F1 score: 0.44525547445255476
Weighted-average F1 score: 0.4084220676945075
F1 score per class: {32: 0.1322314049586777, 2: 0.42105263157894735, 6: 0.392, 39: 0.27601809954751133, 11: 0.6176470588235294, 12: 0.14814814814814814, 19: 0.6889952153110048, 24: 0.0625, 26: 0.8932038834951457, 28: 0.6976744186046512, 29: 0.0903954802259887}
Micro-average F1 score: 0.4445364238410596
Weighted-average F1 score: 0.40168110972570725
cur_acc_wo_na:  ['0.8115', '0.4414']
his_acc_wo_na:  ['0.8115', '0.7300']
cur_acc des_wo_na:  ['0.8315', '0.7111']
his_acc des_wo_na:  ['0.8315', '0.8050']
cur_acc rrf_wo_na:  ['0.8352', '0.7167']
his_acc rrf_wo_na:  ['0.8352', '0.7891']
cur_acc_w_na:  ['0.6877', '0.2916']
his_acc_w_na:  ['0.6877', '0.5333']
cur_acc des_w_na:  ['0.6409', '0.3413']
his_acc des_w_na:  ['0.6409', '0.4453']
cur_acc rrf_w_na:  ['0.6461', '0.3539']
his_acc rrf_w_na:  ['0.6461', '0.4445']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 107.9658762CurrentTrain: epoch  0, batch     1 | loss: 97.1598889CurrentTrain: epoch  0, batch     2 | loss: 63.6495394CurrentTrain: epoch  0, batch     3 | loss: 27.7103398CurrentTrain: epoch  1, batch     0 | loss: 78.2844526CurrentTrain: epoch  1, batch     1 | loss: 60.1044244CurrentTrain: epoch  1, batch     2 | loss: 59.9531596CurrentTrain: epoch  1, batch     3 | loss: 27.5673418CurrentTrain: epoch  2, batch     0 | loss: 88.7278977CurrentTrain: epoch  2, batch     1 | loss: 56.4303882CurrentTrain: epoch  2, batch     2 | loss: 88.8791213CurrentTrain: epoch  2, batch     3 | loss: 6.7324088CurrentTrain: epoch  3, batch     0 | loss: 55.1190938CurrentTrain: epoch  3, batch     1 | loss: 55.8995806CurrentTrain: epoch  3, batch     2 | loss: 67.7694236CurrentTrain: epoch  3, batch     3 | loss: 13.5358964CurrentTrain: epoch  4, batch     0 | loss: 51.9613705CurrentTrain: epoch  4, batch     1 | loss: 69.0010367CurrentTrain: epoch  4, batch     2 | loss: 57.2376914CurrentTrain: epoch  4, batch     3 | loss: 11.9065558CurrentTrain: epoch  5, batch     0 | loss: 114.0802653CurrentTrain: epoch  5, batch     1 | loss: 66.3746211CurrentTrain: epoch  5, batch     2 | loss: 50.0464092CurrentTrain: epoch  5, batch     3 | loss: 27.5289450CurrentTrain: epoch  6, batch     0 | loss: 116.6050552CurrentTrain: epoch  6, batch     1 | loss: 50.8133385CurrentTrain: epoch  6, batch     2 | loss: 54.7354793CurrentTrain: epoch  6, batch     3 | loss: 6.4703344CurrentTrain: epoch  7, batch     0 | loss: 51.5333047CurrentTrain: epoch  7, batch     1 | loss: 82.0232566CurrentTrain: epoch  7, batch     2 | loss: 64.1330389CurrentTrain: epoch  7, batch     3 | loss: 11.2803736CurrentTrain: epoch  8, batch     0 | loss: 51.7118790CurrentTrain: epoch  8, batch     1 | loss: 79.6588471CurrentTrain: epoch  8, batch     2 | loss: 63.1620756CurrentTrain: epoch  8, batch     3 | loss: 27.3722298CurrentTrain: epoch  9, batch     0 | loss: 61.8578968CurrentTrain: epoch  9, batch     1 | loss: 80.3477435CurrentTrain: epoch  9, batch     2 | loss: 62.1596877CurrentTrain: epoch  9, batch     3 | loss: 11.1317415
MemoryTrain:  epoch  0, batch     0 | loss: 0.5922801MemoryTrain:  epoch  1, batch     0 | loss: 0.4241464MemoryTrain:  epoch  2, batch     0 | loss: 0.3903985MemoryTrain:  epoch  3, batch     0 | loss: 0.3629623MemoryTrain:  epoch  4, batch     0 | loss: 0.2498742MemoryTrain:  epoch  5, batch     0 | loss: 0.2155397MemoryTrain:  epoch  6, batch     0 | loss: 0.1786426MemoryTrain:  epoch  7, batch     0 | loss: 0.1562318MemoryTrain:  epoch  8, batch     0 | loss: 0.1254337MemoryTrain:  epoch  9, batch     0 | loss: 0.1067950

F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9387755102040817, 9: 0.0, 19: 0.0, 26: 0.5263157894736842, 27: 0.0, 31: 0.4666666666666667}
Micro-average F1 score: 0.47619047619047616
Weighted-average F1 score: 0.36822636949516646
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6666666666666666, 27: 0.0, 28: 1.0, 31: 0.5773195876288659}
Micro-average F1 score: 0.5753424657534246
Weighted-average F1 score: 0.4658927488521182
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6666666666666666, 27: 0.0, 28: 1.0, 31: 0.5773195876288659}
Micro-average F1 score: 0.5753424657534246
Weighted-average F1 score: 0.4658927488521182

F1 score per class: {32: 0.7142857142857143, 2: 0.36496350364963503, 6: 0.047619047619047616, 7: 0.9387755102040817, 40: 0.20408163265306123, 39: 0.18181818181818182, 11: 0.6822429906542056, 12: 0.10526315789473684, 9: 0.7570621468926554, 19: 0.3225806451612903, 24: 0.4, 26: 0.918918918918919, 27: 0.0, 28: 0.8268156424581006, 29: 0.23529411764705882, 31: 0.4117647058823529}
Micro-average F1 score: 0.5829725829725829
Weighted-average F1 score: 0.6452363459422938
F1 score per class: {32: 0.8, 2: 0.42028985507246375, 6: 0.03636363636363636, 39: 0.9803921568627451, 7: 0.6470588235294118, 40: 0.7, 11: 0.7031963470319634, 12: 0.3076923076923077, 9: 0.7640449438202247, 19: 0.5, 24: 0.38095238095238093, 26: 0.9528795811518325, 27: 0.8, 28: 0.8723404255319149, 29: 0.46153846153846156, 31: 0.45528455284552843}
Micro-average F1 score: 0.6794871794871795
Weighted-average F1 score: 0.6696822642696859
F1 score per class: {32: 0.8, 2: 0.46153846153846156, 6: 0.0392156862745098, 39: 0.9803921568627451, 7: 0.6201550387596899, 40: 0.6875, 11: 0.7069767441860465, 12: 0.24, 9: 0.7640449438202247, 19: 0.45161290322580644, 24: 0.4, 26: 0.9528795811518325, 27: 1.0, 28: 0.8723404255319149, 29: 0.46153846153846156, 31: 0.4375}
Micro-average F1 score: 0.6778135048231512
Weighted-average F1 score: 0.6681953980866782

F1 score per class: {32: 0.0, 2: 0.0, 6: 0.3333333333333333, 7: 0.92, 40: 0.0, 9: 0.0, 19: 0.4166666666666667, 26: 0.0, 27: 0.0, 31: 0.302158273381295}
Micro-average F1 score: 0.352112676056338
Weighted-average F1 score: 0.2849994357455213
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.3333333333333333, 7: 0.819672131147541, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 39: 0.4375, 19: 0.0, 26: 0.0, 27: 0.2, 28: 0.0, 29: 0.0, 31: 0.27860696517412936}
Micro-average F1 score: 0.3073170731707317
Weighted-average F1 score: 0.263476636120461
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.3333333333333333, 7: 0.819672131147541, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 39: 0.4375, 19: 0.0, 26: 0.0, 27: 0.2, 28: 0.0, 29: 0.0, 31: 0.2828282828282828}
Micro-average F1 score: 0.314214463840399
Weighted-average F1 score: 0.270738796833359

F1 score per class: {32: 0.29411764705882354, 2: 0.2717391304347826, 6: 0.03076923076923077, 7: 0.92, 40: 0.17391304347826086, 39: 0.14925373134328357, 11: 0.5195729537366548, 12: 0.07407407407407407, 9: 0.6907216494845361, 19: 0.14492753623188406, 24: 0.23529411764705882, 26: 0.8900523560209425, 27: 0.0, 28: 0.7551020408163265, 29: 0.10256410256410256, 31: 0.23076923076923078}
Micro-average F1 score: 0.45164896590273895
Weighted-average F1 score: 0.4436061957773512
F1 score per class: {32: 0.12244897959183673, 2: 0.26851851851851855, 6: 0.021505376344086023, 39: 0.7936507936507936, 7: 0.37606837606837606, 40: 0.3154929577464789, 11: 0.49517684887459806, 12: 0.17391304347826086, 9: 0.6903553299492385, 19: 0.15730337078651685, 24: 0.09876543209876543, 26: 0.875, 27: 0.05263157894736842, 28: 0.6861924686192469, 29: 0.15789473684210525, 31: 0.15053763440860216}
Micro-average F1 score: 0.3848946986201888
Weighted-average F1 score: 0.3433894017494033
F1 score per class: {32: 0.13186813186813187, 2: 0.2972972972972973, 6: 0.024390243902439025, 39: 0.819672131147541, 7: 0.3686635944700461, 40: 0.32448377581120946, 11: 0.5135135135135135, 12: 0.16216216216216217, 9: 0.6868686868686869, 19: 0.14285714285714285, 24: 0.12307692307692308, 26: 0.8921568627450981, 27: 0.056338028169014086, 28: 0.7008547008547008, 29: 0.14285714285714285, 31: 0.14814814814814814}
Micro-average F1 score: 0.3937243182667165
Weighted-average F1 score: 0.350203971999088
cur_acc_wo_na:  ['0.8115', '0.4414', '0.4762']
his_acc_wo_na:  ['0.8115', '0.7300', '0.5830']
cur_acc des_wo_na:  ['0.8315', '0.7111', '0.5753']
his_acc des_wo_na:  ['0.8315', '0.8050', '0.6795']
cur_acc rrf_wo_na:  ['0.8352', '0.7167', '0.5753']
his_acc rrf_wo_na:  ['0.8352', '0.7891', '0.6778']
cur_acc_w_na:  ['0.6877', '0.2916', '0.3521']
his_acc_w_na:  ['0.6877', '0.5333', '0.4516']
cur_acc des_w_na:  ['0.6409', '0.3413', '0.3073']
his_acc des_w_na:  ['0.6409', '0.4453', '0.3849']
cur_acc rrf_w_na:  ['0.6461', '0.3539', '0.3142']
his_acc rrf_w_na:  ['0.6461', '0.4445', '0.3937']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 65.4015664CurrentTrain: epoch  0, batch     1 | loss: 131.4973661CurrentTrain: epoch  0, batch     2 | loss: 82.1007045CurrentTrain: epoch  0, batch     3 | loss: 94.5471587CurrentTrain: epoch  1, batch     0 | loss: 80.5570805CurrentTrain: epoch  1, batch     1 | loss: 59.5603648CurrentTrain: epoch  1, batch     2 | loss: 63.9608449CurrentTrain: epoch  1, batch     3 | loss: 46.2508410CurrentTrain: epoch  2, batch     0 | loss: 59.8307887CurrentTrain: epoch  2, batch     1 | loss: 117.8088045CurrentTrain: epoch  2, batch     2 | loss: 59.3007371CurrentTrain: epoch  2, batch     3 | loss: 57.3323296CurrentTrain: epoch  3, batch     0 | loss: 57.9590191CurrentTrain: epoch  3, batch     1 | loss: 68.6537237CurrentTrain: epoch  3, batch     2 | loss: 88.7552113CurrentTrain: epoch  3, batch     3 | loss: 123.3707174CurrentTrain: epoch  4, batch     0 | loss: 55.5413044CurrentTrain: epoch  4, batch     1 | loss: 85.8565651CurrentTrain: epoch  4, batch     2 | loss: 67.0170893CurrentTrain: epoch  4, batch     3 | loss: 62.0994394CurrentTrain: epoch  5, batch     0 | loss: 68.1238992CurrentTrain: epoch  5, batch     1 | loss: 55.4138557CurrentTrain: epoch  5, batch     2 | loss: 55.1190610CurrentTrain: epoch  5, batch     3 | loss: 79.2798687CurrentTrain: epoch  6, batch     0 | loss: 83.4170272CurrentTrain: epoch  6, batch     1 | loss: 66.2483401CurrentTrain: epoch  6, batch     2 | loss: 67.8635194CurrentTrain: epoch  6, batch     3 | loss: 44.9729483CurrentTrain: epoch  7, batch     0 | loss: 64.9031400CurrentTrain: epoch  7, batch     1 | loss: 83.1037572CurrentTrain: epoch  7, batch     2 | loss: 117.2273344CurrentTrain: epoch  7, batch     3 | loss: 42.3829256CurrentTrain: epoch  8, batch     0 | loss: 85.9782843CurrentTrain: epoch  8, batch     1 | loss: 64.6237854CurrentTrain: epoch  8, batch     2 | loss: 82.8550815CurrentTrain: epoch  8, batch     3 | loss: 34.2035133CurrentTrain: epoch  9, batch     0 | loss: 115.1148991CurrentTrain: epoch  9, batch     1 | loss: 78.4656261CurrentTrain: epoch  9, batch     2 | loss: 65.3793587CurrentTrain: epoch  9, batch     3 | loss: 55.7625001
MemoryTrain:  epoch  0, batch     0 | loss: 0.5200193MemoryTrain:  epoch  1, batch     0 | loss: 0.4452777MemoryTrain:  epoch  2, batch     0 | loss: 0.3772971MemoryTrain:  epoch  3, batch     0 | loss: 0.2942663MemoryTrain:  epoch  4, batch     0 | loss: 0.2247474MemoryTrain:  epoch  5, batch     0 | loss: 0.1922355MemoryTrain:  epoch  6, batch     0 | loss: 0.1463261MemoryTrain:  epoch  7, batch     0 | loss: 0.1261110MemoryTrain:  epoch  8, batch     0 | loss: 0.1115761MemoryTrain:  epoch  9, batch     0 | loss: 0.0918612

F1 score per class: {32: 0.0, 35: 0.8888888888888888, 37: 0.42424242424242425, 38: 0.0, 11: 0.0, 15: 0.0, 25: 0.4307692307692308, 26: 0.5, 27: 0.723404255319149}
Micro-average F1 score: 0.512280701754386
Weighted-average F1 score: 0.5098327991945013
F1 score per class: {32: 0.0, 35: 0.0, 37: 0.8235294117647058, 6: 0.0, 38: 0.8695652173913043, 11: 0.0, 15: 0.0, 24: 0.0, 25: 0.0, 26: 0.9607843137254902, 27: 0.75, 28: 0.88}
Micro-average F1 score: 0.8392370572207084
Weighted-average F1 score: 0.8187187625181032
F1 score per class: {32: 0.0, 35: 0.8888888888888888, 37: 0.8444444444444444, 38: 0.0, 11: 0.0, 15: 0.0, 25: 0.0, 26: 0.8444444444444444, 27: 0.75, 28: 0.8679245283018868}
Micro-average F1 score: 0.7944444444444444
Weighted-average F1 score: 0.7599462046596257

F1 score per class: {2: 0.7142857142857143, 6: 0.5033112582781457, 7: 0.047619047619047616, 9: 0.96, 11: 0.10526315789473684, 12: 0.18181818181818182, 15: 0.8421052631578947, 19: 0.6051282051282051, 24: 0.10526315789473684, 25: 0.42424242424242425, 26: 0.7666666666666667, 27: 0.3333333333333333, 28: 0.6666666666666666, 29: 0.9417989417989417, 31: 0.0, 32: 0.8497409326424871, 35: 0.42424242424242425, 37: 0.45977011494252873, 38: 0.6071428571428571, 39: 0.0, 40: 0.3617021276595745}
Micro-average F1 score: 0.5724465558194775
Weighted-average F1 score: 0.6414594610610922
F1 score per class: {2: 0.8, 6: 0.5988023952095808, 7: 0.08333333333333333, 9: 0.9803921568627451, 11: 0.35398230088495575, 12: 0.7515151515151515, 15: 0.7, 19: 0.6796116504854369, 24: 0.3076923076923077, 25: 0.8695652173913043, 26: 0.7597765363128491, 27: 0.5161290322580645, 28: 0.5, 29: 0.9637305699481865, 31: 0.5714285714285714, 32: 0.8934010152284264, 35: 0.9074074074074074, 37: 0.5070422535211268, 38: 0.7333333333333333, 39: 0.0, 40: 0.49586776859504134}
Micro-average F1 score: 0.6956962025316455
Weighted-average F1 score: 0.6982642696806066
F1 score per class: {2: 0.8, 6: 0.6024096385542169, 7: 0.0851063829787234, 9: 0.9803921568627451, 11: 0.29906542056074764, 12: 0.7116564417177914, 15: 0.7272727272727273, 19: 0.6862745098039216, 24: 0.2608695652173913, 25: 0.8444444444444444, 26: 0.7597765363128491, 27: 0.47058823529411764, 28: 0.5, 29: 0.9583333333333334, 31: 0.8, 32: 0.8669950738916257, 35: 0.8085106382978723, 37: 0.4931506849315068, 38: 0.7301587301587301, 39: 0.0, 40: 0.45528455284552843}
Micro-average F1 score: 0.6772055073941866
Weighted-average F1 score: 0.6822859695625503

F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.8888888888888888, 19: 0.0, 24: 0.0, 25: 0.42424242424242425, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.3373493975903614, 37: 0.40816326530612246, 38: 0.425, 40: 0.0}
Micro-average F1 score: 0.32808988764044944
Weighted-average F1 score: 0.2518684010452019
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.6086956521739131, 19: 0.0, 24: 0.0, 25: 0.8333333333333334, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5868263473053892, 37: 0.42105263157894735, 38: 0.4943820224719101, 40: 0.0}
Micro-average F1 score: 0.3802469135802469
Weighted-average F1 score: 0.31124090625353423
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.64, 19: 0.0, 24: 0.0, 25: 0.8085106382978723, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5547445255474452, 37: 0.41379310344827586, 38: 0.46, 40: 0.0}
Micro-average F1 score: 0.36666666666666664
Weighted-average F1 score: 0.29630032780430804

F1 score per class: {2: 0.3125, 6: 0.30522088353413657, 7: 0.0273972602739726, 9: 0.8421052631578947, 11: 0.08130081300813008, 12: 0.1388888888888889, 15: 0.5714285714285714, 19: 0.44696969696969696, 24: 0.08695652173913043, 25: 0.42424242424242425, 26: 0.6831683168316832, 27: 0.12345679012345678, 28: 0.21621621621621623, 29: 0.8811881188118812, 31: 0.0, 32: 0.7387387387387387, 35: 0.23728813559322035, 37: 0.272108843537415, 38: 0.1559633027522936, 39: 0.0, 40: 0.24285714285714285}
Micro-average F1 score: 0.3921887713588283
Weighted-average F1 score: 0.37738234711801355
F1 score per class: {2: 0.13636363636363635, 6: 0.30959752321981426, 7: 0.043010752688172046, 9: 0.7352941176470589, 11: 0.21505376344086022, 12: 0.2556701030927835, 15: 0.2222222222222222, 19: 0.4268292682926829, 24: 0.13793103448275862, 25: 0.8333333333333334, 26: 0.6538461538461539, 27: 0.1523809523809524, 28: 0.0970873786407767, 29: 0.8303571428571429, 31: 0.05970149253731343, 32: 0.6821705426356589, 35: 0.2558746736292428, 37: 0.14784394250513347, 38: 0.14814814814814814, 39: 0.0, 40: 0.22727272727272727}
Micro-average F1 score: 0.32722076684924983
Weighted-average F1 score: 0.29432154902883767
F1 score per class: {2: 0.13793103448275862, 6: 0.31446540880503143, 7: 0.04597701149425287, 9: 0.746268656716418, 11: 0.2, 12: 0.2600896860986547, 15: 0.24242424242424243, 19: 0.45751633986928103, 24: 0.15384615384615385, 25: 0.8085106382978723, 26: 0.6507177033492823, 27: 0.13333333333333333, 28: 0.11494252873563218, 29: 0.8518518518518519, 31: 0.06779661016949153, 32: 0.654275092936803, 35: 0.2783882783882784, 37: 0.14007782101167315, 38: 0.13649851632047477, 39: 0.0, 40: 0.2074074074074074}
Micro-average F1 score: 0.328875681030213
Weighted-average F1 score: 0.2947359395441283
cur_acc_wo_na:  ['0.8115', '0.4414', '0.4762', '0.5123']
his_acc_wo_na:  ['0.8115', '0.7300', '0.5830', '0.5724']
cur_acc des_wo_na:  ['0.8315', '0.7111', '0.5753', '0.8392']
his_acc des_wo_na:  ['0.8315', '0.8050', '0.6795', '0.6957']
cur_acc rrf_wo_na:  ['0.8352', '0.7167', '0.5753', '0.7944']
his_acc rrf_wo_na:  ['0.8352', '0.7891', '0.6778', '0.6772']
cur_acc_w_na:  ['0.6877', '0.2916', '0.3521', '0.3281']
his_acc_w_na:  ['0.6877', '0.5333', '0.4516', '0.3922']
cur_acc des_w_na:  ['0.6409', '0.3413', '0.3073', '0.3802']
his_acc des_w_na:  ['0.6409', '0.4453', '0.3849', '0.3272']
cur_acc rrf_w_na:  ['0.6461', '0.3539', '0.3142', '0.3667']
his_acc rrf_w_na:  ['0.6461', '0.4445', '0.3937', '0.3289']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 85.5630690CurrentTrain: epoch  0, batch     1 | loss: 134.2744008CurrentTrain: epoch  0, batch     2 | loss: 83.5143893CurrentTrain: epoch  0, batch     3 | loss: 67.6344037CurrentTrain: epoch  0, batch     4 | loss: 214.9893070CurrentTrain: epoch  1, batch     0 | loss: 89.1167712CurrentTrain: epoch  1, batch     1 | loss: 72.8165194CurrentTrain: epoch  1, batch     2 | loss: 96.7879952CurrentTrain: epoch  1, batch     3 | loss: 72.2647047CurrentTrain: epoch  1, batch     4 | loss: 109.2592482CurrentTrain: epoch  2, batch     0 | loss: 124.4150136CurrentTrain: epoch  2, batch     1 | loss: 75.3803626CurrentTrain: epoch  2, batch     2 | loss: 88.8355660CurrentTrain: epoch  2, batch     3 | loss: 86.8033933CurrentTrain: epoch  2, batch     4 | loss: 69.0221779CurrentTrain: epoch  3, batch     0 | loss: 57.5074668CurrentTrain: epoch  3, batch     1 | loss: 68.8218241CurrentTrain: epoch  3, batch     2 | loss: 120.5465557CurrentTrain: epoch  3, batch     3 | loss: 87.3198708CurrentTrain: epoch  3, batch     4 | loss: 104.4823039CurrentTrain: epoch  4, batch     0 | loss: 116.4588598CurrentTrain: epoch  4, batch     1 | loss: 68.2728721CurrentTrain: epoch  4, batch     2 | loss: 69.0120433CurrentTrain: epoch  4, batch     3 | loss: 85.9034626CurrentTrain: epoch  4, batch     4 | loss: 65.5650067CurrentTrain: epoch  5, batch     0 | loss: 66.7300612CurrentTrain: epoch  5, batch     1 | loss: 84.7639512CurrentTrain: epoch  5, batch     2 | loss: 54.4589547CurrentTrain: epoch  5, batch     3 | loss: 89.3890404CurrentTrain: epoch  5, batch     4 | loss: 104.1706401CurrentTrain: epoch  6, batch     0 | loss: 66.3113560CurrentTrain: epoch  6, batch     1 | loss: 68.2577953CurrentTrain: epoch  6, batch     2 | loss: 53.7707882CurrentTrain: epoch  6, batch     3 | loss: 67.3836122CurrentTrain: epoch  6, batch     4 | loss: 99.6735392CurrentTrain: epoch  7, batch     0 | loss: 50.6625917CurrentTrain: epoch  7, batch     1 | loss: 85.6182801CurrentTrain: epoch  7, batch     2 | loss: 66.2066182CurrentTrain: epoch  7, batch     3 | loss: 86.9596589CurrentTrain: epoch  7, batch     4 | loss: 213.9476933CurrentTrain: epoch  8, batch     0 | loss: 64.3767143CurrentTrain: epoch  8, batch     1 | loss: 86.9459572CurrentTrain: epoch  8, batch     2 | loss: 66.2245323CurrentTrain: epoch  8, batch     3 | loss: 65.7823818CurrentTrain: epoch  8, batch     4 | loss: 63.8824145CurrentTrain: epoch  9, batch     0 | loss: 118.4847198CurrentTrain: epoch  9, batch     1 | loss: 51.7997321CurrentTrain: epoch  9, batch     2 | loss: 83.1865620CurrentTrain: epoch  9, batch     3 | loss: 82.7068214CurrentTrain: epoch  9, batch     4 | loss: 63.5838478
MemoryTrain:  epoch  0, batch     0 | loss: 0.5570247MemoryTrain:  epoch  1, batch     0 | loss: 0.4750261MemoryTrain:  epoch  2, batch     0 | loss: 0.3583838MemoryTrain:  epoch  3, batch     0 | loss: 0.2864295MemoryTrain:  epoch  4, batch     0 | loss: 0.2638275MemoryTrain:  epoch  5, batch     0 | loss: 0.2200155MemoryTrain:  epoch  6, batch     0 | loss: 0.1673896MemoryTrain:  epoch  7, batch     0 | loss: 0.1276382MemoryTrain:  epoch  8, batch     0 | loss: 0.1118083MemoryTrain:  epoch  9, batch     0 | loss: 0.0917090

F1 score per class: {1: 0.46464646464646464, 34: 0.17777777777777778, 35: 0.0, 3: 0.17475728155339806, 37: 0.5789473684210527, 38: 0.0, 11: 0.0, 14: 0.0, 22: 0.3333333333333333, 24: 0.0, 26: 0.0, 27: 0.0}
Micro-average F1 score: 0.3187919463087248
Weighted-average F1 score: 0.2684377713281759
F1 score per class: {32: 0.46808510638297873, 1: 0.3434343434343434, 34: 0.0, 3: 0.1188118811881188, 35: 0.650887573964497, 37: 0.0, 38: 0.0, 11: 0.0, 14: 0.0, 22: 0.6419753086419753, 24: 0.0, 26: 0.0, 27: 0.0}
Micro-average F1 score: 0.37668161434977576
Weighted-average F1 score: 0.2928800710187456
F1 score per class: {32: 0.4631578947368421, 1: 0.2916666666666667, 34: 0.0, 3: 0.1188118811881188, 35: 0.6428571428571429, 37: 0.0, 38: 0.0, 11: 0.0, 14: 0.0, 22: 0.6585365853658537, 24: 0.0, 26: 0.0, 27: 0.0}
Micro-average F1 score: 0.3699248120300752
Weighted-average F1 score: 0.2914062821441388

F1 score per class: {1: 0.4144144144144144, 2: 0.6153846153846154, 3: 0.17391304347826086, 6: 0.3582089552238806, 7: 0.0, 9: 0.9803921568627451, 11: 0.022222222222222223, 12: 0.18181818181818182, 14: 0.15, 15: 0.8235294117647058, 19: 0.31343283582089554, 22: 0.5569620253164557, 24: 0.08, 25: 0.42424242424242425, 26: 0.7582417582417582, 27: 0.125, 28: 0.4, 29: 0.93048128342246, 31: 0.0, 32: 0.47761194029850745, 34: 0.20754716981132076, 35: 0.14736842105263157, 37: 0.32941176470588235, 38: 0.4444444444444444, 39: 0.0, 40: 0.3218390804597701}
Micro-average F1 score: 0.4072188801480796
Weighted-average F1 score: 0.44119973005778623
F1 score per class: {1: 0.4074074074074074, 2: 0.8, 3: 0.3300970873786408, 6: 0.5222929936305732, 7: 0.0425531914893617, 9: 0.9803921568627451, 11: 0.08247422680412371, 12: 0.7169811320754716, 14: 0.11214953271028037, 15: 0.7058823529411765, 19: 0.37142857142857144, 22: 0.6214689265536724, 24: 0.07407407407407407, 25: 0.8314606741573034, 26: 0.7540983606557377, 27: 0.12244897959183673, 28: 0.2857142857142857, 29: 0.9637305699481865, 31: 0.0, 32: 0.7134502923976608, 34: 0.33121019108280253, 35: 0.5679012345679012, 37: 0.46956521739130436, 38: 0.6666666666666666, 39: 0.0, 40: 0.5384615384615384}
Micro-average F1 score: 0.5391578118850846
Weighted-average F1 score: 0.5457294175217975
F1 score per class: {1: 0.4, 2: 0.8, 3: 0.26666666666666666, 6: 0.5283018867924528, 7: 0.044444444444444446, 9: 0.9803921568627451, 11: 0.021505376344086023, 12: 0.5714285714285714, 14: 0.10434782608695652, 15: 0.7058823529411765, 19: 0.4, 22: 0.6136363636363636, 24: 0.07407407407407407, 25: 0.7619047619047619, 26: 0.7608695652173914, 27: 0.12244897959183673, 28: 0.3076923076923077, 29: 0.9473684210526315, 31: 0.0, 32: 0.6052631578947368, 34: 0.32926829268292684, 35: 0.4563758389261745, 37: 0.4778761061946903, 38: 0.6333333333333333, 39: 0.0, 40: 0.5084745762711864}
Micro-average F1 score: 0.5038229376257545
Weighted-average F1 score: 0.5083856932500164

F1 score per class: {1: 0.18930041152263374, 2: 0.0, 3: 0.17582417582417584, 6: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.0967741935483871, 19: 0.0, 22: 0.411214953271028, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.27848101265822783, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.1694915254237288
Weighted-average F1 score: 0.13472396973279738
F1 score per class: {1: 0.18181818181818182, 2: 0.0, 3: 0.3008849557522124, 6: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.05970149253731343, 19: 0.0, 22: 0.44715447154471544, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.39097744360902253, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.15584415584415584
Weighted-average F1 score: 0.11727373197648146
F1 score per class: {1: 0.17670682730923695, 2: 0.0, 3: 0.24778761061946902, 6: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.0547945205479452, 15: 0.0, 19: 0.0, 22: 0.4462809917355372, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.39416058394160586, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.15974025974025974
Weighted-average F1 score: 0.12381129821493941

F1 score per class: {1: 0.12332439678284182, 2: 0.2962962962962963, 3: 0.14814814814814814, 6: 0.22429906542056074, 7: 0.0, 9: 0.8620689655172413, 11: 0.019230769230769232, 12: 0.14492753623188406, 14: 0.05901639344262295, 15: 0.56, 19: 0.25925925925925924, 22: 0.3464566929133858, 24: 0.05128205128205128, 25: 0.42424242424242425, 26: 0.6731707317073171, 27: 0.04580152671755725, 28: 0.2, 29: 0.8246445497630331, 31: 0.0, 32: 0.39263803680981596, 34: 0.11891891891891893, 35: 0.08641975308641975, 37: 0.2616822429906542, 38: 0.11560693641618497, 39: 0.0, 40: 0.25688073394495414}
Micro-average F1 score: 0.25551684088269455
Weighted-average F1 score: 0.2280728832799939
F1 score per class: {1: 0.11369509043927649, 2: 0.1935483870967742, 3: 0.20481927710843373, 6: 0.27796610169491526, 7: 0.022988505747126436, 9: 0.6493506493506493, 11: 0.06201550387596899, 12: 0.29081632653061223, 14: 0.03715170278637771, 15: 0.34285714285714286, 19: 0.2524271844660194, 22: 0.3536977491961415, 24: 0.05263157894736842, 25: 0.7872340425531915, 26: 0.6272727272727273, 27: 0.0425531914893617, 28: 0.043010752688172046, 29: 0.7654320987654321, 31: 0.0, 32: 0.48031496062992124, 34: 0.12121212121212122, 35: 0.18585858585858586, 37: 0.16071428571428573, 38: 0.14022140221402213, 39: 0.0, 40: 0.2928870292887029}
Micro-average F1 score: 0.25512104283054005
Weighted-average F1 score: 0.2317153557528166
F1 score per class: {1: 0.10864197530864197, 2: 0.21428571428571427, 3: 0.1761006289308176, 6: 0.2828282828282828, 7: 0.024390243902439025, 9: 0.746268656716418, 11: 0.017241379310344827, 12: 0.32388663967611336, 14: 0.032171581769437, 15: 0.3333333333333333, 19: 0.2857142857142857, 22: 0.3576158940397351, 24: 0.05263157894736842, 25: 0.7441860465116279, 26: 0.6481481481481481, 27: 0.04225352112676056, 28: 0.07547169811320754, 29: 0.8, 31: 0.0, 32: 0.431924882629108, 34: 0.11713665943600868, 35: 0.16831683168316833, 37: 0.18685121107266436, 38: 0.12063492063492064, 39: 0.0, 40: 0.28846153846153844}
Micro-average F1 score: 0.2491542288557214
Weighted-average F1 score: 0.22078235420493414
cur_acc_wo_na:  ['0.8115', '0.4414', '0.4762', '0.5123', '0.3188']
his_acc_wo_na:  ['0.8115', '0.7300', '0.5830', '0.5724', '0.4072']
cur_acc des_wo_na:  ['0.8315', '0.7111', '0.5753', '0.8392', '0.3767']
his_acc des_wo_na:  ['0.8315', '0.8050', '0.6795', '0.6957', '0.5392']
cur_acc rrf_wo_na:  ['0.8352', '0.7167', '0.5753', '0.7944', '0.3699']
his_acc rrf_wo_na:  ['0.8352', '0.7891', '0.6778', '0.6772', '0.5038']
cur_acc_w_na:  ['0.6877', '0.2916', '0.3521', '0.3281', '0.1695']
his_acc_w_na:  ['0.6877', '0.5333', '0.4516', '0.3922', '0.2555']
cur_acc des_w_na:  ['0.6409', '0.3413', '0.3073', '0.3802', '0.1558']
his_acc des_w_na:  ['0.6409', '0.4453', '0.3849', '0.3272', '0.2551']
cur_acc rrf_w_na:  ['0.6461', '0.3539', '0.3142', '0.3667', '0.1597']
his_acc rrf_w_na:  ['0.6461', '0.4445', '0.3937', '0.3289', '0.2492']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 101.9535070CurrentTrain: epoch  0, batch     1 | loss: 96.5469556CurrentTrain: epoch  0, batch     2 | loss: 98.8899402CurrentTrain: epoch  0, batch     3 | loss: 155.4222121CurrentTrain: epoch  1, batch     0 | loss: 92.3330392CurrentTrain: epoch  1, batch     1 | loss: 65.8268912CurrentTrain: epoch  1, batch     2 | loss: 120.9807576CurrentTrain: epoch  1, batch     3 | loss: 75.9824183CurrentTrain: epoch  2, batch     0 | loss: 66.9845223CurrentTrain: epoch  2, batch     1 | loss: 69.8185288CurrentTrain: epoch  2, batch     2 | loss: 93.1226636CurrentTrain: epoch  2, batch     3 | loss: 57.2664203CurrentTrain: epoch  3, batch     0 | loss: 71.8638760CurrentTrain: epoch  3, batch     1 | loss: 71.4120270CurrentTrain: epoch  3, batch     2 | loss: 56.2682656CurrentTrain: epoch  3, batch     3 | loss: 68.0943722CurrentTrain: epoch  4, batch     0 | loss: 56.8557403CurrentTrain: epoch  4, batch     1 | loss: 70.7188625CurrentTrain: epoch  4, batch     2 | loss: 66.5917676CurrentTrain: epoch  4, batch     3 | loss: 67.8881832CurrentTrain: epoch  5, batch     0 | loss: 51.0036132CurrentTrain: epoch  5, batch     1 | loss: 54.7000670CurrentTrain: epoch  5, batch     2 | loss: 124.7993661CurrentTrain: epoch  5, batch     3 | loss: 57.2978881CurrentTrain: epoch  6, batch     0 | loss: 114.2557192CurrentTrain: epoch  6, batch     1 | loss: 55.6182110CurrentTrain: epoch  6, batch     2 | loss: 66.5681862CurrentTrain: epoch  6, batch     3 | loss: 91.7577777CurrentTrain: epoch  7, batch     0 | loss: 86.5728646CurrentTrain: epoch  7, batch     1 | loss: 81.9998375CurrentTrain: epoch  7, batch     2 | loss: 54.3204356CurrentTrain: epoch  7, batch     3 | loss: 53.9199096CurrentTrain: epoch  8, batch     0 | loss: 65.2056463CurrentTrain: epoch  8, batch     1 | loss: 182.2984995CurrentTrain: epoch  8, batch     2 | loss: 53.5380723CurrentTrain: epoch  8, batch     3 | loss: 51.1292027CurrentTrain: epoch  9, batch     0 | loss: 83.2239676CurrentTrain: epoch  9, batch     1 | loss: 62.9242528CurrentTrain: epoch  9, batch     2 | loss: 67.7612259CurrentTrain: epoch  9, batch     3 | loss: 53.7551447
MemoryTrain:  epoch  0, batch     0 | loss: 0.4504659MemoryTrain:  epoch  1, batch     0 | loss: 0.4339513MemoryTrain:  epoch  2, batch     0 | loss: 0.3219971MemoryTrain:  epoch  3, batch     0 | loss: 0.2251373MemoryTrain:  epoch  4, batch     0 | loss: 0.1686313MemoryTrain:  epoch  5, batch     0 | loss: 0.1401846MemoryTrain:  epoch  6, batch     0 | loss: 0.1142788MemoryTrain:  epoch  7, batch     0 | loss: 0.0953035MemoryTrain:  epoch  8, batch     0 | loss: 0.0932941MemoryTrain:  epoch  9, batch     0 | loss: 0.0833551

F1 score per class: {0: 0.9428571428571428, 1: 0.0, 34: 0.0, 2: 0.907103825136612, 4: 0.3333333333333333, 13: 0.0, 14: 0.0, 15: 0.5581395348837209, 21: 0.0, 22: 0.825, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8
Weighted-average F1 score: 0.7603238429868242
F1 score per class: {0: 0.9863013698630136, 1: 0.0, 34: 0.0, 2: 0.9361702127659575, 4: 0.0, 37: 0.5714285714285714, 38: 0.0, 11: 0.0, 13: 0.7083333333333334, 14: 0.9069767441860465, 15: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8465116279069768
Weighted-average F1 score: 0.7926021184345351
F1 score per class: {0: 0.9863013698630136, 1: 0.0, 34: 0.0, 2: 0.9528795811518325, 4: 0.5714285714285714, 37: 0.0, 38: 0.0, 13: 0.7346938775510204, 14: 0.9069767441860465, 15: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8651162790697674
Weighted-average F1 score: 0.8188491960091265

F1 score per class: {0: 0.9295774647887324, 1: 0.37681159420289856, 2: 0.7058823529411765, 3: 0.32, 4: 0.907103825136612, 6: 0.45390070921985815, 7: 0.0, 9: 0.9803921568627451, 11: 0.021739130434782608, 12: 0.11320754716981132, 13: 0.125, 14: 0.11235955056179775, 15: 0.631578947368421, 19: 0.5287356321839081, 21: 0.3157894736842105, 22: 0.5675675675675675, 23: 0.7857142857142857, 24: 0.09090909090909091, 25: 0.5352112676056338, 26: 0.7282608695652174, 27: 0.10810810810810811, 28: 0.0, 29: 0.925531914893617, 31: 0.0, 32: 0.47761194029850745, 34: 0.205607476635514, 35: 0.25225225225225223, 37: 0.4044943820224719, 38: 0.5116279069767442, 39: 0.0, 40: 0.425531914893617}
Micro-average F1 score: 0.4948998866641481
Weighted-average F1 score: 0.5287162500323408
F1 score per class: {0: 0.96, 1: 0.3511450381679389, 2: 0.6086956521739131, 3: 0.5528455284552846, 4: 0.9312169312169312, 6: 0.5359477124183006, 7: 0.08, 9: 0.9803921568627451, 11: 0.13592233009708737, 12: 0.7088607594936709, 13: 0.2222222222222222, 14: 0.1414141414141414, 15: 0.6666666666666666, 19: 0.5955056179775281, 21: 0.3333333333333333, 22: 0.5285714285714286, 23: 0.8571428571428571, 24: 0.0, 25: 0.8842105263157894, 26: 0.7431693989071039, 27: 0.13636363636363635, 28: 0.2, 29: 0.9587628865979382, 31: 1.0, 32: 0.6787878787878788, 34: 0.34146341463414637, 35: 0.5874125874125874, 37: 0.512396694214876, 38: 0.7407407407407407, 39: 0.13333333333333333, 40: 0.6141732283464567}
Micro-average F1 score: 0.5958566261098323
Weighted-average F1 score: 0.5904030199419562
F1 score per class: {0: 0.96, 1: 0.3458646616541353, 2: 0.6363636363636364, 3: 0.5081967213114754, 4: 0.9479166666666666, 6: 0.5605095541401274, 7: 0.0851063829787234, 9: 0.9803921568627451, 11: 0.14, 12: 0.6666666666666666, 13: 0.2, 14: 0.1188118811881188, 15: 0.6666666666666666, 19: 0.6, 21: 0.35294117647058826, 22: 0.5179856115107914, 23: 0.8571428571428571, 24: 0.0, 25: 0.7816091954022989, 26: 0.7362637362637363, 27: 0.13953488372093023, 28: 0.0, 29: 0.9533678756476683, 31: 1.0, 32: 0.6455696202531646, 34: 0.33136094674556216, 35: 0.4888888888888889, 37: 0.5210084033613446, 38: 0.7407407407407407, 39: 0.13333333333333333, 40: 0.5901639344262295}
Micro-average F1 score: 0.5814802522402921
Weighted-average F1 score: 0.575768297313206

F1 score per class: {0: 0.88, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.907103825136612, 6: 0.0, 7: 0.0, 9: 0.0, 12: 0.0, 13: 0.05405405405405406, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.3333333333333333, 22: 0.0, 23: 0.7764705882352941, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.5159235668789809
Weighted-average F1 score: 0.37654876528352055
F1 score per class: {0: 0.6666666666666666, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8888888888888888, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.08333333333333333, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.35789473684210527, 23: 0.6902654867256637, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3575638506876228
Weighted-average F1 score: 0.25876010408466954
F1 score per class: {0: 0.6990291262135923, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9238578680203046, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.07142857142857142, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.375, 23: 0.78, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.38271604938271603
Weighted-average F1 score: 0.272749990253888

F1 score per class: {0: 0.40993788819875776, 1: 0.09541284403669725, 2: 0.2553191489361702, 3: 0.22535211267605634, 4: 0.8877005347593583, 6: 0.25196850393700787, 7: 0.0, 9: 0.7352941176470589, 11: 0.019230769230769232, 12: 0.09230769230769231, 13: 0.00904977375565611, 14: 0.062111801242236024, 15: 0.3157894736842105, 19: 0.34328358208955223, 21: 0.11320754716981132, 22: 0.45652173913043476, 23: 0.6947368421052632, 24: 0.07407407407407407, 25: 0.5277777777777778, 26: 0.6666666666666666, 27: 0.041237113402061855, 28: 0.0, 29: 0.7802690582959642, 31: 0.0, 32: 0.36363636363636365, 34: 0.11224489795918367, 35: 0.112, 37: 0.288, 38: 0.15714285714285714, 39: 0.0, 40: 0.3252032520325203}
Micro-average F1 score: 0.28540305010893247
Weighted-average F1 score: 0.24759755528871003
F1 score per class: {0: 0.19834710743801653, 1: 0.08829174664107485, 2: 0.10071942446043165, 3: 0.19428571428571428, 4: 0.8341232227488151, 6: 0.24848484848484848, 7: 0.038461538461538464, 9: 0.6024096385542169, 11: 0.08974358974358974, 12: 0.2505592841163311, 13: 0.013377926421404682, 14: 0.040697674418604654, 15: 0.34285714285714286, 19: 0.3081395348837209, 21: 0.10897435897435898, 22: 0.46540880503144655, 23: 0.52, 24: 0.0, 25: 0.7241379310344828, 26: 0.6601941747572816, 27: 0.0425531914893617, 28: 0.0392156862745098, 29: 0.7351778656126482, 31: 0.028368794326241134, 32: 0.42748091603053434, 34: 0.11177644710578842, 35: 0.1486725663716814, 37: 0.16802168021680217, 38: 0.14388489208633093, 39: 0.11764705882352941, 40: 0.3058823529411765}
Micro-average F1 score: 0.24086135850059817
Weighted-average F1 score: 0.21065102680208675
F1 score per class: {0: 0.21176470588235294, 1: 0.08348457350272233, 2: 0.11023622047244094, 3: 0.2152777777777778, 4: 0.900990099009901, 6: 0.2558139534883721, 7: 0.047619047619047616, 9: 0.6666666666666666, 11: 0.09333333333333334, 12: 0.2691292875989446, 13: 0.0125, 14: 0.03582089552238806, 15: 0.2727272727272727, 19: 0.3253012048192771, 21: 0.1125, 22: 0.4528301886792453, 23: 0.6554621848739496, 24: 0.0, 25: 0.6538461538461539, 26: 0.6536585365853659, 27: 0.04285714285714286, 28: 0.0, 29: 0.7540983606557377, 31: 0.035398230088495575, 32: 0.4146341463414634, 34: 0.1095890410958904, 35: 0.13496932515337423, 37: 0.17663817663817663, 38: 0.14336917562724014, 39: 0.13333333333333333, 40: 0.3130434782608696}
Micro-average F1 score: 0.2451035254616676
Weighted-average F1 score: 0.2112427665751309
cur_acc_wo_na:  ['0.8115', '0.4414', '0.4762', '0.5123', '0.3188', '0.8000']
his_acc_wo_na:  ['0.8115', '0.7300', '0.5830', '0.5724', '0.4072', '0.4949']
cur_acc des_wo_na:  ['0.8315', '0.7111', '0.5753', '0.8392', '0.3767', '0.8465']
his_acc des_wo_na:  ['0.8315', '0.8050', '0.6795', '0.6957', '0.5392', '0.5959']
cur_acc rrf_wo_na:  ['0.8352', '0.7167', '0.5753', '0.7944', '0.3699', '0.8651']
his_acc rrf_wo_na:  ['0.8352', '0.7891', '0.6778', '0.6772', '0.5038', '0.5815']
cur_acc_w_na:  ['0.6877', '0.2916', '0.3521', '0.3281', '0.1695', '0.5159']
his_acc_w_na:  ['0.6877', '0.5333', '0.4516', '0.3922', '0.2555', '0.2854']
cur_acc des_w_na:  ['0.6409', '0.3413', '0.3073', '0.3802', '0.1558', '0.3576']
his_acc des_w_na:  ['0.6409', '0.4453', '0.3849', '0.3272', '0.2551', '0.2409']
cur_acc rrf_w_na:  ['0.6461', '0.3539', '0.3142', '0.3667', '0.1597', '0.3827']
his_acc rrf_w_na:  ['0.6461', '0.4445', '0.3937', '0.3289', '0.2492', '0.2451']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 65.4961171CurrentTrain: epoch  0, batch     1 | loss: 96.3942863CurrentTrain: epoch  0, batch     2 | loss: 94.3339083CurrentTrain: epoch  0, batch     3 | loss: 46.4351567CurrentTrain: epoch  1, batch     0 | loss: 57.1709694CurrentTrain: epoch  1, batch     1 | loss: 71.1676815CurrentTrain: epoch  1, batch     2 | loss: 71.0672393CurrentTrain: epoch  1, batch     3 | loss: 232.7888083CurrentTrain: epoch  2, batch     0 | loss: 57.7133093CurrentTrain: epoch  2, batch     1 | loss: 116.2870893CurrentTrain: epoch  2, batch     2 | loss: 56.4619916CurrentTrain: epoch  2, batch     3 | loss: 39.9082654CurrentTrain: epoch  3, batch     0 | loss: 52.7308855CurrentTrain: epoch  3, batch     1 | loss: 68.2287061CurrentTrain: epoch  3, batch     2 | loss: 112.7119581CurrentTrain: epoch  3, batch     3 | loss: 109.0669045CurrentTrain: epoch  4, batch     0 | loss: 65.6207814CurrentTrain: epoch  4, batch     1 | loss: 53.6337756CurrentTrain: epoch  4, batch     2 | loss: 66.2644940CurrentTrain: epoch  4, batch     3 | loss: 51.5837584CurrentTrain: epoch  5, batch     0 | loss: 86.6156084CurrentTrain: epoch  5, batch     1 | loss: 51.2711529CurrentTrain: epoch  5, batch     2 | loss: 65.4022754CurrentTrain: epoch  5, batch     3 | loss: 40.6100435CurrentTrain: epoch  6, batch     0 | loss: 82.4059342CurrentTrain: epoch  6, batch     1 | loss: 62.1912734CurrentTrain: epoch  6, batch     2 | loss: 64.6191445CurrentTrain: epoch  6, batch     3 | loss: 69.7589419CurrentTrain: epoch  7, batch     0 | loss: 83.7489217CurrentTrain: epoch  7, batch     1 | loss: 65.2885433CurrentTrain: epoch  7, batch     2 | loss: 53.6178787CurrentTrain: epoch  7, batch     3 | loss: 35.6177528CurrentTrain: epoch  8, batch     0 | loss: 53.0192271CurrentTrain: epoch  8, batch     1 | loss: 62.0568305CurrentTrain: epoch  8, batch     2 | loss: 65.7163071CurrentTrain: epoch  8, batch     3 | loss: 48.4776312CurrentTrain: epoch  9, batch     0 | loss: 51.4106254CurrentTrain: epoch  9, batch     1 | loss: 65.5913635CurrentTrain: epoch  9, batch     2 | loss: 52.3614873CurrentTrain: epoch  9, batch     3 | loss: 48.6661102
MemoryTrain:  epoch  0, batch     0 | loss: 0.1909156MemoryTrain:  epoch  1, batch     0 | loss: 0.1823649MemoryTrain:  epoch  2, batch     0 | loss: 0.1453176MemoryTrain:  epoch  3, batch     0 | loss: 0.1171474MemoryTrain:  epoch  4, batch     0 | loss: 0.1008281MemoryTrain:  epoch  5, batch     0 | loss: 0.0856513MemoryTrain:  epoch  6, batch     0 | loss: 0.0734334MemoryTrain:  epoch  7, batch     0 | loss: 0.0640908MemoryTrain:  epoch  8, batch     0 | loss: 0.0590024MemoryTrain:  epoch  9, batch     0 | loss: 0.0602338

F1 score per class: {33: 0.0, 34: 0.48598130841121495, 36: 0.0, 37: 0.9306930693069307, 38: 0.0, 7: 0.0, 8: 0.0, 13: 0.972972972972973, 20: 0.42857142857142855, 26: 0.0, 28: 0.3076923076923077, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5955056179775281
Weighted-average F1 score: 0.6309569901412226
F1 score per class: {7: 0.0, 8: 0.7910447761194029, 11: 0.0, 12: 0.0, 13: 0.0, 20: 0.9306930693069307, 21: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9444444444444444, 33: 0.5333333333333333, 34: 0.0, 35: 0.0, 36: 0.7964601769911505, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.7738927738927739
Weighted-average F1 score: 0.7135882050466013
F1 score per class: {33: 0.0, 34: 0.8, 35: 0.0, 36: 0.0, 37: 0.9306930693069307, 38: 0.0, 7: 0.0, 8: 0.0, 12: 0.0, 13: 1.0, 20: 0.42857142857142855, 25: 0.0, 26: 0.0, 28: 0.7592592592592593, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7735849056603774
Weighted-average F1 score: 0.7208143908395281

F1 score per class: {0: 0.9459459459459459, 1: 0.38848920863309355, 2: 0.75, 3: 0.2857142857142857, 4: 0.9247311827956989, 6: 0.42028985507246375, 7: 0.0, 8: 0.4444444444444444, 9: 0.9803921568627451, 11: 0.0, 12: 0.14814814814814814, 13: 0.18181818181818182, 14: 0.0975609756097561, 15: 0.75, 19: 0.5357142857142857, 20: 0.746031746031746, 21: 0.24615384615384617, 22: 0.589041095890411, 23: 0.6301369863013698, 24: 0.08695652173913043, 25: 0.4, 26: 0.7120418848167539, 27: 0.10810810810810811, 28: 0.0, 29: 0.9090909090909091, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.4626865671641791, 33: 0.4, 34: 0.17543859649122806, 35: 0.21782178217821782, 36: 0.3, 37: 0.30952380952380953, 38: 0.2777777777777778, 39: 0.0, 40: 0.30952380952380953}
Micro-average F1 score: 0.4872924432395798
Weighted-average F1 score: 0.5394816465727157
F1 score per class: {0: 0.972972972972973, 1: 0.3870967741935484, 2: 0.6666666666666666, 3: 0.532258064516129, 4: 0.9417989417989417, 6: 0.5911949685534591, 7: 0.1, 8: 0.7066666666666667, 9: 0.9803921568627451, 11: 0.11650485436893204, 12: 0.6875, 13: 0.3076923076923077, 14: 0.06896551724137931, 15: 0.7058823529411765, 19: 0.6285714285714286, 20: 0.8545454545454545, 21: 0.34545454545454546, 22: 0.5850340136054422, 23: 0.7407407407407407, 24: 0.09090909090909091, 25: 0.7126436781609196, 26: 0.7046632124352331, 27: 0.11538461538461539, 28: 0.3076923076923077, 29: 0.9435897435897436, 30: 0.9444444444444444, 31: 0.5714285714285714, 32: 0.7093023255813954, 33: 0.19047619047619047, 34: 0.25862068965517243, 35: 0.5428571428571428, 36: 0.6923076923076923, 37: 0.4864864864864865, 38: 0.6086956521739131, 39: 0.125, 40: 0.656}
Micro-average F1 score: 0.6073298429319371
Weighted-average F1 score: 0.6055912350242308
F1 score per class: {0: 0.96, 1: 0.36923076923076925, 2: 0.631578947368421, 3: 0.5040650406504065, 4: 0.9473684210526315, 6: 0.5962732919254659, 7: 0.10256410256410256, 8: 0.7152317880794702, 9: 0.9803921568627451, 11: 0.0625, 12: 0.6666666666666666, 13: 0.26666666666666666, 14: 0.10416666666666667, 15: 0.7, 19: 0.6187845303867403, 20: 0.7580645161290323, 21: 0.30927835051546393, 22: 0.5850340136054422, 23: 0.7341772151898734, 24: 0.09090909090909091, 25: 0.5, 26: 0.7046632124352331, 27: 0.11538461538461539, 28: 0.18181818181818182, 29: 0.9435897435897436, 30: 1.0, 31: 0.8, 32: 0.6625766871165644, 33: 0.16216216216216217, 34: 0.2595419847328244, 35: 0.4461538461538462, 36: 0.7068965517241379, 37: 0.48598130841121495, 38: 0.6222222222222222, 39: 0.125, 40: 0.5357142857142857}
Micro-average F1 score: 0.5855670103092784
Weighted-average F1 score: 0.5849222420368091

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.4406779661016949, 12: 0.0, 13: 0.0, 15: 0.0, 19: 0.0, 20: 0.6103896103896104, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9230769230769231, 31: 0.0, 32: 0.0, 33: 0.4, 34: 0.0, 36: 0.27586206896551724, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3712784588441331
Weighted-average F1 score: 0.303525552042759
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.5492227979274611, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.6064516129032258, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8947368421052632, 31: 0.0, 32: 0.0, 33: 0.24242424242424243, 34: 0.0, 35: 0.0, 36: 0.569620253164557, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.34511434511434513
Weighted-average F1 score: 0.270181891786011
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.5595854922279793, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.5838509316770186, 21: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.926829268292683, 31: 0.0, 32: 0.0, 33: 0.1935483870967742, 34: 0.0, 35: 0.0, 36: 0.5774647887323944, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.36083608360836084
Weighted-average F1 score: 0.285094631868748

F1 score per class: {0: 0.39106145251396646, 1: 0.10207939508506617, 2: 0.22641509433962265, 3: 0.20437956204379562, 4: 0.9052631578947369, 6: 0.25892857142857145, 7: 0.0, 8: 0.2549019607843137, 9: 0.746268656716418, 11: 0.0, 12: 0.11188811188811189, 13: 0.01834862385321101, 14: 0.06015037593984962, 15: 0.4, 19: 0.3284671532846715, 20: 0.235, 21: 0.09467455621301775, 22: 0.5088757396449705, 23: 0.575, 24: 0.07142857142857142, 25: 0.3939393939393939, 26: 0.6210045662100456, 27: 0.043478260869565216, 28: 0.0, 29: 0.7359307359307359, 30: 0.9230769230769231, 31: 0.029411764705882353, 32: 0.3668639053254438, 33: 0.2608695652173913, 34: 0.10471204188481675, 35: 0.10784313725490197, 36: 0.24489795918367346, 37: 0.25742574257425743, 38: 0.10204081632653061, 39: 0.0, 40: 0.23853211009174313}
Micro-average F1 score: 0.2862830977503484
Weighted-average F1 score: 0.2584989701771518
F1 score per class: {0: 0.1836734693877551, 1: 0.10062893081761007, 2: 0.16091954022988506, 3: 0.19584569732937684, 4: 0.9175257731958762, 6: 0.26330532212885155, 7: 0.043010752688172046, 8: 0.2167689161554192, 9: 0.5813953488372093, 11: 0.09022556390977443, 12: 0.25882352941176473, 13: 0.0273972602739726, 14: 0.023255813953488372, 15: 0.36363636363636365, 19: 0.3374233128834356, 20: 0.22274881516587677, 21: 0.10160427807486631, 22: 0.4574468085106383, 23: 0.5217391304347826, 24: 0.08, 25: 0.6526315789473685, 26: 0.6071428571428571, 27: 0.03550295857988166, 28: 0.10810810810810811, 29: 0.6996197718631179, 30: 0.4788732394366197, 31: 0.0163265306122449, 32: 0.43884892086330934, 33: 0.048484848484848485, 34: 0.12605042016806722, 35: 0.15767634854771784, 36: 0.35019455252918286, 37: 0.25, 38: 0.175, 39: 0.09090909090909091, 40: 0.3166023166023166}
Micro-average F1 score: 0.2565740968296879
Weighted-average F1 score: 0.23018975805242953
F1 score per class: {0: 0.19148936170212766, 1: 0.09430255402750491, 2: 0.17647058823529413, 3: 0.24899598393574296, 4: 0.9278350515463918, 6: 0.26519337016574585, 7: 0.0449438202247191, 8: 0.2222222222222222, 9: 0.6329113924050633, 11: 0.05263157894736842, 12: 0.24880382775119617, 13: 0.022727272727272728, 14: 0.036231884057971016, 15: 0.35, 19: 0.33432835820895523, 20: 0.2079646017699115, 21: 0.09316770186335403, 22: 0.4858757062146893, 23: 0.5686274509803921, 24: 0.07692307692307693, 25: 0.4675324675324675, 26: 0.6044444444444445, 27: 0.034482758620689655, 28: 0.09090909090909091, 29: 0.7272727272727273, 30: 0.6031746031746031, 31: 0.024844720496894408, 32: 0.4251968503937008, 33: 0.04054054054054054, 34: 0.12546125461254612, 35: 0.13151927437641722, 36: 0.4079601990049751, 37: 0.2708333333333333, 38: 0.15730337078651685, 39: 0.125, 40: 0.28846153846153844}
Micro-average F1 score: 0.25704680630980087
Weighted-average F1 score: 0.22851018130536352
cur_acc_wo_na:  ['0.8115', '0.4414', '0.4762', '0.5123', '0.3188', '0.8000', '0.5955']
his_acc_wo_na:  ['0.8115', '0.7300', '0.5830', '0.5724', '0.4072', '0.4949', '0.4873']
cur_acc des_wo_na:  ['0.8315', '0.7111', '0.5753', '0.8392', '0.3767', '0.8465', '0.7739']
his_acc des_wo_na:  ['0.8315', '0.8050', '0.6795', '0.6957', '0.5392', '0.5959', '0.6073']
cur_acc rrf_wo_na:  ['0.8352', '0.7167', '0.5753', '0.7944', '0.3699', '0.8651', '0.7736']
his_acc rrf_wo_na:  ['0.8352', '0.7891', '0.6778', '0.6772', '0.5038', '0.5815', '0.5856']
cur_acc_w_na:  ['0.6877', '0.2916', '0.3521', '0.3281', '0.1695', '0.5159', '0.3713']
his_acc_w_na:  ['0.6877', '0.5333', '0.4516', '0.3922', '0.2555', '0.2854', '0.2863']
cur_acc des_w_na:  ['0.6409', '0.3413', '0.3073', '0.3802', '0.1558', '0.3576', '0.3451']
his_acc des_w_na:  ['0.6409', '0.4453', '0.3849', '0.3272', '0.2551', '0.2409', '0.2566']
cur_acc rrf_w_na:  ['0.6461', '0.3539', '0.3142', '0.3667', '0.1597', '0.3827', '0.3608']
his_acc rrf_w_na:  ['0.6461', '0.4445', '0.3937', '0.3289', '0.2492', '0.2451', '0.2570']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 129.0097941CurrentTrain: epoch  0, batch     1 | loss: 75.6423878CurrentTrain: epoch  0, batch     2 | loss: 94.1159562CurrentTrain: epoch  0, batch     3 | loss: 70.7098297CurrentTrain: epoch  0, batch     4 | loss: 53.8330701CurrentTrain: epoch  1, batch     0 | loss: 71.0514000CurrentTrain: epoch  1, batch     1 | loss: 92.6579810CurrentTrain: epoch  1, batch     2 | loss: 94.6375079CurrentTrain: epoch  1, batch     3 | loss: 91.2803586CurrentTrain: epoch  1, batch     4 | loss: 47.4888924CurrentTrain: epoch  2, batch     0 | loss: 86.7351031CurrentTrain: epoch  2, batch     1 | loss: 86.0014406CurrentTrain: epoch  2, batch     2 | loss: 123.6541271CurrentTrain: epoch  2, batch     3 | loss: 68.8072455CurrentTrain: epoch  2, batch     4 | loss: 43.4967009CurrentTrain: epoch  3, batch     0 | loss: 55.6095010CurrentTrain: epoch  3, batch     1 | loss: 67.1669230CurrentTrain: epoch  3, batch     2 | loss: 69.9904779CurrentTrain: epoch  3, batch     3 | loss: 69.8416541CurrentTrain: epoch  3, batch     4 | loss: 57.2453142CurrentTrain: epoch  4, batch     0 | loss: 54.2995339CurrentTrain: epoch  4, batch     1 | loss: 85.6427703CurrentTrain: epoch  4, batch     2 | loss: 86.6666104CurrentTrain: epoch  4, batch     3 | loss: 116.2823483CurrentTrain: epoch  4, batch     4 | loss: 53.7381288CurrentTrain: epoch  5, batch     0 | loss: 65.8756883CurrentTrain: epoch  5, batch     1 | loss: 55.5227972CurrentTrain: epoch  5, batch     2 | loss: 55.5097087CurrentTrain: epoch  5, batch     3 | loss: 56.0055181CurrentTrain: epoch  5, batch     4 | loss: 241.9067491CurrentTrain: epoch  6, batch     0 | loss: 116.9502842CurrentTrain: epoch  6, batch     1 | loss: 85.2724953CurrentTrain: epoch  6, batch     2 | loss: 62.1650533CurrentTrain: epoch  6, batch     3 | loss: 87.7335114CurrentTrain: epoch  6, batch     4 | loss: 40.6838100CurrentTrain: epoch  7, batch     0 | loss: 85.5548103CurrentTrain: epoch  7, batch     1 | loss: 115.6568487CurrentTrain: epoch  7, batch     2 | loss: 83.9489001CurrentTrain: epoch  7, batch     3 | loss: 51.6250483CurrentTrain: epoch  7, batch     4 | loss: 42.0264928CurrentTrain: epoch  8, batch     0 | loss: 65.0390095CurrentTrain: epoch  8, batch     1 | loss: 63.4113634CurrentTrain: epoch  8, batch     2 | loss: 84.5823730CurrentTrain: epoch  8, batch     3 | loss: 114.7163259CurrentTrain: epoch  8, batch     4 | loss: 110.5137113CurrentTrain: epoch  9, batch     0 | loss: 83.3378228CurrentTrain: epoch  9, batch     1 | loss: 115.9987265CurrentTrain: epoch  9, batch     2 | loss: 81.1898260CurrentTrain: epoch  9, batch     3 | loss: 82.0338276CurrentTrain: epoch  9, batch     4 | loss: 41.2050109
MemoryTrain:  epoch  0, batch     0 | loss: 0.2002853MemoryTrain:  epoch  1, batch     0 | loss: 0.2033451MemoryTrain:  epoch  2, batch     0 | loss: 0.1576708MemoryTrain:  epoch  3, batch     0 | loss: 0.1335345MemoryTrain:  epoch  4, batch     0 | loss: 0.1033529MemoryTrain:  epoch  5, batch     0 | loss: 0.0783270MemoryTrain:  epoch  6, batch     0 | loss: 0.0688089MemoryTrain:  epoch  7, batch     0 | loss: 0.0702415MemoryTrain:  epoch  8, batch     0 | loss: 0.0635305MemoryTrain:  epoch  9, batch     0 | loss: 0.0640259

F1 score per class: {34: 0.9743589743589743, 5: 0.0, 38: 0.0, 6: 0.2905982905982906, 8: 0.0, 10: 0.9090909090909091, 13: 0.5, 16: 0.5769230769230769, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.6813186813186813
Weighted-average F1 score: 0.7284100133262144
F1 score per class: {34: 0.0, 3: 0.9949748743718593, 5: 0.0, 38: 0.0, 6: 0.0, 8: 0.5074626865671642, 10: 0.0, 7: 0.9310344827586207, 13: 0.875, 16: 0.8787878787878788, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7382297551789078
Weighted-average F1 score: 0.6801374819928921
F1 score per class: {34: 0.0, 3: 0.9949748743718593, 5: 0.0, 38: 0.0, 6: 0.5074626865671642, 8: 0.0, 10: 0.9310344827586207, 13: 0.875, 16: 0.8615384615384616, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7372400756143668
Weighted-average F1 score: 0.6801315665866242

F1 score per class: {0: 0.8611111111111112, 1: 0.4117647058823529, 2: 0.5714285714285714, 3: 0.17582417582417584, 4: 0.907103825136612, 5: 0.9047619047619048, 6: 0.3484848484848485, 7: 0.0, 8: 0.2857142857142857, 9: 0.9803921568627451, 10: 0.2857142857142857, 11: 0.0, 12: 0.019801980198019802, 13: 0.1111111111111111, 14: 0.02702702702702703, 15: 0.75, 16: 0.8333333333333334, 17: 0.375, 18: 0.38461538461538464, 19: 0.572972972972973, 20: 0.65625, 21: 0.29508196721311475, 22: 0.5986394557823129, 23: 0.6753246753246753, 24: 0.08333333333333333, 25: 0.42424242424242425, 26: 0.717391304347826, 27: 0.17391304347826086, 28: 0.0, 29: 0.8924731182795699, 30: 0.9444444444444444, 31: 0.6666666666666666, 32: 0.48175182481751827, 33: 0.4, 34: 0.125, 35: 0.13043478260869565, 36: 0.1891891891891892, 37: 0.3764705882352941, 38: 0.17647058823529413, 39: 0.13333333333333333, 40: 0.3595505617977528}
Micro-average F1 score: 0.4911452184179457
Weighted-average F1 score: 0.5646899410553969
F1 score per class: {0: 0.958904109589041, 1: 0.36363636363636365, 2: 0.5714285714285714, 3: 0.5161290322580645, 4: 0.9361702127659575, 5: 0.8608695652173913, 6: 0.5375, 7: 0.0851063829787234, 8: 0.569620253164557, 9: 0.9615384615384616, 10: 0.4755244755244755, 11: 0.043478260869565216, 12: 0.6623376623376623, 13: 0.16, 14: 0.07142857142857142, 15: 0.6666666666666666, 16: 0.8307692307692308, 17: 0.5185185185185185, 18: 0.3314285714285714, 19: 0.6808510638297872, 20: 0.8543689320388349, 21: 0.3404255319148936, 22: 0.675, 23: 0.7804878048780488, 24: 0.08333333333333333, 25: 0.6, 26: 0.7195767195767195, 27: 0.14545454545454545, 28: 0.16666666666666666, 29: 0.9387755102040817, 30: 0.9743589743589743, 31: 0.5714285714285714, 32: 0.7717391304347826, 33: 0.24, 34: 0.10869565217391304, 35: 0.5571428571428572, 36: 0.7213114754098361, 37: 0.4731182795698925, 38: 0.4074074074074074, 39: 0.25, 40: 0.6446280991735537}
Micro-average F1 score: 0.6024813895781638
Weighted-average F1 score: 0.604487263284889
F1 score per class: {0: 0.958904109589041, 1: 0.3442622950819672, 2: 0.6, 3: 0.4444444444444444, 4: 0.918918918918919, 5: 0.868421052631579, 6: 0.5443037974683544, 7: 0.09523809523809523, 8: 0.5838509316770186, 9: 0.9803921568627451, 10: 0.4755244755244755, 11: 0.0, 12: 0.5594405594405595, 13: 0.16, 14: 0.1111111111111111, 15: 0.631578947368421, 16: 0.8181818181818182, 17: 0.4375, 18: 0.33532934131736525, 19: 0.6701030927835051, 20: 0.7652173913043478, 21: 0.2823529411764706, 22: 0.6580645161290323, 23: 0.75, 24: 0.08, 25: 0.5822784810126582, 26: 0.7195767195767195, 27: 0.14814814814814814, 28: 0.16666666666666666, 29: 0.9381443298969072, 30: 1.0, 31: 0.4, 32: 0.7513812154696132, 33: 0.24, 34: 0.1188118811881188, 35: 0.45255474452554745, 36: 0.6346153846153846, 37: 0.4842105263157895, 38: 0.4, 39: 0.125, 40: 0.5714285714285714}
Micro-average F1 score: 0.5804669846849109
Weighted-average F1 score: 0.5834958669211604

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.8962264150943396, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.288135593220339, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.5617977528089888, 17: 0.46153846153846156, 18: 0.23255813953488372, 20: 0.0, 21: 0.0, 26: 0.0, 29: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.442225392296719
Weighted-average F1 score: 0.3810620453355897
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.6226415094339622, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.43312101910828027, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.4462809917355372, 17: 0.6363636363636364, 18: 0.19205298013245034, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.28634039444850257
Weighted-average F1 score: 0.23806229130973122
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.66, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.43037974683544306, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.4426229508196721, 17: 0.5, 18: 0.18791946308724833, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2972560975609756
Weighted-average F1 score: 0.2473265604803449

F1 score per class: {0: 0.49206349206349204, 1: 0.10916179337231968, 2: 0.20512820512820512, 3: 0.13675213675213677, 4: 0.8783068783068783, 5: 0.7755102040816326, 6: 0.2081447963800905, 7: 0.0, 8: 0.20270270270270271, 9: 0.7352941176470589, 10: 0.2111801242236025, 11: 0.0, 12: 0.017241379310344827, 13: 0.023809523809523808, 14: 0.01834862385321101, 15: 0.48, 16: 0.37037037037037035, 17: 0.15, 18: 0.14634146341463414, 19: 0.34415584415584416, 20: 0.225201072386059, 21: 0.12080536912751678, 22: 0.4943820224719101, 23: 0.611764705882353, 24: 0.06666666666666667, 25: 0.417910447761194, 26: 0.6255924170616114, 27: 0.058823529411764705, 28: 0.0, 29: 0.7312775330396476, 30: 0.8947368421052632, 31: 0.041666666666666664, 32: 0.375, 33: 0.25, 34: 0.08275862068965517, 35: 0.06593406593406594, 36: 0.16666666666666666, 37: 0.27586206896551724, 38: 0.13043478260869565, 39: 0.13333333333333333, 40: 0.25196850393700787}
Micro-average F1 score: 0.30370505566709255
Weighted-average F1 score: 0.2848006186701449
F1 score per class: {0: 0.2766798418972332, 1: 0.09592326139088729, 2: 0.16, 3: 0.19814241486068113, 4: 0.9025641025641026, 5: 0.38372093023255816, 6: 0.21608040201005024, 7: 0.0425531914893617, 8: 0.23195876288659795, 9: 0.5102040816326531, 10: 0.2251655629139073, 11: 0.04, 12: 0.2446043165467626, 13: 0.02247191011235955, 14: 0.02643171806167401, 15: 0.3157894736842105, 16: 0.28272251308900526, 17: 0.1590909090909091, 18: 0.0703883495145631, 19: 0.33246753246753247, 20: 0.17635270541082165, 21: 0.09014084507042254, 22: 0.526829268292683, 23: 0.49612403100775193, 24: 0.06666666666666667, 25: 0.5454545454545454, 26: 0.6153846153846154, 27: 0.038834951456310676, 28: 0.038461538461538464, 29: 0.6917293233082706, 30: 0.4810126582278481, 31: 0.021621621621621623, 32: 0.44936708860759494, 33: 0.075, 34: 0.06451612903225806, 35: 0.16595744680851063, 36: 0.3793103448275862, 37: 0.2268041237113402, 38: 0.1286549707602339, 39: 0.09523809523809523, 40: 0.3058823529411765}
Micro-average F1 score: 0.24882147981143676
Weighted-average F1 score: 0.2267469993181326
F1 score per class: {0: 0.26515151515151514, 1: 0.09130434782608696, 2: 0.17647058823529413, 3: 0.22807017543859648, 4: 0.9042553191489362, 5: 0.4541284403669725, 6: 0.2222222222222222, 7: 0.047619047619047616, 8: 0.23095823095823095, 9: 0.5952380952380952, 10: 0.2222222222222222, 11: 0.0, 12: 0.22988505747126436, 13: 0.020833333333333332, 14: 0.04310344827586207, 15: 0.3, 16: 0.27979274611398963, 17: 0.12727272727272726, 18: 0.07017543859649122, 19: 0.3299492385786802, 20: 0.16417910447761194, 21: 0.08, 22: 0.5230769230769231, 23: 0.5405405405405406, 24: 0.06451612903225806, 25: 0.5542168674698795, 26: 0.6181818181818182, 27: 0.038834951456310676, 28: 0.05714285714285714, 29: 0.7054263565891473, 30: 0.6129032258064516, 31: 0.014084507042253521, 32: 0.4610169491525424, 33: 0.07407407407407407, 34: 0.07142857142857142, 35: 0.1399548532731377, 36: 0.39285714285714285, 37: 0.24864864864864866, 38: 0.13496932515337423, 39: 0.125, 40: 0.30917874396135264}
Micro-average F1 score: 0.2507048362611147
Weighted-average F1 score: 0.22533274724015603
cur_acc_wo_na:  ['0.8115', '0.4414', '0.4762', '0.5123', '0.3188', '0.8000', '0.5955', '0.6813']
his_acc_wo_na:  ['0.8115', '0.7300', '0.5830', '0.5724', '0.4072', '0.4949', '0.4873', '0.4911']
cur_acc des_wo_na:  ['0.8315', '0.7111', '0.5753', '0.8392', '0.3767', '0.8465', '0.7739', '0.7382']
his_acc des_wo_na:  ['0.8315', '0.8050', '0.6795', '0.6957', '0.5392', '0.5959', '0.6073', '0.6025']
cur_acc rrf_wo_na:  ['0.8352', '0.7167', '0.5753', '0.7944', '0.3699', '0.8651', '0.7736', '0.7372']
his_acc rrf_wo_na:  ['0.8352', '0.7891', '0.6778', '0.6772', '0.5038', '0.5815', '0.5856', '0.5805']
cur_acc_w_na:  ['0.6877', '0.2916', '0.3521', '0.3281', '0.1695', '0.5159', '0.3713', '0.4422']
his_acc_w_na:  ['0.6877', '0.5333', '0.4516', '0.3922', '0.2555', '0.2854', '0.2863', '0.3037']
cur_acc des_w_na:  ['0.6409', '0.3413', '0.3073', '0.3802', '0.1558', '0.3576', '0.3451', '0.2863']
his_acc des_w_na:  ['0.6409', '0.4453', '0.3849', '0.3272', '0.2551', '0.2409', '0.2566', '0.2488']
cur_acc rrf_w_na:  ['0.6461', '0.3539', '0.3142', '0.3667', '0.1597', '0.3827', '0.3608', '0.2973']
his_acc rrf_w_na:  ['0.6461', '0.4445', '0.3937', '0.3289', '0.2492', '0.2451', '0.2570', '0.2507']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 100.9627413CurrentTrain: epoch  0, batch     1 | loss: 80.8300884CurrentTrain: epoch  0, batch     2 | loss: 66.4415188CurrentTrain: epoch  0, batch     3 | loss: 66.9485000CurrentTrain: epoch  0, batch     4 | loss: 79.3622268CurrentTrain: epoch  0, batch     5 | loss: 97.6027538CurrentTrain: epoch  0, batch     6 | loss: 56.7345057CurrentTrain: epoch  0, batch     7 | loss: 96.9498463CurrentTrain: epoch  0, batch     8 | loss: 57.6716326CurrentTrain: epoch  0, batch     9 | loss: 56.9825212CurrentTrain: epoch  0, batch    10 | loss: 125.2037271CurrentTrain: epoch  0, batch    11 | loss: 65.7503525CurrentTrain: epoch  0, batch    12 | loss: 78.1822644CurrentTrain: epoch  0, batch    13 | loss: 96.9090854CurrentTrain: epoch  0, batch    14 | loss: 95.5394389CurrentTrain: epoch  0, batch    15 | loss: 127.3232261CurrentTrain: epoch  0, batch    16 | loss: 77.2688157CurrentTrain: epoch  0, batch    17 | loss: 96.8400778CurrentTrain: epoch  0, batch    18 | loss: 77.8855354CurrentTrain: epoch  0, batch    19 | loss: 65.9309043CurrentTrain: epoch  0, batch    20 | loss: 64.4021220CurrentTrain: epoch  0, batch    21 | loss: 95.9551726CurrentTrain: epoch  0, batch    22 | loss: 96.6570797CurrentTrain: epoch  0, batch    23 | loss: 55.9336289CurrentTrain: epoch  0, batch    24 | loss: 188.0496890CurrentTrain: epoch  0, batch    25 | loss: 126.4232389CurrentTrain: epoch  0, batch    26 | loss: 77.3581990CurrentTrain: epoch  0, batch    27 | loss: 76.6394837CurrentTrain: epoch  0, batch    28 | loss: 77.4098062CurrentTrain: epoch  0, batch    29 | loss: 76.6105511CurrentTrain: epoch  0, batch    30 | loss: 94.1686197CurrentTrain: epoch  0, batch    31 | loss: 95.6003489CurrentTrain: epoch  0, batch    32 | loss: 94.7427535CurrentTrain: epoch  0, batch    33 | loss: 64.8389106CurrentTrain: epoch  0, batch    34 | loss: 64.3239916CurrentTrain: epoch  0, batch    35 | loss: 77.0530101CurrentTrain: epoch  0, batch    36 | loss: 55.8608089CurrentTrain: epoch  0, batch    37 | loss: 76.0802585CurrentTrain: epoch  0, batch    38 | loss: 188.1682571CurrentTrain: epoch  0, batch    39 | loss: 76.2554598CurrentTrain: epoch  0, batch    40 | loss: 64.3617685CurrentTrain: epoch  0, batch    41 | loss: 76.4914655CurrentTrain: epoch  0, batch    42 | loss: 77.2338287CurrentTrain: epoch  0, batch    43 | loss: 96.3229575CurrentTrain: epoch  0, batch    44 | loss: 76.2128478CurrentTrain: epoch  0, batch    45 | loss: 76.6164932CurrentTrain: epoch  0, batch    46 | loss: 94.7409689CurrentTrain: epoch  0, batch    47 | loss: 93.9178670CurrentTrain: epoch  0, batch    48 | loss: 75.8583114CurrentTrain: epoch  0, batch    49 | loss: 76.2563112CurrentTrain: epoch  0, batch    50 | loss: 75.4757597CurrentTrain: epoch  0, batch    51 | loss: 75.7536173CurrentTrain: epoch  0, batch    52 | loss: 76.3769925CurrentTrain: epoch  0, batch    53 | loss: 74.4021401CurrentTrain: epoch  0, batch    54 | loss: 62.7741463CurrentTrain: epoch  0, batch    55 | loss: 93.9658836CurrentTrain: epoch  0, batch    56 | loss: 75.1566744CurrentTrain: epoch  0, batch    57 | loss: 95.3416574CurrentTrain: epoch  0, batch    58 | loss: 74.0181221CurrentTrain: epoch  0, batch    59 | loss: 123.8976243CurrentTrain: epoch  0, batch    60 | loss: 75.0448382CurrentTrain: epoch  0, batch    61 | loss: 54.1921586CurrentTrain: epoch  0, batch    62 | loss: 75.0251360CurrentTrain: epoch  0, batch    63 | loss: 75.8486281CurrentTrain: epoch  0, batch    64 | loss: 63.6345854CurrentTrain: epoch  0, batch    65 | loss: 63.7214080CurrentTrain: epoch  0, batch    66 | loss: 74.5648736CurrentTrain: epoch  0, batch    67 | loss: 75.2148049CurrentTrain: epoch  0, batch    68 | loss: 92.9382599CurrentTrain: epoch  0, batch    69 | loss: 62.1159995CurrentTrain: epoch  0, batch    70 | loss: 76.5572052CurrentTrain: epoch  0, batch    71 | loss: 74.2716408CurrentTrain: epoch  0, batch    72 | loss: 62.0931420CurrentTrain: epoch  0, batch    73 | loss: 92.5259577CurrentTrain: epoch  0, batch    74 | loss: 63.8833100CurrentTrain: epoch  0, batch    75 | loss: 53.1533699CurrentTrain: epoch  0, batch    76 | loss: 124.2517405CurrentTrain: epoch  0, batch    77 | loss: 53.8027730CurrentTrain: epoch  0, batch    78 | loss: 62.8517953CurrentTrain: epoch  0, batch    79 | loss: 93.9413419CurrentTrain: epoch  0, batch    80 | loss: 73.6097454CurrentTrain: epoch  0, batch    81 | loss: 121.5282991CurrentTrain: epoch  0, batch    82 | loss: 124.9704930CurrentTrain: epoch  0, batch    83 | loss: 59.6272324CurrentTrain: epoch  0, batch    84 | loss: 61.2673629CurrentTrain: epoch  0, batch    85 | loss: 92.5378666CurrentTrain: epoch  0, batch    86 | loss: 72.4119189CurrentTrain: epoch  0, batch    87 | loss: 92.2441708CurrentTrain: epoch  0, batch    88 | loss: 70.8883277CurrentTrain: epoch  0, batch    89 | loss: 72.2638433CurrentTrain: epoch  0, batch    90 | loss: 75.3332907CurrentTrain: epoch  0, batch    91 | loss: 58.3646394CurrentTrain: epoch  0, batch    92 | loss: 60.3131714CurrentTrain: epoch  0, batch    93 | loss: 122.3218166CurrentTrain: epoch  0, batch    94 | loss: 72.5271728CurrentTrain: epoch  0, batch    95 | loss: 61.1581753CurrentTrain: epoch  1, batch     0 | loss: 60.4836468CurrentTrain: epoch  1, batch     1 | loss: 89.5773294CurrentTrain: epoch  1, batch     2 | loss: 61.2375515CurrentTrain: epoch  1, batch     3 | loss: 92.4954950CurrentTrain: epoch  1, batch     4 | loss: 57.1991386CurrentTrain: epoch  1, batch     5 | loss: 93.9760582CurrentTrain: epoch  1, batch     6 | loss: 72.0685789CurrentTrain: epoch  1, batch     7 | loss: 120.4667577CurrentTrain: epoch  1, batch     8 | loss: 90.7924589CurrentTrain: epoch  1, batch     9 | loss: 74.1961864CurrentTrain: epoch  1, batch    10 | loss: 87.7343010CurrentTrain: epoch  1, batch    11 | loss: 59.2386646CurrentTrain: epoch  1, batch    12 | loss: 70.7640094CurrentTrain: epoch  1, batch    13 | loss: 92.1558800CurrentTrain: epoch  1, batch    14 | loss: 53.5487145CurrentTrain: epoch  1, batch    15 | loss: 53.7512023CurrentTrain: epoch  1, batch    16 | loss: 120.9447008CurrentTrain: epoch  1, batch    17 | loss: 89.1617263CurrentTrain: epoch  1, batch    18 | loss: 73.0386876CurrentTrain: epoch  1, batch    19 | loss: 93.4704255CurrentTrain: epoch  1, batch    20 | loss: 71.9603589CurrentTrain: epoch  1, batch    21 | loss: 69.3090937CurrentTrain: epoch  1, batch    22 | loss: 72.8543815CurrentTrain: epoch  1, batch    23 | loss: 71.9605952CurrentTrain: epoch  1, batch    24 | loss: 72.8064734CurrentTrain: epoch  1, batch    25 | loss: 71.9187767CurrentTrain: epoch  1, batch    26 | loss: 72.2268482CurrentTrain: epoch  1, batch    27 | loss: 59.1514370CurrentTrain: epoch  1, batch    28 | loss: 86.6829034CurrentTrain: epoch  1, batch    29 | loss: 86.6130303CurrentTrain: epoch  1, batch    30 | loss: 86.2378716CurrentTrain: epoch  1, batch    31 | loss: 71.3731057CurrentTrain: epoch  1, batch    32 | loss: 92.0604130CurrentTrain: epoch  1, batch    33 | loss: 67.8753516CurrentTrain: epoch  1, batch    34 | loss: 58.4200303CurrentTrain: epoch  1, batch    35 | loss: 121.5139674CurrentTrain: epoch  1, batch    36 | loss: 59.3318069CurrentTrain: epoch  1, batch    37 | loss: 73.4970006CurrentTrain: epoch  1, batch    38 | loss: 62.2786567CurrentTrain: epoch  1, batch    39 | loss: 70.5165180CurrentTrain: epoch  1, batch    40 | loss: 48.6157578CurrentTrain: epoch  1, batch    41 | loss: 57.4025505CurrentTrain: epoch  1, batch    42 | loss: 86.0019068CurrentTrain: epoch  1, batch    43 | loss: 90.4744901CurrentTrain: epoch  1, batch    44 | loss: 54.4272708CurrentTrain: epoch  1, batch    45 | loss: 88.9960894CurrentTrain: epoch  1, batch    46 | loss: 68.8144642CurrentTrain: epoch  1, batch    47 | loss: 69.6678306CurrentTrain: epoch  1, batch    48 | loss: 92.6022694CurrentTrain: epoch  1, batch    49 | loss: 48.2460290CurrentTrain: epoch  1, batch    50 | loss: 85.3117918CurrentTrain: epoch  1, batch    51 | loss: 118.3541781CurrentTrain: epoch  1, batch    52 | loss: 87.7802184CurrentTrain: epoch  1, batch    53 | loss: 67.9793939CurrentTrain: epoch  1, batch    54 | loss: 68.4612956CurrentTrain: epoch  1, batch    55 | loss: 68.6605923CurrentTrain: epoch  1, batch    56 | loss: 86.6296632CurrentTrain: epoch  1, batch    57 | loss: 71.1059874CurrentTrain: epoch  1, batch    58 | loss: 117.3363815CurrentTrain: epoch  1, batch    59 | loss: 70.2868975CurrentTrain: epoch  1, batch    60 | loss: 68.8117284CurrentTrain: epoch  1, batch    61 | loss: 118.8487644CurrentTrain: epoch  1, batch    62 | loss: 70.0619515CurrentTrain: epoch  1, batch    63 | loss: 69.9740549CurrentTrain: epoch  1, batch    64 | loss: 57.8627519CurrentTrain: epoch  1, batch    65 | loss: 68.1102931CurrentTrain: epoch  1, batch    66 | loss: 46.3815338CurrentTrain: epoch  1, batch    67 | loss: 61.5965754CurrentTrain: epoch  1, batch    68 | loss: 68.0034188CurrentTrain: epoch  1, batch    69 | loss: 86.6431362CurrentTrain: epoch  1, batch    70 | loss: 85.9797810CurrentTrain: epoch  1, batch    71 | loss: 86.5880530CurrentTrain: epoch  1, batch    72 | loss: 70.2303685CurrentTrain: epoch  1, batch    73 | loss: 68.8227961CurrentTrain: epoch  1, batch    74 | loss: 64.7707065CurrentTrain: epoch  1, batch    75 | loss: 61.1204189CurrentTrain: epoch  1, batch    76 | loss: 56.6518938CurrentTrain: epoch  1, batch    77 | loss: 84.7137146CurrentTrain: epoch  1, batch    78 | loss: 67.6297692CurrentTrain: epoch  1, batch    79 | loss: 60.5588981CurrentTrain: epoch  1, batch    80 | loss: 72.5539020CurrentTrain: epoch  1, batch    81 | loss: 82.4193728CurrentTrain: epoch  1, batch    82 | loss: 88.0264969CurrentTrain: epoch  1, batch    83 | loss: 89.4912907CurrentTrain: epoch  1, batch    84 | loss: 53.3447258CurrentTrain: epoch  1, batch    85 | loss: 121.6467392CurrentTrain: epoch  1, batch    86 | loss: 83.0643254CurrentTrain: epoch  1, batch    87 | loss: 56.1567962CurrentTrain: epoch  1, batch    88 | loss: 88.9370942CurrentTrain: epoch  1, batch    89 | loss: 90.3728673CurrentTrain: epoch  1, batch    90 | loss: 71.8373973CurrentTrain: epoch  1, batch    91 | loss: 86.7992459CurrentTrain: epoch  1, batch    92 | loss: 59.9211807CurrentTrain: epoch  1, batch    93 | loss: 67.6557799CurrentTrain: epoch  1, batch    94 | loss: 72.6960729CurrentTrain: epoch  1, batch    95 | loss: 73.5357510CurrentTrain: epoch  2, batch     0 | loss: 70.9239510CurrentTrain: epoch  2, batch     1 | loss: 117.0879038CurrentTrain: epoch  2, batch     2 | loss: 67.2321398CurrentTrain: epoch  2, batch     3 | loss: 65.6386559CurrentTrain: epoch  2, batch     4 | loss: 88.0827599CurrentTrain: epoch  2, batch     5 | loss: 87.1332684CurrentTrain: epoch  2, batch     6 | loss: 67.5802432CurrentTrain: epoch  2, batch     7 | loss: 93.8307848CurrentTrain: epoch  2, batch     8 | loss: 55.9293488CurrentTrain: epoch  2, batch     9 | loss: 67.6819219CurrentTrain: epoch  2, batch    10 | loss: 69.2873319CurrentTrain: epoch  2, batch    11 | loss: 58.2467926CurrentTrain: epoch  2, batch    12 | loss: 71.0190825CurrentTrain: epoch  2, batch    13 | loss: 68.5895937CurrentTrain: epoch  2, batch    14 | loss: 84.1569367CurrentTrain: epoch  2, batch    15 | loss: 85.3513766CurrentTrain: epoch  2, batch    16 | loss: 69.5905307CurrentTrain: epoch  2, batch    17 | loss: 70.6473100CurrentTrain: epoch  2, batch    18 | loss: 69.4177758CurrentTrain: epoch  2, batch    19 | loss: 65.8978669CurrentTrain: epoch  2, batch    20 | loss: 66.7828099CurrentTrain: epoch  2, batch    21 | loss: 56.1911608CurrentTrain: epoch  2, batch    22 | loss: 81.8320239CurrentTrain: epoch  2, batch    23 | loss: 65.9042664CurrentTrain: epoch  2, batch    24 | loss: 117.6807269CurrentTrain: epoch  2, batch    25 | loss: 69.2578264CurrentTrain: epoch  2, batch    26 | loss: 62.7074355CurrentTrain: epoch  2, batch    27 | loss: 66.6275646CurrentTrain: epoch  2, batch    28 | loss: 182.1032774CurrentTrain: epoch  2, batch    29 | loss: 55.0767670CurrentTrain: epoch  2, batch    30 | loss: 66.6988022CurrentTrain: epoch  2, batch    31 | loss: 72.6694444CurrentTrain: epoch  2, batch    32 | loss: 57.3028708CurrentTrain: epoch  2, batch    33 | loss: 67.8492049CurrentTrain: epoch  2, batch    34 | loss: 66.5367854CurrentTrain: epoch  2, batch    35 | loss: 58.1477758CurrentTrain: epoch  2, batch    36 | loss: 55.9860240CurrentTrain: epoch  2, batch    37 | loss: 55.3502897CurrentTrain: epoch  2, batch    38 | loss: 64.7439955CurrentTrain: epoch  2, batch    39 | loss: 82.2783541CurrentTrain: epoch  2, batch    40 | loss: 67.8105307CurrentTrain: epoch  2, batch    41 | loss: 66.7802732CurrentTrain: epoch  2, batch    42 | loss: 65.4580155CurrentTrain: epoch  2, batch    43 | loss: 86.8644520CurrentTrain: epoch  2, batch    44 | loss: 70.7535293CurrentTrain: epoch  2, batch    45 | loss: 58.4954952CurrentTrain: epoch  2, batch    46 | loss: 84.3153434CurrentTrain: epoch  2, batch    47 | loss: 70.4036035CurrentTrain: epoch  2, batch    48 | loss: 89.4137206CurrentTrain: epoch  2, batch    49 | loss: 69.3001925CurrentTrain: epoch  2, batch    50 | loss: 55.0492618CurrentTrain: epoch  2, batch    51 | loss: 87.6085192CurrentTrain: epoch  2, batch    52 | loss: 68.2187538CurrentTrain: epoch  2, batch    53 | loss: 68.9487882CurrentTrain: epoch  2, batch    54 | loss: 46.6084104CurrentTrain: epoch  2, batch    55 | loss: 68.9535378CurrentTrain: epoch  2, batch    56 | loss: 68.1640278CurrentTrain: epoch  2, batch    57 | loss: 65.8331835CurrentTrain: epoch  2, batch    58 | loss: 85.3975454CurrentTrain: epoch  2, batch    59 | loss: 69.8754363CurrentTrain: epoch  2, batch    60 | loss: 68.8117794CurrentTrain: epoch  2, batch    61 | loss: 79.0810047CurrentTrain: epoch  2, batch    62 | loss: 51.8877729CurrentTrain: epoch  2, batch    63 | loss: 91.0136188CurrentTrain: epoch  2, batch    64 | loss: 59.1002667CurrentTrain: epoch  2, batch    65 | loss: 55.2293973CurrentTrain: epoch  2, batch    66 | loss: 55.9433659CurrentTrain: epoch  2, batch    67 | loss: 87.2211419CurrentTrain: epoch  2, batch    68 | loss: 83.4724527CurrentTrain: epoch  2, batch    69 | loss: 68.1686410CurrentTrain: epoch  2, batch    70 | loss: 71.4936257CurrentTrain: epoch  2, batch    71 | loss: 84.8869272CurrentTrain: epoch  2, batch    72 | loss: 67.8505582CurrentTrain: epoch  2, batch    73 | loss: 68.4818246CurrentTrain: epoch  2, batch    74 | loss: 69.4701515CurrentTrain: epoch  2, batch    75 | loss: 68.0938150CurrentTrain: epoch  2, batch    76 | loss: 69.4141687CurrentTrain: epoch  2, batch    77 | loss: 56.2699553CurrentTrain: epoch  2, batch    78 | loss: 63.2330330CurrentTrain: epoch  2, batch    79 | loss: 74.4991575CurrentTrain: epoch  2, batch    80 | loss: 121.3214888CurrentTrain: epoch  2, batch    81 | loss: 53.6316691CurrentTrain: epoch  2, batch    82 | loss: 56.8448240CurrentTrain: epoch  2, batch    83 | loss: 53.7293836CurrentTrain: epoch  2, batch    84 | loss: 54.7329149CurrentTrain: epoch  2, batch    85 | loss: 66.2295778CurrentTrain: epoch  2, batch    86 | loss: 54.7792120CurrentTrain: epoch  2, batch    87 | loss: 84.5736899CurrentTrain: epoch  2, batch    88 | loss: 91.4856356CurrentTrain: epoch  2, batch    89 | loss: 56.9071623CurrentTrain: epoch  2, batch    90 | loss: 54.4007071CurrentTrain: epoch  2, batch    91 | loss: 57.2422867CurrentTrain: epoch  2, batch    92 | loss: 84.3625925CurrentTrain: epoch  2, batch    93 | loss: 87.5514571CurrentTrain: epoch  2, batch    94 | loss: 67.9646726CurrentTrain: epoch  2, batch    95 | loss: 57.0646139CurrentTrain: epoch  3, batch     0 | loss: 82.7919331CurrentTrain: epoch  3, batch     1 | loss: 85.5020485CurrentTrain: epoch  3, batch     2 | loss: 52.4248032CurrentTrain: epoch  3, batch     3 | loss: 54.1675351CurrentTrain: epoch  3, batch     4 | loss: 86.8218108CurrentTrain: epoch  3, batch     5 | loss: 86.4259245CurrentTrain: epoch  3, batch     6 | loss: 53.3016157CurrentTrain: epoch  3, batch     7 | loss: 56.6309289CurrentTrain: epoch  3, batch     8 | loss: 56.8559720CurrentTrain: epoch  3, batch     9 | loss: 50.2898704CurrentTrain: epoch  3, batch    10 | loss: 65.9821152CurrentTrain: epoch  3, batch    11 | loss: 70.0306989CurrentTrain: epoch  3, batch    12 | loss: 117.4992060CurrentTrain: epoch  3, batch    13 | loss: 65.7465839CurrentTrain: epoch  3, batch    14 | loss: 54.9290720CurrentTrain: epoch  3, batch    15 | loss: 68.7796348CurrentTrain: epoch  3, batch    16 | loss: 56.9937685CurrentTrain: epoch  3, batch    17 | loss: 65.3100360CurrentTrain: epoch  3, batch    18 | loss: 69.3230113CurrentTrain: epoch  3, batch    19 | loss: 55.6189535CurrentTrain: epoch  3, batch    20 | loss: 45.1433852CurrentTrain: epoch  3, batch    21 | loss: 89.1505928CurrentTrain: epoch  3, batch    22 | loss: 67.5808402CurrentTrain: epoch  3, batch    23 | loss: 70.0762281CurrentTrain: epoch  3, batch    24 | loss: 84.8798602CurrentTrain: epoch  3, batch    25 | loss: 115.5214503CurrentTrain: epoch  3, batch    26 | loss: 58.9142973CurrentTrain: epoch  3, batch    27 | loss: 66.6129079CurrentTrain: epoch  3, batch    28 | loss: 65.2242611CurrentTrain: epoch  3, batch    29 | loss: 86.8956406CurrentTrain: epoch  3, batch    30 | loss: 86.4113494CurrentTrain: epoch  3, batch    31 | loss: 51.0885331CurrentTrain: epoch  3, batch    32 | loss: 67.9713224CurrentTrain: epoch  3, batch    33 | loss: 66.4094708CurrentTrain: epoch  3, batch    34 | loss: 84.8934424CurrentTrain: epoch  3, batch    35 | loss: 65.9190613CurrentTrain: epoch  3, batch    36 | loss: 121.9812428CurrentTrain: epoch  3, batch    37 | loss: 51.9728941CurrentTrain: epoch  3, batch    38 | loss: 87.0802942CurrentTrain: epoch  3, batch    39 | loss: 53.7614107CurrentTrain: epoch  3, batch    40 | loss: 58.3712809CurrentTrain: epoch  3, batch    41 | loss: 54.6845480CurrentTrain: epoch  3, batch    42 | loss: 63.3615555CurrentTrain: epoch  3, batch    43 | loss: 84.6673461CurrentTrain: epoch  3, batch    44 | loss: 86.5773481CurrentTrain: epoch  3, batch    45 | loss: 64.7303700CurrentTrain: epoch  3, batch    46 | loss: 66.8355186CurrentTrain: epoch  3, batch    47 | loss: 43.8776755CurrentTrain: epoch  3, batch    48 | loss: 64.7017425CurrentTrain: epoch  3, batch    49 | loss: 48.4176119CurrentTrain: epoch  3, batch    50 | loss: 66.3595898CurrentTrain: epoch  3, batch    51 | loss: 68.3489961CurrentTrain: epoch  3, batch    52 | loss: 52.8364704CurrentTrain: epoch  3, batch    53 | loss: 68.4594574CurrentTrain: epoch  3, batch    54 | loss: 70.0234984CurrentTrain: epoch  3, batch    55 | loss: 80.4583609CurrentTrain: epoch  3, batch    56 | loss: 59.8243867CurrentTrain: epoch  3, batch    57 | loss: 118.0750823CurrentTrain: epoch  3, batch    58 | loss: 88.1796807CurrentTrain: epoch  3, batch    59 | loss: 63.4432091CurrentTrain: epoch  3, batch    60 | loss: 119.5571807CurrentTrain: epoch  3, batch    61 | loss: 118.6053399CurrentTrain: epoch  3, batch    62 | loss: 82.2328233CurrentTrain: epoch  3, batch    63 | loss: 53.2490855CurrentTrain: epoch  3, batch    64 | loss: 113.9360151CurrentTrain: epoch  3, batch    65 | loss: 114.7174163CurrentTrain: epoch  3, batch    66 | loss: 52.4960804CurrentTrain: epoch  3, batch    67 | loss: 83.3911007CurrentTrain: epoch  3, batch    68 | loss: 86.6595839CurrentTrain: epoch  3, batch    69 | loss: 88.0013147CurrentTrain: epoch  3, batch    70 | loss: 67.7779440CurrentTrain: epoch  3, batch    71 | loss: 65.7533796CurrentTrain: epoch  3, batch    72 | loss: 55.0086754CurrentTrain: epoch  3, batch    73 | loss: 66.6423450CurrentTrain: epoch  3, batch    74 | loss: 52.1493354CurrentTrain: epoch  3, batch    75 | loss: 117.7061613CurrentTrain: epoch  3, batch    76 | loss: 87.9788139CurrentTrain: epoch  3, batch    77 | loss: 88.6658699CurrentTrain: epoch  3, batch    78 | loss: 54.3374973CurrentTrain: epoch  3, batch    79 | loss: 42.8440118CurrentTrain: epoch  3, batch    80 | loss: 50.8252772CurrentTrain: epoch  3, batch    81 | loss: 119.9018621CurrentTrain: epoch  3, batch    82 | loss: 88.0422374CurrentTrain: epoch  3, batch    83 | loss: 46.3861874CurrentTrain: epoch  3, batch    84 | loss: 120.5291858CurrentTrain: epoch  3, batch    85 | loss: 64.1379895CurrentTrain: epoch  3, batch    86 | loss: 62.7565569CurrentTrain: epoch  3, batch    87 | loss: 88.8906344CurrentTrain: epoch  3, batch    88 | loss: 70.0088808CurrentTrain: epoch  3, batch    89 | loss: 121.0535860CurrentTrain: epoch  3, batch    90 | loss: 65.2692975CurrentTrain: epoch  3, batch    91 | loss: 87.1557798CurrentTrain: epoch  3, batch    92 | loss: 68.0057002CurrentTrain: epoch  3, batch    93 | loss: 81.5807549CurrentTrain: epoch  3, batch    94 | loss: 86.1838998CurrentTrain: epoch  3, batch    95 | loss: 46.9270802CurrentTrain: epoch  4, batch     0 | loss: 64.4181508CurrentTrain: epoch  4, batch     1 | loss: 53.8017192CurrentTrain: epoch  4, batch     2 | loss: 83.8677619CurrentTrain: epoch  4, batch     3 | loss: 58.0094958CurrentTrain: epoch  4, batch     4 | loss: 67.3130898CurrentTrain: epoch  4, batch     5 | loss: 53.8643321CurrentTrain: epoch  4, batch     6 | loss: 86.2132013CurrentTrain: epoch  4, batch     7 | loss: 54.1520667CurrentTrain: epoch  4, batch     8 | loss: 86.3462850CurrentTrain: epoch  4, batch     9 | loss: 44.1534901CurrentTrain: epoch  4, batch    10 | loss: 64.8723220CurrentTrain: epoch  4, batch    11 | loss: 68.4700314CurrentTrain: epoch  4, batch    12 | loss: 67.5532739CurrentTrain: epoch  4, batch    13 | loss: 115.7689170CurrentTrain: epoch  4, batch    14 | loss: 81.0781292CurrentTrain: epoch  4, batch    15 | loss: 52.7791638CurrentTrain: epoch  4, batch    16 | loss: 54.9273119CurrentTrain: epoch  4, batch    17 | loss: 64.3752891CurrentTrain: epoch  4, batch    18 | loss: 117.4700507CurrentTrain: epoch  4, batch    19 | loss: 53.5813326CurrentTrain: epoch  4, batch    20 | loss: 84.9172451CurrentTrain: epoch  4, batch    21 | loss: 86.3737786CurrentTrain: epoch  4, batch    22 | loss: 83.0406735CurrentTrain: epoch  4, batch    23 | loss: 112.1598066CurrentTrain: epoch  4, batch    24 | loss: 51.3473107CurrentTrain: epoch  4, batch    25 | loss: 68.8403107CurrentTrain: epoch  4, batch    26 | loss: 65.2065952CurrentTrain: epoch  4, batch    27 | loss: 54.3905647CurrentTrain: epoch  4, batch    28 | loss: 69.5353395CurrentTrain: epoch  4, batch    29 | loss: 67.9121732CurrentTrain: epoch  4, batch    30 | loss: 64.6431725CurrentTrain: epoch  4, batch    31 | loss: 83.8270039CurrentTrain: epoch  4, batch    32 | loss: 66.5315312CurrentTrain: epoch  4, batch    33 | loss: 41.3036407CurrentTrain: epoch  4, batch    34 | loss: 53.4100664CurrentTrain: epoch  4, batch    35 | loss: 84.6120499CurrentTrain: epoch  4, batch    36 | loss: 65.9291965CurrentTrain: epoch  4, batch    37 | loss: 86.0046993CurrentTrain: epoch  4, batch    38 | loss: 45.2145414CurrentTrain: epoch  4, batch    39 | loss: 85.1238513CurrentTrain: epoch  4, batch    40 | loss: 65.8790748CurrentTrain: epoch  4, batch    41 | loss: 67.6457237CurrentTrain: epoch  4, batch    42 | loss: 49.0949039CurrentTrain: epoch  4, batch    43 | loss: 66.0455227CurrentTrain: epoch  4, batch    44 | loss: 62.5497716CurrentTrain: epoch  4, batch    45 | loss: 65.5243135CurrentTrain: epoch  4, batch    46 | loss: 115.7526517CurrentTrain: epoch  4, batch    47 | loss: 84.6192716CurrentTrain: epoch  4, batch    48 | loss: 85.0510196CurrentTrain: epoch  4, batch    49 | loss: 46.7600354CurrentTrain: epoch  4, batch    50 | loss: 54.7008452CurrentTrain: epoch  4, batch    51 | loss: 66.7181865CurrentTrain: epoch  4, batch    52 | loss: 63.9525586CurrentTrain: epoch  4, batch    53 | loss: 46.3063444CurrentTrain: epoch  4, batch    54 | loss: 51.5753445CurrentTrain: epoch  4, batch    55 | loss: 51.9554588CurrentTrain: epoch  4, batch    56 | loss: 114.4616166CurrentTrain: epoch  4, batch    57 | loss: 63.8199744CurrentTrain: epoch  4, batch    58 | loss: 52.0870454CurrentTrain: epoch  4, batch    59 | loss: 54.0402921CurrentTrain: epoch  4, batch    60 | loss: 84.1000398CurrentTrain: epoch  4, batch    61 | loss: 115.9954439CurrentTrain: epoch  4, batch    62 | loss: 87.3726243CurrentTrain: epoch  4, batch    63 | loss: 65.2024064CurrentTrain: epoch  4, batch    64 | loss: 115.1692732CurrentTrain: epoch  4, batch    65 | loss: 63.2363866CurrentTrain: epoch  4, batch    66 | loss: 65.9353030CurrentTrain: epoch  4, batch    67 | loss: 66.4318406CurrentTrain: epoch  4, batch    68 | loss: 52.0205665CurrentTrain: epoch  4, batch    69 | loss: 62.8536524CurrentTrain: epoch  4, batch    70 | loss: 66.2735216CurrentTrain: epoch  4, batch    71 | loss: 65.6755765CurrentTrain: epoch  4, batch    72 | loss: 65.9302403CurrentTrain: epoch  4, batch    73 | loss: 83.2294333CurrentTrain: epoch  4, batch    74 | loss: 51.3206183CurrentTrain: epoch  4, batch    75 | loss: 67.8367888CurrentTrain: epoch  4, batch    76 | loss: 84.6816564CurrentTrain: epoch  4, batch    77 | loss: 53.6473750CurrentTrain: epoch  4, batch    78 | loss: 177.8450890CurrentTrain: epoch  4, batch    79 | loss: 112.6449756CurrentTrain: epoch  4, batch    80 | loss: 64.9670863CurrentTrain: epoch  4, batch    81 | loss: 67.4646473CurrentTrain: epoch  4, batch    82 | loss: 53.5273837CurrentTrain: epoch  4, batch    83 | loss: 70.2798207CurrentTrain: epoch  4, batch    84 | loss: 55.1054843CurrentTrain: epoch  4, batch    85 | loss: 66.2200064CurrentTrain: epoch  4, batch    86 | loss: 64.6861213CurrentTrain: epoch  4, batch    87 | loss: 83.7485300CurrentTrain: epoch  4, batch    88 | loss: 86.4579420CurrentTrain: epoch  4, batch    89 | loss: 82.5106314CurrentTrain: epoch  4, batch    90 | loss: 118.3091627CurrentTrain: epoch  4, batch    91 | loss: 63.7151164CurrentTrain: epoch  4, batch    92 | loss: 81.5203035CurrentTrain: epoch  4, batch    93 | loss: 87.1652032CurrentTrain: epoch  4, batch    94 | loss: 68.9733580CurrentTrain: epoch  4, batch    95 | loss: 38.5540113CurrentTrain: epoch  5, batch     0 | loss: 65.5875331CurrentTrain: epoch  5, batch     1 | loss: 63.6380548CurrentTrain: epoch  5, batch     2 | loss: 53.3289579CurrentTrain: epoch  5, batch     3 | loss: 65.9434110CurrentTrain: epoch  5, batch     4 | loss: 62.4951391CurrentTrain: epoch  5, batch     5 | loss: 177.7336626CurrentTrain: epoch  5, batch     6 | loss: 53.0152747CurrentTrain: epoch  5, batch     7 | loss: 84.4291435CurrentTrain: epoch  5, batch     8 | loss: 113.3999004CurrentTrain: epoch  5, batch     9 | loss: 69.4914102CurrentTrain: epoch  5, batch    10 | loss: 42.9705514CurrentTrain: epoch  5, batch    11 | loss: 65.7271575CurrentTrain: epoch  5, batch    12 | loss: 81.8810608CurrentTrain: epoch  5, batch    13 | loss: 83.6999584CurrentTrain: epoch  5, batch    14 | loss: 45.4363777CurrentTrain: epoch  5, batch    15 | loss: 64.0192195CurrentTrain: epoch  5, batch    16 | loss: 84.5210030CurrentTrain: epoch  5, batch    17 | loss: 115.6433205CurrentTrain: epoch  5, batch    18 | loss: 113.8872011CurrentTrain: epoch  5, batch    19 | loss: 66.5201288CurrentTrain: epoch  5, batch    20 | loss: 87.4171786CurrentTrain: epoch  5, batch    21 | loss: 62.8381491CurrentTrain: epoch  5, batch    22 | loss: 48.4491787CurrentTrain: epoch  5, batch    23 | loss: 84.6255354CurrentTrain: epoch  5, batch    24 | loss: 66.5136909CurrentTrain: epoch  5, batch    25 | loss: 52.0377273CurrentTrain: epoch  5, batch    26 | loss: 117.7831470CurrentTrain: epoch  5, batch    27 | loss: 84.8267142CurrentTrain: epoch  5, batch    28 | loss: 58.7020491CurrentTrain: epoch  5, batch    29 | loss: 66.2320103CurrentTrain: epoch  5, batch    30 | loss: 54.4590236CurrentTrain: epoch  5, batch    31 | loss: 67.4308668CurrentTrain: epoch  5, batch    32 | loss: 43.2340284CurrentTrain: epoch  5, batch    33 | loss: 84.3163578CurrentTrain: epoch  5, batch    34 | loss: 68.0739187CurrentTrain: epoch  5, batch    35 | loss: 51.1814529CurrentTrain: epoch  5, batch    36 | loss: 44.1396682CurrentTrain: epoch  5, batch    37 | loss: 51.6612521CurrentTrain: epoch  5, batch    38 | loss: 61.9052253CurrentTrain: epoch  5, batch    39 | loss: 53.9014516CurrentTrain: epoch  5, batch    40 | loss: 84.7733709CurrentTrain: epoch  5, batch    41 | loss: 56.4791180CurrentTrain: epoch  5, batch    42 | loss: 54.5535525CurrentTrain: epoch  5, batch    43 | loss: 54.1382714CurrentTrain: epoch  5, batch    44 | loss: 83.9115342CurrentTrain: epoch  5, batch    45 | loss: 66.4092753CurrentTrain: epoch  5, batch    46 | loss: 84.5424270CurrentTrain: epoch  5, batch    47 | loss: 53.0901485CurrentTrain: epoch  5, batch    48 | loss: 65.3719065CurrentTrain: epoch  5, batch    49 | loss: 178.0608718CurrentTrain: epoch  5, batch    50 | loss: 68.4344246CurrentTrain: epoch  5, batch    51 | loss: 43.2912998CurrentTrain: epoch  5, batch    52 | loss: 67.0571107CurrentTrain: epoch  5, batch    53 | loss: 55.4320160CurrentTrain: epoch  5, batch    54 | loss: 52.5247177CurrentTrain: epoch  5, batch    55 | loss: 69.5344696CurrentTrain: epoch  5, batch    56 | loss: 83.2908980CurrentTrain: epoch  5, batch    57 | loss: 53.6796479CurrentTrain: epoch  5, batch    58 | loss: 64.3514760CurrentTrain: epoch  5, batch    59 | loss: 118.1902621CurrentTrain: epoch  5, batch    60 | loss: 50.5761006CurrentTrain: epoch  5, batch    61 | loss: 85.0235475CurrentTrain: epoch  5, batch    62 | loss: 87.3612214CurrentTrain: epoch  5, batch    63 | loss: 112.0312907CurrentTrain: epoch  5, batch    64 | loss: 117.5979944CurrentTrain: epoch  5, batch    65 | loss: 63.7632586CurrentTrain: epoch  5, batch    66 | loss: 115.3920004CurrentTrain: epoch  5, batch    67 | loss: 81.3257567CurrentTrain: epoch  5, batch    68 | loss: 52.2362714CurrentTrain: epoch  5, batch    69 | loss: 81.4495719CurrentTrain: epoch  5, batch    70 | loss: 64.3606029CurrentTrain: epoch  5, batch    71 | loss: 54.6099525CurrentTrain: epoch  5, batch    72 | loss: 87.9447146CurrentTrain: epoch  5, batch    73 | loss: 83.3957793CurrentTrain: epoch  5, batch    74 | loss: 55.2288175CurrentTrain: epoch  5, batch    75 | loss: 84.9734765CurrentTrain: epoch  5, batch    76 | loss: 66.0897961CurrentTrain: epoch  5, batch    77 | loss: 83.5337977CurrentTrain: epoch  5, batch    78 | loss: 41.6551812CurrentTrain: epoch  5, batch    79 | loss: 66.3474395CurrentTrain: epoch  5, batch    80 | loss: 82.3831348CurrentTrain: epoch  5, batch    81 | loss: 51.3345887CurrentTrain: epoch  5, batch    82 | loss: 82.8162565CurrentTrain: epoch  5, batch    83 | loss: 69.0059698CurrentTrain: epoch  5, batch    84 | loss: 79.1326653CurrentTrain: epoch  5, batch    85 | loss: 181.7978649CurrentTrain: epoch  5, batch    86 | loss: 54.9047018CurrentTrain: epoch  5, batch    87 | loss: 66.3547138CurrentTrain: epoch  5, batch    88 | loss: 67.0854461CurrentTrain: epoch  5, batch    89 | loss: 66.6006250CurrentTrain: epoch  5, batch    90 | loss: 43.1598565CurrentTrain: epoch  5, batch    91 | loss: 64.8481047CurrentTrain: epoch  5, batch    92 | loss: 53.4372726CurrentTrain: epoch  5, batch    93 | loss: 52.8656832CurrentTrain: epoch  5, batch    94 | loss: 63.1225990CurrentTrain: epoch  5, batch    95 | loss: 54.7215458CurrentTrain: epoch  6, batch     0 | loss: 52.8142906CurrentTrain: epoch  6, batch     1 | loss: 67.1889966CurrentTrain: epoch  6, batch     2 | loss: 65.7287615CurrentTrain: epoch  6, batch     3 | loss: 113.0216388CurrentTrain: epoch  6, batch     4 | loss: 53.8474701CurrentTrain: epoch  6, batch     5 | loss: 67.9686985CurrentTrain: epoch  6, batch     6 | loss: 53.0398511CurrentTrain: epoch  6, batch     7 | loss: 50.5194945CurrentTrain: epoch  6, batch     8 | loss: 51.5866085CurrentTrain: epoch  6, batch     9 | loss: 44.0384601CurrentTrain: epoch  6, batch    10 | loss: 84.0764204CurrentTrain: epoch  6, batch    11 | loss: 66.9258250CurrentTrain: epoch  6, batch    12 | loss: 85.8994161CurrentTrain: epoch  6, batch    13 | loss: 54.5979164CurrentTrain: epoch  6, batch    14 | loss: 43.8673973CurrentTrain: epoch  6, batch    15 | loss: 51.6782871CurrentTrain: epoch  6, batch    16 | loss: 80.5204672CurrentTrain: epoch  6, batch    17 | loss: 49.2904239CurrentTrain: epoch  6, batch    18 | loss: 62.8501403CurrentTrain: epoch  6, batch    19 | loss: 64.5029676CurrentTrain: epoch  6, batch    20 | loss: 43.6711840CurrentTrain: epoch  6, batch    21 | loss: 49.6130454CurrentTrain: epoch  6, batch    22 | loss: 111.5035903CurrentTrain: epoch  6, batch    23 | loss: 64.9133111CurrentTrain: epoch  6, batch    24 | loss: 64.4849676CurrentTrain: epoch  6, batch    25 | loss: 63.9908702CurrentTrain: epoch  6, batch    26 | loss: 67.5729287CurrentTrain: epoch  6, batch    27 | loss: 83.0877643CurrentTrain: epoch  6, batch    28 | loss: 51.9826438CurrentTrain: epoch  6, batch    29 | loss: 51.5613167CurrentTrain: epoch  6, batch    30 | loss: 51.9182627CurrentTrain: epoch  6, batch    31 | loss: 44.6735319CurrentTrain: epoch  6, batch    32 | loss: 43.9649030CurrentTrain: epoch  6, batch    33 | loss: 50.4226932CurrentTrain: epoch  6, batch    34 | loss: 63.9840055CurrentTrain: epoch  6, batch    35 | loss: 81.4507840CurrentTrain: epoch  6, batch    36 | loss: 84.6958796CurrentTrain: epoch  6, batch    37 | loss: 52.3902324CurrentTrain: epoch  6, batch    38 | loss: 50.2244335CurrentTrain: epoch  6, batch    39 | loss: 118.7677994CurrentTrain: epoch  6, batch    40 | loss: 63.9025738CurrentTrain: epoch  6, batch    41 | loss: 65.0827208CurrentTrain: epoch  6, batch    42 | loss: 52.1714049CurrentTrain: epoch  6, batch    43 | loss: 82.4537071CurrentTrain: epoch  6, batch    44 | loss: 54.6480801CurrentTrain: epoch  6, batch    45 | loss: 63.6908378CurrentTrain: epoch  6, batch    46 | loss: 50.3312103CurrentTrain: epoch  6, batch    47 | loss: 65.7955035CurrentTrain: epoch  6, batch    48 | loss: 84.1168381CurrentTrain: epoch  6, batch    49 | loss: 51.8954874CurrentTrain: epoch  6, batch    50 | loss: 84.4004607CurrentTrain: epoch  6, batch    51 | loss: 66.4031637CurrentTrain: epoch  6, batch    52 | loss: 82.4419170CurrentTrain: epoch  6, batch    53 | loss: 84.4160473CurrentTrain: epoch  6, batch    54 | loss: 53.0757280CurrentTrain: epoch  6, batch    55 | loss: 64.4929994CurrentTrain: epoch  6, batch    56 | loss: 62.6880899CurrentTrain: epoch  6, batch    57 | loss: 84.1427240CurrentTrain: epoch  6, batch    58 | loss: 117.6400960CurrentTrain: epoch  6, batch    59 | loss: 54.6163876CurrentTrain: epoch  6, batch    60 | loss: 51.5240948CurrentTrain: epoch  6, batch    61 | loss: 85.7828135CurrentTrain: epoch  6, batch    62 | loss: 43.2574636CurrentTrain: epoch  6, batch    63 | loss: 54.0935279CurrentTrain: epoch  6, batch    64 | loss: 51.3025237CurrentTrain: epoch  6, batch    65 | loss: 67.7288177CurrentTrain: epoch  6, batch    66 | loss: 50.5332578CurrentTrain: epoch  6, batch    67 | loss: 118.0145931CurrentTrain: epoch  6, batch    68 | loss: 66.7945628CurrentTrain: epoch  6, batch    69 | loss: 65.6079219CurrentTrain: epoch  6, batch    70 | loss: 39.3985846CurrentTrain: epoch  6, batch    71 | loss: 81.9879595CurrentTrain: epoch  6, batch    72 | loss: 82.6805163CurrentTrain: epoch  6, batch    73 | loss: 50.9494215CurrentTrain: epoch  6, batch    74 | loss: 67.3220885CurrentTrain: epoch  6, batch    75 | loss: 89.5738900CurrentTrain: epoch  6, batch    76 | loss: 109.6398082CurrentTrain: epoch  6, batch    77 | loss: 52.3622160CurrentTrain: epoch  6, batch    78 | loss: 85.9132407CurrentTrain: epoch  6, batch    79 | loss: 69.9797244CurrentTrain: epoch  6, batch    80 | loss: 50.1744423CurrentTrain: epoch  6, batch    81 | loss: 52.4873465CurrentTrain: epoch  6, batch    82 | loss: 84.8647630CurrentTrain: epoch  6, batch    83 | loss: 53.2895101CurrentTrain: epoch  6, batch    84 | loss: 65.8933408CurrentTrain: epoch  6, batch    85 | loss: 82.5997316CurrentTrain: epoch  6, batch    86 | loss: 117.8194999CurrentTrain: epoch  6, batch    87 | loss: 117.5225679CurrentTrain: epoch  6, batch    88 | loss: 81.0683349CurrentTrain: epoch  6, batch    89 | loss: 84.7635312CurrentTrain: epoch  6, batch    90 | loss: 65.3736984CurrentTrain: epoch  6, batch    91 | loss: 54.0673065CurrentTrain: epoch  6, batch    92 | loss: 86.8311983CurrentTrain: epoch  6, batch    93 | loss: 175.0930235CurrentTrain: epoch  6, batch    94 | loss: 54.9993408CurrentTrain: epoch  6, batch    95 | loss: 71.9401571CurrentTrain: epoch  7, batch     0 | loss: 120.5251814CurrentTrain: epoch  7, batch     1 | loss: 50.6547482CurrentTrain: epoch  7, batch     2 | loss: 59.6721449CurrentTrain: epoch  7, batch     3 | loss: 114.2232533CurrentTrain: epoch  7, batch     4 | loss: 44.5035229CurrentTrain: epoch  7, batch     5 | loss: 50.6476253CurrentTrain: epoch  7, batch     6 | loss: 67.0723569CurrentTrain: epoch  7, batch     7 | loss: 65.5192456CurrentTrain: epoch  7, batch     8 | loss: 115.2869990CurrentTrain: epoch  7, batch     9 | loss: 55.4772091CurrentTrain: epoch  7, batch    10 | loss: 44.1540997CurrentTrain: epoch  7, batch    11 | loss: 117.5646248CurrentTrain: epoch  7, batch    12 | loss: 63.1340989CurrentTrain: epoch  7, batch    13 | loss: 85.6084046CurrentTrain: epoch  7, batch    14 | loss: 53.2690031CurrentTrain: epoch  7, batch    15 | loss: 61.6988954CurrentTrain: epoch  7, batch    16 | loss: 86.1812252CurrentTrain: epoch  7, batch    17 | loss: 86.4841061CurrentTrain: epoch  7, batch    18 | loss: 51.1733821CurrentTrain: epoch  7, batch    19 | loss: 62.3335961CurrentTrain: epoch  7, batch    20 | loss: 57.5295817CurrentTrain: epoch  7, batch    21 | loss: 67.1206717CurrentTrain: epoch  7, batch    22 | loss: 67.2289619CurrentTrain: epoch  7, batch    23 | loss: 66.8291761CurrentTrain: epoch  7, batch    24 | loss: 47.8431326CurrentTrain: epoch  7, batch    25 | loss: 62.9806025CurrentTrain: epoch  7, batch    26 | loss: 52.3957858CurrentTrain: epoch  7, batch    27 | loss: 51.0216790CurrentTrain: epoch  7, batch    28 | loss: 53.3159604CurrentTrain: epoch  7, batch    29 | loss: 53.5115085CurrentTrain: epoch  7, batch    30 | loss: 65.6731188CurrentTrain: epoch  7, batch    31 | loss: 62.9258582CurrentTrain: epoch  7, batch    32 | loss: 82.9493934CurrentTrain: epoch  7, batch    33 | loss: 63.9532817CurrentTrain: epoch  7, batch    34 | loss: 77.2173902CurrentTrain: epoch  7, batch    35 | loss: 85.4294881CurrentTrain: epoch  7, batch    36 | loss: 63.1795898CurrentTrain: epoch  7, batch    37 | loss: 66.8979514CurrentTrain: epoch  7, batch    38 | loss: 113.1827148CurrentTrain: epoch  7, batch    39 | loss: 83.0201850CurrentTrain: epoch  7, batch    40 | loss: 51.0665333CurrentTrain: epoch  7, batch    41 | loss: 80.0058173CurrentTrain: epoch  7, batch    42 | loss: 42.7471687CurrentTrain: epoch  7, batch    43 | loss: 48.6511168CurrentTrain: epoch  7, batch    44 | loss: 115.3802955CurrentTrain: epoch  7, batch    45 | loss: 64.1081576CurrentTrain: epoch  7, batch    46 | loss: 60.5911594CurrentTrain: epoch  7, batch    47 | loss: 117.4719245CurrentTrain: epoch  7, batch    48 | loss: 52.2174382CurrentTrain: epoch  7, batch    49 | loss: 81.4164493CurrentTrain: epoch  7, batch    50 | loss: 61.8609695CurrentTrain: epoch  7, batch    51 | loss: 54.1014079CurrentTrain: epoch  7, batch    52 | loss: 66.9562764CurrentTrain: epoch  7, batch    53 | loss: 50.7161782CurrentTrain: epoch  7, batch    54 | loss: 82.1285703CurrentTrain: epoch  7, batch    55 | loss: 64.9314784CurrentTrain: epoch  7, batch    56 | loss: 60.8900219CurrentTrain: epoch  7, batch    57 | loss: 51.5357399CurrentTrain: epoch  7, batch    58 | loss: 64.2404288CurrentTrain: epoch  7, batch    59 | loss: 51.4129441CurrentTrain: epoch  7, batch    60 | loss: 181.5119692CurrentTrain: epoch  7, batch    61 | loss: 53.4288526CurrentTrain: epoch  7, batch    62 | loss: 45.0365801CurrentTrain: epoch  7, batch    63 | loss: 79.8866867CurrentTrain: epoch  7, batch    64 | loss: 62.5595306CurrentTrain: epoch  7, batch    65 | loss: 67.8133230CurrentTrain: epoch  7, batch    66 | loss: 42.8424680CurrentTrain: epoch  7, batch    67 | loss: 84.9468624CurrentTrain: epoch  7, batch    68 | loss: 52.0185165CurrentTrain: epoch  7, batch    69 | loss: 86.4525376CurrentTrain: epoch  7, batch    70 | loss: 67.5795019CurrentTrain: epoch  7, batch    71 | loss: 114.6013421CurrentTrain: epoch  7, batch    72 | loss: 372.4611510CurrentTrain: epoch  7, batch    73 | loss: 65.6065303CurrentTrain: epoch  7, batch    74 | loss: 80.8898694CurrentTrain: epoch  7, batch    75 | loss: 64.8040481CurrentTrain: epoch  7, batch    76 | loss: 87.2843934CurrentTrain: epoch  7, batch    77 | loss: 57.9021521CurrentTrain: epoch  7, batch    78 | loss: 84.0677758CurrentTrain: epoch  7, batch    79 | loss: 110.4250065CurrentTrain: epoch  7, batch    80 | loss: 111.7320783CurrentTrain: epoch  7, batch    81 | loss: 65.3877895CurrentTrain: epoch  7, batch    82 | loss: 113.5334739CurrentTrain: epoch  7, batch    83 | loss: 64.4603083CurrentTrain: epoch  7, batch    84 | loss: 50.9434255CurrentTrain: epoch  7, batch    85 | loss: 66.1062874CurrentTrain: epoch  7, batch    86 | loss: 63.4368300CurrentTrain: epoch  7, batch    87 | loss: 115.6836717CurrentTrain: epoch  7, batch    88 | loss: 174.8997778CurrentTrain: epoch  7, batch    89 | loss: 58.7821694CurrentTrain: epoch  7, batch    90 | loss: 66.7167865CurrentTrain: epoch  7, batch    91 | loss: 64.0850353CurrentTrain: epoch  7, batch    92 | loss: 65.7105099CurrentTrain: epoch  7, batch    93 | loss: 63.0489076CurrentTrain: epoch  7, batch    94 | loss: 66.7259359CurrentTrain: epoch  7, batch    95 | loss: 41.4098688CurrentTrain: epoch  8, batch     0 | loss: 81.1927444CurrentTrain: epoch  8, batch     1 | loss: 48.0977467CurrentTrain: epoch  8, batch     2 | loss: 43.8146580CurrentTrain: epoch  8, batch     3 | loss: 78.3775893CurrentTrain: epoch  8, batch     4 | loss: 44.1967105CurrentTrain: epoch  8, batch     5 | loss: 117.5777249CurrentTrain: epoch  8, batch     6 | loss: 50.1431604CurrentTrain: epoch  8, batch     7 | loss: 67.2870374CurrentTrain: epoch  8, batch     8 | loss: 52.3861432CurrentTrain: epoch  8, batch     9 | loss: 82.3251952CurrentTrain: epoch  8, batch    10 | loss: 49.9373705CurrentTrain: epoch  8, batch    11 | loss: 64.6739548CurrentTrain: epoch  8, batch    12 | loss: 50.3492947CurrentTrain: epoch  8, batch    13 | loss: 112.4528019CurrentTrain: epoch  8, batch    14 | loss: 67.3555273CurrentTrain: epoch  8, batch    15 | loss: 67.6541980CurrentTrain: epoch  8, batch    16 | loss: 64.2001130CurrentTrain: epoch  8, batch    17 | loss: 42.9606782CurrentTrain: epoch  8, batch    18 | loss: 54.7594820CurrentTrain: epoch  8, batch    19 | loss: 64.3377034CurrentTrain: epoch  8, batch    20 | loss: 82.7971077CurrentTrain: epoch  8, batch    21 | loss: 84.1469955CurrentTrain: epoch  8, batch    22 | loss: 63.4439172CurrentTrain: epoch  8, batch    23 | loss: 66.1204392CurrentTrain: epoch  8, batch    24 | loss: 50.5515353CurrentTrain: epoch  8, batch    25 | loss: 84.1400388CurrentTrain: epoch  8, batch    26 | loss: 69.4059431CurrentTrain: epoch  8, batch    27 | loss: 62.9629813CurrentTrain: epoch  8, batch    28 | loss: 53.0468435CurrentTrain: epoch  8, batch    29 | loss: 64.6307901CurrentTrain: epoch  8, batch    30 | loss: 62.0762064CurrentTrain: epoch  8, batch    31 | loss: 84.4864787CurrentTrain: epoch  8, batch    32 | loss: 46.7633235CurrentTrain: epoch  8, batch    33 | loss: 61.4824320CurrentTrain: epoch  8, batch    34 | loss: 87.7087855CurrentTrain: epoch  8, batch    35 | loss: 65.3728185CurrentTrain: epoch  8, batch    36 | loss: 62.1821934CurrentTrain: epoch  8, batch    37 | loss: 63.3112133CurrentTrain: epoch  8, batch    38 | loss: 55.7904735CurrentTrain: epoch  8, batch    39 | loss: 64.1881665CurrentTrain: epoch  8, batch    40 | loss: 115.2867644CurrentTrain: epoch  8, batch    41 | loss: 64.4321253CurrentTrain: epoch  8, batch    42 | loss: 67.6399849CurrentTrain: epoch  8, batch    43 | loss: 64.1162940CurrentTrain: epoch  8, batch    44 | loss: 80.9523353CurrentTrain: epoch  8, batch    45 | loss: 81.1512574CurrentTrain: epoch  8, batch    46 | loss: 113.3401829CurrentTrain: epoch  8, batch    47 | loss: 60.6497872CurrentTrain: epoch  8, batch    48 | loss: 54.6953922CurrentTrain: epoch  8, batch    49 | loss: 66.9166820CurrentTrain: epoch  8, batch    50 | loss: 40.0455345CurrentTrain: epoch  8, batch    51 | loss: 85.1197839CurrentTrain: epoch  8, batch    52 | loss: 51.9560936CurrentTrain: epoch  8, batch    53 | loss: 51.1876765CurrentTrain: epoch  8, batch    54 | loss: 41.5783968CurrentTrain: epoch  8, batch    55 | loss: 181.6918668CurrentTrain: epoch  8, batch    56 | loss: 117.5125394CurrentTrain: epoch  8, batch    57 | loss: 181.4972200CurrentTrain: epoch  8, batch    58 | loss: 84.0413630CurrentTrain: epoch  8, batch    59 | loss: 85.3206180CurrentTrain: epoch  8, batch    60 | loss: 111.6812259CurrentTrain: epoch  8, batch    61 | loss: 43.6804726CurrentTrain: epoch  8, batch    62 | loss: 181.4607232CurrentTrain: epoch  8, batch    63 | loss: 78.4724278CurrentTrain: epoch  8, batch    64 | loss: 57.3244124CurrentTrain: epoch  8, batch    65 | loss: 66.0709441CurrentTrain: epoch  8, batch    66 | loss: 114.1933624CurrentTrain: epoch  8, batch    67 | loss: 64.3669761CurrentTrain: epoch  8, batch    68 | loss: 52.4140544CurrentTrain: epoch  8, batch    69 | loss: 115.3671869CurrentTrain: epoch  8, batch    70 | loss: 64.1747474CurrentTrain: epoch  8, batch    71 | loss: 50.3593190CurrentTrain: epoch  8, batch    72 | loss: 65.8309350CurrentTrain: epoch  8, batch    73 | loss: 62.7807444CurrentTrain: epoch  8, batch    74 | loss: 85.1346765CurrentTrain: epoch  8, batch    75 | loss: 86.8668677CurrentTrain: epoch  8, batch    76 | loss: 113.3547845CurrentTrain: epoch  8, batch    77 | loss: 79.9236679CurrentTrain: epoch  8, batch    78 | loss: 79.7011741CurrentTrain: epoch  8, batch    79 | loss: 64.1182462CurrentTrain: epoch  8, batch    80 | loss: 52.3112154CurrentTrain: epoch  8, batch    81 | loss: 50.6572132CurrentTrain: epoch  8, batch    82 | loss: 63.2002343CurrentTrain: epoch  8, batch    83 | loss: 53.4046445CurrentTrain: epoch  8, batch    84 | loss: 51.2866623CurrentTrain: epoch  8, batch    85 | loss: 81.0903633CurrentTrain: epoch  8, batch    86 | loss: 54.4038166CurrentTrain: epoch  8, batch    87 | loss: 81.9169952CurrentTrain: epoch  8, batch    88 | loss: 50.6215061CurrentTrain: epoch  8, batch    89 | loss: 82.5846408CurrentTrain: epoch  8, batch    90 | loss: 65.7563443CurrentTrain: epoch  8, batch    91 | loss: 64.4903026CurrentTrain: epoch  8, batch    92 | loss: 63.9991494CurrentTrain: epoch  8, batch    93 | loss: 64.5128406CurrentTrain: epoch  8, batch    94 | loss: 115.2124429CurrentTrain: epoch  8, batch    95 | loss: 96.2538064CurrentTrain: epoch  9, batch     0 | loss: 64.5635032CurrentTrain: epoch  9, batch     1 | loss: 51.8684438CurrentTrain: epoch  9, batch     2 | loss: 85.7342520CurrentTrain: epoch  9, batch     3 | loss: 50.5351966CurrentTrain: epoch  9, batch     4 | loss: 82.7089045CurrentTrain: epoch  9, batch     5 | loss: 82.6192623CurrentTrain: epoch  9, batch     6 | loss: 113.2696262CurrentTrain: epoch  9, batch     7 | loss: 117.4812327CurrentTrain: epoch  9, batch     8 | loss: 117.6555552CurrentTrain: epoch  9, batch     9 | loss: 83.4208517CurrentTrain: epoch  9, batch    10 | loss: 66.8084419CurrentTrain: epoch  9, batch    11 | loss: 49.9736730CurrentTrain: epoch  9, batch    12 | loss: 84.6359432CurrentTrain: epoch  9, batch    13 | loss: 54.0246291CurrentTrain: epoch  9, batch    14 | loss: 66.9295049CurrentTrain: epoch  9, batch    15 | loss: 64.5122925CurrentTrain: epoch  9, batch    16 | loss: 81.7666662CurrentTrain: epoch  9, batch    17 | loss: 62.8562287CurrentTrain: epoch  9, batch    18 | loss: 62.0008067CurrentTrain: epoch  9, batch    19 | loss: 50.8207785CurrentTrain: epoch  9, batch    20 | loss: 51.5069665CurrentTrain: epoch  9, batch    21 | loss: 115.1837068CurrentTrain: epoch  9, batch    22 | loss: 49.3448697CurrentTrain: epoch  9, batch    23 | loss: 63.7392599CurrentTrain: epoch  9, batch    24 | loss: 48.5062673CurrentTrain: epoch  9, batch    25 | loss: 65.8056756CurrentTrain: epoch  9, batch    26 | loss: 63.8622163CurrentTrain: epoch  9, batch    27 | loss: 82.7679428CurrentTrain: epoch  9, batch    28 | loss: 115.2174981CurrentTrain: epoch  9, batch    29 | loss: 54.1263455CurrentTrain: epoch  9, batch    30 | loss: 64.0681219CurrentTrain: epoch  9, batch    31 | loss: 63.5548856CurrentTrain: epoch  9, batch    32 | loss: 50.9572033CurrentTrain: epoch  9, batch    33 | loss: 42.5784509CurrentTrain: epoch  9, batch    34 | loss: 113.8681507CurrentTrain: epoch  9, batch    35 | loss: 83.2741900CurrentTrain: epoch  9, batch    36 | loss: 49.8224153CurrentTrain: epoch  9, batch    37 | loss: 83.6301561CurrentTrain: epoch  9, batch    38 | loss: 115.2125386CurrentTrain: epoch  9, batch    39 | loss: 65.8361728CurrentTrain: epoch  9, batch    40 | loss: 59.3555762CurrentTrain: epoch  9, batch    41 | loss: 66.0478714CurrentTrain: epoch  9, batch    42 | loss: 83.1980315CurrentTrain: epoch  9, batch    43 | loss: 52.9603383CurrentTrain: epoch  9, batch    44 | loss: 82.3558705CurrentTrain: epoch  9, batch    45 | loss: 84.1562303CurrentTrain: epoch  9, batch    46 | loss: 177.5947882CurrentTrain: epoch  9, batch    47 | loss: 63.4277141CurrentTrain: epoch  9, batch    48 | loss: 67.3455585CurrentTrain: epoch  9, batch    49 | loss: 64.1422027CurrentTrain: epoch  9, batch    50 | loss: 84.0370731CurrentTrain: epoch  9, batch    51 | loss: 66.0778782CurrentTrain: epoch  9, batch    52 | loss: 119.2034738CurrentTrain: epoch  9, batch    53 | loss: 63.3797738CurrentTrain: epoch  9, batch    54 | loss: 62.1240467CurrentTrain: epoch  9, batch    55 | loss: 65.9914914CurrentTrain: epoch  9, batch    56 | loss: 52.9655080CurrentTrain: epoch  9, batch    57 | loss: 50.0488935CurrentTrain: epoch  9, batch    58 | loss: 85.7216973CurrentTrain: epoch  9, batch    59 | loss: 84.1744644CurrentTrain: epoch  9, batch    60 | loss: 48.9256396CurrentTrain: epoch  9, batch    61 | loss: 43.8517205CurrentTrain: epoch  9, batch    62 | loss: 86.9775751CurrentTrain: epoch  9, batch    63 | loss: 87.1993568CurrentTrain: epoch  9, batch    64 | loss: 50.1496852CurrentTrain: epoch  9, batch    65 | loss: 63.3739366CurrentTrain: epoch  9, batch    66 | loss: 79.9998837CurrentTrain: epoch  9, batch    67 | loss: 49.6199554CurrentTrain: epoch  9, batch    68 | loss: 67.6197217CurrentTrain: epoch  9, batch    69 | loss: 51.9578797CurrentTrain: epoch  9, batch    70 | loss: 86.6105004CurrentTrain: epoch  9, batch    71 | loss: 50.3119162CurrentTrain: epoch  9, batch    72 | loss: 82.4275056CurrentTrain: epoch  9, batch    73 | loss: 59.4111677CurrentTrain: epoch  9, batch    74 | loss: 64.3651046CurrentTrain: epoch  9, batch    75 | loss: 53.0749278CurrentTrain: epoch  9, batch    76 | loss: 62.9487442CurrentTrain: epoch  9, batch    77 | loss: 48.4622814CurrentTrain: epoch  9, batch    78 | loss: 63.5012571CurrentTrain: epoch  9, batch    79 | loss: 82.7054140CurrentTrain: epoch  9, batch    80 | loss: 62.4663737CurrentTrain: epoch  9, batch    81 | loss: 82.3873777CurrentTrain: epoch  9, batch    82 | loss: 53.1918139CurrentTrain: epoch  9, batch    83 | loss: 65.5113500CurrentTrain: epoch  9, batch    84 | loss: 53.1745800CurrentTrain: epoch  9, batch    85 | loss: 50.8764009CurrentTrain: epoch  9, batch    86 | loss: 64.2218001CurrentTrain: epoch  9, batch    87 | loss: 50.9548279CurrentTrain: epoch  9, batch    88 | loss: 82.7263832CurrentTrain: epoch  9, batch    89 | loss: 64.5898331CurrentTrain: epoch  9, batch    90 | loss: 53.4741781CurrentTrain: epoch  9, batch    91 | loss: 65.3584247CurrentTrain: epoch  9, batch    92 | loss: 64.0923130CurrentTrain: epoch  9, batch    93 | loss: 62.6979552CurrentTrain: epoch  9, batch    94 | loss: 64.6637308CurrentTrain: epoch  9, batch    95 | loss: 52.9021633

F1 score per class: {32: 0.6666666666666666, 6: 0.8505747126436781, 19: 0.4, 24: 0.7659574468085106, 26: 0.9247311827956989, 29: 0.8865979381443299}
Micro-average F1 score: 0.808870116156283
Weighted-average F1 score: 0.8143291498697238
F1 score per class: {32: 0.7589743589743589, 6: 0.8636363636363636, 19: 0.5, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8934010152284264}
Micro-average F1 score: 0.8404907975460123
Weighted-average F1 score: 0.8432518805225842
F1 score per class: {32: 0.7589743589743589, 6: 0.8636363636363636, 19: 0.5, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8934010152284264}
Micro-average F1 score: 0.8404907975460123
Weighted-average F1 score: 0.8432518805225842

F1 score per class: {32: 0.6666666666666666, 6: 0.8505747126436781, 19: 0.4, 24: 0.7659574468085106, 26: 0.9247311827956989, 29: 0.8865979381443299}
Micro-average F1 score: 0.808870116156283
Weighted-average F1 score: 0.8143291498697238
F1 score per class: {32: 0.7589743589743589, 6: 0.8636363636363636, 19: 0.5, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8934010152284264}
Micro-average F1 score: 0.8404907975460123
Weighted-average F1 score: 0.8432518805225842
F1 score per class: {32: 0.7589743589743589, 6: 0.8636363636363636, 19: 0.5, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8934010152284264}
Micro-average F1 score: 0.8404907975460123
Weighted-average F1 score: 0.8432518805225842

F1 score per class: {32: 0.5042016806722689, 6: 0.6636771300448431, 19: 0.19230769230769232, 24: 0.7164179104477612, 26: 0.8958333333333334, 29: 0.819047619047619}
Micro-average F1 score: 0.6863799283154122
Weighted-average F1 score: 0.6732787528158511
F1 score per class: {32: 0.4774193548387097, 6: 0.6468085106382979, 19: 0.13861386138613863, 24: 0.6666666666666666, 26: 0.9215686274509803, 29: 0.7426160337552743}
Micro-average F1 score: 0.6308518802762855
Weighted-average F1 score: 0.6038178740892441
F1 score per class: {32: 0.4774193548387097, 6: 0.6468085106382979, 19: 0.14, 24: 0.6666666666666666, 26: 0.9215686274509803, 29: 0.7426160337552743}
Micro-average F1 score: 0.631336405529954
Weighted-average F1 score: 0.6045562253380192

F1 score per class: {32: 0.5042016806722689, 6: 0.6636771300448431, 19: 0.19230769230769232, 24: 0.7164179104477612, 26: 0.8958333333333334, 29: 0.819047619047619}
Micro-average F1 score: 0.6863799283154122
Weighted-average F1 score: 0.6732787528158511
F1 score per class: {32: 0.4774193548387097, 6: 0.6468085106382979, 19: 0.13861386138613863, 24: 0.6666666666666666, 26: 0.9215686274509803, 29: 0.7426160337552743}
Micro-average F1 score: 0.6308518802762855
Weighted-average F1 score: 0.6038178740892441
F1 score per class: {32: 0.4774193548387097, 6: 0.6468085106382979, 19: 0.14, 24: 0.6666666666666666, 26: 0.9215686274509803, 29: 0.7426160337552743}
Micro-average F1 score: 0.631336405529954
Weighted-average F1 score: 0.6045562253380192
cur_acc_wo_na:  ['0.8089']
his_acc_wo_na:  ['0.8089']
cur_acc des_wo_na:  ['0.8405']
his_acc des_wo_na:  ['0.8405']
cur_acc rrf_wo_na:  ['0.8405']
his_acc rrf_wo_na:  ['0.8405']
cur_acc_w_na:  ['0.6864']
his_acc_w_na:  ['0.6864']
cur_acc des_w_na:  ['0.6309']
his_acc des_w_na:  ['0.6309']
cur_acc rrf_w_na:  ['0.6313']
his_acc rrf_w_na:  ['0.6313']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 79.9531878CurrentTrain: epoch  0, batch     1 | loss: 67.1421651CurrentTrain: epoch  0, batch     2 | loss: 78.5911824CurrentTrain: epoch  0, batch     3 | loss: 43.0704616CurrentTrain: epoch  1, batch     0 | loss: 75.7424993CurrentTrain: epoch  1, batch     1 | loss: 94.2557405CurrentTrain: epoch  1, batch     2 | loss: 75.4708478CurrentTrain: epoch  1, batch     3 | loss: 56.0748984CurrentTrain: epoch  2, batch     0 | loss: 62.3925944CurrentTrain: epoch  2, batch     1 | loss: 70.2089076CurrentTrain: epoch  2, batch     2 | loss: 75.8957382CurrentTrain: epoch  2, batch     3 | loss: 55.6842661CurrentTrain: epoch  3, batch     0 | loss: 59.5407933CurrentTrain: epoch  3, batch     1 | loss: 61.4338057CurrentTrain: epoch  3, batch     2 | loss: 60.6084681CurrentTrain: epoch  3, batch     3 | loss: 109.2229216CurrentTrain: epoch  4, batch     0 | loss: 72.2009174CurrentTrain: epoch  4, batch     1 | loss: 89.4978437CurrentTrain: epoch  4, batch     2 | loss: 56.1096692CurrentTrain: epoch  4, batch     3 | loss: 53.6881397CurrentTrain: epoch  5, batch     0 | loss: 56.2116628CurrentTrain: epoch  5, batch     1 | loss: 69.7638150CurrentTrain: epoch  5, batch     2 | loss: 68.3124275CurrentTrain: epoch  5, batch     3 | loss: 42.7589302CurrentTrain: epoch  6, batch     0 | loss: 68.9041575CurrentTrain: epoch  6, batch     1 | loss: 64.1490402CurrentTrain: epoch  6, batch     2 | loss: 67.7942829CurrentTrain: epoch  6, batch     3 | loss: 69.6809896CurrentTrain: epoch  7, batch     0 | loss: 57.2807975CurrentTrain: epoch  7, batch     1 | loss: 49.9902061CurrentTrain: epoch  7, batch     2 | loss: 85.7660816CurrentTrain: epoch  7, batch     3 | loss: 69.9282403CurrentTrain: epoch  8, batch     0 | loss: 65.0885406CurrentTrain: epoch  8, batch     1 | loss: 52.9182652CurrentTrain: epoch  8, batch     2 | loss: 86.1573260CurrentTrain: epoch  8, batch     3 | loss: 47.0378289CurrentTrain: epoch  9, batch     0 | loss: 51.2818025CurrentTrain: epoch  9, batch     1 | loss: 66.7877953CurrentTrain: epoch  9, batch     2 | loss: 63.8211544CurrentTrain: epoch  9, batch     3 | loss: 49.8908456
MemoryTrain:  epoch  0, batch     0 | loss: 0.4023967MemoryTrain:  epoch  1, batch     0 | loss: 0.3001513MemoryTrain:  epoch  2, batch     0 | loss: 0.2288060MemoryTrain:  epoch  3, batch     0 | loss: 0.1894250MemoryTrain:  epoch  4, batch     0 | loss: 0.1446754MemoryTrain:  epoch  5, batch     0 | loss: 0.1937640MemoryTrain:  epoch  6, batch     0 | loss: 0.0818450MemoryTrain:  epoch  7, batch     0 | loss: 0.0713964MemoryTrain:  epoch  8, batch     0 | loss: 0.0691396MemoryTrain:  epoch  9, batch     0 | loss: 0.0635894

F1 score per class: {33: 0.0, 36: 0.6153846153846154, 6: 0.7294117647058823, 8: 0.0, 20: 0.0, 26: 0.972972972972973, 29: 0.2857142857142857, 30: 0.35}
Micro-average F1 score: 0.6011904761904762
Weighted-average F1 score: 0.6479644498131893
F1 score per class: {33: 0.0, 36: 0.7441860465116279, 6: 0.8979591836734694, 8: 0.0, 20: 0.0, 26: 1.0, 29: 0.5, 30: 0.8739495798319328}
Micro-average F1 score: 0.8246913580246914
Weighted-average F1 score: 0.8221290922142713
F1 score per class: {33: 0.0, 36: 0.7441860465116279, 6: 0.8979591836734694, 8: 0.0, 20: 0.0, 26: 1.0, 29: 0.5, 30: 0.8739495798319328}
Micro-average F1 score: 0.8246913580246914
Weighted-average F1 score: 0.8221290922142713

F1 score per class: {32: 0.4788732394366197, 33: 0.47368421052631576, 36: 0.8636363636363636, 6: 0.7294117647058823, 8: 0.34782608695652173, 19: 0.7675675675675676, 20: 0.9533678756476683, 24: 0.972972972972973, 26: 0.8677248677248677, 29: 0.2857142857142857, 30: 0.35}
Micro-average F1 score: 0.7210031347962382
Weighted-average F1 score: 0.7554765447009127
F1 score per class: {32: 0.5365853658536586, 33: 0.5680473372781065, 36: 0.9010989010989011, 6: 0.8979591836734694, 8: 0.5185185185185185, 19: 0.7486033519553073, 20: 0.9595959595959596, 24: 0.8636363636363636, 26: 0.8911917098445595, 29: 0.47058823529411764, 30: 0.8739495798319328}
Micro-average F1 score: 0.7884892086330936
Weighted-average F1 score: 0.7943701619776915
F1 score per class: {32: 0.5185185185185185, 33: 0.5614035087719298, 36: 0.9010989010989011, 6: 0.8979591836734694, 8: 0.5185185185185185, 19: 0.7486033519553073, 20: 0.9595959595959596, 24: 0.8837209302325582, 26: 0.8795811518324608, 29: 0.47058823529411764, 30: 0.8739495798319328}
Micro-average F1 score: 0.7844268204758471
Weighted-average F1 score: 0.7904632565926283

F1 score per class: {32: 0.0, 33: 0.4768211920529801, 36: 0.0, 6: 0.6078431372549019, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.972972972972973, 26: 0.0, 29: 0.2857142857142857, 30: 0.3181818181818182}
Micro-average F1 score: 0.44493392070484583
Weighted-average F1 score: 0.3942807641372209
F1 score per class: {32: 0.0, 33: 0.45714285714285713, 36: 0.0, 6: 0.567741935483871, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.7037037037037037, 26: 0.0, 29: 0.4, 30: 0.4928909952606635}
Micro-average F1 score: 0.43889618922470436
Weighted-average F1 score: 0.40757016603344404
F1 score per class: {32: 0.0, 33: 0.44651162790697674, 36: 0.0, 6: 0.5827814569536424, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.7916666666666666, 26: 0.0, 29: 0.4, 30: 0.5}
Micro-average F1 score: 0.4489247311827957
Weighted-average F1 score: 0.4169994011644669

F1 score per class: {32: 0.36363636363636365, 33: 0.2938775510204082, 36: 0.6495726495726496, 6: 0.5210084033613446, 8: 0.23529411764705882, 19: 0.6729857819905213, 20: 0.8761904761904762, 24: 0.972972972972973, 26: 0.784688995215311, 29: 0.26666666666666666, 30: 0.3010752688172043}
Micro-average F1 score: 0.5771643663739021
Weighted-average F1 score: 0.5773111908235501
F1 score per class: {32: 0.37130801687763715, 33: 0.25396825396825395, 36: 0.5795053003533569, 6: 0.43137254901960786, 8: 0.1728395061728395, 19: 0.6291079812206573, 20: 0.8189655172413793, 24: 0.40860215053763443, 26: 0.7319148936170212, 29: 0.2857142857142857, 30: 0.42448979591836733}
Micro-average F1 score: 0.4917003140421714
Weighted-average F1 score: 0.4709598291930486
F1 score per class: {32: 0.3700440528634361, 33: 0.24615384615384617, 36: 0.5878136200716846, 6: 0.44221105527638194, 8: 0.22580645161290322, 19: 0.6291079812206573, 20: 0.8189655172413793, 24: 0.4634146341463415, 26: 0.7272727272727273, 29: 0.27586206896551724, 30: 0.4369747899159664}
Micro-average F1 score: 0.4986251145737855
Weighted-average F1 score: 0.47879800316729815
cur_acc_wo_na:  ['0.8089', '0.6012']
his_acc_wo_na:  ['0.8089', '0.7210']
cur_acc des_wo_na:  ['0.8405', '0.8247']
his_acc des_wo_na:  ['0.8405', '0.7885']
cur_acc rrf_wo_na:  ['0.8405', '0.8247']
his_acc rrf_wo_na:  ['0.8405', '0.7844']
cur_acc_w_na:  ['0.6864', '0.4449']
his_acc_w_na:  ['0.6864', '0.5772']
cur_acc des_w_na:  ['0.6309', '0.4389']
his_acc des_w_na:  ['0.6309', '0.4917']
cur_acc rrf_w_na:  ['0.6313', '0.4489']
his_acc rrf_w_na:  ['0.6313', '0.4986']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 79.5580875CurrentTrain: epoch  0, batch     1 | loss: 110.2870269CurrentTrain: epoch  0, batch     2 | loss: 67.6117637CurrentTrain: epoch  0, batch     3 | loss: 97.6802623CurrentTrain: epoch  0, batch     4 | loss: 18.5395720CurrentTrain: epoch  1, batch     0 | loss: 74.9940783CurrentTrain: epoch  1, batch     1 | loss: 94.1919221CurrentTrain: epoch  1, batch     2 | loss: 63.6698837CurrentTrain: epoch  1, batch     3 | loss: 93.1120134CurrentTrain: epoch  1, batch     4 | loss: 18.0609465CurrentTrain: epoch  2, batch     0 | loss: 73.1092161CurrentTrain: epoch  2, batch     1 | loss: 55.6642700CurrentTrain: epoch  2, batch     2 | loss: 70.5148648CurrentTrain: epoch  2, batch     3 | loss: 122.6408193CurrentTrain: epoch  2, batch     4 | loss: 20.7424075CurrentTrain: epoch  3, batch     0 | loss: 71.9303832CurrentTrain: epoch  3, batch     1 | loss: 113.6443769CurrentTrain: epoch  3, batch     2 | loss: 87.2354364CurrentTrain: epoch  3, batch     3 | loss: 87.7565691CurrentTrain: epoch  3, batch     4 | loss: 17.9213622CurrentTrain: epoch  4, batch     0 | loss: 69.8348094CurrentTrain: epoch  4, batch     1 | loss: 66.2143192CurrentTrain: epoch  4, batch     2 | loss: 122.1098706CurrentTrain: epoch  4, batch     3 | loss: 55.6828061CurrentTrain: epoch  4, batch     4 | loss: 26.9970156CurrentTrain: epoch  5, batch     0 | loss: 54.9479057CurrentTrain: epoch  5, batch     1 | loss: 87.7561076CurrentTrain: epoch  5, batch     2 | loss: 66.2217056CurrentTrain: epoch  5, batch     3 | loss: 86.0530529CurrentTrain: epoch  5, batch     4 | loss: 28.3345654CurrentTrain: epoch  6, batch     0 | loss: 65.8115593CurrentTrain: epoch  6, batch     1 | loss: 117.5358178CurrentTrain: epoch  6, batch     2 | loss: 56.3928662CurrentTrain: epoch  6, batch     3 | loss: 65.4350824CurrentTrain: epoch  6, batch     4 | loss: 16.2017187CurrentTrain: epoch  7, batch     0 | loss: 86.4974292CurrentTrain: epoch  7, batch     1 | loss: 115.6078025CurrentTrain: epoch  7, batch     2 | loss: 52.2178600CurrentTrain: epoch  7, batch     3 | loss: 52.5638911CurrentTrain: epoch  7, batch     4 | loss: 27.4733497CurrentTrain: epoch  8, batch     0 | loss: 66.2528859CurrentTrain: epoch  8, batch     1 | loss: 82.7032577CurrentTrain: epoch  8, batch     2 | loss: 64.3020820CurrentTrain: epoch  8, batch     3 | loss: 85.4060023CurrentTrain: epoch  8, batch     4 | loss: 24.6580257CurrentTrain: epoch  9, batch     0 | loss: 82.4421360CurrentTrain: epoch  9, batch     1 | loss: 115.4340659CurrentTrain: epoch  9, batch     2 | loss: 55.6040364CurrentTrain: epoch  9, batch     3 | loss: 63.6527566CurrentTrain: epoch  9, batch     4 | loss: 9.9313230
MemoryTrain:  epoch  0, batch     0 | loss: 0.5872466MemoryTrain:  epoch  1, batch     0 | loss: 0.5624101MemoryTrain:  epoch  2, batch     0 | loss: 0.3457175MemoryTrain:  epoch  3, batch     0 | loss: 0.2513204MemoryTrain:  epoch  4, batch     0 | loss: 0.1929973MemoryTrain:  epoch  5, batch     0 | loss: 0.1376569MemoryTrain:  epoch  6, batch     0 | loss: 0.1497894MemoryTrain:  epoch  7, batch     0 | loss: 0.1300315MemoryTrain:  epoch  8, batch     0 | loss: 0.0981526MemoryTrain:  epoch  9, batch     0 | loss: 0.0870637

F1 score per class: {2: 0.9411764705882353, 39: 0.0, 8: 0.4827586206896552, 11: 0.4126984126984127, 12: 0.0, 19: 0.4, 28: 0.13333333333333333}
Micro-average F1 score: 0.4377104377104377
Weighted-average F1 score: 0.41936083599369606
F1 score per class: {2: 0.9411764705882353, 39: 0.0, 8: 0.7692307692307693, 11: 0.7878787878787878, 12: 0.0, 19: 0.0, 24: 0.7142857142857143, 28: 0.13333333333333333}
Micro-average F1 score: 0.7146666666666667
Weighted-average F1 score: 0.676078672458763
F1 score per class: {2: 0.9411764705882353, 39: 0.0, 8: 0.7692307692307693, 11: 0.8023952095808383, 12: 0.0, 19: 0.0, 24: 0.7142857142857143, 28: 0.13333333333333333}
Micro-average F1 score: 0.7272727272727273
Weighted-average F1 score: 0.6968814886023353

F1 score per class: {32: 0.9411764705882353, 33: 0.5375, 2: 0.46540880503144655, 36: 0.4444444444444444, 6: 0.40625, 39: 0.8092485549132948, 8: 0.7142857142857143, 11: 0.34782608695652173, 12: 0.7640449438202247, 19: 0.3076923076923077, 20: 0.9368421052631579, 24: 0.972972972972973, 26: 0.8736842105263158, 28: 0.0, 29: 0.16666666666666666, 30: 0.13333333333333333}
Micro-average F1 score: 0.6510152284263959
Weighted-average F1 score: 0.706112197767123
F1 score per class: {32: 0.8, 33: 0.5952380952380952, 2: 0.5625, 36: 0.7189542483660131, 6: 0.7428571428571429, 39: 0.8556149732620321, 8: 0.8, 11: 0.4666666666666667, 12: 0.7472527472527473, 19: 0.2564102564102564, 20: 0.9430051813471503, 24: 0.95, 26: 0.8979591836734694, 28: 0.3076923076923077, 29: 0.693069306930693, 30: 0.125}
Micro-average F1 score: 0.7430516165626773
Weighted-average F1 score: 0.7461614114306968
F1 score per class: {32: 0.8421052631578947, 33: 0.5952380952380952, 2: 0.5485714285714286, 36: 0.7189542483660131, 6: 0.7486033519553073, 39: 0.8494623655913979, 8: 0.8, 11: 0.4, 12: 0.7472527472527473, 19: 0.38461538461538464, 20: 0.9430051813471503, 24: 0.95, 26: 0.8969072164948454, 28: 0.3076923076923077, 29: 0.6530612244897959, 30: 0.125}
Micro-average F1 score: 0.7433124644280023
Weighted-average F1 score: 0.7514680836441885

F1 score per class: {32: 0.4444444444444444, 2: 0.0, 6: 0.0, 39: 0.42748091603053434, 8: 0.3795620437956204, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.0, 24: 0.25, 26: 0.0, 28: 0.0, 29: 0.125}
Micro-average F1 score: 0.2838427947598253
Weighted-average F1 score: 0.19529487451778632
F1 score per class: {32: 0.21621621621621623, 2: 0.0, 36: 0.0, 6: 0.48672566371681414, 39: 0.5627705627705628, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.18867924528301888, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.08333333333333333}
Micro-average F1 score: 0.33291925465838507
Weighted-average F1 score: 0.2803225917081695
F1 score per class: {32: 0.25, 2: 0.0, 36: 0.0, 6: 0.4888888888888889, 39: 0.5800865800865801, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.22727272727272727, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.08333333333333333}
Micro-average F1 score: 0.34827144686299616
Weighted-average F1 score: 0.29490083858411265

F1 score per class: {32: 0.2962962962962963, 33: 0.38738738738738737, 2: 0.22629969418960244, 36: 0.3218390804597701, 6: 0.26262626262626265, 39: 0.6334841628959276, 8: 0.4411764705882353, 11: 0.25806451612903225, 12: 0.6903553299492385, 19: 0.15384615384615385, 20: 0.8476190476190476, 24: 0.9, 26: 0.7649769585253456, 28: 0.0, 29: 0.16, 30: 0.1111111111111111}
Micro-average F1 score: 0.4756606397774687
Weighted-average F1 score: 0.47024498623615035
F1 score per class: {32: 0.12030075187969924, 33: 0.3831417624521073, 2: 0.22332506203473945, 36: 0.3374233128834356, 6: 0.25691699604743085, 39: 0.5614035087719298, 8: 0.34951456310679613, 11: 0.17073170731707318, 12: 0.6570048309178744, 19: 0.08333333333333333, 20: 0.8272727272727273, 24: 0.4935064935064935, 26: 0.6470588235294118, 28: 0.2857142857142857, 29: 0.4697986577181208, 30: 0.05405405405405406}
Micro-average F1 score: 0.39721043056397815
Weighted-average F1 score: 0.36950998896292025
F1 score per class: {32: 0.14814814814814814, 33: 0.4, 2: 0.210989010989011, 36: 0.3374233128834356, 6: 0.24860853432282004, 39: 0.572463768115942, 8: 0.33962264150943394, 11: 0.19607843137254902, 12: 0.6634146341463415, 19: 0.1111111111111111, 20: 0.8504672897196262, 24: 0.6909090909090909, 26: 0.696, 28: 0.3076923076923077, 29: 0.463768115942029, 30: 0.07142857142857142}
Micro-average F1 score: 0.40685358255451715
Weighted-average F1 score: 0.3764091773207739
cur_acc_wo_na:  ['0.8089', '0.6012', '0.4377']
his_acc_wo_na:  ['0.8089', '0.7210', '0.6510']
cur_acc des_wo_na:  ['0.8405', '0.8247', '0.7147']
his_acc des_wo_na:  ['0.8405', '0.7885', '0.7431']
cur_acc rrf_wo_na:  ['0.8405', '0.8247', '0.7273']
his_acc rrf_wo_na:  ['0.8405', '0.7844', '0.7433']
cur_acc_w_na:  ['0.6864', '0.4449', '0.2838']
his_acc_w_na:  ['0.6864', '0.5772', '0.4757']
cur_acc des_w_na:  ['0.6309', '0.4389', '0.3329']
his_acc des_w_na:  ['0.6309', '0.4917', '0.3972']
cur_acc rrf_w_na:  ['0.6313', '0.4489', '0.3483']
his_acc rrf_w_na:  ['0.6313', '0.4986', '0.4069']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 82.9797781CurrentTrain: epoch  0, batch     1 | loss: 96.7088238CurrentTrain: epoch  0, batch     2 | loss: 71.3433525CurrentTrain: epoch  0, batch     3 | loss: 74.2085516CurrentTrain: epoch  0, batch     4 | loss: 90.3370263CurrentTrain: epoch  1, batch     0 | loss: 73.7181701CurrentTrain: epoch  1, batch     1 | loss: 88.7091879CurrentTrain: epoch  1, batch     2 | loss: 89.6059922CurrentTrain: epoch  1, batch     3 | loss: 74.3323090CurrentTrain: epoch  1, batch     4 | loss: 46.6988597CurrentTrain: epoch  2, batch     0 | loss: 58.1249547CurrentTrain: epoch  2, batch     1 | loss: 58.8966290CurrentTrain: epoch  2, batch     2 | loss: 88.4225078CurrentTrain: epoch  2, batch     3 | loss: 186.8323628CurrentTrain: epoch  2, batch     4 | loss: 45.9279655CurrentTrain: epoch  3, batch     0 | loss: 56.8939642CurrentTrain: epoch  3, batch     1 | loss: 88.6017788CurrentTrain: epoch  3, batch     2 | loss: 118.3886365CurrentTrain: epoch  3, batch     3 | loss: 69.8247187CurrentTrain: epoch  3, batch     4 | loss: 43.4526592CurrentTrain: epoch  4, batch     0 | loss: 70.1241480CurrentTrain: epoch  4, batch     1 | loss: 55.6656792CurrentTrain: epoch  4, batch     2 | loss: 67.9970909CurrentTrain: epoch  4, batch     3 | loss: 86.0061944CurrentTrain: epoch  4, batch     4 | loss: 75.0404419CurrentTrain: epoch  5, batch     0 | loss: 117.0277995CurrentTrain: epoch  5, batch     1 | loss: 68.3604332CurrentTrain: epoch  5, batch     2 | loss: 52.1479461CurrentTrain: epoch  5, batch     3 | loss: 87.9311584CurrentTrain: epoch  5, batch     4 | loss: 118.0013634CurrentTrain: epoch  6, batch     0 | loss: 54.4719659CurrentTrain: epoch  6, batch     1 | loss: 67.1832479CurrentTrain: epoch  6, batch     2 | loss: 118.8071266CurrentTrain: epoch  6, batch     3 | loss: 53.1135194CurrentTrain: epoch  6, batch     4 | loss: 52.4907714CurrentTrain: epoch  7, batch     0 | loss: 84.2550769CurrentTrain: epoch  7, batch     1 | loss: 65.0190977CurrentTrain: epoch  7, batch     2 | loss: 53.8190499CurrentTrain: epoch  7, batch     3 | loss: 83.4240450CurrentTrain: epoch  7, batch     4 | loss: 74.3033374CurrentTrain: epoch  8, batch     0 | loss: 64.0951057CurrentTrain: epoch  8, batch     1 | loss: 85.7901235CurrentTrain: epoch  8, batch     2 | loss: 65.4839744CurrentTrain: epoch  8, batch     3 | loss: 86.5883563CurrentTrain: epoch  8, batch     4 | loss: 69.1712359CurrentTrain: epoch  9, batch     0 | loss: 63.8633843CurrentTrain: epoch  9, batch     1 | loss: 67.5033471CurrentTrain: epoch  9, batch     2 | loss: 85.2419091CurrentTrain: epoch  9, batch     3 | loss: 65.5980471CurrentTrain: epoch  9, batch     4 | loss: 51.8822789
MemoryTrain:  epoch  0, batch     0 | loss: 0.3660225MemoryTrain:  epoch  1, batch     0 | loss: 0.3473804MemoryTrain:  epoch  2, batch     0 | loss: 0.2828188MemoryTrain:  epoch  3, batch     0 | loss: 0.2095694MemoryTrain:  epoch  4, batch     0 | loss: 0.1687780MemoryTrain:  epoch  5, batch     0 | loss: 0.1469119MemoryTrain:  epoch  6, batch     0 | loss: 0.1138049MemoryTrain:  epoch  7, batch     0 | loss: 0.1064290MemoryTrain:  epoch  8, batch     0 | loss: 0.0814102MemoryTrain:  epoch  9, batch     0 | loss: 0.0692127

F1 score per class: {5: 0.9795918367346939, 6: 0.0, 39: 0.0, 8: 0.18181818181818182, 10: 0.0, 11: 0.8235294117647058, 16: 0.7142857142857143, 17: 0.23809523809523808, 18: 0.0}
Micro-average F1 score: 0.6342592592592593
Weighted-average F1 score: 0.755865516369718
F1 score per class: {5: 0.9950248756218906, 6: 0.0, 39: 0.0, 8: 0.5633802816901409, 10: 0.0, 11: 0.9090909090909091, 16: 0.6153846153846154, 17: 0.7666666666666667, 18: 0.0, 19: 0.0, 20: 0.0, 28: 0.0}
Micro-average F1 score: 0.7427466150870407
Weighted-average F1 score: 0.6928698290080555
F1 score per class: {5: 1.0, 6: 0.0, 39: 0.0, 8: 0.5037037037037037, 10: 0.0, 11: 0.9090909090909091, 16: 0.6153846153846154, 17: 0.7457627118644068, 18: 0.0, 19: 0.0, 20: 0.0, 28: 0.0}
Micro-average F1 score: 0.7184466019417476
Weighted-average F1 score: 0.6662143116295304

F1 score per class: {2: 0.7142857142857143, 5: 0.9696969696969697, 6: 0.49056603773584906, 8: 0.48, 10: 0.17857142857142858, 11: 0.3076923076923077, 12: 0.07692307692307693, 16: 0.7241379310344828, 17: 0.29411764705882354, 18: 0.16393442622950818, 19: 0.768361581920904, 20: 0.6153846153846154, 24: 0.2727272727272727, 26: 0.7513812154696132, 28: 0.26666666666666666, 29: 0.9312169312169312, 30: 0.972972972972973, 32: 0.8958333333333334, 33: 0.3076923076923077, 36: 0.058823529411764705, 39: 0.125}
Micro-average F1 score: 0.6040609137055838
Weighted-average F1 score: 0.7068127123927924
F1 score per class: {2: 0.8, 5: 0.9523809523809523, 6: 0.6010928961748634, 8: 0.5202312138728323, 10: 0.5263157894736842, 11: 0.3140495867768595, 12: 0.6114649681528662, 16: 0.7936507936507936, 17: 0.26666666666666666, 18: 0.36507936507936506, 19: 0.8297872340425532, 20: 0.7415730337078652, 24: 0.5, 26: 0.7540983606557377, 28: 0.35714285714285715, 29: 0.9312169312169312, 30: 0.95, 32: 0.9025641025641026, 33: 0.42857142857142855, 36: 0.4470588235294118, 39: 0.10526315789473684}
Micro-average F1 score: 0.6774475524475524
Weighted-average F1 score: 0.6865607138900256
F1 score per class: {2: 0.8, 5: 0.9569377990430622, 6: 0.585635359116022, 8: 0.5268817204301075, 10: 0.4755244755244755, 11: 0.34146341463414637, 12: 0.5935483870967742, 16: 0.7936507936507936, 17: 0.23529411764705882, 18: 0.35772357723577236, 19: 0.8253968253968254, 20: 0.7415730337078652, 24: 0.4, 26: 0.7540983606557377, 28: 0.43478260869565216, 29: 0.9368421052631579, 30: 0.95, 32: 0.8969072164948454, 33: 0.42857142857142855, 36: 0.3902439024390244, 39: 0.10526315789473684}
Micro-average F1 score: 0.6710526315789473
Weighted-average F1 score: 0.683112482525693

F1 score per class: {32: 0.0, 2: 0.8930232558139535, 5: 0.0, 6: 0.0, 39: 0.18018018018018017, 8: 0.0, 10: 0.0, 11: 0.5833333333333334, 12: 0.4, 16: 0.15151515151515152, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.4491803278688525
Weighted-average F1 score: 0.4190826221392995
F1 score per class: {2: 0.0, 5: 0.5390835579514824, 6: 0.0, 8: 0.0, 10: 0.47619047619047616, 11: 0.0, 12: 0.0, 16: 0.6024096385542169, 17: 0.4, 18: 0.27058823529411763, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.3240506329113924
Weighted-average F1 score: 0.2758949864942742
F1 score per class: {2: 0.0, 5: 0.547945205479452, 6: 0.0, 8: 0.0, 10: 0.4473684210526316, 11: 0.0, 12: 0.0, 16: 0.6024096385542169, 17: 0.38095238095238093, 18: 0.2634730538922156, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.3178694158075601
Weighted-average F1 score: 0.26939107406259205

F1 score per class: {2: 0.2564102564102564, 5: 0.8533333333333334, 6: 0.34513274336283184, 8: 0.2830188679245283, 10: 0.16260162601626016, 11: 0.20809248554913296, 12: 0.06956521739130435, 16: 0.42, 17: 0.125, 18: 0.08547008547008547, 19: 0.5787234042553191, 20: 0.41379310344827586, 24: 0.17647058823529413, 26: 0.6538461538461539, 28: 0.08333333333333333, 29: 0.7857142857142857, 30: 0.9230769230769231, 32: 0.7678571428571429, 33: 0.3076923076923077, 36: 0.056338028169014086, 39: 0.1}
Micro-average F1 score: 0.45041635124905377
Weighted-average F1 score: 0.4689996460648786
F1 score per class: {2: 0.1935483870967742, 5: 0.3952569169960474, 6: 0.3536977491961415, 8: 0.18108651911468812, 10: 0.3018867924528302, 11: 0.2222222222222222, 12: 0.18786692759295498, 16: 0.43859649122807015, 17: 0.09302325581395349, 18: 0.12234042553191489, 19: 0.5324232081911263, 20: 0.358695652173913, 24: 0.1794871794871795, 26: 0.6188340807174888, 28: 0.09615384615384616, 29: 0.7963800904977375, 30: 0.6551724137931034, 32: 0.5925925925925926, 33: 0.3333333333333333, 36: 0.3333333333333333, 39: 0.05555555555555555}
Micro-average F1 score: 0.3425414364640884
Weighted-average F1 score: 0.3216570191346658
F1 score per class: {2: 0.1935483870967742, 5: 0.4065040650406504, 6: 0.35570469798657717, 8: 0.18081180811808117, 10: 0.2869198312236287, 11: 0.22950819672131148, 12: 0.1945031712473573, 16: 0.42735042735042733, 17: 0.09090909090909091, 18: 0.11859838274932614, 19: 0.5252525252525253, 20: 0.3567567567567568, 24: 0.16666666666666666, 26: 0.6188340807174888, 28: 0.11904761904761904, 29: 0.7911111111111111, 30: 0.6333333333333333, 32: 0.6444444444444445, 33: 0.35294117647058826, 36: 0.2857142857142857, 39: 0.058823529411764705}
Micro-average F1 score: 0.345372460496614
Weighted-average F1 score: 0.3246226474838757
cur_acc_wo_na:  ['0.8089', '0.6012', '0.4377', '0.6343']
his_acc_wo_na:  ['0.8089', '0.7210', '0.6510', '0.6041']
cur_acc des_wo_na:  ['0.8405', '0.8247', '0.7147', '0.7427']
his_acc des_wo_na:  ['0.8405', '0.7885', '0.7431', '0.6774']
cur_acc rrf_wo_na:  ['0.8405', '0.8247', '0.7273', '0.7184']
his_acc rrf_wo_na:  ['0.8405', '0.7844', '0.7433', '0.6711']
cur_acc_w_na:  ['0.6864', '0.4449', '0.2838', '0.4492']
his_acc_w_na:  ['0.6864', '0.5772', '0.4757', '0.4504']
cur_acc des_w_na:  ['0.6309', '0.4389', '0.3329', '0.3241']
his_acc des_w_na:  ['0.6309', '0.4917', '0.3972', '0.3425']
cur_acc rrf_w_na:  ['0.6313', '0.4489', '0.3483', '0.3179']
his_acc rrf_w_na:  ['0.6313', '0.4986', '0.4069', '0.3454']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 84.4383952CurrentTrain: epoch  0, batch     1 | loss: 107.3562672CurrentTrain: epoch  0, batch     2 | loss: 82.9792863CurrentTrain: epoch  0, batch     3 | loss: 67.5975191CurrentTrain: epoch  0, batch     4 | loss: 79.0386012CurrentTrain: epoch  1, batch     0 | loss: 65.0886017CurrentTrain: epoch  1, batch     1 | loss: 69.7782356CurrentTrain: epoch  1, batch     2 | loss: 94.8319135CurrentTrain: epoch  1, batch     3 | loss: 94.0836602CurrentTrain: epoch  1, batch     4 | loss: 73.1152827CurrentTrain: epoch  2, batch     0 | loss: 70.2091117CurrentTrain: epoch  2, batch     1 | loss: 94.5691895CurrentTrain: epoch  2, batch     2 | loss: 87.0430135CurrentTrain: epoch  2, batch     3 | loss: 121.9158162CurrentTrain: epoch  2, batch     4 | loss: 31.2521115CurrentTrain: epoch  3, batch     0 | loss: 120.0227852CurrentTrain: epoch  3, batch     1 | loss: 65.8801938CurrentTrain: epoch  3, batch     2 | loss: 69.9863171CurrentTrain: epoch  3, batch     3 | loss: 88.0654911CurrentTrain: epoch  3, batch     4 | loss: 108.5774933CurrentTrain: epoch  4, batch     0 | loss: 66.3432328CurrentTrain: epoch  4, batch     1 | loss: 67.3567855CurrentTrain: epoch  4, batch     2 | loss: 117.3725218CurrentTrain: epoch  4, batch     3 | loss: 69.3319441CurrentTrain: epoch  4, batch     4 | loss: 38.8709301CurrentTrain: epoch  5, batch     0 | loss: 67.6590925CurrentTrain: epoch  5, batch     1 | loss: 65.1021644CurrentTrain: epoch  5, batch     2 | loss: 87.2588933CurrentTrain: epoch  5, batch     3 | loss: 67.6264845CurrentTrain: epoch  5, batch     4 | loss: 49.9738508CurrentTrain: epoch  6, batch     0 | loss: 83.9369487CurrentTrain: epoch  6, batch     1 | loss: 62.7029154CurrentTrain: epoch  6, batch     2 | loss: 83.8075052CurrentTrain: epoch  6, batch     3 | loss: 118.6773917CurrentTrain: epoch  6, batch     4 | loss: 47.8859089CurrentTrain: epoch  7, batch     0 | loss: 83.7416396CurrentTrain: epoch  7, batch     1 | loss: 68.1384672CurrentTrain: epoch  7, batch     2 | loss: 65.2929303CurrentTrain: epoch  7, batch     3 | loss: 83.2162522CurrentTrain: epoch  7, batch     4 | loss: 62.4793078CurrentTrain: epoch  8, batch     0 | loss: 66.4357305CurrentTrain: epoch  8, batch     1 | loss: 84.3770924CurrentTrain: epoch  8, batch     2 | loss: 67.5449316CurrentTrain: epoch  8, batch     3 | loss: 68.0825504CurrentTrain: epoch  8, batch     4 | loss: 44.6876697CurrentTrain: epoch  9, batch     0 | loss: 114.5955453CurrentTrain: epoch  9, batch     1 | loss: 83.9635366CurrentTrain: epoch  9, batch     2 | loss: 64.1124837CurrentTrain: epoch  9, batch     3 | loss: 66.0153979CurrentTrain: epoch  9, batch     4 | loss: 46.0361090
MemoryTrain:  epoch  0, batch     0 | loss: 0.4956382MemoryTrain:  epoch  1, batch     0 | loss: 0.4106700MemoryTrain:  epoch  2, batch     0 | loss: 0.3686293MemoryTrain:  epoch  3, batch     0 | loss: 0.3054308MemoryTrain:  epoch  4, batch     0 | loss: 0.2450651MemoryTrain:  epoch  5, batch     0 | loss: 0.2072128MemoryTrain:  epoch  6, batch     0 | loss: 0.1902269MemoryTrain:  epoch  7, batch     0 | loss: 0.1444715MemoryTrain:  epoch  8, batch     0 | loss: 0.1187937MemoryTrain:  epoch  9, batch     0 | loss: 0.1040037

F1 score per class: {32: 0.2702702702702703, 1: 0.7941176470588235, 34: 0.0, 3: 0.1, 11: 0.0, 14: 0.6885245901639344, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.25396825396825395}
Micro-average F1 score: 0.47761194029850745
Weighted-average F1 score: 0.4691144156332199
F1 score per class: {32: 0.3783783783783784, 1: 0.9487179487179487, 34: 0.0, 3: 0.0, 36: 0.0, 8: 0.11494252873563218, 10: 0.0, 11: 0.7096774193548387, 14: 0.0, 18: 0.0, 22: 0.0, 24: 0.4507042253521127, 26: 0.0}
Micro-average F1 score: 0.5498489425981873
Weighted-average F1 score: 0.5224876753762392
F1 score per class: {32: 0.3486238532110092, 1: 0.8843537414965986, 34: 0.0, 3: 0.0, 36: 0.11235955056179775, 8: 0.0, 11: 0.7058823529411765, 14: 0.0, 18: 0.0, 22: 0.0, 24: 0.3333333333333333, 26: 0.0}
Micro-average F1 score: 0.5139318885448917
Weighted-average F1 score: 0.4892926828175315

F1 score per class: {1: 0.25862068965517243, 2: 0.7142857142857143, 3: 0.7883211678832117, 5: 0.9746192893401016, 6: 0.4520547945205479, 8: 0.5072463768115942, 10: 0.3, 11: 0.12, 12: 0.019801980198019802, 14: 0.08602150537634409, 16: 0.7796610169491526, 17: 0.2857142857142857, 18: 0.08955223880597014, 19: 0.5142857142857142, 20: 0.6153846153846154, 22: 0.6666666666666666, 24: 0.06060606060606061, 26: 0.7513812154696132, 28: 0.26666666666666666, 29: 0.9247311827956989, 30: 0.972972972972973, 32: 0.8449197860962567, 33: 0.3076923076923077, 34: 0.17582417582417584, 36: 0.11428571428571428, 39: 0.13333333333333333}
Micro-average F1 score: 0.5394839718530101
Weighted-average F1 score: 0.6066351912936686
F1 score per class: {1: 0.35294117647058826, 2: 0.6666666666666666, 3: 0.9308176100628931, 5: 0.9478672985781991, 6: 0.4489795918367347, 8: 0.5423728813559322, 10: 0.6060606060606061, 11: 0.061224489795918366, 12: 0.6081081081081081, 14: 0.10989010989010989, 16: 0.8253968253968254, 17: 0.26666666666666666, 18: 0.32, 19: 0.7142857142857143, 20: 0.6823529411764706, 22: 0.673469387755102, 24: 0.0975609756097561, 26: 0.7634408602150538, 28: 0.4166666666666667, 29: 0.93048128342246, 30: 0.9743589743589743, 32: 0.8469387755102041, 33: 0.4, 34: 0.26666666666666666, 36: 0.6041666666666666, 39: 0.1111111111111111}
Micro-average F1 score: 0.6165311653116531
Weighted-average F1 score: 0.624248204478443
F1 score per class: {1: 0.3247863247863248, 2: 0.75, 3: 0.8666666666666667, 5: 0.9560975609756097, 6: 0.45517241379310347, 8: 0.5204081632653061, 10: 0.5369127516778524, 11: 0.16981132075471697, 12: 0.3937007874015748, 14: 0.10416666666666667, 16: 0.8125, 17: 0.22641509433962265, 18: 0.32, 19: 0.6329113924050633, 20: 0.7272727272727273, 22: 0.673469387755102, 24: 0.05128205128205128, 26: 0.7593582887700535, 28: 0.5263157894736842, 29: 0.93048128342246, 30: 1.0, 32: 0.8497409326424871, 33: 0.42857142857142855, 34: 0.21153846153846154, 36: 0.4418604651162791, 39: 0.11764705882352941}
Micro-average F1 score: 0.591304347826087
Weighted-average F1 score: 0.6030556268582383

F1 score per class: {1: 0.11029411764705882, 2: 0.0, 3: 0.5538461538461539, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 14: 0.06060606060606061, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.402555910543131, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.24242424242424243}
Micro-average F1 score: 0.2430379746835443
Weighted-average F1 score: 0.21423417347140453
F1 score per class: {1: 0.13725490196078433, 2: 0.0, 3: 0.4444444444444444, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.06666666666666667, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.42443729903536975, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.35555555555555557, 36: 0.0}
Micro-average F1 score: 0.2139917695473251
Weighted-average F1 score: 0.18805870476940734
F1 score per class: {1: 0.12837837837837837, 2: 0.0, 3: 0.4727272727272727, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.062111801242236024, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.42443729903536975, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.3055555555555556, 36: 0.0}
Micro-average F1 score: 0.21586475942782835
Weighted-average F1 score: 0.18785564673774377

F1 score per class: {1: 0.08955223880597014, 2: 0.23255813953488372, 3: 0.4, 5: 0.8533333333333334, 6: 0.32038834951456313, 8: 0.2978723404255319, 10: 0.2608695652173913, 11: 0.09302325581395349, 12: 0.018691588785046728, 14: 0.028268551236749116, 16: 0.46, 17: 0.136986301369863, 18: 0.057692307692307696, 19: 0.3870967741935484, 20: 0.42105263157894735, 22: 0.3333333333333333, 24: 0.03125, 26: 0.6570048309178744, 28: 0.0625, 29: 0.7644444444444445, 30: 0.9473684210526315, 32: 0.5766423357664233, 33: 0.3076923076923077, 34: 0.11510791366906475, 36: 0.10256410256410256, 39: 0.09523809523809523}
Micro-average F1 score: 0.3408248950358113
Weighted-average F1 score: 0.3302741338270381
F1 score per class: {1: 0.10396039603960396, 2: 0.15584415584415584, 3: 0.2776735459662289, 5: 0.35398230088495575, 6: 0.30697674418604654, 8: 0.17712177121771217, 10: 0.25252525252525254, 11: 0.044444444444444446, 12: 0.18789144050104384, 14: 0.03787878787878788, 16: 0.45217391304347826, 17: 0.09302325581395349, 18: 0.08421052631578947, 19: 0.3973509933774834, 20: 0.3411764705882353, 22: 0.3393316195372751, 24: 0.045454545454545456, 26: 0.6120689655172413, 28: 0.09259259259259259, 29: 0.7435897435897436, 30: 0.6229508196721312, 32: 0.45108695652173914, 33: 0.23076923076923078, 34: 0.13675213675213677, 36: 0.36477987421383645, 39: 0.043478260869565216}
Micro-average F1 score: 0.26808071880983947
Weighted-average F1 score: 0.2541496199900485
F1 score per class: {1: 0.09571788413098237, 2: 0.16, 3: 0.2968036529680365, 5: 0.4792176039119804, 6: 0.3142857142857143, 8: 0.16666666666666666, 10: 0.2867383512544803, 11: 0.125, 12: 0.1736111111111111, 14: 0.034013605442176874, 16: 0.416, 17: 0.08053691275167785, 18: 0.08830022075055188, 19: 0.36764705882352944, 20: 0.3575418994413408, 22: 0.3393316195372751, 24: 0.025, 26: 0.6255506607929515, 28: 0.12345679012345678, 29: 0.7404255319148936, 30: 0.8085106382978723, 32: 0.5046153846153846, 33: 0.2608695652173913, 34: 0.11518324607329843, 36: 0.2992125984251969, 39: 0.0625}
Micro-average F1 score: 0.2795592830126624
Weighted-average F1 score: 0.2620945998540743
cur_acc_wo_na:  ['0.8089', '0.6012', '0.4377', '0.6343', '0.4776']
his_acc_wo_na:  ['0.8089', '0.7210', '0.6510', '0.6041', '0.5395']
cur_acc des_wo_na:  ['0.8405', '0.8247', '0.7147', '0.7427', '0.5498']
his_acc des_wo_na:  ['0.8405', '0.7885', '0.7431', '0.6774', '0.6165']
cur_acc rrf_wo_na:  ['0.8405', '0.8247', '0.7273', '0.7184', '0.5139']
his_acc rrf_wo_na:  ['0.8405', '0.7844', '0.7433', '0.6711', '0.5913']
cur_acc_w_na:  ['0.6864', '0.4449', '0.2838', '0.4492', '0.2430']
his_acc_w_na:  ['0.6864', '0.5772', '0.4757', '0.4504', '0.3408']
cur_acc des_w_na:  ['0.6309', '0.4389', '0.3329', '0.3241', '0.2140']
his_acc des_w_na:  ['0.6309', '0.4917', '0.3972', '0.3425', '0.2681']
cur_acc rrf_w_na:  ['0.6313', '0.4489', '0.3483', '0.3179', '0.2159']
his_acc rrf_w_na:  ['0.6313', '0.4986', '0.4069', '0.3454', '0.2796']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 100.7764663CurrentTrain: epoch  0, batch     1 | loss: 66.5083166CurrentTrain: epoch  0, batch     2 | loss: 68.7656558CurrentTrain: epoch  0, batch     3 | loss: 8.6412333CurrentTrain: epoch  1, batch     0 | loss: 72.1739276CurrentTrain: epoch  1, batch     1 | loss: 71.0912184CurrentTrain: epoch  1, batch     2 | loss: 93.3579914CurrentTrain: epoch  1, batch     3 | loss: 11.9565260CurrentTrain: epoch  2, batch     0 | loss: 87.9459712CurrentTrain: epoch  2, batch     1 | loss: 69.6092130CurrentTrain: epoch  2, batch     2 | loss: 65.9463741CurrentTrain: epoch  2, batch     3 | loss: 4.5235246CurrentTrain: epoch  3, batch     0 | loss: 70.5888461CurrentTrain: epoch  3, batch     1 | loss: 68.6285306CurrentTrain: epoch  3, batch     2 | loss: 52.4838074CurrentTrain: epoch  3, batch     3 | loss: 11.5975606CurrentTrain: epoch  4, batch     0 | loss: 63.9376966CurrentTrain: epoch  4, batch     1 | loss: 69.3830800CurrentTrain: epoch  4, batch     2 | loss: 52.8109120CurrentTrain: epoch  4, batch     3 | loss: 11.2745724CurrentTrain: epoch  5, batch     0 | loss: 52.6982010CurrentTrain: epoch  5, batch     1 | loss: 64.5788822CurrentTrain: epoch  5, batch     2 | loss: 65.0765937CurrentTrain: epoch  5, batch     3 | loss: 5.7716451CurrentTrain: epoch  6, batch     0 | loss: 53.3146200CurrentTrain: epoch  6, batch     1 | loss: 53.0054065CurrentTrain: epoch  6, batch     2 | loss: 51.9448097CurrentTrain: epoch  6, batch     3 | loss: 6.4773836CurrentTrain: epoch  7, batch     0 | loss: 63.9008573CurrentTrain: epoch  7, batch     1 | loss: 51.8732237CurrentTrain: epoch  7, batch     2 | loss: 65.7325019CurrentTrain: epoch  7, batch     3 | loss: 11.0114875CurrentTrain: epoch  8, batch     0 | loss: 49.8575827CurrentTrain: epoch  8, batch     1 | loss: 83.0255457CurrentTrain: epoch  8, batch     2 | loss: 62.1409345CurrentTrain: epoch  8, batch     3 | loss: 10.9525594CurrentTrain: epoch  9, batch     0 | loss: 81.1535627CurrentTrain: epoch  9, batch     1 | loss: 63.5412745CurrentTrain: epoch  9, batch     2 | loss: 48.5204139CurrentTrain: epoch  9, batch     3 | loss: 27.4405171
MemoryTrain:  epoch  0, batch     0 | loss: 0.3605611MemoryTrain:  epoch  1, batch     0 | loss: 0.2874501MemoryTrain:  epoch  2, batch     0 | loss: 0.2416191MemoryTrain:  epoch  3, batch     0 | loss: 0.1807556MemoryTrain:  epoch  4, batch     0 | loss: 0.1805099MemoryTrain:  epoch  5, batch     0 | loss: 0.1242702MemoryTrain:  epoch  6, batch     0 | loss: 0.1219718MemoryTrain:  epoch  7, batch     0 | loss: 0.0927844MemoryTrain:  epoch  8, batch     0 | loss: 0.0971060MemoryTrain:  epoch  9, batch     0 | loss: 0.0777000

F1 score per class: {1: 0.0, 34: 0.0, 3: 0.75, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 22: 0.3333333333333333, 26: 0.0, 27: 0.0, 31: 0.5625}
Micro-average F1 score: 0.5370370370370371
Weighted-average F1 score: 0.43310265282583615
F1 score per class: {1: 0.0, 34: 0.0, 3: 0.75, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 22: 0.6, 26: 0.0, 27: 1.0, 28: 0.0, 31: 0.8870967741935484}
Micro-average F1 score: 0.8053097345132744
Weighted-average F1 score: 0.7378323506822084
F1 score per class: {1: 0.0, 34: 0.0, 3: 0.8888888888888888, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 22: 0.6, 26: 1.0, 27: 0.0, 31: 0.8870967741935484}
Micro-average F1 score: 0.8141592920353983
Weighted-average F1 score: 0.7494890967139545

F1 score per class: {1: 0.23214285714285715, 2: 0.7142857142857143, 3: 0.6825396825396826, 5: 0.9696969696969697, 6: 0.08928571428571429, 7: 0.07058823529411765, 8: 0.453781512605042, 9: 0.9803921568627451, 10: 0.3, 11: 0.08163265306122448, 12: 0.0, 14: 0.12631578947368421, 16: 0.7457627118644068, 17: 0.3076923076923077, 18: 0.12307692307692308, 19: 0.5251396648044693, 20: 0.6329113924050633, 22: 0.6296296296296297, 24: 0.1, 26: 0.7241379310344828, 27: 0.16666666666666666, 28: 0.375, 29: 0.9010989010989011, 30: 0.9230769230769231, 31: 0.0, 32: 0.703030303030303, 33: 0.4, 34: 0.15217391304347827, 36: 0.11428571428571428, 39: 0.11764705882352941, 40: 0.4462809917355372}
Micro-average F1 score: 0.48403483309143686
Weighted-average F1 score: 0.5297510853482184
F1 score per class: {1: 0.29333333333333333, 2: 0.6666666666666666, 3: 0.9, 5: 0.9433962264150944, 6: 0.08928571428571429, 7: 0.06060606060606061, 8: 0.4900662251655629, 9: 0.9803921568627451, 10: 0.5562913907284768, 11: 0.09615384615384616, 12: 0.5035971223021583, 14: 0.11235955056179775, 16: 0.8181818181818182, 17: 0.42857142857142855, 18: 0.288, 19: 0.5838509316770186, 20: 0.735632183908046, 22: 0.6704545454545454, 24: 0.08333333333333333, 26: 0.7675675675675676, 27: 0.14814814814814814, 28: 0.30303030303030304, 29: 0.907103825136612, 30: 0.9047619047619048, 31: 0.6666666666666666, 32: 0.8021390374331551, 33: 0.2608695652173913, 34: 0.16666666666666666, 36: 0.5168539325842697, 39: 0.10526315789473684, 40: 0.6111111111111112}
Micro-average F1 score: 0.5583884167453572
Weighted-average F1 score: 0.5593995471603966
F1 score per class: {1: 0.3170731707317073, 2: 0.7368421052631579, 3: 0.8846153846153846, 5: 0.9478672985781991, 6: 0.09090909090909091, 7: 0.07692307692307693, 8: 0.46540880503144655, 9: 0.9803921568627451, 10: 0.5, 11: 0.16666666666666666, 12: 0.368, 14: 0.14432989690721648, 16: 0.8181818181818182, 17: 0.42857142857142855, 18: 0.2616822429906542, 19: 0.5822784810126582, 20: 0.7640449438202247, 22: 0.6820809248554913, 24: 0.0851063829787234, 26: 0.7675675675675676, 27: 0.16216216216216217, 28: 0.3333333333333333, 29: 0.907103825136612, 30: 0.9047619047619048, 31: 0.8, 32: 0.7802197802197802, 33: 0.3, 34: 0.1414141414141414, 36: 0.4827586206896552, 39: 0.10526315789473684, 40: 0.5851063829787234}
Micro-average F1 score: 0.5517902813299232
Weighted-average F1 score: 0.5567251576830697

F1 score per class: {32: 0.0, 1: 0.0, 34: 0.5454545454545454, 3: 0.9259259259259259, 7: 0.0, 40: 0.0, 9: 0.0, 10: 0.0, 14: 0.0, 16: 0.0, 19: 0.2857142857142857, 22: 0.0, 26: 0.0, 27: 0.0, 31: 0.4090909090909091}
Micro-average F1 score: 0.380327868852459
Weighted-average F1 score: 0.30191720244076264
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.5, 8: 0.0, 9: 0.7936507936507936, 10: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.41379310344827586, 28: 0.0, 31: 0.2, 32: 0.0, 33: 0.0, 34: 0.0, 36: 0.0, 40: 0.5164319248826291}
Micro-average F1 score: 0.3775933609958506
Weighted-average F1 score: 0.3205662992615027
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.6153846153846154, 8: 0.0, 9: 0.7936507936507936, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.4444444444444444, 31: 0.3333333333333333, 32: 0.0, 34: 0.0, 36: 0.0, 40: 0.5092592592592593}
Micro-average F1 score: 0.39913232104121477
Weighted-average F1 score: 0.3435564328936087

F1 score per class: {1: 0.08306709265175719, 2: 0.20833333333333334, 3: 0.3944954128440367, 5: 0.8205128205128205, 6: 0.07936507936507936, 7: 0.02926829268292683, 8: 0.3103448275862069, 9: 0.9259259259259259, 10: 0.24489795918367346, 11: 0.07207207207207207, 12: 0.0, 14: 0.04938271604938271, 16: 0.43564356435643564, 17: 0.2857142857142857, 18: 0.07920792079207921, 19: 0.373015873015873, 20: 0.352112676056338, 22: 0.4163265306122449, 24: 0.046511627906976744, 26: 0.6086956521739131, 27: 0.0759493670886076, 28: 0.11538461538461539, 29: 0.7354260089686099, 30: 0.75, 31: 0.0, 32: 0.4978540772532189, 33: 0.375, 34: 0.09655172413793103, 36: 0.1, 39: 0.09090909090909091, 40: 0.21011673151750973}
Micro-average F1 score: 0.30936920222634506
Weighted-average F1 score: 0.2962736329883813
F1 score per class: {1: 0.10232558139534884, 2: 0.10218978102189781, 3: 0.2909090909090909, 5: 0.3472222222222222, 6: 0.07352941176470588, 7: 0.019867549668874173, 8: 0.19576719576719576, 9: 0.6756756756756757, 10: 0.2754098360655738, 11: 0.07142857142857142, 12: 0.19830028328611898, 14: 0.034482758620689655, 16: 0.39705882352941174, 17: 0.2608695652173913, 18: 0.075, 19: 0.3533834586466165, 20: 0.2723404255319149, 22: 0.4013605442176871, 24: 0.037037037037037035, 26: 0.6016949152542372, 27: 0.043795620437956206, 28: 0.06097560975609756, 29: 0.7248908296943232, 30: 0.6031746031746031, 31: 0.0182648401826484, 32: 0.49019607843137253, 33: 0.14634146341463414, 34: 0.08450704225352113, 36: 0.3262411347517731, 39: 0.05555555555555555, 40: 0.17857142857142858}
Micro-average F1 score: 0.23713407298489506
Weighted-average F1 score: 0.21981057798008893
F1 score per class: {1: 0.10196078431372549, 2: 0.11382113821138211, 3: 0.30131004366812225, 5: 0.3929273084479371, 6: 0.07874015748031496, 7: 0.027303754266211604, 8: 0.1662921348314607, 9: 0.7246376811594203, 10: 0.288, 11: 0.12080536912751678, 12: 0.18181818181818182, 14: 0.043076923076923075, 16: 0.39705882352941174, 17: 0.23076923076923078, 18: 0.0731070496083551, 19: 0.3770491803278688, 20: 0.27530364372469635, 22: 0.4083044982698962, 24: 0.0392156862745098, 26: 0.6200873362445415, 27: 0.04838709677419355, 28: 0.09195402298850575, 29: 0.7312775330396476, 30: 0.6333333333333333, 31: 0.03225806451612903, 32: 0.49477351916376305, 33: 0.2, 34: 0.07106598984771574, 36: 0.3181818181818182, 39: 0.06666666666666667, 40: 0.1751592356687898}
Micro-average F1 score: 0.24791726515369147
Weighted-average F1 score: 0.2294344239939773
cur_acc_wo_na:  ['0.8089', '0.6012', '0.4377', '0.6343', '0.4776', '0.5370']
his_acc_wo_na:  ['0.8089', '0.7210', '0.6510', '0.6041', '0.5395', '0.4840']
cur_acc des_wo_na:  ['0.8405', '0.8247', '0.7147', '0.7427', '0.5498', '0.8053']
his_acc des_wo_na:  ['0.8405', '0.7885', '0.7431', '0.6774', '0.6165', '0.5584']
cur_acc rrf_wo_na:  ['0.8405', '0.8247', '0.7273', '0.7184', '0.5139', '0.8142']
his_acc rrf_wo_na:  ['0.8405', '0.7844', '0.7433', '0.6711', '0.5913', '0.5518']
cur_acc_w_na:  ['0.6864', '0.4449', '0.2838', '0.4492', '0.2430', '0.3803']
his_acc_w_na:  ['0.6864', '0.5772', '0.4757', '0.4504', '0.3408', '0.3094']
cur_acc des_w_na:  ['0.6309', '0.4389', '0.3329', '0.3241', '0.2140', '0.3776']
his_acc des_w_na:  ['0.6309', '0.4917', '0.3972', '0.3425', '0.2681', '0.2371']
cur_acc rrf_w_na:  ['0.6313', '0.4489', '0.3483', '0.3179', '0.2159', '0.3991']
his_acc rrf_w_na:  ['0.6313', '0.4986', '0.4069', '0.3454', '0.2796', '0.2479']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 102.0678961CurrentTrain: epoch  0, batch     1 | loss: 84.8181271CurrentTrain: epoch  0, batch     2 | loss: 98.2182042CurrentTrain: epoch  0, batch     3 | loss: 53.1261142CurrentTrain: epoch  1, batch     0 | loss: 60.6376480CurrentTrain: epoch  1, batch     1 | loss: 81.2465612CurrentTrain: epoch  1, batch     2 | loss: 76.2824246CurrentTrain: epoch  1, batch     3 | loss: 77.6796928CurrentTrain: epoch  2, batch     0 | loss: 52.8057070CurrentTrain: epoch  2, batch     1 | loss: 56.4809691CurrentTrain: epoch  2, batch     2 | loss: 74.0983501CurrentTrain: epoch  2, batch     3 | loss: 83.6351534CurrentTrain: epoch  3, batch     0 | loss: 54.7847349CurrentTrain: epoch  3, batch     1 | loss: 87.7458643CurrentTrain: epoch  3, batch     2 | loss: 68.7956796CurrentTrain: epoch  3, batch     3 | loss: 58.3354892CurrentTrain: epoch  4, batch     0 | loss: 69.2863891CurrentTrain: epoch  4, batch     1 | loss: 52.1123039CurrentTrain: epoch  4, batch     2 | loss: 68.6755943CurrentTrain: epoch  4, batch     3 | loss: 59.5750865CurrentTrain: epoch  5, batch     0 | loss: 55.0852307CurrentTrain: epoch  5, batch     1 | loss: 83.9772395CurrentTrain: epoch  5, batch     2 | loss: 66.6050473CurrentTrain: epoch  5, batch     3 | loss: 45.3441043CurrentTrain: epoch  6, batch     0 | loss: 52.0008048CurrentTrain: epoch  6, batch     1 | loss: 86.8790028CurrentTrain: epoch  6, batch     2 | loss: 66.6365338CurrentTrain: epoch  6, batch     3 | loss: 35.2793376CurrentTrain: epoch  7, batch     0 | loss: 64.4517401CurrentTrain: epoch  7, batch     1 | loss: 81.4720656CurrentTrain: epoch  7, batch     2 | loss: 64.1225749CurrentTrain: epoch  7, batch     3 | loss: 76.6749637CurrentTrain: epoch  8, batch     0 | loss: 64.6781505CurrentTrain: epoch  8, batch     1 | loss: 52.0811096CurrentTrain: epoch  8, batch     2 | loss: 64.6832821CurrentTrain: epoch  8, batch     3 | loss: 76.4956918CurrentTrain: epoch  9, batch     0 | loss: 63.1391353CurrentTrain: epoch  9, batch     1 | loss: 61.7420806CurrentTrain: epoch  9, batch     2 | loss: 83.1780841CurrentTrain: epoch  9, batch     3 | loss: 56.6657354
MemoryTrain:  epoch  0, batch     0 | loss: 0.3898670MemoryTrain:  epoch  1, batch     0 | loss: 0.3792848MemoryTrain:  epoch  2, batch     0 | loss: 0.2582491MemoryTrain:  epoch  3, batch     0 | loss: 0.2220128MemoryTrain:  epoch  4, batch     0 | loss: 0.1763576MemoryTrain:  epoch  5, batch     0 | loss: 0.1304010MemoryTrain:  epoch  6, batch     0 | loss: 0.1236570MemoryTrain:  epoch  7, batch     0 | loss: 0.1178580MemoryTrain:  epoch  8, batch     0 | loss: 0.1017684MemoryTrain:  epoch  9, batch     0 | loss: 0.0822993

F1 score per class: {1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.8888888888888888, 38: 0.0, 8: 0.4927536231884058, 14: 0.0, 15: 0.0, 20: 0.9032258064516129, 25: 0.6046511627906976, 28: 0.4444444444444444}
Micro-average F1 score: 0.5976331360946746
Weighted-average F1 score: 0.5306561486452223
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 14: 0.0, 15: 0.8888888888888888, 20: 0.0, 25: 0.7, 28: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.9148936170212766, 36: 0.0, 37: 0.7755102040816326, 38: 0.6341463414634146}
Micro-average F1 score: 0.6615776081424937
Weighted-average F1 score: 0.5404418064435023
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 14: 0.0, 15: 0.8888888888888888, 20: 0.0, 25: 0.7, 28: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.9166666666666666, 36: 0.0, 37: 0.7755102040816326, 38: 0.7272727272727273}
Micro-average F1 score: 0.6871794871794872
Weighted-average F1 score: 0.5770733465744804

F1 score per class: {1: 0.27419354838709675, 2: 0.7142857142857143, 3: 0.6341463414634146, 5: 0.8847926267281107, 6: 0.05660377358490566, 7: 0.07407407407407407, 8: 0.34951456310679613, 9: 0.9803921568627451, 10: 0.3140495867768595, 11: 0.043478260869565216, 12: 0.0, 14: 0.12903225806451613, 15: 0.8421052631578947, 16: 0.8070175438596491, 17: 0.0, 18: 0.04878048780487805, 19: 0.49079754601226994, 20: 0.4594594594594595, 22: 0.609271523178808, 24: 0.08, 25: 0.4927536231884058, 26: 0.7528089887640449, 27: 0.10810810810810811, 28: 0.3157894736842105, 29: 0.9247311827956989, 30: 0.9230769230769231, 31: 0.0, 32: 0.6013071895424836, 33: 0.4, 34: 0.04819277108433735, 35: 0.6666666666666666, 36: 0.0, 37: 0.41935483870967744, 38: 0.2318840579710145, 39: 0.0, 40: 0.5391304347826087}
Micro-average F1 score: 0.47028086218158066
Weighted-average F1 score: 0.5370061920865361
F1 score per class: {1: 0.30303030303030304, 2: 0.6666666666666666, 3: 0.7272727272727273, 5: 0.8771929824561403, 6: 0.15789473684210525, 7: 0.08247422680412371, 8: 0.5333333333333333, 9: 0.9803921568627451, 10: 0.6107784431137725, 11: 0.0594059405940594, 12: 0.5070422535211268, 14: 0.08791208791208792, 15: 0.8, 16: 0.84375, 17: 0.3076923076923077, 18: 0.26229508196721313, 19: 0.5212121212121212, 20: 0.6823529411764706, 22: 0.7528089887640449, 24: 0.06666666666666667, 25: 0.6829268292682927, 26: 0.7555555555555555, 27: 0.15625, 28: 0.22857142857142856, 29: 0.907103825136612, 30: 0.9047619047619048, 31: 0.8, 32: 0.8020833333333334, 33: 0.3076923076923077, 34: 0.12371134020618557, 35: 0.7350427350427351, 36: 0.3170731707317073, 37: 0.35023041474654376, 38: 0.5, 39: 0.0, 40: 0.5325443786982249}
Micro-average F1 score: 0.5478767693588676
Weighted-average F1 score: 0.5551184295986464
F1 score per class: {1: 0.2912621359223301, 2: 0.8235294117647058, 3: 0.7285714285714285, 5: 0.8849557522123894, 6: 0.10909090909090909, 7: 0.07766990291262135, 8: 0.5421686746987951, 9: 0.9803921568627451, 10: 0.5342465753424658, 11: 0.06, 12: 0.3709677419354839, 14: 0.11764705882352941, 15: 0.8, 16: 0.8615384615384616, 17: 0.3076923076923077, 18: 0.27586206896551724, 19: 0.524390243902439, 20: 0.7191011235955056, 22: 0.7251461988304093, 24: 0.06666666666666667, 25: 0.6829268292682927, 26: 0.7555555555555555, 27: 0.16129032258064516, 28: 0.32, 29: 0.918918918918919, 30: 0.926829268292683, 31: 0.8, 32: 0.7333333333333333, 33: 0.2727272727272727, 34: 0.12121212121212122, 35: 0.6821705426356589, 36: 0.2597402597402597, 37: 0.3486238532110092, 38: 0.47058823529411764, 39: 0.0, 40: 0.5308641975308642}
Micro-average F1 score: 0.5328446574570059
Weighted-average F1 score: 0.5434502367273971

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 14: 0.0, 15: 0.8888888888888888, 16: 0.0, 19: 0.0, 20: 0.0, 25: 0.4927536231884058, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6942148760330579, 37: 0.5360824742268041, 38: 0.35555555555555557, 40: 0.0}
Micro-average F1 score: 0.3953033268101761
Weighted-average F1 score: 0.2902014839305299
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.6956521739130435, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.6436781609195402, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6935483870967742, 36: 0.0, 37: 0.5390070921985816, 38: 0.5652173913043478, 40: 0.0}
Micro-average F1 score: 0.2981651376146789
Weighted-average F1 score: 0.20273432736614208
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.7272727272727273, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.6666666666666666, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6567164179104478, 36: 0.0, 37: 0.5352112676056338, 38: 0.5333333333333333, 40: 0.0}
Micro-average F1 score: 0.32923832923832924
Weighted-average F1 score: 0.23739324528140596

F1 score per class: {1: 0.08564231738035265, 2: 0.20833333333333334, 3: 0.4126984126984127, 5: 0.75, 6: 0.05, 7: 0.02857142857142857, 8: 0.2857142857142857, 9: 0.9259259259259259, 10: 0.2331288343558282, 11: 0.037037037037037035, 12: 0.0, 14: 0.06818181818181818, 15: 0.5, 16: 0.4107142857142857, 17: 0.0, 18: 0.041666666666666664, 19: 0.35874439461883406, 20: 0.2677165354330709, 22: 0.47668393782383417, 24: 0.0425531914893617, 25: 0.4927536231884058, 26: 0.6118721461187214, 27: 0.045454545454545456, 28: 0.09836065573770492, 29: 0.7226890756302521, 30: 0.782608695652174, 31: 0.0, 32: 0.4791666666666667, 33: 0.35294117647058826, 34: 0.03636363636363636, 35: 0.25688073394495414, 36: 0.0, 37: 0.2826086956521739, 38: 0.08743169398907104, 39: 0.0, 40: 0.31313131313131315}
Micro-average F1 score: 0.299875052061641
Weighted-average F1 score: 0.29027708273512026
F1 score per class: {1: 0.08595988538681948, 2: 0.10852713178294573, 3: 0.20626151012891344, 5: 0.37453183520599254, 6: 0.12244897959183673, 7: 0.026490066225165563, 8: 0.20323325635103925, 9: 0.6756756756756757, 10: 0.2595419847328244, 11: 0.04285714285714286, 12: 0.1836734693877551, 14: 0.0299625468164794, 15: 0.18604651162790697, 16: 0.375, 17: 0.14285714285714285, 18: 0.05776173285198556, 19: 0.2925170068027211, 20: 0.3118279569892473, 22: 0.5447154471544715, 24: 0.03278688524590164, 25: 0.6292134831460674, 26: 0.5643153526970954, 27: 0.04716981132075472, 28: 0.043010752688172046, 29: 0.6613545816733067, 30: 0.5757575757575758, 31: 0.024390243902439025, 32: 0.46808510638297873, 33: 0.13333333333333333, 34: 0.07100591715976332, 35: 0.21393034825870647, 36: 0.203125, 37: 0.1156773211567732, 38: 0.174496644295302, 39: 0.0, 40: 0.15734265734265734}
Micro-average F1 score: 0.22648003671408903
Weighted-average F1 score: 0.21053373061297787
F1 score per class: {1: 0.08021390374331551, 2: 0.12173913043478261, 3: 0.27419354838709675, 5: 0.42462845010615713, 6: 0.08695652173913043, 7: 0.026143790849673203, 8: 0.22332506203473945, 9: 0.7692307692307693, 10: 0.2708333333333333, 11: 0.04316546762589928, 12: 0.17358490566037735, 14: 0.040268456375838924, 15: 0.22857142857142856, 16: 0.3708609271523179, 17: 0.19047619047619047, 18: 0.07048458149779736, 19: 0.30604982206405695, 20: 0.3316062176165803, 22: 0.5344827586206896, 24: 0.03571428571428571, 25: 0.6511627906976745, 26: 0.5938864628820961, 27: 0.04784688995215311, 28: 0.06299212598425197, 29: 0.6666666666666666, 30: 0.6229508196721312, 31: 0.03773584905660377, 32: 0.4731182795698925, 33: 0.13043478260869565, 34: 0.0736196319018405, 35: 0.17288801571709234, 36: 0.18691588785046728, 37: 0.11292719167904904, 38: 0.14285714285714285, 39: 0.0, 40: 0.16475095785440613}
Micro-average F1 score: 0.23399777144979572
Weighted-average F1 score: 0.21596072793790422
cur_acc_wo_na:  ['0.8089', '0.6012', '0.4377', '0.6343', '0.4776', '0.5370', '0.5976']
his_acc_wo_na:  ['0.8089', '0.7210', '0.6510', '0.6041', '0.5395', '0.4840', '0.4703']
cur_acc des_wo_na:  ['0.8405', '0.8247', '0.7147', '0.7427', '0.5498', '0.8053', '0.6616']
his_acc des_wo_na:  ['0.8405', '0.7885', '0.7431', '0.6774', '0.6165', '0.5584', '0.5479']
cur_acc rrf_wo_na:  ['0.8405', '0.8247', '0.7273', '0.7184', '0.5139', '0.8142', '0.6872']
his_acc rrf_wo_na:  ['0.8405', '0.7844', '0.7433', '0.6711', '0.5913', '0.5518', '0.5328']
cur_acc_w_na:  ['0.6864', '0.4449', '0.2838', '0.4492', '0.2430', '0.3803', '0.3953']
his_acc_w_na:  ['0.6864', '0.5772', '0.4757', '0.4504', '0.3408', '0.3094', '0.2999']
cur_acc des_w_na:  ['0.6309', '0.4389', '0.3329', '0.3241', '0.2140', '0.3776', '0.2982']
his_acc des_w_na:  ['0.6309', '0.4917', '0.3972', '0.3425', '0.2681', '0.2371', '0.2265']
cur_acc rrf_w_na:  ['0.6313', '0.4489', '0.3483', '0.3179', '0.2159', '0.3991', '0.3292']
his_acc rrf_w_na:  ['0.6313', '0.4986', '0.4069', '0.3454', '0.2796', '0.2479', '0.2340']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 97.8087851CurrentTrain: epoch  0, batch     1 | loss: 82.5035721CurrentTrain: epoch  0, batch     2 | loss: 82.8577012CurrentTrain: epoch  0, batch     3 | loss: 66.8057096CurrentTrain: epoch  1, batch     0 | loss: 73.3590453CurrentTrain: epoch  1, batch     1 | loss: 71.2547839CurrentTrain: epoch  1, batch     2 | loss: 90.1480059CurrentTrain: epoch  1, batch     3 | loss: 79.4047155CurrentTrain: epoch  2, batch     0 | loss: 66.8201191CurrentTrain: epoch  2, batch     1 | loss: 71.9011212CurrentTrain: epoch  2, batch     2 | loss: 66.9332910CurrentTrain: epoch  2, batch     3 | loss: 79.0730912CurrentTrain: epoch  3, batch     0 | loss: 70.2863166CurrentTrain: epoch  3, batch     1 | loss: 114.3629833CurrentTrain: epoch  3, batch     2 | loss: 57.4325487CurrentTrain: epoch  3, batch     3 | loss: 54.1032476CurrentTrain: epoch  4, batch     0 | loss: 69.0249442CurrentTrain: epoch  4, batch     1 | loss: 86.3067531CurrentTrain: epoch  4, batch     2 | loss: 82.1253633CurrentTrain: epoch  4, batch     3 | loss: 53.3450951CurrentTrain: epoch  5, batch     0 | loss: 89.3691425CurrentTrain: epoch  5, batch     1 | loss: 54.0373788CurrentTrain: epoch  5, batch     2 | loss: 54.3212874CurrentTrain: epoch  5, batch     3 | loss: 65.3085139CurrentTrain: epoch  6, batch     0 | loss: 50.2044645CurrentTrain: epoch  6, batch     1 | loss: 71.1168118CurrentTrain: epoch  6, batch     2 | loss: 83.7591279CurrentTrain: epoch  6, batch     3 | loss: 68.9936138CurrentTrain: epoch  7, batch     0 | loss: 61.4137971CurrentTrain: epoch  7, batch     1 | loss: 119.2698212CurrentTrain: epoch  7, batch     2 | loss: 54.1279358CurrentTrain: epoch  7, batch     3 | loss: 52.1479386CurrentTrain: epoch  8, batch     0 | loss: 62.2806756CurrentTrain: epoch  8, batch     1 | loss: 51.3168941CurrentTrain: epoch  8, batch     2 | loss: 86.9222499CurrentTrain: epoch  8, batch     3 | loss: 69.3326219CurrentTrain: epoch  9, batch     0 | loss: 115.5374809CurrentTrain: epoch  9, batch     1 | loss: 81.3157476CurrentTrain: epoch  9, batch     2 | loss: 61.7932752CurrentTrain: epoch  9, batch     3 | loss: 66.3674673
MemoryTrain:  epoch  0, batch     0 | loss: 0.2268495MemoryTrain:  epoch  1, batch     0 | loss: 0.2456570MemoryTrain:  epoch  2, batch     0 | loss: 0.2018376MemoryTrain:  epoch  3, batch     0 | loss: 0.1438576MemoryTrain:  epoch  4, batch     0 | loss: 0.1176698MemoryTrain:  epoch  5, batch     0 | loss: 0.1096411MemoryTrain:  epoch  6, batch     0 | loss: 0.0863288MemoryTrain:  epoch  7, batch     0 | loss: 0.0745434MemoryTrain:  epoch  8, batch     0 | loss: 0.0651038MemoryTrain:  epoch  9, batch     0 | loss: 0.0719991

F1 score per class: {0: 0.9428571428571428, 1: 0.0, 2: 0.0, 4: 0.9417989417989417, 13: 0.3333333333333333, 14: 0.0, 21: 0.17647058823529413, 22: 0.0, 23: 0.8674698795180723, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8202531645569621
Weighted-average F1 score: 0.8401488702576076
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.0, 1: 0.9473684210526315, 4: 0.0, 5: 0.0, 8: 0.0, 11: 0.8888888888888888, 13: 0.0, 14: 0.0, 15: 0.5581395348837209, 21: 0.0, 22: 0.7792207792207793, 23: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.8037383177570093
Weighted-average F1 score: 0.7423070219259084
F1 score per class: {0: 0.9863013698630136, 1: 0.0, 2: 0.0, 4: 0.9473684210526315, 5: 0.0, 8: 0.0, 11: 0.0, 13: 0.8888888888888888, 14: 0.0, 15: 0.0, 21: 0.5581395348837209, 22: 0.0, 23: 0.7792207792207793, 26: 0.0, 27: 0.0}
Micro-average F1 score: 0.8094117647058824
Weighted-average F1 score: 0.7531700515150679

F1 score per class: {0: 0.9295774647887324, 1: 0.2542372881355932, 2: 0.7368421052631579, 3: 0.5423728813559322, 4: 0.9417989417989417, 5: 0.9014084507042254, 6: 0.07339449541284404, 7: 0.06818181818181818, 8: 0.33663366336633666, 9: 0.9803921568627451, 10: 0.4696969696969697, 11: 0.06382978723404255, 12: 0.019801980198019802, 13: 0.06060606060606061, 14: 0.047058823529411764, 15: 0.75, 16: 0.8070175438596491, 17: 0.18181818181818182, 18: 0.04878048780487805, 19: 0.5414364640883977, 20: 0.6896551724137931, 21: 0.14285714285714285, 22: 0.581081081081081, 23: 0.8275862068965517, 24: 0.08695652173913043, 25: 0.5945945945945946, 26: 0.7126436781609196, 27: 0.25, 28: 0.2857142857142857, 29: 0.9139784946236559, 30: 0.972972972972973, 31: 0.8, 32: 0.6163522012578616, 33: 0.42857142857142855, 34: 0.04819277108433735, 35: 0.48333333333333334, 36: 0.058823529411764705, 37: 0.4094488188976378, 38: 0.36, 39: 0.0, 40: 0.4536082474226804}
Micro-average F1 score: 0.5119355766465343
Weighted-average F1 score: 0.5675302532054819
F1 score per class: {0: 0.972972972972973, 1: 0.3076923076923077, 2: 0.4827586206896552, 3: 0.7542857142857143, 4: 0.9473684210526315, 5: 0.8771929824561403, 6: 0.14035087719298245, 7: 0.08695652173913043, 8: 0.5369127516778524, 9: 0.9803921568627451, 10: 0.5174825174825175, 11: 0.0594059405940594, 12: 0.460431654676259, 13: 0.13333333333333333, 14: 0.16091954022988506, 15: 0.631578947368421, 16: 0.8571428571428571, 17: 0.4, 18: 0.29508196721311475, 19: 0.6145251396648045, 20: 0.7640449438202247, 21: 0.2727272727272727, 22: 0.5960264900662252, 23: 0.7407407407407407, 24: 0.06666666666666667, 25: 0.7294117647058823, 26: 0.6910994764397905, 27: 0.13114754098360656, 28: 0.23529411764705882, 29: 0.907103825136612, 30: 0.9473684210526315, 31: 0.6666666666666666, 32: 0.7978142076502732, 33: 0.2727272727272727, 34: 0.08791208791208792, 35: 0.7368421052631579, 36: 0.4318181818181818, 37: 0.3626373626373626, 38: 0.5, 39: 0.0, 40: 0.6442953020134228}
Micro-average F1 score: 0.5706412825651302
Weighted-average F1 score: 0.5750957565960423
F1 score per class: {0: 0.972972972972973, 1: 0.3018867924528302, 2: 0.4827586206896552, 3: 0.7530864197530864, 4: 0.9473684210526315, 5: 0.8771929824561403, 6: 0.09009009009009009, 7: 0.08333333333333333, 8: 0.5394736842105263, 9: 0.9803921568627451, 10: 0.4892086330935252, 11: 0.06, 12: 0.34146341463414637, 13: 0.12698412698412698, 14: 0.15217391304347827, 15: 0.631578947368421, 16: 0.8571428571428571, 17: 0.2857142857142857, 18: 0.3157894736842105, 19: 0.6057142857142858, 20: 0.7472527472527473, 21: 0.32432432432432434, 22: 0.6496815286624203, 23: 0.7228915662650602, 24: 0.07407407407407407, 25: 0.6987951807228916, 26: 0.6910994764397905, 27: 0.125, 28: 0.2857142857142857, 29: 0.9130434782608695, 30: 0.9743589743589743, 31: 0.8, 32: 0.7868852459016393, 33: 0.2857142857142857, 34: 0.10869565217391304, 35: 0.6782608695652174, 36: 0.4186046511627907, 37: 0.36065573770491804, 38: 0.4918032786885246, 39: 0.0, 40: 0.6258503401360545}
Micro-average F1 score: 0.5639211723092471
Weighted-average F1 score: 0.5723425112846642

F1 score per class: {0: 0.88, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9417989417989417, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 13: 0.09523809523809523, 14: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.16666666666666666, 22: 0.0, 23: 0.7659574468085106, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.5409015025041736
Weighted-average F1 score: 0.4105996007315268
F1 score per class: {0: 0.7128712871287128, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.12121212121212122, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3037974683544304, 22: 0.0, 23: 0.5825242718446602, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3333333333333333
Weighted-average F1 score: 0.2342627333429688
F1 score per class: {0: 0.72, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.11428571428571428, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3076923076923077, 22: 0.0, 23: 0.5882352941176471, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3457286432160804
Weighted-average F1 score: 0.2446490981504264

F1 score per class: {0: 0.3333333333333333, 1: 0.08498583569405099, 2: 0.2222222222222222, 3: 0.3137254901960784, 4: 0.9128205128205128, 5: 0.7384615384615385, 6: 0.0625, 7: 0.024390243902439025, 8: 0.25757575757575757, 9: 0.8771929824561403, 10: 0.2594142259414226, 11: 0.05660377358490566, 12: 0.018691588785046728, 13: 0.011428571428571429, 14: 0.03278688524590164, 15: 0.5, 16: 0.3709677419354839, 17: 0.15384615384615385, 18: 0.038461538461538464, 19: 0.3712121212121212, 20: 0.24896265560165975, 21: 0.09230769230769231, 22: 0.49710982658959535, 23: 0.6486486486486487, 24: 0.05128205128205128, 25: 0.5866666666666667, 26: 0.6108374384236454, 27: 0.09090909090909091, 28: 0.18181818181818182, 29: 0.6719367588932806, 30: 0.8181818181818182, 31: 0.03773584905660377, 32: 0.45794392523364486, 33: 0.4, 34: 0.041666666666666664, 35: 0.19795221843003413, 36: 0.05194805194805195, 37: 0.25120772946859904, 38: 0.13953488372093023, 39: 0.0, 40: 0.2875816993464052}
Micro-average F1 score: 0.31217116801122413
Weighted-average F1 score: 0.2935974233045047
F1 score per class: {0: 0.18556701030927836, 1: 0.08602150537634409, 2: 0.11290322580645161, 3: 0.16458852867830423, 4: 0.8530805687203792, 5: 0.31201248049922, 6: 0.1111111111111111, 7: 0.027210884353741496, 8: 0.22727272727272727, 9: 0.6666666666666666, 10: 0.22492401215805471, 11: 0.04477611940298507, 12: 0.16580310880829016, 13: 0.021447721179624665, 14: 0.07, 15: 0.2727272727272727, 16: 0.3776223776223776, 17: 0.15384615384615385, 18: 0.06474820143884892, 19: 0.3188405797101449, 20: 0.22149837133550487, 21: 0.07228915662650602, 22: 0.48128342245989303, 23: 0.375, 24: 0.03278688524590164, 25: 0.6326530612244898, 26: 0.5665236051502146, 27: 0.04060913705583756, 28: 0.044444444444444446, 29: 0.6561264822134387, 30: 0.6206896551724138, 31: 0.017857142857142856, 32: 0.44648318042813456, 33: 0.13636363636363635, 34: 0.05673758865248227, 35: 0.2048780487804878, 36: 0.2878787878787879, 37: 0.14666666666666667, 38: 0.1414141414141414, 39: 0.0, 40: 0.23587223587223588}
Micro-average F1 score: 0.22773168049585124
Weighted-average F1 score: 0.2076157880016965
F1 score per class: {0: 0.17777777777777778, 1: 0.08290155440414508, 2: 0.12727272727272726, 3: 0.21942446043165467, 4: 0.8571428571428571, 5: 0.3460207612456747, 6: 0.07142857142857142, 7: 0.025974025974025976, 8: 0.22777777777777777, 9: 0.684931506849315, 10: 0.21316614420062696, 11: 0.04580152671755725, 12: 0.1377049180327869, 13: 0.020202020202020204, 14: 0.06222222222222222, 15: 0.2727272727272727, 16: 0.3624161073825503, 17: 0.12903225806451613, 18: 0.08571428571428572, 19: 0.31268436578171094, 20: 0.21316614420062696, 21: 0.08921933085501858, 22: 0.5151515151515151, 23: 0.38461538461538464, 24: 0.04, 25: 0.6236559139784946, 26: 0.5764192139737991, 27: 0.04020100502512563, 28: 0.06349206349206349, 29: 0.6461538461538462, 30: 0.7037037037037037, 31: 0.024096385542168676, 32: 0.43636363636363634, 33: 0.14634146341463414, 34: 0.07407407407407407, 35: 0.1703056768558952, 36: 0.2975206611570248, 37: 0.14347826086956522, 38: 0.13574660633484162, 39: 0.0, 40: 0.24731182795698925}
Micro-average F1 score: 0.2352693159059766
Weighted-average F1 score: 0.21533161189128408
cur_acc_wo_na:  ['0.8089', '0.6012', '0.4377', '0.6343', '0.4776', '0.5370', '0.5976', '0.8203']
his_acc_wo_na:  ['0.8089', '0.7210', '0.6510', '0.6041', '0.5395', '0.4840', '0.4703', '0.5119']
cur_acc des_wo_na:  ['0.8405', '0.8247', '0.7147', '0.7427', '0.5498', '0.8053', '0.6616', '0.8037']
his_acc des_wo_na:  ['0.8405', '0.7885', '0.7431', '0.6774', '0.6165', '0.5584', '0.5479', '0.5706']
cur_acc rrf_wo_na:  ['0.8405', '0.8247', '0.7273', '0.7184', '0.5139', '0.8142', '0.6872', '0.8094']
his_acc rrf_wo_na:  ['0.8405', '0.7844', '0.7433', '0.6711', '0.5913', '0.5518', '0.5328', '0.5639']
cur_acc_w_na:  ['0.6864', '0.4449', '0.2838', '0.4492', '0.2430', '0.3803', '0.3953', '0.5409']
his_acc_w_na:  ['0.6864', '0.5772', '0.4757', '0.4504', '0.3408', '0.3094', '0.2999', '0.3122']
cur_acc des_w_na:  ['0.6309', '0.4389', '0.3329', '0.3241', '0.2140', '0.3776', '0.2982', '0.3333']
his_acc des_w_na:  ['0.6309', '0.4917', '0.3972', '0.3425', '0.2681', '0.2371', '0.2265', '0.2277']
cur_acc rrf_w_na:  ['0.6313', '0.4489', '0.3483', '0.3179', '0.2159', '0.3991', '0.3292', '0.3457']
his_acc rrf_w_na:  ['0.6313', '0.4986', '0.4069', '0.3454', '0.2796', '0.2479', '0.2340', '0.2353']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 80.0897453CurrentTrain: epoch  0, batch     1 | loss: 80.0202135CurrentTrain: epoch  0, batch     2 | loss: 99.5001116CurrentTrain: epoch  0, batch     3 | loss: 78.6758756CurrentTrain: epoch  0, batch     4 | loss: 81.8519624CurrentTrain: epoch  0, batch     5 | loss: 79.5642279CurrentTrain: epoch  0, batch     6 | loss: 79.0136687CurrentTrain: epoch  0, batch     7 | loss: 65.6510129CurrentTrain: epoch  0, batch     8 | loss: 78.3908804CurrentTrain: epoch  0, batch     9 | loss: 65.2244813CurrentTrain: epoch  0, batch    10 | loss: 76.9433949CurrentTrain: epoch  0, batch    11 | loss: 77.3194934CurrentTrain: epoch  0, batch    12 | loss: 127.5346787CurrentTrain: epoch  0, batch    13 | loss: 77.7999287CurrentTrain: epoch  0, batch    14 | loss: 65.4748720CurrentTrain: epoch  0, batch    15 | loss: 64.9603149CurrentTrain: epoch  0, batch    16 | loss: 64.9817883CurrentTrain: epoch  0, batch    17 | loss: 65.4591053CurrentTrain: epoch  0, batch    18 | loss: 126.0132219CurrentTrain: epoch  0, batch    19 | loss: 96.1621320CurrentTrain: epoch  0, batch    20 | loss: 76.7881366CurrentTrain: epoch  0, batch    21 | loss: 77.7315892CurrentTrain: epoch  0, batch    22 | loss: 64.7448671CurrentTrain: epoch  0, batch    23 | loss: 77.0716322CurrentTrain: epoch  0, batch    24 | loss: 64.3385743CurrentTrain: epoch  0, batch    25 | loss: 77.1662419CurrentTrain: epoch  0, batch    26 | loss: 64.5876649CurrentTrain: epoch  0, batch    27 | loss: 96.8023049CurrentTrain: epoch  0, batch    28 | loss: 188.0650417CurrentTrain: epoch  0, batch    29 | loss: 77.9652793CurrentTrain: epoch  0, batch    30 | loss: 77.1816269CurrentTrain: epoch  0, batch    31 | loss: 126.3753410CurrentTrain: epoch  0, batch    32 | loss: 125.1710375CurrentTrain: epoch  0, batch    33 | loss: 76.9523528CurrentTrain: epoch  0, batch    34 | loss: 76.3576762CurrentTrain: epoch  0, batch    35 | loss: 125.7300888CurrentTrain: epoch  0, batch    36 | loss: 95.0286932CurrentTrain: epoch  0, batch    37 | loss: 125.5729095CurrentTrain: epoch  0, batch    38 | loss: 55.2007275CurrentTrain: epoch  0, batch    39 | loss: 64.6884097CurrentTrain: epoch  0, batch    40 | loss: 64.5284861CurrentTrain: epoch  0, batch    41 | loss: 55.9309153CurrentTrain: epoch  0, batch    42 | loss: 76.2013810CurrentTrain: epoch  0, batch    43 | loss: 76.6105218CurrentTrain: epoch  0, batch    44 | loss: 76.7472314CurrentTrain: epoch  0, batch    45 | loss: 64.2672009CurrentTrain: epoch  0, batch    46 | loss: 95.2043179CurrentTrain: epoch  0, batch    47 | loss: 63.8670640CurrentTrain: epoch  0, batch    48 | loss: 76.2221379CurrentTrain: epoch  0, batch    49 | loss: 76.1929842CurrentTrain: epoch  0, batch    50 | loss: 125.0676955CurrentTrain: epoch  0, batch    51 | loss: 63.9376370CurrentTrain: epoch  0, batch    52 | loss: 64.8287977CurrentTrain: epoch  0, batch    53 | loss: 75.7518465CurrentTrain: epoch  0, batch    54 | loss: 125.4380938CurrentTrain: epoch  0, batch    55 | loss: 55.3001975CurrentTrain: epoch  0, batch    56 | loss: 94.8260167CurrentTrain: epoch  0, batch    57 | loss: 63.8328420CurrentTrain: epoch  0, batch    58 | loss: 94.4721391CurrentTrain: epoch  0, batch    59 | loss: 75.7036192CurrentTrain: epoch  0, batch    60 | loss: 94.1119586CurrentTrain: epoch  0, batch    61 | loss: 76.0126039CurrentTrain: epoch  0, batch    62 | loss: 64.1966228CurrentTrain: epoch  0, batch    63 | loss: 75.8244114CurrentTrain: epoch  0, batch    64 | loss: 75.9817169CurrentTrain: epoch  0, batch    65 | loss: 92.7758942CurrentTrain: epoch  0, batch    66 | loss: 74.8581997CurrentTrain: epoch  0, batch    67 | loss: 63.1959168CurrentTrain: epoch  0, batch    68 | loss: 92.3320272CurrentTrain: epoch  0, batch    69 | loss: 51.7446200CurrentTrain: epoch  0, batch    70 | loss: 74.1068932CurrentTrain: epoch  0, batch    71 | loss: 63.7046154CurrentTrain: epoch  0, batch    72 | loss: 75.1085185CurrentTrain: epoch  0, batch    73 | loss: 93.5196598CurrentTrain: epoch  0, batch    74 | loss: 61.2965869CurrentTrain: epoch  0, batch    75 | loss: 94.4669493CurrentTrain: epoch  0, batch    76 | loss: 93.8225472CurrentTrain: epoch  0, batch    77 | loss: 74.9366790CurrentTrain: epoch  0, batch    78 | loss: 62.1396589CurrentTrain: epoch  0, batch    79 | loss: 61.3910646CurrentTrain: epoch  0, batch    80 | loss: 73.4589488CurrentTrain: epoch  0, batch    81 | loss: 89.4092179CurrentTrain: epoch  0, batch    82 | loss: 122.3758375CurrentTrain: epoch  0, batch    83 | loss: 92.7850181CurrentTrain: epoch  0, batch    84 | loss: 73.7104377CurrentTrain: epoch  0, batch    85 | loss: 73.1409777CurrentTrain: epoch  0, batch    86 | loss: 90.6266443CurrentTrain: epoch  0, batch    87 | loss: 91.9612431CurrentTrain: epoch  0, batch    88 | loss: 60.2784332CurrentTrain: epoch  0, batch    89 | loss: 72.3630482CurrentTrain: epoch  0, batch    90 | loss: 72.2547347CurrentTrain: epoch  0, batch    91 | loss: 73.1620980CurrentTrain: epoch  0, batch    92 | loss: 72.1980953CurrentTrain: epoch  0, batch    93 | loss: 60.6753063CurrentTrain: epoch  0, batch    94 | loss: 91.9847931CurrentTrain: epoch  0, batch    95 | loss: 61.8755835CurrentTrain: epoch  1, batch     0 | loss: 72.1891631CurrentTrain: epoch  1, batch     1 | loss: 71.0367500CurrentTrain: epoch  1, batch     2 | loss: 73.9254849CurrentTrain: epoch  1, batch     3 | loss: 119.3546991CurrentTrain: epoch  1, batch     4 | loss: 59.0900896CurrentTrain: epoch  1, batch     5 | loss: 59.2476358CurrentTrain: epoch  1, batch     6 | loss: 72.0295097CurrentTrain: epoch  1, batch     7 | loss: 91.0538917CurrentTrain: epoch  1, batch     8 | loss: 58.6332045CurrentTrain: epoch  1, batch     9 | loss: 59.1078598CurrentTrain: epoch  1, batch    10 | loss: 56.9750763CurrentTrain: epoch  1, batch    11 | loss: 58.7683859CurrentTrain: epoch  1, batch    12 | loss: 59.2054657CurrentTrain: epoch  1, batch    13 | loss: 122.2242606CurrentTrain: epoch  1, batch    14 | loss: 73.4480350CurrentTrain: epoch  1, batch    15 | loss: 92.6795804CurrentTrain: epoch  1, batch    16 | loss: 51.1581186CurrentTrain: epoch  1, batch    17 | loss: 59.3347219CurrentTrain: epoch  1, batch    18 | loss: 59.0704185CurrentTrain: epoch  1, batch    19 | loss: 126.1918961CurrentTrain: epoch  1, batch    20 | loss: 187.5244198CurrentTrain: epoch  1, batch    21 | loss: 70.7764788CurrentTrain: epoch  1, batch    22 | loss: 88.0462581CurrentTrain: epoch  1, batch    23 | loss: 50.1069476CurrentTrain: epoch  1, batch    24 | loss: 121.7344597CurrentTrain: epoch  1, batch    25 | loss: 121.6815538CurrentTrain: epoch  1, batch    26 | loss: 48.6767285CurrentTrain: epoch  1, batch    27 | loss: 87.8238660CurrentTrain: epoch  1, batch    28 | loss: 71.4182762CurrentTrain: epoch  1, batch    29 | loss: 58.9719792CurrentTrain: epoch  1, batch    30 | loss: 69.3077754CurrentTrain: epoch  1, batch    31 | loss: 52.1016621CurrentTrain: epoch  1, batch    32 | loss: 70.3333083CurrentTrain: epoch  1, batch    33 | loss: 70.6555192CurrentTrain: epoch  1, batch    34 | loss: 88.7026692CurrentTrain: epoch  1, batch    35 | loss: 88.0732147CurrentTrain: epoch  1, batch    36 | loss: 47.6694975CurrentTrain: epoch  1, batch    37 | loss: 71.1390044CurrentTrain: epoch  1, batch    38 | loss: 70.8216640CurrentTrain: epoch  1, batch    39 | loss: 92.1093051CurrentTrain: epoch  1, batch    40 | loss: 173.7784103CurrentTrain: epoch  1, batch    41 | loss: 85.1941570CurrentTrain: epoch  1, batch    42 | loss: 59.4435967CurrentTrain: epoch  1, batch    43 | loss: 55.4887913CurrentTrain: epoch  1, batch    44 | loss: 90.1624680CurrentTrain: epoch  1, batch    45 | loss: 59.2849490CurrentTrain: epoch  1, batch    46 | loss: 70.7918103CurrentTrain: epoch  1, batch    47 | loss: 91.4914937CurrentTrain: epoch  1, batch    48 | loss: 67.9639917CurrentTrain: epoch  1, batch    49 | loss: 72.1572905CurrentTrain: epoch  1, batch    50 | loss: 70.7357267CurrentTrain: epoch  1, batch    51 | loss: 187.5095045CurrentTrain: epoch  1, batch    52 | loss: 49.6805363CurrentTrain: epoch  1, batch    53 | loss: 87.1138534CurrentTrain: epoch  1, batch    54 | loss: 70.7862567CurrentTrain: epoch  1, batch    55 | loss: 56.4673956CurrentTrain: epoch  1, batch    56 | loss: 49.6504281CurrentTrain: epoch  1, batch    57 | loss: 70.2356825CurrentTrain: epoch  1, batch    58 | loss: 50.6165785CurrentTrain: epoch  1, batch    59 | loss: 69.9338016CurrentTrain: epoch  1, batch    60 | loss: 69.9108458CurrentTrain: epoch  1, batch    61 | loss: 88.9297376CurrentTrain: epoch  1, batch    62 | loss: 183.6503004CurrentTrain: epoch  1, batch    63 | loss: 58.3753117CurrentTrain: epoch  1, batch    64 | loss: 121.8989728CurrentTrain: epoch  1, batch    65 | loss: 121.3849505CurrentTrain: epoch  1, batch    66 | loss: 57.3867386CurrentTrain: epoch  1, batch    67 | loss: 54.4115982CurrentTrain: epoch  1, batch    68 | loss: 69.2221067CurrentTrain: epoch  1, batch    69 | loss: 86.2475096CurrentTrain: epoch  1, batch    70 | loss: 69.5525030CurrentTrain: epoch  1, batch    71 | loss: 56.9084590CurrentTrain: epoch  1, batch    72 | loss: 75.0467240CurrentTrain: epoch  1, batch    73 | loss: 59.0784829CurrentTrain: epoch  1, batch    74 | loss: 72.5426695CurrentTrain: epoch  1, batch    75 | loss: 53.5145622CurrentTrain: epoch  1, batch    76 | loss: 182.4892422CurrentTrain: epoch  1, batch    77 | loss: 91.5791002CurrentTrain: epoch  1, batch    78 | loss: 47.8043590CurrentTrain: epoch  1, batch    79 | loss: 68.3892712CurrentTrain: epoch  1, batch    80 | loss: 55.7901928CurrentTrain: epoch  1, batch    81 | loss: 58.2438276CurrentTrain: epoch  1, batch    82 | loss: 56.2847114CurrentTrain: epoch  1, batch    83 | loss: 69.1826011CurrentTrain: epoch  1, batch    84 | loss: 85.6603472CurrentTrain: epoch  1, batch    85 | loss: 118.1993819CurrentTrain: epoch  1, batch    86 | loss: 124.3969570CurrentTrain: epoch  1, batch    87 | loss: 73.5327560CurrentTrain: epoch  1, batch    88 | loss: 70.1519064CurrentTrain: epoch  1, batch    89 | loss: 60.8847194CurrentTrain: epoch  1, batch    90 | loss: 74.9241208CurrentTrain: epoch  1, batch    91 | loss: 50.4437285CurrentTrain: epoch  1, batch    92 | loss: 87.7209010CurrentTrain: epoch  1, batch    93 | loss: 56.8247572CurrentTrain: epoch  1, batch    94 | loss: 180.0237580CurrentTrain: epoch  1, batch    95 | loss: 75.8941469CurrentTrain: epoch  2, batch     0 | loss: 89.7095651CurrentTrain: epoch  2, batch     1 | loss: 55.8958813CurrentTrain: epoch  2, batch     2 | loss: 56.8649248CurrentTrain: epoch  2, batch     3 | loss: 120.6533559CurrentTrain: epoch  2, batch     4 | loss: 119.4991284CurrentTrain: epoch  2, batch     5 | loss: 55.3182645CurrentTrain: epoch  2, batch     6 | loss: 118.9181406CurrentTrain: epoch  2, batch     7 | loss: 84.2832832CurrentTrain: epoch  2, batch     8 | loss: 89.3050665CurrentTrain: epoch  2, batch     9 | loss: 54.9300812CurrentTrain: epoch  2, batch    10 | loss: 69.6141585CurrentTrain: epoch  2, batch    11 | loss: 45.9888333CurrentTrain: epoch  2, batch    12 | loss: 56.1558726CurrentTrain: epoch  2, batch    13 | loss: 63.5868491CurrentTrain: epoch  2, batch    14 | loss: 55.1334546CurrentTrain: epoch  2, batch    15 | loss: 85.1774787CurrentTrain: epoch  2, batch    16 | loss: 68.3487251CurrentTrain: epoch  2, batch    17 | loss: 55.7455416CurrentTrain: epoch  2, batch    18 | loss: 60.2998760CurrentTrain: epoch  2, batch    19 | loss: 71.8863286CurrentTrain: epoch  2, batch    20 | loss: 65.9852954CurrentTrain: epoch  2, batch    21 | loss: 89.3414497CurrentTrain: epoch  2, batch    22 | loss: 70.4775185CurrentTrain: epoch  2, batch    23 | loss: 57.2307081CurrentTrain: epoch  2, batch    24 | loss: 46.7601223CurrentTrain: epoch  2, batch    25 | loss: 72.7394462CurrentTrain: epoch  2, batch    26 | loss: 70.6658336CurrentTrain: epoch  2, batch    27 | loss: 69.0992828CurrentTrain: epoch  2, batch    28 | loss: 67.6418331CurrentTrain: epoch  2, batch    29 | loss: 69.4748043CurrentTrain: epoch  2, batch    30 | loss: 89.8859556CurrentTrain: epoch  2, batch    31 | loss: 65.2288219CurrentTrain: epoch  2, batch    32 | loss: 46.8850782CurrentTrain: epoch  2, batch    33 | loss: 69.8740047CurrentTrain: epoch  2, batch    34 | loss: 58.8073578CurrentTrain: epoch  2, batch    35 | loss: 87.1515937CurrentTrain: epoch  2, batch    36 | loss: 48.7828900CurrentTrain: epoch  2, batch    37 | loss: 54.5846568CurrentTrain: epoch  2, batch    38 | loss: 87.5126597CurrentTrain: epoch  2, batch    39 | loss: 69.4012732CurrentTrain: epoch  2, batch    40 | loss: 56.4034450CurrentTrain: epoch  2, batch    41 | loss: 70.1476403CurrentTrain: epoch  2, batch    42 | loss: 69.1262771CurrentTrain: epoch  2, batch    43 | loss: 58.2908246CurrentTrain: epoch  2, batch    44 | loss: 67.2835274CurrentTrain: epoch  2, batch    45 | loss: 71.6040918CurrentTrain: epoch  2, batch    46 | loss: 85.7950643CurrentTrain: epoch  2, batch    47 | loss: 72.2300743CurrentTrain: epoch  2, batch    48 | loss: 68.6262170CurrentTrain: epoch  2, batch    49 | loss: 54.4796076CurrentTrain: epoch  2, batch    50 | loss: 120.6117861CurrentTrain: epoch  2, batch    51 | loss: 69.8126376CurrentTrain: epoch  2, batch    52 | loss: 90.4891949CurrentTrain: epoch  2, batch    53 | loss: 57.8773344CurrentTrain: epoch  2, batch    54 | loss: 66.0817253CurrentTrain: epoch  2, batch    55 | loss: 69.1049553CurrentTrain: epoch  2, batch    56 | loss: 53.9323513CurrentTrain: epoch  2, batch    57 | loss: 121.8715532CurrentTrain: epoch  2, batch    58 | loss: 67.9136241CurrentTrain: epoch  2, batch    59 | loss: 60.0511176CurrentTrain: epoch  2, batch    60 | loss: 182.0006387CurrentTrain: epoch  2, batch    61 | loss: 67.7135168CurrentTrain: epoch  2, batch    62 | loss: 86.0435875CurrentTrain: epoch  2, batch    63 | loss: 69.7997303CurrentTrain: epoch  2, batch    64 | loss: 67.0002935CurrentTrain: epoch  2, batch    65 | loss: 65.2576246CurrentTrain: epoch  2, batch    66 | loss: 70.9699499CurrentTrain: epoch  2, batch    67 | loss: 52.4287271CurrentTrain: epoch  2, batch    68 | loss: 86.9532559CurrentTrain: epoch  2, batch    69 | loss: 69.6372492CurrentTrain: epoch  2, batch    70 | loss: 56.2243780CurrentTrain: epoch  2, batch    71 | loss: 56.1468352CurrentTrain: epoch  2, batch    72 | loss: 115.6913239CurrentTrain: epoch  2, batch    73 | loss: 53.8405903CurrentTrain: epoch  2, batch    74 | loss: 86.6391180CurrentTrain: epoch  2, batch    75 | loss: 119.2858940CurrentTrain: epoch  2, batch    76 | loss: 88.9129263CurrentTrain: epoch  2, batch    77 | loss: 56.5497080CurrentTrain: epoch  2, batch    78 | loss: 57.3268177CurrentTrain: epoch  2, batch    79 | loss: 114.3748384CurrentTrain: epoch  2, batch    80 | loss: 65.2731831CurrentTrain: epoch  2, batch    81 | loss: 66.1696854CurrentTrain: epoch  2, batch    82 | loss: 86.2786732CurrentTrain: epoch  2, batch    83 | loss: 44.9291721CurrentTrain: epoch  2, batch    84 | loss: 88.9416446CurrentTrain: epoch  2, batch    85 | loss: 90.7208221CurrentTrain: epoch  2, batch    86 | loss: 83.8680356CurrentTrain: epoch  2, batch    87 | loss: 87.5327640CurrentTrain: epoch  2, batch    88 | loss: 54.6634933CurrentTrain: epoch  2, batch    89 | loss: 85.3242718CurrentTrain: epoch  2, batch    90 | loss: 67.9454093CurrentTrain: epoch  2, batch    91 | loss: 52.1526158CurrentTrain: epoch  2, batch    92 | loss: 56.0905712CurrentTrain: epoch  2, batch    93 | loss: 79.6258935CurrentTrain: epoch  2, batch    94 | loss: 70.2898207CurrentTrain: epoch  2, batch    95 | loss: 47.5956601CurrentTrain: epoch  3, batch     0 | loss: 50.9188169CurrentTrain: epoch  3, batch     1 | loss: 88.4035149CurrentTrain: epoch  3, batch     2 | loss: 68.8921988CurrentTrain: epoch  3, batch     3 | loss: 45.9273687CurrentTrain: epoch  3, batch     4 | loss: 70.3444907CurrentTrain: epoch  3, batch     5 | loss: 55.3959322CurrentTrain: epoch  3, batch     6 | loss: 65.9786772CurrentTrain: epoch  3, batch     7 | loss: 117.9505041CurrentTrain: epoch  3, batch     8 | loss: 118.2024695CurrentTrain: epoch  3, batch     9 | loss: 54.2446702CurrentTrain: epoch  3, batch    10 | loss: 67.9060982CurrentTrain: epoch  3, batch    11 | loss: 82.9834725CurrentTrain: epoch  3, batch    12 | loss: 56.1187782CurrentTrain: epoch  3, batch    13 | loss: 69.1799313CurrentTrain: epoch  3, batch    14 | loss: 44.3709410CurrentTrain: epoch  3, batch    15 | loss: 64.9038735CurrentTrain: epoch  3, batch    16 | loss: 67.3986949CurrentTrain: epoch  3, batch    17 | loss: 53.3451498CurrentTrain: epoch  3, batch    18 | loss: 45.3966043CurrentTrain: epoch  3, batch    19 | loss: 80.9481044CurrentTrain: epoch  3, batch    20 | loss: 87.5661211CurrentTrain: epoch  3, batch    21 | loss: 68.7374115CurrentTrain: epoch  3, batch    22 | loss: 52.7041846CurrentTrain: epoch  3, batch    23 | loss: 49.7118355CurrentTrain: epoch  3, batch    24 | loss: 88.6193235CurrentTrain: epoch  3, batch    25 | loss: 115.2330400CurrentTrain: epoch  3, batch    26 | loss: 54.3364161CurrentTrain: epoch  3, batch    27 | loss: 90.3375661CurrentTrain: epoch  3, batch    28 | loss: 54.0956664CurrentTrain: epoch  3, batch    29 | loss: 92.1520839CurrentTrain: epoch  3, batch    30 | loss: 118.4874808CurrentTrain: epoch  3, batch    31 | loss: 87.0593751CurrentTrain: epoch  3, batch    32 | loss: 53.5779795CurrentTrain: epoch  3, batch    33 | loss: 56.4446739CurrentTrain: epoch  3, batch    34 | loss: 88.7434975CurrentTrain: epoch  3, batch    35 | loss: 52.6378238CurrentTrain: epoch  3, batch    36 | loss: 65.1705230CurrentTrain: epoch  3, batch    37 | loss: 118.0180832CurrentTrain: epoch  3, batch    38 | loss: 69.7991783CurrentTrain: epoch  3, batch    39 | loss: 87.6991913CurrentTrain: epoch  3, batch    40 | loss: 52.6712718CurrentTrain: epoch  3, batch    41 | loss: 85.9763918CurrentTrain: epoch  3, batch    42 | loss: 69.9202532CurrentTrain: epoch  3, batch    43 | loss: 85.5645350CurrentTrain: epoch  3, batch    44 | loss: 56.6055701CurrentTrain: epoch  3, batch    45 | loss: 67.0256239CurrentTrain: epoch  3, batch    46 | loss: 54.6031413CurrentTrain: epoch  3, batch    47 | loss: 62.7745037CurrentTrain: epoch  3, batch    48 | loss: 44.6611310CurrentTrain: epoch  3, batch    49 | loss: 69.0313386CurrentTrain: epoch  3, batch    50 | loss: 53.9684110CurrentTrain: epoch  3, batch    51 | loss: 69.3798676CurrentTrain: epoch  3, batch    52 | loss: 120.3780768CurrentTrain: epoch  3, batch    53 | loss: 53.5158383CurrentTrain: epoch  3, batch    54 | loss: 72.3713100CurrentTrain: epoch  3, batch    55 | loss: 53.8526202CurrentTrain: epoch  3, batch    56 | loss: 85.0023583CurrentTrain: epoch  3, batch    57 | loss: 69.4015103CurrentTrain: epoch  3, batch    58 | loss: 46.4427206CurrentTrain: epoch  3, batch    59 | loss: 87.7137970CurrentTrain: epoch  3, batch    60 | loss: 65.8920893CurrentTrain: epoch  3, batch    61 | loss: 68.5169517CurrentTrain: epoch  3, batch    62 | loss: 86.1197299CurrentTrain: epoch  3, batch    63 | loss: 69.6201062CurrentTrain: epoch  3, batch    64 | loss: 53.0683636CurrentTrain: epoch  3, batch    65 | loss: 54.6403827CurrentTrain: epoch  3, batch    66 | loss: 45.7434208CurrentTrain: epoch  3, batch    67 | loss: 88.4173082CurrentTrain: epoch  3, batch    68 | loss: 53.5062992CurrentTrain: epoch  3, batch    69 | loss: 54.3259662CurrentTrain: epoch  3, batch    70 | loss: 52.8873713CurrentTrain: epoch  3, batch    71 | loss: 90.6260816CurrentTrain: epoch  3, batch    72 | loss: 67.4028258CurrentTrain: epoch  3, batch    73 | loss: 54.4380958CurrentTrain: epoch  3, batch    74 | loss: 70.6621287CurrentTrain: epoch  3, batch    75 | loss: 87.8055467CurrentTrain: epoch  3, batch    76 | loss: 66.7617012CurrentTrain: epoch  3, batch    77 | loss: 84.5271904CurrentTrain: epoch  3, batch    78 | loss: 55.5395062CurrentTrain: epoch  3, batch    79 | loss: 66.9843570CurrentTrain: epoch  3, batch    80 | loss: 68.6497626CurrentTrain: epoch  3, batch    81 | loss: 63.2525356CurrentTrain: epoch  3, batch    82 | loss: 57.3850209CurrentTrain: epoch  3, batch    83 | loss: 66.7617524CurrentTrain: epoch  3, batch    84 | loss: 66.8630487CurrentTrain: epoch  3, batch    85 | loss: 85.6354949CurrentTrain: epoch  3, batch    86 | loss: 67.3655566CurrentTrain: epoch  3, batch    87 | loss: 67.1006155CurrentTrain: epoch  3, batch    88 | loss: 85.8240978CurrentTrain: epoch  3, batch    89 | loss: 84.1350979CurrentTrain: epoch  3, batch    90 | loss: 65.3379664CurrentTrain: epoch  3, batch    91 | loss: 115.8404320CurrentTrain: epoch  3, batch    92 | loss: 86.1806981CurrentTrain: epoch  3, batch    93 | loss: 116.5720430CurrentTrain: epoch  3, batch    94 | loss: 113.6798971CurrentTrain: epoch  3, batch    95 | loss: 70.8551514CurrentTrain: epoch  4, batch     0 | loss: 84.0036966CurrentTrain: epoch  4, batch     1 | loss: 46.3273158CurrentTrain: epoch  4, batch     2 | loss: 51.4203863CurrentTrain: epoch  4, batch     3 | loss: 84.9137899CurrentTrain: epoch  4, batch     4 | loss: 65.6331457CurrentTrain: epoch  4, batch     5 | loss: 66.8316496CurrentTrain: epoch  4, batch     6 | loss: 66.8338950CurrentTrain: epoch  4, batch     7 | loss: 182.5027963CurrentTrain: epoch  4, batch     8 | loss: 69.8521820CurrentTrain: epoch  4, batch     9 | loss: 66.5204899CurrentTrain: epoch  4, batch    10 | loss: 65.7910266CurrentTrain: epoch  4, batch    11 | loss: 50.8803780CurrentTrain: epoch  4, batch    12 | loss: 54.6373057CurrentTrain: epoch  4, batch    13 | loss: 88.1757380CurrentTrain: epoch  4, batch    14 | loss: 66.5911216CurrentTrain: epoch  4, batch    15 | loss: 65.1174394CurrentTrain: epoch  4, batch    16 | loss: 54.3303664CurrentTrain: epoch  4, batch    17 | loss: 65.1275928CurrentTrain: epoch  4, batch    18 | loss: 115.8256312CurrentTrain: epoch  4, batch    19 | loss: 84.0822699CurrentTrain: epoch  4, batch    20 | loss: 61.6378209CurrentTrain: epoch  4, batch    21 | loss: 66.1344935CurrentTrain: epoch  4, batch    22 | loss: 41.3118032CurrentTrain: epoch  4, batch    23 | loss: 67.8780912CurrentTrain: epoch  4, batch    24 | loss: 82.6304072CurrentTrain: epoch  4, batch    25 | loss: 66.0313530CurrentTrain: epoch  4, batch    26 | loss: 62.3644666CurrentTrain: epoch  4, batch    27 | loss: 55.4770229CurrentTrain: epoch  4, batch    28 | loss: 68.7711449CurrentTrain: epoch  4, batch    29 | loss: 65.7000610CurrentTrain: epoch  4, batch    30 | loss: 85.0984602CurrentTrain: epoch  4, batch    31 | loss: 54.9755555CurrentTrain: epoch  4, batch    32 | loss: 54.8262559CurrentTrain: epoch  4, batch    33 | loss: 67.2382019CurrentTrain: epoch  4, batch    34 | loss: 84.8542909CurrentTrain: epoch  4, batch    35 | loss: 64.4856061CurrentTrain: epoch  4, batch    36 | loss: 85.0404719CurrentTrain: epoch  4, batch    37 | loss: 55.0103491CurrentTrain: epoch  4, batch    38 | loss: 65.6193747CurrentTrain: epoch  4, batch    39 | loss: 85.3650666CurrentTrain: epoch  4, batch    40 | loss: 72.1862095CurrentTrain: epoch  4, batch    41 | loss: 64.7011226CurrentTrain: epoch  4, batch    42 | loss: 84.8479314CurrentTrain: epoch  4, batch    43 | loss: 53.8260234CurrentTrain: epoch  4, batch    44 | loss: 117.7102816CurrentTrain: epoch  4, batch    45 | loss: 86.3432715CurrentTrain: epoch  4, batch    46 | loss: 49.1316249CurrentTrain: epoch  4, batch    47 | loss: 83.9325685CurrentTrain: epoch  4, batch    48 | loss: 52.9402085CurrentTrain: epoch  4, batch    49 | loss: 81.9759637CurrentTrain: epoch  4, batch    50 | loss: 54.4193844CurrentTrain: epoch  4, batch    51 | loss: 62.7391608CurrentTrain: epoch  4, batch    52 | loss: 118.6620723CurrentTrain: epoch  4, batch    53 | loss: 44.1005899CurrentTrain: epoch  4, batch    54 | loss: 66.7906198CurrentTrain: epoch  4, batch    55 | loss: 64.9688249CurrentTrain: epoch  4, batch    56 | loss: 45.6201344CurrentTrain: epoch  4, batch    57 | loss: 44.7123982CurrentTrain: epoch  4, batch    58 | loss: 46.1744088CurrentTrain: epoch  4, batch    59 | loss: 121.7481438CurrentTrain: epoch  4, batch    60 | loss: 115.3686480CurrentTrain: epoch  4, batch    61 | loss: 85.8054311CurrentTrain: epoch  4, batch    62 | loss: 86.6591529CurrentTrain: epoch  4, batch    63 | loss: 44.4687670CurrentTrain: epoch  4, batch    64 | loss: 119.6824624CurrentTrain: epoch  4, batch    65 | loss: 52.2547085CurrentTrain: epoch  4, batch    66 | loss: 68.5056965CurrentTrain: epoch  4, batch    67 | loss: 82.6268913CurrentTrain: epoch  4, batch    68 | loss: 85.1334885CurrentTrain: epoch  4, batch    69 | loss: 84.0562706CurrentTrain: epoch  4, batch    70 | loss: 68.4536667CurrentTrain: epoch  4, batch    71 | loss: 62.2859249CurrentTrain: epoch  4, batch    72 | loss: 47.7994572CurrentTrain: epoch  4, batch    73 | loss: 66.1948716CurrentTrain: epoch  4, batch    74 | loss: 116.1361431CurrentTrain: epoch  4, batch    75 | loss: 68.9242676CurrentTrain: epoch  4, batch    76 | loss: 118.0828954CurrentTrain: epoch  4, batch    77 | loss: 55.2920496CurrentTrain: epoch  4, batch    78 | loss: 43.7918224CurrentTrain: epoch  4, batch    79 | loss: 54.8225788CurrentTrain: epoch  4, batch    80 | loss: 68.0389596CurrentTrain: epoch  4, batch    81 | loss: 65.3106532CurrentTrain: epoch  4, batch    82 | loss: 85.4013389CurrentTrain: epoch  4, batch    83 | loss: 112.0624609CurrentTrain: epoch  4, batch    84 | loss: 63.4688790CurrentTrain: epoch  4, batch    85 | loss: 86.1804286CurrentTrain: epoch  4, batch    86 | loss: 116.6325293CurrentTrain: epoch  4, batch    87 | loss: 118.1356525CurrentTrain: epoch  4, batch    88 | loss: 55.9216846CurrentTrain: epoch  4, batch    89 | loss: 65.4635246CurrentTrain: epoch  4, batch    90 | loss: 56.2505096CurrentTrain: epoch  4, batch    91 | loss: 113.1751495CurrentTrain: epoch  4, batch    92 | loss: 58.8508593CurrentTrain: epoch  4, batch    93 | loss: 67.7053841CurrentTrain: epoch  4, batch    94 | loss: 55.8991300CurrentTrain: epoch  4, batch    95 | loss: 56.7348977CurrentTrain: epoch  5, batch     0 | loss: 68.1830468CurrentTrain: epoch  5, batch     1 | loss: 85.3792173CurrentTrain: epoch  5, batch     2 | loss: 55.2709696CurrentTrain: epoch  5, batch     3 | loss: 66.2872230CurrentTrain: epoch  5, batch     4 | loss: 82.9392814CurrentTrain: epoch  5, batch     5 | loss: 68.1573404CurrentTrain: epoch  5, batch     6 | loss: 53.5711878CurrentTrain: epoch  5, batch     7 | loss: 65.1103359CurrentTrain: epoch  5, batch     8 | loss: 118.0320598CurrentTrain: epoch  5, batch     9 | loss: 52.1379406CurrentTrain: epoch  5, batch    10 | loss: 68.0231354CurrentTrain: epoch  5, batch    11 | loss: 67.3234504CurrentTrain: epoch  5, batch    12 | loss: 61.0419207CurrentTrain: epoch  5, batch    13 | loss: 65.8424615CurrentTrain: epoch  5, batch    14 | loss: 67.8219306CurrentTrain: epoch  5, batch    15 | loss: 54.7623994CurrentTrain: epoch  5, batch    16 | loss: 67.5934334CurrentTrain: epoch  5, batch    17 | loss: 44.2287371CurrentTrain: epoch  5, batch    18 | loss: 42.0807320CurrentTrain: epoch  5, batch    19 | loss: 82.9676126CurrentTrain: epoch  5, batch    20 | loss: 84.5949999CurrentTrain: epoch  5, batch    21 | loss: 54.1981306CurrentTrain: epoch  5, batch    22 | loss: 54.7749456CurrentTrain: epoch  5, batch    23 | loss: 45.8337529CurrentTrain: epoch  5, batch    24 | loss: 86.9608636CurrentTrain: epoch  5, batch    25 | loss: 64.0525130CurrentTrain: epoch  5, batch    26 | loss: 83.3492903CurrentTrain: epoch  5, batch    27 | loss: 67.1662903CurrentTrain: epoch  5, batch    28 | loss: 53.4944927CurrentTrain: epoch  5, batch    29 | loss: 87.5916912CurrentTrain: epoch  5, batch    30 | loss: 86.4091621CurrentTrain: epoch  5, batch    31 | loss: 66.7451160CurrentTrain: epoch  5, batch    32 | loss: 67.5009418CurrentTrain: epoch  5, batch    33 | loss: 68.5385530CurrentTrain: epoch  5, batch    34 | loss: 67.4995963CurrentTrain: epoch  5, batch    35 | loss: 67.4146824CurrentTrain: epoch  5, batch    36 | loss: 64.0449128CurrentTrain: epoch  5, batch    37 | loss: 82.3455288CurrentTrain: epoch  5, batch    38 | loss: 66.7382525CurrentTrain: epoch  5, batch    39 | loss: 53.5802226CurrentTrain: epoch  5, batch    40 | loss: 64.5664386CurrentTrain: epoch  5, batch    41 | loss: 67.7882839CurrentTrain: epoch  5, batch    42 | loss: 63.3087653CurrentTrain: epoch  5, batch    43 | loss: 63.1728360CurrentTrain: epoch  5, batch    44 | loss: 68.7750915CurrentTrain: epoch  5, batch    45 | loss: 43.2871074CurrentTrain: epoch  5, batch    46 | loss: 65.2822461CurrentTrain: epoch  5, batch    47 | loss: 53.4134790CurrentTrain: epoch  5, batch    48 | loss: 116.1099670CurrentTrain: epoch  5, batch    49 | loss: 87.2585455CurrentTrain: epoch  5, batch    50 | loss: 66.1424011CurrentTrain: epoch  5, batch    51 | loss: 65.1195352CurrentTrain: epoch  5, batch    52 | loss: 62.5722615CurrentTrain: epoch  5, batch    53 | loss: 50.8307034CurrentTrain: epoch  5, batch    54 | loss: 54.7140037CurrentTrain: epoch  5, batch    55 | loss: 50.2933047CurrentTrain: epoch  5, batch    56 | loss: 80.0592253CurrentTrain: epoch  5, batch    57 | loss: 64.4249651CurrentTrain: epoch  5, batch    58 | loss: 118.8625871CurrentTrain: epoch  5, batch    59 | loss: 64.3849607CurrentTrain: epoch  5, batch    60 | loss: 82.6091871CurrentTrain: epoch  5, batch    61 | loss: 64.9136383CurrentTrain: epoch  5, batch    62 | loss: 67.8890423CurrentTrain: epoch  5, batch    63 | loss: 53.4816095CurrentTrain: epoch  5, batch    64 | loss: 52.2913157CurrentTrain: epoch  5, batch    65 | loss: 54.4274300CurrentTrain: epoch  5, batch    66 | loss: 67.1020618CurrentTrain: epoch  5, batch    67 | loss: 113.8971630CurrentTrain: epoch  5, batch    68 | loss: 82.7455742CurrentTrain: epoch  5, batch    69 | loss: 45.6025281CurrentTrain: epoch  5, batch    70 | loss: 52.9016413CurrentTrain: epoch  5, batch    71 | loss: 48.9153734CurrentTrain: epoch  5, batch    72 | loss: 50.2716202CurrentTrain: epoch  5, batch    73 | loss: 53.6587049CurrentTrain: epoch  5, batch    74 | loss: 67.3544609CurrentTrain: epoch  5, batch    75 | loss: 62.9459798CurrentTrain: epoch  5, batch    76 | loss: 77.8560969CurrentTrain: epoch  5, batch    77 | loss: 84.6622997CurrentTrain: epoch  5, batch    78 | loss: 65.8788412CurrentTrain: epoch  5, batch    79 | loss: 51.4249611CurrentTrain: epoch  5, batch    80 | loss: 68.0408183CurrentTrain: epoch  5, batch    81 | loss: 55.9822208CurrentTrain: epoch  5, batch    82 | loss: 65.4812145CurrentTrain: epoch  5, batch    83 | loss: 67.0217837CurrentTrain: epoch  5, batch    84 | loss: 82.1395567CurrentTrain: epoch  5, batch    85 | loss: 65.9761501CurrentTrain: epoch  5, batch    86 | loss: 63.3670438CurrentTrain: epoch  5, batch    87 | loss: 88.8336985CurrentTrain: epoch  5, batch    88 | loss: 81.7516616CurrentTrain: epoch  5, batch    89 | loss: 81.4731261CurrentTrain: epoch  5, batch    90 | loss: 44.3902610CurrentTrain: epoch  5, batch    91 | loss: 43.8460847CurrentTrain: epoch  5, batch    92 | loss: 85.5312343CurrentTrain: epoch  5, batch    93 | loss: 66.1483393CurrentTrain: epoch  5, batch    94 | loss: 54.1416537CurrentTrain: epoch  5, batch    95 | loss: 70.2902023CurrentTrain: epoch  6, batch     0 | loss: 64.6709334CurrentTrain: epoch  6, batch     1 | loss: 63.8695208CurrentTrain: epoch  6, batch     2 | loss: 84.8841552CurrentTrain: epoch  6, batch     3 | loss: 44.2968891CurrentTrain: epoch  6, batch     4 | loss: 41.2613045CurrentTrain: epoch  6, batch     5 | loss: 112.8614769CurrentTrain: epoch  6, batch     6 | loss: 65.6870647CurrentTrain: epoch  6, batch     7 | loss: 181.5444087CurrentTrain: epoch  6, batch     8 | loss: 52.3956369CurrentTrain: epoch  6, batch     9 | loss: 85.8263514CurrentTrain: epoch  6, batch    10 | loss: 68.8444629CurrentTrain: epoch  6, batch    11 | loss: 61.3466838CurrentTrain: epoch  6, batch    12 | loss: 83.2207832CurrentTrain: epoch  6, batch    13 | loss: 54.1102221CurrentTrain: epoch  6, batch    14 | loss: 87.2036004CurrentTrain: epoch  6, batch    15 | loss: 64.3106083CurrentTrain: epoch  6, batch    16 | loss: 63.9601996CurrentTrain: epoch  6, batch    17 | loss: 61.1457648CurrentTrain: epoch  6, batch    18 | loss: 65.3037370CurrentTrain: epoch  6, batch    19 | loss: 46.3343666CurrentTrain: epoch  6, batch    20 | loss: 43.2424497CurrentTrain: epoch  6, batch    21 | loss: 81.4992245CurrentTrain: epoch  6, batch    22 | loss: 65.8178386CurrentTrain: epoch  6, batch    23 | loss: 117.6274669CurrentTrain: epoch  6, batch    24 | loss: 53.5240794CurrentTrain: epoch  6, batch    25 | loss: 53.4878588CurrentTrain: epoch  6, batch    26 | loss: 113.6824338CurrentTrain: epoch  6, batch    27 | loss: 66.3222491CurrentTrain: epoch  6, batch    28 | loss: 66.4157856CurrentTrain: epoch  6, batch    29 | loss: 65.5909005CurrentTrain: epoch  6, batch    30 | loss: 63.8993258CurrentTrain: epoch  6, batch    31 | loss: 66.0100161CurrentTrain: epoch  6, batch    32 | loss: 67.5351691CurrentTrain: epoch  6, batch    33 | loss: 111.2903965CurrentTrain: epoch  6, batch    34 | loss: 65.1686399CurrentTrain: epoch  6, batch    35 | loss: 65.9213218CurrentTrain: epoch  6, batch    36 | loss: 84.5403097CurrentTrain: epoch  6, batch    37 | loss: 63.1732534CurrentTrain: epoch  6, batch    38 | loss: 51.5560053CurrentTrain: epoch  6, batch    39 | loss: 80.1341997CurrentTrain: epoch  6, batch    40 | loss: 43.8724818CurrentTrain: epoch  6, batch    41 | loss: 81.0908324CurrentTrain: epoch  6, batch    42 | loss: 51.4197178CurrentTrain: epoch  6, batch    43 | loss: 81.0578564CurrentTrain: epoch  6, batch    44 | loss: 85.4436835CurrentTrain: epoch  6, batch    45 | loss: 49.6172307CurrentTrain: epoch  6, batch    46 | loss: 41.7396060CurrentTrain: epoch  6, batch    47 | loss: 65.0766984CurrentTrain: epoch  6, batch    48 | loss: 51.1121319CurrentTrain: epoch  6, batch    49 | loss: 64.8223320CurrentTrain: epoch  6, batch    50 | loss: 53.9714820CurrentTrain: epoch  6, batch    51 | loss: 54.5764451CurrentTrain: epoch  6, batch    52 | loss: 43.8706205CurrentTrain: epoch  6, batch    53 | loss: 86.2433531CurrentTrain: epoch  6, batch    54 | loss: 85.0319869CurrentTrain: epoch  6, batch    55 | loss: 51.6994285CurrentTrain: epoch  6, batch    56 | loss: 43.0428646CurrentTrain: epoch  6, batch    57 | loss: 174.7836428CurrentTrain: epoch  6, batch    58 | loss: 65.4632557CurrentTrain: epoch  6, batch    59 | loss: 82.4701919CurrentTrain: epoch  6, batch    60 | loss: 120.4030612CurrentTrain: epoch  6, batch    61 | loss: 67.5330316CurrentTrain: epoch  6, batch    62 | loss: 83.8387495CurrentTrain: epoch  6, batch    63 | loss: 80.5127066CurrentTrain: epoch  6, batch    64 | loss: 53.5915343CurrentTrain: epoch  6, batch    65 | loss: 87.7580768CurrentTrain: epoch  6, batch    66 | loss: 63.5903439CurrentTrain: epoch  6, batch    67 | loss: 81.9152294CurrentTrain: epoch  6, batch    68 | loss: 63.2897883CurrentTrain: epoch  6, batch    69 | loss: 54.1390724CurrentTrain: epoch  6, batch    70 | loss: 66.7842742CurrentTrain: epoch  6, batch    71 | loss: 52.3871839CurrentTrain: epoch  6, batch    72 | loss: 64.4351956CurrentTrain: epoch  6, batch    73 | loss: 81.5722500CurrentTrain: epoch  6, batch    74 | loss: 65.7765203CurrentTrain: epoch  6, batch    75 | loss: 83.7671930CurrentTrain: epoch  6, batch    76 | loss: 53.8794534CurrentTrain: epoch  6, batch    77 | loss: 86.7055967CurrentTrain: epoch  6, batch    78 | loss: 53.1844645CurrentTrain: epoch  6, batch    79 | loss: 65.7291723CurrentTrain: epoch  6, batch    80 | loss: 83.6776278CurrentTrain: epoch  6, batch    81 | loss: 53.2445760CurrentTrain: epoch  6, batch    82 | loss: 52.0809501CurrentTrain: epoch  6, batch    83 | loss: 63.9547940CurrentTrain: epoch  6, batch    84 | loss: 50.7492319CurrentTrain: epoch  6, batch    85 | loss: 111.0694866CurrentTrain: epoch  6, batch    86 | loss: 52.3877416CurrentTrain: epoch  6, batch    87 | loss: 85.9376749CurrentTrain: epoch  6, batch    88 | loss: 113.4992541CurrentTrain: epoch  6, batch    89 | loss: 68.7300345CurrentTrain: epoch  6, batch    90 | loss: 65.9755101CurrentTrain: epoch  6, batch    91 | loss: 46.5154433CurrentTrain: epoch  6, batch    92 | loss: 64.7779446CurrentTrain: epoch  6, batch    93 | loss: 54.0151015CurrentTrain: epoch  6, batch    94 | loss: 63.4639395CurrentTrain: epoch  6, batch    95 | loss: 55.2598880CurrentTrain: epoch  7, batch     0 | loss: 52.8229151CurrentTrain: epoch  7, batch     1 | loss: 63.3899262CurrentTrain: epoch  7, batch     2 | loss: 83.8282147CurrentTrain: epoch  7, batch     3 | loss: 56.7025547CurrentTrain: epoch  7, batch     4 | loss: 84.2686323CurrentTrain: epoch  7, batch     5 | loss: 84.3735841CurrentTrain: epoch  7, batch     6 | loss: 82.8667795CurrentTrain: epoch  7, batch     7 | loss: 52.1348235CurrentTrain: epoch  7, batch     8 | loss: 78.9467605CurrentTrain: epoch  7, batch     9 | loss: 41.6163584CurrentTrain: epoch  7, batch    10 | loss: 81.0860654CurrentTrain: epoch  7, batch    11 | loss: 64.3934084CurrentTrain: epoch  7, batch    12 | loss: 113.4007899CurrentTrain: epoch  7, batch    13 | loss: 53.5868525CurrentTrain: epoch  7, batch    14 | loss: 64.1516390CurrentTrain: epoch  7, batch    15 | loss: 67.0532864CurrentTrain: epoch  7, batch    16 | loss: 60.9304819CurrentTrain: epoch  7, batch    17 | loss: 81.2752618CurrentTrain: epoch  7, batch    18 | loss: 80.3643234CurrentTrain: epoch  7, batch    19 | loss: 50.2422616CurrentTrain: epoch  7, batch    20 | loss: 113.5265776CurrentTrain: epoch  7, batch    21 | loss: 82.6318623CurrentTrain: epoch  7, batch    22 | loss: 84.3707505CurrentTrain: epoch  7, batch    23 | loss: 85.7717663CurrentTrain: epoch  7, batch    24 | loss: 65.7996422CurrentTrain: epoch  7, batch    25 | loss: 82.4843248CurrentTrain: epoch  7, batch    26 | loss: 82.5409448CurrentTrain: epoch  7, batch    27 | loss: 117.6971837CurrentTrain: epoch  7, batch    28 | loss: 54.2335730CurrentTrain: epoch  7, batch    29 | loss: 86.6099911CurrentTrain: epoch  7, batch    30 | loss: 64.5010821CurrentTrain: epoch  7, batch    31 | loss: 63.7158213CurrentTrain: epoch  7, batch    32 | loss: 51.1929759CurrentTrain: epoch  7, batch    33 | loss: 43.2637341CurrentTrain: epoch  7, batch    34 | loss: 54.1849396CurrentTrain: epoch  7, batch    35 | loss: 117.5008448CurrentTrain: epoch  7, batch    36 | loss: 84.0993034CurrentTrain: epoch  7, batch    37 | loss: 117.5026480CurrentTrain: epoch  7, batch    38 | loss: 50.6727670CurrentTrain: epoch  7, batch    39 | loss: 84.5975106CurrentTrain: epoch  7, batch    40 | loss: 63.5766325CurrentTrain: epoch  7, batch    41 | loss: 67.0510028CurrentTrain: epoch  7, batch    42 | loss: 84.1158087CurrentTrain: epoch  7, batch    43 | loss: 120.4404047CurrentTrain: epoch  7, batch    44 | loss: 63.4460004CurrentTrain: epoch  7, batch    45 | loss: 117.7886276CurrentTrain: epoch  7, batch    46 | loss: 62.9253334CurrentTrain: epoch  7, batch    47 | loss: 66.2747114CurrentTrain: epoch  7, batch    48 | loss: 42.5660980CurrentTrain: epoch  7, batch    49 | loss: 43.2165579CurrentTrain: epoch  7, batch    50 | loss: 81.1189199CurrentTrain: epoch  7, batch    51 | loss: 80.8026060CurrentTrain: epoch  7, batch    52 | loss: 84.0678408CurrentTrain: epoch  7, batch    53 | loss: 83.1429809CurrentTrain: epoch  7, batch    54 | loss: 66.8553238CurrentTrain: epoch  7, batch    55 | loss: 55.2833545CurrentTrain: epoch  7, batch    56 | loss: 118.1297015CurrentTrain: epoch  7, batch    57 | loss: 49.5462122CurrentTrain: epoch  7, batch    58 | loss: 65.6675553CurrentTrain: epoch  7, batch    59 | loss: 59.6191361CurrentTrain: epoch  7, batch    60 | loss: 62.0810808CurrentTrain: epoch  7, batch    61 | loss: 63.1745951CurrentTrain: epoch  7, batch    62 | loss: 66.5999054CurrentTrain: epoch  7, batch    63 | loss: 49.2884511CurrentTrain: epoch  7, batch    64 | loss: 51.2452593CurrentTrain: epoch  7, batch    65 | loss: 117.1255298CurrentTrain: epoch  7, batch    66 | loss: 65.8813875CurrentTrain: epoch  7, batch    67 | loss: 66.8503187CurrentTrain: epoch  7, batch    68 | loss: 63.7454016CurrentTrain: epoch  7, batch    69 | loss: 43.4319380CurrentTrain: epoch  7, batch    70 | loss: 81.8488521CurrentTrain: epoch  7, batch    71 | loss: 58.8437200CurrentTrain: epoch  7, batch    72 | loss: 181.8301401CurrentTrain: epoch  7, batch    73 | loss: 61.4607064CurrentTrain: epoch  7, batch    74 | loss: 52.4666598CurrentTrain: epoch  7, batch    75 | loss: 49.2306534CurrentTrain: epoch  7, batch    76 | loss: 50.0745359CurrentTrain: epoch  7, batch    77 | loss: 82.6923577CurrentTrain: epoch  7, batch    78 | loss: 64.4901649CurrentTrain: epoch  7, batch    79 | loss: 113.8608628CurrentTrain: epoch  7, batch    80 | loss: 40.0144944CurrentTrain: epoch  7, batch    81 | loss: 85.8337154CurrentTrain: epoch  7, batch    82 | loss: 63.3504054CurrentTrain: epoch  7, batch    83 | loss: 52.0998715CurrentTrain: epoch  7, batch    84 | loss: 79.8451943CurrentTrain: epoch  7, batch    85 | loss: 82.9916901CurrentTrain: epoch  7, batch    86 | loss: 84.0866155CurrentTrain: epoch  7, batch    87 | loss: 118.0749634CurrentTrain: epoch  7, batch    88 | loss: 82.6228319CurrentTrain: epoch  7, batch    89 | loss: 61.1079611CurrentTrain: epoch  7, batch    90 | loss: 66.9488558CurrentTrain: epoch  7, batch    91 | loss: 65.8323292CurrentTrain: epoch  7, batch    92 | loss: 66.8852892CurrentTrain: epoch  7, batch    93 | loss: 63.0286498CurrentTrain: epoch  7, batch    94 | loss: 48.5208735CurrentTrain: epoch  7, batch    95 | loss: 68.6188047CurrentTrain: epoch  8, batch     0 | loss: 51.5653096CurrentTrain: epoch  8, batch     1 | loss: 84.6099828CurrentTrain: epoch  8, batch     2 | loss: 50.7725962CurrentTrain: epoch  8, batch     3 | loss: 52.1783606CurrentTrain: epoch  8, batch     4 | loss: 62.3933290CurrentTrain: epoch  8, batch     5 | loss: 81.0899293CurrentTrain: epoch  8, batch     6 | loss: 50.3553017CurrentTrain: epoch  8, batch     7 | loss: 64.1739602CurrentTrain: epoch  8, batch     8 | loss: 49.0838567CurrentTrain: epoch  8, batch     9 | loss: 79.9984323CurrentTrain: epoch  8, batch    10 | loss: 64.1551692CurrentTrain: epoch  8, batch    11 | loss: 115.2512052CurrentTrain: epoch  8, batch    12 | loss: 63.0842380CurrentTrain: epoch  8, batch    13 | loss: 63.9911993CurrentTrain: epoch  8, batch    14 | loss: 63.1922271CurrentTrain: epoch  8, batch    15 | loss: 66.6845488CurrentTrain: epoch  8, batch    16 | loss: 52.2934261CurrentTrain: epoch  8, batch    17 | loss: 45.2907801CurrentTrain: epoch  8, batch    18 | loss: 66.9223753CurrentTrain: epoch  8, batch    19 | loss: 84.2054320CurrentTrain: epoch  8, batch    20 | loss: 117.4878902CurrentTrain: epoch  8, batch    21 | loss: 65.5094407CurrentTrain: epoch  8, batch    22 | loss: 67.4095826CurrentTrain: epoch  8, batch    23 | loss: 62.4028533CurrentTrain: epoch  8, batch    24 | loss: 82.6378652CurrentTrain: epoch  8, batch    25 | loss: 64.2354653CurrentTrain: epoch  8, batch    26 | loss: 66.1265130CurrentTrain: epoch  8, batch    27 | loss: 60.6436135CurrentTrain: epoch  8, batch    28 | loss: 63.8632531CurrentTrain: epoch  8, batch    29 | loss: 42.6339817CurrentTrain: epoch  8, batch    30 | loss: 65.4581320CurrentTrain: epoch  8, batch    31 | loss: 63.4757769CurrentTrain: epoch  8, batch    32 | loss: 64.2508962CurrentTrain: epoch  8, batch    33 | loss: 51.3267966CurrentTrain: epoch  8, batch    34 | loss: 113.5057984CurrentTrain: epoch  8, batch    35 | loss: 43.0557314CurrentTrain: epoch  8, batch    36 | loss: 42.1019878CurrentTrain: epoch  8, batch    37 | loss: 82.8207733CurrentTrain: epoch  8, batch    38 | loss: 67.1485078CurrentTrain: epoch  8, batch    39 | loss: 54.4131053CurrentTrain: epoch  8, batch    40 | loss: 49.3858735CurrentTrain: epoch  8, batch    41 | loss: 65.7716597CurrentTrain: epoch  8, batch    42 | loss: 117.4993448CurrentTrain: epoch  8, batch    43 | loss: 53.0871868CurrentTrain: epoch  8, batch    44 | loss: 87.4427791CurrentTrain: epoch  8, batch    45 | loss: 64.2064839CurrentTrain: epoch  8, batch    46 | loss: 65.6753171CurrentTrain: epoch  8, batch    47 | loss: 66.6742500CurrentTrain: epoch  8, batch    48 | loss: 86.1029629CurrentTrain: epoch  8, batch    49 | loss: 65.9355401CurrentTrain: epoch  8, batch    50 | loss: 43.3677851CurrentTrain: epoch  8, batch    51 | loss: 53.2108415CurrentTrain: epoch  8, batch    52 | loss: 54.1388468CurrentTrain: epoch  8, batch    53 | loss: 50.0857588CurrentTrain: epoch  8, batch    54 | loss: 85.4009191CurrentTrain: epoch  8, batch    55 | loss: 115.1888301CurrentTrain: epoch  8, batch    56 | loss: 66.3248311CurrentTrain: epoch  8, batch    57 | loss: 52.9982321CurrentTrain: epoch  8, batch    58 | loss: 68.4076118CurrentTrain: epoch  8, batch    59 | loss: 64.1665157CurrentTrain: epoch  8, batch    60 | loss: 65.9978493CurrentTrain: epoch  8, batch    61 | loss: 122.3961025CurrentTrain: epoch  8, batch    62 | loss: 50.4330177CurrentTrain: epoch  8, batch    63 | loss: 52.6474706CurrentTrain: epoch  8, batch    64 | loss: 52.6240608CurrentTrain: epoch  8, batch    65 | loss: 84.1285216CurrentTrain: epoch  8, batch    66 | loss: 114.7376625CurrentTrain: epoch  8, batch    67 | loss: 85.7820125CurrentTrain: epoch  8, batch    68 | loss: 63.8266339CurrentTrain: epoch  8, batch    69 | loss: 60.5719416CurrentTrain: epoch  8, batch    70 | loss: 62.2207425CurrentTrain: epoch  8, batch    71 | loss: 55.1539217CurrentTrain: epoch  8, batch    72 | loss: 82.7930055CurrentTrain: epoch  8, batch    73 | loss: 83.0002990CurrentTrain: epoch  8, batch    74 | loss: 50.3970627CurrentTrain: epoch  8, batch    75 | loss: 64.3373840CurrentTrain: epoch  8, batch    76 | loss: 64.6661265CurrentTrain: epoch  8, batch    77 | loss: 63.6222300CurrentTrain: epoch  8, batch    78 | loss: 65.1550172CurrentTrain: epoch  8, batch    79 | loss: 84.2362701CurrentTrain: epoch  8, batch    80 | loss: 51.2980968CurrentTrain: epoch  8, batch    81 | loss: 64.0708571CurrentTrain: epoch  8, batch    82 | loss: 63.8178020CurrentTrain: epoch  8, batch    83 | loss: 65.4169702CurrentTrain: epoch  8, batch    84 | loss: 115.3630743CurrentTrain: epoch  8, batch    85 | loss: 66.4366929CurrentTrain: epoch  8, batch    86 | loss: 117.4572011CurrentTrain: epoch  8, batch    87 | loss: 54.2016531CurrentTrain: epoch  8, batch    88 | loss: 65.5187549CurrentTrain: epoch  8, batch    89 | loss: 80.7669950CurrentTrain: epoch  8, batch    90 | loss: 63.1790376CurrentTrain: epoch  8, batch    91 | loss: 54.1233717CurrentTrain: epoch  8, batch    92 | loss: 80.0765960CurrentTrain: epoch  8, batch    93 | loss: 85.8837453CurrentTrain: epoch  8, batch    94 | loss: 41.9010304CurrentTrain: epoch  8, batch    95 | loss: 70.8597532CurrentTrain: epoch  9, batch     0 | loss: 117.4943810CurrentTrain: epoch  9, batch     1 | loss: 117.4691200CurrentTrain: epoch  9, batch     2 | loss: 49.0979176CurrentTrain: epoch  9, batch     3 | loss: 65.5275903CurrentTrain: epoch  9, batch     4 | loss: 117.5085417CurrentTrain: epoch  9, batch     5 | loss: 43.9250122CurrentTrain: epoch  9, batch     6 | loss: 53.1122566CurrentTrain: epoch  9, batch     7 | loss: 50.7449206CurrentTrain: epoch  9, batch     8 | loss: 51.8354382CurrentTrain: epoch  9, batch     9 | loss: 64.7078195CurrentTrain: epoch  9, batch    10 | loss: 49.7977143CurrentTrain: epoch  9, batch    11 | loss: 50.4422250CurrentTrain: epoch  9, batch    12 | loss: 42.6732142CurrentTrain: epoch  9, batch    13 | loss: 113.3778293CurrentTrain: epoch  9, batch    14 | loss: 81.2739783CurrentTrain: epoch  9, batch    15 | loss: 52.1042596CurrentTrain: epoch  9, batch    16 | loss: 64.0981098CurrentTrain: epoch  9, batch    17 | loss: 63.5250472CurrentTrain: epoch  9, batch    18 | loss: 81.4621220CurrentTrain: epoch  9, batch    19 | loss: 53.0213042CurrentTrain: epoch  9, batch    20 | loss: 64.6651074CurrentTrain: epoch  9, batch    21 | loss: 53.0192802CurrentTrain: epoch  9, batch    22 | loss: 63.2775468CurrentTrain: epoch  9, batch    23 | loss: 82.5139821CurrentTrain: epoch  9, batch    24 | loss: 112.2964065CurrentTrain: epoch  9, batch    25 | loss: 50.7393362CurrentTrain: epoch  9, batch    26 | loss: 60.7652903CurrentTrain: epoch  9, batch    27 | loss: 65.6694286CurrentTrain: epoch  9, batch    28 | loss: 62.1599481CurrentTrain: epoch  9, batch    29 | loss: 49.9121185CurrentTrain: epoch  9, batch    30 | loss: 64.0384798CurrentTrain: epoch  9, batch    31 | loss: 84.0037290CurrentTrain: epoch  9, batch    32 | loss: 41.6196178CurrentTrain: epoch  9, batch    33 | loss: 42.1196397CurrentTrain: epoch  9, batch    34 | loss: 49.0195579CurrentTrain: epoch  9, batch    35 | loss: 61.9135503CurrentTrain: epoch  9, batch    36 | loss: 42.1974286CurrentTrain: epoch  9, batch    37 | loss: 52.8879914CurrentTrain: epoch  9, batch    38 | loss: 71.0920700CurrentTrain: epoch  9, batch    39 | loss: 65.7571230CurrentTrain: epoch  9, batch    40 | loss: 55.3214815CurrentTrain: epoch  9, batch    41 | loss: 41.6623379CurrentTrain: epoch  9, batch    42 | loss: 82.5177346CurrentTrain: epoch  9, batch    43 | loss: 53.1946318CurrentTrain: epoch  9, batch    44 | loss: 114.6532270CurrentTrain: epoch  9, batch    45 | loss: 62.3152110CurrentTrain: epoch  9, batch    46 | loss: 64.0891671CurrentTrain: epoch  9, batch    47 | loss: 117.4679615CurrentTrain: epoch  9, batch    48 | loss: 66.2805264CurrentTrain: epoch  9, batch    49 | loss: 84.1027807CurrentTrain: epoch  9, batch    50 | loss: 65.4287396CurrentTrain: epoch  9, batch    51 | loss: 62.9063092CurrentTrain: epoch  9, batch    52 | loss: 84.0719018CurrentTrain: epoch  9, batch    53 | loss: 64.2562656CurrentTrain: epoch  9, batch    54 | loss: 42.3236450CurrentTrain: epoch  9, batch    55 | loss: 86.1089561CurrentTrain: epoch  9, batch    56 | loss: 79.4682725CurrentTrain: epoch  9, batch    57 | loss: 84.1919283CurrentTrain: epoch  9, batch    58 | loss: 82.3608738CurrentTrain: epoch  9, batch    59 | loss: 50.5741672CurrentTrain: epoch  9, batch    60 | loss: 62.1385438CurrentTrain: epoch  9, batch    61 | loss: 63.0492893CurrentTrain: epoch  9, batch    62 | loss: 66.2241114CurrentTrain: epoch  9, batch    63 | loss: 117.7654024CurrentTrain: epoch  9, batch    64 | loss: 85.7530543CurrentTrain: epoch  9, batch    65 | loss: 54.2054788CurrentTrain: epoch  9, batch    66 | loss: 52.1889527CurrentTrain: epoch  9, batch    67 | loss: 67.1939380CurrentTrain: epoch  9, batch    68 | loss: 84.1713294CurrentTrain: epoch  9, batch    69 | loss: 60.4236026CurrentTrain: epoch  9, batch    70 | loss: 65.6313031CurrentTrain: epoch  9, batch    71 | loss: 84.5193968CurrentTrain: epoch  9, batch    72 | loss: 84.5479436CurrentTrain: epoch  9, batch    73 | loss: 82.6959879CurrentTrain: epoch  9, batch    74 | loss: 82.5404532CurrentTrain: epoch  9, batch    75 | loss: 49.7524513CurrentTrain: epoch  9, batch    76 | loss: 64.5921887CurrentTrain: epoch  9, batch    77 | loss: 65.1344156CurrentTrain: epoch  9, batch    78 | loss: 116.0137479CurrentTrain: epoch  9, batch    79 | loss: 62.8957875CurrentTrain: epoch  9, batch    80 | loss: 65.9188191CurrentTrain: epoch  9, batch    81 | loss: 68.5169518CurrentTrain: epoch  9, batch    82 | loss: 51.0112457CurrentTrain: epoch  9, batch    83 | loss: 53.2014526CurrentTrain: epoch  9, batch    84 | loss: 54.7541625CurrentTrain: epoch  9, batch    85 | loss: 51.1154460CurrentTrain: epoch  9, batch    86 | loss: 64.1966265CurrentTrain: epoch  9, batch    87 | loss: 80.6603102CurrentTrain: epoch  9, batch    88 | loss: 65.4676904CurrentTrain: epoch  9, batch    89 | loss: 51.6555008CurrentTrain: epoch  9, batch    90 | loss: 84.1925470CurrentTrain: epoch  9, batch    91 | loss: 81.4439566CurrentTrain: epoch  9, batch    92 | loss: 61.7226128CurrentTrain: epoch  9, batch    93 | loss: 87.3637538CurrentTrain: epoch  9, batch    94 | loss: 86.0872954CurrentTrain: epoch  9, batch    95 | loss: 96.2531661

F1 score per class: {32: 0.6853932584269663, 6: 0.7577639751552795, 19: 0.4166666666666667, 24: 0.7789473684210526, 26: 0.9361702127659575, 29: 0.9035532994923858}
Micro-average F1 score: 0.8059701492537313
Weighted-average F1 score: 0.8150409251757995
F1 score per class: {32: 0.7821782178217822, 6: 0.8372093023255814, 19: 0.4827586206896552, 24: 0.7608695652173914, 26: 0.9641025641025641, 29: 0.898989898989899}
Micro-average F1 score: 0.8387755102040816
Weighted-average F1 score: 0.8419380735538191
F1 score per class: {32: 0.7821782178217822, 6: 0.8372093023255814, 19: 0.4827586206896552, 24: 0.7608695652173914, 26: 0.9641025641025641, 29: 0.898989898989899}
Micro-average F1 score: 0.8387755102040816
Weighted-average F1 score: 0.8419380735538191

F1 score per class: {32: 0.6853932584269663, 6: 0.7577639751552795, 19: 0.4166666666666667, 24: 0.7789473684210526, 26: 0.9361702127659575, 29: 0.9035532994923858}
Micro-average F1 score: 0.8059701492537313
Weighted-average F1 score: 0.8150409251757995
F1 score per class: {32: 0.7821782178217822, 6: 0.8372093023255814, 19: 0.4827586206896552, 24: 0.7608695652173914, 26: 0.9641025641025641, 29: 0.898989898989899}
Micro-average F1 score: 0.8387755102040816
Weighted-average F1 score: 0.8419380735538191
F1 score per class: {32: 0.7821782178217822, 6: 0.8372093023255814, 19: 0.4827586206896552, 24: 0.7608695652173914, 26: 0.9641025641025641, 29: 0.898989898989899}
Micro-average F1 score: 0.8387755102040816
Weighted-average F1 score: 0.8419380735538191

F1 score per class: {32: 0.4959349593495935, 6: 0.6224489795918368, 19: 0.23255813953488372, 24: 0.7184466019417476, 26: 0.8979591836734694, 29: 0.7911111111111111}
Micro-average F1 score: 0.6798561151079137
Weighted-average F1 score: 0.6720940571070644
F1 score per class: {32: 0.48466257668711654, 6: 0.6792452830188679, 19: 0.14736842105263157, 24: 0.6862745098039216, 26: 0.8952380952380953, 29: 0.7355371900826446}
Micro-average F1 score: 0.6377036462373933
Weighted-average F1 score: 0.611221274887144
F1 score per class: {32: 0.48615384615384616, 6: 0.6792452830188679, 19: 0.15384615384615385, 24: 0.6862745098039216, 26: 0.8952380952380953, 29: 0.7325102880658436}
Micro-average F1 score: 0.6396887159533073
Weighted-average F1 score: 0.6144570625850219

F1 score per class: {32: 0.4959349593495935, 6: 0.6224489795918368, 19: 0.23255813953488372, 24: 0.7184466019417476, 26: 0.8979591836734694, 29: 0.7911111111111111}
Micro-average F1 score: 0.6798561151079137
Weighted-average F1 score: 0.6720940571070644
F1 score per class: {32: 0.48466257668711654, 6: 0.6792452830188679, 19: 0.14736842105263157, 24: 0.6862745098039216, 26: 0.8952380952380953, 29: 0.7355371900826446}
Micro-average F1 score: 0.6377036462373933
Weighted-average F1 score: 0.611221274887144
F1 score per class: {32: 0.48615384615384616, 6: 0.6792452830188679, 19: 0.15384615384615385, 24: 0.6862745098039216, 26: 0.8952380952380953, 29: 0.7325102880658436}
Micro-average F1 score: 0.6396887159533073
Weighted-average F1 score: 0.6144570625850219
cur_acc_wo_na:  ['0.8060']
his_acc_wo_na:  ['0.8060']
cur_acc des_wo_na:  ['0.8388']
his_acc des_wo_na:  ['0.8388']
cur_acc rrf_wo_na:  ['0.8388']
his_acc rrf_wo_na:  ['0.8388']
cur_acc_w_na:  ['0.6799']
his_acc_w_na:  ['0.6799']
cur_acc des_w_na:  ['0.6377']
his_acc des_w_na:  ['0.6377']
cur_acc rrf_w_na:  ['0.6397']
his_acc rrf_w_na:  ['0.6397']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 127.3210910CurrentTrain: epoch  0, batch     1 | loss: 98.2260005CurrentTrain: epoch  0, batch     2 | loss: 76.4682176CurrentTrain: epoch  0, batch     3 | loss: 77.7633210CurrentTrain: epoch  0, batch     4 | loss: 49.7176029CurrentTrain: epoch  1, batch     0 | loss: 76.9505479CurrentTrain: epoch  1, batch     1 | loss: 93.6921985CurrentTrain: epoch  1, batch     2 | loss: 75.1661168CurrentTrain: epoch  1, batch     3 | loss: 72.6409352CurrentTrain: epoch  1, batch     4 | loss: 59.5626300CurrentTrain: epoch  2, batch     0 | loss: 89.2614739CurrentTrain: epoch  2, batch     1 | loss: 72.8557069CurrentTrain: epoch  2, batch     2 | loss: 122.0645743CurrentTrain: epoch  2, batch     3 | loss: 58.5018560CurrentTrain: epoch  2, batch     4 | loss: 77.2467109CurrentTrain: epoch  3, batch     0 | loss: 121.2274698CurrentTrain: epoch  3, batch     1 | loss: 70.7502386CurrentTrain: epoch  3, batch     2 | loss: 89.7633833CurrentTrain: epoch  3, batch     3 | loss: 118.9410464CurrentTrain: epoch  3, batch     4 | loss: 42.8741487CurrentTrain: epoch  4, batch     0 | loss: 87.4221719CurrentTrain: epoch  4, batch     1 | loss: 68.3119342CurrentTrain: epoch  4, batch     2 | loss: 70.1403048CurrentTrain: epoch  4, batch     3 | loss: 119.4531157CurrentTrain: epoch  4, batch     4 | loss: 34.7945232CurrentTrain: epoch  5, batch     0 | loss: 117.3810622CurrentTrain: epoch  5, batch     1 | loss: 54.5740460CurrentTrain: epoch  5, batch     2 | loss: 117.1286887CurrentTrain: epoch  5, batch     3 | loss: 66.8447622CurrentTrain: epoch  5, batch     4 | loss: 53.6991686CurrentTrain: epoch  6, batch     0 | loss: 64.6085915CurrentTrain: epoch  6, batch     1 | loss: 119.1467369CurrentTrain: epoch  6, batch     2 | loss: 66.8403972CurrentTrain: epoch  6, batch     3 | loss: 68.7637732CurrentTrain: epoch  6, batch     4 | loss: 40.8922456CurrentTrain: epoch  7, batch     0 | loss: 85.3668482CurrentTrain: epoch  7, batch     1 | loss: 50.7902244CurrentTrain: epoch  7, batch     2 | loss: 181.9733381CurrentTrain: epoch  7, batch     3 | loss: 85.6525633CurrentTrain: epoch  7, batch     4 | loss: 52.0158742CurrentTrain: epoch  8, batch     0 | loss: 87.4276965CurrentTrain: epoch  8, batch     1 | loss: 53.4827215CurrentTrain: epoch  8, batch     2 | loss: 67.5580663CurrentTrain: epoch  8, batch     3 | loss: 63.5901965CurrentTrain: epoch  8, batch     4 | loss: 53.6153630CurrentTrain: epoch  9, batch     0 | loss: 86.4708751CurrentTrain: epoch  9, batch     1 | loss: 65.2140259CurrentTrain: epoch  9, batch     2 | loss: 68.6354506CurrentTrain: epoch  9, batch     3 | loss: 64.7578069CurrentTrain: epoch  9, batch     4 | loss: 40.1956799
MemoryTrain:  epoch  0, batch     0 | loss: 0.4452518MemoryTrain:  epoch  1, batch     0 | loss: 0.3837112MemoryTrain:  epoch  2, batch     0 | loss: 0.3071574MemoryTrain:  epoch  3, batch     0 | loss: 0.2634192MemoryTrain:  epoch  4, batch     0 | loss: 0.1797739MemoryTrain:  epoch  5, batch     0 | loss: 0.1322171MemoryTrain:  epoch  6, batch     0 | loss: 0.0943490MemoryTrain:  epoch  7, batch     0 | loss: 0.0639169MemoryTrain:  epoch  8, batch     0 | loss: 0.0505658MemoryTrain:  epoch  9, batch     0 | loss: 0.0384862

F1 score per class: {5: 0.9637305699481865, 6: 0.0, 10: 0.5507246376811594, 16: 0.8, 17: 0.0, 18: 0.6037735849056604}
Micro-average F1 score: 0.73568281938326
Weighted-average F1 score: 0.7652520033458201
F1 score per class: {5: 1.0, 6: 0.0, 10: 0.7560975609756098, 16: 0.8679245283018868, 17: 0.0, 18: 0.8615384615384616}
Micro-average F1 score: 0.8452380952380952
Weighted-average F1 score: 0.8441911621774532
F1 score per class: {5: 1.0, 6: 0.0, 10: 0.7636363636363637, 16: 0.8679245283018868, 17: 0.0, 18: 0.8615384615384616}
Micro-average F1 score: 0.8508946322067594
Weighted-average F1 score: 0.8533995802219556

F1 score per class: {32: 0.9637305699481865, 5: 0.5632183908045977, 6: 0.5467625899280576, 10: 0.8, 16: 0.0, 17: 0.6037735849056604, 18: 0.8764044943820225, 19: 0.48, 24: 0.7741935483870968, 26: 0.9473684210526315, 29: 0.8811881188118812}
Micro-average F1 score: 0.7826704545454546
Weighted-average F1 score: 0.8008693225196143
F1 score per class: {32: 0.9803921568627451, 5: 0.6565656565656566, 6: 0.7515151515151515, 10: 0.8518518518518519, 16: 0.0, 17: 0.8235294117647058, 18: 0.9010989010989011, 19: 0.6428571428571429, 24: 0.7865168539325843, 26: 0.9533678756476683, 29: 0.9154228855721394}
Micro-average F1 score: 0.8323313293253173
Weighted-average F1 score: 0.8306038115479901
F1 score per class: {32: 0.9803921568627451, 5: 0.6387434554973822, 6: 0.7590361445783133, 10: 0.8518518518518519, 16: 0.0, 17: 0.8235294117647058, 18: 0.9010989010989011, 19: 0.5925925925925926, 24: 0.7865168539325843, 26: 0.9583333333333334, 29: 0.9064039408866995}
Micro-average F1 score: 0.8303152246814218
Weighted-average F1 score: 0.8296337315975433

F1 score per class: {32: 0.93, 5: 0.0, 6: 0.4418604651162791, 10: 0.5714285714285714, 16: 0.0, 17: 0.3047619047619048, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5138461538461538
Weighted-average F1 score: 0.450253759008889
F1 score per class: {32: 0.6920415224913494, 5: 0.0, 6: 0.543859649122807, 10: 0.48936170212765956, 16: 0.0, 17: 0.224, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.4049429657794677
Weighted-average F1 score: 0.3601044223901476
F1 score per class: {32: 0.6993006993006993, 5: 0.0, 6: 0.5228215767634855, 10: 0.4946236559139785, 16: 0.0, 17: 0.224, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.41392649903288203
Weighted-average F1 score: 0.37290377667039865

F1 score per class: {32: 0.916256157635468, 5: 0.3828125, 6: 0.3838383838383838, 10: 0.5194805194805194, 16: 0.0, 17: 0.2882882882882883, 18: 0.6695278969957081, 19: 0.2033898305084746, 24: 0.7058823529411765, 26: 0.8450704225352113, 29: 0.6953125}
Micro-average F1 score: 0.6012002182214948
Weighted-average F1 score: 0.5892376125170071
F1 score per class: {32: 0.6578947368421053, 5: 0.362116991643454, 6: 0.42758620689655175, 10: 0.42201834862385323, 16: 0.0, 17: 0.1951219512195122, 18: 0.5985401459854015, 19: 0.13636363636363635, 24: 0.6862745098039216, 26: 0.7330677290836654, 29: 0.673992673992674}
Micro-average F1 score: 0.4940523394131642
Weighted-average F1 score: 0.4685868590664581
F1 score per class: {32: 0.6644518272425249, 5: 0.38006230529595014, 6: 0.4117647058823529, 10: 0.42592592592592593, 16: 0.0, 17: 0.19377162629757785, 18: 0.6007326007326007, 19: 0.16842105263157894, 24: 0.693069306930693, 26: 0.7449392712550608, 29: 0.6642599277978339}
Micro-average F1 score: 0.504071661237785
Weighted-average F1 score: 0.4808021050470167
cur_acc_wo_na:  ['0.8060', '0.7357']
his_acc_wo_na:  ['0.8060', '0.7827']
cur_acc des_wo_na:  ['0.8388', '0.8452']
his_acc des_wo_na:  ['0.8388', '0.8323']
cur_acc rrf_wo_na:  ['0.8388', '0.8509']
his_acc rrf_wo_na:  ['0.8388', '0.8303']
cur_acc_w_na:  ['0.6799', '0.5138']
his_acc_w_na:  ['0.6799', '0.6012']
cur_acc des_w_na:  ['0.6377', '0.4049']
his_acc des_w_na:  ['0.6377', '0.4941']
cur_acc rrf_w_na:  ['0.6397', '0.4139']
his_acc rrf_w_na:  ['0.6397', '0.5041']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 83.4729787CurrentTrain: epoch  0, batch     1 | loss: 102.1912702CurrentTrain: epoch  0, batch     2 | loss: 67.2561880CurrentTrain: epoch  0, batch     3 | loss: 79.3656166CurrentTrain: epoch  0, batch     4 | loss: 60.1547863CurrentTrain: epoch  1, batch     0 | loss: 96.3129537CurrentTrain: epoch  1, batch     1 | loss: 60.2549523CurrentTrain: epoch  1, batch     2 | loss: 72.3358524CurrentTrain: epoch  1, batch     3 | loss: 90.0290559CurrentTrain: epoch  1, batch     4 | loss: 28.4579093CurrentTrain: epoch  2, batch     0 | loss: 74.5104893CurrentTrain: epoch  2, batch     1 | loss: 59.6101170CurrentTrain: epoch  2, batch     2 | loss: 121.9311347CurrentTrain: epoch  2, batch     3 | loss: 68.5695154CurrentTrain: epoch  2, batch     4 | loss: 27.2192732CurrentTrain: epoch  3, batch     0 | loss: 58.0880953CurrentTrain: epoch  3, batch     1 | loss: 70.4585243CurrentTrain: epoch  3, batch     2 | loss: 69.0416405CurrentTrain: epoch  3, batch     3 | loss: 70.0994696CurrentTrain: epoch  3, batch     4 | loss: 28.4883649CurrentTrain: epoch  4, batch     0 | loss: 89.1095387CurrentTrain: epoch  4, batch     1 | loss: 70.0470629CurrentTrain: epoch  4, batch     2 | loss: 66.3888555CurrentTrain: epoch  4, batch     3 | loss: 68.0837516CurrentTrain: epoch  4, batch     4 | loss: 27.7549879CurrentTrain: epoch  5, batch     0 | loss: 86.4427334CurrentTrain: epoch  5, batch     1 | loss: 53.9186068CurrentTrain: epoch  5, batch     2 | loss: 120.6614832CurrentTrain: epoch  5, batch     3 | loss: 65.5271922CurrentTrain: epoch  5, batch     4 | loss: 27.0221354CurrentTrain: epoch  6, batch     0 | loss: 117.6584708CurrentTrain: epoch  6, batch     1 | loss: 67.9000397CurrentTrain: epoch  6, batch     2 | loss: 66.8353308CurrentTrain: epoch  6, batch     3 | loss: 52.9666099CurrentTrain: epoch  6, batch     4 | loss: 27.1424760CurrentTrain: epoch  7, batch     0 | loss: 59.9984333CurrentTrain: epoch  7, batch     1 | loss: 67.5019550CurrentTrain: epoch  7, batch     2 | loss: 67.7609994CurrentTrain: epoch  7, batch     3 | loss: 183.2770513CurrentTrain: epoch  7, batch     4 | loss: 26.7302720CurrentTrain: epoch  8, batch     0 | loss: 67.6118557CurrentTrain: epoch  8, batch     1 | loss: 66.7369329CurrentTrain: epoch  8, batch     2 | loss: 63.0624086CurrentTrain: epoch  8, batch     3 | loss: 54.7184304CurrentTrain: epoch  8, batch     4 | loss: 27.4628100CurrentTrain: epoch  9, batch     0 | loss: 66.3760352CurrentTrain: epoch  9, batch     1 | loss: 50.2645069CurrentTrain: epoch  9, batch     2 | loss: 113.6497627CurrentTrain: epoch  9, batch     3 | loss: 66.4571955CurrentTrain: epoch  9, batch     4 | loss: 26.6382215
MemoryTrain:  epoch  0, batch     0 | loss: 0.5457766MemoryTrain:  epoch  1, batch     0 | loss: 0.4648119MemoryTrain:  epoch  2, batch     0 | loss: 0.3288752MemoryTrain:  epoch  3, batch     0 | loss: 0.2580857MemoryTrain:  epoch  4, batch     0 | loss: 0.1881919MemoryTrain:  epoch  5, batch     0 | loss: 0.1491245MemoryTrain:  epoch  6, batch     0 | loss: 0.1225956MemoryTrain:  epoch  7, batch     0 | loss: 0.0948407MemoryTrain:  epoch  8, batch     0 | loss: 0.0795970MemoryTrain:  epoch  9, batch     0 | loss: 0.0680761

F1 score per class: {2: 0.8, 39: 0.0, 10: 0.18556701030927836, 11: 0.5074626865671642, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.4, 28: 0.13333333333333333}
Micro-average F1 score: 0.36551724137931035
Weighted-average F1 score: 0.3754530671746539
F1 score per class: {2: 1.0, 39: 0.0, 10: 0.5166666666666667, 11: 0.8777777777777778, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.45454545454545453, 28: 0.0, 29: 0.25}
Micro-average F1 score: 0.6445012787723785
Weighted-average F1 score: 0.5966232088325112
F1 score per class: {2: 0.9411764705882353, 39: 0.0, 10: 0.5042016806722689, 11: 0.8700564971751412, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.45454545454545453, 28: 0.0, 29: 0.25}
Micro-average F1 score: 0.6304909560723514
Weighted-average F1 score: 0.5774860149504074

F1 score per class: {32: 0.8, 2: 0.9637305699481865, 5: 0.5798816568047337, 6: 0.22608695652173913, 39: 0.18, 10: 0.5037037037037037, 11: 0.7719298245614035, 12: 0.0, 16: 0.4230769230769231, 17: 0.8295454545454546, 18: 0.5384615384615384, 19: 0.7659574468085106, 24: 0.2222222222222222, 26: 0.9473684210526315, 28: 0.9019607843137255, 29: 0.125}
Micro-average F1 score: 0.6865671641791045
Weighted-average F1 score: 0.759976849349483
F1 score per class: {32: 1.0, 2: 0.9900990099009901, 5: 0.6480446927374302, 6: 0.6225165562913907, 39: 0.45588235294117646, 10: 0.8272251308900523, 11: 0.847457627118644, 12: 0.0, 16: 0.4155844155844156, 17: 0.8901098901098901, 18: 0.5454545454545454, 19: 0.7650273224043715, 24: 0.20833333333333334, 26: 0.9430051813471503, 28: 0.916256157635468, 29: 0.23529411764705882}
Micro-average F1 score: 0.7564712097200211
Weighted-average F1 score: 0.7580393641902411
F1 score per class: {32: 0.9411764705882353, 2: 0.9900990099009901, 5: 0.6363636363636364, 6: 0.6405228758169934, 39: 0.44776119402985076, 10: 0.8324324324324325, 11: 0.819672131147541, 12: 0.0, 16: 0.41025641025641024, 17: 0.8901098901098901, 18: 0.5625, 19: 0.7717391304347826, 24: 0.19607843137254902, 26: 0.9430051813471503, 28: 0.9108910891089109, 29: 0.23529411764705882}
Micro-average F1 score: 0.7554376657824934
Weighted-average F1 score: 0.7567118187232199

F1 score per class: {32: 0.3157894736842105, 2: 0.0, 6: 0.0, 39: 0.1782178217821782, 10: 0.4594594594594595, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.14634146341463414, 26: 0.0, 28: 0.0, 29: 0.08}
Micro-average F1 score: 0.2345132743362832
Weighted-average F1 score: 0.16840364277573994
F1 score per class: {32: 0.20930232558139536, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.41333333333333333, 10: 0.6030534351145038, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.07751937984496124, 26: 0.0, 28: 0.0, 29: 0.0975609756097561}
Micro-average F1 score: 0.28378378378378377
Weighted-average F1 score: 0.2263850426422832
F1 score per class: {32: 0.18823529411764706, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.410958904109589, 10: 0.5945945945945946, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.07692307692307693, 26: 0.0, 28: 0.0, 29: 0.12121212121212122}
Micro-average F1 score: 0.2798165137614679
Weighted-average F1 score: 0.22108680707424744

F1 score per class: {32: 0.24, 2: 0.9207920792079208, 5: 0.3712121212121212, 6: 0.20155038759689922, 39: 0.15, 10: 0.30357142857142855, 11: 0.4835164835164835, 12: 0.0, 16: 0.17054263565891473, 17: 0.6008230452674898, 18: 0.2, 19: 0.7024390243902439, 24: 0.058823529411764705, 26: 0.8333333333333334, 28: 0.6642599277978339, 29: 0.06666666666666667}
Micro-average F1 score: 0.48646362098138746
Weighted-average F1 score: 0.4804014594143383
F1 score per class: {32: 0.07317073170731707, 2: 0.5830903790087464, 5: 0.3483483483483483, 6: 0.4292237442922374, 39: 0.23846153846153847, 10: 0.2607260726072607, 11: 0.46296296296296297, 12: 0.0, 16: 0.09846153846153846, 17: 0.5955882352941176, 18: 0.1111111111111111, 19: 0.660377358490566, 24: 0.034722222222222224, 26: 0.7398373983739838, 28: 0.6019417475728155, 29: 0.0784313725490196}
Micro-average F1 score: 0.35764235764235763
Weighted-average F1 score: 0.3234823593533204
F1 score per class: {32: 0.0653061224489796, 2: 0.5988023952095808, 5: 0.35443037974683544, 6: 0.4188034188034188, 39: 0.234375, 10: 0.26970227670753066, 11: 0.42016806722689076, 12: 0.0, 16: 0.0975609756097561, 17: 0.5785714285714286, 18: 0.1487603305785124, 19: 0.6729857819905213, 24: 0.033112582781456956, 26: 0.7489711934156379, 28: 0.5974025974025974, 29: 0.09090909090909091}
Micro-average F1 score: 0.36224879165606716
Weighted-average F1 score: 0.3284407764610844
cur_acc_wo_na:  ['0.8060', '0.7357', '0.3655']
his_acc_wo_na:  ['0.8060', '0.7827', '0.6866']
cur_acc des_wo_na:  ['0.8388', '0.8452', '0.6445']
his_acc des_wo_na:  ['0.8388', '0.8323', '0.7565']
cur_acc rrf_wo_na:  ['0.8388', '0.8509', '0.6305']
his_acc rrf_wo_na:  ['0.8388', '0.8303', '0.7554']
cur_acc_w_na:  ['0.6799', '0.5138', '0.2345']
his_acc_w_na:  ['0.6799', '0.6012', '0.4865']
cur_acc des_w_na:  ['0.6377', '0.4049', '0.2838']
his_acc des_w_na:  ['0.6377', '0.4941', '0.3576']
cur_acc rrf_w_na:  ['0.6397', '0.4139', '0.2798']
his_acc rrf_w_na:  ['0.6397', '0.5041', '0.3622']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 100.4373405CurrentTrain: epoch  0, batch     1 | loss: 119.8559038CurrentTrain: epoch  0, batch     2 | loss: 68.0315997CurrentTrain: epoch  0, batch     3 | loss: 76.2145881CurrentTrain: epoch  1, batch     0 | loss: 70.1358904CurrentTrain: epoch  1, batch     1 | loss: 96.8151067CurrentTrain: epoch  1, batch     2 | loss: 64.4185410CurrentTrain: epoch  1, batch     3 | loss: 72.3457215CurrentTrain: epoch  2, batch     0 | loss: 60.5807032CurrentTrain: epoch  2, batch     1 | loss: 62.7207485CurrentTrain: epoch  2, batch     2 | loss: 126.3161338CurrentTrain: epoch  2, batch     3 | loss: 59.5882963CurrentTrain: epoch  3, batch     0 | loss: 73.0077905CurrentTrain: epoch  3, batch     1 | loss: 70.6799047CurrentTrain: epoch  3, batch     2 | loss: 57.6675062CurrentTrain: epoch  3, batch     3 | loss: 97.4003925CurrentTrain: epoch  4, batch     0 | loss: 71.8519060CurrentTrain: epoch  4, batch     1 | loss: 122.8476540CurrentTrain: epoch  4, batch     2 | loss: 54.3984067CurrentTrain: epoch  4, batch     3 | loss: 53.0582384CurrentTrain: epoch  5, batch     0 | loss: 84.3622523CurrentTrain: epoch  5, batch     1 | loss: 82.8069569CurrentTrain: epoch  5, batch     2 | loss: 69.1907476CurrentTrain: epoch  5, batch     3 | loss: 48.8287632CurrentTrain: epoch  6, batch     0 | loss: 67.7568085CurrentTrain: epoch  6, batch     1 | loss: 64.9069223CurrentTrain: epoch  6, batch     2 | loss: 89.5683396CurrentTrain: epoch  6, batch     3 | loss: 53.3996724CurrentTrain: epoch  7, batch     0 | loss: 88.2420104CurrentTrain: epoch  7, batch     1 | loss: 55.2269391CurrentTrain: epoch  7, batch     2 | loss: 70.8343494CurrentTrain: epoch  7, batch     3 | loss: 51.6153132CurrentTrain: epoch  8, batch     0 | loss: 122.8951442CurrentTrain: epoch  8, batch     1 | loss: 51.8226687CurrentTrain: epoch  8, batch     2 | loss: 86.4317281CurrentTrain: epoch  8, batch     3 | loss: 63.8247473CurrentTrain: epoch  9, batch     0 | loss: 66.5765420CurrentTrain: epoch  9, batch     1 | loss: 67.9672394CurrentTrain: epoch  9, batch     2 | loss: 65.8609257CurrentTrain: epoch  9, batch     3 | loss: 54.9294741
MemoryTrain:  epoch  0, batch     0 | loss: 0.4164384MemoryTrain:  epoch  1, batch     0 | loss: 0.3441565MemoryTrain:  epoch  2, batch     0 | loss: 0.2466479MemoryTrain:  epoch  3, batch     0 | loss: 0.1940316MemoryTrain:  epoch  4, batch     0 | loss: 0.1455036MemoryTrain:  epoch  5, batch     0 | loss: 0.1082780MemoryTrain:  epoch  6, batch     0 | loss: 0.0932078MemoryTrain:  epoch  7, batch     0 | loss: 0.0836531MemoryTrain:  epoch  8, batch     0 | loss: 0.0720661MemoryTrain:  epoch  9, batch     0 | loss: 0.0620165

F1 score per class: {0: 0.9428571428571428, 32: 0.0, 2: 0.9247311827956989, 4: 0.0, 11: 0.3333333333333333, 13: 0.3684210526315789, 21: 0.810126582278481, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7930174563591023
Weighted-average F1 score: 0.7705939158529369
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.9637305699481865, 4: 0.0, 11: 0.3333333333333333, 13: 0.0, 18: 0.6382978723404256, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.8456057007125891
Weighted-average F1 score: 0.8104720975673133
F1 score per class: {0: 0.9722222222222222, 32: 0.0, 2: 0.9690721649484536, 4: 0.3333333333333333, 13: 0.6382978723404256, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.8476190476190476
Weighted-average F1 score: 0.8144583028685628

F1 score per class: {0: 0.9295774647887324, 2: 0.6666666666666666, 4: 0.9247311827956989, 5: 0.9641025641025641, 6: 0.5308641975308642, 10: 0.5142857142857142, 11: 0.23423423423423423, 12: 0.36065573770491804, 13: 0.08333333333333333, 16: 0.8070175438596491, 17: 0.0, 18: 0.4727272727272727, 19: 0.8156424581005587, 21: 0.22950819672131148, 23: 0.7804878048780488, 24: 0.18181818181818182, 26: 0.700507614213198, 28: 0.1, 29: 0.9368421052631579, 32: 0.8736842105263158, 39: 0.125}
Micro-average F1 score: 0.6897533206831119
Weighted-average F1 score: 0.7304262085304959
F1 score per class: {0: 0.972972972972973, 2: 0.7, 4: 0.9637305699481865, 5: 0.9852216748768473, 6: 0.6736842105263158, 10: 0.6363636363636364, 11: 0.34710743801652894, 12: 0.7558139534883721, 13: 0.0625, 16: 0.8666666666666667, 17: 0.0, 18: 0.4878048780487805, 19: 0.8854166666666666, 21: 0.4411764705882353, 23: 0.8048780487804879, 24: 0.2608695652173913, 26: 0.7142857142857143, 28: 0.10810810810810811, 29: 0.9375, 32: 0.9137055837563451, 39: 0.11764705882352941}
Micro-average F1 score: 0.751185855972402
Weighted-average F1 score: 0.7546111871384038
F1 score per class: {0: 0.9459459459459459, 2: 0.6666666666666666, 4: 0.9690721649484536, 5: 0.9850746268656716, 6: 0.6338797814207651, 10: 0.6538461538461539, 11: 0.36363636363636365, 12: 0.6274509803921569, 13: 0.06451612903225806, 16: 0.8524590163934426, 17: 0.0, 18: 0.5063291139240507, 19: 0.8691099476439791, 21: 0.40540540540540543, 23: 0.8048780487804879, 24: 0.2608695652173913, 26: 0.7208121827411168, 28: 0.10526315789473684, 29: 0.9375, 32: 0.9081632653061225, 39: 0.11764705882352941}
Micro-average F1 score: 0.7386759581881533
Weighted-average F1 score: 0.7434589926722037

F1 score per class: {0: 0.8461538461538461, 2: 0.0, 4: 0.9247311827956989, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.125, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.2692307692307692, 23: 0.7111111111111111, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0}
Micro-average F1 score: 0.5668449197860963
Weighted-average F1 score: 0.4452369892796531
F1 score per class: {0: 0.6666666666666666, 2: 0.0, 4: 0.9117647058823529, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.05, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.25862068965517243, 23: 0.6470588235294118, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3723849372384937
Weighted-average F1 score: 0.27374708785313223
F1 score per class: {0: 0.6862745098039216, 2: 0.0, 4: 0.9543147208121827, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.05405405405405406, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.24793388429752067, 23: 0.6346153846153846, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3991031390134529
Weighted-average F1 score: 0.2937399215708633

F1 score per class: {0: 0.4520547945205479, 2: 0.2727272727272727, 4: 0.8958333333333334, 5: 0.8952380952380953, 6: 0.3412698412698413, 10: 0.3333333333333333, 11: 0.19402985074626866, 12: 0.24858757062146894, 13: 0.020833333333333332, 16: 0.45098039215686275, 17: 0.0, 18: 0.15384615384615385, 19: 0.5725490196078431, 21: 0.08139534883720931, 23: 0.6095238095238096, 24: 0.13793103448275862, 26: 0.6359447004608295, 28: 0.02631578947368421, 29: 0.7672413793103449, 32: 0.6666666666666666, 39: 0.07142857142857142}
Micro-average F1 score: 0.4673738347798136
Weighted-average F1 score: 0.44083512267125396
F1 score per class: {0: 0.25622775800711745, 2: 0.05622489959839357, 4: 0.8611111111111112, 5: 0.4728132387706856, 6: 0.3004694835680751, 10: 0.3202614379084967, 11: 0.1875, 12: 0.25096525096525096, 13: 0.014388489208633094, 16: 0.4094488188976378, 17: 0.0, 18: 0.09900990099009901, 19: 0.5345911949685535, 21: 0.08450704225352113, 23: 0.5365853658536586, 24: 0.1875, 26: 0.6306306306306306, 28: 0.016129032258064516, 29: 0.7228915662650602, 32: 0.5625, 39: 0.0273972602739726}
Micro-average F1 score: 0.33067577828397876
Weighted-average F1 score: 0.29792925890509975
F1 score per class: {0: 0.22151898734177214, 2: 0.11666666666666667, 4: 0.9215686274509803, 5: 0.5439560439560439, 6: 0.32954545454545453, 10: 0.3167701863354037, 11: 0.1981981981981982, 12: 0.25, 13: 0.012658227848101266, 16: 0.40625, 17: 0.0, 18: 0.10204081632653061, 19: 0.538961038961039, 21: 0.07936507936507936, 23: 0.515625, 24: 0.1875, 26: 0.6425339366515838, 28: 0.015873015873015872, 29: 0.7258064516129032, 32: 0.5874587458745875, 39: 0.044444444444444446}
Micro-average F1 score: 0.34690120679075476
Weighted-average F1 score: 0.3109428630566924
cur_acc_wo_na:  ['0.8060', '0.7357', '0.3655', '0.7930']
his_acc_wo_na:  ['0.8060', '0.7827', '0.6866', '0.6898']
cur_acc des_wo_na:  ['0.8388', '0.8452', '0.6445', '0.8456']
his_acc des_wo_na:  ['0.8388', '0.8323', '0.7565', '0.7512']
cur_acc rrf_wo_na:  ['0.8388', '0.8509', '0.6305', '0.8476']
his_acc rrf_wo_na:  ['0.8388', '0.8303', '0.7554', '0.7387']
cur_acc_w_na:  ['0.6799', '0.5138', '0.2345', '0.5668']
his_acc_w_na:  ['0.6799', '0.6012', '0.4865', '0.4674']
cur_acc des_w_na:  ['0.6377', '0.4049', '0.2838', '0.3724']
his_acc des_w_na:  ['0.6377', '0.4941', '0.3576', '0.3307']
cur_acc rrf_w_na:  ['0.6397', '0.4139', '0.2798', '0.3991']
his_acc rrf_w_na:  ['0.6397', '0.5041', '0.3622', '0.3469']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 77.0217720CurrentTrain: epoch  0, batch     1 | loss: 63.2336654CurrentTrain: epoch  0, batch     2 | loss: 77.5486788CurrentTrain: epoch  0, batch     3 | loss: 132.2109769CurrentTrain: epoch  1, batch     0 | loss: 69.4011098CurrentTrain: epoch  1, batch     1 | loss: 87.8481146CurrentTrain: epoch  1, batch     2 | loss: 57.0152509CurrentTrain: epoch  1, batch     3 | loss: 47.7256769CurrentTrain: epoch  2, batch     0 | loss: 119.9801430CurrentTrain: epoch  2, batch     1 | loss: 70.6338460CurrentTrain: epoch  2, batch     2 | loss: 55.9280825CurrentTrain: epoch  2, batch     3 | loss: 41.5303622CurrentTrain: epoch  3, batch     0 | loss: 84.8670148CurrentTrain: epoch  3, batch     1 | loss: 65.1776854CurrentTrain: epoch  3, batch     2 | loss: 84.7472665CurrentTrain: epoch  3, batch     3 | loss: 35.0640418CurrentTrain: epoch  4, batch     0 | loss: 64.0987942CurrentTrain: epoch  4, batch     1 | loss: 53.8793712CurrentTrain: epoch  4, batch     2 | loss: 67.1108070CurrentTrain: epoch  4, batch     3 | loss: 57.8902254CurrentTrain: epoch  5, batch     0 | loss: 54.3521926CurrentTrain: epoch  5, batch     1 | loss: 65.8587403CurrentTrain: epoch  5, batch     2 | loss: 65.9921430CurrentTrain: epoch  5, batch     3 | loss: 34.8754702CurrentTrain: epoch  6, batch     0 | loss: 53.0156084CurrentTrain: epoch  6, batch     1 | loss: 63.9519116CurrentTrain: epoch  6, batch     2 | loss: 66.8208653CurrentTrain: epoch  6, batch     3 | loss: 43.7618422CurrentTrain: epoch  7, batch     0 | loss: 51.1874170CurrentTrain: epoch  7, batch     1 | loss: 64.2122293CurrentTrain: epoch  7, batch     2 | loss: 84.5890268CurrentTrain: epoch  7, batch     3 | loss: 43.9985514CurrentTrain: epoch  8, batch     0 | loss: 66.9801192CurrentTrain: epoch  8, batch     1 | loss: 61.2546140CurrentTrain: epoch  8, batch     2 | loss: 52.1245656CurrentTrain: epoch  8, batch     3 | loss: 55.2534498CurrentTrain: epoch  9, batch     0 | loss: 59.5785145CurrentTrain: epoch  9, batch     1 | loss: 62.4079693CurrentTrain: epoch  9, batch     2 | loss: 115.9522204CurrentTrain: epoch  9, batch     3 | loss: 76.1228634
MemoryTrain:  epoch  0, batch     0 | loss: 0.2399987MemoryTrain:  epoch  1, batch     0 | loss: 0.2360635MemoryTrain:  epoch  2, batch     0 | loss: 0.1604040MemoryTrain:  epoch  3, batch     0 | loss: 0.1157180MemoryTrain:  epoch  4, batch     0 | loss: 0.1308178MemoryTrain:  epoch  5, batch     0 | loss: 0.1026757MemoryTrain:  epoch  6, batch     0 | loss: 0.0741076MemoryTrain:  epoch  7, batch     0 | loss: 0.0647308MemoryTrain:  epoch  8, batch     0 | loss: 0.0610087MemoryTrain:  epoch  9, batch     0 | loss: 0.0558307

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.8888888888888888, 38: 0.0, 11: 0.0, 13: 0.0, 15: 0.44776119402985076, 18: 0.0, 21: 0.0, 23: 0.5217391304347826, 25: 0.5, 28: 0.782608695652174}
Micro-average F1 score: 0.49221183800623053
Weighted-average F1 score: 0.3942149157593674
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 10: 0.0, 11: 0.0, 13: 0.0, 15: 0.5352112676056338, 18: 0.0, 21: 0.9263157894736842, 23: 0.6363636363636364, 25: 0.8085106382978723}
Micro-average F1 score: 0.6270270270270271
Weighted-average F1 score: 0.5243005489182871
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 10: 0.0, 11: 0.0, 13: 0.0, 15: 0.5352112676056338, 18: 0.0, 21: 0.8, 23: 0.6363636363636364, 25: 0.8571428571428571}
Micro-average F1 score: 0.5869565217391305
Weighted-average F1 score: 0.46519278983646023

F1 score per class: {0: 0.9142857142857143, 2: 0.43478260869565216, 4: 0.88268156424581, 5: 0.8648648648648649, 6: 0.463768115942029, 10: 0.3968253968253968, 11: 0.10101010101010101, 12: 0.21428571428571427, 13: 0.1, 15: 0.64, 16: 0.8135593220338984, 17: 0.0, 18: 0.5185185185185185, 19: 0.7411764705882353, 21: 0.22641509433962265, 23: 0.7857142857142857, 24: 0.10526315789473684, 25: 0.44776119402985076, 26: 0.7395833333333334, 28: 0.12903225806451613, 29: 0.9424083769633508, 32: 0.84, 35: 0.5142857142857142, 37: 0.4444444444444444, 38: 0.37894736842105264, 39: 0.0}
Micro-average F1 score: 0.6244306418219462
Weighted-average F1 score: 0.6816048592180451
F1 score per class: {0: 0.96, 2: 0.6363636363636364, 4: 0.9637305699481865, 5: 0.851063829787234, 6: 0.6127167630057804, 10: 0.5655172413793104, 11: 0.2037037037037037, 12: 0.7116564417177914, 13: 0.13333333333333333, 15: 0.48, 16: 0.84375, 17: 0.0, 18: 0.5373134328358209, 19: 0.7931034482758621, 21: 0.358974358974359, 23: 0.725, 24: 0.2962962962962963, 25: 0.5277777777777778, 26: 0.7035175879396985, 28: 0.20512820512820512, 29: 0.9326424870466321, 32: 0.8910891089108911, 35: 0.8888888888888888, 37: 0.3971631205673759, 38: 0.4810126582278481, 39: 0.0}
Micro-average F1 score: 0.6865561694290976
Weighted-average F1 score: 0.6943089822945405
F1 score per class: {0: 0.96, 2: 0.56, 4: 0.9417989417989417, 5: 0.8547008547008547, 6: 0.591715976331361, 10: 0.5454545454545454, 11: 0.205607476635514, 12: 0.6133333333333333, 13: 0.08333333333333333, 15: 0.4444444444444444, 16: 0.8852459016393442, 17: 0.0, 18: 0.5151515151515151, 19: 0.7840909090909091, 21: 0.3488372093023256, 23: 0.7341772151898734, 24: 0.25, 25: 0.5277777777777778, 26: 0.7064676616915423, 28: 0.18604651162790697, 29: 0.9484536082474226, 32: 0.8490566037735849, 35: 0.7640449438202247, 37: 0.4, 38: 0.4827586206896552, 39: 0.0}
Micro-average F1 score: 0.6693817104776009
Weighted-average F1 score: 0.6788662760371129

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.8421052631578947, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.44776119402985076, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.3829787234042553, 37: 0.47058823529411764, 38: 0.4864864864864865}
Micro-average F1 score: 0.2936802973977695
Weighted-average F1 score: 0.1926009930147867
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.5714285714285714, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4810126582278481, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.5945945945945946, 37: 0.43410852713178294, 38: 0.4810126582278481, 39: 0.0}
Micro-average F1 score: 0.25026968716289105
Weighted-average F1 score: 0.1810383824728032
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.475, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.5112781954887218, 37: 0.43410852713178294, 38: 0.5060240963855421}
Micro-average F1 score: 0.24462061155152887
Weighted-average F1 score: 0.17450091745362611

F1 score per class: {0: 0.48854961832061067, 2: 0.15384615384615385, 4: 0.8540540540540541, 5: 0.6981818181818182, 6: 0.3004694835680751, 10: 0.2631578947368421, 11: 0.08264462809917356, 12: 0.1348314606741573, 13: 0.025, 15: 0.3404255319148936, 16: 0.42105263157894735, 17: 0.0, 18: 0.11522633744855967, 19: 0.4581818181818182, 21: 0.08888888888888889, 23: 0.717391304347826, 24: 0.07692307692307693, 25: 0.44776119402985076, 26: 0.6635514018691588, 28: 0.03361344537815126, 29: 0.7086614173228346, 32: 0.6511627906976745, 35: 0.21818181818181817, 37: 0.2777777777777778, 38: 0.10465116279069768, 39: 0.0}
Micro-average F1 score: 0.3806158505805149
Weighted-average F1 score: 0.35422574166727144
F1 score per class: {0: 0.2408026755852843, 2: 0.06086956521739131, 4: 0.8611111111111112, 5: 0.3590664272890485, 6: 0.2804232804232804, 10: 0.2751677852348993, 11: 0.1375, 12: 0.1949579831932773, 13: 0.028368794326241134, 15: 0.18461538461538463, 16: 0.3624161073825503, 17: 0.0, 18: 0.09302325581395349, 19: 0.4246153846153846, 21: 0.056795131845841784, 23: 0.48739495798319327, 24: 0.18181818181818182, 25: 0.4691358024691358, 26: 0.5833333333333334, 28: 0.026143790849673203, 29: 0.6642066420664207, 32: 0.5750798722044729, 35: 0.21568627450980393, 37: 0.10810810810810811, 38: 0.12101910828025478, 39: 0.0}
Micro-average F1 score: 0.2681242807825086
Weighted-average F1 score: 0.2411967702983332
F1 score per class: {0: 0.22085889570552147, 2: 0.11290322580645161, 4: 0.8944723618090452, 5: 0.3838771593090211, 6: 0.29411764705882354, 10: 0.2785714285714286, 11: 0.14193548387096774, 12: 0.2021978021978022, 13: 0.018018018018018018, 15: 0.1791044776119403, 16: 0.3829787234042553, 17: 0.0, 18: 0.08947368421052632, 19: 0.4144144144144144, 21: 0.05660377358490566, 23: 0.5, 24: 0.17647058823529413, 25: 0.4634146341463415, 26: 0.6016949152542372, 28: 0.023880597014925373, 29: 0.6891385767790262, 32: 0.547112462006079, 35: 0.2066869300911854, 37: 0.10748560460652591, 38: 0.11229946524064172, 39: 0.0}
Micro-average F1 score: 0.2731530442665055
Weighted-average F1 score: 0.24420958950106106
cur_acc_wo_na:  ['0.8060', '0.7357', '0.3655', '0.7930', '0.4922']
his_acc_wo_na:  ['0.8060', '0.7827', '0.6866', '0.6898', '0.6244']
cur_acc des_wo_na:  ['0.8388', '0.8452', '0.6445', '0.8456', '0.6270']
his_acc des_wo_na:  ['0.8388', '0.8323', '0.7565', '0.7512', '0.6866']
cur_acc rrf_wo_na:  ['0.8388', '0.8509', '0.6305', '0.8476', '0.5870']
his_acc rrf_wo_na:  ['0.8388', '0.8303', '0.7554', '0.7387', '0.6694']
cur_acc_w_na:  ['0.6799', '0.5138', '0.2345', '0.5668', '0.2937']
his_acc_w_na:  ['0.6799', '0.6012', '0.4865', '0.4674', '0.3806']
cur_acc des_w_na:  ['0.6377', '0.4049', '0.2838', '0.3724', '0.2503']
his_acc des_w_na:  ['0.6377', '0.4941', '0.3576', '0.3307', '0.2681']
cur_acc rrf_w_na:  ['0.6397', '0.4139', '0.2798', '0.3991', '0.2446']
his_acc rrf_w_na:  ['0.6397', '0.5041', '0.3622', '0.3469', '0.2732']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 130.4409435CurrentTrain: epoch  0, batch     1 | loss: 76.4318509CurrentTrain: epoch  0, batch     2 | loss: 75.7987261CurrentTrain: epoch  0, batch     3 | loss: 79.1184901CurrentTrain: epoch  1, batch     0 | loss: 123.3445849CurrentTrain: epoch  1, batch     1 | loss: 57.7297926CurrentTrain: epoch  1, batch     2 | loss: 61.5826312CurrentTrain: epoch  1, batch     3 | loss: 44.8694635CurrentTrain: epoch  2, batch     0 | loss: 117.7856876CurrentTrain: epoch  2, batch     1 | loss: 70.9226266CurrentTrain: epoch  2, batch     2 | loss: 56.5723865CurrentTrain: epoch  2, batch     3 | loss: 55.7045788CurrentTrain: epoch  3, batch     0 | loss: 56.8451782CurrentTrain: epoch  3, batch     1 | loss: 114.9901624CurrentTrain: epoch  3, batch     2 | loss: 65.5294429CurrentTrain: epoch  3, batch     3 | loss: 42.7693338CurrentTrain: epoch  4, batch     0 | loss: 65.1564689CurrentTrain: epoch  4, batch     1 | loss: 85.0747986CurrentTrain: epoch  4, batch     2 | loss: 66.0023017CurrentTrain: epoch  4, batch     3 | loss: 74.0350605CurrentTrain: epoch  5, batch     0 | loss: 64.8468249CurrentTrain: epoch  5, batch     1 | loss: 64.5635760CurrentTrain: epoch  5, batch     2 | loss: 65.6459290CurrentTrain: epoch  5, batch     3 | loss: 52.0675707CurrentTrain: epoch  6, batch     0 | loss: 65.8687090CurrentTrain: epoch  6, batch     1 | loss: 54.6522645CurrentTrain: epoch  6, batch     2 | loss: 81.8168268CurrentTrain: epoch  6, batch     3 | loss: 38.3085564CurrentTrain: epoch  7, batch     0 | loss: 62.8590453CurrentTrain: epoch  7, batch     1 | loss: 83.9474246CurrentTrain: epoch  7, batch     2 | loss: 81.1877366CurrentTrain: epoch  7, batch     3 | loss: 66.0112782CurrentTrain: epoch  8, batch     0 | loss: 52.0394977CurrentTrain: epoch  8, batch     1 | loss: 50.7653010CurrentTrain: epoch  8, batch     2 | loss: 83.0373806CurrentTrain: epoch  8, batch     3 | loss: 51.7776193CurrentTrain: epoch  9, batch     0 | loss: 64.8524471CurrentTrain: epoch  9, batch     1 | loss: 64.8408169CurrentTrain: epoch  9, batch     2 | loss: 64.5005176CurrentTrain: epoch  9, batch     3 | loss: 36.1856015
MemoryTrain:  epoch  0, batch     0 | loss: 0.1290888MemoryTrain:  epoch  1, batch     0 | loss: 0.1534930MemoryTrain:  epoch  2, batch     0 | loss: 0.1262371MemoryTrain:  epoch  3, batch     0 | loss: 0.0957840MemoryTrain:  epoch  4, batch     0 | loss: 0.0774893MemoryTrain:  epoch  5, batch     0 | loss: 0.0690983MemoryTrain:  epoch  6, batch     0 | loss: 0.0614379MemoryTrain:  epoch  7, batch     0 | loss: 0.0538804MemoryTrain:  epoch  8, batch     0 | loss: 0.0457483MemoryTrain:  epoch  9, batch     0 | loss: 0.0431017

F1 score per class: {33: 0.0, 35: 0.1797752808988764, 36: 0.0, 37: 0.0, 6: 0.0, 8: 0.6987951807228916, 10: 0.0, 11: 0.0, 18: 0.0, 20: 0.972972972972973, 26: 0.42857142857142855, 28: 0.0, 29: 0.16666666666666666, 30: 0.0}
Micro-average F1 score: 0.3775811209439528
Weighted-average F1 score: 0.38428231747576547
F1 score per class: {2: 0.0, 5: 0.0, 6: 0.0, 8: 0.6271186440677966, 10: 0.0, 11: 0.0, 18: 0.0, 20: 0.92, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.5333333333333333, 35: 0.0, 36: 0.7058823529411765, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.5747101942037137
F1 score per class: {33: 0.0, 35: 0.0, 36: 0.5, 5: 0.0, 6: 0.0, 38: 0.0, 8: 0.9306930693069307, 10: 0.0, 37: 0.0, 11: 0.0, 18: 1.0, 20: 0.5333333333333333, 26: 0.0, 28: 0.6391752577319587, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.6124401913875598
Weighted-average F1 score: 0.5250820351895711

F1 score per class: {0: 0.8985507246376812, 2: 0.631578947368421, 4: 0.8235294117647058, 5: 0.8571428571428571, 6: 0.47619047619047616, 8: 0.17391304347826086, 10: 0.21052631578947367, 11: 0.22608695652173913, 12: 0.24561403508771928, 13: 0.125, 15: 0.7619047619047619, 16: 0.8070175438596491, 17: 0.0, 18: 0.4666666666666667, 19: 0.75, 20: 0.6987951807228916, 21: 0.13953488372093023, 23: 0.6933333333333334, 24: 0.09523809523809523, 25: 0.42424242424242425, 26: 0.7164179104477612, 28: 0.1111111111111111, 29: 0.9214659685863874, 30: 0.9230769230769231, 32: 0.8210526315789474, 33: 0.15789473684210525, 35: 0.4, 36: 0.16666666666666666, 37: 0.37777777777777777, 38: 0.3548387096774194, 39: 0.0}
Micro-average F1 score: 0.5744916820702403
Weighted-average F1 score: 0.6409453654229903
F1 score per class: {0: 0.9459459459459459, 2: 0.6086956521739131, 4: 0.9637305699481865, 5: 0.8264462809917356, 6: 0.6111111111111112, 8: 0.42528735632183906, 10: 0.352, 11: 0.25862068965517243, 12: 0.6962025316455697, 13: 0.16666666666666666, 15: 0.5217391304347826, 16: 0.8709677419354839, 17: 0.0, 18: 0.5263157894736842, 19: 0.8131868131868132, 20: 0.9108910891089109, 21: 0.46153846153846156, 23: 0.7341772151898734, 24: 0.16666666666666666, 25: 0.5142857142857142, 26: 0.7106598984771574, 28: 0.15384615384615385, 29: 0.934010152284264, 30: 0.8837209302325582, 32: 0.8944723618090452, 33: 0.17777777777777778, 35: 0.8888888888888888, 36: 0.6153846153846154, 37: 0.44642857142857145, 38: 0.5405405405405406, 39: 0.0}
Micro-average F1 score: 0.6755980861244019
Weighted-average F1 score: 0.6890393485172687
F1 score per class: {0: 0.9459459459459459, 2: 0.6, 4: 0.9247311827956989, 5: 0.8264462809917356, 6: 0.5909090909090909, 8: 0.3829787234042553, 10: 0.34146341463414637, 11: 0.25, 12: 0.6709677419354839, 13: 0.13333333333333333, 15: 0.64, 16: 0.8852459016393442, 17: 0.0, 18: 0.5135135135135135, 19: 0.7865168539325843, 20: 0.9215686274509803, 21: 0.47761194029850745, 23: 0.7341772151898734, 24: 0.09523809523809523, 25: 0.5142857142857142, 26: 0.7135678391959799, 28: 0.14035087719298245, 29: 0.9285714285714286, 30: 0.8636363636363636, 32: 0.8682926829268293, 33: 0.1509433962264151, 35: 0.735632183908046, 36: 0.6138613861386139, 37: 0.4444444444444444, 38: 0.4819277108433735, 39: 0.0}
Micro-average F1 score: 0.6563407550822846
Weighted-average F1 score: 0.6679838698581393

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.17582417582417584, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.58, 21: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9473684210526315, 32: 0.0, 33: 0.42857142857142855, 35: 0.0, 36: 0.16, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.23486238532110093
Weighted-average F1 score: 0.1565731084202422
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.46540880503144655, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.647887323943662, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.7755102040816326, 32: 0.0, 33: 0.27586206896551724, 35: 0.0, 36: 0.496551724137931, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.2743961352657005
Weighted-average F1 score: 0.19996813541072128
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.432, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6351351351351351, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.7450980392156863, 32: 0.0, 33: 0.25806451612903225, 35: 0.0, 36: 0.5040650406504065, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.2591093117408907
Weighted-average F1 score: 0.18024681320641803

F1 score per class: {0: 0.484375, 2: 0.15789473684210525, 4: 0.7954545454545454, 5: 0.6274509803921569, 6: 0.30434782608695654, 8: 0.128, 10: 0.18604651162790697, 11: 0.1793103448275862, 12: 0.17721518987341772, 13: 0.02702702702702703, 15: 0.4, 16: 0.3865546218487395, 17: 0.0, 18: 0.11155378486055777, 19: 0.49411764705882355, 20: 0.2857142857142857, 21: 0.06593406593406594, 23: 0.5591397849462365, 24: 0.07142857142857142, 25: 0.417910447761194, 26: 0.6206896551724138, 28: 0.0425531914893617, 29: 0.6717557251908397, 30: 0.8372093023255814, 32: 0.6527196652719666, 33: 0.12244897959183673, 35: 0.19047619047619047, 36: 0.1518987341772152, 37: 0.27419354838709675, 38: 0.1164021164021164, 39: 0.0}
Micro-average F1 score: 0.36763662171753014
Weighted-average F1 score: 0.3559159848797337
F1 score per class: {0: 0.2527075812274368, 2: 0.10144927536231885, 4: 0.8857142857142857, 5: 0.2442002442002442, 6: 0.2696078431372549, 8: 0.14566929133858267, 10: 0.21890547263681592, 11: 0.18867924528301888, 12: 0.2301255230125523, 13: 0.023529411764705882, 15: 0.21052631578947367, 16: 0.32142857142857145, 17: 0.0, 18: 0.09237875288683603, 19: 0.42528735632183906, 20: 0.2100456621004566, 21: 0.09287925696594428, 23: 0.47540983606557374, 24: 0.12121212121212122, 25: 0.45569620253164556, 26: 0.5907172995780591, 28: 0.03550295857988166, 29: 0.6216216216216216, 30: 0.42696629213483145, 32: 0.5686900958466453, 33: 0.052980132450331126, 35: 0.25142857142857145, 36: 0.31718061674008813, 37: 0.1597444089456869, 38: 0.13745704467353953, 39: 0.0}
Micro-average F1 score: 0.271921941199127
Weighted-average F1 score: 0.2514614017587035
F1 score per class: {0: 0.2536231884057971, 2: 0.12903225806451613, 4: 0.882051282051282, 5: 0.2574002574002574, 6: 0.28493150684931506, 8: 0.15743440233236153, 10: 0.21105527638190955, 11: 0.17341040462427745, 12: 0.23059866962305986, 13: 0.020618556701030927, 15: 0.25396825396825395, 16: 0.32926829268292684, 17: 0.0, 18: 0.08983451536643026, 19: 0.42168674698795183, 20: 0.20935412026726058, 21: 0.0972644376899696, 23: 0.5043478260869565, 24: 0.06896551724137931, 25: 0.47368421052631576, 26: 0.5892116182572614, 28: 0.034482758620689655, 29: 0.6232876712328768, 30: 0.40860215053763443, 32: 0.5493827160493827, 33: 0.047058823529411764, 35: 0.2064516129032258, 36: 0.3803680981595092, 37: 0.18374558303886926, 38: 0.11527377521613832, 39: 0.0}
Micro-average F1 score: 0.2732401934443847
Weighted-average F1 score: 0.25130589553476074
cur_acc_wo_na:  ['0.8060', '0.7357', '0.3655', '0.7930', '0.4922', '0.3776']
his_acc_wo_na:  ['0.8060', '0.7827', '0.6866', '0.6898', '0.6244', '0.5745']
cur_acc des_wo_na:  ['0.8388', '0.8452', '0.6445', '0.8456', '0.6270', '0.6667']
his_acc des_wo_na:  ['0.8388', '0.8323', '0.7565', '0.7512', '0.6866', '0.6756']
cur_acc rrf_wo_na:  ['0.8388', '0.8509', '0.6305', '0.8476', '0.5870', '0.6124']
his_acc rrf_wo_na:  ['0.8388', '0.8303', '0.7554', '0.7387', '0.6694', '0.6563']
cur_acc_w_na:  ['0.6799', '0.5138', '0.2345', '0.5668', '0.2937', '0.2349']
his_acc_w_na:  ['0.6799', '0.6012', '0.4865', '0.4674', '0.3806', '0.3676']
cur_acc des_w_na:  ['0.6377', '0.4049', '0.2838', '0.3724', '0.2503', '0.2744']
his_acc des_w_na:  ['0.6377', '0.4941', '0.3576', '0.3307', '0.2681', '0.2719']
cur_acc rrf_w_na:  ['0.6397', '0.4139', '0.2798', '0.3991', '0.2446', '0.2591']
his_acc rrf_w_na:  ['0.6397', '0.5041', '0.3622', '0.3469', '0.2732', '0.2732']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 65.1811880CurrentTrain: epoch  0, batch     1 | loss: 96.4277465CurrentTrain: epoch  0, batch     2 | loss: 97.5692934CurrentTrain: epoch  0, batch     3 | loss: 27.9841857CurrentTrain: epoch  1, batch     0 | loss: 62.1241813CurrentTrain: epoch  1, batch     1 | loss: 79.8264095CurrentTrain: epoch  1, batch     2 | loss: 71.3547111CurrentTrain: epoch  1, batch     3 | loss: 6.9051846CurrentTrain: epoch  2, batch     0 | loss: 70.9907385CurrentTrain: epoch  2, batch     1 | loss: 55.2396501CurrentTrain: epoch  2, batch     2 | loss: 66.4255708CurrentTrain: epoch  2, batch     3 | loss: 27.7128219CurrentTrain: epoch  3, batch     0 | loss: 91.6166310CurrentTrain: epoch  3, batch     1 | loss: 55.0294002CurrentTrain: epoch  3, batch     2 | loss: 52.9300984CurrentTrain: epoch  3, batch     3 | loss: 5.7331962CurrentTrain: epoch  4, batch     0 | loss: 65.4090835CurrentTrain: epoch  4, batch     1 | loss: 66.0880258CurrentTrain: epoch  4, batch     2 | loss: 51.8772046CurrentTrain: epoch  4, batch     3 | loss: 27.6809069CurrentTrain: epoch  5, batch     0 | loss: 52.0951746CurrentTrain: epoch  5, batch     1 | loss: 51.7234580CurrentTrain: epoch  5, batch     2 | loss: 114.1289964CurrentTrain: epoch  5, batch     3 | loss: 7.0047551CurrentTrain: epoch  6, batch     0 | loss: 54.5874833CurrentTrain: epoch  6, batch     1 | loss: 52.7941807CurrentTrain: epoch  6, batch     2 | loss: 64.5741915CurrentTrain: epoch  6, batch     3 | loss: 3.1574665CurrentTrain: epoch  7, batch     0 | loss: 52.6409396CurrentTrain: epoch  7, batch     1 | loss: 61.4898888CurrentTrain: epoch  7, batch     2 | loss: 51.8520028CurrentTrain: epoch  7, batch     3 | loss: 27.5404353CurrentTrain: epoch  8, batch     0 | loss: 49.2311686CurrentTrain: epoch  8, batch     1 | loss: 81.4166025CurrentTrain: epoch  8, batch     2 | loss: 62.5974780CurrentTrain: epoch  8, batch     3 | loss: 27.5570957CurrentTrain: epoch  9, batch     0 | loss: 61.0707467CurrentTrain: epoch  9, batch     1 | loss: 51.2183909CurrentTrain: epoch  9, batch     2 | loss: 53.0671437CurrentTrain: epoch  9, batch     3 | loss: 27.5555604
MemoryTrain:  epoch  0, batch     0 | loss: 0.1433643MemoryTrain:  epoch  1, batch     0 | loss: 0.1331851MemoryTrain:  epoch  2, batch     0 | loss: 0.1031047MemoryTrain:  epoch  3, batch     0 | loss: 0.0875026MemoryTrain:  epoch  4, batch     0 | loss: 0.0744832MemoryTrain:  epoch  5, batch     0 | loss: 0.0671684MemoryTrain:  epoch  6, batch     0 | loss: 0.0574596MemoryTrain:  epoch  7, batch     0 | loss: 0.0579433MemoryTrain:  epoch  8, batch     0 | loss: 0.0480668MemoryTrain:  epoch  9, batch     0 | loss: 0.0436616

F1 score per class: {7: 0.75, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6, 27: 0.0, 31: 0.3614457831325301}
Micro-average F1 score: 0.47115384615384615
Weighted-average F1 score: 0.3870371347718785
F1 score per class: {35: 0.8888888888888888, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 11: 0.0, 19: 0.0, 24: 0.6666666666666666, 26: 0.6666666666666666, 27: 0.0, 31: 0.6730769230769231}
Micro-average F1 score: 0.6636771300448431
Weighted-average F1 score: 0.5715241165861781
F1 score per class: {35: 0.8888888888888888, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 11: 0.0, 19: 0.0, 24: 0.6666666666666666, 26: 0.6666666666666666, 27: 0.0, 31: 0.6730769230769231}
Micro-average F1 score: 0.6636771300448431
Weighted-average F1 score: 0.5715241165861781

F1 score per class: {0: 0.8484848484848485, 2: 0.631578947368421, 4: 0.7577639751552795, 5: 0.8646288209606987, 6: 0.11214953271028037, 7: 0.061224489795918366, 8: 0.17391304347826086, 9: 0.9803921568627451, 10: 0.09523809523809523, 11: 0.24347826086956523, 12: 0.11214953271028037, 13: 0.11764705882352941, 15: 0.5714285714285714, 16: 0.7931034482758621, 17: 0.0, 18: 0.46875, 19: 0.5728643216080402, 20: 0.6153846153846154, 21: 0.16666666666666666, 23: 0.7105263157894737, 24: 0.09090909090909091, 25: 0.44776119402985076, 26: 0.7120418848167539, 27: 0.4444444444444444, 28: 0.09836065573770492, 29: 0.9021739130434783, 30: 0.9444444444444444, 31: 0.0, 32: 0.7932960893854749, 33: 0.125, 35: 0.31746031746031744, 36: 0.08695652173913043, 37: 0.36363636363636365, 38: 0.34782608695652173, 39: 0.0, 40: 0.30612244897959184}
Micro-average F1 score: 0.5068870523415978
Weighted-average F1 score: 0.5662604672996792
F1 score per class: {0: 0.9295774647887324, 2: 0.4444444444444444, 4: 0.918918918918919, 5: 0.823045267489712, 6: 0.09523809523809523, 7: 0.06896551724137931, 8: 0.29457364341085274, 9: 0.9803921568627451, 10: 0.23008849557522124, 11: 0.22608695652173913, 12: 0.5294117647058824, 13: 0.05263157894736842, 15: 0.48, 16: 0.8571428571428571, 17: 0.0, 18: 0.5121951219512195, 19: 0.5263157894736842, 20: 0.8541666666666666, 21: 0.5, 23: 0.7160493827160493, 24: 0.23076923076923078, 25: 0.5641025641025641, 26: 0.7076923076923077, 27: 0.43902439024390244, 28: 0.13333333333333333, 29: 0.9222797927461139, 30: 0.9142857142857143, 31: 0.5, 32: 0.8844221105527639, 33: 0.11940298507462686, 35: 0.7865168539325843, 36: 0.6833333333333333, 37: 0.41304347826086957, 38: 0.5490196078431373, 39: 0.11764705882352941, 40: 0.45454545454545453}
Micro-average F1 score: 0.5815687444079929
Weighted-average F1 score: 0.5815727845407473
F1 score per class: {0: 0.9444444444444444, 2: 0.46153846153846156, 4: 0.8439306358381503, 5: 0.8298755186721992, 6: 0.07692307692307693, 7: 0.06779661016949153, 8: 0.1724137931034483, 9: 0.9803921568627451, 10: 0.1651376146788991, 11: 0.2459016393442623, 12: 0.5294117647058824, 13: 0.06666666666666667, 15: 0.5, 16: 0.8709677419354839, 17: 0.0, 18: 0.5316455696202531, 19: 0.5681818181818182, 20: 0.8541666666666666, 21: 0.4642857142857143, 23: 0.7341772151898734, 24: 0.16666666666666666, 25: 0.5641025641025641, 26: 0.7150259067357513, 27: 0.46153846153846156, 28: 0.11235955056179775, 29: 0.9166666666666666, 30: 0.9142857142857143, 31: 0.6666666666666666, 32: 0.88, 33: 0.10126582278481013, 35: 0.6666666666666666, 36: 0.5, 37: 0.4, 38: 0.45901639344262296, 39: 0.0, 40: 0.46357615894039733}
Micro-average F1 score: 0.5596385542168675
Weighted-average F1 score: 0.5609139500887868

F1 score per class: {0: 0.0, 32: 0.5, 33: 0.8928571428571429, 35: 0.0, 37: 0.0, 7: 0.0, 40: 0.0, 9: 0.0, 10: 0.48, 16: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.3125}
Micro-average F1 score: 0.35251798561151076
Weighted-average F1 score: 0.2731445993031359
F1 score per class: {0: 0.0, 5: 0.0, 7: 0.5333333333333333, 8: 0.0, 9: 0.7142857142857143, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.42857142857142855, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.2857142857142857, 32: 0.0, 33: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.4375}
Micro-average F1 score: 0.3238512035010941
Weighted-average F1 score: 0.2649243370817715
F1 score per class: {0: 0.0, 5: 0.0, 7: 0.5333333333333333, 8: 0.0, 9: 0.746268656716418, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.42857142857142855, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.3333333333333333, 32: 0.0, 33: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.43209876543209874}
Micro-average F1 score: 0.33035714285714285
Weighted-average F1 score: 0.27038783524319915

F1 score per class: {0: 0.5185185185185185, 2: 0.2222222222222222, 4: 0.7439024390243902, 5: 0.5515320334261838, 6: 0.10714285714285714, 7: 0.018633540372670808, 8: 0.14035087719298245, 9: 0.8928571428571429, 10: 0.08849557522123894, 11: 0.19718309859154928, 12: 0.09022556390977443, 13: 0.023529411764705882, 15: 0.23529411764705882, 16: 0.3770491803278688, 17: 0.0, 18: 0.10909090909090909, 19: 0.3986013986013986, 20: 0.2874251497005988, 21: 0.09090909090909091, 23: 0.5625, 24: 0.07407407407407407, 25: 0.4411764705882353, 26: 0.5964912280701754, 27: 0.1348314606741573, 28: 0.04054054054054054, 29: 0.7094017094017094, 30: 0.8717948717948718, 31: 0.0, 32: 0.6666666666666666, 33: 0.0759493670886076, 35: 0.1941747572815534, 36: 0.08450704225352113, 37: 0.2601626016260163, 38: 0.14814814814814814, 39: 0.0, 40: 0.2097902097902098}
Micro-average F1 score: 0.32472975954114275
Weighted-average F1 score: 0.3078034163759652
F1 score per class: {0: 0.40993788819875776, 2: 0.08759124087591241, 4: 0.8629441624365483, 5: 0.22805017103762829, 6: 0.08196721311475409, 7: 0.017429193899782137, 8: 0.1386861313868613, 9: 0.6410256410256411, 10: 0.17687074829931973, 11: 0.18571428571428572, 12: 0.23452768729641693, 13: 0.0091324200913242, 15: 0.16901408450704225, 16: 0.31213872832369943, 17: 0.0, 18: 0.08300395256916997, 19: 0.3557312252964427, 20: 0.19069767441860466, 21: 0.12396694214876033, 23: 0.4172661870503597, 24: 0.13043478260869565, 25: 0.4230769230769231, 26: 0.5307692307692308, 27: 0.12413793103448276, 28: 0.02702702702702703, 29: 0.6768060836501901, 30: 0.7272727272727273, 31: 0.031746031746031744, 32: 0.6111111111111112, 33: 0.02564102564102564, 35: 0.28225806451612906, 36: 0.3942307692307692, 37: 0.18719211822660098, 38: 0.15217391304347827, 39: 0.08, 40: 0.19390581717451524}
Micro-average F1 score: 0.24172554853105244
Weighted-average F1 score: 0.21554687514834284
F1 score per class: {0: 0.38857142857142857, 2: 0.11650485436893204, 4: 0.8111111111111111, 5: 0.2347417840375587, 6: 0.06837606837606838, 7: 0.017391304347826087, 8: 0.0851063829787234, 9: 0.704225352112676, 10: 0.1267605633802817, 11: 0.18072289156626506, 12: 0.23376623376623376, 13: 0.011428571428571429, 15: 0.13953488372093023, 16: 0.3312883435582822, 17: 0.0, 18: 0.08917197452229299, 19: 0.38461538461538464, 20: 0.20448877805486285, 21: 0.11926605504587157, 23: 0.4715447154471545, 24: 0.10256410256410256, 25: 0.4536082474226804, 26: 0.5798319327731093, 27: 0.1276595744680851, 28: 0.02557544757033248, 29: 0.6717557251908397, 30: 0.6956521739130435, 31: 0.041666666666666664, 32: 0.6048109965635738, 33: 0.024024024024024024, 35: 0.22950819672131148, 36: 0.36507936507936506, 37: 0.18584070796460178, 38: 0.13333333333333333, 39: 0.0, 40: 0.19943019943019943}
Micro-average F1 score: 0.23878678833054878
Weighted-average F1 score: 0.21211302944806784
cur_acc_wo_na:  ['0.8060', '0.7357', '0.3655', '0.7930', '0.4922', '0.3776', '0.4712']
his_acc_wo_na:  ['0.8060', '0.7827', '0.6866', '0.6898', '0.6244', '0.5745', '0.5069']
cur_acc des_wo_na:  ['0.8388', '0.8452', '0.6445', '0.8456', '0.6270', '0.6667', '0.6637']
his_acc des_wo_na:  ['0.8388', '0.8323', '0.7565', '0.7512', '0.6866', '0.6756', '0.5816']
cur_acc rrf_wo_na:  ['0.8388', '0.8509', '0.6305', '0.8476', '0.5870', '0.6124', '0.6637']
his_acc rrf_wo_na:  ['0.8388', '0.8303', '0.7554', '0.7387', '0.6694', '0.6563', '0.5596']
cur_acc_w_na:  ['0.6799', '0.5138', '0.2345', '0.5668', '0.2937', '0.2349', '0.3525']
his_acc_w_na:  ['0.6799', '0.6012', '0.4865', '0.4674', '0.3806', '0.3676', '0.3247']
cur_acc des_w_na:  ['0.6377', '0.4049', '0.2838', '0.3724', '0.2503', '0.2744', '0.3239']
his_acc des_w_na:  ['0.6377', '0.4941', '0.3576', '0.3307', '0.2681', '0.2719', '0.2417']
cur_acc rrf_w_na:  ['0.6397', '0.4139', '0.2798', '0.3991', '0.2446', '0.2591', '0.3304']
his_acc rrf_w_na:  ['0.6397', '0.5041', '0.3622', '0.3469', '0.2732', '0.2732', '0.2388']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 75.2052129CurrentTrain: epoch  0, batch     1 | loss: 96.6556776CurrentTrain: epoch  0, batch     2 | loss: 131.3396143CurrentTrain: epoch  0, batch     3 | loss: 78.2505088CurrentTrain: epoch  0, batch     4 | loss: 77.2718737CurrentTrain: epoch  1, batch     0 | loss: 59.3875676CurrentTrain: epoch  1, batch     1 | loss: 128.1063741CurrentTrain: epoch  1, batch     2 | loss: 60.6853723CurrentTrain: epoch  1, batch     3 | loss: 78.1354775CurrentTrain: epoch  1, batch     4 | loss: 68.7280541CurrentTrain: epoch  2, batch     0 | loss: 58.9536808CurrentTrain: epoch  2, batch     1 | loss: 68.4125582CurrentTrain: epoch  2, batch     2 | loss: 57.9488802CurrentTrain: epoch  2, batch     3 | loss: 75.4453920CurrentTrain: epoch  2, batch     4 | loss: 106.6676293CurrentTrain: epoch  3, batch     0 | loss: 87.4846570CurrentTrain: epoch  3, batch     1 | loss: 86.9815915CurrentTrain: epoch  3, batch     2 | loss: 56.9671373CurrentTrain: epoch  3, batch     3 | loss: 87.2370998CurrentTrain: epoch  3, batch     4 | loss: 49.2839246CurrentTrain: epoch  4, batch     0 | loss: 85.7882677CurrentTrain: epoch  4, batch     1 | loss: 67.8088839CurrentTrain: epoch  4, batch     2 | loss: 68.5679540CurrentTrain: epoch  4, batch     3 | loss: 68.1823350CurrentTrain: epoch  4, batch     4 | loss: 47.5470630CurrentTrain: epoch  5, batch     0 | loss: 86.3711983CurrentTrain: epoch  5, batch     1 | loss: 64.3860038CurrentTrain: epoch  5, batch     2 | loss: 85.2775455CurrentTrain: epoch  5, batch     3 | loss: 113.8237940CurrentTrain: epoch  5, batch     4 | loss: 67.2254533CurrentTrain: epoch  6, batch     0 | loss: 54.0231299CurrentTrain: epoch  6, batch     1 | loss: 65.2929333CurrentTrain: epoch  6, batch     2 | loss: 116.1221521CurrentTrain: epoch  6, batch     3 | loss: 53.2676235CurrentTrain: epoch  6, batch     4 | loss: 65.8833768CurrentTrain: epoch  7, batch     0 | loss: 54.1775772CurrentTrain: epoch  7, batch     1 | loss: 85.8562415CurrentTrain: epoch  7, batch     2 | loss: 52.5068590CurrentTrain: epoch  7, batch     3 | loss: 85.0122114CurrentTrain: epoch  7, batch     4 | loss: 63.8231590CurrentTrain: epoch  8, batch     0 | loss: 64.1385258CurrentTrain: epoch  8, batch     1 | loss: 373.7809012CurrentTrain: epoch  8, batch     2 | loss: 65.2554348CurrentTrain: epoch  8, batch     3 | loss: 61.8711976CurrentTrain: epoch  8, batch     4 | loss: 47.2092654CurrentTrain: epoch  9, batch     0 | loss: 84.4800369CurrentTrain: epoch  9, batch     1 | loss: 64.2619953CurrentTrain: epoch  9, batch     2 | loss: 65.0324599CurrentTrain: epoch  9, batch     3 | loss: 82.6577308CurrentTrain: epoch  9, batch     4 | loss: 46.9208076
MemoryTrain:  epoch  0, batch     0 | loss: 0.4214760MemoryTrain:  epoch  1, batch     0 | loss: 0.3595325MemoryTrain:  epoch  2, batch     0 | loss: 0.2927775MemoryTrain:  epoch  3, batch     0 | loss: 0.2233921MemoryTrain:  epoch  4, batch     0 | loss: 0.1744445MemoryTrain:  epoch  5, batch     0 | loss: 0.1499409MemoryTrain:  epoch  6, batch     0 | loss: 0.1396256MemoryTrain:  epoch  7, batch     0 | loss: 0.1070371MemoryTrain:  epoch  8, batch     0 | loss: 0.0922435MemoryTrain:  epoch  9, batch     0 | loss: 0.0813599

F1 score per class: {32: 0.33962264150943394, 1: 0.7941176470588235, 34: 0.0, 3: 0.14583333333333334, 37: 0.0, 38: 0.6013071895424836, 11: 0.0, 14: 0.0, 18: 0.0, 22: 0.0, 23: 0.3582089552238806, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.46598639455782315
Weighted-average F1 score: 0.4288370553327874
F1 score per class: {32: 0.43103448275862066, 1: 0.8368794326241135, 34: 0.0, 35: 0.1518987341772152, 3: 0.0, 33: 0.0, 38: 0.7165775401069518, 11: 0.0, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.8775510204081632, 27: 0.0, 31: 0.0}
Micro-average F1 score: 0.6015037593984962
Weighted-average F1 score: 0.5685390920227077
F1 score per class: {32: 0.41025641025641024, 1: 0.8918918918918919, 34: 0.0, 35: 0.13953488372093023, 3: 0.0, 33: 0.6779661016949152, 38: 0.0, 37: 0.0, 11: 0.0, 14: 0.0, 18: 0.0, 22: 0.8541666666666666, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.5960665658093798
Weighted-average F1 score: 0.564108940838764

F1 score per class: {0: 0.7868852459016393, 1: 0.3, 2: 0.6666666666666666, 3: 0.7152317880794702, 4: 0.7654320987654321, 5: 0.8660714285714286, 6: 0.019801980198019802, 7: 0.06593406593406594, 8: 0.11627906976744186, 9: 0.9803921568627451, 10: 0.07692307692307693, 11: 0.08333333333333333, 12: 0.0, 13: 0.125, 14: 0.11864406779661017, 15: 0.631578947368421, 16: 0.75, 17: 0.0, 18: 0.36, 19: 0.3401360544217687, 20: 0.6666666666666666, 21: 0.0, 22: 0.5679012345679012, 23: 0.717948717948718, 24: 0.14814814814814814, 25: 0.375, 26: 0.7157894736842105, 27: 0.13333333333333333, 28: 0.11764705882352941, 29: 0.8901098901098901, 30: 0.972972972972973, 31: 0.0, 32: 0.5935483870967742, 33: 0.17647058823529413, 34: 0.19834710743801653, 35: 0.07547169811320754, 36: 0.058823529411764705, 37: 0.14925373134328357, 38: 0.21212121212121213, 39: 0.0, 40: 0.14814814814814814}
Micro-average F1 score: 0.43924125666864255
Weighted-average F1 score: 0.5113056942941231
F1 score per class: {0: 0.8656716417910447, 1: 0.37593984962406013, 2: 0.5217391304347826, 3: 0.7612903225806451, 4: 0.918918918918919, 5: 0.8438818565400844, 6: 0.05825242718446602, 7: 0.08163265306122448, 8: 0.23140495867768596, 9: 0.9803921568627451, 10: 0.24561403508771928, 11: 0.02197802197802198, 12: 0.5611510791366906, 13: 0.07142857142857142, 14: 0.13636363636363635, 15: 0.631578947368421, 16: 0.8387096774193549, 17: 0.0, 18: 0.43333333333333335, 19: 0.38571428571428573, 20: 0.8541666666666666, 21: 0.16216216216216217, 22: 0.67, 23: 0.7407407407407407, 24: 0.0625, 25: 0.5454545454545454, 26: 0.7040816326530612, 27: 0.14814814814814814, 28: 0.09411764705882353, 29: 0.9157894736842105, 30: 0.9444444444444444, 31: 0.0, 32: 0.7891891891891892, 33: 0.15789473684210525, 34: 0.42574257425742573, 35: 0.367816091954023, 36: 0.594059405940594, 37: 0.06153846153846154, 38: 0.24242424242424243, 39: 0.0, 40: 0.5555555555555556}
Micro-average F1 score: 0.5344295991778006
Weighted-average F1 score: 0.55632414682512
F1 score per class: {0: 0.8656716417910447, 1: 0.35555555555555557, 2: 0.5714285714285714, 3: 0.7674418604651163, 4: 0.8439306358381503, 5: 0.851063829787234, 6: 0.05825242718446602, 7: 0.0784313725490196, 8: 0.14736842105263157, 9: 0.9803921568627451, 10: 0.2608695652173913, 11: 0.06451612903225806, 12: 0.3709677419354839, 13: 0.1, 14: 0.1111111111111111, 15: 0.6, 16: 0.8524590163934426, 17: 0.0, 18: 0.43333333333333335, 19: 0.38095238095238093, 20: 0.865979381443299, 21: 0.0, 22: 0.6451612903225806, 23: 0.725, 24: 0.0625, 25: 0.5277777777777778, 26: 0.711340206185567, 27: 0.17391304347826086, 28: 0.0851063829787234, 29: 0.9312169312169312, 30: 0.972972972972973, 31: 0.0, 32: 0.7912087912087912, 33: 0.1276595744680851, 34: 0.4120603015075377, 35: 0.2318840579710145, 36: 0.3953488372093023, 37: 0.08450704225352113, 38: 0.19753086419753085, 39: 0.0, 40: 0.5223880597014925}
Micro-average F1 score: 0.5095176010430248
Weighted-average F1 score: 0.5349198674508637

F1 score per class: {0: 0.0, 1: 0.14457831325301204, 3: 0.627906976744186, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.09032258064516129, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4107142857142857, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.3380281690140845, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.25276752767527677
Weighted-average F1 score: 0.20362645648669497
F1 score per class: {0: 0.0, 1: 0.1724137931034483, 2: 0.0, 3: 0.5462962962962963, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.09375, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4557823129251701, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.4777777777777778, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.24345709068776628
Weighted-average F1 score: 0.20765306748989018
F1 score per class: {0: 0.0, 1: 0.16271186440677965, 2: 0.0, 3: 0.5689655172413793, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0784313725490196, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.43795620437956206, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.49101796407185627, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.24952501583280556
Weighted-average F1 score: 0.21401387701647337

F1 score per class: {0: 0.5274725274725275, 1: 0.11076923076923077, 2: 0.20408163265306123, 3: 0.3829787234042553, 4: 0.7425149700598802, 5: 0.7054545454545454, 6: 0.018867924528301886, 7: 0.02127659574468085, 8: 0.09174311926605505, 9: 0.819672131147541, 10: 0.07407407407407407, 11: 0.0784313725490196, 12: 0.0, 13: 0.024390243902439025, 14: 0.046357615894039736, 15: 0.3157894736842105, 16: 0.3853211009174312, 17: 0.0, 18: 0.13043478260869565, 19: 0.2777777777777778, 20: 0.26732673267326734, 21: 0.0, 22: 0.3458646616541353, 23: 0.5833333333333334, 24: 0.1, 25: 0.375, 26: 0.5938864628820961, 27: 0.06349206349206349, 28: 0.04316546762589928, 29: 0.6923076923076923, 30: 0.972972972972973, 31: 0.0, 32: 0.45544554455445546, 33: 0.14285714285714285, 34: 0.13333333333333333, 35: 0.05970149253731343, 36: 0.056338028169014086, 37: 0.12658227848101267, 38: 0.06306306306306306, 39: 0.0, 40: 0.12121212121212122}
Micro-average F1 score: 0.27836213373403457
Weighted-average F1 score: 0.2683879054165555
F1 score per class: {0: 0.48739495798319327, 1: 0.12077294685990338, 2: 0.1, 3: 0.3033419023136247, 4: 0.8808290155440415, 5: 0.3333333333333333, 6: 0.05128205128205128, 7: 0.019801980198019802, 8: 0.11618257261410789, 9: 0.5050505050505051, 10: 0.208955223880597, 11: 0.019230769230769232, 12: 0.27956989247311825, 13: 0.010582010582010581, 14: 0.053811659192825115, 15: 0.2222222222222222, 16: 0.33548387096774196, 17: 0.0, 18: 0.07386363636363637, 19: 0.2872340425531915, 20: 0.2433234421364985, 21: 0.0625, 22: 0.3435897435897436, 23: 0.4411764705882353, 24: 0.034482758620689655, 25: 0.3783783783783784, 26: 0.5433070866141733, 27: 0.05263157894736842, 28: 0.019851116625310174, 29: 0.6397058823529411, 30: 0.7906976744186046, 31: 0.0, 32: 0.5195729537366548, 33: 0.05714285714285714, 34: 0.1259150805270864, 35: 0.1568627450980392, 36: 0.4166666666666667, 37: 0.038461538461538464, 38: 0.06722689075630252, 39: 0.0, 40: 0.273972602739726}
Micro-average F1 score: 0.2393005062126093
Weighted-average F1 score: 0.21941386449791284
F1 score per class: {0: 0.4793388429752066, 1: 0.11294117647058824, 2: 0.1348314606741573, 3: 0.3, 4: 0.8342857142857143, 5: 0.40404040404040403, 6: 0.05217391304347826, 7: 0.019704433497536946, 8: 0.09523809523809523, 9: 0.6756756756756757, 10: 0.2222222222222222, 11: 0.05309734513274336, 12: 0.21296296296296297, 13: 0.015503875968992248, 14: 0.03821656050955414, 15: 0.2222222222222222, 16: 0.3466666666666667, 17: 0.0, 18: 0.07807807807807808, 19: 0.2857142857142857, 20: 0.23661971830985915, 21: 0.0, 22: 0.3468208092485549, 23: 0.49572649572649574, 24: 0.03389830508474576, 25: 0.40860215053763443, 26: 0.5702479338842975, 27: 0.06896551724137931, 28: 0.019230769230769232, 29: 0.6591760299625468, 30: 0.75, 31: 0.0, 32: 0.5333333333333333, 33: 0.047619047619047616, 34: 0.12312312312312312, 35: 0.13333333333333333, 36: 0.30357142857142855, 37: 0.057692307692307696, 38: 0.05177993527508091, 39: 0.0, 40: 0.2641509433962264}
Micro-average F1 score: 0.23820553456052665
Weighted-average F1 score: 0.21626926201265734
cur_acc_wo_na:  ['0.8060', '0.7357', '0.3655', '0.7930', '0.4922', '0.3776', '0.4712', '0.4660']
his_acc_wo_na:  ['0.8060', '0.7827', '0.6866', '0.6898', '0.6244', '0.5745', '0.5069', '0.4392']
cur_acc des_wo_na:  ['0.8388', '0.8452', '0.6445', '0.8456', '0.6270', '0.6667', '0.6637', '0.6015']
his_acc des_wo_na:  ['0.8388', '0.8323', '0.7565', '0.7512', '0.6866', '0.6756', '0.5816', '0.5344']
cur_acc rrf_wo_na:  ['0.8388', '0.8509', '0.6305', '0.8476', '0.5870', '0.6124', '0.6637', '0.5961']
his_acc rrf_wo_na:  ['0.8388', '0.8303', '0.7554', '0.7387', '0.6694', '0.6563', '0.5596', '0.5095']
cur_acc_w_na:  ['0.6799', '0.5138', '0.2345', '0.5668', '0.2937', '0.2349', '0.3525', '0.2528']
his_acc_w_na:  ['0.6799', '0.6012', '0.4865', '0.4674', '0.3806', '0.3676', '0.3247', '0.2784']
cur_acc des_w_na:  ['0.6377', '0.4049', '0.2838', '0.3724', '0.2503', '0.2744', '0.3239', '0.2435']
his_acc des_w_na:  ['0.6377', '0.4941', '0.3576', '0.3307', '0.2681', '0.2719', '0.2417', '0.2393']
cur_acc rrf_w_na:  ['0.6397', '0.4139', '0.2798', '0.3991', '0.2446', '0.2591', '0.3304', '0.2495']
his_acc rrf_w_na:  ['0.6397', '0.5041', '0.3622', '0.3469', '0.2732', '0.2732', '0.2388', '0.2382']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 68.1101858CurrentTrain: epoch  0, batch     1 | loss: 98.8349597CurrentTrain: epoch  0, batch     2 | loss: 65.8813162CurrentTrain: epoch  0, batch     3 | loss: 78.8843140CurrentTrain: epoch  0, batch     4 | loss: 77.7049081CurrentTrain: epoch  0, batch     5 | loss: 99.2895847CurrentTrain: epoch  0, batch     6 | loss: 78.3066439CurrentTrain: epoch  0, batch     7 | loss: 78.2211927CurrentTrain: epoch  0, batch     8 | loss: 65.6987219CurrentTrain: epoch  0, batch     9 | loss: 96.7762224CurrentTrain: epoch  0, batch    10 | loss: 79.5155348CurrentTrain: epoch  0, batch    11 | loss: 77.4853878CurrentTrain: epoch  0, batch    12 | loss: 65.2833402CurrentTrain: epoch  0, batch    13 | loss: 78.2248171CurrentTrain: epoch  0, batch    14 | loss: 65.6435909CurrentTrain: epoch  0, batch    15 | loss: 96.2352111CurrentTrain: epoch  0, batch    16 | loss: 64.5525519CurrentTrain: epoch  0, batch    17 | loss: 96.1443454CurrentTrain: epoch  0, batch    18 | loss: 94.9173609CurrentTrain: epoch  0, batch    19 | loss: 64.7418705CurrentTrain: epoch  0, batch    20 | loss: 77.4723207CurrentTrain: epoch  0, batch    21 | loss: 56.0190899CurrentTrain: epoch  0, batch    22 | loss: 64.9123661CurrentTrain: epoch  0, batch    23 | loss: 76.6850713CurrentTrain: epoch  0, batch    24 | loss: 76.9206857CurrentTrain: epoch  0, batch    25 | loss: 65.4002641CurrentTrain: epoch  0, batch    26 | loss: 65.8385552CurrentTrain: epoch  0, batch    27 | loss: 65.2575046CurrentTrain: epoch  0, batch    28 | loss: 95.4696165CurrentTrain: epoch  0, batch    29 | loss: 188.0650613CurrentTrain: epoch  0, batch    30 | loss: 188.0206003CurrentTrain: epoch  0, batch    31 | loss: 65.0652527CurrentTrain: epoch  0, batch    32 | loss: 77.0673924CurrentTrain: epoch  0, batch    33 | loss: 76.9829097CurrentTrain: epoch  0, batch    34 | loss: 64.6329925CurrentTrain: epoch  0, batch    35 | loss: 64.9155320CurrentTrain: epoch  0, batch    36 | loss: 77.0795528CurrentTrain: epoch  0, batch    37 | loss: 64.4190721CurrentTrain: epoch  0, batch    38 | loss: 93.4642576CurrentTrain: epoch  0, batch    39 | loss: 187.5789300CurrentTrain: epoch  0, batch    40 | loss: 77.2897409CurrentTrain: epoch  0, batch    41 | loss: 76.9276512CurrentTrain: epoch  0, batch    42 | loss: 76.0094937CurrentTrain: epoch  0, batch    43 | loss: 63.9012220CurrentTrain: epoch  0, batch    44 | loss: 76.3144062CurrentTrain: epoch  0, batch    45 | loss: 94.1958846CurrentTrain: epoch  0, batch    46 | loss: 64.2952154CurrentTrain: epoch  0, batch    47 | loss: 75.3293545CurrentTrain: epoch  0, batch    48 | loss: 75.5959833CurrentTrain: epoch  0, batch    49 | loss: 123.9551880CurrentTrain: epoch  0, batch    50 | loss: 63.7487081CurrentTrain: epoch  0, batch    51 | loss: 75.5569154CurrentTrain: epoch  0, batch    52 | loss: 123.1307003CurrentTrain: epoch  0, batch    53 | loss: 54.6789922CurrentTrain: epoch  0, batch    54 | loss: 64.0363090CurrentTrain: epoch  0, batch    55 | loss: 125.1058864CurrentTrain: epoch  0, batch    56 | loss: 75.2371707CurrentTrain: epoch  0, batch    57 | loss: 63.4690289CurrentTrain: epoch  0, batch    58 | loss: 92.7037173CurrentTrain: epoch  0, batch    59 | loss: 125.5534471CurrentTrain: epoch  0, batch    60 | loss: 62.6169574CurrentTrain: epoch  0, batch    61 | loss: 62.9167649CurrentTrain: epoch  0, batch    62 | loss: 74.8582463CurrentTrain: epoch  0, batch    63 | loss: 74.5204243CurrentTrain: epoch  0, batch    64 | loss: 73.9671820CurrentTrain: epoch  0, batch    65 | loss: 74.7646620CurrentTrain: epoch  0, batch    66 | loss: 73.7556101CurrentTrain: epoch  0, batch    67 | loss: 92.4281022CurrentTrain: epoch  0, batch    68 | loss: 74.0845383CurrentTrain: epoch  0, batch    69 | loss: 61.9130543CurrentTrain: epoch  0, batch    70 | loss: 73.9786046CurrentTrain: epoch  0, batch    71 | loss: 123.7517226CurrentTrain: epoch  0, batch    72 | loss: 75.3846152CurrentTrain: epoch  0, batch    73 | loss: 71.8715657CurrentTrain: epoch  0, batch    74 | loss: 73.9846580CurrentTrain: epoch  0, batch    75 | loss: 92.6174707CurrentTrain: epoch  0, batch    76 | loss: 122.8539277CurrentTrain: epoch  0, batch    77 | loss: 92.5939329CurrentTrain: epoch  0, batch    78 | loss: 73.3779155CurrentTrain: epoch  0, batch    79 | loss: 61.4594245CurrentTrain: epoch  0, batch    80 | loss: 61.3434762CurrentTrain: epoch  0, batch    81 | loss: 61.8357023CurrentTrain: epoch  0, batch    82 | loss: 74.2729533CurrentTrain: epoch  0, batch    83 | loss: 124.3581013CurrentTrain: epoch  0, batch    84 | loss: 72.2259430CurrentTrain: epoch  0, batch    85 | loss: 51.2405642CurrentTrain: epoch  0, batch    86 | loss: 121.5875566CurrentTrain: epoch  0, batch    87 | loss: 71.8561461CurrentTrain: epoch  0, batch    88 | loss: 72.2735409CurrentTrain: epoch  0, batch    89 | loss: 60.3711779CurrentTrain: epoch  0, batch    90 | loss: 70.9825822CurrentTrain: epoch  0, batch    91 | loss: 126.3511617CurrentTrain: epoch  0, batch    92 | loss: 91.6153001CurrentTrain: epoch  0, batch    93 | loss: 120.6450390CurrentTrain: epoch  0, batch    94 | loss: 90.7934344CurrentTrain: epoch  0, batch    95 | loss: 60.7473681CurrentTrain: epoch  1, batch     0 | loss: 59.7532892CurrentTrain: epoch  1, batch     1 | loss: 72.7042793CurrentTrain: epoch  1, batch     2 | loss: 59.5318270CurrentTrain: epoch  1, batch     3 | loss: 59.6664930CurrentTrain: epoch  1, batch     4 | loss: 58.1859402CurrentTrain: epoch  1, batch     5 | loss: 70.4164223CurrentTrain: epoch  1, batch     6 | loss: 74.1571005CurrentTrain: epoch  1, batch     7 | loss: 69.2447631CurrentTrain: epoch  1, batch     8 | loss: 74.4914576CurrentTrain: epoch  1, batch     9 | loss: 61.4296440CurrentTrain: epoch  1, batch    10 | loss: 70.1010509CurrentTrain: epoch  1, batch    11 | loss: 59.1510463CurrentTrain: epoch  1, batch    12 | loss: 121.3649447CurrentTrain: epoch  1, batch    13 | loss: 56.7046473CurrentTrain: epoch  1, batch    14 | loss: 70.6484193CurrentTrain: epoch  1, batch    15 | loss: 120.9234973CurrentTrain: epoch  1, batch    16 | loss: 73.8598165CurrentTrain: epoch  1, batch    17 | loss: 58.3347651CurrentTrain: epoch  1, batch    18 | loss: 71.0732376CurrentTrain: epoch  1, batch    19 | loss: 72.7503462CurrentTrain: epoch  1, batch    20 | loss: 89.5172950CurrentTrain: epoch  1, batch    21 | loss: 87.5781762CurrentTrain: epoch  1, batch    22 | loss: 73.3515620CurrentTrain: epoch  1, batch    23 | loss: 47.3711670CurrentTrain: epoch  1, batch    24 | loss: 89.3992337CurrentTrain: epoch  1, batch    25 | loss: 92.2297858CurrentTrain: epoch  1, batch    26 | loss: 71.3446507CurrentTrain: epoch  1, batch    27 | loss: 47.0498518CurrentTrain: epoch  1, batch    28 | loss: 71.7884681CurrentTrain: epoch  1, batch    29 | loss: 71.0117392CurrentTrain: epoch  1, batch    30 | loss: 71.6286397CurrentTrain: epoch  1, batch    31 | loss: 55.9873710CurrentTrain: epoch  1, batch    32 | loss: 54.4736805CurrentTrain: epoch  1, batch    33 | loss: 47.3005448CurrentTrain: epoch  1, batch    34 | loss: 71.7233656CurrentTrain: epoch  1, batch    35 | loss: 88.5247325CurrentTrain: epoch  1, batch    36 | loss: 88.1434131CurrentTrain: epoch  1, batch    37 | loss: 89.4497174CurrentTrain: epoch  1, batch    38 | loss: 90.8098609CurrentTrain: epoch  1, batch    39 | loss: 71.1403878CurrentTrain: epoch  1, batch    40 | loss: 86.3162689CurrentTrain: epoch  1, batch    41 | loss: 71.1862747CurrentTrain: epoch  1, batch    42 | loss: 88.6891010CurrentTrain: epoch  1, batch    43 | loss: 69.5968553CurrentTrain: epoch  1, batch    44 | loss: 55.4766192CurrentTrain: epoch  1, batch    45 | loss: 90.1463651CurrentTrain: epoch  1, batch    46 | loss: 70.3689029CurrentTrain: epoch  1, batch    47 | loss: 71.1404249CurrentTrain: epoch  1, batch    48 | loss: 71.9631611CurrentTrain: epoch  1, batch    49 | loss: 56.5297141CurrentTrain: epoch  1, batch    50 | loss: 92.2005742CurrentTrain: epoch  1, batch    51 | loss: 119.0174060CurrentTrain: epoch  1, batch    52 | loss: 91.2626325CurrentTrain: epoch  1, batch    53 | loss: 47.8996870CurrentTrain: epoch  1, batch    54 | loss: 87.8969893CurrentTrain: epoch  1, batch    55 | loss: 115.8979293CurrentTrain: epoch  1, batch    56 | loss: 91.3009384CurrentTrain: epoch  1, batch    57 | loss: 58.0417845CurrentTrain: epoch  1, batch    58 | loss: 91.7194782CurrentTrain: epoch  1, batch    59 | loss: 47.8186345CurrentTrain: epoch  1, batch    60 | loss: 65.1046338CurrentTrain: epoch  1, batch    61 | loss: 83.8640371CurrentTrain: epoch  1, batch    62 | loss: 56.3764915CurrentTrain: epoch  1, batch    63 | loss: 73.8461236CurrentTrain: epoch  1, batch    64 | loss: 92.2894840CurrentTrain: epoch  1, batch    65 | loss: 56.5303076CurrentTrain: epoch  1, batch    66 | loss: 56.8356831CurrentTrain: epoch  1, batch    67 | loss: 180.6583476CurrentTrain: epoch  1, batch    68 | loss: 47.5738348CurrentTrain: epoch  1, batch    69 | loss: 55.9050883CurrentTrain: epoch  1, batch    70 | loss: 58.9545623CurrentTrain: epoch  1, batch    71 | loss: 59.0229523CurrentTrain: epoch  1, batch    72 | loss: 67.2935571CurrentTrain: epoch  1, batch    73 | loss: 89.6277505CurrentTrain: epoch  1, batch    74 | loss: 72.5290885CurrentTrain: epoch  1, batch    75 | loss: 115.4434247CurrentTrain: epoch  1, batch    76 | loss: 72.4158905CurrentTrain: epoch  1, batch    77 | loss: 58.0889697CurrentTrain: epoch  1, batch    78 | loss: 86.5246132CurrentTrain: epoch  1, batch    79 | loss: 72.1216851CurrentTrain: epoch  1, batch    80 | loss: 69.4867805CurrentTrain: epoch  1, batch    81 | loss: 69.5640812CurrentTrain: epoch  1, batch    82 | loss: 70.0743418CurrentTrain: epoch  1, batch    83 | loss: 55.1405029CurrentTrain: epoch  1, batch    84 | loss: 70.1127843CurrentTrain: epoch  1, batch    85 | loss: 52.7078686CurrentTrain: epoch  1, batch    86 | loss: 91.8703503CurrentTrain: epoch  1, batch    87 | loss: 69.9379310CurrentTrain: epoch  1, batch    88 | loss: 74.0113591CurrentTrain: epoch  1, batch    89 | loss: 69.6176663CurrentTrain: epoch  1, batch    90 | loss: 64.7963031CurrentTrain: epoch  1, batch    91 | loss: 83.0419559CurrentTrain: epoch  1, batch    92 | loss: 56.9391352CurrentTrain: epoch  1, batch    93 | loss: 58.9090637CurrentTrain: epoch  1, batch    94 | loss: 88.9172436CurrentTrain: epoch  1, batch    95 | loss: 58.1892148CurrentTrain: epoch  2, batch     0 | loss: 82.5688418CurrentTrain: epoch  2, batch     1 | loss: 118.9178995CurrentTrain: epoch  2, batch     2 | loss: 54.2384230CurrentTrain: epoch  2, batch     3 | loss: 48.5592122CurrentTrain: epoch  2, batch     4 | loss: 56.1222469CurrentTrain: epoch  2, batch     5 | loss: 121.6835437CurrentTrain: epoch  2, batch     6 | loss: 48.7734893CurrentTrain: epoch  2, batch     7 | loss: 68.9285234CurrentTrain: epoch  2, batch     8 | loss: 85.5288580CurrentTrain: epoch  2, batch     9 | loss: 47.9293005CurrentTrain: epoch  2, batch    10 | loss: 52.5253740CurrentTrain: epoch  2, batch    11 | loss: 68.1264584CurrentTrain: epoch  2, batch    12 | loss: 73.6867192CurrentTrain: epoch  2, batch    13 | loss: 92.5528651CurrentTrain: epoch  2, batch    14 | loss: 48.5434620CurrentTrain: epoch  2, batch    15 | loss: 87.4888528CurrentTrain: epoch  2, batch    16 | loss: 185.5012661CurrentTrain: epoch  2, batch    17 | loss: 55.8044918CurrentTrain: epoch  2, batch    18 | loss: 66.8980843CurrentTrain: epoch  2, batch    19 | loss: 53.6899542CurrentTrain: epoch  2, batch    20 | loss: 59.6145379CurrentTrain: epoch  2, batch    21 | loss: 70.7210568CurrentTrain: epoch  2, batch    22 | loss: 87.6124468CurrentTrain: epoch  2, batch    23 | loss: 48.0324532CurrentTrain: epoch  2, batch    24 | loss: 89.4190414CurrentTrain: epoch  2, batch    25 | loss: 68.7195052CurrentTrain: epoch  2, batch    26 | loss: 69.2022781CurrentTrain: epoch  2, batch    27 | loss: 88.1399234CurrentTrain: epoch  2, batch    28 | loss: 68.6883204CurrentTrain: epoch  2, batch    29 | loss: 372.6526571CurrentTrain: epoch  2, batch    30 | loss: 53.1805011CurrentTrain: epoch  2, batch    31 | loss: 70.8015408CurrentTrain: epoch  2, batch    32 | loss: 55.2480778CurrentTrain: epoch  2, batch    33 | loss: 90.1980483CurrentTrain: epoch  2, batch    34 | loss: 178.7530686CurrentTrain: epoch  2, batch    35 | loss: 91.2818122CurrentTrain: epoch  2, batch    36 | loss: 83.3542176CurrentTrain: epoch  2, batch    37 | loss: 69.5002087CurrentTrain: epoch  2, batch    38 | loss: 86.1900075CurrentTrain: epoch  2, batch    39 | loss: 65.0487636CurrentTrain: epoch  2, batch    40 | loss: 55.6940026CurrentTrain: epoch  2, batch    41 | loss: 69.1134099CurrentTrain: epoch  2, batch    42 | loss: 88.3015040CurrentTrain: epoch  2, batch    43 | loss: 55.4319490CurrentTrain: epoch  2, batch    44 | loss: 70.0687886CurrentTrain: epoch  2, batch    45 | loss: 117.7027587CurrentTrain: epoch  2, batch    46 | loss: 55.1374579CurrentTrain: epoch  2, batch    47 | loss: 66.8306283CurrentTrain: epoch  2, batch    48 | loss: 69.5261129CurrentTrain: epoch  2, batch    49 | loss: 54.7331515CurrentTrain: epoch  2, batch    50 | loss: 119.0333464CurrentTrain: epoch  2, batch    51 | loss: 59.1632126CurrentTrain: epoch  2, batch    52 | loss: 70.8402169CurrentTrain: epoch  2, batch    53 | loss: 67.3452488CurrentTrain: epoch  2, batch    54 | loss: 56.1555709CurrentTrain: epoch  2, batch    55 | loss: 56.4081733CurrentTrain: epoch  2, batch    56 | loss: 59.3959506CurrentTrain: epoch  2, batch    57 | loss: 87.0550285CurrentTrain: epoch  2, batch    58 | loss: 122.1763052CurrentTrain: epoch  2, batch    59 | loss: 119.1987091CurrentTrain: epoch  2, batch    60 | loss: 67.8996484CurrentTrain: epoch  2, batch    61 | loss: 55.4612651CurrentTrain: epoch  2, batch    62 | loss: 64.1375833CurrentTrain: epoch  2, batch    63 | loss: 86.2327487CurrentTrain: epoch  2, batch    64 | loss: 55.6318876CurrentTrain: epoch  2, batch    65 | loss: 55.9492795CurrentTrain: epoch  2, batch    66 | loss: 54.8524606CurrentTrain: epoch  2, batch    67 | loss: 55.1544530CurrentTrain: epoch  2, batch    68 | loss: 120.7575567CurrentTrain: epoch  2, batch    69 | loss: 86.6996051CurrentTrain: epoch  2, batch    70 | loss: 187.8250282CurrentTrain: epoch  2, batch    71 | loss: 58.8649816CurrentTrain: epoch  2, batch    72 | loss: 68.4693464CurrentTrain: epoch  2, batch    73 | loss: 68.6555795CurrentTrain: epoch  2, batch    74 | loss: 67.2911598CurrentTrain: epoch  2, batch    75 | loss: 53.9419241CurrentTrain: epoch  2, batch    76 | loss: 68.0705296CurrentTrain: epoch  2, batch    77 | loss: 65.6645273CurrentTrain: epoch  2, batch    78 | loss: 63.6074983CurrentTrain: epoch  2, batch    79 | loss: 89.7745562CurrentTrain: epoch  2, batch    80 | loss: 67.5044258CurrentTrain: epoch  2, batch    81 | loss: 66.2219826CurrentTrain: epoch  2, batch    82 | loss: 69.0114409CurrentTrain: epoch  2, batch    83 | loss: 58.4467992CurrentTrain: epoch  2, batch    84 | loss: 56.2505019CurrentTrain: epoch  2, batch    85 | loss: 57.0524544CurrentTrain: epoch  2, batch    86 | loss: 55.8816383CurrentTrain: epoch  2, batch    87 | loss: 83.7181744CurrentTrain: epoch  2, batch    88 | loss: 68.8139663CurrentTrain: epoch  2, batch    89 | loss: 116.1212875CurrentTrain: epoch  2, batch    90 | loss: 68.6028063CurrentTrain: epoch  2, batch    91 | loss: 65.8314685CurrentTrain: epoch  2, batch    92 | loss: 69.3665582CurrentTrain: epoch  2, batch    93 | loss: 120.1972957CurrentTrain: epoch  2, batch    94 | loss: 88.4712345CurrentTrain: epoch  2, batch    95 | loss: 69.6608318CurrentTrain: epoch  3, batch     0 | loss: 65.2269567CurrentTrain: epoch  3, batch     1 | loss: 182.0560130CurrentTrain: epoch  3, batch     2 | loss: 45.0673209CurrentTrain: epoch  3, batch     3 | loss: 44.3893692CurrentTrain: epoch  3, batch     4 | loss: 45.5632337CurrentTrain: epoch  3, batch     5 | loss: 66.7602037CurrentTrain: epoch  3, batch     6 | loss: 372.6899049CurrentTrain: epoch  3, batch     7 | loss: 71.7622169CurrentTrain: epoch  3, batch     8 | loss: 67.0269779CurrentTrain: epoch  3, batch     9 | loss: 68.5563066CurrentTrain: epoch  3, batch    10 | loss: 84.7835917CurrentTrain: epoch  3, batch    11 | loss: 52.9070728CurrentTrain: epoch  3, batch    12 | loss: 67.4830178CurrentTrain: epoch  3, batch    13 | loss: 68.9070157CurrentTrain: epoch  3, batch    14 | loss: 54.1838267CurrentTrain: epoch  3, batch    15 | loss: 71.9410983CurrentTrain: epoch  3, batch    16 | loss: 58.3184001CurrentTrain: epoch  3, batch    17 | loss: 69.7019330CurrentTrain: epoch  3, batch    18 | loss: 84.5114550CurrentTrain: epoch  3, batch    19 | loss: 64.7959373CurrentTrain: epoch  3, batch    20 | loss: 54.3604212CurrentTrain: epoch  3, batch    21 | loss: 70.8291628CurrentTrain: epoch  3, batch    22 | loss: 57.2484158CurrentTrain: epoch  3, batch    23 | loss: 88.0381017CurrentTrain: epoch  3, batch    24 | loss: 70.3687055CurrentTrain: epoch  3, batch    25 | loss: 66.3140964CurrentTrain: epoch  3, batch    26 | loss: 82.8442466CurrentTrain: epoch  3, batch    27 | loss: 56.8584950CurrentTrain: epoch  3, batch    28 | loss: 70.0224535CurrentTrain: epoch  3, batch    29 | loss: 56.0839317CurrentTrain: epoch  3, batch    30 | loss: 65.9830423CurrentTrain: epoch  3, batch    31 | loss: 45.6990358CurrentTrain: epoch  3, batch    32 | loss: 45.0497283CurrentTrain: epoch  3, batch    33 | loss: 67.2135765CurrentTrain: epoch  3, batch    34 | loss: 69.0172815CurrentTrain: epoch  3, batch    35 | loss: 55.7272758CurrentTrain: epoch  3, batch    36 | loss: 65.0355831CurrentTrain: epoch  3, batch    37 | loss: 53.1395089CurrentTrain: epoch  3, batch    38 | loss: 62.1130768CurrentTrain: epoch  3, batch    39 | loss: 56.4254438CurrentTrain: epoch  3, batch    40 | loss: 56.7724033CurrentTrain: epoch  3, batch    41 | loss: 46.8545377CurrentTrain: epoch  3, batch    42 | loss: 83.5239057CurrentTrain: epoch  3, batch    43 | loss: 67.5919494CurrentTrain: epoch  3, batch    44 | loss: 51.2486349CurrentTrain: epoch  3, batch    45 | loss: 83.8726891CurrentTrain: epoch  3, batch    46 | loss: 55.2912214CurrentTrain: epoch  3, batch    47 | loss: 112.9514270CurrentTrain: epoch  3, batch    48 | loss: 65.2838291CurrentTrain: epoch  3, batch    49 | loss: 69.9893778CurrentTrain: epoch  3, batch    50 | loss: 85.4855446CurrentTrain: epoch  3, batch    51 | loss: 67.2647932CurrentTrain: epoch  3, batch    52 | loss: 57.6852610CurrentTrain: epoch  3, batch    53 | loss: 116.5449935CurrentTrain: epoch  3, batch    54 | loss: 54.7269560CurrentTrain: epoch  3, batch    55 | loss: 47.3331450CurrentTrain: epoch  3, batch    56 | loss: 66.5222799CurrentTrain: epoch  3, batch    57 | loss: 66.3153807CurrentTrain: epoch  3, batch    58 | loss: 52.3303662CurrentTrain: epoch  3, batch    59 | loss: 119.4544618CurrentTrain: epoch  3, batch    60 | loss: 64.8841392CurrentTrain: epoch  3, batch    61 | loss: 63.8798146CurrentTrain: epoch  3, batch    62 | loss: 85.3493868CurrentTrain: epoch  3, batch    63 | loss: 65.4186667CurrentTrain: epoch  3, batch    64 | loss: 80.3431810CurrentTrain: epoch  3, batch    65 | loss: 69.7398967CurrentTrain: epoch  3, batch    66 | loss: 56.8761150CurrentTrain: epoch  3, batch    67 | loss: 67.3563673CurrentTrain: epoch  3, batch    68 | loss: 65.7246237CurrentTrain: epoch  3, batch    69 | loss: 182.8911742CurrentTrain: epoch  3, batch    70 | loss: 84.3805546CurrentTrain: epoch  3, batch    71 | loss: 65.9273582CurrentTrain: epoch  3, batch    72 | loss: 84.9270016CurrentTrain: epoch  3, batch    73 | loss: 69.2266533CurrentTrain: epoch  3, batch    74 | loss: 83.1350619CurrentTrain: epoch  3, batch    75 | loss: 116.0158350CurrentTrain: epoch  3, batch    76 | loss: 53.4711155CurrentTrain: epoch  3, batch    77 | loss: 83.0283069CurrentTrain: epoch  3, batch    78 | loss: 53.9850263CurrentTrain: epoch  3, batch    79 | loss: 68.3311293CurrentTrain: epoch  3, batch    80 | loss: 86.9424551CurrentTrain: epoch  3, batch    81 | loss: 118.7356147CurrentTrain: epoch  3, batch    82 | loss: 53.1886565CurrentTrain: epoch  3, batch    83 | loss: 71.3539495CurrentTrain: epoch  3, batch    84 | loss: 64.4682727CurrentTrain: epoch  3, batch    85 | loss: 55.4495333CurrentTrain: epoch  3, batch    86 | loss: 67.8692373CurrentTrain: epoch  3, batch    87 | loss: 116.4174523CurrentTrain: epoch  3, batch    88 | loss: 116.7982787CurrentTrain: epoch  3, batch    89 | loss: 68.3268907CurrentTrain: epoch  3, batch    90 | loss: 54.4052236CurrentTrain: epoch  3, batch    91 | loss: 66.7545249CurrentTrain: epoch  3, batch    92 | loss: 122.0474948CurrentTrain: epoch  3, batch    93 | loss: 82.1864586CurrentTrain: epoch  3, batch    94 | loss: 86.9117241CurrentTrain: epoch  3, batch    95 | loss: 57.8732699CurrentTrain: epoch  4, batch     0 | loss: 66.9420580CurrentTrain: epoch  4, batch     1 | loss: 50.7303095CurrentTrain: epoch  4, batch     2 | loss: 62.3988419CurrentTrain: epoch  4, batch     3 | loss: 67.3398274CurrentTrain: epoch  4, batch     4 | loss: 65.9192871CurrentTrain: epoch  4, batch     5 | loss: 51.7869769CurrentTrain: epoch  4, batch     6 | loss: 84.1339667CurrentTrain: epoch  4, batch     7 | loss: 66.3835349CurrentTrain: epoch  4, batch     8 | loss: 118.0224479CurrentTrain: epoch  4, batch     9 | loss: 86.1963057CurrentTrain: epoch  4, batch    10 | loss: 51.2077257CurrentTrain: epoch  4, batch    11 | loss: 116.9771006CurrentTrain: epoch  4, batch    12 | loss: 81.0924457CurrentTrain: epoch  4, batch    13 | loss: 68.7458369CurrentTrain: epoch  4, batch    14 | loss: 68.4733633CurrentTrain: epoch  4, batch    15 | loss: 117.4460588CurrentTrain: epoch  4, batch    16 | loss: 63.8974903CurrentTrain: epoch  4, batch    17 | loss: 82.5728950CurrentTrain: epoch  4, batch    18 | loss: 67.0872706CurrentTrain: epoch  4, batch    19 | loss: 67.0592350CurrentTrain: epoch  4, batch    20 | loss: 65.2356541CurrentTrain: epoch  4, batch    21 | loss: 53.2951386CurrentTrain: epoch  4, batch    22 | loss: 54.6069033CurrentTrain: epoch  4, batch    23 | loss: 67.3000690CurrentTrain: epoch  4, batch    24 | loss: 64.8889373CurrentTrain: epoch  4, batch    25 | loss: 88.4126225CurrentTrain: epoch  4, batch    26 | loss: 66.2633384CurrentTrain: epoch  4, batch    27 | loss: 44.6518984CurrentTrain: epoch  4, batch    28 | loss: 51.1653863CurrentTrain: epoch  4, batch    29 | loss: 68.5361881CurrentTrain: epoch  4, batch    30 | loss: 83.5471365CurrentTrain: epoch  4, batch    31 | loss: 82.8051879CurrentTrain: epoch  4, batch    32 | loss: 51.8019140CurrentTrain: epoch  4, batch    33 | loss: 68.8297575CurrentTrain: epoch  4, batch    34 | loss: 55.1787503CurrentTrain: epoch  4, batch    35 | loss: 67.7737321CurrentTrain: epoch  4, batch    36 | loss: 68.8597852CurrentTrain: epoch  4, batch    37 | loss: 65.4657038CurrentTrain: epoch  4, batch    38 | loss: 63.0157395CurrentTrain: epoch  4, batch    39 | loss: 115.3288399CurrentTrain: epoch  4, batch    40 | loss: 67.2135193CurrentTrain: epoch  4, batch    41 | loss: 61.9966939CurrentTrain: epoch  4, batch    42 | loss: 85.8061010CurrentTrain: epoch  4, batch    43 | loss: 64.3589764CurrentTrain: epoch  4, batch    44 | loss: 89.9566857CurrentTrain: epoch  4, batch    45 | loss: 65.6914332CurrentTrain: epoch  4, batch    46 | loss: 87.9915702CurrentTrain: epoch  4, batch    47 | loss: 51.1747178CurrentTrain: epoch  4, batch    48 | loss: 65.5260048CurrentTrain: epoch  4, batch    49 | loss: 85.6776169CurrentTrain: epoch  4, batch    50 | loss: 66.6219915CurrentTrain: epoch  4, batch    51 | loss: 53.2634897CurrentTrain: epoch  4, batch    52 | loss: 87.5125709CurrentTrain: epoch  4, batch    53 | loss: 116.5858480CurrentTrain: epoch  4, batch    54 | loss: 61.5983421CurrentTrain: epoch  4, batch    55 | loss: 65.3569877CurrentTrain: epoch  4, batch    56 | loss: 83.7643429CurrentTrain: epoch  4, batch    57 | loss: 84.6662362CurrentTrain: epoch  4, batch    58 | loss: 55.7239548CurrentTrain: epoch  4, batch    59 | loss: 87.5844964CurrentTrain: epoch  4, batch    60 | loss: 111.4836510CurrentTrain: epoch  4, batch    61 | loss: 46.5827742CurrentTrain: epoch  4, batch    62 | loss: 46.1636017CurrentTrain: epoch  4, batch    63 | loss: 57.1269697CurrentTrain: epoch  4, batch    64 | loss: 87.4881291CurrentTrain: epoch  4, batch    65 | loss: 88.5263968CurrentTrain: epoch  4, batch    66 | loss: 48.8003552CurrentTrain: epoch  4, batch    67 | loss: 68.5022578CurrentTrain: epoch  4, batch    68 | loss: 54.6284919CurrentTrain: epoch  4, batch    69 | loss: 117.0009211CurrentTrain: epoch  4, batch    70 | loss: 49.4781176CurrentTrain: epoch  4, batch    71 | loss: 55.4807800CurrentTrain: epoch  4, batch    72 | loss: 61.2729244CurrentTrain: epoch  4, batch    73 | loss: 56.9302228CurrentTrain: epoch  4, batch    74 | loss: 66.3324253CurrentTrain: epoch  4, batch    75 | loss: 65.9302523CurrentTrain: epoch  4, batch    76 | loss: 82.2858084CurrentTrain: epoch  4, batch    77 | loss: 65.6716696CurrentTrain: epoch  4, batch    78 | loss: 55.9693163CurrentTrain: epoch  4, batch    79 | loss: 119.5746387CurrentTrain: epoch  4, batch    80 | loss: 119.6004177CurrentTrain: epoch  4, batch    81 | loss: 115.9579118CurrentTrain: epoch  4, batch    82 | loss: 64.5106365CurrentTrain: epoch  4, batch    83 | loss: 175.2365529CurrentTrain: epoch  4, batch    84 | loss: 70.4755273CurrentTrain: epoch  4, batch    85 | loss: 54.4812701CurrentTrain: epoch  4, batch    86 | loss: 66.2521900CurrentTrain: epoch  4, batch    87 | loss: 66.2446364CurrentTrain: epoch  4, batch    88 | loss: 117.9640424CurrentTrain: epoch  4, batch    89 | loss: 51.4103070CurrentTrain: epoch  4, batch    90 | loss: 87.0214841CurrentTrain: epoch  4, batch    91 | loss: 65.3075542CurrentTrain: epoch  4, batch    92 | loss: 44.3866162CurrentTrain: epoch  4, batch    93 | loss: 69.5980904CurrentTrain: epoch  4, batch    94 | loss: 84.0573619CurrentTrain: epoch  4, batch    95 | loss: 73.7853834CurrentTrain: epoch  5, batch     0 | loss: 86.3413716CurrentTrain: epoch  5, batch     1 | loss: 67.9730629CurrentTrain: epoch  5, batch     2 | loss: 52.3137907CurrentTrain: epoch  5, batch     3 | loss: 54.4496120CurrentTrain: epoch  5, batch     4 | loss: 56.2025708CurrentTrain: epoch  5, batch     5 | loss: 68.0759457CurrentTrain: epoch  5, batch     6 | loss: 86.5107168CurrentTrain: epoch  5, batch     7 | loss: 175.4612333CurrentTrain: epoch  5, batch     8 | loss: 65.8900842CurrentTrain: epoch  5, batch     9 | loss: 67.0480848CurrentTrain: epoch  5, batch    10 | loss: 85.9262940CurrentTrain: epoch  5, batch    11 | loss: 81.2009351CurrentTrain: epoch  5, batch    12 | loss: 182.0261843CurrentTrain: epoch  5, batch    13 | loss: 84.7909122CurrentTrain: epoch  5, batch    14 | loss: 86.7371284CurrentTrain: epoch  5, batch    15 | loss: 47.7010988CurrentTrain: epoch  5, batch    16 | loss: 85.0609832CurrentTrain: epoch  5, batch    17 | loss: 53.3902881CurrentTrain: epoch  5, batch    18 | loss: 51.3807156CurrentTrain: epoch  5, batch    19 | loss: 84.6555772CurrentTrain: epoch  5, batch    20 | loss: 52.0832310CurrentTrain: epoch  5, batch    21 | loss: 80.0723293CurrentTrain: epoch  5, batch    22 | loss: 50.9702793CurrentTrain: epoch  5, batch    23 | loss: 65.9233837CurrentTrain: epoch  5, batch    24 | loss: 53.4055782CurrentTrain: epoch  5, batch    25 | loss: 54.3235413CurrentTrain: epoch  5, batch    26 | loss: 65.5361255CurrentTrain: epoch  5, batch    27 | loss: 62.7731338CurrentTrain: epoch  5, batch    28 | loss: 61.0486934CurrentTrain: epoch  5, batch    29 | loss: 86.1019728CurrentTrain: epoch  5, batch    30 | loss: 64.9362753CurrentTrain: epoch  5, batch    31 | loss: 84.2767204CurrentTrain: epoch  5, batch    32 | loss: 64.9126114CurrentTrain: epoch  5, batch    33 | loss: 119.4818331CurrentTrain: epoch  5, batch    34 | loss: 82.4171609CurrentTrain: epoch  5, batch    35 | loss: 119.6965162CurrentTrain: epoch  5, batch    36 | loss: 65.0237369CurrentTrain: epoch  5, batch    37 | loss: 81.5732162CurrentTrain: epoch  5, batch    38 | loss: 53.7119356CurrentTrain: epoch  5, batch    39 | loss: 83.0224920CurrentTrain: epoch  5, batch    40 | loss: 54.3020854CurrentTrain: epoch  5, batch    41 | loss: 66.8163551CurrentTrain: epoch  5, batch    42 | loss: 65.8915788CurrentTrain: epoch  5, batch    43 | loss: 183.2629191CurrentTrain: epoch  5, batch    44 | loss: 43.3361223CurrentTrain: epoch  5, batch    45 | loss: 54.0217202CurrentTrain: epoch  5, batch    46 | loss: 83.0471543CurrentTrain: epoch  5, batch    47 | loss: 64.8591466CurrentTrain: epoch  5, batch    48 | loss: 88.1778048CurrentTrain: epoch  5, batch    49 | loss: 50.9789190CurrentTrain: epoch  5, batch    50 | loss: 60.4876123CurrentTrain: epoch  5, batch    51 | loss: 65.5847288CurrentTrain: epoch  5, batch    52 | loss: 54.4848367CurrentTrain: epoch  5, batch    53 | loss: 66.3554744CurrentTrain: epoch  5, batch    54 | loss: 41.3803127CurrentTrain: epoch  5, batch    55 | loss: 64.3999936CurrentTrain: epoch  5, batch    56 | loss: 62.3518325CurrentTrain: epoch  5, batch    57 | loss: 65.2071771CurrentTrain: epoch  5, batch    58 | loss: 47.0839875CurrentTrain: epoch  5, batch    59 | loss: 64.7300866CurrentTrain: epoch  5, batch    60 | loss: 54.6336222CurrentTrain: epoch  5, batch    61 | loss: 54.6334168CurrentTrain: epoch  5, batch    62 | loss: 55.4257950CurrentTrain: epoch  5, batch    63 | loss: 83.3294891CurrentTrain: epoch  5, batch    64 | loss: 117.2026578CurrentTrain: epoch  5, batch    65 | loss: 66.7083272CurrentTrain: epoch  5, batch    66 | loss: 45.0138628CurrentTrain: epoch  5, batch    67 | loss: 86.2632904CurrentTrain: epoch  5, batch    68 | loss: 83.6578093CurrentTrain: epoch  5, batch    69 | loss: 84.7671424CurrentTrain: epoch  5, batch    70 | loss: 118.7072067CurrentTrain: epoch  5, batch    71 | loss: 53.1752440CurrentTrain: epoch  5, batch    72 | loss: 80.2462315CurrentTrain: epoch  5, batch    73 | loss: 84.8118379CurrentTrain: epoch  5, batch    74 | loss: 63.6998685CurrentTrain: epoch  5, batch    75 | loss: 52.4860648CurrentTrain: epoch  5, batch    76 | loss: 66.1568516CurrentTrain: epoch  5, batch    77 | loss: 111.5036431CurrentTrain: epoch  5, batch    78 | loss: 80.2481218CurrentTrain: epoch  5, batch    79 | loss: 64.2444116CurrentTrain: epoch  5, batch    80 | loss: 49.7183586CurrentTrain: epoch  5, batch    81 | loss: 183.7276265CurrentTrain: epoch  5, batch    82 | loss: 64.1630347CurrentTrain: epoch  5, batch    83 | loss: 54.1543255CurrentTrain: epoch  5, batch    84 | loss: 54.2938747CurrentTrain: epoch  5, batch    85 | loss: 63.0390973CurrentTrain: epoch  5, batch    86 | loss: 61.1419557CurrentTrain: epoch  5, batch    87 | loss: 86.6549128CurrentTrain: epoch  5, batch    88 | loss: 84.0733245CurrentTrain: epoch  5, batch    89 | loss: 85.7435287CurrentTrain: epoch  5, batch    90 | loss: 54.6494410CurrentTrain: epoch  5, batch    91 | loss: 65.7526071CurrentTrain: epoch  5, batch    92 | loss: 66.8174689CurrentTrain: epoch  5, batch    93 | loss: 65.7257740CurrentTrain: epoch  5, batch    94 | loss: 53.0625135CurrentTrain: epoch  5, batch    95 | loss: 72.2097801CurrentTrain: epoch  6, batch     0 | loss: 67.2231256CurrentTrain: epoch  6, batch     1 | loss: 53.4065225CurrentTrain: epoch  6, batch     2 | loss: 115.6809671CurrentTrain: epoch  6, batch     3 | loss: 65.5695772CurrentTrain: epoch  6, batch     4 | loss: 46.7705088CurrentTrain: epoch  6, batch     5 | loss: 67.0306981CurrentTrain: epoch  6, batch     6 | loss: 49.9893406CurrentTrain: epoch  6, batch     7 | loss: 53.3180058CurrentTrain: epoch  6, batch     8 | loss: 54.9651926CurrentTrain: epoch  6, batch     9 | loss: 50.8784314CurrentTrain: epoch  6, batch    10 | loss: 83.6176958CurrentTrain: epoch  6, batch    11 | loss: 65.5227236CurrentTrain: epoch  6, batch    12 | loss: 67.4213654CurrentTrain: epoch  6, batch    13 | loss: 84.8970458CurrentTrain: epoch  6, batch    14 | loss: 52.9908210CurrentTrain: epoch  6, batch    15 | loss: 83.7278578CurrentTrain: epoch  6, batch    16 | loss: 69.1562428CurrentTrain: epoch  6, batch    17 | loss: 66.8994496CurrentTrain: epoch  6, batch    18 | loss: 79.0512723CurrentTrain: epoch  6, batch    19 | loss: 84.2418999CurrentTrain: epoch  6, batch    20 | loss: 67.8602116CurrentTrain: epoch  6, batch    21 | loss: 83.4315029CurrentTrain: epoch  6, batch    22 | loss: 110.4908238CurrentTrain: epoch  6, batch    23 | loss: 86.9808673CurrentTrain: epoch  6, batch    24 | loss: 82.6040993CurrentTrain: epoch  6, batch    25 | loss: 113.3053781CurrentTrain: epoch  6, batch    26 | loss: 86.8812815CurrentTrain: epoch  6, batch    27 | loss: 65.8246075CurrentTrain: epoch  6, batch    28 | loss: 66.9000483CurrentTrain: epoch  6, batch    29 | loss: 84.3234585CurrentTrain: epoch  6, batch    30 | loss: 119.6520239CurrentTrain: epoch  6, batch    31 | loss: 84.7447855CurrentTrain: epoch  6, batch    32 | loss: 67.6368443CurrentTrain: epoch  6, batch    33 | loss: 50.7221745CurrentTrain: epoch  6, batch    34 | loss: 51.8200122CurrentTrain: epoch  6, batch    35 | loss: 65.8392092CurrentTrain: epoch  6, batch    36 | loss: 44.8612534CurrentTrain: epoch  6, batch    37 | loss: 62.3294638CurrentTrain: epoch  6, batch    38 | loss: 63.4323564CurrentTrain: epoch  6, batch    39 | loss: 66.9703713CurrentTrain: epoch  6, batch    40 | loss: 86.7283730CurrentTrain: epoch  6, batch    41 | loss: 51.5416180CurrentTrain: epoch  6, batch    42 | loss: 64.5520962CurrentTrain: epoch  6, batch    43 | loss: 75.9368212CurrentTrain: epoch  6, batch    44 | loss: 53.3031044CurrentTrain: epoch  6, batch    45 | loss: 82.6167920CurrentTrain: epoch  6, batch    46 | loss: 64.5461439CurrentTrain: epoch  6, batch    47 | loss: 86.3929521CurrentTrain: epoch  6, batch    48 | loss: 50.6880925CurrentTrain: epoch  6, batch    49 | loss: 62.7126180CurrentTrain: epoch  6, batch    50 | loss: 66.1307160CurrentTrain: epoch  6, batch    51 | loss: 66.9841142CurrentTrain: epoch  6, batch    52 | loss: 84.2927313CurrentTrain: epoch  6, batch    53 | loss: 44.0836122CurrentTrain: epoch  6, batch    54 | loss: 65.4773317CurrentTrain: epoch  6, batch    55 | loss: 66.4320925CurrentTrain: epoch  6, batch    56 | loss: 65.0734978CurrentTrain: epoch  6, batch    57 | loss: 57.9591009CurrentTrain: epoch  6, batch    58 | loss: 49.5430147CurrentTrain: epoch  6, batch    59 | loss: 84.1375080CurrentTrain: epoch  6, batch    60 | loss: 51.4379440CurrentTrain: epoch  6, batch    61 | loss: 51.8675911CurrentTrain: epoch  6, batch    62 | loss: 51.6341169CurrentTrain: epoch  6, batch    63 | loss: 67.3662195CurrentTrain: epoch  6, batch    64 | loss: 49.2641511CurrentTrain: epoch  6, batch    65 | loss: 84.1755442CurrentTrain: epoch  6, batch    66 | loss: 81.7396652CurrentTrain: epoch  6, batch    67 | loss: 82.6301851CurrentTrain: epoch  6, batch    68 | loss: 65.2204243CurrentTrain: epoch  6, batch    69 | loss: 52.7376812CurrentTrain: epoch  6, batch    70 | loss: 65.0487626CurrentTrain: epoch  6, batch    71 | loss: 65.4292836CurrentTrain: epoch  6, batch    72 | loss: 81.3232345CurrentTrain: epoch  6, batch    73 | loss: 63.1632752CurrentTrain: epoch  6, batch    74 | loss: 63.9944989CurrentTrain: epoch  6, batch    75 | loss: 52.1239790CurrentTrain: epoch  6, batch    76 | loss: 85.7717614CurrentTrain: epoch  6, batch    77 | loss: 66.2736532CurrentTrain: epoch  6, batch    78 | loss: 65.0308459CurrentTrain: epoch  6, batch    79 | loss: 67.7962378CurrentTrain: epoch  6, batch    80 | loss: 62.6712117CurrentTrain: epoch  6, batch    81 | loss: 52.1847180CurrentTrain: epoch  6, batch    82 | loss: 44.1974281CurrentTrain: epoch  6, batch    83 | loss: 61.7465096CurrentTrain: epoch  6, batch    84 | loss: 84.5996692CurrentTrain: epoch  6, batch    85 | loss: 67.0732146CurrentTrain: epoch  6, batch    86 | loss: 85.0664361CurrentTrain: epoch  6, batch    87 | loss: 87.1388421CurrentTrain: epoch  6, batch    88 | loss: 82.8503005CurrentTrain: epoch  6, batch    89 | loss: 86.2794772CurrentTrain: epoch  6, batch    90 | loss: 93.1896312CurrentTrain: epoch  6, batch    91 | loss: 67.9105730CurrentTrain: epoch  6, batch    92 | loss: 65.2637881CurrentTrain: epoch  6, batch    93 | loss: 62.2902623CurrentTrain: epoch  6, batch    94 | loss: 118.2631537CurrentTrain: epoch  6, batch    95 | loss: 90.7760412CurrentTrain: epoch  7, batch     0 | loss: 53.6717696CurrentTrain: epoch  7, batch     1 | loss: 53.5737552CurrentTrain: epoch  7, batch     2 | loss: 81.1687632CurrentTrain: epoch  7, batch     3 | loss: 115.3686078CurrentTrain: epoch  7, batch     4 | loss: 61.1267965CurrentTrain: epoch  7, batch     5 | loss: 63.2370159CurrentTrain: epoch  7, batch     6 | loss: 62.2993116CurrentTrain: epoch  7, batch     7 | loss: 66.8976839CurrentTrain: epoch  7, batch     8 | loss: 51.8695288CurrentTrain: epoch  7, batch     9 | loss: 82.0098346CurrentTrain: epoch  7, batch    10 | loss: 54.3730243CurrentTrain: epoch  7, batch    11 | loss: 84.2043753CurrentTrain: epoch  7, batch    12 | loss: 51.4767628CurrentTrain: epoch  7, batch    13 | loss: 65.9700526CurrentTrain: epoch  7, batch    14 | loss: 66.5154032CurrentTrain: epoch  7, batch    15 | loss: 43.0821787CurrentTrain: epoch  7, batch    16 | loss: 84.2242792CurrentTrain: epoch  7, batch    17 | loss: 53.8517309CurrentTrain: epoch  7, batch    18 | loss: 63.5332016CurrentTrain: epoch  7, batch    19 | loss: 51.8059024CurrentTrain: epoch  7, batch    20 | loss: 63.3619091CurrentTrain: epoch  7, batch    21 | loss: 61.9907170CurrentTrain: epoch  7, batch    22 | loss: 46.5231696CurrentTrain: epoch  7, batch    23 | loss: 48.9633086CurrentTrain: epoch  7, batch    24 | loss: 83.9065604CurrentTrain: epoch  7, batch    25 | loss: 54.2606903CurrentTrain: epoch  7, batch    26 | loss: 82.7304252CurrentTrain: epoch  7, batch    27 | loss: 53.5273927CurrentTrain: epoch  7, batch    28 | loss: 84.1938531CurrentTrain: epoch  7, batch    29 | loss: 64.5839005CurrentTrain: epoch  7, batch    30 | loss: 85.1370415CurrentTrain: epoch  7, batch    31 | loss: 82.6583844CurrentTrain: epoch  7, batch    32 | loss: 50.1009067CurrentTrain: epoch  7, batch    33 | loss: 64.3977223CurrentTrain: epoch  7, batch    34 | loss: 83.9407356CurrentTrain: epoch  7, batch    35 | loss: 65.8865019CurrentTrain: epoch  7, batch    36 | loss: 53.4506364CurrentTrain: epoch  7, batch    37 | loss: 89.8912319CurrentTrain: epoch  7, batch    38 | loss: 82.6598570CurrentTrain: epoch  7, batch    39 | loss: 173.1753773CurrentTrain: epoch  7, batch    40 | loss: 66.8799306CurrentTrain: epoch  7, batch    41 | loss: 53.6747045CurrentTrain: epoch  7, batch    42 | loss: 68.4200216CurrentTrain: epoch  7, batch    43 | loss: 85.7712338CurrentTrain: epoch  7, batch    44 | loss: 38.3990730CurrentTrain: epoch  7, batch    45 | loss: 82.5023665CurrentTrain: epoch  7, batch    46 | loss: 65.0100219CurrentTrain: epoch  7, batch    47 | loss: 64.8662464CurrentTrain: epoch  7, batch    48 | loss: 85.7989575CurrentTrain: epoch  7, batch    49 | loss: 44.1333310CurrentTrain: epoch  7, batch    50 | loss: 50.7954042CurrentTrain: epoch  7, batch    51 | loss: 50.5924101CurrentTrain: epoch  7, batch    52 | loss: 66.9614823CurrentTrain: epoch  7, batch    53 | loss: 65.6626875CurrentTrain: epoch  7, batch    54 | loss: 64.3501205CurrentTrain: epoch  7, batch    55 | loss: 66.5112646CurrentTrain: epoch  7, batch    56 | loss: 66.2437447CurrentTrain: epoch  7, batch    57 | loss: 67.5434220CurrentTrain: epoch  7, batch    58 | loss: 80.2127758CurrentTrain: epoch  7, batch    59 | loss: 85.9659325CurrentTrain: epoch  7, batch    60 | loss: 66.2327936CurrentTrain: epoch  7, batch    61 | loss: 50.2056974CurrentTrain: epoch  7, batch    62 | loss: 86.2448958CurrentTrain: epoch  7, batch    63 | loss: 67.2965651CurrentTrain: epoch  7, batch    64 | loss: 64.3247006CurrentTrain: epoch  7, batch    65 | loss: 86.2173363CurrentTrain: epoch  7, batch    66 | loss: 84.2107753CurrentTrain: epoch  7, batch    67 | loss: 83.0326964CurrentTrain: epoch  7, batch    68 | loss: 63.0103124CurrentTrain: epoch  7, batch    69 | loss: 82.0506209CurrentTrain: epoch  7, batch    70 | loss: 51.1359165CurrentTrain: epoch  7, batch    71 | loss: 84.6029743CurrentTrain: epoch  7, batch    72 | loss: 67.3323659CurrentTrain: epoch  7, batch    73 | loss: 52.0606542CurrentTrain: epoch  7, batch    74 | loss: 69.0000250CurrentTrain: epoch  7, batch    75 | loss: 65.1866411CurrentTrain: epoch  7, batch    76 | loss: 83.3369055CurrentTrain: epoch  7, batch    77 | loss: 78.6664649CurrentTrain: epoch  7, batch    78 | loss: 63.6839912CurrentTrain: epoch  7, batch    79 | loss: 52.1753746CurrentTrain: epoch  7, batch    80 | loss: 81.1654247CurrentTrain: epoch  7, batch    81 | loss: 60.8296621CurrentTrain: epoch  7, batch    82 | loss: 51.8138394CurrentTrain: epoch  7, batch    83 | loss: 60.8421023CurrentTrain: epoch  7, batch    84 | loss: 113.4335523CurrentTrain: epoch  7, batch    85 | loss: 117.5165880CurrentTrain: epoch  7, batch    86 | loss: 66.6778970CurrentTrain: epoch  7, batch    87 | loss: 177.6049117CurrentTrain: epoch  7, batch    88 | loss: 68.4755010CurrentTrain: epoch  7, batch    89 | loss: 187.4425900CurrentTrain: epoch  7, batch    90 | loss: 49.7061453CurrentTrain: epoch  7, batch    91 | loss: 76.5809696CurrentTrain: epoch  7, batch    92 | loss: 51.6561820CurrentTrain: epoch  7, batch    93 | loss: 84.1304117CurrentTrain: epoch  7, batch    94 | loss: 64.6426750CurrentTrain: epoch  7, batch    95 | loss: 95.7170409CurrentTrain: epoch  8, batch     0 | loss: 63.0869996CurrentTrain: epoch  8, batch     1 | loss: 63.1567916CurrentTrain: epoch  8, batch     2 | loss: 82.1653112CurrentTrain: epoch  8, batch     3 | loss: 53.5127168CurrentTrain: epoch  8, batch     4 | loss: 81.6895794CurrentTrain: epoch  8, batch     5 | loss: 51.0301098CurrentTrain: epoch  8, batch     6 | loss: 82.4884892CurrentTrain: epoch  8, batch     7 | loss: 40.5368137CurrentTrain: epoch  8, batch     8 | loss: 61.8404365CurrentTrain: epoch  8, batch     9 | loss: 49.5441182CurrentTrain: epoch  8, batch    10 | loss: 84.1258816CurrentTrain: epoch  8, batch    11 | loss: 52.5119349CurrentTrain: epoch  8, batch    12 | loss: 63.4305464CurrentTrain: epoch  8, batch    13 | loss: 81.5611104CurrentTrain: epoch  8, batch    14 | loss: 52.3954761CurrentTrain: epoch  8, batch    15 | loss: 86.1684858CurrentTrain: epoch  8, batch    16 | loss: 41.2273757CurrentTrain: epoch  8, batch    17 | loss: 65.5986254CurrentTrain: epoch  8, batch    18 | loss: 64.9059748CurrentTrain: epoch  8, batch    19 | loss: 83.8902266CurrentTrain: epoch  8, batch    20 | loss: 372.2306544CurrentTrain: epoch  8, batch    21 | loss: 66.3568573CurrentTrain: epoch  8, batch    22 | loss: 63.5965325CurrentTrain: epoch  8, batch    23 | loss: 117.5232827CurrentTrain: epoch  8, batch    24 | loss: 111.2434395CurrentTrain: epoch  8, batch    25 | loss: 54.9948939CurrentTrain: epoch  8, batch    26 | loss: 51.5926676CurrentTrain: epoch  8, batch    27 | loss: 50.8484839CurrentTrain: epoch  8, batch    28 | loss: 52.2478099CurrentTrain: epoch  8, batch    29 | loss: 53.1421319CurrentTrain: epoch  8, batch    30 | loss: 81.2392237CurrentTrain: epoch  8, batch    31 | loss: 64.5404529CurrentTrain: epoch  8, batch    32 | loss: 115.2989854CurrentTrain: epoch  8, batch    33 | loss: 113.2604774CurrentTrain: epoch  8, batch    34 | loss: 84.3296903CurrentTrain: epoch  8, batch    35 | loss: 51.2607029CurrentTrain: epoch  8, batch    36 | loss: 67.6049005CurrentTrain: epoch  8, batch    37 | loss: 64.6317079CurrentTrain: epoch  8, batch    38 | loss: 66.7544619CurrentTrain: epoch  8, batch    39 | loss: 82.4966400CurrentTrain: epoch  8, batch    40 | loss: 65.7128594CurrentTrain: epoch  8, batch    41 | loss: 117.4991382CurrentTrain: epoch  8, batch    42 | loss: 52.2703973CurrentTrain: epoch  8, batch    43 | loss: 82.7829602CurrentTrain: epoch  8, batch    44 | loss: 63.8853302CurrentTrain: epoch  8, batch    45 | loss: 61.8171685CurrentTrain: epoch  8, batch    46 | loss: 52.1860402CurrentTrain: epoch  8, batch    47 | loss: 62.2298491CurrentTrain: epoch  8, batch    48 | loss: 84.0801288CurrentTrain: epoch  8, batch    49 | loss: 64.3930176CurrentTrain: epoch  8, batch    50 | loss: 84.1616755CurrentTrain: epoch  8, batch    51 | loss: 41.5333508CurrentTrain: epoch  8, batch    52 | loss: 62.9342493CurrentTrain: epoch  8, batch    53 | loss: 52.1346723CurrentTrain: epoch  8, batch    54 | loss: 54.2518161CurrentTrain: epoch  8, batch    55 | loss: 55.3916533CurrentTrain: epoch  8, batch    56 | loss: 53.3949940CurrentTrain: epoch  8, batch    57 | loss: 49.2190812CurrentTrain: epoch  8, batch    58 | loss: 84.1012124CurrentTrain: epoch  8, batch    59 | loss: 52.1833274CurrentTrain: epoch  8, batch    60 | loss: 63.1349960CurrentTrain: epoch  8, batch    61 | loss: 65.9337254CurrentTrain: epoch  8, batch    62 | loss: 87.2084389CurrentTrain: epoch  8, batch    63 | loss: 51.4410547CurrentTrain: epoch  8, batch    64 | loss: 60.8735455CurrentTrain: epoch  8, batch    65 | loss: 66.7982615CurrentTrain: epoch  8, batch    66 | loss: 65.6596372CurrentTrain: epoch  8, batch    67 | loss: 45.2671184CurrentTrain: epoch  8, batch    68 | loss: 82.3856164CurrentTrain: epoch  8, batch    69 | loss: 53.0967473CurrentTrain: epoch  8, batch    70 | loss: 82.3983684CurrentTrain: epoch  8, batch    71 | loss: 64.2465503CurrentTrain: epoch  8, batch    72 | loss: 51.3997693CurrentTrain: epoch  8, batch    73 | loss: 66.8895979CurrentTrain: epoch  8, batch    74 | loss: 63.9698288CurrentTrain: epoch  8, batch    75 | loss: 64.6248879CurrentTrain: epoch  8, batch    76 | loss: 64.0506534CurrentTrain: epoch  8, batch    77 | loss: 78.6582294CurrentTrain: epoch  8, batch    78 | loss: 53.1129260CurrentTrain: epoch  8, batch    79 | loss: 66.6975863CurrentTrain: epoch  8, batch    80 | loss: 86.7928484CurrentTrain: epoch  8, batch    81 | loss: 51.9627701CurrentTrain: epoch  8, batch    82 | loss: 52.7740808CurrentTrain: epoch  8, batch    83 | loss: 84.0202166CurrentTrain: epoch  8, batch    84 | loss: 83.5504907CurrentTrain: epoch  8, batch    85 | loss: 64.3132886CurrentTrain: epoch  8, batch    86 | loss: 84.0621945CurrentTrain: epoch  8, batch    87 | loss: 80.7288128CurrentTrain: epoch  8, batch    88 | loss: 64.1551380CurrentTrain: epoch  8, batch    89 | loss: 62.1457027CurrentTrain: epoch  8, batch    90 | loss: 51.4774450CurrentTrain: epoch  8, batch    91 | loss: 65.5251334CurrentTrain: epoch  8, batch    92 | loss: 64.4950665CurrentTrain: epoch  8, batch    93 | loss: 79.3697031CurrentTrain: epoch  8, batch    94 | loss: 117.7202976CurrentTrain: epoch  8, batch    95 | loss: 51.7598612CurrentTrain: epoch  9, batch     0 | loss: 43.2222768CurrentTrain: epoch  9, batch     1 | loss: 65.8751841CurrentTrain: epoch  9, batch     2 | loss: 52.6104660CurrentTrain: epoch  9, batch     3 | loss: 66.7992682CurrentTrain: epoch  9, batch     4 | loss: 78.7422087CurrentTrain: epoch  9, batch     5 | loss: 117.4709759CurrentTrain: epoch  9, batch     6 | loss: 61.8720642CurrentTrain: epoch  9, batch     7 | loss: 66.8608978CurrentTrain: epoch  9, batch     8 | loss: 81.2041814CurrentTrain: epoch  9, batch     9 | loss: 84.0512210CurrentTrain: epoch  9, batch    10 | loss: 61.2673908CurrentTrain: epoch  9, batch    11 | loss: 84.3303578CurrentTrain: epoch  9, batch    12 | loss: 53.6855972CurrentTrain: epoch  9, batch    13 | loss: 65.0823217CurrentTrain: epoch  9, batch    14 | loss: 52.9975886CurrentTrain: epoch  9, batch    15 | loss: 81.5831908CurrentTrain: epoch  9, batch    16 | loss: 79.9686565CurrentTrain: epoch  9, batch    17 | loss: 117.4900442CurrentTrain: epoch  9, batch    18 | loss: 79.5239459CurrentTrain: epoch  9, batch    19 | loss: 49.1687217CurrentTrain: epoch  9, batch    20 | loss: 117.9862692CurrentTrain: epoch  9, batch    21 | loss: 64.1379167CurrentTrain: epoch  9, batch    22 | loss: 53.2868635CurrentTrain: epoch  9, batch    23 | loss: 44.4524940CurrentTrain: epoch  9, batch    24 | loss: 50.5150722CurrentTrain: epoch  9, batch    25 | loss: 65.5612682CurrentTrain: epoch  9, batch    26 | loss: 64.8568145CurrentTrain: epoch  9, batch    27 | loss: 48.4350281CurrentTrain: epoch  9, batch    28 | loss: 39.3786452CurrentTrain: epoch  9, batch    29 | loss: 86.0893592CurrentTrain: epoch  9, batch    30 | loss: 66.8389639CurrentTrain: epoch  9, batch    31 | loss: 66.7128098CurrentTrain: epoch  9, batch    32 | loss: 62.1397006CurrentTrain: epoch  9, batch    33 | loss: 64.4343182CurrentTrain: epoch  9, batch    34 | loss: 51.3875829CurrentTrain: epoch  9, batch    35 | loss: 51.8662205CurrentTrain: epoch  9, batch    36 | loss: 42.6089416CurrentTrain: epoch  9, batch    37 | loss: 84.1140035CurrentTrain: epoch  9, batch    38 | loss: 65.3465006CurrentTrain: epoch  9, batch    39 | loss: 52.5364545CurrentTrain: epoch  9, batch    40 | loss: 65.4168334CurrentTrain: epoch  9, batch    41 | loss: 62.0071392CurrentTrain: epoch  9, batch    42 | loss: 84.3527809CurrentTrain: epoch  9, batch    43 | loss: 111.0535626CurrentTrain: epoch  9, batch    44 | loss: 65.4145879CurrentTrain: epoch  9, batch    45 | loss: 54.1048453CurrentTrain: epoch  9, batch    46 | loss: 41.9684862CurrentTrain: epoch  9, batch    47 | loss: 62.2400371CurrentTrain: epoch  9, batch    48 | loss: 65.6868732CurrentTrain: epoch  9, batch    49 | loss: 65.0588254CurrentTrain: epoch  9, batch    50 | loss: 60.8847090CurrentTrain: epoch  9, batch    51 | loss: 65.5536813CurrentTrain: epoch  9, batch    52 | loss: 41.4649624CurrentTrain: epoch  9, batch    53 | loss: 52.6551220CurrentTrain: epoch  9, batch    54 | loss: 84.1156186CurrentTrain: epoch  9, batch    55 | loss: 53.5529071CurrentTrain: epoch  9, batch    56 | loss: 83.3055958CurrentTrain: epoch  9, batch    57 | loss: 109.6993478CurrentTrain: epoch  9, batch    58 | loss: 64.8381513CurrentTrain: epoch  9, batch    59 | loss: 53.0294822CurrentTrain: epoch  9, batch    60 | loss: 67.3927018CurrentTrain: epoch  9, batch    61 | loss: 110.1937698CurrentTrain: epoch  9, batch    62 | loss: 83.2503405CurrentTrain: epoch  9, batch    63 | loss: 63.2946887CurrentTrain: epoch  9, batch    64 | loss: 53.1222643CurrentTrain: epoch  9, batch    65 | loss: 52.4019409CurrentTrain: epoch  9, batch    66 | loss: 52.0540396CurrentTrain: epoch  9, batch    67 | loss: 66.3543151CurrentTrain: epoch  9, batch    68 | loss: 79.8240745CurrentTrain: epoch  9, batch    69 | loss: 115.2280655CurrentTrain: epoch  9, batch    70 | loss: 51.1329832CurrentTrain: epoch  9, batch    71 | loss: 50.1087278CurrentTrain: epoch  9, batch    72 | loss: 61.9213899CurrentTrain: epoch  9, batch    73 | loss: 54.3783911CurrentTrain: epoch  9, batch    74 | loss: 85.6927825CurrentTrain: epoch  9, batch    75 | loss: 44.0992687CurrentTrain: epoch  9, batch    76 | loss: 63.2462006CurrentTrain: epoch  9, batch    77 | loss: 117.6517849CurrentTrain: epoch  9, batch    78 | loss: 52.8757567CurrentTrain: epoch  9, batch    79 | loss: 115.1593380CurrentTrain: epoch  9, batch    80 | loss: 51.2892596CurrentTrain: epoch  9, batch    81 | loss: 53.1599661CurrentTrain: epoch  9, batch    82 | loss: 67.1552191CurrentTrain: epoch  9, batch    83 | loss: 52.3088504CurrentTrain: epoch  9, batch    84 | loss: 61.9614424CurrentTrain: epoch  9, batch    85 | loss: 117.7330911CurrentTrain: epoch  9, batch    86 | loss: 62.8378706CurrentTrain: epoch  9, batch    87 | loss: 84.2077970CurrentTrain: epoch  9, batch    88 | loss: 62.0167850CurrentTrain: epoch  9, batch    89 | loss: 84.0266339CurrentTrain: epoch  9, batch    90 | loss: 49.8965165CurrentTrain: epoch  9, batch    91 | loss: 65.3307187CurrentTrain: epoch  9, batch    92 | loss: 48.8232049CurrentTrain: epoch  9, batch    93 | loss: 117.4673417CurrentTrain: epoch  9, batch    94 | loss: 66.7063031CurrentTrain: epoch  9, batch    95 | loss: 71.5600810

F1 score per class: {32: 0.5161290322580645, 6: 0.8764044943820225, 19: 0.3076923076923077, 24: 0.7684210526315789, 26: 0.9130434782608695, 29: 0.8762886597938144}
Micro-average F1 score: 0.785329018338727
Weighted-average F1 score: 0.8005693563053875
F1 score per class: {32: 0.6703296703296703, 6: 0.9010989010989011, 19: 0.5517241379310345, 24: 0.7650273224043715, 26: 0.9583333333333334, 29: 0.89}
Micro-average F1 score: 0.8305785123966942
Weighted-average F1 score: 0.8346445395569194
F1 score per class: {32: 0.6740331491712708, 6: 0.9010989010989011, 19: 0.5517241379310345, 24: 0.7717391304347826, 26: 0.9583333333333334, 29: 0.89}
Micro-average F1 score: 0.8326446280991735
Weighted-average F1 score: 0.8367744758075837

F1 score per class: {32: 0.5161290322580645, 6: 0.8764044943820225, 19: 0.3076923076923077, 24: 0.7684210526315789, 26: 0.9130434782608695, 29: 0.8762886597938144}
Micro-average F1 score: 0.785329018338727
Weighted-average F1 score: 0.8005693563053875
F1 score per class: {32: 0.6703296703296703, 6: 0.9010989010989011, 19: 0.5517241379310345, 24: 0.7650273224043715, 26: 0.9583333333333334, 29: 0.89}
Micro-average F1 score: 0.8305785123966942
Weighted-average F1 score: 0.8346445395569194
F1 score per class: {32: 0.6740331491712708, 6: 0.9010989010989011, 19: 0.5517241379310345, 24: 0.7717391304347826, 26: 0.9583333333333334, 29: 0.89}
Micro-average F1 score: 0.8326446280991735
Weighted-average F1 score: 0.8367744758075837

F1 score per class: {32: 0.418848167539267, 6: 0.6812227074235808, 19: 0.1568627450980392, 24: 0.7263681592039801, 26: 0.8842105263157894, 29: 0.8018867924528302}
Micro-average F1 score: 0.6778398510242085
Weighted-average F1 score: 0.6725230850623931
F1 score per class: {32: 0.4939271255060729, 6: 0.6586345381526104, 19: 0.19047619047619047, 24: 0.717948717948718, 26: 0.8975609756097561, 29: 0.717741935483871}
Micro-average F1 score: 0.6547231270358306
Weighted-average F1 score: 0.636605773663839
F1 score per class: {32: 0.49795918367346936, 6: 0.6586345381526104, 19: 0.19047619047619047, 24: 0.7244897959183674, 26: 0.9019607843137255, 29: 0.7325102880658436}
Micro-average F1 score: 0.6601146601146601
Weighted-average F1 score: 0.641558989415401

F1 score per class: {32: 0.418848167539267, 6: 0.6812227074235808, 19: 0.1568627450980392, 24: 0.7263681592039801, 26: 0.8842105263157894, 29: 0.8018867924528302}
Micro-average F1 score: 0.6778398510242085
Weighted-average F1 score: 0.6725230850623931
F1 score per class: {32: 0.4939271255060729, 6: 0.6586345381526104, 19: 0.19047619047619047, 24: 0.717948717948718, 26: 0.8975609756097561, 29: 0.717741935483871}
Micro-average F1 score: 0.6547231270358306
Weighted-average F1 score: 0.636605773663839
F1 score per class: {32: 0.49795918367346936, 6: 0.6586345381526104, 19: 0.19047619047619047, 24: 0.7244897959183674, 26: 0.9019607843137255, 29: 0.7325102880658436}
Micro-average F1 score: 0.6601146601146601
Weighted-average F1 score: 0.641558989415401
cur_acc_wo_na:  ['0.7853']
his_acc_wo_na:  ['0.7853']
cur_acc des_wo_na:  ['0.8306']
his_acc des_wo_na:  ['0.8306']
cur_acc rrf_wo_na:  ['0.8326']
his_acc rrf_wo_na:  ['0.8326']
cur_acc_w_na:  ['0.6778']
his_acc_w_na:  ['0.6778']
cur_acc des_w_na:  ['0.6547']
his_acc des_w_na:  ['0.6547']
cur_acc rrf_w_na:  ['0.6601']
his_acc rrf_w_na:  ['0.6601']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 68.0558990CurrentTrain: epoch  0, batch     1 | loss: 78.9496920CurrentTrain: epoch  0, batch     2 | loss: 136.3579343CurrentTrain: epoch  0, batch     3 | loss: 10.9879344CurrentTrain: epoch  1, batch     0 | loss: 72.6681376CurrentTrain: epoch  1, batch     1 | loss: 61.6836282CurrentTrain: epoch  1, batch     2 | loss: 73.1541157CurrentTrain: epoch  1, batch     3 | loss: 27.4717150CurrentTrain: epoch  2, batch     0 | loss: 56.4297056CurrentTrain: epoch  2, batch     1 | loss: 88.9389529CurrentTrain: epoch  2, batch     2 | loss: 58.5558608CurrentTrain: epoch  2, batch     3 | loss: 14.9860344CurrentTrain: epoch  3, batch     0 | loss: 69.2233757CurrentTrain: epoch  3, batch     1 | loss: 67.6428053CurrentTrain: epoch  3, batch     2 | loss: 68.4207000CurrentTrain: epoch  3, batch     3 | loss: 7.7766779CurrentTrain: epoch  4, batch     0 | loss: 67.1428103CurrentTrain: epoch  4, batch     1 | loss: 67.4677865CurrentTrain: epoch  4, batch     2 | loss: 55.4507472CurrentTrain: epoch  4, batch     3 | loss: 27.3799072CurrentTrain: epoch  5, batch     0 | loss: 65.5959722CurrentTrain: epoch  5, batch     1 | loss: 68.0876356CurrentTrain: epoch  5, batch     2 | loss: 82.7128225CurrentTrain: epoch  5, batch     3 | loss: 9.6747357CurrentTrain: epoch  6, batch     0 | loss: 55.2678123CurrentTrain: epoch  6, batch     1 | loss: 84.3627179CurrentTrain: epoch  6, batch     2 | loss: 51.6299093CurrentTrain: epoch  6, batch     3 | loss: 10.8855278CurrentTrain: epoch  7, batch     0 | loss: 64.9520996CurrentTrain: epoch  7, batch     1 | loss: 49.9076741CurrentTrain: epoch  7, batch     2 | loss: 53.2028241CurrentTrain: epoch  7, batch     3 | loss: 27.4029196CurrentTrain: epoch  8, batch     0 | loss: 51.0888440CurrentTrain: epoch  8, batch     1 | loss: 54.9137699CurrentTrain: epoch  8, batch     2 | loss: 63.2930513CurrentTrain: epoch  8, batch     3 | loss: 5.5262016CurrentTrain: epoch  9, batch     0 | loss: 53.2706811CurrentTrain: epoch  9, batch     1 | loss: 48.8581656CurrentTrain: epoch  9, batch     2 | loss: 65.4593758CurrentTrain: epoch  9, batch     3 | loss: 11.6801860
MemoryTrain:  epoch  0, batch     0 | loss: 0.7496515MemoryTrain:  epoch  1, batch     0 | loss: 0.6800145MemoryTrain:  epoch  2, batch     0 | loss: 0.4894001MemoryTrain:  epoch  3, batch     0 | loss: 0.3208498MemoryTrain:  epoch  4, batch     0 | loss: 0.3171763MemoryTrain:  epoch  5, batch     0 | loss: 0.2157248MemoryTrain:  epoch  6, batch     0 | loss: 0.1809109MemoryTrain:  epoch  7, batch     0 | loss: 0.1552765MemoryTrain:  epoch  8, batch     0 | loss: 0.1374050MemoryTrain:  epoch  9, batch     0 | loss: 0.1100423

F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.96, 9: 0.0, 19: 0.0, 26: 0.6, 27: 0.6666666666666666, 31: 0.27848101265822783}
Micro-average F1 score: 0.41509433962264153
Weighted-average F1 score: 0.32768178519147256
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6956521739130435, 27: 1.0, 31: 0.5217391304347826}
Micro-average F1 score: 0.5495495495495496
Weighted-average F1 score: 0.4363974253147314
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 26: 0.75, 27: 0.6666666666666666, 31: 0.5217391304347826}
Micro-average F1 score: 0.5520361990950227
Weighted-average F1 score: 0.4401034286124039

F1 score per class: {32: 0.423841059602649, 6: 0.08, 7: 0.96, 40: 0.6607142857142857, 9: 0.2727272727272727, 19: 0.7597765363128491, 24: 0.4444444444444444, 26: 0.9247311827956989, 27: 0.6666666666666666, 29: 0.8691099476439791, 31: 0.25}
Micro-average F1 score: 0.6660973526900086
Weighted-average F1 score: 0.6712476749032501
F1 score per class: {32: 0.4473684210526316, 6: 0.08695652173913043, 7: 0.9803921568627451, 40: 0.6635071090047393, 9: 0.3870967741935484, 19: 0.7675675675675676, 24: 0.5517241379310345, 26: 0.9479166666666666, 27: 0.8, 29: 0.8645833333333334, 31: 0.41739130434782606}
Micro-average F1 score: 0.6881720430107527
Weighted-average F1 score: 0.6808629932862038
F1 score per class: {32: 0.4444444444444444, 6: 0.08695652173913043, 7: 0.9803921568627451, 40: 0.6698113207547169, 9: 0.3076923076923077, 19: 0.7675675675675676, 24: 0.5294117647058824, 26: 0.9479166666666666, 27: 0.6666666666666666, 29: 0.8586387434554974, 31: 0.42105263157894735}
Micro-average F1 score: 0.6859983429991715
Weighted-average F1 score: 0.6797416004585112

F1 score per class: {32: 0.0, 6: 0.5714285714285714, 7: 0.9056603773584906, 40: 0.0, 9: 0.0, 19: 0.5, 26: 0.6666666666666666, 27: 0.0, 31: 0.22448979591836735}
Micro-average F1 score: 0.3283582089552239
Weighted-average F1 score: 0.2499808303620394
F1 score per class: {32: 0.0, 6: 0.5, 7: 0.7692307692307693, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.5161290322580645, 26: 0.17391304347826086, 27: 0.0, 31: 0.25806451612903225}
Micro-average F1 score: 0.3096446700507614
Weighted-average F1 score: 0.26747896213183736
F1 score per class: {32: 0.0, 6: 0.5, 7: 0.847457627118644, 40: 0.0, 9: 0.0, 19: 0.5294117647058824, 26: 0.18181818181818182, 27: 0.0, 31: 0.25263157894736843}
Micro-average F1 score: 0.3288409703504043
Weighted-average F1 score: 0.2842068677886435

F1 score per class: {32: 0.2895927601809955, 6: 0.06060606060606061, 7: 0.9056603773584906, 40: 0.5103448275862069, 9: 0.2, 19: 0.7010309278350515, 24: 0.2222222222222222, 26: 0.8911917098445595, 27: 0.3333333333333333, 29: 0.7980769230769231, 31: 0.18333333333333332}
Micro-average F1 score: 0.5435540069686411
Weighted-average F1 score: 0.5199300906737896
F1 score per class: {32: 0.3177570093457944, 6: 0.058823529411764705, 7: 0.7692307692307693, 40: 0.5343511450381679, 9: 0.17391304347826086, 19: 0.7064676616915423, 24: 0.16326530612244897, 26: 0.883495145631068, 27: 0.06557377049180328, 29: 0.6831275720164609, 31: 0.142433234421365}
Micro-average F1 score: 0.45614035087719296
Weighted-average F1 score: 0.40613803501409473
F1 score per class: {32: 0.3148148148148148, 6: 0.05970149253731343, 7: 0.847457627118644, 40: 0.5399239543726235, 9: 0.20512820512820512, 19: 0.7064676616915423, 24: 0.16666666666666666, 26: 0.883495145631068, 27: 0.06060606060606061, 29: 0.7420814479638009, 31: 0.1437125748502994}
Micro-average F1 score: 0.47395535203205497
Weighted-average F1 score: 0.4226337670088223
cur_acc_wo_na:  ['0.7853', '0.4151']
his_acc_wo_na:  ['0.7853', '0.6661']
cur_acc des_wo_na:  ['0.8306', '0.5495']
his_acc des_wo_na:  ['0.8306', '0.6882']
cur_acc rrf_wo_na:  ['0.8326', '0.5520']
his_acc rrf_wo_na:  ['0.8326', '0.6860']
cur_acc_w_na:  ['0.6778', '0.3284']
his_acc_w_na:  ['0.6778', '0.5436']
cur_acc des_w_na:  ['0.6547', '0.3096']
his_acc des_w_na:  ['0.6547', '0.4561']
cur_acc rrf_w_na:  ['0.6601', '0.3288']
his_acc rrf_w_na:  ['0.6601', '0.4740']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 80.9089620CurrentTrain: epoch  0, batch     1 | loss: 84.6098371CurrentTrain: epoch  0, batch     2 | loss: 65.6024167CurrentTrain: epoch  0, batch     3 | loss: 62.5562373CurrentTrain: epoch  1, batch     0 | loss: 96.4435927CurrentTrain: epoch  1, batch     1 | loss: 78.5285126CurrentTrain: epoch  1, batch     2 | loss: 60.8464594CurrentTrain: epoch  1, batch     3 | loss: 72.4127726CurrentTrain: epoch  2, batch     0 | loss: 94.7049204CurrentTrain: epoch  2, batch     1 | loss: 59.6663506CurrentTrain: epoch  2, batch     2 | loss: 88.8174749CurrentTrain: epoch  2, batch     3 | loss: 48.9641704CurrentTrain: epoch  3, batch     0 | loss: 62.7991976CurrentTrain: epoch  3, batch     1 | loss: 88.4936290CurrentTrain: epoch  3, batch     2 | loss: 66.9735976CurrentTrain: epoch  3, batch     3 | loss: 57.7811046CurrentTrain: epoch  4, batch     0 | loss: 119.5143003CurrentTrain: epoch  4, batch     1 | loss: 54.7946042CurrentTrain: epoch  4, batch     2 | loss: 90.5596878CurrentTrain: epoch  4, batch     3 | loss: 73.9473897CurrentTrain: epoch  5, batch     0 | loss: 68.3061298CurrentTrain: epoch  5, batch     1 | loss: 89.0491453CurrentTrain: epoch  5, batch     2 | loss: 55.2800671CurrentTrain: epoch  5, batch     3 | loss: 74.1407645CurrentTrain: epoch  6, batch     0 | loss: 86.6960889CurrentTrain: epoch  6, batch     1 | loss: 66.9825563CurrentTrain: epoch  6, batch     2 | loss: 55.9560900CurrentTrain: epoch  6, batch     3 | loss: 47.3546094CurrentTrain: epoch  7, batch     0 | loss: 67.2590381CurrentTrain: epoch  7, batch     1 | loss: 55.9572656CurrentTrain: epoch  7, batch     2 | loss: 67.2745721CurrentTrain: epoch  7, batch     3 | loss: 70.5823195CurrentTrain: epoch  8, batch     0 | loss: 85.2243571CurrentTrain: epoch  8, batch     1 | loss: 65.1277698CurrentTrain: epoch  8, batch     2 | loss: 70.5672375CurrentTrain: epoch  8, batch     3 | loss: 43.7652451CurrentTrain: epoch  9, batch     0 | loss: 66.2965960CurrentTrain: epoch  9, batch     1 | loss: 68.0945660CurrentTrain: epoch  9, batch     2 | loss: 64.7034945CurrentTrain: epoch  9, batch     3 | loss: 55.4971024
MemoryTrain:  epoch  0, batch     0 | loss: 0.8041721MemoryTrain:  epoch  1, batch     0 | loss: 0.6037653MemoryTrain:  epoch  2, batch     0 | loss: 0.3734736MemoryTrain:  epoch  3, batch     0 | loss: 0.3067515MemoryTrain:  epoch  4, batch     0 | loss: 0.2170902MemoryTrain:  epoch  5, batch     0 | loss: 0.1612655MemoryTrain:  epoch  6, batch     0 | loss: 0.1188757MemoryTrain:  epoch  7, batch     0 | loss: 0.0998991MemoryTrain:  epoch  8, batch     0 | loss: 0.0825813MemoryTrain:  epoch  9, batch     0 | loss: 0.0640135

F1 score per class: {0: 0.9428571428571428, 32: 0.9583333333333334, 4: 0.3333333333333333, 13: 0.7083333333333334, 21: 0.8536585365853658, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8811881188118812
Weighted-average F1 score: 0.8779021802252184
F1 score per class: {0: 0.9117647058823529, 32: 1.0, 4: 0.5714285714285714, 13: 0.8928571428571429, 21: 0.9195402298850575, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.9187935034802784
Weighted-average F1 score: 0.8934151696305259
F1 score per class: {0: 0.927536231884058, 32: 1.0, 4: 0.5714285714285714, 13: 0.9122807017543859, 21: 0.8941176470588236, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.9230769230769231
Weighted-average F1 score: 0.9020085425693604

F1 score per class: {32: 0.9428571428571428, 0: 0.9583333333333334, 4: 0.49333333333333335, 6: 0.09090909090909091, 7: 0.96, 40: 0.16666666666666666, 9: 0.6759259259259259, 13: 0.6071428571428571, 19: 0.8235294117647058, 21: 0.10526315789473684, 23: 0.75, 24: 0.4666666666666667, 26: 0.9533678756476683, 27: 0.6666666666666666, 29: 0.8449197860962567, 31: 0.36363636363636365}
Micro-average F1 score: 0.7305993690851735
Weighted-average F1 score: 0.7336003560695872
F1 score per class: {32: 0.8985507246376812, 0: 0.9950248756218906, 4: 0.5161290322580645, 6: 0.10344827586206896, 7: 0.9803921568627451, 40: 0.3333333333333333, 9: 0.6944444444444444, 13: 0.746268656716418, 19: 0.9090909090909091, 21: 0.09523809523809523, 23: 0.7540983606557377, 24: 0.48484848484848486, 26: 0.9696969696969697, 27: 0.4444444444444444, 29: 0.8934010152284264, 31: 0.4144144144144144}
Micro-average F1 score: 0.7525464349910126
Weighted-average F1 score: 0.7464529536127017
F1 score per class: {32: 0.9014084507042254, 0: 0.9950248756218906, 4: 0.5283018867924528, 6: 0.07547169811320754, 7: 0.9803921568627451, 40: 0.3333333333333333, 9: 0.6976744186046512, 13: 0.7536231884057971, 19: 0.8837209302325582, 21: 0.1, 23: 0.7540983606557377, 24: 0.5, 26: 0.9696969696969697, 27: 0.5, 29: 0.898989898989899, 31: 0.42201834862385323}
Micro-average F1 score: 0.7567567567567568
Weighted-average F1 score: 0.7525714326988776

F1 score per class: {0: 0.7764705882352941, 32: 0.9583333333333334, 4: 0.0, 6: 0.0, 7: 0.0273972602739726, 40: 0.0, 13: 0.37777777777777777, 19: 0.7865168539325843, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5650793650793651
Weighted-average F1 score: 0.4454216228162155
F1 score per class: {0: 0.5535714285714286, 32: 0.9090909090909091, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.04395604395604396, 9: 0.0, 13: 0.4, 19: 0.7017543859649122, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.4351648351648352
Weighted-average F1 score: 0.3463862751791813
F1 score per class: {0: 0.5470085470085471, 32: 0.9569377990430622, 4: 0.0, 6: 0.0, 7: 0.041666666666666664, 40: 0.0, 13: 0.40625, 19: 0.6846846846846847, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.4664310954063604
Weighted-average F1 score: 0.37374346636385375

F1 score per class: {32: 0.4342105263157895, 0: 0.9533678756476683, 4: 0.33183856502242154, 6: 0.06153846153846154, 7: 0.8888888888888888, 40: 0.014285714285714285, 9: 0.5195729537366548, 13: 0.2073170731707317, 19: 0.7368421052631579, 21: 0.09090909090909091, 23: 0.6984126984126984, 24: 0.15217391304347827, 26: 0.863849765258216, 27: 0.09302325581395349, 29: 0.7247706422018348, 31: 0.19889502762430938}
Micro-average F1 score: 0.49806451612903224
Weighted-average F1 score: 0.4418729038672528
F1 score per class: {32: 0.29245283018867924, 0: 0.8658008658008658, 4: 0.22727272727272727, 6: 0.0625, 7: 0.7142857142857143, 40: 0.023809523809523808, 9: 0.49504950495049505, 13: 0.16181229773462782, 19: 0.625, 21: 0.07142857142857142, 23: 0.6934673366834171, 24: 0.12121212121212122, 26: 0.8275862068965517, 27: 0.03571428571428571, 29: 0.546583850931677, 31: 0.1031390134529148}
Micro-average F1 score: 0.37604790419161677
Weighted-average F1 score: 0.3280798574202159
F1 score per class: {32: 0.270042194092827, 0: 0.9345794392523364, 4: 0.2709677419354839, 6: 0.046511627906976744, 7: 0.8333333333333334, 40: 0.021739130434782608, 9: 0.5067567567567568, 13: 0.1625, 19: 0.608, 21: 0.08, 23: 0.6934673366834171, 24: 0.12403100775193798, 26: 0.8347826086956521, 27: 0.057971014492753624, 29: 0.6054421768707483, 31: 0.1377245508982036}
Micro-average F1 score: 0.40488431876606684
Weighted-average F1 score: 0.3527671671459787
cur_acc_wo_na:  ['0.7853', '0.4151', '0.8812']
his_acc_wo_na:  ['0.7853', '0.6661', '0.7306']
cur_acc des_wo_na:  ['0.8306', '0.5495', '0.9188']
his_acc des_wo_na:  ['0.8306', '0.6882', '0.7525']
cur_acc rrf_wo_na:  ['0.8326', '0.5520', '0.9231']
his_acc rrf_wo_na:  ['0.8326', '0.6860', '0.7568']
cur_acc_w_na:  ['0.6778', '0.3284', '0.5651']
his_acc_w_na:  ['0.6778', '0.5436', '0.4981']
cur_acc des_w_na:  ['0.6547', '0.3096', '0.4352']
his_acc des_w_na:  ['0.6547', '0.4561', '0.3760']
cur_acc rrf_w_na:  ['0.6601', '0.3288', '0.4664']
his_acc rrf_w_na:  ['0.6601', '0.4740', '0.4049']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 99.8812609CurrentTrain: epoch  0, batch     1 | loss: 94.7084823CurrentTrain: epoch  0, batch     2 | loss: 124.5041898CurrentTrain: epoch  0, batch     3 | loss: 126.2675957CurrentTrain: epoch  0, batch     4 | loss: 46.6259701CurrentTrain: epoch  1, batch     0 | loss: 72.6810173CurrentTrain: epoch  1, batch     1 | loss: 76.3240182CurrentTrain: epoch  1, batch     2 | loss: 122.9894177CurrentTrain: epoch  1, batch     3 | loss: 118.5047843CurrentTrain: epoch  1, batch     4 | loss: 57.1318031CurrentTrain: epoch  2, batch     0 | loss: 70.3912789CurrentTrain: epoch  2, batch     1 | loss: 89.8275657CurrentTrain: epoch  2, batch     2 | loss: 87.8065860CurrentTrain: epoch  2, batch     3 | loss: 85.5645894CurrentTrain: epoch  2, batch     4 | loss: 56.3617717CurrentTrain: epoch  3, batch     0 | loss: 59.4206832CurrentTrain: epoch  3, batch     1 | loss: 70.3154107CurrentTrain: epoch  3, batch     2 | loss: 85.9019795CurrentTrain: epoch  3, batch     3 | loss: 68.4572972CurrentTrain: epoch  3, batch     4 | loss: 76.3119833CurrentTrain: epoch  4, batch     0 | loss: 69.4726883CurrentTrain: epoch  4, batch     1 | loss: 64.7843264CurrentTrain: epoch  4, batch     2 | loss: 65.2330441CurrentTrain: epoch  4, batch     3 | loss: 118.4384836CurrentTrain: epoch  4, batch     4 | loss: 119.8160297CurrentTrain: epoch  5, batch     0 | loss: 85.4219102CurrentTrain: epoch  5, batch     1 | loss: 67.6021163CurrentTrain: epoch  5, batch     2 | loss: 53.2284312CurrentTrain: epoch  5, batch     3 | loss: 86.2569817CurrentTrain: epoch  5, batch     4 | loss: 114.0318214CurrentTrain: epoch  6, batch     0 | loss: 65.4405729CurrentTrain: epoch  6, batch     1 | loss: 83.3291126CurrentTrain: epoch  6, batch     2 | loss: 53.0817098CurrentTrain: epoch  6, batch     3 | loss: 181.5090266CurrentTrain: epoch  6, batch     4 | loss: 42.1679697CurrentTrain: epoch  7, batch     0 | loss: 64.9420664CurrentTrain: epoch  7, batch     1 | loss: 86.4548297CurrentTrain: epoch  7, batch     2 | loss: 64.6043845CurrentTrain: epoch  7, batch     3 | loss: 84.3536669CurrentTrain: epoch  7, batch     4 | loss: 52.4528959CurrentTrain: epoch  8, batch     0 | loss: 61.3822998CurrentTrain: epoch  8, batch     1 | loss: 115.7997699CurrentTrain: epoch  8, batch     2 | loss: 81.5786450CurrentTrain: epoch  8, batch     3 | loss: 83.9892680CurrentTrain: epoch  8, batch     4 | loss: 74.7859823CurrentTrain: epoch  9, batch     0 | loss: 65.7804750CurrentTrain: epoch  9, batch     1 | loss: 66.5452866CurrentTrain: epoch  9, batch     2 | loss: 82.7656662CurrentTrain: epoch  9, batch     3 | loss: 66.3048200CurrentTrain: epoch  9, batch     4 | loss: 40.3208035
MemoryTrain:  epoch  0, batch     0 | loss: 0.3764755MemoryTrain:  epoch  1, batch     0 | loss: 0.3221983MemoryTrain:  epoch  2, batch     0 | loss: 0.2500914MemoryTrain:  epoch  3, batch     0 | loss: 0.1779242MemoryTrain:  epoch  4, batch     0 | loss: 0.1326049MemoryTrain:  epoch  5, batch     0 | loss: 0.1035116MemoryTrain:  epoch  6, batch     0 | loss: 0.0715498MemoryTrain:  epoch  7, batch     0 | loss: 0.0632600MemoryTrain:  epoch  8, batch     0 | loss: 0.0598054MemoryTrain:  epoch  9, batch     0 | loss: 0.0553148

F1 score per class: {5: 0.9690721649484536, 6: 0.0, 7: 0.0, 10: 0.4094488188976378, 13: 0.0, 16: 0.8, 17: 0.5, 18: 0.6037735849056604}
Micro-average F1 score: 0.7004405286343612
Weighted-average F1 score: 0.7264510054712441
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 10: 0.7375, 13: 0.0, 16: 0.9473684210526315, 17: 0.9473684210526315, 18: 0.8787878787878788}
Micro-average F1 score: 0.8660194174757282
Weighted-average F1 score: 0.8505857432075727
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 7: 0.0, 10: 0.7375, 13: 0.0, 16: 0.9473684210526315, 17: 0.8, 18: 0.8615384615384616}
Micro-average F1 score: 0.8521400778210116
Weighted-average F1 score: 0.8288005750181178

F1 score per class: {0: 0.9295774647887324, 4: 0.9361702127659575, 5: 0.9690721649484536, 6: 0.4305555555555556, 7: 0.0, 9: 0.9803921568627451, 10: 0.40625, 13: 0.10526315789473684, 16: 0.8, 17: 0.46153846153846156, 18: 0.6037735849056604, 19: 0.6634615384615384, 21: 0.4444444444444444, 23: 0.8333333333333334, 24: 0.10526315789473684, 26: 0.7457627118644068, 27: 0.45161290322580644, 29: 0.9424083769633508, 31: 0.6666666666666666, 32: 0.8351648351648352, 40: 0.3191489361702128}
Micro-average F1 score: 0.7137809187279152
Weighted-average F1 score: 0.7344261471814225
F1 score per class: {0: 0.9428571428571428, 4: 0.98989898989899, 5: 0.9949748743718593, 6: 0.3582089552238806, 7: 0.0, 9: 0.9803921568627451, 10: 0.7151515151515152, 13: 0.21052631578947367, 16: 0.9152542372881356, 17: 0.29508196721311475, 18: 0.8656716417910447, 19: 0.6796116504854369, 21: 0.696969696969697, 23: 0.8863636363636364, 24: 0.18181818181818182, 26: 0.7472527472527473, 27: 0.5128205128205128, 29: 0.9591836734693877, 31: 0.6666666666666666, 32: 0.8645833333333334, 40: 0.4576271186440678}
Micro-average F1 score: 0.7602771362586606
Weighted-average F1 score: 0.7578448849688387
F1 score per class: {0: 0.9577464788732394, 4: 0.98989898989899, 5: 0.99, 6: 0.44, 7: 0.058823529411764705, 9: 0.9803921568627451, 10: 0.7108433734939759, 13: 0.21052631578947367, 16: 0.9152542372881356, 17: 0.3333333333333333, 18: 0.8484848484848485, 19: 0.6731707317073171, 21: 0.71875, 23: 0.8863636363636364, 24: 0.18181818181818182, 26: 0.7472527472527473, 27: 0.5, 29: 0.9538461538461539, 31: 0.8, 32: 0.8586387434554974, 40: 0.453781512605042}
Micro-average F1 score: 0.7653863951874132
Weighted-average F1 score: 0.7621873992838721

F1 score per class: {0: 0.0, 4: 0.0, 5: 0.9215686274509803, 6: 0.0, 7: 0.0, 40: 0.3443708609271523, 10: 0.0, 13: 0.5063291139240507, 16: 0.5, 17: 0.2782608695652174, 18: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.496875
Weighted-average F1 score: 0.4434629823366818
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.5625, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.5086206896551724, 13: 0.0, 16: 0.5242718446601942, 17: 0.45, 18: 0.18770226537216828, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.36829066886870354
Weighted-average F1 score: 0.33386608756781466
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.5641025641025641, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.5042735042735043, 13: 0.0, 16: 0.5192307692307693, 17: 0.5714285714285714, 18: 0.1848184818481848, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.37181663837011886
Weighted-average F1 score: 0.33659016531511105

F1 score per class: {0: 0.5038167938931297, 4: 0.9263157894736842, 5: 0.9082125603864735, 6: 0.2897196261682243, 7: 0.0, 9: 0.8928571428571429, 10: 0.23963133640552994, 13: 0.05, 16: 0.42105263157894735, 17: 0.3157894736842105, 18: 0.24427480916030533, 19: 0.5149253731343284, 21: 0.19607843137254902, 23: 0.7608695652173914, 24: 0.09090909090909091, 26: 0.6875, 27: 0.14285714285714285, 29: 0.8108108108108109, 31: 0.125, 32: 0.7524752475247525, 40: 0.189873417721519}
Micro-average F1 score: 0.5181385122755588
Weighted-average F1 score: 0.48465294599343806
F1 score per class: {0: 0.3793103448275862, 4: 0.9245283018867925, 5: 0.4925373134328358, 6: 0.2171945701357466, 7: 0.0, 9: 0.6756756756756757, 10: 0.30809399477806787, 13: 0.0851063829787234, 16: 0.39705882352941174, 17: 0.13333333333333333, 18: 0.13647058823529412, 19: 0.45454545454545453, 21: 0.1625441696113074, 23: 0.639344262295082, 24: 0.10256410256410256, 26: 0.6732673267326733, 27: 0.13245033112582782, 29: 0.734375, 31: 0.05333333333333334, 32: 0.5551839464882943, 40: 0.13466334164588528}
Micro-average F1 score: 0.37485766340241405
Weighted-average F1 score: 0.3407658041229383
F1 score per class: {0: 0.33663366336633666, 4: 0.9607843137254902, 5: 0.495, 6: 0.2509505703422053, 7: 0.03636363636363636, 9: 0.7246376811594203, 10: 0.30256410256410254, 13: 0.08, 16: 0.39705882352941174, 17: 0.15789473684210525, 18: 0.13526570048309178, 19: 0.46938775510204084, 21: 0.15181518151815182, 23: 0.6341463414634146, 24: 0.11764705882352941, 26: 0.6766169154228856, 27: 0.13986013986013987, 29: 0.768595041322314, 31: 0.09302325581395349, 32: 0.6074074074074074, 40: 0.16216216216216217}
Micro-average F1 score: 0.38963486454652535
Weighted-average F1 score: 0.3532222799576511
cur_acc_wo_na:  ['0.7853', '0.4151', '0.8812', '0.7004']
his_acc_wo_na:  ['0.7853', '0.6661', '0.7306', '0.7138']
cur_acc des_wo_na:  ['0.8306', '0.5495', '0.9188', '0.8660']
his_acc des_wo_na:  ['0.8306', '0.6882', '0.7525', '0.7603']
cur_acc rrf_wo_na:  ['0.8326', '0.5520', '0.9231', '0.8521']
his_acc rrf_wo_na:  ['0.8326', '0.6860', '0.7568', '0.7654']
cur_acc_w_na:  ['0.6778', '0.3284', '0.5651', '0.4969']
his_acc_w_na:  ['0.6778', '0.5436', '0.4981', '0.5181']
cur_acc des_w_na:  ['0.6547', '0.3096', '0.4352', '0.3683']
his_acc des_w_na:  ['0.6547', '0.4561', '0.3760', '0.3749']
cur_acc rrf_w_na:  ['0.6601', '0.3288', '0.4664', '0.3718']
his_acc rrf_w_na:  ['0.6601', '0.4740', '0.4049', '0.3896']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 93.0947928CurrentTrain: epoch  0, batch     1 | loss: 67.4336533CurrentTrain: epoch  0, batch     2 | loss: 74.7752246CurrentTrain: epoch  0, batch     3 | loss: 65.1165785CurrentTrain: epoch  1, batch     0 | loss: 70.1672409CurrentTrain: epoch  1, batch     1 | loss: 59.3372718CurrentTrain: epoch  1, batch     2 | loss: 71.4326296CurrentTrain: epoch  1, batch     3 | loss: 122.0819538CurrentTrain: epoch  2, batch     0 | loss: 59.2649407CurrentTrain: epoch  2, batch     1 | loss: 70.1308921CurrentTrain: epoch  2, batch     2 | loss: 65.9704439CurrentTrain: epoch  2, batch     3 | loss: 82.4581612CurrentTrain: epoch  3, batch     0 | loss: 65.8158741CurrentTrain: epoch  3, batch     1 | loss: 68.9314719CurrentTrain: epoch  3, batch     2 | loss: 54.2924878CurrentTrain: epoch  3, batch     3 | loss: 46.7554556CurrentTrain: epoch  4, batch     0 | loss: 117.9602511CurrentTrain: epoch  4, batch     1 | loss: 52.5868566CurrentTrain: epoch  4, batch     2 | loss: 65.6736707CurrentTrain: epoch  4, batch     3 | loss: 36.2108103CurrentTrain: epoch  5, batch     0 | loss: 87.6882707CurrentTrain: epoch  5, batch     1 | loss: 51.6705702CurrentTrain: epoch  5, batch     2 | loss: 81.3350481CurrentTrain: epoch  5, batch     3 | loss: 43.4939716CurrentTrain: epoch  6, batch     0 | loss: 66.6955380CurrentTrain: epoch  6, batch     1 | loss: 53.0168327CurrentTrain: epoch  6, batch     2 | loss: 63.9900678CurrentTrain: epoch  6, batch     3 | loss: 75.8494123CurrentTrain: epoch  7, batch     0 | loss: 86.3888853CurrentTrain: epoch  7, batch     1 | loss: 51.6738426CurrentTrain: epoch  7, batch     2 | loss: 65.8566450CurrentTrain: epoch  7, batch     3 | loss: 41.7183163CurrentTrain: epoch  8, batch     0 | loss: 84.9282122CurrentTrain: epoch  8, batch     1 | loss: 63.1742342CurrentTrain: epoch  8, batch     2 | loss: 51.6571384CurrentTrain: epoch  8, batch     3 | loss: 56.2670487CurrentTrain: epoch  9, batch     0 | loss: 63.2124380CurrentTrain: epoch  9, batch     1 | loss: 64.9726593CurrentTrain: epoch  9, batch     2 | loss: 53.2519777CurrentTrain: epoch  9, batch     3 | loss: 35.3749361
MemoryTrain:  epoch  0, batch     0 | loss: 0.2963518MemoryTrain:  epoch  1, batch     0 | loss: 0.2302557MemoryTrain:  epoch  2, batch     0 | loss: 0.1683835MemoryTrain:  epoch  3, batch     0 | loss: 0.1430710MemoryTrain:  epoch  4, batch     0 | loss: 0.1101849MemoryTrain:  epoch  5, batch     0 | loss: 0.0814915MemoryTrain:  epoch  6, batch     0 | loss: 0.0679315MemoryTrain:  epoch  7, batch     0 | loss: 0.0597202MemoryTrain:  epoch  8, batch     0 | loss: 0.0518167MemoryTrain:  epoch  9, batch     0 | loss: 0.0458447

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.9473684210526315, 38: 0.0, 10: 0.0, 13: 0.4927536231884058, 15: 0.0, 18: 0.0, 23: 0.4788732394366197, 25: 0.46153846153846156, 27: 0.4}
Micro-average F1 score: 0.4503311258278146
Weighted-average F1 score: 0.3721612324762504
F1 score per class: {35: 0.0, 5: 0.0, 37: 0.0, 38: 0.8888888888888888, 10: 0.0, 13: 0.0, 15: 0.0, 18: 0.5945945945945946, 21: 0.0, 23: 0.9333333333333333, 25: 0.5882352941176471, 27: 0.6818181818181818}
Micro-average F1 score: 0.6611111111111111
Weighted-average F1 score: 0.6050765122907188
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 38: 0.8888888888888888, 37: 0.0, 10: 0.0, 13: 0.0, 15: 0.5945945945945946, 18: 0.0, 21: 0.0, 23: 0.8297872340425532, 25: 0.5882352941176471, 27: 0.6666666666666666}
Micro-average F1 score: 0.6123595505617978
Weighted-average F1 score: 0.5284661867244935

F1 score per class: {0: 0.9722222222222222, 4: 0.9130434782608695, 5: 0.8715596330275229, 6: 0.2689075630252101, 7: 0.0, 9: 0.9803921568627451, 10: 0.45454545454545453, 13: 0.06896551724137931, 15: 0.782608695652174, 16: 0.8727272727272727, 17: 0.18181818181818182, 18: 0.3829787234042553, 19: 0.3404255319148936, 21: 0.3902439024390244, 23: 0.8470588235294118, 24: 0.10526315789473684, 25: 0.4927536231884058, 26: 0.7386363636363636, 27: 0.4827586206896552, 29: 0.9368421052631579, 31: 0.6666666666666666, 32: 0.7570621468926554, 35: 0.4657534246575342, 37: 0.4186046511627907, 38: 0.34146341463414637, 40: 0.1951219512195122}
Micro-average F1 score: 0.6326809285389167
Weighted-average F1 score: 0.6814984177006774
F1 score per class: {0: 0.9295774647887324, 4: 0.9690721649484536, 5: 0.88, 6: 0.4722222222222222, 7: 0.11764705882352941, 9: 0.9803921568627451, 10: 0.6447368421052632, 13: 0.15384615384615385, 15: 0.64, 16: 0.8813559322033898, 17: 0.3448275862068966, 18: 0.46153846153846156, 19: 0.425531914893617, 21: 0.6571428571428571, 23: 0.8372093023255814, 24: 0.17391304347826086, 25: 0.5945945945945946, 26: 0.7513812154696132, 27: 0.48484848484848486, 29: 0.9430051813471503, 31: 0.6666666666666666, 32: 0.8426395939086294, 35: 0.8448275862068966, 37: 0.47619047619047616, 38: 0.5, 40: 0.5714285714285714}
Micro-average F1 score: 0.7089314194577353
Weighted-average F1 score: 0.7166728137480153
F1 score per class: {0: 0.9315068493150684, 4: 0.9690721649484536, 5: 0.88, 6: 0.4722222222222222, 7: 0.058823529411764705, 9: 0.9803921568627451, 10: 0.6357615894039735, 13: 0.14814814814814814, 15: 0.6153846153846154, 16: 0.8813559322033898, 17: 0.3448275862068966, 18: 0.49056603773584906, 19: 0.4520547945205479, 21: 0.6571428571428571, 23: 0.8372093023255814, 24: 0.19047619047619047, 25: 0.5945945945945946, 26: 0.7513812154696132, 27: 0.5294117647058824, 29: 0.9430051813471503, 31: 0.8, 32: 0.8374384236453202, 35: 0.7959183673469388, 37: 0.4807692307692308, 38: 0.4838709677419355, 40: 0.5771812080536913}
Micro-average F1 score: 0.7078651685393258
Weighted-average F1 score: 0.7139139366007535

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.8571428571428571, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4857142857142857, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.33663366336633666, 37: 0.4186046511627907, 38: 0.3333333333333333, 40: 0.0}
Micro-average F1 score: 0.30839002267573695
Weighted-average F1 score: 0.2106395805693526
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.7619047619047619, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5714285714285714, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5268817204301075, 37: 0.44642857142857145, 38: 0.46875, 40: 0.0}
Micro-average F1 score: 0.31860776439089694
Weighted-average F1 score: 0.24521059457695402
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.7272727272727273, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.5866666666666667, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.4936708860759494, 37: 0.44642857142857145, 38: 0.4411764705882353, 40: 0.0}
Micro-average F1 score: 0.30575035063113604
Weighted-average F1 score: 0.2263764363448199

F1 score per class: {0: 0.5, 4: 0.9032258064516129, 5: 0.7630522088353414, 6: 0.1893491124260355, 7: 0.0, 9: 0.8771929824561403, 10: 0.2631578947368421, 13: 0.03076923076923077, 15: 0.46153846153846156, 16: 0.39344262295081966, 17: 0.11764705882352941, 18: 0.13846153846153847, 19: 0.2857142857142857, 21: 0.2222222222222222, 23: 0.782608695652174, 24: 0.08, 25: 0.4788732394366197, 26: 0.6770833333333334, 27: 0.14736842105263157, 29: 0.7672413793103449, 31: 0.1111111111111111, 32: 0.638095238095238, 35: 0.18085106382978725, 37: 0.25, 38: 0.2153846153846154, 40: 0.14285714285714285}
Micro-average F1 score: 0.44071020925808496
Weighted-average F1 score: 0.41924781902688607
F1 score per class: {0: 0.3402061855670103, 4: 0.9306930693069307, 5: 0.4714285714285714, 6: 0.27755102040816326, 7: 0.06557377049180328, 9: 0.6493506493506493, 10: 0.2784090909090909, 13: 0.0547945205479452, 15: 0.36363636363636365, 16: 0.2653061224489796, 17: 0.16666666666666666, 18: 0.09795918367346938, 19: 0.2830188679245283, 21: 0.13813813813813813, 23: 0.6101694915254238, 24: 0.1, 25: 0.5641025641025641, 26: 0.6570048309178744, 27: 0.1391304347826087, 29: 0.6791044776119403, 31: 0.08, 32: 0.578397212543554, 35: 0.17594254937163376, 37: 0.15723270440251572, 38: 0.16574585635359115, 40: 0.24533333333333332}
Micro-average F1 score: 0.33496608892238133
Weighted-average F1 score: 0.3040597640836221
F1 score per class: {0: 0.34, 4: 0.94, 5: 0.47255369928400953, 6: 0.27530364372469635, 7: 0.03333333333333333, 9: 0.6666666666666666, 10: 0.28402366863905326, 13: 0.05333333333333334, 15: 0.3333333333333333, 16: 0.28415300546448086, 17: 0.16666666666666666, 18: 0.10526315789473684, 19: 0.3113207547169811, 21: 0.13411078717201166, 23: 0.6428571428571429, 24: 0.13333333333333333, 25: 0.5789473684210527, 26: 0.6601941747572816, 27: 0.14754098360655737, 29: 0.6920152091254753, 31: 0.09523809523809523, 32: 0.5647840531561462, 35: 0.18932038834951456, 37: 0.1597444089456869, 38: 0.14634146341463414, 40: 0.26791277258566976}
Micro-average F1 score: 0.3452054794520548
Weighted-average F1 score: 0.31363132148842104
cur_acc_wo_na:  ['0.7853', '0.4151', '0.8812', '0.7004', '0.4503']
his_acc_wo_na:  ['0.7853', '0.6661', '0.7306', '0.7138', '0.6327']
cur_acc des_wo_na:  ['0.8306', '0.5495', '0.9188', '0.8660', '0.6611']
his_acc des_wo_na:  ['0.8306', '0.6882', '0.7525', '0.7603', '0.7089']
cur_acc rrf_wo_na:  ['0.8326', '0.5520', '0.9231', '0.8521', '0.6124']
his_acc rrf_wo_na:  ['0.8326', '0.6860', '0.7568', '0.7654', '0.7079']
cur_acc_w_na:  ['0.6778', '0.3284', '0.5651', '0.4969', '0.3084']
his_acc_w_na:  ['0.6778', '0.5436', '0.4981', '0.5181', '0.4407']
cur_acc des_w_na:  ['0.6547', '0.3096', '0.4352', '0.3683', '0.3186']
his_acc des_w_na:  ['0.6547', '0.4561', '0.3760', '0.3749', '0.3350']
cur_acc rrf_w_na:  ['0.6601', '0.3288', '0.4664', '0.3718', '0.3058']
his_acc rrf_w_na:  ['0.6601', '0.4740', '0.4049', '0.3896', '0.3452']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 82.8858642CurrentTrain: epoch  0, batch     1 | loss: 102.6004365CurrentTrain: epoch  0, batch     2 | loss: 129.0561944CurrentTrain: epoch  0, batch     3 | loss: 70.8654152CurrentTrain: epoch  0, batch     4 | loss: 30.4139462CurrentTrain: epoch  1, batch     0 | loss: 75.5763790CurrentTrain: epoch  1, batch     1 | loss: 63.7147825CurrentTrain: epoch  1, batch     2 | loss: 92.0413871CurrentTrain: epoch  1, batch     3 | loss: 91.8948181CurrentTrain: epoch  1, batch     4 | loss: 60.4329087CurrentTrain: epoch  2, batch     0 | loss: 89.5528755CurrentTrain: epoch  2, batch     1 | loss: 73.2506193CurrentTrain: epoch  2, batch     2 | loss: 57.2988874CurrentTrain: epoch  2, batch     3 | loss: 75.1289710CurrentTrain: epoch  2, batch     4 | loss: 60.3133306CurrentTrain: epoch  3, batch     0 | loss: 61.7649822CurrentTrain: epoch  3, batch     1 | loss: 66.8248101CurrentTrain: epoch  3, batch     2 | loss: 71.4354631CurrentTrain: epoch  3, batch     3 | loss: 91.2867139CurrentTrain: epoch  3, batch     4 | loss: 19.3159297CurrentTrain: epoch  4, batch     0 | loss: 69.1249877CurrentTrain: epoch  4, batch     1 | loss: 69.9638300CurrentTrain: epoch  4, batch     2 | loss: 70.7382153CurrentTrain: epoch  4, batch     3 | loss: 69.6368125CurrentTrain: epoch  4, batch     4 | loss: 16.7433582CurrentTrain: epoch  5, batch     0 | loss: 89.2596555CurrentTrain: epoch  5, batch     1 | loss: 86.1716046CurrentTrain: epoch  5, batch     2 | loss: 66.2345489CurrentTrain: epoch  5, batch     3 | loss: 67.1841407CurrentTrain: epoch  5, batch     4 | loss: 27.7261713CurrentTrain: epoch  6, batch     0 | loss: 87.5649176CurrentTrain: epoch  6, batch     1 | loss: 52.6053510CurrentTrain: epoch  6, batch     2 | loss: 88.5929919CurrentTrain: epoch  6, batch     3 | loss: 68.7178820CurrentTrain: epoch  6, batch     4 | loss: 16.6141851CurrentTrain: epoch  7, batch     0 | loss: 86.0545965CurrentTrain: epoch  7, batch     1 | loss: 118.6972344CurrentTrain: epoch  7, batch     2 | loss: 51.6679671CurrentTrain: epoch  7, batch     3 | loss: 67.3428209CurrentTrain: epoch  7, batch     4 | loss: 17.2787837CurrentTrain: epoch  8, batch     0 | loss: 67.3901045CurrentTrain: epoch  8, batch     1 | loss: 86.8161431CurrentTrain: epoch  8, batch     2 | loss: 66.7534771CurrentTrain: epoch  8, batch     3 | loss: 54.2216604CurrentTrain: epoch  8, batch     4 | loss: 15.8694716CurrentTrain: epoch  9, batch     0 | loss: 65.3913072CurrentTrain: epoch  9, batch     1 | loss: 54.3595533CurrentTrain: epoch  9, batch     2 | loss: 65.1696096CurrentTrain: epoch  9, batch     3 | loss: 84.1251667CurrentTrain: epoch  9, batch     4 | loss: 59.9457065
MemoryTrain:  epoch  0, batch     0 | loss: 0.3152846MemoryTrain:  epoch  1, batch     0 | loss: 0.2941580MemoryTrain:  epoch  2, batch     0 | loss: 0.2725379MemoryTrain:  epoch  3, batch     0 | loss: 0.1776415MemoryTrain:  epoch  4, batch     0 | loss: 0.1528247MemoryTrain:  epoch  5, batch     0 | loss: 0.1342638MemoryTrain:  epoch  6, batch     0 | loss: 0.1192405MemoryTrain:  epoch  7, batch     0 | loss: 0.0931090MemoryTrain:  epoch  8, batch     0 | loss: 0.0740071MemoryTrain:  epoch  9, batch     0 | loss: 0.0676913

F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 6: 0.0, 38: 0.24, 39: 0.3050847457627119, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.46153846153846156, 19: 0.0, 25: 0.0, 28: 0.0}
Micro-average F1 score: 0.2867383512544803
Weighted-average F1 score: 0.2800702955236853
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.6031746031746031, 12: 0.7530864197530864, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 25: 0.0, 27: 0.0, 28: 0.5882352941176471, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5842105263157895
Weighted-average F1 score: 0.503298823555187
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.6141732283464567, 12: 0.7295597484276729, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 25: 0.0, 27: 0.0, 28: 0.5882352941176471, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5828877005347594
Weighted-average F1 score: 0.5059090239342151

F1 score per class: {0: 0.9295774647887324, 2: 0.7368421052631579, 4: 0.9417989417989417, 5: 0.8727272727272727, 6: 0.3089430894308943, 7: 0.0, 9: 0.96, 10: 0.21238938053097345, 11: 0.2033898305084746, 12: 0.2903225806451613, 13: 0.0, 15: 0.782608695652174, 16: 0.8135593220338984, 17: 0.0, 18: 0.19047619047619047, 19: 0.41025641025641024, 21: 0.3076923076923077, 23: 0.8192771084337349, 24: 0.10526315789473684, 25: 0.4411764705882353, 26: 0.7428571428571429, 27: 0.5185185185185185, 28: 0.13953488372093023, 29: 0.9312169312169312, 31: 0.5, 32: 0.7978142076502732, 35: 0.44776119402985076, 37: 0.47191011235955055, 38: 0.20512820512820512, 39: 0.0, 40: 0.1282051282051282}
Micro-average F1 score: 0.5770623742454728
Weighted-average F1 score: 0.653845553401774
F1 score per class: {0: 0.9444444444444444, 2: 0.6666666666666666, 4: 0.9795918367346939, 5: 0.8761061946902655, 6: 0.41134751773049644, 7: 0.0, 9: 0.9803921568627451, 10: 0.4857142857142857, 11: 0.4840764331210191, 12: 0.6931818181818182, 13: 0.0, 15: 0.6666666666666666, 16: 0.8387096774193549, 17: 0.2857142857142857, 18: 0.33962264150943394, 19: 0.5153374233128835, 21: 0.6376811594202898, 23: 0.813953488372093, 24: 0.09523809523809523, 25: 0.5866666666666667, 26: 0.7444444444444445, 27: 0.4375, 28: 0.2222222222222222, 29: 0.9375, 31: 0.8, 32: 0.8866995073891626, 35: 0.8598130841121495, 37: 0.5319148936170213, 38: 0.4, 39: 0.0, 40: 0.4966442953020134}
Micro-average F1 score: 0.6710254645560908
Weighted-average F1 score: 0.6781749441860229
F1 score per class: {0: 0.9444444444444444, 2: 0.6363636363636364, 4: 0.9743589743589743, 5: 0.8761061946902655, 6: 0.44285714285714284, 7: 0.0, 9: 0.9803921568627451, 10: 0.42962962962962964, 11: 0.4936708860759494, 12: 0.6744186046511628, 13: 0.0, 15: 0.6666666666666666, 16: 0.8253968253968254, 17: 0.14285714285714285, 18: 0.28, 19: 0.5149700598802395, 21: 0.6268656716417911, 23: 0.8235294117647058, 24: 0.09523809523809523, 25: 0.5789473684210527, 26: 0.7444444444444445, 27: 0.47058823529411764, 28: 0.18181818181818182, 29: 0.9430051813471503, 31: 0.8, 32: 0.8725490196078431, 35: 0.723404255319149, 37: 0.5319148936170213, 38: 0.3548387096774194, 39: 0.0, 40: 0.45925925925925926}
Micro-average F1 score: 0.6580779944289693
Weighted-average F1 score: 0.6653948294682143

F1 score per class: {0: 0.0, 2: 0.358974358974359, 4: 0.0, 6: 0.0, 10: 0.0, 11: 0.20869565217391303, 12: 0.2647058823529412, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.15384615384615385, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.1797752808988764
Weighted-average F1 score: 0.13585245978700475
F1 score per class: {0: 0.0, 2: 0.14583333333333334, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.4293785310734463, 12: 0.5281385281385281, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.11235955056179775, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.23319327731092437
Weighted-average F1 score: 0.17624755804527903
F1 score per class: {0: 0.0, 2: 0.14285714285714285, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.45348837209302323, 12: 0.5087719298245614, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.10869565217391304, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.24035281146637266
Weighted-average F1 score: 0.18177405635211077

F1 score per class: {0: 0.528, 2: 0.2222222222222222, 4: 0.9128205128205128, 5: 0.7901234567901234, 6: 0.19791666666666666, 7: 0.0, 9: 0.8571428571428571, 10: 0.1702127659574468, 11: 0.11162790697674418, 12: 0.14342629482071714, 13: 0.0, 15: 0.47368421052631576, 16: 0.43636363636363634, 17: 0.0, 18: 0.10526315789473684, 19: 0.34408602150537637, 21: 0.19672131147540983, 23: 0.7391304347826086, 24: 0.07407407407407407, 25: 0.43478260869565216, 26: 0.6770833333333334, 27: 0.175, 28: 0.040268456375838924, 29: 0.7333333333333333, 31: 0.07692307692307693, 32: 0.6636363636363637, 35: 0.23622047244094488, 37: 0.35, 38: 0.10810810810810811, 39: 0.0, 40: 0.0970873786407767}
Micro-average F1 score: 0.3985547526403558
Weighted-average F1 score: 0.38300582450034093
F1 score per class: {0: 0.3487179487179487, 2: 0.03977272727272727, 4: 0.897196261682243, 5: 0.5012658227848101, 6: 0.21561338289962825, 7: 0.0, 9: 0.6410256410256411, 10: 0.25757575757575757, 11: 0.1974025974025974, 12: 0.16827586206896553, 13: 0.0, 15: 0.37209302325581395, 16: 0.40625, 17: 0.10144927536231885, 18: 0.07171314741035857, 19: 0.38009049773755654, 21: 0.14285714285714285, 23: 0.5426356589147286, 24: 0.05128205128205128, 25: 0.5789473684210527, 26: 0.6536585365853659, 27: 0.1308411214953271, 28: 0.04405286343612335, 29: 0.694980694980695, 31: 0.05, 32: 0.5678233438485805, 35: 0.20353982300884957, 37: 0.2336448598130841, 38: 0.13836477987421383, 39: 0.0, 40: 0.25084745762711863}
Micro-average F1 score: 0.2934978928356412
Weighted-average F1 score: 0.2631737247742742
F1 score per class: {0: 0.3487179487179487, 2: 0.039886039886039885, 4: 0.9359605911330049, 5: 0.5308310991957105, 6: 0.23574144486692014, 7: 0.0, 9: 0.704225352112676, 10: 0.23577235772357724, 11: 0.208, 12: 0.16453900709219857, 13: 0.0, 15: 0.2962962962962963, 16: 0.39097744360902253, 17: 0.06451612903225806, 18: 0.05714285714285714, 19: 0.39090909090909093, 21: 0.13504823151125403, 23: 0.6730769230769231, 24: 0.058823529411764705, 25: 0.5569620253164557, 26: 0.6633663366336634, 27: 0.14678899082568808, 28: 0.039525691699604744, 29: 0.7165354330708661, 31: 0.06153846153846154, 32: 0.5579937304075235, 35: 0.2111801242236025, 37: 0.24752475247524752, 38: 0.11827956989247312, 39: 0.0, 40: 0.24603174603174602}
Micro-average F1 score: 0.2992874109263658
Weighted-average F1 score: 0.26551695389504104
cur_acc_wo_na:  ['0.7853', '0.4151', '0.8812', '0.7004', '0.4503', '0.2867']
his_acc_wo_na:  ['0.7853', '0.6661', '0.7306', '0.7138', '0.6327', '0.5771']
cur_acc des_wo_na:  ['0.8306', '0.5495', '0.9188', '0.8660', '0.6611', '0.5842']
his_acc des_wo_na:  ['0.8306', '0.6882', '0.7525', '0.7603', '0.7089', '0.6710']
cur_acc rrf_wo_na:  ['0.8326', '0.5520', '0.9231', '0.8521', '0.6124', '0.5829']
his_acc rrf_wo_na:  ['0.8326', '0.6860', '0.7568', '0.7654', '0.7079', '0.6581']
cur_acc_w_na:  ['0.6778', '0.3284', '0.5651', '0.4969', '0.3084', '0.1798']
his_acc_w_na:  ['0.6778', '0.5436', '0.4981', '0.5181', '0.4407', '0.3986']
cur_acc des_w_na:  ['0.6547', '0.3096', '0.4352', '0.3683', '0.3186', '0.2332']
his_acc des_w_na:  ['0.6547', '0.4561', '0.3760', '0.3749', '0.3350', '0.2935']
cur_acc rrf_w_na:  ['0.6601', '0.3288', '0.4664', '0.3718', '0.3058', '0.2404']
his_acc rrf_w_na:  ['0.6601', '0.4740', '0.4049', '0.3896', '0.3452', '0.2993']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 69.7217051CurrentTrain: epoch  0, batch     1 | loss: 85.9442548CurrentTrain: epoch  0, batch     2 | loss: 68.0574539CurrentTrain: epoch  0, batch     3 | loss: 131.6576379CurrentTrain: epoch  0, batch     4 | loss: 74.9590385CurrentTrain: epoch  1, batch     0 | loss: 64.0806320CurrentTrain: epoch  1, batch     1 | loss: 124.3798146CurrentTrain: epoch  1, batch     2 | loss: 75.9156181CurrentTrain: epoch  1, batch     3 | loss: 91.7522659CurrentTrain: epoch  1, batch     4 | loss: 69.2918455CurrentTrain: epoch  2, batch     0 | loss: 91.1378182CurrentTrain: epoch  2, batch     1 | loss: 90.7396734CurrentTrain: epoch  2, batch     2 | loss: 87.2691459CurrentTrain: epoch  2, batch     3 | loss: 90.0643041CurrentTrain: epoch  2, batch     4 | loss: 66.6143383CurrentTrain: epoch  3, batch     0 | loss: 121.3503460CurrentTrain: epoch  3, batch     1 | loss: 67.8844315CurrentTrain: epoch  3, batch     2 | loss: 70.6478160CurrentTrain: epoch  3, batch     3 | loss: 69.6541349CurrentTrain: epoch  3, batch     4 | loss: 48.0094841CurrentTrain: epoch  4, batch     0 | loss: 55.6128904CurrentTrain: epoch  4, batch     1 | loss: 117.6875070CurrentTrain: epoch  4, batch     2 | loss: 90.5776612CurrentTrain: epoch  4, batch     3 | loss: 86.1702644CurrentTrain: epoch  4, batch     4 | loss: 65.1679475CurrentTrain: epoch  5, batch     0 | loss: 86.2167102CurrentTrain: epoch  5, batch     1 | loss: 69.5154330CurrentTrain: epoch  5, batch     2 | loss: 66.4595571CurrentTrain: epoch  5, batch     3 | loss: 55.2512998CurrentTrain: epoch  5, batch     4 | loss: 48.6822850CurrentTrain: epoch  6, batch     0 | loss: 85.3094436CurrentTrain: epoch  6, batch     1 | loss: 185.0785464CurrentTrain: epoch  6, batch     2 | loss: 65.6086114CurrentTrain: epoch  6, batch     3 | loss: 63.7210410CurrentTrain: epoch  6, batch     4 | loss: 47.7991474CurrentTrain: epoch  7, batch     0 | loss: 65.8161131CurrentTrain: epoch  7, batch     1 | loss: 86.6847086CurrentTrain: epoch  7, batch     2 | loss: 82.3542381CurrentTrain: epoch  7, batch     3 | loss: 83.9865525CurrentTrain: epoch  7, batch     4 | loss: 64.2305184CurrentTrain: epoch  8, batch     0 | loss: 115.6711360CurrentTrain: epoch  8, batch     1 | loss: 53.0920671CurrentTrain: epoch  8, batch     2 | loss: 373.1471769CurrentTrain: epoch  8, batch     3 | loss: 61.3657850CurrentTrain: epoch  8, batch     4 | loss: 36.7650515CurrentTrain: epoch  9, batch     0 | loss: 65.4988578CurrentTrain: epoch  9, batch     1 | loss: 118.4736567CurrentTrain: epoch  9, batch     2 | loss: 49.5628382CurrentTrain: epoch  9, batch     3 | loss: 67.3324201CurrentTrain: epoch  9, batch     4 | loss: 65.5917371
MemoryTrain:  epoch  0, batch     0 | loss: 0.4681972MemoryTrain:  epoch  1, batch     0 | loss: 0.4009366MemoryTrain:  epoch  2, batch     0 | loss: 0.3166799MemoryTrain:  epoch  3, batch     0 | loss: 0.2425845MemoryTrain:  epoch  4, batch     0 | loss: 0.2048483MemoryTrain:  epoch  5, batch     0 | loss: 0.1751112MemoryTrain:  epoch  6, batch     0 | loss: 0.1358512MemoryTrain:  epoch  7, batch     0 | loss: 0.1216316MemoryTrain:  epoch  8, batch     0 | loss: 0.0999264MemoryTrain:  epoch  9, batch     0 | loss: 0.0863825

F1 score per class: {32: 0.23076923076923078, 1: 0.509090909090909, 34: 0.0, 3: 0.175, 35: 0.0, 37: 0.0, 38: 0.6847826086956522, 11: 0.0, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.07017543859649122, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.3599320882852292
Weighted-average F1 score: 0.31265361507450024
F1 score per class: {32: 0.47761194029850745, 1: 0.5486725663716814, 34: 0.0, 35: 0.2, 3: 0.0, 37: 0.0, 11: 0.6701570680628273, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.735632183908046, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.45714285714285713
Weighted-average F1 score: 0.36682747597255444
F1 score per class: {32: 0.43333333333333335, 1: 0.6218487394957983, 34: 0.0, 35: 0.23404255319148937, 3: 0.0, 37: 0.0, 11: 0.6701570680628273, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.5135135135135135, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.43636363636363634
Weighted-average F1 score: 0.3412370610324276

F1 score per class: {0: 0.8823529411764706, 1: 0.23076923076923078, 2: 0.631578947368421, 3: 0.47058823529411764, 4: 0.918918918918919, 5: 0.8727272727272727, 6: 0.29508196721311475, 7: 0.0, 9: 0.9803921568627451, 10: 0.2857142857142857, 11: 0.14414414414414414, 12: 0.22413793103448276, 13: 0.16666666666666666, 14: 0.15555555555555556, 15: 0.7368421052631579, 16: 0.8135593220338984, 17: 0.0, 18: 0.22727272727272727, 19: 0.11009174311926606, 21: 0.15789473684210525, 22: 0.6461538461538462, 23: 0.8275862068965517, 24: 0.08695652173913043, 25: 0.44776119402985076, 26: 0.7528089887640449, 27: 0.14285714285714285, 28: 0.17857142857142858, 29: 0.925531914893617, 31: 0.0, 32: 0.7956989247311828, 34: 0.05555555555555555, 35: 0.22727272727272727, 37: 0.41304347826086957, 38: 0.21739130434782608, 39: 0.0, 40: 0.10666666666666667}
Micro-average F1 score: 0.5120105297795328
Weighted-average F1 score: 0.5859825384856718
F1 score per class: {0: 0.9295774647887324, 1: 0.4155844155844156, 2: 0.56, 3: 0.45255474452554745, 4: 0.9473684210526315, 5: 0.8761061946902655, 6: 0.38571428571428573, 7: 0.0, 9: 0.9803921568627451, 10: 0.6052631578947368, 11: 0.10434782608695652, 12: 0.7023809523809523, 13: 0.15384615384615385, 14: 0.18, 15: 0.5714285714285714, 16: 0.8333333333333334, 17: 0.13793103448275862, 18: 0.28, 19: 0.3089430894308943, 21: 0.34615384615384615, 22: 0.6183574879227053, 23: 0.813953488372093, 24: 0.09090909090909091, 25: 0.5714285714285714, 26: 0.7351351351351352, 27: 0.1694915254237288, 28: 0.22641509433962265, 29: 0.9032258064516129, 31: 0.6666666666666666, 32: 0.87, 34: 0.3742690058479532, 35: 0.44776119402985076, 37: 0.4835164835164835, 38: 0.4074074074074074, 39: 0.0, 40: 0.5362318840579711}
Micro-average F1 score: 0.5816733067729084
Weighted-average F1 score: 0.590890461876634
F1 score per class: {0: 0.9295774647887324, 1: 0.37142857142857144, 2: 0.56, 3: 0.4966442953020134, 4: 0.9528795811518325, 5: 0.88, 6: 0.35294117647058826, 7: 0.0, 9: 0.9803921568627451, 10: 0.5138888888888888, 11: 0.15873015873015872, 12: 0.6375, 13: 0.125, 14: 0.1981981981981982, 15: 0.5714285714285714, 16: 0.819672131147541, 17: 0.08695652173913043, 18: 0.41379310344827586, 19: 0.2809917355371901, 21: 0.34615384615384615, 22: 0.6213592233009708, 23: 0.813953488372093, 24: 0.09090909090909091, 25: 0.5714285714285714, 26: 0.7351351351351352, 27: 0.16666666666666666, 28: 0.1724137931034483, 29: 0.9081081081081082, 31: 0.6666666666666666, 32: 0.8542713567839196, 34: 0.2900763358778626, 35: 0.38333333333333336, 37: 0.47058823529411764, 38: 0.39285714285714285, 39: 0.0, 40: 0.47619047619047616}
Micro-average F1 score: 0.567318757192175
Weighted-average F1 score: 0.5745642236902151

F1 score per class: {0: 0.0, 1: 0.17647058823529413, 2: 0.0, 3: 0.42748091603053434, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.12173913043478261, 15: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.44680851063829785, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.07017543859649122, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.19757688723205966
Weighted-average F1 score: 0.1549226475747259
F1 score per class: {0: 0.0, 1: 0.23703703703703705, 2: 0.0, 3: 0.36046511627906974, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.1, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.4129032258064516, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.42105263157894735, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.1737142857142857
Weighted-average F1 score: 0.13743000250035492
F1 score per class: {0: 0.0, 1: 0.21487603305785125, 2: 0.0, 3: 0.38341968911917096, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.11518324607329843, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.4155844155844156, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.37254901960784315, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.1705150976909414
Weighted-average F1 score: 0.13218056548608104

F1 score per class: {0: 0.4838709677419355, 1: 0.16, 2: 0.1935483870967742, 3: 0.3010752688172043, 4: 0.8947368421052632, 5: 0.803347280334728, 6: 0.1935483870967742, 7: 0.0, 9: 0.9259259259259259, 10: 0.20481927710843373, 11: 0.08290155440414508, 12: 0.13, 13: 0.05128205128205128, 14: 0.07446808510638298, 15: 0.42424242424242425, 16: 0.4528301886792453, 17: 0.0, 18: 0.1282051282051282, 19: 0.10434782608695652, 21: 0.08333333333333333, 22: 0.3631123919308357, 23: 0.7058823529411765, 24: 0.043478260869565216, 25: 0.42857142857142855, 26: 0.6802030456852792, 27: 0.045454545454545456, 28: 0.05102040816326531, 29: 0.696, 31: 0.0, 32: 0.5543071161048689, 34: 0.05063291139240506, 35: 0.13245033112582782, 37: 0.29457364341085274, 38: 0.11235955056179775, 39: 0.0, 40: 0.09523809523809523}
Micro-average F1 score: 0.3356341673856773
Weighted-average F1 score: 0.3230262423353715
F1 score per class: {0: 0.38823529411764707, 1: 0.15165876777251186, 2: 0.046357615894039736, 3: 0.17174515235457063, 4: 0.8955223880597015, 5: 0.5409836065573771, 6: 0.19081272084805653, 7: 0.0, 9: 0.704225352112676, 10: 0.30363036303630364, 11: 0.0502092050209205, 12: 0.17933130699088146, 13: 0.0392156862745098, 14: 0.05027932960893855, 15: 0.27906976744186046, 16: 0.4032258064516129, 17: 0.06666666666666667, 18: 0.056910569105691054, 19: 0.24050632911392406, 21: 0.09375, 22: 0.3176178660049628, 23: 0.49645390070921985, 24: 0.04878048780487805, 25: 0.5301204819277109, 26: 0.6415094339622641, 27: 0.05154639175257732, 28: 0.04597701149425287, 29: 0.6640316205533597, 31: 0.045454545454545456, 32: 0.4860335195530726, 34: 0.13852813852813853, 35: 0.13513513513513514, 37: 0.22564102564102564, 38: 0.1286549707602339, 39: 0.0, 40: 0.2857142857142857}
Micro-average F1 score: 0.2549264155649788
Weighted-average F1 score: 0.23044494952404154
F1 score per class: {0: 0.38596491228070173, 1: 0.13541666666666666, 2: 0.04590163934426229, 3: 0.18407960199004975, 4: 0.914572864321608, 5: 0.5546218487394958, 6: 0.17777777777777778, 7: 0.0, 9: 0.746268656716418, 10: 0.2690909090909091, 11: 0.06688963210702341, 12: 0.1731748726655348, 13: 0.03773584905660377, 14: 0.057441253263707574, 15: 0.2727272727272727, 16: 0.3968253968253968, 17: 0.05, 18: 0.08540925266903915, 19: 0.22077922077922077, 21: 0.09574468085106383, 22: 0.3208020050125313, 23: 0.56, 24: 0.04878048780487805, 25: 0.5176470588235295, 26: 0.6538461538461539, 27: 0.050505050505050504, 28: 0.03508771929824561, 29: 0.6774193548387096, 31: 0.05405405405405406, 32: 0.4857142857142857, 34: 0.14785992217898833, 35: 0.13105413105413105, 37: 0.22325581395348837, 38: 0.12087912087912088, 39: 0.0, 40: 0.26905829596412556}
Micro-average F1 score: 0.25623700623700624
Weighted-average F1 score: 0.2301758424930516
cur_acc_wo_na:  ['0.7853', '0.4151', '0.8812', '0.7004', '0.4503', '0.2867', '0.3599']
his_acc_wo_na:  ['0.7853', '0.6661', '0.7306', '0.7138', '0.6327', '0.5771', '0.5120']
cur_acc des_wo_na:  ['0.8306', '0.5495', '0.9188', '0.8660', '0.6611', '0.5842', '0.4571']
his_acc des_wo_na:  ['0.8306', '0.6882', '0.7525', '0.7603', '0.7089', '0.6710', '0.5817']
cur_acc rrf_wo_na:  ['0.8326', '0.5520', '0.9231', '0.8521', '0.6124', '0.5829', '0.4364']
his_acc rrf_wo_na:  ['0.8326', '0.6860', '0.7568', '0.7654', '0.7079', '0.6581', '0.5673']
cur_acc_w_na:  ['0.6778', '0.3284', '0.5651', '0.4969', '0.3084', '0.1798', '0.1976']
his_acc_w_na:  ['0.6778', '0.5436', '0.4981', '0.5181', '0.4407', '0.3986', '0.3356']
cur_acc des_w_na:  ['0.6547', '0.3096', '0.4352', '0.3683', '0.3186', '0.2332', '0.1737']
his_acc des_w_na:  ['0.6547', '0.4561', '0.3760', '0.3749', '0.3350', '0.2935', '0.2549']
cur_acc rrf_w_na:  ['0.6601', '0.3288', '0.4664', '0.3718', '0.3058', '0.2404', '0.1705']
his_acc rrf_w_na:  ['0.6601', '0.4740', '0.4049', '0.3896', '0.3452', '0.2993', '0.2562']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 75.9566950CurrentTrain: epoch  0, batch     1 | loss: 81.4496504CurrentTrain: epoch  0, batch     2 | loss: 61.7312090CurrentTrain: epoch  0, batch     3 | loss: 53.7547995CurrentTrain: epoch  1, batch     0 | loss: 71.7661550CurrentTrain: epoch  1, batch     1 | loss: 96.3540329CurrentTrain: epoch  1, batch     2 | loss: 66.3574793CurrentTrain: epoch  1, batch     3 | loss: 71.8885975CurrentTrain: epoch  2, batch     0 | loss: 56.6202307CurrentTrain: epoch  2, batch     1 | loss: 70.1508028CurrentTrain: epoch  2, batch     2 | loss: 68.1489385CurrentTrain: epoch  2, batch     3 | loss: 72.9742387CurrentTrain: epoch  3, batch     0 | loss: 68.0292252CurrentTrain: epoch  3, batch     1 | loss: 86.9631972CurrentTrain: epoch  3, batch     2 | loss: 65.8914739CurrentTrain: epoch  3, batch     3 | loss: 39.2150230CurrentTrain: epoch  4, batch     0 | loss: 86.7895576CurrentTrain: epoch  4, batch     1 | loss: 85.3945635CurrentTrain: epoch  4, batch     2 | loss: 49.3598349CurrentTrain: epoch  4, batch     3 | loss: 50.2852251CurrentTrain: epoch  5, batch     0 | loss: 51.1570320CurrentTrain: epoch  5, batch     1 | loss: 66.4634860CurrentTrain: epoch  5, batch     2 | loss: 65.6241045CurrentTrain: epoch  5, batch     3 | loss: 52.4964361CurrentTrain: epoch  6, batch     0 | loss: 64.0163772CurrentTrain: epoch  6, batch     1 | loss: 66.1530689CurrentTrain: epoch  6, batch     2 | loss: 52.2654389CurrentTrain: epoch  6, batch     3 | loss: 39.0010307CurrentTrain: epoch  7, batch     0 | loss: 80.8817686CurrentTrain: epoch  7, batch     1 | loss: 81.8491446CurrentTrain: epoch  7, batch     2 | loss: 50.4187565CurrentTrain: epoch  7, batch     3 | loss: 51.5310778CurrentTrain: epoch  8, batch     0 | loss: 63.3020546CurrentTrain: epoch  8, batch     1 | loss: 61.3140098CurrentTrain: epoch  8, batch     2 | loss: 117.6951558CurrentTrain: epoch  8, batch     3 | loss: 30.0029365CurrentTrain: epoch  9, batch     0 | loss: 64.3615879CurrentTrain: epoch  9, batch     1 | loss: 61.8952519CurrentTrain: epoch  9, batch     2 | loss: 64.4824372CurrentTrain: epoch  9, batch     3 | loss: 38.1827668
MemoryTrain:  epoch  0, batch     0 | loss: 0.1888144MemoryTrain:  epoch  1, batch     0 | loss: 0.1725932MemoryTrain:  epoch  2, batch     0 | loss: 0.1463029MemoryTrain:  epoch  3, batch     0 | loss: 0.1451608MemoryTrain:  epoch  4, batch     0 | loss: 0.1118874MemoryTrain:  epoch  5, batch     0 | loss: 0.0931825MemoryTrain:  epoch  6, batch     0 | loss: 0.0836328MemoryTrain:  epoch  7, batch     0 | loss: 0.0725622MemoryTrain:  epoch  8, batch     0 | loss: 0.0621191MemoryTrain:  epoch  9, batch     0 | loss: 0.0518564

F1 score per class: {33: 0.0, 34: 0.5789473684210527, 36: 0.0, 37: 0.0, 38: 0.0, 6: 0.0, 8: 0.8260869565217391, 10: 0.0, 11: 0.0, 16: 0.0, 18: 0.972972972972973, 20: 0.42857142857142855, 26: 0.0, 28: 0.37037037037037035, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5831062670299727
Weighted-average F1 score: 0.5503775798566484
F1 score per class: {2: 0.0, 5: 0.0, 6: 0.0, 8: 0.6829268292682927, 10: 0.0, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.88, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.625, 34: 0.0, 35: 0.0, 36: 0.8173913043478261, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.7306791569086651
Weighted-average F1 score: 0.6625184400807412
F1 score per class: {2: 0.0, 5: 0.0, 6: 0.0, 8: 0.6935483870967742, 10: 0.0, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 20: 0.8910891089108911, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.625, 34: 0.0, 35: 0.0, 36: 0.7289719626168224, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.7122641509433962
Weighted-average F1 score: 0.6410187524935901

F1 score per class: {0: 0.8985507246376812, 1: 0.16326530612244897, 2: 0.7058823529411765, 3: 0.4778761061946903, 4: 0.9417989417989417, 5: 0.8789237668161435, 6: 0.20512820512820512, 7: 0.0, 8: 0.44, 9: 0.9803921568627451, 10: 0.12844036697247707, 11: 0.16666666666666666, 12: 0.17857142857142858, 13: 0.18181818181818182, 14: 0.1095890410958904, 15: 0.7368421052631579, 16: 0.8, 17: 0.19047619047619047, 18: 0.3333333333333333, 19: 0.3586206896551724, 20: 0.8260869565217391, 21: 0.16216216216216217, 22: 0.6938775510204082, 23: 0.813953488372093, 24: 0.07407407407407407, 25: 0.4857142857142857, 26: 0.7486033519553073, 27: 0.18604651162790697, 28: 0.17391304347826086, 29: 0.9263157894736842, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.8064516129032258, 33: 0.2857142857142857, 34: 0.05333333333333334, 35: 0.2978723404255319, 36: 0.35714285714285715, 37: 0.48936170212765956, 38: 0.24390243902439024, 39: 0.0, 40: 0.175}
Micro-average F1 score: 0.5300578034682081
Weighted-average F1 score: 0.598953410038162
F1 score per class: {0: 0.9315068493150684, 1: 0.4788732394366197, 2: 0.5833333333333334, 3: 0.48175182481751827, 4: 0.9637305699481865, 5: 0.8571428571428571, 6: 0.37681159420289856, 7: 0.0, 8: 0.4307692307692308, 9: 0.9803921568627451, 10: 0.359375, 11: 0.16393442622950818, 12: 0.6829268292682927, 13: 0.16666666666666666, 14: 0.2549019607843137, 15: 0.5, 16: 0.8333333333333334, 17: 0.26666666666666666, 18: 0.42857142857142855, 19: 0.5, 20: 0.8543689320388349, 21: 0.4166666666666667, 22: 0.6952380952380952, 23: 0.7710843373493976, 24: 0.08333333333333333, 25: 0.5789473684210527, 26: 0.7351351351351352, 27: 0.2028985507246377, 28: 0.15384615384615385, 29: 0.8994708994708994, 30: 0.9230769230769231, 31: 0.8, 32: 0.8656716417910447, 33: 0.3225806451612903, 34: 0.38620689655172413, 35: 0.47761194029850745, 36: 0.6482758620689655, 37: 0.5434782608695652, 38: 0.4727272727272727, 39: 0.125, 40: 0.6119402985074627}
Micro-average F1 score: 0.6024396315658451
Weighted-average F1 score: 0.6097149716953263
F1 score per class: {0: 0.9315068493150684, 1: 0.4788732394366197, 2: 0.6666666666666666, 3: 0.5362318840579711, 4: 0.9690721649484536, 5: 0.8571428571428571, 6: 0.3880597014925373, 7: 0.0, 8: 0.42786069651741293, 9: 0.9803921568627451, 10: 0.3333333333333333, 11: 0.20437956204379562, 12: 0.6419753086419753, 13: 0.18181818181818182, 14: 0.25742574257425743, 15: 0.5, 16: 0.8, 17: 0.17391304347826086, 18: 0.38961038961038963, 19: 0.4968152866242038, 20: 0.8653846153846154, 21: 0.35555555555555557, 22: 0.6952380952380952, 23: 0.7560975609756098, 24: 0.08333333333333333, 25: 0.5789473684210527, 26: 0.7351351351351352, 27: 0.2, 28: 0.21428571428571427, 29: 0.9090909090909091, 30: 0.926829268292683, 31: 0.8, 32: 0.8446601941747572, 33: 0.2631578947368421, 34: 0.2631578947368421, 35: 0.38333333333333336, 36: 0.5909090909090909, 37: 0.5252525252525253, 38: 0.45614035087719296, 39: 0.13333333333333333, 40: 0.5873015873015873}
Micro-average F1 score: 0.5914221218961625
Weighted-average F1 score: 0.597871501055907

F1 score per class: {0: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5454545454545454, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.6440677966101694, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 31: 0.0, 32: 0.0, 33: 0.42857142857142855, 34: 0.0, 35: 0.0, 36: 0.32608695652173914, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3862815884476534
Weighted-average F1 score: 0.27961612839017236
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5153374233128835, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.5789473684210527, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 31: 0.0, 32: 0.0, 33: 0.38461538461538464, 34: 0.0, 35: 0.0, 36: 0.4292237442922374, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2937853107344633
Weighted-average F1 score: 0.22617146567554297
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5308641975308642, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.5882352941176471, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.9743589743589743, 31: 0.0, 32: 0.0, 33: 0.3125, 34: 0.0, 35: 0.0, 36: 0.4126984126984127, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.30109670987038883
Weighted-average F1 score: 0.2306306714594965

F1 score per class: {0: 0.45588235294117646, 1: 0.1111111111111111, 2: 0.2, 3: 0.2903225806451613, 4: 0.9128205128205128, 5: 0.6925795053003534, 6: 0.13872832369942195, 7: 0.0, 8: 0.25384615384615383, 9: 0.7936507936507936, 10: 0.10218978102189781, 11: 0.08968609865470852, 12: 0.10526315789473684, 13: 0.046511627906976744, 14: 0.06299212598425197, 15: 0.4666666666666667, 16: 0.41739130434782606, 17: 0.10526315789473684, 18: 0.11458333333333333, 19: 0.28415300546448086, 20: 0.296875, 21: 0.09836065573770492, 22: 0.38309859154929576, 23: 0.660377358490566, 24: 0.02857142857142857, 25: 0.4594594594594595, 26: 0.6504854368932039, 27: 0.05776173285198556, 28: 0.0547945205479452, 29: 0.6616541353383458, 30: 0.9230769230769231, 31: 0.08695652173913043, 32: 0.5300353356890459, 33: 0.17647058823529413, 34: 0.047619047619047616, 35: 0.15384615384615385, 36: 0.23809523809523808, 37: 0.3150684931506849, 38: 0.14285714285714285, 39: 0.0, 40: 0.14}
Micro-average F1 score: 0.32703281027104136
Weighted-average F1 score: 0.3186928342789778
F1 score per class: {0: 0.31336405529953915, 1: 0.16346153846153846, 2: 0.04046242774566474, 3: 0.16417910447761194, 4: 0.8942307692307693, 5: 0.35294117647058826, 6: 0.17993079584775087, 7: 0.0, 8: 0.168, 9: 0.6097560975609756, 10: 0.18326693227091634, 11: 0.08196721311475409, 12: 0.16208393632416787, 13: 0.03773584905660377, 14: 0.07692307692307693, 15: 0.21818181818181817, 16: 0.373134328358209, 17: 0.09195402298850575, 18: 0.10344827586206896, 19: 0.31666666666666665, 20: 0.20657276995305165, 21: 0.10152284263959391, 22: 0.36683417085427134, 23: 0.4444444444444444, 24: 0.041666666666666664, 25: 0.5057471264367817, 26: 0.6153846153846154, 27: 0.054474708171206226, 28: 0.04395604395604396, 29: 0.604982206405694, 30: 0.4675324675324675, 31: 0.02857142857142857, 32: 0.42962962962962964, 33: 0.07936507936507936, 34: 0.17073170731707318, 35: 0.1350210970464135, 36: 0.1687612208258528, 37: 0.24509803921568626, 38: 0.15204678362573099, 39: 0.06896551724137931, 40: 0.2733333333333333}
Micro-average F1 score: 0.2368369543942063
Weighted-average F1 score: 0.21962508290783214
F1 score per class: {0: 0.29694323144104806, 1: 0.1619047619047619, 2: 0.05737704918032787, 3: 0.1774580335731415, 4: 0.9170731707317074, 5: 0.3694029850746269, 6: 0.19117647058823528, 7: 0.0, 8: 0.16895874263261296, 9: 0.6410256410256411, 10: 0.1810344827586207, 11: 0.099644128113879, 12: 0.160741885625966, 13: 0.03636363636363636, 14: 0.07624633431085044, 15: 0.21052631578947367, 16: 0.35294117647058826, 17: 0.07017543859649122, 18: 0.09523809523809523, 19: 0.32231404958677684, 20: 0.20316027088036118, 21: 0.09411764705882353, 22: 0.3677581863979849, 23: 0.5, 24: 0.0392156862745098, 25: 0.5057471264367817, 26: 0.6181818181818182, 27: 0.05426356589147287, 28: 0.05714285714285714, 29: 0.625, 30: 0.4523809523809524, 31: 0.04878048780487805, 32: 0.4223300970873786, 33: 0.06756756756756757, 34: 0.13100436681222707, 35: 0.11794871794871795, 36: 0.15918367346938775, 37: 0.24413145539906103, 38: 0.14285714285714285, 39: 0.09090909090909091, 40: 0.2824427480916031}
Micro-average F1 score: 0.24147465437788018
Weighted-average F1 score: 0.22355042827946514
cur_acc_wo_na:  ['0.7853', '0.4151', '0.8812', '0.7004', '0.4503', '0.2867', '0.3599', '0.5831']
his_acc_wo_na:  ['0.7853', '0.6661', '0.7306', '0.7138', '0.6327', '0.5771', '0.5120', '0.5301']
cur_acc des_wo_na:  ['0.8306', '0.5495', '0.9188', '0.8660', '0.6611', '0.5842', '0.4571', '0.7307']
his_acc des_wo_na:  ['0.8306', '0.6882', '0.7525', '0.7603', '0.7089', '0.6710', '0.5817', '0.6024']
cur_acc rrf_wo_na:  ['0.8326', '0.5520', '0.9231', '0.8521', '0.6124', '0.5829', '0.4364', '0.7123']
his_acc rrf_wo_na:  ['0.8326', '0.6860', '0.7568', '0.7654', '0.7079', '0.6581', '0.5673', '0.5914']
cur_acc_w_na:  ['0.6778', '0.3284', '0.5651', '0.4969', '0.3084', '0.1798', '0.1976', '0.3863']
his_acc_w_na:  ['0.6778', '0.5436', '0.4981', '0.5181', '0.4407', '0.3986', '0.3356', '0.3270']
cur_acc des_w_na:  ['0.6547', '0.3096', '0.4352', '0.3683', '0.3186', '0.2332', '0.1737', '0.2938']
his_acc des_w_na:  ['0.6547', '0.4561', '0.3760', '0.3749', '0.3350', '0.2935', '0.2549', '0.2368']
cur_acc rrf_w_na:  ['0.6601', '0.3288', '0.4664', '0.3718', '0.3058', '0.2404', '0.1705', '0.3011']
his_acc rrf_w_na:  ['0.6601', '0.4740', '0.4049', '0.3896', '0.3452', '0.2993', '0.2562', '0.2415']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 68.5741473CurrentTrain: epoch  0, batch     1 | loss: 80.5684742CurrentTrain: epoch  0, batch     2 | loss: 57.3401480CurrentTrain: epoch  0, batch     3 | loss: 66.1979521CurrentTrain: epoch  0, batch     4 | loss: 100.8682188CurrentTrain: epoch  0, batch     5 | loss: 128.4340459CurrentTrain: epoch  0, batch     6 | loss: 78.4575092CurrentTrain: epoch  0, batch     7 | loss: 96.0995834CurrentTrain: epoch  0, batch     8 | loss: 56.3690440CurrentTrain: epoch  0, batch     9 | loss: 78.7165669CurrentTrain: epoch  0, batch    10 | loss: 78.8970602CurrentTrain: epoch  0, batch    11 | loss: 65.4726807CurrentTrain: epoch  0, batch    12 | loss: 77.6489433CurrentTrain: epoch  0, batch    13 | loss: 65.7330114CurrentTrain: epoch  0, batch    14 | loss: 56.6717911CurrentTrain: epoch  0, batch    15 | loss: 79.1134880CurrentTrain: epoch  0, batch    16 | loss: 77.9213295CurrentTrain: epoch  0, batch    17 | loss: 186.9848149CurrentTrain: epoch  0, batch    18 | loss: 128.7645839CurrentTrain: epoch  0, batch    19 | loss: 77.2644169CurrentTrain: epoch  0, batch    20 | loss: 64.8782727CurrentTrain: epoch  0, batch    21 | loss: 77.3675354CurrentTrain: epoch  0, batch    22 | loss: 95.9782366CurrentTrain: epoch  0, batch    23 | loss: 189.6487477CurrentTrain: epoch  0, batch    24 | loss: 94.7611736CurrentTrain: epoch  0, batch    25 | loss: 68.2341970CurrentTrain: epoch  0, batch    26 | loss: 95.4255605CurrentTrain: epoch  0, batch    27 | loss: 95.7920850CurrentTrain: epoch  0, batch    28 | loss: 77.4452585CurrentTrain: epoch  0, batch    29 | loss: 127.5064411CurrentTrain: epoch  0, batch    30 | loss: 95.2289175CurrentTrain: epoch  0, batch    31 | loss: 56.9278037CurrentTrain: epoch  0, batch    32 | loss: 96.0407638CurrentTrain: epoch  0, batch    33 | loss: 77.0662637CurrentTrain: epoch  0, batch    34 | loss: 126.1849443CurrentTrain: epoch  0, batch    35 | loss: 96.0238882CurrentTrain: epoch  0, batch    36 | loss: 65.7365923CurrentTrain: epoch  0, batch    37 | loss: 96.1837874CurrentTrain: epoch  0, batch    38 | loss: 95.6078919CurrentTrain: epoch  0, batch    39 | loss: 127.1381935CurrentTrain: epoch  0, batch    40 | loss: 77.2646405CurrentTrain: epoch  0, batch    41 | loss: 75.7317443CurrentTrain: epoch  0, batch    42 | loss: 56.2286088CurrentTrain: epoch  0, batch    43 | loss: 56.2759666CurrentTrain: epoch  0, batch    44 | loss: 77.1527837CurrentTrain: epoch  0, batch    45 | loss: 96.8016394CurrentTrain: epoch  0, batch    46 | loss: 64.9931187CurrentTrain: epoch  0, batch    47 | loss: 56.3234164CurrentTrain: epoch  0, batch    48 | loss: 95.6789769CurrentTrain: epoch  0, batch    49 | loss: 76.7524972CurrentTrain: epoch  0, batch    50 | loss: 77.0416342CurrentTrain: epoch  0, batch    51 | loss: 76.5856159CurrentTrain: epoch  0, batch    52 | loss: 95.3086988CurrentTrain: epoch  0, batch    53 | loss: 94.9910233CurrentTrain: epoch  0, batch    54 | loss: 77.9133034CurrentTrain: epoch  0, batch    55 | loss: 94.8300000CurrentTrain: epoch  0, batch    56 | loss: 76.2192465CurrentTrain: epoch  0, batch    57 | loss: 94.4940937CurrentTrain: epoch  0, batch    58 | loss: 63.7776390CurrentTrain: epoch  0, batch    59 | loss: 125.1111879CurrentTrain: epoch  0, batch    60 | loss: 76.5677294CurrentTrain: epoch  0, batch    61 | loss: 76.4189842CurrentTrain: epoch  0, batch    62 | loss: 76.9930936CurrentTrain: epoch  0, batch    63 | loss: 77.3712248CurrentTrain: epoch  0, batch    64 | loss: 76.3103442CurrentTrain: epoch  0, batch    65 | loss: 64.3749398CurrentTrain: epoch  0, batch    66 | loss: 76.7189251CurrentTrain: epoch  0, batch    67 | loss: 63.7799857CurrentTrain: epoch  0, batch    68 | loss: 93.9899844CurrentTrain: epoch  0, batch    69 | loss: 94.7153532CurrentTrain: epoch  0, batch    70 | loss: 64.2186333CurrentTrain: epoch  0, batch    71 | loss: 95.2601608CurrentTrain: epoch  0, batch    72 | loss: 95.5912980CurrentTrain: epoch  0, batch    73 | loss: 94.9241309CurrentTrain: epoch  0, batch    74 | loss: 64.0702840CurrentTrain: epoch  0, batch    75 | loss: 76.4638846CurrentTrain: epoch  0, batch    76 | loss: 75.7389092CurrentTrain: epoch  0, batch    77 | loss: 65.0554954CurrentTrain: epoch  0, batch    78 | loss: 75.5489670CurrentTrain: epoch  0, batch    79 | loss: 75.6122977CurrentTrain: epoch  0, batch    80 | loss: 63.9813239CurrentTrain: epoch  0, batch    81 | loss: 76.1453896CurrentTrain: epoch  0, batch    82 | loss: 187.3483735CurrentTrain: epoch  0, batch    83 | loss: 64.0783107CurrentTrain: epoch  0, batch    84 | loss: 63.1493893CurrentTrain: epoch  0, batch    85 | loss: 64.0140746CurrentTrain: epoch  0, batch    86 | loss: 64.3639473CurrentTrain: epoch  0, batch    87 | loss: 124.8614008CurrentTrain: epoch  0, batch    88 | loss: 126.0398694CurrentTrain: epoch  0, batch    89 | loss: 63.9582577CurrentTrain: epoch  0, batch    90 | loss: 63.5804981CurrentTrain: epoch  0, batch    91 | loss: 64.0404647CurrentTrain: epoch  0, batch    92 | loss: 76.1983626CurrentTrain: epoch  0, batch    93 | loss: 63.7849396CurrentTrain: epoch  0, batch    94 | loss: 54.1847933CurrentTrain: epoch  0, batch    95 | loss: 64.5703114CurrentTrain: epoch  1, batch     0 | loss: 92.5252438CurrentTrain: epoch  1, batch     1 | loss: 76.0226789CurrentTrain: epoch  1, batch     2 | loss: 75.2309546CurrentTrain: epoch  1, batch     3 | loss: 93.9190970CurrentTrain: epoch  1, batch     4 | loss: 185.5114495CurrentTrain: epoch  1, batch     5 | loss: 62.9715045CurrentTrain: epoch  1, batch     6 | loss: 187.1461958CurrentTrain: epoch  1, batch     7 | loss: 74.9093319CurrentTrain: epoch  1, batch     8 | loss: 75.4660580CurrentTrain: epoch  1, batch     9 | loss: 92.5869962CurrentTrain: epoch  1, batch    10 | loss: 61.0112263CurrentTrain: epoch  1, batch    11 | loss: 92.0996438CurrentTrain: epoch  1, batch    12 | loss: 125.9022388CurrentTrain: epoch  1, batch    13 | loss: 92.0331224CurrentTrain: epoch  1, batch    14 | loss: 61.8230611CurrentTrain: epoch  1, batch    15 | loss: 92.8117847CurrentTrain: epoch  1, batch    16 | loss: 72.9688923CurrentTrain: epoch  1, batch    17 | loss: 61.0117392CurrentTrain: epoch  1, batch    18 | loss: 92.8661578CurrentTrain: epoch  1, batch    19 | loss: 76.2408358CurrentTrain: epoch  1, batch    20 | loss: 92.3153804CurrentTrain: epoch  1, batch    21 | loss: 63.2040761CurrentTrain: epoch  1, batch    22 | loss: 62.1323684CurrentTrain: epoch  1, batch    23 | loss: 75.7770100CurrentTrain: epoch  1, batch    24 | loss: 73.9174516CurrentTrain: epoch  1, batch    25 | loss: 52.4859406CurrentTrain: epoch  1, batch    26 | loss: 63.5040452CurrentTrain: epoch  1, batch    27 | loss: 54.6973308CurrentTrain: epoch  1, batch    28 | loss: 91.4871016CurrentTrain: epoch  1, batch    29 | loss: 91.3332419CurrentTrain: epoch  1, batch    30 | loss: 61.0578386CurrentTrain: epoch  1, batch    31 | loss: 123.4036761CurrentTrain: epoch  1, batch    32 | loss: 63.1927750CurrentTrain: epoch  1, batch    33 | loss: 75.1035289CurrentTrain: epoch  1, batch    34 | loss: 90.8972975CurrentTrain: epoch  1, batch    35 | loss: 123.1443849CurrentTrain: epoch  1, batch    36 | loss: 74.6364714CurrentTrain: epoch  1, batch    37 | loss: 73.3219009CurrentTrain: epoch  1, batch    38 | loss: 62.1149198CurrentTrain: epoch  1, batch    39 | loss: 91.8056502CurrentTrain: epoch  1, batch    40 | loss: 60.2635958CurrentTrain: epoch  1, batch    41 | loss: 61.0417889CurrentTrain: epoch  1, batch    42 | loss: 122.9087975CurrentTrain: epoch  1, batch    43 | loss: 72.9895400CurrentTrain: epoch  1, batch    44 | loss: 72.7335290CurrentTrain: epoch  1, batch    45 | loss: 73.0953646CurrentTrain: epoch  1, batch    46 | loss: 122.6383651CurrentTrain: epoch  1, batch    47 | loss: 86.7434157CurrentTrain: epoch  1, batch    48 | loss: 58.1168218CurrentTrain: epoch  1, batch    49 | loss: 74.3983109CurrentTrain: epoch  1, batch    50 | loss: 89.0970537CurrentTrain: epoch  1, batch    51 | loss: 121.8504946CurrentTrain: epoch  1, batch    52 | loss: 50.0948347CurrentTrain: epoch  1, batch    53 | loss: 49.6771614CurrentTrain: epoch  1, batch    54 | loss: 61.1056543CurrentTrain: epoch  1, batch    55 | loss: 60.2977534CurrentTrain: epoch  1, batch    56 | loss: 121.7158446CurrentTrain: epoch  1, batch    57 | loss: 74.9494882CurrentTrain: epoch  1, batch    58 | loss: 77.0584111CurrentTrain: epoch  1, batch    59 | loss: 122.0509344CurrentTrain: epoch  1, batch    60 | loss: 71.3767177CurrentTrain: epoch  1, batch    61 | loss: 73.6941656CurrentTrain: epoch  1, batch    62 | loss: 72.9583377CurrentTrain: epoch  1, batch    63 | loss: 70.0732788CurrentTrain: epoch  1, batch    64 | loss: 71.2061433CurrentTrain: epoch  1, batch    65 | loss: 67.9046330CurrentTrain: epoch  1, batch    66 | loss: 61.3127409CurrentTrain: epoch  1, batch    67 | loss: 89.5960829CurrentTrain: epoch  1, batch    68 | loss: 58.9590205CurrentTrain: epoch  1, batch    69 | loss: 123.8670279CurrentTrain: epoch  1, batch    70 | loss: 70.5006634CurrentTrain: epoch  1, batch    71 | loss: 91.9698457CurrentTrain: epoch  1, batch    72 | loss: 72.2553177CurrentTrain: epoch  1, batch    73 | loss: 87.0220703CurrentTrain: epoch  1, batch    74 | loss: 59.7514521CurrentTrain: epoch  1, batch    75 | loss: 59.4366971CurrentTrain: epoch  1, batch    76 | loss: 60.4822292CurrentTrain: epoch  1, batch    77 | loss: 59.1881208CurrentTrain: epoch  1, batch    78 | loss: 58.7153056CurrentTrain: epoch  1, batch    79 | loss: 69.5973309CurrentTrain: epoch  1, batch    80 | loss: 89.2719515CurrentTrain: epoch  1, batch    81 | loss: 57.1993892CurrentTrain: epoch  1, batch    82 | loss: 69.2844658CurrentTrain: epoch  1, batch    83 | loss: 92.6887510CurrentTrain: epoch  1, batch    84 | loss: 72.2964674CurrentTrain: epoch  1, batch    85 | loss: 89.3869491CurrentTrain: epoch  1, batch    86 | loss: 57.1983597CurrentTrain: epoch  1, batch    87 | loss: 60.1189806CurrentTrain: epoch  1, batch    88 | loss: 57.6571402CurrentTrain: epoch  1, batch    89 | loss: 55.8104163CurrentTrain: epoch  1, batch    90 | loss: 121.1491466CurrentTrain: epoch  1, batch    91 | loss: 91.3809986CurrentTrain: epoch  1, batch    92 | loss: 68.4433624CurrentTrain: epoch  1, batch    93 | loss: 70.6401832CurrentTrain: epoch  1, batch    94 | loss: 57.3099239CurrentTrain: epoch  1, batch    95 | loss: 43.2234308CurrentTrain: epoch  2, batch     0 | loss: 49.1940014CurrentTrain: epoch  2, batch     1 | loss: 184.5141673CurrentTrain: epoch  2, batch     2 | loss: 71.1653449CurrentTrain: epoch  2, batch     3 | loss: 87.2655771CurrentTrain: epoch  2, batch     4 | loss: 86.9263631CurrentTrain: epoch  2, batch     5 | loss: 48.6572066CurrentTrain: epoch  2, batch     6 | loss: 120.0107429CurrentTrain: epoch  2, batch     7 | loss: 117.2849453CurrentTrain: epoch  2, batch     8 | loss: 72.5637974CurrentTrain: epoch  2, batch     9 | loss: 71.2522417CurrentTrain: epoch  2, batch    10 | loss: 70.8356745CurrentTrain: epoch  2, batch    11 | loss: 88.2530143CurrentTrain: epoch  2, batch    12 | loss: 70.7089604CurrentTrain: epoch  2, batch    13 | loss: 67.9958582CurrentTrain: epoch  2, batch    14 | loss: 69.8330238CurrentTrain: epoch  2, batch    15 | loss: 72.0662535CurrentTrain: epoch  2, batch    16 | loss: 49.1772663CurrentTrain: epoch  2, batch    17 | loss: 85.9574959CurrentTrain: epoch  2, batch    18 | loss: 87.8085509CurrentTrain: epoch  2, batch    19 | loss: 57.2300154CurrentTrain: epoch  2, batch    20 | loss: 84.7419386CurrentTrain: epoch  2, batch    21 | loss: 69.5033568CurrentTrain: epoch  2, batch    22 | loss: 67.7549125CurrentTrain: epoch  2, batch    23 | loss: 57.9750163CurrentTrain: epoch  2, batch    24 | loss: 87.7175711CurrentTrain: epoch  2, batch    25 | loss: 68.0154309CurrentTrain: epoch  2, batch    26 | loss: 71.1547568CurrentTrain: epoch  2, batch    27 | loss: 72.2449189CurrentTrain: epoch  2, batch    28 | loss: 90.7107505CurrentTrain: epoch  2, batch    29 | loss: 68.3278444CurrentTrain: epoch  2, batch    30 | loss: 121.0448106CurrentTrain: epoch  2, batch    31 | loss: 119.6552501CurrentTrain: epoch  2, batch    32 | loss: 53.2368094CurrentTrain: epoch  2, batch    33 | loss: 85.8538585CurrentTrain: epoch  2, batch    34 | loss: 50.2961325CurrentTrain: epoch  2, batch    35 | loss: 70.3093485CurrentTrain: epoch  2, batch    36 | loss: 59.8584169CurrentTrain: epoch  2, batch    37 | loss: 67.4177224CurrentTrain: epoch  2, batch    38 | loss: 70.1824214CurrentTrain: epoch  2, batch    39 | loss: 75.3418189CurrentTrain: epoch  2, batch    40 | loss: 56.9060412CurrentTrain: epoch  2, batch    41 | loss: 58.7884988CurrentTrain: epoch  2, batch    42 | loss: 53.4965180CurrentTrain: epoch  2, batch    43 | loss: 90.8928897CurrentTrain: epoch  2, batch    44 | loss: 85.2381406CurrentTrain: epoch  2, batch    45 | loss: 59.7616672CurrentTrain: epoch  2, batch    46 | loss: 72.7521259CurrentTrain: epoch  2, batch    47 | loss: 87.3726716CurrentTrain: epoch  2, batch    48 | loss: 88.4015317CurrentTrain: epoch  2, batch    49 | loss: 121.6601907CurrentTrain: epoch  2, batch    50 | loss: 72.2641078CurrentTrain: epoch  2, batch    51 | loss: 69.9624621CurrentTrain: epoch  2, batch    52 | loss: 50.1167191CurrentTrain: epoch  2, batch    53 | loss: 58.6401849CurrentTrain: epoch  2, batch    54 | loss: 67.6892554CurrentTrain: epoch  2, batch    55 | loss: 186.5705653CurrentTrain: epoch  2, batch    56 | loss: 64.7919749CurrentTrain: epoch  2, batch    57 | loss: 54.3082956CurrentTrain: epoch  2, batch    58 | loss: 70.4911037CurrentTrain: epoch  2, batch    59 | loss: 70.3472420CurrentTrain: epoch  2, batch    60 | loss: 47.7152903CurrentTrain: epoch  2, batch    61 | loss: 49.6914938CurrentTrain: epoch  2, batch    62 | loss: 56.5536407CurrentTrain: epoch  2, batch    63 | loss: 69.5021970CurrentTrain: epoch  2, batch    64 | loss: 70.2829095CurrentTrain: epoch  2, batch    65 | loss: 56.4674343CurrentTrain: epoch  2, batch    66 | loss: 69.7445045CurrentTrain: epoch  2, batch    67 | loss: 56.1438756CurrentTrain: epoch  2, batch    68 | loss: 90.5277121CurrentTrain: epoch  2, batch    69 | loss: 119.1869630CurrentTrain: epoch  2, batch    70 | loss: 68.2654790CurrentTrain: epoch  2, batch    71 | loss: 90.4709474CurrentTrain: epoch  2, batch    72 | loss: 54.6758625CurrentTrain: epoch  2, batch    73 | loss: 89.1818443CurrentTrain: epoch  2, batch    74 | loss: 88.4645718CurrentTrain: epoch  2, batch    75 | loss: 83.1367453CurrentTrain: epoch  2, batch    76 | loss: 59.2484845CurrentTrain: epoch  2, batch    77 | loss: 119.1816067CurrentTrain: epoch  2, batch    78 | loss: 87.2558301CurrentTrain: epoch  2, batch    79 | loss: 58.1362487CurrentTrain: epoch  2, batch    80 | loss: 88.6699722CurrentTrain: epoch  2, batch    81 | loss: 56.8150199CurrentTrain: epoch  2, batch    82 | loss: 68.4670258CurrentTrain: epoch  2, batch    83 | loss: 70.7140201CurrentTrain: epoch  2, batch    84 | loss: 56.4251028CurrentTrain: epoch  2, batch    85 | loss: 43.9457001CurrentTrain: epoch  2, batch    86 | loss: 83.6168437CurrentTrain: epoch  2, batch    87 | loss: 87.1854892CurrentTrain: epoch  2, batch    88 | loss: 58.2120141CurrentTrain: epoch  2, batch    89 | loss: 57.0043271CurrentTrain: epoch  2, batch    90 | loss: 93.0292173CurrentTrain: epoch  2, batch    91 | loss: 180.8663872CurrentTrain: epoch  2, batch    92 | loss: 89.5818496CurrentTrain: epoch  2, batch    93 | loss: 73.5917182CurrentTrain: epoch  2, batch    94 | loss: 68.0799561CurrentTrain: epoch  2, batch    95 | loss: 60.3599689CurrentTrain: epoch  3, batch     0 | loss: 89.0705846CurrentTrain: epoch  3, batch     1 | loss: 116.5204528CurrentTrain: epoch  3, batch     2 | loss: 67.7723362CurrentTrain: epoch  3, batch     3 | loss: 85.0428130CurrentTrain: epoch  3, batch     4 | loss: 56.9754355CurrentTrain: epoch  3, batch     5 | loss: 53.1945374CurrentTrain: epoch  3, batch     6 | loss: 52.9127798CurrentTrain: epoch  3, batch     7 | loss: 69.5712545CurrentTrain: epoch  3, batch     8 | loss: 55.0313179CurrentTrain: epoch  3, batch     9 | loss: 57.8718070CurrentTrain: epoch  3, batch    10 | loss: 65.6275249CurrentTrain: epoch  3, batch    11 | loss: 56.3846342CurrentTrain: epoch  3, batch    12 | loss: 67.4808457CurrentTrain: epoch  3, batch    13 | loss: 84.9885968CurrentTrain: epoch  3, batch    14 | loss: 85.3891054CurrentTrain: epoch  3, batch    15 | loss: 69.3230435CurrentTrain: epoch  3, batch    16 | loss: 87.2770286CurrentTrain: epoch  3, batch    17 | loss: 118.8191417CurrentTrain: epoch  3, batch    18 | loss: 70.0584950CurrentTrain: epoch  3, batch    19 | loss: 89.7141359CurrentTrain: epoch  3, batch    20 | loss: 122.0622162CurrentTrain: epoch  3, batch    21 | loss: 69.9360319CurrentTrain: epoch  3, batch    22 | loss: 70.6092834CurrentTrain: epoch  3, batch    23 | loss: 66.7299083CurrentTrain: epoch  3, batch    24 | loss: 66.4446430CurrentTrain: epoch  3, batch    25 | loss: 67.7935956CurrentTrain: epoch  3, batch    26 | loss: 54.2956933CurrentTrain: epoch  3, batch    27 | loss: 68.5408612CurrentTrain: epoch  3, batch    28 | loss: 86.6977491CurrentTrain: epoch  3, batch    29 | loss: 66.5676345CurrentTrain: epoch  3, batch    30 | loss: 88.4819592CurrentTrain: epoch  3, batch    31 | loss: 44.4049232CurrentTrain: epoch  3, batch    32 | loss: 88.6812970CurrentTrain: epoch  3, batch    33 | loss: 69.5761616CurrentTrain: epoch  3, batch    34 | loss: 119.7477808CurrentTrain: epoch  3, batch    35 | loss: 86.0232700CurrentTrain: epoch  3, batch    36 | loss: 118.3287467CurrentTrain: epoch  3, batch    37 | loss: 86.2674768CurrentTrain: epoch  3, batch    38 | loss: 68.7450069CurrentTrain: epoch  3, batch    39 | loss: 86.0132524CurrentTrain: epoch  3, batch    40 | loss: 60.2372427CurrentTrain: epoch  3, batch    41 | loss: 65.1513428CurrentTrain: epoch  3, batch    42 | loss: 68.8857781CurrentTrain: epoch  3, batch    43 | loss: 85.4194975CurrentTrain: epoch  3, batch    44 | loss: 54.7740690CurrentTrain: epoch  3, batch    45 | loss: 172.8849388CurrentTrain: epoch  3, batch    46 | loss: 87.7165578CurrentTrain: epoch  3, batch    47 | loss: 68.3799052CurrentTrain: epoch  3, batch    48 | loss: 67.8025411CurrentTrain: epoch  3, batch    49 | loss: 66.6952894CurrentTrain: epoch  3, batch    50 | loss: 68.8912714CurrentTrain: epoch  3, batch    51 | loss: 119.3146734CurrentTrain: epoch  3, batch    52 | loss: 68.3704304CurrentTrain: epoch  3, batch    53 | loss: 55.9331221CurrentTrain: epoch  3, batch    54 | loss: 55.2382095CurrentTrain: epoch  3, batch    55 | loss: 49.8979006CurrentTrain: epoch  3, batch    56 | loss: 65.2561082CurrentTrain: epoch  3, batch    57 | loss: 67.8502309CurrentTrain: epoch  3, batch    58 | loss: 71.0418988CurrentTrain: epoch  3, batch    59 | loss: 85.5925794CurrentTrain: epoch  3, batch    60 | loss: 57.4178796CurrentTrain: epoch  3, batch    61 | loss: 68.8034403CurrentTrain: epoch  3, batch    62 | loss: 70.4419273CurrentTrain: epoch  3, batch    63 | loss: 56.2583715CurrentTrain: epoch  3, batch    64 | loss: 57.3308496CurrentTrain: epoch  3, batch    65 | loss: 89.3730530CurrentTrain: epoch  3, batch    66 | loss: 54.3579418CurrentTrain: epoch  3, batch    67 | loss: 119.0170822CurrentTrain: epoch  3, batch    68 | loss: 84.1779507CurrentTrain: epoch  3, batch    69 | loss: 83.0300574CurrentTrain: epoch  3, batch    70 | loss: 58.0324858CurrentTrain: epoch  3, batch    71 | loss: 80.1968718CurrentTrain: epoch  3, batch    72 | loss: 42.4254936CurrentTrain: epoch  3, batch    73 | loss: 67.0318788CurrentTrain: epoch  3, batch    74 | loss: 83.8126688CurrentTrain: epoch  3, batch    75 | loss: 57.3837436CurrentTrain: epoch  3, batch    76 | loss: 90.7718525CurrentTrain: epoch  3, batch    77 | loss: 85.2749627CurrentTrain: epoch  3, batch    78 | loss: 68.2396053CurrentTrain: epoch  3, batch    79 | loss: 55.3378172CurrentTrain: epoch  3, batch    80 | loss: 83.4304154CurrentTrain: epoch  3, batch    81 | loss: 64.0722188CurrentTrain: epoch  3, batch    82 | loss: 46.4782630CurrentTrain: epoch  3, batch    83 | loss: 86.0876790CurrentTrain: epoch  3, batch    84 | loss: 65.3319317CurrentTrain: epoch  3, batch    85 | loss: 114.1483868CurrentTrain: epoch  3, batch    86 | loss: 90.9570776CurrentTrain: epoch  3, batch    87 | loss: 49.8051877CurrentTrain: epoch  3, batch    88 | loss: 54.8787864CurrentTrain: epoch  3, batch    89 | loss: 71.2575263CurrentTrain: epoch  3, batch    90 | loss: 73.0549831CurrentTrain: epoch  3, batch    91 | loss: 56.2534947CurrentTrain: epoch  3, batch    92 | loss: 120.2881641CurrentTrain: epoch  3, batch    93 | loss: 72.6942117CurrentTrain: epoch  3, batch    94 | loss: 87.1342776CurrentTrain: epoch  3, batch    95 | loss: 74.2654532CurrentTrain: epoch  4, batch     0 | loss: 119.8902783CurrentTrain: epoch  4, batch     1 | loss: 65.2663105CurrentTrain: epoch  4, batch     2 | loss: 83.2532885CurrentTrain: epoch  4, batch     3 | loss: 87.9980281CurrentTrain: epoch  4, batch     4 | loss: 87.4264958CurrentTrain: epoch  4, batch     5 | loss: 89.0179478CurrentTrain: epoch  4, batch     6 | loss: 52.3741063CurrentTrain: epoch  4, batch     7 | loss: 56.8067734CurrentTrain: epoch  4, batch     8 | loss: 114.0380769CurrentTrain: epoch  4, batch     9 | loss: 70.1648617CurrentTrain: epoch  4, batch    10 | loss: 121.4960309CurrentTrain: epoch  4, batch    11 | loss: 66.4198759CurrentTrain: epoch  4, batch    12 | loss: 67.9292756CurrentTrain: epoch  4, batch    13 | loss: 65.0870810CurrentTrain: epoch  4, batch    14 | loss: 66.0969485CurrentTrain: epoch  4, batch    15 | loss: 68.0904592CurrentTrain: epoch  4, batch    16 | loss: 69.6631881CurrentTrain: epoch  4, batch    17 | loss: 69.6142068CurrentTrain: epoch  4, batch    18 | loss: 64.5468705CurrentTrain: epoch  4, batch    19 | loss: 67.4202212CurrentTrain: epoch  4, batch    20 | loss: 82.8860388CurrentTrain: epoch  4, batch    21 | loss: 56.6380627CurrentTrain: epoch  4, batch    22 | loss: 45.4613423CurrentTrain: epoch  4, batch    23 | loss: 44.9138074CurrentTrain: epoch  4, batch    24 | loss: 55.6350824CurrentTrain: epoch  4, batch    25 | loss: 89.1095147CurrentTrain: epoch  4, batch    26 | loss: 69.6842664CurrentTrain: epoch  4, batch    27 | loss: 82.2480796CurrentTrain: epoch  4, batch    28 | loss: 67.7379381CurrentTrain: epoch  4, batch    29 | loss: 65.0865818CurrentTrain: epoch  4, batch    30 | loss: 68.6573211CurrentTrain: epoch  4, batch    31 | loss: 85.2240705CurrentTrain: epoch  4, batch    32 | loss: 115.6319853CurrentTrain: epoch  4, batch    33 | loss: 68.2766000CurrentTrain: epoch  4, batch    34 | loss: 86.0180805CurrentTrain: epoch  4, batch    35 | loss: 55.2414815CurrentTrain: epoch  4, batch    36 | loss: 85.9919922CurrentTrain: epoch  4, batch    37 | loss: 67.1622830CurrentTrain: epoch  4, batch    38 | loss: 66.6269653CurrentTrain: epoch  4, batch    39 | loss: 52.8547499CurrentTrain: epoch  4, batch    40 | loss: 116.2709916CurrentTrain: epoch  4, batch    41 | loss: 86.5471722CurrentTrain: epoch  4, batch    42 | loss: 66.6484422CurrentTrain: epoch  4, batch    43 | loss: 52.9315139CurrentTrain: epoch  4, batch    44 | loss: 54.1843778CurrentTrain: epoch  4, batch    45 | loss: 64.3071700CurrentTrain: epoch  4, batch    46 | loss: 65.5667977CurrentTrain: epoch  4, batch    47 | loss: 52.9497796CurrentTrain: epoch  4, batch    48 | loss: 86.1399961CurrentTrain: epoch  4, batch    49 | loss: 68.1362728CurrentTrain: epoch  4, batch    50 | loss: 66.8020497CurrentTrain: epoch  4, batch    51 | loss: 54.9017891CurrentTrain: epoch  4, batch    52 | loss: 47.8964990CurrentTrain: epoch  4, batch    53 | loss: 84.0159767CurrentTrain: epoch  4, batch    54 | loss: 67.3599648CurrentTrain: epoch  4, batch    55 | loss: 88.4984454CurrentTrain: epoch  4, batch    56 | loss: 84.2686122CurrentTrain: epoch  4, batch    57 | loss: 119.6244871CurrentTrain: epoch  4, batch    58 | loss: 54.5705524CurrentTrain: epoch  4, batch    59 | loss: 66.0802424CurrentTrain: epoch  4, batch    60 | loss: 67.8690811CurrentTrain: epoch  4, batch    61 | loss: 63.7819318CurrentTrain: epoch  4, batch    62 | loss: 115.6157459CurrentTrain: epoch  4, batch    63 | loss: 63.9684806CurrentTrain: epoch  4, batch    64 | loss: 54.6644654CurrentTrain: epoch  4, batch    65 | loss: 70.8627451CurrentTrain: epoch  4, batch    66 | loss: 120.1769839CurrentTrain: epoch  4, batch    67 | loss: 66.9942240CurrentTrain: epoch  4, batch    68 | loss: 67.4506778CurrentTrain: epoch  4, batch    69 | loss: 76.8160588CurrentTrain: epoch  4, batch    70 | loss: 64.1655270CurrentTrain: epoch  4, batch    71 | loss: 46.6986016CurrentTrain: epoch  4, batch    72 | loss: 64.9902914CurrentTrain: epoch  4, batch    73 | loss: 60.0414951CurrentTrain: epoch  4, batch    74 | loss: 120.0290198CurrentTrain: epoch  4, batch    75 | loss: 69.5691985CurrentTrain: epoch  4, batch    76 | loss: 55.9614993CurrentTrain: epoch  4, batch    77 | loss: 82.5224006CurrentTrain: epoch  4, batch    78 | loss: 69.4036138CurrentTrain: epoch  4, batch    79 | loss: 43.8554556CurrentTrain: epoch  4, batch    80 | loss: 69.8225723CurrentTrain: epoch  4, batch    81 | loss: 87.2721182CurrentTrain: epoch  4, batch    82 | loss: 55.9653423CurrentTrain: epoch  4, batch    83 | loss: 43.6020028CurrentTrain: epoch  4, batch    84 | loss: 72.0094362CurrentTrain: epoch  4, batch    85 | loss: 45.6630396CurrentTrain: epoch  4, batch    86 | loss: 86.6247069CurrentTrain: epoch  4, batch    87 | loss: 64.9597504CurrentTrain: epoch  4, batch    88 | loss: 68.8889757CurrentTrain: epoch  4, batch    89 | loss: 52.3427894CurrentTrain: epoch  4, batch    90 | loss: 67.4720167CurrentTrain: epoch  4, batch    91 | loss: 84.0357572CurrentTrain: epoch  4, batch    92 | loss: 85.6103087CurrentTrain: epoch  4, batch    93 | loss: 55.2582448CurrentTrain: epoch  4, batch    94 | loss: 182.0554543CurrentTrain: epoch  4, batch    95 | loss: 36.7750829CurrentTrain: epoch  5, batch     0 | loss: 69.4866940CurrentTrain: epoch  5, batch     1 | loss: 52.7248326CurrentTrain: epoch  5, batch     2 | loss: 85.3928772CurrentTrain: epoch  5, batch     3 | loss: 82.4276441CurrentTrain: epoch  5, batch     4 | loss: 84.6561419CurrentTrain: epoch  5, batch     5 | loss: 54.9706496CurrentTrain: epoch  5, batch     6 | loss: 42.0316462CurrentTrain: epoch  5, batch     7 | loss: 68.2165079CurrentTrain: epoch  5, batch     8 | loss: 53.2080709CurrentTrain: epoch  5, batch     9 | loss: 87.8494766CurrentTrain: epoch  5, batch    10 | loss: 68.1826880CurrentTrain: epoch  5, batch    11 | loss: 86.4021803CurrentTrain: epoch  5, batch    12 | loss: 89.4464612CurrentTrain: epoch  5, batch    13 | loss: 85.0701804CurrentTrain: epoch  5, batch    14 | loss: 68.0800239CurrentTrain: epoch  5, batch    15 | loss: 84.8651233CurrentTrain: epoch  5, batch    16 | loss: 53.5440200CurrentTrain: epoch  5, batch    17 | loss: 70.9923726CurrentTrain: epoch  5, batch    18 | loss: 68.5329960CurrentTrain: epoch  5, batch    19 | loss: 87.0711161CurrentTrain: epoch  5, batch    20 | loss: 117.7920736CurrentTrain: epoch  5, batch    21 | loss: 61.9804103CurrentTrain: epoch  5, batch    22 | loss: 64.0809785CurrentTrain: epoch  5, batch    23 | loss: 54.5265807CurrentTrain: epoch  5, batch    24 | loss: 84.9991774CurrentTrain: epoch  5, batch    25 | loss: 53.1122870CurrentTrain: epoch  5, batch    26 | loss: 53.4950710CurrentTrain: epoch  5, batch    27 | loss: 84.2297607CurrentTrain: epoch  5, batch    28 | loss: 87.6992281CurrentTrain: epoch  5, batch    29 | loss: 70.4003483CurrentTrain: epoch  5, batch    30 | loss: 88.2982843CurrentTrain: epoch  5, batch    31 | loss: 88.7941085CurrentTrain: epoch  5, batch    32 | loss: 64.9366441CurrentTrain: epoch  5, batch    33 | loss: 53.7382032CurrentTrain: epoch  5, batch    34 | loss: 117.1714576CurrentTrain: epoch  5, batch    35 | loss: 44.3822457CurrentTrain: epoch  5, batch    36 | loss: 55.0892666CurrentTrain: epoch  5, batch    37 | loss: 53.9452697CurrentTrain: epoch  5, batch    38 | loss: 67.6066768CurrentTrain: epoch  5, batch    39 | loss: 86.7980238CurrentTrain: epoch  5, batch    40 | loss: 55.0047039CurrentTrain: epoch  5, batch    41 | loss: 62.4216719CurrentTrain: epoch  5, batch    42 | loss: 85.5957165CurrentTrain: epoch  5, batch    43 | loss: 66.5714971CurrentTrain: epoch  5, batch    44 | loss: 89.2252617CurrentTrain: epoch  5, batch    45 | loss: 56.5757732CurrentTrain: epoch  5, batch    46 | loss: 53.7889058CurrentTrain: epoch  5, batch    47 | loss: 115.8034058CurrentTrain: epoch  5, batch    48 | loss: 55.2421958CurrentTrain: epoch  5, batch    49 | loss: 45.7405465CurrentTrain: epoch  5, batch    50 | loss: 53.6355419CurrentTrain: epoch  5, batch    51 | loss: 55.1584802CurrentTrain: epoch  5, batch    52 | loss: 68.5312044CurrentTrain: epoch  5, batch    53 | loss: 65.4700287CurrentTrain: epoch  5, batch    54 | loss: 54.0838081CurrentTrain: epoch  5, batch    55 | loss: 52.6560998CurrentTrain: epoch  5, batch    56 | loss: 68.1991991CurrentTrain: epoch  5, batch    57 | loss: 86.3589457CurrentTrain: epoch  5, batch    58 | loss: 66.1862612CurrentTrain: epoch  5, batch    59 | loss: 85.5161534CurrentTrain: epoch  5, batch    60 | loss: 82.7118953CurrentTrain: epoch  5, batch    61 | loss: 45.8629529CurrentTrain: epoch  5, batch    62 | loss: 53.0419011CurrentTrain: epoch  5, batch    63 | loss: 84.7749246CurrentTrain: epoch  5, batch    64 | loss: 53.9433446CurrentTrain: epoch  5, batch    65 | loss: 69.2166525CurrentTrain: epoch  5, batch    66 | loss: 54.6051650CurrentTrain: epoch  5, batch    67 | loss: 83.3982157CurrentTrain: epoch  5, batch    68 | loss: 53.4399859CurrentTrain: epoch  5, batch    69 | loss: 48.3852315CurrentTrain: epoch  5, batch    70 | loss: 63.1906496CurrentTrain: epoch  5, batch    71 | loss: 112.6320249CurrentTrain: epoch  5, batch    72 | loss: 64.7005979CurrentTrain: epoch  5, batch    73 | loss: 55.7337707CurrentTrain: epoch  5, batch    74 | loss: 54.0319897CurrentTrain: epoch  5, batch    75 | loss: 65.2577269CurrentTrain: epoch  5, batch    76 | loss: 51.9362552CurrentTrain: epoch  5, batch    77 | loss: 84.5158648CurrentTrain: epoch  5, batch    78 | loss: 87.0637073CurrentTrain: epoch  5, batch    79 | loss: 51.4599339CurrentTrain: epoch  5, batch    80 | loss: 84.2583942CurrentTrain: epoch  5, batch    81 | loss: 81.7776311CurrentTrain: epoch  5, batch    82 | loss: 56.0015684CurrentTrain: epoch  5, batch    83 | loss: 85.0560530CurrentTrain: epoch  5, batch    84 | loss: 118.1688654CurrentTrain: epoch  5, batch    85 | loss: 80.4263006CurrentTrain: epoch  5, batch    86 | loss: 116.1868681CurrentTrain: epoch  5, batch    87 | loss: 64.8440554CurrentTrain: epoch  5, batch    88 | loss: 54.9783057CurrentTrain: epoch  5, batch    89 | loss: 53.6351642CurrentTrain: epoch  5, batch    90 | loss: 52.9759576CurrentTrain: epoch  5, batch    91 | loss: 79.6824866CurrentTrain: epoch  5, batch    92 | loss: 79.3630321CurrentTrain: epoch  5, batch    93 | loss: 79.3443359CurrentTrain: epoch  5, batch    94 | loss: 88.8524308CurrentTrain: epoch  5, batch    95 | loss: 71.9854655CurrentTrain: epoch  6, batch     0 | loss: 63.1706351CurrentTrain: epoch  6, batch     1 | loss: 86.1258562CurrentTrain: epoch  6, batch     2 | loss: 68.3315820CurrentTrain: epoch  6, batch     3 | loss: 83.7401339CurrentTrain: epoch  6, batch     4 | loss: 53.7558249CurrentTrain: epoch  6, batch     5 | loss: 53.9550318CurrentTrain: epoch  6, batch     6 | loss: 85.0363695CurrentTrain: epoch  6, batch     7 | loss: 58.8580415CurrentTrain: epoch  6, batch     8 | loss: 67.7805598CurrentTrain: epoch  6, batch     9 | loss: 83.1794662CurrentTrain: epoch  6, batch    10 | loss: 65.9916681CurrentTrain: epoch  6, batch    11 | loss: 48.3928173CurrentTrain: epoch  6, batch    12 | loss: 118.8002911CurrentTrain: epoch  6, batch    13 | loss: 63.3596001CurrentTrain: epoch  6, batch    14 | loss: 84.1554724CurrentTrain: epoch  6, batch    15 | loss: 119.4303464CurrentTrain: epoch  6, batch    16 | loss: 85.9973801CurrentTrain: epoch  6, batch    17 | loss: 86.1559277CurrentTrain: epoch  6, batch    18 | loss: 84.2825065CurrentTrain: epoch  6, batch    19 | loss: 66.7769485CurrentTrain: epoch  6, batch    20 | loss: 63.5933514CurrentTrain: epoch  6, batch    21 | loss: 84.5957327CurrentTrain: epoch  6, batch    22 | loss: 81.8521152CurrentTrain: epoch  6, batch    23 | loss: 85.3088122CurrentTrain: epoch  6, batch    24 | loss: 65.2324862CurrentTrain: epoch  6, batch    25 | loss: 44.8572464CurrentTrain: epoch  6, batch    26 | loss: 86.1901811CurrentTrain: epoch  6, batch    27 | loss: 66.8355732CurrentTrain: epoch  6, batch    28 | loss: 54.6909655CurrentTrain: epoch  6, batch    29 | loss: 114.9114638CurrentTrain: epoch  6, batch    30 | loss: 66.4363773CurrentTrain: epoch  6, batch    31 | loss: 53.7100943CurrentTrain: epoch  6, batch    32 | loss: 67.5361086CurrentTrain: epoch  6, batch    33 | loss: 53.7402931CurrentTrain: epoch  6, batch    34 | loss: 51.3602439CurrentTrain: epoch  6, batch    35 | loss: 79.9820115CurrentTrain: epoch  6, batch    36 | loss: 61.3894570CurrentTrain: epoch  6, batch    37 | loss: 115.7627655CurrentTrain: epoch  6, batch    38 | loss: 54.9397729CurrentTrain: epoch  6, batch    39 | loss: 83.9795135CurrentTrain: epoch  6, batch    40 | loss: 84.3466016CurrentTrain: epoch  6, batch    41 | loss: 84.7099036CurrentTrain: epoch  6, batch    42 | loss: 117.7289591CurrentTrain: epoch  6, batch    43 | loss: 50.6075751CurrentTrain: epoch  6, batch    44 | loss: 64.1635403CurrentTrain: epoch  6, batch    45 | loss: 86.7277883CurrentTrain: epoch  6, batch    46 | loss: 52.5525316CurrentTrain: epoch  6, batch    47 | loss: 47.1847759CurrentTrain: epoch  6, batch    48 | loss: 69.1338309CurrentTrain: epoch  6, batch    49 | loss: 47.4695656CurrentTrain: epoch  6, batch    50 | loss: 115.7096280CurrentTrain: epoch  6, batch    51 | loss: 68.1731989CurrentTrain: epoch  6, batch    52 | loss: 86.0583255CurrentTrain: epoch  6, batch    53 | loss: 66.5644688CurrentTrain: epoch  6, batch    54 | loss: 117.9898895CurrentTrain: epoch  6, batch    55 | loss: 64.2662280CurrentTrain: epoch  6, batch    56 | loss: 64.4634561CurrentTrain: epoch  6, batch    57 | loss: 53.0151386CurrentTrain: epoch  6, batch    58 | loss: 65.9053582CurrentTrain: epoch  6, batch    59 | loss: 117.9358182CurrentTrain: epoch  6, batch    60 | loss: 47.7619841CurrentTrain: epoch  6, batch    61 | loss: 51.4537103CurrentTrain: epoch  6, batch    62 | loss: 41.6957487CurrentTrain: epoch  6, batch    63 | loss: 51.6481702CurrentTrain: epoch  6, batch    64 | loss: 63.0695761CurrentTrain: epoch  6, batch    65 | loss: 82.9000717CurrentTrain: epoch  6, batch    66 | loss: 85.0204264CurrentTrain: epoch  6, batch    67 | loss: 82.9883680CurrentTrain: epoch  6, batch    68 | loss: 54.9889785CurrentTrain: epoch  6, batch    69 | loss: 43.6871918CurrentTrain: epoch  6, batch    70 | loss: 55.5506624CurrentTrain: epoch  6, batch    71 | loss: 51.8796089CurrentTrain: epoch  6, batch    72 | loss: 86.7103839CurrentTrain: epoch  6, batch    73 | loss: 52.0897869CurrentTrain: epoch  6, batch    74 | loss: 52.7714584CurrentTrain: epoch  6, batch    75 | loss: 52.9021395CurrentTrain: epoch  6, batch    76 | loss: 53.5056522CurrentTrain: epoch  6, batch    77 | loss: 65.4947816CurrentTrain: epoch  6, batch    78 | loss: 66.5147098CurrentTrain: epoch  6, batch    79 | loss: 83.5951014CurrentTrain: epoch  6, batch    80 | loss: 64.3638231CurrentTrain: epoch  6, batch    81 | loss: 53.5011154CurrentTrain: epoch  6, batch    82 | loss: 44.4904101CurrentTrain: epoch  6, batch    83 | loss: 68.6504871CurrentTrain: epoch  6, batch    84 | loss: 52.0076062CurrentTrain: epoch  6, batch    85 | loss: 54.3191692CurrentTrain: epoch  6, batch    86 | loss: 83.6736440CurrentTrain: epoch  6, batch    87 | loss: 63.3368898CurrentTrain: epoch  6, batch    88 | loss: 55.3815261CurrentTrain: epoch  6, batch    89 | loss: 82.4193936CurrentTrain: epoch  6, batch    90 | loss: 65.8812471CurrentTrain: epoch  6, batch    91 | loss: 52.8276568CurrentTrain: epoch  6, batch    92 | loss: 117.3290607CurrentTrain: epoch  6, batch    93 | loss: 82.0053363CurrentTrain: epoch  6, batch    94 | loss: 46.0272855CurrentTrain: epoch  6, batch    95 | loss: 56.5796056CurrentTrain: epoch  7, batch     0 | loss: 81.1846658CurrentTrain: epoch  7, batch     1 | loss: 52.6180659CurrentTrain: epoch  7, batch     2 | loss: 51.3594977CurrentTrain: epoch  7, batch     3 | loss: 66.2413706CurrentTrain: epoch  7, batch     4 | loss: 54.8880015CurrentTrain: epoch  7, batch     5 | loss: 64.2082320CurrentTrain: epoch  7, batch     6 | loss: 53.4514171CurrentTrain: epoch  7, batch     7 | loss: 83.7242614CurrentTrain: epoch  7, batch     8 | loss: 65.7542899CurrentTrain: epoch  7, batch     9 | loss: 66.7531053CurrentTrain: epoch  7, batch    10 | loss: 81.1586146CurrentTrain: epoch  7, batch    11 | loss: 120.0246466CurrentTrain: epoch  7, batch    12 | loss: 83.3683994CurrentTrain: epoch  7, batch    13 | loss: 81.4823458CurrentTrain: epoch  7, batch    14 | loss: 54.1023866CurrentTrain: epoch  7, batch    15 | loss: 85.9168869CurrentTrain: epoch  7, batch    16 | loss: 84.4180519CurrentTrain: epoch  7, batch    17 | loss: 42.4622953CurrentTrain: epoch  7, batch    18 | loss: 42.5348894CurrentTrain: epoch  7, batch    19 | loss: 53.8507806CurrentTrain: epoch  7, batch    20 | loss: 84.1754960CurrentTrain: epoch  7, batch    21 | loss: 51.3219382CurrentTrain: epoch  7, batch    22 | loss: 55.7746472CurrentTrain: epoch  7, batch    23 | loss: 86.3454246CurrentTrain: epoch  7, batch    24 | loss: 53.3188168CurrentTrain: epoch  7, batch    25 | loss: 81.8417522CurrentTrain: epoch  7, batch    26 | loss: 81.3583864CurrentTrain: epoch  7, batch    27 | loss: 45.9914018CurrentTrain: epoch  7, batch    28 | loss: 62.4963047CurrentTrain: epoch  7, batch    29 | loss: 114.6161908CurrentTrain: epoch  7, batch    30 | loss: 65.6586812CurrentTrain: epoch  7, batch    31 | loss: 81.0435986CurrentTrain: epoch  7, batch    32 | loss: 114.0955819CurrentTrain: epoch  7, batch    33 | loss: 64.7522312CurrentTrain: epoch  7, batch    34 | loss: 66.3880615CurrentTrain: epoch  7, batch    35 | loss: 66.6283575CurrentTrain: epoch  7, batch    36 | loss: 113.9283927CurrentTrain: epoch  7, batch    37 | loss: 88.2009540CurrentTrain: epoch  7, batch    38 | loss: 43.1609152CurrentTrain: epoch  7, batch    39 | loss: 51.5828775CurrentTrain: epoch  7, batch    40 | loss: 183.4978603CurrentTrain: epoch  7, batch    41 | loss: 181.9231989CurrentTrain: epoch  7, batch    42 | loss: 51.4540313CurrentTrain: epoch  7, batch    43 | loss: 66.2450523CurrentTrain: epoch  7, batch    44 | loss: 62.5311829CurrentTrain: epoch  7, batch    45 | loss: 65.9692804CurrentTrain: epoch  7, batch    46 | loss: 52.1598701CurrentTrain: epoch  7, batch    47 | loss: 110.6558041CurrentTrain: epoch  7, batch    48 | loss: 65.2223969CurrentTrain: epoch  7, batch    49 | loss: 52.1662622CurrentTrain: epoch  7, batch    50 | loss: 64.2997864CurrentTrain: epoch  7, batch    51 | loss: 53.9417952CurrentTrain: epoch  7, batch    52 | loss: 65.9220569CurrentTrain: epoch  7, batch    53 | loss: 65.7626181CurrentTrain: epoch  7, batch    54 | loss: 115.4916468CurrentTrain: epoch  7, batch    55 | loss: 64.5082908CurrentTrain: epoch  7, batch    56 | loss: 63.2000743CurrentTrain: epoch  7, batch    57 | loss: 64.8020975CurrentTrain: epoch  7, batch    58 | loss: 117.1471296CurrentTrain: epoch  7, batch    59 | loss: 116.4275228CurrentTrain: epoch  7, batch    60 | loss: 85.5331324CurrentTrain: epoch  7, batch    61 | loss: 65.7978032CurrentTrain: epoch  7, batch    62 | loss: 89.4732702CurrentTrain: epoch  7, batch    63 | loss: 65.6037464CurrentTrain: epoch  7, batch    64 | loss: 63.5159501CurrentTrain: epoch  7, batch    65 | loss: 51.9257727CurrentTrain: epoch  7, batch    66 | loss: 66.4573418CurrentTrain: epoch  7, batch    67 | loss: 51.5862119CurrentTrain: epoch  7, batch    68 | loss: 67.5596056CurrentTrain: epoch  7, batch    69 | loss: 53.5528468CurrentTrain: epoch  7, batch    70 | loss: 65.5694458CurrentTrain: epoch  7, batch    71 | loss: 82.8348328CurrentTrain: epoch  7, batch    72 | loss: 116.4450014CurrentTrain: epoch  7, batch    73 | loss: 62.5810470CurrentTrain: epoch  7, batch    74 | loss: 89.3401718CurrentTrain: epoch  7, batch    75 | loss: 53.5301302CurrentTrain: epoch  7, batch    76 | loss: 85.6597053CurrentTrain: epoch  7, batch    77 | loss: 55.0709964CurrentTrain: epoch  7, batch    78 | loss: 66.2444547CurrentTrain: epoch  7, batch    79 | loss: 52.4526977CurrentTrain: epoch  7, batch    80 | loss: 53.3648875CurrentTrain: epoch  7, batch    81 | loss: 61.6763798CurrentTrain: epoch  7, batch    82 | loss: 51.7391165CurrentTrain: epoch  7, batch    83 | loss: 51.2940983CurrentTrain: epoch  7, batch    84 | loss: 64.9921754CurrentTrain: epoch  7, batch    85 | loss: 82.2652371CurrentTrain: epoch  7, batch    86 | loss: 54.1194715CurrentTrain: epoch  7, batch    87 | loss: 51.4554028CurrentTrain: epoch  7, batch    88 | loss: 82.8017983CurrentTrain: epoch  7, batch    89 | loss: 45.3378409CurrentTrain: epoch  7, batch    90 | loss: 84.4374567CurrentTrain: epoch  7, batch    91 | loss: 64.1083285CurrentTrain: epoch  7, batch    92 | loss: 55.0757104CurrentTrain: epoch  7, batch    93 | loss: 44.8456823CurrentTrain: epoch  7, batch    94 | loss: 86.2792071CurrentTrain: epoch  7, batch    95 | loss: 53.8129599CurrentTrain: epoch  8, batch     0 | loss: 65.0194050CurrentTrain: epoch  8, batch     1 | loss: 83.6546618CurrentTrain: epoch  8, batch     2 | loss: 51.7978143CurrentTrain: epoch  8, batch     3 | loss: 49.2057786CurrentTrain: epoch  8, batch     4 | loss: 52.9779556CurrentTrain: epoch  8, batch     5 | loss: 64.0545805CurrentTrain: epoch  8, batch     6 | loss: 50.0625005CurrentTrain: epoch  8, batch     7 | loss: 113.5237538CurrentTrain: epoch  8, batch     8 | loss: 66.2787274CurrentTrain: epoch  8, batch     9 | loss: 66.1801446CurrentTrain: epoch  8, batch    10 | loss: 53.0497480CurrentTrain: epoch  8, batch    11 | loss: 52.8393994CurrentTrain: epoch  8, batch    12 | loss: 66.8608711CurrentTrain: epoch  8, batch    13 | loss: 63.7481628CurrentTrain: epoch  8, batch    14 | loss: 48.5708396CurrentTrain: epoch  8, batch    15 | loss: 66.2878068CurrentTrain: epoch  8, batch    16 | loss: 52.5837391CurrentTrain: epoch  8, batch    17 | loss: 87.7937025CurrentTrain: epoch  8, batch    18 | loss: 68.3865440CurrentTrain: epoch  8, batch    19 | loss: 84.1900926CurrentTrain: epoch  8, batch    20 | loss: 64.6276931CurrentTrain: epoch  8, batch    21 | loss: 50.2810080CurrentTrain: epoch  8, batch    22 | loss: 115.3660445CurrentTrain: epoch  8, batch    23 | loss: 63.1065365CurrentTrain: epoch  8, batch    24 | loss: 117.6141613CurrentTrain: epoch  8, batch    25 | loss: 45.5662626CurrentTrain: epoch  8, batch    26 | loss: 51.7109169CurrentTrain: epoch  8, batch    27 | loss: 79.6113649CurrentTrain: epoch  8, batch    28 | loss: 83.0102718CurrentTrain: epoch  8, batch    29 | loss: 86.1282244CurrentTrain: epoch  8, batch    30 | loss: 64.7002305CurrentTrain: epoch  8, batch    31 | loss: 65.4311839CurrentTrain: epoch  8, batch    32 | loss: 46.5623958CurrentTrain: epoch  8, batch    33 | loss: 55.0245680CurrentTrain: epoch  8, batch    34 | loss: 66.9691682CurrentTrain: epoch  8, batch    35 | loss: 66.3381296CurrentTrain: epoch  8, batch    36 | loss: 65.0505197CurrentTrain: epoch  8, batch    37 | loss: 117.6822009CurrentTrain: epoch  8, batch    38 | loss: 64.7060430CurrentTrain: epoch  8, batch    39 | loss: 65.5314425CurrentTrain: epoch  8, batch    40 | loss: 51.3539949CurrentTrain: epoch  8, batch    41 | loss: 67.2452157CurrentTrain: epoch  8, batch    42 | loss: 83.0310029CurrentTrain: epoch  8, batch    43 | loss: 116.0255384CurrentTrain: epoch  8, batch    44 | loss: 85.8782284CurrentTrain: epoch  8, batch    45 | loss: 115.6266343CurrentTrain: epoch  8, batch    46 | loss: 53.2112871CurrentTrain: epoch  8, batch    47 | loss: 85.3766140CurrentTrain: epoch  8, batch    48 | loss: 51.2917334CurrentTrain: epoch  8, batch    49 | loss: 87.4671483CurrentTrain: epoch  8, batch    50 | loss: 64.5974534CurrentTrain: epoch  8, batch    51 | loss: 54.1553058CurrentTrain: epoch  8, batch    52 | loss: 61.9443601CurrentTrain: epoch  8, batch    53 | loss: 51.8498766CurrentTrain: epoch  8, batch    54 | loss: 64.8293772CurrentTrain: epoch  8, batch    55 | loss: 66.0392826CurrentTrain: epoch  8, batch    56 | loss: 65.7070766CurrentTrain: epoch  8, batch    57 | loss: 81.8882695CurrentTrain: epoch  8, batch    58 | loss: 66.6397381CurrentTrain: epoch  8, batch    59 | loss: 64.9399834CurrentTrain: epoch  8, batch    60 | loss: 85.0714867CurrentTrain: epoch  8, batch    61 | loss: 61.7119337CurrentTrain: epoch  8, batch    62 | loss: 52.9961485CurrentTrain: epoch  8, batch    63 | loss: 81.6628784CurrentTrain: epoch  8, batch    64 | loss: 85.7906349CurrentTrain: epoch  8, batch    65 | loss: 63.2107965CurrentTrain: epoch  8, batch    66 | loss: 84.3803197CurrentTrain: epoch  8, batch    67 | loss: 64.4893349CurrentTrain: epoch  8, batch    68 | loss: 61.0085108CurrentTrain: epoch  8, batch    69 | loss: 54.9317814CurrentTrain: epoch  8, batch    70 | loss: 52.6632181CurrentTrain: epoch  8, batch    71 | loss: 50.0617675CurrentTrain: epoch  8, batch    72 | loss: 63.8519514CurrentTrain: epoch  8, batch    73 | loss: 66.5293302CurrentTrain: epoch  8, batch    74 | loss: 51.1769514CurrentTrain: epoch  8, batch    75 | loss: 79.9077074CurrentTrain: epoch  8, batch    76 | loss: 49.4828448CurrentTrain: epoch  8, batch    77 | loss: 87.8495545CurrentTrain: epoch  8, batch    78 | loss: 82.9833455CurrentTrain: epoch  8, batch    79 | loss: 44.5680257CurrentTrain: epoch  8, batch    80 | loss: 78.3143621CurrentTrain: epoch  8, batch    81 | loss: 111.3647199CurrentTrain: epoch  8, batch    82 | loss: 66.3232506CurrentTrain: epoch  8, batch    83 | loss: 85.1008904CurrentTrain: epoch  8, batch    84 | loss: 65.5659607CurrentTrain: epoch  8, batch    85 | loss: 82.9864510CurrentTrain: epoch  8, batch    86 | loss: 66.1469646CurrentTrain: epoch  8, batch    87 | loss: 84.0451729CurrentTrain: epoch  8, batch    88 | loss: 116.2559332CurrentTrain: epoch  8, batch    89 | loss: 64.0623841CurrentTrain: epoch  8, batch    90 | loss: 54.4109417CurrentTrain: epoch  8, batch    91 | loss: 85.8581200CurrentTrain: epoch  8, batch    92 | loss: 46.9283231CurrentTrain: epoch  8, batch    93 | loss: 65.5875808CurrentTrain: epoch  8, batch    94 | loss: 51.7215985CurrentTrain: epoch  8, batch    95 | loss: 55.1014473CurrentTrain: epoch  9, batch     0 | loss: 80.8218061CurrentTrain: epoch  9, batch     1 | loss: 66.8938305CurrentTrain: epoch  9, batch     2 | loss: 48.8370282CurrentTrain: epoch  9, batch     3 | loss: 66.6899704CurrentTrain: epoch  9, batch     4 | loss: 64.0445491CurrentTrain: epoch  9, batch     5 | loss: 64.6966937CurrentTrain: epoch  9, batch     6 | loss: 113.3261806CurrentTrain: epoch  9, batch     7 | loss: 51.3795999CurrentTrain: epoch  9, batch     8 | loss: 85.3305863CurrentTrain: epoch  9, batch     9 | loss: 84.5737746CurrentTrain: epoch  9, batch    10 | loss: 64.8533865CurrentTrain: epoch  9, batch    11 | loss: 66.9096608CurrentTrain: epoch  9, batch    12 | loss: 81.9087702CurrentTrain: epoch  9, batch    13 | loss: 65.2222897CurrentTrain: epoch  9, batch    14 | loss: 64.3031877CurrentTrain: epoch  9, batch    15 | loss: 82.4179161CurrentTrain: epoch  9, batch    16 | loss: 48.0515243CurrentTrain: epoch  9, batch    17 | loss: 63.3023753CurrentTrain: epoch  9, batch    18 | loss: 62.8350156CurrentTrain: epoch  9, batch    19 | loss: 65.5746061CurrentTrain: epoch  9, batch    20 | loss: 65.5138786CurrentTrain: epoch  9, batch    21 | loss: 63.5144029CurrentTrain: epoch  9, batch    22 | loss: 64.4390406CurrentTrain: epoch  9, batch    23 | loss: 50.0070672CurrentTrain: epoch  9, batch    24 | loss: 66.0170337CurrentTrain: epoch  9, batch    25 | loss: 111.4799188CurrentTrain: epoch  9, batch    26 | loss: 43.6135730CurrentTrain: epoch  9, batch    27 | loss: 56.0367782CurrentTrain: epoch  9, batch    28 | loss: 64.4693496CurrentTrain: epoch  9, batch    29 | loss: 62.3737417CurrentTrain: epoch  9, batch    30 | loss: 115.2488011CurrentTrain: epoch  9, batch    31 | loss: 64.5663610CurrentTrain: epoch  9, batch    32 | loss: 67.5150606CurrentTrain: epoch  9, batch    33 | loss: 65.4403171CurrentTrain: epoch  9, batch    34 | loss: 182.1943092CurrentTrain: epoch  9, batch    35 | loss: 64.9589904CurrentTrain: epoch  9, batch    36 | loss: 84.9673782CurrentTrain: epoch  9, batch    37 | loss: 47.2928820CurrentTrain: epoch  9, batch    38 | loss: 51.2991209CurrentTrain: epoch  9, batch    39 | loss: 84.5993821CurrentTrain: epoch  9, batch    40 | loss: 62.5515341CurrentTrain: epoch  9, batch    41 | loss: 51.4732643CurrentTrain: epoch  9, batch    42 | loss: 63.5049643CurrentTrain: epoch  9, batch    43 | loss: 54.1736557CurrentTrain: epoch  9, batch    44 | loss: 85.9393787CurrentTrain: epoch  9, batch    45 | loss: 84.1792329CurrentTrain: epoch  9, batch    46 | loss: 63.1524110CurrentTrain: epoch  9, batch    47 | loss: 82.9707778CurrentTrain: epoch  9, batch    48 | loss: 83.3875163CurrentTrain: epoch  9, batch    49 | loss: 54.2693559CurrentTrain: epoch  9, batch    50 | loss: 83.1811464CurrentTrain: epoch  9, batch    51 | loss: 52.0853901CurrentTrain: epoch  9, batch    52 | loss: 85.7899159CurrentTrain: epoch  9, batch    53 | loss: 54.2369900CurrentTrain: epoch  9, batch    54 | loss: 50.2618945CurrentTrain: epoch  9, batch    55 | loss: 64.3495350CurrentTrain: epoch  9, batch    56 | loss: 66.6816547CurrentTrain: epoch  9, batch    57 | loss: 53.9021867CurrentTrain: epoch  9, batch    58 | loss: 42.2376033CurrentTrain: epoch  9, batch    59 | loss: 82.0044267CurrentTrain: epoch  9, batch    60 | loss: 85.7578627CurrentTrain: epoch  9, batch    61 | loss: 115.1897461CurrentTrain: epoch  9, batch    62 | loss: 84.0666378CurrentTrain: epoch  9, batch    63 | loss: 81.1143660CurrentTrain: epoch  9, batch    64 | loss: 64.2257220CurrentTrain: epoch  9, batch    65 | loss: 51.0234471CurrentTrain: epoch  9, batch    66 | loss: 65.1467187CurrentTrain: epoch  9, batch    67 | loss: 66.6886978CurrentTrain: epoch  9, batch    68 | loss: 63.7812508CurrentTrain: epoch  9, batch    69 | loss: 69.0708292CurrentTrain: epoch  9, batch    70 | loss: 64.1894636CurrentTrain: epoch  9, batch    71 | loss: 51.6837022CurrentTrain: epoch  9, batch    72 | loss: 43.8387112CurrentTrain: epoch  9, batch    73 | loss: 64.1448688CurrentTrain: epoch  9, batch    74 | loss: 85.7931648CurrentTrain: epoch  9, batch    75 | loss: 53.0023209CurrentTrain: epoch  9, batch    76 | loss: 91.1360859CurrentTrain: epoch  9, batch    77 | loss: 51.5504080CurrentTrain: epoch  9, batch    78 | loss: 42.5030313CurrentTrain: epoch  9, batch    79 | loss: 65.2270127CurrentTrain: epoch  9, batch    80 | loss: 117.6950846CurrentTrain: epoch  9, batch    81 | loss: 66.7393679CurrentTrain: epoch  9, batch    82 | loss: 56.7718788CurrentTrain: epoch  9, batch    83 | loss: 82.8645253CurrentTrain: epoch  9, batch    84 | loss: 82.5660623CurrentTrain: epoch  9, batch    85 | loss: 50.1940315CurrentTrain: epoch  9, batch    86 | loss: 62.3240772CurrentTrain: epoch  9, batch    87 | loss: 85.3854650CurrentTrain: epoch  9, batch    88 | loss: 82.3645101CurrentTrain: epoch  9, batch    89 | loss: 63.9985748CurrentTrain: epoch  9, batch    90 | loss: 61.9862022CurrentTrain: epoch  9, batch    91 | loss: 53.5325210CurrentTrain: epoch  9, batch    92 | loss: 65.3706786CurrentTrain: epoch  9, batch    93 | loss: 64.4644974CurrentTrain: epoch  9, batch    94 | loss: 82.5852741CurrentTrain: epoch  9, batch    95 | loss: 55.3119996

F1 score per class: {32: 0.6024096385542169, 6: 0.9010989010989011, 19: 0.4166666666666667, 24: 0.7634408602150538, 26: 0.9361702127659575, 29: 0.8686868686868687}
Micro-average F1 score: 0.809322033898305
Weighted-average F1 score: 0.819998478563146
F1 score per class: {32: 0.6984126984126984, 6: 0.9587628865979382, 19: 0.6451612903225806, 24: 0.7513812154696132, 26: 0.9795918367346939, 29: 0.8780487804878049}
Micro-average F1 score: 0.8493975903614458
Weighted-average F1 score: 0.8530697803429261
F1 score per class: {32: 0.6914893617021277, 6: 0.9587628865979382, 19: 0.6451612903225806, 24: 0.7472527472527473, 26: 0.9795918367346939, 29: 0.8780487804878049}
Micro-average F1 score: 0.8473895582329317
Weighted-average F1 score: 0.8511977730055698

F1 score per class: {32: 0.6024096385542169, 6: 0.9010989010989011, 19: 0.4166666666666667, 24: 0.7634408602150538, 26: 0.9361702127659575, 29: 0.8686868686868687}
Micro-average F1 score: 0.809322033898305
Weighted-average F1 score: 0.819998478563146
F1 score per class: {32: 0.6984126984126984, 6: 0.9587628865979382, 19: 0.6451612903225806, 24: 0.7513812154696132, 26: 0.9795918367346939, 29: 0.8780487804878049}
Micro-average F1 score: 0.8493975903614458
Weighted-average F1 score: 0.8530697803429261
F1 score per class: {32: 0.6914893617021277, 6: 0.9587628865979382, 19: 0.6451612903225806, 24: 0.7472527472527473, 26: 0.9795918367346939, 29: 0.8780487804878049}
Micro-average F1 score: 0.8473895582329317
Weighted-average F1 score: 0.8511977730055698

F1 score per class: {32: 0.45248868778280543, 6: 0.6721311475409836, 19: 0.2, 24: 0.7064676616915423, 26: 0.8934010152284264, 29: 0.7818181818181819}
Micro-average F1 score: 0.6743159752868491
Weighted-average F1 score: 0.6662915944614732
F1 score per class: {32: 0.46808510638297873, 6: 0.58125, 19: 0.136986301369863, 24: 0.6903553299492385, 26: 0.8807339449541285, 29: 0.6716417910447762}
Micro-average F1 score: 0.5911949685534591
Weighted-average F1 score: 0.5633380387100004
F1 score per class: {32: 0.4659498207885305, 6: 0.58125, 19: 0.13793103448275862, 24: 0.6868686868686869, 26: 0.8807339449541285, 29: 0.6691449814126395}
Micro-average F1 score: 0.5906228131560531
Weighted-average F1 score: 0.5632519187452381

F1 score per class: {32: 0.45248868778280543, 6: 0.6721311475409836, 19: 0.2, 24: 0.7064676616915423, 26: 0.8934010152284264, 29: 0.7818181818181819}
Micro-average F1 score: 0.6743159752868491
Weighted-average F1 score: 0.6662915944614732
F1 score per class: {32: 0.46808510638297873, 6: 0.58125, 19: 0.136986301369863, 24: 0.6903553299492385, 26: 0.8807339449541285, 29: 0.6716417910447762}
Micro-average F1 score: 0.5911949685534591
Weighted-average F1 score: 0.5633380387100004
F1 score per class: {32: 0.4659498207885305, 6: 0.58125, 19: 0.13793103448275862, 24: 0.6868686868686869, 26: 0.8807339449541285, 29: 0.6691449814126395}
Micro-average F1 score: 0.5906228131560531
Weighted-average F1 score: 0.5632519187452381
cur_acc_wo_na:  ['0.8093']
his_acc_wo_na:  ['0.8093']
cur_acc des_wo_na:  ['0.8494']
his_acc des_wo_na:  ['0.8494']
cur_acc rrf_wo_na:  ['0.8474']
his_acc rrf_wo_na:  ['0.8474']
cur_acc_w_na:  ['0.6743']
his_acc_w_na:  ['0.6743']
cur_acc des_w_na:  ['0.5912']
his_acc des_w_na:  ['0.5912']
cur_acc rrf_w_na:  ['0.5906']
his_acc rrf_w_na:  ['0.5906']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 73.4831049CurrentTrain: epoch  0, batch     1 | loss: 61.2967203CurrentTrain: epoch  0, batch     2 | loss: 98.1385703CurrentTrain: epoch  0, batch     3 | loss: 258.9761968CurrentTrain: epoch  1, batch     0 | loss: 75.6700026CurrentTrain: epoch  1, batch     1 | loss: 70.9238854CurrentTrain: epoch  1, batch     2 | loss: 91.7762221CurrentTrain: epoch  1, batch     3 | loss: 49.1235792CurrentTrain: epoch  2, batch     0 | loss: 73.5272949CurrentTrain: epoch  2, batch     1 | loss: 69.2309966CurrentTrain: epoch  2, batch     2 | loss: 89.1376984CurrentTrain: epoch  2, batch     3 | loss: 50.7491375CurrentTrain: epoch  3, batch     0 | loss: 89.4333256CurrentTrain: epoch  3, batch     1 | loss: 64.5684765CurrentTrain: epoch  3, batch     2 | loss: 72.2528384CurrentTrain: epoch  3, batch     3 | loss: 62.6776300CurrentTrain: epoch  4, batch     0 | loss: 70.9365009CurrentTrain: epoch  4, batch     1 | loss: 68.9354665CurrentTrain: epoch  4, batch     2 | loss: 56.6162722CurrentTrain: epoch  4, batch     3 | loss: 38.2942902CurrentTrain: epoch  5, batch     0 | loss: 87.0299544CurrentTrain: epoch  5, batch     1 | loss: 55.4752351CurrentTrain: epoch  5, batch     2 | loss: 67.2850755CurrentTrain: epoch  5, batch     3 | loss: 57.9861150CurrentTrain: epoch  6, batch     0 | loss: 118.7576458CurrentTrain: epoch  6, batch     1 | loss: 55.2277151CurrentTrain: epoch  6, batch     2 | loss: 64.9689844CurrentTrain: epoch  6, batch     3 | loss: 36.7950230CurrentTrain: epoch  7, batch     0 | loss: 87.6068210CurrentTrain: epoch  7, batch     1 | loss: 53.3509653CurrentTrain: epoch  7, batch     2 | loss: 68.9384912CurrentTrain: epoch  7, batch     3 | loss: 42.4681356CurrentTrain: epoch  8, batch     0 | loss: 64.8575238CurrentTrain: epoch  8, batch     1 | loss: 53.6423984CurrentTrain: epoch  8, batch     2 | loss: 65.7572619CurrentTrain: epoch  8, batch     3 | loss: 56.5675372CurrentTrain: epoch  9, batch     0 | loss: 64.4529293CurrentTrain: epoch  9, batch     1 | loss: 51.2821940CurrentTrain: epoch  9, batch     2 | loss: 55.8146541CurrentTrain: epoch  9, batch     3 | loss: 57.1042926
MemoryTrain:  epoch  0, batch     0 | loss: 0.5484669MemoryTrain:  epoch  1, batch     0 | loss: 0.3198906MemoryTrain:  epoch  2, batch     0 | loss: 0.2815030MemoryTrain:  epoch  3, batch     0 | loss: 0.2063316MemoryTrain:  epoch  4, batch     0 | loss: 0.1411050MemoryTrain:  epoch  5, batch     0 | loss: 0.1144724MemoryTrain:  epoch  6, batch     0 | loss: 0.1075457MemoryTrain:  epoch  7, batch     0 | loss: 0.0859565MemoryTrain:  epoch  8, batch     0 | loss: 0.0701010MemoryTrain:  epoch  9, batch     0 | loss: 0.0591186

F1 score per class: {32: 0.8888888888888888, 35: 0.0, 37: 0.5555555555555556, 38: 0.0, 15: 0.0, 24: 0.7722772277227723, 25: 0.5365853658536586, 26: 0.4864864864864865}
Micro-average F1 score: 0.6261980830670927
Weighted-average F1 score: 0.6519583931742831
F1 score per class: {35: 0.8235294117647058, 37: 0.0, 38: 0.9052631578947369, 15: 0.0, 24: 0.9433962264150944, 25: 0.8598130841121495, 26: 0.7391304347826086}
Micro-average F1 score: 0.8693333333333333
Weighted-average F1 score: 0.863755039678246
F1 score per class: {35: 0.8235294117647058, 37: 0.0, 38: 0.8571428571428571, 15: 0.0, 24: 0.9433962264150944, 25: 0.8598130841121495, 26: 0.7391304347826086}
Micro-average F1 score: 0.8571428571428571
Weighted-average F1 score: 0.8517390149594855

F1 score per class: {32: 0.5962732919254659, 35: 0.8888888888888888, 37: 0.7341772151898734, 6: 0.18181818181818182, 38: 0.5555555555555556, 15: 0.7619047619047619, 19: 0.9637305699481865, 24: 0.8449197860962567, 25: 0.75, 26: 0.5116279069767442, 29: 0.4864864864864865}
Micro-average F1 score: 0.7334963325183375
Weighted-average F1 score: 0.76039298504491
F1 score per class: {32: 0.7653061224489796, 35: 0.8235294117647058, 37: 0.907103825136612, 6: 0.625, 38: 0.9052631578947369, 15: 0.7553191489361702, 19: 0.9746192893401016, 24: 0.89, 25: 0.9090909090909091, 26: 0.8440366972477065, 29: 0.723404255319149}
Micro-average F1 score: 0.8544395924308588
Weighted-average F1 score: 0.8564046194476419
F1 score per class: {32: 0.7628865979381443, 35: 0.8235294117647058, 37: 0.8888888888888888, 6: 0.4827586206896552, 38: 0.8571428571428571, 15: 0.7553191489361702, 19: 0.9641025641025641, 24: 0.89, 25: 0.9009009009009009, 26: 0.8288288288288288, 29: 0.723404255319149}
Micro-average F1 score: 0.8422597212032281
Weighted-average F1 score: 0.8454237691418044

F1 score per class: {32: 0.0, 35: 0.8888888888888888, 37: 0.0, 38: 0.0, 6: 0.5555555555555556, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.5, 26: 0.35772357723577236, 29: 0.35294117647058826}
Micro-average F1 score: 0.4
Weighted-average F1 score: 0.3507835800930776
F1 score per class: {32: 0.0, 35: 0.6363636363636364, 37: 0.0, 38: 0.0, 6: 0.8775510204081632, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.5524861878453039, 26: 0.4, 29: 0.3617021276595745}
Micro-average F1 score: 0.4244791666666667
Weighted-average F1 score: 0.3733672935106888
F1 score per class: {32: 0.0, 35: 0.7368421052631579, 37: 0.0, 38: 0.0, 6: 0.8387096774193549, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.5494505494505495, 26: 0.3948497854077253, 29: 0.4473684210526316}
Micro-average F1 score: 0.43983402489626555
Weighted-average F1 score: 0.3884662735357217

F1 score per class: {32: 0.4304932735426009, 35: 0.6956521739130435, 37: 0.5686274509803921, 6: 0.14285714285714285, 38: 0.5555555555555556, 15: 0.6792452830188679, 19: 0.8340807174887892, 24: 0.7488151658767772, 25: 0.3291139240506329, 26: 0.2682926829268293, 29: 0.2571428571428571}
Micro-average F1 score: 0.5398920215956808
Weighted-average F1 score: 0.5225470470544487
F1 score per class: {32: 0.43352601156069365, 35: 0.2978723404255319, 37: 0.515527950310559, 6: 0.16393442622950818, 38: 0.8775510204081632, 15: 0.6367713004484304, 19: 0.7741935483870968, 24: 0.7091633466135459, 25: 0.31645569620253167, 26: 0.24468085106382978, 29: 0.1596244131455399}
Micro-average F1 score: 0.45823575331772054
Weighted-average F1 score: 0.42334780954537304
F1 score per class: {32: 0.45121951219512196, 35: 0.3888888888888889, 37: 0.5369127516778524, 6: 0.1728395061728395, 38: 0.8387096774193549, 15: 0.6425339366515838, 19: 0.8, 24: 0.717741935483871, 25: 0.303951367781155, 26: 0.23469387755102042, 29: 0.2328767123287671}
Micro-average F1 score: 0.47694225176568344
Weighted-average F1 score: 0.44220532157478976
cur_acc_wo_na:  ['0.8093', '0.6262']
his_acc_wo_na:  ['0.8093', '0.7335']
cur_acc des_wo_na:  ['0.8494', '0.8693']
his_acc des_wo_na:  ['0.8494', '0.8544']
cur_acc rrf_wo_na:  ['0.8474', '0.8571']
his_acc rrf_wo_na:  ['0.8474', '0.8423']
cur_acc_w_na:  ['0.6743', '0.4000']
his_acc_w_na:  ['0.6743', '0.5399']
cur_acc des_w_na:  ['0.5912', '0.4245']
his_acc des_w_na:  ['0.5912', '0.4582']
cur_acc rrf_w_na:  ['0.5906', '0.4398']
his_acc rrf_w_na:  ['0.5906', '0.4769']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 68.1116149CurrentTrain: epoch  0, batch     1 | loss: 64.0020026CurrentTrain: epoch  0, batch     2 | loss: 79.1633514CurrentTrain: epoch  0, batch     3 | loss: 80.6287586CurrentTrain: epoch  1, batch     0 | loss: 72.5900600CurrentTrain: epoch  1, batch     1 | loss: 60.6273267CurrentTrain: epoch  1, batch     2 | loss: 94.9098904CurrentTrain: epoch  1, batch     3 | loss: 55.6439558CurrentTrain: epoch  2, batch     0 | loss: 58.2633679CurrentTrain: epoch  2, batch     1 | loss: 90.6538182CurrentTrain: epoch  2, batch     2 | loss: 58.4626126CurrentTrain: epoch  2, batch     3 | loss: 42.3032663CurrentTrain: epoch  3, batch     0 | loss: 71.5617483CurrentTrain: epoch  3, batch     1 | loss: 90.4501535CurrentTrain: epoch  3, batch     2 | loss: 58.3691878CurrentTrain: epoch  3, batch     3 | loss: 38.8952776CurrentTrain: epoch  4, batch     0 | loss: 54.7848212CurrentTrain: epoch  4, batch     1 | loss: 86.8772581CurrentTrain: epoch  4, batch     2 | loss: 55.3230443CurrentTrain: epoch  4, batch     3 | loss: 70.3529048CurrentTrain: epoch  5, batch     0 | loss: 55.0430357CurrentTrain: epoch  5, batch     1 | loss: 52.7010433CurrentTrain: epoch  5, batch     2 | loss: 87.3944072CurrentTrain: epoch  5, batch     3 | loss: 51.7802144CurrentTrain: epoch  6, batch     0 | loss: 51.9939964CurrentTrain: epoch  6, batch     1 | loss: 68.1731435CurrentTrain: epoch  6, batch     2 | loss: 65.3051275CurrentTrain: epoch  6, batch     3 | loss: 71.5956273CurrentTrain: epoch  7, batch     0 | loss: 62.9670664CurrentTrain: epoch  7, batch     1 | loss: 83.8769513CurrentTrain: epoch  7, batch     2 | loss: 85.2703216CurrentTrain: epoch  7, batch     3 | loss: 39.5550218CurrentTrain: epoch  8, batch     0 | loss: 52.5211898CurrentTrain: epoch  8, batch     1 | loss: 64.6880105CurrentTrain: epoch  8, batch     2 | loss: 67.2908881CurrentTrain: epoch  8, batch     3 | loss: 48.9474804CurrentTrain: epoch  9, batch     0 | loss: 62.5434579CurrentTrain: epoch  9, batch     1 | loss: 53.3519210CurrentTrain: epoch  9, batch     2 | loss: 83.1524694CurrentTrain: epoch  9, batch     3 | loss: 39.7442034
MemoryTrain:  epoch  0, batch     0 | loss: 0.4133379MemoryTrain:  epoch  1, batch     0 | loss: 0.3297388MemoryTrain:  epoch  2, batch     0 | loss: 0.3132830MemoryTrain:  epoch  3, batch     0 | loss: 0.2037511MemoryTrain:  epoch  4, batch     0 | loss: 0.1642432MemoryTrain:  epoch  5, batch     0 | loss: 0.1354892MemoryTrain:  epoch  6, batch     0 | loss: 0.1190124MemoryTrain:  epoch  7, batch     0 | loss: 0.1027377MemoryTrain:  epoch  8, batch     0 | loss: 0.1022137MemoryTrain:  epoch  9, batch     0 | loss: 0.0804739

F1 score per class: {33: 0.0, 35: 0.3469387755102041, 36: 0.875, 37: 0.0, 6: 0.0, 8: 0.8823529411764706, 20: 0.42857142857142855, 26: 0.0, 29: 0.5494505494505495, 30: 0.0}
Micro-average F1 score: 0.6
Weighted-average F1 score: 0.6505067093880601
F1 score per class: {33: 0.0, 35: 0.7819548872180451, 36: 0.0, 37: 0.8631578947368421, 6: 0.0, 38: 0.0, 8: 0.918918918918919, 15: 0.4, 20: 0.0, 26: 0.8333333333333334, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7874396135265701
Weighted-average F1 score: 0.7606102096179749
F1 score per class: {33: 0.0, 35: 0.7633587786259542, 36: 0.8631578947368421, 37: 0.0, 6: 0.0, 8: 0.8947368421052632, 20: 0.375, 26: 0.0, 29: 0.8305084745762712, 30: 0.0}
Micro-average F1 score: 0.7804878048780488
Weighted-average F1 score: 0.7576472251032086

F1 score per class: {32: 0.4689655172413793, 33: 0.3333333333333333, 35: 0.8888888888888888, 36: 0.7421383647798742, 37: 0.6885245901639344, 6: 0.10526315789473684, 38: 0.4, 8: 0.7619047619047619, 15: 0.9197860962566845, 19: 0.8333333333333334, 20: 0.8387096774193549, 24: 0.3157894736842105, 25: 0.7526881720430108, 26: 0.5376344086021505, 29: 0.41025641025641024, 30: 0.25}
Micro-average F1 score: 0.6584575502268308
Weighted-average F1 score: 0.7121285577382261
F1 score per class: {32: 0.6885245901639344, 33: 0.7074829931972789, 35: 0.7777777777777778, 36: 0.8636363636363636, 37: 0.656, 6: 0.6, 38: 0.5352112676056338, 8: 0.7486631016042781, 15: 0.9489795918367347, 19: 0.723404255319149, 20: 0.8944723618090452, 24: 0.3, 25: 0.8727272727272727, 26: 0.684931506849315, 29: 0.4878048780487805, 30: 0.6511627906976745}
Micro-average F1 score: 0.7539325842696629
Weighted-average F1 score: 0.7640643505036661
F1 score per class: {32: 0.6850828729281768, 33: 0.6802721088435374, 35: 0.8235294117647058, 36: 0.8651685393258427, 37: 0.6456692913385826, 6: 0.38461538461538464, 38: 0.4927536231884058, 8: 0.7486631016042781, 15: 0.9312169312169312, 19: 0.7083333333333334, 20: 0.8944723618090452, 24: 0.23076923076923078, 25: 0.8727272727272727, 26: 0.6950354609929078, 29: 0.5176470588235295, 30: 0.4444444444444444}
Micro-average F1 score: 0.739524348810872
Weighted-average F1 score: 0.7511132146276565

F1 score per class: {32: 0.0, 33: 0.3008849557522124, 35: 0.0, 36: 0.0, 37: 0.60431654676259, 6: 0.0, 38: 0.0, 8: 0.8823529411764706, 15: 0.0, 19: 0.2222222222222222, 20: 0.0, 26: 0.46296296296296297, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.390057361376673
Weighted-average F1 score: 0.33298123005663705
F1 score per class: {32: 0.0, 33: 0.5252525252525253, 35: 0.0, 36: 0.0, 37: 0.5, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.6538461538461539, 19: 0.0, 20: 0.2, 24: 0.0, 26: 0.49261083743842365, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.35243243243243244
Weighted-average F1 score: 0.30161261276733803
F1 score per class: {32: 0.0, 33: 0.5347593582887701, 35: 0.0, 36: 0.0, 37: 0.49696969696969695, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.5230769230769231, 19: 0.0, 20: 0.16216216216216217, 24: 0.0, 26: 0.48756218905472637, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.3615819209039548
Weighted-average F1 score: 0.31490333110456986

F1 score per class: {32: 0.3076923076923077, 33: 0.24817518248175183, 35: 0.6666666666666666, 36: 0.5363636363636364, 37: 0.30656934306569344, 6: 0.08695652173913043, 38: 0.3939393939393939, 8: 0.64, 15: 0.8229665071770335, 19: 0.75, 20: 0.7358490566037735, 24: 0.12, 25: 0.33816425120772947, 26: 0.43103448275862066, 29: 0.36363636363636365, 30: 0.20512820512820512}
Micro-average F1 score: 0.4723384472338447
Weighted-average F1 score: 0.4642071258380527
F1 score per class: {32: 0.3727810650887574, 33: 0.30678466076696165, 35: 0.25925925925925924, 36: 0.5066666666666667, 37: 0.2611464968152866, 6: 0.18181818181818182, 38: 0.4935064935064935, 8: 0.5785123966942148, 15: 0.7018867924528301, 19: 0.2537313432835821, 20: 0.7063492063492064, 24: 0.08695652173913043, 25: 0.2696629213483146, 26: 0.33003300330033003, 29: 0.2597402597402597, 30: 0.22950819672131148}
Micro-average F1 score: 0.3926272674078408
Weighted-average F1 score: 0.37341094482832615
F1 score per class: {32: 0.389937106918239, 33: 0.31645569620253167, 35: 0.30434782608695654, 36: 0.506578947368421, 37: 0.25705329153605017, 6: 0.18181818181818182, 38: 0.4657534246575342, 8: 0.5882352941176471, 15: 0.7892376681614349, 19: 0.2361111111111111, 20: 0.7091633466135459, 24: 0.06382978723404255, 25: 0.2659279778393352, 26: 0.3333333333333333, 29: 0.28205128205128205, 30: 0.2}
Micro-average F1 score: 0.3991442542787286
Weighted-average F1 score: 0.37655553331927744
cur_acc_wo_na:  ['0.8093', '0.6262', '0.6000']
his_acc_wo_na:  ['0.8093', '0.7335', '0.6585']
cur_acc des_wo_na:  ['0.8494', '0.8693', '0.7874']
his_acc des_wo_na:  ['0.8494', '0.8544', '0.7539']
cur_acc rrf_wo_na:  ['0.8474', '0.8571', '0.7805']
his_acc rrf_wo_na:  ['0.8474', '0.8423', '0.7395']
cur_acc_w_na:  ['0.6743', '0.4000', '0.3901']
his_acc_w_na:  ['0.6743', '0.5399', '0.4723']
cur_acc des_w_na:  ['0.5912', '0.4245', '0.3524']
his_acc des_w_na:  ['0.5912', '0.4582', '0.3926']
cur_acc rrf_w_na:  ['0.5906', '0.4398', '0.3616']
his_acc rrf_w_na:  ['0.5906', '0.4769', '0.3991']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 68.9130421CurrentTrain: epoch  0, batch     1 | loss: 77.3582240CurrentTrain: epoch  0, batch     2 | loss: 97.4369139CurrentTrain: epoch  0, batch     3 | loss: 129.1089654CurrentTrain: epoch  0, batch     4 | loss: 58.8541941CurrentTrain: epoch  1, batch     0 | loss: 78.1628583CurrentTrain: epoch  1, batch     1 | loss: 95.4430467CurrentTrain: epoch  1, batch     2 | loss: 70.3476088CurrentTrain: epoch  1, batch     3 | loss: 90.2566337CurrentTrain: epoch  1, batch     4 | loss: 41.9298378CurrentTrain: epoch  2, batch     0 | loss: 73.8027024CurrentTrain: epoch  2, batch     1 | loss: 120.8562052CurrentTrain: epoch  2, batch     2 | loss: 73.5181739CurrentTrain: epoch  2, batch     3 | loss: 70.9633995CurrentTrain: epoch  2, batch     4 | loss: 51.0347826CurrentTrain: epoch  3, batch     0 | loss: 68.7121183CurrentTrain: epoch  3, batch     1 | loss: 88.2742980CurrentTrain: epoch  3, batch     2 | loss: 69.0529494CurrentTrain: epoch  3, batch     3 | loss: 70.6753402CurrentTrain: epoch  3, batch     4 | loss: 50.1213601CurrentTrain: epoch  4, batch     0 | loss: 70.5793586CurrentTrain: epoch  4, batch     1 | loss: 87.5435472CurrentTrain: epoch  4, batch     2 | loss: 67.3286473CurrentTrain: epoch  4, batch     3 | loss: 70.1922625CurrentTrain: epoch  4, batch     4 | loss: 97.2563654CurrentTrain: epoch  5, batch     0 | loss: 87.7529602CurrentTrain: epoch  5, batch     1 | loss: 66.0317230CurrentTrain: epoch  5, batch     2 | loss: 70.1529468CurrentTrain: epoch  5, batch     3 | loss: 87.0787755CurrentTrain: epoch  5, batch     4 | loss: 38.4472260CurrentTrain: epoch  6, batch     0 | loss: 85.5833141CurrentTrain: epoch  6, batch     1 | loss: 119.2242632CurrentTrain: epoch  6, batch     2 | loss: 66.1104946CurrentTrain: epoch  6, batch     3 | loss: 53.8179649CurrentTrain: epoch  6, batch     4 | loss: 64.0443667CurrentTrain: epoch  7, batch     0 | loss: 66.1598352CurrentTrain: epoch  7, batch     1 | loss: 65.5026927CurrentTrain: epoch  7, batch     2 | loss: 67.1122652CurrentTrain: epoch  7, batch     3 | loss: 178.7664245CurrentTrain: epoch  7, batch     4 | loss: 63.4403400CurrentTrain: epoch  8, batch     0 | loss: 116.7966628CurrentTrain: epoch  8, batch     1 | loss: 54.9263781CurrentTrain: epoch  8, batch     2 | loss: 65.3584214CurrentTrain: epoch  8, batch     3 | loss: 53.2577587CurrentTrain: epoch  8, batch     4 | loss: 100.5908487CurrentTrain: epoch  9, batch     0 | loss: 64.7446781CurrentTrain: epoch  9, batch     1 | loss: 83.4889102CurrentTrain: epoch  9, batch     2 | loss: 66.3606339CurrentTrain: epoch  9, batch     3 | loss: 53.3553189CurrentTrain: epoch  9, batch     4 | loss: 103.0871017
MemoryTrain:  epoch  0, batch     0 | loss: 0.6154618MemoryTrain:  epoch  1, batch     0 | loss: 0.5401134MemoryTrain:  epoch  2, batch     0 | loss: 0.4136052MemoryTrain:  epoch  3, batch     0 | loss: 0.4159934MemoryTrain:  epoch  4, batch     0 | loss: 0.2560855MemoryTrain:  epoch  5, batch     0 | loss: 0.2159166MemoryTrain:  epoch  6, batch     0 | loss: 0.1924807MemoryTrain:  epoch  7, batch     0 | loss: 0.1775899MemoryTrain:  epoch  8, batch     0 | loss: 0.1490126MemoryTrain:  epoch  9, batch     0 | loss: 0.1122656

F1 score per class: {32: 0.39344262295081966, 1: 0.39215686274509803, 34: 0.19834710743801653, 3: 0.4888888888888889, 35: 0.0, 36: 0.0, 37: 0.0, 14: 0.3333333333333333, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.338409475465313
Weighted-average F1 score: 0.2863383829100268
F1 score per class: {32: 0.4067796610169492, 1: 0.6666666666666666, 34: 0.0, 3: 0.16161616161616163, 35: 0.7052023121387283, 37: 0.0, 36: 0.0, 8: 0.0, 14: 0.7058823529411765, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.488822652757079
Weighted-average F1 score: 0.4148508097644757
F1 score per class: {32: 0.4297520661157025, 1: 0.6218487394957983, 34: 0.0, 3: 0.16, 35: 0.7023809523809523, 37: 0.0, 36: 0.0, 8: 0.0, 14: 0.6419753086419753, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.46776611694152925
Weighted-average F1 score: 0.39084128160138815

F1 score per class: {1: 0.366412213740458, 3: 0.37037037037037035, 6: 0.4148148148148148, 8: 0.23157894736842105, 14: 0.18604651162790697, 15: 0.75, 19: 0.36065573770491804, 20: 0.6129032258064516, 22: 0.4852941176470588, 24: 0.0, 25: 0.4, 26: 0.7525773195876289, 29: 0.9130434782608695, 30: 0.9444444444444444, 32: 0.7630057803468208, 33: 0.35294117647058826, 34: 0.2268041237113402, 35: 0.3364485981308411, 36: 0.3956043956043956, 37: 0.2972972972972973, 38: 0.4}
Micro-average F1 score: 0.4932950191570881
Weighted-average F1 score: 0.5236115523212229
F1 score per class: {1: 0.36923076923076925, 3: 0.6074074074074074, 6: 0.6144578313253012, 8: 0.5757575757575758, 14: 0.1568627450980392, 15: 0.75, 19: 0.5815602836879432, 20: 0.6456692913385826, 22: 0.6931818181818182, 24: 0.11428571428571428, 25: 0.5142857142857142, 26: 0.7525773195876289, 29: 0.9319371727748691, 30: 0.8292682926829268, 32: 0.8176795580110497, 33: 0.3333333333333333, 34: 0.4411764705882353, 35: 0.5166666666666667, 36: 0.7107438016528925, 37: 0.3076923076923077, 38: 0.5853658536585366}
Micro-average F1 score: 0.6065989847715736
Weighted-average F1 score: 0.6064245578354702
F1 score per class: {1: 0.39097744360902253, 3: 0.5692307692307692, 6: 0.5641025641025641, 8: 0.528, 14: 0.1523809523809524, 15: 0.75, 19: 0.5571428571428572, 20: 0.6456692913385826, 22: 0.6900584795321637, 24: 0.06896551724137931, 25: 0.5142857142857142, 26: 0.7525773195876289, 29: 0.925531914893617, 30: 0.85, 32: 0.8241758241758241, 33: 0.3157894736842105, 34: 0.3851851851851852, 35: 0.5, 36: 0.6837606837606838, 37: 0.3225806451612903, 38: 0.5641025641025641}
Micro-average F1 score: 0.5913564398801883
Weighted-average F1 score: 0.5915789187732222

F1 score per class: {32: 0.1605351170568562, 1: 0.31746031746031744, 34: 0.0, 3: 0.10434782608695652, 35: 0.0, 36: 0.0, 37: 0.38372093023255816, 6: 0.0, 33: 0.0, 14: 0.0, 19: 0.0, 20: 0.0, 22: 0.2894736842105263, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.18050541516245489
Weighted-average F1 score: 0.13874770973271403
F1 score per class: {1: 0.15384615384615385, 3: 0.41836734693877553, 6: 0.0, 8: 0.0, 14: 0.07174887892376682, 15: 0.0, 19: 0.0, 20: 0.0, 22: 0.47470817120622566, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.4166666666666667, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.20372670807453416
Weighted-average F1 score: 0.16739442186363604
F1 score per class: {1: 0.16300940438871472, 3: 0.4088397790055249, 6: 0.0, 8: 0.0, 14: 0.07373271889400922, 15: 0.0, 19: 0.0, 20: 0.0, 22: 0.48360655737704916, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.37410071942446044, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.20077220077220076
Weighted-average F1 score: 0.16333204000905424

F1 score per class: {1: 0.1276595744680851, 3: 0.26666666666666666, 6: 0.2962962962962963, 8: 0.19298245614035087, 14: 0.07643312101910828, 15: 0.5454545454545454, 19: 0.2802547770700637, 20: 0.3206751054852321, 22: 0.36666666666666664, 24: 0.0, 25: 0.3939393939393939, 26: 0.6266094420600858, 29: 0.7636363636363637, 30: 0.8947368421052632, 32: 0.5617021276595745, 33: 0.15, 34: 0.13333333333333333, 35: 0.15384615384615385, 36: 0.32142857142857145, 37: 0.27848101265822783, 38: 0.23728813559322035}
Micro-average F1 score: 0.3176071538698736
Weighted-average F1 score: 0.2928085954778494
F1 score per class: {1: 0.11678832116788321, 3: 0.31297709923664124, 6: 0.3119266055045872, 8: 0.2593856655290102, 14: 0.04481792717086835, 15: 0.23076923076923078, 19: 0.3293172690763052, 20: 0.26973684210526316, 22: 0.41216216216216217, 24: 0.0547945205479452, 25: 0.48, 26: 0.5863453815261044, 29: 0.6846153846153846, 30: 0.5151515151515151, 32: 0.5085910652920962, 33: 0.11538461538461539, 34: 0.15625, 35: 0.19135802469135801, 36: 0.43, 37: 0.1686746987951807, 38: 0.13714285714285715}
Micro-average F1 score: 0.29469790382244143
Weighted-average F1 score: 0.27471090697614936
F1 score per class: {1: 0.12293144208037825, 3: 0.3070539419087137, 6: 0.3120567375886525, 8: 0.29333333333333333, 14: 0.04519774011299435, 15: 0.27906976744186046, 19: 0.3305084745762712, 20: 0.26537216828478966, 22: 0.43703703703703706, 24: 0.03225806451612903, 25: 0.48, 26: 0.6033057851239669, 29: 0.7699115044247787, 30: 0.5074626865671642, 32: 0.5494505494505495, 33: 0.09090909090909091, 34: 0.13231552162849872, 35: 0.1772853185595568, 36: 0.45977011494252873, 37: 0.18072289156626506, 38: 0.15602836879432624}
Micro-average F1 score: 0.29855260315402893
Weighted-average F1 score: 0.272615165855643
cur_acc_wo_na:  ['0.8093', '0.6262', '0.6000', '0.3384']
his_acc_wo_na:  ['0.8093', '0.7335', '0.6585', '0.4933']
cur_acc des_wo_na:  ['0.8494', '0.8693', '0.7874', '0.4888']
his_acc des_wo_na:  ['0.8494', '0.8544', '0.7539', '0.6066']
cur_acc rrf_wo_na:  ['0.8474', '0.8571', '0.7805', '0.4678']
his_acc rrf_wo_na:  ['0.8474', '0.8423', '0.7395', '0.5914']
cur_acc_w_na:  ['0.6743', '0.4000', '0.3901', '0.1805']
his_acc_w_na:  ['0.6743', '0.5399', '0.4723', '0.3176']
cur_acc des_w_na:  ['0.5912', '0.4245', '0.3524', '0.2037']
his_acc des_w_na:  ['0.5912', '0.4582', '0.3926', '0.2947']
cur_acc rrf_w_na:  ['0.5906', '0.4398', '0.3616', '0.2008']
his_acc rrf_w_na:  ['0.5906', '0.4769', '0.3991', '0.2986']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 80.0251828CurrentTrain: epoch  0, batch     1 | loss: 74.5698921CurrentTrain: epoch  0, batch     2 | loss: 92.8666505CurrentTrain: epoch  0, batch     3 | loss: 74.8311928CurrentTrain: epoch  0, batch     4 | loss: 73.6905816CurrentTrain: epoch  1, batch     0 | loss: 119.5586425CurrentTrain: epoch  1, batch     1 | loss: 88.4820103CurrentTrain: epoch  1, batch     2 | loss: 89.8318710CurrentTrain: epoch  1, batch     3 | loss: 59.6195424CurrentTrain: epoch  1, batch     4 | loss: 47.3806699CurrentTrain: epoch  2, batch     0 | loss: 68.7067445CurrentTrain: epoch  2, batch     1 | loss: 183.9229285CurrentTrain: epoch  2, batch     2 | loss: 57.6874641CurrentTrain: epoch  2, batch     3 | loss: 88.0817805CurrentTrain: epoch  2, batch     4 | loss: 36.2109092CurrentTrain: epoch  3, batch     0 | loss: 56.4737434CurrentTrain: epoch  3, batch     1 | loss: 182.1464501CurrentTrain: epoch  3, batch     2 | loss: 67.7352626CurrentTrain: epoch  3, batch     3 | loss: 54.9585778CurrentTrain: epoch  3, batch     4 | loss: 35.8020733CurrentTrain: epoch  4, batch     0 | loss: 66.1223802CurrentTrain: epoch  4, batch     1 | loss: 83.1971398CurrentTrain: epoch  4, batch     2 | loss: 120.2909568CurrentTrain: epoch  4, batch     3 | loss: 87.4061055CurrentTrain: epoch  4, batch     4 | loss: 70.0776636CurrentTrain: epoch  5, batch     0 | loss: 178.2838124CurrentTrain: epoch  5, batch     1 | loss: 69.0315546CurrentTrain: epoch  5, batch     2 | loss: 66.7590607CurrentTrain: epoch  5, batch     3 | loss: 85.9197512CurrentTrain: epoch  5, batch     4 | loss: 40.1469511CurrentTrain: epoch  6, batch     0 | loss: 64.9648474CurrentTrain: epoch  6, batch     1 | loss: 66.0832937CurrentTrain: epoch  6, batch     2 | loss: 85.6170035CurrentTrain: epoch  6, batch     3 | loss: 86.7261891CurrentTrain: epoch  6, batch     4 | loss: 74.1113400CurrentTrain: epoch  7, batch     0 | loss: 65.7311581CurrentTrain: epoch  7, batch     1 | loss: 64.9673837CurrentTrain: epoch  7, batch     2 | loss: 113.8954803CurrentTrain: epoch  7, batch     3 | loss: 113.9301620CurrentTrain: epoch  7, batch     4 | loss: 112.8134483CurrentTrain: epoch  8, batch     0 | loss: 86.3895282CurrentTrain: epoch  8, batch     1 | loss: 113.8480701CurrentTrain: epoch  8, batch     2 | loss: 52.0955668CurrentTrain: epoch  8, batch     3 | loss: 63.7558772CurrentTrain: epoch  8, batch     4 | loss: 112.4953940CurrentTrain: epoch  9, batch     0 | loss: 64.5519032CurrentTrain: epoch  9, batch     1 | loss: 80.3255301CurrentTrain: epoch  9, batch     2 | loss: 116.1159684CurrentTrain: epoch  9, batch     3 | loss: 64.1563500CurrentTrain: epoch  9, batch     4 | loss: 74.3073884
MemoryTrain:  epoch  0, batch     0 | loss: 0.3466199MemoryTrain:  epoch  1, batch     0 | loss: 0.3992942MemoryTrain:  epoch  2, batch     0 | loss: 0.2930757MemoryTrain:  epoch  3, batch     0 | loss: 0.2072981MemoryTrain:  epoch  4, batch     0 | loss: 0.1826172MemoryTrain:  epoch  5, batch     0 | loss: 0.1408925MemoryTrain:  epoch  6, batch     0 | loss: 0.1208972MemoryTrain:  epoch  7, batch     0 | loss: 0.1010656MemoryTrain:  epoch  8, batch     0 | loss: 0.0812955MemoryTrain:  epoch  9, batch     0 | loss: 0.0823059

F1 score per class: {34: 0.9743589743589743, 5: 0.0, 38: 0.4375, 6: 0.7755102040816326, 10: 0.0, 16: 0.5490196078431373, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.7058823529411765
Weighted-average F1 score: 0.7664763310328766
F1 score per class: {34: 1.0, 5: 0.0, 38: 0.0, 6: 0.6923076923076923, 8: 0.9473684210526315, 10: 0.36363636363636365, 16: 0.8064516129032258, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.8015414258188824
Weighted-average F1 score: 0.7622911159979638
F1 score per class: {34: 1.0, 5: 0.0, 38: 0.0, 6: 0.725, 8: 0.9473684210526315, 10: 0.36363636363636365, 16: 0.8253968253968254, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.8255813953488372
Weighted-average F1 score: 0.7969439064833802

F1 score per class: {1: 0.3125, 3: 0.3106796116504854, 5: 0.9090909090909091, 6: 0.366412213740458, 8: 0.1348314606741573, 10: 0.40875912408759124, 14: 0.1935483870967742, 15: 0.8235294117647058, 16: 0.7450980392156863, 17: 0.0, 18: 0.3218390804597701, 19: 0.34710743801652894, 20: 0.6722689075630253, 22: 0.45454545454545453, 24: 0.19047619047619047, 25: 0.4, 26: 0.7553191489361702, 29: 0.907103825136612, 30: 0.9444444444444444, 32: 0.8021978021978022, 33: 0.3333333333333333, 34: 0.0821917808219178, 35: 0.057971014492753624, 36: 0.21621621621621623, 37: 0.09375, 38: 0.24242424242424243}
Micro-average F1 score: 0.49556093623890235
Weighted-average F1 score: 0.5632570169342144
F1 score per class: {1: 0.373134328358209, 3: 0.6222222222222222, 5: 0.8928571428571429, 6: 0.5679012345679012, 8: 0.46153846153846156, 10: 0.6390532544378699, 14: 0.15384615384615385, 15: 0.75, 16: 0.8709677419354839, 17: 0.12121212121212122, 18: 0.3448275862068966, 19: 0.6394557823129252, 20: 0.7358490566037735, 22: 0.5935483870967742, 24: 0.0625, 25: 0.5405405405405406, 26: 0.7634408602150538, 29: 0.9424083769633508, 30: 0.8780487804878049, 32: 0.8324324324324325, 33: 0.3157894736842105, 34: 0.15730337078651685, 35: 0.5223880597014925, 36: 0.4827586206896552, 37: 0.22535211267605634, 38: 0.3684210526315789}
Micro-average F1 score: 0.594031922276197
Weighted-average F1 score: 0.5962912806075096
F1 score per class: {1: 0.36764705882352944, 3: 0.5625, 5: 0.9049773755656109, 6: 0.5679012345679012, 8: 0.42016806722689076, 10: 0.6628571428571428, 14: 0.176, 15: 0.75, 16: 0.8852459016393442, 17: 0.12121212121212122, 18: 0.35135135135135137, 19: 0.6351351351351351, 20: 0.7192982456140351, 22: 0.6040268456375839, 24: 0.07692307692307693, 25: 0.5142857142857142, 26: 0.7593582887700535, 29: 0.9247311827956989, 30: 0.8780487804878049, 32: 0.8449197860962567, 33: 0.2857142857142857, 34: 0.13953488372093023, 35: 0.5, 36: 0.42857142857142855, 37: 0.2, 38: 0.3333333333333333}
Micro-average F1 score: 0.5876612059951203
Weighted-average F1 score: 0.5927885405363877

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.7949790794979079, 6: 0.0, 8: 0.0, 10: 0.3783783783783784, 14: 0.0, 15: 0.0, 16: 0.4935064935064935, 17: 0.0, 18: 0.2978723404255319, 20: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 38: 0.0}
Micro-average F1 score: 0.4508670520231214
Weighted-average F1 score: 0.4058600548351728
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6042296072507553, 6: 0.0, 8: 0.0, 10: 0.5373134328358209, 14: 0.0, 15: 0.0, 16: 0.5567010309278351, 17: 0.3076923076923077, 18: 0.17482517482517482, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.33121019108280253
Weighted-average F1 score: 0.2815376559446378
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6309148264984227, 6: 0.0, 8: 0.0, 10: 0.5576923076923077, 14: 0.0, 15: 0.0, 16: 0.54, 17: 0.3076923076923077, 18: 0.17993079584775087, 19: 0.0, 20: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3506172839506173
Weighted-average F1 score: 0.29980044341355633

F1 score per class: {1: 0.11527377521613832, 3: 0.22695035460992907, 5: 0.6934306569343066, 6: 0.22748815165876776, 8: 0.10714285714285714, 10: 0.26046511627906976, 14: 0.08053691275167785, 15: 0.5833333333333334, 16: 0.4175824175824176, 17: 0.0, 18: 0.1686746987951807, 19: 0.27631578947368424, 20: 0.26490066225165565, 22: 0.3614457831325301, 24: 0.14285714285714285, 25: 0.3880597014925373, 26: 0.6200873362445415, 29: 0.7443946188340808, 30: 0.918918918918919, 32: 0.5615384615384615, 33: 0.17647058823529413, 34: 0.06521739130434782, 35: 0.031007751937984496, 36: 0.21333333333333335, 37: 0.09375, 38: 0.1951219512195122}
Micro-average F1 score: 0.32230971128608926
Weighted-average F1 score: 0.31447355306654534
F1 score per class: {1: 0.11494252873563218, 3: 0.3442622950819672, 5: 0.44642857142857145, 6: 0.2911392405063291, 8: 0.2830188679245283, 10: 0.26666666666666666, 14: 0.046511627906976744, 15: 0.2608695652173913, 16: 0.38571428571428573, 17: 0.07547169811320754, 18: 0.08503401360544217, 19: 0.32752613240418116, 20: 0.22941176470588234, 22: 0.4144144144144144, 24: 0.030303030303030304, 25: 0.47619047619047616, 26: 0.5819672131147541, 29: 0.6844106463878327, 30: 0.5373134328358209, 32: 0.4597014925373134, 33: 0.13953488372093023, 34: 0.09210526315789473, 35: 0.1437371663244353, 36: 0.32558139534883723, 37: 0.21052631578947367, 38: 0.15730337078651685}
Micro-average F1 score: 0.27801234166937316
Weighted-average F1 score: 0.2587303206855154
F1 score per class: {1: 0.11441647597254005, 3: 0.3157894736842105, 5: 0.4878048780487805, 6: 0.2939297124600639, 8: 0.26455026455026454, 10: 0.27951807228915665, 14: 0.05314009661835749, 15: 0.3157894736842105, 16: 0.375, 17: 0.0784313725490196, 18: 0.08783783783783784, 19: 0.32867132867132864, 20: 0.22282608695652173, 22: 0.4265402843601896, 24: 0.0392156862745098, 25: 0.48, 26: 0.5966386554621849, 29: 0.7350427350427351, 30: 0.5806451612903226, 32: 0.4773413897280967, 33: 0.12244897959183673, 34: 0.08450704225352113, 35: 0.14583333333333334, 36: 0.2975206611570248, 37: 0.1917808219178082, 38: 0.1935483870967742}
Micro-average F1 score: 0.2803458596607915
Weighted-average F1 score: 0.2588772902356471
cur_acc_wo_na:  ['0.8093', '0.6262', '0.6000', '0.3384', '0.7059']
his_acc_wo_na:  ['0.8093', '0.7335', '0.6585', '0.4933', '0.4956']
cur_acc des_wo_na:  ['0.8494', '0.8693', '0.7874', '0.4888', '0.8015']
his_acc des_wo_na:  ['0.8494', '0.8544', '0.7539', '0.6066', '0.5940']
cur_acc rrf_wo_na:  ['0.8474', '0.8571', '0.7805', '0.4678', '0.8256']
his_acc rrf_wo_na:  ['0.8474', '0.8423', '0.7395', '0.5914', '0.5877']
cur_acc_w_na:  ['0.6743', '0.4000', '0.3901', '0.1805', '0.4509']
his_acc_w_na:  ['0.6743', '0.5399', '0.4723', '0.3176', '0.3223']
cur_acc des_w_na:  ['0.5912', '0.4245', '0.3524', '0.2037', '0.3312']
his_acc des_w_na:  ['0.5912', '0.4582', '0.3926', '0.2947', '0.2780']
cur_acc rrf_w_na:  ['0.5906', '0.4398', '0.3616', '0.2008', '0.3506']
his_acc rrf_w_na:  ['0.5906', '0.4769', '0.3991', '0.2986', '0.2803']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 94.6999748CurrentTrain: epoch  0, batch     1 | loss: 85.0683052CurrentTrain: epoch  0, batch     2 | loss: 117.9833602CurrentTrain: epoch  0, batch     3 | loss: 89.4987268CurrentTrain: epoch  1, batch     0 | loss: 120.4790844CurrentTrain: epoch  1, batch     1 | loss: 64.2719541CurrentTrain: epoch  1, batch     2 | loss: 66.2632892CurrentTrain: epoch  1, batch     3 | loss: 59.1910264CurrentTrain: epoch  2, batch     0 | loss: 74.5068625CurrentTrain: epoch  2, batch     1 | loss: 87.7841879CurrentTrain: epoch  2, batch     2 | loss: 61.2170364CurrentTrain: epoch  2, batch     3 | loss: 56.2707946CurrentTrain: epoch  3, batch     0 | loss: 89.6589773CurrentTrain: epoch  3, batch     1 | loss: 70.5885616CurrentTrain: epoch  3, batch     2 | loss: 59.5362749CurrentTrain: epoch  3, batch     3 | loss: 69.1532950CurrentTrain: epoch  4, batch     0 | loss: 56.4031031CurrentTrain: epoch  4, batch     1 | loss: 66.1622703CurrentTrain: epoch  4, batch     2 | loss: 87.4806622CurrentTrain: epoch  4, batch     3 | loss: 57.3300625CurrentTrain: epoch  5, batch     0 | loss: 86.2018887CurrentTrain: epoch  5, batch     1 | loss: 118.1023681CurrentTrain: epoch  5, batch     2 | loss: 68.1544052CurrentTrain: epoch  5, batch     3 | loss: 40.8287289CurrentTrain: epoch  6, batch     0 | loss: 86.0524924CurrentTrain: epoch  6, batch     1 | loss: 67.0959843CurrentTrain: epoch  6, batch     2 | loss: 83.2795859CurrentTrain: epoch  6, batch     3 | loss: 53.0714717CurrentTrain: epoch  7, batch     0 | loss: 68.1179039CurrentTrain: epoch  7, batch     1 | loss: 64.4382546CurrentTrain: epoch  7, batch     2 | loss: 66.3439761CurrentTrain: epoch  7, batch     3 | loss: 55.3305073CurrentTrain: epoch  8, batch     0 | loss: 63.2352427CurrentTrain: epoch  8, batch     1 | loss: 116.3082048CurrentTrain: epoch  8, batch     2 | loss: 54.3875376CurrentTrain: epoch  8, batch     3 | loss: 53.4378996CurrentTrain: epoch  9, batch     0 | loss: 64.6851763CurrentTrain: epoch  9, batch     1 | loss: 61.0340442CurrentTrain: epoch  9, batch     2 | loss: 116.2128869CurrentTrain: epoch  9, batch     3 | loss: 68.9330260
MemoryTrain:  epoch  0, batch     0 | loss: 0.3451956MemoryTrain:  epoch  1, batch     0 | loss: 0.3397098MemoryTrain:  epoch  2, batch     0 | loss: 0.2373683MemoryTrain:  epoch  3, batch     0 | loss: 0.1869849MemoryTrain:  epoch  4, batch     0 | loss: 0.1498534MemoryTrain:  epoch  5, batch     0 | loss: 0.1343134MemoryTrain:  epoch  6, batch     0 | loss: 0.1132412MemoryTrain:  epoch  7, batch     0 | loss: 0.1054255MemoryTrain:  epoch  8, batch     0 | loss: 0.0809249MemoryTrain:  epoch  9, batch     0 | loss: 0.0737513

F1 score per class: {0: 0.9428571428571428, 32: 0.0, 1: 0.88268156424581, 4: 0.75, 13: 0.0, 20: 0.6222222222222222, 21: 0.0, 22: 0.7297297297297297, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7959183673469388
Weighted-average F1 score: 0.7645926924623169
F1 score per class: {0: 0.9722222222222222, 33: 0.0, 34: 0.9637305699481865, 32: 0.8888888888888888, 4: 0.0, 1: 0.0, 13: 0.7083333333333334, 14: 0.7631578947368421, 15: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8356807511737089
Weighted-average F1 score: 0.7834151551094648
F1 score per class: {0: 0.9722222222222222, 33: 0.9583333333333334, 34: 0.8888888888888888, 32: 0.0, 4: 0.0, 13: 0.7346938775510204, 14: 0.7466666666666667, 15: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8368794326241135
Weighted-average F1 score: 0.7875919037565765

F1 score per class: {0: 0.9428571428571428, 1: 0.25, 3: 0.6349206349206349, 4: 0.88268156424581, 5: 0.9090909090909091, 6: 0.3125, 8: 0.1348314606741573, 10: 0.1981981981981982, 13: 0.08450704225352113, 14: 0.029850746268656716, 15: 0.75, 16: 0.8235294117647058, 17: 0.15384615384615385, 18: 0.3023255813953488, 19: 0.4126984126984127, 20: 0.704, 21: 0.22764227642276422, 22: 0.42748091603053434, 23: 0.6923076923076923, 24: 0.0, 25: 0.4, 26: 0.7282051282051282, 29: 0.8901098901098901, 30: 0.9444444444444444, 32: 0.7657142857142857, 33: 0.35294117647058826, 34: 0.08450704225352113, 35: 0.22535211267605634, 36: 0.11428571428571428, 37: 0.11940298507462686, 38: 0.24242424242424243}
Micro-average F1 score: 0.5175588135015343
Weighted-average F1 score: 0.5695260136179521
F1 score per class: {0: 0.958904109589041, 1: 0.35384615384615387, 3: 0.8496732026143791, 4: 0.9637305699481865, 5: 0.9049773755656109, 6: 0.588957055214724, 8: 0.5882352941176471, 10: 0.45112781954887216, 13: 0.11594202898550725, 14: 0.0963855421686747, 15: 0.7058823529411765, 16: 0.9032258064516129, 17: 0.24, 18: 0.3466666666666667, 19: 0.7421383647798742, 20: 0.7735849056603774, 21: 0.26356589147286824, 22: 0.45454545454545453, 23: 0.725, 24: 0.09090909090909091, 25: 0.5789473684210527, 26: 0.6956521739130435, 29: 0.9484536082474226, 30: 0.9230769230769231, 32: 0.8324324324324325, 33: 0.3, 34: 0.19148936170212766, 35: 0.7238095238095238, 36: 0.4470588235294118, 37: 0.21333333333333335, 38: 0.3333333333333333}
Micro-average F1 score: 0.6241050119331742
Weighted-average F1 score: 0.6231906973776555
F1 score per class: {0: 0.958904109589041, 1: 0.35658914728682173, 3: 0.8187919463087249, 4: 0.9583333333333334, 5: 0.9216589861751152, 6: 0.5802469135802469, 8: 0.5426356589147286, 10: 0.38095238095238093, 13: 0.1, 14: 0.09876543209876543, 15: 0.6666666666666666, 16: 0.9032258064516129, 17: 0.16, 18: 0.33986928104575165, 19: 0.6967741935483871, 20: 0.7241379310344828, 21: 0.2727272727272727, 22: 0.44274809160305345, 23: 0.7088607594936709, 24: 0.1, 25: 0.5, 26: 0.6990291262135923, 29: 0.9312169312169312, 30: 0.9473684210526315, 32: 0.8324324324324325, 33: 0.25, 34: 0.1590909090909091, 35: 0.6728971962616822, 36: 0.3902439024390244, 37: 0.21621621621621623, 38: 0.34285714285714286}
Micro-average F1 score: 0.6055872634424753
Weighted-average F1 score: 0.6066062471238272

F1 score per class: {0: 0.8461538461538461, 1: 0.0, 3: 0.0, 4: 0.88268156424581, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.15384615384615385, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3076923076923077, 22: 0.0, 23: 0.6666666666666666, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.542608695652174
Weighted-average F1 score: 0.4247411989484537
F1 score per class: {0: 0.6306306306306306, 1: 0.0, 3: 0.0, 4: 0.8942307692307693, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.12698412698412698, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3177570093457944, 22: 0.0, 23: 0.6666666666666666, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.38907103825136613
Weighted-average F1 score: 0.2898199546555628
F1 score per class: {0: 0.6363636363636364, 1: 0.0, 3: 0.0, 4: 0.92, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.11764705882352941, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3302752293577982, 22: 0.0, 23: 0.6511627906976745, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.4013605442176871
Weighted-average F1 score: 0.29761801644644315

F1 score per class: {0: 0.39520958083832336, 1: 0.0967741935483871, 3: 0.4, 4: 0.8681318681318682, 5: 0.7983193277310925, 6: 0.19230769230769232, 8: 0.1016949152542373, 10: 0.16793893129770993, 13: 0.02181818181818182, 14: 0.023529411764705882, 15: 0.5217391304347826, 16: 0.3925233644859813, 17: 0.10810810810810811, 18: 0.12149532710280374, 19: 0.30952380952380953, 20: 0.27848101265822783, 21: 0.08211143695014662, 22: 0.3708609271523179, 23: 0.6, 24: 0.0, 25: 0.38235294117647056, 26: 0.6228070175438597, 29: 0.6952789699570815, 30: 0.9444444444444444, 32: 0.5491803278688525, 33: 0.21428571428571427, 34: 0.06666666666666667, 35: 0.10738255033557047, 36: 0.1111111111111111, 37: 0.11764705882352941, 38: 0.1951219512195122}
Micro-average F1 score: 0.32729624838292365
Weighted-average F1 score: 0.30244000745882416
F1 score per class: {0: 0.15384615384615385, 1: 0.115, 3: 0.3282828282828283, 4: 0.8773584905660378, 5: 0.45248868778280543, 6: 0.2727272727272727, 8: 0.2857142857142857, 10: 0.21818181818181817, 13: 0.020671834625323, 14: 0.03619909502262444, 15: 0.4, 16: 0.27722772277227725, 17: 0.10714285714285714, 18: 0.08150470219435736, 19: 0.3333333333333333, 20: 0.205, 21: 0.07692307692307693, 22: 0.37267080745341613, 23: 0.5, 24: 0.08695652173913043, 25: 0.5116279069767442, 26: 0.5353159851301115, 29: 0.6501766784452296, 30: 0.5714285714285714, 32: 0.4797507788161994, 33: 0.12244897959183673, 34: 0.1005586592178771, 35: 0.21529745042492918, 36: 0.30158730158730157, 37: 0.17582417582417584, 38: 0.1518987341772152}
Micro-average F1 score: 0.27024932179305
Weighted-average F1 score: 0.24610112982991494
F1 score per class: {0: 0.16018306636155608, 1: 0.12073490813648294, 3: 0.3436619718309859, 4: 0.9064039408866995, 5: 0.5063291139240507, 6: 0.2678062678062678, 8: 0.2788844621513944, 10: 0.2033898305084746, 13: 0.01904761904761905, 14: 0.039603960396039604, 15: 0.4, 16: 0.2828282828282828, 17: 0.09090909090909091, 18: 0.08099688473520249, 19: 0.3375, 20: 0.19444444444444445, 21: 0.07792207792207792, 22: 0.3625, 23: 0.5333333333333333, 24: 0.09523809523809523, 25: 0.45, 26: 0.5538461538461539, 29: 0.6795366795366795, 30: 0.6, 32: 0.49201277955271566, 33: 0.1016949152542373, 34: 0.08383233532934131, 35: 0.20454545454545456, 36: 0.2807017543859649, 37: 0.1797752808988764, 38: 0.20689655172413793}
Micro-average F1 score: 0.2703862660944206
Weighted-average F1 score: 0.24324054677909585
cur_acc_wo_na:  ['0.8093', '0.6262', '0.6000', '0.3384', '0.7059', '0.7959']
his_acc_wo_na:  ['0.8093', '0.7335', '0.6585', '0.4933', '0.4956', '0.5176']
cur_acc des_wo_na:  ['0.8494', '0.8693', '0.7874', '0.4888', '0.8015', '0.8357']
his_acc des_wo_na:  ['0.8494', '0.8544', '0.7539', '0.6066', '0.5940', '0.6241']
cur_acc rrf_wo_na:  ['0.8474', '0.8571', '0.7805', '0.4678', '0.8256', '0.8369']
his_acc rrf_wo_na:  ['0.8474', '0.8423', '0.7395', '0.5914', '0.5877', '0.6056']
cur_acc_w_na:  ['0.6743', '0.4000', '0.3901', '0.1805', '0.4509', '0.5426']
his_acc_w_na:  ['0.6743', '0.5399', '0.4723', '0.3176', '0.3223', '0.3273']
cur_acc des_w_na:  ['0.5912', '0.4245', '0.3524', '0.2037', '0.3312', '0.3891']
his_acc des_w_na:  ['0.5912', '0.4582', '0.3926', '0.2947', '0.2780', '0.2702']
cur_acc rrf_w_na:  ['0.5906', '0.4398', '0.3616', '0.2008', '0.3506', '0.4014']
his_acc rrf_w_na:  ['0.5906', '0.4769', '0.3991', '0.2986', '0.2803', '0.2704']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 199.5246188CurrentTrain: epoch  0, batch     1 | loss: 68.3293818CurrentTrain: epoch  0, batch     2 | loss: 83.8071534CurrentTrain: epoch  0, batch     3 | loss: 71.1352294CurrentTrain: epoch  0, batch     4 | loss: 18.6685292CurrentTrain: epoch  1, batch     0 | loss: 97.1109981CurrentTrain: epoch  1, batch     1 | loss: 83.2195250CurrentTrain: epoch  1, batch     2 | loss: 62.3335492CurrentTrain: epoch  1, batch     3 | loss: 95.2757584CurrentTrain: epoch  1, batch     4 | loss: 19.7758127CurrentTrain: epoch  2, batch     0 | loss: 73.1433048CurrentTrain: epoch  2, batch     1 | loss: 91.6717771CurrentTrain: epoch  2, batch     2 | loss: 93.7027258CurrentTrain: epoch  2, batch     3 | loss: 91.3144476CurrentTrain: epoch  2, batch     4 | loss: 29.0398098CurrentTrain: epoch  3, batch     0 | loss: 123.4937628CurrentTrain: epoch  3, batch     1 | loss: 87.7580374CurrentTrain: epoch  3, batch     2 | loss: 59.3811359CurrentTrain: epoch  3, batch     3 | loss: 69.4149389CurrentTrain: epoch  3, batch     4 | loss: 18.8389262CurrentTrain: epoch  4, batch     0 | loss: 70.1850288CurrentTrain: epoch  4, batch     1 | loss: 91.0034988CurrentTrain: epoch  4, batch     2 | loss: 70.4936538CurrentTrain: epoch  4, batch     3 | loss: 68.2991049CurrentTrain: epoch  4, batch     4 | loss: 18.3862173CurrentTrain: epoch  5, batch     0 | loss: 69.0753723CurrentTrain: epoch  5, batch     1 | loss: 89.7175614CurrentTrain: epoch  5, batch     2 | loss: 66.5645299CurrentTrain: epoch  5, batch     3 | loss: 89.7869037CurrentTrain: epoch  5, batch     4 | loss: 7.0806807CurrentTrain: epoch  6, batch     0 | loss: 85.1898624CurrentTrain: epoch  6, batch     1 | loss: 119.5999790CurrentTrain: epoch  6, batch     2 | loss: 64.2709924CurrentTrain: epoch  6, batch     3 | loss: 85.8195842CurrentTrain: epoch  6, batch     4 | loss: 17.6420503CurrentTrain: epoch  7, batch     0 | loss: 54.1000833CurrentTrain: epoch  7, batch     1 | loss: 66.6942511CurrentTrain: epoch  7, batch     2 | loss: 85.5250993CurrentTrain: epoch  7, batch     3 | loss: 69.3487305CurrentTrain: epoch  7, batch     4 | loss: 16.3000399CurrentTrain: epoch  8, batch     0 | loss: 63.8089157CurrentTrain: epoch  8, batch     1 | loss: 66.6848990CurrentTrain: epoch  8, batch     2 | loss: 114.9633710CurrentTrain: epoch  8, batch     3 | loss: 53.7947155CurrentTrain: epoch  8, batch     4 | loss: 28.0535654CurrentTrain: epoch  9, batch     0 | loss: 67.4004459CurrentTrain: epoch  9, batch     1 | loss: 54.0351293CurrentTrain: epoch  9, batch     2 | loss: 86.2722436CurrentTrain: epoch  9, batch     3 | loss: 63.0500211CurrentTrain: epoch  9, batch     4 | loss: 27.3755347
MemoryTrain:  epoch  0, batch     0 | loss: 0.3302709MemoryTrain:  epoch  1, batch     0 | loss: 0.3128952MemoryTrain:  epoch  2, batch     0 | loss: 0.2620678MemoryTrain:  epoch  3, batch     0 | loss: 0.2016064MemoryTrain:  epoch  4, batch     0 | loss: 0.1604927MemoryTrain:  epoch  5, batch     0 | loss: 0.1295731MemoryTrain:  epoch  6, batch     0 | loss: 0.1139540MemoryTrain:  epoch  7, batch     0 | loss: 0.1019555MemoryTrain:  epoch  8, batch     0 | loss: 0.0865520MemoryTrain:  epoch  9, batch     0 | loss: 0.0744881

F1 score per class: {0: 0.0, 2: 0.875, 34: 0.0, 35: 0.0, 39: 0.5084745762711864, 8: 0.34710743801652894, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.5, 15: 0.0, 16: 0.0, 28: 0.0}
Micro-average F1 score: 0.4
Weighted-average F1 score: 0.3566103893777058
F1 score per class: {0: 0.0, 2: 0.875, 8: 0.0, 10: 0.0, 11: 0.625, 12: 0.8505747126436781, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 26: 0.0, 28: 0.631578947368421, 33: 0.0, 34: 0.0, 35: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.6271604938271605
Weighted-average F1 score: 0.543090844928413
F1 score per class: {0: 0.0, 2: 0.875, 8: 0.0, 10: 0.0, 11: 0.6461538461538462, 12: 0.8372093023255814, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 26: 0.0, 28: 0.631578947368421, 33: 0.0, 34: 0.0, 35: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.630272952853598
Weighted-average F1 score: 0.5461407593856307

F1 score per class: {0: 0.9444444444444444, 1: 0.3333333333333333, 2: 0.6086956521739131, 3: 0.5873015873015873, 4: 0.9010989010989011, 5: 0.9186602870813397, 6: 0.5555555555555556, 8: 0.32142857142857145, 10: 0.14285714285714285, 11: 0.34285714285714286, 12: 0.34146341463414637, 13: 0.09836065573770492, 14: 0.029850746268656716, 15: 0.5714285714285714, 16: 0.8307692307692308, 17: 0.18181818181818182, 18: 0.36, 19: 0.5074626865671642, 20: 0.736, 21: 0.21138211382113822, 22: 0.3305785123966942, 23: 0.8275862068965517, 24: 0.19047619047619047, 25: 0.4, 26: 0.7329842931937173, 28: 0.20689655172413793, 29: 0.9032258064516129, 30: 1.0, 32: 0.8342245989304813, 33: 0.35294117647058826, 34: 0.02857142857142857, 35: 0.13333333333333333, 36: 0.14084507042253522, 37: 0.15384615384615385, 38: 0.43243243243243246, 39: 0.0}
Micro-average F1 score: 0.5317436661698957
Weighted-average F1 score: 0.5841364400232932
F1 score per class: {0: 0.96, 1: 0.34328358208955223, 2: 0.4666666666666667, 3: 0.8625, 4: 0.93048128342246, 5: 0.8849557522123894, 6: 0.6285714285714286, 8: 0.352112676056338, 10: 0.49635036496350365, 11: 0.4419889502762431, 12: 0.7708333333333334, 13: 0.13333333333333333, 14: 0.1095890410958904, 15: 0.5714285714285714, 16: 0.8484848484848485, 17: 0.32432432432432434, 18: 0.2711864406779661, 19: 0.7906976744186046, 20: 0.74, 21: 0.2695035460992908, 22: 0.368, 23: 0.725, 24: 0.07692307692307693, 25: 0.5205479452054794, 26: 0.7024390243902439, 28: 0.19672131147540983, 29: 0.9270833333333334, 30: 0.8837209302325582, 32: 0.8615384615384616, 33: 0.35294117647058826, 34: 0.025974025974025976, 35: 0.5393258426966292, 36: 0.5806451612903226, 37: 0.21951219512195122, 38: 0.38095238095238093, 39: 0.0}
Micro-average F1 score: 0.6016727652901203
Weighted-average F1 score: 0.6035053678659753
F1 score per class: {0: 0.96, 1: 0.34074074074074073, 2: 0.4666666666666667, 3: 0.8481012658227848, 4: 0.9130434782608695, 5: 0.9049773755656109, 6: 0.5988700564971752, 8: 0.36231884057971014, 10: 0.3937007874015748, 11: 0.4421052631578947, 12: 0.7700534759358288, 13: 0.1, 14: 0.1095890410958904, 15: 0.5714285714285714, 16: 0.835820895522388, 17: 0.24242424242424243, 18: 0.26785714285714285, 19: 0.7931034482758621, 20: 0.7339449541284404, 21: 0.2714285714285714, 22: 0.3709677419354839, 23: 0.725, 24: 0.08, 25: 0.5142857142857142, 26: 0.7024390243902439, 28: 0.23529411764705882, 29: 0.9263157894736842, 30: 0.8837209302325582, 32: 0.8673469387755102, 33: 0.35294117647058826, 34: 0.02631578947368421, 35: 0.5, 36: 0.5333333333333333, 37: 0.2716049382716049, 38: 0.38095238095238093, 39: 0.0}
Micro-average F1 score: 0.5952193328079853
Weighted-average F1 score: 0.598811190548973

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.30434782608695654, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.4225352112676056, 12: 0.32558139534883723, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 28: 0.2857142857142857, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.20299500831946754
Weighted-average F1 score: 0.12364899778290599
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.1794871794871795, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.4678362573099415, 12: 0.5648854961832062, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.18461538461538463, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.21877691645133507
Weighted-average F1 score: 0.16268529941199242
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.19444444444444445, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.48554913294797686, 12: 0.5691699604743083, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.21818181818181817, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.22986425339366515
Weighted-average F1 score: 0.17027000644209103

F1 score per class: {0: 0.24817518248175183, 1: 0.12087912087912088, 2: 0.13333333333333333, 3: 0.37948717948717947, 4: 0.8677248677248677, 5: 0.7441860465116279, 6: 0.23684210526315788, 8: 0.1875, 10: 0.10738255033557047, 11: 0.16666666666666666, 12: 0.18025751072961374, 13: 0.024291497975708502, 14: 0.021505376344086023, 15: 0.3333333333333333, 16: 0.3673469387755102, 17: 0.11428571428571428, 18: 0.14634146341463414, 19: 0.29955947136563876, 20: 0.24864864864864866, 21: 0.07580174927113703, 22: 0.31007751937984496, 23: 0.6923076923076923, 24: 0.17391304347826086, 25: 0.38235294117647056, 26: 0.6306306306306306, 28: 0.10714285714285714, 29: 0.6942148760330579, 30: 0.926829268292683, 32: 0.5114754098360655, 33: 0.3, 34: 0.024691358024691357, 35: 0.058823529411764705, 36: 0.13513513513513514, 37: 0.1388888888888889, 38: 0.1951219512195122, 39: 0.0}
Micro-average F1 score: 0.297879445650359
Weighted-average F1 score: 0.2779882350715921
F1 score per class: {0: 0.14545454545454545, 1: 0.11031175059952038, 2: 0.05185185185185185, 3: 0.3194444444444444, 4: 0.8529411764705882, 5: 0.477326968973747, 6: 0.2231237322515213, 8: 0.15337423312883436, 10: 0.24548736462093862, 11: 0.17777777777777778, 12: 0.13590449954086317, 13: 0.02097902097902098, 14: 0.04, 15: 0.2727272727272727, 16: 0.2978723404255319, 17: 0.14457831325301204, 18: 0.05536332179930796, 19: 0.34, 20: 0.2222222222222222, 21: 0.06333333333333334, 22: 0.32857142857142857, 23: 0.3918918918918919, 24: 0.058823529411764705, 25: 0.4523809523809524, 26: 0.5647058823529412, 28: 0.06091370558375635, 29: 0.6691729323308271, 30: 0.5, 32: 0.41075794621026895, 33: 0.2727272727272727, 34: 0.018867924528301886, 35: 0.16608996539792387, 36: 0.4122137404580153, 37: 0.16666666666666666, 38: 0.16326530612244897, 39: 0.0}
Micro-average F1 score: 0.23040736662996697
Weighted-average F1 score: 0.20962048891063034
F1 score per class: {0: 0.15584415584415584, 1: 0.11004784688995216, 2: 0.06635071090047394, 3: 0.35356200527704484, 4: 0.8842105263157894, 5: 0.5194805194805194, 6: 0.2150101419878296, 8: 0.16556291390728478, 10: 0.22624434389140272, 11: 0.1731958762886598, 12: 0.145748987854251, 13: 0.018072289156626505, 14: 0.03980099502487562, 15: 0.2727272727272727, 16: 0.30939226519337015, 17: 0.1111111111111111, 18: 0.055658627087198514, 19: 0.34413965087281795, 20: 0.20512820512820512, 21: 0.06397306397306397, 22: 0.33093525179856115, 23: 0.4461538461538462, 24: 0.06060606060606061, 25: 0.48, 26: 0.576, 28: 0.08053691275167785, 29: 0.704, 30: 0.5277777777777778, 32: 0.4239401496259352, 33: 0.2608695652173913, 34: 0.02197802197802198, 35: 0.16428571428571428, 36: 0.40336134453781514, 37: 0.1981981981981982, 38: 0.13445378151260504, 39: 0.0}
Micro-average F1 score: 0.23707888679640093
Weighted-average F1 score: 0.21418086376503895
cur_acc_wo_na:  ['0.8093', '0.6262', '0.6000', '0.3384', '0.7059', '0.7959', '0.4000']
his_acc_wo_na:  ['0.8093', '0.7335', '0.6585', '0.4933', '0.4956', '0.5176', '0.5317']
cur_acc des_wo_na:  ['0.8494', '0.8693', '0.7874', '0.4888', '0.8015', '0.8357', '0.6272']
his_acc des_wo_na:  ['0.8494', '0.8544', '0.7539', '0.6066', '0.5940', '0.6241', '0.6017']
cur_acc rrf_wo_na:  ['0.8474', '0.8571', '0.7805', '0.4678', '0.8256', '0.8369', '0.6303']
his_acc rrf_wo_na:  ['0.8474', '0.8423', '0.7395', '0.5914', '0.5877', '0.6056', '0.5952']
cur_acc_w_na:  ['0.6743', '0.4000', '0.3901', '0.1805', '0.4509', '0.5426', '0.2030']
his_acc_w_na:  ['0.6743', '0.5399', '0.4723', '0.3176', '0.3223', '0.3273', '0.2979']
cur_acc des_w_na:  ['0.5912', '0.4245', '0.3524', '0.2037', '0.3312', '0.3891', '0.2188']
his_acc des_w_na:  ['0.5912', '0.4582', '0.3926', '0.2947', '0.2780', '0.2702', '0.2304']
cur_acc rrf_w_na:  ['0.5906', '0.4398', '0.3616', '0.2008', '0.3506', '0.4014', '0.2299']
his_acc rrf_w_na:  ['0.5906', '0.4769', '0.3991', '0.2986', '0.2803', '0.2704', '0.2371']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 65.8033268CurrentTrain: epoch  0, batch     1 | loss: 98.1372597CurrentTrain: epoch  0, batch     2 | loss: 71.4715448CurrentTrain: epoch  0, batch     3 | loss: 27.8993793CurrentTrain: epoch  1, batch     0 | loss: 75.3484713CurrentTrain: epoch  1, batch     1 | loss: 59.8482250CurrentTrain: epoch  1, batch     2 | loss: 61.9531234CurrentTrain: epoch  1, batch     3 | loss: 27.9797940CurrentTrain: epoch  2, batch     0 | loss: 57.8902556CurrentTrain: epoch  2, batch     1 | loss: 59.9915619CurrentTrain: epoch  2, batch     2 | loss: 57.8625284CurrentTrain: epoch  2, batch     3 | loss: 27.7230913CurrentTrain: epoch  3, batch     0 | loss: 53.4323431CurrentTrain: epoch  3, batch     1 | loss: 56.4610679CurrentTrain: epoch  3, batch     2 | loss: 55.5914422CurrentTrain: epoch  3, batch     3 | loss: 27.5935930CurrentTrain: epoch  4, batch     0 | loss: 55.1855539CurrentTrain: epoch  4, batch     1 | loss: 67.4415963CurrentTrain: epoch  4, batch     2 | loss: 84.1143404CurrentTrain: epoch  4, batch     3 | loss: 9.6435238CurrentTrain: epoch  5, batch     0 | loss: 66.3365152CurrentTrain: epoch  5, batch     1 | loss: 49.6976317CurrentTrain: epoch  5, batch     2 | loss: 84.7507370CurrentTrain: epoch  5, batch     3 | loss: 11.3674304CurrentTrain: epoch  6, batch     0 | loss: 63.8048811CurrentTrain: epoch  6, batch     1 | loss: 65.2005197CurrentTrain: epoch  6, batch     2 | loss: 51.6593449CurrentTrain: epoch  6, batch     3 | loss: 27.5084833CurrentTrain: epoch  7, batch     0 | loss: 83.1853644CurrentTrain: epoch  7, batch     1 | loss: 51.2661192CurrentTrain: epoch  7, batch     2 | loss: 51.8819744CurrentTrain: epoch  7, batch     3 | loss: 27.5966314CurrentTrain: epoch  8, batch     0 | loss: 83.1368661CurrentTrain: epoch  8, batch     1 | loss: 48.5864573CurrentTrain: epoch  8, batch     2 | loss: 54.1195565CurrentTrain: epoch  8, batch     3 | loss: 5.7861511CurrentTrain: epoch  9, batch     0 | loss: 52.8014604CurrentTrain: epoch  9, batch     1 | loss: 50.0260758CurrentTrain: epoch  9, batch     2 | loss: 52.6722389CurrentTrain: epoch  9, batch     3 | loss: 10.9712197
MemoryTrain:  epoch  0, batch     0 | loss: 0.3133613MemoryTrain:  epoch  1, batch     0 | loss: 0.2965023MemoryTrain:  epoch  2, batch     0 | loss: 0.2058148MemoryTrain:  epoch  3, batch     0 | loss: 0.2030004MemoryTrain:  epoch  4, batch     0 | loss: 0.1745636MemoryTrain:  epoch  5, batch     0 | loss: 0.1491410MemoryTrain:  epoch  6, batch     0 | loss: 0.1221634MemoryTrain:  epoch  7, batch     0 | loss: 0.1026808MemoryTrain:  epoch  8, batch     0 | loss: 0.0937210MemoryTrain:  epoch  9, batch     0 | loss: 0.0842370

F1 score per class: {0: 0.0, 3: 0.0, 7: 0.3333333333333333, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 26: 0.6363636363636364, 27: 0.0, 31: 0.5833333333333334}
Micro-average F1 score: 0.5980392156862745
Weighted-average F1 score: 0.5211527035056447
F1 score per class: {1: 0.0, 3: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 21: 0.0, 26: 0.7272727272727273, 27: 0.6666666666666666, 31: 0.8}
Micro-average F1 score: 0.7557603686635944
Weighted-average F1 score: 0.6841240997723008
F1 score per class: {1: 0.0, 3: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 21: 0.0, 26: 0.6956521739130435, 27: 0.6666666666666666, 31: 0.7787610619469026}
Micro-average F1 score: 0.7441860465116279
Weighted-average F1 score: 0.6764366065132029

F1 score per class: {0: 0.9166666666666666, 1: 0.13043478260869565, 2: 0.5185185185185185, 3: 0.43243243243243246, 4: 0.8304093567251462, 5: 0.9023255813953488, 6: 0.12727272727272726, 7: 0.030303030303030304, 8: 0.1978021978021978, 9: 0.9803921568627451, 10: 0.0196078431372549, 11: 0.39751552795031053, 12: 0.26666666666666666, 13: 0.1111111111111111, 14: 0.02702702702702703, 15: 0.631578947368421, 16: 0.7868852459016393, 17: 0.0, 18: 0.3728813559322034, 19: 0.5714285714285714, 20: 0.6857142857142857, 21: 0.19607843137254902, 22: 0.3305785123966942, 23: 0.7857142857142857, 24: 0.1, 25: 0.4, 26: 0.6853932584269663, 27: 0.19718309859154928, 28: 0.2222222222222222, 29: 0.8715083798882681, 30: 0.972972972972973, 31: 0.0, 32: 0.7337278106508875, 33: 0.4, 34: 0.028169014084507043, 35: 0.1016949152542373, 36: 0.27848101265822783, 37: 0.32432432432432434, 38: 0.3076923076923077, 39: 0.0, 40: 0.48695652173913045}
Micro-average F1 score: 0.4807308021695689
Weighted-average F1 score: 0.5297352443111913
F1 score per class: {0: 0.9142857142857143, 1: 0.2564102564102564, 2: 0.56, 3: 0.7814569536423841, 4: 0.907103825136612, 5: 0.8608695652173913, 6: 0.038461538461538464, 7: 0.045454545454545456, 8: 0.19801980198019803, 9: 0.9803921568627451, 10: 0.2542372881355932, 11: 0.36470588235294116, 12: 0.6467065868263473, 13: 0.11320754716981132, 14: 0.1095890410958904, 15: 0.6666666666666666, 16: 0.835820895522388, 17: 0.42857142857142855, 18: 0.26804123711340205, 19: 0.6741573033707865, 20: 0.7547169811320755, 21: 0.26153846153846155, 22: 0.3968253968253968, 23: 0.725, 24: 0.08695652173913043, 25: 0.5405405405405406, 26: 0.6700507614213198, 27: 0.18181818181818182, 28: 0.18181818181818182, 29: 0.9081081081081082, 30: 0.9, 31: 0.5, 32: 0.7865168539325843, 33: 0.4, 34: 0.04938271604938271, 35: 0.5777777777777777, 36: 0.6262626262626263, 37: 0.3, 38: 0.34146341463414637, 39: 0.125, 40: 0.6433566433566433}
Micro-average F1 score: 0.543355566957414
Weighted-average F1 score: 0.5502445271060592
F1 score per class: {0: 0.9295774647887324, 1: 0.2564102564102564, 2: 0.5, 3: 0.7083333333333334, 4: 0.88268156424581, 5: 0.8646288209606987, 6: 0.09259259259259259, 7: 0.05063291139240506, 8: 0.3063063063063063, 9: 0.9803921568627451, 10: 0.17857142857142858, 11: 0.36046511627906974, 12: 0.6107784431137725, 13: 0.10526315789473684, 14: 0.10810810810810811, 15: 0.631578947368421, 16: 0.8615384615384616, 17: 0.16666666666666666, 18: 0.3023255813953488, 19: 0.6705202312138728, 20: 0.7592592592592593, 21: 0.23622047244094488, 22: 0.3968253968253968, 23: 0.6923076923076923, 24: 0.09090909090909091, 25: 0.5135135135135135, 26: 0.6700507614213198, 27: 0.17391304347826086, 28: 0.2727272727272727, 29: 0.8901098901098901, 30: 0.926829268292683, 31: 0.6666666666666666, 32: 0.7865168539325843, 33: 0.4, 34: 0.05063291139240506, 35: 0.449438202247191, 36: 0.6185567010309279, 37: 0.2962962962962963, 38: 0.34146341463414637, 39: 0.0, 40: 0.6068965517241379}
Micro-average F1 score: 0.5352039490776825
Weighted-average F1 score: 0.5448383216972544

F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 7: 0.2222222222222222, 8: 0.0, 9: 0.8771929824561403, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.32558139534883723, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.4148148148148148}
Micro-average F1 score: 0.3719512195121951
Weighted-average F1 score: 0.306737877561154
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4, 9: 0.746268656716418, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.42105263157894735, 28: 0.0, 30: 0.0, 31: 0.11764705882352941, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.5054945054945055}
Micro-average F1 score: 0.34893617021276596
Weighted-average F1 score: 0.2879618264707117
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4, 9: 0.78125, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.37209302325581395, 30: 0.0, 31: 0.14285714285714285, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.4888888888888889}
Micro-average F1 score: 0.36281179138321995
Weighted-average F1 score: 0.30373129042053726

F1 score per class: {0: 0.30697674418604654, 1: 0.06451612903225806, 2: 0.1891891891891892, 3: 0.27586206896551724, 4: 0.8114285714285714, 5: 0.61198738170347, 6: 0.08860759493670886, 7: 0.012048192771084338, 8: 0.13636363636363635, 9: 0.7936507936507936, 10: 0.018018018018018018, 11: 0.1945288753799392, 12: 0.15023474178403756, 13: 0.025423728813559324, 14: 0.01652892561983471, 15: 0.41379310344827586, 16: 0.366412213740458, 17: 0.0, 18: 0.1301775147928994, 19: 0.36231884057971014, 20: 0.1875, 21: 0.07352941176470588, 22: 0.31007751937984496, 23: 0.6226415094339622, 24: 0.09090909090909091, 25: 0.3880597014925373, 26: 0.5980392156862745, 27: 0.052434456928838954, 28: 0.11764705882352941, 29: 0.6995515695067265, 30: 0.782608695652174, 31: 0.0, 32: 0.5232067510548524, 33: 0.2857142857142857, 34: 0.024390243902439025, 35: 0.06, 36: 0.23157894736842105, 37: 0.25, 38: 0.10714285714285714, 39: 0.0, 40: 0.23931623931623933}
Micro-average F1 score: 0.2711755233494364
Weighted-average F1 score: 0.24977894997807248
F1 score per class: {0: 0.29767441860465116, 1: 0.13513513513513514, 2: 0.14, 3: 0.3430232558139535, 4: 0.8601036269430051, 5: 0.3983903420523139, 6: 0.027972027972027972, 7: 0.014336917562724014, 8: 0.12903225806451613, 9: 0.5102040816326531, 10: 0.17647058823529413, 11: 0.14220183486238533, 12: 0.15083798882681565, 13: 0.02097902097902098, 14: 0.046511627906976744, 15: 0.375, 16: 0.2962962962962963, 17: 0.21428571428571427, 18: 0.052, 19: 0.33613445378151263, 20: 0.20408163265306123, 21: 0.07423580786026202, 22: 0.373134328358209, 23: 0.42028985507246375, 24: 0.07142857142857142, 25: 0.45454545454545453, 26: 0.55, 27: 0.05194805194805195, 28: 0.05970149253731343, 29: 0.7058823529411765, 30: 0.5714285714285714, 31: 0.0125, 32: 0.4682274247491639, 33: 0.2727272727272727, 34: 0.04, 35: 0.25365853658536586, 36: 0.44285714285714284, 37: 0.22857142857142856, 38: 0.1686746987951807, 39: 0.03508771929824561, 40: 0.2340966921119593}
Micro-average F1 score: 0.2377104377104377
Weighted-average F1 score: 0.21492483835740597
F1 score per class: {0: 0.25384615384615383, 1: 0.13793103448275862, 2: 0.13861386138613863, 3: 0.3167701863354037, 4: 0.8540540540540541, 5: 0.4439461883408072, 6: 0.06097560975609756, 7: 0.01639344262295082, 8: 0.20238095238095238, 9: 0.625, 10: 0.1342281879194631, 11: 0.14797136038186157, 12: 0.16267942583732056, 13: 0.019672131147540985, 14: 0.045714285714285714, 15: 0.34285714285714286, 16: 0.33532934131736525, 17: 0.1, 18: 0.05803571428571429, 19: 0.36363636363636365, 20: 0.19248826291079812, 21: 0.06928406466512702, 22: 0.373134328358209, 23: 0.4864864864864865, 24: 0.07407407407407407, 25: 0.4523809523809524, 26: 0.5714285714285714, 27: 0.0469208211143695, 28: 0.096, 29: 0.6952789699570815, 30: 0.5846153846153846, 31: 0.024691358024691357, 32: 0.48109965635738833, 33: 0.2857142857142857, 34: 0.0425531914893617, 35: 0.20618556701030927, 36: 0.44776119402985076, 37: 0.21428571428571427, 38: 0.14432989690721648, 39: 0.0, 40: 0.23783783783783785}
Micro-average F1 score: 0.24471370871941078
Weighted-average F1 score: 0.2202404103706877
cur_acc_wo_na:  ['0.8093', '0.6262', '0.6000', '0.3384', '0.7059', '0.7959', '0.4000', '0.5980']
his_acc_wo_na:  ['0.8093', '0.7335', '0.6585', '0.4933', '0.4956', '0.5176', '0.5317', '0.4807']
cur_acc des_wo_na:  ['0.8494', '0.8693', '0.7874', '0.4888', '0.8015', '0.8357', '0.6272', '0.7558']
his_acc des_wo_na:  ['0.8494', '0.8544', '0.7539', '0.6066', '0.5940', '0.6241', '0.6017', '0.5434']
cur_acc rrf_wo_na:  ['0.8474', '0.8571', '0.7805', '0.4678', '0.8256', '0.8369', '0.6303', '0.7442']
his_acc rrf_wo_na:  ['0.8474', '0.8423', '0.7395', '0.5914', '0.5877', '0.6056', '0.5952', '0.5352']
cur_acc_w_na:  ['0.6743', '0.4000', '0.3901', '0.1805', '0.4509', '0.5426', '0.2030', '0.3720']
his_acc_w_na:  ['0.6743', '0.5399', '0.4723', '0.3176', '0.3223', '0.3273', '0.2979', '0.2712']
cur_acc des_w_na:  ['0.5912', '0.4245', '0.3524', '0.2037', '0.3312', '0.3891', '0.2188', '0.3489']
his_acc des_w_na:  ['0.5912', '0.4582', '0.3926', '0.2947', '0.2780', '0.2702', '0.2304', '0.2377']
cur_acc rrf_w_na:  ['0.5906', '0.4398', '0.3616', '0.2008', '0.3506', '0.4014', '0.2299', '0.3628']
his_acc rrf_w_na:  ['0.5906', '0.4769', '0.3991', '0.2986', '0.2803', '0.2704', '0.2371', '0.2447']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 130.2322568CurrentTrain: epoch  0, batch     1 | loss: 79.8286588CurrentTrain: epoch  0, batch     2 | loss: 68.5562481CurrentTrain: epoch  0, batch     3 | loss: 79.5698073CurrentTrain: epoch  0, batch     4 | loss: 78.7693729CurrentTrain: epoch  0, batch     5 | loss: 97.5318361CurrentTrain: epoch  0, batch     6 | loss: 96.8581755CurrentTrain: epoch  0, batch     7 | loss: 56.8099185CurrentTrain: epoch  0, batch     8 | loss: 96.8663270CurrentTrain: epoch  0, batch     9 | loss: 77.6388442CurrentTrain: epoch  0, batch    10 | loss: 97.1591755CurrentTrain: epoch  0, batch    11 | loss: 78.7578692CurrentTrain: epoch  0, batch    12 | loss: 65.0554594CurrentTrain: epoch  0, batch    13 | loss: 77.6964165CurrentTrain: epoch  0, batch    14 | loss: 95.6836227CurrentTrain: epoch  0, batch    15 | loss: 79.6866574CurrentTrain: epoch  0, batch    16 | loss: 77.8412464CurrentTrain: epoch  0, batch    17 | loss: 126.5835596CurrentTrain: epoch  0, batch    18 | loss: 65.5932694CurrentTrain: epoch  0, batch    19 | loss: 76.9706839CurrentTrain: epoch  0, batch    20 | loss: 64.4665833CurrentTrain: epoch  0, batch    21 | loss: 95.0270418CurrentTrain: epoch  0, batch    22 | loss: 56.4482438CurrentTrain: epoch  0, batch    23 | loss: 76.7568438CurrentTrain: epoch  0, batch    24 | loss: 77.4009935CurrentTrain: epoch  0, batch    25 | loss: 75.4872600CurrentTrain: epoch  0, batch    26 | loss: 76.5846686CurrentTrain: epoch  0, batch    27 | loss: 64.6769866CurrentTrain: epoch  0, batch    28 | loss: 124.9141647CurrentTrain: epoch  0, batch    29 | loss: 64.8846838CurrentTrain: epoch  0, batch    30 | loss: 96.0227689CurrentTrain: epoch  0, batch    31 | loss: 64.7096459CurrentTrain: epoch  0, batch    32 | loss: 76.4191385CurrentTrain: epoch  0, batch    33 | loss: 55.7085954CurrentTrain: epoch  0, batch    34 | loss: 76.4720947CurrentTrain: epoch  0, batch    35 | loss: 64.7250396CurrentTrain: epoch  0, batch    36 | loss: 56.3714420CurrentTrain: epoch  0, batch    37 | loss: 64.3443477CurrentTrain: epoch  0, batch    38 | loss: 126.9367703CurrentTrain: epoch  0, batch    39 | loss: 76.8624932CurrentTrain: epoch  0, batch    40 | loss: 93.8310695CurrentTrain: epoch  0, batch    41 | loss: 54.9839538CurrentTrain: epoch  0, batch    42 | loss: 56.1282848CurrentTrain: epoch  0, batch    43 | loss: 126.0027340CurrentTrain: epoch  0, batch    44 | loss: 64.2486401CurrentTrain: epoch  0, batch    45 | loss: 54.5678339CurrentTrain: epoch  0, batch    46 | loss: 63.2384599CurrentTrain: epoch  0, batch    47 | loss: 76.3112671CurrentTrain: epoch  0, batch    48 | loss: 76.5628048CurrentTrain: epoch  0, batch    49 | loss: 55.9371052CurrentTrain: epoch  0, batch    50 | loss: 93.8691738CurrentTrain: epoch  0, batch    51 | loss: 55.7587968CurrentTrain: epoch  0, batch    52 | loss: 187.9700921CurrentTrain: epoch  0, batch    53 | loss: 75.3013179CurrentTrain: epoch  0, batch    54 | loss: 63.1341524CurrentTrain: epoch  0, batch    55 | loss: 75.4893045CurrentTrain: epoch  0, batch    56 | loss: 76.3814453CurrentTrain: epoch  0, batch    57 | loss: 64.4703922CurrentTrain: epoch  0, batch    58 | loss: 74.8971438CurrentTrain: epoch  0, batch    59 | loss: 74.6667164CurrentTrain: epoch  0, batch    60 | loss: 62.9756444CurrentTrain: epoch  0, batch    61 | loss: 93.8916646CurrentTrain: epoch  0, batch    62 | loss: 61.1810774CurrentTrain: epoch  0, batch    63 | loss: 93.6218602CurrentTrain: epoch  0, batch    64 | loss: 93.7803941CurrentTrain: epoch  0, batch    65 | loss: 94.0608265CurrentTrain: epoch  0, batch    66 | loss: 52.6407391CurrentTrain: epoch  0, batch    67 | loss: 75.0761444CurrentTrain: epoch  0, batch    68 | loss: 74.4932771CurrentTrain: epoch  0, batch    69 | loss: 74.3287568CurrentTrain: epoch  0, batch    70 | loss: 55.2393313CurrentTrain: epoch  0, batch    71 | loss: 124.1210471CurrentTrain: epoch  0, batch    72 | loss: 60.1092021CurrentTrain: epoch  0, batch    73 | loss: 91.3868754CurrentTrain: epoch  0, batch    74 | loss: 94.8729771CurrentTrain: epoch  0, batch    75 | loss: 62.7812028CurrentTrain: epoch  0, batch    76 | loss: 93.5519907CurrentTrain: epoch  0, batch    77 | loss: 75.5234672CurrentTrain: epoch  0, batch    78 | loss: 59.8261779CurrentTrain: epoch  0, batch    79 | loss: 63.0553725CurrentTrain: epoch  0, batch    80 | loss: 74.0878658CurrentTrain: epoch  0, batch    81 | loss: 125.0956849CurrentTrain: epoch  0, batch    82 | loss: 71.6168900CurrentTrain: epoch  0, batch    83 | loss: 122.9663163CurrentTrain: epoch  0, batch    84 | loss: 75.4753338CurrentTrain: epoch  0, batch    85 | loss: 72.9022906CurrentTrain: epoch  0, batch    86 | loss: 91.9935759CurrentTrain: epoch  0, batch    87 | loss: 62.2447467CurrentTrain: epoch  0, batch    88 | loss: 74.0686976CurrentTrain: epoch  0, batch    89 | loss: 61.2990316CurrentTrain: epoch  0, batch    90 | loss: 73.0857829CurrentTrain: epoch  0, batch    91 | loss: 92.8557766CurrentTrain: epoch  0, batch    92 | loss: 74.4414604CurrentTrain: epoch  0, batch    93 | loss: 93.1260960CurrentTrain: epoch  0, batch    94 | loss: 50.3461409CurrentTrain: epoch  0, batch    95 | loss: 74.8969018CurrentTrain: epoch  1, batch     0 | loss: 73.7854504CurrentTrain: epoch  1, batch     1 | loss: 59.6645854CurrentTrain: epoch  1, batch     2 | loss: 120.2896469CurrentTrain: epoch  1, batch     3 | loss: 70.1793080CurrentTrain: epoch  1, batch     4 | loss: 60.5355112CurrentTrain: epoch  1, batch     5 | loss: 71.7413809CurrentTrain: epoch  1, batch     6 | loss: 56.9323621CurrentTrain: epoch  1, batch     7 | loss: 121.2040476CurrentTrain: epoch  1, batch     8 | loss: 73.3052796CurrentTrain: epoch  1, batch     9 | loss: 59.6678109CurrentTrain: epoch  1, batch    10 | loss: 60.8719769CurrentTrain: epoch  1, batch    11 | loss: 71.6596707CurrentTrain: epoch  1, batch    12 | loss: 72.2107179CurrentTrain: epoch  1, batch    13 | loss: 58.9111881CurrentTrain: epoch  1, batch    14 | loss: 58.5362411CurrentTrain: epoch  1, batch    15 | loss: 57.0814592CurrentTrain: epoch  1, batch    16 | loss: 70.8532335CurrentTrain: epoch  1, batch    17 | loss: 70.9806008CurrentTrain: epoch  1, batch    18 | loss: 59.4629407CurrentTrain: epoch  1, batch    19 | loss: 72.1197479CurrentTrain: epoch  1, batch    20 | loss: 70.1432446CurrentTrain: epoch  1, batch    21 | loss: 71.1310926CurrentTrain: epoch  1, batch    22 | loss: 70.4855147CurrentTrain: epoch  1, batch    23 | loss: 60.4401302CurrentTrain: epoch  1, batch    24 | loss: 73.0926957CurrentTrain: epoch  1, batch    25 | loss: 120.3990443CurrentTrain: epoch  1, batch    26 | loss: 68.8177210CurrentTrain: epoch  1, batch    27 | loss: 84.9349682CurrentTrain: epoch  1, batch    28 | loss: 85.4252164CurrentTrain: epoch  1, batch    29 | loss: 92.1344775CurrentTrain: epoch  1, batch    30 | loss: 119.7130984CurrentTrain: epoch  1, batch    31 | loss: 122.4533989CurrentTrain: epoch  1, batch    32 | loss: 56.1270311CurrentTrain: epoch  1, batch    33 | loss: 58.3300165CurrentTrain: epoch  1, batch    34 | loss: 125.1267736CurrentTrain: epoch  1, batch    35 | loss: 56.9577337CurrentTrain: epoch  1, batch    36 | loss: 119.5546447CurrentTrain: epoch  1, batch    37 | loss: 70.8689591CurrentTrain: epoch  1, batch    38 | loss: 70.4309317CurrentTrain: epoch  1, batch    39 | loss: 71.9916816CurrentTrain: epoch  1, batch    40 | loss: 121.2953238CurrentTrain: epoch  1, batch    41 | loss: 90.3219927CurrentTrain: epoch  1, batch    42 | loss: 92.3926877CurrentTrain: epoch  1, batch    43 | loss: 89.6483215CurrentTrain: epoch  1, batch    44 | loss: 86.4573371CurrentTrain: epoch  1, batch    45 | loss: 71.0719721CurrentTrain: epoch  1, batch    46 | loss: 71.4346411CurrentTrain: epoch  1, batch    47 | loss: 57.8225574CurrentTrain: epoch  1, batch    48 | loss: 85.1504265CurrentTrain: epoch  1, batch    49 | loss: 88.2333574CurrentTrain: epoch  1, batch    50 | loss: 70.2084413CurrentTrain: epoch  1, batch    51 | loss: 57.9278145CurrentTrain: epoch  1, batch    52 | loss: 70.8237012CurrentTrain: epoch  1, batch    53 | loss: 56.6018894CurrentTrain: epoch  1, batch    54 | loss: 54.1515732CurrentTrain: epoch  1, batch    55 | loss: 63.1220279CurrentTrain: epoch  1, batch    56 | loss: 116.8640416CurrentTrain: epoch  1, batch    57 | loss: 59.1532100CurrentTrain: epoch  1, batch    58 | loss: 121.5463049CurrentTrain: epoch  1, batch    59 | loss: 91.2665424CurrentTrain: epoch  1, batch    60 | loss: 54.6843258CurrentTrain: epoch  1, batch    61 | loss: 90.3514991CurrentTrain: epoch  1, batch    62 | loss: 71.2443900CurrentTrain: epoch  1, batch    63 | loss: 122.9321274CurrentTrain: epoch  1, batch    64 | loss: 119.0694892CurrentTrain: epoch  1, batch    65 | loss: 58.9503601CurrentTrain: epoch  1, batch    66 | loss: 48.7914411CurrentTrain: epoch  1, batch    67 | loss: 70.1711987CurrentTrain: epoch  1, batch    68 | loss: 69.8793597CurrentTrain: epoch  1, batch    69 | loss: 56.0193522CurrentTrain: epoch  1, batch    70 | loss: 57.3068208CurrentTrain: epoch  1, batch    71 | loss: 57.5000494CurrentTrain: epoch  1, batch    72 | loss: 58.3901839CurrentTrain: epoch  1, batch    73 | loss: 87.5615623CurrentTrain: epoch  1, batch    74 | loss: 48.2564444CurrentTrain: epoch  1, batch    75 | loss: 55.0242574CurrentTrain: epoch  1, batch    76 | loss: 80.7494804CurrentTrain: epoch  1, batch    77 | loss: 49.9642277CurrentTrain: epoch  1, batch    78 | loss: 54.2332771CurrentTrain: epoch  1, batch    79 | loss: 72.9334579CurrentTrain: epoch  1, batch    80 | loss: 48.7475336CurrentTrain: epoch  1, batch    81 | loss: 87.7730822CurrentTrain: epoch  1, batch    82 | loss: 89.5075005CurrentTrain: epoch  1, batch    83 | loss: 86.2144332CurrentTrain: epoch  1, batch    84 | loss: 56.5793238CurrentTrain: epoch  1, batch    85 | loss: 88.4749849CurrentTrain: epoch  1, batch    86 | loss: 53.6998468CurrentTrain: epoch  1, batch    87 | loss: 87.3400907CurrentTrain: epoch  1, batch    88 | loss: 59.0948241CurrentTrain: epoch  1, batch    89 | loss: 55.9851778CurrentTrain: epoch  1, batch    90 | loss: 90.2356039CurrentTrain: epoch  1, batch    91 | loss: 87.3638251CurrentTrain: epoch  1, batch    92 | loss: 55.2139713CurrentTrain: epoch  1, batch    93 | loss: 57.6175953CurrentTrain: epoch  1, batch    94 | loss: 70.0153632CurrentTrain: epoch  1, batch    95 | loss: 49.1767676CurrentTrain: epoch  2, batch     0 | loss: 65.4577478CurrentTrain: epoch  2, batch     1 | loss: 85.9868608CurrentTrain: epoch  2, batch     2 | loss: 55.7602814CurrentTrain: epoch  2, batch     3 | loss: 63.0505804CurrentTrain: epoch  2, batch     4 | loss: 57.9605446CurrentTrain: epoch  2, batch     5 | loss: 51.8757911CurrentTrain: epoch  2, batch     6 | loss: 71.2598885CurrentTrain: epoch  2, batch     7 | loss: 69.0502405CurrentTrain: epoch  2, batch     8 | loss: 66.9984678CurrentTrain: epoch  2, batch     9 | loss: 84.6780761CurrentTrain: epoch  2, batch    10 | loss: 89.5887974CurrentTrain: epoch  2, batch    11 | loss: 74.0216896CurrentTrain: epoch  2, batch    12 | loss: 58.2404052CurrentTrain: epoch  2, batch    13 | loss: 54.6656293CurrentTrain: epoch  2, batch    14 | loss: 55.0250767CurrentTrain: epoch  2, batch    15 | loss: 113.9429373CurrentTrain: epoch  2, batch    16 | loss: 90.8410622CurrentTrain: epoch  2, batch    17 | loss: 54.4955499CurrentTrain: epoch  2, batch    18 | loss: 53.5517192CurrentTrain: epoch  2, batch    19 | loss: 67.8743400CurrentTrain: epoch  2, batch    20 | loss: 65.6300050CurrentTrain: epoch  2, batch    21 | loss: 54.9491111CurrentTrain: epoch  2, batch    22 | loss: 50.6337796CurrentTrain: epoch  2, batch    23 | loss: 69.6301743CurrentTrain: epoch  2, batch    24 | loss: 86.9544480CurrentTrain: epoch  2, batch    25 | loss: 85.8618172CurrentTrain: epoch  2, batch    26 | loss: 66.4967294CurrentTrain: epoch  2, batch    27 | loss: 71.2763803CurrentTrain: epoch  2, batch    28 | loss: 58.8396455CurrentTrain: epoch  2, batch    29 | loss: 121.8754911CurrentTrain: epoch  2, batch    30 | loss: 66.8049275CurrentTrain: epoch  2, batch    31 | loss: 68.1675905CurrentTrain: epoch  2, batch    32 | loss: 58.6708285CurrentTrain: epoch  2, batch    33 | loss: 59.8950739CurrentTrain: epoch  2, batch    34 | loss: 119.3777533CurrentTrain: epoch  2, batch    35 | loss: 88.0418363CurrentTrain: epoch  2, batch    36 | loss: 44.8560971CurrentTrain: epoch  2, batch    37 | loss: 90.7590189CurrentTrain: epoch  2, batch    38 | loss: 116.3011778CurrentTrain: epoch  2, batch    39 | loss: 84.9220588CurrentTrain: epoch  2, batch    40 | loss: 47.4414809CurrentTrain: epoch  2, batch    41 | loss: 45.8076355CurrentTrain: epoch  2, batch    42 | loss: 84.0990669CurrentTrain: epoch  2, batch    43 | loss: 70.2775512CurrentTrain: epoch  2, batch    44 | loss: 54.5278620CurrentTrain: epoch  2, batch    45 | loss: 125.9803305CurrentTrain: epoch  2, batch    46 | loss: 68.8471641CurrentTrain: epoch  2, batch    47 | loss: 85.6629082CurrentTrain: epoch  2, batch    48 | loss: 120.8518454CurrentTrain: epoch  2, batch    49 | loss: 57.5408024CurrentTrain: epoch  2, batch    50 | loss: 58.2754965CurrentTrain: epoch  2, batch    51 | loss: 87.5998644CurrentTrain: epoch  2, batch    52 | loss: 119.6352048CurrentTrain: epoch  2, batch    53 | loss: 46.6443597CurrentTrain: epoch  2, batch    54 | loss: 53.4927127CurrentTrain: epoch  2, batch    55 | loss: 55.1562059CurrentTrain: epoch  2, batch    56 | loss: 89.6439171CurrentTrain: epoch  2, batch    57 | loss: 88.8096348CurrentTrain: epoch  2, batch    58 | loss: 69.0486682CurrentTrain: epoch  2, batch    59 | loss: 88.1183895CurrentTrain: epoch  2, batch    60 | loss: 88.2028931CurrentTrain: epoch  2, batch    61 | loss: 67.5053796CurrentTrain: epoch  2, batch    62 | loss: 58.2909488CurrentTrain: epoch  2, batch    63 | loss: 179.9087873CurrentTrain: epoch  2, batch    64 | loss: 90.6090993CurrentTrain: epoch  2, batch    65 | loss: 64.9745622CurrentTrain: epoch  2, batch    66 | loss: 68.3408802CurrentTrain: epoch  2, batch    67 | loss: 54.8709391CurrentTrain: epoch  2, batch    68 | loss: 121.6359500CurrentTrain: epoch  2, batch    69 | loss: 118.5582137CurrentTrain: epoch  2, batch    70 | loss: 52.8041816CurrentTrain: epoch  2, batch    71 | loss: 66.6465849CurrentTrain: epoch  2, batch    72 | loss: 67.4012153CurrentTrain: epoch  2, batch    73 | loss: 85.5175628CurrentTrain: epoch  2, batch    74 | loss: 56.7895471CurrentTrain: epoch  2, batch    75 | loss: 69.8190951CurrentTrain: epoch  2, batch    76 | loss: 72.2195226CurrentTrain: epoch  2, batch    77 | loss: 58.3885544CurrentTrain: epoch  2, batch    78 | loss: 112.6941711CurrentTrain: epoch  2, batch    79 | loss: 88.6788123CurrentTrain: epoch  2, batch    80 | loss: 118.5802080CurrentTrain: epoch  2, batch    81 | loss: 53.5904905CurrentTrain: epoch  2, batch    82 | loss: 58.3556294CurrentTrain: epoch  2, batch    83 | loss: 70.1161208CurrentTrain: epoch  2, batch    84 | loss: 54.4663602CurrentTrain: epoch  2, batch    85 | loss: 87.9177490CurrentTrain: epoch  2, batch    86 | loss: 52.1046831CurrentTrain: epoch  2, batch    87 | loss: 85.5254746CurrentTrain: epoch  2, batch    88 | loss: 71.0323850CurrentTrain: epoch  2, batch    89 | loss: 67.2169304CurrentTrain: epoch  2, batch    90 | loss: 52.2305724CurrentTrain: epoch  2, batch    91 | loss: 86.2733886CurrentTrain: epoch  2, batch    92 | loss: 90.3613298CurrentTrain: epoch  2, batch    93 | loss: 87.6513793CurrentTrain: epoch  2, batch    94 | loss: 184.4422732CurrentTrain: epoch  2, batch    95 | loss: 48.7244210CurrentTrain: epoch  3, batch     0 | loss: 57.2931946CurrentTrain: epoch  3, batch     1 | loss: 69.6312413CurrentTrain: epoch  3, batch     2 | loss: 67.6982277CurrentTrain: epoch  3, batch     3 | loss: 69.4000654CurrentTrain: epoch  3, batch     4 | loss: 64.3541548CurrentTrain: epoch  3, batch     5 | loss: 82.2814660CurrentTrain: epoch  3, batch     6 | loss: 59.5431750CurrentTrain: epoch  3, batch     7 | loss: 63.6432645CurrentTrain: epoch  3, batch     8 | loss: 45.8521829CurrentTrain: epoch  3, batch     9 | loss: 55.0699365CurrentTrain: epoch  3, batch    10 | loss: 53.1075015CurrentTrain: epoch  3, batch    11 | loss: 47.6160425CurrentTrain: epoch  3, batch    12 | loss: 64.0565133CurrentTrain: epoch  3, batch    13 | loss: 82.5725077CurrentTrain: epoch  3, batch    14 | loss: 119.1563188CurrentTrain: epoch  3, batch    15 | loss: 46.1618500CurrentTrain: epoch  3, batch    16 | loss: 87.3953267CurrentTrain: epoch  3, batch    17 | loss: 66.7498063CurrentTrain: epoch  3, batch    18 | loss: 81.8127903CurrentTrain: epoch  3, batch    19 | loss: 58.3232232CurrentTrain: epoch  3, batch    20 | loss: 44.8591365CurrentTrain: epoch  3, batch    21 | loss: 116.5830245CurrentTrain: epoch  3, batch    22 | loss: 85.4971388CurrentTrain: epoch  3, batch    23 | loss: 56.2449903CurrentTrain: epoch  3, batch    24 | loss: 69.8009558CurrentTrain: epoch  3, batch    25 | loss: 67.8380497CurrentTrain: epoch  3, batch    26 | loss: 88.5247127CurrentTrain: epoch  3, batch    27 | loss: 68.5939388CurrentTrain: epoch  3, batch    28 | loss: 118.4692906CurrentTrain: epoch  3, batch    29 | loss: 87.4940421CurrentTrain: epoch  3, batch    30 | loss: 68.5556909CurrentTrain: epoch  3, batch    31 | loss: 52.9542174CurrentTrain: epoch  3, batch    32 | loss: 89.1778432CurrentTrain: epoch  3, batch    33 | loss: 55.6719921CurrentTrain: epoch  3, batch    34 | loss: 48.1176779CurrentTrain: epoch  3, batch    35 | loss: 82.1317173CurrentTrain: epoch  3, batch    36 | loss: 52.4527311CurrentTrain: epoch  3, batch    37 | loss: 68.6941023CurrentTrain: epoch  3, batch    38 | loss: 87.5386910CurrentTrain: epoch  3, batch    39 | loss: 66.2913065CurrentTrain: epoch  3, batch    40 | loss: 67.4830256CurrentTrain: epoch  3, batch    41 | loss: 117.3904841CurrentTrain: epoch  3, batch    42 | loss: 85.0566625CurrentTrain: epoch  3, batch    43 | loss: 54.7916552CurrentTrain: epoch  3, batch    44 | loss: 66.9784782CurrentTrain: epoch  3, batch    45 | loss: 65.0468320CurrentTrain: epoch  3, batch    46 | loss: 52.0650458CurrentTrain: epoch  3, batch    47 | loss: 67.9105793CurrentTrain: epoch  3, batch    48 | loss: 114.5423269CurrentTrain: epoch  3, batch    49 | loss: 90.6014144CurrentTrain: epoch  3, batch    50 | loss: 55.4062736CurrentTrain: epoch  3, batch    51 | loss: 65.0878238CurrentTrain: epoch  3, batch    52 | loss: 86.8953593CurrentTrain: epoch  3, batch    53 | loss: 85.5891598CurrentTrain: epoch  3, batch    54 | loss: 53.8052816CurrentTrain: epoch  3, batch    55 | loss: 64.6866418CurrentTrain: epoch  3, batch    56 | loss: 57.9852750CurrentTrain: epoch  3, batch    57 | loss: 68.4025809CurrentTrain: epoch  3, batch    58 | loss: 45.0683877CurrentTrain: epoch  3, batch    59 | loss: 184.2044638CurrentTrain: epoch  3, batch    60 | loss: 116.5073730CurrentTrain: epoch  3, batch    61 | loss: 47.2990877CurrentTrain: epoch  3, batch    62 | loss: 57.2651777CurrentTrain: epoch  3, batch    63 | loss: 62.4752761CurrentTrain: epoch  3, batch    64 | loss: 67.3998849CurrentTrain: epoch  3, batch    65 | loss: 84.0582882CurrentTrain: epoch  3, batch    66 | loss: 67.8472608CurrentTrain: epoch  3, batch    67 | loss: 119.6299114CurrentTrain: epoch  3, batch    68 | loss: 57.4365775CurrentTrain: epoch  3, batch    69 | loss: 53.0332446CurrentTrain: epoch  3, batch    70 | loss: 57.6051926CurrentTrain: epoch  3, batch    71 | loss: 50.2114740CurrentTrain: epoch  3, batch    72 | loss: 64.0860989CurrentTrain: epoch  3, batch    73 | loss: 68.5610832CurrentTrain: epoch  3, batch    74 | loss: 56.1744480CurrentTrain: epoch  3, batch    75 | loss: 83.5589893CurrentTrain: epoch  3, batch    76 | loss: 178.6260417CurrentTrain: epoch  3, batch    77 | loss: 54.5488558CurrentTrain: epoch  3, batch    78 | loss: 70.5014326CurrentTrain: epoch  3, batch    79 | loss: 117.7906240CurrentTrain: epoch  3, batch    80 | loss: 179.8536810CurrentTrain: epoch  3, batch    81 | loss: 54.7199532CurrentTrain: epoch  3, batch    82 | loss: 57.0274088CurrentTrain: epoch  3, batch    83 | loss: 120.6106179CurrentTrain: epoch  3, batch    84 | loss: 88.4070599CurrentTrain: epoch  3, batch    85 | loss: 84.2469195CurrentTrain: epoch  3, batch    86 | loss: 55.7782022CurrentTrain: epoch  3, batch    87 | loss: 82.3766471CurrentTrain: epoch  3, batch    88 | loss: 65.9761207CurrentTrain: epoch  3, batch    89 | loss: 60.6229054CurrentTrain: epoch  3, batch    90 | loss: 64.8917993CurrentTrain: epoch  3, batch    91 | loss: 53.2246845CurrentTrain: epoch  3, batch    92 | loss: 118.8947910CurrentTrain: epoch  3, batch    93 | loss: 86.9414380CurrentTrain: epoch  3, batch    94 | loss: 57.3730222CurrentTrain: epoch  3, batch    95 | loss: 97.6968069CurrentTrain: epoch  4, batch     0 | loss: 86.6746765CurrentTrain: epoch  4, batch     1 | loss: 50.4191441CurrentTrain: epoch  4, batch     2 | loss: 57.6269391CurrentTrain: epoch  4, batch     3 | loss: 67.3865462CurrentTrain: epoch  4, batch     4 | loss: 44.4790977CurrentTrain: epoch  4, batch     5 | loss: 85.2075653CurrentTrain: epoch  4, batch     6 | loss: 183.5375142CurrentTrain: epoch  4, batch     7 | loss: 54.8684087CurrentTrain: epoch  4, batch     8 | loss: 53.8943687CurrentTrain: epoch  4, batch     9 | loss: 53.6997031CurrentTrain: epoch  4, batch    10 | loss: 117.7005612CurrentTrain: epoch  4, batch    11 | loss: 52.9279294CurrentTrain: epoch  4, batch    12 | loss: 64.6081685CurrentTrain: epoch  4, batch    13 | loss: 86.2452877CurrentTrain: epoch  4, batch    14 | loss: 85.9942652CurrentTrain: epoch  4, batch    15 | loss: 65.7587541CurrentTrain: epoch  4, batch    16 | loss: 66.6042415CurrentTrain: epoch  4, batch    17 | loss: 55.0321924CurrentTrain: epoch  4, batch    18 | loss: 63.6574930CurrentTrain: epoch  4, batch    19 | loss: 65.8116356CurrentTrain: epoch  4, batch    20 | loss: 182.3881669CurrentTrain: epoch  4, batch    21 | loss: 82.6236104CurrentTrain: epoch  4, batch    22 | loss: 84.7656796CurrentTrain: epoch  4, batch    23 | loss: 67.0121246CurrentTrain: epoch  4, batch    24 | loss: 55.2306322CurrentTrain: epoch  4, batch    25 | loss: 87.0243412CurrentTrain: epoch  4, batch    26 | loss: 87.1331796CurrentTrain: epoch  4, batch    27 | loss: 65.6939840CurrentTrain: epoch  4, batch    28 | loss: 66.4475327CurrentTrain: epoch  4, batch    29 | loss: 69.2153673CurrentTrain: epoch  4, batch    30 | loss: 53.9336432CurrentTrain: epoch  4, batch    31 | loss: 119.3293956CurrentTrain: epoch  4, batch    32 | loss: 64.3123252CurrentTrain: epoch  4, batch    33 | loss: 66.9786470CurrentTrain: epoch  4, batch    34 | loss: 54.0347457CurrentTrain: epoch  4, batch    35 | loss: 62.7823441CurrentTrain: epoch  4, batch    36 | loss: 66.6407445CurrentTrain: epoch  4, batch    37 | loss: 65.6817849CurrentTrain: epoch  4, batch    38 | loss: 64.8832935CurrentTrain: epoch  4, batch    39 | loss: 87.9375594CurrentTrain: epoch  4, batch    40 | loss: 81.7012821CurrentTrain: epoch  4, batch    41 | loss: 84.2215766CurrentTrain: epoch  4, batch    42 | loss: 67.4792900CurrentTrain: epoch  4, batch    43 | loss: 90.2751140CurrentTrain: epoch  4, batch    44 | loss: 82.7550511CurrentTrain: epoch  4, batch    45 | loss: 46.5906218CurrentTrain: epoch  4, batch    46 | loss: 372.5175419CurrentTrain: epoch  4, batch    47 | loss: 53.3642226CurrentTrain: epoch  4, batch    48 | loss: 65.4745329CurrentTrain: epoch  4, batch    49 | loss: 48.1633065CurrentTrain: epoch  4, batch    50 | loss: 86.6941891CurrentTrain: epoch  4, batch    51 | loss: 117.7137665CurrentTrain: epoch  4, batch    52 | loss: 79.1800808CurrentTrain: epoch  4, batch    53 | loss: 65.3842075CurrentTrain: epoch  4, batch    54 | loss: 51.1837051CurrentTrain: epoch  4, batch    55 | loss: 52.4748214CurrentTrain: epoch  4, batch    56 | loss: 53.4009144CurrentTrain: epoch  4, batch    57 | loss: 43.8511071CurrentTrain: epoch  4, batch    58 | loss: 113.2636169CurrentTrain: epoch  4, batch    59 | loss: 51.3929709CurrentTrain: epoch  4, batch    60 | loss: 54.5256097CurrentTrain: epoch  4, batch    61 | loss: 65.8002040CurrentTrain: epoch  4, batch    62 | loss: 52.3082204CurrentTrain: epoch  4, batch    63 | loss: 67.1653922CurrentTrain: epoch  4, batch    64 | loss: 65.2166612CurrentTrain: epoch  4, batch    65 | loss: 66.4656229CurrentTrain: epoch  4, batch    66 | loss: 84.4868740CurrentTrain: epoch  4, batch    67 | loss: 66.0051924CurrentTrain: epoch  4, batch    68 | loss: 83.9711159CurrentTrain: epoch  4, batch    69 | loss: 68.4883505CurrentTrain: epoch  4, batch    70 | loss: 83.2240323CurrentTrain: epoch  4, batch    71 | loss: 83.8907751CurrentTrain: epoch  4, batch    72 | loss: 52.2069566CurrentTrain: epoch  4, batch    73 | loss: 82.4992545CurrentTrain: epoch  4, batch    74 | loss: 43.2612536CurrentTrain: epoch  4, batch    75 | loss: 84.5451021CurrentTrain: epoch  4, batch    76 | loss: 64.7312145CurrentTrain: epoch  4, batch    77 | loss: 86.8862435CurrentTrain: epoch  4, batch    78 | loss: 86.5793219CurrentTrain: epoch  4, batch    79 | loss: 44.3899176CurrentTrain: epoch  4, batch    80 | loss: 67.2339074CurrentTrain: epoch  4, batch    81 | loss: 84.1679884CurrentTrain: epoch  4, batch    82 | loss: 64.8937961CurrentTrain: epoch  4, batch    83 | loss: 64.3480957CurrentTrain: epoch  4, batch    84 | loss: 88.7309585CurrentTrain: epoch  4, batch    85 | loss: 53.1847064CurrentTrain: epoch  4, batch    86 | loss: 47.7024165CurrentTrain: epoch  4, batch    87 | loss: 43.1146544CurrentTrain: epoch  4, batch    88 | loss: 89.8234461CurrentTrain: epoch  4, batch    89 | loss: 70.3528088CurrentTrain: epoch  4, batch    90 | loss: 65.7690872CurrentTrain: epoch  4, batch    91 | loss: 83.5487153CurrentTrain: epoch  4, batch    92 | loss: 64.4149883CurrentTrain: epoch  4, batch    93 | loss: 65.6478097CurrentTrain: epoch  4, batch    94 | loss: 54.3364649CurrentTrain: epoch  4, batch    95 | loss: 55.5577699CurrentTrain: epoch  5, batch     0 | loss: 70.4187754CurrentTrain: epoch  5, batch     1 | loss: 81.6322088CurrentTrain: epoch  5, batch     2 | loss: 62.4200989CurrentTrain: epoch  5, batch     3 | loss: 65.5061926CurrentTrain: epoch  5, batch     4 | loss: 53.0995521CurrentTrain: epoch  5, batch     5 | loss: 65.9391098CurrentTrain: epoch  5, batch     6 | loss: 54.7120351CurrentTrain: epoch  5, batch     7 | loss: 51.3745467CurrentTrain: epoch  5, batch     8 | loss: 61.0590858CurrentTrain: epoch  5, batch     9 | loss: 84.8459434CurrentTrain: epoch  5, batch    10 | loss: 115.4025416CurrentTrain: epoch  5, batch    11 | loss: 83.9152719CurrentTrain: epoch  5, batch    12 | loss: 62.6793993CurrentTrain: epoch  5, batch    13 | loss: 64.1221295CurrentTrain: epoch  5, batch    14 | loss: 66.6102616CurrentTrain: epoch  5, batch    15 | loss: 84.4293082CurrentTrain: epoch  5, batch    16 | loss: 78.7646518CurrentTrain: epoch  5, batch    17 | loss: 51.3123454CurrentTrain: epoch  5, batch    18 | loss: 64.1588872CurrentTrain: epoch  5, batch    19 | loss: 86.4641572CurrentTrain: epoch  5, batch    20 | loss: 82.1254201CurrentTrain: epoch  5, batch    21 | loss: 88.7451519CurrentTrain: epoch  5, batch    22 | loss: 62.4149253CurrentTrain: epoch  5, batch    23 | loss: 84.8583653CurrentTrain: epoch  5, batch    24 | loss: 64.5770162CurrentTrain: epoch  5, batch    25 | loss: 53.1716652CurrentTrain: epoch  5, batch    26 | loss: 61.3497774CurrentTrain: epoch  5, batch    27 | loss: 120.3347440CurrentTrain: epoch  5, batch    28 | loss: 67.9163595CurrentTrain: epoch  5, batch    29 | loss: 87.9402039CurrentTrain: epoch  5, batch    30 | loss: 45.8162290CurrentTrain: epoch  5, batch    31 | loss: 64.8805518CurrentTrain: epoch  5, batch    32 | loss: 66.0899240CurrentTrain: epoch  5, batch    33 | loss: 117.4442805CurrentTrain: epoch  5, batch    34 | loss: 63.7956115CurrentTrain: epoch  5, batch    35 | loss: 56.4754321CurrentTrain: epoch  5, batch    36 | loss: 51.8854730CurrentTrain: epoch  5, batch    37 | loss: 52.8190891CurrentTrain: epoch  5, batch    38 | loss: 64.9042803CurrentTrain: epoch  5, batch    39 | loss: 52.2574082CurrentTrain: epoch  5, batch    40 | loss: 51.2766608CurrentTrain: epoch  5, batch    41 | loss: 51.2319522CurrentTrain: epoch  5, batch    42 | loss: 66.0326708CurrentTrain: epoch  5, batch    43 | loss: 41.3555185CurrentTrain: epoch  5, batch    44 | loss: 67.5181593CurrentTrain: epoch  5, batch    45 | loss: 63.9607841CurrentTrain: epoch  5, batch    46 | loss: 53.1536524CurrentTrain: epoch  5, batch    47 | loss: 63.1213863CurrentTrain: epoch  5, batch    48 | loss: 65.7759323CurrentTrain: epoch  5, batch    49 | loss: 64.6247393CurrentTrain: epoch  5, batch    50 | loss: 67.0747208CurrentTrain: epoch  5, batch    51 | loss: 66.2772430CurrentTrain: epoch  5, batch    52 | loss: 64.5117375CurrentTrain: epoch  5, batch    53 | loss: 87.5277982CurrentTrain: epoch  5, batch    54 | loss: 61.5182393CurrentTrain: epoch  5, batch    55 | loss: 66.3939008CurrentTrain: epoch  5, batch    56 | loss: 64.2949588CurrentTrain: epoch  5, batch    57 | loss: 117.7730055CurrentTrain: epoch  5, batch    58 | loss: 69.6330936CurrentTrain: epoch  5, batch    59 | loss: 52.5227394CurrentTrain: epoch  5, batch    60 | loss: 45.4037019CurrentTrain: epoch  5, batch    61 | loss: 372.5413292CurrentTrain: epoch  5, batch    62 | loss: 51.8671045CurrentTrain: epoch  5, batch    63 | loss: 86.1265668CurrentTrain: epoch  5, batch    64 | loss: 84.3099126CurrentTrain: epoch  5, batch    65 | loss: 55.3089141CurrentTrain: epoch  5, batch    66 | loss: 120.0416478CurrentTrain: epoch  5, batch    67 | loss: 81.6725513CurrentTrain: epoch  5, batch    68 | loss: 119.3620688CurrentTrain: epoch  5, batch    69 | loss: 51.8008944CurrentTrain: epoch  5, batch    70 | loss: 81.5727762CurrentTrain: epoch  5, batch    71 | loss: 69.5972249CurrentTrain: epoch  5, batch    72 | loss: 45.1088284CurrentTrain: epoch  5, batch    73 | loss: 68.5593856CurrentTrain: epoch  5, batch    74 | loss: 65.5834092CurrentTrain: epoch  5, batch    75 | loss: 89.1589059CurrentTrain: epoch  5, batch    76 | loss: 82.8827499CurrentTrain: epoch  5, batch    77 | loss: 54.3195961CurrentTrain: epoch  5, batch    78 | loss: 43.6511930CurrentTrain: epoch  5, batch    79 | loss: 114.0925095CurrentTrain: epoch  5, batch    80 | loss: 83.4083979CurrentTrain: epoch  5, batch    81 | loss: 45.2301550CurrentTrain: epoch  5, batch    82 | loss: 82.6768689CurrentTrain: epoch  5, batch    83 | loss: 84.1965103CurrentTrain: epoch  5, batch    84 | loss: 84.0708925CurrentTrain: epoch  5, batch    85 | loss: 82.4198155CurrentTrain: epoch  5, batch    86 | loss: 57.8133390CurrentTrain: epoch  5, batch    87 | loss: 57.0725508CurrentTrain: epoch  5, batch    88 | loss: 55.8498325CurrentTrain: epoch  5, batch    89 | loss: 84.2526726CurrentTrain: epoch  5, batch    90 | loss: 113.8969612CurrentTrain: epoch  5, batch    91 | loss: 40.6533265CurrentTrain: epoch  5, batch    92 | loss: 117.8696040CurrentTrain: epoch  5, batch    93 | loss: 86.3680351CurrentTrain: epoch  5, batch    94 | loss: 43.9178279CurrentTrain: epoch  5, batch    95 | loss: 54.5891733CurrentTrain: epoch  6, batch     0 | loss: 86.8931492CurrentTrain: epoch  6, batch     1 | loss: 63.6688144CurrentTrain: epoch  6, batch     2 | loss: 86.4171725CurrentTrain: epoch  6, batch     3 | loss: 64.9495505CurrentTrain: epoch  6, batch     4 | loss: 119.4890362CurrentTrain: epoch  6, batch     5 | loss: 66.7626985CurrentTrain: epoch  6, batch     6 | loss: 65.8351106CurrentTrain: epoch  6, batch     7 | loss: 52.6822397CurrentTrain: epoch  6, batch     8 | loss: 50.7095469CurrentTrain: epoch  6, batch     9 | loss: 65.3879497CurrentTrain: epoch  6, batch    10 | loss: 82.6639514CurrentTrain: epoch  6, batch    11 | loss: 62.9302944CurrentTrain: epoch  6, batch    12 | loss: 44.4592397CurrentTrain: epoch  6, batch    13 | loss: 52.3821913CurrentTrain: epoch  6, batch    14 | loss: 66.6497031CurrentTrain: epoch  6, batch    15 | loss: 64.7175436CurrentTrain: epoch  6, batch    16 | loss: 65.2276663CurrentTrain: epoch  6, batch    17 | loss: 43.5470887CurrentTrain: epoch  6, batch    18 | loss: 67.3016409CurrentTrain: epoch  6, batch    19 | loss: 53.0463522CurrentTrain: epoch  6, batch    20 | loss: 66.5465240CurrentTrain: epoch  6, batch    21 | loss: 45.5975349CurrentTrain: epoch  6, batch    22 | loss: 80.1861826CurrentTrain: epoch  6, batch    23 | loss: 63.3383617CurrentTrain: epoch  6, batch    24 | loss: 118.5610714CurrentTrain: epoch  6, batch    25 | loss: 64.9776055CurrentTrain: epoch  6, batch    26 | loss: 117.9697353CurrentTrain: epoch  6, batch    27 | loss: 86.4119909CurrentTrain: epoch  6, batch    28 | loss: 85.0731643CurrentTrain: epoch  6, batch    29 | loss: 80.0563098CurrentTrain: epoch  6, batch    30 | loss: 82.6838745CurrentTrain: epoch  6, batch    31 | loss: 66.5277011CurrentTrain: epoch  6, batch    32 | loss: 50.0505984CurrentTrain: epoch  6, batch    33 | loss: 86.2224883CurrentTrain: epoch  6, batch    34 | loss: 67.4095517CurrentTrain: epoch  6, batch    35 | loss: 54.9763423CurrentTrain: epoch  6, batch    36 | loss: 65.5500120CurrentTrain: epoch  6, batch    37 | loss: 42.9287813CurrentTrain: epoch  6, batch    38 | loss: 57.0008801CurrentTrain: epoch  6, batch    39 | loss: 66.6591273CurrentTrain: epoch  6, batch    40 | loss: 42.1057369CurrentTrain: epoch  6, batch    41 | loss: 44.9966369CurrentTrain: epoch  6, batch    42 | loss: 65.9255496CurrentTrain: epoch  6, batch    43 | loss: 50.1691610CurrentTrain: epoch  6, batch    44 | loss: 88.0210006CurrentTrain: epoch  6, batch    45 | loss: 82.8450682CurrentTrain: epoch  6, batch    46 | loss: 87.5820851CurrentTrain: epoch  6, batch    47 | loss: 69.2031119CurrentTrain: epoch  6, batch    48 | loss: 81.4534525CurrentTrain: epoch  6, batch    49 | loss: 45.5534423CurrentTrain: epoch  6, batch    50 | loss: 45.1175903CurrentTrain: epoch  6, batch    51 | loss: 86.3811199CurrentTrain: epoch  6, batch    52 | loss: 66.3136707CurrentTrain: epoch  6, batch    53 | loss: 49.2993108CurrentTrain: epoch  6, batch    54 | loss: 87.3046741CurrentTrain: epoch  6, batch    55 | loss: 62.2419710CurrentTrain: epoch  6, batch    56 | loss: 44.9360245CurrentTrain: epoch  6, batch    57 | loss: 117.5353927CurrentTrain: epoch  6, batch    58 | loss: 69.7631212CurrentTrain: epoch  6, batch    59 | loss: 53.7318706CurrentTrain: epoch  6, batch    60 | loss: 63.9918114CurrentTrain: epoch  6, batch    61 | loss: 53.9917254CurrentTrain: epoch  6, batch    62 | loss: 56.3128506CurrentTrain: epoch  6, batch    63 | loss: 65.8411998CurrentTrain: epoch  6, batch    64 | loss: 70.7463208CurrentTrain: epoch  6, batch    65 | loss: 49.4570203CurrentTrain: epoch  6, batch    66 | loss: 62.5889892CurrentTrain: epoch  6, batch    67 | loss: 83.2020827CurrentTrain: epoch  6, batch    68 | loss: 67.1655661CurrentTrain: epoch  6, batch    69 | loss: 82.6772684CurrentTrain: epoch  6, batch    70 | loss: 67.2004708CurrentTrain: epoch  6, batch    71 | loss: 51.9932177CurrentTrain: epoch  6, batch    72 | loss: 119.6681547CurrentTrain: epoch  6, batch    73 | loss: 51.2469774CurrentTrain: epoch  6, batch    74 | loss: 91.8153564CurrentTrain: epoch  6, batch    75 | loss: 54.3396539CurrentTrain: epoch  6, batch    76 | loss: 43.6956161CurrentTrain: epoch  6, batch    77 | loss: 50.2657394CurrentTrain: epoch  6, batch    78 | loss: 65.1832272CurrentTrain: epoch  6, batch    79 | loss: 50.6011297CurrentTrain: epoch  6, batch    80 | loss: 43.3990979CurrentTrain: epoch  6, batch    81 | loss: 85.1358648CurrentTrain: epoch  6, batch    82 | loss: 52.9342705CurrentTrain: epoch  6, batch    83 | loss: 56.2165686CurrentTrain: epoch  6, batch    84 | loss: 65.7999807CurrentTrain: epoch  6, batch    85 | loss: 46.0040210CurrentTrain: epoch  6, batch    86 | loss: 115.7146355CurrentTrain: epoch  6, batch    87 | loss: 81.1967509CurrentTrain: epoch  6, batch    88 | loss: 62.0555037CurrentTrain: epoch  6, batch    89 | loss: 52.9295364CurrentTrain: epoch  6, batch    90 | loss: 52.7437905CurrentTrain: epoch  6, batch    91 | loss: 81.1841888CurrentTrain: epoch  6, batch    92 | loss: 112.9027042CurrentTrain: epoch  6, batch    93 | loss: 64.8760486CurrentTrain: epoch  6, batch    94 | loss: 84.1089607CurrentTrain: epoch  6, batch    95 | loss: 42.1614223CurrentTrain: epoch  7, batch     0 | loss: 51.3256186CurrentTrain: epoch  7, batch     1 | loss: 51.5256568CurrentTrain: epoch  7, batch     2 | loss: 67.6743231CurrentTrain: epoch  7, batch     3 | loss: 48.6143780CurrentTrain: epoch  7, batch     4 | loss: 116.0267319CurrentTrain: epoch  7, batch     5 | loss: 51.2254586CurrentTrain: epoch  7, batch     6 | loss: 66.9265143CurrentTrain: epoch  7, batch     7 | loss: 65.5496647CurrentTrain: epoch  7, batch     8 | loss: 64.8833033CurrentTrain: epoch  7, batch     9 | loss: 82.3988731CurrentTrain: epoch  7, batch    10 | loss: 86.5371295CurrentTrain: epoch  7, batch    11 | loss: 114.3932013CurrentTrain: epoch  7, batch    12 | loss: 81.2605324CurrentTrain: epoch  7, batch    13 | loss: 115.7199583CurrentTrain: epoch  7, batch    14 | loss: 83.3102027CurrentTrain: epoch  7, batch    15 | loss: 63.5042161CurrentTrain: epoch  7, batch    16 | loss: 63.9628363CurrentTrain: epoch  7, batch    17 | loss: 61.1405183CurrentTrain: epoch  7, batch    18 | loss: 65.6425800CurrentTrain: epoch  7, batch    19 | loss: 52.0847649CurrentTrain: epoch  7, batch    20 | loss: 49.2876835CurrentTrain: epoch  7, batch    21 | loss: 112.1137356CurrentTrain: epoch  7, batch    22 | loss: 65.7029167CurrentTrain: epoch  7, batch    23 | loss: 68.3498898CurrentTrain: epoch  7, batch    24 | loss: 65.5558279CurrentTrain: epoch  7, batch    25 | loss: 51.2972888CurrentTrain: epoch  7, batch    26 | loss: 65.7068992CurrentTrain: epoch  7, batch    27 | loss: 64.3728745CurrentTrain: epoch  7, batch    28 | loss: 81.0753018CurrentTrain: epoch  7, batch    29 | loss: 63.0190598CurrentTrain: epoch  7, batch    30 | loss: 62.9497297CurrentTrain: epoch  7, batch    31 | loss: 83.4031020CurrentTrain: epoch  7, batch    32 | loss: 62.4527155CurrentTrain: epoch  7, batch    33 | loss: 83.8546002CurrentTrain: epoch  7, batch    34 | loss: 63.0946532CurrentTrain: epoch  7, batch    35 | loss: 64.5386628CurrentTrain: epoch  7, batch    36 | loss: 84.8561043CurrentTrain: epoch  7, batch    37 | loss: 67.3430295CurrentTrain: epoch  7, batch    38 | loss: 61.8083963CurrentTrain: epoch  7, batch    39 | loss: 54.9958240CurrentTrain: epoch  7, batch    40 | loss: 86.8136829CurrentTrain: epoch  7, batch    41 | loss: 86.7993813CurrentTrain: epoch  7, batch    42 | loss: 45.0647352CurrentTrain: epoch  7, batch    43 | loss: 54.8375877CurrentTrain: epoch  7, batch    44 | loss: 65.5107527CurrentTrain: epoch  7, batch    45 | loss: 67.1944832CurrentTrain: epoch  7, batch    46 | loss: 43.5116950CurrentTrain: epoch  7, batch    47 | loss: 43.4376585CurrentTrain: epoch  7, batch    48 | loss: 55.1117014CurrentTrain: epoch  7, batch    49 | loss: 86.2836564CurrentTrain: epoch  7, batch    50 | loss: 80.0751681CurrentTrain: epoch  7, batch    51 | loss: 85.7647889CurrentTrain: epoch  7, batch    52 | loss: 54.2967116CurrentTrain: epoch  7, batch    53 | loss: 117.9675949CurrentTrain: epoch  7, batch    54 | loss: 112.7453029CurrentTrain: epoch  7, batch    55 | loss: 54.6474236CurrentTrain: epoch  7, batch    56 | loss: 50.4377036CurrentTrain: epoch  7, batch    57 | loss: 115.4428747CurrentTrain: epoch  7, batch    58 | loss: 85.8908047CurrentTrain: epoch  7, batch    59 | loss: 113.6505344CurrentTrain: epoch  7, batch    60 | loss: 51.3355911CurrentTrain: epoch  7, batch    61 | loss: 65.5393999CurrentTrain: epoch  7, batch    62 | loss: 84.1464764CurrentTrain: epoch  7, batch    63 | loss: 49.2757461CurrentTrain: epoch  7, batch    64 | loss: 51.8598866CurrentTrain: epoch  7, batch    65 | loss: 82.4093853CurrentTrain: epoch  7, batch    66 | loss: 43.0276854CurrentTrain: epoch  7, batch    67 | loss: 84.1805249CurrentTrain: epoch  7, batch    68 | loss: 65.2738087CurrentTrain: epoch  7, batch    69 | loss: 83.9696515CurrentTrain: epoch  7, batch    70 | loss: 67.6726123CurrentTrain: epoch  7, batch    71 | loss: 49.5869530CurrentTrain: epoch  7, batch    72 | loss: 54.2956219CurrentTrain: epoch  7, batch    73 | loss: 82.4389279CurrentTrain: epoch  7, batch    74 | loss: 85.3016608CurrentTrain: epoch  7, batch    75 | loss: 80.3234135CurrentTrain: epoch  7, batch    76 | loss: 52.5442834CurrentTrain: epoch  7, batch    77 | loss: 65.5694132CurrentTrain: epoch  7, batch    78 | loss: 65.4531445CurrentTrain: epoch  7, batch    79 | loss: 115.2619285CurrentTrain: epoch  7, batch    80 | loss: 52.9472793CurrentTrain: epoch  7, batch    81 | loss: 111.5994579CurrentTrain: epoch  7, batch    82 | loss: 63.5459265CurrentTrain: epoch  7, batch    83 | loss: 40.2909690CurrentTrain: epoch  7, batch    84 | loss: 91.3765114CurrentTrain: epoch  7, batch    85 | loss: 84.6631196CurrentTrain: epoch  7, batch    86 | loss: 82.7731121CurrentTrain: epoch  7, batch    87 | loss: 84.3184937CurrentTrain: epoch  7, batch    88 | loss: 45.3184069CurrentTrain: epoch  7, batch    89 | loss: 50.2330571CurrentTrain: epoch  7, batch    90 | loss: 81.2838903CurrentTrain: epoch  7, batch    91 | loss: 54.1919531CurrentTrain: epoch  7, batch    92 | loss: 111.6288937CurrentTrain: epoch  7, batch    93 | loss: 63.5635521CurrentTrain: epoch  7, batch    94 | loss: 40.4241538CurrentTrain: epoch  7, batch    95 | loss: 54.4826902CurrentTrain: epoch  8, batch     0 | loss: 85.7639237CurrentTrain: epoch  8, batch     1 | loss: 114.0378138CurrentTrain: epoch  8, batch     2 | loss: 116.3888377CurrentTrain: epoch  8, batch     3 | loss: 80.9659897CurrentTrain: epoch  8, batch     4 | loss: 117.8753610CurrentTrain: epoch  8, batch     5 | loss: 64.2822667CurrentTrain: epoch  8, batch     6 | loss: 44.5419132CurrentTrain: epoch  8, batch     7 | loss: 112.1522143CurrentTrain: epoch  8, batch     8 | loss: 49.5311322CurrentTrain: epoch  8, batch     9 | loss: 67.1012215CurrentTrain: epoch  8, batch    10 | loss: 65.4873644CurrentTrain: epoch  8, batch    11 | loss: 83.6586151CurrentTrain: epoch  8, batch    12 | loss: 114.3657451CurrentTrain: epoch  8, batch    13 | loss: 64.7555441CurrentTrain: epoch  8, batch    14 | loss: 115.4033462CurrentTrain: epoch  8, batch    15 | loss: 85.5218640CurrentTrain: epoch  8, batch    16 | loss: 86.8220842CurrentTrain: epoch  8, batch    17 | loss: 84.0670846CurrentTrain: epoch  8, batch    18 | loss: 43.1777944CurrentTrain: epoch  8, batch    19 | loss: 66.5560493CurrentTrain: epoch  8, batch    20 | loss: 63.3957203CurrentTrain: epoch  8, batch    21 | loss: 82.6611709CurrentTrain: epoch  8, batch    22 | loss: 63.5130424CurrentTrain: epoch  8, batch    23 | loss: 52.9787150CurrentTrain: epoch  8, batch    24 | loss: 64.2175828CurrentTrain: epoch  8, batch    25 | loss: 84.0745573CurrentTrain: epoch  8, batch    26 | loss: 66.8599485CurrentTrain: epoch  8, batch    27 | loss: 82.9222517CurrentTrain: epoch  8, batch    28 | loss: 66.4912595CurrentTrain: epoch  8, batch    29 | loss: 111.0399082CurrentTrain: epoch  8, batch    30 | loss: 60.6881017CurrentTrain: epoch  8, batch    31 | loss: 64.5971488CurrentTrain: epoch  8, batch    32 | loss: 79.5649747CurrentTrain: epoch  8, batch    33 | loss: 64.2368193CurrentTrain: epoch  8, batch    34 | loss: 60.3631682CurrentTrain: epoch  8, batch    35 | loss: 65.7587341CurrentTrain: epoch  8, batch    36 | loss: 52.7911215CurrentTrain: epoch  8, batch    37 | loss: 44.0149002CurrentTrain: epoch  8, batch    38 | loss: 64.4003380CurrentTrain: epoch  8, batch    39 | loss: 66.9125187CurrentTrain: epoch  8, batch    40 | loss: 86.1909960CurrentTrain: epoch  8, batch    41 | loss: 65.9709127CurrentTrain: epoch  8, batch    42 | loss: 82.8818112CurrentTrain: epoch  8, batch    43 | loss: 64.8156632CurrentTrain: epoch  8, batch    44 | loss: 65.3451464CurrentTrain: epoch  8, batch    45 | loss: 40.6085156CurrentTrain: epoch  8, batch    46 | loss: 49.9196820CurrentTrain: epoch  8, batch    47 | loss: 117.5066407CurrentTrain: epoch  8, batch    48 | loss: 50.8990892CurrentTrain: epoch  8, batch    49 | loss: 54.6317892CurrentTrain: epoch  8, batch    50 | loss: 41.8325895CurrentTrain: epoch  8, batch    51 | loss: 65.4925079CurrentTrain: epoch  8, batch    52 | loss: 84.2774458CurrentTrain: epoch  8, batch    53 | loss: 82.8454127CurrentTrain: epoch  8, batch    54 | loss: 119.0182286CurrentTrain: epoch  8, batch    55 | loss: 51.2268500CurrentTrain: epoch  8, batch    56 | loss: 82.1889262CurrentTrain: epoch  8, batch    57 | loss: 52.5116276CurrentTrain: epoch  8, batch    58 | loss: 64.9005919CurrentTrain: epoch  8, batch    59 | loss: 51.0061077CurrentTrain: epoch  8, batch    60 | loss: 63.1166942CurrentTrain: epoch  8, batch    61 | loss: 62.0559049CurrentTrain: epoch  8, batch    62 | loss: 60.4091937CurrentTrain: epoch  8, batch    63 | loss: 53.6852533CurrentTrain: epoch  8, batch    64 | loss: 81.3501846CurrentTrain: epoch  8, batch    65 | loss: 63.4020254CurrentTrain: epoch  8, batch    66 | loss: 40.9153992CurrentTrain: epoch  8, batch    67 | loss: 66.2793219CurrentTrain: epoch  8, batch    68 | loss: 44.1287645CurrentTrain: epoch  8, batch    69 | loss: 115.3427128CurrentTrain: epoch  8, batch    70 | loss: 83.2366293CurrentTrain: epoch  8, batch    71 | loss: 85.7807547CurrentTrain: epoch  8, batch    72 | loss: 54.5186401CurrentTrain: epoch  8, batch    73 | loss: 79.6434831CurrentTrain: epoch  8, batch    74 | loss: 81.8917584CurrentTrain: epoch  8, batch    75 | loss: 64.4240495CurrentTrain: epoch  8, batch    76 | loss: 53.2996147CurrentTrain: epoch  8, batch    77 | loss: 117.5889405CurrentTrain: epoch  8, batch    78 | loss: 54.4673067CurrentTrain: epoch  8, batch    79 | loss: 66.5088666CurrentTrain: epoch  8, batch    80 | loss: 42.9915473CurrentTrain: epoch  8, batch    81 | loss: 113.6516758CurrentTrain: epoch  8, batch    82 | loss: 85.7298984CurrentTrain: epoch  8, batch    83 | loss: 49.3456073CurrentTrain: epoch  8, batch    84 | loss: 52.1292734CurrentTrain: epoch  8, batch    85 | loss: 61.3595294CurrentTrain: epoch  8, batch    86 | loss: 49.5598652CurrentTrain: epoch  8, batch    87 | loss: 64.2508179CurrentTrain: epoch  8, batch    88 | loss: 65.0787744CurrentTrain: epoch  8, batch    89 | loss: 85.0483813CurrentTrain: epoch  8, batch    90 | loss: 84.2114324CurrentTrain: epoch  8, batch    91 | loss: 64.1726162CurrentTrain: epoch  8, batch    92 | loss: 51.9263118CurrentTrain: epoch  8, batch    93 | loss: 66.3301893CurrentTrain: epoch  8, batch    94 | loss: 61.6237608CurrentTrain: epoch  8, batch    95 | loss: 53.7505902CurrentTrain: epoch  9, batch     0 | loss: 80.9719880CurrentTrain: epoch  9, batch     1 | loss: 79.7561125CurrentTrain: epoch  9, batch     2 | loss: 53.1866901CurrentTrain: epoch  9, batch     3 | loss: 43.7837842CurrentTrain: epoch  9, batch     4 | loss: 85.8812892CurrentTrain: epoch  9, batch     5 | loss: 42.5025211CurrentTrain: epoch  9, batch     6 | loss: 65.8094612CurrentTrain: epoch  9, batch     7 | loss: 53.2983964CurrentTrain: epoch  9, batch     8 | loss: 86.4614139CurrentTrain: epoch  9, batch     9 | loss: 181.8029366CurrentTrain: epoch  9, batch    10 | loss: 63.9244463CurrentTrain: epoch  9, batch    11 | loss: 174.7493612CurrentTrain: epoch  9, batch    12 | loss: 65.3775443CurrentTrain: epoch  9, batch    13 | loss: 62.9658362CurrentTrain: epoch  9, batch    14 | loss: 63.3289867CurrentTrain: epoch  9, batch    15 | loss: 65.7274642CurrentTrain: epoch  9, batch    16 | loss: 82.6584747CurrentTrain: epoch  9, batch    17 | loss: 50.5995214CurrentTrain: epoch  9, batch    18 | loss: 65.5403689CurrentTrain: epoch  9, batch    19 | loss: 49.6750764CurrentTrain: epoch  9, batch    20 | loss: 50.2339053CurrentTrain: epoch  9, batch    21 | loss: 64.2235922CurrentTrain: epoch  9, batch    22 | loss: 49.2714323CurrentTrain: epoch  9, batch    23 | loss: 43.0438499CurrentTrain: epoch  9, batch    24 | loss: 47.5523446CurrentTrain: epoch  9, batch    25 | loss: 82.6987326CurrentTrain: epoch  9, batch    26 | loss: 61.5476695CurrentTrain: epoch  9, batch    27 | loss: 51.1716831CurrentTrain: epoch  9, batch    28 | loss: 63.9131014CurrentTrain: epoch  9, batch    29 | loss: 63.5200114CurrentTrain: epoch  9, batch    30 | loss: 84.3999914CurrentTrain: epoch  9, batch    31 | loss: 67.1458930CurrentTrain: epoch  9, batch    32 | loss: 50.2939459CurrentTrain: epoch  9, batch    33 | loss: 50.6238113CurrentTrain: epoch  9, batch    34 | loss: 49.9585694CurrentTrain: epoch  9, batch    35 | loss: 53.0191940CurrentTrain: epoch  9, batch    36 | loss: 50.8074286CurrentTrain: epoch  9, batch    37 | loss: 78.6926949CurrentTrain: epoch  9, batch    38 | loss: 181.9782857CurrentTrain: epoch  9, batch    39 | loss: 66.6803142CurrentTrain: epoch  9, batch    40 | loss: 84.1922668CurrentTrain: epoch  9, batch    41 | loss: 65.4793605CurrentTrain: epoch  9, batch    42 | loss: 44.0866959CurrentTrain: epoch  9, batch    43 | loss: 85.1477322CurrentTrain: epoch  9, batch    44 | loss: 64.6461628CurrentTrain: epoch  9, batch    45 | loss: 52.9013176CurrentTrain: epoch  9, batch    46 | loss: 41.3079559CurrentTrain: epoch  9, batch    47 | loss: 115.7588952CurrentTrain: epoch  9, batch    48 | loss: 66.2009868CurrentTrain: epoch  9, batch    49 | loss: 115.1783262CurrentTrain: epoch  9, batch    50 | loss: 62.6993024CurrentTrain: epoch  9, batch    51 | loss: 51.5422752CurrentTrain: epoch  9, batch    52 | loss: 82.8060986CurrentTrain: epoch  9, batch    53 | loss: 80.0306174CurrentTrain: epoch  9, batch    54 | loss: 79.3834320CurrentTrain: epoch  9, batch    55 | loss: 52.9598931CurrentTrain: epoch  9, batch    56 | loss: 54.2228857CurrentTrain: epoch  9, batch    57 | loss: 118.6444541CurrentTrain: epoch  9, batch    58 | loss: 117.4855985CurrentTrain: epoch  9, batch    59 | loss: 41.3871444CurrentTrain: epoch  9, batch    60 | loss: 49.1766472CurrentTrain: epoch  9, batch    61 | loss: 113.4452453CurrentTrain: epoch  9, batch    62 | loss: 84.0518868CurrentTrain: epoch  9, batch    63 | loss: 67.5427522CurrentTrain: epoch  9, batch    64 | loss: 53.2524877CurrentTrain: epoch  9, batch    65 | loss: 84.8810873CurrentTrain: epoch  9, batch    66 | loss: 44.4022949CurrentTrain: epoch  9, batch    67 | loss: 54.0532247CurrentTrain: epoch  9, batch    68 | loss: 65.9090876CurrentTrain: epoch  9, batch    69 | loss: 113.3077193CurrentTrain: epoch  9, batch    70 | loss: 53.5237927CurrentTrain: epoch  9, batch    71 | loss: 64.0146731CurrentTrain: epoch  9, batch    72 | loss: 51.5724178CurrentTrain: epoch  9, batch    73 | loss: 84.5136186CurrentTrain: epoch  9, batch    74 | loss: 48.2768035CurrentTrain: epoch  9, batch    75 | loss: 55.7932469CurrentTrain: epoch  9, batch    76 | loss: 63.5076143CurrentTrain: epoch  9, batch    77 | loss: 82.8132275CurrentTrain: epoch  9, batch    78 | loss: 51.5417865CurrentTrain: epoch  9, batch    79 | loss: 67.7379945CurrentTrain: epoch  9, batch    80 | loss: 66.8854819CurrentTrain: epoch  9, batch    81 | loss: 55.5231829CurrentTrain: epoch  9, batch    82 | loss: 50.7366727CurrentTrain: epoch  9, batch    83 | loss: 51.4162865CurrentTrain: epoch  9, batch    84 | loss: 63.2853388CurrentTrain: epoch  9, batch    85 | loss: 66.8344082CurrentTrain: epoch  9, batch    86 | loss: 85.7010213CurrentTrain: epoch  9, batch    87 | loss: 63.0459934CurrentTrain: epoch  9, batch    88 | loss: 181.5073827CurrentTrain: epoch  9, batch    89 | loss: 63.0573164CurrentTrain: epoch  9, batch    90 | loss: 69.9660076CurrentTrain: epoch  9, batch    91 | loss: 66.1366586CurrentTrain: epoch  9, batch    92 | loss: 64.1482247CurrentTrain: epoch  9, batch    93 | loss: 81.0689737CurrentTrain: epoch  9, batch    94 | loss: 49.6032565CurrentTrain: epoch  9, batch    95 | loss: 52.5617782

F1 score per class: {32: 0.6358381502890174, 6: 0.8764044943820225, 19: 0.4, 24: 0.7717391304347826, 26: 0.907103825136612, 29: 0.8808290155440415}
Micro-average F1 score: 0.8055555555555556
Weighted-average F1 score: 0.8124606182329005
F1 score per class: {32: 0.6850828729281768, 6: 0.907103825136612, 19: 0.4444444444444444, 24: 0.75, 26: 0.9583333333333334, 29: 0.8526315789473684}
Micro-average F1 score: 0.8172484599589322
Weighted-average F1 score: 0.8172913135207258
F1 score per class: {32: 0.6813186813186813, 6: 0.907103825136612, 19: 0.4375, 24: 0.7539267015706806, 26: 0.9583333333333334, 29: 0.8704663212435233}
Micro-average F1 score: 0.8221993833504625
Weighted-average F1 score: 0.824198052175202

F1 score per class: {32: 0.6358381502890174, 6: 0.8764044943820225, 19: 0.4, 24: 0.7717391304347826, 26: 0.907103825136612, 29: 0.8808290155440415}
Micro-average F1 score: 0.8055555555555556
Weighted-average F1 score: 0.8124606182329005
F1 score per class: {32: 0.6850828729281768, 6: 0.907103825136612, 19: 0.4444444444444444, 24: 0.75, 26: 0.9583333333333334, 29: 0.8526315789473684}
Micro-average F1 score: 0.8172484599589322
Weighted-average F1 score: 0.8172913135207258
F1 score per class: {32: 0.6813186813186813, 6: 0.907103825136612, 19: 0.4375, 24: 0.7539267015706806, 26: 0.9583333333333334, 29: 0.8704663212435233}
Micro-average F1 score: 0.8221993833504625
Weighted-average F1 score: 0.824198052175202

F1 score per class: {32: 0.48034934497816595, 6: 0.6812227074235808, 19: 0.23255813953488372, 24: 0.7282051282051282, 26: 0.8736842105263158, 29: 0.7798165137614679}
Micro-average F1 score: 0.6829710144927537
Weighted-average F1 score: 0.6748910631380652
F1 score per class: {32: 0.4940239043824701, 6: 0.6747967479674797, 19: 0.14953271028037382, 24: 0.6605504587155964, 26: 0.9019607843137255, 29: 0.7431192660550459}
Micro-average F1 score: 0.639871382636656
Weighted-average F1 score: 0.6141367700709662
F1 score per class: {32: 0.49206349206349204, 6: 0.6747967479674797, 19: 0.19444444444444445, 24: 0.6666666666666666, 26: 0.8975609756097561, 29: 0.7433628318584071}
Micro-average F1 score: 0.657354149548069
Weighted-average F1 score: 0.6424247905342201

F1 score per class: {32: 0.48034934497816595, 6: 0.6812227074235808, 19: 0.23255813953488372, 24: 0.7282051282051282, 26: 0.8736842105263158, 29: 0.7798165137614679}
Micro-average F1 score: 0.6829710144927537
Weighted-average F1 score: 0.6748910631380652
F1 score per class: {32: 0.4940239043824701, 6: 0.6747967479674797, 19: 0.14953271028037382, 24: 0.6605504587155964, 26: 0.9019607843137255, 29: 0.7431192660550459}
Micro-average F1 score: 0.639871382636656
Weighted-average F1 score: 0.6141367700709662
F1 score per class: {32: 0.49206349206349204, 6: 0.6747967479674797, 19: 0.19444444444444445, 24: 0.6666666666666666, 26: 0.8975609756097561, 29: 0.7433628318584071}
Micro-average F1 score: 0.657354149548069
Weighted-average F1 score: 0.6424247905342201
cur_acc_wo_na:  ['0.8056']
his_acc_wo_na:  ['0.8056']
cur_acc des_wo_na:  ['0.8172']
his_acc des_wo_na:  ['0.8172']
cur_acc rrf_wo_na:  ['0.8222']
his_acc rrf_wo_na:  ['0.8222']
cur_acc_w_na:  ['0.6830']
his_acc_w_na:  ['0.6830']
cur_acc des_w_na:  ['0.6399']
his_acc des_w_na:  ['0.6399']
cur_acc rrf_w_na:  ['0.6574']
his_acc rrf_w_na:  ['0.6574']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 191.8818061CurrentTrain: epoch  0, batch     1 | loss: 123.9070032CurrentTrain: epoch  0, batch     2 | loss: 76.9505352CurrentTrain: epoch  0, batch     3 | loss: 77.4691519CurrentTrain: epoch  0, batch     4 | loss: 62.5637762CurrentTrain: epoch  1, batch     0 | loss: 62.6263006CurrentTrain: epoch  1, batch     1 | loss: 91.4680125CurrentTrain: epoch  1, batch     2 | loss: 91.0111286CurrentTrain: epoch  1, batch     3 | loss: 121.7813788CurrentTrain: epoch  1, batch     4 | loss: 47.3981623CurrentTrain: epoch  2, batch     0 | loss: 60.7844101CurrentTrain: epoch  2, batch     1 | loss: 180.9480658CurrentTrain: epoch  2, batch     2 | loss: 60.4680860CurrentTrain: epoch  2, batch     3 | loss: 73.3235881CurrentTrain: epoch  2, batch     4 | loss: 118.0305148CurrentTrain: epoch  3, batch     0 | loss: 88.1680056CurrentTrain: epoch  3, batch     1 | loss: 59.2420770CurrentTrain: epoch  3, batch     2 | loss: 71.0407190CurrentTrain: epoch  3, batch     3 | loss: 183.1774356CurrentTrain: epoch  3, batch     4 | loss: 45.0383916CurrentTrain: epoch  4, batch     0 | loss: 120.5640063CurrentTrain: epoch  4, batch     1 | loss: 116.3086105CurrentTrain: epoch  4, batch     2 | loss: 54.0705010CurrentTrain: epoch  4, batch     3 | loss: 90.3577675CurrentTrain: epoch  4, batch     4 | loss: 42.5809117CurrentTrain: epoch  5, batch     0 | loss: 66.6277823CurrentTrain: epoch  5, batch     1 | loss: 89.2922423CurrentTrain: epoch  5, batch     2 | loss: 54.0686260CurrentTrain: epoch  5, batch     3 | loss: 120.1150219CurrentTrain: epoch  5, batch     4 | loss: 57.4042738CurrentTrain: epoch  6, batch     0 | loss: 119.3796094CurrentTrain: epoch  6, batch     1 | loss: 85.1480577CurrentTrain: epoch  6, batch     2 | loss: 83.2660242CurrentTrain: epoch  6, batch     3 | loss: 68.8826989CurrentTrain: epoch  6, batch     4 | loss: 39.4870054CurrentTrain: epoch  7, batch     0 | loss: 118.8904832CurrentTrain: epoch  7, batch     1 | loss: 65.9367570CurrentTrain: epoch  7, batch     2 | loss: 55.0111346CurrentTrain: epoch  7, batch     3 | loss: 52.0069905CurrentTrain: epoch  7, batch     4 | loss: 74.6536364CurrentTrain: epoch  8, batch     0 | loss: 66.8985243CurrentTrain: epoch  8, batch     1 | loss: 68.2427687CurrentTrain: epoch  8, batch     2 | loss: 60.6521539CurrentTrain: epoch  8, batch     3 | loss: 117.9596922CurrentTrain: epoch  8, batch     4 | loss: 53.0039203CurrentTrain: epoch  9, batch     0 | loss: 84.9526222CurrentTrain: epoch  9, batch     1 | loss: 117.9612444CurrentTrain: epoch  9, batch     2 | loss: 53.2213586CurrentTrain: epoch  9, batch     3 | loss: 51.5057933CurrentTrain: epoch  9, batch     4 | loss: 72.6074693
MemoryTrain:  epoch  0, batch     0 | loss: 0.4705141MemoryTrain:  epoch  1, batch     0 | loss: 0.4308403MemoryTrain:  epoch  2, batch     0 | loss: 0.3237977MemoryTrain:  epoch  3, batch     0 | loss: 0.2478526MemoryTrain:  epoch  4, batch     0 | loss: 0.2321333MemoryTrain:  epoch  5, batch     0 | loss: 0.1492020MemoryTrain:  epoch  6, batch     0 | loss: 0.1195518MemoryTrain:  epoch  7, batch     0 | loss: 0.0821988MemoryTrain:  epoch  8, batch     0 | loss: 0.0967264MemoryTrain:  epoch  9, batch     0 | loss: 0.0745034

F1 score per class: {5: 0.9528795811518325, 6: 0.0, 10: 0.46153846153846156, 16: 0.8, 17: 0.8, 18: 0.3181818181818182}
Micro-average F1 score: 0.7031963470319634
Weighted-average F1 score: 0.7628732620879217
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 10: 0.7607361963190185, 16: 0.8235294117647058, 17: 0.8235294117647058, 18: 0.7868852459016393, 19: 0.0}
Micro-average F1 score: 0.8537074148296593
Weighted-average F1 score: 0.8484147613171661
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 10: 0.7530864197530864, 16: 0.8235294117647058, 17: 0.875, 18: 0.7868852459016393, 19: 0.0}
Micro-average F1 score: 0.8514056224899599
Weighted-average F1 score: 0.8445866402531692

F1 score per class: {32: 0.9479166666666666, 5: 0.5, 6: 0.4580152671755725, 10: 0.8, 16: 0.20689655172413793, 17: 0.3181818181818182, 18: 0.8505747126436781, 19: 0.3333333333333333, 24: 0.7777777777777778, 26: 0.9473684210526315, 29: 0.8923076923076924}
Micro-average F1 score: 0.7424892703862661
Weighted-average F1 score: 0.7565714403962808
F1 score per class: {32: 0.9801980198019802, 5: 0.47435897435897434, 6: 0.7515151515151515, 10: 0.7924528301886793, 16: 0.20588235294117646, 17: 0.7741935483870968, 18: 0.9032258064516129, 19: 0.6206896551724138, 24: 0.7821229050279329, 26: 0.9637305699481865, 29: 0.91}
Micro-average F1 score: 0.7997320830542531
Weighted-average F1 score: 0.7897578771445768
F1 score per class: {32: 0.9801980198019802, 5: 0.4810126582278481, 6: 0.7439024390243902, 10: 0.7924528301886793, 16: 0.21212121212121213, 17: 0.7741935483870968, 18: 0.9032258064516129, 19: 0.5185185185185185, 24: 0.7821229050279329, 26: 0.9637305699481865, 29: 0.91}
Micro-average F1 score: 0.7986577181208053
Weighted-average F1 score: 0.790114353036491

F1 score per class: {32: 0.9381443298969072, 5: 0.0, 6: 0.3973509933774834, 10: 0.625, 16: 0.35294117647058826, 17: 0.21212121212121213, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5264957264957265
Weighted-average F1 score: 0.4682055412163132
F1 score per class: {32: 0.6556291390728477, 5: 0.0, 6: 0.543859649122807, 10: 0.4375, 16: 0.358974358974359, 17: 0.2823529411764706, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.4333672431332655
Weighted-average F1 score: 0.39497636934389807
F1 score per class: {32: 0.6513157894736842, 5: 0.0, 6: 0.5422222222222223, 10: 0.4375, 16: 0.35, 17: 0.2823529411764706, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.43666323377960864
Weighted-average F1 score: 0.39989084893076254

F1 score per class: {32: 0.9333333333333333, 5: 0.3187250996015936, 6: 0.3508771929824561, 10: 0.5797101449275363, 16: 0.12631578947368421, 17: 0.2, 18: 0.6462882096069869, 19: 0.18181818181818182, 24: 0.7142857142857143, 26: 0.8411214953271028, 29: 0.7280334728033473}
Micro-average F1 score: 0.5854483925549916
Weighted-average F1 score: 0.5674491056998264
F1 score per class: {32: 0.616822429906542, 5: 0.27205882352941174, 6: 0.4175084175084175, 10: 0.3684210526315789, 16: 0.11475409836065574, 17: 0.23529411764705882, 18: 0.5853658536585366, 19: 0.15384615384615385, 24: 0.6763285024154589, 26: 0.7782426778242678, 29: 0.6385964912280702}
Micro-average F1 score: 0.48438133874239353
Weighted-average F1 score: 0.46196717462990083
F1 score per class: {32: 0.6073619631901841, 5: 0.2753623188405797, 6: 0.42214532871972316, 10: 0.3684210526315789, 16: 0.11382113821138211, 17: 0.23414634146341465, 18: 0.5853658536585366, 19: 0.2, 24: 0.6965174129353234, 26: 0.7717842323651453, 29: 0.6618181818181819}
Micro-average F1 score: 0.4943913585375987
Weighted-average F1 score: 0.4734184962121738
cur_acc_wo_na:  ['0.8056', '0.7032']
his_acc_wo_na:  ['0.8056', '0.7425']
cur_acc des_wo_na:  ['0.8172', '0.8537']
his_acc des_wo_na:  ['0.8172', '0.7997']
cur_acc rrf_wo_na:  ['0.8222', '0.8514']
his_acc rrf_wo_na:  ['0.8222', '0.7987']
cur_acc_w_na:  ['0.6830', '0.5265']
his_acc_w_na:  ['0.6830', '0.5854']
cur_acc des_w_na:  ['0.6399', '0.4334']
his_acc des_w_na:  ['0.6399', '0.4844']
cur_acc rrf_w_na:  ['0.6574', '0.4367']
his_acc rrf_w_na:  ['0.6574', '0.4944']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 67.8548847CurrentTrain: epoch  0, batch     1 | loss: 70.3515064CurrentTrain: epoch  0, batch     2 | loss: 81.6727277CurrentTrain: epoch  0, batch     3 | loss: 10.9842580CurrentTrain: epoch  1, batch     0 | loss: 61.6656548CurrentTrain: epoch  1, batch     1 | loss: 75.4403282CurrentTrain: epoch  1, batch     2 | loss: 75.1987696CurrentTrain: epoch  1, batch     3 | loss: 15.9458396CurrentTrain: epoch  2, batch     0 | loss: 72.1013807CurrentTrain: epoch  2, batch     1 | loss: 58.3874795CurrentTrain: epoch  2, batch     2 | loss: 55.8022623CurrentTrain: epoch  2, batch     3 | loss: 27.5258981CurrentTrain: epoch  3, batch     0 | loss: 71.0547913CurrentTrain: epoch  3, batch     1 | loss: 64.7952727CurrentTrain: epoch  3, batch     2 | loss: 89.2457822CurrentTrain: epoch  3, batch     3 | loss: 11.1705373CurrentTrain: epoch  4, batch     0 | loss: 68.9037712CurrentTrain: epoch  4, batch     1 | loss: 90.5163000CurrentTrain: epoch  4, batch     2 | loss: 52.0225411CurrentTrain: epoch  4, batch     3 | loss: 3.5612558CurrentTrain: epoch  5, batch     0 | loss: 54.7637136CurrentTrain: epoch  5, batch     1 | loss: 85.1153233CurrentTrain: epoch  5, batch     2 | loss: 63.3936473CurrentTrain: epoch  5, batch     3 | loss: 5.9543028CurrentTrain: epoch  6, batch     0 | loss: 83.3610391CurrentTrain: epoch  6, batch     1 | loss: 61.2476151CurrentTrain: epoch  6, batch     2 | loss: 56.0387021CurrentTrain: epoch  6, batch     3 | loss: 6.2589591CurrentTrain: epoch  7, batch     0 | loss: 65.4186402CurrentTrain: epoch  7, batch     1 | loss: 51.4984623CurrentTrain: epoch  7, batch     2 | loss: 66.4967294CurrentTrain: epoch  7, batch     3 | loss: 11.3038766CurrentTrain: epoch  8, batch     0 | loss: 55.8674212CurrentTrain: epoch  8, batch     1 | loss: 64.8973623CurrentTrain: epoch  8, batch     2 | loss: 63.2307854CurrentTrain: epoch  8, batch     3 | loss: 9.6141931CurrentTrain: epoch  9, batch     0 | loss: 48.5895638CurrentTrain: epoch  9, batch     1 | loss: 53.7651210CurrentTrain: epoch  9, batch     2 | loss: 65.6189683CurrentTrain: epoch  9, batch     3 | loss: 27.3422993
MemoryTrain:  epoch  0, batch     0 | loss: 0.5213562MemoryTrain:  epoch  1, batch     0 | loss: 0.3636454MemoryTrain:  epoch  2, batch     0 | loss: 0.3232716MemoryTrain:  epoch  3, batch     0 | loss: 0.3074856MemoryTrain:  epoch  4, batch     0 | loss: 0.2481550MemoryTrain:  epoch  5, batch     0 | loss: 0.2330713MemoryTrain:  epoch  6, batch     0 | loss: 0.1527815MemoryTrain:  epoch  7, batch     0 | loss: 0.1056716MemoryTrain:  epoch  8, batch     0 | loss: 0.0996582MemoryTrain:  epoch  9, batch     0 | loss: 0.0850303

F1 score per class: {6: 0.0, 7: 0.0, 40: 0.9387755102040817, 9: 0.0, 17: 0.0, 19: 0.0, 26: 0.6, 27: 0.0, 31: 0.4367816091954023}
Micro-average F1 score: 0.45714285714285713
Weighted-average F1 score: 0.3551113261396513
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 17: 0.0, 19: 0.0, 24: 0.0, 26: 0.6956521739130435, 27: 1.0, 31: 0.6122448979591837}
Micro-average F1 score: 0.600896860986547
Weighted-average F1 score: 0.4860232086452451
F1 score per class: {6: 0.0, 7: 0.0, 40: 0.9803921568627451, 9: 0.0, 17: 0.0, 19: 0.0, 26: 0.75, 27: 1.0, 31: 0.6122448979591837}
Micro-average F1 score: 0.5919282511210763
Weighted-average F1 score: 0.4874050537646251

F1 score per class: {32: 0.9533678756476683, 5: 0.4370860927152318, 6: 0.0, 7: 0.9387755102040817, 40: 0.4496124031007752, 10: 0.7755102040816326, 9: 0.0, 16: 0.4897959183673469, 17: 0.5728643216080402, 18: 0.1, 19: 0.7777777777777778, 24: 0.4444444444444444, 26: 0.907103825136612, 27: 0.0, 29: 0.8645833333333334, 31: 0.3619047619047619}
Micro-average F1 score: 0.6591619762351469
Weighted-average F1 score: 0.6653794748385059
F1 score per class: {32: 0.9615384615384616, 5: 0.38571428571428573, 6: 0.05063291139240506, 7: 0.9803921568627451, 40: 0.6923076923076923, 10: 0.8148148148148148, 9: 0.0, 16: 0.8615384615384616, 17: 0.6435643564356436, 18: 0.3076923076923077, 19: 0.7692307692307693, 24: 0.5333333333333333, 26: 0.9361702127659575, 27: 0.8, 29: 0.8934010152284264, 31: 0.47619047619047616}
Micro-average F1 score: 0.7119628339140535
Weighted-average F1 score: 0.6993732742854647
F1 score per class: {32: 0.9611650485436893, 5: 0.4342105263157895, 6: 0.0, 7: 0.9803921568627451, 40: 0.6838709677419355, 10: 0.8235294117647058, 9: 0.0, 16: 0.8253968253968254, 17: 0.6435643564356436, 18: 0.32, 19: 0.7692307692307693, 24: 0.5806451612903226, 26: 0.9473684210526315, 27: 0.8, 29: 0.8934010152284264, 31: 0.47619047619047616}
Micro-average F1 score: 0.717201166180758
Weighted-average F1 score: 0.7047011017302725

F1 score per class: {32: 0.0, 6: 0.0, 7: 0.9387755102040817, 40: 0.0, 9: 0.0, 10: 0.0, 17: 0.0, 19: 0.0, 24: 0.5, 26: 0.0, 27: 0.0, 31: 0.29457364341085274}
Micro-average F1 score: 0.33217993079584773
Weighted-average F1 score: 0.2574904513300337
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.5, 7: 0.8928571428571429, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.5, 26: 0.23529411764705882, 27: 0.0, 31: 0.2843601895734597}
Micro-average F1 score: 0.31529411764705884
Weighted-average F1 score: 0.26359689118786145
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.0, 7: 0.9433962264150944, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.5, 26: 0.26666666666666666, 27: 0.0, 31: 0.2830188679245283}
Micro-average F1 score: 0.32038834951456313
Weighted-average F1 score: 0.2724600059094171

F1 score per class: {32: 0.92, 5: 0.28820960698689957, 6: 0.0, 7: 0.9387755102040817, 40: 0.3372093023255814, 10: 0.5588235294117647, 9: 0.0, 16: 0.2696629213483146, 17: 0.44357976653696496, 18: 0.07692307692307693, 19: 0.717948717948718, 24: 0.16666666666666666, 26: 0.8217821782178217, 27: 0.0, 29: 0.7477477477477478, 31: 0.2159090909090909}
Micro-average F1 score: 0.5064872657376261
Weighted-average F1 score: 0.4748241863196759
F1 score per class: {32: 0.4106776180698152, 5: 0.2231404958677686, 6: 0.028169014084507043, 7: 0.8771929824561403, 40: 0.43548387096774194, 10: 0.38596491228070173, 9: 0.0, 16: 0.1830065359477124, 17: 0.5058365758754864, 18: 0.1509433962264151, 19: 0.6862745098039216, 24: 0.13559322033898305, 26: 0.822429906542056, 27: 0.06666666666666667, 29: 0.6875, 31: 0.14354066985645933}
Micro-average F1 score: 0.383364602876798
Weighted-average F1 score: 0.34637286401506445
F1 score per class: {32: 0.4267241379310345, 5: 0.2509505703422053, 6: 0.0, 7: 0.9259259259259259, 40: 0.4189723320158103, 10: 0.40384615384615385, 9: 0.0, 16: 0.1863799283154122, 17: 0.5058365758754864, 18: 0.2, 19: 0.7, 24: 0.140625, 26: 0.8333333333333334, 27: 0.07272727272727272, 29: 0.7154471544715447, 31: 0.14423076923076922}
Micro-average F1 score: 0.39486356340288925
Weighted-average F1 score: 0.3563238793442604
cur_acc_wo_na:  ['0.8056', '0.7032', '0.4571']
his_acc_wo_na:  ['0.8056', '0.7425', '0.6592']
cur_acc des_wo_na:  ['0.8172', '0.8537', '0.6009']
his_acc des_wo_na:  ['0.8172', '0.7997', '0.7120']
cur_acc rrf_wo_na:  ['0.8222', '0.8514', '0.5919']
his_acc rrf_wo_na:  ['0.8222', '0.7987', '0.7172']
cur_acc_w_na:  ['0.6830', '0.5265', '0.3322']
his_acc_w_na:  ['0.6830', '0.5854', '0.5065']
cur_acc des_w_na:  ['0.6399', '0.4334', '0.3153']
his_acc des_w_na:  ['0.6399', '0.4844', '0.3834']
cur_acc rrf_w_na:  ['0.6574', '0.4367', '0.3204']
his_acc rrf_w_na:  ['0.6574', '0.4944', '0.3949']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 83.1810244CurrentTrain: epoch  0, batch     1 | loss: 79.9051566CurrentTrain: epoch  0, batch     2 | loss: 85.4308729CurrentTrain: epoch  0, batch     3 | loss: 72.1336124CurrentTrain: epoch  1, batch     0 | loss: 60.7224869CurrentTrain: epoch  1, batch     1 | loss: 63.1285627CurrentTrain: epoch  1, batch     2 | loss: 128.1596396CurrentTrain: epoch  1, batch     3 | loss: 61.9922019CurrentTrain: epoch  2, batch     0 | loss: 76.1428003CurrentTrain: epoch  2, batch     1 | loss: 69.9454136CurrentTrain: epoch  2, batch     2 | loss: 87.1788864CurrentTrain: epoch  2, batch     3 | loss: 59.5088115CurrentTrain: epoch  3, batch     0 | loss: 57.3504958CurrentTrain: epoch  3, batch     1 | loss: 69.2428645CurrentTrain: epoch  3, batch     2 | loss: 89.3772775CurrentTrain: epoch  3, batch     3 | loss: 61.4702975CurrentTrain: epoch  4, batch     0 | loss: 117.7559750CurrentTrain: epoch  4, batch     1 | loss: 70.7814388CurrentTrain: epoch  4, batch     2 | loss: 56.9055682CurrentTrain: epoch  4, batch     3 | loss: 69.0367901CurrentTrain: epoch  5, batch     0 | loss: 69.8823940CurrentTrain: epoch  5, batch     1 | loss: 68.9765841CurrentTrain: epoch  5, batch     2 | loss: 112.4392257CurrentTrain: epoch  5, batch     3 | loss: 44.6695805CurrentTrain: epoch  6, batch     0 | loss: 67.9139287CurrentTrain: epoch  6, batch     1 | loss: 62.8972633CurrentTrain: epoch  6, batch     2 | loss: 88.9998814CurrentTrain: epoch  6, batch     3 | loss: 54.3040285CurrentTrain: epoch  7, batch     0 | loss: 57.5609552CurrentTrain: epoch  7, batch     1 | loss: 66.9999324CurrentTrain: epoch  7, batch     2 | loss: 84.5939423CurrentTrain: epoch  7, batch     3 | loss: 51.6232700CurrentTrain: epoch  8, batch     0 | loss: 52.7382954CurrentTrain: epoch  8, batch     1 | loss: 82.8940713CurrentTrain: epoch  8, batch     2 | loss: 86.8406250CurrentTrain: epoch  8, batch     3 | loss: 69.2665186CurrentTrain: epoch  9, batch     0 | loss: 64.2452456CurrentTrain: epoch  9, batch     1 | loss: 87.6396482CurrentTrain: epoch  9, batch     2 | loss: 81.2826935CurrentTrain: epoch  9, batch     3 | loss: 53.1283734
MemoryTrain:  epoch  0, batch     0 | loss: 0.5422589MemoryTrain:  epoch  1, batch     0 | loss: 0.4710574MemoryTrain:  epoch  2, batch     0 | loss: 0.3197709MemoryTrain:  epoch  3, batch     0 | loss: 0.2284947MemoryTrain:  epoch  4, batch     0 | loss: 0.1851319MemoryTrain:  epoch  5, batch     0 | loss: 0.1669564MemoryTrain:  epoch  6, batch     0 | loss: 0.1129282MemoryTrain:  epoch  7, batch     0 | loss: 0.1415330MemoryTrain:  epoch  8, batch     0 | loss: 0.0946296MemoryTrain:  epoch  9, batch     0 | loss: 0.0654161

F1 score per class: {0: 0.927536231884058, 32: 0.9583333333333334, 4: 0.5714285714285714, 13: 0.41025641025641024, 21: 0.7792207792207793, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8261964735516373
Weighted-average F1 score: 0.82287772818355
F1 score per class: {0: 0.9577464788732394, 32: 0.9795918367346939, 4: 0.0, 5: 0.5714285714285714, 13: 0.0, 18: 0.7843137254901961, 21: 0.7792207792207793, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8564705882352941
Weighted-average F1 score: 0.8137070811585619
F1 score per class: {0: 0.9577464788732394, 32: 0.9795918367346939, 4: 0.0, 5: 0.5714285714285714, 13: 0.0, 18: 0.7843137254901961, 21: 0.7792207792207793, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8564705882352941
Weighted-average F1 score: 0.8137070811585619

F1 score per class: {0: 0.9142857142857143, 4: 0.9583333333333334, 5: 0.9637305699481865, 6: 0.40816326530612246, 7: 0.0, 9: 0.96, 10: 0.5467625899280576, 13: 0.0975609756097561, 16: 0.8235294117647058, 17: 0.0, 18: 0.6037735849056604, 19: 0.6666666666666666, 21: 0.3404255319148936, 23: 0.7692307692307693, 24: 0.1, 26: 0.7407407407407407, 27: 0.5185185185185185, 29: 0.9479166666666666, 31: 0.6666666666666666, 32: 0.7771428571428571, 40: 0.4421052631578947}
Micro-average F1 score: 0.7020648967551623
Weighted-average F1 score: 0.6998765784918843
F1 score per class: {0: 0.9577464788732394, 4: 0.9795918367346939, 5: 0.966183574879227, 6: 0.4266666666666667, 7: 0.05970149253731343, 9: 0.9803921568627451, 10: 0.7484662576687117, 13: 0.125, 16: 0.9333333333333333, 17: 0.0, 18: 0.8, 19: 0.6923076923076923, 21: 0.6557377049180327, 23: 0.7792207792207793, 24: 0.1, 26: 0.711340206185567, 27: 0.5454545454545454, 29: 0.9479166666666666, 31: 0.6666666666666666, 32: 0.8601036269430051, 40: 0.5081967213114754}
Micro-average F1 score: 0.7461044912923923
Weighted-average F1 score: 0.7339270958122304
F1 score per class: {0: 0.9444444444444444, 4: 0.9795918367346939, 5: 0.975609756097561, 6: 0.44871794871794873, 7: 0.03278688524590164, 9: 0.9803921568627451, 10: 0.725, 13: 0.12121212121212122, 16: 0.9491525423728814, 17: 0.0, 18: 0.8, 19: 0.6985645933014354, 21: 0.6666666666666666, 23: 0.7692307692307693, 24: 0.1, 26: 0.7150259067357513, 27: 0.45161290322580644, 29: 0.9533678756476683, 31: 0.6666666666666666, 32: 0.8601036269430051, 40: 0.5128205128205128}
Micro-average F1 score: 0.7473538886332259
Weighted-average F1 score: 0.735335909079679

F1 score per class: {0: 0.810126582278481, 4: 0.9583333333333334, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.08695652173913043, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.3018867924528302, 23: 0.7317073170731707, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.5494137353433836
Weighted-average F1 score: 0.4191205995893646
F1 score per class: {0: 0.6476190476190476, 4: 0.9142857142857143, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.07692307692307693, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3389830508474576, 23: 0.5882352941176471, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.38195173137460653
Weighted-average F1 score: 0.2865885061734077
F1 score per class: {0: 0.6126126126126126, 4: 0.9365853658536586, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.07407407407407407, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3418803418803419, 23: 0.594059405940594, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4057971014492754
Weighted-average F1 score: 0.30839784889231703

F1 score per class: {0: 0.3516483516483517, 4: 0.9435897435897436, 5: 0.8571428571428571, 6: 0.242914979757085, 7: 0.0, 9: 0.9411764705882353, 10: 0.3206751054852321, 13: 0.02702702702702703, 16: 0.46153846153846156, 17: 0.0, 18: 0.22377622377622378, 19: 0.47719298245614034, 21: 0.1951219512195122, 23: 0.6521739130434783, 24: 0.07692307692307693, 26: 0.6862745098039216, 27: 0.1686746987951807, 29: 0.7947598253275109, 31: 0.08333333333333333, 32: 0.5964912280701754, 40: 0.267515923566879}
Micro-average F1 score: 0.465905383360522
Weighted-average F1 score: 0.4221565307126616
F1 score per class: {0: 0.2905982905982906, 4: 0.8847926267281107, 5: 0.32362459546925565, 6: 0.21052631578947367, 7: 0.03007518796992481, 9: 0.746268656716418, 10: 0.3388888888888889, 13: 0.023391812865497075, 16: 0.36363636363636365, 17: 0.0, 18: 0.13402061855670103, 19: 0.4444444444444444, 21: 0.1606425702811245, 23: 0.5172413793103449, 24: 0.09090909090909091, 26: 0.6244343891402715, 27: 0.1258741258741259, 29: 0.708171206225681, 31: 0.03508771929824561, 32: 0.5286624203821656, 40: 0.15308641975308643}
Micro-average F1 score: 0.33692052980132453
Weighted-average F1 score: 0.3056998263784305
F1 score per class: {0: 0.2490842490842491, 4: 0.9142857142857143, 5: 0.43010752688172044, 6: 0.22364217252396165, 7: 0.016666666666666666, 9: 0.8064516129032258, 10: 0.327683615819209, 13: 0.023121387283236993, 16: 0.38620689655172413, 17: 0.0, 18: 0.1343669250645995, 19: 0.45482866043613707, 21: 0.15267175572519084, 23: 0.5128205128205128, 24: 0.09090909090909091, 26: 0.6330275229357798, 27: 0.10144927536231885, 29: 0.7104247104247104, 31: 0.0547945205479452, 32: 0.5303514376996805, 40: 0.18461538461538463}
Micro-average F1 score: 0.3551279247758583
Weighted-average F1 score: 0.32225551623017573
cur_acc_wo_na:  ['0.8056', '0.7032', '0.4571', '0.8262']
his_acc_wo_na:  ['0.8056', '0.7425', '0.6592', '0.7021']
cur_acc des_wo_na:  ['0.8172', '0.8537', '0.6009', '0.8565']
his_acc des_wo_na:  ['0.8172', '0.7997', '0.7120', '0.7461']
cur_acc rrf_wo_na:  ['0.8222', '0.8514', '0.5919', '0.8565']
his_acc rrf_wo_na:  ['0.8222', '0.7987', '0.7172', '0.7474']
cur_acc_w_na:  ['0.6830', '0.5265', '0.3322', '0.5494']
his_acc_w_na:  ['0.6830', '0.5854', '0.5065', '0.4659']
cur_acc des_w_na:  ['0.6399', '0.4334', '0.3153', '0.3820']
his_acc des_w_na:  ['0.6399', '0.4844', '0.3834', '0.3369']
cur_acc rrf_w_na:  ['0.6574', '0.4367', '0.3204', '0.4058']
his_acc rrf_w_na:  ['0.6574', '0.4944', '0.3949', '0.3551']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 73.0704342CurrentTrain: epoch  0, batch     1 | loss: 64.7738806CurrentTrain: epoch  0, batch     2 | loss: 79.6492522CurrentTrain: epoch  0, batch     3 | loss: 60.4992671CurrentTrain: epoch  1, batch     0 | loss: 87.2872259CurrentTrain: epoch  1, batch     1 | loss: 71.9895872CurrentTrain: epoch  1, batch     2 | loss: 60.0594078CurrentTrain: epoch  1, batch     3 | loss: 70.4203031CurrentTrain: epoch  2, batch     0 | loss: 68.7829788CurrentTrain: epoch  2, batch     1 | loss: 69.3021981CurrentTrain: epoch  2, batch     2 | loss: 181.4727093CurrentTrain: epoch  2, batch     3 | loss: 31.2463528CurrentTrain: epoch  3, batch     0 | loss: 86.3714463CurrentTrain: epoch  3, batch     1 | loss: 68.0642578CurrentTrain: epoch  3, batch     2 | loss: 86.3597970CurrentTrain: epoch  3, batch     3 | loss: 33.2603631CurrentTrain: epoch  4, batch     0 | loss: 54.5520242CurrentTrain: epoch  4, batch     1 | loss: 119.4513348CurrentTrain: epoch  4, batch     2 | loss: 65.6270503CurrentTrain: epoch  4, batch     3 | loss: 31.2429862CurrentTrain: epoch  5, batch     0 | loss: 63.5840749CurrentTrain: epoch  5, batch     1 | loss: 51.8572458CurrentTrain: epoch  5, batch     2 | loss: 120.6148467CurrentTrain: epoch  5, batch     3 | loss: 49.3736792CurrentTrain: epoch  6, batch     0 | loss: 85.9654176CurrentTrain: epoch  6, batch     1 | loss: 53.1409886CurrentTrain: epoch  6, batch     2 | loss: 52.4085420CurrentTrain: epoch  6, batch     3 | loss: 32.1998970CurrentTrain: epoch  7, batch     0 | loss: 61.7909008CurrentTrain: epoch  7, batch     1 | loss: 116.1064927CurrentTrain: epoch  7, batch     2 | loss: 52.6914829CurrentTrain: epoch  7, batch     3 | loss: 50.6887560CurrentTrain: epoch  8, batch     0 | loss: 67.0986829CurrentTrain: epoch  8, batch     1 | loss: 49.9963675CurrentTrain: epoch  8, batch     2 | loss: 50.6987240CurrentTrain: epoch  8, batch     3 | loss: 71.2264370CurrentTrain: epoch  9, batch     0 | loss: 85.2254200CurrentTrain: epoch  9, batch     1 | loss: 53.2827907CurrentTrain: epoch  9, batch     2 | loss: 61.3777799CurrentTrain: epoch  9, batch     3 | loss: 37.2351558
MemoryTrain:  epoch  0, batch     0 | loss: 0.2324364MemoryTrain:  epoch  1, batch     0 | loss: 0.1978041MemoryTrain:  epoch  2, batch     0 | loss: 0.1656337MemoryTrain:  epoch  3, batch     0 | loss: 0.1206697MemoryTrain:  epoch  4, batch     0 | loss: 0.0977537MemoryTrain:  epoch  5, batch     0 | loss: 0.0831125MemoryTrain:  epoch  6, batch     0 | loss: 0.0673632MemoryTrain:  epoch  7, batch     0 | loss: 0.0547472MemoryTrain:  epoch  8, batch     0 | loss: 0.0520823MemoryTrain:  epoch  9, batch     0 | loss: 0.0519646

F1 score per class: {33: 0.0, 36: 0.0, 5: 0.3469387755102041, 6: 0.0, 8: 0.0, 10: 0.8, 13: 0.0, 20: 0.0, 21: 0.0, 26: 0.8823529411764706, 29: 0.42857142857142855, 30: 0.42857142857142855}
Micro-average F1 score: 0.541033434650456
Weighted-average F1 score: 0.580951564299189
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.0, 6: 0.7538461538461538, 7: 0.0, 8: 0.0, 10: 0.0, 16: 0.8979591836734694, 18: 0.0, 20: 0.0, 21: 0.0, 26: 0.9142857142857143, 29: 0.375, 30: 0.8666666666666667}
Micro-average F1 score: 0.7865707434052758
Weighted-average F1 score: 0.7513561131648689
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.0, 6: 0.6829268292682927, 7: 0.0, 8: 0.0, 10: 0.0, 13: 0.0, 16: 0.9090909090909091, 18: 0.0, 20: 0.0, 21: 0.0, 26: 0.9142857142857143, 29: 0.375, 30: 0.847457627118644}
Micro-average F1 score: 0.7554479418886199
Weighted-average F1 score: 0.7151834383357148

F1 score per class: {0: 0.927536231884058, 4: 0.9247311827956989, 5: 0.9693877551020408, 6: 0.34108527131782945, 7: 0.0, 8: 0.3119266055045872, 9: 0.96, 10: 0.22807017543859648, 13: 0.14285714285714285, 16: 0.7755102040816326, 17: 0.0, 18: 0.35555555555555557, 19: 0.6305418719211823, 20: 0.8, 21: 0.36363636363636365, 23: 0.6944444444444444, 24: 0.1, 26: 0.7225130890052356, 27: 0.48, 29: 0.9319371727748691, 30: 0.8823529411764706, 31: 0.5, 32: 0.7455621301775148, 33: 0.3333333333333333, 36: 0.42857142857142855, 40: 0.2857142857142857}
Micro-average F1 score: 0.6459351399378054
Weighted-average F1 score: 0.6935423965290468
F1 score per class: {0: 0.9577464788732394, 4: 0.9690721649484536, 5: 0.9216589861751152, 6: 0.42758620689655175, 7: 0.06557377049180328, 8: 0.5185185185185185, 9: 0.9803921568627451, 10: 0.25862068965517243, 13: 0.18181818181818182, 16: 0.9152542372881356, 17: 0.15384615384615385, 18: 0.7575757575757576, 19: 0.6730769230769231, 20: 0.88, 21: 0.6129032258064516, 23: 0.7631578947368421, 24: 0.08695652173913043, 26: 0.711340206185567, 27: 0.5142857142857142, 29: 0.9278350515463918, 30: 0.8888888888888888, 31: 0.8, 32: 0.8064516129032258, 33: 0.2727272727272727, 36: 0.8387096774193549, 40: 0.5}
Micro-average F1 score: 0.7086247086247086
Weighted-average F1 score: 0.7176266785524195
F1 score per class: {0: 0.9577464788732394, 4: 0.9637305699481865, 5: 0.9428571428571428, 6: 0.4189189189189189, 7: 0.06779661016949153, 8: 0.4827586206896552, 9: 0.9803921568627451, 10: 0.3387096774193548, 13: 0.14285714285714285, 16: 0.9152542372881356, 17: 0.14285714285714285, 18: 0.6333333333333333, 19: 0.6857142857142857, 20: 0.8737864077669902, 21: 0.6229508196721312, 23: 0.7631578947368421, 24: 0.08695652173913043, 26: 0.711340206185567, 27: 0.5625, 29: 0.9278350515463918, 30: 0.8888888888888888, 31: 0.8, 32: 0.8191489361702128, 33: 0.2608695652173913, 36: 0.8064516129032258, 40: 0.5225225225225225}
Micro-average F1 score: 0.7086429409464216
Weighted-average F1 score: 0.7149886824590582

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.3300970873786408, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.5806451612903226, 21: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8333333333333334, 31: 0.0, 32: 0.0, 33: 0.3157894736842105, 36: 0.3829787234042553, 40: 0.0}
Micro-average F1 score: 0.3662551440329218
Weighted-average F1 score: 0.2953869957808321
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5157894736842106, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5028571428571429, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.7804878048780488, 31: 0.0, 32: 0.0, 33: 0.1875, 36: 0.5621621621621622, 40: 0.0}
Micro-average F1 score: 0.34273772204806685
Weighted-average F1 score: 0.28246872716516197
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.509090909090909, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5172413793103449, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.7804878048780488, 31: 0.0, 32: 0.0, 33: 0.15789473684210525, 36: 0.5235602094240838, 40: 0.0}
Micro-average F1 score: 0.34210526315789475
Weighted-average F1 score: 0.2815147102216478

F1 score per class: {0: 0.5765765765765766, 4: 0.91005291005291, 5: 0.753968253968254, 6: 0.24581005586592178, 7: 0.0, 8: 0.24285714285714285, 9: 0.9411764705882353, 10: 0.2047244094488189, 13: 0.04081632653061224, 16: 0.4175824175824176, 17: 0.0, 18: 0.18604651162790697, 19: 0.48484848484848486, 20: 0.3850267379679144, 21: 0.2318840579710145, 23: 0.5952380952380952, 24: 0.08, 26: 0.6359447004608295, 27: 0.14457831325301204, 29: 0.7385892116182573, 30: 0.8333333333333334, 31: 0.0425531914893617, 32: 0.5701357466063348, 33: 0.18181818181818182, 36: 0.3302752293577982, 40: 0.1951219512195122}
Micro-average F1 score: 0.4645367412140575
Weighted-average F1 score: 0.45298788354031777
F1 score per class: {0: 0.31336405529953915, 4: 0.8952380952380953, 5: 0.2336448598130841, 6: 0.2198581560283688, 7: 0.028985507246376812, 8: 0.21444201312910285, 9: 0.7142857142857143, 10: 0.18518518518518517, 13: 0.047619047619047616, 16: 0.31952662721893493, 17: 0.1, 18: 0.15673981191222572, 19: 0.4294478527607362, 20: 0.2565597667638484, 21: 0.1254125412541254, 23: 0.4915254237288136, 24: 0.0625, 26: 0.5822784810126582, 27: 0.1267605633802817, 29: 0.627177700348432, 30: 0.5423728813559322, 31: 0.03305785123966942, 32: 0.47619047619047616, 33: 0.0625, 36: 0.36619718309859156, 40: 0.18831168831168832}
Micro-average F1 score: 0.3084728564180619
Weighted-average F1 score: 0.2861757828872228
F1 score per class: {0: 0.30493273542600896, 4: 0.916256157635468, 5: 0.28530259365994237, 6: 0.2108843537414966, 7: 0.03076923076923077, 8: 0.2204724409448819, 9: 0.8064516129032258, 10: 0.24561403508771928, 13: 0.03278688524590164, 16: 0.34838709677419355, 17: 0.09523809523809523, 18: 0.17194570135746606, 19: 0.43902439024390244, 20: 0.2535211267605634, 21: 0.12179487179487179, 23: 0.49572649572649574, 24: 0.06666666666666667, 26: 0.6106194690265486, 27: 0.13533834586466165, 29: 0.631578947368421, 30: 0.5517241379310345, 31: 0.03571428571428571, 32: 0.4827586206896552, 33: 0.049586776859504134, 36: 0.3333333333333333, 40: 0.21323529411764705}
Micro-average F1 score: 0.32449856733524357
Weighted-average F1 score: 0.30068050359705617
cur_acc_wo_na:  ['0.8056', '0.7032', '0.4571', '0.8262', '0.5410']
his_acc_wo_na:  ['0.8056', '0.7425', '0.6592', '0.7021', '0.6459']
cur_acc des_wo_na:  ['0.8172', '0.8537', '0.6009', '0.8565', '0.7866']
his_acc des_wo_na:  ['0.8172', '0.7997', '0.7120', '0.7461', '0.7086']
cur_acc rrf_wo_na:  ['0.8222', '0.8514', '0.5919', '0.8565', '0.7554']
his_acc rrf_wo_na:  ['0.8222', '0.7987', '0.7172', '0.7474', '0.7086']
cur_acc_w_na:  ['0.6830', '0.5265', '0.3322', '0.5494', '0.3663']
his_acc_w_na:  ['0.6830', '0.5854', '0.5065', '0.4659', '0.4645']
cur_acc des_w_na:  ['0.6399', '0.4334', '0.3153', '0.3820', '0.3427']
his_acc des_w_na:  ['0.6399', '0.4844', '0.3834', '0.3369', '0.3085']
cur_acc rrf_w_na:  ['0.6574', '0.4367', '0.3204', '0.4058', '0.3421']
his_acc rrf_w_na:  ['0.6574', '0.4944', '0.3949', '0.3551', '0.3245']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 90.1656369CurrentTrain: epoch  0, batch     1 | loss: 84.6527270CurrentTrain: epoch  0, batch     2 | loss: 123.0219065CurrentTrain: epoch  0, batch     3 | loss: 77.5420280CurrentTrain: epoch  0, batch     4 | loss: 25.9656229CurrentTrain: epoch  1, batch     0 | loss: 96.4090011CurrentTrain: epoch  1, batch     1 | loss: 87.5286679CurrentTrain: epoch  1, batch     2 | loss: 73.0250709CurrentTrain: epoch  1, batch     3 | loss: 93.4212965CurrentTrain: epoch  1, batch     4 | loss: 15.0620468CurrentTrain: epoch  2, batch     0 | loss: 73.1433357CurrentTrain: epoch  2, batch     1 | loss: 73.1045601CurrentTrain: epoch  2, batch     2 | loss: 57.6278163CurrentTrain: epoch  2, batch     3 | loss: 120.4299958CurrentTrain: epoch  2, batch     4 | loss: 29.3766388CurrentTrain: epoch  3, batch     0 | loss: 66.7185537CurrentTrain: epoch  3, batch     1 | loss: 120.4321793CurrentTrain: epoch  3, batch     2 | loss: 118.5543314CurrentTrain: epoch  3, batch     3 | loss: 88.5394331CurrentTrain: epoch  3, batch     4 | loss: 17.1081284CurrentTrain: epoch  4, batch     0 | loss: 70.0559476CurrentTrain: epoch  4, batch     1 | loss: 85.2608211CurrentTrain: epoch  4, batch     2 | loss: 67.3763963CurrentTrain: epoch  4, batch     3 | loss: 69.1451983CurrentTrain: epoch  4, batch     4 | loss: 12.7527083CurrentTrain: epoch  5, batch     0 | loss: 85.7166797CurrentTrain: epoch  5, batch     1 | loss: 55.6822219CurrentTrain: epoch  5, batch     2 | loss: 67.7831679CurrentTrain: epoch  5, batch     3 | loss: 53.2444990CurrentTrain: epoch  5, batch     4 | loss: 27.8169423CurrentTrain: epoch  6, batch     0 | loss: 67.6085750CurrentTrain: epoch  6, batch     1 | loss: 87.3141133CurrentTrain: epoch  6, batch     2 | loss: 56.0873408CurrentTrain: epoch  6, batch     3 | loss: 51.3116521CurrentTrain: epoch  6, batch     4 | loss: 17.0127009CurrentTrain: epoch  7, batch     0 | loss: 64.3269209CurrentTrain: epoch  7, batch     1 | loss: 183.3096718CurrentTrain: epoch  7, batch     2 | loss: 54.5019453CurrentTrain: epoch  7, batch     3 | loss: 83.0055185CurrentTrain: epoch  7, batch     4 | loss: 10.7159570CurrentTrain: epoch  8, batch     0 | loss: 65.4473961CurrentTrain: epoch  8, batch     1 | loss: 116.7093381CurrentTrain: epoch  8, batch     2 | loss: 63.1469450CurrentTrain: epoch  8, batch     3 | loss: 65.4103667CurrentTrain: epoch  8, batch     4 | loss: 26.9594508CurrentTrain: epoch  9, batch     0 | loss: 64.8665129CurrentTrain: epoch  9, batch     1 | loss: 83.2174269CurrentTrain: epoch  9, batch     2 | loss: 112.0811496CurrentTrain: epoch  9, batch     3 | loss: 64.6087219CurrentTrain: epoch  9, batch     4 | loss: 27.1095423
MemoryTrain:  epoch  0, batch     0 | loss: 0.3334891MemoryTrain:  epoch  1, batch     0 | loss: 0.2626776MemoryTrain:  epoch  2, batch     0 | loss: 0.1877452MemoryTrain:  epoch  3, batch     0 | loss: 0.1826340MemoryTrain:  epoch  4, batch     0 | loss: 0.1491171MemoryTrain:  epoch  5, batch     0 | loss: 0.1352295MemoryTrain:  epoch  6, batch     0 | loss: 0.1169792MemoryTrain:  epoch  7, batch     0 | loss: 0.1083488MemoryTrain:  epoch  8, batch     0 | loss: 0.1317178MemoryTrain:  epoch  9, batch     0 | loss: 0.0907843

F1 score per class: {0: 0.0, 2: 0.875, 39: 0.0, 8: 0.5333333333333333, 11: 0.4126984126984127, 12: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.5454545454545454, 28: 0.0, 29: 0.13333333333333333}
Micro-average F1 score: 0.44805194805194803
Weighted-average F1 score: 0.40102834119688047
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.8027210884353742, 12: 0.7607361963190185, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 27: 0.0, 28: 0.46153846153846156, 29: 0.0, 33: 0.0, 36: 0.0, 39: 0.42105263157894735, 40: 0.0}
Micro-average F1 score: 0.6537530266343826
Weighted-average F1 score: 0.5454890724297541
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8366013071895425, 12: 0.7607361963190185, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 27: 0.0, 28: 0.46153846153846156, 29: 0.0, 33: 0.0, 36: 0.0, 39: 0.42105263157894735, 40: 0.0}
Micro-average F1 score: 0.6829268292682927
Weighted-average F1 score: 0.5908032502670443

F1 score per class: {0: 0.9142857142857143, 2: 0.7368421052631579, 4: 0.9130434782608695, 5: 0.9690721649484536, 6: 0.3125, 7: 0.08695652173913043, 8: 0.3684210526315789, 9: 0.96, 10: 0.23008849557522124, 11: 0.39751552795031053, 12: 0.3969465648854962, 13: 0.26666666666666666, 16: 0.8, 17: 0.0, 18: 0.05, 19: 0.6388888888888888, 20: 0.5974025974025974, 21: 0.40816326530612246, 23: 0.825, 24: 0.1, 26: 0.745945945945946, 27: 0.4166666666666667, 28: 0.42857142857142855, 29: 0.9326424870466321, 30: 0.8823529411764706, 31: 0.5, 32: 0.8066298342541437, 33: 0.375, 36: 0.16666666666666666, 39: 0.13333333333333333, 40: 0.4}
Micro-average F1 score: 0.6108786610878661
Weighted-average F1 score: 0.6634377391533711
F1 score per class: {0: 0.958904109589041, 2: 0.6086956521739131, 4: 0.9528795811518325, 5: 0.966183574879227, 6: 0.4413793103448276, 7: 0.075, 8: 0.46808510638297873, 9: 0.9803921568627451, 10: 0.2905982905982906, 11: 0.6519337016574586, 12: 0.7085714285714285, 13: 0.06666666666666667, 16: 0.8571428571428571, 17: 0.0, 18: 0.43333333333333335, 19: 0.6607929515418502, 20: 0.8131868131868132, 21: 0.6666666666666666, 23: 0.8148148148148148, 24: 0.09090909090909091, 26: 0.7225130890052356, 27: 0.5454545454545454, 28: 0.2222222222222222, 29: 0.9278350515463918, 30: 0.918918918918919, 31: 0.8, 32: 0.883248730964467, 33: 0.3, 36: 0.7521367521367521, 39: 0.32, 40: 0.43478260869565216}
Micro-average F1 score: 0.6813258943222842
Weighted-average F1 score: 0.6786828281287494
F1 score per class: {0: 0.958904109589041, 2: 0.5833333333333334, 4: 0.9361702127659575, 5: 0.975609756097561, 6: 0.42953020134228187, 7: 0.07692307692307693, 8: 0.4970414201183432, 9: 0.9803921568627451, 10: 0.368, 11: 0.6336633663366337, 12: 0.7045454545454546, 13: 0.10256410256410256, 16: 0.8571428571428571, 17: 0.0, 18: 0.32, 19: 0.6666666666666666, 20: 0.8131868131868132, 21: 0.6388888888888888, 23: 0.8148148148148148, 24: 0.09090909090909091, 26: 0.7225130890052356, 27: 0.5454545454545454, 28: 0.3157894736842105, 29: 0.9326424870466321, 30: 0.918918918918919, 31: 0.8, 32: 0.8787878787878788, 33: 0.2857142857142857, 36: 0.5882352941176471, 39: 0.34782608695652173, 40: 0.4672897196261682}
Micro-average F1 score: 0.6787838730998017
Weighted-average F1 score: 0.6774314115437416

F1 score per class: {0: 0.0, 2: 0.3888888888888889, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.42953020134228187, 12: 0.3611111111111111, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 28: 0.3, 29: 0.0, 31: 0.0, 32: 0.0, 39: 0.13333333333333333, 40: 0.0}
Micro-average F1 score: 0.2732673267326733
Weighted-average F1 score: 0.19693554022413082
F1 score per class: {0: 0.0, 2: 0.24561403508771928, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.4306569343065693, 12: 0.5020242914979757, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0967741935483871, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.27586206896551724, 40: 0.0}
Micro-average F1 score: 0.22167487684729065
Weighted-average F1 score: 0.175228200001532
F1 score per class: {0: 0.0, 2: 0.25925925925925924, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.41025641025641024, 12: 0.4940239043824701, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.16216216216216217, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.36363636363636365, 40: 0.0}
Micro-average F1 score: 0.23870417732310314
Weighted-average F1 score: 0.19473113808977902

F1 score per class: {0: 0.47761194029850745, 2: 0.22580645161290322, 4: 0.8842105263157894, 5: 0.8281938325991189, 6: 0.19230769230769232, 7: 0.03773584905660377, 8: 0.25301204819277107, 9: 0.9056603773584906, 10: 0.20155038759689922, 11: 0.191044776119403, 12: 0.20233463035019456, 13: 0.05263157894736842, 16: 0.35036496350364965, 17: 0.0, 18: 0.04, 19: 0.46, 20: 0.3108108108108108, 21: 0.21505376344086022, 23: 0.6804123711340206, 24: 0.07142857142857142, 26: 0.6666666666666666, 27: 0.1388888888888889, 28: 0.23076923076923078, 29: 0.703125, 30: 0.8333333333333334, 31: 0.043478260869565216, 32: 0.6033057851239669, 33: 0.2608695652173913, 36: 0.15584415584415584, 39: 0.13333333333333333, 40: 0.2465753424657534}
Micro-average F1 score: 0.40069860279441116
Weighted-average F1 score: 0.38302640374326963
F1 score per class: {0: 0.29045643153526973, 2: 0.10294117647058823, 4: 0.8921568627450981, 5: 0.31746031746031744, 6: 0.17827298050139276, 7: 0.028169014084507043, 8: 0.17849898580121704, 9: 0.7142857142857143, 10: 0.17346938775510204, 11: 0.19536423841059603, 12: 0.13701657458563535, 13: 0.007246376811594203, 16: 0.3253012048192771, 17: 0.0, 18: 0.08176100628930817, 19: 0.41899441340782123, 20: 0.2781954887218045, 21: 0.1402439024390244, 23: 0.46153846153846156, 24: 0.05263157894736842, 26: 0.6188340807174888, 27: 0.14285714285714285, 28: 0.05217391304347826, 29: 0.6741573033707865, 30: 0.5151515151515151, 31: 0.034482758620689655, 32: 0.4339152119700748, 33: 0.08695652173913043, 36: 0.41706161137440756, 39: 0.125, 40: 0.14084507042253522}
Micro-average F1 score: 0.2601829803233488
Weighted-average F1 score: 0.2378947873199864
F1 score per class: {0: 0.28225806451612906, 2: 0.125, 4: 0.9166666666666666, 5: 0.4175365344467641, 6: 0.1679790026246719, 7: 0.03, 8: 0.22702702702702704, 9: 0.7936507936507936, 10: 0.19827586206896552, 11: 0.1711229946524064, 12: 0.13025210084033614, 13: 0.013793103448275862, 16: 0.3176470588235294, 17: 0.0, 18: 0.08205128205128205, 19: 0.4367816091954023, 20: 0.25874125874125875, 21: 0.1348973607038123, 23: 0.5196850393700787, 24: 0.0625, 26: 0.6359447004608295, 27: 0.14754098360655737, 28: 0.08955223880597014, 29: 0.6870229007633588, 30: 0.5964912280701754, 31: 0.04081632653061224, 32: 0.4223300970873786, 33: 0.08823529411764706, 36: 0.32786885245901637, 39: 0.16326530612244897, 40: 0.18248175182481752}
Micro-average F1 score: 0.27051231397339653
Weighted-average F1 score: 0.24592309734509418
cur_acc_wo_na:  ['0.8056', '0.7032', '0.4571', '0.8262', '0.5410', '0.4481']
his_acc_wo_na:  ['0.8056', '0.7425', '0.6592', '0.7021', '0.6459', '0.6109']
cur_acc des_wo_na:  ['0.8172', '0.8537', '0.6009', '0.8565', '0.7866', '0.6538']
his_acc des_wo_na:  ['0.8172', '0.7997', '0.7120', '0.7461', '0.7086', '0.6813']
cur_acc rrf_wo_na:  ['0.8222', '0.8514', '0.5919', '0.8565', '0.7554', '0.6829']
his_acc rrf_wo_na:  ['0.8222', '0.7987', '0.7172', '0.7474', '0.7086', '0.6788']
cur_acc_w_na:  ['0.6830', '0.5265', '0.3322', '0.5494', '0.3663', '0.2733']
his_acc_w_na:  ['0.6830', '0.5854', '0.5065', '0.4659', '0.4645', '0.4007']
cur_acc des_w_na:  ['0.6399', '0.4334', '0.3153', '0.3820', '0.3427', '0.2217']
his_acc des_w_na:  ['0.6399', '0.4844', '0.3834', '0.3369', '0.3085', '0.2602']
cur_acc rrf_w_na:  ['0.6574', '0.4367', '0.3204', '0.4058', '0.3421', '0.2387']
his_acc rrf_w_na:  ['0.6574', '0.4944', '0.3949', '0.3551', '0.3245', '0.2705']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 84.6484566CurrentTrain: epoch  0, batch     1 | loss: 95.8693765CurrentTrain: epoch  0, batch     2 | loss: 66.1284191CurrentTrain: epoch  0, batch     3 | loss: 95.4227031CurrentTrain: epoch  0, batch     4 | loss: 46.0506220CurrentTrain: epoch  1, batch     0 | loss: 96.4227395CurrentTrain: epoch  1, batch     1 | loss: 76.8004952CurrentTrain: epoch  1, batch     2 | loss: 118.7984760CurrentTrain: epoch  1, batch     3 | loss: 87.3827116CurrentTrain: epoch  1, batch     4 | loss: 104.6709499CurrentTrain: epoch  2, batch     0 | loss: 58.3334048CurrentTrain: epoch  2, batch     1 | loss: 90.8422242CurrentTrain: epoch  2, batch     2 | loss: 119.6109099CurrentTrain: epoch  2, batch     3 | loss: 68.9876841CurrentTrain: epoch  2, batch     4 | loss: 50.7508699CurrentTrain: epoch  3, batch     0 | loss: 184.5228567CurrentTrain: epoch  3, batch     1 | loss: 56.5069551CurrentTrain: epoch  3, batch     2 | loss: 176.0898132CurrentTrain: epoch  3, batch     3 | loss: 56.4188346CurrentTrain: epoch  3, batch     4 | loss: 47.7337489CurrentTrain: epoch  4, batch     0 | loss: 68.7387616CurrentTrain: epoch  4, batch     1 | loss: 88.8486466CurrentTrain: epoch  4, batch     2 | loss: 66.5356771CurrentTrain: epoch  4, batch     3 | loss: 56.7172382CurrentTrain: epoch  4, batch     4 | loss: 46.6449557CurrentTrain: epoch  5, batch     0 | loss: 67.3947591CurrentTrain: epoch  5, batch     1 | loss: 83.7243149CurrentTrain: epoch  5, batch     2 | loss: 83.9829068CurrentTrain: epoch  5, batch     3 | loss: 88.4613028CurrentTrain: epoch  5, batch     4 | loss: 29.8351253CurrentTrain: epoch  6, batch     0 | loss: 67.2824667CurrentTrain: epoch  6, batch     1 | loss: 64.3270612CurrentTrain: epoch  6, batch     2 | loss: 182.9411773CurrentTrain: epoch  6, batch     3 | loss: 85.8221584CurrentTrain: epoch  6, batch     4 | loss: 59.2239084CurrentTrain: epoch  7, batch     0 | loss: 84.8504221CurrentTrain: epoch  7, batch     1 | loss: 88.3462551CurrentTrain: epoch  7, batch     2 | loss: 66.7352803CurrentTrain: epoch  7, batch     3 | loss: 63.9185012CurrentTrain: epoch  7, batch     4 | loss: 45.3329640CurrentTrain: epoch  8, batch     0 | loss: 66.6892824CurrentTrain: epoch  8, batch     1 | loss: 67.8041304CurrentTrain: epoch  8, batch     2 | loss: 67.4264467CurrentTrain: epoch  8, batch     3 | loss: 52.9594713CurrentTrain: epoch  8, batch     4 | loss: 44.9807426CurrentTrain: epoch  9, batch     0 | loss: 80.4056614CurrentTrain: epoch  9, batch     1 | loss: 67.6932129CurrentTrain: epoch  9, batch     2 | loss: 118.0459409CurrentTrain: epoch  9, batch     3 | loss: 63.1236838CurrentTrain: epoch  9, batch     4 | loss: 46.0265520
MemoryTrain:  epoch  0, batch     0 | loss: 0.3644190MemoryTrain:  epoch  1, batch     0 | loss: 0.3578219MemoryTrain:  epoch  2, batch     0 | loss: 0.2690016MemoryTrain:  epoch  3, batch     0 | loss: 0.2426710MemoryTrain:  epoch  4, batch     0 | loss: 0.1846823MemoryTrain:  epoch  5, batch     0 | loss: 0.1714196MemoryTrain:  epoch  6, batch     0 | loss: 0.1385584MemoryTrain:  epoch  7, batch     0 | loss: 0.0999298MemoryTrain:  epoch  8, batch     0 | loss: 0.0942666MemoryTrain:  epoch  9, batch     0 | loss: 0.0815832

F1 score per class: {32: 0.3925233644859813, 1: 0.5614035087719298, 34: 0.0, 3: 0.0, 9: 0.09615384615384616, 11: 0.0, 14: 0.0, 18: 0.49635036496350365, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.16666666666666666}
Micro-average F1 score: 0.33797909407665505
Weighted-average F1 score: 0.28541403731066267
F1 score per class: {32: 0.35185185185185186, 1: 0.935064935064935, 34: 0.0, 3: 0.0, 36: 0.0, 40: 0.16071428571428573, 9: 0.0, 10: 0.0, 11: 0.6075949367088608, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.4931506849315068, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.4992481203007519
Weighted-average F1 score: 0.4409817602074199
F1 score per class: {32: 0.4482758620689655, 1: 0.9139072847682119, 34: 0.0, 3: 0.0, 33: 0.0, 40: 0.15384615384615385, 9: 0.0, 10: 0.0, 11: 0.6103896103896104, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.3076923076923077, 27: 0.0}
Micro-average F1 score: 0.4892966360856269
Weighted-average F1 score: 0.44319924507225933

F1 score per class: {0: 0.8307692307692308, 1: 0.336, 2: 0.631578947368421, 3: 0.5614035087719298, 4: 0.8636363636363636, 5: 0.9637305699481865, 6: 0.2764227642276423, 7: 0.057971014492753624, 8: 0.38016528925619836, 9: 0.9615384615384616, 10: 0.27586206896551724, 11: 0.33962264150943394, 12: 0.19130434782608696, 13: 0.08333333333333333, 14: 0.09090909090909091, 16: 0.7586206896551724, 17: 0.0, 18: 0.13636363636363635, 19: 0.5058823529411764, 20: 0.29411764705882354, 21: 0.23255813953488372, 22: 0.48226950354609927, 23: 0.7027027027027027, 24: 0.06896551724137931, 26: 0.7225130890052356, 27: 0.11764705882352941, 28: 0.3157894736842105, 29: 0.9214659685863874, 30: 0.8823529411764706, 31: 0.0, 32: 0.735632183908046, 33: 0.3333333333333333, 34: 0.12195121951219512, 36: 0.24, 39: 0.13333333333333333, 40: 0.2962962962962963}
Micro-average F1 score: 0.5081287854638189
Weighted-average F1 score: 0.5400081281812087
F1 score per class: {0: 0.9142857142857143, 1: 0.2992125984251969, 2: 0.45161290322580644, 3: 0.9, 4: 0.8764044943820225, 5: 0.9523809523809523, 6: 0.4057971014492754, 7: 0.056338028169014086, 8: 0.5027932960893855, 9: 0.9615384615384616, 10: 0.40625, 11: 0.42857142857142855, 12: 0.6705202312138728, 13: 0.08, 14: 0.14634146341463414, 16: 0.8620689655172413, 17: 0.0, 18: 0.32941176470588235, 19: 0.550561797752809, 20: 0.4225352112676056, 21: 0.3137254901960784, 22: 0.5748502994011976, 23: 0.7692307692307693, 24: 0.07407407407407407, 26: 0.711340206185567, 27: 0.12121212121212122, 28: 0.21428571428571427, 29: 0.9270833333333334, 30: 0.918918918918919, 31: 0.6666666666666666, 32: 0.8, 33: 0.25, 34: 0.28125, 36: 0.4888888888888889, 39: 0.3, 40: 0.5774647887323944}
Micro-average F1 score: 0.5891430146045743
Weighted-average F1 score: 0.578636755970174
F1 score per class: {0: 0.9142857142857143, 1: 0.37681159420289856, 2: 0.45161290322580644, 3: 0.8846153846153846, 4: 0.88268156424581, 5: 0.966183574879227, 6: 0.3971631205673759, 7: 0.056338028169014086, 8: 0.49032258064516127, 9: 0.9615384615384616, 10: 0.45454545454545453, 11: 0.46236559139784944, 12: 0.6588235294117647, 13: 0.0625, 14: 0.1415929203539823, 16: 0.847457627118644, 17: 0.0, 18: 0.3870967741935484, 19: 0.5760869565217391, 20: 0.4444444444444444, 21: 0.37735849056603776, 22: 0.5875, 23: 0.7692307692307693, 24: 0.07142857142857142, 26: 0.711340206185567, 27: 0.12903225806451613, 28: 0.2222222222222222, 29: 0.9270833333333334, 30: 0.918918918918919, 31: 0.6666666666666666, 32: 0.7936507936507936, 33: 0.23076923076923078, 34: 0.19607843137254902, 36: 0.5591397849462365, 39: 0.36363636363636365, 40: 0.5901639344262295}
Micro-average F1 score: 0.5992739458251884
Weighted-average F1 score: 0.5892322918152447

F1 score per class: {0: 0.0, 1: 0.16535433070866143, 2: 0.0, 3: 0.463768115942029, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.04608294930875576, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4473684210526316, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.16393442622950818, 40: 0.0}
Micro-average F1 score: 0.1685490877497828
Weighted-average F1 score: 0.11396664051552421
F1 score per class: {0: 0.0, 1: 0.1347517730496454, 2: 0.0, 3: 0.43902439024390244, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0728744939271255, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4085106382978723, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.3302752293577982, 36: 0.0, 40: 0.0}
Micro-average F1 score: 0.17547568710359407
Weighted-average F1 score: 0.14624693474362022
F1 score per class: {0: 0.0, 1: 0.17105263157894737, 2: 0.0, 3: 0.4709897610921502, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.06779661016949153, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.46534653465346537, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.273972602739726, 36: 0.0, 40: 0.0}
Micro-average F1 score: 0.18018018018018017
Weighted-average F1 score: 0.1451821265557781

F1 score per class: {0: 0.5567010309278351, 1: 0.11965811965811966, 2: 0.24, 3: 0.3386243386243386, 4: 0.8539325842696629, 5: 0.8532110091743119, 6: 0.17616580310880828, 7: 0.024539877300613498, 8: 0.24864864864864866, 9: 0.7352941176470589, 10: 0.24060150375939848, 11: 0.18620689655172415, 12: 0.14102564102564102, 13: 0.015384615384615385, 14: 0.03424657534246575, 16: 0.4036697247706422, 17: 0.0, 18: 0.08333333333333333, 19: 0.3481781376518219, 20: 0.18181818181818182, 21: 0.15151515151515152, 22: 0.41975308641975306, 23: 0.6341463414634146, 24: 0.04081632653061224, 26: 0.6330275229357798, 27: 0.038461538461538464, 28: 0.15789473684210525, 29: 0.6929133858267716, 30: 0.8333333333333334, 31: 0.0, 32: 0.4830188679245283, 33: 0.2, 34: 0.08264462809917356, 36: 0.20689655172413793, 39: 0.125, 40: 0.23300970873786409}
Micro-average F1 score: 0.3258381030253475
Weighted-average F1 score: 0.29627059006525286
F1 score per class: {0: 0.3595505617977528, 1: 0.09134615384615384, 2: 0.11382113821138211, 3: 0.21428571428571427, 4: 0.8387096774193549, 5: 0.3883495145631068, 6: 0.17391304347826086, 7: 0.021739130434782608, 8: 0.1956521739130435, 9: 0.5882352941176471, 10: 0.2736842105263158, 11: 0.1625615763546798, 12: 0.1514360313315927, 13: 0.01, 14: 0.037815126050420166, 16: 0.4132231404958678, 17: 0.0, 18: 0.07253886010362694, 19: 0.30914826498422715, 20: 0.18867924528301888, 21: 0.11347517730496454, 22: 0.35424354243542433, 23: 0.5, 24: 0.043478260869565216, 26: 0.5847457627118644, 27: 0.042105263157894736, 28: 0.05555555555555555, 29: 0.6793893129770993, 30: 0.576271186440678, 31: 0.0273972602739726, 32: 0.4075067024128686, 33: 0.08823529411764706, 34: 0.09375, 36: 0.2953020134228188, 39: 0.13043478260869565, 40: 0.24404761904761904}
Micro-average F1 score: 0.23882931188561216
Weighted-average F1 score: 0.21745340219697135
F1 score per class: {0: 0.35555555555555557, 1: 0.11158798283261803, 2: 0.1320754716981132, 3: 0.2522851919561243, 4: 0.8633879781420765, 5: 0.49019607843137253, 6: 0.16666666666666666, 7: 0.022222222222222223, 8: 0.21714285714285714, 9: 0.625, 10: 0.26548672566371684, 11: 0.13129770992366413, 12: 0.16592592592592592, 13: 0.008438818565400843, 14: 0.03864734299516908, 16: 0.3968253968253968, 17: 0.0, 18: 0.11822660098522167, 19: 0.32515337423312884, 20: 0.1951219512195122, 21: 0.14084507042253522, 22: 0.4253393665158371, 23: 0.5309734513274337, 24: 0.034482758620689655, 26: 0.6160714285714286, 27: 0.047619047619047616, 28: 0.07407407407407407, 29: 0.6819923371647509, 30: 0.6296296296296297, 31: 0.031746031746031744, 32: 0.40431266846361186, 33: 0.07692307692307693, 34: 0.08888888888888889, 36: 0.33121019108280253, 39: 0.17777777777777778, 40: 0.3050847457627119}
Micro-average F1 score: 0.25864770398939374
Weighted-average F1 score: 0.23476113709562338
cur_acc_wo_na:  ['0.8056', '0.7032', '0.4571', '0.8262', '0.5410', '0.4481', '0.3380']
his_acc_wo_na:  ['0.8056', '0.7425', '0.6592', '0.7021', '0.6459', '0.6109', '0.5081']
cur_acc des_wo_na:  ['0.8172', '0.8537', '0.6009', '0.8565', '0.7866', '0.6538', '0.4992']
his_acc des_wo_na:  ['0.8172', '0.7997', '0.7120', '0.7461', '0.7086', '0.6813', '0.5891']
cur_acc rrf_wo_na:  ['0.8222', '0.8514', '0.5919', '0.8565', '0.7554', '0.6829', '0.4893']
his_acc rrf_wo_na:  ['0.8222', '0.7987', '0.7172', '0.7474', '0.7086', '0.6788', '0.5993']
cur_acc_w_na:  ['0.6830', '0.5265', '0.3322', '0.5494', '0.3663', '0.2733', '0.1685']
his_acc_w_na:  ['0.6830', '0.5854', '0.5065', '0.4659', '0.4645', '0.4007', '0.3258']
cur_acc des_w_na:  ['0.6399', '0.4334', '0.3153', '0.3820', '0.3427', '0.2217', '0.1755']
his_acc des_w_na:  ['0.6399', '0.4844', '0.3834', '0.3369', '0.3085', '0.2602', '0.2388']
cur_acc rrf_w_na:  ['0.6574', '0.4367', '0.3204', '0.4058', '0.3421', '0.2387', '0.1802']
his_acc rrf_w_na:  ['0.6574', '0.4944', '0.3949', '0.3551', '0.3245', '0.2705', '0.2586']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 80.2146779CurrentTrain: epoch  0, batch     1 | loss: 79.3501239CurrentTrain: epoch  0, batch     2 | loss: 64.0936784CurrentTrain: epoch  0, batch     3 | loss: 91.3847325CurrentTrain: epoch  1, batch     0 | loss: 93.4150704CurrentTrain: epoch  1, batch     1 | loss: 95.1000755CurrentTrain: epoch  1, batch     2 | loss: 80.6549325CurrentTrain: epoch  1, batch     3 | loss: 87.6035488CurrentTrain: epoch  2, batch     0 | loss: 67.9623595CurrentTrain: epoch  2, batch     1 | loss: 91.2756792CurrentTrain: epoch  2, batch     2 | loss: 58.1989291CurrentTrain: epoch  2, batch     3 | loss: 48.5312107CurrentTrain: epoch  3, batch     0 | loss: 57.0987493CurrentTrain: epoch  3, batch     1 | loss: 52.6122237CurrentTrain: epoch  3, batch     2 | loss: 119.7165387CurrentTrain: epoch  3, batch     3 | loss: 37.4507635CurrentTrain: epoch  4, batch     0 | loss: 65.7458352CurrentTrain: epoch  4, batch     1 | loss: 88.1172751CurrentTrain: epoch  4, batch     2 | loss: 52.3103655CurrentTrain: epoch  4, batch     3 | loss: 58.8352268CurrentTrain: epoch  5, batch     0 | loss: 55.3928138CurrentTrain: epoch  5, batch     1 | loss: 52.5061679CurrentTrain: epoch  5, batch     2 | loss: 84.0626730CurrentTrain: epoch  5, batch     3 | loss: 44.5838694CurrentTrain: epoch  6, batch     0 | loss: 67.2832532CurrentTrain: epoch  6, batch     1 | loss: 51.8625407CurrentTrain: epoch  6, batch     2 | loss: 51.8092143CurrentTrain: epoch  6, batch     3 | loss: 81.4150184CurrentTrain: epoch  7, batch     0 | loss: 67.8994804CurrentTrain: epoch  7, batch     1 | loss: 56.6133122CurrentTrain: epoch  7, batch     2 | loss: 51.0743888CurrentTrain: epoch  7, batch     3 | loss: 44.0011448CurrentTrain: epoch  8, batch     0 | loss: 64.4064280CurrentTrain: epoch  8, batch     1 | loss: 51.2356556CurrentTrain: epoch  8, batch     2 | loss: 66.1251982CurrentTrain: epoch  8, batch     3 | loss: 57.8576664CurrentTrain: epoch  9, batch     0 | loss: 67.7385265CurrentTrain: epoch  9, batch     1 | loss: 82.8718594CurrentTrain: epoch  9, batch     2 | loss: 80.7816250CurrentTrain: epoch  9, batch     3 | loss: 33.2866282
MemoryTrain:  epoch  0, batch     0 | loss: 0.3245828MemoryTrain:  epoch  1, batch     0 | loss: 0.3518798MemoryTrain:  epoch  2, batch     0 | loss: 0.2441940MemoryTrain:  epoch  3, batch     0 | loss: 0.1817285MemoryTrain:  epoch  4, batch     0 | loss: 0.1465703MemoryTrain:  epoch  5, batch     0 | loss: 0.1351140MemoryTrain:  epoch  6, batch     0 | loss: 0.1105482MemoryTrain:  epoch  7, batch     0 | loss: 0.0896650MemoryTrain:  epoch  8, batch     0 | loss: 0.0919486MemoryTrain:  epoch  9, batch     0 | loss: 0.0810770

F1 score per class: {32: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.8235294117647058, 36: 0.0, 38: 0.0, 8: 0.5555555555555556, 11: 0.0, 13: 0.0, 15: 0.813953488372093, 18: 0.0, 23: 0.5542168674698795, 25: 0.4444444444444444}
Micro-average F1 score: 0.5552238805970149
Weighted-average F1 score: 0.4601994969690306
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.75, 18: 0.0, 23: 0.0, 25: 0.5945945945945946, 32: 0.0, 34: 0.0, 35: 0.9263157894736842, 36: 0.0, 37: 0.6956521739130435, 38: 0.7272727272727273, 39: 0.0}
Micro-average F1 score: 0.6201550387596899
Weighted-average F1 score: 0.49589359687905493
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 13: 0.0, 14: 0.0, 15: 0.75, 18: 0.0, 23: 0.0, 25: 0.5753424657534246, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.9494949494949495, 36: 0.0, 37: 0.6956521739130435, 38: 0.7272727272727273, 39: 0.0}
Micro-average F1 score: 0.6421052631578947
Weighted-average F1 score: 0.5366211316098354

F1 score per class: {0: 0.9142857142857143, 1: 0.3142857142857143, 2: 0.5454545454545454, 3: 0.7076923076923077, 4: 0.8700564971751412, 5: 0.8796296296296297, 6: 0.3064516129032258, 7: 0.09090909090909091, 8: 0.38333333333333336, 9: 0.96, 10: 0.22807017543859648, 11: 0.37037037037037035, 12: 0.05660377358490566, 13: 0.13333333333333333, 14: 0.11235955056179775, 15: 0.7, 16: 0.8135593220338984, 17: 0.0, 18: 0.4, 19: 0.4484848484848485, 20: 0.25806451612903225, 21: 0.10526315789473684, 22: 0.463768115942029, 23: 0.7012987012987013, 24: 0.08695652173913043, 25: 0.5555555555555556, 26: 0.7301587301587301, 27: 0.1111111111111111, 28: 0.2857142857142857, 29: 0.9263157894736842, 30: 0.9142857142857143, 31: 0.0, 32: 0.7241379310344828, 33: 0.3333333333333333, 34: 0.028169014084507043, 35: 0.693069306930693, 36: 0.21333333333333335, 37: 0.3194444444444444, 38: 0.21621621621621623, 39: 0.13333333333333333, 40: 0.43956043956043955}
Micro-average F1 score: 0.5128785734503255
Weighted-average F1 score: 0.5540772795393447
F1 score per class: {0: 0.958904109589041, 1: 0.291970802919708, 2: 0.4117647058823529, 3: 0.8875739644970414, 4: 0.8636363636363636, 5: 0.847457627118644, 6: 0.5098039215686274, 7: 0.1016949152542373, 8: 0.5274725274725275, 9: 0.9803921568627451, 10: 0.31932773109243695, 11: 0.3582089552238806, 12: 0.632183908045977, 13: 0.16666666666666666, 14: 0.13986013986013987, 15: 0.5454545454545454, 16: 0.847457627118644, 17: 0.15384615384615385, 18: 0.4444444444444444, 19: 0.562874251497006, 20: 0.2857142857142857, 21: 0.32142857142857145, 22: 0.373134328358209, 23: 0.725, 24: 0.07407407407407407, 25: 0.5789473684210527, 26: 0.7225130890052356, 27: 0.14285714285714285, 28: 0.25, 29: 0.9214659685863874, 30: 0.9444444444444444, 31: 0.6666666666666666, 32: 0.8102564102564103, 33: 0.24, 34: 0.038461538461538464, 35: 0.822429906542056, 36: 0.5, 37: 0.3575418994413408, 38: 0.367816091954023, 39: 0.2, 40: 0.6754966887417219}
Micro-average F1 score: 0.5714988924440069
Weighted-average F1 score: 0.5709620670908495
F1 score per class: {0: 0.958904109589041, 1: 0.34965034965034963, 2: 0.4, 3: 0.84, 4: 0.8636363636363636, 5: 0.8461538461538461, 6: 0.49673202614379086, 7: 0.09375, 8: 0.4691358024691358, 9: 0.9803921568627451, 10: 0.2608695652173913, 11: 0.423841059602649, 12: 0.6335403726708074, 13: 0.14285714285714285, 14: 0.125, 15: 0.5217391304347826, 16: 0.819672131147541, 17: 0.0, 18: 0.4, 19: 0.576271186440678, 20: 0.3125, 21: 0.3333333333333333, 22: 0.4148148148148148, 23: 0.717948717948718, 24: 0.07407407407407407, 25: 0.5675675675675675, 26: 0.7225130890052356, 27: 0.16, 28: 0.23529411764705882, 29: 0.9270833333333334, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.8223350253807107, 33: 0.2727272727272727, 34: 0.043478260869565216, 35: 0.746031746031746, 36: 0.4186046511627907, 37: 0.3248730964467005, 38: 0.3018867924528302, 39: 0.21052631578947367, 40: 0.71875}
Micro-average F1 score: 0.5656615998006479
Weighted-average F1 score: 0.5655826613172286

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.8235294117647058, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5555555555555556, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.693069306930693, 36: 0.0, 37: 0.44660194174757284, 38: 0.32653061224489793}
Micro-average F1 score: 0.33941605839416056
Weighted-average F1 score: 0.22360319507863746
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.631578947368421, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5301204819277109, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7154471544715447, 36: 0.0, 37: 0.48854961832061067, 38: 0.5333333333333333, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.25316455696202533
Weighted-average F1 score: 0.16784983956553692
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.631578947368421, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5060240963855421, 26: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.6266666666666667, 36: 0.0, 37: 0.4740740740740741, 38: 0.463768115942029, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.27570621468926554
Weighted-average F1 score: 0.201726616700945

F1 score per class: {0: 0.49230769230769234, 1: 0.09799554565701558, 2: 0.1348314606741573, 3: 0.44660194174757284, 4: 0.8555555555555555, 5: 0.7116104868913857, 6: 0.17592592592592593, 7: 0.03409090909090909, 8: 0.22439024390243903, 9: 0.8275862068965517, 10: 0.17218543046357615, 11: 0.15113350125944586, 12: 0.0425531914893617, 13: 0.017241379310344827, 14: 0.05263157894736842, 15: 0.358974358974359, 16: 0.3310344827586207, 17: 0.0, 18: 0.10784313725490197, 19: 0.28793774319066145, 20: 0.1553398058252427, 21: 0.07692307692307693, 22: 0.43243243243243246, 23: 0.6, 24: 0.08, 25: 0.5555555555555556, 26: 0.6359447004608295, 27: 0.045454545454545456, 28: 0.14285714285714285, 29: 0.64, 30: 0.8888888888888888, 31: 0.0, 32: 0.4632352941176471, 33: 0.18181818181818182, 34: 0.025, 35: 0.3017241379310345, 36: 0.1951219512195122, 37: 0.11734693877551021, 38: 0.054982817869415807, 39: 0.13333333333333333, 40: 0.29850746268656714}
Micro-average F1 score: 0.2887189292543021
Weighted-average F1 score: 0.2613301642122688
F1 score per class: {0: 0.32407407407407407, 1: 0.08565310492505353, 2: 0.09395973154362416, 3: 0.228310502283105, 4: 0.8172043010752689, 5: 0.3508771929824561, 6: 0.1897810218978102, 7: 0.03592814371257485, 8: 0.18604651162790697, 9: 0.5882352941176471, 10: 0.19895287958115182, 11: 0.1791044776119403, 12: 0.14157014157014158, 13: 0.019230769230769232, 14: 0.03824091778202677, 15: 0.20689655172413793, 16: 0.3333333333333333, 17: 0.10526315789473684, 18: 0.06936416184971098, 19: 0.30718954248366015, 20: 0.15, 21: 0.10227272727272728, 22: 0.3105590062111801, 23: 0.47540983606557374, 24: 0.05128205128205128, 25: 0.5116279069767442, 26: 0.5847457627118644, 27: 0.043010752688172046, 28: 0.06666666666666667, 29: 0.624113475177305, 30: 0.5573770491803278, 31: 0.022988505747126436, 32: 0.4082687338501292, 33: 0.0759493670886076, 34: 0.01834862385321101, 35: 0.18448637316561844, 36: 0.27218934911242604, 37: 0.090014064697609, 38: 0.0851063829787234, 39: 0.07692307692307693, 40: 0.2531017369727047}
Micro-average F1 score: 0.2157190635451505
Weighted-average F1 score: 0.19680999275572175
F1 score per class: {0: 0.3349282296650718, 1: 0.09746588693957114, 2: 0.10526315789473684, 3: 0.38181818181818183, 4: 0.8491620111731844, 5: 0.391304347826087, 6: 0.17194570135746606, 7: 0.033707865168539325, 8: 0.19095477386934673, 9: 0.684931506849315, 10: 0.16129032258064516, 11: 0.16623376623376623, 12: 0.15716486902927582, 13: 0.01694915254237288, 14: 0.037825059101654845, 15: 0.1791044776119403, 16: 0.2994011976047904, 17: 0.0, 18: 0.06779661016949153, 19: 0.2914285714285714, 20: 0.15384615384615385, 21: 0.10778443113772455, 22: 0.35, 23: 0.5185185185185185, 24: 0.05128205128205128, 25: 0.5, 26: 0.6106194690265486, 27: 0.05555555555555555, 28: 0.08333333333333333, 29: 0.6289752650176679, 30: 0.5454545454545454, 31: 0.023809523809523808, 32: 0.41012658227848103, 33: 0.09090909090909091, 34: 0.026845637583892617, 35: 0.14779874213836477, 36: 0.26277372262773724, 37: 0.07655502392344497, 38: 0.062135922330097085, 39: 0.1111111111111111, 40: 0.3382352941176471}
Micro-average F1 score: 0.2228330224796309
Weighted-average F1 score: 0.2011268798709601
cur_acc_wo_na:  ['0.8056', '0.7032', '0.4571', '0.8262', '0.5410', '0.4481', '0.3380', '0.5552']
his_acc_wo_na:  ['0.8056', '0.7425', '0.6592', '0.7021', '0.6459', '0.6109', '0.5081', '0.5129']
cur_acc des_wo_na:  ['0.8172', '0.8537', '0.6009', '0.8565', '0.7866', '0.6538', '0.4992', '0.6202']
his_acc des_wo_na:  ['0.8172', '0.7997', '0.7120', '0.7461', '0.7086', '0.6813', '0.5891', '0.5715']
cur_acc rrf_wo_na:  ['0.8222', '0.8514', '0.5919', '0.8565', '0.7554', '0.6829', '0.4893', '0.6421']
his_acc rrf_wo_na:  ['0.8222', '0.7987', '0.7172', '0.7474', '0.7086', '0.6788', '0.5993', '0.5657']
cur_acc_w_na:  ['0.6830', '0.5265', '0.3322', '0.5494', '0.3663', '0.2733', '0.1685', '0.3394']
his_acc_w_na:  ['0.6830', '0.5854', '0.5065', '0.4659', '0.4645', '0.4007', '0.3258', '0.2887']
cur_acc des_w_na:  ['0.6399', '0.4334', '0.3153', '0.3820', '0.3427', '0.2217', '0.1755', '0.2532']
his_acc des_w_na:  ['0.6399', '0.4844', '0.3834', '0.3369', '0.3085', '0.2602', '0.2388', '0.2157']
cur_acc rrf_w_na:  ['0.6574', '0.4367', '0.3204', '0.4058', '0.3421', '0.2387', '0.1802', '0.2757']
his_acc rrf_w_na:  ['0.6574', '0.4944', '0.3949', '0.3551', '0.3245', '0.2705', '0.2586', '0.2228']
----------END
his_acc mean_wo_na:  [0.8044 0.7293 0.6615 0.6292 0.5576 0.5432 0.5027 0.4943]
his_acc des mean_wo_na:  [0.8347 0.7947 0.7329 0.7062 0.6423 0.6344 0.5849 0.5708]
his_acc rrf mean_wo_na:  [0.8361 0.7885 0.7317 0.6985 0.6281 0.622  0.5733 0.5577]
his_acc mean_w_na:  [0.6815 0.5634 0.4818 0.4353 0.3674 0.3482 0.3117 0.2969]
his_acc des mean_w_na:  [0.6325 0.4716 0.382  0.3345 0.2855 0.2623 0.2415 0.2344]
his_acc rrf mean_w_na:  [0.6375 0.4821 0.3936 0.3441 0.292  0.2677 0.247  0.2389]
