#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
Labels: [ 6  6  6 ... 41 41 41]
Length Labels: 4602
self.majority_label: 41
self.majority_ratio: 0.2
Length self.majority_indices: 4002
Length self.minority_indices: 600
CurrentTrain: epoch  0, batch     0 | loss: 59.7515103CurrentTrain: epoch  0, batch     1 | loss: 57.9676939CurrentTrain: epoch  0, batch     2 | loss: 57.1314288CurrentTrain: epoch  0, batch     3 | loss: 56.8604383CurrentTrain: epoch  0, batch     4 | loss: 57.2312401CurrentTrain: epoch  0, batch     5 | loss: 56.2886247CurrentTrain: epoch  0, batch     6 | loss: 55.9189960CurrentTrain: epoch  0, batch     7 | loss: 54.9208089CurrentTrain: epoch  0, batch     8 | loss: 55.3279626CurrentTrain: epoch  0, batch     9 | loss: 54.1533952CurrentTrain: epoch  0, batch    10 | loss: 54.3430197CurrentTrain: epoch  0, batch    11 | loss: 50.9254547CurrentTrain: epoch  0, batch    12 | loss: 52.5289444CurrentTrain: epoch  0, batch    13 | loss: 54.2047822CurrentTrain: epoch  0, batch    14 | loss: 51.9128614CurrentTrain: epoch  0, batch    15 | loss: 26.8344062CurrentTrain: epoch  1, batch     0 | loss: 51.1441887CurrentTrain: epoch  1, batch     1 | loss: 49.5370451CurrentTrain: epoch  1, batch     2 | loss: 49.1088989CurrentTrain: epoch  1, batch     3 | loss: 46.0901100CurrentTrain: epoch  1, batch     4 | loss: 48.7413359CurrentTrain: epoch  1, batch     5 | loss: 47.6514580CurrentTrain: epoch  1, batch     6 | loss: 45.0986382CurrentTrain: epoch  1, batch     7 | loss: 45.0807226CurrentTrain: epoch  1, batch     8 | loss: 47.6270099CurrentTrain: epoch  1, batch     9 | loss: 44.6349475CurrentTrain: epoch  1, batch    10 | loss: 42.9307917CurrentTrain: epoch  1, batch    11 | loss: 41.6008575CurrentTrain: epoch  1, batch    12 | loss: 42.1610679CurrentTrain: epoch  1, batch    13 | loss: 42.9547255CurrentTrain: epoch  1, batch    14 | loss: 41.4426289CurrentTrain: epoch  1, batch    15 | loss: 22.4502125CurrentTrain: epoch  2, batch     0 | loss: 39.8628908CurrentTrain: epoch  2, batch     1 | loss: 40.6158781CurrentTrain: epoch  2, batch     2 | loss: 44.8868356CurrentTrain: epoch  2, batch     3 | loss: 41.1507540CurrentTrain: epoch  2, batch     4 | loss: 40.0738221CurrentTrain: epoch  2, batch     5 | loss: 37.6608549CurrentTrain: epoch  2, batch     6 | loss: 40.9146379CurrentTrain: epoch  2, batch     7 | loss: 39.5126093CurrentTrain: epoch  2, batch     8 | loss: 38.3979915CurrentTrain: epoch  2, batch     9 | loss: 36.6328831CurrentTrain: epoch  2, batch    10 | loss: 41.7714086CurrentTrain: epoch  2, batch    11 | loss: 38.3342760CurrentTrain: epoch  2, batch    12 | loss: 39.7822391CurrentTrain: epoch  2, batch    13 | loss: 42.7393699CurrentTrain: epoch  2, batch    14 | loss: 37.0086064CurrentTrain: epoch  2, batch    15 | loss: 16.7386644CurrentTrain: epoch  3, batch     0 | loss: 37.3814723CurrentTrain: epoch  3, batch     1 | loss: 35.6417168CurrentTrain: epoch  3, batch     2 | loss: 32.8942299CurrentTrain: epoch  3, batch     3 | loss: 36.3516884CurrentTrain: epoch  3, batch     4 | loss: 41.9873893CurrentTrain: epoch  3, batch     5 | loss: 37.4334301CurrentTrain: epoch  3, batch     6 | loss: 39.5335155CurrentTrain: epoch  3, batch     7 | loss: 36.4365352CurrentTrain: epoch  3, batch     8 | loss: 38.3259176CurrentTrain: epoch  3, batch     9 | loss: 37.0348110CurrentTrain: epoch  3, batch    10 | loss: 35.5121653CurrentTrain: epoch  3, batch    11 | loss: 37.8285979CurrentTrain: epoch  3, batch    12 | loss: 36.5594666CurrentTrain: epoch  3, batch    13 | loss: 33.8997453CurrentTrain: epoch  3, batch    14 | loss: 34.7111020CurrentTrain: epoch  3, batch    15 | loss: 17.0611201CurrentTrain: epoch  4, batch     0 | loss: 33.2513167CurrentTrain: epoch  4, batch     1 | loss: 37.5770401CurrentTrain: epoch  4, batch     2 | loss: 34.8364281CurrentTrain: epoch  4, batch     3 | loss: 37.4371352CurrentTrain: epoch  4, batch     4 | loss: 35.0852247CurrentTrain: epoch  4, batch     5 | loss: 34.3607854CurrentTrain: epoch  4, batch     6 | loss: 35.4041199CurrentTrain: epoch  4, batch     7 | loss: 34.1260046CurrentTrain: epoch  4, batch     8 | loss: 34.1861231CurrentTrain: epoch  4, batch     9 | loss: 34.1072517CurrentTrain: epoch  4, batch    10 | loss: 32.9599833CurrentTrain: epoch  4, batch    11 | loss: 37.8337093CurrentTrain: epoch  4, batch    12 | loss: 32.7185629CurrentTrain: epoch  4, batch    13 | loss: 35.2992365CurrentTrain: epoch  4, batch    14 | loss: 33.2323090CurrentTrain: epoch  4, batch    15 | loss: 19.5817436CurrentTrain: epoch  5, batch     0 | loss: 36.4231546CurrentTrain: epoch  5, batch     1 | loss: 35.8154035CurrentTrain: epoch  5, batch     2 | loss: 35.0333770CurrentTrain: epoch  5, batch     3 | loss: 31.7967408CurrentTrain: epoch  5, batch     4 | loss: 32.1833841CurrentTrain: epoch  5, batch     5 | loss: 32.3913337CurrentTrain: epoch  5, batch     6 | loss: 35.7904483CurrentTrain: epoch  5, batch     7 | loss: 35.6887002CurrentTrain: epoch  5, batch     8 | loss: 33.2304445CurrentTrain: epoch  5, batch     9 | loss: 32.5944986CurrentTrain: epoch  5, batch    10 | loss: 33.8587717CurrentTrain: epoch  5, batch    11 | loss: 34.7343258CurrentTrain: epoch  5, batch    12 | loss: 36.2647591CurrentTrain: epoch  5, batch    13 | loss: 31.0556440CurrentTrain: epoch  5, batch    14 | loss: 34.9252327CurrentTrain: epoch  5, batch    15 | loss: 14.3369996CurrentTrain: epoch  6, batch     0 | loss: 34.2503808CurrentTrain: epoch  6, batch     1 | loss: 31.7680281CurrentTrain: epoch  6, batch     2 | loss: 35.4849080CurrentTrain: epoch  6, batch     3 | loss: 33.9339720CurrentTrain: epoch  6, batch     4 | loss: 31.4642383CurrentTrain: epoch  6, batch     5 | loss: 35.4264452CurrentTrain: epoch  6, batch     6 | loss: 31.7887062CurrentTrain: epoch  6, batch     7 | loss: 31.8685147CurrentTrain: epoch  6, batch     8 | loss: 30.4920932CurrentTrain: epoch  6, batch     9 | loss: 34.4036726CurrentTrain: epoch  6, batch    10 | loss: 34.7381379CurrentTrain: epoch  6, batch    11 | loss: 34.2418760CurrentTrain: epoch  6, batch    12 | loss: 34.8982871CurrentTrain: epoch  6, batch    13 | loss: 31.6630274CurrentTrain: epoch  6, batch    14 | loss: 33.2185063CurrentTrain: epoch  6, batch    15 | loss: 13.3322840CurrentTrain: epoch  7, batch     0 | loss: 32.7256140CurrentTrain: epoch  7, batch     1 | loss: 31.9967037CurrentTrain: epoch  7, batch     2 | loss: 33.5966246CurrentTrain: epoch  7, batch     3 | loss: 33.9564015CurrentTrain: epoch  7, batch     4 | loss: 34.8009290CurrentTrain: epoch  7, batch     5 | loss: 33.9265698CurrentTrain: epoch  7, batch     6 | loss: 32.0322596CurrentTrain: epoch  7, batch     7 | loss: 33.9069311CurrentTrain: epoch  7, batch     8 | loss: 33.3853777CurrentTrain: epoch  7, batch     9 | loss: 30.8908285CurrentTrain: epoch  7, batch    10 | loss: 33.2519528CurrentTrain: epoch  7, batch    11 | loss: 30.7656132CurrentTrain: epoch  7, batch    12 | loss: 31.0791315CurrentTrain: epoch  7, batch    13 | loss: 32.5296117CurrentTrain: epoch  7, batch    14 | loss: 35.3588021CurrentTrain: epoch  7, batch    15 | loss: 14.9753889CurrentTrain: epoch  8, batch     0 | loss: 33.2059539CurrentTrain: epoch  8, batch     1 | loss: 34.2387951CurrentTrain: epoch  8, batch     2 | loss: 32.6117178CurrentTrain: epoch  8, batch     3 | loss: 34.4340704CurrentTrain: epoch  8, batch     4 | loss: 31.5252708CurrentTrain: epoch  8, batch     5 | loss: 34.1991729CurrentTrain: epoch  8, batch     6 | loss: 31.2353677CurrentTrain: epoch  8, batch     7 | loss: 30.6775647CurrentTrain: epoch  8, batch     8 | loss: 33.4585657CurrentTrain: epoch  8, batch     9 | loss: 33.4291105CurrentTrain: epoch  8, batch    10 | loss: 31.5181348CurrentTrain: epoch  8, batch    11 | loss: 31.2468805CurrentTrain: epoch  8, batch    12 | loss: 34.4578898CurrentTrain: epoch  8, batch    13 | loss: 30.9390923CurrentTrain: epoch  8, batch    14 | loss: 35.3605941CurrentTrain: epoch  8, batch    15 | loss: 14.2690491CurrentTrain: epoch  9, batch     0 | loss: 32.2088689CurrentTrain: epoch  9, batch     1 | loss: 31.5513269CurrentTrain: epoch  9, batch     2 | loss: 33.4969310CurrentTrain: epoch  9, batch     3 | loss: 31.9905220CurrentTrain: epoch  9, batch     4 | loss: 30.2693243CurrentTrain: epoch  9, batch     5 | loss: 33.3193909CurrentTrain: epoch  9, batch     6 | loss: 34.0731482CurrentTrain: epoch  9, batch     7 | loss: 31.7619845CurrentTrain: epoch  9, batch     8 | loss: 33.2407341CurrentTrain: epoch  9, batch     9 | loss: 31.6177639CurrentTrain: epoch  9, batch    10 | loss: 31.3074521CurrentTrain: epoch  9, batch    11 | loss: 35.3738605CurrentTrain: epoch  9, batch    12 | loss: 32.0956852CurrentTrain: epoch  9, batch    13 | loss: 31.3778190CurrentTrain: epoch  9, batch    14 | loss: 32.1888006CurrentTrain: epoch  9, batch    15 | loss: 15.6958106

F1 score per class: {32: 0.36923076923076925, 6: 0.5470085470085471, 19: 0.125, 24: 0.6764705882352942, 26: 0.8290598290598291, 29: 0.5749235474006116}
Micro-average F1 score: 0.5092748735244519
Weighted-average F1 score: 0.4790489071423434
F1 score per class: {32: 0.4045584045584046, 6: 0.6168831168831169, 19: 0.12903225806451613, 24: 0.6008230452674898, 26: 0.8738738738738738, 29: 0.706766917293233}
Micro-average F1 score: 0.5785997357992074
Weighted-average F1 score: 0.5556093208506294
F1 score per class: {32: 0.40226628895184136, 6: 0.6168831168831169, 19: 0.12903225806451613, 24: 0.6058091286307054, 26: 0.8738738738738738, 29: 0.704119850187266}
Micro-average F1 score: 0.5782178217821782
Weighted-average F1 score: 0.5550473355086843

F1 score per class: {32: 0.36923076923076925, 6: 0.5470085470085471, 19: 0.125, 24: 0.6764705882352942, 26: 0.8290598290598291, 29: 0.5749235474006116}
Micro-average F1 score: 0.5092748735244519
Weighted-average F1 score: 0.4790489071423434
F1 score per class: {32: 0.4045584045584046, 6: 0.6168831168831169, 19: 0.12903225806451613, 24: 0.6008230452674898, 26: 0.8738738738738738, 29: 0.706766917293233}
Micro-average F1 score: 0.5785997357992074
Weighted-average F1 score: 0.5556093208506294
F1 score per class: {32: 0.40226628895184136, 6: 0.6168831168831169, 19: 0.12903225806451613, 24: 0.6058091286307054, 26: 0.8738738738738738, 29: 0.704119850187266}
Micro-average F1 score: 0.5782178217821782
Weighted-average F1 score: 0.5550473355086843
cur_acc:  ['0.5093']
his_acc:  ['0.5093']
cur_acc des:  ['0.5786']
his_acc des:  ['0.5786']
cur_acc rrf:  ['0.5782']
his_acc rrf:  ['0.5782']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
Labels: [ 2  2  2  2  2 28 28 28 28 28 11 11 11 11 11 12 12 12 12 12 39 39 39 39
 39 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41
 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41
 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41
 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41
 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41
 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41
 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41
 41 41 41 41 41 41 41 41 41 41 41 41]
Length Labels: 204
self.majority_label: 41
self.majority_ratio: 0.2
Length self.majority_indices: 179
Length self.minority_indices: 25
CurrentTrain: epoch  0, batch     0 | loss: 39.5784410CurrentTrain: epoch  1, batch     0 | loss: 37.2744202CurrentTrain: epoch  2, batch     0 | loss: 36.7065673CurrentTrain: epoch  3, batch     0 | loss: 33.6136002CurrentTrain: epoch  4, batch     0 | loss: 31.0036546CurrentTrain: epoch  5, batch     0 | loss: 32.3770572CurrentTrain: epoch  6, batch     0 | loss: 29.4924475CurrentTrain: epoch  7, batch     0 | loss: 28.4381501CurrentTrain: epoch  8, batch     0 | loss: 27.8276596CurrentTrain: epoch  9, batch     0 | loss: 30.0369272
MemoryTrain:  epoch  0, batch     0 | loss: 0.5557917MemoryTrain:  epoch  1, batch     0 | loss: 0.4646224MemoryTrain:  epoch  2, batch     0 | loss: 0.3434126MemoryTrain:  epoch  3, batch     0 | loss: 0.2287345MemoryTrain:  epoch  4, batch     0 | loss: 0.2038404MemoryTrain:  epoch  5, batch     0 | loss: 0.1373318MemoryTrain:  epoch  6, batch     0 | loss: 0.1143615MemoryTrain:  epoch  7, batch     0 | loss: 0.1104912MemoryTrain:  epoch  8, batch     0 | loss: 0.0828867MemoryTrain:  epoch  9, batch     0 | loss: 0.0613960
