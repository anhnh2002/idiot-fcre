#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 158.4854195CurrentTrain: epoch  0, batch     1 | loss: 106.6980182CurrentTrain: epoch  0, batch     2 | loss: 88.5967685CurrentTrain: epoch  0, batch     3 | loss: 100.7323387CurrentTrain: epoch  0, batch     4 | loss: 99.4227059CurrentTrain: epoch  0, batch     5 | loss: 100.3203441CurrentTrain: epoch  0, batch     6 | loss: 118.3823328CurrentTrain: epoch  0, batch     7 | loss: 117.5781373CurrentTrain: epoch  0, batch     8 | loss: 98.4441485CurrentTrain: epoch  0, batch     9 | loss: 99.1285312CurrentTrain: epoch  0, batch    10 | loss: 87.4957238CurrentTrain: epoch  0, batch    11 | loss: 116.9691454CurrentTrain: epoch  0, batch    12 | loss: 117.0467518CurrentTrain: epoch  0, batch    13 | loss: 284.3067925CurrentTrain: epoch  0, batch    14 | loss: 116.0577420CurrentTrain: epoch  0, batch    15 | loss: 97.2319536CurrentTrain: epoch  0, batch    16 | loss: 98.2210847CurrentTrain: epoch  0, batch    17 | loss: 142.0882891CurrentTrain: epoch  0, batch    18 | loss: 116.0119112CurrentTrain: epoch  0, batch    19 | loss: 96.7079898CurrentTrain: epoch  0, batch    20 | loss: 190.1970139CurrentTrain: epoch  0, batch    21 | loss: 189.0804039CurrentTrain: epoch  0, batch    22 | loss: 281.0375299CurrentTrain: epoch  0, batch    23 | loss: 188.9574946CurrentTrain: epoch  0, batch    24 | loss: 115.8995257CurrentTrain: epoch  0, batch    25 | loss: 281.3592263CurrentTrain: epoch  0, batch    26 | loss: 97.7702578CurrentTrain: epoch  0, batch    27 | loss: 145.3886601CurrentTrain: epoch  0, batch    28 | loss: 189.4998745CurrentTrain: epoch  0, batch    29 | loss: 117.6576014CurrentTrain: epoch  0, batch    30 | loss: 187.7025843CurrentTrain: epoch  0, batch    31 | loss: 189.0754932CurrentTrain: epoch  0, batch    32 | loss: 141.6259316CurrentTrain: epoch  0, batch    33 | loss: 96.7289729CurrentTrain: epoch  0, batch    34 | loss: 188.3139044CurrentTrain: epoch  0, batch    35 | loss: 96.5166647CurrentTrain: epoch  0, batch    36 | loss: 97.3524435CurrentTrain: epoch  0, batch    37 | loss: 114.8637721CurrentTrain: epoch  0, batch    38 | loss: 114.9046175CurrentTrain: epoch  0, batch    39 | loss: 96.0511838CurrentTrain: epoch  0, batch    40 | loss: 141.7316550CurrentTrain: epoch  0, batch    41 | loss: 96.0116703CurrentTrain: epoch  0, batch    42 | loss: 96.0352488CurrentTrain: epoch  0, batch    43 | loss: 84.2358469CurrentTrain: epoch  0, batch    44 | loss: 114.9404621CurrentTrain: epoch  0, batch    45 | loss: 96.6700333CurrentTrain: epoch  0, batch    46 | loss: 141.1340326CurrentTrain: epoch  0, batch    47 | loss: 96.0609413CurrentTrain: epoch  0, batch    48 | loss: 113.9875616CurrentTrain: epoch  0, batch    49 | loss: 113.7288863CurrentTrain: epoch  0, batch    50 | loss: 113.1583114CurrentTrain: epoch  0, batch    51 | loss: 114.5152761CurrentTrain: epoch  0, batch    52 | loss: 113.3722347CurrentTrain: epoch  0, batch    53 | loss: 187.8700588CurrentTrain: epoch  0, batch    54 | loss: 141.5526298CurrentTrain: epoch  0, batch    55 | loss: 96.1013081CurrentTrain: epoch  0, batch    56 | loss: 95.5363941CurrentTrain: epoch  0, batch    57 | loss: 114.4211856CurrentTrain: epoch  0, batch    58 | loss: 113.5004712CurrentTrain: epoch  0, batch    59 | loss: 112.6295436CurrentTrain: epoch  0, batch    60 | loss: 113.4277996CurrentTrain: epoch  0, batch    61 | loss: 112.9527579CurrentTrain: epoch  0, batch    62 | loss: 95.0708733CurrentTrain: epoch  0, batch    63 | loss: 140.5613717CurrentTrain: epoch  0, batch    64 | loss: 113.3192061CurrentTrain: epoch  0, batch    65 | loss: 81.4999514CurrentTrain: epoch  0, batch    66 | loss: 111.1210387CurrentTrain: epoch  0, batch    67 | loss: 112.6319721CurrentTrain: epoch  0, batch    68 | loss: 94.5710862CurrentTrain: epoch  0, batch    69 | loss: 187.3194039CurrentTrain: epoch  0, batch    70 | loss: 141.6871542CurrentTrain: epoch  0, batch    71 | loss: 94.2869330CurrentTrain: epoch  0, batch    72 | loss: 113.3938719CurrentTrain: epoch  0, batch    73 | loss: 138.5104041CurrentTrain: epoch  0, batch    74 | loss: 188.0572579CurrentTrain: epoch  0, batch    75 | loss: 113.3803633CurrentTrain: epoch  0, batch    76 | loss: 137.2447910CurrentTrain: epoch  0, batch    77 | loss: 110.5264717CurrentTrain: epoch  0, batch    78 | loss: 138.9804023CurrentTrain: epoch  0, batch    79 | loss: 138.4414945CurrentTrain: epoch  0, batch    80 | loss: 91.8967649CurrentTrain: epoch  0, batch    81 | loss: 111.6292301CurrentTrain: epoch  0, batch    82 | loss: 139.2891562CurrentTrain: epoch  0, batch    83 | loss: 111.1948441CurrentTrain: epoch  0, batch    84 | loss: 136.3299317CurrentTrain: epoch  0, batch    85 | loss: 94.1642148CurrentTrain: epoch  0, batch    86 | loss: 181.9590256CurrentTrain: epoch  0, batch    87 | loss: 91.6077042CurrentTrain: epoch  0, batch    88 | loss: 94.4290564CurrentTrain: epoch  0, batch    89 | loss: 80.1609463CurrentTrain: epoch  0, batch    90 | loss: 111.1510535CurrentTrain: epoch  0, batch    91 | loss: 92.3932919CurrentTrain: epoch  0, batch    92 | loss: 109.7158121CurrentTrain: epoch  0, batch    93 | loss: 180.7853617CurrentTrain: epoch  0, batch    94 | loss: 92.4973282CurrentTrain: epoch  0, batch    95 | loss: 117.9796469CurrentTrain: epoch  1, batch     0 | loss: 90.7853861CurrentTrain: epoch  1, batch     1 | loss: 109.7866849CurrentTrain: epoch  1, batch     2 | loss: 92.1558836CurrentTrain: epoch  1, batch     3 | loss: 91.5767654CurrentTrain: epoch  1, batch     4 | loss: 106.5714308CurrentTrain: epoch  1, batch     5 | loss: 88.0689805CurrentTrain: epoch  1, batch     6 | loss: 111.3649195CurrentTrain: epoch  1, batch     7 | loss: 180.4579092CurrentTrain: epoch  1, batch     8 | loss: 77.3266084CurrentTrain: epoch  1, batch     9 | loss: 136.4806374CurrentTrain: epoch  1, batch    10 | loss: 107.0799316CurrentTrain: epoch  1, batch    11 | loss: 88.9887848CurrentTrain: epoch  1, batch    12 | loss: 86.5299027CurrentTrain: epoch  1, batch    13 | loss: 136.5755844CurrentTrain: epoch  1, batch    14 | loss: 131.4855334CurrentTrain: epoch  1, batch    15 | loss: 109.5923647CurrentTrain: epoch  1, batch    16 | loss: 179.5432513CurrentTrain: epoch  1, batch    17 | loss: 87.8799957CurrentTrain: epoch  1, batch    18 | loss: 101.8961833CurrentTrain: epoch  1, batch    19 | loss: 136.2501531CurrentTrain: epoch  1, batch    20 | loss: 185.1064190CurrentTrain: epoch  1, batch    21 | loss: 86.5083844CurrentTrain: epoch  1, batch    22 | loss: 87.0736435CurrentTrain: epoch  1, batch    23 | loss: 106.0178745CurrentTrain: epoch  1, batch    24 | loss: 182.2112335CurrentTrain: epoch  1, batch    25 | loss: 108.2739395CurrentTrain: epoch  1, batch    26 | loss: 74.5037851CurrentTrain: epoch  1, batch    27 | loss: 86.9037414CurrentTrain: epoch  1, batch    28 | loss: 75.2374404CurrentTrain: epoch  1, batch    29 | loss: 78.2556216CurrentTrain: epoch  1, batch    30 | loss: 105.2908573CurrentTrain: epoch  1, batch    31 | loss: 106.9183082CurrentTrain: epoch  1, batch    32 | loss: 135.7104878CurrentTrain: epoch  1, batch    33 | loss: 106.1702539CurrentTrain: epoch  1, batch    34 | loss: 123.2873978CurrentTrain: epoch  1, batch    35 | loss: 182.3665594CurrentTrain: epoch  1, batch    36 | loss: 88.6987922CurrentTrain: epoch  1, batch    37 | loss: 104.6988753CurrentTrain: epoch  1, batch    38 | loss: 105.7403443CurrentTrain: epoch  1, batch    39 | loss: 75.4330754CurrentTrain: epoch  1, batch    40 | loss: 84.0055961CurrentTrain: epoch  1, batch    41 | loss: 85.4456216CurrentTrain: epoch  1, batch    42 | loss: 87.8610098CurrentTrain: epoch  1, batch    43 | loss: 105.7798679CurrentTrain: epoch  1, batch    44 | loss: 185.6800290CurrentTrain: epoch  1, batch    45 | loss: 107.0974710CurrentTrain: epoch  1, batch    46 | loss: 134.6182339CurrentTrain: epoch  1, batch    47 | loss: 84.2683189CurrentTrain: epoch  1, batch    48 | loss: 105.5800444CurrentTrain: epoch  1, batch    49 | loss: 103.9667291CurrentTrain: epoch  1, batch    50 | loss: 105.8747289CurrentTrain: epoch  1, batch    51 | loss: 137.6000625CurrentTrain: epoch  1, batch    52 | loss: 139.3076480CurrentTrain: epoch  1, batch    53 | loss: 179.3745991CurrentTrain: epoch  1, batch    54 | loss: 88.9485253CurrentTrain: epoch  1, batch    55 | loss: 105.5198813CurrentTrain: epoch  1, batch    56 | loss: 103.5511676CurrentTrain: epoch  1, batch    57 | loss: 130.9064780CurrentTrain: epoch  1, batch    58 | loss: 107.0998324CurrentTrain: epoch  1, batch    59 | loss: 85.3312623CurrentTrain: epoch  1, batch    60 | loss: 105.3446256CurrentTrain: epoch  1, batch    61 | loss: 178.6057304CurrentTrain: epoch  1, batch    62 | loss: 182.3907351CurrentTrain: epoch  1, batch    63 | loss: 87.0382983CurrentTrain: epoch  1, batch    64 | loss: 104.0478488CurrentTrain: epoch  1, batch    65 | loss: 132.4740974CurrentTrain: epoch  1, batch    66 | loss: 73.4151846CurrentTrain: epoch  1, batch    67 | loss: 180.5388019CurrentTrain: epoch  1, batch    68 | loss: 83.6084570CurrentTrain: epoch  1, batch    69 | loss: 91.3368528CurrentTrain: epoch  1, batch    70 | loss: 84.2878598CurrentTrain: epoch  1, batch    71 | loss: 106.2054776CurrentTrain: epoch  1, batch    72 | loss: 132.4723986CurrentTrain: epoch  1, batch    73 | loss: 129.9311015CurrentTrain: epoch  1, batch    74 | loss: 132.3751460CurrentTrain: epoch  1, batch    75 | loss: 103.0331122CurrentTrain: epoch  1, batch    76 | loss: 106.3323794CurrentTrain: epoch  1, batch    77 | loss: 112.1027081CurrentTrain: epoch  1, batch    78 | loss: 104.8558009CurrentTrain: epoch  1, batch    79 | loss: 102.0219742CurrentTrain: epoch  1, batch    80 | loss: 71.7384056CurrentTrain: epoch  1, batch    81 | loss: 132.5618428CurrentTrain: epoch  1, batch    82 | loss: 84.0190320CurrentTrain: epoch  1, batch    83 | loss: 279.8030834CurrentTrain: epoch  1, batch    84 | loss: 134.1228567CurrentTrain: epoch  1, batch    85 | loss: 85.3014347CurrentTrain: epoch  1, batch    86 | loss: 71.0187050CurrentTrain: epoch  1, batch    87 | loss: 84.6108271CurrentTrain: epoch  1, batch    88 | loss: 135.2278107CurrentTrain: epoch  1, batch    89 | loss: 108.6857459CurrentTrain: epoch  1, batch    90 | loss: 85.8972192CurrentTrain: epoch  1, batch    91 | loss: 105.2725078CurrentTrain: epoch  1, batch    92 | loss: 104.1647739CurrentTrain: epoch  1, batch    93 | loss: 181.7149189CurrentTrain: epoch  1, batch    94 | loss: 105.6578306CurrentTrain: epoch  1, batch    95 | loss: 81.9810524CurrentTrain: epoch  2, batch     0 | loss: 71.6277534CurrentTrain: epoch  2, batch     1 | loss: 135.9324531CurrentTrain: epoch  2, batch     2 | loss: 103.3102128CurrentTrain: epoch  2, batch     3 | loss: 104.4587998CurrentTrain: epoch  2, batch     4 | loss: 177.5042869CurrentTrain: epoch  2, batch     5 | loss: 83.2248329CurrentTrain: epoch  2, batch     6 | loss: 180.4577337CurrentTrain: epoch  2, batch     7 | loss: 100.4253289CurrentTrain: epoch  2, batch     8 | loss: 128.9084539CurrentTrain: epoch  2, batch     9 | loss: 70.7552400CurrentTrain: epoch  2, batch    10 | loss: 129.1760027CurrentTrain: epoch  2, batch    11 | loss: 70.4155787CurrentTrain: epoch  2, batch    12 | loss: 70.1636843CurrentTrain: epoch  2, batch    13 | loss: 133.4722126CurrentTrain: epoch  2, batch    14 | loss: 130.8533464CurrentTrain: epoch  2, batch    15 | loss: 134.7323635CurrentTrain: epoch  2, batch    16 | loss: 100.6550992CurrentTrain: epoch  2, batch    17 | loss: 101.2641323CurrentTrain: epoch  2, batch    18 | loss: 100.1839213CurrentTrain: epoch  2, batch    19 | loss: 130.4059044CurrentTrain: epoch  2, batch    20 | loss: 83.7383985CurrentTrain: epoch  2, batch    21 | loss: 127.6845462CurrentTrain: epoch  2, batch    22 | loss: 83.3626382CurrentTrain: epoch  2, batch    23 | loss: 101.3706147CurrentTrain: epoch  2, batch    24 | loss: 96.9376703CurrentTrain: epoch  2, batch    25 | loss: 97.5857276CurrentTrain: epoch  2, batch    26 | loss: 105.0963468CurrentTrain: epoch  2, batch    27 | loss: 98.7247109CurrentTrain: epoch  2, batch    28 | loss: 131.2837172CurrentTrain: epoch  2, batch    29 | loss: 131.3644049CurrentTrain: epoch  2, batch    30 | loss: 105.5312284CurrentTrain: epoch  2, batch    31 | loss: 98.9349052CurrentTrain: epoch  2, batch    32 | loss: 85.8669460CurrentTrain: epoch  2, batch    33 | loss: 143.0321616CurrentTrain: epoch  2, batch    34 | loss: 132.3344635CurrentTrain: epoch  2, batch    35 | loss: 130.7403118CurrentTrain: epoch  2, batch    36 | loss: 102.4033595CurrentTrain: epoch  2, batch    37 | loss: 124.9747776CurrentTrain: epoch  2, batch    38 | loss: 85.6775922CurrentTrain: epoch  2, batch    39 | loss: 129.6878199CurrentTrain: epoch  2, batch    40 | loss: 69.7438965CurrentTrain: epoch  2, batch    41 | loss: 131.1979940CurrentTrain: epoch  2, batch    42 | loss: 128.0641979CurrentTrain: epoch  2, batch    43 | loss: 85.0013319CurrentTrain: epoch  2, batch    44 | loss: 101.5306876CurrentTrain: epoch  2, batch    45 | loss: 87.4628856CurrentTrain: epoch  2, batch    46 | loss: 103.4000550CurrentTrain: epoch  2, batch    47 | loss: 84.9228982CurrentTrain: epoch  2, batch    48 | loss: 177.9317524CurrentTrain: epoch  2, batch    49 | loss: 181.2303238CurrentTrain: epoch  2, batch    50 | loss: 82.6272490CurrentTrain: epoch  2, batch    51 | loss: 184.4376618CurrentTrain: epoch  2, batch    52 | loss: 84.7300171CurrentTrain: epoch  2, batch    53 | loss: 104.2847332CurrentTrain: epoch  2, batch    54 | loss: 128.3577039CurrentTrain: epoch  2, batch    55 | loss: 101.6593513CurrentTrain: epoch  2, batch    56 | loss: 97.7445391CurrentTrain: epoch  2, batch    57 | loss: 80.3197609CurrentTrain: epoch  2, batch    58 | loss: 86.3065400CurrentTrain: epoch  2, batch    59 | loss: 87.5921934CurrentTrain: epoch  2, batch    60 | loss: 101.3842652CurrentTrain: epoch  2, batch    61 | loss: 79.4636029CurrentTrain: epoch  2, batch    62 | loss: 101.3077146CurrentTrain: epoch  2, batch    63 | loss: 82.7347324CurrentTrain: epoch  2, batch    64 | loss: 83.0884770CurrentTrain: epoch  2, batch    65 | loss: 101.5155057CurrentTrain: epoch  2, batch    66 | loss: 178.3073465CurrentTrain: epoch  2, batch    67 | loss: 133.8876620CurrentTrain: epoch  2, batch    68 | loss: 84.9820811CurrentTrain: epoch  2, batch    69 | loss: 79.9415357CurrentTrain: epoch  2, batch    70 | loss: 102.3230446CurrentTrain: epoch  2, batch    71 | loss: 82.2734754CurrentTrain: epoch  2, batch    72 | loss: 273.8914020CurrentTrain: epoch  2, batch    73 | loss: 105.9768369CurrentTrain: epoch  2, batch    74 | loss: 100.0522488CurrentTrain: epoch  2, batch    75 | loss: 101.5829554CurrentTrain: epoch  2, batch    76 | loss: 126.6148008CurrentTrain: epoch  2, batch    77 | loss: 69.4197528CurrentTrain: epoch  2, batch    78 | loss: 100.9987612CurrentTrain: epoch  2, batch    79 | loss: 101.6701347CurrentTrain: epoch  2, batch    80 | loss: 80.5304290CurrentTrain: epoch  2, batch    81 | loss: 101.1213691CurrentTrain: epoch  2, batch    82 | loss: 132.0026122CurrentTrain: epoch  2, batch    83 | loss: 132.9776328CurrentTrain: epoch  2, batch    84 | loss: 84.4816186CurrentTrain: epoch  2, batch    85 | loss: 174.6332856CurrentTrain: epoch  2, batch    86 | loss: 70.1575723CurrentTrain: epoch  2, batch    87 | loss: 70.8286047CurrentTrain: epoch  2, batch    88 | loss: 73.2601565CurrentTrain: epoch  2, batch    89 | loss: 125.9430975CurrentTrain: epoch  2, batch    90 | loss: 101.9617611CurrentTrain: epoch  2, batch    91 | loss: 88.7463061CurrentTrain: epoch  2, batch    92 | loss: 89.6291464CurrentTrain: epoch  2, batch    93 | loss: 105.0286112CurrentTrain: epoch  2, batch    94 | loss: 101.4859528CurrentTrain: epoch  2, batch    95 | loss: 83.3671267CurrentTrain: epoch  3, batch     0 | loss: 101.7970768CurrentTrain: epoch  3, batch     1 | loss: 127.4897062CurrentTrain: epoch  3, batch     2 | loss: 103.8864082CurrentTrain: epoch  3, batch     3 | loss: 101.8457025CurrentTrain: epoch  3, batch     4 | loss: 99.5129236CurrentTrain: epoch  3, batch     5 | loss: 127.6961479CurrentTrain: epoch  3, batch     6 | loss: 96.8180209CurrentTrain: epoch  3, batch     7 | loss: 71.4755229CurrentTrain: epoch  3, batch     8 | loss: 84.1189198CurrentTrain: epoch  3, batch     9 | loss: 171.9699376CurrentTrain: epoch  3, batch    10 | loss: 84.5037802CurrentTrain: epoch  3, batch    11 | loss: 97.3868703CurrentTrain: epoch  3, batch    12 | loss: 128.1576599CurrentTrain: epoch  3, batch    13 | loss: 126.7265665CurrentTrain: epoch  3, batch    14 | loss: 102.6678539CurrentTrain: epoch  3, batch    15 | loss: 129.2733986CurrentTrain: epoch  3, batch    16 | loss: 128.6814484CurrentTrain: epoch  3, batch    17 | loss: 80.1762454CurrentTrain: epoch  3, batch    18 | loss: 82.4817928CurrentTrain: epoch  3, batch    19 | loss: 82.7622543CurrentTrain: epoch  3, batch    20 | loss: 131.6362503CurrentTrain: epoch  3, batch    21 | loss: 98.2441856CurrentTrain: epoch  3, batch    22 | loss: 126.6456939CurrentTrain: epoch  3, batch    23 | loss: 100.4028718CurrentTrain: epoch  3, batch    24 | loss: 82.1470525CurrentTrain: epoch  3, batch    25 | loss: 102.5900791CurrentTrain: epoch  3, batch    26 | loss: 129.5750846CurrentTrain: epoch  3, batch    27 | loss: 134.7811709CurrentTrain: epoch  3, batch    28 | loss: 125.9909303CurrentTrain: epoch  3, batch    29 | loss: 83.2265872CurrentTrain: epoch  3, batch    30 | loss: 105.1834903CurrentTrain: epoch  3, batch    31 | loss: 95.9245303CurrentTrain: epoch  3, batch    32 | loss: 101.6988083CurrentTrain: epoch  3, batch    33 | loss: 173.8456646CurrentTrain: epoch  3, batch    34 | loss: 104.0423502CurrentTrain: epoch  3, batch    35 | loss: 171.4641331CurrentTrain: epoch  3, batch    36 | loss: 66.8663313CurrentTrain: epoch  3, batch    37 | loss: 174.6058018CurrentTrain: epoch  3, batch    38 | loss: 76.4549646CurrentTrain: epoch  3, batch    39 | loss: 101.3805692CurrentTrain: epoch  3, batch    40 | loss: 98.8874221CurrentTrain: epoch  3, batch    41 | loss: 96.5468529CurrentTrain: epoch  3, batch    42 | loss: 99.5599326CurrentTrain: epoch  3, batch    43 | loss: 78.9535215CurrentTrain: epoch  3, batch    44 | loss: 71.6416319CurrentTrain: epoch  3, batch    45 | loss: 84.0240428CurrentTrain: epoch  3, batch    46 | loss: 128.3816306CurrentTrain: epoch  3, batch    47 | loss: 101.7533956CurrentTrain: epoch  3, batch    48 | loss: 176.4444783CurrentTrain: epoch  3, batch    49 | loss: 71.3581223CurrentTrain: epoch  3, batch    50 | loss: 84.1517112CurrentTrain: epoch  3, batch    51 | loss: 83.3069838CurrentTrain: epoch  3, batch    52 | loss: 175.2604347CurrentTrain: epoch  3, batch    53 | loss: 82.5609480CurrentTrain: epoch  3, batch    54 | loss: 127.1464204CurrentTrain: epoch  3, batch    55 | loss: 97.7793879CurrentTrain: epoch  3, batch    56 | loss: 91.9401609CurrentTrain: epoch  3, batch    57 | loss: 85.8779704CurrentTrain: epoch  3, batch    58 | loss: 104.1121381CurrentTrain: epoch  3, batch    59 | loss: 170.7627224CurrentTrain: epoch  3, batch    60 | loss: 101.2913948CurrentTrain: epoch  3, batch    61 | loss: 83.1269164CurrentTrain: epoch  3, batch    62 | loss: 95.9834988CurrentTrain: epoch  3, batch    63 | loss: 125.2445633CurrentTrain: epoch  3, batch    64 | loss: 95.0448212CurrentTrain: epoch  3, batch    65 | loss: 128.7508674CurrentTrain: epoch  3, batch    66 | loss: 129.6457581CurrentTrain: epoch  3, batch    67 | loss: 82.0669367CurrentTrain: epoch  3, batch    68 | loss: 100.3811664CurrentTrain: epoch  3, batch    69 | loss: 80.9003675CurrentTrain: epoch  3, batch    70 | loss: 103.4343636CurrentTrain: epoch  3, batch    71 | loss: 67.9987399CurrentTrain: epoch  3, batch    72 | loss: 83.9304702CurrentTrain: epoch  3, batch    73 | loss: 66.4772154CurrentTrain: epoch  3, batch    74 | loss: 101.7305477CurrentTrain: epoch  3, batch    75 | loss: 129.0648552CurrentTrain: epoch  3, batch    76 | loss: 122.6491426CurrentTrain: epoch  3, batch    77 | loss: 104.0696319CurrentTrain: epoch  3, batch    78 | loss: 169.8801756CurrentTrain: epoch  3, batch    79 | loss: 96.3909645CurrentTrain: epoch  3, batch    80 | loss: 130.5067507CurrentTrain: epoch  3, batch    81 | loss: 67.7491375CurrentTrain: epoch  3, batch    82 | loss: 134.9998176CurrentTrain: epoch  3, batch    83 | loss: 99.5531044CurrentTrain: epoch  3, batch    84 | loss: 81.0968387CurrentTrain: epoch  3, batch    85 | loss: 130.9243522CurrentTrain: epoch  3, batch    86 | loss: 84.2783408CurrentTrain: epoch  3, batch    87 | loss: 103.3618339CurrentTrain: epoch  3, batch    88 | loss: 78.4625085CurrentTrain: epoch  3, batch    89 | loss: 79.7413933CurrentTrain: epoch  3, batch    90 | loss: 124.3231529CurrentTrain: epoch  3, batch    91 | loss: 128.2012022CurrentTrain: epoch  3, batch    92 | loss: 84.5956919CurrentTrain: epoch  3, batch    93 | loss: 103.3457764CurrentTrain: epoch  3, batch    94 | loss: 130.2512415CurrentTrain: epoch  3, batch    95 | loss: 62.8438828CurrentTrain: epoch  4, batch     0 | loss: 99.1842983CurrentTrain: epoch  4, batch     1 | loss: 130.0929281CurrentTrain: epoch  4, batch     2 | loss: 123.4864529CurrentTrain: epoch  4, batch     3 | loss: 93.1412090CurrentTrain: epoch  4, batch     4 | loss: 181.0420356CurrentTrain: epoch  4, batch     5 | loss: 123.7809468CurrentTrain: epoch  4, batch     6 | loss: 80.9291525CurrentTrain: epoch  4, batch     7 | loss: 76.5719901CurrentTrain: epoch  4, batch     8 | loss: 130.3539855CurrentTrain: epoch  4, batch     9 | loss: 176.0281645CurrentTrain: epoch  4, batch    10 | loss: 129.8975766CurrentTrain: epoch  4, batch    11 | loss: 82.1049338CurrentTrain: epoch  4, batch    12 | loss: 100.5216418CurrentTrain: epoch  4, batch    13 | loss: 82.9091811CurrentTrain: epoch  4, batch    14 | loss: 93.5105772CurrentTrain: epoch  4, batch    15 | loss: 127.4094189CurrentTrain: epoch  4, batch    16 | loss: 124.6002457CurrentTrain: epoch  4, batch    17 | loss: 176.7342706CurrentTrain: epoch  4, batch    18 | loss: 98.8971534CurrentTrain: epoch  4, batch    19 | loss: 97.2125019CurrentTrain: epoch  4, batch    20 | loss: 97.4213810CurrentTrain: epoch  4, batch    21 | loss: 84.3435171CurrentTrain: epoch  4, batch    22 | loss: 129.5180072CurrentTrain: epoch  4, batch    23 | loss: 94.2310186CurrentTrain: epoch  4, batch    24 | loss: 121.1371295CurrentTrain: epoch  4, batch    25 | loss: 125.5648273CurrentTrain: epoch  4, batch    26 | loss: 99.8850243CurrentTrain: epoch  4, batch    27 | loss: 70.2029857CurrentTrain: epoch  4, batch    28 | loss: 126.8132545CurrentTrain: epoch  4, batch    29 | loss: 96.1327013CurrentTrain: epoch  4, batch    30 | loss: 102.4531247CurrentTrain: epoch  4, batch    31 | loss: 81.1439361CurrentTrain: epoch  4, batch    32 | loss: 118.5506352CurrentTrain: epoch  4, batch    33 | loss: 65.5451073CurrentTrain: epoch  4, batch    34 | loss: 127.5175294CurrentTrain: epoch  4, batch    35 | loss: 102.6911132CurrentTrain: epoch  4, batch    36 | loss: 80.8766824CurrentTrain: epoch  4, batch    37 | loss: 70.7314949CurrentTrain: epoch  4, batch    38 | loss: 95.4893712CurrentTrain: epoch  4, batch    39 | loss: 83.6598424CurrentTrain: epoch  4, batch    40 | loss: 98.8623614CurrentTrain: epoch  4, batch    41 | loss: 81.9315339CurrentTrain: epoch  4, batch    42 | loss: 105.5136080CurrentTrain: epoch  4, batch    43 | loss: 66.3019642CurrentTrain: epoch  4, batch    44 | loss: 77.0242155CurrentTrain: epoch  4, batch    45 | loss: 102.5174822CurrentTrain: epoch  4, batch    46 | loss: 86.4386656CurrentTrain: epoch  4, batch    47 | loss: 123.3603041CurrentTrain: epoch  4, batch    48 | loss: 83.4803697CurrentTrain: epoch  4, batch    49 | loss: 101.1790875CurrentTrain: epoch  4, batch    50 | loss: 97.9688563CurrentTrain: epoch  4, batch    51 | loss: 82.5772636CurrentTrain: epoch  4, batch    52 | loss: 101.4487081CurrentTrain: epoch  4, batch    53 | loss: 171.2641632CurrentTrain: epoch  4, batch    54 | loss: 99.8307397CurrentTrain: epoch  4, batch    55 | loss: 128.5336226CurrentTrain: epoch  4, batch    56 | loss: 177.1178531CurrentTrain: epoch  4, batch    57 | loss: 81.1788682CurrentTrain: epoch  4, batch    58 | loss: 171.3331141CurrentTrain: epoch  4, batch    59 | loss: 129.4660238CurrentTrain: epoch  4, batch    60 | loss: 103.4680996CurrentTrain: epoch  4, batch    61 | loss: 68.7271336CurrentTrain: epoch  4, batch    62 | loss: 64.9797906CurrentTrain: epoch  4, batch    63 | loss: 79.6675168CurrentTrain: epoch  4, batch    64 | loss: 100.0282699CurrentTrain: epoch  4, batch    65 | loss: 175.2042245CurrentTrain: epoch  4, batch    66 | loss: 81.6993673CurrentTrain: epoch  4, batch    67 | loss: 79.4270551CurrentTrain: epoch  4, batch    68 | loss: 129.8115493CurrentTrain: epoch  4, batch    69 | loss: 78.4809385CurrentTrain: epoch  4, batch    70 | loss: 117.5282064CurrentTrain: epoch  4, batch    71 | loss: 130.1797087CurrentTrain: epoch  4, batch    72 | loss: 74.1293603CurrentTrain: epoch  4, batch    73 | loss: 85.9607268CurrentTrain: epoch  4, batch    74 | loss: 177.5390417CurrentTrain: epoch  4, batch    75 | loss: 98.2112643CurrentTrain: epoch  4, batch    76 | loss: 177.6749068CurrentTrain: epoch  4, batch    77 | loss: 123.4991198CurrentTrain: epoch  4, batch    78 | loss: 80.6830193CurrentTrain: epoch  4, batch    79 | loss: 123.0547256CurrentTrain: epoch  4, batch    80 | loss: 79.9321048CurrentTrain: epoch  4, batch    81 | loss: 177.0375778CurrentTrain: epoch  4, batch    82 | loss: 94.0973730CurrentTrain: epoch  4, batch    83 | loss: 78.8629311CurrentTrain: epoch  4, batch    84 | loss: 93.9509779CurrentTrain: epoch  4, batch    85 | loss: 97.3148607CurrentTrain: epoch  4, batch    86 | loss: 130.8888658CurrentTrain: epoch  4, batch    87 | loss: 175.3387212CurrentTrain: epoch  4, batch    88 | loss: 129.3643209CurrentTrain: epoch  4, batch    89 | loss: 86.5926004CurrentTrain: epoch  4, batch    90 | loss: 170.8457436CurrentTrain: epoch  4, batch    91 | loss: 121.3853472CurrentTrain: epoch  4, batch    92 | loss: 100.5616551CurrentTrain: epoch  4, batch    93 | loss: 97.0878105CurrentTrain: epoch  4, batch    94 | loss: 104.0614008CurrentTrain: epoch  4, batch    95 | loss: 68.7887707CurrentTrain: epoch  5, batch     0 | loss: 96.8377136CurrentTrain: epoch  5, batch     1 | loss: 95.4168515CurrentTrain: epoch  5, batch     2 | loss: 125.8395187CurrentTrain: epoch  5, batch     3 | loss: 98.8649963CurrentTrain: epoch  5, batch     4 | loss: 95.2421569CurrentTrain: epoch  5, batch     5 | loss: 79.5314349CurrentTrain: epoch  5, batch     6 | loss: 183.8204058CurrentTrain: epoch  5, batch     7 | loss: 95.0418783CurrentTrain: epoch  5, batch     8 | loss: 97.1443178CurrentTrain: epoch  5, batch     9 | loss: 95.4753024CurrentTrain: epoch  5, batch    10 | loss: 81.1209556CurrentTrain: epoch  5, batch    11 | loss: 127.7044449CurrentTrain: epoch  5, batch    12 | loss: 176.4695495CurrentTrain: epoch  5, batch    13 | loss: 79.5815493CurrentTrain: epoch  5, batch    14 | loss: 125.4440207CurrentTrain: epoch  5, batch    15 | loss: 79.7950646CurrentTrain: epoch  5, batch    16 | loss: 98.6348898CurrentTrain: epoch  5, batch    17 | loss: 75.7182372CurrentTrain: epoch  5, batch    18 | loss: 97.9374191CurrentTrain: epoch  5, batch    19 | loss: 124.4914972CurrentTrain: epoch  5, batch    20 | loss: 97.3101488CurrentTrain: epoch  5, batch    21 | loss: 80.4508744CurrentTrain: epoch  5, batch    22 | loss: 99.3982731CurrentTrain: epoch  5, batch    23 | loss: 123.0195274CurrentTrain: epoch  5, batch    24 | loss: 68.3045152CurrentTrain: epoch  5, batch    25 | loss: 99.0466234CurrentTrain: epoch  5, batch    26 | loss: 129.3204135CurrentTrain: epoch  5, batch    27 | loss: 121.9914115CurrentTrain: epoch  5, batch    28 | loss: 177.7406432CurrentTrain: epoch  5, batch    29 | loss: 559.0273479CurrentTrain: epoch  5, batch    30 | loss: 82.3515690CurrentTrain: epoch  5, batch    31 | loss: 266.4176100CurrentTrain: epoch  5, batch    32 | loss: 78.2785840CurrentTrain: epoch  5, batch    33 | loss: 90.4696148CurrentTrain: epoch  5, batch    34 | loss: 97.2362988CurrentTrain: epoch  5, batch    35 | loss: 77.6313922CurrentTrain: epoch  5, batch    36 | loss: 174.5887833CurrentTrain: epoch  5, batch    37 | loss: 96.4624007CurrentTrain: epoch  5, batch    38 | loss: 117.7775630CurrentTrain: epoch  5, batch    39 | loss: 130.1959705CurrentTrain: epoch  5, batch    40 | loss: 95.0451524CurrentTrain: epoch  5, batch    41 | loss: 129.4829569CurrentTrain: epoch  5, batch    42 | loss: 178.4094027CurrentTrain: epoch  5, batch    43 | loss: 80.5816145CurrentTrain: epoch  5, batch    44 | loss: 79.6132521CurrentTrain: epoch  5, batch    45 | loss: 98.4495736CurrentTrain: epoch  5, batch    46 | loss: 99.5718008CurrentTrain: epoch  5, batch    47 | loss: 78.9864473CurrentTrain: epoch  5, batch    48 | loss: 77.6579993CurrentTrain: epoch  5, batch    49 | loss: 80.6572569CurrentTrain: epoch  5, batch    50 | loss: 68.9927718CurrentTrain: epoch  5, batch    51 | loss: 93.9466159CurrentTrain: epoch  5, batch    52 | loss: 103.6396984CurrentTrain: epoch  5, batch    53 | loss: 92.4566714CurrentTrain: epoch  5, batch    54 | loss: 96.8006553CurrentTrain: epoch  5, batch    55 | loss: 128.7321868CurrentTrain: epoch  5, batch    56 | loss: 176.6403156CurrentTrain: epoch  5, batch    57 | loss: 100.8699456CurrentTrain: epoch  5, batch    58 | loss: 100.2677971CurrentTrain: epoch  5, batch    59 | loss: 173.7280323CurrentTrain: epoch  5, batch    60 | loss: 128.8029221CurrentTrain: epoch  5, batch    61 | loss: 99.3248918CurrentTrain: epoch  5, batch    62 | loss: 134.4874035CurrentTrain: epoch  5, batch    63 | loss: 96.4716912CurrentTrain: epoch  5, batch    64 | loss: 64.2409601CurrentTrain: epoch  5, batch    65 | loss: 95.1585311CurrentTrain: epoch  5, batch    66 | loss: 95.1419047CurrentTrain: epoch  5, batch    67 | loss: 95.9241083CurrentTrain: epoch  5, batch    68 | loss: 79.7210466CurrentTrain: epoch  5, batch    69 | loss: 101.0337034CurrentTrain: epoch  5, batch    70 | loss: 127.8320575CurrentTrain: epoch  5, batch    71 | loss: 129.2086611CurrentTrain: epoch  5, batch    72 | loss: 127.8658129CurrentTrain: epoch  5, batch    73 | loss: 95.7795069CurrentTrain: epoch  5, batch    74 | loss: 75.6826657CurrentTrain: epoch  5, batch    75 | loss: 77.8134505CurrentTrain: epoch  5, batch    76 | loss: 176.0610923CurrentTrain: epoch  5, batch    77 | loss: 99.6863227CurrentTrain: epoch  5, batch    78 | loss: 94.7198621CurrentTrain: epoch  5, batch    79 | loss: 98.3990779CurrentTrain: epoch  5, batch    80 | loss: 121.1599396CurrentTrain: epoch  5, batch    81 | loss: 126.6517125CurrentTrain: epoch  5, batch    82 | loss: 98.6677752CurrentTrain: epoch  5, batch    83 | loss: 127.8589942CurrentTrain: epoch  5, batch    84 | loss: 124.5925222CurrentTrain: epoch  5, batch    85 | loss: 98.8377933CurrentTrain: epoch  5, batch    86 | loss: 126.8159975CurrentTrain: epoch  5, batch    87 | loss: 79.8374150CurrentTrain: epoch  5, batch    88 | loss: 125.8991926CurrentTrain: epoch  5, batch    89 | loss: 99.1969943CurrentTrain: epoch  5, batch    90 | loss: 97.7608043CurrentTrain: epoch  5, batch    91 | loss: 101.8101261CurrentTrain: epoch  5, batch    92 | loss: 96.2017532CurrentTrain: epoch  5, batch    93 | loss: 78.4631401CurrentTrain: epoch  5, batch    94 | loss: 173.5077356CurrentTrain: epoch  5, batch    95 | loss: 68.9487536CurrentTrain: epoch  6, batch     0 | loss: 65.6617491CurrentTrain: epoch  6, batch     1 | loss: 98.7054723CurrentTrain: epoch  6, batch     2 | loss: 93.8359577CurrentTrain: epoch  6, batch     3 | loss: 128.6136686CurrentTrain: epoch  6, batch     4 | loss: 177.7514921CurrentTrain: epoch  6, batch     5 | loss: 272.5810129CurrentTrain: epoch  6, batch     6 | loss: 100.6557797CurrentTrain: epoch  6, batch     7 | loss: 102.4128770CurrentTrain: epoch  6, batch     8 | loss: 127.4996083CurrentTrain: epoch  6, batch     9 | loss: 101.5373388CurrentTrain: epoch  6, batch    10 | loss: 167.9752330CurrentTrain: epoch  6, batch    11 | loss: 100.6208170CurrentTrain: epoch  6, batch    12 | loss: 128.8209415CurrentTrain: epoch  6, batch    13 | loss: 125.5902204CurrentTrain: epoch  6, batch    14 | loss: 66.4035066CurrentTrain: epoch  6, batch    15 | loss: 96.5991689CurrentTrain: epoch  6, batch    16 | loss: 77.3607897CurrentTrain: epoch  6, batch    17 | loss: 97.6485979CurrentTrain: epoch  6, batch    18 | loss: 77.6510315CurrentTrain: epoch  6, batch    19 | loss: 77.2692908CurrentTrain: epoch  6, batch    20 | loss: 76.5391846CurrentTrain: epoch  6, batch    21 | loss: 99.4237280CurrentTrain: epoch  6, batch    22 | loss: 101.7116166CurrentTrain: epoch  6, batch    23 | loss: 124.3110108CurrentTrain: epoch  6, batch    24 | loss: 127.6825871CurrentTrain: epoch  6, batch    25 | loss: 76.6864422CurrentTrain: epoch  6, batch    26 | loss: 79.0305690CurrentTrain: epoch  6, batch    27 | loss: 76.6494787CurrentTrain: epoch  6, batch    28 | loss: 83.1236996CurrentTrain: epoch  6, batch    29 | loss: 121.6265457CurrentTrain: epoch  6, batch    30 | loss: 126.4965530CurrentTrain: epoch  6, batch    31 | loss: 63.8256039CurrentTrain: epoch  6, batch    32 | loss: 124.0568237CurrentTrain: epoch  6, batch    33 | loss: 100.9495593CurrentTrain: epoch  6, batch    34 | loss: 123.1802883CurrentTrain: epoch  6, batch    35 | loss: 77.7987528CurrentTrain: epoch  6, batch    36 | loss: 78.4022619CurrentTrain: epoch  6, batch    37 | loss: 82.7725577CurrentTrain: epoch  6, batch    38 | loss: 126.1653281CurrentTrain: epoch  6, batch    39 | loss: 95.7906225CurrentTrain: epoch  6, batch    40 | loss: 76.7870572CurrentTrain: epoch  6, batch    41 | loss: 123.6644919CurrentTrain: epoch  6, batch    42 | loss: 95.6313646CurrentTrain: epoch  6, batch    43 | loss: 67.0428596CurrentTrain: epoch  6, batch    44 | loss: 62.1084367CurrentTrain: epoch  6, batch    45 | loss: 100.1036736CurrentTrain: epoch  6, batch    46 | loss: 96.5661032CurrentTrain: epoch  6, batch    47 | loss: 176.2841417CurrentTrain: epoch  6, batch    48 | loss: 80.8746197CurrentTrain: epoch  6, batch    49 | loss: 79.7591223CurrentTrain: epoch  6, batch    50 | loss: 129.6652255CurrentTrain: epoch  6, batch    51 | loss: 124.4381766CurrentTrain: epoch  6, batch    52 | loss: 176.5573170CurrentTrain: epoch  6, batch    53 | loss: 131.5899015CurrentTrain: epoch  6, batch    54 | loss: 99.5575487CurrentTrain: epoch  6, batch    55 | loss: 98.2366941CurrentTrain: epoch  6, batch    56 | loss: 128.9541072CurrentTrain: epoch  6, batch    57 | loss: 72.7476994CurrentTrain: epoch  6, batch    58 | loss: 129.0083907CurrentTrain: epoch  6, batch    59 | loss: 66.1810710CurrentTrain: epoch  6, batch    60 | loss: 95.1152154CurrentTrain: epoch  6, batch    61 | loss: 100.7933283CurrentTrain: epoch  6, batch    62 | loss: 122.8568365CurrentTrain: epoch  6, batch    63 | loss: 98.1504372CurrentTrain: epoch  6, batch    64 | loss: 95.2022047CurrentTrain: epoch  6, batch    65 | loss: 126.8019934CurrentTrain: epoch  6, batch    66 | loss: 102.4307057CurrentTrain: epoch  6, batch    67 | loss: 98.0884787CurrentTrain: epoch  6, batch    68 | loss: 124.6095624CurrentTrain: epoch  6, batch    69 | loss: 95.1509779CurrentTrain: epoch  6, batch    70 | loss: 127.9405069CurrentTrain: epoch  6, batch    71 | loss: 65.7932487CurrentTrain: epoch  6, batch    72 | loss: 79.9065628CurrentTrain: epoch  6, batch    73 | loss: 74.5449153CurrentTrain: epoch  6, batch    74 | loss: 128.8136162CurrentTrain: epoch  6, batch    75 | loss: 126.3798101CurrentTrain: epoch  6, batch    76 | loss: 63.0930710CurrentTrain: epoch  6, batch    77 | loss: 66.6672076CurrentTrain: epoch  6, batch    78 | loss: 126.4807200CurrentTrain: epoch  6, batch    79 | loss: 130.0327731CurrentTrain: epoch  6, batch    80 | loss: 97.1706739CurrentTrain: epoch  6, batch    81 | loss: 68.9854212CurrentTrain: epoch  6, batch    82 | loss: 81.0056643CurrentTrain: epoch  6, batch    83 | loss: 120.0783178CurrentTrain: epoch  6, batch    84 | loss: 98.2383635CurrentTrain: epoch  6, batch    85 | loss: 66.4190892CurrentTrain: epoch  6, batch    86 | loss: 94.9206287CurrentTrain: epoch  6, batch    87 | loss: 125.0437944CurrentTrain: epoch  6, batch    88 | loss: 101.2079062CurrentTrain: epoch  6, batch    89 | loss: 132.4973605CurrentTrain: epoch  6, batch    90 | loss: 125.2244918CurrentTrain: epoch  6, batch    91 | loss: 64.8541621CurrentTrain: epoch  6, batch    92 | loss: 100.5998777CurrentTrain: epoch  6, batch    93 | loss: 94.9200179CurrentTrain: epoch  6, batch    94 | loss: 83.2371246CurrentTrain: epoch  6, batch    95 | loss: 139.9060641CurrentTrain: epoch  7, batch     0 | loss: 98.5143517CurrentTrain: epoch  7, batch     1 | loss: 124.6959375CurrentTrain: epoch  7, batch     2 | loss: 99.8301615CurrentTrain: epoch  7, batch     3 | loss: 99.2474624CurrentTrain: epoch  7, batch     4 | loss: 95.7870967CurrentTrain: epoch  7, batch     5 | loss: 124.6162937CurrentTrain: epoch  7, batch     6 | loss: 99.3961681CurrentTrain: epoch  7, batch     7 | loss: 95.9248077CurrentTrain: epoch  7, batch     8 | loss: 77.2260132CurrentTrain: epoch  7, batch     9 | loss: 76.5228332CurrentTrain: epoch  7, batch    10 | loss: 79.2730465CurrentTrain: epoch  7, batch    11 | loss: 79.1228755CurrentTrain: epoch  7, batch    12 | loss: 135.9661407CurrentTrain: epoch  7, batch    13 | loss: 98.1811197CurrentTrain: epoch  7, batch    14 | loss: 103.6671820CurrentTrain: epoch  7, batch    15 | loss: 82.2507688CurrentTrain: epoch  7, batch    16 | loss: 126.5258253CurrentTrain: epoch  7, batch    17 | loss: 89.3633187CurrentTrain: epoch  7, batch    18 | loss: 80.7498919CurrentTrain: epoch  7, batch    19 | loss: 76.1649127CurrentTrain: epoch  7, batch    20 | loss: 92.9104549CurrentTrain: epoch  7, batch    21 | loss: 93.7389158CurrentTrain: epoch  7, batch    22 | loss: 127.1992571CurrentTrain: epoch  7, batch    23 | loss: 124.5725252CurrentTrain: epoch  7, batch    24 | loss: 80.1640323CurrentTrain: epoch  7, batch    25 | loss: 65.0679281CurrentTrain: epoch  7, batch    26 | loss: 98.7824375CurrentTrain: epoch  7, batch    27 | loss: 129.1288330CurrentTrain: epoch  7, batch    28 | loss: 79.3666441CurrentTrain: epoch  7, batch    29 | loss: 131.5407075CurrentTrain: epoch  7, batch    30 | loss: 97.9001993CurrentTrain: epoch  7, batch    31 | loss: 100.5149018CurrentTrain: epoch  7, batch    32 | loss: 168.3902654CurrentTrain: epoch  7, batch    33 | loss: 98.8702418CurrentTrain: epoch  7, batch    34 | loss: 100.4780430CurrentTrain: epoch  7, batch    35 | loss: 129.5985290CurrentTrain: epoch  7, batch    36 | loss: 99.2472038CurrentTrain: epoch  7, batch    37 | loss: 101.6723525CurrentTrain: epoch  7, batch    38 | loss: 98.8745930CurrentTrain: epoch  7, batch    39 | loss: 128.4763949CurrentTrain: epoch  7, batch    40 | loss: 97.0031413CurrentTrain: epoch  7, batch    41 | loss: 75.2577071CurrentTrain: epoch  7, batch    42 | loss: 128.7319377CurrentTrain: epoch  7, batch    43 | loss: 81.5174183CurrentTrain: epoch  7, batch    44 | loss: 93.7437205CurrentTrain: epoch  7, batch    45 | loss: 127.2441094CurrentTrain: epoch  7, batch    46 | loss: 80.6963412CurrentTrain: epoch  7, batch    47 | loss: 90.6874459CurrentTrain: epoch  7, batch    48 | loss: 168.1138680CurrentTrain: epoch  7, batch    49 | loss: 67.6830758CurrentTrain: epoch  7, batch    50 | loss: 122.2448825CurrentTrain: epoch  7, batch    51 | loss: 92.4548809CurrentTrain: epoch  7, batch    52 | loss: 77.8464112CurrentTrain: epoch  7, batch    53 | loss: 125.0674260CurrentTrain: epoch  7, batch    54 | loss: 78.2659965CurrentTrain: epoch  7, batch    55 | loss: 128.8901976CurrentTrain: epoch  7, batch    56 | loss: 125.4696196CurrentTrain: epoch  7, batch    57 | loss: 97.7792641CurrentTrain: epoch  7, batch    58 | loss: 96.9295991CurrentTrain: epoch  7, batch    59 | loss: 176.6105145CurrentTrain: epoch  7, batch    60 | loss: 78.5415794CurrentTrain: epoch  7, batch    61 | loss: 79.7085747CurrentTrain: epoch  7, batch    62 | loss: 170.3581653CurrentTrain: epoch  7, batch    63 | loss: 100.7328215CurrentTrain: epoch  7, batch    64 | loss: 73.6634858CurrentTrain: epoch  7, batch    65 | loss: 96.3388736CurrentTrain: epoch  7, batch    66 | loss: 97.1669329CurrentTrain: epoch  7, batch    67 | loss: 101.0952655CurrentTrain: epoch  7, batch    68 | loss: 76.4427191CurrentTrain: epoch  7, batch    69 | loss: 98.7047213CurrentTrain: epoch  7, batch    70 | loss: 81.3518841CurrentTrain: epoch  7, batch    71 | loss: 130.2743148CurrentTrain: epoch  7, batch    72 | loss: 76.5492378CurrentTrain: epoch  7, batch    73 | loss: 96.4703547CurrentTrain: epoch  7, batch    74 | loss: 63.4646682CurrentTrain: epoch  7, batch    75 | loss: 75.3174346CurrentTrain: epoch  7, batch    76 | loss: 79.8652766CurrentTrain: epoch  7, batch    77 | loss: 128.0500830CurrentTrain: epoch  7, batch    78 | loss: 99.1945180CurrentTrain: epoch  7, batch    79 | loss: 176.7681479CurrentTrain: epoch  7, batch    80 | loss: 83.0062163CurrentTrain: epoch  7, batch    81 | loss: 76.8887850CurrentTrain: epoch  7, batch    82 | loss: 97.5069273CurrentTrain: epoch  7, batch    83 | loss: 74.3118074CurrentTrain: epoch  7, batch    84 | loss: 95.2354909CurrentTrain: epoch  7, batch    85 | loss: 98.8589549CurrentTrain: epoch  7, batch    86 | loss: 81.1051926CurrentTrain: epoch  7, batch    87 | loss: 124.5093083CurrentTrain: epoch  7, batch    88 | loss: 121.0236739CurrentTrain: epoch  7, batch    89 | loss: 170.6076411CurrentTrain: epoch  7, batch    90 | loss: 98.4837297CurrentTrain: epoch  7, batch    91 | loss: 96.0602173CurrentTrain: epoch  7, batch    92 | loss: 125.2210378CurrentTrain: epoch  7, batch    93 | loss: 67.1899746CurrentTrain: epoch  7, batch    94 | loss: 69.6444745CurrentTrain: epoch  7, batch    95 | loss: 105.1182134CurrentTrain: epoch  8, batch     0 | loss: 78.7635342CurrentTrain: epoch  8, batch     1 | loss: 77.6553920CurrentTrain: epoch  8, batch     2 | loss: 127.6836160CurrentTrain: epoch  8, batch     3 | loss: 176.2266442CurrentTrain: epoch  8, batch     4 | loss: 98.6735384CurrentTrain: epoch  8, batch     5 | loss: 81.6943277CurrentTrain: epoch  8, batch     6 | loss: 100.7173532CurrentTrain: epoch  8, batch     7 | loss: 95.2653718CurrentTrain: epoch  8, batch     8 | loss: 82.4197427CurrentTrain: epoch  8, batch     9 | loss: 130.2847457CurrentTrain: epoch  8, batch    10 | loss: 94.8033351CurrentTrain: epoch  8, batch    11 | loss: 95.2178741CurrentTrain: epoch  8, batch    12 | loss: 97.1261095CurrentTrain: epoch  8, batch    13 | loss: 121.1968648CurrentTrain: epoch  8, batch    14 | loss: 95.3225270CurrentTrain: epoch  8, batch    15 | loss: 98.8047856CurrentTrain: epoch  8, batch    16 | loss: 124.0665939CurrentTrain: epoch  8, batch    17 | loss: 96.9269460CurrentTrain: epoch  8, batch    18 | loss: 128.3128345CurrentTrain: epoch  8, batch    19 | loss: 79.6169326CurrentTrain: epoch  8, batch    20 | loss: 94.0095689CurrentTrain: epoch  8, batch    21 | loss: 93.9192304CurrentTrain: epoch  8, batch    22 | loss: 121.5611809CurrentTrain: epoch  8, batch    23 | loss: 80.0409080CurrentTrain: epoch  8, batch    24 | loss: 76.9443606CurrentTrain: epoch  8, batch    25 | loss: 124.2371898CurrentTrain: epoch  8, batch    26 | loss: 75.6115491CurrentTrain: epoch  8, batch    27 | loss: 95.8282042CurrentTrain: epoch  8, batch    28 | loss: 77.8594208CurrentTrain: epoch  8, batch    29 | loss: 78.8134638CurrentTrain: epoch  8, batch    30 | loss: 129.8557477CurrentTrain: epoch  8, batch    31 | loss: 63.1133945CurrentTrain: epoch  8, batch    32 | loss: 266.4099088CurrentTrain: epoch  8, batch    33 | loss: 126.1916988CurrentTrain: epoch  8, batch    34 | loss: 124.3304859CurrentTrain: epoch  8, batch    35 | loss: 75.7866081CurrentTrain: epoch  8, batch    36 | loss: 120.2986748CurrentTrain: epoch  8, batch    37 | loss: 80.7916151CurrentTrain: epoch  8, batch    38 | loss: 94.9938897CurrentTrain: epoch  8, batch    39 | loss: 100.5251782CurrentTrain: epoch  8, batch    40 | loss: 99.6215014CurrentTrain: epoch  8, batch    41 | loss: 129.3990903CurrentTrain: epoch  8, batch    42 | loss: 98.2034837CurrentTrain: epoch  8, batch    43 | loss: 128.8986678CurrentTrain: epoch  8, batch    44 | loss: 173.8871133CurrentTrain: epoch  8, batch    45 | loss: 98.4733923CurrentTrain: epoch  8, batch    46 | loss: 76.9838487CurrentTrain: epoch  8, batch    47 | loss: 98.3335723CurrentTrain: epoch  8, batch    48 | loss: 76.7950346CurrentTrain: epoch  8, batch    49 | loss: 95.5904234CurrentTrain: epoch  8, batch    50 | loss: 124.5001117CurrentTrain: epoch  8, batch    51 | loss: 76.8615230CurrentTrain: epoch  8, batch    52 | loss: 98.2914246CurrentTrain: epoch  8, batch    53 | loss: 100.6416307CurrentTrain: epoch  8, batch    54 | loss: 124.0284848CurrentTrain: epoch  8, batch    55 | loss: 122.2327323CurrentTrain: epoch  8, batch    56 | loss: 78.5716981CurrentTrain: epoch  8, batch    57 | loss: 77.9434672CurrentTrain: epoch  8, batch    58 | loss: 96.6519789CurrentTrain: epoch  8, batch    59 | loss: 62.3835809CurrentTrain: epoch  8, batch    60 | loss: 173.6010293CurrentTrain: epoch  8, batch    61 | loss: 126.5073405CurrentTrain: epoch  8, batch    62 | loss: 65.0602476CurrentTrain: epoch  8, batch    63 | loss: 100.0530005CurrentTrain: epoch  8, batch    64 | loss: 97.1637840CurrentTrain: epoch  8, batch    65 | loss: 95.4642717CurrentTrain: epoch  8, batch    66 | loss: 90.7206873CurrentTrain: epoch  8, batch    67 | loss: 121.0495952CurrentTrain: epoch  8, batch    68 | loss: 122.5725682CurrentTrain: epoch  8, batch    69 | loss: 101.6462029CurrentTrain: epoch  8, batch    70 | loss: 123.6055132CurrentTrain: epoch  8, batch    71 | loss: 168.9274613CurrentTrain: epoch  8, batch    72 | loss: 99.1565011CurrentTrain: epoch  8, batch    73 | loss: 94.5368560CurrentTrain: epoch  8, batch    74 | loss: 126.3278284CurrentTrain: epoch  8, batch    75 | loss: 97.2058268CurrentTrain: epoch  8, batch    76 | loss: 64.6940265CurrentTrain: epoch  8, batch    77 | loss: 170.0915334CurrentTrain: epoch  8, batch    78 | loss: 75.3283634CurrentTrain: epoch  8, batch    79 | loss: 81.5064260CurrentTrain: epoch  8, batch    80 | loss: 122.6370296CurrentTrain: epoch  8, batch    81 | loss: 100.6401893CurrentTrain: epoch  8, batch    82 | loss: 81.5707217CurrentTrain: epoch  8, batch    83 | loss: 78.6985143CurrentTrain: epoch  8, batch    84 | loss: 174.5015688CurrentTrain: epoch  8, batch    85 | loss: 166.9227669CurrentTrain: epoch  8, batch    86 | loss: 126.3305522CurrentTrain: epoch  8, batch    87 | loss: 97.5666093CurrentTrain: epoch  8, batch    88 | loss: 59.1188402CurrentTrain: epoch  8, batch    89 | loss: 95.4110170CurrentTrain: epoch  8, batch    90 | loss: 95.1988775CurrentTrain: epoch  8, batch    91 | loss: 105.7314902CurrentTrain: epoch  8, batch    92 | loss: 96.8904378CurrentTrain: epoch  8, batch    93 | loss: 96.5672736CurrentTrain: epoch  8, batch    94 | loss: 122.5479887CurrentTrain: epoch  8, batch    95 | loss: 144.4461952CurrentTrain: epoch  9, batch     0 | loss: 79.7095806CurrentTrain: epoch  9, batch     1 | loss: 166.7764005CurrentTrain: epoch  9, batch     2 | loss: 95.7376063CurrentTrain: epoch  9, batch     3 | loss: 80.1094440CurrentTrain: epoch  9, batch     4 | loss: 98.0572774CurrentTrain: epoch  9, batch     5 | loss: 61.5820020CurrentTrain: epoch  9, batch     6 | loss: 123.9849689CurrentTrain: epoch  9, batch     7 | loss: 95.2151201CurrentTrain: epoch  9, batch     8 | loss: 123.6290697CurrentTrain: epoch  9, batch     9 | loss: 77.4820501CurrentTrain: epoch  9, batch    10 | loss: 131.3944394CurrentTrain: epoch  9, batch    11 | loss: 79.9464601CurrentTrain: epoch  9, batch    12 | loss: 132.7934031CurrentTrain: epoch  9, batch    13 | loss: 126.4019512CurrentTrain: epoch  9, batch    14 | loss: 123.6240210CurrentTrain: epoch  9, batch    15 | loss: 93.4813925CurrentTrain: epoch  9, batch    16 | loss: 176.3591163CurrentTrain: epoch  9, batch    17 | loss: 98.7230859CurrentTrain: epoch  9, batch    18 | loss: 96.3632928CurrentTrain: epoch  9, batch    19 | loss: 104.3919562CurrentTrain: epoch  9, batch    20 | loss: 75.6207783CurrentTrain: epoch  9, batch    21 | loss: 126.5575934CurrentTrain: epoch  9, batch    22 | loss: 80.0009401CurrentTrain: epoch  9, batch    23 | loss: 80.0181030CurrentTrain: epoch  9, batch    24 | loss: 172.8146226CurrentTrain: epoch  9, batch    25 | loss: 124.3190097CurrentTrain: epoch  9, batch    26 | loss: 72.3013460CurrentTrain: epoch  9, batch    27 | loss: 166.7194184CurrentTrain: epoch  9, batch    28 | loss: 77.4798433CurrentTrain: epoch  9, batch    29 | loss: 79.1687931CurrentTrain: epoch  9, batch    30 | loss: 77.3224967CurrentTrain: epoch  9, batch    31 | loss: 95.9080827CurrentTrain: epoch  9, batch    32 | loss: 126.3384168CurrentTrain: epoch  9, batch    33 | loss: 81.4554471CurrentTrain: epoch  9, batch    34 | loss: 67.8557907CurrentTrain: epoch  9, batch    35 | loss: 170.2518966CurrentTrain: epoch  9, batch    36 | loss: 94.7197746CurrentTrain: epoch  9, batch    37 | loss: 98.5313470CurrentTrain: epoch  9, batch    38 | loss: 96.4144149CurrentTrain: epoch  9, batch    39 | loss: 79.5645663CurrentTrain: epoch  9, batch    40 | loss: 75.4968565CurrentTrain: epoch  9, batch    41 | loss: 101.1175060CurrentTrain: epoch  9, batch    42 | loss: 75.1397204CurrentTrain: epoch  9, batch    43 | loss: 76.3575262CurrentTrain: epoch  9, batch    44 | loss: 170.2935673CurrentTrain: epoch  9, batch    45 | loss: 81.8216218CurrentTrain: epoch  9, batch    46 | loss: 98.4323227CurrentTrain: epoch  9, batch    47 | loss: 126.6897991CurrentTrain: epoch  9, batch    48 | loss: 94.6920964CurrentTrain: epoch  9, batch    49 | loss: 80.0691713CurrentTrain: epoch  9, batch    50 | loss: 93.1507794CurrentTrain: epoch  9, batch    51 | loss: 99.8646862CurrentTrain: epoch  9, batch    52 | loss: 80.8424114CurrentTrain: epoch  9, batch    53 | loss: 98.7342600CurrentTrain: epoch  9, batch    54 | loss: 78.8679705CurrentTrain: epoch  9, batch    55 | loss: 78.5341559CurrentTrain: epoch  9, batch    56 | loss: 98.4389305CurrentTrain: epoch  9, batch    57 | loss: 62.0232585CurrentTrain: epoch  9, batch    58 | loss: 119.7200714CurrentTrain: epoch  9, batch    59 | loss: 126.5783506CurrentTrain: epoch  9, batch    60 | loss: 91.8905864CurrentTrain: epoch  9, batch    61 | loss: 64.7176777CurrentTrain: epoch  9, batch    62 | loss: 124.7872530CurrentTrain: epoch  9, batch    63 | loss: 74.1818794CurrentTrain: epoch  9, batch    64 | loss: 67.6446921CurrentTrain: epoch  9, batch    65 | loss: 126.4554116CurrentTrain: epoch  9, batch    66 | loss: 124.3558549CurrentTrain: epoch  9, batch    67 | loss: 95.6363279CurrentTrain: epoch  9, batch    68 | loss: 176.2183332CurrentTrain: epoch  9, batch    69 | loss: 127.1811414CurrentTrain: epoch  9, batch    70 | loss: 96.9746411CurrentTrain: epoch  9, batch    71 | loss: 124.4059629CurrentTrain: epoch  9, batch    72 | loss: 76.2706201CurrentTrain: epoch  9, batch    73 | loss: 127.0537761CurrentTrain: epoch  9, batch    74 | loss: 176.3754471CurrentTrain: epoch  9, batch    75 | loss: 128.8388984CurrentTrain: epoch  9, batch    76 | loss: 123.7949167CurrentTrain: epoch  9, batch    77 | loss: 78.2155476CurrentTrain: epoch  9, batch    78 | loss: 169.4577660CurrentTrain: epoch  9, batch    79 | loss: 95.4905132CurrentTrain: epoch  9, batch    80 | loss: 65.6517323CurrentTrain: epoch  9, batch    81 | loss: 62.7566343CurrentTrain: epoch  9, batch    82 | loss: 129.1931906CurrentTrain: epoch  9, batch    83 | loss: 176.3867253CurrentTrain: epoch  9, batch    84 | loss: 176.2874730CurrentTrain: epoch  9, batch    85 | loss: 78.4511458CurrentTrain: epoch  9, batch    86 | loss: 95.4077288CurrentTrain: epoch  9, batch    87 | loss: 78.8206868CurrentTrain: epoch  9, batch    88 | loss: 176.1796514CurrentTrain: epoch  9, batch    89 | loss: 65.5477441CurrentTrain: epoch  9, batch    90 | loss: 96.8672983CurrentTrain: epoch  9, batch    91 | loss: 95.7701397CurrentTrain: epoch  9, batch    92 | loss: 61.2805128CurrentTrain: epoch  9, batch    93 | loss: 79.5972242CurrentTrain: epoch  9, batch    94 | loss: 119.5978700CurrentTrain: epoch  9, batch    95 | loss: 78.3116690

F1 score per class: {32: 0.6931818181818182, 6: 0.9010989010989011, 19: 0.3333333333333333, 24: 0.7853403141361257, 26: 0.9361702127659575, 29: 0.9081632653061225}
Micro-average F1 score: 0.8338557993730408
Weighted-average F1 score: 0.8419238014831607
F1 score per class: {32: 0.7157894736842105, 6: 0.925531914893617, 19: 0.5882352941176471, 24: 0.7628865979381443, 26: 0.9637305699481865, 29: 0.8911917098445595}
Micro-average F1 score: 0.842741935483871
Weighted-average F1 score: 0.8428243419304018
F1 score per class: {32: 0.7195767195767195, 6: 0.925531914893617, 19: 0.5882352941176471, 24: 0.7628865979381443, 26: 0.9690721649484536, 29: 0.8911917098445595}
Micro-average F1 score: 0.844758064516129
Weighted-average F1 score: 0.8451178354129386

F1 score per class: {32: 0.6931818181818182, 6: 0.9010989010989011, 19: 0.3333333333333333, 24: 0.7853403141361257, 26: 0.9361702127659575, 29: 0.9081632653061225}
Micro-average F1 score: 0.8338557993730408
Weighted-average F1 score: 0.8419238014831607
F1 score per class: {32: 0.7157894736842105, 6: 0.925531914893617, 19: 0.5882352941176471, 24: 0.7628865979381443, 26: 0.9637305699481865, 29: 0.8911917098445595}
Micro-average F1 score: 0.842741935483871
Weighted-average F1 score: 0.8428243419304018
F1 score per class: {32: 0.7195767195767195, 6: 0.925531914893617, 19: 0.5882352941176471, 24: 0.7628865979381443, 26: 0.9690721649484536, 29: 0.8911917098445595}
Micro-average F1 score: 0.844758064516129
Weighted-average F1 score: 0.8451178354129386

F1 score per class: {32: 0.5041322314049587, 6: 0.7068965517241379, 19: 0.19047619047619047, 24: 0.7389162561576355, 26: 0.8979591836734694, 29: 0.7911111111111111}
Micro-average F1 score: 0.7
Weighted-average F1 score: 0.6923953298461372
F1 score per class: {32: 0.4610169491525424, 6: 0.6541353383458647, 19: 0.19230769230769232, 24: 0.6915887850467289, 26: 0.8985507246376812, 29: 0.6935483870967742}
Micro-average F1 score: 0.6266866566716641
Weighted-average F1 score: 0.6037431900864
F1 score per class: {32: 0.46258503401360546, 6: 0.6541353383458647, 19: 0.19801980198019803, 24: 0.6915887850467289, 26: 0.9038461538461539, 29: 0.6963562753036437}
Micro-average F1 score: 0.6300751879699248
Weighted-average F1 score: 0.607862666143789

F1 score per class: {32: 0.5041322314049587, 6: 0.7068965517241379, 19: 0.19047619047619047, 24: 0.7389162561576355, 26: 0.8979591836734694, 29: 0.7911111111111111}
Micro-average F1 score: 0.7
Weighted-average F1 score: 0.6923953298461372
F1 score per class: {32: 0.4610169491525424, 6: 0.6541353383458647, 19: 0.19230769230769232, 24: 0.6915887850467289, 26: 0.8985507246376812, 29: 0.6935483870967742}
Micro-average F1 score: 0.6266866566716641
Weighted-average F1 score: 0.6037431900864
F1 score per class: {32: 0.46258503401360546, 6: 0.6541353383458647, 19: 0.19801980198019803, 24: 0.6915887850467289, 26: 0.9038461538461539, 29: 0.6963562753036437}
Micro-average F1 score: 0.6300751879699248
Weighted-average F1 score: 0.607862666143789
cur_acc_wo_na:  ['0.8339']
his_acc_wo_na:  ['0.8339']
cur_acc des_wo_na:  ['0.8427']
his_acc des_wo_na:  ['0.8427']
cur_acc rrf_wo_na:  ['0.8448']
his_acc rrf_wo_na:  ['0.8448']
cur_acc_w_na:  ['0.7000']
his_acc_w_na:  ['0.7000']
cur_acc des_w_na:  ['0.6267']
his_acc des_w_na:  ['0.6267']
cur_acc rrf_w_na:  ['0.6301']
his_acc rrf_w_na:  ['0.6301']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 145.6895725CurrentTrain: epoch  0, batch     1 | loss: 126.5570572CurrentTrain: epoch  0, batch     2 | loss: 146.0355469CurrentTrain: epoch  0, batch     3 | loss: 109.5498961CurrentTrain: epoch  0, batch     4 | loss: 33.9723407CurrentTrain: epoch  1, batch     0 | loss: 89.9486157CurrentTrain: epoch  1, batch     1 | loss: 184.2607264CurrentTrain: epoch  1, batch     2 | loss: 139.5790274CurrentTrain: epoch  1, batch     3 | loss: 136.8044561CurrentTrain: epoch  1, batch     4 | loss: 28.5888349CurrentTrain: epoch  2, batch     0 | loss: 136.2797194CurrentTrain: epoch  2, batch     1 | loss: 107.5894910CurrentTrain: epoch  2, batch     2 | loss: 133.5367316CurrentTrain: epoch  2, batch     3 | loss: 105.3666890CurrentTrain: epoch  2, batch     4 | loss: 43.8349679CurrentTrain: epoch  3, batch     0 | loss: 173.1597499CurrentTrain: epoch  3, batch     1 | loss: 91.1020554CurrentTrain: epoch  3, batch     2 | loss: 107.8489314CurrentTrain: epoch  3, batch     3 | loss: 104.3348437CurrentTrain: epoch  3, batch     4 | loss: 25.7510971CurrentTrain: epoch  4, batch     0 | loss: 101.7913010CurrentTrain: epoch  4, batch     1 | loss: 105.1885937CurrentTrain: epoch  4, batch     2 | loss: 102.9298400CurrentTrain: epoch  4, batch     3 | loss: 135.3488785CurrentTrain: epoch  4, batch     4 | loss: 42.2399902CurrentTrain: epoch  5, batch     0 | loss: 104.1868516CurrentTrain: epoch  5, batch     1 | loss: 134.6748547CurrentTrain: epoch  5, batch     2 | loss: 83.6295505CurrentTrain: epoch  5, batch     3 | loss: 130.3094427CurrentTrain: epoch  5, batch     4 | loss: 16.1584092CurrentTrain: epoch  6, batch     0 | loss: 103.1664297CurrentTrain: epoch  6, batch     1 | loss: 100.9572794CurrentTrain: epoch  6, batch     2 | loss: 98.0917595CurrentTrain: epoch  6, batch     3 | loss: 179.7087974CurrentTrain: epoch  6, batch     4 | loss: 23.1295089CurrentTrain: epoch  7, batch     0 | loss: 98.5786738CurrentTrain: epoch  7, batch     1 | loss: 128.3819107CurrentTrain: epoch  7, batch     2 | loss: 100.5118847CurrentTrain: epoch  7, batch     3 | loss: 124.6075671CurrentTrain: epoch  7, batch     4 | loss: 40.0332433CurrentTrain: epoch  8, batch     0 | loss: 171.1623253CurrentTrain: epoch  8, batch     1 | loss: 119.6110911CurrentTrain: epoch  8, batch     2 | loss: 130.4499664CurrentTrain: epoch  8, batch     3 | loss: 99.9516000CurrentTrain: epoch  8, batch     4 | loss: 22.1114404CurrentTrain: epoch  9, batch     0 | loss: 123.5578803CurrentTrain: epoch  9, batch     1 | loss: 79.9833156CurrentTrain: epoch  9, batch     2 | loss: 101.4455382CurrentTrain: epoch  9, batch     3 | loss: 173.1643625CurrentTrain: epoch  9, batch     4 | loss: 19.7203615
MemoryTrain:  epoch  0, batch     0 | loss: 1.5441639MemoryTrain:  epoch  1, batch     0 | loss: 1.3357749MemoryTrain:  epoch  2, batch     0 | loss: 0.9137009MemoryTrain:  epoch  3, batch     0 | loss: 0.7448269MemoryTrain:  epoch  4, batch     0 | loss: 0.5757605MemoryTrain:  epoch  5, batch     0 | loss: 0.4570450MemoryTrain:  epoch  6, batch     0 | loss: 0.4183510MemoryTrain:  epoch  7, batch     0 | loss: 0.2879114MemoryTrain:  epoch  8, batch     0 | loss: 0.2739755MemoryTrain:  epoch  9, batch     0 | loss: 0.1991267

F1 score per class: {2: 0.875, 39: 0.5573770491803278, 11: 0.4126984126984127, 12: 0.0, 19: 0.375, 28: 0.0}
Micro-average F1 score: 0.4666666666666667
Weighted-average F1 score: 0.47908615311468983
F1 score per class: {2: 0.9411764705882353, 6: 0.0, 39: 0.7638888888888888, 11: 0.7421383647798742, 12: 0.0, 19: 0.0, 24: 0.0, 26: 0.6153846153846154, 28: 0.6666666666666666}
Micro-average F1 score: 0.7243243243243244
Weighted-average F1 score: 0.6876618729012316
F1 score per class: {2: 0.9411764705882353, 39: 0.7638888888888888, 11: 0.7577639751552795, 12: 0.0, 19: 0.0, 24: 0.5333333333333333, 28: 0.6086956521739131}
Micro-average F1 score: 0.7336956521739131
Weighted-average F1 score: 0.7096804444956661

F1 score per class: {32: 0.875, 2: 0.6628571428571428, 6: 0.544, 39: 0.40310077519379844, 11: 0.8415300546448088, 12: 0.4, 19: 0.7724867724867724, 24: 0.25, 26: 0.9361702127659575, 28: 0.9183673469387755, 29: 0.0}
Micro-average F1 score: 0.7294303797468354
Weighted-average F1 score: 0.765847759983443
F1 score per class: {32: 0.9411764705882353, 2: 0.7839195979899497, 6: 0.7534246575342466, 39: 0.7239263803680982, 11: 0.8888888888888888, 12: 0.6, 19: 0.7684210526315789, 24: 0.47058823529411764, 26: 0.9743589743589743, 28: 0.9183673469387755, 29: 0.5925925925925926}
Micro-average F1 score: 0.8229317851959361
Weighted-average F1 score: 0.827736061786921
F1 score per class: {32: 0.9411764705882353, 2: 0.7407407407407407, 6: 0.7482993197278912, 39: 0.7134502923976608, 11: 0.9025641025641026, 12: 0.38461538461538464, 19: 0.7724867724867724, 24: 0.3333333333333333, 26: 0.9637305699481865, 28: 0.9183673469387755, 29: 0.5384615384615384}
Micro-average F1 score: 0.8069919883466861
Weighted-average F1 score: 0.8104039056490919

F1 score per class: {32: 0.45161290322580644, 2: 0.0, 6: 0.44155844155844154, 39: 0.36879432624113473, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.1016949152542373, 26: 0.0, 28: 0.0, 29: 0.0}
Micro-average F1 score: 0.3023758099352052
Weighted-average F1 score: 0.2433825781462193
F1 score per class: {32: 0.22857142857142856, 2: 0.0, 6: 0.49327354260089684, 39: 0.5339366515837104, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.15384615384615385, 26: 0.0, 28: 0.0, 29: 0.22857142857142856}
Micro-average F1 score: 0.360699865410498
Weighted-average F1 score: 0.31433273092526326
F1 score per class: {32: 0.2962962962962963, 2: 0.0, 6: 0.49107142857142855, 39: 0.5570776255707762, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.1038961038961039, 26: 0.0, 28: 0.0, 29: 0.2545454545454545}
Micro-average F1 score: 0.38028169014084506
Weighted-average F1 score: 0.3340457838435128

F1 score per class: {32: 0.3684210526315789, 2: 0.45136186770428016, 6: 0.36363636363636365, 39: 0.27225130890052357, 11: 0.6581196581196581, 12: 0.20408163265306123, 19: 0.7121951219512195, 24: 0.06060606060606061, 26: 0.8712871287128713, 28: 0.7317073170731707, 29: 0.0}
Micro-average F1 score: 0.5326400924321202
Weighted-average F1 score: 0.5159292277741849
F1 score per class: {32: 0.1509433962264151, 2: 0.4273972602739726, 6: 0.39568345323741005, 39: 0.2789598108747045, 11: 0.624113475177305, 12: 0.1978021978021978, 19: 0.6854460093896714, 24: 0.10526315789473684, 26: 0.8796296296296297, 28: 0.6642066420664207, 29: 0.15841584158415842}
Micro-average F1 score: 0.4682080924855491
Weighted-average F1 score: 0.43633573661847125
F1 score per class: {32: 0.20512820512820512, 2: 0.4444444444444444, 6: 0.38596491228070173, 39: 0.28045977011494255, 11: 0.6423357664233577, 12: 0.16129032258064516, 19: 0.7053140096618358, 24: 0.06722689075630252, 26: 0.8899521531100478, 28: 0.6766917293233082, 29: 0.16091954022988506}
Micro-average F1 score: 0.47411210954214805
Weighted-average F1 score: 0.439109520043519
cur_acc_wo_na:  ['0.8339', '0.4667']
his_acc_wo_na:  ['0.8339', '0.7294']
cur_acc des_wo_na:  ['0.8427', '0.7243']
his_acc des_wo_na:  ['0.8427', '0.8229']
cur_acc rrf_wo_na:  ['0.8448', '0.7337']
his_acc rrf_wo_na:  ['0.8448', '0.8070']
cur_acc_w_na:  ['0.7000', '0.3024']
his_acc_w_na:  ['0.7000', '0.5326']
cur_acc des_w_na:  ['0.6267', '0.3607']
his_acc des_w_na:  ['0.6267', '0.4682']
cur_acc rrf_w_na:  ['0.6301', '0.3803']
his_acc rrf_w_na:  ['0.6301', '0.4741']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 160.8583967CurrentTrain: epoch  0, batch     1 | loss: 139.1757482CurrentTrain: epoch  0, batch     2 | loss: 94.0486767CurrentTrain: epoch  0, batch     3 | loss: 41.7329177CurrentTrain: epoch  1, batch     0 | loss: 113.0955742CurrentTrain: epoch  1, batch     1 | loss: 93.0494923CurrentTrain: epoch  1, batch     2 | loss: 88.1499621CurrentTrain: epoch  1, batch     3 | loss: 41.4282114CurrentTrain: epoch  2, batch     0 | loss: 135.7936617CurrentTrain: epoch  2, batch     1 | loss: 82.8680828CurrentTrain: epoch  2, batch     2 | loss: 133.9109616CurrentTrain: epoch  2, batch     3 | loss: 9.3161682CurrentTrain: epoch  3, batch     0 | loss: 83.9655603CurrentTrain: epoch  3, batch     1 | loss: 81.9350435CurrentTrain: epoch  3, batch     2 | loss: 101.5921138CurrentTrain: epoch  3, batch     3 | loss: 19.0585045CurrentTrain: epoch  4, batch     0 | loss: 80.9122495CurrentTrain: epoch  4, batch     1 | loss: 102.6466177CurrentTrain: epoch  4, batch     2 | loss: 83.6459612CurrentTrain: epoch  4, batch     3 | loss: 17.4865565CurrentTrain: epoch  5, batch     0 | loss: 172.5434929CurrentTrain: epoch  5, batch     1 | loss: 98.5813658CurrentTrain: epoch  5, batch     2 | loss: 77.1192419CurrentTrain: epoch  5, batch     3 | loss: 41.3089343CurrentTrain: epoch  6, batch     0 | loss: 174.4881788CurrentTrain: epoch  6, batch     1 | loss: 76.9945661CurrentTrain: epoch  6, batch     2 | loss: 79.0047185CurrentTrain: epoch  6, batch     3 | loss: 9.4188201CurrentTrain: epoch  7, batch     0 | loss: 77.6295352CurrentTrain: epoch  7, batch     1 | loss: 121.7689497CurrentTrain: epoch  7, batch     2 | loss: 96.6569086CurrentTrain: epoch  7, batch     3 | loss: 16.6989203CurrentTrain: epoch  8, batch     0 | loss: 77.3794124CurrentTrain: epoch  8, batch     1 | loss: 120.0559882CurrentTrain: epoch  8, batch     2 | loss: 94.8894542CurrentTrain: epoch  8, batch     3 | loss: 41.1209521CurrentTrain: epoch  9, batch     0 | loss: 93.0752845CurrentTrain: epoch  9, batch     1 | loss: 121.7345893CurrentTrain: epoch  9, batch     2 | loss: 93.6875817CurrentTrain: epoch  9, batch     3 | loss: 16.7469076
MemoryTrain:  epoch  0, batch     0 | loss: 1.0086153MemoryTrain:  epoch  1, batch     0 | loss: 0.9586441MemoryTrain:  epoch  2, batch     0 | loss: 0.7205502MemoryTrain:  epoch  3, batch     0 | loss: 0.5852718MemoryTrain:  epoch  4, batch     0 | loss: 0.4644317MemoryTrain:  epoch  5, batch     0 | loss: 0.3779864MemoryTrain:  epoch  6, batch     0 | loss: 0.3087693MemoryTrain:  epoch  7, batch     0 | loss: 0.2357019MemoryTrain:  epoch  8, batch     0 | loss: 0.2002895MemoryTrain:  epoch  9, batch     0 | loss: 0.1653827

F1 score per class: {7: 0.75, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6, 27: 0.0, 31: 0.5319148936170213}
Micro-average F1 score: 0.5514018691588785
Weighted-average F1 score: 0.44789591155611175
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9615384615384616, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6666666666666666, 27: 0.0, 28: 0.6666666666666666, 31: 0.5773195876288659}
Micro-average F1 score: 0.5662100456621004
Weighted-average F1 score: 0.45786287023400424
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9615384615384616, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6666666666666666, 27: 0.0, 28: 0.6666666666666666, 31: 0.5773195876288659}
Micro-average F1 score: 0.5662100456621004
Weighted-average F1 score: 0.45786287023400424

F1 score per class: {32: 0.8, 2: 0.2608695652173913, 6: 0.08450704225352113, 7: 0.9615384615384616, 40: 0.6906474820143885, 9: 0.3140495867768595, 11: 0.6948356807511737, 12: 0.19047619047619047, 39: 0.7777777777777778, 19: 0.4444444444444444, 24: 0.375, 26: 0.9361702127659575, 27: 0.0, 28: 0.8783068783068783, 29: 0.13333333333333333, 31: 0.45454545454545453}
Micro-average F1 score: 0.6350067842605156
Weighted-average F1 score: 0.6603810745260924
F1 score per class: {32: 0.875, 2: 0.4444444444444444, 6: 0.029411764705882353, 7: 0.9615384615384616, 39: 0.8053691275167785, 40: 0.6951219512195121, 11: 0.7181818181818181, 12: 0.4, 9: 0.7650273224043715, 19: 0.5384615384615384, 24: 0.5, 26: 0.9528795811518325, 27: 0.4, 28: 0.8911917098445595, 29: 0.6086956521739131, 31: 0.4745762711864407}
Micro-average F1 score: 0.7040302267002518
Weighted-average F1 score: 0.6887759154127917
F1 score per class: {32: 0.875, 2: 0.4, 6: 0.028985507246376812, 7: 0.9433962264150944, 39: 0.8, 40: 0.6451612903225806, 9: 0.7129629629629629, 12: 0.2608695652173913, 11: 0.7692307692307693, 19: 0.5, 24: 0.45454545454545453, 26: 0.9473684210526315, 27: 0.5, 28: 0.8911917098445595, 29: 0.5, 31: 0.448}
Micro-average F1 score: 0.6865482233502538
Weighted-average F1 score: 0.6747092957591014

F1 score per class: {32: 0.0, 2: 0.0, 6: 0.6, 7: 0.8333333333333334, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 19: 0.41379310344827586, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.3448275862068966}
Micro-average F1 score: 0.3782051282051282
Weighted-average F1 score: 0.3257866016486706
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.25, 7: 0.6944444444444444, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.4666666666666667, 26: 0.0, 27: 0.0, 28: 0.16666666666666666, 29: 0.0, 31: 0.27586206896551724}
Micro-average F1 score: 0.2973621103117506
Weighted-average F1 score: 0.2624944678375884
F1 score per class: {32: 0.0, 2: 0.0, 6: 0.2222222222222222, 7: 0.684931506849315, 40: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 19: 0.42424242424242425, 26: 0.0, 27: 0.0, 28: 0.16666666666666666, 29: 0.0, 31: 0.27450980392156865}
Micro-average F1 score: 0.30024213075060535
Weighted-average F1 score: 0.2694498936309538

F1 score per class: {32: 0.3076923076923077, 2: 0.21739130434782608, 6: 0.043478260869565216, 7: 0.7692307692307693, 40: 0.42857142857142855, 9: 0.2585034013605442, 11: 0.5441176470588235, 12: 0.125, 39: 0.7142857142857143, 19: 0.16901408450704225, 24: 0.09836065573770492, 26: 0.88, 27: 0.0, 28: 0.7942583732057417, 29: 0.1, 31: 0.2564102564102564}
Micro-average F1 score: 0.4642857142857143
Weighted-average F1 score: 0.43808882609946787
F1 score per class: {32: 0.15730337078651685, 2: 0.32608695652173914, 6: 0.012195121951219513, 7: 0.5319148936170213, 39: 0.41379310344827586, 40: 0.32294617563739375, 11: 0.5266666666666666, 12: 0.15873015873015872, 9: 0.6829268292682927, 19: 0.18666666666666668, 24: 0.10204081632653061, 26: 0.8666666666666667, 27: 0.038461538461538464, 28: 0.6907630522088354, 29: 0.17073170731707318, 31: 0.17391304347826086}
Micro-average F1 score: 0.3950530035335689
Weighted-average F1 score: 0.35587705357676075
F1 score per class: {32: 0.19444444444444445, 2: 0.3023255813953488, 6: 0.012195121951219513, 7: 0.5, 39: 0.40540540540540543, 40: 0.3546099290780142, 9: 0.5202702702702703, 12: 0.13636363636363635, 11: 0.7, 19: 0.16091954022988506, 24: 0.08695652173913043, 26: 0.8695652173913043, 27: 0.04081632653061224, 28: 0.7049180327868853, 29: 0.18518518518518517, 31: 0.16816816816816818}
Micro-average F1 score: 0.3985267034990792
Weighted-average F1 score: 0.3578286659280309
cur_acc_wo_na:  ['0.8339', '0.4667', '0.5514']
his_acc_wo_na:  ['0.8339', '0.7294', '0.6350']
cur_acc des_wo_na:  ['0.8427', '0.7243', '0.5662']
his_acc des_wo_na:  ['0.8427', '0.8229', '0.7040']
cur_acc rrf_wo_na:  ['0.8448', '0.7337', '0.5662']
his_acc rrf_wo_na:  ['0.8448', '0.8070', '0.6865']
cur_acc_w_na:  ['0.7000', '0.3024', '0.3782']
his_acc_w_na:  ['0.7000', '0.5326', '0.4643']
cur_acc des_w_na:  ['0.6267', '0.3607', '0.2974']
his_acc des_w_na:  ['0.6267', '0.4682', '0.3951']
cur_acc rrf_w_na:  ['0.6301', '0.3803', '0.3002']
his_acc rrf_w_na:  ['0.6301', '0.4741', '0.3985']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 92.7176640CurrentTrain: epoch  0, batch     1 | loss: 196.0562581CurrentTrain: epoch  0, batch     2 | loss: 122.3376504CurrentTrain: epoch  0, batch     3 | loss: 146.6650848CurrentTrain: epoch  1, batch     0 | loss: 117.4256236CurrentTrain: epoch  1, batch     1 | loss: 88.1226318CurrentTrain: epoch  1, batch     2 | loss: 94.5356938CurrentTrain: epoch  1, batch     3 | loss: 68.0806396CurrentTrain: epoch  2, batch     0 | loss: 88.7070915CurrentTrain: epoch  2, batch     1 | loss: 175.4400610CurrentTrain: epoch  2, batch     2 | loss: 91.5626399CurrentTrain: epoch  2, batch     3 | loss: 86.2745952CurrentTrain: epoch  3, batch     0 | loss: 84.7382236CurrentTrain: epoch  3, batch     1 | loss: 98.2582432CurrentTrain: epoch  3, batch     2 | loss: 133.4718923CurrentTrain: epoch  3, batch     3 | loss: 183.1867126CurrentTrain: epoch  4, batch     0 | loss: 81.0549928CurrentTrain: epoch  4, batch     1 | loss: 128.5639756CurrentTrain: epoch  4, batch     2 | loss: 99.4893440CurrentTrain: epoch  4, batch     3 | loss: 91.9456683CurrentTrain: epoch  5, batch     0 | loss: 100.4360235CurrentTrain: epoch  5, batch     1 | loss: 81.9495461CurrentTrain: epoch  5, batch     2 | loss: 80.6507903CurrentTrain: epoch  5, batch     3 | loss: 118.4847782CurrentTrain: epoch  6, batch     0 | loss: 125.6134129CurrentTrain: epoch  6, batch     1 | loss: 98.8734348CurrentTrain: epoch  6, batch     2 | loss: 100.1243303CurrentTrain: epoch  6, batch     3 | loss: 67.8558476CurrentTrain: epoch  7, batch     0 | loss: 96.8592215CurrentTrain: epoch  7, batch     1 | loss: 125.4525548CurrentTrain: epoch  7, batch     2 | loss: 173.8968483CurrentTrain: epoch  7, batch     3 | loss: 61.6841938CurrentTrain: epoch  8, batch     0 | loss: 128.6977169CurrentTrain: epoch  8, batch     1 | loss: 96.2136503CurrentTrain: epoch  8, batch     2 | loss: 122.8897725CurrentTrain: epoch  8, batch     3 | loss: 50.5884906CurrentTrain: epoch  9, batch     0 | loss: 175.2044505CurrentTrain: epoch  9, batch     1 | loss: 115.6542472CurrentTrain: epoch  9, batch     2 | loss: 96.8791786CurrentTrain: epoch  9, batch     3 | loss: 83.9528481
MemoryTrain:  epoch  0, batch     0 | loss: 0.8557423MemoryTrain:  epoch  1, batch     0 | loss: 0.7219710MemoryTrain:  epoch  2, batch     0 | loss: 0.5768820MemoryTrain:  epoch  3, batch     0 | loss: 0.4928448MemoryTrain:  epoch  4, batch     0 | loss: 0.3648188MemoryTrain:  epoch  5, batch     0 | loss: 0.2809282MemoryTrain:  epoch  6, batch     0 | loss: 0.2096327MemoryTrain:  epoch  7, batch     0 | loss: 0.1642644MemoryTrain:  epoch  8, batch     0 | loss: 0.1541611MemoryTrain:  epoch  9, batch     0 | loss: 0.1194671

F1 score per class: {35: 0.0, 37: 0.8888888888888888, 38: 0.44776119402985076, 11: 0.0, 15: 0.0, 25: 0.8260869565217391, 26: 0.5714285714285714, 27: 0.25}
Micro-average F1 score: 0.5874587458745875
Weighted-average F1 score: 0.6118762740513323
F1 score per class: {35: 0.0, 37: 0.0, 38: 0.8235294117647058, 39: 0.8314606741573034, 11: 0.0, 12: 0.0, 15: 0.0, 25: 0.9702970297029703, 26: 0.5714285714285714, 27: 0.8085106382978723, 28: 0.0}
Micro-average F1 score: 0.7431693989071039
Weighted-average F1 score: 0.6916338797167695
F1 score per class: {35: 0.0, 37: 0.8235294117647058, 38: 0.8045977011494253, 39: 0.0, 11: 0.0, 15: 0.0, 25: 0.9702970297029703, 26: 0.5714285714285714, 27: 0.782608695652174, 28: 0.0}
Micro-average F1 score: 0.7450980392156863
Weighted-average F1 score: 0.7066776868189499

F1 score per class: {2: 0.8, 6: 0.3050847457627119, 7: 0.06451612903225806, 9: 0.9803921568627451, 11: 0.40336134453781514, 12: 0.1981981981981982, 15: 0.8, 19: 0.5668449197860963, 24: 0.2, 25: 0.44776119402985076, 26: 0.7528089887640449, 27: 0.4666666666666667, 28: 0.7692307692307693, 29: 0.9247311827956989, 31: 0.6666666666666666, 32: 0.8066298342541437, 35: 0.7916666666666666, 37: 0.4485981308411215, 38: 0.25, 39: 0.0, 40: 0.42424242424242425}
Micro-average F1 score: 0.5734347571679345
Weighted-average F1 score: 0.615654242098386
F1 score per class: {2: 0.875, 6: 0.5106382978723404, 7: 0.0625, 9: 0.9803921568627451, 11: 0.6708860759493671, 12: 0.6705202312138728, 15: 0.6666666666666666, 19: 0.6868686868686869, 24: 0.32, 25: 0.8314606741573034, 26: 0.75, 27: 0.5, 28: 0.5882352941176471, 29: 0.9417989417989417, 31: 0.5, 32: 0.8923076923076924, 35: 0.9158878504672897, 37: 0.4528301886792453, 38: 0.7169811320754716, 39: 0.45454545454545453, 40: 0.5354330708661418}
Micro-average F1 score: 0.6967545638945233
Weighted-average F1 score: 0.6863845560661409
F1 score per class: {2: 0.875, 6: 0.5, 7: 0.0625, 9: 0.9803921568627451, 11: 0.6405228758169934, 12: 0.675, 15: 0.6363636363636364, 19: 0.68, 24: 0.34782608695652173, 25: 0.8045977011494253, 26: 0.7540983606557377, 27: 0.5, 28: 0.5263157894736842, 29: 0.9417989417989417, 31: 0.5, 32: 0.8865979381443299, 35: 0.8990825688073395, 37: 0.43636363636363634, 38: 0.6923076923076923, 39: 0.125, 40: 0.528}
Micro-average F1 score: 0.6865059004617753
Weighted-average F1 score: 0.6788840685727436

F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.7619047619047619, 19: 0.0, 25: 0.4411764705882353, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6229508196721312, 37: 0.48484848484848486, 38: 0.22857142857142856, 40: 0.0}
Micro-average F1 score: 0.3920704845814978
Weighted-average F1 score: 0.3169105731864678
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 15: 0.4827586206896552, 19: 0.0, 24: 0.0, 25: 0.74, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6447368421052632, 37: 0.42857142857142855, 38: 0.4691358024691358, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.34782608695652173
Weighted-average F1 score: 0.27015619046869793
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 15: 0.4827586206896552, 19: 0.0, 25: 0.7142857142857143, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6405228758169934, 37: 0.41025641025641024, 38: 0.46153846153846156, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.36289222373806274
Weighted-average F1 score: 0.28914366249082346

F1 score per class: {2: 0.3076923076923077, 6: 0.24161073825503357, 7: 0.032520325203252036, 9: 0.8333333333333334, 11: 0.2436548223350254, 12: 0.17054263565891473, 15: 0.48484848484848486, 19: 0.44537815126050423, 24: 0.15384615384615385, 25: 0.4411764705882353, 26: 0.694300518134715, 27: 0.19718309859154928, 28: 0.23809523809523808, 29: 0.8269230769230769, 31: 0.14285714285714285, 32: 0.7192118226600985, 35: 0.37623762376237624, 37: 0.2376237623762376, 38: 0.12698412698412698, 39: 0.0, 40: 0.30656934306569344}
Micro-average F1 score: 0.4057971014492754
Weighted-average F1 score: 0.3895837258139557
F1 score per class: {2: 0.175, 6: 0.32286995515695066, 7: 0.025974025974025976, 9: 0.4132231404958678, 11: 0.2888283378746594, 12: 0.26303854875283444, 15: 0.2153846153846154, 19: 0.4689655172413793, 24: 0.1568627450980392, 25: 0.7326732673267327, 26: 0.6571428571428571, 27: 0.1523809523809524, 28: 0.07042253521126761, 29: 0.8127853881278538, 31: 0.044444444444444446, 32: 0.6615969581749049, 35: 0.29253731343283584, 37: 0.16161616161616163, 38: 0.18009478672985782, 39: 0.18867924528301888, 40: 0.22742474916387959}
Micro-average F1 score: 0.337426326129666
Weighted-average F1 score: 0.30596671719167895
F1 score per class: {2: 0.24561403508771928, 6: 0.33653846153846156, 7: 0.026143790849673203, 9: 0.5434782608695652, 11: 0.2890855457227139, 12: 0.3112391930835735, 15: 0.208955223880597, 19: 0.4738675958188153, 24: 0.24242424242424243, 25: 0.7070707070707071, 26: 0.6699029126213593, 27: 0.13445378151260504, 28: 0.06944444444444445, 29: 0.8165137614678899, 31: 0.05128205128205128, 32: 0.6825396825396826, 35: 0.28, 37: 0.1476923076923077, 38: 0.18652849740932642, 39: 0.06060606060606061, 40: 0.2268041237113402}
Micro-average F1 score: 0.34735202492211836
Weighted-average F1 score: 0.31313080192107795
cur_acc_wo_na:  ['0.8339', '0.4667', '0.5514', '0.5875']
his_acc_wo_na:  ['0.8339', '0.7294', '0.6350', '0.5734']
cur_acc des_wo_na:  ['0.8427', '0.7243', '0.5662', '0.7432']
his_acc des_wo_na:  ['0.8427', '0.8229', '0.7040', '0.6968']
cur_acc rrf_wo_na:  ['0.8448', '0.7337', '0.5662', '0.7451']
his_acc rrf_wo_na:  ['0.8448', '0.8070', '0.6865', '0.6865']
cur_acc_w_na:  ['0.7000', '0.3024', '0.3782', '0.3921']
his_acc_w_na:  ['0.7000', '0.5326', '0.4643', '0.4058']
cur_acc des_w_na:  ['0.6267', '0.3607', '0.2974', '0.3478']
his_acc des_w_na:  ['0.6267', '0.4682', '0.3951', '0.3374']
cur_acc rrf_w_na:  ['0.6301', '0.3803', '0.3002', '0.3629']
his_acc rrf_w_na:  ['0.6301', '0.4741', '0.3985', '0.3474']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 119.1006540CurrentTrain: epoch  0, batch     1 | loss: 190.8043546CurrentTrain: epoch  0, batch     2 | loss: 113.9234525CurrentTrain: epoch  0, batch     3 | loss: 99.4116455CurrentTrain: epoch  0, batch     4 | loss: 323.5923668CurrentTrain: epoch  1, batch     0 | loss: 134.8897759CurrentTrain: epoch  1, batch     1 | loss: 108.5500605CurrentTrain: epoch  1, batch     2 | loss: 139.4064138CurrentTrain: epoch  1, batch     3 | loss: 111.1757649CurrentTrain: epoch  1, batch     4 | loss: 165.9379822CurrentTrain: epoch  2, batch     0 | loss: 179.7837808CurrentTrain: epoch  2, batch     1 | loss: 108.4845045CurrentTrain: epoch  2, batch     2 | loss: 133.1914125CurrentTrain: epoch  2, batch     3 | loss: 129.1708439CurrentTrain: epoch  2, batch     4 | loss: 98.7326825CurrentTrain: epoch  3, batch     0 | loss: 82.8726609CurrentTrain: epoch  3, batch     1 | loss: 105.7292223CurrentTrain: epoch  3, batch     2 | loss: 178.0479519CurrentTrain: epoch  3, batch     3 | loss: 131.7641086CurrentTrain: epoch  3, batch     4 | loss: 158.4577943CurrentTrain: epoch  4, batch     0 | loss: 174.5539410CurrentTrain: epoch  4, batch     1 | loss: 101.0043156CurrentTrain: epoch  4, batch     2 | loss: 102.2413448CurrentTrain: epoch  4, batch     3 | loss: 129.9591156CurrentTrain: epoch  4, batch     4 | loss: 97.2650129CurrentTrain: epoch  5, batch     0 | loss: 98.8395546CurrentTrain: epoch  5, batch     1 | loss: 126.5129628CurrentTrain: epoch  5, batch     2 | loss: 84.2181867CurrentTrain: epoch  5, batch     3 | loss: 131.3316926CurrentTrain: epoch  5, batch     4 | loss: 155.0996999CurrentTrain: epoch  6, batch     0 | loss: 98.7299850CurrentTrain: epoch  6, batch     1 | loss: 101.6176592CurrentTrain: epoch  6, batch     2 | loss: 81.3198247CurrentTrain: epoch  6, batch     3 | loss: 100.9438442CurrentTrain: epoch  6, batch     4 | loss: 150.2913890CurrentTrain: epoch  7, batch     0 | loss: 76.6034795CurrentTrain: epoch  7, batch     1 | loss: 128.9832474CurrentTrain: epoch  7, batch     2 | loss: 98.8622082CurrentTrain: epoch  7, batch     3 | loss: 130.6693220CurrentTrain: epoch  7, batch     4 | loss: 320.9169802CurrentTrain: epoch  8, batch     0 | loss: 97.9016275CurrentTrain: epoch  8, batch     1 | loss: 130.8079418CurrentTrain: epoch  8, batch     2 | loss: 99.7468548CurrentTrain: epoch  8, batch     3 | loss: 97.8023841CurrentTrain: epoch  8, batch     4 | loss: 96.3601589CurrentTrain: epoch  9, batch     0 | loss: 177.1623720CurrentTrain: epoch  9, batch     1 | loss: 78.7184240CurrentTrain: epoch  9, batch     2 | loss: 124.5069961CurrentTrain: epoch  9, batch     3 | loss: 124.8762999CurrentTrain: epoch  9, batch     4 | loss: 95.6914763
MemoryTrain:  epoch  0, batch     0 | loss: 1.2013807MemoryTrain:  epoch  1, batch     0 | loss: 1.0603323MemoryTrain:  epoch  2, batch     0 | loss: 0.8256031MemoryTrain:  epoch  3, batch     0 | loss: 0.6878022MemoryTrain:  epoch  4, batch     0 | loss: 0.6757198MemoryTrain:  epoch  5, batch     0 | loss: 0.5553908MemoryTrain:  epoch  6, batch     0 | loss: 0.4381713MemoryTrain:  epoch  7, batch     0 | loss: 0.3313568MemoryTrain:  epoch  8, batch     0 | loss: 0.2779099MemoryTrain:  epoch  9, batch     0 | loss: 0.2138926

F1 score per class: {1: 0.31932773109243695, 34: 0.48148148148148145, 3: 0.0, 35: 0.08571428571428572, 37: 0.8102564102564103, 40: 0.0, 11: 0.0, 14: 0.6075949367088608, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.48165869218500795
Weighted-average F1 score: 0.45320581018146905
F1 score per class: {32: 0.4032258064516129, 1: 0.5862068965517241, 34: 0.22988505747126436, 3: 0.8108108108108109, 35: 0.0, 37: 0.0, 38: 0.0, 14: 0.0, 22: 0.9423076923076923, 24: 0.0, 26: 0.0, 27: 0.0}
Micro-average F1 score: 0.5718518518518518
Weighted-average F1 score: 0.5211693411977865
F1 score per class: {32: 0.4032258064516129, 1: 0.5739130434782609, 34: 0.0, 3: 0.23255813953488372, 35: 0.8172043010752689, 37: 0.0, 38: 0.0, 40: 0.0, 11: 0.0, 14: 0.9108910891089109, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.0}
Micro-average F1 score: 0.5637982195845698
Weighted-average F1 score: 0.5104506051094618

F1 score per class: {1: 0.2835820895522388, 2: 0.8, 3: 0.43333333333333335, 6: 0.25862068965517243, 7: 0.038461538461538464, 9: 0.9803921568627451, 11: 0.24299065420560748, 12: 0.23008849557522124, 14: 0.08333333333333333, 15: 0.75, 19: 0.5341614906832298, 22: 0.7783251231527094, 24: 0.08333333333333333, 25: 0.44776119402985076, 26: 0.7759562841530054, 27: 0.14285714285714285, 28: 0.7142857142857143, 29: 0.9247311827956989, 31: 0.0, 32: 0.6225165562913907, 34: 0.366412213740458, 35: 0.3364485981308411, 37: 0.3888888888888889, 38: 0.25, 39: 0.0, 40: 0.5370370370370371}
Micro-average F1 score: 0.4963059539330726
Weighted-average F1 score: 0.5277023308112532
F1 score per class: {1: 0.3424657534246575, 2: 0.875, 3: 0.46258503401360546, 6: 0.5135135135135135, 7: 0.08333333333333333, 9: 0.9803921568627451, 11: 0.27522935779816515, 12: 0.6820809248554913, 14: 0.21978021978021978, 15: 0.631578947368421, 19: 0.5730994152046783, 22: 0.7853403141361257, 24: 0.06896551724137931, 25: 0.7764705882352941, 26: 0.7553191489361702, 27: 0.125, 28: 0.5555555555555556, 29: 0.9361702127659575, 31: 0.6666666666666666, 32: 0.7845303867403315, 34: 0.49746192893401014, 35: 0.49230769230769234, 37: 0.43902439024390244, 38: 0.6296296296296297, 39: 0.125, 40: 0.6382978723404256}
Micro-average F1 score: 0.5898407884761183
Weighted-average F1 score: 0.5884439731896579
F1 score per class: {1: 0.3424657534246575, 2: 0.875, 3: 0.4520547945205479, 6: 0.496551724137931, 7: 0.08163265306122448, 9: 0.9803921568627451, 11: 0.2905982905982906, 12: 0.5578231292517006, 14: 0.21739130434782608, 15: 0.6666666666666666, 19: 0.5780346820809249, 22: 0.7835051546391752, 24: 0.06896551724137931, 25: 0.7619047619047619, 26: 0.7593582887700535, 27: 0.125, 28: 0.5263157894736842, 29: 0.9361702127659575, 31: 0.6666666666666666, 32: 0.7888888888888889, 34: 0.48677248677248675, 35: 0.48854961832061067, 37: 0.40476190476190477, 38: 0.6415094339622641, 39: 0.0, 40: 0.6222222222222222}
Micro-average F1 score: 0.5769083237437668
Weighted-average F1 score: 0.5759587258741139

F1 score per class: {1: 0.125, 2: 0.0, 3: 0.40625, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.07692307692307693, 19: 0.0, 22: 0.4744744744744745, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.4247787610619469, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.24794745484400657
Weighted-average F1 score: 0.21440913134275966
F1 score per class: {1: 0.1388888888888889, 2: 0.0, 3: 0.2943722943722944, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.136986301369863, 15: 0.0, 19: 0.0, 22: 0.49504950495049505, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.46445497630331756, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.20730397422126745
Weighted-average F1 score: 0.17920160360348736
F1 score per class: {1: 0.13812154696132597, 2: 0.0, 3: 0.3013698630136986, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.1342281879194631, 15: 0.0, 19: 0.0, 22: 0.47058823529411764, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.46, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2112284602556976
Weighted-average F1 score: 0.18442949999049443

F1 score per class: {1: 0.0979381443298969, 2: 0.3157894736842105, 3: 0.3023255813953488, 6: 0.21897810218978103, 7: 0.017241379310344827, 9: 0.7936507936507936, 11: 0.15294117647058825, 12: 0.18840579710144928, 14: 0.06741573033707865, 15: 0.6, 19: 0.37554585152838427, 22: 0.3853658536585366, 24: 0.0625, 25: 0.44776119402985076, 26: 0.71, 27: 0.125, 28: 0.15151515151515152, 29: 0.7713004484304933, 31: 0.0, 32: 0.5, 34: 0.1811320754716981, 35: 0.17733990147783252, 37: 0.1826086956521739, 38: 0.11267605633802817, 39: 0.0, 40: 0.3741935483870968}
Micro-average F1 score: 0.30756800430918396
Weighted-average F1 score: 0.28685490597254376
F1 score per class: {1: 0.09803921568627451, 2: 0.17721518987341772, 3: 0.1574074074074074, 6: 0.3247863247863248, 7: 0.031496062992125984, 9: 0.36764705882352944, 11: 0.16853932584269662, 12: 0.271264367816092, 14: 0.091324200913242, 15: 0.3, 19: 0.34385964912280703, 22: 0.38461538461538464, 24: 0.038461538461538464, 25: 0.6947368421052632, 26: 0.6543778801843319, 27: 0.08333333333333333, 28: 0.05319148936170213, 29: 0.7521367521367521, 31: 0.04081632653061224, 32: 0.5461538461538461, 34: 0.1491628614916286, 35: 0.15130023640661938, 37: 0.17142857142857143, 38: 0.1504424778761062, 39: 0.08333333333333333, 40: 0.2903225806451613}
Micro-average F1 score: 0.25787205833609544
Weighted-average F1 score: 0.23421395738266906
F1 score per class: {1: 0.09652509652509653, 2: 0.19718309859154928, 3: 0.16216216216216217, 6: 0.3380281690140845, 7: 0.031746031746031744, 9: 0.6410256410256411, 11: 0.16346153846153846, 12: 0.28771929824561404, 14: 0.09259259259259259, 15: 0.3870967741935484, 19: 0.352112676056338, 22: 0.35185185185185186, 24: 0.04, 25: 0.6808510638297872, 26: 0.6666666666666666, 27: 0.09523809523809523, 28: 0.05235602094240838, 29: 0.7652173913043478, 31: 0.043478260869565216, 32: 0.5568627450980392, 34: 0.15916955017301038, 35: 0.14849187935034802, 37: 0.12878787878787878, 38: 0.16585365853658537, 39: 0.0, 40: 0.30324909747292417}
Micro-average F1 score: 0.2618384401114206
Weighted-average F1 score: 0.23564627958255463
cur_acc_wo_na:  ['0.8339', '0.4667', '0.5514', '0.5875', '0.4817']
his_acc_wo_na:  ['0.8339', '0.7294', '0.6350', '0.5734', '0.4963']
cur_acc des_wo_na:  ['0.8427', '0.7243', '0.5662', '0.7432', '0.5719']
his_acc des_wo_na:  ['0.8427', '0.8229', '0.7040', '0.6968', '0.5898']
cur_acc rrf_wo_na:  ['0.8448', '0.7337', '0.5662', '0.7451', '0.5638']
his_acc rrf_wo_na:  ['0.8448', '0.8070', '0.6865', '0.6865', '0.5769']
cur_acc_w_na:  ['0.7000', '0.3024', '0.3782', '0.3921', '0.2479']
his_acc_w_na:  ['0.7000', '0.5326', '0.4643', '0.4058', '0.3076']
cur_acc des_w_na:  ['0.6267', '0.3607', '0.2974', '0.3478', '0.2073']
his_acc des_w_na:  ['0.6267', '0.4682', '0.3951', '0.3374', '0.2579']
cur_acc rrf_w_na:  ['0.6301', '0.3803', '0.3002', '0.3629', '0.2112']
his_acc rrf_w_na:  ['0.6301', '0.4741', '0.3985', '0.3474', '0.2618']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 145.3739747CurrentTrain: epoch  0, batch     1 | loss: 137.5785302CurrentTrain: epoch  0, batch     2 | loss: 132.6577360CurrentTrain: epoch  0, batch     3 | loss: 232.6294162CurrentTrain: epoch  1, batch     0 | loss: 138.4790698CurrentTrain: epoch  1, batch     1 | loss: 95.1936254CurrentTrain: epoch  1, batch     2 | loss: 180.7211850CurrentTrain: epoch  1, batch     3 | loss: 109.0162941CurrentTrain: epoch  2, batch     0 | loss: 101.0437673CurrentTrain: epoch  2, batch     1 | loss: 103.3765198CurrentTrain: epoch  2, batch     2 | loss: 138.7376189CurrentTrain: epoch  2, batch     3 | loss: 83.2489049CurrentTrain: epoch  3, batch     0 | loss: 105.7061023CurrentTrain: epoch  3, batch     1 | loss: 105.8856271CurrentTrain: epoch  3, batch     2 | loss: 82.5582442CurrentTrain: epoch  3, batch     3 | loss: 100.6320512CurrentTrain: epoch  4, batch     0 | loss: 82.1493472CurrentTrain: epoch  4, batch     1 | loss: 107.6409406CurrentTrain: epoch  4, batch     2 | loss: 98.4349969CurrentTrain: epoch  4, batch     3 | loss: 103.0313674CurrentTrain: epoch  5, batch     0 | loss: 75.7650531CurrentTrain: epoch  5, batch     1 | loss: 80.6034759CurrentTrain: epoch  5, batch     2 | loss: 183.8226880CurrentTrain: epoch  5, batch     3 | loss: 85.9876221CurrentTrain: epoch  6, batch     0 | loss: 170.0310674CurrentTrain: epoch  6, batch     1 | loss: 81.4714131CurrentTrain: epoch  6, batch     2 | loss: 99.7818789CurrentTrain: epoch  6, batch     3 | loss: 137.6373868CurrentTrain: epoch  7, batch     0 | loss: 128.6122962CurrentTrain: epoch  7, batch     1 | loss: 122.6315722CurrentTrain: epoch  7, batch     2 | loss: 80.9473597CurrentTrain: epoch  7, batch     3 | loss: 80.0718713CurrentTrain: epoch  8, batch     0 | loss: 98.3221585CurrentTrain: epoch  8, batch     1 | loss: 273.7871677CurrentTrain: epoch  8, batch     2 | loss: 78.3039346CurrentTrain: epoch  8, batch     3 | loss: 76.2850828CurrentTrain: epoch  9, batch     0 | loss: 124.3929454CurrentTrain: epoch  9, batch     1 | loss: 93.5839388CurrentTrain: epoch  9, batch     2 | loss: 99.0263082CurrentTrain: epoch  9, batch     3 | loss: 80.0729504
MemoryTrain:  epoch  0, batch     0 | loss: 0.9298120MemoryTrain:  epoch  1, batch     0 | loss: 0.8227483MemoryTrain:  epoch  2, batch     0 | loss: 0.6582247MemoryTrain:  epoch  3, batch     0 | loss: 0.5281104MemoryTrain:  epoch  4, batch     0 | loss: 0.4324067MemoryTrain:  epoch  5, batch     0 | loss: 0.3514310MemoryTrain:  epoch  6, batch     0 | loss: 0.2742715MemoryTrain:  epoch  7, batch     0 | loss: 0.2640997MemoryTrain:  epoch  8, batch     0 | loss: 0.2182845MemoryTrain:  epoch  9, batch     0 | loss: 0.1916529

F1 score per class: {0: 0.9428571428571428, 1: 0.0, 34: 0.0, 2: 0.9361702127659575, 4: 0.0, 37: 0.3333333333333333, 11: 0.0, 13: 0.45, 14: 0.0, 21: 0.9318181818181818, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8452088452088452
Weighted-average F1 score: 0.834679910297159
F1 score per class: {0: 1.0, 1: 0.0, 34: 0.0, 2: 0.9417989417989417, 4: 0.75, 37: 0.0, 38: 0.0, 13: 0.5909090909090909, 14: 0.0, 15: 0.810126582278481, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8169014084507042
Weighted-average F1 score: 0.7605629836642493
F1 score per class: {0: 1.0, 1: 0.0, 34: 0.0, 2: 0.9361702127659575, 4: 0.0, 37: 0.75, 38: 0.0, 11: 0.0, 13: 0.5909090909090909, 14: 0.0, 15: 0.810126582278481, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8141176470588235
Weighted-average F1 score: 0.7572626709177259

F1 score per class: {0: 0.9295774647887324, 1: 0.30344827586206896, 2: 0.7777777777777778, 3: 0.546875, 4: 0.9361702127659575, 6: 0.3305785123966942, 7: 0.044444444444444446, 9: 0.9803921568627451, 11: 0.2702702702702703, 12: 0.19642857142857142, 13: 0.09523809523809523, 14: 0.08695652173913043, 15: 0.75, 19: 0.5664739884393064, 21: 0.2535211267605634, 22: 0.7160493827160493, 23: 0.8913043478260869, 24: 0.08695652173913043, 25: 0.6835443037974683, 26: 0.7650273224043715, 27: 0.14285714285714285, 28: 0.2222222222222222, 29: 0.9139784946236559, 31: 0.5, 32: 0.4740740740740741, 34: 0.30927835051546393, 35: 0.32653061224489793, 37: 0.32142857142857145, 38: 0.13333333333333333, 39: 0.0, 40: 0.49523809523809526}
Micro-average F1 score: 0.5359672008945211
Weighted-average F1 score: 0.5653697539067307
F1 score per class: {0: 0.9736842105263158, 1: 0.3333333333333333, 2: 0.6666666666666666, 3: 0.5174825174825175, 4: 0.9417989417989417, 6: 0.543046357615894, 7: 0.08888888888888889, 9: 0.9615384615384616, 11: 0.23423423423423423, 12: 0.6545454545454545, 13: 0.3333333333333333, 14: 0.1411764705882353, 15: 0.6666666666666666, 19: 0.6222222222222222, 21: 0.27956989247311825, 22: 0.6708860759493671, 23: 0.7804878048780488, 24: 0.09523809523809523, 25: 0.8314606741573034, 26: 0.7135678391959799, 27: 0.21052631578947367, 28: 0.4, 29: 0.9312169312169312, 31: 1.0, 32: 0.7073170731707317, 34: 0.4973544973544973, 35: 0.49612403100775193, 37: 0.3684210526315789, 38: 0.6153846153846154, 39: 0.23529411764705882, 40: 0.6417910447761194}
Micro-average F1 score: 0.6034995047870584
Weighted-average F1 score: 0.6012055088994517
F1 score per class: {0: 0.9736842105263158, 1: 0.33557046979865773, 2: 0.6666666666666666, 3: 0.5, 4: 0.9361702127659575, 6: 0.5333333333333333, 7: 0.09090909090909091, 9: 0.9803921568627451, 11: 0.2644628099173554, 12: 0.6025641025641025, 13: 0.2222222222222222, 14: 0.1, 15: 0.6666666666666666, 19: 0.6338797814207651, 21: 0.2708333333333333, 22: 0.6835443037974683, 23: 0.7710843373493976, 24: 0.09090909090909091, 25: 0.7906976744186046, 26: 0.7171717171717171, 27: 0.21052631578947367, 28: 0.4, 29: 0.9197860962566845, 31: 1.0, 32: 0.7073170731707317, 34: 0.4563758389261745, 35: 0.49612403100775193, 37: 0.35555555555555557, 38: 0.43902439024390244, 39: 0.125, 40: 0.6259541984732825}
Micro-average F1 score: 0.5897693079237714
Weighted-average F1 score: 0.585731463974562

F1 score per class: {0: 0.8571428571428571, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9312169312169312, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.04081632653061224, 14: 0.0, 19: 0.0, 21: 0.33962264150943394, 22: 0.0, 23: 0.845360824742268, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.0}
Micro-average F1 score: 0.5752508361204013
Weighted-average F1 score: 0.4462954467769552
F1 score per class: {0: 0.5967741935483871, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8516746411483254, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.10344827586206896, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.29545454545454547, 22: 0.0, 23: 0.5981308411214953, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3515151515151515
Weighted-average F1 score: 0.2635888427308644
F1 score per class: {0: 0.592, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8712871287128713, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.08695652173913043, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.29213483146067415, 22: 0.0, 23: 0.6213592233009708, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.36613756613756615
Weighted-average F1 score: 0.2734792293097802

F1 score per class: {0: 0.3626373626373626, 1: 0.10401891252955082, 2: 0.27450980392156865, 3: 0.35, 4: 0.9119170984455959, 6: 0.28169014084507044, 7: 0.018018018018018018, 9: 0.7142857142857143, 11: 0.18404907975460122, 12: 0.1506849315068493, 13: 0.007326007326007326, 14: 0.07792207792207792, 15: 0.631578947368421, 19: 0.36704119850187267, 21: 0.08530805687203792, 22: 0.5201793721973094, 23: 0.780952380952381, 24: 0.07142857142857142, 25: 0.6428571428571429, 26: 0.7035175879396985, 27: 0.1111111111111111, 28: 0.2222222222222222, 29: 0.7522123893805309, 31: 0.03636363636363636, 32: 0.38323353293413176, 34: 0.19230769230769232, 35: 0.1693121693121693, 37: 0.2, 38: 0.07692307692307693, 39: 0.0, 40: 0.3291139240506329}
Micro-average F1 score: 0.3273389483268837
Weighted-average F1 score: 0.28892799223223453
F1 score per class: {0: 0.15384615384615385, 1: 0.09057971014492754, 2: 0.10526315789473684, 3: 0.14682539682539683, 4: 0.7946428571428571, 6: 0.296028880866426, 7: 0.03305785123966942, 9: 0.43478260869565216, 11: 0.1368421052631579, 12: 0.24770642201834864, 13: 0.02120141342756184, 14: 0.05660377358490566, 15: 0.4, 19: 0.33633633633633636, 21: 0.07878787878787878, 22: 0.45689655172413796, 23: 0.4507042253521127, 24: 0.09523809523809523, 25: 0.6434782608695652, 26: 0.6311111111111111, 27: 0.11428571428571428, 28: 0.057971014492753624, 29: 0.7333333333333333, 31: 0.038461538461538464, 32: 0.4915254237288136, 34: 0.17028985507246377, 35: 0.1693121693121693, 37: 0.19310344827586207, 38: 0.15458937198067632, 39: 0.16, 40: 0.3104693140794224}
Micro-average F1 score: 0.2530454042081949
Weighted-average F1 score: 0.22416746125409617
F1 score per class: {0: 0.15416666666666667, 1: 0.09208103130755065, 2: 0.12844036697247707, 3: 0.144, 4: 0.8380952380952381, 6: 0.3065134099616858, 7: 0.031496062992125984, 9: 0.5376344086021505, 11: 0.13733905579399142, 12: 0.2568306010928962, 13: 0.017595307917888565, 14: 0.045714285714285714, 15: 0.41379310344827586, 19: 0.3431952662721893, 21: 0.07344632768361582, 22: 0.45188284518828453, 23: 0.5203252032520326, 24: 0.08333333333333333, 25: 0.6538461538461539, 26: 0.6425339366515838, 27: 0.11764705882352941, 28: 0.08333333333333333, 29: 0.7257383966244726, 31: 0.042105263157894736, 32: 0.5043478260869565, 34: 0.17989417989417988, 35: 0.17297297297297298, 37: 0.17777777777777778, 38: 0.13138686131386862, 39: 0.09090909090909091, 40: 0.31417624521072796}
Micro-average F1 score: 0.25706791023025355
Weighted-average F1 score: 0.22505700888199603
cur_acc_wo_na:  ['0.8339', '0.4667', '0.5514', '0.5875', '0.4817', '0.8452']
his_acc_wo_na:  ['0.8339', '0.7294', '0.6350', '0.5734', '0.4963', '0.5360']
cur_acc des_wo_na:  ['0.8427', '0.7243', '0.5662', '0.7432', '0.5719', '0.8169']
his_acc des_wo_na:  ['0.8427', '0.8229', '0.7040', '0.6968', '0.5898', '0.6035']
cur_acc rrf_wo_na:  ['0.8448', '0.7337', '0.5662', '0.7451', '0.5638', '0.8141']
his_acc rrf_wo_na:  ['0.8448', '0.8070', '0.6865', '0.6865', '0.5769', '0.5898']
cur_acc_w_na:  ['0.7000', '0.3024', '0.3782', '0.3921', '0.2479', '0.5753']
his_acc_w_na:  ['0.7000', '0.5326', '0.4643', '0.4058', '0.3076', '0.3273']
cur_acc des_w_na:  ['0.6267', '0.3607', '0.2974', '0.3478', '0.2073', '0.3515']
his_acc des_w_na:  ['0.6267', '0.4682', '0.3951', '0.3374', '0.2579', '0.2530']
cur_acc rrf_w_na:  ['0.6301', '0.3803', '0.3002', '0.3629', '0.2112', '0.3661']
his_acc rrf_w_na:  ['0.6301', '0.4741', '0.3985', '0.3474', '0.2618', '0.2571']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 99.3015250CurrentTrain: epoch  0, batch     1 | loss: 146.0643344CurrentTrain: epoch  0, batch     2 | loss: 138.8743330CurrentTrain: epoch  0, batch     3 | loss: 69.9179969CurrentTrain: epoch  1, batch     0 | loss: 88.3580308CurrentTrain: epoch  1, batch     1 | loss: 109.1310396CurrentTrain: epoch  1, batch     2 | loss: 105.0500283CurrentTrain: epoch  1, batch     3 | loss: 349.0272814CurrentTrain: epoch  2, batch     0 | loss: 84.0928772CurrentTrain: epoch  2, batch     1 | loss: 173.5521078CurrentTrain: epoch  2, batch     2 | loss: 84.8224068CurrentTrain: epoch  2, batch     3 | loss: 61.1791817CurrentTrain: epoch  3, batch     0 | loss: 80.2759898CurrentTrain: epoch  3, batch     1 | loss: 101.3725689CurrentTrain: epoch  3, batch     2 | loss: 167.8096963CurrentTrain: epoch  3, batch     3 | loss: 164.7719188CurrentTrain: epoch  4, batch     0 | loss: 99.2513481CurrentTrain: epoch  4, batch     1 | loss: 80.1286513CurrentTrain: epoch  4, batch     2 | loss: 99.4059284CurrentTrain: epoch  4, batch     3 | loss: 77.8386289CurrentTrain: epoch  5, batch     0 | loss: 127.5338341CurrentTrain: epoch  5, batch     1 | loss: 76.5655513CurrentTrain: epoch  5, batch     2 | loss: 97.9435404CurrentTrain: epoch  5, batch     3 | loss: 60.8588032CurrentTrain: epoch  6, batch     0 | loss: 124.3478243CurrentTrain: epoch  6, batch     1 | loss: 93.2279339CurrentTrain: epoch  6, batch     2 | loss: 96.4737914CurrentTrain: epoch  6, batch     3 | loss: 104.9941979CurrentTrain: epoch  7, batch     0 | loss: 126.0073708CurrentTrain: epoch  7, batch     1 | loss: 97.9058811CurrentTrain: epoch  7, batch     2 | loss: 80.2574955CurrentTrain: epoch  7, batch     3 | loss: 53.3400797CurrentTrain: epoch  8, batch     0 | loss: 79.8587633CurrentTrain: epoch  8, batch     1 | loss: 93.3500580CurrentTrain: epoch  8, batch     2 | loss: 98.8555256CurrentTrain: epoch  8, batch     3 | loss: 72.9146744CurrentTrain: epoch  9, batch     0 | loss: 77.0055159CurrentTrain: epoch  9, batch     1 | loss: 98.5199220CurrentTrain: epoch  9, batch     2 | loss: 78.1429296CurrentTrain: epoch  9, batch     3 | loss: 72.8818774
MemoryTrain:  epoch  0, batch     0 | loss: 0.5305341MemoryTrain:  epoch  1, batch     0 | loss: 0.4380888MemoryTrain:  epoch  2, batch     0 | loss: 0.3211216MemoryTrain:  epoch  3, batch     0 | loss: 0.2839401MemoryTrain:  epoch  4, batch     0 | loss: 0.2353694MemoryTrain:  epoch  5, batch     0 | loss: 0.2004097MemoryTrain:  epoch  6, batch     0 | loss: 0.1976252MemoryTrain:  epoch  7, batch     0 | loss: 0.1463962MemoryTrain:  epoch  8, batch     0 | loss: 0.1589115MemoryTrain:  epoch  9, batch     0 | loss: 0.1274804

F1 score per class: {33: 0.5663716814159292, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.88, 8: 0.0, 11: 0.0, 12: 0.0, 13: 0.9444444444444444, 20: 0.42857142857142855, 26: 0.0, 28: 0.5, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.6310160427807486
Weighted-average F1 score: 0.6080081373886683
F1 score per class: {2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.7727272727272727, 12: 0.0, 13: 0.0, 20: 0.92, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.42857142857142855, 34: 0.0, 36: 0.8780487804878049, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.810304449648712
Weighted-average F1 score: 0.7758346208296432
F1 score per class: {2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.7633587786259542, 11: 0.0, 12: 0.0, 13: 0.0, 20: 0.9108910891089109, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.42857142857142855, 34: 0.0, 36: 0.8205128205128205, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.7793427230046949
Weighted-average F1 score: 0.7339060935963315

F1 score per class: {0: 0.9315068493150684, 1: 0.26666666666666666, 2: 0.7058823529411765, 3: 0.5625, 4: 0.9010989010989011, 6: 0.29508196721311475, 7: 0.0, 8: 0.48484848484848486, 9: 0.9803921568627451, 11: 0.22413793103448276, 12: 0.14545454545454545, 13: 0.125, 14: 0.03076923076923077, 15: 0.75, 19: 0.5238095238095238, 20: 0.7096774193548387, 21: 0.20588235294117646, 22: 0.6447368421052632, 23: 0.8095238095238095, 24: 0.08695652173913043, 25: 0.3225806451612903, 26: 0.7374301675977654, 27: 0.14285714285714285, 28: 0.2, 29: 0.9090909090909091, 30: 0.918918918918919, 31: 0.6666666666666666, 32: 0.48175182481751827, 33: 0.2857142857142857, 34: 0.23255813953488372, 35: 0.25882352941176473, 36: 0.4536082474226804, 37: 0.2716049382716049, 38: 0.125, 39: 0.0, 40: 0.4489795918367347}
Micro-average F1 score: 0.5096316323082122
Weighted-average F1 score: 0.5554512519721008
F1 score per class: {0: 0.9473684210526315, 1: 0.3287671232876712, 2: 0.6086956521739131, 3: 0.6013071895424836, 4: 0.9417989417989417, 6: 0.5679012345679012, 7: 0.058823529411764705, 8: 0.6335403726708074, 9: 0.9803921568627451, 11: 0.32142857142857145, 12: 0.5769230769230769, 13: 0.18181818181818182, 14: 0.07407407407407407, 15: 0.75, 19: 0.6054054054054054, 20: 0.8440366972477065, 21: 0.2391304347826087, 22: 0.6832298136645962, 23: 0.8275862068965517, 24: 0.09090909090909091, 25: 0.7294117647058823, 26: 0.7165775401069518, 27: 0.23529411764705882, 28: 0.36363636363636365, 29: 0.9319371727748691, 30: 0.8260869565217391, 31: 0.5, 32: 0.7228915662650602, 33: 0.2222222222222222, 34: 0.45217391304347826, 35: 0.4727272727272727, 36: 0.6352941176470588, 37: 0.2962962962962963, 38: 0.5714285714285714, 39: 0.0, 40: 0.6341463414634146}
Micro-average F1 score: 0.612280701754386
Weighted-average F1 score: 0.6177077811526428
F1 score per class: {0: 0.9473684210526315, 1: 0.3108108108108108, 2: 0.6086956521739131, 3: 0.6103896103896104, 4: 0.93048128342246, 6: 0.5298013245033113, 7: 0.05714285714285714, 8: 0.5988023952095808, 9: 0.9803921568627451, 11: 0.3969465648854962, 12: 0.5844155844155844, 13: 0.14285714285714285, 14: 0.0759493670886076, 15: 0.75, 19: 0.6129032258064516, 20: 0.773109243697479, 21: 0.2608695652173913, 22: 0.6708860759493671, 23: 0.8275862068965517, 24: 0.09090909090909091, 25: 0.6153846153846154, 26: 0.7165775401069518, 27: 0.23529411764705882, 28: 0.2, 29: 0.9263157894736842, 30: 0.7450980392156863, 31: 1.0, 32: 0.7228915662650602, 33: 0.1935483870967742, 34: 0.3076923076923077, 35: 0.48598130841121495, 36: 0.631578947368421, 37: 0.29213483146067415, 38: 0.24242424242424243, 39: 0.0, 40: 0.6166666666666667}
Micro-average F1 score: 0.5958702064896755
Weighted-average F1 score: 0.6018308099145439

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.48120300751879697, 11: 0.0, 12: 0.0, 13: 0.0, 19: 0.0, 20: 0.567741935483871, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.918918918918919, 31: 0.0, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 36: 0.39285714285714285, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3979763912310287
Weighted-average F1 score: 0.3301181678903072
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.5125628140703518, 9: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.5227272727272727, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8837209302325582, 31: 0.0, 32: 0.0, 33: 0.2857142857142857, 34: 0.0, 35: 0.0, 36: 0.5684210526315789, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3559670781893004
Weighted-average F1 score: 0.2952844737409836
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.5, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.4946236559139785, 21: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8837209302325582, 31: 0.0, 32: 0.0, 33: 0.25, 34: 0.0, 35: 0.0, 36: 0.5680473372781065, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.35853131749460043
Weighted-average F1 score: 0.2989330944544914

F1 score per class: {0: 0.4121212121212121, 1: 0.09113924050632911, 2: 0.23529411764705882, 3: 0.36180904522613067, 4: 0.8770053475935828, 6: 0.2535211267605634, 7: 0.0, 8: 0.2064516129032258, 9: 0.7246376811594203, 11: 0.1625, 12: 0.12403100775193798, 13: 0.016129032258064516, 14: 0.027777777777777776, 15: 0.75, 19: 0.3371647509578544, 20: 0.25071225071225073, 21: 0.08383233532934131, 22: 0.49246231155778897, 23: 0.7311827956989247, 24: 0.07142857142857142, 25: 0.3125, 26: 0.6534653465346535, 27: 0.125, 28: 0.2, 29: 0.7296137339055794, 30: 0.8717948717948718, 31: 0.044444444444444446, 32: 0.3815028901734104, 33: 0.16216216216216217, 34: 0.16666666666666666, 35: 0.1456953642384106, 36: 0.3013698630136986, 37: 0.21568627450980393, 38: 0.09523809523809523, 39: 0.0, 40: 0.3188405797101449}
Micro-average F1 score: 0.31915343915343913
Weighted-average F1 score: 0.295045443531167
F1 score per class: {0: 0.12413793103448276, 1: 0.0893854748603352, 2: 0.1590909090909091, 3: 0.21100917431192662, 4: 0.89, 6: 0.2977346278317152, 7: 0.025, 8: 0.1639871382636656, 9: 0.423728813559322, 11: 0.20224719101123595, 12: 0.21686746987951808, 13: 0.01818181818181818, 14: 0.04195804195804196, 15: 0.48, 19: 0.3303834808259587, 20: 0.18326693227091634, 21: 0.06432748538011696, 22: 0.47413793103448276, 23: 0.496551724137931, 24: 0.08, 25: 0.5585585585585585, 26: 0.6063348416289592, 27: 0.14285714285714285, 28: 0.18181818181818182, 29: 0.689922480620155, 30: 0.336283185840708, 31: 0.020833333333333332, 32: 0.48, 33: 0.07228915662650602, 34: 0.24761904761904763, 35: 0.15160349854227406, 36: 0.26535626535626533, 37: 0.19834710743801653, 38: 0.17266187050359713, 39: 0.0, 40: 0.30952380952380953}
Micro-average F1 score: 0.25533471527862456
Weighted-average F1 score: 0.23061959685599054
F1 score per class: {0: 0.1348314606741573, 1: 0.08487084870848709, 2: 0.18666666666666668, 3: 0.2493368700265252, 4: 0.8923076923076924, 6: 0.311284046692607, 7: 0.022988505747126436, 8: 0.1519756838905775, 9: 0.5434782608695652, 11: 0.2184873949579832, 12: 0.22784810126582278, 13: 0.01639344262295082, 14: 0.043478260869565216, 15: 0.48, 19: 0.33827893175074186, 20: 0.17358490566037735, 21: 0.06818181818181818, 22: 0.4669603524229075, 23: 0.5625, 24: 0.08, 25: 0.5161290322580645, 26: 0.6118721461187214, 27: 0.14814814814814814, 28: 0.15384615384615385, 29: 0.7096774193548387, 30: 0.296875, 31: 0.028985507246376812, 32: 0.48, 33: 0.058823529411764705, 34: 0.1958041958041958, 35: 0.16455696202531644, 36: 0.27586206896551724, 37: 0.18705035971223022, 38: 0.08695652173913043, 39: 0.0, 40: 0.30327868852459017}
Micro-average F1 score: 0.2572920647051331
Weighted-average F1 score: 0.2305767593824783
cur_acc_wo_na:  ['0.8339', '0.4667', '0.5514', '0.5875', '0.4817', '0.8452', '0.6310']
his_acc_wo_na:  ['0.8339', '0.7294', '0.6350', '0.5734', '0.4963', '0.5360', '0.5096']
cur_acc des_wo_na:  ['0.8427', '0.7243', '0.5662', '0.7432', '0.5719', '0.8169', '0.8103']
his_acc des_wo_na:  ['0.8427', '0.8229', '0.7040', '0.6968', '0.5898', '0.6035', '0.6123']
cur_acc rrf_wo_na:  ['0.8448', '0.7337', '0.5662', '0.7451', '0.5638', '0.8141', '0.7793']
his_acc rrf_wo_na:  ['0.8448', '0.8070', '0.6865', '0.6865', '0.5769', '0.5898', '0.5959']
cur_acc_w_na:  ['0.7000', '0.3024', '0.3782', '0.3921', '0.2479', '0.5753', '0.3980']
his_acc_w_na:  ['0.7000', '0.5326', '0.4643', '0.4058', '0.3076', '0.3273', '0.3192']
cur_acc des_w_na:  ['0.6267', '0.3607', '0.2974', '0.3478', '0.2073', '0.3515', '0.3560']
his_acc des_w_na:  ['0.6267', '0.4682', '0.3951', '0.3374', '0.2579', '0.2530', '0.2553']
cur_acc rrf_w_na:  ['0.6301', '0.3803', '0.3002', '0.3629', '0.2112', '0.3661', '0.3585']
his_acc rrf_w_na:  ['0.6301', '0.4741', '0.3985', '0.3474', '0.2618', '0.2571', '0.2573']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 190.2144361CurrentTrain: epoch  0, batch     1 | loss: 110.9685737CurrentTrain: epoch  0, batch     2 | loss: 145.6340257CurrentTrain: epoch  0, batch     3 | loss: 101.2688318CurrentTrain: epoch  0, batch     4 | loss: 77.9603180CurrentTrain: epoch  1, batch     0 | loss: 104.4250509CurrentTrain: epoch  1, batch     1 | loss: 140.3064165CurrentTrain: epoch  1, batch     2 | loss: 134.0599255CurrentTrain: epoch  1, batch     3 | loss: 136.6819211CurrentTrain: epoch  1, batch     4 | loss: 67.7514466CurrentTrain: epoch  2, batch     0 | loss: 129.0090824CurrentTrain: epoch  2, batch     1 | loss: 127.4659518CurrentTrain: epoch  2, batch     2 | loss: 179.9861828CurrentTrain: epoch  2, batch     3 | loss: 108.4328463CurrentTrain: epoch  2, batch     4 | loss: 67.7826570CurrentTrain: epoch  3, batch     0 | loss: 84.6480787CurrentTrain: epoch  3, batch     1 | loss: 102.7527040CurrentTrain: epoch  3, batch     2 | loss: 104.5899038CurrentTrain: epoch  3, batch     3 | loss: 104.3479443CurrentTrain: epoch  3, batch     4 | loss: 83.1872991CurrentTrain: epoch  4, batch     0 | loss: 81.7501290CurrentTrain: epoch  4, batch     1 | loss: 127.5021527CurrentTrain: epoch  4, batch     2 | loss: 129.6056716CurrentTrain: epoch  4, batch     3 | loss: 174.9681624CurrentTrain: epoch  4, batch     4 | loss: 80.8845246CurrentTrain: epoch  5, batch     0 | loss: 99.9050469CurrentTrain: epoch  5, batch     1 | loss: 81.5829866CurrentTrain: epoch  5, batch     2 | loss: 81.7504371CurrentTrain: epoch  5, batch     3 | loss: 82.7739993CurrentTrain: epoch  5, batch     4 | loss: 362.3351793CurrentTrain: epoch  6, batch     0 | loss: 174.6315494CurrentTrain: epoch  6, batch     1 | loss: 128.0704860CurrentTrain: epoch  6, batch     2 | loss: 95.5100069CurrentTrain: epoch  6, batch     3 | loss: 130.1642193CurrentTrain: epoch  6, batch     4 | loss: 62.3250707CurrentTrain: epoch  7, batch     0 | loss: 127.2574451CurrentTrain: epoch  7, batch     1 | loss: 173.8548577CurrentTrain: epoch  7, batch     2 | loss: 126.3265608CurrentTrain: epoch  7, batch     3 | loss: 77.1555783CurrentTrain: epoch  7, batch     4 | loss: 62.6148102CurrentTrain: epoch  8, batch     0 | loss: 97.7027654CurrentTrain: epoch  8, batch     1 | loss: 95.4967161CurrentTrain: epoch  8, batch     2 | loss: 126.6494556CurrentTrain: epoch  8, batch     3 | loss: 171.8446000CurrentTrain: epoch  8, batch     4 | loss: 166.3307203CurrentTrain: epoch  9, batch     0 | loss: 126.2109699CurrentTrain: epoch  9, batch     1 | loss: 173.4117318CurrentTrain: epoch  9, batch     2 | loss: 122.4418087CurrentTrain: epoch  9, batch     3 | loss: 122.6420147CurrentTrain: epoch  9, batch     4 | loss: 60.3882415
MemoryTrain:  epoch  0, batch     0 | loss: 0.4881160MemoryTrain:  epoch  1, batch     0 | loss: 0.4267939MemoryTrain:  epoch  2, batch     0 | loss: 0.3294191MemoryTrain:  epoch  3, batch     0 | loss: 0.2725669MemoryTrain:  epoch  4, batch     0 | loss: 0.2096308MemoryTrain:  epoch  5, batch     0 | loss: 0.1690913MemoryTrain:  epoch  6, batch     0 | loss: 0.1502602MemoryTrain:  epoch  7, batch     0 | loss: 0.1278583MemoryTrain:  epoch  8, batch     0 | loss: 0.1208879MemoryTrain:  epoch  9, batch     0 | loss: 0.1247896

F1 score per class: {5: 0.9795918367346939, 38: 0.0, 7: 0.0, 8: 0.48484848484848486, 10: 0.0, 11: 0.0, 13: 0.9473684210526315, 16: 0.0, 17: 0.52, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.721030042918455
Weighted-average F1 score: 0.7468153432110692
F1 score per class: {5: 0.9949748743718593, 38: 0.0, 6: 0.0, 8: 0.0, 10: 0.7006369426751592, 7: 0.0, 11: 0.0, 13: 0.9655172413793104, 16: 0.5, 17: 0.7666666666666667, 18: 0.0}
Micro-average F1 score: 0.7804878048780488
Weighted-average F1 score: 0.7183137524795803
F1 score per class: {5: 0.9949748743718593, 38: 0.0, 6: 0.0, 8: 0.0, 10: 0.7329192546583851, 7: 0.0, 11: 0.0, 13: 0.9655172413793104, 16: 0.0, 17: 0.7666666666666667, 18: 0.0}
Micro-average F1 score: 0.7901701323251418
Weighted-average F1 score: 0.7426023841459667

F1 score per class: {0: 0.9166666666666666, 1: 0.30985915492957744, 2: 0.631578947368421, 3: 0.5081967213114754, 4: 0.8764044943820225, 5: 0.8930232558139535, 6: 0.2222222222222222, 7: 0.0, 8: 0.12844036697247707, 9: 0.9803921568627451, 10: 0.4740740740740741, 11: 0.17543859649122806, 12: 0.19642857142857142, 13: 0.09523809523809523, 14: 0.03076923076923077, 15: 0.75, 16: 0.8709677419354839, 17: 0.0, 18: 0.3466666666666667, 19: 0.5497076023391813, 20: 0.6086956521739131, 21: 0.23880597014925373, 22: 0.6794871794871795, 23: 0.7654320987654321, 24: 0.08333333333333333, 25: 0.3492063492063492, 26: 0.7292817679558011, 27: 0.13333333333333333, 28: 0.2, 29: 0.8972972972972973, 30: 0.9444444444444444, 31: 0.6666666666666666, 32: 0.6193548387096774, 33: 0.2222222222222222, 34: 0.21951219512195122, 35: 0.21176470588235294, 36: 0.21621621621621623, 37: 0.22535211267605634, 38: 0.06666666666666667, 39: 0.0, 40: 0.42857142857142855}
Micro-average F1 score: 0.5117096018735363
Weighted-average F1 score: 0.5692092567605789
F1 score per class: {0: 0.9473684210526315, 1: 0.3380281690140845, 2: 0.6086956521739131, 3: 0.5170068027210885, 4: 0.9417989417989417, 5: 0.825, 6: 0.4935064935064935, 7: 0.044444444444444446, 8: 0.5660377358490566, 9: 0.9259259259259259, 10: 0.6395348837209303, 11: 0.32592592592592595, 12: 0.5789473684210527, 13: 0.18181818181818182, 14: 0.0975609756097561, 15: 0.7058823529411765, 16: 0.9032258064516129, 17: 0.4, 18: 0.4107142857142857, 19: 0.6170212765957447, 20: 0.8118811881188119, 21: 0.26506024096385544, 22: 0.6832298136645962, 23: 0.7764705882352941, 24: 0.08333333333333333, 25: 0.5974025974025974, 26: 0.7135135135135136, 27: 0.19047619047619047, 28: 0.3076923076923077, 29: 0.9270833333333334, 30: 0.8085106382978723, 31: 0.5, 32: 0.7597765363128491, 33: 0.2857142857142857, 34: 0.4672897196261682, 35: 0.4297520661157025, 36: 0.6486486486486487, 37: 0.29333333333333333, 38: 0.391304347826087, 39: 0.1111111111111111, 40: 0.5619834710743802}
Micro-average F1 score: 0.60416144397092
Weighted-average F1 score: 0.6058958304733408
F1 score per class: {0: 0.9473684210526315, 1: 0.3380281690140845, 2: 0.5714285714285714, 3: 0.5369127516778524, 4: 0.9361702127659575, 5: 0.8319327731092437, 6: 0.4305555555555556, 7: 0.04081632653061224, 8: 0.569620253164557, 9: 0.9433962264150944, 10: 0.6666666666666666, 11: 0.3380281690140845, 12: 0.5827814569536424, 13: 0.15384615384615385, 14: 0.1, 15: 0.7058823529411765, 16: 0.8888888888888888, 17: 0.0, 18: 0.4339622641509434, 19: 0.631578947368421, 20: 0.8155339805825242, 21: 0.2619047619047619, 22: 0.675, 23: 0.7764705882352941, 24: 0.08333333333333333, 25: 0.5405405405405406, 26: 0.7135135135135136, 27: 0.19047619047619047, 28: 0.3333333333333333, 29: 0.9270833333333334, 30: 0.7755102040816326, 31: 0.6666666666666666, 32: 0.7597765363128491, 33: 0.26666666666666666, 34: 0.42990654205607476, 35: 0.39655172413793105, 36: 0.6019417475728155, 37: 0.29333333333333333, 38: 0.05714285714285714, 39: 0.125, 40: 0.5714285714285714}
Micro-average F1 score: 0.5978755690440061
Weighted-average F1 score: 0.6038321381301051

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.8205128205128205, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.4413793103448276, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.6, 17: 0.0, 18: 0.22608695652173913, 20: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0, 34: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.4628099173553719
Weighted-average F1 score: 0.40765682116206853
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.5380434782608695, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.5851063829787234, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.5, 17: 0.4, 18: 0.1862348178137652, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.30387143900657415
Weighted-average F1 score: 0.2545236281948137
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.5875370919881305, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.6020408163265306, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.48695652173913045, 17: 0.0, 18: 0.19574468085106383, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3217859892224788
Weighted-average F1 score: 0.27095890549840723

F1 score per class: {0: 0.4583333333333333, 1: 0.10208816705336426, 2: 0.24489795918367346, 3: 0.3147208121827411, 4: 0.861878453038674, 5: 0.6832740213523132, 6: 0.1897810218978102, 7: 0.0, 8: 0.08383233532934131, 9: 0.6756756756756757, 10: 0.3076923076923077, 11: 0.14184397163120568, 12: 0.15602836879432624, 13: 0.016260162601626018, 14: 0.028169014084507043, 15: 0.7058823529411765, 16: 0.40298507462686567, 17: 0.0, 18: 0.12682926829268293, 19: 0.3560606060606061, 20: 0.22012578616352202, 21: 0.10596026490066225, 22: 0.48847926267281105, 23: 0.6391752577319587, 24: 0.05, 25: 0.34375, 26: 0.631578947368421, 27: 0.11764705882352941, 28: 0.2, 29: 0.6859504132231405, 30: 0.8947368421052632, 31: 0.05263157894736842, 32: 0.4507042253521127, 33: 0.12, 34: 0.15384615384615385, 35: 0.11688311688311688, 36: 0.1927710843373494, 37: 0.16666666666666666, 38: 0.05, 39: 0.0, 40: 0.2916666666666667}
Micro-average F1 score: 0.32150082766231375
Weighted-average F1 score: 0.30496695828127135
F1 score per class: {0: 0.1575492341356674, 1: 0.08921933085501858, 2: 0.14583333333333334, 3: 0.17471264367816092, 4: 0.8640776699029126, 5: 0.30697674418604654, 6: 0.2576271186440678, 7: 0.016666666666666666, 8: 0.1906779661016949, 9: 0.3105590062111801, 10: 0.2702702702702703, 11: 0.21568627450980393, 12: 0.20754716981132076, 13: 0.03361344537815126, 14: 0.04419889502762431, 15: 0.4, 16: 0.34782608695652173, 17: 0.14285714285714285, 18: 0.09663865546218488, 19: 0.31266846361185985, 20: 0.1842696629213483, 21: 0.07432432432432433, 22: 0.44176706827309237, 23: 0.4647887323943662, 24: 0.06451612903225806, 25: 0.5287356321839081, 26: 0.5972850678733032, 27: 0.1111111111111111, 28: 0.09302325581395349, 29: 0.652014652014652, 30: 0.3486238532110092, 31: 0.034782608695652174, 32: 0.4788732394366197, 33: 0.09876543209876543, 34: 0.2358490566037736, 35: 0.1087866108786611, 36: 0.4, 37: 0.1981981981981982, 38: 0.11464968152866242, 39: 0.06666666666666667, 40: 0.2556390977443609}
Micro-average F1 score: 0.24881271938880858
Weighted-average F1 score: 0.22869450759633858
F1 score per class: {0: 0.1769041769041769, 1: 0.08823529411764706, 2: 0.1518987341772152, 3: 0.20151133501259447, 4: 0.9166666666666666, 5: 0.3619744058500914, 6: 0.23938223938223938, 7: 0.014925373134328358, 8: 0.19955654101995565, 9: 0.46296296296296297, 10: 0.2757009345794392, 11: 0.22119815668202766, 12: 0.21674876847290642, 13: 0.026845637583892617, 14: 0.050955414012738856, 15: 0.48, 16: 0.33532934131736525, 17: 0.0, 18: 0.1050228310502283, 19: 0.3225806451612903, 20: 0.18142548596112312, 21: 0.06832298136645963, 22: 0.432, 23: 0.4925373134328358, 24: 0.06451612903225806, 25: 0.4878048780487805, 26: 0.6055045871559633, 27: 0.12121212121212122, 28: 0.14285714285714285, 29: 0.6926070038910506, 30: 0.29457364341085274, 31: 0.0425531914893617, 32: 0.4857142857142857, 33: 0.0898876404494382, 34: 0.2358974358974359, 35: 0.11358024691358025, 36: 0.36470588235294116, 37: 0.19130434782608696, 38: 0.019230769230769232, 39: 0.09090909090909091, 40: 0.265625}
Micro-average F1 score: 0.25771285293797014
Weighted-average F1 score: 0.23625621758018028
cur_acc_wo_na:  ['0.8339', '0.4667', '0.5514', '0.5875', '0.4817', '0.8452', '0.6310', '0.7210']
his_acc_wo_na:  ['0.8339', '0.7294', '0.6350', '0.5734', '0.4963', '0.5360', '0.5096', '0.5117']
cur_acc des_wo_na:  ['0.8427', '0.7243', '0.5662', '0.7432', '0.5719', '0.8169', '0.8103', '0.7805']
his_acc des_wo_na:  ['0.8427', '0.8229', '0.7040', '0.6968', '0.5898', '0.6035', '0.6123', '0.6042']
cur_acc rrf_wo_na:  ['0.8448', '0.7337', '0.5662', '0.7451', '0.5638', '0.8141', '0.7793', '0.7902']
his_acc rrf_wo_na:  ['0.8448', '0.8070', '0.6865', '0.6865', '0.5769', '0.5898', '0.5959', '0.5979']
cur_acc_w_na:  ['0.7000', '0.3024', '0.3782', '0.3921', '0.2479', '0.5753', '0.3980', '0.4628']
his_acc_w_na:  ['0.7000', '0.5326', '0.4643', '0.4058', '0.3076', '0.3273', '0.3192', '0.3215']
cur_acc des_w_na:  ['0.6267', '0.3607', '0.2974', '0.3478', '0.2073', '0.3515', '0.3560', '0.3039']
his_acc des_w_na:  ['0.6267', '0.4682', '0.3951', '0.3374', '0.2579', '0.2530', '0.2553', '0.2488']
cur_acc rrf_w_na:  ['0.6301', '0.3803', '0.3002', '0.3629', '0.2112', '0.3661', '0.3585', '0.3218']
his_acc rrf_w_na:  ['0.6301', '0.4741', '0.3985', '0.3474', '0.2618', '0.2571', '0.2573', '0.2577']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 159.3951128CurrentTrain: epoch  0, batch     1 | loss: 125.5727202CurrentTrain: epoch  0, batch     2 | loss: 101.9511744CurrentTrain: epoch  0, batch     3 | loss: 100.5630402CurrentTrain: epoch  0, batch     4 | loss: 120.0838658CurrentTrain: epoch  0, batch     5 | loss: 147.5310555CurrentTrain: epoch  0, batch     6 | loss: 86.8106194CurrentTrain: epoch  0, batch     7 | loss: 145.3209428CurrentTrain: epoch  0, batch     8 | loss: 86.1970381CurrentTrain: epoch  0, batch     9 | loss: 86.3797722CurrentTrain: epoch  0, batch    10 | loss: 189.7257547CurrentTrain: epoch  0, batch    11 | loss: 97.8776687CurrentTrain: epoch  0, batch    12 | loss: 117.7687039CurrentTrain: epoch  0, batch    13 | loss: 144.5065792CurrentTrain: epoch  0, batch    14 | loss: 143.9194537CurrentTrain: epoch  0, batch    15 | loss: 189.4538682CurrentTrain: epoch  0, batch    16 | loss: 116.5944316CurrentTrain: epoch  0, batch    17 | loss: 143.9204394CurrentTrain: epoch  0, batch    18 | loss: 116.4836510CurrentTrain: epoch  0, batch    19 | loss: 99.8398812CurrentTrain: epoch  0, batch    20 | loss: 97.7288632CurrentTrain: epoch  0, batch    21 | loss: 144.1482957CurrentTrain: epoch  0, batch    22 | loss: 143.8548202CurrentTrain: epoch  0, batch    23 | loss: 84.5278993CurrentTrain: epoch  0, batch    24 | loss: 280.8372097CurrentTrain: epoch  0, batch    25 | loss: 188.9506716CurrentTrain: epoch  0, batch    26 | loss: 116.1015652CurrentTrain: epoch  0, batch    27 | loss: 115.7658512CurrentTrain: epoch  0, batch    28 | loss: 114.5207008CurrentTrain: epoch  0, batch    29 | loss: 114.5486206CurrentTrain: epoch  0, batch    30 | loss: 141.7238889CurrentTrain: epoch  0, batch    31 | loss: 142.6364429CurrentTrain: epoch  0, batch    32 | loss: 142.1374410CurrentTrain: epoch  0, batch    33 | loss: 97.4132787CurrentTrain: epoch  0, batch    34 | loss: 95.9597798CurrentTrain: epoch  0, batch    35 | loss: 114.8024586CurrentTrain: epoch  0, batch    36 | loss: 84.3724228CurrentTrain: epoch  0, batch    37 | loss: 115.4639511CurrentTrain: epoch  0, batch    38 | loss: 280.8928253CurrentTrain: epoch  0, batch    39 | loss: 114.5261100CurrentTrain: epoch  0, batch    40 | loss: 97.2699732CurrentTrain: epoch  0, batch    41 | loss: 113.9654227CurrentTrain: epoch  0, batch    42 | loss: 114.7825658CurrentTrain: epoch  0, batch    43 | loss: 141.7182342CurrentTrain: epoch  0, batch    44 | loss: 114.3476821CurrentTrain: epoch  0, batch    45 | loss: 114.6917598CurrentTrain: epoch  0, batch    46 | loss: 141.8110256CurrentTrain: epoch  0, batch    47 | loss: 141.7924742CurrentTrain: epoch  0, batch    48 | loss: 113.6796883CurrentTrain: epoch  0, batch    49 | loss: 114.2672009CurrentTrain: epoch  0, batch    50 | loss: 112.4137902CurrentTrain: epoch  0, batch    51 | loss: 114.9300492CurrentTrain: epoch  0, batch    52 | loss: 114.2469626CurrentTrain: epoch  0, batch    53 | loss: 112.5625129CurrentTrain: epoch  0, batch    54 | loss: 95.2667442CurrentTrain: epoch  0, batch    55 | loss: 141.0468594CurrentTrain: epoch  0, batch    56 | loss: 114.3376432CurrentTrain: epoch  0, batch    57 | loss: 141.5881524CurrentTrain: epoch  0, batch    58 | loss: 113.7211800CurrentTrain: epoch  0, batch    59 | loss: 186.5429329CurrentTrain: epoch  0, batch    60 | loss: 113.1228937CurrentTrain: epoch  0, batch    61 | loss: 82.1479409CurrentTrain: epoch  0, batch    62 | loss: 113.0832120CurrentTrain: epoch  0, batch    63 | loss: 113.9606236CurrentTrain: epoch  0, batch    64 | loss: 95.1781071CurrentTrain: epoch  0, batch    65 | loss: 94.4259539CurrentTrain: epoch  0, batch    66 | loss: 112.8624956CurrentTrain: epoch  0, batch    67 | loss: 113.1179356CurrentTrain: epoch  0, batch    68 | loss: 139.8700920CurrentTrain: epoch  0, batch    69 | loss: 94.3642220CurrentTrain: epoch  0, batch    70 | loss: 112.3810836CurrentTrain: epoch  0, batch    71 | loss: 111.6077918CurrentTrain: epoch  0, batch    72 | loss: 93.7651461CurrentTrain: epoch  0, batch    73 | loss: 138.4073809CurrentTrain: epoch  0, batch    74 | loss: 95.6399352CurrentTrain: epoch  0, batch    75 | loss: 80.8382774CurrentTrain: epoch  0, batch    76 | loss: 182.3024626CurrentTrain: epoch  0, batch    77 | loss: 81.2841016CurrentTrain: epoch  0, batch    78 | loss: 94.7718567CurrentTrain: epoch  0, batch    79 | loss: 139.0978957CurrentTrain: epoch  0, batch    80 | loss: 111.1003077CurrentTrain: epoch  0, batch    81 | loss: 185.2830292CurrentTrain: epoch  0, batch    82 | loss: 187.5526246CurrentTrain: epoch  0, batch    83 | loss: 91.4484505CurrentTrain: epoch  0, batch    84 | loss: 92.3478486CurrentTrain: epoch  0, batch    85 | loss: 139.5659596CurrentTrain: epoch  0, batch    86 | loss: 109.5692603CurrentTrain: epoch  0, batch    87 | loss: 138.6693727CurrentTrain: epoch  0, batch    88 | loss: 110.3501314CurrentTrain: epoch  0, batch    89 | loss: 110.8833861CurrentTrain: epoch  0, batch    90 | loss: 110.7916118CurrentTrain: epoch  0, batch    91 | loss: 90.1864973CurrentTrain: epoch  0, batch    92 | loss: 90.4712705CurrentTrain: epoch  0, batch    93 | loss: 186.7649713CurrentTrain: epoch  0, batch    94 | loss: 109.2080872CurrentTrain: epoch  0, batch    95 | loss: 93.8974765CurrentTrain: epoch  1, batch     0 | loss: 92.5729426CurrentTrain: epoch  1, batch     1 | loss: 136.2722493CurrentTrain: epoch  1, batch     2 | loss: 89.8697439CurrentTrain: epoch  1, batch     3 | loss: 137.4746481CurrentTrain: epoch  1, batch     4 | loss: 88.0607937CurrentTrain: epoch  1, batch     5 | loss: 137.5867527CurrentTrain: epoch  1, batch     6 | loss: 107.9222910CurrentTrain: epoch  1, batch     7 | loss: 182.7054420CurrentTrain: epoch  1, batch     8 | loss: 135.7729570CurrentTrain: epoch  1, batch     9 | loss: 110.1034113CurrentTrain: epoch  1, batch    10 | loss: 132.1332792CurrentTrain: epoch  1, batch    11 | loss: 89.3035367CurrentTrain: epoch  1, batch    12 | loss: 106.4493523CurrentTrain: epoch  1, batch    13 | loss: 138.4099967CurrentTrain: epoch  1, batch    14 | loss: 79.7983814CurrentTrain: epoch  1, batch    15 | loss: 79.7053723CurrentTrain: epoch  1, batch    16 | loss: 182.2959594CurrentTrain: epoch  1, batch    17 | loss: 135.0768046CurrentTrain: epoch  1, batch    18 | loss: 108.8867992CurrentTrain: epoch  1, batch    19 | loss: 139.0181064CurrentTrain: epoch  1, batch    20 | loss: 109.7093866CurrentTrain: epoch  1, batch    21 | loss: 102.4326170CurrentTrain: epoch  1, batch    22 | loss: 109.8441870CurrentTrain: epoch  1, batch    23 | loss: 107.6435077CurrentTrain: epoch  1, batch    24 | loss: 110.1918616CurrentTrain: epoch  1, batch    25 | loss: 107.4523801CurrentTrain: epoch  1, batch    26 | loss: 108.5226640CurrentTrain: epoch  1, batch    27 | loss: 89.4771745CurrentTrain: epoch  1, batch    28 | loss: 131.0469223CurrentTrain: epoch  1, batch    29 | loss: 131.9809943CurrentTrain: epoch  1, batch    30 | loss: 130.3475178CurrentTrain: epoch  1, batch    31 | loss: 106.4536478CurrentTrain: epoch  1, batch    32 | loss: 139.3573960CurrentTrain: epoch  1, batch    33 | loss: 100.2389699CurrentTrain: epoch  1, batch    34 | loss: 88.3990184CurrentTrain: epoch  1, batch    35 | loss: 184.3115940CurrentTrain: epoch  1, batch    36 | loss: 90.1382522CurrentTrain: epoch  1, batch    37 | loss: 108.5513135CurrentTrain: epoch  1, batch    38 | loss: 93.4729418CurrentTrain: epoch  1, batch    39 | loss: 108.0366768CurrentTrain: epoch  1, batch    40 | loss: 73.4636381CurrentTrain: epoch  1, batch    41 | loss: 85.0897646CurrentTrain: epoch  1, batch    42 | loss: 129.2603860CurrentTrain: epoch  1, batch    43 | loss: 135.6099942CurrentTrain: epoch  1, batch    44 | loss: 82.5453551CurrentTrain: epoch  1, batch    45 | loss: 133.5091460CurrentTrain: epoch  1, batch    46 | loss: 102.9824165CurrentTrain: epoch  1, batch    47 | loss: 105.8912492CurrentTrain: epoch  1, batch    48 | loss: 135.5638629CurrentTrain: epoch  1, batch    49 | loss: 71.7558919CurrentTrain: epoch  1, batch    50 | loss: 128.1046969CurrentTrain: epoch  1, batch    51 | loss: 177.7031961CurrentTrain: epoch  1, batch    52 | loss: 133.9925704CurrentTrain: epoch  1, batch    53 | loss: 103.4232316CurrentTrain: epoch  1, batch    54 | loss: 106.1945799CurrentTrain: epoch  1, batch    55 | loss: 103.6686363CurrentTrain: epoch  1, batch    56 | loss: 130.5249886CurrentTrain: epoch  1, batch    57 | loss: 104.0308109CurrentTrain: epoch  1, batch    58 | loss: 174.3373256CurrentTrain: epoch  1, batch    59 | loss: 104.8773685CurrentTrain: epoch  1, batch    60 | loss: 104.5242618CurrentTrain: epoch  1, batch    61 | loss: 177.1201173CurrentTrain: epoch  1, batch    62 | loss: 105.4700018CurrentTrain: epoch  1, batch    63 | loss: 107.4280729CurrentTrain: epoch  1, batch    64 | loss: 87.7412379CurrentTrain: epoch  1, batch    65 | loss: 102.9816192CurrentTrain: epoch  1, batch    66 | loss: 71.0632512CurrentTrain: epoch  1, batch    67 | loss: 90.0076245CurrentTrain: epoch  1, batch    68 | loss: 100.1335817CurrentTrain: epoch  1, batch    69 | loss: 130.9464841CurrentTrain: epoch  1, batch    70 | loss: 131.8894500CurrentTrain: epoch  1, batch    71 | loss: 130.9198222CurrentTrain: epoch  1, batch    72 | loss: 106.0539573CurrentTrain: epoch  1, batch    73 | loss: 101.6451863CurrentTrain: epoch  1, batch    74 | loss: 98.5716303CurrentTrain: epoch  1, batch    75 | loss: 89.3064326CurrentTrain: epoch  1, batch    76 | loss: 84.3507009CurrentTrain: epoch  1, batch    77 | loss: 128.2432739CurrentTrain: epoch  1, batch    78 | loss: 101.7529376CurrentTrain: epoch  1, batch    79 | loss: 91.2862319CurrentTrain: epoch  1, batch    80 | loss: 108.2721489CurrentTrain: epoch  1, batch    81 | loss: 124.1206542CurrentTrain: epoch  1, batch    82 | loss: 130.9935136CurrentTrain: epoch  1, batch    83 | loss: 131.8264245CurrentTrain: epoch  1, batch    84 | loss: 81.2267760CurrentTrain: epoch  1, batch    85 | loss: 179.8080892CurrentTrain: epoch  1, batch    86 | loss: 125.5400103CurrentTrain: epoch  1, batch    87 | loss: 84.7589299CurrentTrain: epoch  1, batch    88 | loss: 133.9709326CurrentTrain: epoch  1, batch    89 | loss: 136.6749935CurrentTrain: epoch  1, batch    90 | loss: 107.8257844CurrentTrain: epoch  1, batch    91 | loss: 132.5547493CurrentTrain: epoch  1, batch    92 | loss: 89.1587441CurrentTrain: epoch  1, batch    93 | loss: 102.4282181CurrentTrain: epoch  1, batch    94 | loss: 106.8784532CurrentTrain: epoch  1, batch    95 | loss: 107.4929830CurrentTrain: epoch  2, batch     0 | loss: 104.3528772CurrentTrain: epoch  2, batch     1 | loss: 174.2711400CurrentTrain: epoch  2, batch     2 | loss: 102.1257147CurrentTrain: epoch  2, batch     3 | loss: 98.4295634CurrentTrain: epoch  2, batch     4 | loss: 132.7324935CurrentTrain: epoch  2, batch     5 | loss: 132.6415886CurrentTrain: epoch  2, batch     6 | loss: 101.1352141CurrentTrain: epoch  2, batch     7 | loss: 135.7585181CurrentTrain: epoch  2, batch     8 | loss: 86.3940344CurrentTrain: epoch  2, batch     9 | loss: 100.9154803CurrentTrain: epoch  2, batch    10 | loss: 103.0347048CurrentTrain: epoch  2, batch    11 | loss: 85.6714475CurrentTrain: epoch  2, batch    12 | loss: 106.2921765CurrentTrain: epoch  2, batch    13 | loss: 102.0553098CurrentTrain: epoch  2, batch    14 | loss: 128.3249352CurrentTrain: epoch  2, batch    15 | loss: 128.8552108CurrentTrain: epoch  2, batch    16 | loss: 103.9788218CurrentTrain: epoch  2, batch    17 | loss: 106.1183643CurrentTrain: epoch  2, batch    18 | loss: 103.3002161CurrentTrain: epoch  2, batch    19 | loss: 98.4081049CurrentTrain: epoch  2, batch    20 | loss: 100.7450756CurrentTrain: epoch  2, batch    21 | loss: 82.1115332CurrentTrain: epoch  2, batch    22 | loss: 124.0315193CurrentTrain: epoch  2, batch    23 | loss: 100.2171080CurrentTrain: epoch  2, batch    24 | loss: 175.8540744CurrentTrain: epoch  2, batch    25 | loss: 103.1433452CurrentTrain: epoch  2, batch    26 | loss: 92.1081696CurrentTrain: epoch  2, batch    27 | loss: 102.2997009CurrentTrain: epoch  2, batch    28 | loss: 273.4635546CurrentTrain: epoch  2, batch    29 | loss: 84.7456303CurrentTrain: epoch  2, batch    30 | loss: 103.9713118CurrentTrain: epoch  2, batch    31 | loss: 107.7529273CurrentTrain: epoch  2, batch    32 | loss: 83.6233113CurrentTrain: epoch  2, batch    33 | loss: 100.1610453CurrentTrain: epoch  2, batch    34 | loss: 99.8808270CurrentTrain: epoch  2, batch    35 | loss: 88.4217979CurrentTrain: epoch  2, batch    36 | loss: 84.6230918CurrentTrain: epoch  2, batch    37 | loss: 82.6049815CurrentTrain: epoch  2, batch    38 | loss: 96.7637660CurrentTrain: epoch  2, batch    39 | loss: 123.3996627CurrentTrain: epoch  2, batch    40 | loss: 101.7451192CurrentTrain: epoch  2, batch    41 | loss: 101.6265272CurrentTrain: epoch  2, batch    42 | loss: 99.5603261CurrentTrain: epoch  2, batch    43 | loss: 128.9122310CurrentTrain: epoch  2, batch    44 | loss: 105.0429908CurrentTrain: epoch  2, batch    45 | loss: 87.9576562CurrentTrain: epoch  2, batch    46 | loss: 126.6432573CurrentTrain: epoch  2, batch    47 | loss: 106.5573710CurrentTrain: epoch  2, batch    48 | loss: 129.6200396CurrentTrain: epoch  2, batch    49 | loss: 103.1596746CurrentTrain: epoch  2, batch    50 | loss: 85.9543189CurrentTrain: epoch  2, batch    51 | loss: 129.9886785CurrentTrain: epoch  2, batch    52 | loss: 99.6157585CurrentTrain: epoch  2, batch    53 | loss: 105.9564541CurrentTrain: epoch  2, batch    54 | loss: 71.1958276CurrentTrain: epoch  2, batch    55 | loss: 103.1907330CurrentTrain: epoch  2, batch    56 | loss: 103.2029114CurrentTrain: epoch  2, batch    57 | loss: 98.2653759CurrentTrain: epoch  2, batch    58 | loss: 128.6418228CurrentTrain: epoch  2, batch    59 | loss: 106.2295748CurrentTrain: epoch  2, batch    60 | loss: 100.0769986CurrentTrain: epoch  2, batch    61 | loss: 120.0021713CurrentTrain: epoch  2, batch    62 | loss: 77.8469688CurrentTrain: epoch  2, batch    63 | loss: 133.6311499CurrentTrain: epoch  2, batch    64 | loss: 87.9189362CurrentTrain: epoch  2, batch    65 | loss: 83.7224468CurrentTrain: epoch  2, batch    66 | loss: 81.9592153CurrentTrain: epoch  2, batch    67 | loss: 129.7237780CurrentTrain: epoch  2, batch    68 | loss: 126.0406502CurrentTrain: epoch  2, batch    69 | loss: 103.3001385CurrentTrain: epoch  2, batch    70 | loss: 106.0001159CurrentTrain: epoch  2, batch    71 | loss: 129.7732802CurrentTrain: epoch  2, batch    72 | loss: 101.9729806CurrentTrain: epoch  2, batch    73 | loss: 102.0463403CurrentTrain: epoch  2, batch    74 | loss: 102.4250049CurrentTrain: epoch  2, batch    75 | loss: 101.6664353CurrentTrain: epoch  2, batch    76 | loss: 104.8582464CurrentTrain: epoch  2, batch    77 | loss: 84.9367667CurrentTrain: epoch  2, batch    78 | loss: 93.4823351CurrentTrain: epoch  2, batch    79 | loss: 112.9815652CurrentTrain: epoch  2, batch    80 | loss: 182.1006518CurrentTrain: epoch  2, batch    81 | loss: 82.0541556CurrentTrain: epoch  2, batch    82 | loss: 84.1265229CurrentTrain: epoch  2, batch    83 | loss: 81.1175922CurrentTrain: epoch  2, batch    84 | loss: 82.6432550CurrentTrain: epoch  2, batch    85 | loss: 97.8048205CurrentTrain: epoch  2, batch    86 | loss: 83.4950401CurrentTrain: epoch  2, batch    87 | loss: 129.2512361CurrentTrain: epoch  2, batch    88 | loss: 138.2752043CurrentTrain: epoch  2, batch    89 | loss: 86.5884770CurrentTrain: epoch  2, batch    90 | loss: 81.7581261CurrentTrain: epoch  2, batch    91 | loss: 85.5367417CurrentTrain: epoch  2, batch    92 | loss: 127.5042701CurrentTrain: epoch  2, batch    93 | loss: 129.5919007CurrentTrain: epoch  2, batch    94 | loss: 101.1958821CurrentTrain: epoch  2, batch    95 | loss: 85.3696129CurrentTrain: epoch  3, batch     0 | loss: 125.2568768CurrentTrain: epoch  3, batch     1 | loss: 128.7135245CurrentTrain: epoch  3, batch     2 | loss: 79.3679905CurrentTrain: epoch  3, batch     3 | loss: 81.0604578CurrentTrain: epoch  3, batch     4 | loss: 127.6637605CurrentTrain: epoch  3, batch     5 | loss: 129.5917543CurrentTrain: epoch  3, batch     6 | loss: 80.3636121CurrentTrain: epoch  3, batch     7 | loss: 83.9826700CurrentTrain: epoch  3, batch     8 | loss: 85.2027019CurrentTrain: epoch  3, batch     9 | loss: 76.5496893CurrentTrain: epoch  3, batch    10 | loss: 100.0243055CurrentTrain: epoch  3, batch    11 | loss: 103.9483783CurrentTrain: epoch  3, batch    12 | loss: 175.9090515CurrentTrain: epoch  3, batch    13 | loss: 99.0975406CurrentTrain: epoch  3, batch    14 | loss: 82.1670704CurrentTrain: epoch  3, batch    15 | loss: 102.9767568CurrentTrain: epoch  3, batch    16 | loss: 85.8967411CurrentTrain: epoch  3, batch    17 | loss: 98.7165447CurrentTrain: epoch  3, batch    18 | loss: 101.9941851CurrentTrain: epoch  3, batch    19 | loss: 84.3954917CurrentTrain: epoch  3, batch    20 | loss: 67.5563061CurrentTrain: epoch  3, batch    21 | loss: 131.4653396CurrentTrain: epoch  3, batch    22 | loss: 99.4845684CurrentTrain: epoch  3, batch    23 | loss: 101.3047885CurrentTrain: epoch  3, batch    24 | loss: 127.2092691CurrentTrain: epoch  3, batch    25 | loss: 171.5375596CurrentTrain: epoch  3, batch    26 | loss: 87.1741859CurrentTrain: epoch  3, batch    27 | loss: 96.7137407CurrentTrain: epoch  3, batch    28 | loss: 99.6721420CurrentTrain: epoch  3, batch    29 | loss: 127.5760052CurrentTrain: epoch  3, batch    30 | loss: 130.6296167CurrentTrain: epoch  3, batch    31 | loss: 77.3709352CurrentTrain: epoch  3, batch    32 | loss: 99.9904158CurrentTrain: epoch  3, batch    33 | loss: 99.9285661CurrentTrain: epoch  3, batch    34 | loss: 126.8456635CurrentTrain: epoch  3, batch    35 | loss: 99.3952440CurrentTrain: epoch  3, batch    36 | loss: 180.2216023CurrentTrain: epoch  3, batch    37 | loss: 78.5515451CurrentTrain: epoch  3, batch    38 | loss: 128.9250048CurrentTrain: epoch  3, batch    39 | loss: 80.3214521CurrentTrain: epoch  3, batch    40 | loss: 86.4921061CurrentTrain: epoch  3, batch    41 | loss: 83.0470295CurrentTrain: epoch  3, batch    42 | loss: 94.6331988CurrentTrain: epoch  3, batch    43 | loss: 127.7174709CurrentTrain: epoch  3, batch    44 | loss: 128.4982243CurrentTrain: epoch  3, batch    45 | loss: 97.2129064CurrentTrain: epoch  3, batch    46 | loss: 99.8072064CurrentTrain: epoch  3, batch    47 | loss: 66.9260193CurrentTrain: epoch  3, batch    48 | loss: 93.8317883CurrentTrain: epoch  3, batch    49 | loss: 72.5970605CurrentTrain: epoch  3, batch    50 | loss: 100.7011730CurrentTrain: epoch  3, batch    51 | loss: 99.8297912CurrentTrain: epoch  3, batch    52 | loss: 82.0953300CurrentTrain: epoch  3, batch    53 | loss: 101.7332049CurrentTrain: epoch  3, batch    54 | loss: 104.9591120CurrentTrain: epoch  3, batch    55 | loss: 121.6814853CurrentTrain: epoch  3, batch    56 | loss: 88.5260326CurrentTrain: epoch  3, batch    57 | loss: 172.5799893CurrentTrain: epoch  3, batch    58 | loss: 131.7417904CurrentTrain: epoch  3, batch    59 | loss: 96.4960239CurrentTrain: epoch  3, batch    60 | loss: 178.5951329CurrentTrain: epoch  3, batch    61 | loss: 177.5259657CurrentTrain: epoch  3, batch    62 | loss: 122.7352353CurrentTrain: epoch  3, batch    63 | loss: 83.3145979CurrentTrain: epoch  3, batch    64 | loss: 170.6038969CurrentTrain: epoch  3, batch    65 | loss: 172.5190590CurrentTrain: epoch  3, batch    66 | loss: 79.3935193CurrentTrain: epoch  3, batch    67 | loss: 126.1856107CurrentTrain: epoch  3, batch    68 | loss: 129.7361031CurrentTrain: epoch  3, batch    69 | loss: 135.4632888CurrentTrain: epoch  3, batch    70 | loss: 101.9005734CurrentTrain: epoch  3, batch    71 | loss: 97.6085594CurrentTrain: epoch  3, batch    72 | loss: 82.6295252CurrentTrain: epoch  3, batch    73 | loss: 99.3350667CurrentTrain: epoch  3, batch    74 | loss: 77.8634413CurrentTrain: epoch  3, batch    75 | loss: 177.5335612CurrentTrain: epoch  3, batch    76 | loss: 133.6576600CurrentTrain: epoch  3, batch    77 | loss: 132.9041882CurrentTrain: epoch  3, batch    78 | loss: 82.5502946CurrentTrain: epoch  3, batch    79 | loss: 64.3993863CurrentTrain: epoch  3, batch    80 | loss: 76.4648062CurrentTrain: epoch  3, batch    81 | loss: 180.2068384CurrentTrain: epoch  3, batch    82 | loss: 132.5927983CurrentTrain: epoch  3, batch    83 | loss: 69.4903267CurrentTrain: epoch  3, batch    84 | loss: 180.8522644CurrentTrain: epoch  3, batch    85 | loss: 95.5207351CurrentTrain: epoch  3, batch    86 | loss: 95.5494863CurrentTrain: epoch  3, batch    87 | loss: 131.0610863CurrentTrain: epoch  3, batch    88 | loss: 102.6823301CurrentTrain: epoch  3, batch    89 | loss: 180.0965829CurrentTrain: epoch  3, batch    90 | loss: 99.0180360CurrentTrain: epoch  3, batch    91 | loss: 126.1057802CurrentTrain: epoch  3, batch    92 | loss: 101.9848372CurrentTrain: epoch  3, batch    93 | loss: 122.9436822CurrentTrain: epoch  3, batch    94 | loss: 129.4005201CurrentTrain: epoch  3, batch    95 | loss: 72.0309170CurrentTrain: epoch  4, batch     0 | loss: 97.5613369CurrentTrain: epoch  4, batch     1 | loss: 77.4639477CurrentTrain: epoch  4, batch     2 | loss: 126.0076458CurrentTrain: epoch  4, batch     3 | loss: 82.7148985CurrentTrain: epoch  4, batch     4 | loss: 101.0546969CurrentTrain: epoch  4, batch     5 | loss: 82.0191098CurrentTrain: epoch  4, batch     6 | loss: 128.7253312CurrentTrain: epoch  4, batch     7 | loss: 82.3035417CurrentTrain: epoch  4, batch     8 | loss: 130.0047601CurrentTrain: epoch  4, batch     9 | loss: 68.2142483CurrentTrain: epoch  4, batch    10 | loss: 98.3389459CurrentTrain: epoch  4, batch    11 | loss: 102.4064342CurrentTrain: epoch  4, batch    12 | loss: 102.4870498CurrentTrain: epoch  4, batch    13 | loss: 174.2541555CurrentTrain: epoch  4, batch    14 | loss: 122.1631053CurrentTrain: epoch  4, batch    15 | loss: 80.7381175CurrentTrain: epoch  4, batch    16 | loss: 83.2166437CurrentTrain: epoch  4, batch    17 | loss: 96.4841181CurrentTrain: epoch  4, batch    18 | loss: 178.6112827CurrentTrain: epoch  4, batch    19 | loss: 80.7478630CurrentTrain: epoch  4, batch    20 | loss: 127.0273920CurrentTrain: epoch  4, batch    21 | loss: 129.3773956CurrentTrain: epoch  4, batch    22 | loss: 125.0440529CurrentTrain: epoch  4, batch    23 | loss: 168.5258347CurrentTrain: epoch  4, batch    24 | loss: 77.0019695CurrentTrain: epoch  4, batch    25 | loss: 102.5331928CurrentTrain: epoch  4, batch    26 | loss: 99.3020566CurrentTrain: epoch  4, batch    27 | loss: 85.6603003CurrentTrain: epoch  4, batch    28 | loss: 103.3033847CurrentTrain: epoch  4, batch    29 | loss: 101.3164646CurrentTrain: epoch  4, batch    30 | loss: 99.5704623CurrentTrain: epoch  4, batch    31 | loss: 126.3390720CurrentTrain: epoch  4, batch    32 | loss: 98.5582235CurrentTrain: epoch  4, batch    33 | loss: 63.2514465CurrentTrain: epoch  4, batch    34 | loss: 82.3247055CurrentTrain: epoch  4, batch    35 | loss: 127.4784536CurrentTrain: epoch  4, batch    36 | loss: 98.8930124CurrentTrain: epoch  4, batch    37 | loss: 131.1133251CurrentTrain: epoch  4, batch    38 | loss: 67.1934421CurrentTrain: epoch  4, batch    39 | loss: 129.0007515CurrentTrain: epoch  4, batch    40 | loss: 100.3629576CurrentTrain: epoch  4, batch    41 | loss: 102.2396815CurrentTrain: epoch  4, batch    42 | loss: 77.0462919CurrentTrain: epoch  4, batch    43 | loss: 99.0625490CurrentTrain: epoch  4, batch    44 | loss: 95.3328141CurrentTrain: epoch  4, batch    45 | loss: 98.8500515CurrentTrain: epoch  4, batch    46 | loss: 173.4249490CurrentTrain: epoch  4, batch    47 | loss: 127.2471567CurrentTrain: epoch  4, batch    48 | loss: 127.8156187CurrentTrain: epoch  4, batch    49 | loss: 70.3451588CurrentTrain: epoch  4, batch    50 | loss: 80.7985784CurrentTrain: epoch  4, batch    51 | loss: 101.4875089CurrentTrain: epoch  4, batch    52 | loss: 96.8494535CurrentTrain: epoch  4, batch    53 | loss: 68.4491495CurrentTrain: epoch  4, batch    54 | loss: 79.3483119CurrentTrain: epoch  4, batch    55 | loss: 79.3634591CurrentTrain: epoch  4, batch    56 | loss: 179.2300938CurrentTrain: epoch  4, batch    57 | loss: 96.8636364CurrentTrain: epoch  4, batch    58 | loss: 75.4267021CurrentTrain: epoch  4, batch    59 | loss: 81.9681577CurrentTrain: epoch  4, batch    60 | loss: 128.0991196CurrentTrain: epoch  4, batch    61 | loss: 172.3743232CurrentTrain: epoch  4, batch    62 | loss: 132.3035108CurrentTrain: epoch  4, batch    63 | loss: 96.8160695CurrentTrain: epoch  4, batch    64 | loss: 170.6136149CurrentTrain: epoch  4, batch    65 | loss: 95.5678744CurrentTrain: epoch  4, batch    66 | loss: 100.4920582CurrentTrain: epoch  4, batch    67 | loss: 97.4195007CurrentTrain: epoch  4, batch    68 | loss: 79.4512656CurrentTrain: epoch  4, batch    69 | loss: 97.5521926CurrentTrain: epoch  4, batch    70 | loss: 95.8641355CurrentTrain: epoch  4, batch    71 | loss: 98.7148044CurrentTrain: epoch  4, batch    72 | loss: 99.8611306CurrentTrain: epoch  4, batch    73 | loss: 128.5424543CurrentTrain: epoch  4, batch    74 | loss: 79.4969289CurrentTrain: epoch  4, batch    75 | loss: 100.7008713CurrentTrain: epoch  4, batch    76 | loss: 126.8985903CurrentTrain: epoch  4, batch    77 | loss: 81.9801716CurrentTrain: epoch  4, batch    78 | loss: 267.2682116CurrentTrain: epoch  4, batch    79 | loss: 169.5348266CurrentTrain: epoch  4, batch    80 | loss: 99.3069863CurrentTrain: epoch  4, batch    81 | loss: 97.1576330CurrentTrain: epoch  4, batch    82 | loss: 81.5052318CurrentTrain: epoch  4, batch    83 | loss: 99.9390116CurrentTrain: epoch  4, batch    84 | loss: 85.8465033CurrentTrain: epoch  4, batch    85 | loss: 99.8205855CurrentTrain: epoch  4, batch    86 | loss: 95.0228724CurrentTrain: epoch  4, batch    87 | loss: 126.8518425CurrentTrain: epoch  4, batch    88 | loss: 127.6636157CurrentTrain: epoch  4, batch    89 | loss: 123.0848526CurrentTrain: epoch  4, batch    90 | loss: 176.9432680CurrentTrain: epoch  4, batch    91 | loss: 97.7304031CurrentTrain: epoch  4, batch    92 | loss: 122.9587933CurrentTrain: epoch  4, batch    93 | loss: 129.4398994CurrentTrain: epoch  4, batch    94 | loss: 101.7161231CurrentTrain: epoch  4, batch    95 | loss: 58.6803687CurrentTrain: epoch  5, batch     0 | loss: 97.5855255CurrentTrain: epoch  5, batch     1 | loss: 95.6983599CurrentTrain: epoch  5, batch     2 | loss: 80.9571740CurrentTrain: epoch  5, batch     3 | loss: 96.7559325CurrentTrain: epoch  5, batch     4 | loss: 93.9376536CurrentTrain: epoch  5, batch     5 | loss: 269.0055274CurrentTrain: epoch  5, batch     6 | loss: 79.5392938CurrentTrain: epoch  5, batch     7 | loss: 130.6979333CurrentTrain: epoch  5, batch     8 | loss: 171.2861920CurrentTrain: epoch  5, batch     9 | loss: 103.7944845CurrentTrain: epoch  5, batch    10 | loss: 64.3047536CurrentTrain: epoch  5, batch    11 | loss: 96.9736375CurrentTrain: epoch  5, batch    12 | loss: 122.6624349CurrentTrain: epoch  5, batch    13 | loss: 124.9105177CurrentTrain: epoch  5, batch    14 | loss: 68.5601335CurrentTrain: epoch  5, batch    15 | loss: 95.6325871CurrentTrain: epoch  5, batch    16 | loss: 126.6038744CurrentTrain: epoch  5, batch    17 | loss: 173.6567784CurrentTrain: epoch  5, batch    18 | loss: 169.3198178CurrentTrain: epoch  5, batch    19 | loss: 100.5104364CurrentTrain: epoch  5, batch    20 | loss: 132.0264490CurrentTrain: epoch  5, batch    21 | loss: 94.8569557CurrentTrain: epoch  5, batch    22 | loss: 73.0365175CurrentTrain: epoch  5, batch    23 | loss: 127.5485947CurrentTrain: epoch  5, batch    24 | loss: 102.2006637CurrentTrain: epoch  5, batch    25 | loss: 77.6586626CurrentTrain: epoch  5, batch    26 | loss: 176.7799916CurrentTrain: epoch  5, batch    27 | loss: 123.8043933CurrentTrain: epoch  5, batch    28 | loss: 79.1107333CurrentTrain: epoch  5, batch    29 | loss: 100.1180317CurrentTrain: epoch  5, batch    30 | loss: 81.3913942CurrentTrain: epoch  5, batch    31 | loss: 97.8273307CurrentTrain: epoch  5, batch    32 | loss: 65.3039653CurrentTrain: epoch  5, batch    33 | loss: 128.1727440CurrentTrain: epoch  5, batch    34 | loss: 98.3143651CurrentTrain: epoch  5, batch    35 | loss: 78.6331140CurrentTrain: epoch  5, batch    36 | loss: 66.0094887CurrentTrain: epoch  5, batch    37 | loss: 78.4960391CurrentTrain: epoch  5, batch    38 | loss: 92.2882165CurrentTrain: epoch  5, batch    39 | loss: 80.0401362CurrentTrain: epoch  5, batch    40 | loss: 129.2211867CurrentTrain: epoch  5, batch    41 | loss: 84.4147785CurrentTrain: epoch  5, batch    42 | loss: 84.3442567CurrentTrain: epoch  5, batch    43 | loss: 83.1342446CurrentTrain: epoch  5, batch    44 | loss: 125.2287517CurrentTrain: epoch  5, batch    45 | loss: 99.1020770CurrentTrain: epoch  5, batch    46 | loss: 126.8498044CurrentTrain: epoch  5, batch    47 | loss: 78.0732006CurrentTrain: epoch  5, batch    48 | loss: 96.3541956CurrentTrain: epoch  5, batch    49 | loss: 266.9494249CurrentTrain: epoch  5, batch    50 | loss: 101.0741266CurrentTrain: epoch  5, batch    51 | loss: 65.8735089CurrentTrain: epoch  5, batch    52 | loss: 101.9223121CurrentTrain: epoch  5, batch    53 | loss: 84.4036546CurrentTrain: epoch  5, batch    54 | loss: 79.3445078CurrentTrain: epoch  5, batch    55 | loss: 101.0809307CurrentTrain: epoch  5, batch    56 | loss: 124.7980824CurrentTrain: epoch  5, batch    57 | loss: 77.9101787CurrentTrain: epoch  5, batch    58 | loss: 95.6366090CurrentTrain: epoch  5, batch    59 | loss: 177.4915500CurrentTrain: epoch  5, batch    60 | loss: 76.5299865CurrentTrain: epoch  5, batch    61 | loss: 127.2388706CurrentTrain: epoch  5, batch    62 | loss: 128.5183335CurrentTrain: epoch  5, batch    63 | loss: 167.7511386CurrentTrain: epoch  5, batch    64 | loss: 176.3982238CurrentTrain: epoch  5, batch    65 | loss: 96.4568538CurrentTrain: epoch  5, batch    66 | loss: 173.0628760CurrentTrain: epoch  5, batch    67 | loss: 122.3404240CurrentTrain: epoch  5, batch    68 | loss: 81.5731155CurrentTrain: epoch  5, batch    69 | loss: 120.2245634CurrentTrain: epoch  5, batch    70 | loss: 95.9901874CurrentTrain: epoch  5, batch    71 | loss: 81.5435313CurrentTrain: epoch  5, batch    72 | loss: 132.3719061CurrentTrain: epoch  5, batch    73 | loss: 125.7831891CurrentTrain: epoch  5, batch    74 | loss: 81.5956020CurrentTrain: epoch  5, batch    75 | loss: 130.1907335CurrentTrain: epoch  5, batch    76 | loss: 101.5368080CurrentTrain: epoch  5, batch    77 | loss: 124.5138678CurrentTrain: epoch  5, batch    78 | loss: 62.5243567CurrentTrain: epoch  5, batch    79 | loss: 102.1812215CurrentTrain: epoch  5, batch    80 | loss: 122.0109655CurrentTrain: epoch  5, batch    81 | loss: 78.6530134CurrentTrain: epoch  5, batch    82 | loss: 125.4195628CurrentTrain: epoch  5, batch    83 | loss: 96.9228115CurrentTrain: epoch  5, batch    84 | loss: 119.2071710CurrentTrain: epoch  5, batch    85 | loss: 272.9134413CurrentTrain: epoch  5, batch    86 | loss: 82.9254219CurrentTrain: epoch  5, batch    87 | loss: 100.0644079CurrentTrain: epoch  5, batch    88 | loss: 100.6185478CurrentTrain: epoch  5, batch    89 | loss: 99.4745939CurrentTrain: epoch  5, batch    90 | loss: 62.7723970CurrentTrain: epoch  5, batch    91 | loss: 98.7698125CurrentTrain: epoch  5, batch    92 | loss: 78.6354982CurrentTrain: epoch  5, batch    93 | loss: 82.1599538CurrentTrain: epoch  5, batch    94 | loss: 96.1151313CurrentTrain: epoch  5, batch    95 | loss: 80.8263441CurrentTrain: epoch  6, batch     0 | loss: 79.5811680CurrentTrain: epoch  6, batch     1 | loss: 100.1496367CurrentTrain: epoch  6, batch     2 | loss: 98.6669471CurrentTrain: epoch  6, batch     3 | loss: 169.6851448CurrentTrain: epoch  6, batch     4 | loss: 81.8118486CurrentTrain: epoch  6, batch     5 | loss: 101.3984526CurrentTrain: epoch  6, batch     6 | loss: 78.0486350CurrentTrain: epoch  6, batch     7 | loss: 76.9510970CurrentTrain: epoch  6, batch     8 | loss: 77.1772341CurrentTrain: epoch  6, batch     9 | loss: 65.7526791CurrentTrain: epoch  6, batch    10 | loss: 126.2511473CurrentTrain: epoch  6, batch    11 | loss: 99.9624355CurrentTrain: epoch  6, batch    12 | loss: 129.3427131CurrentTrain: epoch  6, batch    13 | loss: 81.6284585CurrentTrain: epoch  6, batch    14 | loss: 66.0469691CurrentTrain: epoch  6, batch    15 | loss: 77.3114630CurrentTrain: epoch  6, batch    16 | loss: 120.9241400CurrentTrain: epoch  6, batch    17 | loss: 74.5898578CurrentTrain: epoch  6, batch    18 | loss: 95.9106785CurrentTrain: epoch  6, batch    19 | loss: 96.8943007CurrentTrain: epoch  6, batch    20 | loss: 64.5576004CurrentTrain: epoch  6, batch    21 | loss: 75.8993330CurrentTrain: epoch  6, batch    22 | loss: 165.8888092CurrentTrain: epoch  6, batch    23 | loss: 97.5912191CurrentTrain: epoch  6, batch    24 | loss: 96.5405273CurrentTrain: epoch  6, batch    25 | loss: 96.0892551CurrentTrain: epoch  6, batch    26 | loss: 101.3318445CurrentTrain: epoch  6, batch    27 | loss: 126.0337369CurrentTrain: epoch  6, batch    28 | loss: 78.4728607CurrentTrain: epoch  6, batch    29 | loss: 77.5041549CurrentTrain: epoch  6, batch    30 | loss: 79.1783075CurrentTrain: epoch  6, batch    31 | loss: 64.5715236CurrentTrain: epoch  6, batch    32 | loss: 68.0191252CurrentTrain: epoch  6, batch    33 | loss: 75.7421407CurrentTrain: epoch  6, batch    34 | loss: 96.8892129CurrentTrain: epoch  6, batch    35 | loss: 123.0803816CurrentTrain: epoch  6, batch    36 | loss: 128.6221548CurrentTrain: epoch  6, batch    37 | loss: 76.9790044CurrentTrain: epoch  6, batch    38 | loss: 78.7092580CurrentTrain: epoch  6, batch    39 | loss: 176.3506641CurrentTrain: epoch  6, batch    40 | loss: 95.1105017CurrentTrain: epoch  6, batch    41 | loss: 97.3285293CurrentTrain: epoch  6, batch    42 | loss: 80.6137108CurrentTrain: epoch  6, batch    43 | loss: 124.6322261CurrentTrain: epoch  6, batch    44 | loss: 82.4185454CurrentTrain: epoch  6, batch    45 | loss: 94.9353668CurrentTrain: epoch  6, batch    46 | loss: 75.8886519CurrentTrain: epoch  6, batch    47 | loss: 103.8188225CurrentTrain: epoch  6, batch    48 | loss: 126.8645096CurrentTrain: epoch  6, batch    49 | loss: 77.9233309CurrentTrain: epoch  6, batch    50 | loss: 126.7156454CurrentTrain: epoch  6, batch    51 | loss: 99.8270349CurrentTrain: epoch  6, batch    52 | loss: 124.1850965CurrentTrain: epoch  6, batch    53 | loss: 127.7716375CurrentTrain: epoch  6, batch    54 | loss: 79.6913867CurrentTrain: epoch  6, batch    55 | loss: 97.4935326CurrentTrain: epoch  6, batch    56 | loss: 93.8241521CurrentTrain: epoch  6, batch    57 | loss: 126.4480668CurrentTrain: epoch  6, batch    58 | loss: 176.5380662CurrentTrain: epoch  6, batch    59 | loss: 80.4944910CurrentTrain: epoch  6, batch    60 | loss: 78.4078927CurrentTrain: epoch  6, batch    61 | loss: 129.0345322CurrentTrain: epoch  6, batch    62 | loss: 64.5370252CurrentTrain: epoch  6, batch    63 | loss: 80.8112962CurrentTrain: epoch  6, batch    64 | loss: 78.8791954CurrentTrain: epoch  6, batch    65 | loss: 101.9624512CurrentTrain: epoch  6, batch    66 | loss: 75.9551629CurrentTrain: epoch  6, batch    67 | loss: 177.7244595CurrentTrain: epoch  6, batch    68 | loss: 99.1209327CurrentTrain: epoch  6, batch    69 | loss: 98.1643677CurrentTrain: epoch  6, batch    70 | loss: 61.3774085CurrentTrain: epoch  6, batch    71 | loss: 117.6036201CurrentTrain: epoch  6, batch    72 | loss: 124.9989517CurrentTrain: epoch  6, batch    73 | loss: 77.8852033CurrentTrain: epoch  6, batch    74 | loss: 100.9353895CurrentTrain: epoch  6, batch    75 | loss: 130.9198267CurrentTrain: epoch  6, batch    76 | loss: 165.5354949CurrentTrain: epoch  6, batch    77 | loss: 80.0448657CurrentTrain: epoch  6, batch    78 | loss: 131.0107416CurrentTrain: epoch  6, batch    79 | loss: 101.4686352CurrentTrain: epoch  6, batch    80 | loss: 76.0269012CurrentTrain: epoch  6, batch    81 | loss: 78.2783186CurrentTrain: epoch  6, batch    82 | loss: 127.2264983CurrentTrain: epoch  6, batch    83 | loss: 78.9326650CurrentTrain: epoch  6, batch    84 | loss: 104.3047910CurrentTrain: epoch  6, batch    85 | loss: 125.0101866CurrentTrain: epoch  6, batch    86 | loss: 176.4016057CurrentTrain: epoch  6, batch    87 | loss: 176.5241729CurrentTrain: epoch  6, batch    88 | loss: 123.7251774CurrentTrain: epoch  6, batch    89 | loss: 127.4537023CurrentTrain: epoch  6, batch    90 | loss: 98.2624607CurrentTrain: epoch  6, batch    91 | loss: 81.4740218CurrentTrain: epoch  6, batch    92 | loss: 129.5481715CurrentTrain: epoch  6, batch    93 | loss: 262.7238815CurrentTrain: epoch  6, batch    94 | loss: 84.7561157CurrentTrain: epoch  6, batch    95 | loss: 105.7394395CurrentTrain: epoch  7, batch     0 | loss: 177.9613380CurrentTrain: epoch  7, batch     1 | loss: 77.4123472CurrentTrain: epoch  7, batch     2 | loss: 90.6414997CurrentTrain: epoch  7, batch     3 | loss: 171.2073081CurrentTrain: epoch  7, batch     4 | loss: 66.9748641CurrentTrain: epoch  7, batch     5 | loss: 76.0380282CurrentTrain: epoch  7, batch     6 | loss: 101.0053824CurrentTrain: epoch  7, batch     7 | loss: 98.6838621CurrentTrain: epoch  7, batch     8 | loss: 173.0694091CurrentTrain: epoch  7, batch     9 | loss: 79.8700372CurrentTrain: epoch  7, batch    10 | loss: 64.8844636CurrentTrain: epoch  7, batch    11 | loss: 176.4668182CurrentTrain: epoch  7, batch    12 | loss: 95.8884614CurrentTrain: epoch  7, batch    13 | loss: 127.1736744CurrentTrain: epoch  7, batch    14 | loss: 84.0501604CurrentTrain: epoch  7, batch    15 | loss: 93.7499377CurrentTrain: epoch  7, batch    16 | loss: 131.1019721CurrentTrain: epoch  7, batch    17 | loss: 129.0625290CurrentTrain: epoch  7, batch    18 | loss: 77.6287845CurrentTrain: epoch  7, batch    19 | loss: 93.5371394CurrentTrain: epoch  7, batch    20 | loss: 81.0977676CurrentTrain: epoch  7, batch    21 | loss: 100.7040521CurrentTrain: epoch  7, batch    22 | loss: 103.6607729CurrentTrain: epoch  7, batch    23 | loss: 101.1290651CurrentTrain: epoch  7, batch    24 | loss: 72.7720570CurrentTrain: epoch  7, batch    25 | loss: 95.8947152CurrentTrain: epoch  7, batch    26 | loss: 77.2471299CurrentTrain: epoch  7, batch    27 | loss: 76.8449762CurrentTrain: epoch  7, batch    28 | loss: 78.8745891CurrentTrain: epoch  7, batch    29 | loss: 80.1325837CurrentTrain: epoch  7, batch    30 | loss: 100.6443942CurrentTrain: epoch  7, batch    31 | loss: 96.0314419CurrentTrain: epoch  7, batch    32 | loss: 124.7648486CurrentTrain: epoch  7, batch    33 | loss: 94.8993549CurrentTrain: epoch  7, batch    34 | loss: 124.4332734CurrentTrain: epoch  7, batch    35 | loss: 127.2534331CurrentTrain: epoch  7, batch    36 | loss: 95.2388255CurrentTrain: epoch  7, batch    37 | loss: 101.3967967CurrentTrain: epoch  7, batch    38 | loss: 170.3242863CurrentTrain: epoch  7, batch    39 | loss: 124.1498989CurrentTrain: epoch  7, batch    40 | loss: 75.8988850CurrentTrain: epoch  7, batch    41 | loss: 120.6825985CurrentTrain: epoch  7, batch    42 | loss: 64.9970046CurrentTrain: epoch  7, batch    43 | loss: 74.2803573CurrentTrain: epoch  7, batch    44 | loss: 173.0148009CurrentTrain: epoch  7, batch    45 | loss: 96.7115181CurrentTrain: epoch  7, batch    46 | loss: 91.2264396CurrentTrain: epoch  7, batch    47 | loss: 176.1876138CurrentTrain: epoch  7, batch    48 | loss: 77.3516362CurrentTrain: epoch  7, batch    49 | loss: 123.3305000CurrentTrain: epoch  7, batch    50 | loss: 94.1117132CurrentTrain: epoch  7, batch    51 | loss: 80.2631796CurrentTrain: epoch  7, batch    52 | loss: 102.5057953CurrentTrain: epoch  7, batch    53 | loss: 76.8216002CurrentTrain: epoch  7, batch    54 | loss: 123.2834490CurrentTrain: epoch  7, batch    55 | loss: 96.5872028CurrentTrain: epoch  7, batch    56 | loss: 93.2081784CurrentTrain: epoch  7, batch    57 | loss: 77.2641111CurrentTrain: epoch  7, batch    58 | loss: 97.8315456CurrentTrain: epoch  7, batch    59 | loss: 79.3294601CurrentTrain: epoch  7, batch    60 | loss: 272.7652954CurrentTrain: epoch  7, batch    61 | loss: 78.5446986CurrentTrain: epoch  7, batch    62 | loss: 66.6132614CurrentTrain: epoch  7, batch    63 | loss: 121.1919737CurrentTrain: epoch  7, batch    64 | loss: 93.7796204CurrentTrain: epoch  7, batch    65 | loss: 102.5953190CurrentTrain: epoch  7, batch    66 | loss: 63.7563711CurrentTrain: epoch  7, batch    67 | loss: 126.2637350CurrentTrain: epoch  7, batch    68 | loss: 78.3352719CurrentTrain: epoch  7, batch    69 | loss: 124.9358220CurrentTrain: epoch  7, batch    70 | loss: 99.8362022CurrentTrain: epoch  7, batch    71 | loss: 171.8630858CurrentTrain: epoch  7, batch    72 | loss: 558.9907063CurrentTrain: epoch  7, batch    73 | loss: 98.7839116CurrentTrain: epoch  7, batch    74 | loss: 122.5379497CurrentTrain: epoch  7, batch    75 | loss: 98.0635701CurrentTrain: epoch  7, batch    76 | loss: 129.5389922CurrentTrain: epoch  7, batch    77 | loss: 81.8502179CurrentTrain: epoch  7, batch    78 | loss: 126.6418504CurrentTrain: epoch  7, batch    79 | loss: 164.3325228CurrentTrain: epoch  7, batch    80 | loss: 167.0904983CurrentTrain: epoch  7, batch    81 | loss: 97.3225801CurrentTrain: epoch  7, batch    82 | loss: 171.4198790CurrentTrain: epoch  7, batch    83 | loss: 96.8160891CurrentTrain: epoch  7, batch    84 | loss: 78.8688433CurrentTrain: epoch  7, batch    85 | loss: 99.3327965CurrentTrain: epoch  7, batch    86 | loss: 96.7769809CurrentTrain: epoch  7, batch    87 | loss: 174.1627341CurrentTrain: epoch  7, batch    88 | loss: 262.3591582CurrentTrain: epoch  7, batch    89 | loss: 88.6483026CurrentTrain: epoch  7, batch    90 | loss: 99.3115701CurrentTrain: epoch  7, batch    91 | loss: 97.5846837CurrentTrain: epoch  7, batch    92 | loss: 98.5038548CurrentTrain: epoch  7, batch    93 | loss: 94.9389775CurrentTrain: epoch  7, batch    94 | loss: 100.0607652CurrentTrain: epoch  7, batch    95 | loss: 63.2274000CurrentTrain: epoch  8, batch     0 | loss: 124.3596385CurrentTrain: epoch  8, batch     1 | loss: 73.5609055CurrentTrain: epoch  8, batch     2 | loss: 67.2213371CurrentTrain: epoch  8, batch     3 | loss: 116.9491095CurrentTrain: epoch  8, batch     4 | loss: 67.0386389CurrentTrain: epoch  8, batch     5 | loss: 176.7688354CurrentTrain: epoch  8, batch     6 | loss: 75.3664634CurrentTrain: epoch  8, batch     7 | loss: 100.3071322CurrentTrain: epoch  8, batch     8 | loss: 77.4481189CurrentTrain: epoch  8, batch     9 | loss: 121.9883716CurrentTrain: epoch  8, batch    10 | loss: 75.8804115CurrentTrain: epoch  8, batch    11 | loss: 97.4757633CurrentTrain: epoch  8, batch    12 | loss: 76.8816796CurrentTrain: epoch  8, batch    13 | loss: 167.9805411CurrentTrain: epoch  8, batch    14 | loss: 104.2248747CurrentTrain: epoch  8, batch    15 | loss: 101.1133176CurrentTrain: epoch  8, batch    16 | loss: 96.4782964CurrentTrain: epoch  8, batch    17 | loss: 60.9933332CurrentTrain: epoch  8, batch    18 | loss: 82.0931842CurrentTrain: epoch  8, batch    19 | loss: 96.4626493CurrentTrain: epoch  8, batch    20 | loss: 124.1673948CurrentTrain: epoch  8, batch    21 | loss: 126.2231589CurrentTrain: epoch  8, batch    22 | loss: 95.8000429CurrentTrain: epoch  8, batch    23 | loss: 99.5564258CurrentTrain: epoch  8, batch    24 | loss: 76.5457501CurrentTrain: epoch  8, batch    25 | loss: 126.0346311CurrentTrain: epoch  8, batch    26 | loss: 100.2059398CurrentTrain: epoch  8, batch    27 | loss: 94.7544534CurrentTrain: epoch  8, batch    28 | loss: 79.9331767CurrentTrain: epoch  8, batch    29 | loss: 97.0412279CurrentTrain: epoch  8, batch    30 | loss: 93.7146355CurrentTrain: epoch  8, batch    31 | loss: 126.7658531CurrentTrain: epoch  8, batch    32 | loss: 71.0318586CurrentTrain: epoch  8, batch    33 | loss: 93.6585243CurrentTrain: epoch  8, batch    34 | loss: 129.8930883CurrentTrain: epoch  8, batch    35 | loss: 102.4075411CurrentTrain: epoch  8, batch    36 | loss: 94.7798987CurrentTrain: epoch  8, batch    37 | loss: 95.6678344CurrentTrain: epoch  8, batch    38 | loss: 80.0173288CurrentTrain: epoch  8, batch    39 | loss: 97.2251115CurrentTrain: epoch  8, batch    40 | loss: 172.9470358CurrentTrain: epoch  8, batch    41 | loss: 97.2819288CurrentTrain: epoch  8, batch    42 | loss: 99.7266407CurrentTrain: epoch  8, batch    43 | loss: 97.1198485CurrentTrain: epoch  8, batch    44 | loss: 121.9974969CurrentTrain: epoch  8, batch    45 | loss: 122.2132680CurrentTrain: epoch  8, batch    46 | loss: 170.0687777CurrentTrain: epoch  8, batch    47 | loss: 92.1208255CurrentTrain: epoch  8, batch    48 | loss: 81.6437599CurrentTrain: epoch  8, batch    49 | loss: 99.1513222CurrentTrain: epoch  8, batch    50 | loss: 62.2685342CurrentTrain: epoch  8, batch    51 | loss: 126.1281042CurrentTrain: epoch  8, batch    52 | loss: 77.2388996CurrentTrain: epoch  8, batch    53 | loss: 76.7908689CurrentTrain: epoch  8, batch    54 | loss: 64.5533629CurrentTrain: epoch  8, batch    55 | loss: 273.2986111CurrentTrain: epoch  8, batch    56 | loss: 177.8415434CurrentTrain: epoch  8, batch    57 | loss: 273.4319908CurrentTrain: epoch  8, batch    58 | loss: 126.2330614CurrentTrain: epoch  8, batch    59 | loss: 134.5903648CurrentTrain: epoch  8, batch    60 | loss: 167.6904269CurrentTrain: epoch  8, batch    61 | loss: 65.8717402CurrentTrain: epoch  8, batch    62 | loss: 272.4787167CurrentTrain: epoch  8, batch    63 | loss: 118.1828018CurrentTrain: epoch  8, batch    64 | loss: 80.7038286CurrentTrain: epoch  8, batch    65 | loss: 99.3911074CurrentTrain: epoch  8, batch    66 | loss: 167.1408264CurrentTrain: epoch  8, batch    67 | loss: 97.7265736CurrentTrain: epoch  8, batch    68 | loss: 78.6849034CurrentTrain: epoch  8, batch    69 | loss: 173.0880124CurrentTrain: epoch  8, batch    70 | loss: 96.7314611CurrentTrain: epoch  8, batch    71 | loss: 77.1746139CurrentTrain: epoch  8, batch    72 | loss: 99.3233414CurrentTrain: epoch  8, batch    73 | loss: 94.0162635CurrentTrain: epoch  8, batch    74 | loss: 127.5786776CurrentTrain: epoch  8, batch    75 | loss: 123.6622411CurrentTrain: epoch  8, batch    76 | loss: 170.0073280CurrentTrain: epoch  8, batch    77 | loss: 124.5884927CurrentTrain: epoch  8, batch    78 | loss: 120.3914939CurrentTrain: epoch  8, batch    79 | loss: 97.7445055CurrentTrain: epoch  8, batch    80 | loss: 78.4309550CurrentTrain: epoch  8, batch    81 | loss: 81.1113595CurrentTrain: epoch  8, batch    82 | loss: 94.8990327CurrentTrain: epoch  8, batch    83 | loss: 79.9417395CurrentTrain: epoch  8, batch    84 | loss: 78.2380904CurrentTrain: epoch  8, batch    85 | loss: 123.2338730CurrentTrain: epoch  8, batch    86 | loss: 80.4654926CurrentTrain: epoch  8, batch    87 | loss: 121.6613652CurrentTrain: epoch  8, batch    88 | loss: 77.3108920CurrentTrain: epoch  8, batch    89 | loss: 124.2179064CurrentTrain: epoch  8, batch    90 | loss: 99.6450952CurrentTrain: epoch  8, batch    91 | loss: 97.4154179CurrentTrain: epoch  8, batch    92 | loss: 96.7162188CurrentTrain: epoch  8, batch    93 | loss: 97.2480112CurrentTrain: epoch  8, batch    94 | loss: 174.5622741CurrentTrain: epoch  8, batch    95 | loss: 144.3896540CurrentTrain: epoch  9, batch     0 | loss: 96.9136316CurrentTrain: epoch  9, batch     1 | loss: 75.4155975CurrentTrain: epoch  9, batch     2 | loss: 128.9650455CurrentTrain: epoch  9, batch     3 | loss: 75.9980406CurrentTrain: epoch  9, batch     4 | loss: 124.0426283CurrentTrain: epoch  9, batch     5 | loss: 123.7696965CurrentTrain: epoch  9, batch     6 | loss: 169.9776756CurrentTrain: epoch  9, batch     7 | loss: 176.4794432CurrentTrain: epoch  9, batch     8 | loss: 177.3839494CurrentTrain: epoch  9, batch     9 | loss: 125.0465897CurrentTrain: epoch  9, batch    10 | loss: 100.1358962CurrentTrain: epoch  9, batch    11 | loss: 76.4883692CurrentTrain: epoch  9, batch    12 | loss: 127.2996557CurrentTrain: epoch  9, batch    13 | loss: 82.5471332CurrentTrain: epoch  9, batch    14 | loss: 102.2829369CurrentTrain: epoch  9, batch    15 | loss: 96.3653068CurrentTrain: epoch  9, batch    16 | loss: 122.3372047CurrentTrain: epoch  9, batch    17 | loss: 94.9499794CurrentTrain: epoch  9, batch    18 | loss: 95.0986404CurrentTrain: epoch  9, batch    19 | loss: 76.7057756CurrentTrain: epoch  9, batch    20 | loss: 78.2286366CurrentTrain: epoch  9, batch    21 | loss: 172.8069925CurrentTrain: epoch  9, batch    22 | loss: 76.4100350CurrentTrain: epoch  9, batch    23 | loss: 93.9287770CurrentTrain: epoch  9, batch    24 | loss: 74.7569445CurrentTrain: epoch  9, batch    25 | loss: 95.9252044CurrentTrain: epoch  9, batch    26 | loss: 94.6531099CurrentTrain: epoch  9, batch    27 | loss: 124.2386442CurrentTrain: epoch  9, batch    28 | loss: 172.9726404CurrentTrain: epoch  9, batch    29 | loss: 83.6192806CurrentTrain: epoch  9, batch    30 | loss: 96.7243787CurrentTrain: epoch  9, batch    31 | loss: 95.3475266CurrentTrain: epoch  9, batch    32 | loss: 77.2551261CurrentTrain: epoch  9, batch    33 | loss: 64.9986127CurrentTrain: epoch  9, batch    34 | loss: 170.2974763CurrentTrain: epoch  9, batch    35 | loss: 124.8878960CurrentTrain: epoch  9, batch    36 | loss: 75.2341534CurrentTrain: epoch  9, batch    37 | loss: 125.0916725CurrentTrain: epoch  9, batch    38 | loss: 177.0657953CurrentTrain: epoch  9, batch    39 | loss: 98.4067954CurrentTrain: epoch  9, batch    40 | loss: 90.1851860CurrentTrain: epoch  9, batch    41 | loss: 98.2283479CurrentTrain: epoch  9, batch    42 | loss: 126.2995539CurrentTrain: epoch  9, batch    43 | loss: 80.6147148CurrentTrain: epoch  9, batch    44 | loss: 124.0923782CurrentTrain: epoch  9, batch    45 | loss: 126.8301344CurrentTrain: epoch  9, batch    46 | loss: 266.5588421CurrentTrain: epoch  9, batch    47 | loss: 95.3253333CurrentTrain: epoch  9, batch    48 | loss: 101.5321146CurrentTrain: epoch  9, batch    49 | loss: 94.8684921CurrentTrain: epoch  9, batch    50 | loss: 126.2637494CurrentTrain: epoch  9, batch    51 | loss: 100.1293646CurrentTrain: epoch  9, batch    52 | loss: 176.7433545CurrentTrain: epoch  9, batch    53 | loss: 95.3953160CurrentTrain: epoch  9, batch    54 | loss: 95.6654981CurrentTrain: epoch  9, batch    55 | loss: 98.2624296CurrentTrain: epoch  9, batch    56 | loss: 79.6936426CurrentTrain: epoch  9, batch    57 | loss: 76.0538108CurrentTrain: epoch  9, batch    58 | loss: 128.6522571CurrentTrain: epoch  9, batch    59 | loss: 124.0071606CurrentTrain: epoch  9, batch    60 | loss: 74.5838999CurrentTrain: epoch  9, batch    61 | loss: 64.2879131CurrentTrain: epoch  9, batch    62 | loss: 132.5685437CurrentTrain: epoch  9, batch    63 | loss: 133.8332542CurrentTrain: epoch  9, batch    64 | loss: 76.0707608CurrentTrain: epoch  9, batch    65 | loss: 95.2230901CurrentTrain: epoch  9, batch    66 | loss: 120.2811962CurrentTrain: epoch  9, batch    67 | loss: 74.5033196CurrentTrain: epoch  9, batch    68 | loss: 100.1277250CurrentTrain: epoch  9, batch    69 | loss: 78.2217878CurrentTrain: epoch  9, batch    70 | loss: 129.7299900CurrentTrain: epoch  9, batch    71 | loss: 75.8201223CurrentTrain: epoch  9, batch    72 | loss: 124.1921522CurrentTrain: epoch  9, batch    73 | loss: 89.7314980CurrentTrain: epoch  9, batch    74 | loss: 96.1319006CurrentTrain: epoch  9, batch    75 | loss: 79.7533824CurrentTrain: epoch  9, batch    76 | loss: 94.7621683CurrentTrain: epoch  9, batch    77 | loss: 73.2488803CurrentTrain: epoch  9, batch    78 | loss: 95.7991531CurrentTrain: epoch  9, batch    79 | loss: 124.7110447CurrentTrain: epoch  9, batch    80 | loss: 93.4732358CurrentTrain: epoch  9, batch    81 | loss: 124.3331862CurrentTrain: epoch  9, batch    82 | loss: 79.6619270CurrentTrain: epoch  9, batch    83 | loss: 96.2079504CurrentTrain: epoch  9, batch    84 | loss: 79.5777045CurrentTrain: epoch  9, batch    85 | loss: 81.5541031CurrentTrain: epoch  9, batch    86 | loss: 97.2461679CurrentTrain: epoch  9, batch    87 | loss: 77.0047285CurrentTrain: epoch  9, batch    88 | loss: 124.3002644CurrentTrain: epoch  9, batch    89 | loss: 95.4855196CurrentTrain: epoch  9, batch    90 | loss: 83.1557199CurrentTrain: epoch  9, batch    91 | loss: 95.8320209CurrentTrain: epoch  9, batch    92 | loss: 96.6151013CurrentTrain: epoch  9, batch    93 | loss: 94.0800459CurrentTrain: epoch  9, batch    94 | loss: 94.8139465CurrentTrain: epoch  9, batch    95 | loss: 79.7344871

F1 score per class: {32: 0.6436781609195402, 6: 0.88268156424581, 19: 0.5925925925925926, 24: 0.7684210526315789, 26: 0.9361702127659575, 29: 0.9081632653061225}
Micro-average F1 score: 0.8238993710691824
Weighted-average F1 score: 0.8289493296936524
F1 score per class: {32: 0.7319587628865979, 6: 0.925531914893617, 19: 0.5517241379310345, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8979591836734694}
Micro-average F1 score: 0.8493427704752275
Weighted-average F1 score: 0.8514246670831772
F1 score per class: {32: 0.7319587628865979, 6: 0.925531914893617, 19: 0.5517241379310345, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8979591836734694}
Micro-average F1 score: 0.8493427704752275
Weighted-average F1 score: 0.8514246670831772

F1 score per class: {32: 0.6436781609195402, 6: 0.88268156424581, 19: 0.5925925925925926, 24: 0.7684210526315789, 26: 0.9361702127659575, 29: 0.9081632653061225}
Micro-average F1 score: 0.8238993710691824
Weighted-average F1 score: 0.8289493296936524
F1 score per class: {32: 0.7319587628865979, 6: 0.925531914893617, 19: 0.5517241379310345, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8979591836734694}
Micro-average F1 score: 0.8493427704752275
Weighted-average F1 score: 0.8514246670831772
F1 score per class: {32: 0.7319587628865979, 6: 0.925531914893617, 19: 0.5517241379310345, 24: 0.7659574468085106, 26: 0.9690721649484536, 29: 0.8979591836734694}
Micro-average F1 score: 0.8493427704752275
Weighted-average F1 score: 0.8514246670831772

F1 score per class: {32: 0.4745762711864407, 6: 0.6396761133603239, 19: 0.2222222222222222, 24: 0.7192118226600985, 26: 0.9025641025641026, 29: 0.8165137614678899}
Micro-average F1 score: 0.6712211784799317
Weighted-average F1 score: 0.6535157624442794
F1 score per class: {32: 0.48299319727891155, 6: 0.6304347826086957, 19: 0.1415929203539823, 24: 0.6728971962616822, 26: 0.9082125603864735, 29: 0.7426160337552743}
Micro-average F1 score: 0.6263982102908278
Weighted-average F1 score: 0.5999221754611478
F1 score per class: {32: 0.48299319727891155, 6: 0.6304347826086957, 19: 0.1415929203539823, 24: 0.6728971962616822, 26: 0.9082125603864735, 29: 0.7426160337552743}
Micro-average F1 score: 0.6263982102908278
Weighted-average F1 score: 0.5999221754611478

F1 score per class: {32: 0.4745762711864407, 6: 0.6396761133603239, 19: 0.2222222222222222, 24: 0.7192118226600985, 26: 0.9025641025641026, 29: 0.8165137614678899}
Micro-average F1 score: 0.6712211784799317
Weighted-average F1 score: 0.6535157624442794
F1 score per class: {32: 0.48299319727891155, 6: 0.6304347826086957, 19: 0.1415929203539823, 24: 0.6728971962616822, 26: 0.9082125603864735, 29: 0.7426160337552743}
Micro-average F1 score: 0.6263982102908278
Weighted-average F1 score: 0.5999221754611478
F1 score per class: {32: 0.48299319727891155, 6: 0.6304347826086957, 19: 0.1415929203539823, 24: 0.6728971962616822, 26: 0.9082125603864735, 29: 0.7426160337552743}
Micro-average F1 score: 0.6263982102908278
Weighted-average F1 score: 0.5999221754611478
cur_acc_wo_na:  ['0.8239']
his_acc_wo_na:  ['0.8239']
cur_acc des_wo_na:  ['0.8493']
his_acc des_wo_na:  ['0.8493']
cur_acc rrf_wo_na:  ['0.8493']
his_acc rrf_wo_na:  ['0.8493']
cur_acc_w_na:  ['0.6712']
his_acc_w_na:  ['0.6712']
cur_acc des_w_na:  ['0.6264']
his_acc des_w_na:  ['0.6264']
cur_acc rrf_w_na:  ['0.6264']
his_acc rrf_w_na:  ['0.6264']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 121.3385996CurrentTrain: epoch  0, batch     1 | loss: 101.3670552CurrentTrain: epoch  0, batch     2 | loss: 119.0959233CurrentTrain: epoch  0, batch     3 | loss: 64.7270642CurrentTrain: epoch  1, batch     0 | loss: 115.6849777CurrentTrain: epoch  1, batch     1 | loss: 137.7870018CurrentTrain: epoch  1, batch     2 | loss: 111.3524033CurrentTrain: epoch  1, batch     3 | loss: 81.5404071CurrentTrain: epoch  2, batch     0 | loss: 93.0937054CurrentTrain: epoch  2, batch     1 | loss: 104.5108664CurrentTrain: epoch  2, batch     2 | loss: 111.9203389CurrentTrain: epoch  2, batch     3 | loss: 83.5950690CurrentTrain: epoch  3, batch     0 | loss: 86.7200310CurrentTrain: epoch  3, batch     1 | loss: 90.8030029CurrentTrain: epoch  3, batch     2 | loss: 88.0972960CurrentTrain: epoch  3, batch     3 | loss: 163.6533345CurrentTrain: epoch  4, batch     0 | loss: 106.7339485CurrentTrain: epoch  4, batch     1 | loss: 131.7279838CurrentTrain: epoch  4, batch     2 | loss: 82.3245038CurrentTrain: epoch  4, batch     3 | loss: 80.8940882CurrentTrain: epoch  5, batch     0 | loss: 84.2225092CurrentTrain: epoch  5, batch     1 | loss: 104.5707368CurrentTrain: epoch  5, batch     2 | loss: 101.6034270CurrentTrain: epoch  5, batch     3 | loss: 61.6177851CurrentTrain: epoch  6, batch     0 | loss: 102.9797683CurrentTrain: epoch  6, batch     1 | loss: 94.9977972CurrentTrain: epoch  6, batch     2 | loss: 103.1107773CurrentTrain: epoch  6, batch     3 | loss: 103.5545832CurrentTrain: epoch  7, batch     0 | loss: 84.2280680CurrentTrain: epoch  7, batch     1 | loss: 73.6904188CurrentTrain: epoch  7, batch     2 | loss: 128.3939018CurrentTrain: epoch  7, batch     3 | loss: 105.0562899CurrentTrain: epoch  8, batch     0 | loss: 97.2613249CurrentTrain: epoch  8, batch     1 | loss: 79.0378299CurrentTrain: epoch  8, batch     2 | loss: 128.4700338CurrentTrain: epoch  8, batch     3 | loss: 71.1659783CurrentTrain: epoch  9, batch     0 | loss: 77.0058959CurrentTrain: epoch  9, batch     1 | loss: 100.0982381CurrentTrain: epoch  9, batch     2 | loss: 95.5602619CurrentTrain: epoch  9, batch     3 | loss: 74.6865606
MemoryTrain:  epoch  0, batch     0 | loss: 1.1722164MemoryTrain:  epoch  1, batch     0 | loss: 0.9363075MemoryTrain:  epoch  2, batch     0 | loss: 0.6169462MemoryTrain:  epoch  3, batch     0 | loss: 0.4721907MemoryTrain:  epoch  4, batch     0 | loss: 0.3574526MemoryTrain:  epoch  5, batch     0 | loss: 0.2706140MemoryTrain:  epoch  6, batch     0 | loss: 0.1868682MemoryTrain:  epoch  7, batch     0 | loss: 0.1856997MemoryTrain:  epoch  8, batch     0 | loss: 0.1333033MemoryTrain:  epoch  9, batch     0 | loss: 0.1268844

F1 score per class: {33: 0.5405405405405406, 36: 0.8, 8: 0.0, 20: 0.8823529411764706, 26: 0.42857142857142855, 30: 0.5957446808510638}
Micro-average F1 score: 0.6511627906976745
Weighted-average F1 score: 0.6744962449859058
F1 score per class: {32: 0.0, 33: 0.8, 36: 0.8631578947368421, 6: 0.0, 8: 0.0, 20: 0.9473684210526315, 26: 0.0, 29: 0.42857142857142855, 30: 0.926829268292683}
Micro-average F1 score: 0.8439024390243902
Weighted-average F1 score: 0.8419243366626131
F1 score per class: {32: 0.8088235294117647, 33: 0.8631578947368421, 36: 0.0, 8: 0.0, 20: 0.972972972972973, 26: 0.0, 29: 0.42857142857142855, 30: 0.9193548387096774}
Micro-average F1 score: 0.8508557457212714
Weighted-average F1 score: 0.8539133496980151

F1 score per class: {32: 0.5165562913907285, 33: 0.5128205128205128, 36: 0.8888888888888888, 6: 0.8, 8: 0.2, 19: 0.7741935483870968, 20: 0.8888888888888888, 24: 0.8823529411764706, 26: 0.9072164948453608, 29: 0.42857142857142855, 30: 0.5957446808510638}
Micro-average F1 score: 0.7507936507936508
Weighted-average F1 score: 0.7813874494885191
F1 score per class: {32: 0.6914893617021277, 33: 0.7448275862068966, 36: 0.918918918918919, 6: 0.8631578947368421, 8: 0.5925925925925926, 19: 0.7608695652173914, 20: 0.9381443298969072, 24: 0.8, 26: 0.9072164948453608, 29: 0.4, 30: 0.8976377952755905}
Micro-average F1 score: 0.8291636883488206
Weighted-average F1 score: 0.8336674015208086
F1 score per class: {32: 0.655367231638418, 33: 0.7051282051282052, 36: 0.9130434782608695, 6: 0.8631578947368421, 8: 0.5384615384615384, 19: 0.7608695652173914, 20: 0.9375, 24: 0.8571428571428571, 26: 0.9072164948453608, 29: 0.4, 30: 0.8976377952755905}
Micro-average F1 score: 0.8204022988505747
Weighted-average F1 score: 0.8262088578637806

F1 score per class: {32: 0.0, 33: 0.43478260869565216, 36: 0.0, 6: 0.6428571428571429, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.8571428571428571, 26: 0.0, 29: 0.375, 30: 0.5137614678899083}
Micro-average F1 score: 0.47863247863247865
Weighted-average F1 score: 0.42088334099121616
F1 score per class: {32: 0.0, 33: 0.5023255813953489, 36: 0.0, 6: 0.5061728395061729, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.631578947368421, 26: 0.0, 29: 0.3, 30: 0.5615763546798029}
Micro-average F1 score: 0.4511082138200782
Weighted-average F1 score: 0.4209221178447321
F1 score per class: {32: 0.0, 33: 0.5045871559633027, 36: 0.0, 6: 0.5061728395061729, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.7659574468085106, 26: 0.0, 29: 0.35294117647058826, 30: 0.5507246376811594}
Micro-average F1 score: 0.4627659574468085
Weighted-average F1 score: 0.4318843311250373

F1 score per class: {32: 0.37142857142857144, 33: 0.32967032967032966, 36: 0.6666666666666666, 6: 0.5294117647058824, 8: 0.1111111111111111, 19: 0.6792452830188679, 20: 0.8163265306122449, 24: 0.8571428571428571, 26: 0.7892376681614349, 29: 0.35294117647058826, 30: 0.5}
Micro-average F1 score: 0.5916197623514696
Weighted-average F1 score: 0.5920587397493133
F1 score per class: {32: 0.429042904290429, 33: 0.3323076923076923, 36: 0.5782312925170068, 6: 0.3778801843317972, 8: 0.19753086419753085, 19: 0.6306306306306306, 20: 0.8161434977578476, 24: 0.3076923076923077, 26: 0.7364016736401674, 29: 0.18181818181818182, 30: 0.4769874476987448}
Micro-average F1 score: 0.5058874836458788
Weighted-average F1 score: 0.48623683039878574
F1 score per class: {32: 0.4249084249084249, 33: 0.3160919540229885, 36: 0.5813148788927336, 6: 0.3778801843317972, 8: 0.17721518987341772, 19: 0.6334841628959276, 20: 0.8450704225352113, 24: 0.42857142857142855, 26: 0.7394957983193278, 29: 0.2, 30: 0.4634146341463415}
Micro-average F1 score: 0.5102770330652369
Weighted-average F1 score: 0.48988956118255284
cur_acc_wo_na:  ['0.8239', '0.6512']
his_acc_wo_na:  ['0.8239', '0.7508']
cur_acc des_wo_na:  ['0.8493', '0.8439']
his_acc des_wo_na:  ['0.8493', '0.8292']
cur_acc rrf_wo_na:  ['0.8493', '0.8509']
his_acc rrf_wo_na:  ['0.8493', '0.8204']
cur_acc_w_na:  ['0.6712', '0.4786']
his_acc_w_na:  ['0.6712', '0.5916']
cur_acc des_w_na:  ['0.6264', '0.4511']
his_acc des_w_na:  ['0.6264', '0.5059']
cur_acc rrf_w_na:  ['0.6264', '0.4628']
his_acc rrf_w_na:  ['0.6264', '0.5103']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 117.6696703CurrentTrain: epoch  0, batch     1 | loss: 152.9826332CurrentTrain: epoch  0, batch     2 | loss: 93.1834619CurrentTrain: epoch  0, batch     3 | loss: 142.6387991CurrentTrain: epoch  0, batch     4 | loss: 26.3184869CurrentTrain: epoch  1, batch     0 | loss: 111.3317053CurrentTrain: epoch  1, batch     1 | loss: 137.7549925CurrentTrain: epoch  1, batch     2 | loss: 90.7202431CurrentTrain: epoch  1, batch     3 | loss: 140.0358644CurrentTrain: epoch  1, batch     4 | loss: 26.2147477CurrentTrain: epoch  2, batch     0 | loss: 106.5771119CurrentTrain: epoch  2, batch     1 | loss: 82.1925373CurrentTrain: epoch  2, batch     2 | loss: 103.1117676CurrentTrain: epoch  2, batch     3 | loss: 181.9616044CurrentTrain: epoch  2, batch     4 | loss: 29.1113183CurrentTrain: epoch  3, batch     0 | loss: 106.1397562CurrentTrain: epoch  3, batch     1 | loss: 170.9141166CurrentTrain: epoch  3, batch     2 | loss: 129.8962643CurrentTrain: epoch  3, batch     3 | loss: 128.1327568CurrentTrain: epoch  3, batch     4 | loss: 26.1385778CurrentTrain: epoch  4, batch     0 | loss: 102.6027400CurrentTrain: epoch  4, batch     1 | loss: 98.5192602CurrentTrain: epoch  4, batch     2 | loss: 179.7275217CurrentTrain: epoch  4, batch     3 | loss: 83.1151941CurrentTrain: epoch  4, batch     4 | loss: 40.6555319CurrentTrain: epoch  5, batch     0 | loss: 81.6659673CurrentTrain: epoch  5, batch     1 | loss: 131.3884705CurrentTrain: epoch  5, batch     2 | loss: 97.2304136CurrentTrain: epoch  5, batch     3 | loss: 128.7479949CurrentTrain: epoch  5, batch     4 | loss: 41.7062710CurrentTrain: epoch  6, batch     0 | loss: 97.7060093CurrentTrain: epoch  6, batch     1 | loss: 175.4050597CurrentTrain: epoch  6, batch     2 | loss: 83.7017283CurrentTrain: epoch  6, batch     3 | loss: 97.8197404CurrentTrain: epoch  6, batch     4 | loss: 23.9044202CurrentTrain: epoch  7, batch     0 | loss: 129.0956619CurrentTrain: epoch  7, batch     1 | loss: 173.6848156CurrentTrain: epoch  7, batch     2 | loss: 77.3421776CurrentTrain: epoch  7, batch     3 | loss: 78.9038040CurrentTrain: epoch  7, batch     4 | loss: 40.5935612CurrentTrain: epoch  8, batch     0 | loss: 99.5540049CurrentTrain: epoch  8, batch     1 | loss: 123.1056673CurrentTrain: epoch  8, batch     2 | loss: 96.3590457CurrentTrain: epoch  8, batch     3 | loss: 126.8092339CurrentTrain: epoch  8, batch     4 | loss: 36.9046800CurrentTrain: epoch  9, batch     0 | loss: 122.9882504CurrentTrain: epoch  9, batch     1 | loss: 173.4402555CurrentTrain: epoch  9, batch     2 | loss: 82.6565856CurrentTrain: epoch  9, batch     3 | loss: 95.6403795CurrentTrain: epoch  9, batch     4 | loss: 14.4818613
MemoryTrain:  epoch  0, batch     0 | loss: 0.9445697MemoryTrain:  epoch  1, batch     0 | loss: 0.7821633MemoryTrain:  epoch  2, batch     0 | loss: 0.5833867MemoryTrain:  epoch  3, batch     0 | loss: 0.4327762MemoryTrain:  epoch  4, batch     0 | loss: 0.3693348MemoryTrain:  epoch  5, batch     0 | loss: 0.2441306MemoryTrain:  epoch  6, batch     0 | loss: 0.2371522MemoryTrain:  epoch  7, batch     0 | loss: 0.1998701MemoryTrain:  epoch  8, batch     0 | loss: 0.1696987MemoryTrain:  epoch  9, batch     0 | loss: 0.1327056

F1 score per class: {2: 0.9411764705882353, 39: 0.0, 8: 0.592, 11: 0.3870967741935484, 12: 0.0, 19: 0.5, 28: 0.13333333333333333}
Micro-average F1 score: 0.4868421052631579
Weighted-average F1 score: 0.5042007962198162
F1 score per class: {33: 0.9411764705882353, 2: 0.0, 6: 0.0, 39: 0.7862068965517242, 8: 0.6578947368421053, 11: 0.0, 12: 0.0, 19: 0.7692307692307693, 24: 0.0, 28: 0.3333333333333333}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6115547890895388
F1 score per class: {33: 0.9411764705882353, 2: 0.0, 39: 0.7862068965517242, 8: 0.6753246753246753, 11: 0.0, 12: 0.0, 19: 0.625, 24: 0.0, 28: 0.13333333333333333}
Micro-average F1 score: 0.670299727520436
Weighted-average F1 score: 0.6347572342501347

F1 score per class: {32: 0.9411764705882353, 33: 0.5316455696202531, 2: 0.1978021978021978, 36: 0.5362318840579711, 6: 0.3333333333333333, 39: 0.8022598870056498, 8: 0.7865168539325843, 11: 0.2, 12: 0.7513812154696132, 19: 0.14814814814814814, 20: 0.8505747126436781, 24: 0.8571428571428571, 26: 0.8979591836734694, 28: 0.3076923076923077, 29: 0.42857142857142855, 30: 0.13333333333333333}
Micro-average F1 score: 0.6279949558638083
Weighted-average F1 score: 0.664278045065973
F1 score per class: {32: 0.8888888888888888, 33: 0.7058823529411765, 2: 0.5801526717557252, 36: 0.7450980392156863, 6: 0.5524861878453039, 39: 0.8393782383419689, 8: 0.7865168539325843, 11: 0.4, 12: 0.7582417582417582, 19: 0.37037037037037035, 20: 0.88268156424581, 24: 0.8085106382978723, 26: 0.9090909090909091, 28: 0.4, 29: 0.7777777777777778, 30: 0.3}
Micro-average F1 score: 0.7412429378531074
Weighted-average F1 score: 0.7465498709847853
F1 score per class: {32: 0.9411764705882353, 33: 0.6368715083798883, 2: 0.515625, 36: 0.7402597402597403, 6: 0.5591397849462365, 39: 0.8481675392670157, 8: 0.8, 11: 0.4, 12: 0.7540983606557377, 19: 0.23255813953488372, 20: 0.8700564971751412, 24: 0.8837209302325582, 26: 0.9045226130653267, 28: 0.4, 29: 0.7663551401869159, 30: 0.11764705882352941}
Micro-average F1 score: 0.7229190421892816
Weighted-average F1 score: 0.7247552651480287

F1 score per class: {32: 0.3902439024390244, 2: 0.0, 36: 0.0, 6: 0.5285714285714286, 39: 0.36363636363636365, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.1111111111111111, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.1111111111111111}
Micro-average F1 score: 0.30327868852459017
Weighted-average F1 score: 0.21994586749324258
F1 score per class: {32: 0.3076923076923077, 33: 0.0, 2: 0.0, 36: 0.6263736263736264, 6: 0.4878048780487805, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.17543859649122806, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.24}
Micro-average F1 score: 0.3464788732394366
Weighted-average F1 score: 0.27406495627918986
F1 score per class: {32: 0.32653061224489793, 33: 0.0, 2: 0.0, 36: 0.6195652173913043, 6: 0.5, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.15873015873015872, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.10526315789473684}
Micro-average F1 score: 0.3524355300859599
Weighted-average F1 score: 0.28349887239873395

F1 score per class: {32: 0.27586206896551724, 33: 0.37333333333333335, 2: 0.1232876712328767, 36: 0.3333333333333333, 6: 0.23300970873786409, 39: 0.6255506607929515, 8: 0.4, 11: 0.1111111111111111, 12: 0.68, 19: 0.04790419161676647, 20: 0.7872340425531915, 24: 0.8108108108108109, 26: 0.7272727272727273, 28: 0.2857142857142857, 29: 0.3673469387755102, 30: 0.09523809523809523}
Micro-average F1 score: 0.4403183023872679
Weighted-average F1 score: 0.41997514067495634
F1 score per class: {32: 0.19047619047619047, 33: 0.37994722955145116, 2: 0.29571984435797666, 36: 0.4175824175824176, 6: 0.22727272727272727, 39: 0.5890909090909091, 8: 0.3626943005181347, 11: 0.136986301369863, 12: 0.6448598130841121, 19: 0.09345794392523364, 20: 0.8102564102564103, 24: 0.2857142857142857, 26: 0.6766917293233082, 28: 0.3, 29: 0.5753424657534246, 30: 0.13333333333333333}
Micro-average F1 score: 0.4232258064516129
Weighted-average F1 score: 0.3965321398914497
F1 score per class: {32: 0.21052631578947367, 33: 0.397212543554007, 2: 0.24812030075187969, 36: 0.4028268551236749, 6: 0.2290748898678414, 39: 0.5955882352941176, 8: 0.3711340206185567, 11: 0.14285714285714285, 12: 0.6634615384615384, 19: 0.06993006993006994, 20: 0.8020833333333334, 24: 0.3838383838383838, 26: 0.6691449814126395, 28: 0.35294117647058826, 29: 0.5774647887323944, 30: 0.07142857142857142}
Micro-average F1 score: 0.4226666666666667
Weighted-average F1 score: 0.39441050138732603
cur_acc_wo_na:  ['0.8239', '0.6512', '0.4868']
his_acc_wo_na:  ['0.8239', '0.7508', '0.6280']
cur_acc des_wo_na:  ['0.8493', '0.8439', '0.6667']
his_acc des_wo_na:  ['0.8493', '0.8292', '0.7412']
cur_acc rrf_wo_na:  ['0.8493', '0.8509', '0.6703']
his_acc rrf_wo_na:  ['0.8493', '0.8204', '0.7229']
cur_acc_w_na:  ['0.6712', '0.4786', '0.3033']
his_acc_w_na:  ['0.6712', '0.5916', '0.4403']
cur_acc des_w_na:  ['0.6264', '0.4511', '0.3465']
his_acc des_w_na:  ['0.6264', '0.5059', '0.4232']
cur_acc rrf_w_na:  ['0.6264', '0.4628', '0.3524']
his_acc rrf_w_na:  ['0.6264', '0.5103', '0.4227']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 116.6046622CurrentTrain: epoch  0, batch     1 | loss: 146.7002111CurrentTrain: epoch  0, batch     2 | loss: 111.1605948CurrentTrain: epoch  0, batch     3 | loss: 115.8564539CurrentTrain: epoch  0, batch     4 | loss: 129.1388574CurrentTrain: epoch  1, batch     0 | loss: 113.3102316CurrentTrain: epoch  1, batch     1 | loss: 135.8721410CurrentTrain: epoch  1, batch     2 | loss: 134.1540395CurrentTrain: epoch  1, batch     3 | loss: 108.2066834CurrentTrain: epoch  1, batch     4 | loss: 68.6550958CurrentTrain: epoch  2, batch     0 | loss: 87.1069758CurrentTrain: epoch  2, batch     1 | loss: 87.6132519CurrentTrain: epoch  2, batch     2 | loss: 131.5875472CurrentTrain: epoch  2, batch     3 | loss: 277.3543698CurrentTrain: epoch  2, batch     4 | loss: 68.1846293CurrentTrain: epoch  3, batch     0 | loss: 85.5126696CurrentTrain: epoch  3, batch     1 | loss: 129.9633644CurrentTrain: epoch  3, batch     2 | loss: 178.7727368CurrentTrain: epoch  3, batch     3 | loss: 104.7919653CurrentTrain: epoch  3, batch     4 | loss: 66.3554955CurrentTrain: epoch  4, batch     0 | loss: 102.9149917CurrentTrain: epoch  4, batch     1 | loss: 83.0533173CurrentTrain: epoch  4, batch     2 | loss: 103.3576042CurrentTrain: epoch  4, batch     3 | loss: 127.7774850CurrentTrain: epoch  4, batch     4 | loss: 112.4383547CurrentTrain: epoch  5, batch     0 | loss: 175.4192605CurrentTrain: epoch  5, batch     1 | loss: 98.5603376CurrentTrain: epoch  5, batch     2 | loss: 79.7937191CurrentTrain: epoch  5, batch     3 | loss: 131.8752123CurrentTrain: epoch  5, batch     4 | loss: 176.1077420CurrentTrain: epoch  6, batch     0 | loss: 81.4123030CurrentTrain: epoch  6, batch     1 | loss: 102.3357306CurrentTrain: epoch  6, batch     2 | loss: 177.4619027CurrentTrain: epoch  6, batch     3 | loss: 79.9557152CurrentTrain: epoch  6, batch     4 | loss: 79.3348749CurrentTrain: epoch  7, batch     0 | loss: 125.7473301CurrentTrain: epoch  7, batch     1 | loss: 98.6381895CurrentTrain: epoch  7, batch     2 | loss: 81.1473186CurrentTrain: epoch  7, batch     3 | loss: 125.9246600CurrentTrain: epoch  7, batch     4 | loss: 111.7431899CurrentTrain: epoch  8, batch     0 | loss: 97.4470607CurrentTrain: epoch  8, batch     1 | loss: 127.5449262CurrentTrain: epoch  8, batch     2 | loss: 98.3972147CurrentTrain: epoch  8, batch     3 | loss: 130.7942283CurrentTrain: epoch  8, batch     4 | loss: 104.0965787CurrentTrain: epoch  9, batch     0 | loss: 96.5116964CurrentTrain: epoch  9, batch     1 | loss: 100.8313036CurrentTrain: epoch  9, batch     2 | loss: 127.5804829CurrentTrain: epoch  9, batch     3 | loss: 98.5951317CurrentTrain: epoch  9, batch     4 | loss: 77.4359520
MemoryTrain:  epoch  0, batch     0 | loss: 0.9200200MemoryTrain:  epoch  1, batch     0 | loss: 0.8110043MemoryTrain:  epoch  2, batch     0 | loss: 0.5893949MemoryTrain:  epoch  3, batch     0 | loss: 0.4911042MemoryTrain:  epoch  4, batch     0 | loss: 0.3541593MemoryTrain:  epoch  5, batch     0 | loss: 0.3482828MemoryTrain:  epoch  6, batch     0 | loss: 0.2330236MemoryTrain:  epoch  7, batch     0 | loss: 0.1711046MemoryTrain:  epoch  8, batch     0 | loss: 0.1557025MemoryTrain:  epoch  9, batch     0 | loss: 0.1289538

F1 score per class: {5: 0.9583333333333334, 6: 0.0, 10: 0.4732824427480916, 11: 0.0, 12: 0.0, 16: 0.8461538461538461, 17: 0.5, 18: 0.391304347826087, 28: 0.0}
Micro-average F1 score: 0.6811279826464208
Weighted-average F1 score: 0.6836516007441995
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 39: 0.0, 8: 0.7421383647798742, 10: 0.0, 11: 0.0, 12: 0.9473684210526315, 16: 0.6153846153846154, 17: 0.6545454545454545, 18: 0.0, 20: 0.0, 28: 0.0}
Micro-average F1 score: 0.7885714285714286
Weighted-average F1 score: 0.7313693979783306
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 39: 0.0, 8: 0.7654320987654321, 10: 0.0, 11: 0.0, 12: 0.9473684210526315, 16: 0.6153846153846154, 17: 0.6545454545454545, 18: 0.0, 20: 0.0, 28: 0.0}
Micro-average F1 score: 0.8030592734225621
Weighted-average F1 score: 0.7521522538382531

F1 score per class: {2: 0.7142857142857143, 5: 0.9387755102040817, 6: 0.5061728395061729, 8: 0.13793103448275862, 10: 0.4305555555555556, 11: 0.40298507462686567, 12: 0.4666666666666667, 16: 0.8461538461538461, 17: 0.4, 18: 0.29508196721311475, 19: 0.7796610169491526, 20: 0.65, 24: 0.2857142857142857, 26: 0.7472527472527473, 28: 0.15384615384615385, 29: 0.8700564971751412, 30: 0.8648648648648649, 32: 0.883248730964467, 33: 0.2857142857142857, 36: 0.14084507042253522, 39: 0.1}
Micro-average F1 score: 0.6128404669260701
Weighted-average F1 score: 0.6564655579008111
F1 score per class: {2: 0.75, 5: 0.9252336448598131, 6: 0.6575342465753424, 8: 0.2653061224489796, 10: 0.659217877094972, 11: 0.5882352941176471, 12: 0.5465116279069767, 16: 0.8571428571428571, 17: 0.5, 18: 0.42857142857142855, 19: 0.7849462365591398, 20: 0.735632183908046, 24: 0.5185185185185185, 26: 0.7431693989071039, 28: 0.20408163265306123, 29: 0.9081081081081082, 30: 0.76, 32: 0.9090909090909091, 33: 0.4, 36: 0.7058823529411765, 39: 0.16}
Micro-average F1 score: 0.6971133132270573
Weighted-average F1 score: 0.7058351950769775
F1 score per class: {2: 0.75, 5: 0.9124423963133641, 6: 0.6701570680628273, 8: 0.23157894736842105, 10: 0.6702702702702703, 11: 0.5875, 12: 0.5402298850574713, 16: 0.8571428571428571, 17: 0.5333333333333333, 18: 0.42857142857142855, 19: 0.7914438502673797, 20: 0.735632183908046, 24: 0.4166666666666667, 26: 0.7472527472527473, 28: 0.15873015873015872, 29: 0.9010989010989011, 30: 0.76, 32: 0.8910891089108911, 33: 0.4, 36: 0.5333333333333333, 39: 0.08695652173913043}
Micro-average F1 score: 0.6837310195227766
Weighted-average F1 score: 0.6939405122178327

F1 score per class: {2: 0.0, 5: 0.8679245283018868, 6: 0.0, 8: 0.0, 10: 0.40789473684210525, 11: 0.0, 12: 0.0, 16: 0.6285714285714286, 17: 0.5, 18: 0.28125, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.44857142857142857
Weighted-average F1 score: 0.3600346241237215
F1 score per class: {2: 0.0, 5: 0.515625, 6: 0.0, 8: 0.0, 10: 0.5462962962962963, 11: 0.0, 12: 0.0, 16: 0.6067415730337079, 17: 0.47058823529411764, 18: 0.3157894736842105, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.34644351464435147
Weighted-average F1 score: 0.29780861646941925
F1 score per class: {2: 0.0, 5: 0.518324607329843, 6: 0.0, 8: 0.0, 10: 0.5391304347826087, 11: 0.0, 12: 0.0, 16: 0.6206896551724138, 17: 0.5, 18: 0.3185840707964602, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.35443037974683544
Weighted-average F1 score: 0.3073115462201467

F1 score per class: {2: 0.30303030303030304, 5: 0.8141592920353983, 6: 0.3106060606060606, 8: 0.09302325581395349, 10: 0.3054187192118227, 11: 0.2872340425531915, 12: 0.25089605734767023, 16: 0.5365853658536586, 17: 0.2608695652173913, 18: 0.1487603305785124, 19: 0.5948275862068966, 20: 0.41935483870967744, 24: 0.14634146341463414, 26: 0.6634146341463415, 28: 0.056179775280898875, 29: 0.7738693467336684, 30: 0.8, 32: 0.635036496350365, 33: 0.26666666666666666, 36: 0.13157894736842105, 39: 0.05128205128205128}
Micro-average F1 score: 0.4240996297542915
Weighted-average F1 score: 0.41252152335708836
F1 score per class: {2: 0.23076923076923078, 5: 0.375, 6: 0.31718061674008813, 8: 0.12206572769953052, 10: 0.30569948186528495, 11: 0.375, 12: 0.1828793774319066, 16: 0.46153846153846156, 17: 0.18604651162790697, 18: 0.16143497757847533, 19: 0.5427509293680297, 20: 0.3902439024390244, 24: 0.12612612612612611, 26: 0.6181818181818182, 28: 0.05952380952380952, 29: 0.7924528301886793, 30: 0.25333333333333335, 32: 0.569620253164557, 33: 0.3, 36: 0.45, 39: 0.05970149253731343}
Micro-average F1 score: 0.34968662200129674
Weighted-average F1 score: 0.33068187110764
F1 score per class: {2: 0.24489795918367346, 5: 0.376425855513308, 6: 0.3667621776504298, 8: 0.10679611650485436, 10: 0.3123425692695214, 11: 0.352059925093633, 12: 0.17938931297709923, 16: 0.46551724137931033, 17: 0.24242424242424243, 18: 0.15789473684210525, 19: 0.5522388059701493, 20: 0.378698224852071, 24: 0.11363636363636363, 26: 0.6325581395348837, 28: 0.047619047619047616, 29: 0.7884615384615384, 30: 0.2620689655172414, 32: 0.547112462006079, 33: 0.3157894736842105, 36: 0.40336134453781514, 39: 0.03225806451612903}
Micro-average F1 score: 0.3481334216920698
Weighted-average F1 score: 0.32843224655225817
cur_acc_wo_na:  ['0.8239', '0.6512', '0.4868', '0.6811']
his_acc_wo_na:  ['0.8239', '0.7508', '0.6280', '0.6128']
cur_acc des_wo_na:  ['0.8493', '0.8439', '0.6667', '0.7886']
his_acc des_wo_na:  ['0.8493', '0.8292', '0.7412', '0.6971']
cur_acc rrf_wo_na:  ['0.8493', '0.8509', '0.6703', '0.8031']
his_acc rrf_wo_na:  ['0.8493', '0.8204', '0.7229', '0.6837']
cur_acc_w_na:  ['0.6712', '0.4786', '0.3033', '0.4486']
his_acc_w_na:  ['0.6712', '0.5916', '0.4403', '0.4241']
cur_acc des_w_na:  ['0.6264', '0.4511', '0.3465', '0.3464']
his_acc des_w_na:  ['0.6264', '0.5059', '0.4232', '0.3497']
cur_acc rrf_w_na:  ['0.6264', '0.4628', '0.3524', '0.3544']
his_acc rrf_w_na:  ['0.6264', '0.5103', '0.4227', '0.3481']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 121.5323643CurrentTrain: epoch  0, batch     1 | loss: 153.1792525CurrentTrain: epoch  0, batch     2 | loss: 114.6904483CurrentTrain: epoch  0, batch     3 | loss: 98.4236418CurrentTrain: epoch  0, batch     4 | loss: 115.2795105CurrentTrain: epoch  1, batch     0 | loss: 94.4867533CurrentTrain: epoch  1, batch     1 | loss: 104.9124899CurrentTrain: epoch  1, batch     2 | loss: 135.3189469CurrentTrain: epoch  1, batch     3 | loss: 141.7615113CurrentTrain: epoch  1, batch     4 | loss: 103.0108738CurrentTrain: epoch  2, batch     0 | loss: 105.5858840CurrentTrain: epoch  2, batch     1 | loss: 141.9664910CurrentTrain: epoch  2, batch     2 | loss: 129.7975086CurrentTrain: epoch  2, batch     3 | loss: 178.8802181CurrentTrain: epoch  2, batch     4 | loss: 46.9745617CurrentTrain: epoch  3, batch     0 | loss: 177.8720100CurrentTrain: epoch  3, batch     1 | loss: 98.1410502CurrentTrain: epoch  3, batch     2 | loss: 104.9197123CurrentTrain: epoch  3, batch     3 | loss: 127.5428217CurrentTrain: epoch  3, batch     4 | loss: 159.4225915CurrentTrain: epoch  4, batch     0 | loss: 97.7702921CurrentTrain: epoch  4, batch     1 | loss: 100.3024913CurrentTrain: epoch  4, batch     2 | loss: 175.6088520CurrentTrain: epoch  4, batch     3 | loss: 103.5114214CurrentTrain: epoch  4, batch     4 | loss: 59.5215165CurrentTrain: epoch  5, batch     0 | loss: 100.9270427CurrentTrain: epoch  5, batch     1 | loss: 97.6643998CurrentTrain: epoch  5, batch     2 | loss: 130.4581001CurrentTrain: epoch  5, batch     3 | loss: 99.1655077CurrentTrain: epoch  5, batch     4 | loss: 74.5688599CurrentTrain: epoch  6, batch     0 | loss: 125.6910268CurrentTrain: epoch  6, batch     1 | loss: 93.5318655CurrentTrain: epoch  6, batch     2 | loss: 126.6114436CurrentTrain: epoch  6, batch     3 | loss: 177.0729815CurrentTrain: epoch  6, batch     4 | loss: 71.5463675CurrentTrain: epoch  7, batch     0 | loss: 124.6814481CurrentTrain: epoch  7, batch     1 | loss: 102.1957708CurrentTrain: epoch  7, batch     2 | loss: 97.3881824CurrentTrain: epoch  7, batch     3 | loss: 125.4097732CurrentTrain: epoch  7, batch     4 | loss: 93.7136290CurrentTrain: epoch  8, batch     0 | loss: 100.6966526CurrentTrain: epoch  8, batch     1 | loss: 125.8659320CurrentTrain: epoch  8, batch     2 | loss: 100.8678283CurrentTrain: epoch  8, batch     3 | loss: 99.2393583CurrentTrain: epoch  8, batch     4 | loss: 67.3305069CurrentTrain: epoch  9, batch     0 | loss: 171.3227136CurrentTrain: epoch  9, batch     1 | loss: 126.5570569CurrentTrain: epoch  9, batch     2 | loss: 95.9329954CurrentTrain: epoch  9, batch     3 | loss: 99.1420638CurrentTrain: epoch  9, batch     4 | loss: 69.0911225
MemoryTrain:  epoch  0, batch     0 | loss: 1.0819970MemoryTrain:  epoch  1, batch     0 | loss: 0.8967431MemoryTrain:  epoch  2, batch     0 | loss: 0.6145514MemoryTrain:  epoch  3, batch     0 | loss: 0.5049697MemoryTrain:  epoch  4, batch     0 | loss: 0.4590323MemoryTrain:  epoch  5, batch     0 | loss: 0.3394178MemoryTrain:  epoch  6, batch     0 | loss: 0.2629823MemoryTrain:  epoch  7, batch     0 | loss: 0.2230133MemoryTrain:  epoch  8, batch     0 | loss: 0.1624649MemoryTrain:  epoch  9, batch     0 | loss: 0.1503804

F1 score per class: {32: 0.3548387096774194, 1: 0.703125, 34: 0.0, 3: 0.14634146341463414, 11: 0.0, 14: 0.6666666666666666, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.5333333333333333}
Micro-average F1 score: 0.49230769230769234
Weighted-average F1 score: 0.4531308927375464
F1 score per class: {32: 0.421875, 1: 0.9493670886075949, 34: 0.0, 3: 0.0, 10: 0.09876543209876543, 11: 0.0, 14: 0.6666666666666666, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 30: 0.7640449438202247}
Micro-average F1 score: 0.596875
Weighted-average F1 score: 0.575006071943686
F1 score per class: {32: 0.3968253968253968, 1: 0.9554140127388535, 34: 0.0, 3: 0.0, 10: 0.16091954022988506, 11: 0.0, 14: 0.6838709677419355, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 30: 0.7640449438202247}
Micro-average F1 score: 0.6081504702194357
Weighted-average F1 score: 0.5824039095475857

F1 score per class: {1: 0.3283582089552239, 2: 0.7142857142857143, 3: 0.6818181818181818, 5: 0.9591836734693877, 6: 0.4864864864864865, 8: 0.09411764705882353, 10: 0.37593984962406013, 11: 0.24390243902439024, 12: 0.36496350364963503, 14: 0.13636363636363635, 16: 0.7843137254901961, 17: 0.23529411764705882, 18: 0.26229508196721313, 19: 0.5277777777777778, 20: 0.6172839506172839, 22: 0.6580645161290323, 24: 0.05714285714285714, 26: 0.7252747252747253, 28: 0.18518518518518517, 29: 0.8304093567251462, 30: 0.8484848484848485, 32: 0.8324324324324325, 33: 0.4, 34: 0.3333333333333333, 36: 0.058823529411764705, 39: 0.1111111111111111}
Micro-average F1 score: 0.5279069767441861
Weighted-average F1 score: 0.560052404592148
F1 score per class: {1: 0.40298507462686567, 2: 0.75, 3: 0.9036144578313253, 5: 0.9166666666666666, 6: 0.6486486486486487, 8: 0.2828282828282828, 10: 0.6304347826086957, 11: 0.26548672566371684, 12: 0.5269461077844312, 14: 0.09411764705882353, 16: 0.8813559322033898, 17: 0.6, 18: 0.3835616438356164, 19: 0.6900584795321637, 20: 0.6746987951807228, 22: 0.6496815286624203, 24: 0.0851063829787234, 26: 0.7473684210526316, 28: 0.19230769230769232, 29: 0.8700564971751412, 30: 0.8444444444444444, 32: 0.8481675392670157, 33: 0.4, 34: 0.43312101910828027, 36: 0.7663551401869159, 39: 0.1}
Micro-average F1 score: 0.6282007511095937
Weighted-average F1 score: 0.6369076581533005
F1 score per class: {1: 0.37037037037037035, 2: 0.8, 3: 0.8928571428571429, 5: 0.9383886255924171, 6: 0.5988023952095808, 8: 0.17582417582417584, 10: 0.6187845303867403, 11: 0.2542372881355932, 12: 0.5029940119760479, 14: 0.15053763440860216, 16: 0.8813559322033898, 17: 0.3333333333333333, 18: 0.3783783783783784, 19: 0.6823529411764706, 20: 0.6823529411764706, 22: 0.6666666666666666, 24: 0.0975609756097561, 26: 0.7379679144385026, 28: 0.15625, 29: 0.8505747126436781, 30: 0.95, 32: 0.84375, 33: 0.4, 34: 0.4276729559748428, 36: 0.5806451612903226, 39: 0.19047619047619047}
Micro-average F1 score: 0.6089057645840524
Weighted-average F1 score: 0.6160749397010173

F1 score per class: {1: 0.1375, 3: 0.4945054945054945, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.11009174311926606, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.4473684210526316, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.43956043956043955}
Micro-average F1 score: 0.24806201550387597
Weighted-average F1 score: 0.20252102089983817
F1 score per class: {1: 0.15561959654178675, 2: 0.0, 3: 0.45045045045045046, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.057971014492753624, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.4594594594594595, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.48226950354609927}
Micro-average F1 score: 0.2347879532882606
Weighted-average F1 score: 0.2041673125200096
F1 score per class: {1: 0.14450867052023122, 2: 0.0, 3: 0.4601226993865031, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.09722222222222222, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.462882096069869, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.4788732394366197}
Micro-average F1 score: 0.24356559949780288
Weighted-average F1 score: 0.21220603327491025

F1 score per class: {1: 0.10918114143920596, 2: 0.3225806451612903, 3: 0.375, 5: 0.7833333333333333, 6: 0.3050847457627119, 8: 0.06557377049180328, 10: 0.2717391304347826, 11: 0.18867924528301888, 12: 0.21929824561403508, 14: 0.0784313725490196, 16: 0.5263157894736842, 17: 0.15384615384615385, 18: 0.14414414414414414, 19: 0.38190954773869346, 20: 0.37593984962406013, 22: 0.4, 24: 0.030303030303030304, 26: 0.6502463054187192, 28: 0.06666666666666667, 29: 0.6995073891625616, 30: 0.8, 32: 0.4782608695652174, 33: 0.4, 34: 0.16194331983805668, 36: 0.056338028169014086, 39: 0.05405405405405406}
Micro-average F1 score: 0.3285886610373944
Weighted-average F1 score: 0.313118820096476
F1 score per class: {1: 0.11868131868131868, 2: 0.22641509433962265, 3: 0.2862595419847328, 5: 0.35804701627486435, 6: 0.32786885245901637, 8: 0.16766467065868262, 10: 0.2648401826484018, 11: 0.19736842105263158, 12: 0.1900647948164147, 14: 0.03669724770642202, 16: 0.5252525252525253, 17: 0.23529411764705882, 18: 0.1339712918660287, 19: 0.4338235294117647, 20: 0.3163841807909605, 22: 0.3968871595330739, 24: 0.0425531914893617, 26: 0.6068376068376068, 28: 0.05555555555555555, 29: 0.719626168224299, 30: 0.4, 32: 0.43902439024390244, 33: 0.2608695652173913, 34: 0.15315315315315314, 36: 0.5093167701863354, 39: 0.037037037037037035}
Micro-average F1 score: 0.2910471369819677
Weighted-average F1 score: 0.27628021597602137
F1 score per class: {1: 0.1059322033898305, 2: 0.2727272727272727, 3: 0.2824858757062147, 5: 0.406570841889117, 6: 0.33003300330033003, 8: 0.1032258064516129, 10: 0.2711864406779661, 11: 0.17964071856287425, 12: 0.1838074398249453, 14: 0.0611353711790393, 16: 0.5416666666666666, 17: 0.16216216216216217, 18: 0.14507772020725387, 19: 0.43609022556390975, 20: 0.3118279569892473, 22: 0.40304182509505704, 24: 0.04938271604938271, 26: 0.6244343891402715, 28: 0.05, 29: 0.7014218009478673, 30: 0.6785714285714286, 32: 0.43783783783783786, 33: 0.35294117647058826, 34: 0.1440677966101695, 36: 0.46153846153846156, 39: 0.09090909090909091}
Micro-average F1 score: 0.2897503285151117
Weighted-average F1 score: 0.27297934025464415
cur_acc_wo_na:  ['0.8239', '0.6512', '0.4868', '0.6811', '0.4923']
his_acc_wo_na:  ['0.8239', '0.7508', '0.6280', '0.6128', '0.5279']
cur_acc des_wo_na:  ['0.8493', '0.8439', '0.6667', '0.7886', '0.5969']
his_acc des_wo_na:  ['0.8493', '0.8292', '0.7412', '0.6971', '0.6282']
cur_acc rrf_wo_na:  ['0.8493', '0.8509', '0.6703', '0.8031', '0.6082']
his_acc rrf_wo_na:  ['0.8493', '0.8204', '0.7229', '0.6837', '0.6089']
cur_acc_w_na:  ['0.6712', '0.4786', '0.3033', '0.4486', '0.2481']
his_acc_w_na:  ['0.6712', '0.5916', '0.4403', '0.4241', '0.3286']
cur_acc des_w_na:  ['0.6264', '0.4511', '0.3465', '0.3464', '0.2348']
his_acc des_w_na:  ['0.6264', '0.5059', '0.4232', '0.3497', '0.2910']
cur_acc rrf_w_na:  ['0.6264', '0.4628', '0.3524', '0.3544', '0.2436']
his_acc rrf_w_na:  ['0.6264', '0.5103', '0.4227', '0.3481', '0.2898']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 153.3154410CurrentTrain: epoch  0, batch     1 | loss: 104.2748414CurrentTrain: epoch  0, batch     2 | loss: 99.6377131CurrentTrain: epoch  0, batch     3 | loss: 13.2811156CurrentTrain: epoch  1, batch     0 | loss: 111.2898012CurrentTrain: epoch  1, batch     1 | loss: 104.8219501CurrentTrain: epoch  1, batch     2 | loss: 138.7929779CurrentTrain: epoch  1, batch     3 | loss: 17.4041828CurrentTrain: epoch  2, batch     0 | loss: 132.1055228CurrentTrain: epoch  2, batch     1 | loss: 108.8276147CurrentTrain: epoch  2, batch     2 | loss: 100.9552125CurrentTrain: epoch  2, batch     3 | loss: 7.7716951CurrentTrain: epoch  3, batch     0 | loss: 108.2045633CurrentTrain: epoch  3, batch     1 | loss: 103.6421669CurrentTrain: epoch  3, batch     2 | loss: 80.3976052CurrentTrain: epoch  3, batch     3 | loss: 20.1481104CurrentTrain: epoch  4, batch     0 | loss: 97.6252116CurrentTrain: epoch  4, batch     1 | loss: 102.5398317CurrentTrain: epoch  4, batch     2 | loss: 82.7729800CurrentTrain: epoch  4, batch     3 | loss: 17.0043538CurrentTrain: epoch  5, batch     0 | loss: 82.7566221CurrentTrain: epoch  5, batch     1 | loss: 98.3927156CurrentTrain: epoch  5, batch     2 | loss: 99.2078889CurrentTrain: epoch  5, batch     3 | loss: 8.6770849CurrentTrain: epoch  6, batch     0 | loss: 80.2427391CurrentTrain: epoch  6, batch     1 | loss: 83.2120567CurrentTrain: epoch  6, batch     2 | loss: 79.1696316CurrentTrain: epoch  6, batch     3 | loss: 9.6564305CurrentTrain: epoch  7, batch     0 | loss: 96.7058158CurrentTrain: epoch  7, batch     1 | loss: 79.0944110CurrentTrain: epoch  7, batch     2 | loss: 96.9265932CurrentTrain: epoch  7, batch     3 | loss: 17.2213033CurrentTrain: epoch  8, batch     0 | loss: 75.9636468CurrentTrain: epoch  8, batch     1 | loss: 125.6084500CurrentTrain: epoch  8, batch     2 | loss: 94.4777065CurrentTrain: epoch  8, batch     3 | loss: 16.5771250CurrentTrain: epoch  9, batch     0 | loss: 122.4546530CurrentTrain: epoch  9, batch     1 | loss: 96.3363376CurrentTrain: epoch  9, batch     2 | loss: 74.2351112CurrentTrain: epoch  9, batch     3 | loss: 41.2390813
MemoryTrain:  epoch  0, batch     0 | loss: 0.8634461MemoryTrain:  epoch  1, batch     0 | loss: 0.7432870MemoryTrain:  epoch  2, batch     0 | loss: 0.5802562MemoryTrain:  epoch  3, batch     0 | loss: 0.4388279MemoryTrain:  epoch  4, batch     0 | loss: 0.3650822MemoryTrain:  epoch  5, batch     0 | loss: 0.2993555MemoryTrain:  epoch  6, batch     0 | loss: 0.2555948MemoryTrain:  epoch  7, batch     0 | loss: 0.2100973MemoryTrain:  epoch  8, batch     0 | loss: 0.1830721MemoryTrain:  epoch  9, batch     0 | loss: 0.1815688

F1 score per class: {1: 0.0, 7: 0.8888888888888888, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 31: 0.7787610619469026}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.60104387871176
F1 score per class: {1: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 22: 0.0, 26: 0.42105263157894735, 27: 0.0, 28: 1.0, 31: 0.8870967741935484}
Micro-average F1 score: 0.7822222222222223
Weighted-average F1 score: 0.719426992871515
F1 score per class: {1: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 22: 0.0, 26: 0.3333333333333333, 27: 0.0, 28: 1.0, 31: 0.8870967741935484}
Micro-average F1 score: 0.7733333333333333
Weighted-average F1 score: 0.7116824061795599

F1 score per class: {1: 0.2835820895522388, 2: 0.7142857142857143, 3: 0.7230769230769231, 5: 0.9405940594059405, 6: 0.2857142857142857, 7: 0.09411764705882353, 8: 0.09411764705882353, 9: 0.9803921568627451, 10: 0.3851851851851852, 11: 0.1935483870967742, 12: 0.3971631205673759, 14: 0.125, 16: 0.8679245283018868, 17: 0.0, 18: 0.3548387096774194, 19: 0.64, 20: 0.6904761904761905, 22: 0.7080745341614907, 24: 0.0625, 26: 0.7294117647058823, 27: 0.0, 28: 0.16666666666666666, 29: 0.8095238095238095, 30: 0.8823529411764706, 31: 0.0, 32: 0.819672131147541, 33: 0.4, 34: 0.35398230088495575, 36: 0.1917808219178082, 39: 0.10526315789473684, 40: 0.567741935483871}
Micro-average F1 score: 0.5294525294525294
Weighted-average F1 score: 0.5465137804898446
F1 score per class: {1: 0.3140495867768595, 2: 0.8, 3: 0.9135802469135802, 5: 0.9049773755656109, 6: 0.35384615384615387, 7: 0.04819277108433735, 8: 0.1348314606741573, 9: 0.8928571428571429, 10: 0.5301204819277109, 11: 0.2833333333333333, 12: 0.4880952380952381, 14: 0.0975609756097561, 16: 0.896551724137931, 17: 0.35294117647058826, 18: 0.3333333333333333, 19: 0.7528089887640449, 20: 0.8125, 22: 0.6987951807228916, 24: 0.08, 26: 0.7555555555555555, 27: 0.22857142857142856, 28: 0.1282051282051282, 29: 0.8439306358381503, 30: 0.95, 31: 0.8, 32: 0.8324324324324325, 33: 0.3, 34: 0.4878048780487805, 36: 0.7927927927927928, 39: 0.1, 40: 0.6832298136645962}
Micro-average F1 score: 0.5965785381026438
Weighted-average F1 score: 0.5937725738214648
F1 score per class: {1: 0.2992125984251969, 2: 0.875, 3: 0.9079754601226994, 5: 0.9049773755656109, 6: 0.2903225806451613, 7: 0.04597701149425287, 8: 0.1348314606741573, 9: 0.9803921568627451, 10: 0.5, 11: 0.25196850393700787, 12: 0.46706586826347307, 14: 0.11904761904761904, 16: 0.896551724137931, 17: 0.2, 18: 0.41935483870967744, 19: 0.7457627118644068, 20: 0.8367346938775511, 22: 0.7455621301775148, 24: 0.09090909090909091, 26: 0.7597765363128491, 27: 0.1875, 28: 0.12345679012345678, 29: 0.8439306358381503, 30: 0.972972972972973, 31: 0.8, 32: 0.8324324324324325, 33: 0.3333333333333333, 34: 0.4645161290322581, 36: 0.6185567010309279, 39: 0.1, 40: 0.6547619047619048}
Micro-average F1 score: 0.5866834170854272
Weighted-average F1 score: 0.5845037467474299

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.6666666666666666, 8: 0.0, 9: 0.9090909090909091, 10: 0.0, 11: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 40: 0.4656084656084656}
Micro-average F1 score: 0.42318840579710143
Weighted-average F1 score: 0.3782204648005513
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4444444444444444, 8: 0.0, 9: 0.6666666666666666, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.2962962962962963, 28: 0.0, 30: 0.0, 31: 0.16666666666666666, 32: 0.0, 34: 0.0, 36: 0.0, 40: 0.5045871559633027}
Micro-average F1 score: 0.3628865979381443
Weighted-average F1 score: 0.3179173386893656
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.4444444444444444, 9: 0.7692307692307693, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 27: 0.24, 28: 0.0, 30: 0.0, 31: 0.16666666666666666, 32: 0.0, 34: 0.0, 40: 0.49107142857142855}
Micro-average F1 score: 0.3782608695652174
Weighted-average F1 score: 0.33217221763464533

F1 score per class: {1: 0.10354223433242507, 2: 0.24390243902439024, 3: 0.3805668016194332, 5: 0.6666666666666666, 6: 0.23943661971830985, 7: 0.035555555555555556, 8: 0.07547169811320754, 9: 0.847457627118644, 10: 0.26804123711340205, 11: 0.14285714285714285, 12: 0.2066420664206642, 14: 0.07352941176470588, 16: 0.5822784810126582, 17: 0.0, 18: 0.1437908496732026, 19: 0.4444444444444444, 20: 0.3391812865497076, 22: 0.5377358490566038, 24: 0.03636363636363636, 26: 0.6595744680851063, 27: 0.0, 28: 0.05714285714285714, 29: 0.7157894736842105, 30: 0.8333333333333334, 31: 0.0, 32: 0.5190311418685121, 33: 0.375, 34: 0.20100502512562815, 36: 0.1686746987951807, 39: 0.04878048780487805, 40: 0.21621621621621623}
Micro-average F1 score: 0.31466227347611203
Weighted-average F1 score: 0.291316433137033
F1 score per class: {1: 0.10919540229885058, 2: 0.1797752808988764, 3: 0.31223628691983124, 5: 0.2724795640326976, 6: 0.25, 7: 0.015625, 8: 0.08391608391608392, 9: 0.37037037037037035, 10: 0.24043715846994534, 11: 0.2073170731707317, 12: 0.16942148760330578, 14: 0.04371584699453552, 16: 0.45217391304347826, 17: 0.14634146341463414, 18: 0.1092896174863388, 19: 0.4652777777777778, 20: 0.26262626262626265, 22: 0.4549019607843137, 24: 0.0380952380952381, 26: 0.6181818181818182, 27: 0.06837606837606838, 28: 0.03134796238244514, 29: 0.7053140096618358, 30: 0.5846153846153846, 31: 0.01818181818181818, 32: 0.5082508250825083, 33: 0.20689655172413793, 34: 0.1423487544483986, 36: 0.5116279069767442, 39: 0.0392156862745098, 40: 0.21568627450980393}
Micro-average F1 score: 0.25173907336920853
Weighted-average F1 score: 0.23232898066867635
F1 score per class: {1: 0.10079575596816977, 2: 0.2222222222222222, 3: 0.30578512396694213, 5: 0.291970802919708, 6: 0.22085889570552147, 7: 0.0149812734082397, 8: 0.08695652173913043, 9: 0.6172839506172839, 10: 0.23809523809523808, 11: 0.1693121693121693, 12: 0.17333333333333334, 14: 0.05405405405405406, 16: 0.46017699115044247, 17: 0.1, 18: 0.12807881773399016, 19: 0.4782608695652174, 20: 0.27796610169491526, 22: 0.4790874524714829, 24: 0.046511627906976744, 26: 0.6445497630331753, 27: 0.06, 28: 0.03215434083601286, 29: 0.7053140096618358, 30: 0.782608695652174, 31: 0.018433179723502304, 32: 0.5049180327868853, 33: 0.2608695652173913, 34: 0.15584415584415584, 36: 0.47244094488188976, 39: 0.04, 40: 0.19748653500897667}
Micro-average F1 score: 0.256241426611797
Weighted-average F1 score: 0.2350307256397219
cur_acc_wo_na:  ['0.8239', '0.6512', '0.4868', '0.6811', '0.4923', '0.6667']
his_acc_wo_na:  ['0.8239', '0.7508', '0.6280', '0.6128', '0.5279', '0.5295']
cur_acc des_wo_na:  ['0.8493', '0.8439', '0.6667', '0.7886', '0.5969', '0.7822']
his_acc des_wo_na:  ['0.8493', '0.8292', '0.7412', '0.6971', '0.6282', '0.5966']
cur_acc rrf_wo_na:  ['0.8493', '0.8509', '0.6703', '0.8031', '0.6082', '0.7733']
his_acc rrf_wo_na:  ['0.8493', '0.8204', '0.7229', '0.6837', '0.6089', '0.5867']
cur_acc_w_na:  ['0.6712', '0.4786', '0.3033', '0.4486', '0.2481', '0.4232']
his_acc_w_na:  ['0.6712', '0.5916', '0.4403', '0.4241', '0.3286', '0.3147']
cur_acc des_w_na:  ['0.6264', '0.4511', '0.3465', '0.3464', '0.2348', '0.3629']
his_acc des_w_na:  ['0.6264', '0.5059', '0.4232', '0.3497', '0.2910', '0.2517']
cur_acc rrf_w_na:  ['0.6264', '0.4628', '0.3524', '0.3544', '0.2436', '0.3783']
his_acc rrf_w_na:  ['0.6264', '0.5103', '0.4227', '0.3481', '0.2898', '0.2562']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 150.0920890CurrentTrain: epoch  0, batch     1 | loss: 117.7793998CurrentTrain: epoch  0, batch     2 | loss: 140.1364889CurrentTrain: epoch  0, batch     3 | loss: 72.6271251CurrentTrain: epoch  1, batch     0 | loss: 87.7725452CurrentTrain: epoch  1, batch     1 | loss: 121.8633207CurrentTrain: epoch  1, batch     2 | loss: 111.5667371CurrentTrain: epoch  1, batch     3 | loss: 117.1672604CurrentTrain: epoch  2, batch     0 | loss: 78.5259101CurrentTrain: epoch  2, batch     1 | loss: 83.3719518CurrentTrain: epoch  2, batch     2 | loss: 111.7025002CurrentTrain: epoch  2, batch     3 | loss: 125.3126806CurrentTrain: epoch  3, batch     0 | loss: 80.5125861CurrentTrain: epoch  3, batch     1 | loss: 129.9249646CurrentTrain: epoch  3, batch     2 | loss: 103.3653017CurrentTrain: epoch  3, batch     3 | loss: 86.9006710CurrentTrain: epoch  4, batch     0 | loss: 101.0578535CurrentTrain: epoch  4, batch     1 | loss: 77.4884145CurrentTrain: epoch  4, batch     2 | loss: 103.0641163CurrentTrain: epoch  4, batch     3 | loss: 87.4396991CurrentTrain: epoch  5, batch     0 | loss: 82.6277069CurrentTrain: epoch  5, batch     1 | loss: 125.0909872CurrentTrain: epoch  5, batch     2 | loss: 99.7046150CurrentTrain: epoch  5, batch     3 | loss: 69.2795079CurrentTrain: epoch  6, batch     0 | loss: 78.5462326CurrentTrain: epoch  6, batch     1 | loss: 130.7330656CurrentTrain: epoch  6, batch     2 | loss: 99.9791183CurrentTrain: epoch  6, batch     3 | loss: 52.7513012CurrentTrain: epoch  7, batch     0 | loss: 97.3513477CurrentTrain: epoch  7, batch     1 | loss: 122.7134398CurrentTrain: epoch  7, batch     2 | loss: 98.9162611CurrentTrain: epoch  7, batch     3 | loss: 115.1374247CurrentTrain: epoch  8, batch     0 | loss: 97.9121194CurrentTrain: epoch  8, batch     1 | loss: 79.5588651CurrentTrain: epoch  8, batch     2 | loss: 97.5942235CurrentTrain: epoch  8, batch     3 | loss: 113.7780848CurrentTrain: epoch  9, batch     0 | loss: 94.6884294CurrentTrain: epoch  9, batch     1 | loss: 93.5940101CurrentTrain: epoch  9, batch     2 | loss: 124.4908371CurrentTrain: epoch  9, batch     3 | loss: 86.3017527
MemoryTrain:  epoch  0, batch     0 | loss: 0.7749096MemoryTrain:  epoch  1, batch     0 | loss: 0.6755693MemoryTrain:  epoch  2, batch     0 | loss: 0.5766628MemoryTrain:  epoch  3, batch     0 | loss: 0.4362565MemoryTrain:  epoch  4, batch     0 | loss: 0.3534855MemoryTrain:  epoch  5, batch     0 | loss: 0.3348558MemoryTrain:  epoch  6, batch     0 | loss: 0.2711789MemoryTrain:  epoch  7, batch     0 | loss: 0.2133858MemoryTrain:  epoch  8, batch     0 | loss: 0.1920919MemoryTrain:  epoch  9, batch     0 | loss: 0.1727714

F1 score per class: {34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 38: 0.8888888888888888, 8: 0.0, 10: 0.6493506493506493, 14: 0.0, 15: 0.0, 18: 0.5428571428571428, 25: 0.6666666666666666, 28: 0.4444444444444444}
Micro-average F1 score: 0.5590062111801242
Weighted-average F1 score: 0.47284891417122826
F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.75, 36: 0.0, 8: 0.631578947368421, 38: 0.0, 11: 0.0, 10: 0.0, 15: 0.0, 18: 0.8539325842696629, 25: 0.0, 26: 0.723404255319149, 28: 0.7555555555555555}
Micro-average F1 score: 0.6197916666666666
Weighted-average F1 score: 0.4893307216633049
F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.8235294117647058, 38: 0.0, 8: 0.6493506493506493, 10: 0.0, 11: 0.0, 15: 0.0, 18: 0.0, 25: 0.8791208791208791, 26: 0.7368421052631579, 28: 0.7555555555555555}
Micro-average F1 score: 0.6526315789473685
Weighted-average F1 score: 0.535180030221281

F1 score per class: {1: 0.27692307692307694, 2: 0.8, 3: 0.3883495145631068, 5: 0.8636363636363636, 6: 0.3114754098360656, 7: 0.04938271604938271, 8: 0.06896551724137931, 9: 0.9803921568627451, 10: 0.288, 11: 0.1346153846153846, 12: 0.2459016393442623, 14: 0.05405405405405406, 15: 0.8421052631578947, 16: 0.8421052631578947, 17: 0.0, 18: 0.36363636363636365, 19: 0.6162790697674418, 20: 0.4788732394366197, 22: 0.725, 24: 0.07142857142857142, 25: 0.6493506493506493, 26: 0.718562874251497, 27: 0.14285714285714285, 28: 0.2857142857142857, 29: 0.8439306358381503, 30: 0.8823529411764706, 31: 0.0, 32: 0.7932960893854749, 33: 0.42857142857142855, 34: 0.17582417582417584, 35: 0.3486238532110092, 36: 0.0, 37: 0.32608695652173914, 38: 0.22535211267605634, 39: 0.0, 40: 0.6222222222222222}
Micro-average F1 score: 0.4801261829652997
Weighted-average F1 score: 0.5160360038795192
F1 score per class: {1: 0.2714285714285714, 2: 0.7368421052631579, 3: 0.6842105263157895, 5: 0.8114754098360656, 6: 0.4195804195804196, 7: 0.08333333333333333, 8: 0.08695652173913043, 9: 0.8064516129032258, 10: 0.4605263157894737, 11: 0.2956521739130435, 12: 0.40993788819875776, 14: 0.14285714285714285, 15: 0.7058823529411765, 16: 0.896551724137931, 17: 0.0, 18: 0.39344262295081966, 19: 0.6626506024096386, 20: 0.6329113924050633, 22: 0.7283950617283951, 24: 0.0975609756097561, 25: 0.631578947368421, 26: 0.7555555555555555, 27: 0.2727272727272727, 28: 0.23529411764705882, 29: 0.8439306358381503, 30: 0.9230769230769231, 31: 0.8, 32: 0.8279569892473119, 33: 0.42857142857142855, 34: 0.3787878787878788, 35: 0.6178861788617886, 36: 0.5684210526315789, 37: 0.4171779141104294, 38: 0.2857142857142857, 39: 0.0, 40: 0.6829268292682927}
Micro-average F1 score: 0.5500697350069735
Weighted-average F1 score: 0.5502776290475571
F1 score per class: {1: 0.2714285714285714, 2: 0.875, 3: 0.6805555555555556, 5: 0.8389830508474576, 6: 0.3609022556390977, 7: 0.07894736842105263, 8: 0.06666666666666667, 9: 0.9615384615384616, 10: 0.44755244755244755, 11: 0.23423423423423423, 12: 0.4074074074074074, 14: 0.15053763440860216, 15: 0.7777777777777778, 16: 0.896551724137931, 17: 0.0, 18: 0.4, 19: 0.6867469879518072, 20: 0.6829268292682927, 22: 0.7764705882352941, 24: 0.05714285714285714, 25: 0.6493506493506493, 26: 0.7640449438202247, 27: 0.2608695652173913, 28: 0.2, 29: 0.8439306358381503, 30: 0.9473684210526315, 31: 0.8, 32: 0.8279569892473119, 33: 0.42857142857142855, 34: 0.35772357723577236, 35: 0.625, 36: 0.24, 37: 0.3783783783783784, 38: 0.26153846153846155, 39: 0.0, 40: 0.6909090909090909}
Micro-average F1 score: 0.5424395727937044
Weighted-average F1 score: 0.5455230711139676

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.8888888888888888, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.6329113924050633, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.4222222222222222, 37: 0.5607476635514018, 38: 0.34782608695652173, 40: 0.0}
Micro-average F1 score: 0.3543307086614173
Weighted-average F1 score: 0.23870487011351063
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.5714285714285714, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.0, 25: 0.5783132530120482, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.5588235294117647, 36: 0.0, 37: 0.48226950354609927, 38: 0.4788732394366197, 40: 0.0}
Micro-average F1 score: 0.271689497716895
Weighted-average F1 score: 0.1946204506845874
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.6666666666666666, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.5813953488372093, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.583941605839416, 37: 0.49295774647887325, 38: 0.4788732394366197, 40: 0.0}
Micro-average F1 score: 0.29594272076372313
Weighted-average F1 score: 0.2171645805385094

F1 score per class: {1: 0.09523809523809523, 2: 0.27906976744186046, 3: 0.2597402597402597, 5: 0.6109324758842444, 6: 0.24203821656050956, 7: 0.020100502512562814, 8: 0.05357142857142857, 9: 0.8064516129032258, 10: 0.21686746987951808, 11: 0.11475409836065574, 12: 0.14150943396226415, 14: 0.038834951456310676, 15: 0.41025641025641024, 16: 0.46601941747572817, 17: 0.0, 18: 0.12121212121212122, 19: 0.41245136186770426, 20: 0.265625, 22: 0.5272727272727272, 24: 0.05, 25: 0.625, 26: 0.6153846153846154, 27: 0.09523809523809523, 28: 0.07751937984496124, 29: 0.7053140096618358, 30: 0.8333333333333334, 31: 0.0, 32: 0.5278810408921933, 33: 0.4, 34: 0.1032258064516129, 35: 0.1589958158995816, 36: 0.0, 37: 0.1488833746898263, 38: 0.0761904761904762, 39: 0.0, 40: 0.302158273381295}
Micro-average F1 score: 0.2850187265917603
Weighted-average F1 score: 0.2671081351270163
F1 score per class: {1: 0.08735632183908046, 2: 0.14893617021276595, 3: 0.1894353369763206, 5: 0.28285714285714286, 6: 0.26548672566371684, 7: 0.028708133971291867, 8: 0.047619047619047616, 9: 0.28735632183908044, 10: 0.24734982332155478, 11: 0.2463768115942029, 12: 0.13441955193482688, 14: 0.06976744186046512, 15: 0.17142857142857143, 16: 0.3969465648854962, 17: 0.0, 18: 0.0967741935483871, 19: 0.38461538461538464, 20: 0.28735632183908044, 22: 0.5086206896551724, 24: 0.046511627906976744, 25: 0.5783132530120482, 26: 0.5738396624472574, 27: 0.13043478260869565, 28: 0.05150214592274678, 29: 0.7336683417085427, 30: 0.5217391304347826, 31: 0.017316017316017316, 32: 0.463855421686747, 33: 0.2857142857142857, 34: 0.14619883040935672, 35: 0.18811881188118812, 36: 0.35526315789473684, 37: 0.11867364746945899, 38: 0.07311827956989247, 39: 0.0, 40: 0.2222222222222222}
Micro-average F1 score: 0.2244479854313681
Weighted-average F1 score: 0.20484351548974825
F1 score per class: {1: 0.08695652173913043, 2: 0.18666666666666668, 3: 0.2327790973871734, 5: 0.3256578947368421, 6: 0.23880597014925373, 7: 0.026905829596412557, 8: 0.0375, 9: 0.5747126436781609, 10: 0.2723404255319149, 11: 0.18840579710144928, 12: 0.14012738853503184, 14: 0.06829268292682927, 15: 0.2, 16: 0.40310077519379844, 17: 0.0, 18: 0.10084033613445378, 19: 0.3986013986013986, 20: 0.3146067415730337, 22: 0.5057471264367817, 24: 0.034482758620689655, 25: 0.5681818181818182, 26: 0.6267281105990783, 27: 0.1276595744680851, 28: 0.04310344827586207, 29: 0.7263681592039801, 30: 0.6101694915254238, 31: 0.020202020202020204, 32: 0.463855421686747, 33: 0.375, 34: 0.1437908496732026, 35: 0.17817371937639198, 36: 0.1935483870967742, 37: 0.10638297872340426, 38: 0.06576402321083172, 39: 0.0, 40: 0.2261904761904762}
Micro-average F1 score: 0.2291345126439511
Weighted-average F1 score: 0.2086393956516967
cur_acc_wo_na:  ['0.8239', '0.6512', '0.4868', '0.6811', '0.4923', '0.6667', '0.5590']
his_acc_wo_na:  ['0.8239', '0.7508', '0.6280', '0.6128', '0.5279', '0.5295', '0.4801']
cur_acc des_wo_na:  ['0.8493', '0.8439', '0.6667', '0.7886', '0.5969', '0.7822', '0.6198']
his_acc des_wo_na:  ['0.8493', '0.8292', '0.7412', '0.6971', '0.6282', '0.5966', '0.5501']
cur_acc rrf_wo_na:  ['0.8493', '0.8509', '0.6703', '0.8031', '0.6082', '0.7733', '0.6526']
his_acc rrf_wo_na:  ['0.8493', '0.8204', '0.7229', '0.6837', '0.6089', '0.5867', '0.5424']
cur_acc_w_na:  ['0.6712', '0.4786', '0.3033', '0.4486', '0.2481', '0.4232', '0.3543']
his_acc_w_na:  ['0.6712', '0.5916', '0.4403', '0.4241', '0.3286', '0.3147', '0.2850']
cur_acc des_w_na:  ['0.6264', '0.4511', '0.3465', '0.3464', '0.2348', '0.3629', '0.2717']
his_acc des_w_na:  ['0.6264', '0.5059', '0.4232', '0.3497', '0.2910', '0.2517', '0.2244']
cur_acc rrf_w_na:  ['0.6264', '0.4628', '0.3524', '0.3544', '0.2436', '0.3783', '0.2959']
his_acc rrf_w_na:  ['0.6264', '0.5103', '0.4227', '0.3481', '0.2898', '0.2562', '0.2291']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 156.4027619CurrentTrain: epoch  0, batch     1 | loss: 115.5441950CurrentTrain: epoch  0, batch     2 | loss: 106.5597830CurrentTrain: epoch  0, batch     3 | loss: 97.9524680CurrentTrain: epoch  1, batch     0 | loss: 108.6282190CurrentTrain: epoch  1, batch     1 | loss: 106.0982335CurrentTrain: epoch  1, batch     2 | loss: 135.7723305CurrentTrain: epoch  1, batch     3 | loss: 114.1783959CurrentTrain: epoch  2, batch     0 | loss: 98.4013418CurrentTrain: epoch  2, batch     1 | loss: 106.0342422CurrentTrain: epoch  2, batch     2 | loss: 98.3903215CurrentTrain: epoch  2, batch     3 | loss: 113.0029842CurrentTrain: epoch  3, batch     0 | loss: 103.3055483CurrentTrain: epoch  3, batch     1 | loss: 176.6617036CurrentTrain: epoch  3, batch     2 | loss: 85.3785076CurrentTrain: epoch  3, batch     3 | loss: 79.1357354CurrentTrain: epoch  4, batch     0 | loss: 99.7885588CurrentTrain: epoch  4, batch     1 | loss: 128.4717435CurrentTrain: epoch  4, batch     2 | loss: 124.4764290CurrentTrain: epoch  4, batch     3 | loss: 78.9788118CurrentTrain: epoch  5, batch     0 | loss: 129.1669736CurrentTrain: epoch  5, batch     1 | loss: 80.7372118CurrentTrain: epoch  5, batch     2 | loss: 81.5796186CurrentTrain: epoch  5, batch     3 | loss: 97.5294356CurrentTrain: epoch  6, batch     0 | loss: 74.1038223CurrentTrain: epoch  6, batch     1 | loss: 102.2301988CurrentTrain: epoch  6, batch     2 | loss: 124.6436601CurrentTrain: epoch  6, batch     3 | loss: 102.4786786CurrentTrain: epoch  7, batch     0 | loss: 92.1436081CurrentTrain: epoch  7, batch     1 | loss: 177.9709932CurrentTrain: epoch  7, batch     2 | loss: 79.9191587CurrentTrain: epoch  7, batch     3 | loss: 78.7248665CurrentTrain: epoch  8, batch     0 | loss: 92.1928663CurrentTrain: epoch  8, batch     1 | loss: 76.3639499CurrentTrain: epoch  8, batch     2 | loss: 128.1461672CurrentTrain: epoch  8, batch     3 | loss: 103.9479936CurrentTrain: epoch  9, batch     0 | loss: 173.1597515CurrentTrain: epoch  9, batch     1 | loss: 120.3444095CurrentTrain: epoch  9, batch     2 | loss: 91.5802980CurrentTrain: epoch  9, batch     3 | loss: 99.2418038
MemoryTrain:  epoch  0, batch     0 | loss: 0.6198486MemoryTrain:  epoch  1, batch     0 | loss: 0.5554082MemoryTrain:  epoch  2, batch     0 | loss: 0.4727000MemoryTrain:  epoch  3, batch     0 | loss: 0.3953928MemoryTrain:  epoch  4, batch     0 | loss: 0.2896151MemoryTrain:  epoch  5, batch     0 | loss: 0.2621486MemoryTrain:  epoch  6, batch     0 | loss: 0.2205457MemoryTrain:  epoch  7, batch     0 | loss: 0.1941361MemoryTrain:  epoch  8, batch     0 | loss: 0.1623439MemoryTrain:  epoch  9, batch     0 | loss: 0.1375187

F1 score per class: {0: 0.9863013698630136, 32: 0.0, 34: 0.9528795811518325, 2: 0.0, 4: 0.3333333333333333, 38: 0.0, 9: 0.0, 13: 0.12121212121212122, 14: 0.0, 15: 0.9069767441860465, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8428927680798005
Weighted-average F1 score: 0.8738494022038599
F1 score per class: {0: 0.9577464788732394, 1: 0.0, 2: 0.0, 4: 0.9417989417989417, 5: 0.0, 9: 0.0, 13: 0.5714285714285714, 14: 0.0, 21: 0.4878048780487805, 23: 0.9069767441860465, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.8188235294117647
Weighted-average F1 score: 0.769641390479984
F1 score per class: {0: 0.9577464788732394, 1: 0.0, 2: 0.0, 4: 0.9417989417989417, 5: 0.0, 9: 0.0, 13: 0.5714285714285714, 14: 0.0, 15: 0.0, 21: 0.4878048780487805, 23: 0.9069767441860465, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.8188235294117647
Weighted-average F1 score: 0.7696413904799839

F1 score per class: {0: 0.935064935064935, 1: 0.24615384615384617, 2: 0.8235294117647058, 3: 0.4909090909090909, 4: 0.9528795811518325, 5: 0.8755760368663594, 6: 0.25862068965517243, 7: 0.02702702702702703, 8: 0.0, 9: 0.9615384615384616, 10: 0.21238938053097345, 11: 0.1651376146788991, 12: 0.22413793103448276, 13: 0.03076923076923077, 14: 0.057971014492753624, 15: 0.7058823529411765, 16: 0.8421052631578947, 17: 0.0, 18: 0.19607843137254902, 19: 0.6309523809523809, 20: 0.6172839506172839, 21: 0.08333333333333333, 22: 0.6493506493506493, 23: 0.8666666666666667, 24: 0.08, 25: 0.5945945945945946, 26: 0.6946107784431138, 27: 0.14285714285714285, 28: 0.35294117647058826, 29: 0.8390804597701149, 30: 0.8333333333333334, 31: 0.0, 32: 0.7771428571428571, 33: 0.42857142857142855, 34: 0.13513513513513514, 35: 0.4, 36: 0.0, 37: 0.2962962962962963, 38: 0.22535211267605634, 39: 0.0, 40: 0.6614173228346457}
Micro-average F1 score: 0.5105653912050258
Weighted-average F1 score: 0.5574678321043894
F1 score per class: {0: 0.918918918918919, 1: 0.27972027972027974, 2: 0.56, 3: 0.7023809523809523, 4: 0.9417989417989417, 5: 0.8368200836820083, 6: 0.3787878787878788, 7: 0.05970149253731343, 8: 0.16494845360824742, 9: 0.8125, 10: 0.36363636363636365, 11: 0.23008849557522124, 12: 0.40522875816993464, 13: 0.05555555555555555, 14: 0.07228915662650602, 15: 0.75, 16: 0.896551724137931, 17: 0.0, 18: 0.46875, 19: 0.7045454545454546, 20: 0.6666666666666666, 21: 0.2222222222222222, 22: 0.6258503401360545, 23: 0.8297872340425532, 24: 0.0, 25: 0.6493506493506493, 26: 0.7282608695652174, 27: 0.11764705882352941, 28: 0.34285714285714286, 29: 0.8539325842696629, 30: 0.926829268292683, 31: 0.6666666666666666, 32: 0.7955801104972375, 33: 0.4, 34: 0.41025641025641024, 35: 0.5981308411214953, 36: 0.5434782608695652, 37: 0.4129032258064516, 38: 0.3488372093023256, 39: 0.13333333333333333, 40: 0.6933333333333334}
Micro-average F1 score: 0.5685432971471851
Weighted-average F1 score: 0.572222410900166
F1 score per class: {0: 0.8831168831168831, 1: 0.2916666666666667, 2: 0.6363636363636364, 3: 0.75, 4: 0.9368421052631579, 5: 0.847457627118644, 6: 0.359375, 7: 0.05714285714285714, 8: 0.06818181818181818, 9: 0.9433962264150944, 10: 0.3225806451612903, 11: 0.24561403508771928, 12: 0.40789473684210525, 13: 0.05263157894736842, 14: 0.0963855421686747, 15: 0.7058823529411765, 16: 0.896551724137931, 17: 0.0, 18: 0.4444444444444444, 19: 0.7085714285714285, 20: 0.6904761904761905, 21: 0.2222222222222222, 22: 0.6351351351351351, 23: 0.8387096774193549, 24: 0.0, 25: 0.6493506493506493, 26: 0.7282608695652174, 27: 0.21052631578947367, 28: 0.32432432432432434, 29: 0.8522727272727273, 30: 0.9743589743589743, 31: 0.0, 32: 0.8021978021978022, 33: 0.42857142857142855, 34: 0.3392857142857143, 35: 0.6440677966101694, 36: 0.2597402597402597, 37: 0.378698224852071, 38: 0.3368421052631579, 39: 0.13333333333333333, 40: 0.7152317880794702}
Micro-average F1 score: 0.5614837398373984
Weighted-average F1 score: 0.5685721815625207

F1 score per class: {0: 0.7128712871287128, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.9528795811518325, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.058823529411764705, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.11764705882352941, 22: 0.0, 23: 0.8041237113402062, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.5522875816993464
Weighted-average F1 score: 0.4454102381568277
F1 score per class: {0: 0.53125, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8682926829268293, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.08, 14: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.3125, 22: 0.0, 23: 0.6666666666666666, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3522267206477733
Weighted-average F1 score: 0.26053860465362466
F1 score per class: {0: 0.5230769230769231, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8682926829268293, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.06779661016949153, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.30303030303030304, 22: 0.0, 23: 0.7428571428571429, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.37540453074433655
Weighted-average F1 score: 0.2788821462438449

F1 score per class: {0: 0.147239263803681, 1: 0.08602150537634409, 2: 0.3181818181818182, 3: 0.313953488372093, 4: 0.914572864321608, 5: 0.662020905923345, 6: 0.20833333333333334, 7: 0.01020408163265306, 8: 0.0, 9: 0.78125, 10: 0.17777777777777778, 11: 0.13846153846153847, 12: 0.13829787234042554, 13: 0.006329113924050633, 14: 0.04597701149425287, 15: 0.4, 16: 0.46153846153846156, 17: 0.0, 18: 0.07142857142857142, 19: 0.41568627450980394, 20: 0.27932960893854747, 21: 0.03669724770642202, 22: 0.5263157894736842, 23: 0.7289719626168224, 24: 0.06896551724137931, 25: 0.5641025641025641, 26: 0.6105263157894737, 27: 0.058823529411764705, 28: 0.15, 29: 0.6728110599078341, 30: 0.75, 31: 0.0, 32: 0.5506072874493927, 33: 0.42857142857142855, 34: 0.10204081632653061, 35: 0.20388349514563106, 36: 0.0, 37: 0.19138755980861244, 38: 0.06956521739130435, 39: 0.0, 40: 0.42}
Micro-average F1 score: 0.30020147750167897
Weighted-average F1 score: 0.2729673658255315
F1 score per class: {0: 0.09405255878284924, 1: 0.08658008658008658, 2: 0.13725490196078433, 3: 0.16549789621318373, 4: 0.8165137614678899, 5: 0.3007518796992481, 6: 0.2347417840375587, 7: 0.01904761904761905, 8: 0.09815950920245399, 9: 0.2751322751322751, 10: 0.1797752808988764, 11: 0.18439716312056736, 12: 0.13747228381374724, 13: 0.012345679012345678, 14: 0.03260869565217391, 15: 0.3333333333333333, 16: 0.3969465648854962, 17: 0.0, 18: 0.09646302250803858, 19: 0.3712574850299401, 20: 0.2317596566523605, 21: 0.07092198581560284, 22: 0.519774011299435, 23: 0.45348837209302323, 24: 0.0, 25: 0.5681818181818182, 26: 0.575107296137339, 27: 0.045454545454545456, 28: 0.08695652173913043, 29: 0.6940639269406392, 30: 0.5588235294117647, 31: 0.044444444444444446, 32: 0.46601941747572817, 33: 0.2727272727272727, 34: 0.17647058823529413, 35: 0.19814241486068113, 36: 0.3333333333333333, 37: 0.18497109826589594, 38: 0.07653061224489796, 39: 0.09523809523809523, 40: 0.2826086956521739}
Micro-average F1 score: 0.22944472745797248
Weighted-average F1 score: 0.20692405021295518
F1 score per class: {0: 0.08740359897172237, 1: 0.09032258064516129, 2: 0.21212121212121213, 3: 0.2544642857142857, 4: 0.8127853881278538, 5: 0.3418803418803419, 6: 0.23, 7: 0.017937219730941704, 8: 0.044444444444444446, 9: 0.5681818181818182, 10: 0.2185792349726776, 11: 0.1891891891891892, 12: 0.1486810551558753, 13: 0.010666666666666666, 14: 0.045714285714285714, 15: 0.25, 16: 0.3939393939393939, 17: 0.0, 18: 0.10218978102189781, 19: 0.38153846153846155, 20: 0.23868312757201646, 21: 0.06993006993006994, 22: 0.5164835164835165, 23: 0.5416666666666666, 24: 0.0, 25: 0.5494505494505495, 26: 0.6261682242990654, 27: 0.07407407407407407, 28: 0.08571428571428572, 29: 0.6912442396313364, 30: 0.6909090909090909, 31: 0.0, 32: 0.4605678233438486, 33: 0.3333333333333333, 34: 0.14901960784313725, 35: 0.1953727506426735, 36: 0.20618556701030927, 37: 0.16161616161616163, 38: 0.06986899563318777, 39: 0.09523809523809523, 40: 0.2849604221635884}
Micro-average F1 score: 0.23702273702273702
Weighted-average F1 score: 0.21236665068994337
cur_acc_wo_na:  ['0.8239', '0.6512', '0.4868', '0.6811', '0.4923', '0.6667', '0.5590', '0.8429']
his_acc_wo_na:  ['0.8239', '0.7508', '0.6280', '0.6128', '0.5279', '0.5295', '0.4801', '0.5106']
cur_acc des_wo_na:  ['0.8493', '0.8439', '0.6667', '0.7886', '0.5969', '0.7822', '0.6198', '0.8188']
his_acc des_wo_na:  ['0.8493', '0.8292', '0.7412', '0.6971', '0.6282', '0.5966', '0.5501', '0.5685']
cur_acc rrf_wo_na:  ['0.8493', '0.8509', '0.6703', '0.8031', '0.6082', '0.7733', '0.6526', '0.8188']
his_acc rrf_wo_na:  ['0.8493', '0.8204', '0.7229', '0.6837', '0.6089', '0.5867', '0.5424', '0.5615']
cur_acc_w_na:  ['0.6712', '0.4786', '0.3033', '0.4486', '0.2481', '0.4232', '0.3543', '0.5523']
his_acc_w_na:  ['0.6712', '0.5916', '0.4403', '0.4241', '0.3286', '0.3147', '0.2850', '0.3002']
cur_acc des_w_na:  ['0.6264', '0.4511', '0.3465', '0.3464', '0.2348', '0.3629', '0.2717', '0.3522']
his_acc des_w_na:  ['0.6264', '0.5059', '0.4232', '0.3497', '0.2910', '0.2517', '0.2244', '0.2294']
cur_acc rrf_w_na:  ['0.6264', '0.4628', '0.3524', '0.3544', '0.2436', '0.3783', '0.2959', '0.3754']
his_acc rrf_w_na:  ['0.6264', '0.5103', '0.4227', '0.3481', '0.2898', '0.2562', '0.2291', '0.2370']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 127.5582940CurrentTrain: epoch  0, batch     1 | loss: 122.7646717CurrentTrain: epoch  0, batch     2 | loss: 152.6410170CurrentTrain: epoch  0, batch     3 | loss: 119.4125211CurrentTrain: epoch  0, batch     4 | loss: 120.1062100CurrentTrain: epoch  0, batch     5 | loss: 118.1205498CurrentTrain: epoch  0, batch     6 | loss: 117.8334845CurrentTrain: epoch  0, batch     7 | loss: 98.9416970CurrentTrain: epoch  0, batch     8 | loss: 117.9141573CurrentTrain: epoch  0, batch     9 | loss: 98.6462811CurrentTrain: epoch  0, batch    10 | loss: 115.7264949CurrentTrain: epoch  0, batch    11 | loss: 115.8084300CurrentTrain: epoch  0, batch    12 | loss: 190.4062389CurrentTrain: epoch  0, batch    13 | loss: 115.9952089CurrentTrain: epoch  0, batch    14 | loss: 97.8135361CurrentTrain: epoch  0, batch    15 | loss: 98.5772936CurrentTrain: epoch  0, batch    16 | loss: 97.6725476CurrentTrain: epoch  0, batch    17 | loss: 97.7349582CurrentTrain: epoch  0, batch    18 | loss: 189.9931392CurrentTrain: epoch  0, batch    19 | loss: 143.6879498CurrentTrain: epoch  0, batch    20 | loss: 115.3311284CurrentTrain: epoch  0, batch    21 | loss: 116.1880414CurrentTrain: epoch  0, batch    22 | loss: 97.5451133CurrentTrain: epoch  0, batch    23 | loss: 116.1062536CurrentTrain: epoch  0, batch    24 | loss: 97.4220026CurrentTrain: epoch  0, batch    25 | loss: 114.9792905CurrentTrain: epoch  0, batch    26 | loss: 97.5700136CurrentTrain: epoch  0, batch    27 | loss: 144.3449954CurrentTrain: epoch  0, batch    28 | loss: 281.0466589CurrentTrain: epoch  0, batch    29 | loss: 115.0458633CurrentTrain: epoch  0, batch    30 | loss: 114.8782829CurrentTrain: epoch  0, batch    31 | loss: 188.7729441CurrentTrain: epoch  0, batch    32 | loss: 187.9687540CurrentTrain: epoch  0, batch    33 | loss: 115.7702579CurrentTrain: epoch  0, batch    34 | loss: 114.8989034CurrentTrain: epoch  0, batch    35 | loss: 188.0020907CurrentTrain: epoch  0, batch    36 | loss: 143.8271701CurrentTrain: epoch  0, batch    37 | loss: 187.5793754CurrentTrain: epoch  0, batch    38 | loss: 82.8546785CurrentTrain: epoch  0, batch    39 | loss: 97.1579932CurrentTrain: epoch  0, batch    40 | loss: 96.1741872CurrentTrain: epoch  0, batch    41 | loss: 82.5917271CurrentTrain: epoch  0, batch    42 | loss: 113.5100859CurrentTrain: epoch  0, batch    43 | loss: 113.9884894CurrentTrain: epoch  0, batch    44 | loss: 113.9646787CurrentTrain: epoch  0, batch    45 | loss: 96.6184568CurrentTrain: epoch  0, batch    46 | loss: 142.3053046CurrentTrain: epoch  0, batch    47 | loss: 94.8150575CurrentTrain: epoch  0, batch    48 | loss: 113.4485460CurrentTrain: epoch  0, batch    49 | loss: 113.0964621CurrentTrain: epoch  0, batch    50 | loss: 187.0440003CurrentTrain: epoch  0, batch    51 | loss: 94.7188435CurrentTrain: epoch  0, batch    52 | loss: 95.4810400CurrentTrain: epoch  0, batch    53 | loss: 113.0561033CurrentTrain: epoch  0, batch    54 | loss: 186.5838927CurrentTrain: epoch  0, batch    55 | loss: 82.4315329CurrentTrain: epoch  0, batch    56 | loss: 141.2135725CurrentTrain: epoch  0, batch    57 | loss: 93.5995791CurrentTrain: epoch  0, batch    58 | loss: 140.5519135CurrentTrain: epoch  0, batch    59 | loss: 111.9981707CurrentTrain: epoch  0, batch    60 | loss: 141.3730036CurrentTrain: epoch  0, batch    61 | loss: 112.9646184CurrentTrain: epoch  0, batch    62 | loss: 94.9628192CurrentTrain: epoch  0, batch    63 | loss: 112.7897639CurrentTrain: epoch  0, batch    64 | loss: 113.4838332CurrentTrain: epoch  0, batch    65 | loss: 139.0219857CurrentTrain: epoch  0, batch    66 | loss: 112.6486795CurrentTrain: epoch  0, batch    67 | loss: 92.8842417CurrentTrain: epoch  0, batch    68 | loss: 136.7754901CurrentTrain: epoch  0, batch    69 | loss: 78.9901660CurrentTrain: epoch  0, batch    70 | loss: 111.4000436CurrentTrain: epoch  0, batch    71 | loss: 94.9893670CurrentTrain: epoch  0, batch    72 | loss: 111.4992030CurrentTrain: epoch  0, batch    73 | loss: 139.0227124CurrentTrain: epoch  0, batch    74 | loss: 92.5708930CurrentTrain: epoch  0, batch    75 | loss: 138.7768099CurrentTrain: epoch  0, batch    76 | loss: 140.4827068CurrentTrain: epoch  0, batch    77 | loss: 112.4056599CurrentTrain: epoch  0, batch    78 | loss: 92.4804898CurrentTrain: epoch  0, batch    79 | loss: 91.7246083CurrentTrain: epoch  0, batch    80 | loss: 109.8089119CurrentTrain: epoch  0, batch    81 | loss: 136.0614184CurrentTrain: epoch  0, batch    82 | loss: 184.8384829CurrentTrain: epoch  0, batch    83 | loss: 138.8800395CurrentTrain: epoch  0, batch    84 | loss: 111.4952327CurrentTrain: epoch  0, batch    85 | loss: 109.2046203CurrentTrain: epoch  0, batch    86 | loss: 135.8282567CurrentTrain: epoch  0, batch    87 | loss: 137.9413775CurrentTrain: epoch  0, batch    88 | loss: 88.4062583CurrentTrain: epoch  0, batch    89 | loss: 108.8724816CurrentTrain: epoch  0, batch    90 | loss: 106.9839284CurrentTrain: epoch  0, batch    91 | loss: 108.2879516CurrentTrain: epoch  0, batch    92 | loss: 108.7757033CurrentTrain: epoch  0, batch    93 | loss: 88.6591879CurrentTrain: epoch  0, batch    94 | loss: 136.4548607CurrentTrain: epoch  0, batch    95 | loss: 91.9565433CurrentTrain: epoch  1, batch     0 | loss: 108.7378142CurrentTrain: epoch  1, batch     1 | loss: 104.7742495CurrentTrain: epoch  1, batch     2 | loss: 109.7945084CurrentTrain: epoch  1, batch     3 | loss: 177.2751503CurrentTrain: epoch  1, batch     4 | loss: 90.4682334CurrentTrain: epoch  1, batch     5 | loss: 88.9803901CurrentTrain: epoch  1, batch     6 | loss: 105.8635374CurrentTrain: epoch  1, batch     7 | loss: 139.7733676CurrentTrain: epoch  1, batch     8 | loss: 89.0279895CurrentTrain: epoch  1, batch     9 | loss: 90.6701563CurrentTrain: epoch  1, batch    10 | loss: 84.1583109CurrentTrain: epoch  1, batch    11 | loss: 88.7701410CurrentTrain: epoch  1, batch    12 | loss: 89.1380106CurrentTrain: epoch  1, batch    13 | loss: 182.0902196CurrentTrain: epoch  1, batch    14 | loss: 109.2589215CurrentTrain: epoch  1, batch    15 | loss: 137.7926590CurrentTrain: epoch  1, batch    16 | loss: 73.0310233CurrentTrain: epoch  1, batch    17 | loss: 85.8357307CurrentTrain: epoch  1, batch    18 | loss: 87.5525116CurrentTrain: epoch  1, batch    19 | loss: 189.3021221CurrentTrain: epoch  1, batch    20 | loss: 281.5976320CurrentTrain: epoch  1, batch    21 | loss: 104.4336297CurrentTrain: epoch  1, batch    22 | loss: 132.0037067CurrentTrain: epoch  1, batch    23 | loss: 73.5610610CurrentTrain: epoch  1, batch    24 | loss: 182.0100670CurrentTrain: epoch  1, batch    25 | loss: 180.7877389CurrentTrain: epoch  1, batch    26 | loss: 71.8982859CurrentTrain: epoch  1, batch    27 | loss: 132.6130725CurrentTrain: epoch  1, batch    28 | loss: 108.4427764CurrentTrain: epoch  1, batch    29 | loss: 87.1438431CurrentTrain: epoch  1, batch    30 | loss: 103.6705855CurrentTrain: epoch  1, batch    31 | loss: 74.3601829CurrentTrain: epoch  1, batch    32 | loss: 105.2675453CurrentTrain: epoch  1, batch    33 | loss: 103.5711691CurrentTrain: epoch  1, batch    34 | loss: 131.2788923CurrentTrain: epoch  1, batch    35 | loss: 130.4953356CurrentTrain: epoch  1, batch    36 | loss: 71.4121602CurrentTrain: epoch  1, batch    37 | loss: 103.7772095CurrentTrain: epoch  1, batch    38 | loss: 105.2434441CurrentTrain: epoch  1, batch    39 | loss: 138.0680374CurrentTrain: epoch  1, batch    40 | loss: 261.4942639CurrentTrain: epoch  1, batch    41 | loss: 127.4935273CurrentTrain: epoch  1, batch    42 | loss: 90.0335897CurrentTrain: epoch  1, batch    43 | loss: 83.8786129CurrentTrain: epoch  1, batch    44 | loss: 134.0706272CurrentTrain: epoch  1, batch    45 | loss: 87.4705827CurrentTrain: epoch  1, batch    46 | loss: 105.7295328CurrentTrain: epoch  1, batch    47 | loss: 135.7773533CurrentTrain: epoch  1, batch    48 | loss: 102.7818238CurrentTrain: epoch  1, batch    49 | loss: 104.9088129CurrentTrain: epoch  1, batch    50 | loss: 104.2843670CurrentTrain: epoch  1, batch    51 | loss: 279.6871204CurrentTrain: epoch  1, batch    52 | loss: 72.7987718CurrentTrain: epoch  1, batch    53 | loss: 130.0056890CurrentTrain: epoch  1, batch    54 | loss: 104.0346584CurrentTrain: epoch  1, batch    55 | loss: 83.7639896CurrentTrain: epoch  1, batch    56 | loss: 73.1865025CurrentTrain: epoch  1, batch    57 | loss: 104.9338830CurrentTrain: epoch  1, batch    58 | loss: 72.1816020CurrentTrain: epoch  1, batch    59 | loss: 103.2153548CurrentTrain: epoch  1, batch    60 | loss: 106.7489743CurrentTrain: epoch  1, batch    61 | loss: 133.2705921CurrentTrain: epoch  1, batch    62 | loss: 277.5682349CurrentTrain: epoch  1, batch    63 | loss: 86.9088503CurrentTrain: epoch  1, batch    64 | loss: 181.5528501CurrentTrain: epoch  1, batch    65 | loss: 185.2911333CurrentTrain: epoch  1, batch    66 | loss: 87.9658957CurrentTrain: epoch  1, batch    67 | loss: 81.9289881CurrentTrain: epoch  1, batch    68 | loss: 104.4955036CurrentTrain: epoch  1, batch    69 | loss: 127.5858835CurrentTrain: epoch  1, batch    70 | loss: 104.6196489CurrentTrain: epoch  1, batch    71 | loss: 86.0625406CurrentTrain: epoch  1, batch    72 | loss: 111.2614218CurrentTrain: epoch  1, batch    73 | loss: 87.1990911CurrentTrain: epoch  1, batch    74 | loss: 105.6324312CurrentTrain: epoch  1, batch    75 | loss: 81.3416960CurrentTrain: epoch  1, batch    76 | loss: 273.8299882CurrentTrain: epoch  1, batch    77 | loss: 136.0803612CurrentTrain: epoch  1, batch    78 | loss: 70.4563028CurrentTrain: epoch  1, batch    79 | loss: 101.8486769CurrentTrain: epoch  1, batch    80 | loss: 84.3072314CurrentTrain: epoch  1, batch    81 | loss: 84.2232024CurrentTrain: epoch  1, batch    82 | loss: 84.5467876CurrentTrain: epoch  1, batch    83 | loss: 104.2679733CurrentTrain: epoch  1, batch    84 | loss: 129.2269617CurrentTrain: epoch  1, batch    85 | loss: 175.4165823CurrentTrain: epoch  1, batch    86 | loss: 184.9076856CurrentTrain: epoch  1, batch    87 | loss: 106.9551866CurrentTrain: epoch  1, batch    88 | loss: 106.5183445CurrentTrain: epoch  1, batch    89 | loss: 90.3189972CurrentTrain: epoch  1, batch    90 | loss: 109.6191898CurrentTrain: epoch  1, batch    91 | loss: 76.5366551CurrentTrain: epoch  1, batch    92 | loss: 130.2434674CurrentTrain: epoch  1, batch    93 | loss: 84.9639307CurrentTrain: epoch  1, batch    94 | loss: 268.8255686CurrentTrain: epoch  1, batch    95 | loss: 112.2263786CurrentTrain: epoch  2, batch     0 | loss: 130.1489261CurrentTrain: epoch  2, batch     1 | loss: 83.1662207CurrentTrain: epoch  2, batch     2 | loss: 85.2317583CurrentTrain: epoch  2, batch     3 | loss: 180.1716704CurrentTrain: epoch  2, batch     4 | loss: 179.9341346CurrentTrain: epoch  2, batch     5 | loss: 81.7574130CurrentTrain: epoch  2, batch     6 | loss: 178.5894564CurrentTrain: epoch  2, batch     7 | loss: 128.0725559CurrentTrain: epoch  2, batch     8 | loss: 134.1078175CurrentTrain: epoch  2, batch     9 | loss: 80.7817840CurrentTrain: epoch  2, batch    10 | loss: 105.0436900CurrentTrain: epoch  2, batch    11 | loss: 68.0764422CurrentTrain: epoch  2, batch    12 | loss: 83.1651764CurrentTrain: epoch  2, batch    13 | loss: 98.0772245CurrentTrain: epoch  2, batch    14 | loss: 83.4195245CurrentTrain: epoch  2, batch    15 | loss: 129.3977018CurrentTrain: epoch  2, batch    16 | loss: 103.9975180CurrentTrain: epoch  2, batch    17 | loss: 82.5089558CurrentTrain: epoch  2, batch    18 | loss: 88.3485493CurrentTrain: epoch  2, batch    19 | loss: 103.9554094CurrentTrain: epoch  2, batch    20 | loss: 94.8636804CurrentTrain: epoch  2, batch    21 | loss: 133.1628228CurrentTrain: epoch  2, batch    22 | loss: 102.6177468CurrentTrain: epoch  2, batch    23 | loss: 86.8566029CurrentTrain: epoch  2, batch    24 | loss: 70.3398653CurrentTrain: epoch  2, batch    25 | loss: 106.4998981CurrentTrain: epoch  2, batch    26 | loss: 104.9667152CurrentTrain: epoch  2, batch    27 | loss: 101.9596060CurrentTrain: epoch  2, batch    28 | loss: 101.1420189CurrentTrain: epoch  2, batch    29 | loss: 102.9836499CurrentTrain: epoch  2, batch    30 | loss: 133.8714342CurrentTrain: epoch  2, batch    31 | loss: 96.6499564CurrentTrain: epoch  2, batch    32 | loss: 68.6495860CurrentTrain: epoch  2, batch    33 | loss: 104.3972578CurrentTrain: epoch  2, batch    34 | loss: 86.3673860CurrentTrain: epoch  2, batch    35 | loss: 130.3027988CurrentTrain: epoch  2, batch    36 | loss: 70.2645546CurrentTrain: epoch  2, batch    37 | loss: 82.2496956CurrentTrain: epoch  2, batch    38 | loss: 130.7804154CurrentTrain: epoch  2, batch    39 | loss: 102.6811523CurrentTrain: epoch  2, batch    40 | loss: 85.3579761CurrentTrain: epoch  2, batch    41 | loss: 104.5276741CurrentTrain: epoch  2, batch    42 | loss: 101.9268661CurrentTrain: epoch  2, batch    43 | loss: 85.2546713CurrentTrain: epoch  2, batch    44 | loss: 99.8369289CurrentTrain: epoch  2, batch    45 | loss: 108.8592260CurrentTrain: epoch  2, batch    46 | loss: 129.7953161CurrentTrain: epoch  2, batch    47 | loss: 105.0862908CurrentTrain: epoch  2, batch    48 | loss: 100.6968298CurrentTrain: epoch  2, batch    49 | loss: 81.3456882CurrentTrain: epoch  2, batch    50 | loss: 180.0003635CurrentTrain: epoch  2, batch    51 | loss: 104.7970284CurrentTrain: epoch  2, batch    52 | loss: 134.3539468CurrentTrain: epoch  2, batch    53 | loss: 88.4243137CurrentTrain: epoch  2, batch    54 | loss: 100.1077036CurrentTrain: epoch  2, batch    55 | loss: 102.9280052CurrentTrain: epoch  2, batch    56 | loss: 81.4013808CurrentTrain: epoch  2, batch    57 | loss: 179.3057894CurrentTrain: epoch  2, batch    58 | loss: 101.3702955CurrentTrain: epoch  2, batch    59 | loss: 87.2923524CurrentTrain: epoch  2, batch    60 | loss: 272.5743442CurrentTrain: epoch  2, batch    61 | loss: 101.7791729CurrentTrain: epoch  2, batch    62 | loss: 128.8533302CurrentTrain: epoch  2, batch    63 | loss: 104.3153592CurrentTrain: epoch  2, batch    64 | loss: 98.9397005CurrentTrain: epoch  2, batch    65 | loss: 96.5242494CurrentTrain: epoch  2, batch    66 | loss: 106.0916781CurrentTrain: epoch  2, batch    67 | loss: 78.0861672CurrentTrain: epoch  2, batch    68 | loss: 130.9344355CurrentTrain: epoch  2, batch    69 | loss: 105.7911999CurrentTrain: epoch  2, batch    70 | loss: 84.3172877CurrentTrain: epoch  2, batch    71 | loss: 83.7841201CurrentTrain: epoch  2, batch    72 | loss: 172.3930448CurrentTrain: epoch  2, batch    73 | loss: 81.0931718CurrentTrain: epoch  2, batch    74 | loss: 128.9102446CurrentTrain: epoch  2, batch    75 | loss: 177.7962709CurrentTrain: epoch  2, batch    76 | loss: 133.9932161CurrentTrain: epoch  2, batch    77 | loss: 82.8026566CurrentTrain: epoch  2, batch    78 | loss: 84.1120999CurrentTrain: epoch  2, batch    79 | loss: 171.6128007CurrentTrain: epoch  2, batch    80 | loss: 100.3277224CurrentTrain: epoch  2, batch    81 | loss: 97.6712664CurrentTrain: epoch  2, batch    82 | loss: 129.5141460CurrentTrain: epoch  2, batch    83 | loss: 68.0230438CurrentTrain: epoch  2, batch    84 | loss: 130.0083723CurrentTrain: epoch  2, batch    85 | loss: 131.2135115CurrentTrain: epoch  2, batch    86 | loss: 125.5644969CurrentTrain: epoch  2, batch    87 | loss: 130.7123449CurrentTrain: epoch  2, batch    88 | loss: 82.0818456CurrentTrain: epoch  2, batch    89 | loss: 129.2086901CurrentTrain: epoch  2, batch    90 | loss: 103.5593911CurrentTrain: epoch  2, batch    91 | loss: 80.7408997CurrentTrain: epoch  2, batch    92 | loss: 82.4709941CurrentTrain: epoch  2, batch    93 | loss: 119.8906715CurrentTrain: epoch  2, batch    94 | loss: 104.0698969CurrentTrain: epoch  2, batch    95 | loss: 69.2391488CurrentTrain: epoch  3, batch     0 | loss: 77.0808177CurrentTrain: epoch  3, batch     1 | loss: 130.6255658CurrentTrain: epoch  3, batch     2 | loss: 101.3265329CurrentTrain: epoch  3, batch     3 | loss: 69.5590739CurrentTrain: epoch  3, batch     4 | loss: 103.6704125CurrentTrain: epoch  3, batch     5 | loss: 82.0047662CurrentTrain: epoch  3, batch     6 | loss: 99.2170265CurrentTrain: epoch  3, batch     7 | loss: 177.1821966CurrentTrain: epoch  3, batch     8 | loss: 178.5110977CurrentTrain: epoch  3, batch     9 | loss: 81.3833748CurrentTrain: epoch  3, batch    10 | loss: 98.9152894CurrentTrain: epoch  3, batch    11 | loss: 128.0327339CurrentTrain: epoch  3, batch    12 | loss: 84.2004148CurrentTrain: epoch  3, batch    13 | loss: 103.6770891CurrentTrain: epoch  3, batch    14 | loss: 68.3428970CurrentTrain: epoch  3, batch    15 | loss: 96.5765216CurrentTrain: epoch  3, batch    16 | loss: 101.0512981CurrentTrain: epoch  3, batch    17 | loss: 77.9338683CurrentTrain: epoch  3, batch    18 | loss: 70.3180594CurrentTrain: epoch  3, batch    19 | loss: 122.6307474CurrentTrain: epoch  3, batch    20 | loss: 131.1613698CurrentTrain: epoch  3, batch    21 | loss: 103.2099616CurrentTrain: epoch  3, batch    22 | loss: 79.3049476CurrentTrain: epoch  3, batch    23 | loss: 77.0276761CurrentTrain: epoch  3, batch    24 | loss: 131.6127576CurrentTrain: epoch  3, batch    25 | loss: 172.6876565CurrentTrain: epoch  3, batch    26 | loss: 80.9933033CurrentTrain: epoch  3, batch    27 | loss: 132.0017513CurrentTrain: epoch  3, batch    28 | loss: 82.1430753CurrentTrain: epoch  3, batch    29 | loss: 133.6130565CurrentTrain: epoch  3, batch    30 | loss: 177.3766426CurrentTrain: epoch  3, batch    31 | loss: 130.8420537CurrentTrain: epoch  3, batch    32 | loss: 81.1640276CurrentTrain: epoch  3, batch    33 | loss: 83.2685135CurrentTrain: epoch  3, batch    34 | loss: 129.2728404CurrentTrain: epoch  3, batch    35 | loss: 79.4173611CurrentTrain: epoch  3, batch    36 | loss: 99.2082558CurrentTrain: epoch  3, batch    37 | loss: 176.5175471CurrentTrain: epoch  3, batch    38 | loss: 106.0572266CurrentTrain: epoch  3, batch    39 | loss: 129.3373571CurrentTrain: epoch  3, batch    40 | loss: 78.4397586CurrentTrain: epoch  3, batch    41 | loss: 127.9117206CurrentTrain: epoch  3, batch    42 | loss: 102.0592453CurrentTrain: epoch  3, batch    43 | loss: 126.5277657CurrentTrain: epoch  3, batch    44 | loss: 83.7112817CurrentTrain: epoch  3, batch    45 | loss: 101.1428614CurrentTrain: epoch  3, batch    46 | loss: 81.7187925CurrentTrain: epoch  3, batch    47 | loss: 92.4256380CurrentTrain: epoch  3, batch    48 | loss: 67.8804672CurrentTrain: epoch  3, batch    49 | loss: 106.1857310CurrentTrain: epoch  3, batch    50 | loss: 79.8763500CurrentTrain: epoch  3, batch    51 | loss: 102.4846840CurrentTrain: epoch  3, batch    52 | loss: 177.9753645CurrentTrain: epoch  3, batch    53 | loss: 79.6828065CurrentTrain: epoch  3, batch    54 | loss: 107.5888407CurrentTrain: epoch  3, batch    55 | loss: 82.4338400CurrentTrain: epoch  3, batch    56 | loss: 127.7467791CurrentTrain: epoch  3, batch    57 | loss: 100.0142067CurrentTrain: epoch  3, batch    58 | loss: 70.7452703CurrentTrain: epoch  3, batch    59 | loss: 131.1046773CurrentTrain: epoch  3, batch    60 | loss: 95.4609923CurrentTrain: epoch  3, batch    61 | loss: 98.2033869CurrentTrain: epoch  3, batch    62 | loss: 130.4989498CurrentTrain: epoch  3, batch    63 | loss: 103.6798177CurrentTrain: epoch  3, batch    64 | loss: 78.5975790CurrentTrain: epoch  3, batch    65 | loss: 81.4448576CurrentTrain: epoch  3, batch    66 | loss: 70.4745870CurrentTrain: epoch  3, batch    67 | loss: 130.2247590CurrentTrain: epoch  3, batch    68 | loss: 80.8755550CurrentTrain: epoch  3, batch    69 | loss: 81.9471190CurrentTrain: epoch  3, batch    70 | loss: 78.8465566CurrentTrain: epoch  3, batch    71 | loss: 135.4587870CurrentTrain: epoch  3, batch    72 | loss: 104.0144393CurrentTrain: epoch  3, batch    73 | loss: 81.4992355CurrentTrain: epoch  3, batch    74 | loss: 98.7854729CurrentTrain: epoch  3, batch    75 | loss: 130.2697566CurrentTrain: epoch  3, batch    76 | loss: 99.9985179CurrentTrain: epoch  3, batch    77 | loss: 124.7125743CurrentTrain: epoch  3, batch    78 | loss: 80.0332766CurrentTrain: epoch  3, batch    79 | loss: 100.5162388CurrentTrain: epoch  3, batch    80 | loss: 105.3408884CurrentTrain: epoch  3, batch    81 | loss: 95.9604317CurrentTrain: epoch  3, batch    82 | loss: 82.9954059CurrentTrain: epoch  3, batch    83 | loss: 104.5630779CurrentTrain: epoch  3, batch    84 | loss: 101.2537338CurrentTrain: epoch  3, batch    85 | loss: 127.0202119CurrentTrain: epoch  3, batch    86 | loss: 99.7197611CurrentTrain: epoch  3, batch    87 | loss: 100.2086693CurrentTrain: epoch  3, batch    88 | loss: 128.5059749CurrentTrain: epoch  3, batch    89 | loss: 123.9177766CurrentTrain: epoch  3, batch    90 | loss: 98.1160243CurrentTrain: epoch  3, batch    91 | loss: 173.9606936CurrentTrain: epoch  3, batch    92 | loss: 129.0511799CurrentTrain: epoch  3, batch    93 | loss: 176.4073428CurrentTrain: epoch  3, batch    94 | loss: 169.3137812CurrentTrain: epoch  3, batch    95 | loss: 105.7563144CurrentTrain: epoch  4, batch     0 | loss: 125.8878484CurrentTrain: epoch  4, batch     1 | loss: 71.3156971CurrentTrain: epoch  4, batch     2 | loss: 76.6095583CurrentTrain: epoch  4, batch     3 | loss: 128.2225990CurrentTrain: epoch  4, batch     4 | loss: 98.0301533CurrentTrain: epoch  4, batch     5 | loss: 99.2249398CurrentTrain: epoch  4, batch     6 | loss: 100.3819945CurrentTrain: epoch  4, batch     7 | loss: 273.4346820CurrentTrain: epoch  4, batch     8 | loss: 101.0331273CurrentTrain: epoch  4, batch     9 | loss: 99.0809155CurrentTrain: epoch  4, batch    10 | loss: 98.9532967CurrentTrain: epoch  4, batch    11 | loss: 75.7488663CurrentTrain: epoch  4, batch    12 | loss: 81.3290388CurrentTrain: epoch  4, batch    13 | loss: 129.9611974CurrentTrain: epoch  4, batch    14 | loss: 100.1698942CurrentTrain: epoch  4, batch    15 | loss: 97.3911643CurrentTrain: epoch  4, batch    16 | loss: 77.9703617CurrentTrain: epoch  4, batch    17 | loss: 99.6402759CurrentTrain: epoch  4, batch    18 | loss: 175.6326314CurrentTrain: epoch  4, batch    19 | loss: 126.4383895CurrentTrain: epoch  4, batch    20 | loss: 93.3267575CurrentTrain: epoch  4, batch    21 | loss: 98.5173414CurrentTrain: epoch  4, batch    22 | loss: 64.0358949CurrentTrain: epoch  4, batch    23 | loss: 100.7623299CurrentTrain: epoch  4, batch    24 | loss: 125.5939324CurrentTrain: epoch  4, batch    25 | loss: 98.5089238CurrentTrain: epoch  4, batch    26 | loss: 93.3132453CurrentTrain: epoch  4, batch    27 | loss: 81.4528777CurrentTrain: epoch  4, batch    28 | loss: 99.6527288CurrentTrain: epoch  4, batch    29 | loss: 99.1558942CurrentTrain: epoch  4, batch    30 | loss: 127.7500412CurrentTrain: epoch  4, batch    31 | loss: 82.8407635CurrentTrain: epoch  4, batch    32 | loss: 82.8530487CurrentTrain: epoch  4, batch    33 | loss: 98.9453691CurrentTrain: epoch  4, batch    34 | loss: 127.3955627CurrentTrain: epoch  4, batch    35 | loss: 96.3884958CurrentTrain: epoch  4, batch    36 | loss: 124.8045081CurrentTrain: epoch  4, batch    37 | loss: 81.4349647CurrentTrain: epoch  4, batch    38 | loss: 99.3202821CurrentTrain: epoch  4, batch    39 | loss: 130.4953305CurrentTrain: epoch  4, batch    40 | loss: 102.3433188CurrentTrain: epoch  4, batch    41 | loss: 95.0159064CurrentTrain: epoch  4, batch    42 | loss: 125.3464455CurrentTrain: epoch  4, batch    43 | loss: 79.9874510CurrentTrain: epoch  4, batch    44 | loss: 176.9842522CurrentTrain: epoch  4, batch    45 | loss: 127.9985238CurrentTrain: epoch  4, batch    46 | loss: 74.1386525CurrentTrain: epoch  4, batch    47 | loss: 124.8535377CurrentTrain: epoch  4, batch    48 | loss: 79.7944972CurrentTrain: epoch  4, batch    49 | loss: 124.1376811CurrentTrain: epoch  4, batch    50 | loss: 80.3823062CurrentTrain: epoch  4, batch    51 | loss: 94.5595593CurrentTrain: epoch  4, batch    52 | loss: 176.3923527CurrentTrain: epoch  4, batch    53 | loss: 68.3461355CurrentTrain: epoch  4, batch    54 | loss: 97.4821501CurrentTrain: epoch  4, batch    55 | loss: 97.6470704CurrentTrain: epoch  4, batch    56 | loss: 67.4271561CurrentTrain: epoch  4, batch    57 | loss: 65.4032436CurrentTrain: epoch  4, batch    58 | loss: 69.6467993CurrentTrain: epoch  4, batch    59 | loss: 179.6605544CurrentTrain: epoch  4, batch    60 | loss: 172.9072490CurrentTrain: epoch  4, batch    61 | loss: 128.3755914CurrentTrain: epoch  4, batch    62 | loss: 128.8573125CurrentTrain: epoch  4, batch    63 | loss: 66.0733774CurrentTrain: epoch  4, batch    64 | loss: 177.3252672CurrentTrain: epoch  4, batch    65 | loss: 76.1598197CurrentTrain: epoch  4, batch    66 | loss: 101.1209275CurrentTrain: epoch  4, batch    67 | loss: 125.0068414CurrentTrain: epoch  4, batch    68 | loss: 126.9766299CurrentTrain: epoch  4, batch    69 | loss: 126.2986968CurrentTrain: epoch  4, batch    70 | loss: 101.0170378CurrentTrain: epoch  4, batch    71 | loss: 94.6257414CurrentTrain: epoch  4, batch    72 | loss: 71.6441814CurrentTrain: epoch  4, batch    73 | loss: 99.5206669CurrentTrain: epoch  4, batch    74 | loss: 174.9897033CurrentTrain: epoch  4, batch    75 | loss: 104.4798707CurrentTrain: epoch  4, batch    76 | loss: 176.8834535CurrentTrain: epoch  4, batch    77 | loss: 83.7128936CurrentTrain: epoch  4, batch    78 | loss: 67.7594034CurrentTrain: epoch  4, batch    79 | loss: 81.1384688CurrentTrain: epoch  4, batch    80 | loss: 100.8556625CurrentTrain: epoch  4, batch    81 | loss: 100.3494180CurrentTrain: epoch  4, batch    82 | loss: 126.8970118CurrentTrain: epoch  4, batch    83 | loss: 168.2118525CurrentTrain: epoch  4, batch    84 | loss: 96.8165605CurrentTrain: epoch  4, batch    85 | loss: 128.3377417CurrentTrain: epoch  4, batch    86 | loss: 173.7882067CurrentTrain: epoch  4, batch    87 | loss: 179.1405625CurrentTrain: epoch  4, batch    88 | loss: 83.4179734CurrentTrain: epoch  4, batch    89 | loss: 98.7787019CurrentTrain: epoch  4, batch    90 | loss: 86.3178505CurrentTrain: epoch  4, batch    91 | loss: 170.2276072CurrentTrain: epoch  4, batch    92 | loss: 83.8670024CurrentTrain: epoch  4, batch    93 | loss: 99.5126336CurrentTrain: epoch  4, batch    94 | loss: 84.3473196CurrentTrain: epoch  4, batch    95 | loss: 82.6780730CurrentTrain: epoch  5, batch     0 | loss: 101.9493008CurrentTrain: epoch  5, batch     1 | loss: 127.7011141CurrentTrain: epoch  5, batch     2 | loss: 82.0211749CurrentTrain: epoch  5, batch     3 | loss: 98.7602041CurrentTrain: epoch  5, batch     4 | loss: 125.0781430CurrentTrain: epoch  5, batch     5 | loss: 101.0780440CurrentTrain: epoch  5, batch     6 | loss: 80.2577489CurrentTrain: epoch  5, batch     7 | loss: 97.6461980CurrentTrain: epoch  5, batch     8 | loss: 176.3670884CurrentTrain: epoch  5, batch     9 | loss: 78.2525367CurrentTrain: epoch  5, batch    10 | loss: 100.7547160CurrentTrain: epoch  5, batch    11 | loss: 101.4865938CurrentTrain: epoch  5, batch    12 | loss: 92.7146199CurrentTrain: epoch  5, batch    13 | loss: 98.7495980CurrentTrain: epoch  5, batch    14 | loss: 97.7036166CurrentTrain: epoch  5, batch    15 | loss: 82.9685588CurrentTrain: epoch  5, batch    16 | loss: 98.7373294CurrentTrain: epoch  5, batch    17 | loss: 66.8361963CurrentTrain: epoch  5, batch    18 | loss: 63.8864443CurrentTrain: epoch  5, batch    19 | loss: 124.6595039CurrentTrain: epoch  5, batch    20 | loss: 125.1714590CurrentTrain: epoch  5, batch    21 | loss: 80.4616171CurrentTrain: epoch  5, batch    22 | loss: 80.4689389CurrentTrain: epoch  5, batch    23 | loss: 70.7532792CurrentTrain: epoch  5, batch    24 | loss: 129.5870367CurrentTrain: epoch  5, batch    25 | loss: 97.7270801CurrentTrain: epoch  5, batch    26 | loss: 124.6981135CurrentTrain: epoch  5, batch    27 | loss: 101.3933139CurrentTrain: epoch  5, batch    28 | loss: 80.5468878CurrentTrain: epoch  5, batch    29 | loss: 131.1385191CurrentTrain: epoch  5, batch    30 | loss: 127.9322785CurrentTrain: epoch  5, batch    31 | loss: 100.0509681CurrentTrain: epoch  5, batch    32 | loss: 103.0577985CurrentTrain: epoch  5, batch    33 | loss: 100.6325760CurrentTrain: epoch  5, batch    34 | loss: 99.5668290CurrentTrain: epoch  5, batch    35 | loss: 100.9288052CurrentTrain: epoch  5, batch    36 | loss: 94.2217622CurrentTrain: epoch  5, batch    37 | loss: 123.1524355CurrentTrain: epoch  5, batch    38 | loss: 99.0101946CurrentTrain: epoch  5, batch    39 | loss: 81.6021476CurrentTrain: epoch  5, batch    40 | loss: 100.3377966CurrentTrain: epoch  5, batch    41 | loss: 101.0156614CurrentTrain: epoch  5, batch    42 | loss: 94.3327737CurrentTrain: epoch  5, batch    43 | loss: 95.1264885CurrentTrain: epoch  5, batch    44 | loss: 102.4857671CurrentTrain: epoch  5, batch    45 | loss: 66.8149652CurrentTrain: epoch  5, batch    46 | loss: 97.5187656CurrentTrain: epoch  5, batch    47 | loss: 80.5877672CurrentTrain: epoch  5, batch    48 | loss: 175.2207337CurrentTrain: epoch  5, batch    49 | loss: 125.6592737CurrentTrain: epoch  5, batch    50 | loss: 100.7989636CurrentTrain: epoch  5, batch    51 | loss: 97.2883343CurrentTrain: epoch  5, batch    52 | loss: 95.0851702CurrentTrain: epoch  5, batch    53 | loss: 76.9593722CurrentTrain: epoch  5, batch    54 | loss: 83.7684865CurrentTrain: epoch  5, batch    55 | loss: 77.2131079CurrentTrain: epoch  5, batch    56 | loss: 117.6377114CurrentTrain: epoch  5, batch    57 | loss: 97.8189423CurrentTrain: epoch  5, batch    58 | loss: 176.8236331CurrentTrain: epoch  5, batch    59 | loss: 98.9999695CurrentTrain: epoch  5, batch    60 | loss: 124.0772240CurrentTrain: epoch  5, batch    61 | loss: 95.0642091CurrentTrain: epoch  5, batch    62 | loss: 100.7441239CurrentTrain: epoch  5, batch    63 | loss: 77.5401669CurrentTrain: epoch  5, batch    64 | loss: 81.6749913CurrentTrain: epoch  5, batch    65 | loss: 81.4674367CurrentTrain: epoch  5, batch    66 | loss: 98.2182358CurrentTrain: epoch  5, batch    67 | loss: 171.6015364CurrentTrain: epoch  5, batch    68 | loss: 123.9376792CurrentTrain: epoch  5, batch    69 | loss: 64.8786119CurrentTrain: epoch  5, batch    70 | loss: 78.9942076CurrentTrain: epoch  5, batch    71 | loss: 69.4778903CurrentTrain: epoch  5, batch    72 | loss: 75.9391787CurrentTrain: epoch  5, batch    73 | loss: 81.4171947CurrentTrain: epoch  5, batch    74 | loss: 100.7059628CurrentTrain: epoch  5, batch    75 | loss: 94.7710197CurrentTrain: epoch  5, batch    76 | loss: 119.8351164CurrentTrain: epoch  5, batch    77 | loss: 126.9493576CurrentTrain: epoch  5, batch    78 | loss: 99.5816484CurrentTrain: epoch  5, batch    79 | loss: 77.6813120CurrentTrain: epoch  5, batch    80 | loss: 100.7373017CurrentTrain: epoch  5, batch    81 | loss: 83.1251427CurrentTrain: epoch  5, batch    82 | loss: 97.7586181CurrentTrain: epoch  5, batch    83 | loss: 100.5192698CurrentTrain: epoch  5, batch    84 | loss: 124.6769613CurrentTrain: epoch  5, batch    85 | loss: 97.9935401CurrentTrain: epoch  5, batch    86 | loss: 93.5428308CurrentTrain: epoch  5, batch    87 | loss: 136.1692557CurrentTrain: epoch  5, batch    88 | loss: 123.3771699CurrentTrain: epoch  5, batch    89 | loss: 125.3391399CurrentTrain: epoch  5, batch    90 | loss: 66.1883839CurrentTrain: epoch  5, batch    91 | loss: 66.6256037CurrentTrain: epoch  5, batch    92 | loss: 128.9229758CurrentTrain: epoch  5, batch    93 | loss: 100.6447370CurrentTrain: epoch  5, batch    94 | loss: 80.2419790CurrentTrain: epoch  5, batch    95 | loss: 106.1499612CurrentTrain: epoch  6, batch     0 | loss: 96.6621455CurrentTrain: epoch  6, batch     1 | loss: 91.1946596CurrentTrain: epoch  6, batch     2 | loss: 128.7468573CurrentTrain: epoch  6, batch     3 | loss: 66.6264723CurrentTrain: epoch  6, batch     4 | loss: 62.0041744CurrentTrain: epoch  6, batch     5 | loss: 168.8059289CurrentTrain: epoch  6, batch     6 | loss: 99.4304038CurrentTrain: epoch  6, batch     7 | loss: 272.6392746CurrentTrain: epoch  6, batch     8 | loss: 78.4163506CurrentTrain: epoch  6, batch     9 | loss: 129.4215663CurrentTrain: epoch  6, batch    10 | loss: 102.4067512CurrentTrain: epoch  6, batch    11 | loss: 92.3570709CurrentTrain: epoch  6, batch    12 | loss: 128.1434067CurrentTrain: epoch  6, batch    13 | loss: 81.8706146CurrentTrain: epoch  6, batch    14 | loss: 132.5211749CurrentTrain: epoch  6, batch    15 | loss: 97.4849520CurrentTrain: epoch  6, batch    16 | loss: 96.9261298CurrentTrain: epoch  6, batch    17 | loss: 91.1488083CurrentTrain: epoch  6, batch    18 | loss: 101.8224234CurrentTrain: epoch  6, batch    19 | loss: 67.4297737CurrentTrain: epoch  6, batch    20 | loss: 66.8685504CurrentTrain: epoch  6, batch    21 | loss: 122.6952653CurrentTrain: epoch  6, batch    22 | loss: 99.4778816CurrentTrain: epoch  6, batch    23 | loss: 176.4371091CurrentTrain: epoch  6, batch    24 | loss: 83.1730509CurrentTrain: epoch  6, batch    25 | loss: 80.2644051CurrentTrain: epoch  6, batch    26 | loss: 169.7277003CurrentTrain: epoch  6, batch    27 | loss: 99.5813381CurrentTrain: epoch  6, batch    28 | loss: 99.7045860CurrentTrain: epoch  6, batch    29 | loss: 98.5954572CurrentTrain: epoch  6, batch    30 | loss: 95.7455576CurrentTrain: epoch  6, batch    31 | loss: 98.4401176CurrentTrain: epoch  6, batch    32 | loss: 100.5543816CurrentTrain: epoch  6, batch    33 | loss: 167.2953617CurrentTrain: epoch  6, batch    34 | loss: 98.3384925CurrentTrain: epoch  6, batch    35 | loss: 103.2758925CurrentTrain: epoch  6, batch    36 | loss: 125.6653373CurrentTrain: epoch  6, batch    37 | loss: 95.8374102CurrentTrain: epoch  6, batch    38 | loss: 77.6876872CurrentTrain: epoch  6, batch    39 | loss: 121.2740293CurrentTrain: epoch  6, batch    40 | loss: 66.8059822CurrentTrain: epoch  6, batch    41 | loss: 122.2066960CurrentTrain: epoch  6, batch    42 | loss: 78.0594278CurrentTrain: epoch  6, batch    43 | loss: 121.8831250CurrentTrain: epoch  6, batch    44 | loss: 126.3852894CurrentTrain: epoch  6, batch    45 | loss: 75.1705580CurrentTrain: epoch  6, batch    46 | loss: 63.5805359CurrentTrain: epoch  6, batch    47 | loss: 97.0994956CurrentTrain: epoch  6, batch    48 | loss: 77.5335883CurrentTrain: epoch  6, batch    49 | loss: 97.9537636CurrentTrain: epoch  6, batch    50 | loss: 81.0548088CurrentTrain: epoch  6, batch    51 | loss: 78.3388309CurrentTrain: epoch  6, batch    52 | loss: 66.8599898CurrentTrain: epoch  6, batch    53 | loss: 127.5427144CurrentTrain: epoch  6, batch    54 | loss: 130.3457311CurrentTrain: epoch  6, batch    55 | loss: 82.0018866CurrentTrain: epoch  6, batch    56 | loss: 64.9697022CurrentTrain: epoch  6, batch    57 | loss: 262.1483772CurrentTrain: epoch  6, batch    58 | loss: 98.6689962CurrentTrain: epoch  6, batch    59 | loss: 123.6368938CurrentTrain: epoch  6, batch    60 | loss: 178.3377296CurrentTrain: epoch  6, batch    61 | loss: 102.6264580CurrentTrain: epoch  6, batch    62 | loss: 130.3480082CurrentTrain: epoch  6, batch    63 | loss: 120.7527612CurrentTrain: epoch  6, batch    64 | loss: 82.5443259CurrentTrain: epoch  6, batch    65 | loss: 128.6228752CurrentTrain: epoch  6, batch    66 | loss: 95.4495467CurrentTrain: epoch  6, batch    67 | loss: 122.1033533CurrentTrain: epoch  6, batch    68 | loss: 95.8542441CurrentTrain: epoch  6, batch    69 | loss: 80.9541433CurrentTrain: epoch  6, batch    70 | loss: 102.6768252CurrentTrain: epoch  6, batch    71 | loss: 77.3085571CurrentTrain: epoch  6, batch    72 | loss: 97.5410692CurrentTrain: epoch  6, batch    73 | loss: 122.1223521CurrentTrain: epoch  6, batch    74 | loss: 99.2732341CurrentTrain: epoch  6, batch    75 | loss: 125.5253881CurrentTrain: epoch  6, batch    76 | loss: 81.2667296CurrentTrain: epoch  6, batch    77 | loss: 122.8341287CurrentTrain: epoch  6, batch    78 | loss: 80.2576369CurrentTrain: epoch  6, batch    79 | loss: 98.9379120CurrentTrain: epoch  6, batch    80 | loss: 124.6941007CurrentTrain: epoch  6, batch    81 | loss: 80.3535233CurrentTrain: epoch  6, batch    82 | loss: 78.4979787CurrentTrain: epoch  6, batch    83 | loss: 96.3752657CurrentTrain: epoch  6, batch    84 | loss: 76.7782005CurrentTrain: epoch  6, batch    85 | loss: 167.0115257CurrentTrain: epoch  6, batch    86 | loss: 80.1370773CurrentTrain: epoch  6, batch    87 | loss: 128.9074712CurrentTrain: epoch  6, batch    88 | loss: 169.6857298CurrentTrain: epoch  6, batch    89 | loss: 102.7807540CurrentTrain: epoch  6, batch    90 | loss: 97.7069584CurrentTrain: epoch  6, batch    91 | loss: 67.2400404CurrentTrain: epoch  6, batch    92 | loss: 97.3903916CurrentTrain: epoch  6, batch    93 | loss: 78.2924361CurrentTrain: epoch  6, batch    94 | loss: 95.3435493CurrentTrain: epoch  6, batch    95 | loss: 82.1075686CurrentTrain: epoch  7, batch     0 | loss: 79.1019085CurrentTrain: epoch  7, batch     1 | loss: 95.0432758CurrentTrain: epoch  7, batch     2 | loss: 125.9265624CurrentTrain: epoch  7, batch     3 | loss: 81.2339631CurrentTrain: epoch  7, batch     4 | loss: 126.6543414CurrentTrain: epoch  7, batch     5 | loss: 126.4304638CurrentTrain: epoch  7, batch     6 | loss: 124.8007186CurrentTrain: epoch  7, batch     7 | loss: 78.9830709CurrentTrain: epoch  7, batch     8 | loss: 119.4476543CurrentTrain: epoch  7, batch     9 | loss: 62.2103479CurrentTrain: epoch  7, batch    10 | loss: 122.8630757CurrentTrain: epoch  7, batch    11 | loss: 97.0581103CurrentTrain: epoch  7, batch    12 | loss: 176.1782588CurrentTrain: epoch  7, batch    13 | loss: 76.2808099CurrentTrain: epoch  7, batch    14 | loss: 95.2482664CurrentTrain: epoch  7, batch    15 | loss: 99.0289578CurrentTrain: epoch  7, batch    16 | loss: 91.9426781CurrentTrain: epoch  7, batch    17 | loss: 123.3401389CurrentTrain: epoch  7, batch    18 | loss: 121.0236975CurrentTrain: epoch  7, batch    19 | loss: 77.1773919CurrentTrain: epoch  7, batch    20 | loss: 170.8538973CurrentTrain: epoch  7, batch    21 | loss: 123.9465373CurrentTrain: epoch  7, batch    22 | loss: 126.3128695CurrentTrain: epoch  7, batch    23 | loss: 128.9380898CurrentTrain: epoch  7, batch    24 | loss: 98.1362739CurrentTrain: epoch  7, batch    25 | loss: 124.2196884CurrentTrain: epoch  7, batch    26 | loss: 126.8715009CurrentTrain: epoch  7, batch    27 | loss: 177.0509719CurrentTrain: epoch  7, batch    28 | loss: 79.4892694CurrentTrain: epoch  7, batch    29 | loss: 131.4147426CurrentTrain: epoch  7, batch    30 | loss: 100.2947950CurrentTrain: epoch  7, batch    31 | loss: 95.1511001CurrentTrain: epoch  7, batch    32 | loss: 77.4659798CurrentTrain: epoch  7, batch    33 | loss: 63.7152203CurrentTrain: epoch  7, batch    34 | loss: 81.3972428CurrentTrain: epoch  7, batch    35 | loss: 176.2602783CurrentTrain: epoch  7, batch    36 | loss: 126.3287443CurrentTrain: epoch  7, batch    37 | loss: 176.4312288CurrentTrain: epoch  7, batch    38 | loss: 75.6950712CurrentTrain: epoch  7, batch    39 | loss: 128.5904546CurrentTrain: epoch  7, batch    40 | loss: 96.2109810CurrentTrain: epoch  7, batch    41 | loss: 100.5334795CurrentTrain: epoch  7, batch    42 | loss: 126.2128773CurrentTrain: epoch  7, batch    43 | loss: 176.7321060CurrentTrain: epoch  7, batch    44 | loss: 95.2748571CurrentTrain: epoch  7, batch    45 | loss: 173.6024963CurrentTrain: epoch  7, batch    46 | loss: 97.8231899CurrentTrain: epoch  7, batch    47 | loss: 100.1756644CurrentTrain: epoch  7, batch    48 | loss: 64.0292696CurrentTrain: epoch  7, batch    49 | loss: 68.7209617CurrentTrain: epoch  7, batch    50 | loss: 121.7557905CurrentTrain: epoch  7, batch    51 | loss: 122.5566844CurrentTrain: epoch  7, batch    52 | loss: 126.3488421CurrentTrain: epoch  7, batch    53 | loss: 126.8072601CurrentTrain: epoch  7, batch    54 | loss: 100.7744851CurrentTrain: epoch  7, batch    55 | loss: 79.7703403CurrentTrain: epoch  7, batch    56 | loss: 177.2975716CurrentTrain: epoch  7, batch    57 | loss: 76.4652489CurrentTrain: epoch  7, batch    58 | loss: 98.6732521CurrentTrain: epoch  7, batch    59 | loss: 91.3493820CurrentTrain: epoch  7, batch    60 | loss: 94.1425968CurrentTrain: epoch  7, batch    61 | loss: 95.6612250CurrentTrain: epoch  7, batch    62 | loss: 93.4843883CurrentTrain: epoch  7, batch    63 | loss: 74.4592997CurrentTrain: epoch  7, batch    64 | loss: 77.9516884CurrentTrain: epoch  7, batch    65 | loss: 174.5247086CurrentTrain: epoch  7, batch    66 | loss: 98.5525343CurrentTrain: epoch  7, batch    67 | loss: 100.8262641CurrentTrain: epoch  7, batch    68 | loss: 95.9209226CurrentTrain: epoch  7, batch    69 | loss: 66.0140977CurrentTrain: epoch  7, batch    70 | loss: 122.9837496CurrentTrain: epoch  7, batch    71 | loss: 89.2344469CurrentTrain: epoch  7, batch    72 | loss: 273.4466471CurrentTrain: epoch  7, batch    73 | loss: 92.4488208CurrentTrain: epoch  7, batch    74 | loss: 80.3050534CurrentTrain: epoch  7, batch    75 | loss: 75.1338311CurrentTrain: epoch  7, batch    76 | loss: 75.9345391CurrentTrain: epoch  7, batch    77 | loss: 124.7119291CurrentTrain: epoch  7, batch    78 | loss: 96.8516857CurrentTrain: epoch  7, batch    79 | loss: 168.4436523CurrentTrain: epoch  7, batch    80 | loss: 60.4467076CurrentTrain: epoch  7, batch    81 | loss: 129.1698655CurrentTrain: epoch  7, batch    82 | loss: 97.3297770CurrentTrain: epoch  7, batch    83 | loss: 79.7064831CurrentTrain: epoch  7, batch    84 | loss: 120.9113470CurrentTrain: epoch  7, batch    85 | loss: 124.7306320CurrentTrain: epoch  7, batch    86 | loss: 127.6213116CurrentTrain: epoch  7, batch    87 | loss: 176.7113227CurrentTrain: epoch  7, batch    88 | loss: 125.2252020CurrentTrain: epoch  7, batch    89 | loss: 93.3687768CurrentTrain: epoch  7, batch    90 | loss: 100.7700642CurrentTrain: epoch  7, batch    91 | loss: 98.6891956CurrentTrain: epoch  7, batch    92 | loss: 101.8160057CurrentTrain: epoch  7, batch    93 | loss: 95.4329262CurrentTrain: epoch  7, batch    94 | loss: 73.7692504CurrentTrain: epoch  7, batch    95 | loss: 103.1496044CurrentTrain: epoch  8, batch     0 | loss: 81.9655015CurrentTrain: epoch  8, batch     1 | loss: 126.6620901CurrentTrain: epoch  8, batch     2 | loss: 77.0664980CurrentTrain: epoch  8, batch     3 | loss: 79.4567838CurrentTrain: epoch  8, batch     4 | loss: 93.2366260CurrentTrain: epoch  8, batch     5 | loss: 122.5100750CurrentTrain: epoch  8, batch     6 | loss: 76.8369896CurrentTrain: epoch  8, batch     7 | loss: 96.3255468CurrentTrain: epoch  8, batch     8 | loss: 74.3422679CurrentTrain: epoch  8, batch     9 | loss: 119.4671532CurrentTrain: epoch  8, batch    10 | loss: 98.1944354CurrentTrain: epoch  8, batch    11 | loss: 173.0096902CurrentTrain: epoch  8, batch    12 | loss: 97.4475347CurrentTrain: epoch  8, batch    13 | loss: 93.0853560CurrentTrain: epoch  8, batch    14 | loss: 97.1753738CurrentTrain: epoch  8, batch    15 | loss: 100.0746728CurrentTrain: epoch  8, batch    16 | loss: 79.1202346CurrentTrain: epoch  8, batch    17 | loss: 69.6425289CurrentTrain: epoch  8, batch    18 | loss: 100.2693185CurrentTrain: epoch  8, batch    19 | loss: 127.1548762CurrentTrain: epoch  8, batch    20 | loss: 176.2521509CurrentTrain: epoch  8, batch    21 | loss: 99.9139687CurrentTrain: epoch  8, batch    22 | loss: 96.5621128CurrentTrain: epoch  8, batch    23 | loss: 93.1546620CurrentTrain: epoch  8, batch    24 | loss: 124.2609230CurrentTrain: epoch  8, batch    25 | loss: 97.1834546CurrentTrain: epoch  8, batch    26 | loss: 99.3284410CurrentTrain: epoch  8, batch    27 | loss: 92.2341478CurrentTrain: epoch  8, batch    28 | loss: 97.8573876CurrentTrain: epoch  8, batch    29 | loss: 64.5705906CurrentTrain: epoch  8, batch    30 | loss: 98.4417172CurrentTrain: epoch  8, batch    31 | loss: 97.7157905CurrentTrain: epoch  8, batch    32 | loss: 98.0751839CurrentTrain: epoch  8, batch    33 | loss: 78.8349160CurrentTrain: epoch  8, batch    34 | loss: 170.2631541CurrentTrain: epoch  8, batch    35 | loss: 66.0130532CurrentTrain: epoch  8, batch    36 | loss: 64.0755671CurrentTrain: epoch  8, batch    37 | loss: 125.2304652CurrentTrain: epoch  8, batch    38 | loss: 101.3839245CurrentTrain: epoch  8, batch    39 | loss: 79.2462803CurrentTrain: epoch  8, batch    40 | loss: 76.0383131CurrentTrain: epoch  8, batch    41 | loss: 98.7581129CurrentTrain: epoch  8, batch    42 | loss: 176.2891266CurrentTrain: epoch  8, batch    43 | loss: 79.6365531CurrentTrain: epoch  8, batch    44 | loss: 131.0485932CurrentTrain: epoch  8, batch    45 | loss: 97.3291484CurrentTrain: epoch  8, batch    46 | loss: 99.6488679CurrentTrain: epoch  8, batch    47 | loss: 100.3357616CurrentTrain: epoch  8, batch    48 | loss: 128.8866522CurrentTrain: epoch  8, batch    49 | loss: 99.3131057CurrentTrain: epoch  8, batch    50 | loss: 66.9656594CurrentTrain: epoch  8, batch    51 | loss: 80.0525693CurrentTrain: epoch  8, batch    52 | loss: 81.9141570CurrentTrain: epoch  8, batch    53 | loss: 75.8215349CurrentTrain: epoch  8, batch    54 | loss: 127.0935160CurrentTrain: epoch  8, batch    55 | loss: 172.9910645CurrentTrain: epoch  8, batch    56 | loss: 99.2196674CurrentTrain: epoch  8, batch    57 | loss: 80.3124756CurrentTrain: epoch  8, batch    58 | loss: 103.2050368CurrentTrain: epoch  8, batch    59 | loss: 97.7047455CurrentTrain: epoch  8, batch    60 | loss: 98.3463681CurrentTrain: epoch  8, batch    61 | loss: 179.7826755CurrentTrain: epoch  8, batch    62 | loss: 75.4480863CurrentTrain: epoch  8, batch    63 | loss: 78.6868489CurrentTrain: epoch  8, batch    64 | loss: 81.1723953CurrentTrain: epoch  8, batch    65 | loss: 126.2132492CurrentTrain: epoch  8, batch    66 | loss: 172.9380582CurrentTrain: epoch  8, batch    67 | loss: 123.6441557CurrentTrain: epoch  8, batch    68 | loss: 95.3674409CurrentTrain: epoch  8, batch    69 | loss: 91.6336294CurrentTrain: epoch  8, batch    70 | loss: 93.0983593CurrentTrain: epoch  8, batch    71 | loss: 81.4969676CurrentTrain: epoch  8, batch    72 | loss: 124.3675054CurrentTrain: epoch  8, batch    73 | loss: 124.8227345CurrentTrain: epoch  8, batch    74 | loss: 78.0393157CurrentTrain: epoch  8, batch    75 | loss: 97.2678273CurrentTrain: epoch  8, batch    76 | loss: 94.5432425CurrentTrain: epoch  8, batch    77 | loss: 95.4327517CurrentTrain: epoch  8, batch    78 | loss: 97.0274347CurrentTrain: epoch  8, batch    79 | loss: 126.6143178CurrentTrain: epoch  8, batch    80 | loss: 77.0825657CurrentTrain: epoch  8, batch    81 | loss: 97.3532902CurrentTrain: epoch  8, batch    82 | loss: 96.2725573CurrentTrain: epoch  8, batch    83 | loss: 98.2334996CurrentTrain: epoch  8, batch    84 | loss: 173.5618792CurrentTrain: epoch  8, batch    85 | loss: 99.4617937CurrentTrain: epoch  8, batch    86 | loss: 176.2920566CurrentTrain: epoch  8, batch    87 | loss: 81.6331735CurrentTrain: epoch  8, batch    88 | loss: 99.8461975CurrentTrain: epoch  8, batch    89 | loss: 121.2758008CurrentTrain: epoch  8, batch    90 | loss: 95.0577795CurrentTrain: epoch  8, batch    91 | loss: 77.6390332CurrentTrain: epoch  8, batch    92 | loss: 121.7719552CurrentTrain: epoch  8, batch    93 | loss: 128.8057727CurrentTrain: epoch  8, batch    94 | loss: 65.8583601CurrentTrain: epoch  8, batch    95 | loss: 104.9282046CurrentTrain: epoch  9, batch     0 | loss: 176.2540176CurrentTrain: epoch  9, batch     1 | loss: 179.0033931CurrentTrain: epoch  9, batch     2 | loss: 74.1313277CurrentTrain: epoch  9, batch     3 | loss: 98.0811980CurrentTrain: epoch  9, batch     4 | loss: 177.4706329CurrentTrain: epoch  9, batch     5 | loss: 63.4551037CurrentTrain: epoch  9, batch     6 | loss: 79.9996166CurrentTrain: epoch  9, batch     7 | loss: 76.8653057CurrentTrain: epoch  9, batch     8 | loss: 78.5973134CurrentTrain: epoch  9, batch     9 | loss: 97.3654019CurrentTrain: epoch  9, batch    10 | loss: 75.1353142CurrentTrain: epoch  9, batch    11 | loss: 76.6817619CurrentTrain: epoch  9, batch    12 | loss: 64.6561390CurrentTrain: epoch  9, batch    13 | loss: 170.0210410CurrentTrain: epoch  9, batch    14 | loss: 124.6427390CurrentTrain: epoch  9, batch    15 | loss: 78.9083984CurrentTrain: epoch  9, batch    16 | loss: 96.2891857CurrentTrain: epoch  9, batch    17 | loss: 97.1869299CurrentTrain: epoch  9, batch    18 | loss: 120.9294981CurrentTrain: epoch  9, batch    19 | loss: 79.5977858CurrentTrain: epoch  9, batch    20 | loss: 96.7688099CurrentTrain: epoch  9, batch    21 | loss: 80.1966583CurrentTrain: epoch  9, batch    22 | loss: 94.9981630CurrentTrain: epoch  9, batch    23 | loss: 123.9470793CurrentTrain: epoch  9, batch    24 | loss: 169.7924021CurrentTrain: epoch  9, batch    25 | loss: 76.6929045CurrentTrain: epoch  9, batch    26 | loss: 92.9422418CurrentTrain: epoch  9, batch    27 | loss: 97.3774200CurrentTrain: epoch  9, batch    28 | loss: 98.5380240CurrentTrain: epoch  9, batch    29 | loss: 75.5466951CurrentTrain: epoch  9, batch    30 | loss: 95.5738717CurrentTrain: epoch  9, batch    31 | loss: 126.3130773CurrentTrain: epoch  9, batch    32 | loss: 64.2810998CurrentTrain: epoch  9, batch    33 | loss: 62.9421107CurrentTrain: epoch  9, batch    34 | loss: 74.4818994CurrentTrain: epoch  9, batch    35 | loss: 93.4385304CurrentTrain: epoch  9, batch    36 | loss: 64.7419061CurrentTrain: epoch  9, batch    37 | loss: 80.5471706CurrentTrain: epoch  9, batch    38 | loss: 99.1459318CurrentTrain: epoch  9, batch    39 | loss: 98.8221852CurrentTrain: epoch  9, batch    40 | loss: 82.0718927CurrentTrain: epoch  9, batch    41 | loss: 62.9430671CurrentTrain: epoch  9, batch    42 | loss: 123.9907353CurrentTrain: epoch  9, batch    43 | loss: 79.8628336CurrentTrain: epoch  9, batch    44 | loss: 170.6793374CurrentTrain: epoch  9, batch    45 | loss: 93.2326373CurrentTrain: epoch  9, batch    46 | loss: 96.6058544CurrentTrain: epoch  9, batch    47 | loss: 176.2315478CurrentTrain: epoch  9, batch    48 | loss: 99.0362994CurrentTrain: epoch  9, batch    49 | loss: 126.2394714CurrentTrain: epoch  9, batch    50 | loss: 98.2910467CurrentTrain: epoch  9, batch    51 | loss: 94.7686893CurrentTrain: epoch  9, batch    52 | loss: 126.2486952CurrentTrain: epoch  9, batch    53 | loss: 96.3689402CurrentTrain: epoch  9, batch    54 | loss: 63.6127391CurrentTrain: epoch  9, batch    55 | loss: 127.6714928CurrentTrain: epoch  9, batch    56 | loss: 120.4200994CurrentTrain: epoch  9, batch    57 | loss: 126.4071291CurrentTrain: epoch  9, batch    58 | loss: 123.7497521CurrentTrain: epoch  9, batch    59 | loss: 75.5861094CurrentTrain: epoch  9, batch    60 | loss: 93.7658154CurrentTrain: epoch  9, batch    61 | loss: 94.6992745CurrentTrain: epoch  9, batch    62 | loss: 98.8305170CurrentTrain: epoch  9, batch    63 | loss: 170.2065725CurrentTrain: epoch  9, batch    64 | loss: 128.7596242CurrentTrain: epoch  9, batch    65 | loss: 80.2430084CurrentTrain: epoch  9, batch    66 | loss: 79.8647804CurrentTrain: epoch  9, batch    67 | loss: 100.8061323CurrentTrain: epoch  9, batch    68 | loss: 126.6424465CurrentTrain: epoch  9, batch    69 | loss: 91.5774085CurrentTrain: epoch  9, batch    70 | loss: 101.0800102CurrentTrain: epoch  9, batch    71 | loss: 126.6281418CurrentTrain: epoch  9, batch    72 | loss: 127.3158681CurrentTrain: epoch  9, batch    73 | loss: 124.2081567CurrentTrain: epoch  9, batch    74 | loss: 124.2249291CurrentTrain: epoch  9, batch    75 | loss: 67.2788473CurrentTrain: epoch  9, batch    76 | loss: 97.1389664CurrentTrain: epoch  9, batch    77 | loss: 96.4821267CurrentTrain: epoch  9, batch    78 | loss: 172.8998678CurrentTrain: epoch  9, batch    79 | loss: 95.2403326CurrentTrain: epoch  9, batch    80 | loss: 99.4042393CurrentTrain: epoch  9, batch    81 | loss: 98.6483127CurrentTrain: epoch  9, batch    82 | loss: 77.4615942CurrentTrain: epoch  9, batch    83 | loss: 79.7696461CurrentTrain: epoch  9, batch    84 | loss: 81.6080887CurrentTrain: epoch  9, batch    85 | loss: 77.5313251CurrentTrain: epoch  9, batch    86 | loss: 95.4208845CurrentTrain: epoch  9, batch    87 | loss: 121.2398460CurrentTrain: epoch  9, batch    88 | loss: 98.9598312CurrentTrain: epoch  9, batch    89 | loss: 79.9754059CurrentTrain: epoch  9, batch    90 | loss: 126.2406751CurrentTrain: epoch  9, batch    91 | loss: 121.9420912CurrentTrain: epoch  9, batch    92 | loss: 93.1189862CurrentTrain: epoch  9, batch    93 | loss: 132.5459260CurrentTrain: epoch  9, batch    94 | loss: 126.7511445CurrentTrain: epoch  9, batch    95 | loss: 144.4311036

F1 score per class: {32: 0.7096774193548387, 6: 0.8571428571428571, 19: 0.34782608695652173, 24: 0.7692307692307693, 26: 0.8950276243093923, 29: 0.8923076923076924}
Micro-average F1 score: 0.8131634819532909
Weighted-average F1 score: 0.819340788208084
F1 score per class: {32: 0.7864077669902912, 6: 0.8715083798882681, 19: 0.6206896551724138, 24: 0.7666666666666667, 26: 0.9690721649484536, 29: 0.9045226130653267}
Micro-average F1 score: 0.8530901722391084
Weighted-average F1 score: 0.855031614181228
F1 score per class: {32: 0.7864077669902912, 6: 0.8715083798882681, 19: 0.6428571428571429, 24: 0.7666666666666667, 26: 0.9690721649484536, 29: 0.91}
Micro-average F1 score: 0.8551165146909828
Weighted-average F1 score: 0.8572773423863612

F1 score per class: {32: 0.7096774193548387, 6: 0.8571428571428571, 19: 0.34782608695652173, 24: 0.7692307692307693, 26: 0.8950276243093923, 29: 0.8923076923076924}
Micro-average F1 score: 0.8131634819532909
Weighted-average F1 score: 0.819340788208084
F1 score per class: {32: 0.7864077669902912, 6: 0.8715083798882681, 19: 0.6206896551724138, 24: 0.7666666666666667, 26: 0.9690721649484536, 29: 0.9045226130653267}
Micro-average F1 score: 0.8530901722391084
Weighted-average F1 score: 0.855031614181228
F1 score per class: {32: 0.7864077669902912, 6: 0.8715083798882681, 19: 0.6428571428571429, 24: 0.7666666666666667, 26: 0.9690721649484536, 29: 0.91}
Micro-average F1 score: 0.8551165146909828
Weighted-average F1 score: 0.8572773423863612

F1 score per class: {32: 0.528, 6: 0.7009345794392523, 19: 0.18604651162790697, 24: 0.7106598984771574, 26: 0.8663101604278075, 29: 0.8130841121495327}
Micro-average F1 score: 0.6932126696832579
Weighted-average F1 score: 0.6827125855899875
F1 score per class: {32: 0.54, 6: 0.6527196652719666, 19: 0.18556701030927836, 24: 0.6699029126213593, 26: 0.9038461538461539, 29: 0.7692307692307693}
Micro-average F1 score: 0.6557632398753894
Weighted-average F1 score: 0.6332765583778174
F1 score per class: {32: 0.54, 6: 0.6527196652719666, 19: 0.19148936170212766, 24: 0.6699029126213593, 26: 0.9038461538461539, 29: 0.774468085106383}
Micro-average F1 score: 0.6583463338533542
Weighted-average F1 score: 0.6367271070743135

F1 score per class: {32: 0.528, 6: 0.7009345794392523, 19: 0.18604651162790697, 24: 0.7106598984771574, 26: 0.8663101604278075, 29: 0.8130841121495327}
Micro-average F1 score: 0.6932126696832579
Weighted-average F1 score: 0.6827125855899875
F1 score per class: {32: 0.54, 6: 0.6527196652719666, 19: 0.18556701030927836, 24: 0.6699029126213593, 26: 0.9038461538461539, 29: 0.7692307692307693}
Micro-average F1 score: 0.6557632398753894
Weighted-average F1 score: 0.6332765583778174
F1 score per class: {32: 0.54, 6: 0.6527196652719666, 19: 0.19148936170212766, 24: 0.6699029126213593, 26: 0.9038461538461539, 29: 0.774468085106383}
Micro-average F1 score: 0.6583463338533542
Weighted-average F1 score: 0.6367271070743135
cur_acc_wo_na:  ['0.8132']
his_acc_wo_na:  ['0.8132']
cur_acc des_wo_na:  ['0.8531']
his_acc des_wo_na:  ['0.8531']
cur_acc rrf_wo_na:  ['0.8551']
his_acc rrf_wo_na:  ['0.8551']
cur_acc_w_na:  ['0.6932']
his_acc_w_na:  ['0.6932']
cur_acc des_w_na:  ['0.6558']
his_acc des_w_na:  ['0.6558']
cur_acc rrf_w_na:  ['0.6583']
his_acc rrf_w_na:  ['0.6583']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 182.4501389CurrentTrain: epoch  0, batch     1 | loss: 147.6220141CurrentTrain: epoch  0, batch     2 | loss: 112.8626935CurrentTrain: epoch  0, batch     3 | loss: 117.1842885CurrentTrain: epoch  0, batch     4 | loss: 74.4471918CurrentTrain: epoch  1, batch     0 | loss: 113.7608814CurrentTrain: epoch  1, batch     1 | loss: 136.2083391CurrentTrain: epoch  1, batch     2 | loss: 112.2038503CurrentTrain: epoch  1, batch     3 | loss: 106.4612076CurrentTrain: epoch  1, batch     4 | loss: 89.1027602CurrentTrain: epoch  2, batch     0 | loss: 134.5824776CurrentTrain: epoch  2, batch     1 | loss: 108.8044906CurrentTrain: epoch  2, batch     2 | loss: 184.2615268CurrentTrain: epoch  2, batch     3 | loss: 85.1341736CurrentTrain: epoch  2, batch     4 | loss: 113.9234923CurrentTrain: epoch  3, batch     0 | loss: 177.1283148CurrentTrain: epoch  3, batch     1 | loss: 104.9146394CurrentTrain: epoch  3, batch     2 | loss: 132.1668138CurrentTrain: epoch  3, batch     3 | loss: 177.5297424CurrentTrain: epoch  3, batch     4 | loss: 63.4219348CurrentTrain: epoch  4, batch     0 | loss: 131.5216509CurrentTrain: epoch  4, batch     1 | loss: 100.2905338CurrentTrain: epoch  4, batch     2 | loss: 103.2486299CurrentTrain: epoch  4, batch     3 | loss: 178.9751584CurrentTrain: epoch  4, batch     4 | loss: 51.4805829CurrentTrain: epoch  5, batch     0 | loss: 176.8923175CurrentTrain: epoch  5, batch     1 | loss: 82.3895448CurrentTrain: epoch  5, batch     2 | loss: 175.7339346CurrentTrain: epoch  5, batch     3 | loss: 99.2062666CurrentTrain: epoch  5, batch     4 | loss: 79.4473442CurrentTrain: epoch  6, batch     0 | loss: 96.9388093CurrentTrain: epoch  6, batch     1 | loss: 178.0387260CurrentTrain: epoch  6, batch     2 | loss: 98.7214503CurrentTrain: epoch  6, batch     3 | loss: 102.2924848CurrentTrain: epoch  6, batch     4 | loss: 61.3324912CurrentTrain: epoch  7, batch     0 | loss: 128.3839831CurrentTrain: epoch  7, batch     1 | loss: 75.8366743CurrentTrain: epoch  7, batch     2 | loss: 273.4062226CurrentTrain: epoch  7, batch     3 | loss: 127.1873642CurrentTrain: epoch  7, batch     4 | loss: 76.8284790CurrentTrain: epoch  8, batch     0 | loss: 130.5123034CurrentTrain: epoch  8, batch     1 | loss: 79.7971720CurrentTrain: epoch  8, batch     2 | loss: 100.4673380CurrentTrain: epoch  8, batch     3 | loss: 95.5323448CurrentTrain: epoch  8, batch     4 | loss: 80.3037078CurrentTrain: epoch  9, batch     0 | loss: 127.2688517CurrentTrain: epoch  9, batch     1 | loss: 97.9976168CurrentTrain: epoch  9, batch     2 | loss: 100.3998529CurrentTrain: epoch  9, batch     3 | loss: 96.9308133CurrentTrain: epoch  9, batch     4 | loss: 60.1943369
MemoryTrain:  epoch  0, batch     0 | loss: 0.7674997MemoryTrain:  epoch  1, batch     0 | loss: 0.6376560MemoryTrain:  epoch  2, batch     0 | loss: 0.5086581MemoryTrain:  epoch  3, batch     0 | loss: 0.3016996MemoryTrain:  epoch  4, batch     0 | loss: 0.2707657MemoryTrain:  epoch  5, batch     0 | loss: 0.1632896MemoryTrain:  epoch  6, batch     0 | loss: 0.1456821MemoryTrain:  epoch  7, batch     0 | loss: 0.0867974MemoryTrain:  epoch  8, batch     0 | loss: 0.0750422MemoryTrain:  epoch  9, batch     0 | loss: 0.0787415

F1 score per class: {5: 0.9637305699481865, 6: 0.0, 10: 0.6486486486486487, 16: 0.8, 17: 0.0, 18: 0.391304347826087}
Micro-average F1 score: 0.7456140350877193
Weighted-average F1 score: 0.7793545403930625
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.7215189873417721, 16: 0.8679245283018868, 17: 0.0, 18: 0.8064516129032258}
Micro-average F1 score: 0.8235294117647058
Weighted-average F1 score: 0.8248007223476859
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.7375, 16: 0.8679245283018868, 17: 0.0, 18: 0.8064516129032258}
Micro-average F1 score: 0.8316430020283976
Weighted-average F1 score: 0.8358693801088712

F1 score per class: {32: 0.9538461538461539, 5: 0.6666666666666666, 6: 0.6233766233766234, 10: 0.8, 16: 0.0, 17: 0.391304347826087, 18: 0.8235294117647058, 19: 0.4166666666666667, 24: 0.7734806629834254, 26: 0.9247311827956989, 29: 0.9191919191919192}
Micro-average F1 score: 0.7830388692579505
Weighted-average F1 score: 0.7947495604958381
F1 score per class: {32: 0.98, 5: 0.7029702970297029, 6: 0.7169811320754716, 10: 0.8363636363636363, 16: 0.0, 17: 0.7936507936507936, 18: 0.8888888888888888, 19: 0.6896551724137931, 24: 0.7752808988764045, 26: 0.9587628865979382, 29: 0.9246231155778895}
Micro-average F1 score: 0.8289738430583501
Weighted-average F1 score: 0.8240738831218561
F1 score per class: {32: 0.98, 5: 0.6836734693877551, 6: 0.7239263803680982, 10: 0.8363636363636363, 16: 0.0, 17: 0.7936507936507936, 18: 0.8764044943820225, 19: 0.5925925925925926, 24: 0.7752808988764045, 26: 0.9637305699481865, 29: 0.9246231155778895}
Micro-average F1 score: 0.8236877523553163
Weighted-average F1 score: 0.8180830231550712

F1 score per class: {32: 0.93, 5: 0.0, 6: 0.4729064039408867, 10: 0.5882352941176471, 16: 0.0, 17: 0.2903225806451613, 18: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5295950155763239
Weighted-average F1 score: 0.4680884297773524
F1 score per class: {32: 0.6533333333333333, 5: 0.0, 6: 0.5089285714285714, 10: 0.4842105263157895, 16: 0.0, 17: 0.37593984962406013, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.42917547568710357
Weighted-average F1 score: 0.39005629746006804
F1 score per class: {32: 0.6555183946488294, 5: 0.0, 6: 0.4957983193277311, 10: 0.5111111111111111, 16: 0.0, 17: 0.3875968992248062, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.44906900328587074
Weighted-average F1 score: 0.4163169701690625

F1 score per class: {32: 0.9207920792079208, 5: 0.43508771929824563, 6: 0.3764705882352941, 10: 0.547945205479452, 16: 0.0, 17: 0.26865671641791045, 18: 0.6422018348623854, 19: 0.14925373134328357, 24: 0.6829268292682927, 26: 0.8113207547169812, 29: 0.7368421052631579}
Micro-average F1 score: 0.5906183368869936
Weighted-average F1 score: 0.5715070120875665
F1 score per class: {32: 0.6049382716049383, 5: 0.39444444444444443, 6: 0.4014084507042254, 10: 0.39655172413793105, 16: 0.0, 17: 0.29069767441860467, 18: 0.5594405594405595, 19: 0.12121212121212122, 24: 0.641860465116279, 26: 0.7530364372469636, 29: 0.673992673992674}
Micro-average F1 score: 0.4934131736526946
Weighted-average F1 score: 0.4723867246554922
F1 score per class: {32: 0.6242038216560509, 5: 0.4240506329113924, 6: 0.37942122186495175, 10: 0.42990654205607476, 16: 0.0, 17: 0.30120481927710846, 18: 0.582089552238806, 19: 0.14814814814814814, 24: 0.6666666666666666, 26: 0.775, 29: 0.6891385767790262}
Micro-average F1 score: 0.5132075471698113
Weighted-average F1 score: 0.4922907049319351
cur_acc_wo_na:  ['0.8132', '0.7456']
his_acc_wo_na:  ['0.8132', '0.7830']
cur_acc des_wo_na:  ['0.8531', '0.8235']
his_acc des_wo_na:  ['0.8531', '0.8290']
cur_acc rrf_wo_na:  ['0.8551', '0.8316']
his_acc rrf_wo_na:  ['0.8551', '0.8237']
cur_acc_w_na:  ['0.6932', '0.5296']
his_acc_w_na:  ['0.6932', '0.5906']
cur_acc des_w_na:  ['0.6558', '0.4292']
his_acc des_w_na:  ['0.6558', '0.4934']
cur_acc rrf_w_na:  ['0.6583', '0.4491']
his_acc rrf_w_na:  ['0.6583', '0.5132']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 124.0550934CurrentTrain: epoch  0, batch     1 | loss: 155.2805747CurrentTrain: epoch  0, batch     2 | loss: 102.8123484CurrentTrain: epoch  0, batch     3 | loss: 117.2003322CurrentTrain: epoch  0, batch     4 | loss: 90.2894640CurrentTrain: epoch  1, batch     0 | loss: 145.5109695CurrentTrain: epoch  1, batch     1 | loss: 93.4949511CurrentTrain: epoch  1, batch     2 | loss: 107.2192552CurrentTrain: epoch  1, batch     3 | loss: 136.4106561CurrentTrain: epoch  1, batch     4 | loss: 43.6170919CurrentTrain: epoch  2, batch     0 | loss: 110.2465295CurrentTrain: epoch  2, batch     1 | loss: 88.2298028CurrentTrain: epoch  2, batch     2 | loss: 184.8310498CurrentTrain: epoch  2, batch     3 | loss: 104.6438581CurrentTrain: epoch  2, batch     4 | loss: 40.2442633CurrentTrain: epoch  3, batch     0 | loss: 88.5649997CurrentTrain: epoch  3, batch     1 | loss: 105.3388914CurrentTrain: epoch  3, batch     2 | loss: 103.9157344CurrentTrain: epoch  3, batch     3 | loss: 106.7927654CurrentTrain: epoch  3, batch     4 | loss: 43.6952256CurrentTrain: epoch  4, batch     0 | loss: 132.8772246CurrentTrain: epoch  4, batch     1 | loss: 105.5415817CurrentTrain: epoch  4, batch     2 | loss: 101.1654771CurrentTrain: epoch  4, batch     3 | loss: 100.7972163CurrentTrain: epoch  4, batch     4 | loss: 41.8250328CurrentTrain: epoch  5, batch     0 | loss: 130.1666086CurrentTrain: epoch  5, batch     1 | loss: 81.7389041CurrentTrain: epoch  5, batch     2 | loss: 180.5248162CurrentTrain: epoch  5, batch     3 | loss: 97.4874363CurrentTrain: epoch  5, batch     4 | loss: 40.7954604CurrentTrain: epoch  6, batch     0 | loss: 177.2462696CurrentTrain: epoch  6, batch     1 | loss: 101.6096770CurrentTrain: epoch  6, batch     2 | loss: 100.3904118CurrentTrain: epoch  6, batch     3 | loss: 78.1401522CurrentTrain: epoch  6, batch     4 | loss: 40.5680437CurrentTrain: epoch  7, batch     0 | loss: 90.4077726CurrentTrain: epoch  7, batch     1 | loss: 101.0001219CurrentTrain: epoch  7, batch     2 | loss: 101.1249445CurrentTrain: epoch  7, batch     3 | loss: 274.2719425CurrentTrain: epoch  7, batch     4 | loss: 39.5483421CurrentTrain: epoch  8, batch     0 | loss: 100.6643659CurrentTrain: epoch  8, batch     1 | loss: 100.4389177CurrentTrain: epoch  8, batch     2 | loss: 94.5557094CurrentTrain: epoch  8, batch     3 | loss: 83.1606179CurrentTrain: epoch  8, batch     4 | loss: 40.6912816CurrentTrain: epoch  9, batch     0 | loss: 99.3869272CurrentTrain: epoch  9, batch     1 | loss: 76.5450198CurrentTrain: epoch  9, batch     2 | loss: 170.6844503CurrentTrain: epoch  9, batch     3 | loss: 99.3984627CurrentTrain: epoch  9, batch     4 | loss: 39.9920011
MemoryTrain:  epoch  0, batch     0 | loss: 1.0840790MemoryTrain:  epoch  1, batch     0 | loss: 0.8399180MemoryTrain:  epoch  2, batch     0 | loss: 0.7198850MemoryTrain:  epoch  3, batch     0 | loss: 0.5719388MemoryTrain:  epoch  4, batch     0 | loss: 0.4238678MemoryTrain:  epoch  5, batch     0 | loss: 0.3344076MemoryTrain:  epoch  6, batch     0 | loss: 0.2579409MemoryTrain:  epoch  7, batch     0 | loss: 0.2413526MemoryTrain:  epoch  8, batch     0 | loss: 0.2056817MemoryTrain:  epoch  9, batch     0 | loss: 0.1277606

F1 score per class: {2: 0.875, 39: 0.0, 10: 0.3853211009174312, 11: 0.46153846153846156, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.36363636363636365, 28: 0.4444444444444444}
Micro-average F1 score: 0.43097643097643096
Weighted-average F1 score: 0.39655876710393295
F1 score per class: {2: 1.0, 6: 0.0, 39: 0.0, 10: 0.75177304964539, 11: 0.8457142857142858, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.7692307692307693, 28: 0.88}
Micro-average F1 score: 0.7638190954773869
Weighted-average F1 score: 0.7027860162342511
F1 score per class: {2: 0.9411764705882353, 6: 0.0, 39: 0.0, 10: 0.7605633802816901, 11: 0.8372093023255814, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.7692307692307693, 28: 0.88}
Micro-average F1 score: 0.7633587786259542
Weighted-average F1 score: 0.7034773443305337

F1 score per class: {32: 0.875, 2: 0.9693877551020408, 5: 0.729064039408867, 6: 0.32786885245901637, 39: 0.3333333333333333, 10: 0.45454545454545453, 11: 0.75, 12: 0.0, 16: 0.19047619047619047, 17: 0.8068181818181818, 18: 0.34782608695652173, 19: 0.7597765363128491, 24: 0.23529411764705882, 26: 0.9247311827956989, 28: 0.9191919191919192, 29: 0.4}
Micro-average F1 score: 0.7006444053895724
Weighted-average F1 score: 0.7584473097251454
F1 score per class: {32: 1.0, 2: 0.9950248756218906, 5: 0.7323943661971831, 6: 0.7455621301775148, 39: 0.6625, 10: 0.7628865979381443, 11: 0.8666666666666667, 12: 0.0, 16: 0.49122807017543857, 17: 0.8526315789473684, 18: 0.6, 19: 0.745945945945946, 24: 0.45454545454545453, 26: 0.9312169312169312, 28: 0.9306930693069307, 29: 0.7586206896551724}
Micro-average F1 score: 0.8020725388601037
Weighted-average F1 score: 0.8089770540641023
F1 score per class: {32: 0.9411764705882353, 2: 0.9950248756218906, 5: 0.7428571428571429, 6: 0.7602339181286549, 39: 0.6585365853658537, 10: 0.7659574468085106, 11: 0.8571428571428571, 12: 0.0, 16: 0.39215686274509803, 17: 0.8526315789473684, 18: 0.5714285714285714, 19: 0.7431693989071039, 24: 0.43478260869565216, 26: 0.9312169312169312, 28: 0.9306930693069307, 29: 0.7586206896551724}
Micro-average F1 score: 0.8010416666666667
Weighted-average F1 score: 0.8101334086292736

F1 score per class: {32: 0.4, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.35, 10: 0.4166666666666667, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.17391304347826086, 26: 0.0, 28: 0.0, 29: 0.25}
Micro-average F1 score: 0.2711864406779661
Weighted-average F1 score: 0.18593687346050294
F1 score per class: {32: 0.2, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.5170731707317073, 10: 0.5714285714285714, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.14084507042253522, 26: 0.0, 28: 0.0, 29: 0.34375}
Micro-average F1 score: 0.2974559686888454
Weighted-average F1 score: 0.2411151223777986
F1 score per class: {32: 0.21333333333333335, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.5142857142857142, 10: 0.5581395348837209, 11: 0.0, 12: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 24: 0.12987012987012986, 26: 0.0, 28: 0.0, 29: 0.41509433962264153}
Micro-average F1 score: 0.31612223393045313
Weighted-average F1 score: 0.2604912628230234

F1 score per class: {32: 0.3181818181818182, 2: 0.9047619047619048, 5: 0.4010840108401084, 6: 0.24242424242424243, 39: 0.20388349514563106, 10: 0.2702702702702703, 11: 0.4827586206896552, 12: 0.0, 16: 0.1038961038961039, 17: 0.6228070175438597, 18: 0.1568627450980392, 19: 0.68, 24: 0.08888888888888889, 26: 0.8075117370892019, 28: 0.6893939393939394, 29: 0.17391304347826086}
Micro-average F1 score: 0.4895620139173148
Weighted-average F1 score: 0.4818692071571877
F1 score per class: {32: 0.0967741935483871, 2: 0.40160642570281124, 5: 0.2899628252788104, 6: 0.3387096774193548, 39: 0.2896174863387978, 10: 0.21574344023323616, 11: 0.46017699115044247, 12: 0.0, 16: 0.15555555555555556, 17: 0.574468085106383, 18: 0.12, 19: 0.6052631578947368, 24: 0.08064516129032258, 26: 0.7857142857142857, 28: 0.5449275362318841, 29: 0.20754716981132076}
Micro-average F1 score: 0.3501470255598281
Weighted-average F1 score: 0.32885312885691026
F1 score per class: {32: 0.11594202898550725, 2: 0.5208333333333334, 5: 0.3391304347826087, 6: 0.32418952618453867, 39: 0.2748091603053435, 10: 0.21818181818181817, 11: 0.453781512605042, 12: 0.0, 16: 0.1342281879194631, 17: 0.5912408759124088, 18: 0.1523809523809524, 19: 0.6384976525821596, 24: 0.07092198581560284, 26: 0.7927927927927928, 28: 0.5529411764705883, 29: 0.23404255319148937}
Micro-average F1 score: 0.37384540593096743
Weighted-average F1 score: 0.3514433529516289
cur_acc_wo_na:  ['0.8132', '0.7456', '0.4310']
his_acc_wo_na:  ['0.8132', '0.7830', '0.7006']
cur_acc des_wo_na:  ['0.8531', '0.8235', '0.7638']
his_acc des_wo_na:  ['0.8531', '0.8290', '0.8021']
cur_acc rrf_wo_na:  ['0.8551', '0.8316', '0.7634']
his_acc rrf_wo_na:  ['0.8551', '0.8237', '0.8010']
cur_acc_w_na:  ['0.6932', '0.5296', '0.2712']
his_acc_w_na:  ['0.6932', '0.5906', '0.4896']
cur_acc des_w_na:  ['0.6558', '0.4292', '0.2975']
his_acc des_w_na:  ['0.6558', '0.4934', '0.3501']
cur_acc rrf_w_na:  ['0.6583', '0.4491', '0.3161']
his_acc rrf_w_na:  ['0.6583', '0.5132', '0.3738']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 132.6210191CurrentTrain: epoch  0, batch     1 | loss: 165.9482504CurrentTrain: epoch  0, batch     2 | loss: 99.1303275CurrentTrain: epoch  0, batch     3 | loss: 110.0255450CurrentTrain: epoch  1, batch     0 | loss: 96.5995294CurrentTrain: epoch  1, batch     1 | loss: 142.2098523CurrentTrain: epoch  1, batch     2 | loss: 93.9918229CurrentTrain: epoch  1, batch     3 | loss: 111.3237381CurrentTrain: epoch  2, batch     0 | loss: 89.0014862CurrentTrain: epoch  2, batch     1 | loss: 90.2886227CurrentTrain: epoch  2, batch     2 | loss: 184.8626020CurrentTrain: epoch  2, batch     3 | loss: 87.1762415CurrentTrain: epoch  3, batch     0 | loss: 109.1860329CurrentTrain: epoch  3, batch     1 | loss: 105.7242475CurrentTrain: epoch  3, batch     2 | loss: 85.3937972CurrentTrain: epoch  3, batch     3 | loss: 143.3160456CurrentTrain: epoch  4, batch     0 | loss: 106.6924693CurrentTrain: epoch  4, batch     1 | loss: 182.5753636CurrentTrain: epoch  4, batch     2 | loss: 80.8969283CurrentTrain: epoch  4, batch     3 | loss: 79.4560348CurrentTrain: epoch  5, batch     0 | loss: 126.6120024CurrentTrain: epoch  5, batch     1 | loss: 125.7089132CurrentTrain: epoch  5, batch     2 | loss: 103.1393700CurrentTrain: epoch  5, batch     3 | loss: 71.8311290CurrentTrain: epoch  6, batch     0 | loss: 101.2602390CurrentTrain: epoch  6, batch     1 | loss: 96.6284843CurrentTrain: epoch  6, batch     2 | loss: 133.2568172CurrentTrain: epoch  6, batch     3 | loss: 79.1204323CurrentTrain: epoch  7, batch     0 | loss: 129.2563197CurrentTrain: epoch  7, batch     1 | loss: 81.8101300CurrentTrain: epoch  7, batch     2 | loss: 104.9496195CurrentTrain: epoch  7, batch     3 | loss: 76.9617427CurrentTrain: epoch  8, batch     0 | loss: 182.6934362CurrentTrain: epoch  8, batch     1 | loss: 78.0237416CurrentTrain: epoch  8, batch     2 | loss: 128.1367079CurrentTrain: epoch  8, batch     3 | loss: 95.7415947CurrentTrain: epoch  9, batch     0 | loss: 98.9947849CurrentTrain: epoch  9, batch     1 | loss: 99.5769620CurrentTrain: epoch  9, batch     2 | loss: 96.9787195CurrentTrain: epoch  9, batch     3 | loss: 81.2761527
MemoryTrain:  epoch  0, batch     0 | loss: 1.1206907MemoryTrain:  epoch  1, batch     0 | loss: 0.8708814MemoryTrain:  epoch  2, batch     0 | loss: 0.6997196MemoryTrain:  epoch  3, batch     0 | loss: 0.5178111MemoryTrain:  epoch  4, batch     0 | loss: 0.4329581MemoryTrain:  epoch  5, batch     0 | loss: 0.3796370MemoryTrain:  epoch  6, batch     0 | loss: 0.2973883MemoryTrain:  epoch  7, batch     0 | loss: 0.2913892MemoryTrain:  epoch  8, batch     0 | loss: 0.2118749MemoryTrain:  epoch  9, batch     0 | loss: 0.1966258

F1 score per class: {0: 0.927536231884058, 32: 0.0, 2: 0.9690721649484536, 4: 0.0, 11: 0.5714285714285714, 13: 0.68, 21: 0.7792207792207793, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8495145631067961
Weighted-average F1 score: 0.8240282471089845
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.9690721649484536, 4: 0.0, 11: 0.0, 12: 0.5714285714285714, 13: 0.6808510638297872, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.851764705882353
Weighted-average F1 score: 0.8090785706553993
F1 score per class: {0: 0.9722222222222222, 32: 0.0, 2: 0.9743589743589743, 4: 0.0, 11: 0.0, 12: 0.5714285714285714, 13: 0.6666666666666666, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8557919621749409
Weighted-average F1 score: 0.8191776887589696

F1 score per class: {0: 0.9142857142857143, 2: 0.7777777777777778, 4: 0.9690721649484536, 5: 0.9743589743589743, 6: 0.6907216494845361, 10: 0.4782608695652174, 11: 0.47619047619047616, 12: 0.25862068965517243, 13: 0.09090909090909091, 16: 0.7857142857142857, 17: 0.0, 18: 0.18181818181818182, 19: 0.7978723404255319, 21: 0.4722222222222222, 23: 0.7792207792207793, 24: 0.18181818181818182, 26: 0.7046632124352331, 28: 0.0, 29: 0.9368421052631579, 32: 0.8829787234042553, 39: 0.2}
Micro-average F1 score: 0.7066361556064074
Weighted-average F1 score: 0.7393270527041743
F1 score per class: {0: 0.96, 2: 0.6666666666666666, 4: 0.9690721649484536, 5: 0.9850746268656716, 6: 0.6976744186046512, 10: 0.6956521739130435, 11: 0.7073170731707317, 12: 0.7225130890052356, 13: 0.10810810810810811, 16: 0.896551724137931, 17: 0.0, 18: 0.5423728813559322, 19: 0.8571428571428571, 21: 0.5079365079365079, 23: 0.8048780487804879, 24: 0.18181818181818182, 26: 0.6938775510204082, 28: 0.0, 29: 0.9148936170212766, 32: 0.9128205128205128, 39: 0.6896551724137931}
Micro-average F1 score: 0.781986531986532
Weighted-average F1 score: 0.783338502035592
F1 score per class: {0: 0.9459459459459459, 2: 0.7, 4: 0.9743589743589743, 5: 0.99, 6: 0.7184466019417476, 10: 0.6951219512195121, 11: 0.7176470588235294, 12: 0.6024096385542169, 13: 0.10256410256410256, 16: 0.8620689655172413, 17: 0.0, 18: 0.509090909090909, 19: 0.8514851485148515, 21: 0.47761194029850745, 23: 0.8148148148148148, 24: 0.18181818181818182, 26: 0.6938775510204082, 28: 0.0, 29: 0.9312169312169312, 32: 0.9183673469387755, 39: 0.6896551724137931}
Micro-average F1 score: 0.7758400680561464
Weighted-average F1 score: 0.7788705055458102

F1 score per class: {0: 0.8648648648648649, 2: 0.0, 4: 0.9591836734693877, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.14285714285714285, 16: 0.0, 19: 0.0, 21: 0.3469387755102041, 23: 0.6666666666666666, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.5208333333333334
Weighted-average F1 score: 0.3966347600987297
F1 score per class: {0: 0.6260869565217392, 2: 0.0, 4: 0.8703703703703703, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.1111111111111111, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.2782608695652174, 23: 0.5365853658536586, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3197879858657244
Weighted-average F1 score: 0.23837345489524053
F1 score per class: {0: 0.6422018348623854, 2: 0.0, 4: 0.9134615384615384, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.10810810810810811, 16: 0.0, 17: 0.0, 19: 0.0, 21: 0.25196850393700787, 23: 0.5945945945945946, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3584158415841584
Weighted-average F1 score: 0.26657578247519514

F1 score per class: {0: 0.5818181818181818, 2: 0.2857142857142857, 4: 0.9306930693069307, 5: 0.7421875, 6: 0.35170603674540685, 10: 0.2727272727272727, 11: 0.21406727828746178, 12: 0.15625, 13: 0.02702702702702703, 16: 0.4835164835164835, 17: 0.0, 18: 0.08695652173913043, 19: 0.5434782608695652, 21: 0.12363636363636364, 23: 0.6, 24: 0.13333333333333333, 26: 0.6181818181818182, 28: 0.0, 29: 0.7946428571428571, 32: 0.7345132743362832, 39: 0.08163265306122448}
Micro-average F1 score: 0.439510389980074
Weighted-average F1 score: 0.4092946332984328
F1 score per class: {0: 0.2857142857142857, 2: 0.07216494845360824, 4: 0.8209606986899564, 5: 0.3, 6: 0.24429967426710097, 10: 0.25396825396825395, 11: 0.2572062084257206, 12: 0.1796875, 13: 0.022988505747126436, 16: 0.4369747899159664, 17: 0.0, 18: 0.1461187214611872, 19: 0.46648793565683644, 21: 0.10631229235880399, 23: 0.40993788819875776, 24: 0.11428571428571428, 26: 0.5714285714285714, 28: 0.0, 29: 0.7577092511013216, 32: 0.5313432835820896, 39: 0.15384615384615385}
Micro-average F1 score: 0.31085829011209637
Weighted-average F1 score: 0.2879747246926392
F1 score per class: {0: 0.2928870292887029, 2: 0.10852713178294573, 4: 0.8636363636363636, 5: 0.43043478260869567, 6: 0.2884990253411306, 10: 0.2384937238493724, 11: 0.22889305816135083, 12: 0.22026431718061673, 13: 0.0196078431372549, 16: 0.4, 17: 0.0, 18: 0.17721518987341772, 19: 0.4777777777777778, 21: 0.0903954802259887, 23: 0.48175182481751827, 24: 0.11428571428571428, 26: 0.5862068965517241, 28: 0.0, 29: 0.7787610619469026, 32: 0.6040268456375839, 39: 0.18867924528301888}
Micro-average F1 score: 0.3432442604441099
Weighted-average F1 score: 0.31718586571269763
cur_acc_wo_na:  ['0.8132', '0.7456', '0.4310', '0.8495']
his_acc_wo_na:  ['0.8132', '0.7830', '0.7006', '0.7066']
cur_acc des_wo_na:  ['0.8531', '0.8235', '0.7638', '0.8518']
his_acc des_wo_na:  ['0.8531', '0.8290', '0.8021', '0.7820']
cur_acc rrf_wo_na:  ['0.8551', '0.8316', '0.7634', '0.8558']
his_acc rrf_wo_na:  ['0.8551', '0.8237', '0.8010', '0.7758']
cur_acc_w_na:  ['0.6932', '0.5296', '0.2712', '0.5208']
his_acc_w_na:  ['0.6932', '0.5906', '0.4896', '0.4395']
cur_acc des_w_na:  ['0.6558', '0.4292', '0.2975', '0.3198']
his_acc des_w_na:  ['0.6558', '0.4934', '0.3501', '0.3109']
cur_acc rrf_w_na:  ['0.6583', '0.4491', '0.3161', '0.3584']
his_acc rrf_w_na:  ['0.6583', '0.5132', '0.3738', '0.3432']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 113.6062558CurrentTrain: epoch  0, batch     1 | loss: 90.9785042CurrentTrain: epoch  0, batch     2 | loss: 114.7687666CurrentTrain: epoch  0, batch     3 | loss: 201.6885073CurrentTrain: epoch  1, batch     0 | loss: 105.4569036CurrentTrain: epoch  1, batch     1 | loss: 131.0193501CurrentTrain: epoch  1, batch     2 | loss: 89.5264531CurrentTrain: epoch  1, batch     3 | loss: 74.5200712CurrentTrain: epoch  2, batch     0 | loss: 181.9088096CurrentTrain: epoch  2, batch     1 | loss: 104.7084061CurrentTrain: epoch  2, batch     2 | loss: 86.4670404CurrentTrain: epoch  2, batch     3 | loss: 65.1963932CurrentTrain: epoch  3, batch     0 | loss: 129.2646744CurrentTrain: epoch  3, batch     1 | loss: 98.8324311CurrentTrain: epoch  3, batch     2 | loss: 126.3822255CurrentTrain: epoch  3, batch     3 | loss: 54.5545340CurrentTrain: epoch  4, batch     0 | loss: 98.4791968CurrentTrain: epoch  4, batch     1 | loss: 80.6832039CurrentTrain: epoch  4, batch     2 | loss: 99.5702471CurrentTrain: epoch  4, batch     3 | loss: 86.5207520CurrentTrain: epoch  5, batch     0 | loss: 81.3186367CurrentTrain: epoch  5, batch     1 | loss: 98.4197629CurrentTrain: epoch  5, batch     2 | loss: 99.8502884CurrentTrain: epoch  5, batch     3 | loss: 52.6128163CurrentTrain: epoch  6, batch     0 | loss: 79.3089098CurrentTrain: epoch  6, batch     1 | loss: 96.2367508CurrentTrain: epoch  6, batch     2 | loss: 100.4629701CurrentTrain: epoch  6, batch     3 | loss: 65.6897202CurrentTrain: epoch  7, batch     0 | loss: 75.9065554CurrentTrain: epoch  7, batch     1 | loss: 96.1533007CurrentTrain: epoch  7, batch     2 | loss: 127.3015944CurrentTrain: epoch  7, batch     3 | loss: 66.0516921CurrentTrain: epoch  8, batch     0 | loss: 100.5302342CurrentTrain: epoch  8, batch     1 | loss: 93.0829197CurrentTrain: epoch  8, batch     2 | loss: 79.0665396CurrentTrain: epoch  8, batch     3 | loss: 82.9328625CurrentTrain: epoch  9, batch     0 | loss: 89.8251834CurrentTrain: epoch  9, batch     1 | loss: 94.4260027CurrentTrain: epoch  9, batch     2 | loss: 174.4877504CurrentTrain: epoch  9, batch     3 | loss: 114.1936523
MemoryTrain:  epoch  0, batch     0 | loss: 0.8217036MemoryTrain:  epoch  1, batch     0 | loss: 0.7018929MemoryTrain:  epoch  2, batch     0 | loss: 0.5755727MemoryTrain:  epoch  3, batch     0 | loss: 0.5067568MemoryTrain:  epoch  4, batch     0 | loss: 0.4376465MemoryTrain:  epoch  5, batch     0 | loss: 0.3988147MemoryTrain:  epoch  6, batch     0 | loss: 0.2781308MemoryTrain:  epoch  7, batch     0 | loss: 0.2443434MemoryTrain:  epoch  8, batch     0 | loss: 0.2049571MemoryTrain:  epoch  9, batch     0 | loss: 0.1940862

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.8235294117647058, 10: 0.0, 11: 0.0, 13: 0.0, 15: 0.5352112676056338, 18: 0.0, 21: 0.4927536231884058, 23: 0.5882352941176471, 25: 0.6666666666666666}
Micro-average F1 score: 0.5046153846153846
Weighted-average F1 score: 0.3938911360526274
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.0, 10: 0.75, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 18: 0.5142857142857142, 21: 0.0, 23: 0.9292929292929293, 24: 0.6363636363636364, 25: 0.8085106382978723}
Micro-average F1 score: 0.6256684491978609
Weighted-average F1 score: 0.5291507942719356
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 10: 0.0, 11: 0.0, 13: 0.0, 15: 0.0, 18: 0.5352112676056338, 21: 0.0, 23: 0.9292929292929293, 24: 0.6363636363636364, 25: 0.8085106382978723}
Micro-average F1 score: 0.6344086021505376
Weighted-average F1 score: 0.5406722727275382

F1 score per class: {0: 0.8695652173913043, 2: 0.7777777777777778, 4: 0.9690721649484536, 5: 0.8558558558558559, 6: 0.6559139784946236, 10: 0.4626865671641791, 11: 0.34074074074074073, 12: 0.24347826086956523, 13: 0.10810810810810811, 15: 0.6666666666666666, 16: 0.8275862068965517, 17: 0.0, 18: 0.09523809523809523, 19: 0.7415730337078652, 21: 0.3333333333333333, 23: 0.7560975609756098, 24: 0.18181818181818182, 25: 0.5352112676056338, 26: 0.7127659574468085, 28: 0.2222222222222222, 29: 0.9312169312169312, 32: 0.8432432432432433, 35: 0.4927536231884058, 37: 0.373134328358209, 38: 0.45901639344262296, 39: 0.0}
Micro-average F1 score: 0.6463512429831596
Weighted-average F1 score: 0.6854522041254631
F1 score per class: {0: 0.9459459459459459, 2: 0.6666666666666666, 4: 0.9583333333333334, 5: 0.8547008547008547, 6: 0.6926829268292682, 10: 0.6075949367088608, 11: 0.4148148148148148, 12: 0.6892655367231638, 13: 0.1111111111111111, 15: 0.5, 16: 0.8620689655172413, 17: 0.0, 18: 0.4727272727272727, 19: 0.7936507936507936, 21: 0.4666666666666667, 23: 0.7407407407407407, 24: 0.23076923076923078, 25: 0.5142857142857142, 26: 0.7010309278350515, 28: 0.4, 29: 0.9263157894736842, 32: 0.8944723618090452, 35: 0.8761904761904762, 37: 0.3971631205673759, 38: 0.5588235294117647, 39: 0.21052631578947367}
Micro-average F1 score: 0.7106227106227107
Weighted-average F1 score: 0.7185917461692787
F1 score per class: {0: 0.9459459459459459, 2: 0.7, 4: 0.9583333333333334, 5: 0.8583690987124464, 6: 0.6889952153110048, 10: 0.6103896103896104, 11: 0.4142857142857143, 12: 0.5526315789473685, 13: 0.1111111111111111, 15: 0.5, 16: 0.8813559322033898, 17: 0.0, 18: 0.2857142857142857, 19: 0.7604166666666666, 21: 0.45161290322580644, 23: 0.75, 24: 0.23076923076923078, 25: 0.5352112676056338, 26: 0.7046632124352331, 28: 0.4, 29: 0.9424083769633508, 32: 0.8944723618090452, 35: 0.8846153846153846, 37: 0.3916083916083916, 38: 0.5277777777777778, 39: 0.0}
Micro-average F1 score: 0.6986706056129985
Weighted-average F1 score: 0.7117604015965152

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.7368421052631579, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.5066666666666667, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.34, 37: 0.43859649122807015, 38: 0.5384615384615384}
Micro-average F1 score: 0.28422876949740034
Weighted-average F1 score: 0.1902984035374853
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.48, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.5974025974025974, 37: 0.448, 38: 0.5846153846153846}
Micro-average F1 score: 0.26804123711340205
Weighted-average F1 score: 0.19244529280243566
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.609271523178808, 37: 0.43410852713178294, 38: 0.5428571428571428}
Micro-average F1 score: 0.2909987669543773
Weighted-average F1 score: 0.21586990277044887

F1 score per class: {0: 0.5263157894736842, 2: 0.2545454545454545, 4: 0.9353233830845771, 5: 0.6312292358803987, 6: 0.32707774798927614, 10: 0.25833333333333336, 11: 0.16606498194945848, 12: 0.14736842105263157, 13: 0.027586206896551724, 15: 0.3888888888888889, 16: 0.44036697247706424, 17: 0.0, 18: 0.045454545454545456, 19: 0.4981132075471698, 21: 0.10126582278481013, 23: 0.6019417475728155, 24: 0.12903225806451613, 25: 0.5066666666666667, 26: 0.5955555555555555, 28: 0.2222222222222222, 29: 0.7242798353909465, 32: 0.7222222222222222, 35: 0.21935483870967742, 37: 0.10460251046025104, 38: 0.16666666666666666, 39: 0.0}
Micro-average F1 score: 0.37593283582089554
Weighted-average F1 score: 0.346441592502301
F1 score per class: {0: 0.3286384976525822, 2: 0.06306306306306306, 4: 0.832579185520362, 5: 0.3968253968253968, 6: 0.27788649706457924, 10: 0.22119815668202766, 11: 0.20973782771535582, 12: 0.17062937062937064, 13: 0.024242424242424242, 15: 0.24, 16: 0.42735042735042733, 17: 0.0, 18: 0.14054054054054055, 19: 0.4132231404958678, 21: 0.0875, 23: 0.4316546762589928, 24: 0.125, 25: 0.48, 26: 0.5596707818930041, 28: 0.16, 29: 0.6929133858267716, 32: 0.6666666666666666, 35: 0.21149425287356322, 37: 0.08615384615384615, 38: 0.168141592920354, 39: 0.14285714285714285}
Micro-average F1 score: 0.2898550724637681
Weighted-average F1 score: 0.2644915809139534
F1 score per class: {0: 0.3167420814479638, 2: 0.07865168539325842, 4: 0.8720379146919431, 5: 0.4608294930875576, 6: 0.27852998065764023, 10: 0.2379746835443038, 11: 0.17791411042944785, 12: 0.19489559164733178, 13: 0.024539877300613498, 15: 0.23076923076923078, 16: 0.416, 17: 0.0, 18: 0.11290322580645161, 19: 0.3989071038251366, 21: 0.08284023668639054, 23: 0.5084745762711864, 24: 0.13043478260869565, 25: 0.5, 26: 0.5714285714285714, 28: 0.23529411764705882, 29: 0.7114624505928854, 32: 0.6666666666666666, 35: 0.2216867469879518, 37: 0.07988587731811697, 38: 0.1444866920152091, 39: 0.0}
Micro-average F1 score: 0.29984152139461173
Weighted-average F1 score: 0.272500788325431
cur_acc_wo_na:  ['0.8132', '0.7456', '0.4310', '0.8495', '0.5046']
his_acc_wo_na:  ['0.8132', '0.7830', '0.7006', '0.7066', '0.6464']
cur_acc des_wo_na:  ['0.8531', '0.8235', '0.7638', '0.8518', '0.6257']
his_acc des_wo_na:  ['0.8531', '0.8290', '0.8021', '0.7820', '0.7106']
cur_acc rrf_wo_na:  ['0.8551', '0.8316', '0.7634', '0.8558', '0.6344']
his_acc rrf_wo_na:  ['0.8551', '0.8237', '0.8010', '0.7758', '0.6987']
cur_acc_w_na:  ['0.6932', '0.5296', '0.2712', '0.5208', '0.2842']
his_acc_w_na:  ['0.6932', '0.5906', '0.4896', '0.4395', '0.3759']
cur_acc des_w_na:  ['0.6558', '0.4292', '0.2975', '0.3198', '0.2680']
his_acc des_w_na:  ['0.6558', '0.4934', '0.3501', '0.3109', '0.2899']
cur_acc rrf_w_na:  ['0.6583', '0.4491', '0.3161', '0.3584', '0.2910']
his_acc rrf_w_na:  ['0.6583', '0.5132', '0.3738', '0.3432', '0.2998']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 188.5250881CurrentTrain: epoch  0, batch     1 | loss: 115.1712732CurrentTrain: epoch  0, batch     2 | loss: 116.4118544CurrentTrain: epoch  0, batch     3 | loss: 117.3089264CurrentTrain: epoch  1, batch     0 | loss: 184.3367352CurrentTrain: epoch  1, batch     1 | loss: 85.4005103CurrentTrain: epoch  1, batch     2 | loss: 89.7922234CurrentTrain: epoch  1, batch     3 | loss: 66.2130180CurrentTrain: epoch  2, batch     0 | loss: 175.7034349CurrentTrain: epoch  2, batch     1 | loss: 103.6197313CurrentTrain: epoch  2, batch     2 | loss: 80.7759801CurrentTrain: epoch  2, batch     3 | loss: 84.0031550CurrentTrain: epoch  3, batch     0 | loss: 84.2336734CurrentTrain: epoch  3, batch     1 | loss: 173.3038253CurrentTrain: epoch  3, batch     2 | loss: 97.7353945CurrentTrain: epoch  3, batch     3 | loss: 64.3726160CurrentTrain: epoch  4, batch     0 | loss: 99.5795553CurrentTrain: epoch  4, batch     1 | loss: 125.5311684CurrentTrain: epoch  4, batch     2 | loss: 98.6311393CurrentTrain: epoch  4, batch     3 | loss: 109.1300945CurrentTrain: epoch  5, batch     0 | loss: 99.3462505CurrentTrain: epoch  5, batch     1 | loss: 97.0002523CurrentTrain: epoch  5, batch     2 | loss: 99.1711844CurrentTrain: epoch  5, batch     3 | loss: 78.4170450CurrentTrain: epoch  6, batch     0 | loss: 98.2286541CurrentTrain: epoch  6, batch     1 | loss: 80.1764517CurrentTrain: epoch  6, batch     2 | loss: 123.1496536CurrentTrain: epoch  6, batch     3 | loss: 57.0900899CurrentTrain: epoch  7, batch     0 | loss: 93.8019521CurrentTrain: epoch  7, batch     1 | loss: 126.0740468CurrentTrain: epoch  7, batch     2 | loss: 122.1749114CurrentTrain: epoch  7, batch     3 | loss: 99.2701271CurrentTrain: epoch  8, batch     0 | loss: 77.2671709CurrentTrain: epoch  8, batch     1 | loss: 75.8212926CurrentTrain: epoch  8, batch     2 | loss: 124.6780872CurrentTrain: epoch  8, batch     3 | loss: 77.0100234CurrentTrain: epoch  9, batch     0 | loss: 97.1932878CurrentTrain: epoch  9, batch     1 | loss: 96.7811647CurrentTrain: epoch  9, batch     2 | loss: 97.1225422CurrentTrain: epoch  9, batch     3 | loss: 54.7006300
MemoryTrain:  epoch  0, batch     0 | loss: 0.5113239MemoryTrain:  epoch  1, batch     0 | loss: 0.5163250MemoryTrain:  epoch  2, batch     0 | loss: 0.3837379MemoryTrain:  epoch  3, batch     0 | loss: 0.2704173MemoryTrain:  epoch  4, batch     0 | loss: 0.2373267MemoryTrain:  epoch  5, batch     0 | loss: 0.1889912MemoryTrain:  epoch  6, batch     0 | loss: 0.1583478MemoryTrain:  epoch  7, batch     0 | loss: 0.1352256MemoryTrain:  epoch  8, batch     0 | loss: 0.1193057MemoryTrain:  epoch  9, batch     0 | loss: 0.1105683

F1 score per class: {33: 0.0, 35: 0.0, 36: 0.07142857142857142, 5: 0.0, 6: 0.0, 38: 0.0, 8: 0.0, 10: 0.0, 11: 0.8979591836734694, 12: 0.0, 13: 0.9142857142857143, 18: 0.42857142857142855, 20: 0.0, 26: 0.5806451612903226, 30: 0.0}
Micro-average F1 score: 0.5329512893982808
Weighted-average F1 score: 0.6043745327546
F1 score per class: {5: 0.0, 6: 0.0, 8: 0.7441860465116279, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.0, 20: 0.9090909090909091, 21: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9444444444444444, 33: 0.42857142857142855, 35: 0.0, 36: 0.7964601769911505, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7505938242280285
Weighted-average F1 score: 0.6916048384289629
F1 score per class: {5: 0.0, 6: 0.0, 8: 0.603448275862069, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.0, 20: 0.92, 21: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 30: 0.9444444444444444, 33: 0.4, 35: 0.0, 36: 0.7857142857142857, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7004830917874396
Weighted-average F1 score: 0.640650832533252

F1 score per class: {0: 0.8787878787878788, 2: 0.7058823529411765, 4: 0.9637305699481865, 5: 0.8755760368663594, 6: 0.5833333333333334, 8: 0.07058823529411765, 10: 0.2903225806451613, 11: 0.31654676258992803, 12: 0.057692307692307696, 13: 0.14285714285714285, 15: 0.8235294117647058, 16: 0.7636363636363637, 17: 0.0, 18: 0.2553191489361702, 19: 0.6748466257668712, 20: 0.8148148148148148, 21: 0.2926829268292683, 23: 0.6933333333333334, 24: 0.17391304347826086, 25: 0.44776119402985076, 26: 0.7046632124352331, 28: 0.2222222222222222, 29: 0.9139784946236559, 30: 0.8888888888888888, 32: 0.7932960893854749, 33: 0.3333333333333333, 35: 0.5070422535211268, 36: 0.5625, 37: 0.475, 38: 0.20512820512820512, 39: 0.0}
Micro-average F1 score: 0.6099662795054327
Weighted-average F1 score: 0.6940974188678137
F1 score per class: {0: 0.9315068493150684, 2: 0.6363636363636364, 4: 0.9528795811518325, 5: 0.8403361344537815, 6: 0.6421052631578947, 8: 0.5133689839572193, 10: 0.417910447761194, 11: 0.48366013071895425, 12: 0.5454545454545454, 13: 0.2222222222222222, 15: 0.5, 16: 0.847457627118644, 17: 0.0, 18: 0.4642857142857143, 19: 0.7182320441988951, 20: 0.9, 21: 0.4, 23: 0.6835443037974683, 24: 0.2222222222222222, 25: 0.5, 26: 0.6938775510204082, 28: 0.0, 29: 0.91005291005291, 30: 0.7391304347826086, 32: 0.8673469387755102, 33: 0.2727272727272727, 35: 0.84, 36: 0.6923076923076923, 37: 0.4489795918367347, 38: 0.47058823529411764, 39: 0.1}
Micro-average F1 score: 0.6770327178490444
Weighted-average F1 score: 0.6947337745135234
F1 score per class: {0: 0.9315068493150684, 2: 0.6, 4: 0.9583333333333334, 5: 0.8461538461538461, 6: 0.6138613861386139, 8: 0.5303030303030303, 10: 0.52, 11: 0.5125, 12: 0.4788732394366197, 13: 0.16, 15: 0.5, 16: 0.8666666666666667, 17: 0.0, 18: 0.4727272727272727, 19: 0.7142857142857143, 20: 0.9019607843137255, 21: 0.375, 23: 0.7012987012987013, 24: 0.15384615384615385, 25: 0.5, 26: 0.6974358974358974, 28: 0.0, 29: 0.925531914893617, 30: 0.7906976744186046, 32: 0.8673469387755102, 33: 0.25, 35: 0.8125, 36: 0.6875, 37: 0.45652173913043476, 38: 0.4444444444444444, 39: 0.0}
Micro-average F1 score: 0.679109364767518
Weighted-average F1 score: 0.6969934544961394

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.07058823529411765, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6717557251908397, 21: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.8888888888888888, 32: 0.0, 33: 0.3157894736842105, 35: 0.0, 36: 0.5046728971962616, 38: 0.0}
Micro-average F1 score: 0.3412844036697248
Weighted-average F1 score: 0.2876960492563779
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5614035087719298, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5521472392638037, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.7391304347826086, 32: 0.0, 33: 0.2222222222222222, 35: 0.0, 36: 0.5625, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.32476875642343267
Weighted-average F1 score: 0.2521528870133615
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5343511450381679, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5508982035928144, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.7906976744186046, 32: 0.0, 33: 0.21428571428571427, 35: 0.0, 36: 0.567741935483871, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.32222222222222224
Weighted-average F1 score: 0.24232838242879612

F1 score per class: {0: 0.6444444444444445, 2: 0.21818181818181817, 4: 0.9253731343283582, 5: 0.6737588652482269, 6: 0.3052959501557632, 8: 0.05263157894736842, 10: 0.2222222222222222, 11: 0.19213973799126638, 12: 0.047619047619047616, 13: 0.03773584905660377, 15: 0.4375, 16: 0.42424242424242425, 17: 0.0, 18: 0.14634146341463414, 19: 0.4845814977973568, 20: 0.24376731301939059, 21: 0.14634146341463414, 23: 0.5714285714285714, 24: 0.12121212121212122, 25: 0.43478260869565216, 26: 0.5836909871244635, 28: 0.2222222222222222, 29: 0.7203389830508474, 30: 0.8648648648648649, 32: 0.696078431372549, 33: 0.13953488372093023, 35: 0.21052631578947367, 36: 0.43548387096774194, 37: 0.3584905660377358, 38: 0.11267605633802817, 39: 0.0}
Micro-average F1 score: 0.4041708043694141
Weighted-average F1 score: 0.39745368358998345
F1 score per class: {0: 0.3148148148148148, 2: 0.12280701754385964, 4: 0.8161434977578476, 5: 0.3367003367003367, 6: 0.2824074074074074, 8: 0.19875776397515527, 10: 0.20437956204379562, 11: 0.22910216718266255, 12: 0.1761006289308176, 13: 0.05063291139240506, 15: 0.21818181818181817, 16: 0.3597122302158273, 17: 0.0, 18: 0.1925925925925926, 19: 0.42207792207792205, 20: 0.1724137931034483, 21: 0.08955223880597014, 23: 0.39705882352941174, 24: 0.13333333333333333, 25: 0.4444444444444444, 26: 0.5271317829457365, 28: 0.0, 29: 0.6539923954372624, 30: 0.3119266055045872, 32: 0.6854838709677419, 33: 0.06382978723404255, 35: 0.21265822784810126, 36: 0.30716723549488056, 37: 0.2146341463414634, 38: 0.16783216783216784, 39: 0.06060606060606061}
Micro-average F1 score: 0.2989985693848355
Weighted-average F1 score: 0.28126942505143904
F1 score per class: {0: 0.3541666666666667, 2: 0.13953488372093023, 4: 0.8846153846153846, 5: 0.4449438202247191, 6: 0.259958071278826, 8: 0.2723735408560311, 10: 0.23853211009174313, 11: 0.2349570200573066, 12: 0.17616580310880828, 13: 0.038461538461538464, 15: 0.19672131147540983, 16: 0.3939393939393939, 17: 0.0, 18: 0.20634920634920634, 19: 0.41533546325878595, 20: 0.16666666666666666, 21: 0.08450704225352113, 23: 0.5, 24: 0.1, 25: 0.48, 26: 0.544, 28: 0.0, 29: 0.6718146718146718, 30: 0.38636363636363635, 32: 0.6910569105691057, 33: 0.057692307692307696, 35: 0.20526315789473684, 36: 0.3076923076923077, 37: 0.23595505617977527, 38: 0.14659685863874344, 39: 0.0}
Micro-average F1 score: 0.31596587446678853
Weighted-average F1 score: 0.29470367962660043
cur_acc_wo_na:  ['0.8132', '0.7456', '0.4310', '0.8495', '0.5046', '0.5330']
his_acc_wo_na:  ['0.8132', '0.7830', '0.7006', '0.7066', '0.6464', '0.6100']
cur_acc des_wo_na:  ['0.8531', '0.8235', '0.7638', '0.8518', '0.6257', '0.7506']
his_acc des_wo_na:  ['0.8531', '0.8290', '0.8021', '0.7820', '0.7106', '0.6770']
cur_acc rrf_wo_na:  ['0.8551', '0.8316', '0.7634', '0.8558', '0.6344', '0.7005']
his_acc rrf_wo_na:  ['0.8551', '0.8237', '0.8010', '0.7758', '0.6987', '0.6791']
cur_acc_w_na:  ['0.6932', '0.5296', '0.2712', '0.5208', '0.2842', '0.3413']
his_acc_w_na:  ['0.6932', '0.5906', '0.4896', '0.4395', '0.3759', '0.4042']
cur_acc des_w_na:  ['0.6558', '0.4292', '0.2975', '0.3198', '0.2680', '0.3248']
his_acc des_w_na:  ['0.6558', '0.4934', '0.3501', '0.3109', '0.2899', '0.2990']
cur_acc rrf_w_na:  ['0.6583', '0.4491', '0.3161', '0.3584', '0.2910', '0.3222']
his_acc rrf_w_na:  ['0.6583', '0.5132', '0.3738', '0.3432', '0.2998', '0.3160']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 104.1065904CurrentTrain: epoch  0, batch     1 | loss: 145.8200250CurrentTrain: epoch  0, batch     2 | loss: 150.7132187CurrentTrain: epoch  0, batch     3 | loss: 42.1975743CurrentTrain: epoch  1, batch     0 | loss: 91.8357268CurrentTrain: epoch  1, batch     1 | loss: 115.3626991CurrentTrain: epoch  1, batch     2 | loss: 109.7504244CurrentTrain: epoch  1, batch     3 | loss: 10.0275340CurrentTrain: epoch  2, batch     0 | loss: 104.7570165CurrentTrain: epoch  2, batch     1 | loss: 84.7283215CurrentTrain: epoch  2, batch     2 | loss: 102.6650658CurrentTrain: epoch  2, batch     3 | loss: 41.4107048CurrentTrain: epoch  3, batch     0 | loss: 136.2433169CurrentTrain: epoch  3, batch     1 | loss: 85.1598723CurrentTrain: epoch  3, batch     2 | loss: 79.2005125CurrentTrain: epoch  3, batch     3 | loss: 8.1939968CurrentTrain: epoch  4, batch     0 | loss: 98.1875628CurrentTrain: epoch  4, batch     1 | loss: 101.3109261CurrentTrain: epoch  4, batch     2 | loss: 77.8057951CurrentTrain: epoch  4, batch     3 | loss: 41.4658175CurrentTrain: epoch  5, batch     0 | loss: 80.3062103CurrentTrain: epoch  5, batch     1 | loss: 78.3727274CurrentTrain: epoch  5, batch     2 | loss: 172.4076796CurrentTrain: epoch  5, batch     3 | loss: 9.8153672CurrentTrain: epoch  6, batch     0 | loss: 82.1564251CurrentTrain: epoch  6, batch     1 | loss: 79.2186214CurrentTrain: epoch  6, batch     2 | loss: 96.0561548CurrentTrain: epoch  6, batch     3 | loss: 5.4473687CurrentTrain: epoch  7, batch     0 | loss: 79.2767072CurrentTrain: epoch  7, batch     1 | loss: 93.6253468CurrentTrain: epoch  7, batch     2 | loss: 79.3991341CurrentTrain: epoch  7, batch     3 | loss: 41.3202084CurrentTrain: epoch  8, batch     0 | loss: 73.6702554CurrentTrain: epoch  8, batch     1 | loss: 122.4640760CurrentTrain: epoch  8, batch     2 | loss: 95.6545419CurrentTrain: epoch  8, batch     3 | loss: 41.3023150CurrentTrain: epoch  9, batch     0 | loss: 92.2282142CurrentTrain: epoch  9, batch     1 | loss: 78.3684180CurrentTrain: epoch  9, batch     2 | loss: 79.5322020CurrentTrain: epoch  9, batch     3 | loss: 41.2693754
MemoryTrain:  epoch  0, batch     0 | loss: 0.6601146MemoryTrain:  epoch  1, batch     0 | loss: 0.5150397MemoryTrain:  epoch  2, batch     0 | loss: 0.3780158MemoryTrain:  epoch  3, batch     0 | loss: 0.2993298MemoryTrain:  epoch  4, batch     0 | loss: 0.2253442MemoryTrain:  epoch  5, batch     0 | loss: 0.1812286MemoryTrain:  epoch  6, batch     0 | loss: 0.1788731MemoryTrain:  epoch  7, batch     0 | loss: 0.1363104MemoryTrain:  epoch  8, batch     0 | loss: 0.1209526MemoryTrain:  epoch  9, batch     0 | loss: 0.1028263

F1 score per class: {7: 0.75, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.6956521739130435, 27: 0.0, 31: 0.5531914893617021}
Micro-average F1 score: 0.5821596244131455
Weighted-average F1 score: 0.48585155943538705
F1 score per class: {35: 0.75, 7: 0.9615384615384616, 40: 0.0, 9: 0.0, 11: 0.0, 19: 0.75, 26: 0.0, 27: 1.0, 30: 0.0, 31: 0.8666666666666667}
Micro-average F1 score: 0.8161434977578476
Weighted-average F1 score: 0.7574923547400612
F1 score per class: {35: 0.75, 7: 0.9615384615384616, 40: 0.0, 9: 0.0, 11: 0.0, 19: 0.75, 26: 0.0, 27: 1.0, 30: 0.0, 31: 0.8666666666666667}
Micro-average F1 score: 0.8161434977578476
Weighted-average F1 score: 0.7574923547400612

F1 score per class: {0: 0.8064516129032258, 2: 0.75, 4: 0.9528795811518325, 5: 0.8738738738738738, 6: 0.37593984962406013, 7: 0.08823529411764706, 8: 0.13793103448275862, 9: 0.9803921568627451, 10: 0.23529411764705882, 11: 0.32679738562091504, 12: 0.09433962264150944, 13: 0.058823529411764705, 15: 0.631578947368421, 16: 0.7857142857142857, 17: 0.0, 18: 0.28, 19: 0.63, 20: 0.8070175438596491, 21: 0.2631578947368421, 23: 0.6933333333333334, 24: 0.09090909090909091, 25: 0.42424242424242425, 26: 0.7120418848167539, 27: 0.47058823529411764, 28: 0.2222222222222222, 29: 0.8961748633879781, 30: 0.8823529411764706, 31: 0.0, 32: 0.7861271676300579, 33: 0.35294117647058826, 35: 0.4307692307692308, 36: 0.5918367346938775, 37: 0.475, 38: 0.21052631578947367, 39: 0.0, 40: 0.47706422018348627}
Micro-average F1 score: 0.5782312925170068
Weighted-average F1 score: 0.6347221197878498
F1 score per class: {0: 0.9117647058823529, 2: 0.5714285714285714, 4: 0.9528795811518325, 5: 0.8130081300813008, 6: 0.4225352112676056, 7: 0.06976744186046512, 8: 0.5620915032679739, 9: 0.8620689655172413, 10: 0.39097744360902253, 11: 0.4785276073619632, 12: 0.5384615384615384, 13: 0.1, 15: 0.6, 16: 0.8666666666666667, 17: 0.36363636363636365, 18: 0.5396825396825397, 19: 0.7346938775510204, 20: 0.86, 21: 0.43478260869565216, 23: 0.7317073170731707, 24: 0.15384615384615385, 25: 0.4931506849315068, 26: 0.6938775510204082, 27: 0.5142857142857142, 28: 0.3076923076923077, 29: 0.8983957219251337, 30: 0.95, 31: 0.4444444444444444, 32: 0.8586387434554974, 33: 0.23076923076923078, 35: 0.8043478260869565, 36: 0.746031746031746, 37: 0.48, 38: 0.52, 39: 0.11764705882352941, 40: 0.7323943661971831}
Micro-average F1 score: 0.6593210244192972
Weighted-average F1 score: 0.6587813373623338
F1 score per class: {0: 0.9117647058823529, 2: 0.6, 4: 0.9528795811518325, 5: 0.8333333333333334, 6: 0.4305555555555556, 7: 0.06976744186046512, 8: 0.5633802816901409, 9: 0.9433962264150944, 10: 0.39705882352941174, 11: 0.47058823529411764, 12: 0.569620253164557, 13: 0.09523809523809523, 15: 0.6, 16: 0.8666666666666667, 17: 0.36363636363636365, 18: 0.5172413793103449, 19: 0.7619047619047619, 20: 0.8823529411764706, 21: 0.4489795918367347, 23: 0.7407407407407407, 24: 0.16666666666666666, 25: 0.5, 26: 0.6974358974358974, 27: 0.5, 28: 0.3333333333333333, 29: 0.9032258064516129, 30: 0.95, 31: 0.4444444444444444, 32: 0.8586387434554974, 33: 0.2222222222222222, 35: 0.8043478260869565, 36: 0.7333333333333333, 37: 0.4897959183673469, 38: 0.4528301886792453, 39: 0.0, 40: 0.7222222222222222}
Micro-average F1 score: 0.6636663666366637
Weighted-average F1 score: 0.6622006327348385

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.6, 9: 0.8771929824561403, 10: 0.0, 11: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 26: 0.0, 27: 0.4444444444444444, 31: 0.0, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.40310077519379844}
Micro-average F1 score: 0.39490445859872614
Weighted-average F1 score: 0.32502175982592135
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.6, 8: 0.0, 9: 0.684931506849315, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.46153846153846156, 29: 0.0, 30: 0.0, 31: 0.14814814814814814, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.5502645502645502}
Micro-average F1 score: 0.36693548387096775
Weighted-average F1 score: 0.3075327097058432
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.6, 8: 0.0, 9: 0.7575757575757576, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.45, 29: 0.0, 30: 0.0, 31: 0.16, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.5226130653266332}
Micro-average F1 score: 0.37995824634655534
Weighted-average F1 score: 0.3221790187967651

F1 score per class: {0: 0.6172839506172839, 2: 0.23529411764705882, 4: 0.9191919191919192, 5: 0.5672514619883041, 6: 0.24271844660194175, 7: 0.02857142857142857, 8: 0.10714285714285714, 9: 0.847457627118644, 10: 0.2, 11: 0.19157088122605365, 12: 0.07194244604316546, 13: 0.018018018018018018, 15: 0.375, 16: 0.4444444444444444, 17: 0.0, 18: 0.12844036697247707, 19: 0.45161290322580644, 20: 0.2216867469879518, 21: 0.1724137931034483, 23: 0.5473684210526316, 24: 0.06896551724137931, 25: 0.4057971014492754, 26: 0.5738396624472574, 27: 0.13445378151260504, 28: 0.2222222222222222, 29: 0.6919831223628692, 30: 0.8823529411764706, 31: 0.0, 32: 0.6834170854271356, 33: 0.13636363636363635, 35: 0.2413793103448276, 36: 0.4393939393939394, 37: 0.34234234234234234, 38: 0.11428571428571428, 39: 0.0, 40: 0.2765957446808511}
Micro-average F1 score: 0.365277180919639
Weighted-average F1 score: 0.3457993132134711
F1 score per class: {0: 0.484375, 2: 0.12631578947368421, 4: 0.8425925925925926, 5: 0.23809523809523808, 6: 0.22388059701492538, 7: 0.020618556701030927, 8: 0.25219941348973607, 9: 0.45871559633027525, 10: 0.18571428571428572, 11: 0.22413793103448276, 12: 0.1728395061728395, 13: 0.023529411764705882, 15: 0.3333333333333333, 16: 0.33548387096774196, 17: 0.16666666666666666, 18: 0.14049586776859505, 19: 0.41379310344827586, 20: 0.1762295081967213, 21: 0.12269938650306748, 23: 0.3592814371257485, 24: 0.11764705882352941, 25: 0.3711340206185567, 26: 0.5230769230769231, 27: 0.13432835820895522, 28: 0.11764705882352941, 29: 0.6536964980544747, 30: 0.6785714285714286, 31: 0.020942408376963352, 32: 0.6666666666666666, 33: 0.046153846153846156, 35: 0.2792452830188679, 36: 0.3715415019762846, 37: 0.19834710743801653, 38: 0.16666666666666666, 39: 0.05555555555555555, 40: 0.29295774647887324}
Micro-average F1 score: 0.2788061956932376
Weighted-average F1 score: 0.25750202300556335
F1 score per class: {0: 0.4881889763779528, 2: 0.16, 4: 0.900990099009901, 5: 0.33557046979865773, 6: 0.22545454545454546, 7: 0.020761245674740483, 8: 0.273972602739726, 9: 0.6024096385542169, 10: 0.18685121107266436, 11: 0.20833333333333334, 12: 0.2200488997555012, 13: 0.022598870056497175, 15: 0.25, 16: 0.35374149659863946, 17: 0.17391304347826086, 18: 0.18404907975460122, 19: 0.46601941747572817, 20: 0.16853932584269662, 21: 0.12429378531073447, 23: 0.4580152671755725, 24: 0.125, 25: 0.4235294117647059, 26: 0.5375494071146245, 27: 0.1267605633802817, 28: 0.13793103448275862, 29: 0.6511627906976745, 30: 0.7755102040816326, 31: 0.024390243902439025, 32: 0.6586345381526104, 33: 0.04, 35: 0.27611940298507465, 36: 0.3176895306859206, 37: 0.22748815165876776, 38: 0.14906832298136646, 39: 0.0, 40: 0.2773333333333333}
Micro-average F1 score: 0.2966738197424893
Weighted-average F1 score: 0.2726644832099396
cur_acc_wo_na:  ['0.8132', '0.7456', '0.4310', '0.8495', '0.5046', '0.5330', '0.5822']
his_acc_wo_na:  ['0.8132', '0.7830', '0.7006', '0.7066', '0.6464', '0.6100', '0.5782']
cur_acc des_wo_na:  ['0.8531', '0.8235', '0.7638', '0.8518', '0.6257', '0.7506', '0.8161']
his_acc des_wo_na:  ['0.8531', '0.8290', '0.8021', '0.7820', '0.7106', '0.6770', '0.6593']
cur_acc rrf_wo_na:  ['0.8551', '0.8316', '0.7634', '0.8558', '0.6344', '0.7005', '0.8161']
his_acc rrf_wo_na:  ['0.8551', '0.8237', '0.8010', '0.7758', '0.6987', '0.6791', '0.6637']
cur_acc_w_na:  ['0.6932', '0.5296', '0.2712', '0.5208', '0.2842', '0.3413', '0.3949']
his_acc_w_na:  ['0.6932', '0.5906', '0.4896', '0.4395', '0.3759', '0.4042', '0.3653']
cur_acc des_w_na:  ['0.6558', '0.4292', '0.2975', '0.3198', '0.2680', '0.3248', '0.3669']
his_acc des_w_na:  ['0.6558', '0.4934', '0.3501', '0.3109', '0.2899', '0.2990', '0.2788']
cur_acc rrf_w_na:  ['0.6583', '0.4491', '0.3161', '0.3584', '0.2910', '0.3222', '0.3800']
his_acc rrf_w_na:  ['0.6583', '0.5132', '0.3738', '0.3432', '0.2998', '0.3160', '0.2967']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 105.9577437CurrentTrain: epoch  0, batch     1 | loss: 141.4587405CurrentTrain: epoch  0, batch     2 | loss: 191.1548033CurrentTrain: epoch  0, batch     3 | loss: 121.5873786CurrentTrain: epoch  0, batch     4 | loss: 110.0138456CurrentTrain: epoch  1, batch     0 | loss: 93.9402091CurrentTrain: epoch  1, batch     1 | loss: 191.9407770CurrentTrain: epoch  1, batch     2 | loss: 87.5063116CurrentTrain: epoch  1, batch     3 | loss: 113.9124995CurrentTrain: epoch  1, batch     4 | loss: 99.1566935CurrentTrain: epoch  2, batch     0 | loss: 86.8557998CurrentTrain: epoch  2, batch     1 | loss: 103.0939186CurrentTrain: epoch  2, batch     2 | loss: 84.4807378CurrentTrain: epoch  2, batch     3 | loss: 106.3837765CurrentTrain: epoch  2, batch     4 | loss: 162.5978981CurrentTrain: epoch  3, batch     0 | loss: 130.9450263CurrentTrain: epoch  3, batch     1 | loss: 128.8121533CurrentTrain: epoch  3, batch     2 | loss: 82.0736948CurrentTrain: epoch  3, batch     3 | loss: 128.1704524CurrentTrain: epoch  3, batch     4 | loss: 70.2835046CurrentTrain: epoch  4, batch     0 | loss: 129.8996072CurrentTrain: epoch  4, batch     1 | loss: 99.0288259CurrentTrain: epoch  4, batch     2 | loss: 102.5650075CurrentTrain: epoch  4, batch     3 | loss: 102.3351489CurrentTrain: epoch  4, batch     4 | loss: 70.0344698CurrentTrain: epoch  5, batch     0 | loss: 127.8502769CurrentTrain: epoch  5, batch     1 | loss: 97.2369284CurrentTrain: epoch  5, batch     2 | loss: 130.8204582CurrentTrain: epoch  5, batch     3 | loss: 168.8607055CurrentTrain: epoch  5, batch     4 | loss: 99.0639514CurrentTrain: epoch  6, batch     0 | loss: 81.7095425CurrentTrain: epoch  6, batch     1 | loss: 98.2361076CurrentTrain: epoch  6, batch     2 | loss: 174.7896657CurrentTrain: epoch  6, batch     3 | loss: 80.6500292CurrentTrain: epoch  6, batch     4 | loss: 99.6477347CurrentTrain: epoch  7, batch     0 | loss: 81.2789005CurrentTrain: epoch  7, batch     1 | loss: 129.5384871CurrentTrain: epoch  7, batch     2 | loss: 79.4030792CurrentTrain: epoch  7, batch     3 | loss: 127.1678611CurrentTrain: epoch  7, batch     4 | loss: 95.5575871CurrentTrain: epoch  8, batch     0 | loss: 97.4582205CurrentTrain: epoch  8, batch     1 | loss: 560.1491164CurrentTrain: epoch  8, batch     2 | loss: 97.2519913CurrentTrain: epoch  8, batch     3 | loss: 94.3647060CurrentTrain: epoch  8, batch     4 | loss: 70.7528862CurrentTrain: epoch  9, batch     0 | loss: 126.8204442CurrentTrain: epoch  9, batch     1 | loss: 96.5934418CurrentTrain: epoch  9, batch     2 | loss: 97.3764058CurrentTrain: epoch  9, batch     3 | loss: 124.9482428CurrentTrain: epoch  9, batch     4 | loss: 70.2442473
MemoryTrain:  epoch  0, batch     0 | loss: 0.8426784MemoryTrain:  epoch  1, batch     0 | loss: 0.7079313MemoryTrain:  epoch  2, batch     0 | loss: 0.5886227MemoryTrain:  epoch  3, batch     0 | loss: 0.4746349MemoryTrain:  epoch  4, batch     0 | loss: 0.3828720MemoryTrain:  epoch  5, batch     0 | loss: 0.3723257MemoryTrain:  epoch  6, batch     0 | loss: 0.2631874MemoryTrain:  epoch  7, batch     0 | loss: 0.2002600MemoryTrain:  epoch  8, batch     0 | loss: 0.1965988MemoryTrain:  epoch  9, batch     0 | loss: 0.1540280

F1 score per class: {32: 0.3937007874015748, 1: 0.4230769230769231, 34: 0.0, 35: 0.11428571428571428, 3: 0.0, 37: 0.6748466257668712, 11: 0.0, 14: 0.0, 21: 0.0, 22: 0.0, 23: 0.3582089552238806, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.4
Weighted-average F1 score: 0.36321047879477
F1 score per class: {32: 0.4806201550387597, 1: 0.6333333333333333, 34: 0.0, 35: 0.0, 3: 0.0, 37: 0.08695652173913043, 36: 0.0, 9: 0.0, 10: 0.6826347305389222, 11: 0.0, 14: 0.0, 19: 0.0, 21: 0.0, 22: 0.8421052631578947, 23: 0.0, 24: 0.0, 31: 0.0}
Micro-average F1 score: 0.5144596651445966
Weighted-average F1 score: 0.46162042346993226
F1 score per class: {32: 0.48854961832061067, 1: 0.6218487394957983, 34: 0.0, 35: 0.0, 3: 0.08571428571428572, 37: 0.0, 36: 0.7209302325581395, 10: 0.0, 11: 0.0, 14: 0.0, 21: 0.6904761904761905, 22: 0.0, 23: 0.0, 24: 0.0}
Micro-average F1 score: 0.5
Weighted-average F1 score: 0.4470185136211894

F1 score per class: {0: 0.6037735849056604, 1: 0.3448275862068966, 2: 0.7142857142857143, 3: 0.4036697247706422, 4: 0.918918918918919, 5: 0.8755760368663594, 6: 0.3305785123966942, 7: 0.06060606060606061, 8: 0.09302325581395349, 9: 0.9803921568627451, 10: 0.25210084033613445, 11: 0.2857142857142857, 12: 0.07692307692307693, 13: 0.05263157894736842, 14: 0.10810810810810811, 15: 0.625, 16: 0.7272727272727273, 17: 0.0, 18: 0.05128205128205128, 19: 0.55, 20: 0.7962962962962963, 21: 0.06060606060606061, 22: 0.6358381502890174, 23: 0.725, 24: 0.08695652173913043, 25: 0.4, 26: 0.7120418848167539, 27: 0.0, 28: 0.2222222222222222, 29: 0.8777777777777778, 30: 0.8823529411764706, 31: 0.0, 32: 0.7954545454545454, 33: 0.2857142857142857, 34: 0.20689655172413793, 35: 0.32075471698113206, 36: 0.3855421686746988, 37: 0.30952380952380953, 38: 0.2222222222222222, 39: 0.0, 40: 0.6608695652173913}
Micro-average F1 score: 0.5147484094852516
Weighted-average F1 score: 0.5681993588506586
F1 score per class: {0: 0.8253968253968254, 1: 0.3974358974358974, 2: 0.5217391304347826, 3: 0.5671641791044776, 4: 0.918918918918919, 5: 0.8298755186721992, 6: 0.45390070921985815, 7: 0.08450704225352113, 8: 0.6, 9: 0.8620689655172413, 10: 0.4264705882352941, 11: 0.21818181818181817, 12: 0.5170068027210885, 13: 0.08695652173913043, 14: 0.08333333333333333, 15: 0.6, 16: 0.8666666666666667, 17: 0.36363636363636365, 18: 0.14634146341463414, 19: 0.6480446927374302, 20: 0.8571428571428571, 21: 0.21739130434782608, 22: 0.6440677966101694, 23: 0.7228915662650602, 24: 0.07407407407407407, 25: 0.5, 26: 0.700507614213198, 27: 0.0, 28: 0.2222222222222222, 29: 0.9032258064516129, 30: 0.972972972972973, 31: 0.5, 32: 0.8541666666666666, 33: 0.20689655172413793, 34: 0.365296803652968, 35: 0.5223880597014925, 36: 0.7413793103448276, 37: 0.22784810126582278, 38: 0.3333333333333333, 39: 0.11764705882352941, 40: 0.723404255319149}
Micro-average F1 score: 0.5883534136546185
Weighted-average F1 score: 0.5965871360549878
F1 score per class: {0: 0.8253968253968254, 1: 0.3950617283950617, 2: 0.6, 3: 0.5692307692307692, 4: 0.9361702127659575, 5: 0.8497854077253219, 6: 0.43243243243243246, 7: 0.08333333333333333, 8: 0.512396694214876, 9: 0.9615384615384616, 10: 0.425531914893617, 11: 0.3111111111111111, 12: 0.3937007874015748, 13: 0.08, 14: 0.08, 15: 0.6, 16: 0.8852459016393442, 17: 0.36363636363636365, 18: 0.18181818181818182, 19: 0.6395348837209303, 20: 0.84, 21: 0.23809523809523808, 22: 0.6850828729281768, 23: 0.7317073170731707, 24: 0.07692307692307693, 25: 0.4788732394366197, 26: 0.700507614213198, 27: 0.0, 28: 0.2222222222222222, 29: 0.9032258064516129, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.84375, 33: 0.17647058823529413, 34: 0.3391812865497076, 35: 0.5401459854014599, 36: 0.660377358490566, 37: 0.20454545454545456, 38: 0.28, 39: 0.0, 40: 0.723404255319149}
Micro-average F1 score: 0.5814369726412683
Weighted-average F1 score: 0.5895835655999373

F1 score per class: {0: 0.0, 1: 0.16556291390728478, 3: 0.39285714285714285, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 13: 0.0, 14: 0.10810810810810811, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4330708661417323, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.3116883116883117, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.21299638989169675
Weighted-average F1 score: 0.1690827445935434
F1 score per class: {0: 0.0, 1: 0.18507462686567164, 2: 0.0, 3: 0.3958333333333333, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.05555555555555555, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4769874476987448, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.4678362573099415, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.19503750721292556
Weighted-average F1 score: 0.15865050339439668
F1 score per class: {0: 0.0, 1: 0.1833810888252149, 2: 0.0, 3: 0.4134078212290503, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.05504587155963303, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.48627450980392156, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.4915254237288136, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2
Weighted-average F1 score: 0.159252212454339

F1 score per class: {0: 0.463768115942029, 1: 0.1187648456057007, 2: 0.2127659574468085, 3: 0.29931972789115646, 4: 0.8854166666666666, 5: 0.6834532374100719, 6: 0.23529411764705882, 7: 0.019704433497536946, 8: 0.06451612903225806, 9: 0.819672131147541, 10: 0.18633540372670807, 11: 0.18009478672985782, 12: 0.07079646017699115, 13: 0.015503875968992248, 14: 0.08421052631578947, 15: 0.3225806451612903, 16: 0.42105263157894735, 17: 0.0, 18: 0.038461538461538464, 19: 0.3793103448275862, 20: 0.30935251798561153, 21: 0.05555555555555555, 22: 0.3536977491961415, 23: 0.5576923076923077, 24: 0.07692307692307693, 25: 0.3939393939393939, 26: 0.5690376569037657, 27: 0.0, 28: 0.2222222222222222, 29: 0.6448979591836734, 30: 0.8571428571428571, 31: 0.0, 32: 0.5785123966942148, 33: 0.13636363636363635, 34: 0.13714285714285715, 35: 0.1596244131455399, 36: 0.32, 37: 0.22033898305084745, 38: 0.1111111111111111, 39: 0.0, 40: 0.44970414201183434}
Micro-average F1 score: 0.3304864463423691
Weighted-average F1 score: 0.31790154712294055
F1 score per class: {0: 0.5098039215686274, 1: 0.12180746561886051, 2: 0.14814814814814814, 3: 0.21652421652421652, 4: 0.8585858585858586, 5: 0.3305785123966942, 6: 0.23529411764705882, 7: 0.023255813953488372, 8: 0.23645320197044334, 9: 0.2824858757062147, 10: 0.19931271477663232, 11: 0.11320754716981132, 12: 0.19095477386934673, 13: 0.018518518518518517, 14: 0.040268456375838924, 15: 0.23076923076923078, 16: 0.4094488188976378, 17: 0.21052631578947367, 18: 0.0759493670886076, 19: 0.336231884057971, 20: 0.2736156351791531, 21: 0.072992700729927, 22: 0.3917525773195876, 23: 0.3614457831325301, 24: 0.05263157894736842, 25: 0.4444444444444444, 26: 0.5130111524163569, 27: 0.0, 28: 0.125, 29: 0.631578947368421, 30: 0.6545454545454545, 31: 0.029411764705882353, 32: 0.5157232704402516, 33: 0.05263157894736842, 34: 0.09512485136741974, 35: 0.1694915254237288, 36: 0.38222222222222224, 37: 0.13333333333333333, 38: 0.10457516339869281, 39: 0.07407407407407407, 40: 0.3063063063063063}
Micro-average F1 score: 0.2550043516100957
Weighted-average F1 score: 0.23732259240135933
F1 score per class: {0: 0.49523809523809526, 1: 0.11962616822429907, 2: 0.19047619047619047, 3: 0.23125, 4: 0.8934010152284264, 5: 0.48292682926829267, 6: 0.2206896551724138, 7: 0.022727272727272728, 8: 0.27802690582959644, 9: 0.5952380952380952, 10: 0.19230769230769232, 11: 0.12923076923076923, 12: 0.18796992481203006, 13: 0.017241379310344827, 14: 0.03870967741935484, 15: 0.25, 16: 0.4251968503937008, 17: 0.21052631578947367, 18: 0.08080808080808081, 19: 0.34375, 20: 0.26332288401253917, 21: 0.08695652173913043, 22: 0.3862928348909657, 23: 0.46875, 24: 0.05714285714285714, 25: 0.4533333333333333, 26: 0.5369649805447471, 27: 0.0, 28: 0.2, 29: 0.6387832699619772, 30: 0.72, 31: 0.037383177570093455, 32: 0.5126582278481012, 33: 0.049586776859504134, 34: 0.1228813559322034, 35: 0.1783132530120482, 36: 0.35714285714285715, 37: 0.11392405063291139, 38: 0.0875, 39: 0.0, 40: 0.30538922155688625}
Micro-average F1 score: 0.27427330840670605
Weighted-average F1 score: 0.2543569614983993
cur_acc_wo_na:  ['0.8132', '0.7456', '0.4310', '0.8495', '0.5046', '0.5330', '0.5822', '0.4000']
his_acc_wo_na:  ['0.8132', '0.7830', '0.7006', '0.7066', '0.6464', '0.6100', '0.5782', '0.5147']
cur_acc des_wo_na:  ['0.8531', '0.8235', '0.7638', '0.8518', '0.6257', '0.7506', '0.8161', '0.5145']
his_acc des_wo_na:  ['0.8531', '0.8290', '0.8021', '0.7820', '0.7106', '0.6770', '0.6593', '0.5884']
cur_acc rrf_wo_na:  ['0.8551', '0.8316', '0.7634', '0.8558', '0.6344', '0.7005', '0.8161', '0.5000']
his_acc rrf_wo_na:  ['0.8551', '0.8237', '0.8010', '0.7758', '0.6987', '0.6791', '0.6637', '0.5814']
cur_acc_w_na:  ['0.6932', '0.5296', '0.2712', '0.5208', '0.2842', '0.3413', '0.3949', '0.2130']
his_acc_w_na:  ['0.6932', '0.5906', '0.4896', '0.4395', '0.3759', '0.4042', '0.3653', '0.3305']
cur_acc des_w_na:  ['0.6558', '0.4292', '0.2975', '0.3198', '0.2680', '0.3248', '0.3669', '0.1950']
his_acc des_w_na:  ['0.6558', '0.4934', '0.3501', '0.3109', '0.2899', '0.2990', '0.2788', '0.2550']
cur_acc rrf_w_na:  ['0.6583', '0.4491', '0.3161', '0.3584', '0.2910', '0.3222', '0.3800', '0.2000']
his_acc rrf_w_na:  ['0.6583', '0.5132', '0.3738', '0.3432', '0.2998', '0.3160', '0.2967', '0.2743']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 114.0789428CurrentTrain: epoch  0, batch     1 | loss: 147.7819197CurrentTrain: epoch  0, batch     2 | loss: 102.9166557CurrentTrain: epoch  0, batch     3 | loss: 118.6515072CurrentTrain: epoch  0, batch     4 | loss: 119.2699711CurrentTrain: epoch  0, batch     5 | loss: 149.5670997CurrentTrain: epoch  0, batch     6 | loss: 118.0294721CurrentTrain: epoch  0, batch     7 | loss: 121.7874368CurrentTrain: epoch  0, batch     8 | loss: 98.6306066CurrentTrain: epoch  0, batch     9 | loss: 144.6021995CurrentTrain: epoch  0, batch    10 | loss: 117.9083412CurrentTrain: epoch  0, batch    11 | loss: 116.4383738CurrentTrain: epoch  0, batch    12 | loss: 98.8001941CurrentTrain: epoch  0, batch    13 | loss: 116.5198664CurrentTrain: epoch  0, batch    14 | loss: 99.1792040CurrentTrain: epoch  0, batch    15 | loss: 144.0804037CurrentTrain: epoch  0, batch    16 | loss: 96.8864397CurrentTrain: epoch  0, batch    17 | loss: 144.5197652CurrentTrain: epoch  0, batch    18 | loss: 143.2938065CurrentTrain: epoch  0, batch    19 | loss: 97.1199416CurrentTrain: epoch  0, batch    20 | loss: 116.4381360CurrentTrain: epoch  0, batch    21 | loss: 87.8211877CurrentTrain: epoch  0, batch    22 | loss: 97.3215858CurrentTrain: epoch  0, batch    23 | loss: 116.1794522CurrentTrain: epoch  0, batch    24 | loss: 114.9937564CurrentTrain: epoch  0, batch    25 | loss: 97.8525074CurrentTrain: epoch  0, batch    26 | loss: 98.5142268CurrentTrain: epoch  0, batch    27 | loss: 98.0977863CurrentTrain: epoch  0, batch    28 | loss: 143.2382813CurrentTrain: epoch  0, batch    29 | loss: 282.4027125CurrentTrain: epoch  0, batch    30 | loss: 281.5799456CurrentTrain: epoch  0, batch    31 | loss: 97.1807593CurrentTrain: epoch  0, batch    32 | loss: 115.6819479CurrentTrain: epoch  0, batch    33 | loss: 116.1057906CurrentTrain: epoch  0, batch    34 | loss: 97.5554021CurrentTrain: epoch  0, batch    35 | loss: 96.7526998CurrentTrain: epoch  0, batch    36 | loss: 115.1291709CurrentTrain: epoch  0, batch    37 | loss: 96.6851297CurrentTrain: epoch  0, batch    38 | loss: 141.3364967CurrentTrain: epoch  0, batch    39 | loss: 280.5603958CurrentTrain: epoch  0, batch    40 | loss: 115.5148421CurrentTrain: epoch  0, batch    41 | loss: 115.7572925CurrentTrain: epoch  0, batch    42 | loss: 114.2660133CurrentTrain: epoch  0, batch    43 | loss: 96.4664080CurrentTrain: epoch  0, batch    44 | loss: 114.1774511CurrentTrain: epoch  0, batch    45 | loss: 141.0653242CurrentTrain: epoch  0, batch    46 | loss: 97.0340198CurrentTrain: epoch  0, batch    47 | loss: 112.8336827CurrentTrain: epoch  0, batch    48 | loss: 115.4814732CurrentTrain: epoch  0, batch    49 | loss: 188.2910066CurrentTrain: epoch  0, batch    50 | loss: 95.3178239CurrentTrain: epoch  0, batch    51 | loss: 114.5129282CurrentTrain: epoch  0, batch    52 | loss: 186.8108680CurrentTrain: epoch  0, batch    53 | loss: 82.1310044CurrentTrain: epoch  0, batch    54 | loss: 95.3447214CurrentTrain: epoch  0, batch    55 | loss: 187.2980604CurrentTrain: epoch  0, batch    56 | loss: 113.4913660CurrentTrain: epoch  0, batch    57 | loss: 96.3100917CurrentTrain: epoch  0, batch    58 | loss: 139.0527042CurrentTrain: epoch  0, batch    59 | loss: 188.9161185CurrentTrain: epoch  0, batch    60 | loss: 95.0185488CurrentTrain: epoch  0, batch    61 | loss: 94.1900826CurrentTrain: epoch  0, batch    62 | loss: 112.5556907CurrentTrain: epoch  0, batch    63 | loss: 112.6208181CurrentTrain: epoch  0, batch    64 | loss: 113.0292854CurrentTrain: epoch  0, batch    65 | loss: 113.2787199CurrentTrain: epoch  0, batch    66 | loss: 112.0027912CurrentTrain: epoch  0, batch    67 | loss: 140.8855818CurrentTrain: epoch  0, batch    68 | loss: 112.1538827CurrentTrain: epoch  0, batch    69 | loss: 92.8558608CurrentTrain: epoch  0, batch    70 | loss: 112.8487928CurrentTrain: epoch  0, batch    71 | loss: 185.3205035CurrentTrain: epoch  0, batch    72 | loss: 113.5847348CurrentTrain: epoch  0, batch    73 | loss: 109.6616433CurrentTrain: epoch  0, batch    74 | loss: 112.0626545CurrentTrain: epoch  0, batch    75 | loss: 140.2224428CurrentTrain: epoch  0, batch    76 | loss: 185.8988821CurrentTrain: epoch  0, batch    77 | loss: 138.7713105CurrentTrain: epoch  0, batch    78 | loss: 112.8376171CurrentTrain: epoch  0, batch    79 | loss: 92.8521352CurrentTrain: epoch  0, batch    80 | loss: 90.9245313CurrentTrain: epoch  0, batch    81 | loss: 91.9237578CurrentTrain: epoch  0, batch    82 | loss: 111.1951818CurrentTrain: epoch  0, batch    83 | loss: 185.7468672CurrentTrain: epoch  0, batch    84 | loss: 109.4402078CurrentTrain: epoch  0, batch    85 | loss: 78.9329937CurrentTrain: epoch  0, batch    86 | loss: 181.9304826CurrentTrain: epoch  0, batch    87 | loss: 108.1451821CurrentTrain: epoch  0, batch    88 | loss: 109.4992059CurrentTrain: epoch  0, batch    89 | loss: 91.3560672CurrentTrain: epoch  0, batch    90 | loss: 109.7160355CurrentTrain: epoch  0, batch    91 | loss: 186.9217343CurrentTrain: epoch  0, batch    92 | loss: 138.5777678CurrentTrain: epoch  0, batch    93 | loss: 182.6661681CurrentTrain: epoch  0, batch    94 | loss: 136.2852071CurrentTrain: epoch  0, batch    95 | loss: 94.9777899CurrentTrain: epoch  1, batch     0 | loss: 92.0716175CurrentTrain: epoch  1, batch     1 | loss: 111.3893962CurrentTrain: epoch  1, batch     2 | loss: 90.2290058CurrentTrain: epoch  1, batch     3 | loss: 91.0610150CurrentTrain: epoch  1, batch     4 | loss: 89.0019478CurrentTrain: epoch  1, batch     5 | loss: 108.8744251CurrentTrain: epoch  1, batch     6 | loss: 111.2763178CurrentTrain: epoch  1, batch     7 | loss: 108.3018869CurrentTrain: epoch  1, batch     8 | loss: 111.8379251CurrentTrain: epoch  1, batch     9 | loss: 91.4163871CurrentTrain: epoch  1, batch    10 | loss: 108.9020026CurrentTrain: epoch  1, batch    11 | loss: 88.3962151CurrentTrain: epoch  1, batch    12 | loss: 183.5781267CurrentTrain: epoch  1, batch    13 | loss: 87.2306016CurrentTrain: epoch  1, batch    14 | loss: 108.0301981CurrentTrain: epoch  1, batch    15 | loss: 185.0364089CurrentTrain: epoch  1, batch    16 | loss: 109.5189535CurrentTrain: epoch  1, batch    17 | loss: 89.4397983CurrentTrain: epoch  1, batch    18 | loss: 106.0538370CurrentTrain: epoch  1, batch    19 | loss: 108.1160876CurrentTrain: epoch  1, batch    20 | loss: 133.9513763CurrentTrain: epoch  1, batch    21 | loss: 132.5242158CurrentTrain: epoch  1, batch    22 | loss: 108.2181724CurrentTrain: epoch  1, batch    23 | loss: 73.8304902CurrentTrain: epoch  1, batch    24 | loss: 133.6530671CurrentTrain: epoch  1, batch    25 | loss: 137.9404477CurrentTrain: epoch  1, batch    26 | loss: 107.1640593CurrentTrain: epoch  1, batch    27 | loss: 71.8655534CurrentTrain: epoch  1, batch    28 | loss: 106.7934050CurrentTrain: epoch  1, batch    29 | loss: 105.6975270CurrentTrain: epoch  1, batch    30 | loss: 109.9327746CurrentTrain: epoch  1, batch    31 | loss: 84.6086209CurrentTrain: epoch  1, batch    32 | loss: 81.6404187CurrentTrain: epoch  1, batch    33 | loss: 72.1522812CurrentTrain: epoch  1, batch    34 | loss: 106.1982671CurrentTrain: epoch  1, batch    35 | loss: 134.5431318CurrentTrain: epoch  1, batch    36 | loss: 134.1157850CurrentTrain: epoch  1, batch    37 | loss: 135.1909227CurrentTrain: epoch  1, batch    38 | loss: 136.3294547CurrentTrain: epoch  1, batch    39 | loss: 108.4137380CurrentTrain: epoch  1, batch    40 | loss: 130.9031177CurrentTrain: epoch  1, batch    41 | loss: 105.9555386CurrentTrain: epoch  1, batch    42 | loss: 131.9348846CurrentTrain: epoch  1, batch    43 | loss: 103.0485297CurrentTrain: epoch  1, batch    44 | loss: 85.9054228CurrentTrain: epoch  1, batch    45 | loss: 136.1001763CurrentTrain: epoch  1, batch    46 | loss: 106.2277456CurrentTrain: epoch  1, batch    47 | loss: 106.2067452CurrentTrain: epoch  1, batch    48 | loss: 106.9471086CurrentTrain: epoch  1, batch    49 | loss: 84.5901385CurrentTrain: epoch  1, batch    50 | loss: 135.6535266CurrentTrain: epoch  1, batch    51 | loss: 179.2830944CurrentTrain: epoch  1, batch    52 | loss: 136.5276319CurrentTrain: epoch  1, batch    53 | loss: 72.3078744CurrentTrain: epoch  1, batch    54 | loss: 130.5166451CurrentTrain: epoch  1, batch    55 | loss: 173.6943939CurrentTrain: epoch  1, batch    56 | loss: 133.8892153CurrentTrain: epoch  1, batch    57 | loss: 90.1022656CurrentTrain: epoch  1, batch    58 | loss: 135.5993008CurrentTrain: epoch  1, batch    59 | loss: 71.7885506CurrentTrain: epoch  1, batch    60 | loss: 95.3791751CurrentTrain: epoch  1, batch    61 | loss: 125.8504236CurrentTrain: epoch  1, batch    62 | loss: 85.0521291CurrentTrain: epoch  1, batch    63 | loss: 109.8342027CurrentTrain: epoch  1, batch    64 | loss: 138.7474176CurrentTrain: epoch  1, batch    65 | loss: 86.3705604CurrentTrain: epoch  1, batch    66 | loss: 85.6954863CurrentTrain: epoch  1, batch    67 | loss: 269.7301330CurrentTrain: epoch  1, batch    68 | loss: 71.0728927CurrentTrain: epoch  1, batch    69 | loss: 82.9542719CurrentTrain: epoch  1, batch    70 | loss: 86.2460621CurrentTrain: epoch  1, batch    71 | loss: 87.3087166CurrentTrain: epoch  1, batch    72 | loss: 100.1989850CurrentTrain: epoch  1, batch    73 | loss: 132.3990878CurrentTrain: epoch  1, batch    74 | loss: 107.7385582CurrentTrain: epoch  1, batch    75 | loss: 177.2591818CurrentTrain: epoch  1, batch    76 | loss: 106.4996831CurrentTrain: epoch  1, batch    77 | loss: 87.0692655CurrentTrain: epoch  1, batch    78 | loss: 132.9937508CurrentTrain: epoch  1, batch    79 | loss: 106.4051379CurrentTrain: epoch  1, batch    80 | loss: 105.7406371CurrentTrain: epoch  1, batch    81 | loss: 103.8551959CurrentTrain: epoch  1, batch    82 | loss: 107.7244293CurrentTrain: epoch  1, batch    83 | loss: 83.2550335CurrentTrain: epoch  1, batch    84 | loss: 104.6439274CurrentTrain: epoch  1, batch    85 | loss: 79.8967797CurrentTrain: epoch  1, batch    86 | loss: 136.4389489CurrentTrain: epoch  1, batch    87 | loss: 102.8246853CurrentTrain: epoch  1, batch    88 | loss: 110.1067337CurrentTrain: epoch  1, batch    89 | loss: 104.4753633CurrentTrain: epoch  1, batch    90 | loss: 99.3613379CurrentTrain: epoch  1, batch    91 | loss: 124.6072856CurrentTrain: epoch  1, batch    92 | loss: 85.8955473CurrentTrain: epoch  1, batch    93 | loss: 88.2528847CurrentTrain: epoch  1, batch    94 | loss: 133.2310980CurrentTrain: epoch  1, batch    95 | loss: 87.0831236CurrentTrain: epoch  2, batch     0 | loss: 125.4622410CurrentTrain: epoch  2, batch     1 | loss: 178.5547046CurrentTrain: epoch  2, batch     2 | loss: 80.6845305CurrentTrain: epoch  2, batch     3 | loss: 73.1392108CurrentTrain: epoch  2, batch     4 | loss: 83.9805348CurrentTrain: epoch  2, batch     5 | loss: 183.3185329CurrentTrain: epoch  2, batch     6 | loss: 72.2865180CurrentTrain: epoch  2, batch     7 | loss: 101.8149100CurrentTrain: epoch  2, batch     8 | loss: 127.6641887CurrentTrain: epoch  2, batch     9 | loss: 71.4511374CurrentTrain: epoch  2, batch    10 | loss: 79.9842349CurrentTrain: epoch  2, batch    11 | loss: 104.6398670CurrentTrain: epoch  2, batch    12 | loss: 112.3651651CurrentTrain: epoch  2, batch    13 | loss: 135.2376895CurrentTrain: epoch  2, batch    14 | loss: 72.7987995CurrentTrain: epoch  2, batch    15 | loss: 133.9515725CurrentTrain: epoch  2, batch    16 | loss: 276.6934287CurrentTrain: epoch  2, batch    17 | loss: 84.6317022CurrentTrain: epoch  2, batch    18 | loss: 99.4135435CurrentTrain: epoch  2, batch    19 | loss: 79.7402764CurrentTrain: epoch  2, batch    20 | loss: 88.7991021CurrentTrain: epoch  2, batch    21 | loss: 103.0822401CurrentTrain: epoch  2, batch    22 | loss: 130.5333586CurrentTrain: epoch  2, batch    23 | loss: 72.6300467CurrentTrain: epoch  2, batch    24 | loss: 131.8706612CurrentTrain: epoch  2, batch    25 | loss: 103.3250420CurrentTrain: epoch  2, batch    26 | loss: 101.2112902CurrentTrain: epoch  2, batch    27 | loss: 131.4907455CurrentTrain: epoch  2, batch    28 | loss: 99.1377403CurrentTrain: epoch  2, batch    29 | loss: 559.3459640CurrentTrain: epoch  2, batch    30 | loss: 79.8215876CurrentTrain: epoch  2, batch    31 | loss: 101.7006295CurrentTrain: epoch  2, batch    32 | loss: 83.5545804CurrentTrain: epoch  2, batch    33 | loss: 134.7248473CurrentTrain: epoch  2, batch    34 | loss: 266.8654962CurrentTrain: epoch  2, batch    35 | loss: 132.0312159CurrentTrain: epoch  2, batch    36 | loss: 125.7453214CurrentTrain: epoch  2, batch    37 | loss: 100.4641546CurrentTrain: epoch  2, batch    38 | loss: 130.1040535CurrentTrain: epoch  2, batch    39 | loss: 97.2885738CurrentTrain: epoch  2, batch    40 | loss: 84.1291847CurrentTrain: epoch  2, batch    41 | loss: 103.0238253CurrentTrain: epoch  2, batch    42 | loss: 133.8264150CurrentTrain: epoch  2, batch    43 | loss: 84.2061408CurrentTrain: epoch  2, batch    44 | loss: 103.6505081CurrentTrain: epoch  2, batch    45 | loss: 174.3319704CurrentTrain: epoch  2, batch    46 | loss: 84.6628395CurrentTrain: epoch  2, batch    47 | loss: 101.4414592CurrentTrain: epoch  2, batch    48 | loss: 102.5721240CurrentTrain: epoch  2, batch    49 | loss: 83.4508919CurrentTrain: epoch  2, batch    50 | loss: 179.2406679CurrentTrain: epoch  2, batch    51 | loss: 87.8998846CurrentTrain: epoch  2, batch    52 | loss: 107.2790284CurrentTrain: epoch  2, batch    53 | loss: 100.9492135CurrentTrain: epoch  2, batch    54 | loss: 82.9132127CurrentTrain: epoch  2, batch    55 | loss: 85.8639231CurrentTrain: epoch  2, batch    56 | loss: 87.3450729CurrentTrain: epoch  2, batch    57 | loss: 129.6944619CurrentTrain: epoch  2, batch    58 | loss: 182.6608522CurrentTrain: epoch  2, batch    59 | loss: 179.8801959CurrentTrain: epoch  2, batch    60 | loss: 101.0847684CurrentTrain: epoch  2, batch    61 | loss: 81.7867612CurrentTrain: epoch  2, batch    62 | loss: 100.0494222CurrentTrain: epoch  2, batch    63 | loss: 129.3823707CurrentTrain: epoch  2, batch    64 | loss: 83.6816185CurrentTrain: epoch  2, batch    65 | loss: 83.1872756CurrentTrain: epoch  2, batch    66 | loss: 79.1761094CurrentTrain: epoch  2, batch    67 | loss: 83.1270632CurrentTrain: epoch  2, batch    68 | loss: 178.0444920CurrentTrain: epoch  2, batch    69 | loss: 132.0588246CurrentTrain: epoch  2, batch    70 | loss: 277.9706401CurrentTrain: epoch  2, batch    71 | loss: 85.0030930CurrentTrain: epoch  2, batch    72 | loss: 102.2961773CurrentTrain: epoch  2, batch    73 | loss: 103.3464968CurrentTrain: epoch  2, batch    74 | loss: 100.2679444CurrentTrain: epoch  2, batch    75 | loss: 79.4860045CurrentTrain: epoch  2, batch    76 | loss: 101.0751610CurrentTrain: epoch  2, batch    77 | loss: 99.0566780CurrentTrain: epoch  2, batch    78 | loss: 97.0297962CurrentTrain: epoch  2, batch    79 | loss: 133.0571050CurrentTrain: epoch  2, batch    80 | loss: 100.9753888CurrentTrain: epoch  2, batch    81 | loss: 99.6772801CurrentTrain: epoch  2, batch    82 | loss: 103.0336102CurrentTrain: epoch  2, batch    83 | loss: 83.8319474CurrentTrain: epoch  2, batch    84 | loss: 87.7441552CurrentTrain: epoch  2, batch    85 | loss: 83.9262016CurrentTrain: epoch  2, batch    86 | loss: 85.0158389CurrentTrain: epoch  2, batch    87 | loss: 125.7987691CurrentTrain: epoch  2, batch    88 | loss: 104.7333813CurrentTrain: epoch  2, batch    89 | loss: 175.9964127CurrentTrain: epoch  2, batch    90 | loss: 100.1838930CurrentTrain: epoch  2, batch    91 | loss: 99.6994219CurrentTrain: epoch  2, batch    92 | loss: 101.3743081CurrentTrain: epoch  2, batch    93 | loss: 181.4846760CurrentTrain: epoch  2, batch    94 | loss: 136.5775550CurrentTrain: epoch  2, batch    95 | loss: 100.6050533CurrentTrain: epoch  3, batch     0 | loss: 100.4055383CurrentTrain: epoch  3, batch     1 | loss: 273.4640740CurrentTrain: epoch  3, batch     2 | loss: 68.0219372CurrentTrain: epoch  3, batch     3 | loss: 67.1508813CurrentTrain: epoch  3, batch     4 | loss: 68.2535035CurrentTrain: epoch  3, batch     5 | loss: 102.9411476CurrentTrain: epoch  3, batch     6 | loss: 559.1694750CurrentTrain: epoch  3, batch     7 | loss: 107.7725663CurrentTrain: epoch  3, batch     8 | loss: 97.7225376CurrentTrain: epoch  3, batch     9 | loss: 101.3389320CurrentTrain: epoch  3, batch    10 | loss: 127.2979603CurrentTrain: epoch  3, batch    11 | loss: 79.1886595CurrentTrain: epoch  3, batch    12 | loss: 102.0118869CurrentTrain: epoch  3, batch    13 | loss: 103.4177663CurrentTrain: epoch  3, batch    14 | loss: 82.9380348CurrentTrain: epoch  3, batch    15 | loss: 106.5921454CurrentTrain: epoch  3, batch    16 | loss: 84.7159881CurrentTrain: epoch  3, batch    17 | loss: 103.0003544CurrentTrain: epoch  3, batch    18 | loss: 127.4041897CurrentTrain: epoch  3, batch    19 | loss: 98.5229862CurrentTrain: epoch  3, batch    20 | loss: 81.5162644CurrentTrain: epoch  3, batch    21 | loss: 103.9962125CurrentTrain: epoch  3, batch    22 | loss: 85.6765239CurrentTrain: epoch  3, batch    23 | loss: 131.7747353CurrentTrain: epoch  3, batch    24 | loss: 107.2616327CurrentTrain: epoch  3, batch    25 | loss: 98.0894191CurrentTrain: epoch  3, batch    26 | loss: 123.7305073CurrentTrain: epoch  3, batch    27 | loss: 87.2805349CurrentTrain: epoch  3, batch    28 | loss: 105.0194749CurrentTrain: epoch  3, batch    29 | loss: 80.9755436CurrentTrain: epoch  3, batch    30 | loss: 101.1662685CurrentTrain: epoch  3, batch    31 | loss: 69.1645869CurrentTrain: epoch  3, batch    32 | loss: 67.6980404CurrentTrain: epoch  3, batch    33 | loss: 103.5913429CurrentTrain: epoch  3, batch    34 | loss: 105.7275842CurrentTrain: epoch  3, batch    35 | loss: 83.9542767CurrentTrain: epoch  3, batch    36 | loss: 98.2500072CurrentTrain: epoch  3, batch    37 | loss: 79.1986815CurrentTrain: epoch  3, batch    38 | loss: 94.4863498CurrentTrain: epoch  3, batch    39 | loss: 80.1654598CurrentTrain: epoch  3, batch    40 | loss: 88.0513424CurrentTrain: epoch  3, batch    41 | loss: 69.0753457CurrentTrain: epoch  3, batch    42 | loss: 129.2889788CurrentTrain: epoch  3, batch    43 | loss: 98.1650198CurrentTrain: epoch  3, batch    44 | loss: 77.4875236CurrentTrain: epoch  3, batch    45 | loss: 126.7567309CurrentTrain: epoch  3, batch    46 | loss: 83.0285856CurrentTrain: epoch  3, batch    47 | loss: 166.9438543CurrentTrain: epoch  3, batch    48 | loss: 99.2701927CurrentTrain: epoch  3, batch    49 | loss: 105.2502956CurrentTrain: epoch  3, batch    50 | loss: 128.1583085CurrentTrain: epoch  3, batch    51 | loss: 98.6377726CurrentTrain: epoch  3, batch    52 | loss: 83.7001783CurrentTrain: epoch  3, batch    53 | loss: 175.5077632CurrentTrain: epoch  3, batch    54 | loss: 81.8098496CurrentTrain: epoch  3, batch    55 | loss: 70.6829861CurrentTrain: epoch  3, batch    56 | loss: 99.5303025CurrentTrain: epoch  3, batch    57 | loss: 101.2456989CurrentTrain: epoch  3, batch    58 | loss: 79.4484219CurrentTrain: epoch  3, batch    59 | loss: 181.0741428CurrentTrain: epoch  3, batch    60 | loss: 96.1524155CurrentTrain: epoch  3, batch    61 | loss: 96.4941419CurrentTrain: epoch  3, batch    62 | loss: 131.0116151CurrentTrain: epoch  3, batch    63 | loss: 97.9497960CurrentTrain: epoch  3, batch    64 | loss: 120.4291934CurrentTrain: epoch  3, batch    65 | loss: 106.1041152CurrentTrain: epoch  3, batch    66 | loss: 83.7884614CurrentTrain: epoch  3, batch    67 | loss: 100.5990662CurrentTrain: epoch  3, batch    68 | loss: 98.5985797CurrentTrain: epoch  3, batch    69 | loss: 274.1798147CurrentTrain: epoch  3, batch    70 | loss: 127.0094220CurrentTrain: epoch  3, batch    71 | loss: 97.8273845CurrentTrain: epoch  3, batch    72 | loss: 126.1453337CurrentTrain: epoch  3, batch    73 | loss: 100.0790630CurrentTrain: epoch  3, batch    74 | loss: 125.2714288CurrentTrain: epoch  3, batch    75 | loss: 174.3796865CurrentTrain: epoch  3, batch    76 | loss: 83.6043321CurrentTrain: epoch  3, batch    77 | loss: 127.2158192CurrentTrain: epoch  3, batch    78 | loss: 81.4045584CurrentTrain: epoch  3, batch    79 | loss: 102.7392945CurrentTrain: epoch  3, batch    80 | loss: 133.3087138CurrentTrain: epoch  3, batch    81 | loss: 179.6791298CurrentTrain: epoch  3, batch    82 | loss: 79.1142361CurrentTrain: epoch  3, batch    83 | loss: 106.9179220CurrentTrain: epoch  3, batch    84 | loss: 96.4723407CurrentTrain: epoch  3, batch    85 | loss: 84.6507562CurrentTrain: epoch  3, batch    86 | loss: 102.7878400CurrentTrain: epoch  3, batch    87 | loss: 172.9241587CurrentTrain: epoch  3, batch    88 | loss: 173.2983809CurrentTrain: epoch  3, batch    89 | loss: 101.1856573CurrentTrain: epoch  3, batch    90 | loss: 79.0051054CurrentTrain: epoch  3, batch    91 | loss: 101.1926700CurrentTrain: epoch  3, batch    92 | loss: 185.2994636CurrentTrain: epoch  3, batch    93 | loss: 123.1583156CurrentTrain: epoch  3, batch    94 | loss: 130.3405145CurrentTrain: epoch  3, batch    95 | loss: 84.5642814CurrentTrain: epoch  4, batch     0 | loss: 101.4040796CurrentTrain: epoch  4, batch     1 | loss: 79.8727679CurrentTrain: epoch  4, batch     2 | loss: 94.5202254CurrentTrain: epoch  4, batch     3 | loss: 101.4844755CurrentTrain: epoch  4, batch     4 | loss: 99.8945510CurrentTrain: epoch  4, batch     5 | loss: 77.5320295CurrentTrain: epoch  4, batch     6 | loss: 126.0778104CurrentTrain: epoch  4, batch     7 | loss: 101.4258908CurrentTrain: epoch  4, batch     8 | loss: 177.1355608CurrentTrain: epoch  4, batch     9 | loss: 128.8020794CurrentTrain: epoch  4, batch    10 | loss: 79.0857234CurrentTrain: epoch  4, batch    11 | loss: 173.5194247CurrentTrain: epoch  4, batch    12 | loss: 122.7263573CurrentTrain: epoch  4, batch    13 | loss: 105.2357017CurrentTrain: epoch  4, batch    14 | loss: 102.2416954CurrentTrain: epoch  4, batch    15 | loss: 176.3819616CurrentTrain: epoch  4, batch    16 | loss: 97.3359083CurrentTrain: epoch  4, batch    17 | loss: 124.2313022CurrentTrain: epoch  4, batch    18 | loss: 100.6598977CurrentTrain: epoch  4, batch    19 | loss: 101.9049650CurrentTrain: epoch  4, batch    20 | loss: 98.6335786CurrentTrain: epoch  4, batch    21 | loss: 78.0612521CurrentTrain: epoch  4, batch    22 | loss: 82.6143601CurrentTrain: epoch  4, batch    23 | loss: 102.4297805CurrentTrain: epoch  4, batch    24 | loss: 97.2078353CurrentTrain: epoch  4, batch    25 | loss: 135.6866671CurrentTrain: epoch  4, batch    26 | loss: 103.4361957CurrentTrain: epoch  4, batch    27 | loss: 66.0233097CurrentTrain: epoch  4, batch    28 | loss: 76.8811436CurrentTrain: epoch  4, batch    29 | loss: 101.3458769CurrentTrain: epoch  4, batch    30 | loss: 124.7029150CurrentTrain: epoch  4, batch    31 | loss: 125.2331520CurrentTrain: epoch  4, batch    32 | loss: 76.8934309CurrentTrain: epoch  4, batch    33 | loss: 103.8208534CurrentTrain: epoch  4, batch    34 | loss: 82.9735743CurrentTrain: epoch  4, batch    35 | loss: 102.7112406CurrentTrain: epoch  4, batch    36 | loss: 104.0506186CurrentTrain: epoch  4, batch    37 | loss: 100.9864207CurrentTrain: epoch  4, batch    38 | loss: 95.8499430CurrentTrain: epoch  4, batch    39 | loss: 174.0437660CurrentTrain: epoch  4, batch    40 | loss: 101.4259434CurrentTrain: epoch  4, batch    41 | loss: 94.4658681CurrentTrain: epoch  4, batch    42 | loss: 125.4008139CurrentTrain: epoch  4, batch    43 | loss: 98.1871229CurrentTrain: epoch  4, batch    44 | loss: 133.0148764CurrentTrain: epoch  4, batch    45 | loss: 99.4821011CurrentTrain: epoch  4, batch    46 | loss: 128.6514502CurrentTrain: epoch  4, batch    47 | loss: 79.0643136CurrentTrain: epoch  4, batch    48 | loss: 95.6407962CurrentTrain: epoch  4, batch    49 | loss: 131.8815448CurrentTrain: epoch  4, batch    50 | loss: 98.8276746CurrentTrain: epoch  4, batch    51 | loss: 80.9473714CurrentTrain: epoch  4, batch    52 | loss: 129.8053354CurrentTrain: epoch  4, batch    53 | loss: 177.3351997CurrentTrain: epoch  4, batch    54 | loss: 92.8547713CurrentTrain: epoch  4, batch    55 | loss: 98.1013911CurrentTrain: epoch  4, batch    56 | loss: 127.4152514CurrentTrain: epoch  4, batch    57 | loss: 125.8420512CurrentTrain: epoch  4, batch    58 | loss: 80.9594515CurrentTrain: epoch  4, batch    59 | loss: 126.7995492CurrentTrain: epoch  4, batch    60 | loss: 166.2784069CurrentTrain: epoch  4, batch    61 | loss: 70.7010470CurrentTrain: epoch  4, batch    62 | loss: 69.9917201CurrentTrain: epoch  4, batch    63 | loss: 85.6253600CurrentTrain: epoch  4, batch    64 | loss: 130.0255050CurrentTrain: epoch  4, batch    65 | loss: 130.8506417CurrentTrain: epoch  4, batch    66 | loss: 70.9360965CurrentTrain: epoch  4, batch    67 | loss: 101.1077150CurrentTrain: epoch  4, batch    68 | loss: 81.0341678CurrentTrain: epoch  4, batch    69 | loss: 176.7976324CurrentTrain: epoch  4, batch    70 | loss: 74.3039712CurrentTrain: epoch  4, batch    71 | loss: 84.9026492CurrentTrain: epoch  4, batch    72 | loss: 93.2828761CurrentTrain: epoch  4, batch    73 | loss: 86.5959117CurrentTrain: epoch  4, batch    74 | loss: 100.0008331CurrentTrain: epoch  4, batch    75 | loss: 99.0520559CurrentTrain: epoch  4, batch    76 | loss: 125.7444212CurrentTrain: epoch  4, batch    77 | loss: 95.9141118CurrentTrain: epoch  4, batch    78 | loss: 80.4081547CurrentTrain: epoch  4, batch    79 | loss: 177.9854481CurrentTrain: epoch  4, batch    80 | loss: 176.9193188CurrentTrain: epoch  4, batch    81 | loss: 173.8951216CurrentTrain: epoch  4, batch    82 | loss: 97.5971588CurrentTrain: epoch  4, batch    83 | loss: 263.8280050CurrentTrain: epoch  4, batch    84 | loss: 104.7353072CurrentTrain: epoch  4, batch    85 | loss: 80.2681629CurrentTrain: epoch  4, batch    86 | loss: 100.1526185CurrentTrain: epoch  4, batch    87 | loss: 99.5840339CurrentTrain: epoch  4, batch    88 | loss: 176.5086853CurrentTrain: epoch  4, batch    89 | loss: 77.8104320CurrentTrain: epoch  4, batch    90 | loss: 129.6689365CurrentTrain: epoch  4, batch    91 | loss: 99.7995063CurrentTrain: epoch  4, batch    92 | loss: 67.0300471CurrentTrain: epoch  4, batch    93 | loss: 99.2815365CurrentTrain: epoch  4, batch    94 | loss: 125.6314470CurrentTrain: epoch  4, batch    95 | loss: 110.1079302CurrentTrain: epoch  5, batch     0 | loss: 127.1522689CurrentTrain: epoch  5, batch     1 | loss: 103.2863368CurrentTrain: epoch  5, batch     2 | loss: 78.3054082CurrentTrain: epoch  5, batch     3 | loss: 80.8675600CurrentTrain: epoch  5, batch     4 | loss: 82.1773338CurrentTrain: epoch  5, batch     5 | loss: 97.6078561CurrentTrain: epoch  5, batch     6 | loss: 130.9467885CurrentTrain: epoch  5, batch     7 | loss: 262.9657179CurrentTrain: epoch  5, batch     8 | loss: 97.8303997CurrentTrain: epoch  5, batch     9 | loss: 101.1167048CurrentTrain: epoch  5, batch    10 | loss: 128.5932791CurrentTrain: epoch  5, batch    11 | loss: 122.3378336CurrentTrain: epoch  5, batch    12 | loss: 272.9407181CurrentTrain: epoch  5, batch    13 | loss: 127.8767708CurrentTrain: epoch  5, batch    14 | loss: 127.1031244CurrentTrain: epoch  5, batch    15 | loss: 68.6869937CurrentTrain: epoch  5, batch    16 | loss: 125.2545178CurrentTrain: epoch  5, batch    17 | loss: 80.1766362CurrentTrain: epoch  5, batch    18 | loss: 77.0715692CurrentTrain: epoch  5, batch    19 | loss: 126.9199762CurrentTrain: epoch  5, batch    20 | loss: 79.4024323CurrentTrain: epoch  5, batch    21 | loss: 121.6436790CurrentTrain: epoch  5, batch    22 | loss: 78.5705625CurrentTrain: epoch  5, batch    23 | loss: 98.2283483CurrentTrain: epoch  5, batch    24 | loss: 79.1514237CurrentTrain: epoch  5, batch    25 | loss: 82.3726453CurrentTrain: epoch  5, batch    26 | loss: 98.3106181CurrentTrain: epoch  5, batch    27 | loss: 94.2108217CurrentTrain: epoch  5, batch    28 | loss: 93.0489424CurrentTrain: epoch  5, batch    29 | loss: 128.7910508CurrentTrain: epoch  5, batch    30 | loss: 98.8459478CurrentTrain: epoch  5, batch    31 | loss: 126.6326608CurrentTrain: epoch  5, batch    32 | loss: 99.3373874CurrentTrain: epoch  5, batch    33 | loss: 179.0410772CurrentTrain: epoch  5, batch    34 | loss: 122.8569612CurrentTrain: epoch  5, batch    35 | loss: 179.0435384CurrentTrain: epoch  5, batch    36 | loss: 97.9338802CurrentTrain: epoch  5, batch    37 | loss: 122.9965181CurrentTrain: epoch  5, batch    38 | loss: 78.9158002CurrentTrain: epoch  5, batch    39 | loss: 123.4645130CurrentTrain: epoch  5, batch    40 | loss: 82.7792734CurrentTrain: epoch  5, batch    41 | loss: 99.8758908CurrentTrain: epoch  5, batch    42 | loss: 99.0709880CurrentTrain: epoch  5, batch    43 | loss: 273.0236615CurrentTrain: epoch  5, batch    44 | loss: 66.2314815CurrentTrain: epoch  5, batch    45 | loss: 78.3204643CurrentTrain: epoch  5, batch    46 | loss: 121.3345994CurrentTrain: epoch  5, batch    47 | loss: 97.9928044CurrentTrain: epoch  5, batch    48 | loss: 129.4522496CurrentTrain: epoch  5, batch    49 | loss: 77.2872186CurrentTrain: epoch  5, batch    50 | loss: 92.5585262CurrentTrain: epoch  5, batch    51 | loss: 98.4503497CurrentTrain: epoch  5, batch    52 | loss: 81.1629398CurrentTrain: epoch  5, batch    53 | loss: 98.7437084CurrentTrain: epoch  5, batch    54 | loss: 61.5931671CurrentTrain: epoch  5, batch    55 | loss: 94.9588355CurrentTrain: epoch  5, batch    56 | loss: 93.5835948CurrentTrain: epoch  5, batch    57 | loss: 97.2261626CurrentTrain: epoch  5, batch    58 | loss: 71.3963955CurrentTrain: epoch  5, batch    59 | loss: 97.6516129CurrentTrain: epoch  5, batch    60 | loss: 82.4812258CurrentTrain: epoch  5, batch    61 | loss: 83.2044441CurrentTrain: epoch  5, batch    62 | loss: 81.8282541CurrentTrain: epoch  5, batch    63 | loss: 125.5814950CurrentTrain: epoch  5, batch    64 | loss: 174.7476673CurrentTrain: epoch  5, batch    65 | loss: 102.3126404CurrentTrain: epoch  5, batch    66 | loss: 66.0093705CurrentTrain: epoch  5, batch    67 | loss: 130.0927277CurrentTrain: epoch  5, batch    68 | loss: 127.0517117CurrentTrain: epoch  5, batch    69 | loss: 129.2069371CurrentTrain: epoch  5, batch    70 | loss: 181.2806654CurrentTrain: epoch  5, batch    71 | loss: 81.5047870CurrentTrain: epoch  5, batch    72 | loss: 121.8232002CurrentTrain: epoch  5, batch    73 | loss: 127.4172892CurrentTrain: epoch  5, batch    74 | loss: 96.8495018CurrentTrain: epoch  5, batch    75 | loss: 75.4953295CurrentTrain: epoch  5, batch    76 | loss: 97.8006222CurrentTrain: epoch  5, batch    77 | loss: 168.4652305CurrentTrain: epoch  5, batch    78 | loss: 119.6799255CurrentTrain: epoch  5, batch    79 | loss: 93.0486266CurrentTrain: epoch  5, batch    80 | loss: 77.3014292CurrentTrain: epoch  5, batch    81 | loss: 272.9635440CurrentTrain: epoch  5, batch    82 | loss: 97.2433848CurrentTrain: epoch  5, batch    83 | loss: 81.0323619CurrentTrain: epoch  5, batch    84 | loss: 78.3606740CurrentTrain: epoch  5, batch    85 | loss: 96.5797591CurrentTrain: epoch  5, batch    86 | loss: 92.4836796CurrentTrain: epoch  5, batch    87 | loss: 129.3346334CurrentTrain: epoch  5, batch    88 | loss: 128.9462653CurrentTrain: epoch  5, batch    89 | loss: 128.2960818CurrentTrain: epoch  5, batch    90 | loss: 81.8924547CurrentTrain: epoch  5, batch    91 | loss: 96.5903754CurrentTrain: epoch  5, batch    92 | loss: 101.5536828CurrentTrain: epoch  5, batch    93 | loss: 99.3070807CurrentTrain: epoch  5, batch    94 | loss: 81.3531814CurrentTrain: epoch  5, batch    95 | loss: 108.6301884CurrentTrain: epoch  6, batch     0 | loss: 102.1773304CurrentTrain: epoch  6, batch     1 | loss: 79.9016245CurrentTrain: epoch  6, batch     2 | loss: 171.1010302CurrentTrain: epoch  6, batch     3 | loss: 96.4477832CurrentTrain: epoch  6, batch     4 | loss: 68.6393111CurrentTrain: epoch  6, batch     5 | loss: 102.7526886CurrentTrain: epoch  6, batch     6 | loss: 75.2404263CurrentTrain: epoch  6, batch     7 | loss: 81.6726246CurrentTrain: epoch  6, batch     8 | loss: 81.2154084CurrentTrain: epoch  6, batch     9 | loss: 78.1390034CurrentTrain: epoch  6, batch    10 | loss: 125.4070158CurrentTrain: epoch  6, batch    11 | loss: 100.6966359CurrentTrain: epoch  6, batch    12 | loss: 102.4388750CurrentTrain: epoch  6, batch    13 | loss: 129.2262457CurrentTrain: epoch  6, batch    14 | loss: 83.7327602CurrentTrain: epoch  6, batch    15 | loss: 126.1236807CurrentTrain: epoch  6, batch    16 | loss: 100.7434438CurrentTrain: epoch  6, batch    17 | loss: 101.9572860CurrentTrain: epoch  6, batch    18 | loss: 119.5650254CurrentTrain: epoch  6, batch    19 | loss: 126.2494797CurrentTrain: epoch  6, batch    20 | loss: 99.0071450CurrentTrain: epoch  6, batch    21 | loss: 126.6527015CurrentTrain: epoch  6, batch    22 | loss: 166.2556982CurrentTrain: epoch  6, batch    23 | loss: 132.4370344CurrentTrain: epoch  6, batch    24 | loss: 123.6992356CurrentTrain: epoch  6, batch    25 | loss: 169.6142120CurrentTrain: epoch  6, batch    26 | loss: 129.0897962CurrentTrain: epoch  6, batch    27 | loss: 99.5132989CurrentTrain: epoch  6, batch    28 | loss: 104.9348889CurrentTrain: epoch  6, batch    29 | loss: 131.1160707CurrentTrain: epoch  6, batch    30 | loss: 178.2050700CurrentTrain: epoch  6, batch    31 | loss: 128.2884509CurrentTrain: epoch  6, batch    32 | loss: 101.7850450CurrentTrain: epoch  6, batch    33 | loss: 78.2506716CurrentTrain: epoch  6, batch    34 | loss: 77.8098831CurrentTrain: epoch  6, batch    35 | loss: 99.6106819CurrentTrain: epoch  6, batch    36 | loss: 67.2434884CurrentTrain: epoch  6, batch    37 | loss: 93.7367645CurrentTrain: epoch  6, batch    38 | loss: 95.4310115CurrentTrain: epoch  6, batch    39 | loss: 104.6314755CurrentTrain: epoch  6, batch    40 | loss: 132.9585997CurrentTrain: epoch  6, batch    41 | loss: 78.8304234CurrentTrain: epoch  6, batch    42 | loss: 97.2566660CurrentTrain: epoch  6, batch    43 | loss: 114.4990834CurrentTrain: epoch  6, batch    44 | loss: 79.3402954CurrentTrain: epoch  6, batch    45 | loss: 127.9244022CurrentTrain: epoch  6, batch    46 | loss: 97.3679652CurrentTrain: epoch  6, batch    47 | loss: 125.1772110CurrentTrain: epoch  6, batch    48 | loss: 76.5800290CurrentTrain: epoch  6, batch    49 | loss: 94.1769628CurrentTrain: epoch  6, batch    50 | loss: 96.2349566CurrentTrain: epoch  6, batch    51 | loss: 98.5195959CurrentTrain: epoch  6, batch    52 | loss: 126.2994205CurrentTrain: epoch  6, batch    53 | loss: 66.6489494CurrentTrain: epoch  6, batch    54 | loss: 96.9711116CurrentTrain: epoch  6, batch    55 | loss: 98.5570393CurrentTrain: epoch  6, batch    56 | loss: 97.0453219CurrentTrain: epoch  6, batch    57 | loss: 89.8536559CurrentTrain: epoch  6, batch    58 | loss: 74.7628207CurrentTrain: epoch  6, batch    59 | loss: 124.3209127CurrentTrain: epoch  6, batch    60 | loss: 76.4579600CurrentTrain: epoch  6, batch    61 | loss: 79.2639937CurrentTrain: epoch  6, batch    62 | loss: 77.4482125CurrentTrain: epoch  6, batch    63 | loss: 98.6704875CurrentTrain: epoch  6, batch    64 | loss: 75.9714962CurrentTrain: epoch  6, batch    65 | loss: 125.7284208CurrentTrain: epoch  6, batch    66 | loss: 121.5473515CurrentTrain: epoch  6, batch    67 | loss: 126.7244542CurrentTrain: epoch  6, batch    68 | loss: 98.0287478CurrentTrain: epoch  6, batch    69 | loss: 76.2039036CurrentTrain: epoch  6, batch    70 | loss: 101.5635111CurrentTrain: epoch  6, batch    71 | loss: 99.1134772CurrentTrain: epoch  6, batch    72 | loss: 121.6363480CurrentTrain: epoch  6, batch    73 | loss: 92.6270125CurrentTrain: epoch  6, batch    74 | loss: 95.1535524CurrentTrain: epoch  6, batch    75 | loss: 78.1486380CurrentTrain: epoch  6, batch    76 | loss: 132.3134585CurrentTrain: epoch  6, batch    77 | loss: 96.5602357CurrentTrain: epoch  6, batch    78 | loss: 97.7133075CurrentTrain: epoch  6, batch    79 | loss: 104.0566394CurrentTrain: epoch  6, batch    80 | loss: 94.9152049CurrentTrain: epoch  6, batch    81 | loss: 76.9508857CurrentTrain: epoch  6, batch    82 | loss: 66.1585612CurrentTrain: epoch  6, batch    83 | loss: 92.3594935CurrentTrain: epoch  6, batch    84 | loss: 125.3872063CurrentTrain: epoch  6, batch    85 | loss: 100.8357757CurrentTrain: epoch  6, batch    86 | loss: 129.2121931CurrentTrain: epoch  6, batch    87 | loss: 129.3920456CurrentTrain: epoch  6, batch    88 | loss: 126.4391756CurrentTrain: epoch  6, batch    89 | loss: 128.8747741CurrentTrain: epoch  6, batch    90 | loss: 127.2151300CurrentTrain: epoch  6, batch    91 | loss: 102.1619718CurrentTrain: epoch  6, batch    92 | loss: 99.4698655CurrentTrain: epoch  6, batch    93 | loss: 95.4964623CurrentTrain: epoch  6, batch    94 | loss: 176.5361364CurrentTrain: epoch  6, batch    95 | loss: 136.4453475CurrentTrain: epoch  7, batch     0 | loss: 79.2601910CurrentTrain: epoch  7, batch     1 | loss: 82.7197086CurrentTrain: epoch  7, batch     2 | loss: 122.1834970CurrentTrain: epoch  7, batch     3 | loss: 172.9233063CurrentTrain: epoch  7, batch     4 | loss: 88.7531099CurrentTrain: epoch  7, batch     5 | loss: 95.7109995CurrentTrain: epoch  7, batch     6 | loss: 93.7012623CurrentTrain: epoch  7, batch     7 | loss: 102.9132308CurrentTrain: epoch  7, batch     8 | loss: 79.5418204CurrentTrain: epoch  7, batch     9 | loss: 126.7582552CurrentTrain: epoch  7, batch    10 | loss: 80.7144867CurrentTrain: epoch  7, batch    11 | loss: 127.6146261CurrentTrain: epoch  7, batch    12 | loss: 77.7657897CurrentTrain: epoch  7, batch    13 | loss: 99.1254564CurrentTrain: epoch  7, batch    14 | loss: 99.9633665CurrentTrain: epoch  7, batch    15 | loss: 64.8418900CurrentTrain: epoch  7, batch    16 | loss: 127.5615508CurrentTrain: epoch  7, batch    17 | loss: 79.7451272CurrentTrain: epoch  7, batch    18 | loss: 94.3789052CurrentTrain: epoch  7, batch    19 | loss: 77.1355067CurrentTrain: epoch  7, batch    20 | loss: 95.7262345CurrentTrain: epoch  7, batch    21 | loss: 92.9731304CurrentTrain: epoch  7, batch    22 | loss: 70.1171690CurrentTrain: epoch  7, batch    23 | loss: 74.3360470CurrentTrain: epoch  7, batch    24 | loss: 124.4680182CurrentTrain: epoch  7, batch    25 | loss: 81.9819739CurrentTrain: epoch  7, batch    26 | loss: 124.9612972CurrentTrain: epoch  7, batch    27 | loss: 79.1226114CurrentTrain: epoch  7, batch    28 | loss: 126.2211938CurrentTrain: epoch  7, batch    29 | loss: 97.5264210CurrentTrain: epoch  7, batch    30 | loss: 126.6031754CurrentTrain: epoch  7, batch    31 | loss: 124.0166175CurrentTrain: epoch  7, batch    32 | loss: 75.5826397CurrentTrain: epoch  7, batch    33 | loss: 96.9442621CurrentTrain: epoch  7, batch    34 | loss: 124.6683883CurrentTrain: epoch  7, batch    35 | loss: 98.7732695CurrentTrain: epoch  7, batch    36 | loss: 80.2382228CurrentTrain: epoch  7, batch    37 | loss: 125.5613043CurrentTrain: epoch  7, batch    38 | loss: 125.1008757CurrentTrain: epoch  7, batch    39 | loss: 265.7457909CurrentTrain: epoch  7, batch    40 | loss: 100.4814959CurrentTrain: epoch  7, batch    41 | loss: 80.9927910CurrentTrain: epoch  7, batch    42 | loss: 97.5558848CurrentTrain: epoch  7, batch    43 | loss: 128.8641071CurrentTrain: epoch  7, batch    44 | loss: 58.2613666CurrentTrain: epoch  7, batch    45 | loss: 124.0323088CurrentTrain: epoch  7, batch    46 | loss: 97.1220033CurrentTrain: epoch  7, batch    47 | loss: 100.2162538CurrentTrain: epoch  7, batch    48 | loss: 124.5947153CurrentTrain: epoch  7, batch    49 | loss: 66.7830718CurrentTrain: epoch  7, batch    50 | loss: 77.4318991CurrentTrain: epoch  7, batch    51 | loss: 76.5702468CurrentTrain: epoch  7, batch    52 | loss: 101.2197766CurrentTrain: epoch  7, batch    53 | loss: 98.3840420CurrentTrain: epoch  7, batch    54 | loss: 98.0744566CurrentTrain: epoch  7, batch    55 | loss: 102.5230605CurrentTrain: epoch  7, batch    56 | loss: 97.8016539CurrentTrain: epoch  7, batch    57 | loss: 101.0341401CurrentTrain: epoch  7, batch    58 | loss: 120.4630189CurrentTrain: epoch  7, batch    59 | loss: 128.6517122CurrentTrain: epoch  7, batch    60 | loss: 99.8011551CurrentTrain: epoch  7, batch    61 | loss: 74.7812972CurrentTrain: epoch  7, batch    62 | loss: 129.5645342CurrentTrain: epoch  7, batch    63 | loss: 100.1751524CurrentTrain: epoch  7, batch    64 | loss: 97.6091763CurrentTrain: epoch  7, batch    65 | loss: 129.0109874CurrentTrain: epoch  7, batch    66 | loss: 127.9000607CurrentTrain: epoch  7, batch    67 | loss: 124.6654442CurrentTrain: epoch  7, batch    68 | loss: 95.9144511CurrentTrain: epoch  7, batch    69 | loss: 123.5262202CurrentTrain: epoch  7, batch    70 | loss: 77.4527836CurrentTrain: epoch  7, batch    71 | loss: 128.2596371CurrentTrain: epoch  7, batch    72 | loss: 100.9159459CurrentTrain: epoch  7, batch    73 | loss: 78.9194796CurrentTrain: epoch  7, batch    74 | loss: 101.5993508CurrentTrain: epoch  7, batch    75 | loss: 99.6662651CurrentTrain: epoch  7, batch    76 | loss: 124.7257549CurrentTrain: epoch  7, batch    77 | loss: 118.5887521CurrentTrain: epoch  7, batch    78 | loss: 96.9036115CurrentTrain: epoch  7, batch    79 | loss: 80.3816197CurrentTrain: epoch  7, batch    80 | loss: 123.5017940CurrentTrain: epoch  7, batch    81 | loss: 92.0275998CurrentTrain: epoch  7, batch    82 | loss: 76.1358365CurrentTrain: epoch  7, batch    83 | loss: 92.1419068CurrentTrain: epoch  7, batch    84 | loss: 170.5850172CurrentTrain: epoch  7, batch    85 | loss: 176.4350602CurrentTrain: epoch  7, batch    86 | loss: 102.8350488CurrentTrain: epoch  7, batch    87 | loss: 266.5967652CurrentTrain: epoch  7, batch    88 | loss: 100.0728860CurrentTrain: epoch  7, batch    89 | loss: 273.7920118CurrentTrain: epoch  7, batch    90 | loss: 76.4521517CurrentTrain: epoch  7, batch    91 | loss: 115.8774729CurrentTrain: epoch  7, batch    92 | loss: 77.2257585CurrentTrain: epoch  7, batch    93 | loss: 126.3601112CurrentTrain: epoch  7, batch    94 | loss: 94.7332743CurrentTrain: epoch  7, batch    95 | loss: 139.4237870CurrentTrain: epoch  8, batch     0 | loss: 95.2929657CurrentTrain: epoch  8, batch     1 | loss: 96.8892185CurrentTrain: epoch  8, batch     2 | loss: 126.1117259CurrentTrain: epoch  8, batch     3 | loss: 80.7439136CurrentTrain: epoch  8, batch     4 | loss: 122.2247597CurrentTrain: epoch  8, batch     5 | loss: 78.3962838CurrentTrain: epoch  8, batch     6 | loss: 122.2977292CurrentTrain: epoch  8, batch     7 | loss: 61.0362687CurrentTrain: epoch  8, batch     8 | loss: 94.1030051CurrentTrain: epoch  8, batch     9 | loss: 75.5298983CurrentTrain: epoch  8, batch    10 | loss: 126.3841561CurrentTrain: epoch  8, batch    11 | loss: 78.5755861CurrentTrain: epoch  8, batch    12 | loss: 95.4846852CurrentTrain: epoch  8, batch    13 | loss: 122.3069061CurrentTrain: epoch  8, batch    14 | loss: 78.5514558CurrentTrain: epoch  8, batch    15 | loss: 126.5348046CurrentTrain: epoch  8, batch    16 | loss: 62.8594274CurrentTrain: epoch  8, batch    17 | loss: 98.2391524CurrentTrain: epoch  8, batch    18 | loss: 97.4887462CurrentTrain: epoch  8, batch    19 | loss: 126.7752377CurrentTrain: epoch  8, batch    20 | loss: 558.7315144CurrentTrain: epoch  8, batch    21 | loss: 96.5071530CurrentTrain: epoch  8, batch    22 | loss: 94.6836593CurrentTrain: epoch  8, batch    23 | loss: 176.2492654CurrentTrain: epoch  8, batch    24 | loss: 166.6251510CurrentTrain: epoch  8, batch    25 | loss: 82.5051510CurrentTrain: epoch  8, batch    26 | loss: 77.8503100CurrentTrain: epoch  8, batch    27 | loss: 72.1766773CurrentTrain: epoch  8, batch    28 | loss: 79.4448428CurrentTrain: epoch  8, batch    29 | loss: 78.8788003CurrentTrain: epoch  8, batch    30 | loss: 123.0065641CurrentTrain: epoch  8, batch    31 | loss: 97.1800687CurrentTrain: epoch  8, batch    32 | loss: 173.3599973CurrentTrain: epoch  8, batch    33 | loss: 170.5844822CurrentTrain: epoch  8, batch    34 | loss: 126.7260564CurrentTrain: epoch  8, batch    35 | loss: 77.3532948CurrentTrain: epoch  8, batch    36 | loss: 101.8976077CurrentTrain: epoch  8, batch    37 | loss: 96.8526155CurrentTrain: epoch  8, batch    38 | loss: 100.1973075CurrentTrain: epoch  8, batch    39 | loss: 124.0756900CurrentTrain: epoch  8, batch    40 | loss: 98.8878458CurrentTrain: epoch  8, batch    41 | loss: 176.3537258CurrentTrain: epoch  8, batch    42 | loss: 78.2943238CurrentTrain: epoch  8, batch    43 | loss: 126.1648380CurrentTrain: epoch  8, batch    44 | loss: 95.4327323CurrentTrain: epoch  8, batch    45 | loss: 93.6353328CurrentTrain: epoch  8, batch    46 | loss: 78.9441504CurrentTrain: epoch  8, batch    47 | loss: 94.6243616CurrentTrain: epoch  8, batch    48 | loss: 126.3569873CurrentTrain: epoch  8, batch    49 | loss: 97.1983082CurrentTrain: epoch  8, batch    50 | loss: 126.1760118CurrentTrain: epoch  8, batch    51 | loss: 65.2965717CurrentTrain: epoch  8, batch    52 | loss: 94.9198750CurrentTrain: epoch  8, batch    53 | loss: 78.6984124CurrentTrain: epoch  8, batch    54 | loss: 82.7954488CurrentTrain: epoch  8, batch    55 | loss: 82.5667096CurrentTrain: epoch  8, batch    56 | loss: 79.2517945CurrentTrain: epoch  8, batch    57 | loss: 75.3920875CurrentTrain: epoch  8, batch    58 | loss: 127.4204156CurrentTrain: epoch  8, batch    59 | loss: 78.6760133CurrentTrain: epoch  8, batch    60 | loss: 95.7207854CurrentTrain: epoch  8, batch    61 | loss: 99.7726431CurrentTrain: epoch  8, batch    62 | loss: 127.6772412CurrentTrain: epoch  8, batch    63 | loss: 76.6285976CurrentTrain: epoch  8, batch    64 | loss: 92.1568765CurrentTrain: epoch  8, batch    65 | loss: 101.4026801CurrentTrain: epoch  8, batch    66 | loss: 98.3347740CurrentTrain: epoch  8, batch    67 | loss: 68.7072784CurrentTrain: epoch  8, batch    68 | loss: 124.1274686CurrentTrain: epoch  8, batch    69 | loss: 79.7942372CurrentTrain: epoch  8, batch    70 | loss: 124.0982543CurrentTrain: epoch  8, batch    71 | loss: 97.1804531CurrentTrain: epoch  8, batch    72 | loss: 81.3919791CurrentTrain: epoch  8, batch    73 | loss: 100.4124275CurrentTrain: epoch  8, batch    74 | loss: 94.7241877CurrentTrain: epoch  8, batch    75 | loss: 96.9445301CurrentTrain: epoch  8, batch    76 | loss: 96.2454923CurrentTrain: epoch  8, batch    77 | loss: 117.8441963CurrentTrain: epoch  8, batch    78 | loss: 81.6666250CurrentTrain: epoch  8, batch    79 | loss: 101.7218418CurrentTrain: epoch  8, batch    80 | loss: 128.8411799CurrentTrain: epoch  8, batch    81 | loss: 77.8149675CurrentTrain: epoch  8, batch    82 | loss: 78.4368459CurrentTrain: epoch  8, batch    83 | loss: 126.4637494CurrentTrain: epoch  8, batch    84 | loss: 124.7540343CurrentTrain: epoch  8, batch    85 | loss: 98.5813783CurrentTrain: epoch  8, batch    86 | loss: 126.2052484CurrentTrain: epoch  8, batch    87 | loss: 121.2489798CurrentTrain: epoch  8, batch    88 | loss: 96.9071164CurrentTrain: epoch  8, batch    89 | loss: 93.7902248CurrentTrain: epoch  8, batch    90 | loss: 78.3447390CurrentTrain: epoch  8, batch    91 | loss: 98.7917956CurrentTrain: epoch  8, batch    92 | loss: 99.0866013CurrentTrain: epoch  8, batch    93 | loss: 120.1412873CurrentTrain: epoch  8, batch    94 | loss: 176.7463114CurrentTrain: epoch  8, batch    95 | loss: 78.3038046CurrentTrain: epoch  9, batch     0 | loss: 66.1623096CurrentTrain: epoch  9, batch     1 | loss: 101.2317869CurrentTrain: epoch  9, batch     2 | loss: 80.9505107CurrentTrain: epoch  9, batch     3 | loss: 100.3793750CurrentTrain: epoch  9, batch     4 | loss: 118.1646640CurrentTrain: epoch  9, batch     5 | loss: 176.2294975CurrentTrain: epoch  9, batch     6 | loss: 93.0484153CurrentTrain: epoch  9, batch     7 | loss: 99.6210013CurrentTrain: epoch  9, batch     8 | loss: 122.6792565CurrentTrain: epoch  9, batch     9 | loss: 126.4279765CurrentTrain: epoch  9, batch    10 | loss: 92.4219088CurrentTrain: epoch  9, batch    11 | loss: 126.3986640CurrentTrain: epoch  9, batch    12 | loss: 82.4722609CurrentTrain: epoch  9, batch    13 | loss: 96.8354098CurrentTrain: epoch  9, batch    14 | loss: 79.9781474CurrentTrain: epoch  9, batch    15 | loss: 122.2307592CurrentTrain: epoch  9, batch    16 | loss: 120.4437224CurrentTrain: epoch  9, batch    17 | loss: 180.0618947CurrentTrain: epoch  9, batch    18 | loss: 120.2555676CurrentTrain: epoch  9, batch    19 | loss: 75.0295547CurrentTrain: epoch  9, batch    20 | loss: 175.6244268CurrentTrain: epoch  9, batch    21 | loss: 96.7586523CurrentTrain: epoch  9, batch    22 | loss: 80.5894476CurrentTrain: epoch  9, batch    23 | loss: 66.9275655CurrentTrain: epoch  9, batch    24 | loss: 76.2392142CurrentTrain: epoch  9, batch    25 | loss: 99.2531830CurrentTrain: epoch  9, batch    26 | loss: 97.4362639CurrentTrain: epoch  9, batch    27 | loss: 72.8222172CurrentTrain: epoch  9, batch    28 | loss: 62.1418022CurrentTrain: epoch  9, batch    29 | loss: 128.9798537CurrentTrain: epoch  9, batch    30 | loss: 100.3964890CurrentTrain: epoch  9, batch    31 | loss: 102.0577723CurrentTrain: epoch  9, batch    32 | loss: 94.3892155CurrentTrain: epoch  9, batch    33 | loss: 97.5628039CurrentTrain: epoch  9, batch    34 | loss: 77.8687807CurrentTrain: epoch  9, batch    35 | loss: 78.6965143CurrentTrain: epoch  9, batch    36 | loss: 63.6614982CurrentTrain: epoch  9, batch    37 | loss: 126.2935238CurrentTrain: epoch  9, batch    38 | loss: 98.4267020CurrentTrain: epoch  9, batch    39 | loss: 80.8876339CurrentTrain: epoch  9, batch    40 | loss: 99.6145159CurrentTrain: epoch  9, batch    41 | loss: 94.3510372CurrentTrain: epoch  9, batch    42 | loss: 126.2793445CurrentTrain: epoch  9, batch    43 | loss: 167.2627206CurrentTrain: epoch  9, batch    44 | loss: 98.1634305CurrentTrain: epoch  9, batch    45 | loss: 81.1423296CurrentTrain: epoch  9, batch    46 | loss: 62.9576829CurrentTrain: epoch  9, batch    47 | loss: 94.2050723CurrentTrain: epoch  9, batch    48 | loss: 99.1964894CurrentTrain: epoch  9, batch    49 | loss: 96.6522589CurrentTrain: epoch  9, batch    50 | loss: 92.2742100CurrentTrain: epoch  9, batch    51 | loss: 98.6129081CurrentTrain: epoch  9, batch    52 | loss: 62.4272792CurrentTrain: epoch  9, batch    53 | loss: 78.6891077CurrentTrain: epoch  9, batch    54 | loss: 126.2844582CurrentTrain: epoch  9, batch    55 | loss: 80.6226037CurrentTrain: epoch  9, batch    56 | loss: 125.4004834CurrentTrain: epoch  9, batch    57 | loss: 164.2297179CurrentTrain: epoch  9, batch    58 | loss: 97.5573248CurrentTrain: epoch  9, batch    59 | loss: 79.7169623CurrentTrain: epoch  9, batch    60 | loss: 100.7949500CurrentTrain: epoch  9, batch    61 | loss: 165.4430103CurrentTrain: epoch  9, batch    62 | loss: 123.1809157CurrentTrain: epoch  9, batch    63 | loss: 95.2361296CurrentTrain: epoch  9, batch    64 | loss: 80.1226918CurrentTrain: epoch  9, batch    65 | loss: 79.1428997CurrentTrain: epoch  9, batch    66 | loss: 84.0445608CurrentTrain: epoch  9, batch    67 | loss: 102.8277283CurrentTrain: epoch  9, batch    68 | loss: 120.0858841CurrentTrain: epoch  9, batch    69 | loss: 173.1047174CurrentTrain: epoch  9, batch    70 | loss: 77.5794153CurrentTrain: epoch  9, batch    71 | loss: 75.5536345CurrentTrain: epoch  9, batch    72 | loss: 94.4231859CurrentTrain: epoch  9, batch    73 | loss: 80.1082258CurrentTrain: epoch  9, batch    74 | loss: 128.5646958CurrentTrain: epoch  9, batch    75 | loss: 67.0712033CurrentTrain: epoch  9, batch    76 | loss: 94.3664384CurrentTrain: epoch  9, batch    77 | loss: 176.3071985CurrentTrain: epoch  9, batch    78 | loss: 79.5300287CurrentTrain: epoch  9, batch    79 | loss: 172.8986521CurrentTrain: epoch  9, batch    80 | loss: 77.0690019CurrentTrain: epoch  9, batch    81 | loss: 80.0906560CurrentTrain: epoch  9, batch    82 | loss: 102.8349880CurrentTrain: epoch  9, batch    83 | loss: 78.9987429CurrentTrain: epoch  9, batch    84 | loss: 93.0863009CurrentTrain: epoch  9, batch    85 | loss: 176.2098402CurrentTrain: epoch  9, batch    86 | loss: 95.2734539CurrentTrain: epoch  9, batch    87 | loss: 127.1097405CurrentTrain: epoch  9, batch    88 | loss: 94.4737252CurrentTrain: epoch  9, batch    89 | loss: 126.5573485CurrentTrain: epoch  9, batch    90 | loss: 75.2598507CurrentTrain: epoch  9, batch    91 | loss: 98.0365973CurrentTrain: epoch  9, batch    92 | loss: 74.3438182CurrentTrain: epoch  9, batch    93 | loss: 176.3170503CurrentTrain: epoch  9, batch    94 | loss: 100.9940653CurrentTrain: epoch  9, batch    95 | loss: 107.5522025

F1 score per class: {32: 0.6514285714285715, 6: 0.8439306358381503, 19: 0.4166666666666667, 24: 0.7659574468085106, 26: 0.9361702127659575, 29: 0.9090909090909091}
Micro-average F1 score: 0.813953488372093
Weighted-average F1 score: 0.8220612673055845
F1 score per class: {32: 0.7422680412371134, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7700534759358288, 26: 0.9693877551020408, 29: 0.898989898989899}
Micro-average F1 score: 0.8463886063072228
Weighted-average F1 score: 0.8505645171095255
F1 score per class: {32: 0.7422680412371134, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7700534759358288, 26: 0.9693877551020408, 29: 0.898989898989899}
Micro-average F1 score: 0.8463886063072228
Weighted-average F1 score: 0.8505645171095255

F1 score per class: {32: 0.6514285714285715, 6: 0.8439306358381503, 19: 0.4166666666666667, 24: 0.7659574468085106, 26: 0.9361702127659575, 29: 0.9090909090909091}
Micro-average F1 score: 0.813953488372093
Weighted-average F1 score: 0.8220612673055845
F1 score per class: {32: 0.7422680412371134, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7700534759358288, 26: 0.9693877551020408, 29: 0.898989898989899}
Micro-average F1 score: 0.8463886063072228
Weighted-average F1 score: 0.8505645171095255
F1 score per class: {32: 0.7422680412371134, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7700534759358288, 26: 0.9693877551020408, 29: 0.898989898989899}
Micro-average F1 score: 0.8463886063072228
Weighted-average F1 score: 0.8505645171095255

F1 score per class: {32: 0.4851063829787234, 6: 0.6666666666666666, 19: 0.18867924528301888, 24: 0.7236180904522613, 26: 0.8934010152284264, 29: 0.7894736842105263}
Micro-average F1 score: 0.6808134394341291
Weighted-average F1 score: 0.6701096078813136
F1 score per class: {32: 0.5017421602787456, 6: 0.6693877551020408, 19: 0.17647058823529413, 24: 0.72, 26: 0.9090909090909091, 29: 0.7672413793103449}
Micro-average F1 score: 0.6704270749395649
Weighted-average F1 score: 0.6529423361460037
F1 score per class: {32: 0.5124555160142349, 6: 0.6693877551020408, 19: 0.17647058823529413, 24: 0.72, 26: 0.9090909090909091, 29: 0.7606837606837606}
Micro-average F1 score: 0.6725949878738885
Weighted-average F1 score: 0.6559968502401531

F1 score per class: {32: 0.4851063829787234, 6: 0.6666666666666666, 19: 0.18867924528301888, 24: 0.7236180904522613, 26: 0.8934010152284264, 29: 0.7894736842105263}
Micro-average F1 score: 0.6808134394341291
Weighted-average F1 score: 0.6701096078813136
F1 score per class: {32: 0.5017421602787456, 6: 0.6693877551020408, 19: 0.17647058823529413, 24: 0.72, 26: 0.9090909090909091, 29: 0.7672413793103449}
Micro-average F1 score: 0.6704270749395649
Weighted-average F1 score: 0.6529423361460037
F1 score per class: {32: 0.5124555160142349, 6: 0.6693877551020408, 19: 0.17647058823529413, 24: 0.72, 26: 0.9090909090909091, 29: 0.7606837606837606}
Micro-average F1 score: 0.6725949878738885
Weighted-average F1 score: 0.6559968502401531
cur_acc_wo_na:  ['0.8140']
his_acc_wo_na:  ['0.8140']
cur_acc des_wo_na:  ['0.8464']
his_acc des_wo_na:  ['0.8464']
cur_acc rrf_wo_na:  ['0.8464']
his_acc rrf_wo_na:  ['0.8464']
cur_acc_w_na:  ['0.6808']
his_acc_w_na:  ['0.6808']
cur_acc des_w_na:  ['0.6704']
his_acc des_w_na:  ['0.6704']
cur_acc rrf_w_na:  ['0.6726']
his_acc rrf_w_na:  ['0.6726']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 97.2290484CurrentTrain: epoch  0, batch     1 | loss: 115.0692018CurrentTrain: epoch  0, batch     2 | loss: 196.2781529CurrentTrain: epoch  0, batch     3 | loss: 16.3653837CurrentTrain: epoch  1, batch     0 | loss: 107.0865118CurrentTrain: epoch  1, batch     1 | loss: 92.0464997CurrentTrain: epoch  1, batch     2 | loss: 106.6430007CurrentTrain: epoch  1, batch     3 | loss: 41.1075924CurrentTrain: epoch  2, batch     0 | loss: 82.3915226CurrentTrain: epoch  2, batch     1 | loss: 130.6108178CurrentTrain: epoch  2, batch     2 | loss: 90.1674807CurrentTrain: epoch  2, batch     3 | loss: 20.6857158CurrentTrain: epoch  3, batch     0 | loss: 102.9031159CurrentTrain: epoch  3, batch     1 | loss: 102.7354693CurrentTrain: epoch  3, batch     2 | loss: 100.7258227CurrentTrain: epoch  3, batch     3 | loss: 12.8252744CurrentTrain: epoch  4, batch     0 | loss: 99.0062589CurrentTrain: epoch  4, batch     1 | loss: 101.3132883CurrentTrain: epoch  4, batch     2 | loss: 82.4001370CurrentTrain: epoch  4, batch     3 | loss: 41.0987578CurrentTrain: epoch  5, batch     0 | loss: 99.6508069CurrentTrain: epoch  5, batch     1 | loss: 100.5988975CurrentTrain: epoch  5, batch     2 | loss: 124.3606168CurrentTrain: epoch  5, batch     3 | loss: 14.6264454CurrentTrain: epoch  6, batch     0 | loss: 80.9617482CurrentTrain: epoch  6, batch     1 | loss: 127.7977967CurrentTrain: epoch  6, batch     2 | loss: 78.5730253CurrentTrain: epoch  6, batch     3 | loss: 16.7654259CurrentTrain: epoch  7, batch     0 | loss: 98.2855608CurrentTrain: epoch  7, batch     1 | loss: 76.3958875CurrentTrain: epoch  7, batch     2 | loss: 81.9376162CurrentTrain: epoch  7, batch     3 | loss: 41.1658355CurrentTrain: epoch  8, batch     0 | loss: 77.5160072CurrentTrain: epoch  8, batch     1 | loss: 81.9404788CurrentTrain: epoch  8, batch     2 | loss: 95.6051688CurrentTrain: epoch  8, batch     3 | loss: 9.0113252CurrentTrain: epoch  9, batch     0 | loss: 80.6453805CurrentTrain: epoch  9, batch     1 | loss: 74.3082203CurrentTrain: epoch  9, batch     2 | loss: 97.6900533CurrentTrain: epoch  9, batch     3 | loss: 16.6525289
MemoryTrain:  epoch  0, batch     0 | loss: 1.5834084MemoryTrain:  epoch  1, batch     0 | loss: 1.4193437MemoryTrain:  epoch  2, batch     0 | loss: 1.1566727MemoryTrain:  epoch  3, batch     0 | loss: 1.0007173MemoryTrain:  epoch  4, batch     0 | loss: 0.8204035MemoryTrain:  epoch  5, batch     0 | loss: 0.5673308MemoryTrain:  epoch  6, batch     0 | loss: 0.5069779MemoryTrain:  epoch  7, batch     0 | loss: 0.3515514MemoryTrain:  epoch  8, batch     0 | loss: 0.3036574MemoryTrain:  epoch  9, batch     0 | loss: 0.2503694

F1 score per class: {7: 0.75, 40: 0.96, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.4444444444444444, 27: 0.6666666666666666, 31: 0.6060606060606061}
Micro-average F1 score: 0.5767441860465117
Weighted-average F1 score: 0.465017501750175
F1 score per class: {7: 0.8888888888888888, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.4444444444444444, 27: 0.5, 31: 0.5918367346938775}
Micro-average F1 score: 0.5753424657534246
Weighted-average F1 score: 0.46707317847774027
F1 score per class: {7: 0.8888888888888888, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.5263157894736842, 27: 0.6666666666666666, 31: 0.5918367346938775}
Micro-average F1 score: 0.5871559633027523
Weighted-average F1 score: 0.47735598962932

F1 score per class: {32: 0.4090909090909091, 6: 0.10909090909090909, 7: 0.96, 40: 0.6956521739130435, 9: 0.1, 19: 0.7555555555555555, 24: 0.3333333333333333, 26: 0.925531914893617, 27: 0.6666666666666666, 29: 0.9015544041450777, 31: 0.5217391304347826}
Micro-average F1 score: 0.6923736075407027
Weighted-average F1 score: 0.6931049012308286
F1 score per class: {32: 0.5987261146496815, 6: 0.16, 7: 0.9803921568627451, 40: 0.7047619047619048, 9: 0.4444444444444444, 19: 0.7675675675675676, 24: 0.36363636363636365, 26: 0.9479166666666666, 27: 0.4, 29: 0.9183673469387755, 31: 0.4915254237288136}
Micro-average F1 score: 0.7287716405605935
Weighted-average F1 score: 0.7177708129471204
F1 score per class: {32: 0.5897435897435898, 6: 0.16, 7: 0.9803921568627451, 40: 0.7047619047619048, 9: 0.2608695652173913, 19: 0.7675675675675676, 24: 0.4, 26: 0.9424083769633508, 27: 0.6666666666666666, 29: 0.9183673469387755, 31: 0.4915254237288136}
Micro-average F1 score: 0.7251655629139073
Weighted-average F1 score: 0.7166184218806323

F1 score per class: {32: 0.0, 6: 0.6666666666666666, 7: 0.9230769230769231, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.36363636363636365, 26: 0.5, 27: 0.0, 31: 0.40816326530612246}
Micro-average F1 score: 0.43508771929824563
Weighted-average F1 score: 0.36949878303261763
F1 score per class: {32: 0.0, 6: 0.8, 7: 0.684931506849315, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.3076923076923077, 26: 0.14285714285714285, 27: 0.0, 31: 0.3411764705882353}
Micro-average F1 score: 0.3490304709141274
Weighted-average F1 score: 0.3105508766647916
F1 score per class: {32: 0.0, 6: 0.8, 7: 0.8064516129032258, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.35714285714285715, 26: 0.16666666666666666, 27: 0.0, 31: 0.3411764705882353}
Micro-average F1 score: 0.37209302325581395
Weighted-average F1 score: 0.3254611634275029

F1 score per class: {32: 0.32727272727272727, 6: 0.06976744186046512, 7: 0.9230769230769231, 40: 0.5647058823529412, 9: 0.0625, 19: 0.7046632124352331, 24: 0.18181818181818182, 26: 0.8656716417910447, 27: 0.2, 29: 0.8169014084507042, 31: 0.2857142857142857}
Micro-average F1 score: 0.5530458590006845
Weighted-average F1 score: 0.5216073193826596
F1 score per class: {32: 0.4351851851851852, 6: 0.08602150537634409, 7: 0.6578947368421053, 40: 0.5543071161048689, 9: 0.21818181818181817, 19: 0.71, 24: 0.14545454545454545, 26: 0.875, 27: 0.03636363636363636, 29: 0.7228915662650602, 31: 0.19931271477663232}
Micro-average F1 score: 0.5008498583569405
Weighted-average F1 score: 0.4565596371262065
F1 score per class: {32: 0.42592592592592593, 6: 0.08602150537634409, 7: 0.8064516129032258, 40: 0.556390977443609, 9: 0.13953488372093023, 19: 0.71, 24: 0.15873015873015872, 26: 0.8695652173913043, 27: 0.044444444444444446, 29: 0.75, 31: 0.19863013698630136}
Micro-average F1 score: 0.5072379849449913
Weighted-average F1 score: 0.461409205741666
cur_acc_wo_na:  ['0.8140', '0.5767']
his_acc_wo_na:  ['0.8140', '0.6924']
cur_acc des_wo_na:  ['0.8464', '0.5753']
his_acc des_wo_na:  ['0.8464', '0.7288']
cur_acc rrf_wo_na:  ['0.8464', '0.5872']
his_acc rrf_wo_na:  ['0.8464', '0.7252']
cur_acc_w_na:  ['0.6808', '0.4351']
his_acc_w_na:  ['0.6808', '0.5530']
cur_acc des_w_na:  ['0.6704', '0.3490']
his_acc des_w_na:  ['0.6704', '0.5008']
cur_acc rrf_w_na:  ['0.6726', '0.3721']
his_acc rrf_w_na:  ['0.6726', '0.5072']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 117.0066792CurrentTrain: epoch  0, batch     1 | loss: 122.4480307CurrentTrain: epoch  0, batch     2 | loss: 94.8185217CurrentTrain: epoch  0, batch     3 | loss: 92.1831331CurrentTrain: epoch  1, batch     0 | loss: 141.9041384CurrentTrain: epoch  1, batch     1 | loss: 116.8832526CurrentTrain: epoch  1, batch     2 | loss: 89.8036204CurrentTrain: epoch  1, batch     3 | loss: 108.1877842CurrentTrain: epoch  2, batch     0 | loss: 138.4604524CurrentTrain: epoch  2, batch     1 | loss: 89.3101442CurrentTrain: epoch  2, batch     2 | loss: 132.2320076CurrentTrain: epoch  2, batch     3 | loss: 70.7764483CurrentTrain: epoch  3, batch     0 | loss: 93.4082782CurrentTrain: epoch  3, batch     1 | loss: 131.9903828CurrentTrain: epoch  3, batch     2 | loss: 99.2802534CurrentTrain: epoch  3, batch     3 | loss: 85.7081541CurrentTrain: epoch  4, batch     0 | loss: 173.9945771CurrentTrain: epoch  4, batch     1 | loss: 80.0423238CurrentTrain: epoch  4, batch     2 | loss: 136.0223925CurrentTrain: epoch  4, batch     3 | loss: 108.8074648CurrentTrain: epoch  5, batch     0 | loss: 100.6413430CurrentTrain: epoch  5, batch     1 | loss: 131.9653439CurrentTrain: epoch  5, batch     2 | loss: 80.7739240CurrentTrain: epoch  5, batch     3 | loss: 107.2801888CurrentTrain: epoch  6, batch     0 | loss: 127.6353951CurrentTrain: epoch  6, batch     1 | loss: 99.9351751CurrentTrain: epoch  6, batch     2 | loss: 82.0383851CurrentTrain: epoch  6, batch     3 | loss: 70.3278741CurrentTrain: epoch  7, batch     0 | loss: 98.4561167CurrentTrain: epoch  7, batch     1 | loss: 83.4948157CurrentTrain: epoch  7, batch     2 | loss: 97.1724306CurrentTrain: epoch  7, batch     3 | loss: 104.8136568CurrentTrain: epoch  8, batch     0 | loss: 126.5526822CurrentTrain: epoch  8, batch     1 | loss: 97.3043645CurrentTrain: epoch  8, batch     2 | loss: 101.4780204CurrentTrain: epoch  8, batch     3 | loss: 67.7533021CurrentTrain: epoch  9, batch     0 | loss: 97.8385941CurrentTrain: epoch  9, batch     1 | loss: 100.3623667CurrentTrain: epoch  9, batch     2 | loss: 97.2022025CurrentTrain: epoch  9, batch     3 | loss: 82.6770959
MemoryTrain:  epoch  0, batch     0 | loss: 1.2901533MemoryTrain:  epoch  1, batch     0 | loss: 1.0864434MemoryTrain:  epoch  2, batch     0 | loss: 0.8499529MemoryTrain:  epoch  3, batch     0 | loss: 0.7099459MemoryTrain:  epoch  4, batch     0 | loss: 0.6083179MemoryTrain:  epoch  5, batch     0 | loss: 0.4791366MemoryTrain:  epoch  6, batch     0 | loss: 0.4403019MemoryTrain:  epoch  7, batch     0 | loss: 0.3453398MemoryTrain:  epoch  8, batch     0 | loss: 0.3409581MemoryTrain:  epoch  9, batch     0 | loss: 0.2640005

F1 score per class: {0: 0.9863013698630136, 32: 0.9473684210526315, 4: 0.3333333333333333, 13: 0.7083333333333334, 21: 0.7466666666666667, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0}
Micro-average F1 score: 0.8472906403940886
Weighted-average F1 score: 0.8282348057874839
F1 score per class: {0: 0.9722222222222222, 32: 0.98989898989899, 4: 0.0, 9: 0.5714285714285714, 13: 0.6808510638297872, 21: 0.810126582278481, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8693586698337292
Weighted-average F1 score: 0.840788077536935
F1 score per class: {0: 0.9722222222222222, 32: 0.98989898989899, 4: 0.5714285714285714, 13: 0.7083333333333334, 21: 0.7792207792207793, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8687350835322196
Weighted-average F1 score: 0.8422061606734972

F1 score per class: {32: 0.9863013698630136, 0: 0.9473684210526315, 4: 0.4393939393939394, 6: 0.16666666666666666, 7: 0.96, 40: 0.18181818181818182, 9: 0.7336683417085427, 13: 0.5666666666666667, 19: 0.7466666666666667, 21: 0.09523809523809523, 23: 0.7225130890052356, 24: 0.48, 26: 0.9375, 27: 0.6666666666666666, 29: 0.8677248677248677, 31: 0.6386554621848739}
Micro-average F1 score: 0.7509578544061303
Weighted-average F1 score: 0.7599599001511654
F1 score per class: {32: 0.9722222222222222, 0: 0.98, 4: 0.5234899328859061, 6: 0.05555555555555555, 7: 0.9615384615384616, 40: 0.17391304347826086, 9: 0.7745098039215687, 13: 0.5714285714285714, 19: 0.8, 21: 0.08695652173913043, 23: 0.7150259067357513, 24: 0.5161290322580645, 26: 0.9538461538461539, 27: 0.8, 29: 0.8979591836734694, 31: 0.6612903225806451}
Micro-average F1 score: 0.7675411836485662
Weighted-average F1 score: 0.7639649481777574
F1 score per class: {32: 0.9722222222222222, 0: 0.98989898989899, 4: 0.5170068027210885, 6: 0.05263157894736842, 7: 0.96, 40: 0.17391304347826086, 9: 0.7761194029850746, 13: 0.5666666666666667, 19: 0.7692307692307693, 21: 0.08695652173913043, 23: 0.7150259067357513, 24: 0.4166666666666667, 26: 0.9538461538461539, 27: 0.8, 29: 0.8979591836734694, 31: 0.6825396825396826}
Micro-average F1 score: 0.7661141804788214
Weighted-average F1 score: 0.7622063261112157

F1 score per class: {0: 0.7578947368421053, 32: 0.9375, 4: 0.0, 6: 0.0, 7: 0.03333333333333333, 40: 0.0, 13: 0.35789473684210527, 19: 0.6829268292682927, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5539452495974235
Weighted-average F1 score: 0.44600730736274413
F1 score per class: {0: 0.5982905982905983, 32: 0.9158878504672897, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.043478260869565216, 9: 0.0, 13: 0.2689075630252101, 19: 0.5614035087719298, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.42117376294591485
Weighted-average F1 score: 0.3348766905178873
F1 score per class: {0: 0.5932203389830508, 32: 0.9468599033816425, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.0425531914893617, 9: 0.0, 13: 0.2786885245901639, 19: 0.6060606060606061, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.4422843256379101
Weighted-average F1 score: 0.34830497249564263

F1 score per class: {32: 0.4528301886792453, 0: 0.9326424870466321, 4: 0.36942675159235666, 6: 0.09523809523809523, 7: 0.8135593220338984, 40: 0.016260162601626018, 9: 0.5551330798479087, 13: 0.16346153846153846, 19: 0.6588235294117647, 21: 0.06451612903225806, 23: 0.6798029556650246, 24: 0.1935483870967742, 26: 0.8256880733944955, 27: 0.05714285714285714, 29: 0.7354260089686099, 31: 0.31666666666666665}
Micro-average F1 score: 0.5064599483204134
Weighted-average F1 score: 0.4533103821280046
F1 score per class: {32: 0.37037037037037035, 0: 0.8596491228070176, 4: 0.35294117647058826, 6: 0.022727272727272728, 7: 0.5376344086021505, 40: 0.017777777777777778, 9: 0.5266666666666666, 13: 0.13559322033898305, 19: 0.5, 21: 0.05405405405405406, 23: 0.6699029126213593, 24: 0.1509433962264151, 26: 0.8017241379310345, 27: 0.04395604395604396, 29: 0.5587301587301587, 31: 0.22527472527472528}
Micro-average F1 score: 0.41124550506701535
Weighted-average F1 score: 0.3657039594637129
F1 score per class: {32: 0.33980582524271846, 0: 0.92018779342723, 4: 0.36538461538461536, 6: 0.022727272727272728, 7: 0.7164179104477612, 40: 0.017316017316017316, 9: 0.5571428571428572, 13: 0.136, 19: 0.5405405405405406, 21: 0.05555555555555555, 23: 0.6699029126213593, 24: 0.11904761904761904, 26: 0.8230088495575221, 27: 0.06779661016949153, 29: 0.6153846153846154, 31: 0.24089635854341737}
Micro-average F1 score: 0.42916093535075656
Weighted-average F1 score: 0.375881988296705
cur_acc_wo_na:  ['0.8140', '0.5767', '0.8473']
his_acc_wo_na:  ['0.8140', '0.6924', '0.7510']
cur_acc des_wo_na:  ['0.8464', '0.5753', '0.8694']
his_acc des_wo_na:  ['0.8464', '0.7288', '0.7675']
cur_acc rrf_wo_na:  ['0.8464', '0.5872', '0.8687']
his_acc rrf_wo_na:  ['0.8464', '0.7252', '0.7661']
cur_acc_w_na:  ['0.6808', '0.4351', '0.5539']
his_acc_w_na:  ['0.6808', '0.5530', '0.5065']
cur_acc des_w_na:  ['0.6704', '0.3490', '0.4212']
his_acc des_w_na:  ['0.6704', '0.5008', '0.4112']
cur_acc rrf_w_na:  ['0.6726', '0.3721', '0.4423']
his_acc rrf_w_na:  ['0.6726', '0.5072', '0.4292']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 145.6633949CurrentTrain: epoch  0, batch     1 | loss: 137.5807555CurrentTrain: epoch  0, batch     2 | loss: 187.5835880CurrentTrain: epoch  0, batch     3 | loss: 190.3363965CurrentTrain: epoch  0, batch     4 | loss: 68.2783441CurrentTrain: epoch  1, batch     0 | loss: 110.2523503CurrentTrain: epoch  1, batch     1 | loss: 109.3388145CurrentTrain: epoch  1, batch     2 | loss: 189.5444616CurrentTrain: epoch  1, batch     3 | loss: 183.6233754CurrentTrain: epoch  1, batch     4 | loss: 84.1615308CurrentTrain: epoch  2, batch     0 | loss: 104.8516854CurrentTrain: epoch  2, batch     1 | loss: 132.9540102CurrentTrain: epoch  2, batch     2 | loss: 131.5637651CurrentTrain: epoch  2, batch     3 | loss: 129.0378662CurrentTrain: epoch  2, batch     4 | loss: 81.8857541CurrentTrain: epoch  3, batch     0 | loss: 88.3942666CurrentTrain: epoch  3, batch     1 | loss: 104.9427878CurrentTrain: epoch  3, batch     2 | loss: 129.8962902CurrentTrain: epoch  3, batch     3 | loss: 100.1604880CurrentTrain: epoch  3, batch     4 | loss: 112.6825796CurrentTrain: epoch  4, batch     0 | loss: 104.4350750CurrentTrain: epoch  4, batch     1 | loss: 98.6029988CurrentTrain: epoch  4, batch     2 | loss: 96.0306084CurrentTrain: epoch  4, batch     3 | loss: 176.9070506CurrentTrain: epoch  4, batch     4 | loss: 180.8067520CurrentTrain: epoch  5, batch     0 | loss: 128.5356705CurrentTrain: epoch  5, batch     1 | loss: 102.9882945CurrentTrain: epoch  5, batch     2 | loss: 78.5407036CurrentTrain: epoch  5, batch     3 | loss: 128.4488427CurrentTrain: epoch  5, batch     4 | loss: 169.8513667CurrentTrain: epoch  6, batch     0 | loss: 98.7755229CurrentTrain: epoch  6, batch     1 | loss: 125.3657620CurrentTrain: epoch  6, batch     2 | loss: 81.3710250CurrentTrain: epoch  6, batch     3 | loss: 271.7839946CurrentTrain: epoch  6, batch     4 | loss: 62.6523292CurrentTrain: epoch  7, batch     0 | loss: 98.4942697CurrentTrain: epoch  7, batch     1 | loss: 129.9045360CurrentTrain: epoch  7, batch     2 | loss: 96.6838676CurrentTrain: epoch  7, batch     3 | loss: 126.8608280CurrentTrain: epoch  7, batch     4 | loss: 78.7765321CurrentTrain: epoch  8, batch     0 | loss: 92.7463036CurrentTrain: epoch  8, batch     1 | loss: 173.8263713CurrentTrain: epoch  8, batch     2 | loss: 124.8343940CurrentTrain: epoch  8, batch     3 | loss: 128.2636304CurrentTrain: epoch  8, batch     4 | loss: 112.5194181CurrentTrain: epoch  9, batch     0 | loss: 98.8100899CurrentTrain: epoch  9, batch     1 | loss: 99.7612292CurrentTrain: epoch  9, batch     2 | loss: 124.5962394CurrentTrain: epoch  9, batch     3 | loss: 99.5283069CurrentTrain: epoch  9, batch     4 | loss: 60.6812617
MemoryTrain:  epoch  0, batch     0 | loss: 0.7817530MemoryTrain:  epoch  1, batch     0 | loss: 0.6460751MemoryTrain:  epoch  2, batch     0 | loss: 0.5102386MemoryTrain:  epoch  3, batch     0 | loss: 0.4064274MemoryTrain:  epoch  4, batch     0 | loss: 0.3541145MemoryTrain:  epoch  5, batch     0 | loss: 0.2799928MemoryTrain:  epoch  6, batch     0 | loss: 0.2319133MemoryTrain:  epoch  7, batch     0 | loss: 0.1853140MemoryTrain:  epoch  8, batch     0 | loss: 0.1780718MemoryTrain:  epoch  9, batch     0 | loss: 0.1405470

F1 score per class: {5: 0.9847715736040609, 6: 0.0, 7: 0.0, 10: 0.5074626865671642, 13: 0.0, 16: 0.8461538461538461, 17: 0.0, 18: 0.52}
Micro-average F1 score: 0.7124463519313304
Weighted-average F1 score: 0.7271155715698007
F1 score per class: {5: 1.0, 6: 0.0, 7: 0.0, 40: 0.36065573770491804, 10: 0.0, 13: 0.9473684210526315, 16: 0.6153846153846154, 17: 0.8787878787878788, 18: 0.0}
Micro-average F1 score: 0.708171206225681
Weighted-average F1 score: 0.6784023552282195
F1 score per class: {5: 1.0, 6: 0.0, 7: 0.0, 10: 0.4126984126984127, 13: 0.0, 16: 0.9655172413793104, 17: 0.0, 18: 0.8615384615384616}
Micro-average F1 score: 0.7207920792079208
Weighted-average F1 score: 0.7069332681740452

F1 score per class: {0: 0.8955223880597015, 4: 0.9247311827956989, 5: 0.9748743718592965, 6: 0.3382352941176471, 7: 0.1935483870967742, 9: 0.9803921568627451, 10: 0.5037037037037037, 13: 0.1111111111111111, 16: 0.8461538461538461, 17: 0.0, 18: 0.52, 19: 0.7619047619047619, 21: 0.4583333333333333, 23: 0.7948717948717948, 24: 0.1, 26: 0.7301587301587301, 27: 0.42857142857142855, 29: 0.9424083769633508, 31: 0.6666666666666666, 32: 0.8736842105263158, 40: 0.7049180327868853}
Micro-average F1 score: 0.7421131697546319
Weighted-average F1 score: 0.7641553551877751
F1 score per class: {0: 0.9722222222222222, 4: 0.9690721649484536, 5: 0.9523809523809523, 6: 0.46153846153846156, 7: 0.09090909090909091, 9: 0.9615384615384616, 10: 0.36065573770491804, 13: 0.07272727272727272, 16: 0.9152542372881356, 17: 0.25, 18: 0.8656716417910447, 19: 0.7807486631016043, 21: 0.5483870967741935, 23: 0.825, 24: 0.08, 26: 0.7150259067357513, 27: 0.5806451612903226, 29: 0.9430051813471503, 31: 1.0, 32: 0.8979591836734694, 40: 0.7482014388489209}
Micro-average F1 score: 0.7481481481481481
Weighted-average F1 score: 0.7430158992375061
F1 score per class: {0: 0.9722222222222222, 4: 0.9690721649484536, 5: 0.9523809523809523, 6: 0.4413793103448276, 7: 0.09090909090909091, 9: 0.9615384615384616, 10: 0.4126984126984127, 13: 0.046511627906976744, 16: 0.9333333333333333, 17: 0.0, 18: 0.8484848484848485, 19: 0.7936507936507936, 21: 0.5806451612903226, 23: 0.825, 24: 0.08333333333333333, 26: 0.7150259067357513, 27: 0.5333333333333333, 29: 0.9430051813471503, 31: 1.0, 32: 0.882051282051282, 40: 0.7647058823529411}
Micro-average F1 score: 0.7532710280373832
Weighted-average F1 score: 0.7517905148623146

F1 score per class: {0: 0.0, 32: 0.0, 4: 0.7983539094650206, 5: 0.0, 6: 0.0, 7: 0.44155844155844154, 40: 0.0, 10: 0.5176470588235295, 13: 0.0, 16: 0.2857142857142857, 17: 0.0, 18: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.49039881831610044
Weighted-average F1 score: 0.4536357220985519
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.411522633744856, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.3333333333333333, 13: 0.0, 16: 0.5192307692307693, 17: 0.32, 18: 0.25327510917030566, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.28482003129890454
Weighted-average F1 score: 0.2611639732294977
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.4166666666666667, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.36879432624113473, 13: 0.0, 16: 0.5283018867924528, 17: 0.0, 18: 0.25, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.30057803468208094
Weighted-average F1 score: 0.2784543787224024

F1 score per class: {0: 0.6521739130434783, 4: 0.91005291005291, 5: 0.754863813229572, 6: 0.26436781609195403, 7: 0.1016949152542373, 9: 0.8064516129032258, 10: 0.29955947136563876, 13: 0.044444444444444446, 16: 0.43137254901960786, 17: 0.0, 18: 0.26262626262626265, 19: 0.5538461538461539, 21: 0.19130434782608696, 23: 0.7209302325581395, 24: 0.06451612903225806, 26: 0.6731707317073171, 27: 0.13793103448275862, 29: 0.8, 31: 0.15384615384615385, 32: 0.751131221719457, 40: 0.39631336405529954}
Micro-average F1 score: 0.5329018338727076
Weighted-average F1 score: 0.5098536091634625
F1 score per class: {0: 0.4861111111111111, 4: 0.8867924528301887, 5: 0.273224043715847, 6: 0.28820960698689957, 7: 0.03333333333333333, 9: 0.5102040816326531, 10: 0.22564102564102564, 13: 0.023255813953488372, 16: 0.421875, 17: 0.11940298507462686, 18: 0.17846153846153845, 19: 0.49491525423728816, 21: 0.09042553191489362, 23: 0.5038167938931297, 24: 0.043478260869565216, 26: 0.647887323943662, 27: 0.14285714285714285, 29: 0.7109375, 31: 0.06451612903225806, 32: 0.5886287625418061, 40: 0.24821002386634844}
Micro-average F1 score: 0.3479009687836383
Weighted-average F1 score: 0.3145801649176785
F1 score per class: {0: 0.46357615894039733, 4: 0.9170731707317074, 5: 0.2785515320334262, 6: 0.2782608695652174, 7: 0.03361344537815126, 9: 0.6666666666666666, 10: 0.2374429223744292, 13: 0.014184397163120567, 16: 0.42748091603053434, 17: 0.0, 18: 0.175, 19: 0.5244755244755245, 21: 0.09523809523809523, 23: 0.5789473684210527, 24: 0.044444444444444446, 26: 0.6448598130841121, 27: 0.12903225806451613, 29: 0.7583333333333333, 31: 0.08333333333333333, 32: 0.6466165413533834, 40: 0.28888888888888886}
Micro-average F1 score: 0.365118912797282
Weighted-average F1 score: 0.3276431772628585
cur_acc_wo_na:  ['0.8140', '0.5767', '0.8473', '0.7124']
his_acc_wo_na:  ['0.8140', '0.6924', '0.7510', '0.7421']
cur_acc des_wo_na:  ['0.8464', '0.5753', '0.8694', '0.7082']
his_acc des_wo_na:  ['0.8464', '0.7288', '0.7675', '0.7481']
cur_acc rrf_wo_na:  ['0.8464', '0.5872', '0.8687', '0.7208']
his_acc rrf_wo_na:  ['0.8464', '0.7252', '0.7661', '0.7533']
cur_acc_w_na:  ['0.6808', '0.4351', '0.5539', '0.4904']
his_acc_w_na:  ['0.6808', '0.5530', '0.5065', '0.5329']
cur_acc des_w_na:  ['0.6704', '0.3490', '0.4212', '0.2848']
his_acc des_w_na:  ['0.6704', '0.5008', '0.4112', '0.3479']
cur_acc rrf_w_na:  ['0.6726', '0.3721', '0.4423', '0.3006']
his_acc rrf_w_na:  ['0.6726', '0.5072', '0.4292', '0.3651']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 142.0751592CurrentTrain: epoch  0, batch     1 | loss: 96.1937553CurrentTrain: epoch  0, batch     2 | loss: 112.2065647CurrentTrain: epoch  0, batch     3 | loss: 100.7317743CurrentTrain: epoch  1, batch     0 | loss: 102.4462149CurrentTrain: epoch  1, batch     1 | loss: 84.0628040CurrentTrain: epoch  1, batch     2 | loss: 108.5357371CurrentTrain: epoch  1, batch     3 | loss: 182.9141165CurrentTrain: epoch  2, batch     0 | loss: 84.0436648CurrentTrain: epoch  2, batch     1 | loss: 105.8826160CurrentTrain: epoch  2, batch     2 | loss: 97.2992927CurrentTrain: epoch  2, batch     3 | loss: 121.1750823CurrentTrain: epoch  3, batch     0 | loss: 100.4291136CurrentTrain: epoch  3, batch     1 | loss: 101.5907815CurrentTrain: epoch  3, batch     2 | loss: 80.2353309CurrentTrain: epoch  3, batch     3 | loss: 70.6230085CurrentTrain: epoch  4, batch     0 | loss: 174.7227664CurrentTrain: epoch  4, batch     1 | loss: 77.4942137CurrentTrain: epoch  4, batch     2 | loss: 98.3391207CurrentTrain: epoch  4, batch     3 | loss: 55.1845480CurrentTrain: epoch  5, batch     0 | loss: 131.9903640CurrentTrain: epoch  5, batch     1 | loss: 76.6835045CurrentTrain: epoch  5, batch     2 | loss: 122.3732713CurrentTrain: epoch  5, batch     3 | loss: 65.0382478CurrentTrain: epoch  6, batch     0 | loss: 99.3285998CurrentTrain: epoch  6, batch     1 | loss: 79.4629144CurrentTrain: epoch  6, batch     2 | loss: 95.9197797CurrentTrain: epoch  6, batch     3 | loss: 112.4358616CurrentTrain: epoch  7, batch     0 | loss: 128.3561733CurrentTrain: epoch  7, batch     1 | loss: 77.1507141CurrentTrain: epoch  7, batch     2 | loss: 98.8069182CurrentTrain: epoch  7, batch     3 | loss: 61.6668673CurrentTrain: epoch  8, batch     0 | loss: 126.9770655CurrentTrain: epoch  8, batch     1 | loss: 94.3708782CurrentTrain: epoch  8, batch     2 | loss: 77.4984223CurrentTrain: epoch  8, batch     3 | loss: 83.9390678CurrentTrain: epoch  9, batch     0 | loss: 94.9328553CurrentTrain: epoch  9, batch     1 | loss: 97.1978530CurrentTrain: epoch  9, batch     2 | loss: 80.2902457CurrentTrain: epoch  9, batch     3 | loss: 52.6861878
MemoryTrain:  epoch  0, batch     0 | loss: 0.7713843MemoryTrain:  epoch  1, batch     0 | loss: 0.5831127MemoryTrain:  epoch  2, batch     0 | loss: 0.4550833MemoryTrain:  epoch  3, batch     0 | loss: 0.3761203MemoryTrain:  epoch  4, batch     0 | loss: 0.3040219MemoryTrain:  epoch  5, batch     0 | loss: 0.2220526MemoryTrain:  epoch  6, batch     0 | loss: 0.2151642MemoryTrain:  epoch  7, batch     0 | loss: 0.1848278MemoryTrain:  epoch  8, batch     0 | loss: 0.1663751MemoryTrain:  epoch  9, batch     0 | loss: 0.1273246

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.8235294117647058, 38: 0.0, 10: 0.0, 13: 0.47058823529411764, 15: 0.0, 18: 0.0, 23: 0.5352112676056338, 25: 0.5, 27: 0.5263157894736842}
Micro-average F1 score: 0.46153846153846156
Weighted-average F1 score: 0.3537072152604355
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.75, 38: 0.0, 10: 0.0, 13: 0.0, 15: 0.5945945945945946, 18: 0.0, 21: 0.0, 23: 0.9072164948453608, 25: 0.7096774193548387, 27: 0.7111111111111111}
Micro-average F1 score: 0.6470588235294118
Weighted-average F1 score: 0.5480999050379552
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.7058823529411765, 38: 0.0, 10: 0.0, 13: 0.0, 15: 0.5753424657534246, 18: 0.0, 21: 0.0, 23: 0.8222222222222222, 25: 0.723404255319149, 27: 0.7111111111111111}
Micro-average F1 score: 0.6162162162162163
Weighted-average F1 score: 0.5075424170876732

F1 score per class: {0: 0.8955223880597015, 4: 0.907103825136612, 5: 0.8646288209606987, 6: 0.30656934306569344, 7: 0.0, 9: 0.96, 10: 0.21238938053097345, 13: 0.06666666666666667, 15: 0.7777777777777778, 16: 0.8727272727272727, 17: 0.0, 18: 0.44, 19: 0.5324675324675324, 21: 0.32558139534883723, 23: 0.8192771084337349, 24: 0.1, 25: 0.47058823529411764, 26: 0.7319587628865979, 27: 0.32, 29: 0.9424083769633508, 31: 0.6666666666666666, 32: 0.8044692737430168, 35: 0.5205479452054794, 37: 0.43956043956043955, 38: 0.35714285714285715, 40: 0.6440677966101694}
Micro-average F1 score: 0.6498896247240619
Weighted-average F1 score: 0.6902265599401601
F1 score per class: {0: 0.9863013698630136, 4: 0.9690721649484536, 5: 0.8163265306122449, 6: 0.5157232704402516, 7: 0.1875, 9: 0.9615384615384616, 10: 0.3442622950819672, 13: 0.08163265306122448, 15: 0.7058823529411765, 16: 0.8666666666666667, 17: 0.13333333333333333, 18: 0.5964912280701754, 19: 0.6103896103896104, 21: 0.5, 23: 0.8372093023255814, 24: 0.09090909090909091, 25: 0.5945945945945946, 26: 0.7253886010362695, 27: 0.5161290322580645, 29: 0.9538461538461539, 31: 1.0, 32: 0.8888888888888888, 35: 0.8543689320388349, 37: 0.5892857142857143, 38: 0.4444444444444444, 40: 0.7482993197278912}
Micro-average F1 score: 0.714172604908947
Weighted-average F1 score: 0.7218628920604561
F1 score per class: {0: 0.9863013698630136, 4: 0.9690721649484536, 5: 0.819672131147541, 6: 0.5157232704402516, 7: 0.1875, 9: 0.9803921568627451, 10: 0.31666666666666665, 13: 0.043478260869565216, 15: 0.631578947368421, 16: 0.8666666666666667, 17: 0.0, 18: 0.5818181818181818, 19: 0.620253164556962, 21: 0.5161290322580645, 23: 0.8333333333333334, 24: 0.09090909090909091, 25: 0.5753424657534246, 26: 0.7253886010362695, 27: 0.5161290322580645, 29: 0.9538461538461539, 31: 1.0, 32: 0.8811881188118812, 35: 0.7789473684210526, 37: 0.5964912280701754, 38: 0.4266666666666667, 40: 0.7375886524822695}
Micro-average F1 score: 0.7077534791252486
Weighted-average F1 score: 0.7165165869605448

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.7368421052631579, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.463768115942029, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.41304347826086957, 37: 0.46511627906976744, 38: 0.425531914893617, 40: 0.0}
Micro-average F1 score: 0.3185840707964602
Weighted-average F1 score: 0.20569143093972606
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.0, 15: 0.5714285714285714, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.5301204819277109, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5827814569536424, 37: 0.4888888888888889, 38: 0.47058823529411764, 40: 0.0}
Micro-average F1 score: 0.2962056303549572
Weighted-average F1 score: 0.22157108928452748
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.5454545454545454, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.525, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5211267605633803, 37: 0.49635036496350365, 38: 0.47058823529411764, 40: 0.0}
Micro-average F1 score: 0.287515762925599
Weighted-average F1 score: 0.21234881295722405

F1 score per class: {0: 0.6060606060606061, 4: 0.8924731182795699, 5: 0.591044776119403, 6: 0.23076923076923078, 7: 0.0, 9: 0.8, 10: 0.15483870967741936, 13: 0.02631578947368421, 15: 0.56, 16: 0.41739130434782606, 17: 0.0, 18: 0.16666666666666666, 19: 0.39805825242718446, 21: 0.17073170731707318, 23: 0.7391304347826086, 24: 0.07407407407407407, 25: 0.463768115942029, 26: 0.6543778801843319, 27: 0.1, 29: 0.7758620689655172, 31: 0.15384615384615385, 32: 0.6728971962616822, 35: 0.2261904761904762, 37: 0.20618556701030927, 38: 0.15503875968992248, 40: 0.37254901960784315}
Micro-average F1 score: 0.4380952380952381
Weighted-average F1 score: 0.4168653873068669
F1 score per class: {0: 0.4528301886792453, 4: 0.8703703703703703, 5: 0.25220680958385877, 6: 0.3253968253968254, 7: 0.06593406593406594, 9: 0.47619047619047616, 10: 0.20388349514563106, 13: 0.02197802197802198, 15: 0.36363636363636365, 16: 0.348993288590604, 17: 0.07142857142857142, 18: 0.1033434650455927, 19: 0.3900414937759336, 21: 0.09803921568627451, 23: 0.5333333333333333, 24: 0.05405405405405406, 25: 0.5238095238095238, 26: 0.625, 27: 0.13445378151260504, 29: 0.6838235294117647, 31: 0.08888888888888889, 32: 0.6666666666666666, 35: 0.21256038647342995, 37: 0.14732142857142858, 38: 0.13168724279835392, 40: 0.3107344632768362}
Micro-average F1 score: 0.31488916041193926
Weighted-average F1 score: 0.28440305552103723
F1 score per class: {0: 0.45, 4: 0.9261083743842364, 5: 0.2631578947368421, 6: 0.3253968253968254, 7: 0.06741573033707865, 9: 0.6097560975609756, 10: 0.18811881188118812, 13: 0.012578616352201259, 15: 0.3157894736842105, 16: 0.35135135135135137, 17: 0.0, 18: 0.09846153846153846, 19: 0.392, 21: 0.10158730158730159, 23: 0.6194690265486725, 24: 0.05714285714285714, 25: 0.5185185185185185, 26: 0.6306306306306306, 27: 0.13675213675213677, 29: 0.744, 31: 0.09523809523809523, 32: 0.6592592592592592, 35: 0.19733333333333333, 37: 0.1459227467811159, 38: 0.12213740458015267, 40: 0.32}
Micro-average F1 score: 0.320201475085447
Weighted-average F1 score: 0.2862090843519426
cur_acc_wo_na:  ['0.8140', '0.5767', '0.8473', '0.7124', '0.4615']
his_acc_wo_na:  ['0.8140', '0.6924', '0.7510', '0.7421', '0.6499']
cur_acc des_wo_na:  ['0.8464', '0.5753', '0.8694', '0.7082', '0.6471']
his_acc des_wo_na:  ['0.8464', '0.7288', '0.7675', '0.7481', '0.7142']
cur_acc rrf_wo_na:  ['0.8464', '0.5872', '0.8687', '0.7208', '0.6162']
his_acc rrf_wo_na:  ['0.8464', '0.7252', '0.7661', '0.7533', '0.7078']
cur_acc_w_na:  ['0.6808', '0.4351', '0.5539', '0.4904', '0.3186']
his_acc_w_na:  ['0.6808', '0.5530', '0.5065', '0.5329', '0.4381']
cur_acc des_w_na:  ['0.6704', '0.3490', '0.4212', '0.2848', '0.2962']
his_acc des_w_na:  ['0.6704', '0.5008', '0.4112', '0.3479', '0.3149']
cur_acc rrf_w_na:  ['0.6726', '0.3721', '0.4423', '0.3006', '0.2875']
his_acc rrf_w_na:  ['0.6726', '0.5072', '0.4292', '0.3651', '0.3202']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 125.5576766CurrentTrain: epoch  0, batch     1 | loss: 147.5580536CurrentTrain: epoch  0, batch     2 | loss: 190.5312371CurrentTrain: epoch  0, batch     3 | loss: 103.8029507CurrentTrain: epoch  0, batch     4 | loss: 36.1508730CurrentTrain: epoch  1, batch     0 | loss: 114.2431668CurrentTrain: epoch  1, batch     1 | loss: 91.6888412CurrentTrain: epoch  1, batch     2 | loss: 133.2223762CurrentTrain: epoch  1, batch     3 | loss: 135.4595447CurrentTrain: epoch  1, batch     4 | loss: 90.9966331CurrentTrain: epoch  2, batch     0 | loss: 136.0892518CurrentTrain: epoch  2, batch     1 | loss: 109.7382631CurrentTrain: epoch  2, batch     2 | loss: 85.3661033CurrentTrain: epoch  2, batch     3 | loss: 106.6681744CurrentTrain: epoch  2, batch     4 | loss: 90.6377845CurrentTrain: epoch  3, batch     0 | loss: 91.0459862CurrentTrain: epoch  3, batch     1 | loss: 99.7283386CurrentTrain: epoch  3, batch     2 | loss: 105.5835446CurrentTrain: epoch  3, batch     3 | loss: 134.4651291CurrentTrain: epoch  3, batch     4 | loss: 25.9388497CurrentTrain: epoch  4, batch     0 | loss: 103.0094502CurrentTrain: epoch  4, batch     1 | loss: 103.9206596CurrentTrain: epoch  4, batch     2 | loss: 104.1250306CurrentTrain: epoch  4, batch     3 | loss: 102.6480448CurrentTrain: epoch  4, batch     4 | loss: 24.5302813CurrentTrain: epoch  5, batch     0 | loss: 129.3541978CurrentTrain: epoch  5, batch     1 | loss: 129.3974449CurrentTrain: epoch  5, batch     2 | loss: 97.2874158CurrentTrain: epoch  5, batch     3 | loss: 97.6122348CurrentTrain: epoch  5, batch     4 | loss: 41.6763698CurrentTrain: epoch  6, batch     0 | loss: 128.2973484CurrentTrain: epoch  6, batch     1 | loss: 77.3779061CurrentTrain: epoch  6, batch     2 | loss: 131.5161598CurrentTrain: epoch  6, batch     3 | loss: 101.4639000CurrentTrain: epoch  6, batch     4 | loss: 24.7638091CurrentTrain: epoch  7, batch     0 | loss: 126.8787799CurrentTrain: epoch  7, batch     1 | loss: 178.1693781CurrentTrain: epoch  7, batch     2 | loss: 77.5283808CurrentTrain: epoch  7, batch     3 | loss: 99.6268226CurrentTrain: epoch  7, batch     4 | loss: 24.1471835CurrentTrain: epoch  8, batch     0 | loss: 97.8931372CurrentTrain: epoch  8, batch     1 | loss: 127.7335064CurrentTrain: epoch  8, batch     2 | loss: 99.4750317CurrentTrain: epoch  8, batch     3 | loss: 79.9023744CurrentTrain: epoch  8, batch     4 | loss: 23.6738916CurrentTrain: epoch  9, batch     0 | loss: 98.8465750CurrentTrain: epoch  9, batch     1 | loss: 79.0171104CurrentTrain: epoch  9, batch     2 | loss: 98.0965425CurrentTrain: epoch  9, batch     3 | loss: 125.1305296CurrentTrain: epoch  9, batch     4 | loss: 89.9950892
MemoryTrain:  epoch  0, batch     0 | loss: 0.8785626MemoryTrain:  epoch  1, batch     0 | loss: 0.7830571MemoryTrain:  epoch  2, batch     0 | loss: 0.6214944MemoryTrain:  epoch  3, batch     0 | loss: 0.5504697MemoryTrain:  epoch  4, batch     0 | loss: 0.4213252MemoryTrain:  epoch  5, batch     0 | loss: 0.3476183MemoryTrain:  epoch  6, batch     0 | loss: 0.3263723MemoryTrain:  epoch  7, batch     0 | loss: 0.2886891MemoryTrain:  epoch  8, batch     0 | loss: 0.2584670MemoryTrain:  epoch  9, batch     0 | loss: 0.2102297

F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 5: 0.0, 38: 0.6716417910447762, 40: 0.5074626865671642, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 39: 0.5333333333333333, 16: 0.0, 18: 0.0, 19: 0.0, 28: 0.0}
Micro-average F1 score: 0.5373134328358209
Weighted-average F1 score: 0.5000654057299708
F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 5: 0.0, 39: 0.0, 6: 0.8027210884353742, 10: 0.7577639751552795, 11: 0.0, 12: 0.0, 40: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.6666666666666666, 23: 0.0, 26: 0.3333333333333333, 28: 0.0}
Micro-average F1 score: 0.6921119592875318
Weighted-average F1 score: 0.6190180844951674
F1 score per class: {0: 0.0, 2: 0.875, 37: 0.0, 5: 0.0, 39: 0.8027210884353742, 40: 0.7757575757575758, 10: 0.0, 11: 0.0, 12: 0.0, 38: 0.0, 16: 0.0, 18: 0.6, 19: 0.0, 21: 0.0, 26: 0.125, 28: 0.0}
Micro-average F1 score: 0.7007672634271099
Weighted-average F1 score: 0.647440620011218

F1 score per class: {0: 0.9444444444444444, 2: 0.56, 4: 0.8888888888888888, 5: 0.8546255506607929, 6: 0.3484848484848485, 7: 0.07407407407407407, 9: 0.96, 10: 0.19469026548672566, 11: 0.4639175257731959, 12: 0.4358974358974359, 13: 0.2, 15: 0.7368421052631579, 16: 0.8275862068965517, 17: 0.0, 18: 0.08888888888888889, 19: 0.5157232704402516, 21: 0.32558139534883723, 23: 0.8604651162790697, 24: 0.10526315789473684, 25: 0.44776119402985076, 26: 0.7640449438202247, 27: 0.43478260869565216, 28: 0.21621621621621623, 29: 0.8901098901098901, 31: 0.6666666666666666, 32: 0.8306010928961749, 35: 0.42424242424242425, 37: 0.32, 38: 0.2564102564102564, 39: 0.0, 40: 0.5321100917431193}
Micro-average F1 score: 0.6039185555128698
Weighted-average F1 score: 0.6525508544238166
F1 score per class: {0: 0.972972972972973, 2: 0.6666666666666666, 4: 0.9473684210526315, 5: 0.8, 6: 0.5161290322580645, 7: 0.058823529411764705, 9: 0.9615384615384616, 10: 0.2689075630252101, 11: 0.5756097560975609, 12: 0.6069651741293532, 13: 0.16666666666666666, 15: 0.7058823529411765, 16: 0.8666666666666667, 17: 0.0, 18: 0.13333333333333333, 19: 0.6060606060606061, 21: 0.5333333333333333, 23: 0.8539325842696629, 24: 0.09523809523809523, 25: 0.5352112676056338, 26: 0.7379679144385026, 27: 0.4827586206896552, 28: 0.2926829268292683, 29: 0.8791208791208791, 31: 1.0, 32: 0.8768472906403941, 35: 0.7878787878787878, 37: 0.3157894736842105, 38: 0.32098765432098764, 39: 0.25, 40: 0.7172413793103448}
Micro-average F1 score: 0.6605191256830601
Weighted-average F1 score: 0.678311615161859
F1 score per class: {0: 0.972972972972973, 2: 0.5384615384615384, 4: 0.9130434782608695, 5: 0.819672131147541, 6: 0.47297297297297297, 7: 0.06451612903225806, 9: 0.96, 10: 0.2564102564102564, 11: 0.5700483091787439, 12: 0.5981308411214953, 13: 0.125, 15: 0.6666666666666666, 16: 0.8666666666666667, 17: 0.0, 18: 0.13333333333333333, 19: 0.593939393939394, 21: 0.5573770491803278, 23: 0.8192771084337349, 24: 0.09523809523809523, 25: 0.5142857142857142, 26: 0.7379679144385026, 27: 0.42857142857142855, 28: 0.23529411764705882, 29: 0.8839779005524862, 31: 0.6666666666666666, 32: 0.8640776699029126, 35: 0.6956521739130435, 37: 0.3157894736842105, 38: 0.325, 39: 0.10526315789473684, 40: 0.6615384615384615}
Micro-average F1 score: 0.6427586206896552
Weighted-average F1 score: 0.660096049716343

F1 score per class: {0: 0.0, 2: 0.2978723404255319, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.4712041884816754, 12: 0.40476190476190477, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.17777777777777778, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3005008347245409
Weighted-average F1 score: 0.24724938855623704
F1 score per class: {0: 0.0, 2: 0.1794871794871795, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.5221238938053098, 12: 0.5236051502145923, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.10256410256410256, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.20689655172413793, 40: 0.0}
Micro-average F1 score: 0.2625482625482625
Weighted-average F1 score: 0.2060711544230577
F1 score per class: {0: 0.0, 2: 0.208955223880597, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.5042735042735043, 12: 0.5333333333333333, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0967741935483871, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.08695652173913043, 40: 0.0}
Micro-average F1 score: 0.2781725888324873
Weighted-average F1 score: 0.22508931647903135

F1 score per class: {0: 0.5666666666666667, 2: 0.16279069767441862, 4: 0.8556149732620321, 5: 0.671280276816609, 6: 0.24468085106382978, 7: 0.0392156862745098, 9: 0.7272727272727273, 10: 0.16923076923076924, 11: 0.17751479289940827, 12: 0.1384928716904277, 13: 0.07692307692307693, 15: 0.4375, 16: 0.42857142857142855, 17: 0.0, 18: 0.045454545454545456, 19: 0.3961352657004831, 21: 0.2, 23: 0.8043478260869565, 24: 0.07407407407407407, 25: 0.4411764705882353, 26: 0.6974358974358974, 27: 0.12987012987012986, 28: 0.05970149253731343, 29: 0.7677725118483413, 31: 0.08333333333333333, 32: 0.6755555555555556, 35: 0.2413793103448276, 37: 0.2376237623762376, 38: 0.12048192771084337, 39: 0.0, 40: 0.36024844720496896}
Micro-average F1 score: 0.37410756782484533
Weighted-average F1 score: 0.3424542823562147
F1 score per class: {0: 0.3050847457627119, 2: 0.051094890510948905, 4: 0.8737864077669902, 5: 0.272108843537415, 6: 0.27586206896551724, 7: 0.02040816326530612, 9: 0.45454545454545453, 10: 0.16753926701570682, 11: 0.19666666666666666, 12: 0.14969325153374233, 13: 0.06060606060606061, 15: 0.35294117647058826, 16: 0.3795620437956204, 17: 0.0, 18: 0.05555555555555555, 19: 0.3861003861003861, 21: 0.11678832116788321, 23: 0.5277777777777778, 24: 0.06451612903225806, 25: 0.4810126582278481, 26: 0.6244343891402715, 27: 0.12612612612612611, 28: 0.04878048780487805, 29: 0.7407407407407407, 31: 0.07272727272727272, 32: 0.6095890410958904, 35: 0.203125, 37: 0.17142857142857143, 38: 0.08496732026143791, 39: 0.07058823529411765, 40: 0.35374149659863946}
Micro-average F1 score: 0.27479397556123897
Weighted-average F1 score: 0.24789120420739494
F1 score per class: {0: 0.34615384615384615, 2: 0.06306306306306306, 4: 0.8704663212435233, 5: 0.3472222222222222, 6: 0.28225806451612906, 7: 0.027777777777777776, 9: 0.64, 10: 0.16304347826086957, 11: 0.18641390205371247, 12: 0.14206437291897892, 13: 0.05128205128205128, 15: 0.32432432432432434, 16: 0.4, 17: 0.0, 18: 0.05825242718446602, 19: 0.40329218106995884, 21: 0.12546125461254612, 23: 0.6355140186915887, 24: 0.06896551724137931, 25: 0.4675324675324675, 26: 0.6540284360189573, 27: 0.11428571428571428, 28: 0.041379310344827586, 29: 0.7582938388625592, 31: 0.04081632653061224, 32: 0.5933333333333334, 35: 0.191044776119403, 37: 0.1951219512195122, 38: 0.08099688473520249, 39: 0.044444444444444446, 40: 0.3805309734513274}
Micro-average F1 score: 0.2830245976313392
Weighted-average F1 score: 0.25148063934997483
cur_acc_wo_na:  ['0.8140', '0.5767', '0.8473', '0.7124', '0.4615', '0.5373']
his_acc_wo_na:  ['0.8140', '0.6924', '0.7510', '0.7421', '0.6499', '0.6039']
cur_acc des_wo_na:  ['0.8464', '0.5753', '0.8694', '0.7082', '0.6471', '0.6921']
his_acc des_wo_na:  ['0.8464', '0.7288', '0.7675', '0.7481', '0.7142', '0.6605']
cur_acc rrf_wo_na:  ['0.8464', '0.5872', '0.8687', '0.7208', '0.6162', '0.7008']
his_acc rrf_wo_na:  ['0.8464', '0.7252', '0.7661', '0.7533', '0.7078', '0.6428']
cur_acc_w_na:  ['0.6808', '0.4351', '0.5539', '0.4904', '0.3186', '0.3005']
his_acc_w_na:  ['0.6808', '0.5530', '0.5065', '0.5329', '0.4381', '0.3741']
cur_acc des_w_na:  ['0.6704', '0.3490', '0.4212', '0.2848', '0.2962', '0.2625']
his_acc des_w_na:  ['0.6704', '0.5008', '0.4112', '0.3479', '0.3149', '0.2748']
cur_acc rrf_w_na:  ['0.6726', '0.3721', '0.4423', '0.3006', '0.2875', '0.2782']
his_acc rrf_w_na:  ['0.6726', '0.5072', '0.4292', '0.3651', '0.3202', '0.2830']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 97.9394642CurrentTrain: epoch  0, batch     1 | loss: 121.5527836CurrentTrain: epoch  0, batch     2 | loss: 95.9611041CurrentTrain: epoch  0, batch     3 | loss: 195.8160618CurrentTrain: epoch  0, batch     4 | loss: 113.6022381CurrentTrain: epoch  1, batch     0 | loss: 92.6789403CurrentTrain: epoch  1, batch     1 | loss: 182.1244462CurrentTrain: epoch  1, batch     2 | loss: 111.6178867CurrentTrain: epoch  1, batch     3 | loss: 140.1123185CurrentTrain: epoch  1, batch     4 | loss: 105.7588348CurrentTrain: epoch  2, batch     0 | loss: 135.1847086CurrentTrain: epoch  2, batch     1 | loss: 130.2547814CurrentTrain: epoch  2, batch     2 | loss: 133.8508373CurrentTrain: epoch  2, batch     3 | loss: 132.9220168CurrentTrain: epoch  2, batch     4 | loss: 97.9451634CurrentTrain: epoch  3, batch     0 | loss: 177.5433323CurrentTrain: epoch  3, batch     1 | loss: 103.3091142CurrentTrain: epoch  3, batch     2 | loss: 107.1282305CurrentTrain: epoch  3, batch     3 | loss: 105.1997171CurrentTrain: epoch  3, batch     4 | loss: 72.3836434CurrentTrain: epoch  4, batch     0 | loss: 79.6152089CurrentTrain: epoch  4, batch     1 | loss: 175.4034131CurrentTrain: epoch  4, batch     2 | loss: 132.9045631CurrentTrain: epoch  4, batch     3 | loss: 128.2901709CurrentTrain: epoch  4, batch     4 | loss: 98.3376495CurrentTrain: epoch  5, batch     0 | loss: 127.9084738CurrentTrain: epoch  5, batch     1 | loss: 101.9451367CurrentTrain: epoch  5, batch     2 | loss: 99.9676137CurrentTrain: epoch  5, batch     3 | loss: 82.7897503CurrentTrain: epoch  5, batch     4 | loss: 72.6545823CurrentTrain: epoch  6, batch     0 | loss: 127.2030505CurrentTrain: epoch  6, batch     1 | loss: 277.3206801CurrentTrain: epoch  6, batch     2 | loss: 99.2553279CurrentTrain: epoch  6, batch     3 | loss: 95.7108942CurrentTrain: epoch  6, batch     4 | loss: 71.3882587CurrentTrain: epoch  7, batch     0 | loss: 97.2303725CurrentTrain: epoch  7, batch     1 | loss: 130.3845387CurrentTrain: epoch  7, batch     2 | loss: 122.5154344CurrentTrain: epoch  7, batch     3 | loss: 126.1650389CurrentTrain: epoch  7, batch     4 | loss: 96.3773060CurrentTrain: epoch  8, batch     0 | loss: 173.5494974CurrentTrain: epoch  8, batch     1 | loss: 80.4426945CurrentTrain: epoch  8, batch     2 | loss: 560.1445313CurrentTrain: epoch  8, batch     3 | loss: 92.8381852CurrentTrain: epoch  8, batch     4 | loss: 55.0400717CurrentTrain: epoch  9, batch     0 | loss: 99.1260945CurrentTrain: epoch  9, batch     1 | loss: 178.0051459CurrentTrain: epoch  9, batch     2 | loss: 75.5757017CurrentTrain: epoch  9, batch     3 | loss: 101.6098344CurrentTrain: epoch  9, batch     4 | loss: 98.6829780
MemoryTrain:  epoch  0, batch     0 | loss: 1.0897325MemoryTrain:  epoch  1, batch     0 | loss: 1.0552055MemoryTrain:  epoch  2, batch     0 | loss: 0.7893193MemoryTrain:  epoch  3, batch     0 | loss: 0.6497733MemoryTrain:  epoch  4, batch     0 | loss: 0.5799599MemoryTrain:  epoch  5, batch     0 | loss: 0.4668367MemoryTrain:  epoch  6, batch     0 | loss: 0.4130443MemoryTrain:  epoch  7, batch     0 | loss: 0.3316345MemoryTrain:  epoch  8, batch     0 | loss: 0.3056811MemoryTrain:  epoch  9, batch     0 | loss: 0.2676713

F1 score per class: {32: 0.42735042735042733, 1: 0.5862068965517241, 34: 0.0, 3: 0.1839080459770115, 35: 0.0, 37: 0.0, 40: 0.7361963190184049, 11: 0.0, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.5897435897435898, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.4830917874396135
Weighted-average F1 score: 0.4160783722698819
F1 score per class: {32: 0.44036697247706424, 1: 0.8115942028985508, 34: 0.0, 3: 0.0, 35: 0.0, 37: 0.18604651162790697, 40: 0.0, 9: 0.0, 10: 0.7368421052631579, 11: 0.0, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.9215686274509803, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.591044776119403
Weighted-average F1 score: 0.5326231187042143
F1 score per class: {32: 0.4144144144144144, 1: 0.8029197080291971, 34: 0.0, 3: 0.0, 35: 0.1797752808988764, 37: 0.0, 38: 0.0, 40: 0.7142857142857143, 9: 0.0, 11: 0.0, 14: 0.0, 18: 0.0, 21: 0.9, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.5735735735735735
Weighted-average F1 score: 0.5134727890497519

F1 score per class: {0: 0.9142857142857143, 1: 0.373134328358209, 2: 0.5, 3: 0.5037037037037037, 4: 0.8636363636363636, 5: 0.8727272727272727, 6: 0.3089430894308943, 7: 0.07142857142857142, 9: 0.9615384615384616, 10: 0.19642857142857142, 11: 0.2550335570469799, 12: 0.3464566929133858, 13: 0.11764705882352941, 14: 0.17204301075268819, 15: 0.75, 16: 0.8275862068965517, 17: 0.0, 18: 0.08333333333333333, 19: 0.3387096774193548, 21: 0.21052631578947367, 22: 0.7100591715976331, 23: 0.9010989010989011, 24: 0.08, 25: 0.44776119402985076, 26: 0.7759562841530054, 27: 0.15384615384615385, 28: 0.2413793103448276, 29: 0.9081081081081082, 31: 0.6666666666666666, 32: 0.7934782608695652, 34: 0.34074074074074073, 35: 0.20224719101123595, 37: 0.2465753424657534, 38: 0.25, 39: 0.0, 40: 0.35789473684210527}
Micro-average F1 score: 0.5320412628946546
Weighted-average F1 score: 0.5703218070573154
F1 score per class: {0: 0.972972972972973, 1: 0.375, 2: 0.5384615384615384, 3: 0.6292134831460674, 4: 0.907103825136612, 5: 0.8081632653061225, 6: 0.48, 7: 0.04878048780487805, 9: 0.8928571428571429, 10: 0.3387096774193548, 11: 0.24193548387096775, 12: 0.6022727272727273, 13: 0.07692307692307693, 14: 0.1702127659574468, 15: 0.7058823529411765, 16: 0.8813559322033898, 17: 0.0, 18: 0.15384615384615385, 19: 0.4316546762589928, 21: 0.23809523809523808, 22: 0.6961325966850829, 23: 0.8351648351648352, 24: 0.07407407407407407, 25: 0.5352112676056338, 26: 0.7340425531914894, 27: 0.12903225806451613, 28: 0.21052631578947367, 29: 0.8972972972972973, 31: 0.6666666666666666, 32: 0.8686868686868687, 34: 0.4051724137931034, 35: 0.3333333333333333, 37: 0.2318840579710145, 38: 0.44776119402985076, 39: 0.11764705882352941, 40: 0.6933333333333334}
Micro-average F1 score: 0.5782463928967814
Weighted-average F1 score: 0.5888805124096516
F1 score per class: {0: 0.972972972972973, 1: 0.3458646616541353, 2: 0.5384615384615384, 3: 0.6395348837209303, 4: 0.8888888888888888, 5: 0.8319327731092437, 6: 0.37593984962406013, 7: 0.05128205128205128, 9: 0.9433962264150944, 10: 0.2711864406779661, 11: 0.22695035460992907, 12: 0.5792349726775956, 13: 0.08333333333333333, 14: 0.16, 15: 0.7058823529411765, 16: 0.8813559322033898, 17: 0.0, 18: 0.12, 19: 0.4689655172413793, 21: 0.2, 22: 0.6857142857142857, 23: 0.8409090909090909, 24: 0.08333333333333333, 25: 0.5352112676056338, 26: 0.7379679144385026, 27: 0.12903225806451613, 28: 0.16666666666666666, 29: 0.9021739130434783, 31: 0.6666666666666666, 32: 0.85, 34: 0.41284403669724773, 35: 0.35555555555555557, 37: 0.2318840579710145, 38: 0.38961038961038963, 39: 0.125, 40: 0.5982905982905983}
Micro-average F1 score: 0.5628338487489457
Weighted-average F1 score: 0.5724895101385541

F1 score per class: {0: 0.0, 1: 0.1524390243902439, 2: 0.0, 3: 0.44155844155844154, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.11594202898550725, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5581395348837209, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.45098039215686275, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.23715415019762845
Weighted-average F1 score: 0.18359935994027773
F1 score per class: {0: 0.0, 1: 0.14906832298136646, 2: 0.0, 3: 0.39436619718309857, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.11594202898550725, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.50199203187251, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.4069264069264069, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2087506589351608
Weighted-average F1 score: 0.17846486760263894
F1 score per class: {0: 0.0, 1: 0.1377245508982036, 2: 0.0, 3: 0.39855072463768115, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.10526315789473684, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.45, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.21012101210121012
Weighted-average F1 score: 0.1776153522377732

F1 score per class: {0: 0.5079365079365079, 1: 0.11312217194570136, 2: 0.17647058823529413, 3: 0.2821576763485477, 4: 0.8306010928961749, 5: 0.7218045112781954, 6: 0.20994475138121546, 7: 0.034482758620689655, 9: 0.6578947368421053, 10: 0.1746031746031746, 11: 0.10644257703081232, 12: 0.15770609318996415, 13: 0.045454545454545456, 14: 0.06986899563318777, 15: 0.6666666666666666, 16: 0.45714285714285713, 17: 0.0, 18: 0.0380952380952381, 19: 0.2781456953642384, 21: 0.16326530612244897, 22: 0.502092050209205, 23: 0.780952380952381, 24: 0.045454545454545456, 25: 0.4411764705882353, 26: 0.6995073891625616, 27: 0.08888888888888889, 28: 0.06086956521739131, 29: 0.717948717948718, 31: 0.11764705882352941, 32: 0.553030303030303, 34: 0.18775510204081633, 35: 0.11042944785276074, 37: 0.1836734693877551, 38: 0.0916030534351145, 39: 0.0, 40: 0.288135593220339}
Micro-average F1 score: 0.318905752295297
Weighted-average F1 score: 0.29002643159417046
F1 score per class: {0: 0.38095238095238093, 1: 0.09795918367346938, 2: 0.07035175879396985, 3: 0.19343696027633853, 4: 0.83, 5: 0.27461858529819694, 6: 0.2553191489361702, 7: 0.017241379310344827, 9: 0.33557046979865773, 10: 0.208955223880597, 11: 0.07751937984496124, 12: 0.15703703703703703, 13: 0.02127659574468085, 14: 0.0653061224489796, 15: 0.41379310344827586, 16: 0.42276422764227645, 17: 0.0, 18: 0.0625, 19: 0.26548672566371684, 21: 0.07407407407407407, 22: 0.42857142857142855, 23: 0.5066666666666667, 24: 0.038461538461538464, 25: 0.4523809523809524, 26: 0.6079295154185022, 27: 0.05405405405405406, 28: 0.03498542274052478, 29: 0.70042194092827, 31: 0.044444444444444446, 32: 0.49283667621776506, 34: 0.09552845528455285, 35: 0.11029411764705882, 37: 0.1322314049586777, 38: 0.1079136690647482, 39: 0.05263157894736842, 40: 0.36619718309859156}
Micro-average F1 score: 0.23093971631205673
Weighted-average F1 score: 0.2087747740974284
F1 score per class: {0: 0.3829787234042553, 1: 0.09145129224652088, 2: 0.09271523178807947, 3: 0.20220588235294118, 4: 0.851063829787234, 5: 0.38596491228070173, 6: 0.20920502092050208, 7: 0.022988505747126436, 9: 0.5376344086021505, 10: 0.18181818181818182, 11: 0.06570841889117043, 12: 0.14500683994528044, 13: 0.024390243902439025, 14: 0.056338028169014086, 15: 0.46153846153846156, 16: 0.41935483870967744, 17: 0.0, 18: 0.049586776859504134, 19: 0.3063063063063063, 21: 0.08080808080808081, 22: 0.43010752688172044, 23: 0.6548672566371682, 24: 0.04081632653061224, 25: 0.475, 26: 0.6359447004608295, 27: 0.056338028169014086, 28: 0.03007518796992481, 29: 0.7094017094017094, 31: 0.05128205128205128, 32: 0.4956268221574344, 34: 0.10250569476082004, 35: 0.12030075187969924, 37: 0.13333333333333333, 38: 0.10238907849829351, 39: 0.06896551724137931, 40: 0.35353535353535354}
Micro-average F1 score: 0.23602923838717282
Weighted-average F1 score: 0.20958649157705583
cur_acc_wo_na:  ['0.8140', '0.5767', '0.8473', '0.7124', '0.4615', '0.5373', '0.4831']
his_acc_wo_na:  ['0.8140', '0.6924', '0.7510', '0.7421', '0.6499', '0.6039', '0.5320']
cur_acc des_wo_na:  ['0.8464', '0.5753', '0.8694', '0.7082', '0.6471', '0.6921', '0.5910']
his_acc des_wo_na:  ['0.8464', '0.7288', '0.7675', '0.7481', '0.7142', '0.6605', '0.5782']
cur_acc rrf_wo_na:  ['0.8464', '0.5872', '0.8687', '0.7208', '0.6162', '0.7008', '0.5736']
his_acc rrf_wo_na:  ['0.8464', '0.7252', '0.7661', '0.7533', '0.7078', '0.6428', '0.5628']
cur_acc_w_na:  ['0.6808', '0.4351', '0.5539', '0.4904', '0.3186', '0.3005', '0.2372']
his_acc_w_na:  ['0.6808', '0.5530', '0.5065', '0.5329', '0.4381', '0.3741', '0.3189']
cur_acc des_w_na:  ['0.6704', '0.3490', '0.4212', '0.2848', '0.2962', '0.2625', '0.2088']
his_acc des_w_na:  ['0.6704', '0.5008', '0.4112', '0.3479', '0.3149', '0.2748', '0.2309']
cur_acc rrf_w_na:  ['0.6726', '0.3721', '0.4423', '0.3006', '0.2875', '0.2782', '0.2101']
his_acc rrf_w_na:  ['0.6726', '0.5072', '0.4292', '0.3651', '0.3202', '0.2830', '0.2360']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 114.5052614CurrentTrain: epoch  0, batch     1 | loss: 122.7382523CurrentTrain: epoch  0, batch     2 | loss: 90.4358145CurrentTrain: epoch  0, batch     3 | loss: 81.3713297CurrentTrain: epoch  1, batch     0 | loss: 107.6457369CurrentTrain: epoch  1, batch     1 | loss: 139.8971867CurrentTrain: epoch  1, batch     2 | loss: 102.8459676CurrentTrain: epoch  1, batch     3 | loss: 106.2333938CurrentTrain: epoch  2, batch     0 | loss: 86.5800685CurrentTrain: epoch  2, batch     1 | loss: 101.8747690CurrentTrain: epoch  2, batch     2 | loss: 96.7567205CurrentTrain: epoch  2, batch     3 | loss: 112.0555106CurrentTrain: epoch  3, batch     0 | loss: 103.6206133CurrentTrain: epoch  3, batch     1 | loss: 133.5088192CurrentTrain: epoch  3, batch     2 | loss: 96.4002542CurrentTrain: epoch  3, batch     3 | loss: 61.3298894CurrentTrain: epoch  4, batch     0 | loss: 130.3123797CurrentTrain: epoch  4, batch     1 | loss: 129.8468881CurrentTrain: epoch  4, batch     2 | loss: 75.0260135CurrentTrain: epoch  4, batch     3 | loss: 78.5909373CurrentTrain: epoch  5, batch     0 | loss: 77.7769191CurrentTrain: epoch  5, batch     1 | loss: 98.0438007CurrentTrain: epoch  5, batch     2 | loss: 98.0274766CurrentTrain: epoch  5, batch     3 | loss: 78.1238370CurrentTrain: epoch  6, batch     0 | loss: 96.9808089CurrentTrain: epoch  6, batch     1 | loss: 100.1933112CurrentTrain: epoch  6, batch     2 | loss: 77.9358224CurrentTrain: epoch  6, batch     3 | loss: 58.3701744CurrentTrain: epoch  7, batch     0 | loss: 120.9400758CurrentTrain: epoch  7, batch     1 | loss: 123.6370824CurrentTrain: epoch  7, batch     2 | loss: 76.7498323CurrentTrain: epoch  7, batch     3 | loss: 77.2709256CurrentTrain: epoch  8, batch     0 | loss: 96.0468482CurrentTrain: epoch  8, batch     1 | loss: 92.6979920CurrentTrain: epoch  8, batch     2 | loss: 176.5986768CurrentTrain: epoch  8, batch     3 | loss: 45.0859716CurrentTrain: epoch  9, batch     0 | loss: 96.6070328CurrentTrain: epoch  9, batch     1 | loss: 94.5075198CurrentTrain: epoch  9, batch     2 | loss: 96.5176840CurrentTrain: epoch  9, batch     3 | loss: 57.2450898
MemoryTrain:  epoch  0, batch     0 | loss: 0.7652045MemoryTrain:  epoch  1, batch     0 | loss: 0.6253588MemoryTrain:  epoch  2, batch     0 | loss: 0.5042899MemoryTrain:  epoch  3, batch     0 | loss: 0.4010566MemoryTrain:  epoch  4, batch     0 | loss: 0.3706972MemoryTrain:  epoch  5, batch     0 | loss: 0.3070879MemoryTrain:  epoch  6, batch     0 | loss: 0.2418039MemoryTrain:  epoch  7, batch     0 | loss: 0.2437924MemoryTrain:  epoch  8, batch     0 | loss: 0.1904783MemoryTrain:  epoch  9, batch     0 | loss: 0.2113258

F1 score per class: {33: 0.0, 34: 0.45714285714285713, 36: 0.0, 37: 0.0, 6: 0.0, 39: 0.8131868131868132, 8: 0.0, 38: 0.0, 11: 0.0, 12: 0.972972972972973, 18: 0.42857142857142855, 20: 0.0, 26: 0.5, 28: 0.0, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5652173913043478
Weighted-average F1 score: 0.5172158281647332
F1 score per class: {2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.7142857142857143, 10: 0.0, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.875, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.5333333333333333, 34: 0.0, 35: 0.0, 36: 0.8852459016393442, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7610208816705336
Weighted-average F1 score: 0.6979998048399687
F1 score per class: {2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.6386554621848739, 10: 0.0, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.875, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.5333333333333333, 34: 0.0, 36: 0.7927927927927928, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.6901408450704225
Weighted-average F1 score: 0.6042457260104318

F1 score per class: {0: 0.8656716417910447, 1: 0.37681159420289856, 2: 0.75, 3: 0.43902439024390244, 4: 0.907103825136612, 5: 0.8508771929824561, 6: 0.3053435114503817, 7: 0.0, 8: 0.3870967741935484, 9: 0.9615384615384616, 10: 0.07692307692307693, 11: 0.18840579710144928, 12: 0.272, 13: 0.16666666666666666, 14: 0.16279069767441862, 15: 0.75, 16: 0.7857142857142857, 17: 0.0, 18: 0.11764705882352941, 19: 0.3582089552238806, 20: 0.7956989247311828, 21: 0.15789473684210525, 22: 0.6941176470588235, 23: 0.8372093023255814, 24: 0.08, 25: 0.47058823529411764, 26: 0.7608695652173914, 27: 0.16, 28: 0.21052631578947367, 29: 0.8972972972972973, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.768361581920904, 33: 0.21428571428571427, 34: 0.43243243243243246, 35: 0.1927710843373494, 36: 0.4888888888888889, 37: 0.379746835443038, 38: 0.3157894736842105, 39: 0.09523809523809523, 40: 0.34782608695652173}
Micro-average F1 score: 0.5332948976650331
Weighted-average F1 score: 0.5855141965722597
F1 score per class: {0: 0.958904109589041, 1: 0.4032258064516129, 2: 0.6, 3: 0.4935064935064935, 4: 0.9583333333333334, 5: 0.8064516129032258, 6: 0.45161290322580644, 7: 0.06896551724137931, 8: 0.4945054945054945, 9: 0.8928571428571429, 10: 0.14545454545454545, 11: 0.3287671232876712, 12: 0.5121951219512195, 13: 0.2222222222222222, 14: 0.20224719101123595, 15: 0.6666666666666666, 16: 0.896551724137931, 17: 0.0, 18: 0.25, 19: 0.46357615894039733, 20: 0.865979381443299, 21: 0.2857142857142857, 22: 0.7653061224489796, 23: 0.8045977011494253, 24: 0.08, 25: 0.5, 26: 0.7263157894736842, 27: 0.1111111111111111, 28: 0.09523809523809523, 29: 0.8877005347593583, 30: 0.76, 31: 0.5, 32: 0.8272251308900523, 33: 0.25, 34: 0.5432098765432098, 35: 0.34782608695652173, 36: 0.8, 37: 0.2535211267605634, 38: 0.35443037974683544, 39: 0.08333333333333333, 40: 0.6956521739130435}
Micro-average F1 score: 0.5922038980509745
Weighted-average F1 score: 0.6128671580398753
F1 score per class: {0: 0.958904109589041, 1: 0.3875968992248062, 2: 0.5714285714285714, 3: 0.5, 4: 0.9247311827956989, 5: 0.8097165991902834, 6: 0.40789473684210525, 7: 0.06666666666666667, 8: 0.48717948717948717, 9: 0.9259259259259259, 10: 0.14545454545454545, 11: 0.2981366459627329, 12: 0.5263157894736842, 13: 0.2222222222222222, 14: 0.1797752808988764, 15: 0.6666666666666666, 16: 0.896551724137931, 17: 0.0, 18: 0.25, 19: 0.46540880503144655, 20: 0.8571428571428571, 21: 0.24390243902439024, 22: 0.78125, 23: 0.7951807228915663, 24: 0.08, 25: 0.4788732394366197, 26: 0.7263157894736842, 27: 0.1111111111111111, 28: 0.07692307692307693, 29: 0.8972972972972973, 30: 0.8444444444444444, 31: 0.6666666666666666, 32: 0.8210526315789474, 33: 0.22857142857142856, 34: 0.5398773006134969, 35: 0.29906542056074764, 36: 0.7457627118644068, 37: 0.3013698630136986, 38: 0.35294117647058826, 39: 0.16, 40: 0.6218487394957983}
Micro-average F1 score: 0.5800252844500632
Weighted-average F1 score: 0.5965273600896347

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.4067796610169492, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6324786324786325, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.9473684210526315, 31: 0.0, 32: 0.0, 33: 0.42857142857142855, 34: 0.0, 35: 0.0, 36: 0.46808510638297873, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.36684303350970016
Weighted-average F1 score: 0.25978898383398413
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5960264900662252, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5637583892617449, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.9047619047619048, 31: 0.0, 32: 0.0, 33: 0.42105263157894735, 34: 0.0, 35: 0.0, 36: 0.6352941176470588, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.32315270935960594
Weighted-average F1 score: 0.23664213569126405
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.562962962962963, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5562913907284768, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.926829268292683, 31: 0.0, 32: 0.0, 33: 0.4, 34: 0.0, 35: 0.0, 36: 0.6330935251798561, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.30849947534102834
Weighted-average F1 score: 0.21408079797954396

F1 score per class: {0: 0.46774193548387094, 1: 0.12037037037037036, 2: 0.27906976744186046, 3: 0.23788546255506607, 4: 0.8736842105263158, 5: 0.6006191950464397, 6: 0.20512820512820512, 7: 0.0, 8: 0.26519337016574585, 9: 0.6329113924050633, 10: 0.06956521739130435, 11: 0.10077519379844961, 12: 0.14049586776859505, 13: 0.04878048780487805, 14: 0.08139534883720931, 15: 0.5, 16: 0.42718446601941745, 17: 0.0, 18: 0.0594059405940594, 19: 0.2823529411764706, 20: 0.31092436974789917, 21: 0.12, 22: 0.44696969696969696, 23: 0.7128712871287128, 24: 0.045454545454545456, 25: 0.4444444444444444, 26: 0.6730769230769231, 27: 0.0784313725490196, 28: 0.08163265306122448, 29: 0.7094017094017094, 30: 0.9473684210526315, 31: 0.07407407407407407, 32: 0.5291828793774319, 33: 0.13333333333333333, 34: 0.25668449197860965, 35: 0.10596026490066225, 36: 0.4, 37: 0.30927835051546393, 38: 0.14457831325301204, 39: 0.05263157894736842, 40: 0.26666666666666666}
Micro-average F1 score: 0.33423667570009036
Weighted-average F1 score: 0.3204202913846316
F1 score per class: {0: 0.30701754385964913, 1: 0.10204081632653061, 2: 0.07947019867549669, 3: 0.14960629921259844, 4: 0.8932038834951457, 5: 0.2304147465437788, 6: 0.24390243902439024, 7: 0.037037037037037035, 8: 0.2452316076294278, 9: 0.3048780487804878, 10: 0.07881773399014778, 11: 0.10786516853932585, 12: 0.13680781758957655, 13: 0.04, 14: 0.0782608695652174, 15: 0.36363636363636365, 16: 0.40625, 17: 0.0, 18: 0.0915032679738562, 19: 0.2545454545454545, 20: 0.18834080717488788, 21: 0.09448818897637795, 22: 0.43103448275862066, 23: 0.4605263157894737, 24: 0.04, 25: 0.3829787234042553, 26: 0.5872340425531914, 27: 0.039603960396039604, 28: 0.01904761904761905, 29: 0.6859504132231405, 30: 0.3275862068965517, 31: 0.01694915254237288, 32: 0.46607669616519176, 33: 0.07407407407407407, 34: 0.21256038647342995, 35: 0.10443864229765012, 36: 0.3517915309446254, 37: 0.1592920353982301, 38: 0.08383233532934131, 39: 0.034482758620689655, 40: 0.34532374100719426}
Micro-average F1 score: 0.2380235010545345
Weighted-average F1 score: 0.22200574125180947
F1 score per class: {0: 0.3111111111111111, 1: 0.09784735812133072, 2: 0.13793103448275862, 3: 0.15476190476190477, 4: 0.8775510204081632, 5: 0.2857142857142857, 6: 0.2339622641509434, 7: 0.037037037037037035, 8: 0.2846441947565543, 9: 0.46296296296296297, 10: 0.08290155440414508, 11: 0.09448818897637795, 12: 0.13119533527696792, 13: 0.03773584905660377, 14: 0.06808510638297872, 15: 0.36363636363636365, 16: 0.3969465648854962, 17: 0.0, 18: 0.09655172413793103, 19: 0.275092936802974, 20: 0.1794871794871795, 21: 0.08403361344537816, 22: 0.46439628482972134, 23: 0.584070796460177, 24: 0.041666666666666664, 25: 0.3953488372093023, 26: 0.5948275862068966, 27: 0.041237113402061855, 28: 0.015267175572519083, 29: 0.70042194092827, 30: 0.42696629213483145, 31: 0.022727272727272728, 32: 0.46153846153846156, 33: 0.08080808080808081, 34: 0.21256038647342995, 35: 0.10561056105610561, 36: 0.3776824034334764, 37: 0.1981981981981982, 38: 0.08797653958944282, 39: 0.0784313725490196, 40: 0.35406698564593303}
Micro-average F1 score: 0.2461901695642842
Weighted-average F1 score: 0.22708108983912012
cur_acc_wo_na:  ['0.8140', '0.5767', '0.8473', '0.7124', '0.4615', '0.5373', '0.4831', '0.5652']
his_acc_wo_na:  ['0.8140', '0.6924', '0.7510', '0.7421', '0.6499', '0.6039', '0.5320', '0.5333']
cur_acc des_wo_na:  ['0.8464', '0.5753', '0.8694', '0.7082', '0.6471', '0.6921', '0.5910', '0.7610']
his_acc des_wo_na:  ['0.8464', '0.7288', '0.7675', '0.7481', '0.7142', '0.6605', '0.5782', '0.5922']
cur_acc rrf_wo_na:  ['0.8464', '0.5872', '0.8687', '0.7208', '0.6162', '0.7008', '0.5736', '0.6901']
his_acc rrf_wo_na:  ['0.8464', '0.7252', '0.7661', '0.7533', '0.7078', '0.6428', '0.5628', '0.5800']
cur_acc_w_na:  ['0.6808', '0.4351', '0.5539', '0.4904', '0.3186', '0.3005', '0.2372', '0.3668']
his_acc_w_na:  ['0.6808', '0.5530', '0.5065', '0.5329', '0.4381', '0.3741', '0.3189', '0.3342']
cur_acc des_w_na:  ['0.6704', '0.3490', '0.4212', '0.2848', '0.2962', '0.2625', '0.2088', '0.3232']
his_acc des_w_na:  ['0.6704', '0.5008', '0.4112', '0.3479', '0.3149', '0.2748', '0.2309', '0.2380']
cur_acc rrf_w_na:  ['0.6726', '0.3721', '0.4423', '0.3006', '0.2875', '0.2782', '0.2101', '0.3085']
his_acc rrf_w_na:  ['0.6726', '0.5072', '0.4292', '0.3651', '0.3202', '0.2830', '0.2360', '0.2462']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 111.9505488CurrentTrain: epoch  0, batch     1 | loss: 124.9246115CurrentTrain: epoch  0, batch     2 | loss: 87.4830686CurrentTrain: epoch  0, batch     3 | loss: 99.9522829CurrentTrain: epoch  0, batch     4 | loss: 147.1283784CurrentTrain: epoch  0, batch     5 | loss: 193.9139398CurrentTrain: epoch  0, batch     6 | loss: 119.9243030CurrentTrain: epoch  0, batch     7 | loss: 145.7906125CurrentTrain: epoch  0, batch     8 | loss: 85.7417985CurrentTrain: epoch  0, batch     9 | loss: 119.4975353CurrentTrain: epoch  0, batch    10 | loss: 117.8914830CurrentTrain: epoch  0, batch    11 | loss: 98.7678622CurrentTrain: epoch  0, batch    12 | loss: 116.9785432CurrentTrain: epoch  0, batch    13 | loss: 98.1127119CurrentTrain: epoch  0, batch    14 | loss: 85.2636688CurrentTrain: epoch  0, batch    15 | loss: 116.6400215CurrentTrain: epoch  0, batch    16 | loss: 116.2273179CurrentTrain: epoch  0, batch    17 | loss: 281.5389505CurrentTrain: epoch  0, batch    18 | loss: 189.5397504CurrentTrain: epoch  0, batch    19 | loss: 115.2867618CurrentTrain: epoch  0, batch    20 | loss: 96.8224618CurrentTrain: epoch  0, batch    21 | loss: 115.1001969CurrentTrain: epoch  0, batch    22 | loss: 143.3216498CurrentTrain: epoch  0, batch    23 | loss: 281.6343722CurrentTrain: epoch  0, batch    24 | loss: 142.8026377CurrentTrain: epoch  0, batch    25 | loss: 96.5747501CurrentTrain: epoch  0, batch    26 | loss: 141.7699399CurrentTrain: epoch  0, batch    27 | loss: 142.0747261CurrentTrain: epoch  0, batch    28 | loss: 114.9704433CurrentTrain: epoch  0, batch    29 | loss: 188.3193251CurrentTrain: epoch  0, batch    30 | loss: 141.9271031CurrentTrain: epoch  0, batch    31 | loss: 85.4795431CurrentTrain: epoch  0, batch    32 | loss: 142.8039565CurrentTrain: epoch  0, batch    33 | loss: 116.2515188CurrentTrain: epoch  0, batch    34 | loss: 187.7740712CurrentTrain: epoch  0, batch    35 | loss: 142.3026288CurrentTrain: epoch  0, batch    36 | loss: 97.0553383CurrentTrain: epoch  0, batch    37 | loss: 143.1135382CurrentTrain: epoch  0, batch    38 | loss: 142.2869092CurrentTrain: epoch  0, batch    39 | loss: 188.2655747CurrentTrain: epoch  0, batch    40 | loss: 114.5126622CurrentTrain: epoch  0, batch    41 | loss: 112.9709524CurrentTrain: epoch  0, batch    42 | loss: 83.3433089CurrentTrain: epoch  0, batch    43 | loss: 82.8759774CurrentTrain: epoch  0, batch    44 | loss: 113.7512445CurrentTrain: epoch  0, batch    45 | loss: 140.9078811CurrentTrain: epoch  0, batch    46 | loss: 95.6185973CurrentTrain: epoch  0, batch    47 | loss: 83.2697628CurrentTrain: epoch  0, batch    48 | loss: 141.1826941CurrentTrain: epoch  0, batch    49 | loss: 113.6074787CurrentTrain: epoch  0, batch    50 | loss: 113.8545998CurrentTrain: epoch  0, batch    51 | loss: 113.8902714CurrentTrain: epoch  0, batch    52 | loss: 142.3552047CurrentTrain: epoch  0, batch    53 | loss: 140.5325828CurrentTrain: epoch  0, batch    54 | loss: 112.7620315CurrentTrain: epoch  0, batch    55 | loss: 139.8724406CurrentTrain: epoch  0, batch    56 | loss: 112.9117089CurrentTrain: epoch  0, batch    57 | loss: 139.4033980CurrentTrain: epoch  0, batch    58 | loss: 92.8219835CurrentTrain: epoch  0, batch    59 | loss: 186.4551567CurrentTrain: epoch  0, batch    60 | loss: 112.5178839CurrentTrain: epoch  0, batch    61 | loss: 111.8627516CurrentTrain: epoch  0, batch    62 | loss: 113.1876005CurrentTrain: epoch  0, batch    63 | loss: 112.0676460CurrentTrain: epoch  0, batch    64 | loss: 113.1492830CurrentTrain: epoch  0, batch    65 | loss: 93.5574276CurrentTrain: epoch  0, batch    66 | loss: 111.1579014CurrentTrain: epoch  0, batch    67 | loss: 90.6763116CurrentTrain: epoch  0, batch    68 | loss: 136.8432385CurrentTrain: epoch  0, batch    69 | loss: 139.2562889CurrentTrain: epoch  0, batch    70 | loss: 94.0263920CurrentTrain: epoch  0, batch    71 | loss: 139.7138999CurrentTrain: epoch  0, batch    72 | loss: 140.8826562CurrentTrain: epoch  0, batch    73 | loss: 138.7025071CurrentTrain: epoch  0, batch    74 | loss: 92.2151105CurrentTrain: epoch  0, batch    75 | loss: 111.6518416CurrentTrain: epoch  0, batch    76 | loss: 108.4535611CurrentTrain: epoch  0, batch    77 | loss: 92.0873446CurrentTrain: epoch  0, batch    78 | loss: 110.2903608CurrentTrain: epoch  0, batch    79 | loss: 108.0580533CurrentTrain: epoch  0, batch    80 | loss: 87.6941123CurrentTrain: epoch  0, batch    81 | loss: 109.3563110CurrentTrain: epoch  0, batch    82 | loss: 281.4360512CurrentTrain: epoch  0, batch    83 | loss: 90.9284795CurrentTrain: epoch  0, batch    84 | loss: 91.3985541CurrentTrain: epoch  0, batch    85 | loss: 88.3502108CurrentTrain: epoch  0, batch    86 | loss: 91.9462087CurrentTrain: epoch  0, batch    87 | loss: 183.2754759CurrentTrain: epoch  0, batch    88 | loss: 187.8826408CurrentTrain: epoch  0, batch    89 | loss: 91.3622533CurrentTrain: epoch  0, batch    90 | loss: 90.0837848CurrentTrain: epoch  0, batch    91 | loss: 90.5244219CurrentTrain: epoch  0, batch    92 | loss: 108.6761151CurrentTrain: epoch  0, batch    93 | loss: 89.1993695CurrentTrain: epoch  0, batch    94 | loss: 75.8421276CurrentTrain: epoch  0, batch    95 | loss: 95.2150844CurrentTrain: epoch  1, batch     0 | loss: 133.5845705CurrentTrain: epoch  1, batch     1 | loss: 108.0455965CurrentTrain: epoch  1, batch     2 | loss: 107.0233597CurrentTrain: epoch  1, batch     3 | loss: 135.2513068CurrentTrain: epoch  1, batch     4 | loss: 270.0091588CurrentTrain: epoch  1, batch     5 | loss: 89.1762096CurrentTrain: epoch  1, batch     6 | loss: 276.1416283CurrentTrain: epoch  1, batch     7 | loss: 107.4351746CurrentTrain: epoch  1, batch     8 | loss: 109.7118512CurrentTrain: epoch  1, batch     9 | loss: 133.6575360CurrentTrain: epoch  1, batch    10 | loss: 83.2693436CurrentTrain: epoch  1, batch    11 | loss: 131.6181130CurrentTrain: epoch  1, batch    12 | loss: 182.1291309CurrentTrain: epoch  1, batch    13 | loss: 131.7608035CurrentTrain: epoch  1, batch    14 | loss: 85.8217509CurrentTrain: epoch  1, batch    15 | loss: 134.6040246CurrentTrain: epoch  1, batch    16 | loss: 105.2569477CurrentTrain: epoch  1, batch    17 | loss: 88.6104925CurrentTrain: epoch  1, batch    18 | loss: 135.3208607CurrentTrain: epoch  1, batch    19 | loss: 109.7851137CurrentTrain: epoch  1, batch    20 | loss: 134.8019323CurrentTrain: epoch  1, batch    21 | loss: 87.2968302CurrentTrain: epoch  1, batch    22 | loss: 90.8891762CurrentTrain: epoch  1, batch    23 | loss: 111.3290026CurrentTrain: epoch  1, batch    24 | loss: 105.7132472CurrentTrain: epoch  1, batch    25 | loss: 73.8469140CurrentTrain: epoch  1, batch    26 | loss: 89.3247487CurrentTrain: epoch  1, batch    27 | loss: 77.7965421CurrentTrain: epoch  1, batch    28 | loss: 131.0049980CurrentTrain: epoch  1, batch    29 | loss: 131.7822465CurrentTrain: epoch  1, batch    30 | loss: 85.9641488CurrentTrain: epoch  1, batch    31 | loss: 182.5727965CurrentTrain: epoch  1, batch    32 | loss: 89.1971119CurrentTrain: epoch  1, batch    33 | loss: 106.9251968CurrentTrain: epoch  1, batch    34 | loss: 133.6311419CurrentTrain: epoch  1, batch    35 | loss: 179.4579773CurrentTrain: epoch  1, batch    36 | loss: 108.2247995CurrentTrain: epoch  1, batch    37 | loss: 102.4590121CurrentTrain: epoch  1, batch    38 | loss: 85.9966878CurrentTrain: epoch  1, batch    39 | loss: 134.0613743CurrentTrain: epoch  1, batch    40 | loss: 85.9205177CurrentTrain: epoch  1, batch    41 | loss: 87.7729095CurrentTrain: epoch  1, batch    42 | loss: 183.2501994CurrentTrain: epoch  1, batch    43 | loss: 107.7992896CurrentTrain: epoch  1, batch    44 | loss: 102.5293824CurrentTrain: epoch  1, batch    45 | loss: 103.8598255CurrentTrain: epoch  1, batch    46 | loss: 182.5827631CurrentTrain: epoch  1, batch    47 | loss: 128.2324467CurrentTrain: epoch  1, batch    48 | loss: 84.3153772CurrentTrain: epoch  1, batch    49 | loss: 107.9354541CurrentTrain: epoch  1, batch    50 | loss: 131.1732228CurrentTrain: epoch  1, batch    51 | loss: 183.1457950CurrentTrain: epoch  1, batch    52 | loss: 72.4679665CurrentTrain: epoch  1, batch    53 | loss: 72.7785037CurrentTrain: epoch  1, batch    54 | loss: 87.9970587CurrentTrain: epoch  1, batch    55 | loss: 85.5667248CurrentTrain: epoch  1, batch    56 | loss: 177.3335480CurrentTrain: epoch  1, batch    57 | loss: 107.8576491CurrentTrain: epoch  1, batch    58 | loss: 112.8447081CurrentTrain: epoch  1, batch    59 | loss: 180.6905372CurrentTrain: epoch  1, batch    60 | loss: 101.8023077CurrentTrain: epoch  1, batch    61 | loss: 108.9640183CurrentTrain: epoch  1, batch    62 | loss: 105.9943050CurrentTrain: epoch  1, batch    63 | loss: 103.3212849CurrentTrain: epoch  1, batch    64 | loss: 104.5013522CurrentTrain: epoch  1, batch    65 | loss: 98.9373712CurrentTrain: epoch  1, batch    66 | loss: 88.6280082CurrentTrain: epoch  1, batch    67 | loss: 131.8625236CurrentTrain: epoch  1, batch    68 | loss: 87.1336406CurrentTrain: epoch  1, batch    69 | loss: 183.2363962CurrentTrain: epoch  1, batch    70 | loss: 102.6499804CurrentTrain: epoch  1, batch    71 | loss: 135.4344005CurrentTrain: epoch  1, batch    72 | loss: 103.9370317CurrentTrain: epoch  1, batch    73 | loss: 126.8317498CurrentTrain: epoch  1, batch    74 | loss: 88.3089262CurrentTrain: epoch  1, batch    75 | loss: 86.1276883CurrentTrain: epoch  1, batch    76 | loss: 86.3604560CurrentTrain: epoch  1, batch    77 | loss: 85.9894433CurrentTrain: epoch  1, batch    78 | loss: 84.7486780CurrentTrain: epoch  1, batch    79 | loss: 100.9351031CurrentTrain: epoch  1, batch    80 | loss: 131.6444122CurrentTrain: epoch  1, batch    81 | loss: 81.8789259CurrentTrain: epoch  1, batch    82 | loss: 99.8233725CurrentTrain: epoch  1, batch    83 | loss: 134.3305878CurrentTrain: epoch  1, batch    84 | loss: 105.2724789CurrentTrain: epoch  1, batch    85 | loss: 130.6242578CurrentTrain: epoch  1, batch    86 | loss: 84.7279845CurrentTrain: epoch  1, batch    87 | loss: 86.8091860CurrentTrain: epoch  1, batch    88 | loss: 86.4652392CurrentTrain: epoch  1, batch    89 | loss: 81.3672123CurrentTrain: epoch  1, batch    90 | loss: 177.8366406CurrentTrain: epoch  1, batch    91 | loss: 132.4403995CurrentTrain: epoch  1, batch    92 | loss: 100.5641562CurrentTrain: epoch  1, batch    93 | loss: 103.2693668CurrentTrain: epoch  1, batch    94 | loss: 81.4397183CurrentTrain: epoch  1, batch    95 | loss: 62.5737568CurrentTrain: epoch  2, batch     0 | loss: 71.4277087CurrentTrain: epoch  2, batch     1 | loss: 276.0644055CurrentTrain: epoch  2, batch     2 | loss: 101.3636588CurrentTrain: epoch  2, batch     3 | loss: 128.5037901CurrentTrain: epoch  2, batch     4 | loss: 126.4002884CurrentTrain: epoch  2, batch     5 | loss: 69.0420328CurrentTrain: epoch  2, batch     6 | loss: 178.5603370CurrentTrain: epoch  2, batch     7 | loss: 173.2858818CurrentTrain: epoch  2, batch     8 | loss: 105.0316628CurrentTrain: epoch  2, batch     9 | loss: 102.5893601CurrentTrain: epoch  2, batch    10 | loss: 102.3844955CurrentTrain: epoch  2, batch    11 | loss: 131.0498234CurrentTrain: epoch  2, batch    12 | loss: 102.4282537CurrentTrain: epoch  2, batch    13 | loss: 99.9385694CurrentTrain: epoch  2, batch    14 | loss: 101.9029538CurrentTrain: epoch  2, batch    15 | loss: 106.8101858CurrentTrain: epoch  2, batch    16 | loss: 70.4445512CurrentTrain: epoch  2, batch    17 | loss: 125.7856289CurrentTrain: epoch  2, batch    18 | loss: 134.9777450CurrentTrain: epoch  2, batch    19 | loss: 84.4386495CurrentTrain: epoch  2, batch    20 | loss: 126.5378297CurrentTrain: epoch  2, batch    21 | loss: 104.5998296CurrentTrain: epoch  2, batch    22 | loss: 100.9956571CurrentTrain: epoch  2, batch    23 | loss: 82.4914298CurrentTrain: epoch  2, batch    24 | loss: 128.8129269CurrentTrain: epoch  2, batch    25 | loss: 99.5753575CurrentTrain: epoch  2, batch    26 | loss: 102.5077211CurrentTrain: epoch  2, batch    27 | loss: 102.3525475CurrentTrain: epoch  2, batch    28 | loss: 132.4293526CurrentTrain: epoch  2, batch    29 | loss: 99.4679038CurrentTrain: epoch  2, batch    30 | loss: 180.4598530CurrentTrain: epoch  2, batch    31 | loss: 177.1651766CurrentTrain: epoch  2, batch    32 | loss: 77.3435404CurrentTrain: epoch  2, batch    33 | loss: 125.2973131CurrentTrain: epoch  2, batch    34 | loss: 72.5012414CurrentTrain: epoch  2, batch    35 | loss: 99.7010702CurrentTrain: epoch  2, batch    36 | loss: 84.7533334CurrentTrain: epoch  2, batch    37 | loss: 99.2063210CurrentTrain: epoch  2, batch    38 | loss: 102.7142583CurrentTrain: epoch  2, batch    39 | loss: 113.8864807CurrentTrain: epoch  2, batch    40 | loss: 83.9601481CurrentTrain: epoch  2, batch    41 | loss: 85.6629238CurrentTrain: epoch  2, batch    42 | loss: 77.2548613CurrentTrain: epoch  2, batch    43 | loss: 133.8010382CurrentTrain: epoch  2, batch    44 | loss: 125.9300147CurrentTrain: epoch  2, batch    45 | loss: 85.5985754CurrentTrain: epoch  2, batch    46 | loss: 106.9800690CurrentTrain: epoch  2, batch    47 | loss: 133.3849280CurrentTrain: epoch  2, batch    48 | loss: 129.9689408CurrentTrain: epoch  2, batch    49 | loss: 176.3193651CurrentTrain: epoch  2, batch    50 | loss: 103.3717874CurrentTrain: epoch  2, batch    51 | loss: 102.6548405CurrentTrain: epoch  2, batch    52 | loss: 71.0359640CurrentTrain: epoch  2, batch    53 | loss: 88.1700229CurrentTrain: epoch  2, batch    54 | loss: 98.1113221CurrentTrain: epoch  2, batch    55 | loss: 278.3075871CurrentTrain: epoch  2, batch    56 | loss: 96.1117803CurrentTrain: epoch  2, batch    57 | loss: 79.5703206CurrentTrain: epoch  2, batch    58 | loss: 101.6244853CurrentTrain: epoch  2, batch    59 | loss: 102.5875219CurrentTrain: epoch  2, batch    60 | loss: 67.7616744CurrentTrain: epoch  2, batch    61 | loss: 72.6207356CurrentTrain: epoch  2, batch    62 | loss: 85.4954904CurrentTrain: epoch  2, batch    63 | loss: 100.7763902CurrentTrain: epoch  2, batch    64 | loss: 102.0604907CurrentTrain: epoch  2, batch    65 | loss: 81.6213720CurrentTrain: epoch  2, batch    66 | loss: 103.5853590CurrentTrain: epoch  2, batch    67 | loss: 83.0764102CurrentTrain: epoch  2, batch    68 | loss: 131.7675665CurrentTrain: epoch  2, batch    69 | loss: 178.4622207CurrentTrain: epoch  2, batch    70 | loss: 99.7606381CurrentTrain: epoch  2, batch    71 | loss: 135.4477531CurrentTrain: epoch  2, batch    72 | loss: 82.8573544CurrentTrain: epoch  2, batch    73 | loss: 130.1693592CurrentTrain: epoch  2, batch    74 | loss: 133.6175822CurrentTrain: epoch  2, batch    75 | loss: 122.8406013CurrentTrain: epoch  2, batch    76 | loss: 84.9878468CurrentTrain: epoch  2, batch    77 | loss: 177.3390434CurrentTrain: epoch  2, batch    78 | loss: 128.8168758CurrentTrain: epoch  2, batch    79 | loss: 84.3068446CurrentTrain: epoch  2, batch    80 | loss: 131.0747049CurrentTrain: epoch  2, batch    81 | loss: 82.3095363CurrentTrain: epoch  2, batch    82 | loss: 100.6850950CurrentTrain: epoch  2, batch    83 | loss: 105.4119028CurrentTrain: epoch  2, batch    84 | loss: 82.9404053CurrentTrain: epoch  2, batch    85 | loss: 64.7565240CurrentTrain: epoch  2, batch    86 | loss: 124.2795604CurrentTrain: epoch  2, batch    87 | loss: 130.4386275CurrentTrain: epoch  2, batch    88 | loss: 86.4569237CurrentTrain: epoch  2, batch    89 | loss: 83.5153568CurrentTrain: epoch  2, batch    90 | loss: 133.6468741CurrentTrain: epoch  2, batch    91 | loss: 269.3113345CurrentTrain: epoch  2, batch    92 | loss: 132.9463076CurrentTrain: epoch  2, batch    93 | loss: 104.8682145CurrentTrain: epoch  2, batch    94 | loss: 101.4983690CurrentTrain: epoch  2, batch    95 | loss: 91.3795819CurrentTrain: epoch  3, batch     0 | loss: 130.1589818CurrentTrain: epoch  3, batch     1 | loss: 171.3007376CurrentTrain: epoch  3, batch     2 | loss: 100.7537918CurrentTrain: epoch  3, batch     3 | loss: 123.0300010CurrentTrain: epoch  3, batch     4 | loss: 80.9847716CurrentTrain: epoch  3, batch     5 | loss: 79.1948379CurrentTrain: epoch  3, batch     6 | loss: 80.2556331CurrentTrain: epoch  3, batch     7 | loss: 101.9468735CurrentTrain: epoch  3, batch     8 | loss: 80.6312558CurrentTrain: epoch  3, batch     9 | loss: 84.1685651CurrentTrain: epoch  3, batch    10 | loss: 96.1040113CurrentTrain: epoch  3, batch    11 | loss: 79.9620406CurrentTrain: epoch  3, batch    12 | loss: 98.5825900CurrentTrain: epoch  3, batch    13 | loss: 127.2974632CurrentTrain: epoch  3, batch    14 | loss: 126.3269274CurrentTrain: epoch  3, batch    15 | loss: 100.9116642CurrentTrain: epoch  3, batch    16 | loss: 129.9611243CurrentTrain: epoch  3, batch    17 | loss: 177.8514535CurrentTrain: epoch  3, batch    18 | loss: 103.4374256CurrentTrain: epoch  3, batch    19 | loss: 130.6314255CurrentTrain: epoch  3, batch    20 | loss: 180.9051908CurrentTrain: epoch  3, batch    21 | loss: 102.2692049CurrentTrain: epoch  3, batch    22 | loss: 103.7523204CurrentTrain: epoch  3, batch    23 | loss: 101.5226197CurrentTrain: epoch  3, batch    24 | loss: 98.3747168CurrentTrain: epoch  3, batch    25 | loss: 99.5498767CurrentTrain: epoch  3, batch    26 | loss: 78.2659515CurrentTrain: epoch  3, batch    27 | loss: 101.9901994CurrentTrain: epoch  3, batch    28 | loss: 128.7268218CurrentTrain: epoch  3, batch    29 | loss: 99.6010431CurrentTrain: epoch  3, batch    30 | loss: 131.5656945CurrentTrain: epoch  3, batch    31 | loss: 66.7451134CurrentTrain: epoch  3, batch    32 | loss: 130.7845842CurrentTrain: epoch  3, batch    33 | loss: 101.8975404CurrentTrain: epoch  3, batch    34 | loss: 174.6069769CurrentTrain: epoch  3, batch    35 | loss: 127.7313040CurrentTrain: epoch  3, batch    36 | loss: 178.4716395CurrentTrain: epoch  3, batch    37 | loss: 125.7213440CurrentTrain: epoch  3, batch    38 | loss: 101.9962027CurrentTrain: epoch  3, batch    39 | loss: 131.6120648CurrentTrain: epoch  3, batch    40 | loss: 88.3965104CurrentTrain: epoch  3, batch    41 | loss: 97.3242613CurrentTrain: epoch  3, batch    42 | loss: 102.1416662CurrentTrain: epoch  3, batch    43 | loss: 127.1368496CurrentTrain: epoch  3, batch    44 | loss: 81.2132663CurrentTrain: epoch  3, batch    45 | loss: 260.4139711CurrentTrain: epoch  3, batch    46 | loss: 130.5252468CurrentTrain: epoch  3, batch    47 | loss: 102.3081200CurrentTrain: epoch  3, batch    48 | loss: 102.8789714CurrentTrain: epoch  3, batch    49 | loss: 96.4506932CurrentTrain: epoch  3, batch    50 | loss: 101.9990160CurrentTrain: epoch  3, batch    51 | loss: 175.0474556CurrentTrain: epoch  3, batch    52 | loss: 102.9619314CurrentTrain: epoch  3, batch    53 | loss: 81.1778512CurrentTrain: epoch  3, batch    54 | loss: 80.7430514CurrentTrain: epoch  3, batch    55 | loss: 72.3177545CurrentTrain: epoch  3, batch    56 | loss: 95.1962391CurrentTrain: epoch  3, batch    57 | loss: 101.7103397CurrentTrain: epoch  3, batch    58 | loss: 101.1688941CurrentTrain: epoch  3, batch    59 | loss: 123.2147302CurrentTrain: epoch  3, batch    60 | loss: 86.0439444CurrentTrain: epoch  3, batch    61 | loss: 101.9782810CurrentTrain: epoch  3, batch    62 | loss: 100.3819662CurrentTrain: epoch  3, batch    63 | loss: 81.5603940CurrentTrain: epoch  3, batch    64 | loss: 83.9969785CurrentTrain: epoch  3, batch    65 | loss: 133.0742501CurrentTrain: epoch  3, batch    66 | loss: 78.9996254CurrentTrain: epoch  3, batch    67 | loss: 174.4715304CurrentTrain: epoch  3, batch    68 | loss: 123.1230553CurrentTrain: epoch  3, batch    69 | loss: 122.3118911CurrentTrain: epoch  3, batch    70 | loss: 79.2149352CurrentTrain: epoch  3, batch    71 | loss: 118.7949616CurrentTrain: epoch  3, batch    72 | loss: 62.2630371CurrentTrain: epoch  3, batch    73 | loss: 100.2875420CurrentTrain: epoch  3, batch    74 | loss: 123.9979175CurrentTrain: epoch  3, batch    75 | loss: 85.6605729CurrentTrain: epoch  3, batch    76 | loss: 134.1430391CurrentTrain: epoch  3, batch    77 | loss: 126.1817338CurrentTrain: epoch  3, batch    78 | loss: 99.4324476CurrentTrain: epoch  3, batch    79 | loss: 81.2405406CurrentTrain: epoch  3, batch    80 | loss: 127.8520973CurrentTrain: epoch  3, batch    81 | loss: 93.0751356CurrentTrain: epoch  3, batch    82 | loss: 67.5473709CurrentTrain: epoch  3, batch    83 | loss: 130.2704065CurrentTrain: epoch  3, batch    84 | loss: 98.2758976CurrentTrain: epoch  3, batch    85 | loss: 170.9159121CurrentTrain: epoch  3, batch    86 | loss: 131.9779369CurrentTrain: epoch  3, batch    87 | loss: 74.5368609CurrentTrain: epoch  3, batch    88 | loss: 82.3240672CurrentTrain: epoch  3, batch    89 | loss: 109.2667097CurrentTrain: epoch  3, batch    90 | loss: 104.7805179CurrentTrain: epoch  3, batch    91 | loss: 80.8603358CurrentTrain: epoch  3, batch    92 | loss: 180.0972691CurrentTrain: epoch  3, batch    93 | loss: 104.5738737CurrentTrain: epoch  3, batch    94 | loss: 128.6946110CurrentTrain: epoch  3, batch    95 | loss: 108.9957723CurrentTrain: epoch  4, batch     0 | loss: 177.5886546CurrentTrain: epoch  4, batch     1 | loss: 94.6622256CurrentTrain: epoch  4, batch     2 | loss: 122.8629125CurrentTrain: epoch  4, batch     3 | loss: 130.6705737CurrentTrain: epoch  4, batch     4 | loss: 127.1323850CurrentTrain: epoch  4, batch     5 | loss: 134.2021792CurrentTrain: epoch  4, batch     6 | loss: 77.4546183CurrentTrain: epoch  4, batch     7 | loss: 82.3331385CurrentTrain: epoch  4, batch     8 | loss: 169.1179476CurrentTrain: epoch  4, batch     9 | loss: 103.5226369CurrentTrain: epoch  4, batch    10 | loss: 177.4185402CurrentTrain: epoch  4, batch    11 | loss: 98.7943745CurrentTrain: epoch  4, batch    12 | loss: 100.5487087CurrentTrain: epoch  4, batch    13 | loss: 94.9243069CurrentTrain: epoch  4, batch    14 | loss: 98.4503114CurrentTrain: epoch  4, batch    15 | loss: 98.8932448CurrentTrain: epoch  4, batch    16 | loss: 102.4189346CurrentTrain: epoch  4, batch    17 | loss: 101.3288057CurrentTrain: epoch  4, batch    18 | loss: 97.7100373CurrentTrain: epoch  4, batch    19 | loss: 99.7383752CurrentTrain: epoch  4, batch    20 | loss: 123.6876491CurrentTrain: epoch  4, batch    21 | loss: 84.1888723CurrentTrain: epoch  4, batch    22 | loss: 67.9229524CurrentTrain: epoch  4, batch    23 | loss: 66.8964496CurrentTrain: epoch  4, batch    24 | loss: 81.0909280CurrentTrain: epoch  4, batch    25 | loss: 132.0082506CurrentTrain: epoch  4, batch    26 | loss: 104.6809844CurrentTrain: epoch  4, batch    27 | loss: 122.9046216CurrentTrain: epoch  4, batch    28 | loss: 98.9905721CurrentTrain: epoch  4, batch    29 | loss: 96.4336539CurrentTrain: epoch  4, batch    30 | loss: 97.1374984CurrentTrain: epoch  4, batch    31 | loss: 125.3226994CurrentTrain: epoch  4, batch    32 | loss: 174.8710170CurrentTrain: epoch  4, batch    33 | loss: 100.0629520CurrentTrain: epoch  4, batch    34 | loss: 126.2524767CurrentTrain: epoch  4, batch    35 | loss: 78.3057368CurrentTrain: epoch  4, batch    36 | loss: 128.4365851CurrentTrain: epoch  4, batch    37 | loss: 101.5682616CurrentTrain: epoch  4, batch    38 | loss: 97.9316119CurrentTrain: epoch  4, batch    39 | loss: 77.1604905CurrentTrain: epoch  4, batch    40 | loss: 173.8757476CurrentTrain: epoch  4, batch    41 | loss: 126.2008715CurrentTrain: epoch  4, batch    42 | loss: 97.4726866CurrentTrain: epoch  4, batch    43 | loss: 78.0498162CurrentTrain: epoch  4, batch    44 | loss: 82.1042451CurrentTrain: epoch  4, batch    45 | loss: 97.2061207CurrentTrain: epoch  4, batch    46 | loss: 99.5856635CurrentTrain: epoch  4, batch    47 | loss: 78.4295579CurrentTrain: epoch  4, batch    48 | loss: 126.0971075CurrentTrain: epoch  4, batch    49 | loss: 98.6435467CurrentTrain: epoch  4, batch    50 | loss: 98.5714399CurrentTrain: epoch  4, batch    51 | loss: 76.7639675CurrentTrain: epoch  4, batch    52 | loss: 67.6519318CurrentTrain: epoch  4, batch    53 | loss: 127.0954768CurrentTrain: epoch  4, batch    54 | loss: 101.9661788CurrentTrain: epoch  4, batch    55 | loss: 132.6272114CurrentTrain: epoch  4, batch    56 | loss: 125.9879033CurrentTrain: epoch  4, batch    57 | loss: 175.0299000CurrentTrain: epoch  4, batch    58 | loss: 80.6690203CurrentTrain: epoch  4, batch    59 | loss: 97.9392584CurrentTrain: epoch  4, batch    60 | loss: 96.9128949CurrentTrain: epoch  4, batch    61 | loss: 98.2795729CurrentTrain: epoch  4, batch    62 | loss: 171.4978753CurrentTrain: epoch  4, batch    63 | loss: 97.5694221CurrentTrain: epoch  4, batch    64 | loss: 80.9555116CurrentTrain: epoch  4, batch    65 | loss: 105.2235514CurrentTrain: epoch  4, batch    66 | loss: 178.2282254CurrentTrain: epoch  4, batch    67 | loss: 96.4386303CurrentTrain: epoch  4, batch    68 | loss: 100.3853651CurrentTrain: epoch  4, batch    69 | loss: 116.0584028CurrentTrain: epoch  4, batch    70 | loss: 96.2679277CurrentTrain: epoch  4, batch    71 | loss: 71.3833010CurrentTrain: epoch  4, batch    72 | loss: 95.9483970CurrentTrain: epoch  4, batch    73 | loss: 82.2182718CurrentTrain: epoch  4, batch    74 | loss: 175.6645220CurrentTrain: epoch  4, batch    75 | loss: 101.7098067CurrentTrain: epoch  4, batch    76 | loss: 78.4755835CurrentTrain: epoch  4, batch    77 | loss: 123.5728771CurrentTrain: epoch  4, batch    78 | loss: 101.8320626CurrentTrain: epoch  4, batch    79 | loss: 65.8234355CurrentTrain: epoch  4, batch    80 | loss: 104.9001276CurrentTrain: epoch  4, batch    81 | loss: 125.2904718CurrentTrain: epoch  4, batch    82 | loss: 82.6350304CurrentTrain: epoch  4, batch    83 | loss: 65.4485922CurrentTrain: epoch  4, batch    84 | loss: 104.9480543CurrentTrain: epoch  4, batch    85 | loss: 67.3408787CurrentTrain: epoch  4, batch    86 | loss: 129.2720714CurrentTrain: epoch  4, batch    87 | loss: 93.0498535CurrentTrain: epoch  4, batch    88 | loss: 100.5745098CurrentTrain: epoch  4, batch    89 | loss: 79.1049678CurrentTrain: epoch  4, batch    90 | loss: 102.1601951CurrentTrain: epoch  4, batch    91 | loss: 121.8654574CurrentTrain: epoch  4, batch    92 | loss: 127.8959300CurrentTrain: epoch  4, batch    93 | loss: 83.4971412CurrentTrain: epoch  4, batch    94 | loss: 273.5188332CurrentTrain: epoch  4, batch    95 | loss: 52.7071057CurrentTrain: epoch  5, batch     0 | loss: 101.1488892CurrentTrain: epoch  5, batch     1 | loss: 78.6855285CurrentTrain: epoch  5, batch     2 | loss: 126.9232720CurrentTrain: epoch  5, batch     3 | loss: 122.5410939CurrentTrain: epoch  5, batch     4 | loss: 126.1435187CurrentTrain: epoch  5, batch     5 | loss: 80.9524054CurrentTrain: epoch  5, batch     6 | loss: 65.6931533CurrentTrain: epoch  5, batch     7 | loss: 100.2974189CurrentTrain: epoch  5, batch     8 | loss: 78.2698659CurrentTrain: epoch  5, batch     9 | loss: 130.8010997CurrentTrain: epoch  5, batch    10 | loss: 102.3440516CurrentTrain: epoch  5, batch    11 | loss: 126.9101434CurrentTrain: epoch  5, batch    12 | loss: 131.1187455CurrentTrain: epoch  5, batch    13 | loss: 127.3473341CurrentTrain: epoch  5, batch    14 | loss: 99.4043342CurrentTrain: epoch  5, batch    15 | loss: 127.7252545CurrentTrain: epoch  5, batch    16 | loss: 79.2860751CurrentTrain: epoch  5, batch    17 | loss: 102.3666940CurrentTrain: epoch  5, batch    18 | loss: 104.6101985CurrentTrain: epoch  5, batch    19 | loss: 129.6192472CurrentTrain: epoch  5, batch    20 | loss: 177.0600200CurrentTrain: epoch  5, batch    21 | loss: 91.0361236CurrentTrain: epoch  5, batch    22 | loss: 95.8956568CurrentTrain: epoch  5, batch    23 | loss: 83.1079395CurrentTrain: epoch  5, batch    24 | loss: 128.0033659CurrentTrain: epoch  5, batch    25 | loss: 79.6117011CurrentTrain: epoch  5, batch    26 | loss: 77.1041578CurrentTrain: epoch  5, batch    27 | loss: 125.8096538CurrentTrain: epoch  5, batch    28 | loss: 127.8956139CurrentTrain: epoch  5, batch    29 | loss: 100.6294770CurrentTrain: epoch  5, batch    30 | loss: 129.3531193CurrentTrain: epoch  5, batch    31 | loss: 130.9434322CurrentTrain: epoch  5, batch    32 | loss: 97.0837053CurrentTrain: epoch  5, batch    33 | loss: 80.2006639CurrentTrain: epoch  5, batch    34 | loss: 174.3757233CurrentTrain: epoch  5, batch    35 | loss: 65.3815012CurrentTrain: epoch  5, batch    36 | loss: 81.8023502CurrentTrain: epoch  5, batch    37 | loss: 80.7753023CurrentTrain: epoch  5, batch    38 | loss: 98.3439293CurrentTrain: epoch  5, batch    39 | loss: 128.6213221CurrentTrain: epoch  5, batch    40 | loss: 81.1203948CurrentTrain: epoch  5, batch    41 | loss: 93.3322533CurrentTrain: epoch  5, batch    42 | loss: 127.0637852CurrentTrain: epoch  5, batch    43 | loss: 100.1792741CurrentTrain: epoch  5, batch    44 | loss: 133.5127511CurrentTrain: epoch  5, batch    45 | loss: 81.2449491CurrentTrain: epoch  5, batch    46 | loss: 79.5495056CurrentTrain: epoch  5, batch    47 | loss: 175.4692178CurrentTrain: epoch  5, batch    48 | loss: 80.0650081CurrentTrain: epoch  5, batch    49 | loss: 65.3525397CurrentTrain: epoch  5, batch    50 | loss: 80.3862441CurrentTrain: epoch  5, batch    51 | loss: 81.6725610CurrentTrain: epoch  5, batch    52 | loss: 97.0858624CurrentTrain: epoch  5, batch    53 | loss: 98.3799126CurrentTrain: epoch  5, batch    54 | loss: 79.9813304CurrentTrain: epoch  5, batch    55 | loss: 78.4623156CurrentTrain: epoch  5, batch    56 | loss: 97.6505114CurrentTrain: epoch  5, batch    57 | loss: 129.1958025CurrentTrain: epoch  5, batch    58 | loss: 97.7793925CurrentTrain: epoch  5, batch    59 | loss: 126.7736203CurrentTrain: epoch  5, batch    60 | loss: 124.3440776CurrentTrain: epoch  5, batch    61 | loss: 68.3889743CurrentTrain: epoch  5, batch    62 | loss: 77.7950763CurrentTrain: epoch  5, batch    63 | loss: 132.3205686CurrentTrain: epoch  5, batch    64 | loss: 80.2270653CurrentTrain: epoch  5, batch    65 | loss: 106.7049222CurrentTrain: epoch  5, batch    66 | loss: 82.7719411CurrentTrain: epoch  5, batch    67 | loss: 122.4224836CurrentTrain: epoch  5, batch    68 | loss: 79.0068812CurrentTrain: epoch  5, batch    69 | loss: 73.0351827CurrentTrain: epoch  5, batch    70 | loss: 93.8165367CurrentTrain: epoch  5, batch    71 | loss: 167.4989274CurrentTrain: epoch  5, batch    72 | loss: 98.4212371CurrentTrain: epoch  5, batch    73 | loss: 81.2817081CurrentTrain: epoch  5, batch    74 | loss: 80.5641728CurrentTrain: epoch  5, batch    75 | loss: 100.7582842CurrentTrain: epoch  5, batch    76 | loss: 75.4028009CurrentTrain: epoch  5, batch    77 | loss: 127.4517754CurrentTrain: epoch  5, batch    78 | loss: 129.1378051CurrentTrain: epoch  5, batch    79 | loss: 76.6627959CurrentTrain: epoch  5, batch    80 | loss: 125.2104915CurrentTrain: epoch  5, batch    81 | loss: 122.5130571CurrentTrain: epoch  5, batch    82 | loss: 81.0560456CurrentTrain: epoch  5, batch    83 | loss: 128.2889009CurrentTrain: epoch  5, batch    84 | loss: 176.4837398CurrentTrain: epoch  5, batch    85 | loss: 119.9347785CurrentTrain: epoch  5, batch    86 | loss: 174.1179698CurrentTrain: epoch  5, batch    87 | loss: 97.3110801CurrentTrain: epoch  5, batch    88 | loss: 82.0572688CurrentTrain: epoch  5, batch    89 | loss: 80.8509110CurrentTrain: epoch  5, batch    90 | loss: 79.1493504CurrentTrain: epoch  5, batch    91 | loss: 119.4679132CurrentTrain: epoch  5, batch    92 | loss: 122.6544039CurrentTrain: epoch  5, batch    93 | loss: 119.4611373CurrentTrain: epoch  5, batch    94 | loss: 130.8640045CurrentTrain: epoch  5, batch    95 | loss: 107.8664430CurrentTrain: epoch  6, batch     0 | loss: 95.5310113CurrentTrain: epoch  6, batch     1 | loss: 129.0713934CurrentTrain: epoch  6, batch     2 | loss: 101.2883535CurrentTrain: epoch  6, batch     3 | loss: 124.2789433CurrentTrain: epoch  6, batch     4 | loss: 80.6597323CurrentTrain: epoch  6, batch     5 | loss: 81.9940229CurrentTrain: epoch  6, batch     6 | loss: 125.1179355CurrentTrain: epoch  6, batch     7 | loss: 85.1389717CurrentTrain: epoch  6, batch     8 | loss: 101.6268943CurrentTrain: epoch  6, batch     9 | loss: 124.2744125CurrentTrain: epoch  6, batch    10 | loss: 98.8594072CurrentTrain: epoch  6, batch    11 | loss: 72.4122321CurrentTrain: epoch  6, batch    12 | loss: 176.8162420CurrentTrain: epoch  6, batch    13 | loss: 93.9956849CurrentTrain: epoch  6, batch    14 | loss: 127.2607269CurrentTrain: epoch  6, batch    15 | loss: 176.4616199CurrentTrain: epoch  6, batch    16 | loss: 128.8476895CurrentTrain: epoch  6, batch    17 | loss: 126.8444607CurrentTrain: epoch  6, batch    18 | loss: 124.4073640CurrentTrain: epoch  6, batch    19 | loss: 96.9056645CurrentTrain: epoch  6, batch    20 | loss: 91.6715036CurrentTrain: epoch  6, batch    21 | loss: 126.3414724CurrentTrain: epoch  6, batch    22 | loss: 122.6838137CurrentTrain: epoch  6, batch    23 | loss: 128.6341206CurrentTrain: epoch  6, batch    24 | loss: 100.4392037CurrentTrain: epoch  6, batch    25 | loss: 66.3974774CurrentTrain: epoch  6, batch    26 | loss: 129.2868419CurrentTrain: epoch  6, batch    27 | loss: 99.9046264CurrentTrain: epoch  6, batch    28 | loss: 81.9322189CurrentTrain: epoch  6, batch    29 | loss: 170.4208134CurrentTrain: epoch  6, batch    30 | loss: 99.4651089CurrentTrain: epoch  6, batch    31 | loss: 81.0038394CurrentTrain: epoch  6, batch    32 | loss: 101.0582390CurrentTrain: epoch  6, batch    33 | loss: 79.6700564CurrentTrain: epoch  6, batch    34 | loss: 76.5973547CurrentTrain: epoch  6, batch    35 | loss: 120.2134815CurrentTrain: epoch  6, batch    36 | loss: 88.6828923CurrentTrain: epoch  6, batch    37 | loss: 173.6929919CurrentTrain: epoch  6, batch    38 | loss: 80.7742107CurrentTrain: epoch  6, batch    39 | loss: 124.2511349CurrentTrain: epoch  6, batch    40 | loss: 127.1860802CurrentTrain: epoch  6, batch    41 | loss: 124.9763845CurrentTrain: epoch  6, batch    42 | loss: 176.4578526CurrentTrain: epoch  6, batch    43 | loss: 76.4875677CurrentTrain: epoch  6, batch    44 | loss: 96.1915800CurrentTrain: epoch  6, batch    45 | loss: 126.9599424CurrentTrain: epoch  6, batch    46 | loss: 78.9848699CurrentTrain: epoch  6, batch    47 | loss: 69.2666917CurrentTrain: epoch  6, batch    48 | loss: 104.7249347CurrentTrain: epoch  6, batch    49 | loss: 66.0844674CurrentTrain: epoch  6, batch    50 | loss: 173.7055902CurrentTrain: epoch  6, batch    51 | loss: 99.0945211CurrentTrain: epoch  6, batch    52 | loss: 134.5241535CurrentTrain: epoch  6, batch    53 | loss: 98.6277819CurrentTrain: epoch  6, batch    54 | loss: 177.3757489CurrentTrain: epoch  6, batch    55 | loss: 96.2279517CurrentTrain: epoch  6, batch    56 | loss: 96.9796443CurrentTrain: epoch  6, batch    57 | loss: 79.4179063CurrentTrain: epoch  6, batch    58 | loss: 103.5649258CurrentTrain: epoch  6, batch    59 | loss: 176.4924972CurrentTrain: epoch  6, batch    60 | loss: 67.6616943CurrentTrain: epoch  6, batch    61 | loss: 74.9751100CurrentTrain: epoch  6, batch    62 | loss: 63.3328344CurrentTrain: epoch  6, batch    63 | loss: 77.7641919CurrentTrain: epoch  6, batch    64 | loss: 94.4873112CurrentTrain: epoch  6, batch    65 | loss: 122.0441200CurrentTrain: epoch  6, batch    66 | loss: 124.8981325CurrentTrain: epoch  6, batch    67 | loss: 124.1910388CurrentTrain: epoch  6, batch    68 | loss: 80.9680720CurrentTrain: epoch  6, batch    69 | loss: 63.7973942CurrentTrain: epoch  6, batch    70 | loss: 81.5443462CurrentTrain: epoch  6, batch    71 | loss: 76.7100820CurrentTrain: epoch  6, batch    72 | loss: 127.5405920CurrentTrain: epoch  6, batch    73 | loss: 77.6530816CurrentTrain: epoch  6, batch    74 | loss: 79.4416877CurrentTrain: epoch  6, batch    75 | loss: 79.4392017CurrentTrain: epoch  6, batch    76 | loss: 80.2112387CurrentTrain: epoch  6, batch    77 | loss: 95.2765276CurrentTrain: epoch  6, batch    78 | loss: 99.3665823CurrentTrain: epoch  6, batch    79 | loss: 125.1099601CurrentTrain: epoch  6, batch    80 | loss: 101.1886814CurrentTrain: epoch  6, batch    81 | loss: 78.8926578CurrentTrain: epoch  6, batch    82 | loss: 63.5755254CurrentTrain: epoch  6, batch    83 | loss: 100.4773522CurrentTrain: epoch  6, batch    84 | loss: 77.9523481CurrentTrain: epoch  6, batch    85 | loss: 81.7218596CurrentTrain: epoch  6, batch    86 | loss: 125.5267050CurrentTrain: epoch  6, batch    87 | loss: 94.7999805CurrentTrain: epoch  6, batch    88 | loss: 82.2449644CurrentTrain: epoch  6, batch    89 | loss: 123.0936837CurrentTrain: epoch  6, batch    90 | loss: 95.8760223CurrentTrain: epoch  6, batch    91 | loss: 79.3262172CurrentTrain: epoch  6, batch    92 | loss: 173.2778078CurrentTrain: epoch  6, batch    93 | loss: 121.7009306CurrentTrain: epoch  6, batch    94 | loss: 75.8208811CurrentTrain: epoch  6, batch    95 | loss: 83.8397742CurrentTrain: epoch  7, batch     0 | loss: 123.3345400CurrentTrain: epoch  7, batch     1 | loss: 77.3763208CurrentTrain: epoch  7, batch     2 | loss: 78.8058568CurrentTrain: epoch  7, batch     3 | loss: 99.3080639CurrentTrain: epoch  7, batch     4 | loss: 82.1220331CurrentTrain: epoch  7, batch     5 | loss: 97.0106899CurrentTrain: epoch  7, batch     6 | loss: 77.2450017CurrentTrain: epoch  7, batch     7 | loss: 124.1387241CurrentTrain: epoch  7, batch     8 | loss: 98.8024164CurrentTrain: epoch  7, batch     9 | loss: 99.2695058CurrentTrain: epoch  7, batch    10 | loss: 121.9900470CurrentTrain: epoch  7, batch    11 | loss: 179.0151434CurrentTrain: epoch  7, batch    12 | loss: 124.9426590CurrentTrain: epoch  7, batch    13 | loss: 122.7633472CurrentTrain: epoch  7, batch    14 | loss: 79.2688240CurrentTrain: epoch  7, batch    15 | loss: 128.9999227CurrentTrain: epoch  7, batch    16 | loss: 126.9245149CurrentTrain: epoch  7, batch    17 | loss: 63.3287317CurrentTrain: epoch  7, batch    18 | loss: 62.8225451CurrentTrain: epoch  7, batch    19 | loss: 81.4792345CurrentTrain: epoch  7, batch    20 | loss: 124.7636903CurrentTrain: epoch  7, batch    21 | loss: 76.3479560CurrentTrain: epoch  7, batch    22 | loss: 80.2229504CurrentTrain: epoch  7, batch    23 | loss: 129.5968086CurrentTrain: epoch  7, batch    24 | loss: 79.8310762CurrentTrain: epoch  7, batch    25 | loss: 123.4655703CurrentTrain: epoch  7, batch    26 | loss: 122.0967088CurrentTrain: epoch  7, batch    27 | loss: 67.8701872CurrentTrain: epoch  7, batch    28 | loss: 94.1283757CurrentTrain: epoch  7, batch    29 | loss: 170.7321720CurrentTrain: epoch  7, batch    30 | loss: 96.9160823CurrentTrain: epoch  7, batch    31 | loss: 122.7859427CurrentTrain: epoch  7, batch    32 | loss: 170.3993451CurrentTrain: epoch  7, batch    33 | loss: 96.9541580CurrentTrain: epoch  7, batch    34 | loss: 97.1440298CurrentTrain: epoch  7, batch    35 | loss: 98.9382394CurrentTrain: epoch  7, batch    36 | loss: 170.8209493CurrentTrain: epoch  7, batch    37 | loss: 126.9413244CurrentTrain: epoch  7, batch    38 | loss: 64.8840355CurrentTrain: epoch  7, batch    39 | loss: 79.8515988CurrentTrain: epoch  7, batch    40 | loss: 273.6345031CurrentTrain: epoch  7, batch    41 | loss: 272.9780925CurrentTrain: epoch  7, batch    42 | loss: 78.0458160CurrentTrain: epoch  7, batch    43 | loss: 98.6706857CurrentTrain: epoch  7, batch    44 | loss: 93.6019952CurrentTrain: epoch  7, batch    45 | loss: 100.1530239CurrentTrain: epoch  7, batch    46 | loss: 76.7155124CurrentTrain: epoch  7, batch    47 | loss: 164.7235802CurrentTrain: epoch  7, batch    48 | loss: 93.8837411CurrentTrain: epoch  7, batch    49 | loss: 76.1775519CurrentTrain: epoch  7, batch    50 | loss: 95.4821732CurrentTrain: epoch  7, batch    51 | loss: 75.1437982CurrentTrain: epoch  7, batch    52 | loss: 98.6384519CurrentTrain: epoch  7, batch    53 | loss: 97.8112514CurrentTrain: epoch  7, batch    54 | loss: 172.9468707CurrentTrain: epoch  7, batch    55 | loss: 96.3184466CurrentTrain: epoch  7, batch    56 | loss: 94.8914579CurrentTrain: epoch  7, batch    57 | loss: 95.0276491CurrentTrain: epoch  7, batch    58 | loss: 170.7331449CurrentTrain: epoch  7, batch    59 | loss: 174.0492549CurrentTrain: epoch  7, batch    60 | loss: 126.9599247CurrentTrain: epoch  7, batch    61 | loss: 99.2564754CurrentTrain: epoch  7, batch    62 | loss: 126.8428689CurrentTrain: epoch  7, batch    63 | loss: 100.9634217CurrentTrain: epoch  7, batch    64 | loss: 95.5376933CurrentTrain: epoch  7, batch    65 | loss: 78.6068997CurrentTrain: epoch  7, batch    66 | loss: 96.6374429CurrentTrain: epoch  7, batch    67 | loss: 78.3892245CurrentTrain: epoch  7, batch    68 | loss: 101.2674630CurrentTrain: epoch  7, batch    69 | loss: 84.7368846CurrentTrain: epoch  7, batch    70 | loss: 96.5584527CurrentTrain: epoch  7, batch    71 | loss: 123.2742050CurrentTrain: epoch  7, batch    72 | loss: 172.9673318CurrentTrain: epoch  7, batch    73 | loss: 93.2552329CurrentTrain: epoch  7, batch    74 | loss: 134.0403583CurrentTrain: epoch  7, batch    75 | loss: 80.2554848CurrentTrain: epoch  7, batch    76 | loss: 127.0004134CurrentTrain: epoch  7, batch    77 | loss: 79.9371807CurrentTrain: epoch  7, batch    78 | loss: 99.5725063CurrentTrain: epoch  7, batch    79 | loss: 79.1374535CurrentTrain: epoch  7, batch    80 | loss: 81.2023139CurrentTrain: epoch  7, batch    81 | loss: 93.6225623CurrentTrain: epoch  7, batch    82 | loss: 78.8768674CurrentTrain: epoch  7, batch    83 | loss: 77.3840597CurrentTrain: epoch  7, batch    84 | loss: 97.0904986CurrentTrain: epoch  7, batch    85 | loss: 129.6739368CurrentTrain: epoch  7, batch    86 | loss: 81.2487944CurrentTrain: epoch  7, batch    87 | loss: 76.9641113CurrentTrain: epoch  7, batch    88 | loss: 124.5706611CurrentTrain: epoch  7, batch    89 | loss: 64.4309172CurrentTrain: epoch  7, batch    90 | loss: 126.9816897CurrentTrain: epoch  7, batch    91 | loss: 97.0023644CurrentTrain: epoch  7, batch    92 | loss: 81.8436183CurrentTrain: epoch  7, batch    93 | loss: 66.0604147CurrentTrain: epoch  7, batch    94 | loss: 126.5183324CurrentTrain: epoch  7, batch    95 | loss: 79.8115134CurrentTrain: epoch  8, batch     0 | loss: 97.6574102CurrentTrain: epoch  8, batch     1 | loss: 123.5567932CurrentTrain: epoch  8, batch     2 | loss: 77.5762697CurrentTrain: epoch  8, batch     3 | loss: 72.9682987CurrentTrain: epoch  8, batch     4 | loss: 79.1615006CurrentTrain: epoch  8, batch     5 | loss: 97.7426229CurrentTrain: epoch  8, batch     6 | loss: 75.1643297CurrentTrain: epoch  8, batch     7 | loss: 173.4594156CurrentTrain: epoch  8, batch     8 | loss: 99.2459288CurrentTrain: epoch  8, batch     9 | loss: 98.4641014CurrentTrain: epoch  8, batch    10 | loss: 79.7372466CurrentTrain: epoch  8, batch    11 | loss: 80.2230938CurrentTrain: epoch  8, batch    12 | loss: 99.8298432CurrentTrain: epoch  8, batch    13 | loss: 95.7703231CurrentTrain: epoch  8, batch    14 | loss: 67.6205215CurrentTrain: epoch  8, batch    15 | loss: 99.4874010CurrentTrain: epoch  8, batch    16 | loss: 78.2113708CurrentTrain: epoch  8, batch    17 | loss: 126.4189260CurrentTrain: epoch  8, batch    18 | loss: 99.6847621CurrentTrain: epoch  8, batch    19 | loss: 124.4064738CurrentTrain: epoch  8, batch    20 | loss: 95.7800356CurrentTrain: epoch  8, batch    21 | loss: 76.3560648CurrentTrain: epoch  8, batch    22 | loss: 172.9874212CurrentTrain: epoch  8, batch    23 | loss: 95.6416758CurrentTrain: epoch  8, batch    24 | loss: 176.3103377CurrentTrain: epoch  8, batch    25 | loss: 65.5237850CurrentTrain: epoch  8, batch    26 | loss: 77.6294968CurrentTrain: epoch  8, batch    27 | loss: 120.2481507CurrentTrain: epoch  8, batch    28 | loss: 125.5148945CurrentTrain: epoch  8, batch    29 | loss: 127.3521509CurrentTrain: epoch  8, batch    30 | loss: 94.1043677CurrentTrain: epoch  8, batch    31 | loss: 98.5003933CurrentTrain: epoch  8, batch    32 | loss: 71.0757934CurrentTrain: epoch  8, batch    33 | loss: 79.0882512CurrentTrain: epoch  8, batch    34 | loss: 100.3787122CurrentTrain: epoch  8, batch    35 | loss: 98.8974275CurrentTrain: epoch  8, batch    36 | loss: 98.2443452CurrentTrain: epoch  8, batch    37 | loss: 176.3504580CurrentTrain: epoch  8, batch    38 | loss: 96.8251657CurrentTrain: epoch  8, batch    39 | loss: 96.4038050CurrentTrain: epoch  8, batch    40 | loss: 77.5730533CurrentTrain: epoch  8, batch    41 | loss: 100.6448234CurrentTrain: epoch  8, batch    42 | loss: 123.8773236CurrentTrain: epoch  8, batch    43 | loss: 173.0139992CurrentTrain: epoch  8, batch    44 | loss: 128.8324168CurrentTrain: epoch  8, batch    45 | loss: 172.9396149CurrentTrain: epoch  8, batch    46 | loss: 81.9721177CurrentTrain: epoch  8, batch    47 | loss: 126.0921143CurrentTrain: epoch  8, batch    48 | loss: 77.1522065CurrentTrain: epoch  8, batch    49 | loss: 126.3918760CurrentTrain: epoch  8, batch    50 | loss: 95.1223194CurrentTrain: epoch  8, batch    51 | loss: 82.0330690CurrentTrain: epoch  8, batch    52 | loss: 92.7198672CurrentTrain: epoch  8, batch    53 | loss: 75.8748286CurrentTrain: epoch  8, batch    54 | loss: 97.2572862CurrentTrain: epoch  8, batch    55 | loss: 98.3427020CurrentTrain: epoch  8, batch    56 | loss: 98.9498153CurrentTrain: epoch  8, batch    57 | loss: 122.7111524CurrentTrain: epoch  8, batch    58 | loss: 98.2475041CurrentTrain: epoch  8, batch    59 | loss: 101.0651456CurrentTrain: epoch  8, batch    60 | loss: 126.7093839CurrentTrain: epoch  8, batch    61 | loss: 92.7866580CurrentTrain: epoch  8, batch    62 | loss: 79.5319795CurrentTrain: epoch  8, batch    63 | loss: 122.5023174CurrentTrain: epoch  8, batch    64 | loss: 128.6962254CurrentTrain: epoch  8, batch    65 | loss: 95.4415097CurrentTrain: epoch  8, batch    66 | loss: 126.7594115CurrentTrain: epoch  8, batch    67 | loss: 96.7140719CurrentTrain: epoch  8, batch    68 | loss: 95.3534850CurrentTrain: epoch  8, batch    69 | loss: 80.9929577CurrentTrain: epoch  8, batch    70 | loss: 79.5420985CurrentTrain: epoch  8, batch    71 | loss: 74.6086223CurrentTrain: epoch  8, batch    72 | loss: 94.5181862CurrentTrain: epoch  8, batch    73 | loss: 99.2422741CurrentTrain: epoch  8, batch    74 | loss: 77.4805109CurrentTrain: epoch  8, batch    75 | loss: 119.7665005CurrentTrain: epoch  8, batch    76 | loss: 74.9408461CurrentTrain: epoch  8, batch    77 | loss: 128.5459678CurrentTrain: epoch  8, batch    78 | loss: 125.8678360CurrentTrain: epoch  8, batch    79 | loss: 67.0896496CurrentTrain: epoch  8, batch    80 | loss: 119.6626124CurrentTrain: epoch  8, batch    81 | loss: 166.8124484CurrentTrain: epoch  8, batch    82 | loss: 99.6571125CurrentTrain: epoch  8, batch    83 | loss: 126.8034708CurrentTrain: epoch  8, batch    84 | loss: 96.9411208CurrentTrain: epoch  8, batch    85 | loss: 124.1000449CurrentTrain: epoch  8, batch    86 | loss: 99.1597845CurrentTrain: epoch  8, batch    87 | loss: 126.2028746CurrentTrain: epoch  8, batch    88 | loss: 174.1921167CurrentTrain: epoch  8, batch    89 | loss: 94.3817035CurrentTrain: epoch  8, batch    90 | loss: 79.7364603CurrentTrain: epoch  8, batch    91 | loss: 128.8754820CurrentTrain: epoch  8, batch    92 | loss: 71.4935943CurrentTrain: epoch  8, batch    93 | loss: 99.2129923CurrentTrain: epoch  8, batch    94 | loss: 78.7136102CurrentTrain: epoch  8, batch    95 | loss: 81.8802919CurrentTrain: epoch  9, batch     0 | loss: 121.4398961CurrentTrain: epoch  9, batch     1 | loss: 100.0346337CurrentTrain: epoch  9, batch     2 | loss: 73.6914816CurrentTrain: epoch  9, batch     3 | loss: 96.8200143CurrentTrain: epoch  9, batch     4 | loss: 96.5068911CurrentTrain: epoch  9, batch     5 | loss: 96.9574089CurrentTrain: epoch  9, batch     6 | loss: 170.2288091CurrentTrain: epoch  9, batch     7 | loss: 76.1399933CurrentTrain: epoch  9, batch     8 | loss: 126.2812673CurrentTrain: epoch  9, batch     9 | loss: 126.6939860CurrentTrain: epoch  9, batch    10 | loss: 98.0210756CurrentTrain: epoch  9, batch    11 | loss: 97.6743952CurrentTrain: epoch  9, batch    12 | loss: 123.2464339CurrentTrain: epoch  9, batch    13 | loss: 97.8829564CurrentTrain: epoch  9, batch    14 | loss: 96.3826291CurrentTrain: epoch  9, batch    15 | loss: 123.5857045CurrentTrain: epoch  9, batch    16 | loss: 71.7934950CurrentTrain: epoch  9, batch    17 | loss: 95.3229099CurrentTrain: epoch  9, batch    18 | loss: 94.8971440CurrentTrain: epoch  9, batch    19 | loss: 101.2834249CurrentTrain: epoch  9, batch    20 | loss: 98.1916279CurrentTrain: epoch  9, batch    21 | loss: 99.7870936CurrentTrain: epoch  9, batch    22 | loss: 97.2433238CurrentTrain: epoch  9, batch    23 | loss: 74.8975796CurrentTrain: epoch  9, batch    24 | loss: 100.0502251CurrentTrain: epoch  9, batch    25 | loss: 166.8919946CurrentTrain: epoch  9, batch    26 | loss: 65.2341814CurrentTrain: epoch  9, batch    27 | loss: 79.8434233CurrentTrain: epoch  9, batch    28 | loss: 97.2627897CurrentTrain: epoch  9, batch    29 | loss: 93.8045584CurrentTrain: epoch  9, batch    30 | loss: 173.1131119CurrentTrain: epoch  9, batch    31 | loss: 97.1664953CurrentTrain: epoch  9, batch    32 | loss: 100.4939024CurrentTrain: epoch  9, batch    33 | loss: 98.5882189CurrentTrain: epoch  9, batch    34 | loss: 272.5743907CurrentTrain: epoch  9, batch    35 | loss: 99.9344030CurrentTrain: epoch  9, batch    36 | loss: 126.8281790CurrentTrain: epoch  9, batch    37 | loss: 72.5656249CurrentTrain: epoch  9, batch    38 | loss: 78.1386159CurrentTrain: epoch  9, batch    39 | loss: 126.4975239CurrentTrain: epoch  9, batch    40 | loss: 93.8365446CurrentTrain: epoch  9, batch    41 | loss: 77.1376359CurrentTrain: epoch  9, batch    42 | loss: 94.3178159CurrentTrain: epoch  9, batch    43 | loss: 80.3722527CurrentTrain: epoch  9, batch    44 | loss: 130.3545594CurrentTrain: epoch  9, batch    45 | loss: 126.7414659CurrentTrain: epoch  9, batch    46 | loss: 95.0705253CurrentTrain: epoch  9, batch    47 | loss: 124.3701237CurrentTrain: epoch  9, batch    48 | loss: 123.9284961CurrentTrain: epoch  9, batch    49 | loss: 81.4004990CurrentTrain: epoch  9, batch    50 | loss: 124.7989091CurrentTrain: epoch  9, batch    51 | loss: 79.6801241CurrentTrain: epoch  9, batch    52 | loss: 128.9431844CurrentTrain: epoch  9, batch    53 | loss: 81.3355112CurrentTrain: epoch  9, batch    54 | loss: 77.0484938CurrentTrain: epoch  9, batch    55 | loss: 96.8673578CurrentTrain: epoch  9, batch    56 | loss: 98.4154198CurrentTrain: epoch  9, batch    57 | loss: 77.6752726CurrentTrain: epoch  9, batch    58 | loss: 63.6018005CurrentTrain: epoch  9, batch    59 | loss: 122.5665751CurrentTrain: epoch  9, batch    60 | loss: 124.0543211CurrentTrain: epoch  9, batch    61 | loss: 172.8430684CurrentTrain: epoch  9, batch    62 | loss: 126.0529621CurrentTrain: epoch  9, batch    63 | loss: 122.4819224CurrentTrain: epoch  9, batch    64 | loss: 96.5250576CurrentTrain: epoch  9, batch    65 | loss: 77.1162580CurrentTrain: epoch  9, batch    66 | loss: 96.3420746CurrentTrain: epoch  9, batch    67 | loss: 100.0122367CurrentTrain: epoch  9, batch    68 | loss: 95.5682372CurrentTrain: epoch  9, batch    69 | loss: 99.8882814CurrentTrain: epoch  9, batch    70 | loss: 96.3046022CurrentTrain: epoch  9, batch    71 | loss: 78.5670227CurrentTrain: epoch  9, batch    72 | loss: 63.5439048CurrentTrain: epoch  9, batch    73 | loss: 96.4525949CurrentTrain: epoch  9, batch    74 | loss: 128.6278216CurrentTrain: epoch  9, batch    75 | loss: 80.1417638CurrentTrain: epoch  9, batch    76 | loss: 131.3036751CurrentTrain: epoch  9, batch    77 | loss: 76.4074651CurrentTrain: epoch  9, batch    78 | loss: 64.6146907CurrentTrain: epoch  9, batch    79 | loss: 95.1871894CurrentTrain: epoch  9, batch    80 | loss: 176.2439345CurrentTrain: epoch  9, batch    81 | loss: 101.7192298CurrentTrain: epoch  9, batch    82 | loss: 81.8136380CurrentTrain: epoch  9, batch    83 | loss: 125.4625791CurrentTrain: epoch  9, batch    84 | loss: 124.5131795CurrentTrain: epoch  9, batch    85 | loss: 75.5304440CurrentTrain: epoch  9, batch    86 | loss: 93.4314734CurrentTrain: epoch  9, batch    87 | loss: 128.6120189CurrentTrain: epoch  9, batch    88 | loss: 124.8939450CurrentTrain: epoch  9, batch    89 | loss: 95.7475120CurrentTrain: epoch  9, batch    90 | loss: 93.9302874CurrentTrain: epoch  9, batch    91 | loss: 78.0352704CurrentTrain: epoch  9, batch    92 | loss: 98.0795065CurrentTrain: epoch  9, batch    93 | loss: 96.9418919CurrentTrain: epoch  9, batch    94 | loss: 121.5113888CurrentTrain: epoch  9, batch    95 | loss: 81.6729520

F1 score per class: {32: 0.6555555555555556, 6: 0.8636363636363636, 19: 0.19047619047619047, 24: 0.7684210526315789, 26: 0.9583333333333334, 29: 0.9090909090909091}
Micro-average F1 score: 0.819226750261233
Weighted-average F1 score: 0.8315893101290536
F1 score per class: {32: 0.7046632124352331, 6: 0.8888888888888888, 19: 0.5714285714285714, 24: 0.7675675675675676, 26: 0.9795918367346939, 29: 0.9054726368159204}
Micro-average F1 score: 0.8423194303153612
Weighted-average F1 score: 0.8453674655270007
F1 score per class: {32: 0.7046632124352331, 6: 0.8888888888888888, 19: 0.5714285714285714, 24: 0.7675675675675676, 26: 0.9795918367346939, 29: 0.9054726368159204}
Micro-average F1 score: 0.8423194303153612
Weighted-average F1 score: 0.8453674655270007

F1 score per class: {32: 0.6555555555555556, 6: 0.8636363636363636, 19: 0.19047619047619047, 24: 0.7684210526315789, 26: 0.9583333333333334, 29: 0.9090909090909091}
Micro-average F1 score: 0.819226750261233
Weighted-average F1 score: 0.8315893101290536
F1 score per class: {32: 0.7046632124352331, 6: 0.8888888888888888, 19: 0.5714285714285714, 24: 0.7675675675675676, 26: 0.9795918367346939, 29: 0.9054726368159204}
Micro-average F1 score: 0.8423194303153612
Weighted-average F1 score: 0.8453674655270007
F1 score per class: {32: 0.7046632124352331, 6: 0.8888888888888888, 19: 0.5714285714285714, 24: 0.7675675675675676, 26: 0.9795918367346939, 29: 0.9054726368159204}
Micro-average F1 score: 0.8423194303153612
Weighted-average F1 score: 0.8453674655270007

F1 score per class: {32: 0.47580645161290325, 6: 0.6495726495726496, 19: 0.12121212121212122, 24: 0.7053140096618358, 26: 0.9064039408866995, 29: 0.8071748878923767}
Micro-average F1 score: 0.6829268292682927
Weighted-average F1 score: 0.6783983934214847
F1 score per class: {32: 0.4722222222222222, 6: 0.64, 19: 0.1702127659574468, 24: 0.6859903381642513, 26: 0.8930232558139535, 29: 0.6816479400749064}
Micro-average F1 score: 0.6267978803936411
Weighted-average F1 score: 0.6072824341036521
F1 score per class: {32: 0.47058823529411764, 6: 0.64, 19: 0.18181818181818182, 24: 0.6859903381642513, 26: 0.897196261682243, 29: 0.6867924528301886}
Micro-average F1 score: 0.6306169078446306
Weighted-average F1 score: 0.6121642062015002

F1 score per class: {32: 0.47580645161290325, 6: 0.6495726495726496, 19: 0.12121212121212122, 24: 0.7053140096618358, 26: 0.9064039408866995, 29: 0.8071748878923767}
Micro-average F1 score: 0.6829268292682927
Weighted-average F1 score: 0.6783983934214847
F1 score per class: {32: 0.4722222222222222, 6: 0.64, 19: 0.1702127659574468, 24: 0.6859903381642513, 26: 0.8930232558139535, 29: 0.6816479400749064}
Micro-average F1 score: 0.6267978803936411
Weighted-average F1 score: 0.6072824341036521
F1 score per class: {32: 0.47058823529411764, 6: 0.64, 19: 0.18181818181818182, 24: 0.6859903381642513, 26: 0.897196261682243, 29: 0.6867924528301886}
Micro-average F1 score: 0.6306169078446306
Weighted-average F1 score: 0.6121642062015002
cur_acc_wo_na:  ['0.8192']
his_acc_wo_na:  ['0.8192']
cur_acc des_wo_na:  ['0.8423']
his_acc des_wo_na:  ['0.8423']
cur_acc rrf_wo_na:  ['0.8423']
his_acc rrf_wo_na:  ['0.8423']
cur_acc_w_na:  ['0.6829']
his_acc_w_na:  ['0.6829']
cur_acc des_w_na:  ['0.6268']
his_acc des_w_na:  ['0.6268']
cur_acc rrf_w_na:  ['0.6306']
his_acc rrf_w_na:  ['0.6306']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 112.4163426CurrentTrain: epoch  0, batch     1 | loss: 92.2440235CurrentTrain: epoch  0, batch     2 | loss: 155.1227781CurrentTrain: epoch  0, batch     3 | loss: 388.5183259CurrentTrain: epoch  1, batch     0 | loss: 111.5097080CurrentTrain: epoch  1, batch     1 | loss: 106.3595654CurrentTrain: epoch  1, batch     2 | loss: 140.9277702CurrentTrain: epoch  1, batch     3 | loss: 74.5752793CurrentTrain: epoch  2, batch     0 | loss: 110.9037015CurrentTrain: epoch  2, batch     1 | loss: 101.2540852CurrentTrain: epoch  2, batch     2 | loss: 132.5324117CurrentTrain: epoch  2, batch     3 | loss: 74.4547937CurrentTrain: epoch  3, batch     0 | loss: 133.2852275CurrentTrain: epoch  3, batch     1 | loss: 96.0948166CurrentTrain: epoch  3, batch     2 | loss: 106.9412507CurrentTrain: epoch  3, batch     3 | loss: 93.5636157CurrentTrain: epoch  4, batch     0 | loss: 105.5583649CurrentTrain: epoch  4, batch     1 | loss: 104.8487575CurrentTrain: epoch  4, batch     2 | loss: 83.5753152CurrentTrain: epoch  4, batch     3 | loss: 58.2088376CurrentTrain: epoch  5, batch     0 | loss: 130.2874350CurrentTrain: epoch  5, batch     1 | loss: 84.3915688CurrentTrain: epoch  5, batch     2 | loss: 100.8911636CurrentTrain: epoch  5, batch     3 | loss: 85.5710028CurrentTrain: epoch  6, batch     0 | loss: 177.9656404CurrentTrain: epoch  6, batch     1 | loss: 83.2149178CurrentTrain: epoch  6, batch     2 | loss: 97.4558789CurrentTrain: epoch  6, batch     3 | loss: 54.8394375CurrentTrain: epoch  7, batch     0 | loss: 131.3789582CurrentTrain: epoch  7, batch     1 | loss: 79.2839519CurrentTrain: epoch  7, batch     2 | loss: 101.7964417CurrentTrain: epoch  7, batch     3 | loss: 64.4232220CurrentTrain: epoch  8, batch     0 | loss: 98.2684909CurrentTrain: epoch  8, batch     1 | loss: 81.1888699CurrentTrain: epoch  8, batch     2 | loss: 98.0030156CurrentTrain: epoch  8, batch     3 | loss: 85.2155112CurrentTrain: epoch  9, batch     0 | loss: 97.7311269CurrentTrain: epoch  9, batch     1 | loss: 77.2629618CurrentTrain: epoch  9, batch     2 | loss: 82.7256810CurrentTrain: epoch  9, batch     3 | loss: 85.2592817
MemoryTrain:  epoch  0, batch     0 | loss: 1.3460871MemoryTrain:  epoch  1, batch     0 | loss: 1.0551616MemoryTrain:  epoch  2, batch     0 | loss: 0.8388236MemoryTrain:  epoch  3, batch     0 | loss: 0.6977792MemoryTrain:  epoch  4, batch     0 | loss: 0.6397194MemoryTrain:  epoch  5, batch     0 | loss: 0.5005614MemoryTrain:  epoch  6, batch     0 | loss: 0.3437516MemoryTrain:  epoch  7, batch     0 | loss: 0.2495504MemoryTrain:  epoch  8, batch     0 | loss: 0.1985476MemoryTrain:  epoch  9, batch     0 | loss: 0.1827227

F1 score per class: {35: 0.9473684210526315, 37: 0.44776119402985076, 38: 0.0, 15: 0.9607843137254902, 25: 0.42105263157894735, 26: 0.8333333333333334}
Micro-average F1 score: 0.6964856230031949
Weighted-average F1 score: 0.7825557363558149
F1 score per class: {35: 0.0, 37: 0.8888888888888888, 38: 0.0, 6: 0.8695652173913043, 15: 0.0, 24: 0.9803921568627451, 25: 0.6813186813186813, 26: 0.8771929824561403}
Micro-average F1 score: 0.8484848484848485
Weighted-average F1 score: 0.8546493544374721
F1 score per class: {35: 0.9473684210526315, 37: 0.8571428571428571, 38: 0.0, 15: 0.970873786407767, 25: 0.6739130434782609, 26: 0.8679245283018868}
Micro-average F1 score: 0.8467966573816156
Weighted-average F1 score: 0.8588838905702563

F1 score per class: {32: 0.6704545454545454, 35: 0.9473684210526315, 37: 0.7577639751552795, 6: 0.10526315789473684, 38: 0.44776119402985076, 15: 0.7634408602150538, 19: 0.9583333333333334, 24: 0.8022598870056498, 25: 0.7716535433070866, 26: 0.4, 29: 0.7843137254901961}
Micro-average F1 score: 0.7394422310756972
Weighted-average F1 score: 0.7701634733564237
F1 score per class: {32: 0.7437185929648241, 35: 0.8888888888888888, 37: 0.907103825136612, 6: 0.5185185185185185, 38: 0.8695652173913043, 15: 0.7526881720430108, 19: 0.9795918367346939, 24: 0.8762886597938144, 25: 0.8695652173913043, 26: 0.6739130434782609, 29: 0.8333333333333334}
Micro-average F1 score: 0.8355359765051396
Weighted-average F1 score: 0.8405992780360015
F1 score per class: {32: 0.7474747474747475, 35: 0.9473684210526315, 37: 0.8235294117647058, 6: 0.19047619047619047, 38: 0.8571428571428571, 15: 0.7526881720430108, 19: 0.9690721649484536, 24: 0.8808290155440415, 25: 0.8620689655172413, 26: 0.6526315789473685, 29: 0.8363636363636363}
Micro-average F1 score: 0.8176382660687593
Weighted-average F1 score: 0.8281508413821883

F1 score per class: {32: 0.0, 35: 0.9473684210526315, 37: 0.0, 38: 0.44776119402985076, 6: 0.0, 15: 0.0, 19: 0.0, 25: 0.6363636363636364, 26: 0.3404255319148936, 29: 0.5555555555555556}
Micro-average F1 score: 0.46284501061571126
Weighted-average F1 score: 0.4326188917589882
F1 score per class: {32: 0.0, 35: 0.7619047619047619, 37: 0.0, 38: 0.0, 6: 0.8247422680412371, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.5847953216374269, 26: 0.4025974025974026, 29: 0.3875968992248062}
Micro-average F1 score: 0.4536082474226804
Weighted-average F1 score: 0.40305697846267896
F1 score per class: {32: 0.0, 35: 0.8571428571428571, 37: 0.0, 38: 0.8125, 6: 0.0, 15: 0.0, 19: 0.0, 25: 0.5617977528089888, 26: 0.39490445859872614, 29: 0.4339622641509434}
Micro-average F1 score: 0.475
Weighted-average F1 score: 0.4297988043807814

F1 score per class: {32: 0.46825396825396826, 35: 0.782608695652174, 37: 0.5648148148148148, 6: 0.09090909090909091, 38: 0.44776119402985076, 15: 0.6893203883495146, 19: 0.832579185520362, 24: 0.7064676616915423, 25: 0.35251798561151076, 26: 0.2689075630252101, 29: 0.3053435114503817}
Micro-average F1 score: 0.5345622119815668
Weighted-average F1 score: 0.5176478624250067
F1 score per class: {32: 0.4444444444444444, 35: 0.42105263157894735, 37: 0.5424836601307189, 6: 0.175, 38: 0.8247422680412371, 15: 0.660377358490566, 19: 0.8240343347639485, 24: 0.7623318385650224, 25: 0.28901734104046245, 26: 0.26956521739130435, 29: 0.18587360594795538}
Micro-average F1 score: 0.4807773553020701
Weighted-average F1 score: 0.44190986268416965
F1 score per class: {32: 0.46540880503144655, 35: 0.5142857142857142, 37: 0.5490196078431373, 6: 0.14285714285714285, 38: 0.8125, 15: 0.6698564593301436, 19: 0.8245614035087719, 24: 0.7657657657657657, 25: 0.2702702702702703, 26: 0.2551440329218107, 29: 0.2222222222222222}
Micro-average F1 score: 0.4947987336047038
Weighted-average F1 score: 0.45660848670004905
cur_acc_wo_na:  ['0.8192', '0.6965']
his_acc_wo_na:  ['0.8192', '0.7394']
cur_acc des_wo_na:  ['0.8423', '0.8485']
his_acc des_wo_na:  ['0.8423', '0.8355']
cur_acc rrf_wo_na:  ['0.8423', '0.8468']
his_acc rrf_wo_na:  ['0.8423', '0.8176']
cur_acc_w_na:  ['0.6829', '0.4628']
his_acc_w_na:  ['0.6829', '0.5346']
cur_acc des_w_na:  ['0.6268', '0.4536']
his_acc des_w_na:  ['0.6268', '0.4808']
cur_acc rrf_w_na:  ['0.6306', '0.4750']
his_acc rrf_w_na:  ['0.6306', '0.4948']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 94.1509860CurrentTrain: epoch  0, batch     1 | loss: 97.3805817CurrentTrain: epoch  0, batch     2 | loss: 118.1737194CurrentTrain: epoch  0, batch     3 | loss: 120.3250520CurrentTrain: epoch  1, batch     0 | loss: 108.7091972CurrentTrain: epoch  1, batch     1 | loss: 87.4681901CurrentTrain: epoch  1, batch     2 | loss: 140.2741049CurrentTrain: epoch  1, batch     3 | loss: 82.6649020CurrentTrain: epoch  2, batch     0 | loss: 87.0256109CurrentTrain: epoch  2, batch     1 | loss: 133.5861168CurrentTrain: epoch  2, batch     2 | loss: 82.4710679CurrentTrain: epoch  2, batch     3 | loss: 61.6539600CurrentTrain: epoch  3, batch     0 | loss: 101.1515565CurrentTrain: epoch  3, batch     1 | loss: 129.5113297CurrentTrain: epoch  3, batch     2 | loss: 85.6062355CurrentTrain: epoch  3, batch     3 | loss: 60.1598093CurrentTrain: epoch  4, batch     0 | loss: 80.9919859CurrentTrain: epoch  4, batch     1 | loss: 129.7715272CurrentTrain: epoch  4, batch     2 | loss: 85.1883691CurrentTrain: epoch  4, batch     3 | loss: 106.7146368CurrentTrain: epoch  5, batch     0 | loss: 83.0744617CurrentTrain: epoch  5, batch     1 | loss: 77.4870008CurrentTrain: epoch  5, batch     2 | loss: 129.8503244CurrentTrain: epoch  5, batch     3 | loss: 77.4221783CurrentTrain: epoch  6, batch     0 | loss: 78.2186540CurrentTrain: epoch  6, batch     1 | loss: 101.1373930CurrentTrain: epoch  6, batch     2 | loss: 97.0650972CurrentTrain: epoch  6, batch     3 | loss: 107.4956415CurrentTrain: epoch  7, batch     0 | loss: 94.2036160CurrentTrain: epoch  7, batch     1 | loss: 123.6825834CurrentTrain: epoch  7, batch     2 | loss: 127.4677109CurrentTrain: epoch  7, batch     3 | loss: 58.2957763CurrentTrain: epoch  8, batch     0 | loss: 78.6856032CurrentTrain: epoch  8, batch     1 | loss: 96.5695138CurrentTrain: epoch  8, batch     2 | loss: 98.7865997CurrentTrain: epoch  8, batch     3 | loss: 73.0009929CurrentTrain: epoch  9, batch     0 | loss: 92.5320089CurrentTrain: epoch  9, batch     1 | loss: 79.7232452CurrentTrain: epoch  9, batch     2 | loss: 124.1156902CurrentTrain: epoch  9, batch     3 | loss: 60.7843349
MemoryTrain:  epoch  0, batch     0 | loss: 0.9170506MemoryTrain:  epoch  1, batch     0 | loss: 0.7157788MemoryTrain:  epoch  2, batch     0 | loss: 0.5649004MemoryTrain:  epoch  3, batch     0 | loss: 0.4182670MemoryTrain:  epoch  4, batch     0 | loss: 0.3451486MemoryTrain:  epoch  5, batch     0 | loss: 0.3232006MemoryTrain:  epoch  6, batch     0 | loss: 0.2398873MemoryTrain:  epoch  7, batch     0 | loss: 0.2002337MemoryTrain:  epoch  8, batch     0 | loss: 0.1703949MemoryTrain:  epoch  9, batch     0 | loss: 0.1418003

F1 score per class: {33: 0.3125, 35: 0.8571428571428571, 36: 0.0, 37: 0.0, 38: 0.8823529411764706, 8: 0.42857142857142855, 20: 0.0, 26: 0.345679012345679, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.536144578313253
Weighted-average F1 score: 0.6149304881468539
F1 score per class: {33: 0.8, 35: 0.8, 36: 0.0, 37: 0.0, 38: 0.0, 8: 0.918918918918919, 20: 0.5333333333333333, 25: 0.0, 26: 0.847457627118644, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7893462469733656
Weighted-average F1 score: 0.762316870791447
F1 score per class: {33: 0.7441860465116279, 35: 0.8247422680412371, 36: 0.0, 37: 0.0, 38: 0.918918918918919, 8: 0.5333333333333333, 20: 0.0, 26: 0.847457627118644, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7794117647058824
Weighted-average F1 score: 0.756646995719631

F1 score per class: {32: 0.5477707006369427, 33: 0.30612244897959184, 35: 0.9473684210526315, 36: 0.7096774193548387, 37: 0.6461538461538462, 6: 0.10526315789473684, 38: 0.4, 8: 0.7692307692307693, 15: 0.9197860962566845, 19: 0.8823529411764706, 20: 0.7954545454545454, 24: 0.35294117647058826, 25: 0.7572815533980582, 26: 0.34146341463414637, 29: 0.5116279069767442, 30: 0.34285714285714286}
Micro-average F1 score: 0.6511326860841424
Weighted-average F1 score: 0.7022667596017598
F1 score per class: {32: 0.6740331491712708, 33: 0.7397260273972602, 35: 0.8888888888888888, 36: 0.7878787878787878, 37: 0.608, 6: 0.4, 38: 0.45714285714285713, 8: 0.7692307692307693, 15: 0.9312169312169312, 19: 0.6538461538461539, 20: 0.8691099476439791, 24: 0.4444444444444444, 25: 0.8245614035087719, 26: 0.7246376811594203, 29: 0.6153846153846154, 30: 0.7450980392156863}
Micro-average F1 score: 0.7437357630979499
Weighted-average F1 score: 0.7509508801957564
F1 score per class: {32: 0.6514285714285715, 33: 0.6808510638297872, 35: 0.9473684210526315, 36: 0.8095238095238095, 37: 0.6201550387596899, 6: 0.2, 38: 0.42424242424242425, 8: 0.7692307692307693, 15: 0.9312169312169312, 19: 0.6938775510204082, 20: 0.851063829787234, 24: 0.42105263157894735, 25: 0.8034188034188035, 26: 0.7299270072992701, 29: 0.6451612903225806, 30: 0.5777777777777777}
Micro-average F1 score: 0.7334484743811168
Weighted-average F1 score: 0.7460352748118176

F1 score per class: {32: 0.0, 33: 0.26548672566371684, 35: 0.0, 36: 0.0, 37: 0.5915492957746479, 38: 0.0, 6: 0.0, 8: 0.8333333333333334, 15: 0.0, 19: 0.42857142857142855, 20: 0.0, 26: 0.3146067415730337, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.3677685950413223
Weighted-average F1 score: 0.3290128429168723
F1 score per class: {32: 0.0, 33: 0.6242774566473989, 35: 0.0, 36: 0.0, 37: 0.4840764331210191, 38: 0.0, 6: 0.0, 8: 0.0, 15: 0.0, 19: 0.7083333333333334, 20: 0.0, 24: 0.42105263157894735, 25: 0.0, 26: 0.5494505494505495, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.3918269230769231
Weighted-average F1 score: 0.3243589851557198
F1 score per class: {32: 0.0, 33: 0.5818181818181818, 35: 0.0, 36: 0.0, 37: 0.49079754601226994, 38: 0.0, 6: 0.0, 8: 0.7391304347826086, 15: 0.0, 19: 0.4, 20: 0.0, 26: 0.5617977528089888, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.4005037783375315
Weighted-average F1 score: 0.33543078124653974

F1 score per class: {32: 0.3739130434782609, 33: 0.23809523809523808, 35: 0.6923076923076923, 36: 0.5238095238095238, 37: 0.27906976744186046, 6: 0.09523809523809523, 38: 0.4, 8: 0.6730769230769231, 15: 0.819047619047619, 19: 0.8333333333333334, 20: 0.693069306930693, 24: 0.2, 25: 0.35294117647058826, 26: 0.28865979381443296, 29: 0.4731182795698925, 30: 0.25}
Micro-average F1 score: 0.4736346516007533
Weighted-average F1 score: 0.46289909497570797
F1 score per class: {32: 0.3948220064724919, 33: 0.44081632653061226, 35: 0.41025641025641024, 36: 0.49429657794676807, 37: 0.23974763406940064, 6: 0.19230769230769232, 38: 0.43243243243243246, 8: 0.6222222222222222, 15: 0.7963800904977375, 19: 0.2556390977443609, 20: 0.7124463519313304, 24: 0.2, 25: 0.27167630057803466, 26: 0.37174721189591076, 29: 0.30601092896174864, 30: 0.2289156626506024}
Micro-average F1 score: 0.4192616372391653
Weighted-average F1 score: 0.39494778147341997
F1 score per class: {32: 0.3890784982935154, 33: 0.41739130434782606, 35: 0.5294117647058824, 36: 0.5112781954887218, 37: 0.24464831804281345, 6: 0.18181818181818182, 38: 0.4117647058823529, 8: 0.6481481481481481, 15: 0.8148148148148148, 19: 0.3177570093457944, 20: 0.7111111111111111, 24: 0.1568627450980392, 25: 0.2640449438202247, 26: 0.3816793893129771, 29: 0.3468208092485549, 30: 0.203125}
Micro-average F1 score: 0.42837928715534634
Weighted-average F1 score: 0.4034791773735992
cur_acc_wo_na:  ['0.8192', '0.6965', '0.5361']
his_acc_wo_na:  ['0.8192', '0.7394', '0.6511']
cur_acc des_wo_na:  ['0.8423', '0.8485', '0.7893']
his_acc des_wo_na:  ['0.8423', '0.8355', '0.7437']
cur_acc rrf_wo_na:  ['0.8423', '0.8468', '0.7794']
his_acc rrf_wo_na:  ['0.8423', '0.8176', '0.7334']
cur_acc_w_na:  ['0.6829', '0.4628', '0.3678']
his_acc_w_na:  ['0.6829', '0.5346', '0.4736']
cur_acc des_w_na:  ['0.6268', '0.4536', '0.3918']
his_acc des_w_na:  ['0.6268', '0.4808', '0.4193']
cur_acc rrf_w_na:  ['0.6306', '0.4750', '0.4005']
his_acc rrf_w_na:  ['0.6306', '0.4948', '0.4284']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 103.9740357CurrentTrain: epoch  0, batch     1 | loss: 115.4507264CurrentTrain: epoch  0, batch     2 | loss: 145.9255354CurrentTrain: epoch  0, batch     3 | loss: 193.5353199CurrentTrain: epoch  0, batch     4 | loss: 81.5160657CurrentTrain: epoch  1, batch     0 | loss: 114.2395002CurrentTrain: epoch  1, batch     1 | loss: 139.5876287CurrentTrain: epoch  1, batch     2 | loss: 103.4769988CurrentTrain: epoch  1, batch     3 | loss: 136.4147087CurrentTrain: epoch  1, batch     4 | loss: 62.6867629CurrentTrain: epoch  2, batch     0 | loss: 103.6782279CurrentTrain: epoch  2, batch     1 | loss: 175.6749301CurrentTrain: epoch  2, batch     2 | loss: 109.9235414CurrentTrain: epoch  2, batch     3 | loss: 103.2485387CurrentTrain: epoch  2, batch     4 | loss: 75.4210943CurrentTrain: epoch  3, batch     0 | loss: 103.0887181CurrentTrain: epoch  3, batch     1 | loss: 132.5425981CurrentTrain: epoch  3, batch     2 | loss: 101.8534267CurrentTrain: epoch  3, batch     3 | loss: 102.4566992CurrentTrain: epoch  3, batch     4 | loss: 73.2165596CurrentTrain: epoch  4, batch     0 | loss: 104.0383342CurrentTrain: epoch  4, batch     1 | loss: 132.1901930CurrentTrain: epoch  4, batch     2 | loss: 101.2335643CurrentTrain: epoch  4, batch     3 | loss: 102.8439219CurrentTrain: epoch  4, batch     4 | loss: 145.8211095CurrentTrain: epoch  5, batch     0 | loss: 128.5120620CurrentTrain: epoch  5, batch     1 | loss: 98.1445194CurrentTrain: epoch  5, batch     2 | loss: 101.9541563CurrentTrain: epoch  5, batch     3 | loss: 130.6409774CurrentTrain: epoch  5, batch     4 | loss: 53.9217156CurrentTrain: epoch  6, batch     0 | loss: 128.5442877CurrentTrain: epoch  6, batch     1 | loss: 179.3569875CurrentTrain: epoch  6, batch     2 | loss: 101.4714236CurrentTrain: epoch  6, batch     3 | loss: 79.3139146CurrentTrain: epoch  6, batch     4 | loss: 96.7767552CurrentTrain: epoch  7, batch     0 | loss: 99.3710429CurrentTrain: epoch  7, batch     1 | loss: 99.4174422CurrentTrain: epoch  7, batch     2 | loss: 99.0176356CurrentTrain: epoch  7, batch     3 | loss: 267.2648194CurrentTrain: epoch  7, batch     4 | loss: 95.1911660CurrentTrain: epoch  8, batch     0 | loss: 176.0967354CurrentTrain: epoch  8, batch     1 | loss: 82.5554591CurrentTrain: epoch  8, batch     2 | loss: 97.1489415CurrentTrain: epoch  8, batch     3 | loss: 79.0748468CurrentTrain: epoch  8, batch     4 | loss: 149.6384709CurrentTrain: epoch  9, batch     0 | loss: 97.7943051CurrentTrain: epoch  9, batch     1 | loss: 124.7180997CurrentTrain: epoch  9, batch     2 | loss: 98.9541996CurrentTrain: epoch  9, batch     3 | loss: 80.2052807CurrentTrain: epoch  9, batch     4 | loss: 155.1188581
MemoryTrain:  epoch  0, batch     0 | loss: 1.3844339MemoryTrain:  epoch  1, batch     0 | loss: 1.2454302MemoryTrain:  epoch  2, batch     0 | loss: 1.0681890MemoryTrain:  epoch  3, batch     0 | loss: 0.9091547MemoryTrain:  epoch  4, batch     0 | loss: 0.7438387MemoryTrain:  epoch  5, batch     0 | loss: 0.5963555MemoryTrain:  epoch  6, batch     0 | loss: 0.5182593MemoryTrain:  epoch  7, batch     0 | loss: 0.4678289MemoryTrain:  epoch  8, batch     0 | loss: 0.3751686MemoryTrain:  epoch  9, batch     0 | loss: 0.3530307

F1 score per class: {32: 0.3291139240506329, 1: 0.21739130434782608, 34: 0.5825242718446602, 35: 0.8137254901960784, 3: 0.0, 37: 0.0, 14: 0.0, 22: 0.5526315789473685, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.49139280125195617
Weighted-average F1 score: 0.45205780293637965
F1 score per class: {32: 0.21875, 1: 0.4230769230769231, 34: 0.5777777777777777, 3: 0.8235294117647058, 35: 0.0, 37: 0.0, 36: 0.0, 14: 0.625, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.513595166163142
Weighted-average F1 score: 0.4486913131943558
F1 score per class: {32: 0.27692307692307694, 1: 0.4077669902912621, 34: 0.6060606060606061, 3: 0.8315789473684211, 35: 0.0, 37: 0.0, 14: 0.0, 22: 0.6419753086419753, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.5303030303030303
Weighted-average F1 score: 0.4679375303583369

F1 score per class: {1: 0.3132530120481928, 3: 0.20408163265306123, 6: 0.5298013245033113, 8: 0.2857142857142857, 14: 0.5504587155963303, 15: 0.8235294117647058, 19: 0.4251968503937008, 20: 0.6, 22: 0.7649769585253456, 24: 0.09523809523809523, 25: 0.4, 26: 0.7700534759358288, 29: 0.9139784946236559, 30: 0.8823529411764706, 32: 0.7195121951219512, 33: 0.35294117647058826, 34: 0.3620689655172414, 35: 0.4457831325301205, 36: 0.45977011494252873, 37: 0.3829787234042553, 38: 0.41025641025641024}
Micro-average F1 score: 0.5573770491803278
Weighted-average F1 score: 0.5898148883461032
F1 score per class: {1: 0.2153846153846154, 3: 0.39285714285714285, 6: 0.632183908045977, 8: 0.7007299270072993, 14: 0.5492957746478874, 15: 0.8235294117647058, 19: 0.6164383561643836, 20: 0.6666666666666666, 22: 0.8020833333333334, 24: 0.06666666666666667, 25: 0.44776119402985076, 26: 0.7741935483870968, 29: 0.9312169312169312, 30: 0.8717948717948718, 32: 0.8066298342541437, 33: 0.3333333333333333, 34: 0.4098360655737705, 35: 0.5189873417721519, 36: 0.688, 37: 0.38181818181818183, 38: 0.72}
Micro-average F1 score: 0.6363636363636364
Weighted-average F1 score: 0.6432422294740705
F1 score per class: {1: 0.26865671641791045, 3: 0.38181818181818183, 6: 0.6024096385542169, 8: 0.5454545454545454, 14: 0.5755395683453237, 15: 0.8235294117647058, 19: 0.6068965517241379, 20: 0.647887323943662, 22: 0.8061224489795918, 24: 0.08333333333333333, 25: 0.42424242424242425, 26: 0.7741935483870968, 29: 0.925531914893617, 30: 0.9142857142857143, 32: 0.7657142857142857, 33: 0.3157894736842105, 34: 0.4, 35: 0.4880952380952381, 36: 0.6956521739130435, 37: 0.34285714285714286, 38: 0.5416666666666666}
Micro-average F1 score: 0.615580016934801
Weighted-average F1 score: 0.6237109251745736

F1 score per class: {32: 0.13829787234042554, 1: 0.19607843137254902, 34: 0.0, 35: 0.33519553072625696, 3: 0.0, 37: 0.0, 6: 0.4213197969543147, 38: 0.0, 14: 0.0, 19: 0.0, 20: 0.0, 22: 0.3783783783783784, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.247244094488189
Weighted-average F1 score: 0.22450005267875087
F1 score per class: {1: 0.08484848484848485, 3: 0.3826086956521739, 6: 0.0, 8: 0.0, 14: 0.2727272727272727, 15: 0.0, 19: 0.0, 20: 0.0, 22: 0.4873417721518987, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.3067484662576687, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.21223470661672908
Weighted-average F1 score: 0.1766309133079713
F1 score per class: {1: 0.10778443113772455, 3: 0.37168141592920356, 6: 0.0, 8: 0.0, 14: 0.3041825095057034, 19: 0.0, 20: 0.0, 22: 0.46607669616519176, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.3151515151515151, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.22551546391752578
Weighted-average F1 score: 0.1918769978366744

F1 score per class: {1: 0.1145374449339207, 3: 0.145985401459854, 6: 0.35398230088495575, 8: 0.22950819672131148, 14: 0.28169014084507044, 15: 0.5833333333333334, 19: 0.32727272727272727, 20: 0.2823529411764706, 22: 0.29537366548042704, 24: 0.07692307692307693, 25: 0.3880597014925373, 26: 0.6666666666666666, 29: 0.7555555555555555, 30: 0.8823529411764706, 32: 0.59, 33: 0.2608695652173913, 34: 0.15217391304347827, 35: 0.17494089834515367, 36: 0.37735849056603776, 37: 0.3185840707964602, 38: 0.2}
Micro-average F1 score: 0.32903225806451614
Weighted-average F1 score: 0.30608111515178044
F1 score per class: {1: 0.06542056074766354, 3: 0.2913907284768212, 6: 0.3470031545741325, 8: 0.3870967741935484, 14: 0.21787709497206703, 15: 0.2641509433962264, 19: 0.35294117647058826, 20: 0.20674157303370785, 22: 0.36065573770491804, 24: 0.03636363636363636, 25: 0.410958904109589, 26: 0.5950413223140496, 29: 0.7364016736401674, 30: 0.5151515151515151, 32: 0.5387453874538746, 33: 0.15789473684210525, 34: 0.12437810945273632, 35: 0.15018315018315018, 36: 0.3346303501945525, 37: 0.23076923076923078, 38: 0.17391304347826086}
Micro-average F1 score: 0.3024177566389219
Weighted-average F1 score: 0.28207547273740735
F1 score per class: {1: 0.08530805687203792, 3: 0.2641509433962264, 6: 0.35842293906810035, 8: 0.36666666666666664, 14: 0.24691358024691357, 15: 0.4666666666666667, 19: 0.37606837606837606, 20: 0.2021978021978022, 22: 0.33403805496828753, 24: 0.058823529411764705, 25: 0.4057971014492754, 26: 0.62882096069869, 29: 0.759825327510917, 30: 0.7441860465116279, 32: 0.5775862068965517, 33: 0.14285714285714285, 34: 0.11899313501144165, 35: 0.13712374581939799, 36: 0.42780748663101603, 37: 0.21686746987951808, 38: 0.14444444444444443}
Micro-average F1 score: 0.303485702358589
Weighted-average F1 score: 0.2766766538536838
cur_acc_wo_na:  ['0.8192', '0.6965', '0.5361', '0.4914']
his_acc_wo_na:  ['0.8192', '0.7394', '0.6511', '0.5574']
cur_acc des_wo_na:  ['0.8423', '0.8485', '0.7893', '0.5136']
his_acc des_wo_na:  ['0.8423', '0.8355', '0.7437', '0.6364']
cur_acc rrf_wo_na:  ['0.8423', '0.8468', '0.7794', '0.5303']
his_acc rrf_wo_na:  ['0.8423', '0.8176', '0.7334', '0.6156']
cur_acc_w_na:  ['0.6829', '0.4628', '0.3678', '0.2472']
his_acc_w_na:  ['0.6829', '0.5346', '0.4736', '0.3290']
cur_acc des_w_na:  ['0.6268', '0.4536', '0.3918', '0.2122']
his_acc des_w_na:  ['0.6268', '0.4808', '0.4193', '0.3024']
cur_acc rrf_w_na:  ['0.6306', '0.4750', '0.4005', '0.2255']
his_acc rrf_w_na:  ['0.6306', '0.4948', '0.4284', '0.3035']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 125.1870669CurrentTrain: epoch  0, batch     1 | loss: 110.1581179CurrentTrain: epoch  0, batch     2 | loss: 142.8472036CurrentTrain: epoch  0, batch     3 | loss: 108.3872657CurrentTrain: epoch  0, batch     4 | loss: 111.1425181CurrentTrain: epoch  1, batch     0 | loss: 177.6097129CurrentTrain: epoch  1, batch     1 | loss: 133.3400436CurrentTrain: epoch  1, batch     2 | loss: 130.2593791CurrentTrain: epoch  1, batch     3 | loss: 87.8324521CurrentTrain: epoch  1, batch     4 | loss: 70.7411601CurrentTrain: epoch  2, batch     0 | loss: 102.2709478CurrentTrain: epoch  2, batch     1 | loss: 276.4704006CurrentTrain: epoch  2, batch     2 | loss: 86.0255053CurrentTrain: epoch  2, batch     3 | loss: 135.8875300CurrentTrain: epoch  2, batch     4 | loss: 51.9174242CurrentTrain: epoch  3, batch     0 | loss: 84.7313881CurrentTrain: epoch  3, batch     1 | loss: 280.2925791CurrentTrain: epoch  3, batch     2 | loss: 101.6002622CurrentTrain: epoch  3, batch     3 | loss: 83.9911480CurrentTrain: epoch  3, batch     4 | loss: 51.6030389CurrentTrain: epoch  4, batch     0 | loss: 100.1641005CurrentTrain: epoch  4, batch     1 | loss: 126.0372919CurrentTrain: epoch  4, batch     2 | loss: 177.8375325CurrentTrain: epoch  4, batch     3 | loss: 132.5373766CurrentTrain: epoch  4, batch     4 | loss: 104.6583908CurrentTrain: epoch  5, batch     0 | loss: 267.3383088CurrentTrain: epoch  5, batch     1 | loss: 98.2070351CurrentTrain: epoch  5, batch     2 | loss: 100.0664058CurrentTrain: epoch  5, batch     3 | loss: 128.9320935CurrentTrain: epoch  5, batch     4 | loss: 62.4624022CurrentTrain: epoch  6, batch     0 | loss: 98.4022505CurrentTrain: epoch  6, batch     1 | loss: 94.0470576CurrentTrain: epoch  6, batch     2 | loss: 127.9691387CurrentTrain: epoch  6, batch     3 | loss: 129.3676564CurrentTrain: epoch  6, batch     4 | loss: 110.0630613CurrentTrain: epoch  7, batch     0 | loss: 95.8244805CurrentTrain: epoch  7, batch     1 | loss: 97.1868527CurrentTrain: epoch  7, batch     2 | loss: 170.8522813CurrentTrain: epoch  7, batch     3 | loss: 170.3663710CurrentTrain: epoch  7, batch     4 | loss: 170.1681680CurrentTrain: epoch  8, batch     0 | loss: 130.0739355CurrentTrain: epoch  8, batch     1 | loss: 171.0253195CurrentTrain: epoch  8, batch     2 | loss: 77.9065166CurrentTrain: epoch  8, batch     3 | loss: 95.1255328CurrentTrain: epoch  8, batch     4 | loss: 168.9741519CurrentTrain: epoch  9, batch     0 | loss: 96.9984252CurrentTrain: epoch  9, batch     1 | loss: 120.4965383CurrentTrain: epoch  9, batch     2 | loss: 173.8902278CurrentTrain: epoch  9, batch     3 | loss: 95.5373041CurrentTrain: epoch  9, batch     4 | loss: 111.6617506
MemoryTrain:  epoch  0, batch     0 | loss: 1.1505198MemoryTrain:  epoch  1, batch     0 | loss: 0.8957669MemoryTrain:  epoch  2, batch     0 | loss: 0.7305390MemoryTrain:  epoch  3, batch     0 | loss: 0.6008592MemoryTrain:  epoch  4, batch     0 | loss: 0.4871555MemoryTrain:  epoch  5, batch     0 | loss: 0.3931783MemoryTrain:  epoch  6, batch     0 | loss: 0.3311544MemoryTrain:  epoch  7, batch     0 | loss: 0.2795337MemoryTrain:  epoch  8, batch     0 | loss: 0.2437249MemoryTrain:  epoch  9, batch     0 | loss: 0.2057222

F1 score per class: {34: 0.9637305699481865, 5: 0.0, 38: 0.6622516556291391, 6: 0.8461538461538461, 10: 0.36363636363636365, 16: 0.7457627118644068, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.7907949790794979
Weighted-average F1 score: 0.7878772992373353
F1 score per class: {34: 0.9949748743718593, 36: 0.0, 37: 0.0, 38: 0.8, 6: 0.9473684210526315, 8: 0.2, 10: 0.7457627118644068, 5: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0}
Micro-average F1 score: 0.8188679245283019
Weighted-average F1 score: 0.7743631480009922
F1 score per class: {34: 0.9949748743718593, 5: 0.0, 38: 0.0, 6: 0.813953488372093, 8: 0.9473684210526315, 10: 0.2, 16: 0.7868852459016393, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.8387096774193549
Weighted-average F1 score: 0.803869948189901

F1 score per class: {1: 0.1875, 3: 0.20408163265306123, 5: 0.8985507246376812, 6: 0.55, 8: 0.15384615384615385, 10: 0.6024096385542169, 14: 0.5846153846153846, 15: 0.8888888888888888, 16: 0.8461538461538461, 17: 0.2222222222222222, 18: 0.3728813559322034, 19: 0.5294117647058824, 20: 0.5585585585585585, 22: 0.801980198019802, 24: 0.09523809523809523, 25: 0.4, 26: 0.7640449438202247, 29: 0.9197860962566845, 30: 0.9142857142857143, 32: 0.797752808988764, 33: 0.4, 34: 0.13793103448275862, 35: 0.30357142857142855, 36: 0.029850746268656716, 37: 0.2972972972972973, 38: 0.24242424242424243}
Micro-average F1 score: 0.5695768204346169
Weighted-average F1 score: 0.625931489366619
F1 score per class: {1: 0.1694915254237288, 3: 0.46153846153846156, 5: 0.8608695652173913, 6: 0.6859903381642513, 8: 0.5042016806722689, 10: 0.6868686868686869, 14: 0.5793103448275863, 15: 0.8235294117647058, 16: 0.9, 17: 0.16666666666666666, 18: 0.5116279069767442, 19: 0.6258503401360545, 20: 0.7339449541284404, 22: 0.7939698492462312, 24: 0.06666666666666667, 25: 0.42424242424242425, 26: 0.7570621468926554, 29: 0.9368421052631579, 30: 0.9230769230769231, 32: 0.8586387434554974, 33: 0.35294117647058826, 34: 0.28846153846153844, 35: 0.5359477124183006, 36: 0.5631067961165048, 37: 0.3853211009174312, 38: 0.425531914893617}
Micro-average F1 score: 0.6509723643807575
Weighted-average F1 score: 0.6680995323741591
F1 score per class: {1: 0.1694915254237288, 3: 0.46551724137931033, 5: 0.8761061946902655, 6: 0.6632653061224489, 8: 0.46153846153846156, 10: 0.6896551724137931, 14: 0.5833333333333334, 15: 0.8888888888888888, 16: 0.9, 17: 0.16666666666666666, 18: 0.4485981308411215, 19: 0.6301369863013698, 20: 0.7017543859649122, 22: 0.7860696517412935, 24: 0.07407407407407407, 25: 0.42424242424242425, 26: 0.768361581920904, 29: 0.9368421052631579, 30: 0.972972972972973, 32: 0.8586387434554974, 33: 0.35294117647058826, 34: 0.3047619047619048, 35: 0.5454545454545454, 36: 0.275, 37: 0.32, 38: 0.3902439024390244}
Micro-average F1 score: 0.6391184573002755
Weighted-average F1 score: 0.6632282034486893

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.8416289592760181, 6: 0.0, 8: 0.0, 10: 0.546448087431694, 14: 0.0, 15: 0.0, 16: 0.5176470588235295, 17: 0.3333333333333333, 18: 0.24719101123595505, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.4748743718592965
Weighted-average F1 score: 0.4067631079784473
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.61875, 6: 0.0, 8: 0.0, 10: 0.5964912280701754, 14: 0.0, 15: 0.0, 16: 0.5346534653465347, 17: 0.18181818181818182, 18: 0.18333333333333332, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.34146341463414637
Weighted-average F1 score: 0.2894638957415974
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6366559485530546, 6: 0.0, 8: 0.0, 10: 0.5982905982905983, 14: 0.0, 15: 0.0, 16: 0.5242718446601942, 17: 0.16666666666666666, 18: 0.18461538461538463, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.35789473684210527
Weighted-average F1 score: 0.3070348495777473

F1 score per class: {1: 0.06629834254143646, 3: 0.15748031496062992, 5: 0.7530364372469636, 6: 0.3384615384615385, 8: 0.12962962962962962, 10: 0.352112676056338, 14: 0.25418060200668896, 15: 0.6153846153846154, 16: 0.4036697247706422, 17: 0.15384615384615385, 18: 0.11924119241192412, 19: 0.38095238095238093, 20: 0.2421875, 22: 0.37850467289719625, 24: 0.08, 25: 0.3939393939393939, 26: 0.6445497630331753, 29: 0.7350427350427351, 30: 0.8648648648648649, 32: 0.6068376068376068, 33: 0.2608695652173913, 34: 0.09523809523809523, 35: 0.15246636771300448, 36: 0.02857142857142857, 37: 0.27848101265822783, 38: 0.19047619047619047}
Micro-average F1 score: 0.349146996961907
Weighted-average F1 score: 0.3386174434755362
F1 score per class: {1: 0.04830917874396135, 3: 0.2608695652173913, 5: 0.4593967517401392, 6: 0.3558897243107769, 8: 0.37037037037037035, 10: 0.3261390887290168, 14: 0.2033898305084746, 15: 0.35, 16: 0.3829787234042553, 17: 0.08695652173913043, 18: 0.11733333333333333, 19: 0.33093525179856115, 20: 0.19607843137254902, 22: 0.31225296442687744, 24: 0.03571428571428571, 25: 0.4, 26: 0.5826086956521739, 29: 0.7206477732793523, 30: 0.6923076923076923, 32: 0.5256410256410257, 33: 0.11764705882352941, 34: 0.15151515151515152, 35: 0.155893536121673, 36: 0.28431372549019607, 37: 0.2131979695431472, 38: 0.17094017094017094}
Micro-average F1 score: 0.3044518908568693
Weighted-average F1 score: 0.28976215884468365
F1 score per class: {1: 0.047619047619047616, 3: 0.2660098522167488, 5: 0.49624060150375937, 6: 0.3561643835616438, 8: 0.3375, 10: 0.3225806451612903, 14: 0.21212121212121213, 15: 0.41025641025641024, 16: 0.3776223776223776, 17: 0.08, 18: 0.1054945054945055, 19: 0.3458646616541353, 20: 0.18691588785046728, 22: 0.30326295585412666, 24: 0.047619047619047616, 25: 0.4, 26: 0.6181818181818182, 29: 0.726530612244898, 30: 0.782608695652174, 32: 0.5559322033898305, 33: 0.125, 34: 0.1568627450980392, 35: 0.1564245810055866, 36: 0.18803418803418803, 37: 0.21052631578947367, 38: 0.1951219512195122}
Micro-average F1 score: 0.30416256964929533
Weighted-average F1 score: 0.287660952561192
cur_acc_wo_na:  ['0.8192', '0.6965', '0.5361', '0.4914', '0.7908']
his_acc_wo_na:  ['0.8192', '0.7394', '0.6511', '0.5574', '0.5696']
cur_acc des_wo_na:  ['0.8423', '0.8485', '0.7893', '0.5136', '0.8189']
his_acc des_wo_na:  ['0.8423', '0.8355', '0.7437', '0.6364', '0.6510']
cur_acc rrf_wo_na:  ['0.8423', '0.8468', '0.7794', '0.5303', '0.8387']
his_acc rrf_wo_na:  ['0.8423', '0.8176', '0.7334', '0.6156', '0.6391']
cur_acc_w_na:  ['0.6829', '0.4628', '0.3678', '0.2472', '0.4749']
his_acc_w_na:  ['0.6829', '0.5346', '0.4736', '0.3290', '0.3491']
cur_acc des_w_na:  ['0.6268', '0.4536', '0.3918', '0.2122', '0.3415']
his_acc des_w_na:  ['0.6268', '0.4808', '0.4193', '0.3024', '0.3045']
cur_acc rrf_w_na:  ['0.6306', '0.4750', '0.4005', '0.2255', '0.3579']
his_acc rrf_w_na:  ['0.6306', '0.4948', '0.4284', '0.3035', '0.3042']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 154.4604490CurrentTrain: epoch  0, batch     1 | loss: 128.6835920CurrentTrain: epoch  0, batch     2 | loss: 180.1217466CurrentTrain: epoch  0, batch     3 | loss: 131.4579854CurrentTrain: epoch  1, batch     0 | loss: 180.6845189CurrentTrain: epoch  1, batch     1 | loss: 102.0686988CurrentTrain: epoch  1, batch     2 | loss: 95.5390969CurrentTrain: epoch  1, batch     3 | loss: 80.9916369CurrentTrain: epoch  2, batch     0 | loss: 114.8077055CurrentTrain: epoch  2, batch     1 | loss: 126.2123580CurrentTrain: epoch  2, batch     2 | loss: 93.1383354CurrentTrain: epoch  2, batch     3 | loss: 82.9162752CurrentTrain: epoch  3, batch     0 | loss: 133.6560400CurrentTrain: epoch  3, batch     1 | loss: 109.6497607CurrentTrain: epoch  3, batch     2 | loss: 89.5537224CurrentTrain: epoch  3, batch     3 | loss: 102.2894882CurrentTrain: epoch  4, batch     0 | loss: 82.8831289CurrentTrain: epoch  4, batch     1 | loss: 101.4572432CurrentTrain: epoch  4, batch     2 | loss: 133.4282335CurrentTrain: epoch  4, batch     3 | loss: 86.0514120CurrentTrain: epoch  5, batch     0 | loss: 126.4989292CurrentTrain: epoch  5, batch     1 | loss: 178.7161634CurrentTrain: epoch  5, batch     2 | loss: 101.2705852CurrentTrain: epoch  5, batch     3 | loss: 62.6886432CurrentTrain: epoch  6, batch     0 | loss: 128.6308132CurrentTrain: epoch  6, batch     1 | loss: 100.8508498CurrentTrain: epoch  6, batch     2 | loss: 124.7297062CurrentTrain: epoch  6, batch     3 | loss: 79.8680793CurrentTrain: epoch  7, batch     0 | loss: 105.5486524CurrentTrain: epoch  7, batch     1 | loss: 95.9604600CurrentTrain: epoch  7, batch     2 | loss: 98.2371609CurrentTrain: epoch  7, batch     3 | loss: 81.3879063CurrentTrain: epoch  8, batch     0 | loss: 93.6985606CurrentTrain: epoch  8, batch     1 | loss: 174.6536288CurrentTrain: epoch  8, batch     2 | loss: 82.2451533CurrentTrain: epoch  8, batch     3 | loss: 81.9916248CurrentTrain: epoch  9, batch     0 | loss: 96.5346208CurrentTrain: epoch  9, batch     1 | loss: 91.8656782CurrentTrain: epoch  9, batch     2 | loss: 174.4503131CurrentTrain: epoch  9, batch     3 | loss: 109.2588456
MemoryTrain:  epoch  0, batch     0 | loss: 1.2878074MemoryTrain:  epoch  1, batch     0 | loss: 1.1439787MemoryTrain:  epoch  2, batch     0 | loss: 0.8910984MemoryTrain:  epoch  3, batch     0 | loss: 0.6692981MemoryTrain:  epoch  4, batch     0 | loss: 0.5432951MemoryTrain:  epoch  5, batch     0 | loss: 0.4414301MemoryTrain:  epoch  6, batch     0 | loss: 0.3582899MemoryTrain:  epoch  7, batch     0 | loss: 0.2861542MemoryTrain:  epoch  8, batch     0 | loss: 0.2520585MemoryTrain:  epoch  9, batch     0 | loss: 0.2115207

F1 score per class: {0: 0.927536231884058, 32: 0.9417989417989417, 4: 0.0, 5: 0.8888888888888888, 13: 0.0, 14: 0.0, 15: 0.0, 20: 0.4878048780487805, 21: 0.0, 22: 0.8809523809523809, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8472906403940886
Weighted-average F1 score: 0.8308070304928877
F1 score per class: {0: 0.9863013698630136, 32: 0.9473684210526315, 34: 0.0, 4: 0.0, 5: 0.8888888888888888, 37: 0.0, 10: 0.0, 13: 0.0, 14: 0.32432432432432434, 15: 0.0, 20: 0.684931506849315, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7505827505827506
Weighted-average F1 score: 0.6893766884659762
F1 score per class: {0: 0.9863013698630136, 32: 0.9473684210526315, 34: 0.0, 4: 0.0, 5: 0.8888888888888888, 10: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 18: 0.4878048780487805, 20: 0.0, 21: 0.6944444444444444, 22: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7764705882352941
Weighted-average F1 score: 0.714949866619316

F1 score per class: {0: 0.927536231884058, 1: 0.2608695652173913, 3: 0.33663366336633666, 4: 0.9417989417989417, 5: 0.8952380952380953, 6: 0.5605095541401274, 8: 0.1956521739130435, 10: 0.425531914893617, 13: 0.11594202898550725, 14: 0.5645161290322581, 15: 0.7058823529411765, 16: 0.9285714285714286, 17: 0.23529411764705882, 18: 0.39669421487603307, 19: 0.6111111111111112, 20: 0.6875, 21: 0.23809523809523808, 22: 0.774869109947644, 23: 0.8409090909090909, 24: 0.1, 25: 0.4, 26: 0.7570621468926554, 29: 0.9148936170212766, 30: 0.9142857142857143, 32: 0.7558139534883721, 33: 0.4, 34: 0.13793103448275862, 35: 0.3247863247863248, 36: 0.029850746268656716, 37: 0.35443037974683544, 38: 0.3333333333333333}
Micro-average F1 score: 0.59328
Weighted-average F1 score: 0.6202526459702008
F1 score per class: {0: 0.972972972972973, 1: 0.2535211267605634, 3: 0.5901639344262295, 4: 0.9473684210526315, 5: 0.8425531914893617, 6: 0.68, 8: 0.6466165413533834, 10: 0.4794520547945205, 13: 0.11940298507462686, 14: 0.5454545454545454, 15: 0.5454545454545454, 16: 0.9333333333333333, 17: 0.42857142857142855, 18: 0.49504950495049505, 19: 0.7261146496815286, 20: 0.8, 21: 0.13953488372093023, 22: 0.7243243243243244, 23: 0.6493506493506493, 24: 0.07142857142857142, 25: 0.43478260869565216, 26: 0.6974358974358974, 29: 0.9375, 30: 0.918918918918919, 32: 0.8526315789473684, 33: 0.35294117647058826, 34: 0.24, 35: 0.5945945945945946, 36: 0.52, 37: 0.40707964601769914, 38: 0.6}
Micro-average F1 score: 0.6503210741389376
Weighted-average F1 score: 0.6468806182652692
F1 score per class: {0: 0.972972972972973, 1: 0.2535211267605634, 3: 0.5737704918032787, 4: 0.9473684210526315, 5: 0.8722466960352423, 6: 0.6338797814207651, 8: 0.5546218487394958, 10: 0.49333333333333335, 13: 0.1, 14: 0.5454545454545454, 15: 0.5454545454545454, 16: 0.9333333333333333, 17: 0.42857142857142855, 18: 0.4262295081967213, 19: 0.6842105263157895, 20: 0.7741935483870968, 21: 0.21739130434782608, 22: 0.7597765363128491, 23: 0.6578947368421053, 24: 0.07692307692307693, 25: 0.4117647058823529, 26: 0.7010309278350515, 29: 0.9319371727748691, 30: 0.918918918918919, 32: 0.8465608465608465, 33: 0.35294117647058826, 34: 0.20408163265306123, 35: 0.5673758865248227, 36: 0.23376623376623376, 37: 0.3584905660377358, 38: 0.45}
Micro-average F1 score: 0.6297064927364364
Weighted-average F1 score: 0.6295033091411377

F1 score per class: {0: 0.8648648648648649, 1: 0.0, 3: 0.0, 4: 0.9368421052631579, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.17777777777777778, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.29850746268656714, 22: 0.0, 23: 0.8131868131868132, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5400313971742543
Weighted-average F1 score: 0.40756208398496774
F1 score per class: {0: 0.576, 1: 0.0, 3: 0.0, 4: 0.8411214953271028, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.16, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.1935483870967742, 22: 0.0, 23: 0.49504950495049505, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.30931796349663787
Weighted-average F1 score: 0.22717237970111628
F1 score per class: {0: 0.782608695652174, 1: 0.0, 3: 0.0, 4: 0.8910891089108911, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.13559322033898305, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.22988505747126436, 22: 0.0, 23: 0.5555555555555556, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3544575725026853
Weighted-average F1 score: 0.25037841543651934

F1 score per class: {0: 0.41830065359477125, 1: 0.09944751381215469, 3: 0.22666666666666666, 4: 0.9270833333333334, 5: 0.749003984063745, 6: 0.33587786259541985, 8: 0.17307692307692307, 10: 0.2608695652173913, 13: 0.0273972602739726, 14: 0.25089605734767023, 15: 0.6, 16: 0.3969465648854962, 17: 0.13793103448275862, 18: 0.10835214446952596, 19: 0.3562753036437247, 20: 0.2194513715710723, 21: 0.06688963210702341, 22: 0.41456582633053224, 23: 0.7047619047619048, 24: 0.08695652173913043, 25: 0.3939393939393939, 26: 0.6442307692307693, 29: 0.6798418972332015, 30: 0.8648648648648649, 32: 0.5963302752293578, 33: 0.35294117647058826, 34: 0.09375, 35: 0.13768115942028986, 36: 0.028985507246376812, 37: 0.2828282828282828, 38: 0.22641509433962265}
Micro-average F1 score: 0.3326753992463664
Weighted-average F1 score: 0.30112819581733014
F1 score per class: {0: 0.13432835820895522, 1: 0.06976744186046512, 3: 0.21114369501466276, 4: 0.821917808219178, 5: 0.3542039355992844, 6: 0.3090909090909091, 8: 0.38392857142857145, 10: 0.21084337349397592, 13: 0.024615384615384615, 14: 0.19672131147540983, 15: 0.24489795918367346, 16: 0.30434782608695654, 17: 0.15, 18: 0.10482180293501048, 19: 0.304, 20: 0.15619694397283532, 21: 0.0332409972299169, 22: 0.32057416267942584, 23: 0.3448275862068966, 24: 0.03636363636363636, 25: 0.38961038961038963, 26: 0.5596707818930041, 29: 0.6844106463878327, 30: 0.5, 32: 0.5294117647058824, 33: 0.13636363636363635, 34: 0.12, 35: 0.16236162361623616, 36: 0.2524271844660194, 37: 0.23958333333333334, 38: 0.18867924528301888}
Micro-average F1 score: 0.25928080996159664
Weighted-average F1 score: 0.23871592952692078
F1 score per class: {0: 0.22784810126582278, 1: 0.075, 3: 0.2348993288590604, 4: 0.8695652173913043, 5: 0.4794188861985472, 6: 0.3143631436314363, 8: 0.3687150837988827, 10: 0.22289156626506024, 13: 0.021333333333333333, 14: 0.21176470588235294, 15: 0.2727272727272727, 16: 0.32941176470588235, 17: 0.14285714285714285, 18: 0.09403254972875226, 19: 0.32398753894080995, 20: 0.15141955835962145, 21: 0.04830917874396135, 22: 0.3541666666666667, 23: 0.42735042735042733, 24: 0.0425531914893617, 25: 0.3835616438356164, 26: 0.5836909871244635, 29: 0.6793893129770993, 30: 0.68, 32: 0.5594405594405595, 33: 0.13953488372093023, 34: 0.10416666666666667, 35: 0.1702127659574468, 36: 0.15517241379310345, 37: 0.24203821656050956, 38: 0.19148936170212766}
Micro-average F1 score: 0.27332389653841205
Weighted-average F1 score: 0.2481408882184291
cur_acc_wo_na:  ['0.8192', '0.6965', '0.5361', '0.4914', '0.7908', '0.8473']
his_acc_wo_na:  ['0.8192', '0.7394', '0.6511', '0.5574', '0.5696', '0.5933']
cur_acc des_wo_na:  ['0.8423', '0.8485', '0.7893', '0.5136', '0.8189', '0.7506']
his_acc des_wo_na:  ['0.8423', '0.8355', '0.7437', '0.6364', '0.6510', '0.6503']
cur_acc rrf_wo_na:  ['0.8423', '0.8468', '0.7794', '0.5303', '0.8387', '0.7765']
his_acc rrf_wo_na:  ['0.8423', '0.8176', '0.7334', '0.6156', '0.6391', '0.6297']
cur_acc_w_na:  ['0.6829', '0.4628', '0.3678', '0.2472', '0.4749', '0.5400']
his_acc_w_na:  ['0.6829', '0.5346', '0.4736', '0.3290', '0.3491', '0.3327']
cur_acc des_w_na:  ['0.6268', '0.4536', '0.3918', '0.2122', '0.3415', '0.3093']
his_acc des_w_na:  ['0.6268', '0.4808', '0.4193', '0.3024', '0.3045', '0.2593']
cur_acc rrf_w_na:  ['0.6306', '0.4750', '0.4005', '0.2255', '0.3579', '0.3545']
his_acc rrf_w_na:  ['0.6306', '0.4948', '0.4284', '0.3035', '0.3042', '0.2733']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 294.3828651CurrentTrain: epoch  0, batch     1 | loss: 102.6796878CurrentTrain: epoch  0, batch     2 | loss: 118.2249454CurrentTrain: epoch  0, batch     3 | loss: 103.6829567CurrentTrain: epoch  0, batch     4 | loss: 25.5654935CurrentTrain: epoch  1, batch     0 | loss: 144.1325471CurrentTrain: epoch  1, batch     1 | loss: 118.6625067CurrentTrain: epoch  1, batch     2 | loss: 90.4282506CurrentTrain: epoch  1, batch     3 | loss: 139.7581371CurrentTrain: epoch  1, batch     4 | loss: 31.6548950CurrentTrain: epoch  2, batch     0 | loss: 103.2646679CurrentTrain: epoch  2, batch     1 | loss: 139.1464970CurrentTrain: epoch  2, batch     2 | loss: 139.1819435CurrentTrain: epoch  2, batch     3 | loss: 131.2066114CurrentTrain: epoch  2, batch     4 | loss: 46.2179041CurrentTrain: epoch  3, batch     0 | loss: 184.5969058CurrentTrain: epoch  3, batch     1 | loss: 133.5435790CurrentTrain: epoch  3, batch     2 | loss: 87.6998000CurrentTrain: epoch  3, batch     3 | loss: 100.7743149CurrentTrain: epoch  3, batch     4 | loss: 26.7437255CurrentTrain: epoch  4, batch     0 | loss: 102.1102391CurrentTrain: epoch  4, batch     1 | loss: 132.4235956CurrentTrain: epoch  4, batch     2 | loss: 103.6926494CurrentTrain: epoch  4, batch     3 | loss: 102.3371059CurrentTrain: epoch  4, batch     4 | loss: 26.1246614CurrentTrain: epoch  5, batch     0 | loss: 102.0138149CurrentTrain: epoch  5, batch     1 | loss: 132.7776738CurrentTrain: epoch  5, batch     2 | loss: 99.0520127CurrentTrain: epoch  5, batch     3 | loss: 131.0727387CurrentTrain: epoch  5, batch     4 | loss: 10.1063596CurrentTrain: epoch  6, batch     0 | loss: 125.8272053CurrentTrain: epoch  6, batch     1 | loss: 177.0935881CurrentTrain: epoch  6, batch     2 | loss: 94.8567533CurrentTrain: epoch  6, batch     3 | loss: 127.3711998CurrentTrain: epoch  6, batch     4 | loss: 25.9762568CurrentTrain: epoch  7, batch     0 | loss: 79.2580137CurrentTrain: epoch  7, batch     1 | loss: 98.8168220CurrentTrain: epoch  7, batch     2 | loss: 126.2780525CurrentTrain: epoch  7, batch     3 | loss: 104.1425456CurrentTrain: epoch  7, batch     4 | loss: 24.3946170CurrentTrain: epoch  8, batch     0 | loss: 95.5570775CurrentTrain: epoch  8, batch     1 | loss: 100.5028529CurrentTrain: epoch  8, batch     2 | loss: 174.0728452CurrentTrain: epoch  8, batch     3 | loss: 80.4321477CurrentTrain: epoch  8, batch     4 | loss: 41.5437738CurrentTrain: epoch  9, batch     0 | loss: 100.1948057CurrentTrain: epoch  9, batch     1 | loss: 80.0278431CurrentTrain: epoch  9, batch     2 | loss: 128.3443116CurrentTrain: epoch  9, batch     3 | loss: 94.5979662CurrentTrain: epoch  9, batch     4 | loss: 40.3952632
MemoryTrain:  epoch  0, batch     0 | loss: 0.8980894MemoryTrain:  epoch  1, batch     0 | loss: 0.7849346MemoryTrain:  epoch  2, batch     0 | loss: 0.6261256MemoryTrain:  epoch  3, batch     0 | loss: 0.5220356MemoryTrain:  epoch  4, batch     0 | loss: 0.4228218MemoryTrain:  epoch  5, batch     0 | loss: 0.3385635MemoryTrain:  epoch  6, batch     0 | loss: 0.2831034MemoryTrain:  epoch  7, batch     0 | loss: 0.2892041MemoryTrain:  epoch  8, batch     0 | loss: 0.2101414MemoryTrain:  epoch  9, batch     0 | loss: 0.1896551

F1 score per class: {0: 0.0, 2: 0.875, 34: 0.0, 5: 0.0, 39: 0.0, 8: 0.6461538461538462, 10: 0.5147058823529411, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.6666666666666666, 19: 0.0, 28: 0.35294117647058826}
Micro-average F1 score: 0.5432835820895522
Weighted-average F1 score: 0.4786065818900505
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8266666666666667, 12: 0.7757575757575758, 13: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 28: 0.6666666666666666, 33: 0.0, 34: 0.0, 37: 0.0, 39: 0.8148148148148148}
Micro-average F1 score: 0.7167070217917676
Weighted-average F1 score: 0.6240300774063661
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8789808917197452, 12: 0.7530864197530864, 13: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 28: 0.6666666666666666, 33: 0.0, 34: 0.0, 37: 0.0, 38: 0.0, 39: 0.72}
Micro-average F1 score: 0.7317073170731707
Weighted-average F1 score: 0.6494906084817825

F1 score per class: {0: 0.9295774647887324, 1: 0.26865671641791045, 2: 0.5185185185185185, 3: 0.14736842105263157, 4: 0.8372093023255814, 5: 0.9090909090909091, 6: 0.6214689265536724, 8: 0.25, 10: 0.31746031746031744, 11: 0.36681222707423583, 12: 0.45751633986928103, 13: 0.07142857142857142, 14: 0.592, 15: 0.75, 16: 0.8, 17: 0.18181818181818182, 18: 0.26229508196721313, 19: 0.6303030303030303, 20: 0.6929133858267716, 21: 0.21739130434782608, 22: 0.7419354838709677, 23: 0.8409090909090909, 24: 0.09090909090909091, 25: 0.4, 26: 0.7428571428571429, 28: 0.27586206896551724, 29: 0.9021739130434783, 30: 0.9142857142857143, 32: 0.7471264367816092, 33: 0.2857142857142857, 34: 0.05555555555555555, 35: 0.27450980392156865, 36: 0.0, 37: 0.23376623376623376, 38: 0.2857142857142857, 39: 0.24}
Micro-average F1 score: 0.5576753742295274
Weighted-average F1 score: 0.6022941560664493
F1 score per class: {0: 0.96, 1: 0.23529411764705882, 2: 0.5, 3: 0.5166666666666667, 4: 0.8764044943820225, 5: 0.8497854077253219, 6: 0.7511737089201878, 8: 0.5426356589147286, 10: 0.5035971223021583, 11: 0.47692307692307695, 12: 0.649746192893401, 13: 0.08888888888888889, 14: 0.5547445255474452, 15: 0.5714285714285714, 16: 0.7083333333333334, 17: 0.5, 18: 0.125, 19: 0.7513812154696132, 20: 0.8148148148148148, 21: 0.12903225806451613, 22: 0.7282608695652174, 23: 0.6923076923076923, 24: 0.06451612903225806, 25: 0.43478260869565216, 26: 0.7120418848167539, 28: 0.3333333333333333, 29: 0.9032258064516129, 30: 0.918918918918919, 32: 0.8787878787878788, 33: 0.35294117647058826, 34: 0.25287356321839083, 35: 0.5507246376811594, 36: 0.16216216216216217, 37: 0.3853211009174312, 38: 0.5769230769230769, 39: 0.3728813559322034}
Micro-average F1 score: 0.6190845616757176
Weighted-average F1 score: 0.6231636585086258
F1 score per class: {0: 0.96, 1: 0.2318840579710145, 2: 0.4827586206896552, 3: 0.5, 4: 0.8700564971751412, 5: 0.88, 6: 0.7333333333333333, 8: 0.4247787610619469, 10: 0.4788732394366197, 11: 0.5018181818181818, 12: 0.6161616161616161, 13: 0.08163265306122448, 14: 0.5588235294117647, 15: 0.6, 16: 0.7346938775510204, 17: 0.36363636363636365, 18: 0.16, 19: 0.7231638418079096, 20: 0.7796610169491526, 21: 0.1702127659574468, 22: 0.723404255319149, 23: 0.7012987012987013, 24: 0.07692307692307693, 25: 0.4117647058823529, 26: 0.7120418848167539, 28: 0.27586206896551724, 29: 0.9032258064516129, 30: 0.9444444444444444, 32: 0.8629441624365483, 33: 0.35294117647058826, 34: 0.21333333333333335, 35: 0.5147058823529411, 36: 0.029411764705882353, 37: 0.34285714285714286, 38: 0.5925925925925926, 39: 0.33962264150943394}
Micro-average F1 score: 0.6077462958149207
Weighted-average F1 score: 0.6181312198135928

F1 score per class: {0: 0.0, 2: 0.3684210526315789, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.4692737430167598, 12: 0.45161290322580644, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 28: 0.27586206896551724, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.2608695652173913}
Micro-average F1 score: 0.29260450160771706
Weighted-average F1 score: 0.21431206043050288
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.2692307692307692, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.44285714285714284, 12: 0.540084388185654, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.1951219512195122, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.41509433962264153}
Micro-average F1 score: 0.2591943957968476
Weighted-average F1 score: 0.20934426795499025
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.27450980392156865, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.46464646464646464, 12: 0.5191489361702127, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 28: 0.18181818181818182, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.32142857142857145}
Micro-average F1 score: 0.273972602739726
Weighted-average F1 score: 0.22690762991352773

F1 score per class: {0: 0.4024390243902439, 1: 0.10650887573964497, 2: 0.17721518987341772, 3: 0.10852713178294573, 4: 0.8275862068965517, 5: 0.7661290322580645, 6: 0.3216374269005848, 8: 0.1951219512195122, 10: 0.23952095808383234, 11: 0.16, 12: 0.17412935323383086, 13: 0.017543859649122806, 14: 0.25084745762711863, 15: 0.6, 16: 0.43010752688172044, 17: 0.08, 18: 0.09696969696969697, 19: 0.35738831615120276, 20: 0.22916666666666666, 21: 0.12658227848101267, 22: 0.4094955489614243, 23: 0.7115384615384616, 24: 0.07692307692307693, 25: 0.3939393939393939, 26: 0.65, 28: 0.12121212121212122, 29: 0.6747967479674797, 30: 0.8648648648648649, 32: 0.5138339920948617, 33: 0.26666666666666666, 34: 0.05063291139240506, 35: 0.13930348258706468, 36: 0.0, 37: 0.1956521739130435, 38: 0.10810810810810811, 39: 0.11538461538461539}
Micro-average F1 score: 0.31986531986531985
Weighted-average F1 score: 0.3025239284677189
F1 score per class: {0: 0.15483870967741936, 1: 0.064, 2: 0.08917197452229299, 3: 0.25203252032520324, 4: 0.8125, 5: 0.3687150837988827, 6: 0.2821869488536155, 8: 0.2681992337164751, 10: 0.23411371237458195, 11: 0.12930135557872785, 12: 0.12403100775193798, 13: 0.01904761904761905, 14: 0.209366391184573, 15: 0.19047619047619047, 16: 0.3434343434343434, 17: 0.14285714285714285, 18: 0.06315789473684211, 19: 0.34782608695652173, 20: 0.19909502262443438, 21: 0.028846153846153848, 22: 0.3094688221709007, 23: 0.40298507462686567, 24: 0.031746031746031744, 25: 0.39473684210526316, 26: 0.5787234042553191, 28: 0.09523809523809523, 29: 0.6486486486486487, 30: 0.5396825396825397, 32: 0.4306930693069307, 33: 0.16216216216216217, 34: 0.13924050632911392, 35: 0.16593886462882096, 36: 0.11538461538461539, 37: 0.23595505617977527, 38: 0.18072289156626506, 39: 0.10837438423645321}
Micro-average F1 score: 0.23607139335371266
Weighted-average F1 score: 0.2187792662966852
F1 score per class: {0: 0.2011173184357542, 1: 0.06274509803921569, 2: 0.10071942446043165, 3: 0.248, 4: 0.8555555555555555, 5: 0.4913151364764268, 6: 0.2978723404255319, 8: 0.26666666666666666, 10: 0.24028268551236748, 11: 0.13827655310621242, 12: 0.12103174603174603, 13: 0.016666666666666666, 14: 0.2177650429799427, 15: 0.21818181818181817, 16: 0.35294117647058826, 17: 0.12121212121212122, 18: 0.06837606837606838, 19: 0.3422459893048128, 20: 0.18813905930470348, 21: 0.03950617283950617, 22: 0.29694323144104806, 23: 0.5094339622641509, 24: 0.04, 25: 0.3835616438356164, 26: 0.5938864628820961, 28: 0.0761904761904762, 29: 0.6666666666666666, 30: 0.6666666666666666, 32: 0.4735376044568245, 33: 0.2, 34: 0.14285714285714285, 35: 0.15384615384615385, 36: 0.02702702702702703, 37: 0.21951219512195122, 38: 0.1693121693121693, 39: 0.10112359550561797}
Micro-average F1 score: 0.24303534303534305
Weighted-average F1 score: 0.22288418318782854
cur_acc_wo_na:  ['0.8192', '0.6965', '0.5361', '0.4914', '0.7908', '0.8473', '0.5433']
his_acc_wo_na:  ['0.8192', '0.7394', '0.6511', '0.5574', '0.5696', '0.5933', '0.5577']
cur_acc des_wo_na:  ['0.8423', '0.8485', '0.7893', '0.5136', '0.8189', '0.7506', '0.7167']
his_acc des_wo_na:  ['0.8423', '0.8355', '0.7437', '0.6364', '0.6510', '0.6503', '0.6191']
cur_acc rrf_wo_na:  ['0.8423', '0.8468', '0.7794', '0.5303', '0.8387', '0.7765', '0.7317']
his_acc rrf_wo_na:  ['0.8423', '0.8176', '0.7334', '0.6156', '0.6391', '0.6297', '0.6077']
cur_acc_w_na:  ['0.6829', '0.4628', '0.3678', '0.2472', '0.4749', '0.5400', '0.2926']
his_acc_w_na:  ['0.6829', '0.5346', '0.4736', '0.3290', '0.3491', '0.3327', '0.3199']
cur_acc des_w_na:  ['0.6268', '0.4536', '0.3918', '0.2122', '0.3415', '0.3093', '0.2592']
his_acc des_w_na:  ['0.6268', '0.4808', '0.4193', '0.3024', '0.3045', '0.2593', '0.2361']
cur_acc rrf_w_na:  ['0.6306', '0.4750', '0.4005', '0.2255', '0.3579', '0.3545', '0.2740']
his_acc rrf_w_na:  ['0.6306', '0.4948', '0.4284', '0.3035', '0.3042', '0.2733', '0.2430']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 102.2570046CurrentTrain: epoch  0, batch     1 | loss: 149.7136399CurrentTrain: epoch  0, batch     2 | loss: 107.1130394CurrentTrain: epoch  0, batch     3 | loss: 41.8945286CurrentTrain: epoch  1, batch     0 | loss: 116.3836756CurrentTrain: epoch  1, batch     1 | loss: 90.7434463CurrentTrain: epoch  1, batch     2 | loss: 93.3899493CurrentTrain: epoch  1, batch     3 | loss: 41.9442725CurrentTrain: epoch  2, batch     0 | loss: 90.6459657CurrentTrain: epoch  2, batch     1 | loss: 88.7673904CurrentTrain: epoch  2, batch     2 | loss: 86.1611908CurrentTrain: epoch  2, batch     3 | loss: 41.5358537CurrentTrain: epoch  3, batch     0 | loss: 81.7986617CurrentTrain: epoch  3, batch     1 | loss: 85.0015946CurrentTrain: epoch  3, batch     2 | loss: 86.2901651CurrentTrain: epoch  3, batch     3 | loss: 41.5241970CurrentTrain: epoch  4, batch     0 | loss: 81.5555696CurrentTrain: epoch  4, batch     1 | loss: 98.8922712CurrentTrain: epoch  4, batch     2 | loss: 128.8822585CurrentTrain: epoch  4, batch     3 | loss: 15.4600885CurrentTrain: epoch  5, batch     0 | loss: 100.7376073CurrentTrain: epoch  5, batch     1 | loss: 76.5747958CurrentTrain: epoch  5, batch     2 | loss: 127.4995746CurrentTrain: epoch  5, batch     3 | loss: 17.3876962CurrentTrain: epoch  6, batch     0 | loss: 96.3108312CurrentTrain: epoch  6, batch     1 | loss: 97.5571454CurrentTrain: epoch  6, batch     2 | loss: 78.1072733CurrentTrain: epoch  6, batch     3 | loss: 41.2745941CurrentTrain: epoch  7, batch     0 | loss: 125.1094806CurrentTrain: epoch  7, batch     1 | loss: 77.4659228CurrentTrain: epoch  7, batch     2 | loss: 77.9253286CurrentTrain: epoch  7, batch     3 | loss: 41.3189570CurrentTrain: epoch  8, batch     0 | loss: 125.2680488CurrentTrain: epoch  8, batch     1 | loss: 74.2257544CurrentTrain: epoch  8, batch     2 | loss: 81.4247855CurrentTrain: epoch  8, batch     3 | loss: 9.9083826CurrentTrain: epoch  9, batch     0 | loss: 79.2018520CurrentTrain: epoch  9, batch     1 | loss: 76.7415221CurrentTrain: epoch  9, batch     2 | loss: 79.0488931CurrentTrain: epoch  9, batch     3 | loss: 16.4814282
MemoryTrain:  epoch  0, batch     0 | loss: 0.6750611MemoryTrain:  epoch  1, batch     0 | loss: 0.5814548MemoryTrain:  epoch  2, batch     0 | loss: 0.4574186MemoryTrain:  epoch  3, batch     0 | loss: 0.3826450MemoryTrain:  epoch  4, batch     0 | loss: 0.2987670MemoryTrain:  epoch  5, batch     0 | loss: 0.2328848MemoryTrain:  epoch  6, batch     0 | loss: 0.2393288MemoryTrain:  epoch  7, batch     0 | loss: 0.2016086MemoryTrain:  epoch  8, batch     0 | loss: 0.1661800MemoryTrain:  epoch  9, batch     0 | loss: 0.1477485

F1 score per class: {3: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 14: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.6666666666666666, 27: 0.6666666666666666, 31: 0.5979381443298969}
Micro-average F1 score: 0.5981308411214953
Weighted-average F1 score: 0.4899286724999278
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 14: 0.0, 19: 0.0, 22: 0.0, 26: 0.6363636363636364, 27: 1.0, 31: 0.847457627118644}
Micro-average F1 score: 0.7644444444444445
Weighted-average F1 score: 0.682457794122799
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 11: 0.0, 14: 0.0, 19: 0.0, 22: 0.0, 26: 0.6363636363636364, 27: 1.0, 31: 0.847457627118644}
Micro-average F1 score: 0.7644444444444445
Weighted-average F1 score: 0.682457794122799

F1 score per class: {0: 0.958904109589041, 1: 0.20408163265306123, 2: 0.5833333333333334, 3: 0.25742574257425743, 4: 0.8505747126436781, 5: 0.9065420560747663, 6: 0.43356643356643354, 7: 0.06779661016949153, 8: 0.25263157894736843, 9: 0.9803921568627451, 10: 0.2689075630252101, 11: 0.3745019920318725, 12: 0.49079754601226994, 13: 0.046511627906976744, 14: 0.18666666666666668, 15: 0.7058823529411765, 16: 0.75, 17: 0.0, 18: 0.3188405797101449, 19: 0.6331658291457286, 20: 0.7022900763358778, 21: 0.2413793103448276, 22: 0.745945945945946, 23: 0.8539325842696629, 24: 0.09090909090909091, 25: 0.4, 26: 0.6909090909090909, 27: 0.17721518987341772, 28: 0.21428571428571427, 29: 0.9081081081081082, 30: 0.9142857142857143, 31: 0.6666666666666666, 32: 0.6993865030674846, 33: 0.375, 34: 0.08108108108108109, 35: 0.24299065420560748, 36: 0.0, 37: 0.23809523809523808, 38: 0.2978723404255319, 39: 0.23076923076923078, 40: 0.5178571428571429}
Micro-average F1 score: 0.5258342303552207
Weighted-average F1 score: 0.5490794837385431
F1 score per class: {0: 0.9459459459459459, 1: 0.2857142857142857, 2: 0.6086956521739131, 3: 0.5785123966942148, 4: 0.8700564971751412, 5: 0.8497854077253219, 6: 0.625, 7: 0.09090909090909091, 8: 0.43103448275862066, 9: 0.8333333333333334, 10: 0.463768115942029, 11: 0.4050632911392405, 12: 0.6236559139784946, 13: 0.05, 14: 0.3125, 15: 0.631578947368421, 16: 0.7346938775510204, 17: 0.5, 18: 0.2857142857142857, 19: 0.7346938775510204, 20: 0.8, 21: 0.18666666666666668, 22: 0.7659574468085106, 23: 0.75, 24: 0.07142857142857142, 25: 0.4057971014492754, 26: 0.7027027027027027, 27: 0.1917808219178082, 28: 0.3333333333333333, 29: 0.9032258064516129, 30: 0.918918918918919, 31: 0.8, 32: 0.8601036269430051, 33: 0.3, 34: 0.22448979591836735, 35: 0.578125, 36: 0.16216216216216217, 37: 0.36036036036036034, 38: 0.52, 39: 0.27586206896551724, 40: 0.704225352112676}
Micro-average F1 score: 0.5973320158102767
Weighted-average F1 score: 0.5956778853779465
F1 score per class: {0: 0.9459459459459459, 1: 0.25925925925925924, 2: 0.6086956521739131, 3: 0.5245901639344263, 4: 0.8700564971751412, 5: 0.8646288209606987, 6: 0.5696969696969697, 7: 0.07547169811320754, 8: 0.4247787610619469, 9: 0.9803921568627451, 10: 0.37209302325581395, 11: 0.4470588235294118, 12: 0.5968586387434555, 13: 0.044444444444444446, 14: 0.33707865168539325, 15: 0.631578947368421, 16: 0.7346938775510204, 17: 0.36363636363636365, 18: 0.2807017543859649, 19: 0.7346938775510204, 20: 0.8034188034188035, 21: 0.2222222222222222, 22: 0.7708333333333334, 23: 0.717948717948718, 24: 0.08, 25: 0.4117647058823529, 26: 0.7027027027027027, 27: 0.175, 28: 0.23076923076923078, 29: 0.9032258064516129, 30: 0.9444444444444444, 31: 1.0, 32: 0.8021978021978022, 33: 0.3, 34: 0.18823529411764706, 35: 0.5072463768115942, 36: 0.029411764705882353, 37: 0.33962264150943394, 38: 0.52, 39: 0.30434782608695654, 40: 0.7246376811594203}
Micro-average F1 score: 0.585098430102168
Weighted-average F1 score: 0.5874009801815747

F1 score per class: {0: 0.0, 3: 0.0, 6: 0.0, 7: 0.5714285714285714, 8: 0.0, 9: 0.8620689655172413, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.4827586206896552, 31: 0.125, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.3841059602649007}
Micro-average F1 score: 0.3368421052631579
Weighted-average F1 score: 0.26347367170086505
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5714285714285714, 8: 0.0, 9: 0.6756756756756757, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.32558139534883723, 31: 0.16666666666666666, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.4975124378109453}
Micro-average F1 score: 0.32761904761904764
Weighted-average F1 score: 0.275373606645443
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5714285714285714, 8: 0.0, 9: 0.7692307692307693, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.3181818181818182, 28: 0.0, 31: 0.2, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.5102040816326531}
Micro-average F1 score: 0.33725490196078434
Weighted-average F1 score: 0.2775571109986694

F1 score per class: {0: 0.39106145251396646, 1: 0.17543859649122806, 2: 0.208955223880597, 3: 0.16666666666666666, 4: 0.8409090909090909, 5: 0.6759581881533101, 6: 0.2572614107883817, 7: 0.028368794326241134, 8: 0.21238938053097345, 9: 0.7692307692307693, 10: 0.2318840579710145, 11: 0.12417437252311757, 12: 0.18306636155606407, 13: 0.01098901098901099, 14: 0.14736842105263157, 15: 0.5217391304347826, 16: 0.4186046511627907, 17: 0.0, 18: 0.08148148148148149, 19: 0.35, 20: 0.19574468085106383, 21: 0.10687022900763359, 22: 0.3865546218487395, 23: 0.7238095238095238, 24: 0.08, 25: 0.3880597014925373, 26: 0.6063829787234043, 27: 0.046511627906976744, 28: 0.1016949152542373, 29: 0.691358024691358, 30: 0.8648648648648649, 31: 0.03571428571428571, 32: 0.5302325581395348, 33: 0.3, 34: 0.06666666666666667, 35: 0.11764705882352941, 36: 0.0, 37: 0.17543859649122806, 38: 0.112, 39: 0.15, 40: 0.2577777777777778}
Micro-average F1 score: 0.27862540995294455
Weighted-average F1 score: 0.25126708011964627
F1 score per class: {0: 0.22508038585209003, 1: 0.1415929203539823, 2: 0.13861386138613863, 3: 0.29411764705882354, 4: 0.8020833333333334, 5: 0.391304347826087, 6: 0.26763990267639903, 7: 0.03571428571428571, 8: 0.273224043715847, 9: 0.37593984962406013, 10: 0.25196850393700787, 11: 0.1136094674556213, 12: 0.14910025706940874, 13: 0.010638297872340425, 14: 0.19607843137254902, 15: 0.26666666666666666, 16: 0.34285714285714286, 17: 0.13953488372093023, 18: 0.11188811188811189, 19: 0.3185840707964602, 20: 0.18969072164948453, 21: 0.05263157894736842, 22: 0.32, 23: 0.43478260869565216, 24: 0.046511627906976744, 25: 0.36363636363636365, 26: 0.5882352941176471, 27: 0.04472843450479233, 28: 0.13793103448275862, 29: 0.680161943319838, 30: 0.6666666666666666, 31: 0.01809954751131222, 32: 0.5123456790123457, 33: 0.13953488372093023, 34: 0.12941176470588237, 35: 0.2144927536231884, 36: 0.12631578947368421, 37: 0.18018018018018017, 38: 0.18439716312056736, 39: 0.08602150537634409, 40: 0.2518891687657431}
Micro-average F1 score: 0.24675987345647515
Weighted-average F1 score: 0.2277732956961951
F1 score per class: {0: 0.21604938271604937, 1: 0.13861386138613863, 2: 0.1686746987951807, 3: 0.2807017543859649, 4: 0.8555555555555555, 5: 0.4852941176470588, 6: 0.24671916010498687, 7: 0.029850746268656716, 8: 0.294478527607362, 9: 0.6329113924050633, 10: 0.2436548223350254, 11: 0.12431842966194111, 12: 0.14540816326530612, 13: 0.009302325581395349, 14: 0.2158273381294964, 15: 0.27906976744186046, 16: 0.3302752293577982, 17: 0.10810810810810811, 18: 0.10810810810810811, 19: 0.3444976076555024, 20: 0.17904761904761904, 21: 0.06405693950177936, 22: 0.3252747252747253, 23: 0.5283018867924528, 24: 0.05128205128205128, 25: 0.37333333333333335, 26: 0.6046511627906976, 27: 0.04281345565749235, 28: 0.07407407407407407, 29: 0.6829268292682927, 30: 0.723404255319149, 31: 0.025806451612903226, 32: 0.5407407407407407, 33: 0.16666666666666666, 34: 0.14285714285714285, 35: 0.1837270341207349, 36: 0.02564102564102564, 37: 0.1925133689839572, 38: 0.17105263157894737, 39: 0.10144927536231885, 40: 0.27624309392265195}
Micro-average F1 score: 0.2509619495510902
Weighted-average F1 score: 0.22813622287507931
cur_acc_wo_na:  ['0.8192', '0.6965', '0.5361', '0.4914', '0.7908', '0.8473', '0.5433', '0.5981']
his_acc_wo_na:  ['0.8192', '0.7394', '0.6511', '0.5574', '0.5696', '0.5933', '0.5577', '0.5258']
cur_acc des_wo_na:  ['0.8423', '0.8485', '0.7893', '0.5136', '0.8189', '0.7506', '0.7167', '0.7644']
his_acc des_wo_na:  ['0.8423', '0.8355', '0.7437', '0.6364', '0.6510', '0.6503', '0.6191', '0.5973']
cur_acc rrf_wo_na:  ['0.8423', '0.8468', '0.7794', '0.5303', '0.8387', '0.7765', '0.7317', '0.7644']
his_acc rrf_wo_na:  ['0.8423', '0.8176', '0.7334', '0.6156', '0.6391', '0.6297', '0.6077', '0.5851']
cur_acc_w_na:  ['0.6829', '0.4628', '0.3678', '0.2472', '0.4749', '0.5400', '0.2926', '0.3368']
his_acc_w_na:  ['0.6829', '0.5346', '0.4736', '0.3290', '0.3491', '0.3327', '0.3199', '0.2786']
cur_acc des_w_na:  ['0.6268', '0.4536', '0.3918', '0.2122', '0.3415', '0.3093', '0.2592', '0.3276']
his_acc des_w_na:  ['0.6268', '0.4808', '0.4193', '0.3024', '0.3045', '0.2593', '0.2361', '0.2468']
cur_acc rrf_w_na:  ['0.6306', '0.4750', '0.4005', '0.2255', '0.3579', '0.3545', '0.2740', '0.3373']
his_acc rrf_w_na:  ['0.6306', '0.4948', '0.4284', '0.3035', '0.3042', '0.2733', '0.2430', '0.2510']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 214.5390827CurrentTrain: epoch  0, batch     1 | loss: 118.2666481CurrentTrain: epoch  0, batch     2 | loss: 102.0298967CurrentTrain: epoch  0, batch     3 | loss: 121.2127685CurrentTrain: epoch  0, batch     4 | loss: 119.5667081CurrentTrain: epoch  0, batch     5 | loss: 146.4221984CurrentTrain: epoch  0, batch     6 | loss: 146.1988548CurrentTrain: epoch  0, batch     7 | loss: 86.8919645CurrentTrain: epoch  0, batch     8 | loss: 144.2008523CurrentTrain: epoch  0, batch     9 | loss: 115.2226754CurrentTrain: epoch  0, batch    10 | loss: 144.2285957CurrentTrain: epoch  0, batch    11 | loss: 118.1557667CurrentTrain: epoch  0, batch    12 | loss: 98.0776762CurrentTrain: epoch  0, batch    13 | loss: 116.9582582CurrentTrain: epoch  0, batch    14 | loss: 144.2152704CurrentTrain: epoch  0, batch    15 | loss: 115.8304685CurrentTrain: epoch  0, batch    16 | loss: 117.2249345CurrentTrain: epoch  0, batch    17 | loss: 190.4859167CurrentTrain: epoch  0, batch    18 | loss: 96.9565056CurrentTrain: epoch  0, batch    19 | loss: 115.5694661CurrentTrain: epoch  0, batch    20 | loss: 99.7692668CurrentTrain: epoch  0, batch    21 | loss: 143.3641113CurrentTrain: epoch  0, batch    22 | loss: 86.3549121CurrentTrain: epoch  0, batch    23 | loss: 114.7422067CurrentTrain: epoch  0, batch    24 | loss: 115.2606305CurrentTrain: epoch  0, batch    25 | loss: 115.4881358CurrentTrain: epoch  0, batch    26 | loss: 114.4218441CurrentTrain: epoch  0, batch    27 | loss: 96.1710539CurrentTrain: epoch  0, batch    28 | loss: 187.9832187CurrentTrain: epoch  0, batch    29 | loss: 96.4698619CurrentTrain: epoch  0, batch    30 | loss: 142.5147203CurrentTrain: epoch  0, batch    31 | loss: 96.2545302CurrentTrain: epoch  0, batch    32 | loss: 114.6057923CurrentTrain: epoch  0, batch    33 | loss: 84.4435235CurrentTrain: epoch  0, batch    34 | loss: 114.9814672CurrentTrain: epoch  0, batch    35 | loss: 96.9845493CurrentTrain: epoch  0, batch    36 | loss: 83.1411259CurrentTrain: epoch  0, batch    37 | loss: 97.0354660CurrentTrain: epoch  0, batch    38 | loss: 189.0473318CurrentTrain: epoch  0, batch    39 | loss: 115.1603763CurrentTrain: epoch  0, batch    40 | loss: 141.5573275CurrentTrain: epoch  0, batch    41 | loss: 83.1518847CurrentTrain: epoch  0, batch    42 | loss: 84.1840321CurrentTrain: epoch  0, batch    43 | loss: 187.9090123CurrentTrain: epoch  0, batch    44 | loss: 96.1180651CurrentTrain: epoch  0, batch    45 | loss: 82.6781843CurrentTrain: epoch  0, batch    46 | loss: 95.4629099CurrentTrain: epoch  0, batch    47 | loss: 114.7433277CurrentTrain: epoch  0, batch    48 | loss: 114.6034219CurrentTrain: epoch  0, batch    49 | loss: 82.5322517CurrentTrain: epoch  0, batch    50 | loss: 140.8513195CurrentTrain: epoch  0, batch    51 | loss: 83.1625425CurrentTrain: epoch  0, batch    52 | loss: 280.2708314CurrentTrain: epoch  0, batch    53 | loss: 113.3781101CurrentTrain: epoch  0, batch    54 | loss: 95.4627112CurrentTrain: epoch  0, batch    55 | loss: 113.2364105CurrentTrain: epoch  0, batch    56 | loss: 114.3259970CurrentTrain: epoch  0, batch    57 | loss: 96.1395367CurrentTrain: epoch  0, batch    58 | loss: 112.8027017CurrentTrain: epoch  0, batch    59 | loss: 113.2864749CurrentTrain: epoch  0, batch    60 | loss: 95.4176743CurrentTrain: epoch  0, batch    61 | loss: 141.5137014CurrentTrain: epoch  0, batch    62 | loss: 94.0547417CurrentTrain: epoch  0, batch    63 | loss: 140.3456554CurrentTrain: epoch  0, batch    64 | loss: 140.2704850CurrentTrain: epoch  0, batch    65 | loss: 140.8244842CurrentTrain: epoch  0, batch    66 | loss: 80.7624091CurrentTrain: epoch  0, batch    67 | loss: 113.3379017CurrentTrain: epoch  0, batch    68 | loss: 113.8081108CurrentTrain: epoch  0, batch    69 | loss: 112.7083618CurrentTrain: epoch  0, batch    70 | loss: 82.3302632CurrentTrain: epoch  0, batch    71 | loss: 187.0164809CurrentTrain: epoch  0, batch    72 | loss: 92.1282754CurrentTrain: epoch  0, batch    73 | loss: 138.3763128CurrentTrain: epoch  0, batch    74 | loss: 142.0337423CurrentTrain: epoch  0, batch    75 | loss: 95.9978011CurrentTrain: epoch  0, batch    76 | loss: 138.9049413CurrentTrain: epoch  0, batch    77 | loss: 115.1657874CurrentTrain: epoch  0, batch    78 | loss: 92.1163516CurrentTrain: epoch  0, batch    79 | loss: 93.5588745CurrentTrain: epoch  0, batch    80 | loss: 111.6375432CurrentTrain: epoch  0, batch    81 | loss: 185.7320328CurrentTrain: epoch  0, batch    82 | loss: 109.6013627CurrentTrain: epoch  0, batch    83 | loss: 183.5457293CurrentTrain: epoch  0, batch    84 | loss: 111.4543845CurrentTrain: epoch  0, batch    85 | loss: 110.9998904CurrentTrain: epoch  0, batch    86 | loss: 140.4016728CurrentTrain: epoch  0, batch    87 | loss: 93.0955310CurrentTrain: epoch  0, batch    88 | loss: 110.3869045CurrentTrain: epoch  0, batch    89 | loss: 92.5441577CurrentTrain: epoch  0, batch    90 | loss: 111.6720579CurrentTrain: epoch  0, batch    91 | loss: 138.6096591CurrentTrain: epoch  0, batch    92 | loss: 111.8750805CurrentTrain: epoch  0, batch    93 | loss: 139.3580988CurrentTrain: epoch  0, batch    94 | loss: 76.5752408CurrentTrain: epoch  0, batch    95 | loss: 111.5284601CurrentTrain: epoch  1, batch     0 | loss: 109.6634518CurrentTrain: epoch  1, batch     1 | loss: 90.5708402CurrentTrain: epoch  1, batch     2 | loss: 182.3896694CurrentTrain: epoch  1, batch     3 | loss: 108.7977108CurrentTrain: epoch  1, batch     4 | loss: 92.8272119CurrentTrain: epoch  1, batch     5 | loss: 109.5545716CurrentTrain: epoch  1, batch     6 | loss: 87.3268649CurrentTrain: epoch  1, batch     7 | loss: 182.9830805CurrentTrain: epoch  1, batch     8 | loss: 111.9369535CurrentTrain: epoch  1, batch     9 | loss: 90.5890547CurrentTrain: epoch  1, batch    10 | loss: 92.2732514CurrentTrain: epoch  1, batch    11 | loss: 108.9425399CurrentTrain: epoch  1, batch    12 | loss: 108.6976387CurrentTrain: epoch  1, batch    13 | loss: 91.1935517CurrentTrain: epoch  1, batch    14 | loss: 90.6383634CurrentTrain: epoch  1, batch    15 | loss: 86.9607929CurrentTrain: epoch  1, batch    16 | loss: 107.8121409CurrentTrain: epoch  1, batch    17 | loss: 109.3341313CurrentTrain: epoch  1, batch    18 | loss: 89.0730156CurrentTrain: epoch  1, batch    19 | loss: 107.8967315CurrentTrain: epoch  1, batch    20 | loss: 107.3526849CurrentTrain: epoch  1, batch    21 | loss: 106.6610889CurrentTrain: epoch  1, batch    22 | loss: 107.3148807CurrentTrain: epoch  1, batch    23 | loss: 89.4534873CurrentTrain: epoch  1, batch    24 | loss: 108.2133396CurrentTrain: epoch  1, batch    25 | loss: 178.5002789CurrentTrain: epoch  1, batch    26 | loss: 104.1840710CurrentTrain: epoch  1, batch    27 | loss: 130.5501117CurrentTrain: epoch  1, batch    28 | loss: 130.7966883CurrentTrain: epoch  1, batch    29 | loss: 140.4398052CurrentTrain: epoch  1, batch    30 | loss: 182.6145136CurrentTrain: epoch  1, batch    31 | loss: 180.0727103CurrentTrain: epoch  1, batch    32 | loss: 85.2125306CurrentTrain: epoch  1, batch    33 | loss: 91.1655452CurrentTrain: epoch  1, batch    34 | loss: 185.4810889CurrentTrain: epoch  1, batch    35 | loss: 85.7843474CurrentTrain: epoch  1, batch    36 | loss: 179.1270329CurrentTrain: epoch  1, batch    37 | loss: 106.2075839CurrentTrain: epoch  1, batch    38 | loss: 106.9191433CurrentTrain: epoch  1, batch    39 | loss: 108.5049031CurrentTrain: epoch  1, batch    40 | loss: 184.1179892CurrentTrain: epoch  1, batch    41 | loss: 137.6623769CurrentTrain: epoch  1, batch    42 | loss: 136.8520068CurrentTrain: epoch  1, batch    43 | loss: 132.1683090CurrentTrain: epoch  1, batch    44 | loss: 133.7843761CurrentTrain: epoch  1, batch    45 | loss: 107.6246951CurrentTrain: epoch  1, batch    46 | loss: 105.6913557CurrentTrain: epoch  1, batch    47 | loss: 84.4920167CurrentTrain: epoch  1, batch    48 | loss: 130.5076203CurrentTrain: epoch  1, batch    49 | loss: 132.0987279CurrentTrain: epoch  1, batch    50 | loss: 106.0235229CurrentTrain: epoch  1, batch    51 | loss: 87.8421295CurrentTrain: epoch  1, batch    52 | loss: 105.2570848CurrentTrain: epoch  1, batch    53 | loss: 83.8756195CurrentTrain: epoch  1, batch    54 | loss: 83.1818289CurrentTrain: epoch  1, batch    55 | loss: 90.4781665CurrentTrain: epoch  1, batch    56 | loss: 176.6857938CurrentTrain: epoch  1, batch    57 | loss: 89.5233540CurrentTrain: epoch  1, batch    58 | loss: 181.1148550CurrentTrain: epoch  1, batch    59 | loss: 136.8083569CurrentTrain: epoch  1, batch    60 | loss: 82.5085563CurrentTrain: epoch  1, batch    61 | loss: 134.6910020CurrentTrain: epoch  1, batch    62 | loss: 105.6063840CurrentTrain: epoch  1, batch    63 | loss: 184.7373412CurrentTrain: epoch  1, batch    64 | loss: 178.6281845CurrentTrain: epoch  1, batch    65 | loss: 89.5564653CurrentTrain: epoch  1, batch    66 | loss: 73.7534215CurrentTrain: epoch  1, batch    67 | loss: 105.2989834CurrentTrain: epoch  1, batch    68 | loss: 104.5078091CurrentTrain: epoch  1, batch    69 | loss: 85.6049882CurrentTrain: epoch  1, batch    70 | loss: 85.5367933CurrentTrain: epoch  1, batch    71 | loss: 86.5564855CurrentTrain: epoch  1, batch    72 | loss: 87.5957091CurrentTrain: epoch  1, batch    73 | loss: 133.5561892CurrentTrain: epoch  1, batch    74 | loss: 72.0570074CurrentTrain: epoch  1, batch    75 | loss: 83.5991054CurrentTrain: epoch  1, batch    76 | loss: 120.8080177CurrentTrain: epoch  1, batch    77 | loss: 74.1916112CurrentTrain: epoch  1, batch    78 | loss: 82.9191772CurrentTrain: epoch  1, batch    79 | loss: 106.1932528CurrentTrain: epoch  1, batch    80 | loss: 73.3277819CurrentTrain: epoch  1, batch    81 | loss: 134.0380401CurrentTrain: epoch  1, batch    82 | loss: 133.1943677CurrentTrain: epoch  1, batch    83 | loss: 130.3499802CurrentTrain: epoch  1, batch    84 | loss: 84.9531154CurrentTrain: epoch  1, batch    85 | loss: 130.1449614CurrentTrain: epoch  1, batch    86 | loss: 80.4165439CurrentTrain: epoch  1, batch    87 | loss: 134.0634146CurrentTrain: epoch  1, batch    88 | loss: 88.9417786CurrentTrain: epoch  1, batch    89 | loss: 84.7043280CurrentTrain: epoch  1, batch    90 | loss: 135.2268213CurrentTrain: epoch  1, batch    91 | loss: 131.0158618CurrentTrain: epoch  1, batch    92 | loss: 80.9102052CurrentTrain: epoch  1, batch    93 | loss: 85.7786248CurrentTrain: epoch  1, batch    94 | loss: 102.5289866CurrentTrain: epoch  1, batch    95 | loss: 71.8752478CurrentTrain: epoch  2, batch     0 | loss: 98.4787982CurrentTrain: epoch  2, batch     1 | loss: 129.0452639CurrentTrain: epoch  2, batch     2 | loss: 84.4547954CurrentTrain: epoch  2, batch     3 | loss: 96.2017972CurrentTrain: epoch  2, batch     4 | loss: 85.9309917CurrentTrain: epoch  2, batch     5 | loss: 77.5433252CurrentTrain: epoch  2, batch     6 | loss: 103.3093053CurrentTrain: epoch  2, batch     7 | loss: 104.3541190CurrentTrain: epoch  2, batch     8 | loss: 98.0647759CurrentTrain: epoch  2, batch     9 | loss: 128.7054472CurrentTrain: epoch  2, batch    10 | loss: 132.5209034CurrentTrain: epoch  2, batch    11 | loss: 107.5991731CurrentTrain: epoch  2, batch    12 | loss: 85.8807417CurrentTrain: epoch  2, batch    13 | loss: 84.5043688CurrentTrain: epoch  2, batch    14 | loss: 83.0412364CurrentTrain: epoch  2, batch    15 | loss: 169.5388258CurrentTrain: epoch  2, batch    16 | loss: 135.8058900CurrentTrain: epoch  2, batch    17 | loss: 80.9661018CurrentTrain: epoch  2, batch    18 | loss: 79.5681872CurrentTrain: epoch  2, batch    19 | loss: 103.2985139CurrentTrain: epoch  2, batch    20 | loss: 99.8915014CurrentTrain: epoch  2, batch    21 | loss: 82.9086201CurrentTrain: epoch  2, batch    22 | loss: 77.1057999CurrentTrain: epoch  2, batch    23 | loss: 105.4638033CurrentTrain: epoch  2, batch    24 | loss: 131.6670454CurrentTrain: epoch  2, batch    25 | loss: 130.1755487CurrentTrain: epoch  2, batch    26 | loss: 100.4483669CurrentTrain: epoch  2, batch    27 | loss: 106.7085079CurrentTrain: epoch  2, batch    28 | loss: 84.9647185CurrentTrain: epoch  2, batch    29 | loss: 180.5875457CurrentTrain: epoch  2, batch    30 | loss: 102.5168591CurrentTrain: epoch  2, batch    31 | loss: 103.2029356CurrentTrain: epoch  2, batch    32 | loss: 84.6028717CurrentTrain: epoch  2, batch    33 | loss: 87.8047631CurrentTrain: epoch  2, batch    34 | loss: 179.8129278CurrentTrain: epoch  2, batch    35 | loss: 130.1984386CurrentTrain: epoch  2, batch    36 | loss: 67.6637737CurrentTrain: epoch  2, batch    37 | loss: 137.2608295CurrentTrain: epoch  2, batch    38 | loss: 173.1464641CurrentTrain: epoch  2, batch    39 | loss: 125.8215976CurrentTrain: epoch  2, batch    40 | loss: 70.0356301CurrentTrain: epoch  2, batch    41 | loss: 69.2634805CurrentTrain: epoch  2, batch    42 | loss: 125.7664556CurrentTrain: epoch  2, batch    43 | loss: 104.9495683CurrentTrain: epoch  2, batch    44 | loss: 82.0611239CurrentTrain: epoch  2, batch    45 | loss: 183.4543521CurrentTrain: epoch  2, batch    46 | loss: 99.7264061CurrentTrain: epoch  2, batch    47 | loss: 129.2939468CurrentTrain: epoch  2, batch    48 | loss: 182.2613617CurrentTrain: epoch  2, batch    49 | loss: 85.7917121CurrentTrain: epoch  2, batch    50 | loss: 88.0918350CurrentTrain: epoch  2, batch    51 | loss: 130.2142482CurrentTrain: epoch  2, batch    52 | loss: 180.3093185CurrentTrain: epoch  2, batch    53 | loss: 71.8574050CurrentTrain: epoch  2, batch    54 | loss: 80.2004551CurrentTrain: epoch  2, batch    55 | loss: 82.3055278CurrentTrain: epoch  2, batch    56 | loss: 130.6739396CurrentTrain: epoch  2, batch    57 | loss: 130.5743221CurrentTrain: epoch  2, batch    58 | loss: 105.6417498CurrentTrain: epoch  2, batch    59 | loss: 133.6491650CurrentTrain: epoch  2, batch    60 | loss: 132.4870653CurrentTrain: epoch  2, batch    61 | loss: 101.7314061CurrentTrain: epoch  2, batch    62 | loss: 90.2253985CurrentTrain: epoch  2, batch    63 | loss: 270.9087097CurrentTrain: epoch  2, batch    64 | loss: 135.0531547CurrentTrain: epoch  2, batch    65 | loss: 97.0775239CurrentTrain: epoch  2, batch    66 | loss: 102.8204207CurrentTrain: epoch  2, batch    67 | loss: 83.1390247CurrentTrain: epoch  2, batch    68 | loss: 181.3919217CurrentTrain: epoch  2, batch    69 | loss: 177.4940870CurrentTrain: epoch  2, batch    70 | loss: 78.5019309CurrentTrain: epoch  2, batch    71 | loss: 100.4036526CurrentTrain: epoch  2, batch    72 | loss: 102.2560868CurrentTrain: epoch  2, batch    73 | loss: 127.0523097CurrentTrain: epoch  2, batch    74 | loss: 84.3956840CurrentTrain: epoch  2, batch    75 | loss: 105.7723067CurrentTrain: epoch  2, batch    76 | loss: 103.5812440CurrentTrain: epoch  2, batch    77 | loss: 85.2846105CurrentTrain: epoch  2, batch    78 | loss: 169.1757296CurrentTrain: epoch  2, batch    79 | loss: 127.6217224CurrentTrain: epoch  2, batch    80 | loss: 177.8214045CurrentTrain: epoch  2, batch    81 | loss: 83.2794488CurrentTrain: epoch  2, batch    82 | loss: 86.8940989CurrentTrain: epoch  2, batch    83 | loss: 103.8472768CurrentTrain: epoch  2, batch    84 | loss: 82.1476505CurrentTrain: epoch  2, batch    85 | loss: 131.2721061CurrentTrain: epoch  2, batch    86 | loss: 76.8220963CurrentTrain: epoch  2, batch    87 | loss: 128.4360601CurrentTrain: epoch  2, batch    88 | loss: 108.8916654CurrentTrain: epoch  2, batch    89 | loss: 104.9115817CurrentTrain: epoch  2, batch    90 | loss: 80.4495524CurrentTrain: epoch  2, batch    91 | loss: 130.5339442CurrentTrain: epoch  2, batch    92 | loss: 135.8527620CurrentTrain: epoch  2, batch    93 | loss: 130.5550689CurrentTrain: epoch  2, batch    94 | loss: 276.8291813CurrentTrain: epoch  2, batch    95 | loss: 73.3093926CurrentTrain: epoch  3, batch     0 | loss: 85.3488956CurrentTrain: epoch  3, batch     1 | loss: 103.2067068CurrentTrain: epoch  3, batch     2 | loss: 100.8244004CurrentTrain: epoch  3, batch     3 | loss: 103.9492402CurrentTrain: epoch  3, batch     4 | loss: 96.6573423CurrentTrain: epoch  3, batch     5 | loss: 125.1388324CurrentTrain: epoch  3, batch     6 | loss: 85.8203485CurrentTrain: epoch  3, batch     7 | loss: 96.7869217CurrentTrain: epoch  3, batch     8 | loss: 67.5534152CurrentTrain: epoch  3, batch     9 | loss: 82.8242902CurrentTrain: epoch  3, batch    10 | loss: 81.1872176CurrentTrain: epoch  3, batch    11 | loss: 72.1732560CurrentTrain: epoch  3, batch    12 | loss: 95.2876305CurrentTrain: epoch  3, batch    13 | loss: 124.8870241CurrentTrain: epoch  3, batch    14 | loss: 178.4721953CurrentTrain: epoch  3, batch    15 | loss: 69.0295686CurrentTrain: epoch  3, batch    16 | loss: 131.9733394CurrentTrain: epoch  3, batch    17 | loss: 100.6175968CurrentTrain: epoch  3, batch    18 | loss: 122.4670653CurrentTrain: epoch  3, batch    19 | loss: 87.9203850CurrentTrain: epoch  3, batch    20 | loss: 67.5270974CurrentTrain: epoch  3, batch    21 | loss: 175.7372173CurrentTrain: epoch  3, batch    22 | loss: 126.1155358CurrentTrain: epoch  3, batch    23 | loss: 81.0050381CurrentTrain: epoch  3, batch    24 | loss: 99.7053694CurrentTrain: epoch  3, batch    25 | loss: 102.0029508CurrentTrain: epoch  3, batch    26 | loss: 133.2886594CurrentTrain: epoch  3, batch    27 | loss: 105.3822699CurrentTrain: epoch  3, batch    28 | loss: 176.9641819CurrentTrain: epoch  3, batch    29 | loss: 130.2864305CurrentTrain: epoch  3, batch    30 | loss: 100.8645713CurrentTrain: epoch  3, batch    31 | loss: 77.5116037CurrentTrain: epoch  3, batch    32 | loss: 132.9818919CurrentTrain: epoch  3, batch    33 | loss: 81.8944515CurrentTrain: epoch  3, batch    34 | loss: 70.1890903CurrentTrain: epoch  3, batch    35 | loss: 123.0758947CurrentTrain: epoch  3, batch    36 | loss: 78.9176522CurrentTrain: epoch  3, batch    37 | loss: 101.6092809CurrentTrain: epoch  3, batch    38 | loss: 130.3221547CurrentTrain: epoch  3, batch    39 | loss: 99.8823298CurrentTrain: epoch  3, batch    40 | loss: 100.7147802CurrentTrain: epoch  3, batch    41 | loss: 174.1879225CurrentTrain: epoch  3, batch    42 | loss: 127.7301899CurrentTrain: epoch  3, batch    43 | loss: 79.6762319CurrentTrain: epoch  3, batch    44 | loss: 99.5787984CurrentTrain: epoch  3, batch    45 | loss: 97.9606182CurrentTrain: epoch  3, batch    46 | loss: 78.2461596CurrentTrain: epoch  3, batch    47 | loss: 100.6325180CurrentTrain: epoch  3, batch    48 | loss: 171.5439523CurrentTrain: epoch  3, batch    49 | loss: 134.0278927CurrentTrain: epoch  3, batch    50 | loss: 85.8477289CurrentTrain: epoch  3, batch    51 | loss: 97.9911583CurrentTrain: epoch  3, batch    52 | loss: 130.3271229CurrentTrain: epoch  3, batch    53 | loss: 126.8799760CurrentTrain: epoch  3, batch    54 | loss: 80.8605614CurrentTrain: epoch  3, batch    55 | loss: 98.7903994CurrentTrain: epoch  3, batch    56 | loss: 85.4119667CurrentTrain: epoch  3, batch    57 | loss: 103.5239310CurrentTrain: epoch  3, batch    58 | loss: 68.6184930CurrentTrain: epoch  3, batch    59 | loss: 271.0888975CurrentTrain: epoch  3, batch    60 | loss: 174.3007513CurrentTrain: epoch  3, batch    61 | loss: 68.7881062CurrentTrain: epoch  3, batch    62 | loss: 87.4506400CurrentTrain: epoch  3, batch    63 | loss: 95.1135485CurrentTrain: epoch  3, batch    64 | loss: 101.2669267CurrentTrain: epoch  3, batch    65 | loss: 126.2758739CurrentTrain: epoch  3, batch    66 | loss: 99.6295693CurrentTrain: epoch  3, batch    67 | loss: 178.8078246CurrentTrain: epoch  3, batch    68 | loss: 85.6230593CurrentTrain: epoch  3, batch    69 | loss: 79.9045336CurrentTrain: epoch  3, batch    70 | loss: 87.9239739CurrentTrain: epoch  3, batch    71 | loss: 76.1792950CurrentTrain: epoch  3, batch    72 | loss: 96.0918034CurrentTrain: epoch  3, batch    73 | loss: 102.2724838CurrentTrain: epoch  3, batch    74 | loss: 84.0933088CurrentTrain: epoch  3, batch    75 | loss: 125.4559494CurrentTrain: epoch  3, batch    76 | loss: 267.1613626CurrentTrain: epoch  3, batch    77 | loss: 80.2853758CurrentTrain: epoch  3, batch    78 | loss: 106.9587947CurrentTrain: epoch  3, batch    79 | loss: 176.2242776CurrentTrain: epoch  3, batch    80 | loss: 273.6860269CurrentTrain: epoch  3, batch    81 | loss: 81.6204308CurrentTrain: epoch  3, batch    82 | loss: 82.9308890CurrentTrain: epoch  3, batch    83 | loss: 179.7270410CurrentTrain: epoch  3, batch    84 | loss: 130.9579740CurrentTrain: epoch  3, batch    85 | loss: 126.3566116CurrentTrain: epoch  3, batch    86 | loss: 81.9642382CurrentTrain: epoch  3, batch    87 | loss: 123.1451387CurrentTrain: epoch  3, batch    88 | loss: 96.9896722CurrentTrain: epoch  3, batch    89 | loss: 89.5266387CurrentTrain: epoch  3, batch    90 | loss: 97.5495781CurrentTrain: epoch  3, batch    91 | loss: 82.2052436CurrentTrain: epoch  3, batch    92 | loss: 178.2414370CurrentTrain: epoch  3, batch    93 | loss: 131.9826462CurrentTrain: epoch  3, batch    94 | loss: 84.3614541CurrentTrain: epoch  3, batch    95 | loss: 143.2726391CurrentTrain: epoch  4, batch     0 | loss: 130.7823636CurrentTrain: epoch  4, batch     1 | loss: 76.9299302CurrentTrain: epoch  4, batch     2 | loss: 84.3585494CurrentTrain: epoch  4, batch     3 | loss: 99.9295154CurrentTrain: epoch  4, batch     4 | loss: 66.1339939CurrentTrain: epoch  4, batch     5 | loss: 129.5539742CurrentTrain: epoch  4, batch     6 | loss: 275.3438927CurrentTrain: epoch  4, batch     7 | loss: 83.4764068CurrentTrain: epoch  4, batch     8 | loss: 82.0107491CurrentTrain: epoch  4, batch     9 | loss: 81.1278303CurrentTrain: epoch  4, batch    10 | loss: 174.2013819CurrentTrain: epoch  4, batch    11 | loss: 79.7992842CurrentTrain: epoch  4, batch    12 | loss: 95.9455258CurrentTrain: epoch  4, batch    13 | loss: 130.1563527CurrentTrain: epoch  4, batch    14 | loss: 128.0594757CurrentTrain: epoch  4, batch    15 | loss: 98.7786100CurrentTrain: epoch  4, batch    16 | loss: 96.7971991CurrentTrain: epoch  4, batch    17 | loss: 80.5092212CurrentTrain: epoch  4, batch    18 | loss: 95.9060009CurrentTrain: epoch  4, batch    19 | loss: 100.6894482CurrentTrain: epoch  4, batch    20 | loss: 273.8147262CurrentTrain: epoch  4, batch    21 | loss: 124.3795337CurrentTrain: epoch  4, batch    22 | loss: 127.2334223CurrentTrain: epoch  4, batch    23 | loss: 99.3572735CurrentTrain: epoch  4, batch    24 | loss: 78.8881757CurrentTrain: epoch  4, batch    25 | loss: 128.6225567CurrentTrain: epoch  4, batch    26 | loss: 129.6646632CurrentTrain: epoch  4, batch    27 | loss: 98.6791606CurrentTrain: epoch  4, batch    28 | loss: 99.0796588CurrentTrain: epoch  4, batch    29 | loss: 101.3276052CurrentTrain: epoch  4, batch    30 | loss: 83.9788388CurrentTrain: epoch  4, batch    31 | loss: 178.4891823CurrentTrain: epoch  4, batch    32 | loss: 96.4853281CurrentTrain: epoch  4, batch    33 | loss: 99.2724543CurrentTrain: epoch  4, batch    34 | loss: 81.8635718CurrentTrain: epoch  4, batch    35 | loss: 94.6839743CurrentTrain: epoch  4, batch    36 | loss: 98.1230834CurrentTrain: epoch  4, batch    37 | loss: 99.0378918CurrentTrain: epoch  4, batch    38 | loss: 94.7507265CurrentTrain: epoch  4, batch    39 | loss: 133.9084240CurrentTrain: epoch  4, batch    40 | loss: 123.6690268CurrentTrain: epoch  4, batch    41 | loss: 130.1997549CurrentTrain: epoch  4, batch    42 | loss: 103.9963377CurrentTrain: epoch  4, batch    43 | loss: 131.5829193CurrentTrain: epoch  4, batch    44 | loss: 123.0474491CurrentTrain: epoch  4, batch    45 | loss: 69.4330791CurrentTrain: epoch  4, batch    46 | loss: 559.1075798CurrentTrain: epoch  4, batch    47 | loss: 84.3101426CurrentTrain: epoch  4, batch    48 | loss: 98.4778565CurrentTrain: epoch  4, batch    49 | loss: 70.2493273CurrentTrain: epoch  4, batch    50 | loss: 129.7192464CurrentTrain: epoch  4, batch    51 | loss: 177.4222188CurrentTrain: epoch  4, batch    52 | loss: 119.8047241CurrentTrain: epoch  4, batch    53 | loss: 99.2792804CurrentTrain: epoch  4, batch    54 | loss: 78.8585933CurrentTrain: epoch  4, batch    55 | loss: 79.8896270CurrentTrain: epoch  4, batch    56 | loss: 80.4393979CurrentTrain: epoch  4, batch    57 | loss: 67.6452548CurrentTrain: epoch  4, batch    58 | loss: 170.1689684CurrentTrain: epoch  4, batch    59 | loss: 73.0626918CurrentTrain: epoch  4, batch    60 | loss: 81.2324066CurrentTrain: epoch  4, batch    61 | loss: 98.9336586CurrentTrain: epoch  4, batch    62 | loss: 76.8718460CurrentTrain: epoch  4, batch    63 | loss: 102.2706012CurrentTrain: epoch  4, batch    64 | loss: 97.2262190CurrentTrain: epoch  4, batch    65 | loss: 98.0234412CurrentTrain: epoch  4, batch    66 | loss: 126.3005705CurrentTrain: epoch  4, batch    67 | loss: 99.3732986CurrentTrain: epoch  4, batch    68 | loss: 128.8480646CurrentTrain: epoch  4, batch    69 | loss: 102.8218043CurrentTrain: epoch  4, batch    70 | loss: 126.0605541CurrentTrain: epoch  4, batch    71 | loss: 126.5286696CurrentTrain: epoch  4, batch    72 | loss: 79.7054012CurrentTrain: epoch  4, batch    73 | loss: 126.4095092CurrentTrain: epoch  4, batch    74 | loss: 65.0891649CurrentTrain: epoch  4, batch    75 | loss: 125.8201782CurrentTrain: epoch  4, batch    76 | loss: 97.2809845CurrentTrain: epoch  4, batch    77 | loss: 129.6627201CurrentTrain: epoch  4, batch    78 | loss: 126.6652479CurrentTrain: epoch  4, batch    79 | loss: 65.3047502CurrentTrain: epoch  4, batch    80 | loss: 102.8142424CurrentTrain: epoch  4, batch    81 | loss: 125.2826848CurrentTrain: epoch  4, batch    82 | loss: 97.1514219CurrentTrain: epoch  4, batch    83 | loss: 99.2111407CurrentTrain: epoch  4, batch    84 | loss: 131.7221328CurrentTrain: epoch  4, batch    85 | loss: 82.6267949CurrentTrain: epoch  4, batch    86 | loss: 70.6019143CurrentTrain: epoch  4, batch    87 | loss: 67.6954508CurrentTrain: epoch  4, batch    88 | loss: 128.6268375CurrentTrain: epoch  4, batch    89 | loss: 101.8819423CurrentTrain: epoch  4, batch    90 | loss: 97.4708858CurrentTrain: epoch  4, batch    91 | loss: 126.2753444CurrentTrain: epoch  4, batch    92 | loss: 98.3381923CurrentTrain: epoch  4, batch    93 | loss: 100.4401501CurrentTrain: epoch  4, batch    94 | loss: 81.6303782CurrentTrain: epoch  4, batch    95 | loss: 85.5622455CurrentTrain: epoch  5, batch     0 | loss: 101.2210316CurrentTrain: epoch  5, batch     1 | loss: 123.0441222CurrentTrain: epoch  5, batch     2 | loss: 94.3512860CurrentTrain: epoch  5, batch     3 | loss: 97.7650498CurrentTrain: epoch  5, batch     4 | loss: 81.0109011CurrentTrain: epoch  5, batch     5 | loss: 98.5625432CurrentTrain: epoch  5, batch     6 | loss: 81.0780923CurrentTrain: epoch  5, batch     7 | loss: 79.1356771CurrentTrain: epoch  5, batch     8 | loss: 93.8038894CurrentTrain: epoch  5, batch     9 | loss: 127.9488659CurrentTrain: epoch  5, batch    10 | loss: 173.8439705CurrentTrain: epoch  5, batch    11 | loss: 125.4896487CurrentTrain: epoch  5, batch    12 | loss: 95.9334389CurrentTrain: epoch  5, batch    13 | loss: 98.3710282CurrentTrain: epoch  5, batch    14 | loss: 97.4951336CurrentTrain: epoch  5, batch    15 | loss: 124.1560752CurrentTrain: epoch  5, batch    16 | loss: 119.2449922CurrentTrain: epoch  5, batch    17 | loss: 78.6835774CurrentTrain: epoch  5, batch    18 | loss: 96.4103074CurrentTrain: epoch  5, batch    19 | loss: 134.4780436CurrentTrain: epoch  5, batch    20 | loss: 124.7629650CurrentTrain: epoch  5, batch    21 | loss: 131.3651701CurrentTrain: epoch  5, batch    22 | loss: 94.6504966CurrentTrain: epoch  5, batch    23 | loss: 127.6342882CurrentTrain: epoch  5, batch    24 | loss: 97.1130630CurrentTrain: epoch  5, batch    25 | loss: 79.5637241CurrentTrain: epoch  5, batch    26 | loss: 92.2010083CurrentTrain: epoch  5, batch    27 | loss: 180.3671004CurrentTrain: epoch  5, batch    28 | loss: 102.5777148CurrentTrain: epoch  5, batch    29 | loss: 131.0735747CurrentTrain: epoch  5, batch    30 | loss: 67.2047428CurrentTrain: epoch  5, batch    31 | loss: 97.6452500CurrentTrain: epoch  5, batch    32 | loss: 98.9483292CurrentTrain: epoch  5, batch    33 | loss: 174.1176042CurrentTrain: epoch  5, batch    34 | loss: 95.5120812CurrentTrain: epoch  5, batch    35 | loss: 83.5883503CurrentTrain: epoch  5, batch    36 | loss: 77.4069464CurrentTrain: epoch  5, batch    37 | loss: 79.7731667CurrentTrain: epoch  5, batch    38 | loss: 101.0773180CurrentTrain: epoch  5, batch    39 | loss: 77.4992388CurrentTrain: epoch  5, batch    40 | loss: 78.8127980CurrentTrain: epoch  5, batch    41 | loss: 77.7061055CurrentTrain: epoch  5, batch    42 | loss: 98.9706665CurrentTrain: epoch  5, batch    43 | loss: 63.1849365CurrentTrain: epoch  5, batch    44 | loss: 105.8282286CurrentTrain: epoch  5, batch    45 | loss: 97.3228222CurrentTrain: epoch  5, batch    46 | loss: 82.0311944CurrentTrain: epoch  5, batch    47 | loss: 95.3255257CurrentTrain: epoch  5, batch    48 | loss: 100.7816123CurrentTrain: epoch  5, batch    49 | loss: 98.7576712CurrentTrain: epoch  5, batch    50 | loss: 104.1142938CurrentTrain: epoch  5, batch    51 | loss: 97.1042456CurrentTrain: epoch  5, batch    52 | loss: 96.8035913CurrentTrain: epoch  5, batch    53 | loss: 126.6707473CurrentTrain: epoch  5, batch    54 | loss: 91.8077789CurrentTrain: epoch  5, batch    55 | loss: 101.8491964CurrentTrain: epoch  5, batch    56 | loss: 96.5868308CurrentTrain: epoch  5, batch    57 | loss: 176.3047211CurrentTrain: epoch  5, batch    58 | loss: 100.3983579CurrentTrain: epoch  5, batch    59 | loss: 78.8166274CurrentTrain: epoch  5, batch    60 | loss: 66.6424724CurrentTrain: epoch  5, batch    61 | loss: 559.0019781CurrentTrain: epoch  5, batch    62 | loss: 80.2932537CurrentTrain: epoch  5, batch    63 | loss: 129.3553117CurrentTrain: epoch  5, batch    64 | loss: 126.1022822CurrentTrain: epoch  5, batch    65 | loss: 82.4904695CurrentTrain: epoch  5, batch    66 | loss: 177.5184181CurrentTrain: epoch  5, batch    67 | loss: 122.3453518CurrentTrain: epoch  5, batch    68 | loss: 178.3959721CurrentTrain: epoch  5, batch    69 | loss: 76.5052365CurrentTrain: epoch  5, batch    70 | loss: 121.6117029CurrentTrain: epoch  5, batch    71 | loss: 100.1301034CurrentTrain: epoch  5, batch    72 | loss: 67.4501957CurrentTrain: epoch  5, batch    73 | loss: 100.4548794CurrentTrain: epoch  5, batch    74 | loss: 97.9705523CurrentTrain: epoch  5, batch    75 | loss: 128.7258141CurrentTrain: epoch  5, batch    76 | loss: 126.3895633CurrentTrain: epoch  5, batch    77 | loss: 80.9419760CurrentTrain: epoch  5, batch    78 | loss: 66.0987105CurrentTrain: epoch  5, batch    79 | loss: 170.7095828CurrentTrain: epoch  5, batch    80 | loss: 124.9110352CurrentTrain: epoch  5, batch    81 | loss: 68.8756540CurrentTrain: epoch  5, batch    82 | loss: 123.6336189CurrentTrain: epoch  5, batch    83 | loss: 128.2689083CurrentTrain: epoch  5, batch    84 | loss: 129.3532465CurrentTrain: epoch  5, batch    85 | loss: 127.9759111CurrentTrain: epoch  5, batch    86 | loss: 81.4401621CurrentTrain: epoch  5, batch    87 | loss: 83.5932728CurrentTrain: epoch  5, batch    88 | loss: 83.3241970CurrentTrain: epoch  5, batch    89 | loss: 126.8060030CurrentTrain: epoch  5, batch    90 | loss: 171.4903979CurrentTrain: epoch  5, batch    91 | loss: 61.6841231CurrentTrain: epoch  5, batch    92 | loss: 176.6394746CurrentTrain: epoch  5, batch    93 | loss: 129.8141583CurrentTrain: epoch  5, batch    94 | loss: 62.6281059CurrentTrain: epoch  5, batch    95 | loss: 82.8676815CurrentTrain: epoch  6, batch     0 | loss: 129.5008709CurrentTrain: epoch  6, batch     1 | loss: 95.3268243CurrentTrain: epoch  6, batch     2 | loss: 131.0639058CurrentTrain: epoch  6, batch     3 | loss: 96.2939711CurrentTrain: epoch  6, batch     4 | loss: 178.0875953CurrentTrain: epoch  6, batch     5 | loss: 96.1073421CurrentTrain: epoch  6, batch     6 | loss: 98.0168686CurrentTrain: epoch  6, batch     7 | loss: 80.0263501CurrentTrain: epoch  6, batch     8 | loss: 76.4369517CurrentTrain: epoch  6, batch     9 | loss: 94.3772944CurrentTrain: epoch  6, batch    10 | loss: 124.0173352CurrentTrain: epoch  6, batch    11 | loss: 93.6296130CurrentTrain: epoch  6, batch    12 | loss: 67.6551718CurrentTrain: epoch  6, batch    13 | loss: 78.9053864CurrentTrain: epoch  6, batch    14 | loss: 98.8832972CurrentTrain: epoch  6, batch    15 | loss: 96.3538958CurrentTrain: epoch  6, batch    16 | loss: 99.5435392CurrentTrain: epoch  6, batch    17 | loss: 66.1174335CurrentTrain: epoch  6, batch    18 | loss: 98.7807543CurrentTrain: epoch  6, batch    19 | loss: 79.9019215CurrentTrain: epoch  6, batch    20 | loss: 97.2472818CurrentTrain: epoch  6, batch    21 | loss: 67.7091459CurrentTrain: epoch  6, batch    22 | loss: 122.2992816CurrentTrain: epoch  6, batch    23 | loss: 92.9335647CurrentTrain: epoch  6, batch    24 | loss: 176.6720929CurrentTrain: epoch  6, batch    25 | loss: 96.7330820CurrentTrain: epoch  6, batch    26 | loss: 176.5550167CurrentTrain: epoch  6, batch    27 | loss: 130.2399575CurrentTrain: epoch  6, batch    28 | loss: 127.6135607CurrentTrain: epoch  6, batch    29 | loss: 123.1604554CurrentTrain: epoch  6, batch    30 | loss: 128.9389701CurrentTrain: epoch  6, batch    31 | loss: 98.6730868CurrentTrain: epoch  6, batch    32 | loss: 75.6245213CurrentTrain: epoch  6, batch    33 | loss: 129.0889187CurrentTrain: epoch  6, batch    34 | loss: 98.4285325CurrentTrain: epoch  6, batch    35 | loss: 83.4462701CurrentTrain: epoch  6, batch    36 | loss: 99.5196900CurrentTrain: epoch  6, batch    37 | loss: 65.5591457CurrentTrain: epoch  6, batch    38 | loss: 81.2833888CurrentTrain: epoch  6, batch    39 | loss: 99.2543061CurrentTrain: epoch  6, batch    40 | loss: 64.2595520CurrentTrain: epoch  6, batch    41 | loss: 66.3769934CurrentTrain: epoch  6, batch    42 | loss: 98.7614422CurrentTrain: epoch  6, batch    43 | loss: 75.7859091CurrentTrain: epoch  6, batch    44 | loss: 133.9226266CurrentTrain: epoch  6, batch    45 | loss: 124.9378311CurrentTrain: epoch  6, batch    46 | loss: 132.1062575CurrentTrain: epoch  6, batch    47 | loss: 100.9414381CurrentTrain: epoch  6, batch    48 | loss: 122.5186381CurrentTrain: epoch  6, batch    49 | loss: 66.8236625CurrentTrain: epoch  6, batch    50 | loss: 66.0469008CurrentTrain: epoch  6, batch    51 | loss: 130.4568359CurrentTrain: epoch  6, batch    52 | loss: 99.4797226CurrentTrain: epoch  6, batch    53 | loss: 74.6244989CurrentTrain: epoch  6, batch    54 | loss: 131.0943444CurrentTrain: epoch  6, batch    55 | loss: 94.9271427CurrentTrain: epoch  6, batch    56 | loss: 67.5732414CurrentTrain: epoch  6, batch    57 | loss: 176.3307436CurrentTrain: epoch  6, batch    58 | loss: 105.8187841CurrentTrain: epoch  6, batch    59 | loss: 80.0486108CurrentTrain: epoch  6, batch    60 | loss: 96.0252368CurrentTrain: epoch  6, batch    61 | loss: 79.0891322CurrentTrain: epoch  6, batch    62 | loss: 82.8239238CurrentTrain: epoch  6, batch    63 | loss: 98.4593208CurrentTrain: epoch  6, batch    64 | loss: 104.3341095CurrentTrain: epoch  6, batch    65 | loss: 75.8671406CurrentTrain: epoch  6, batch    66 | loss: 95.2561993CurrentTrain: epoch  6, batch    67 | loss: 131.2171175CurrentTrain: epoch  6, batch    68 | loss: 98.8649740CurrentTrain: epoch  6, batch    69 | loss: 124.6533693CurrentTrain: epoch  6, batch    70 | loss: 97.9375130CurrentTrain: epoch  6, batch    71 | loss: 77.1638288CurrentTrain: epoch  6, batch    72 | loss: 176.7745799CurrentTrain: epoch  6, batch    73 | loss: 78.6485197CurrentTrain: epoch  6, batch    74 | loss: 134.9611946CurrentTrain: epoch  6, batch    75 | loss: 80.5362462CurrentTrain: epoch  6, batch    76 | loss: 67.3401933CurrentTrain: epoch  6, batch    77 | loss: 79.6039238CurrentTrain: epoch  6, batch    78 | loss: 97.3697205CurrentTrain: epoch  6, batch    79 | loss: 76.6205568CurrentTrain: epoch  6, batch    80 | loss: 64.4308161CurrentTrain: epoch  6, batch    81 | loss: 127.8855416CurrentTrain: epoch  6, batch    82 | loss: 82.4965836CurrentTrain: epoch  6, batch    83 | loss: 81.4524651CurrentTrain: epoch  6, batch    84 | loss: 98.7868395CurrentTrain: epoch  6, batch    85 | loss: 68.7482080CurrentTrain: epoch  6, batch    86 | loss: 173.2661195CurrentTrain: epoch  6, batch    87 | loss: 121.6338795CurrentTrain: epoch  6, batch    88 | loss: 93.0773367CurrentTrain: epoch  6, batch    89 | loss: 79.6169181CurrentTrain: epoch  6, batch    90 | loss: 79.8258575CurrentTrain: epoch  6, batch    91 | loss: 122.2171427CurrentTrain: epoch  6, batch    92 | loss: 174.5348631CurrentTrain: epoch  6, batch    93 | loss: 100.3160524CurrentTrain: epoch  6, batch    94 | loss: 126.2954655CurrentTrain: epoch  6, batch    95 | loss: 63.3258379CurrentTrain: epoch  7, batch     0 | loss: 77.4059769CurrentTrain: epoch  7, batch     1 | loss: 77.8283589CurrentTrain: epoch  7, batch     2 | loss: 100.4472139CurrentTrain: epoch  7, batch     3 | loss: 73.5589776CurrentTrain: epoch  7, batch     4 | loss: 173.1759591CurrentTrain: epoch  7, batch     5 | loss: 77.7743654CurrentTrain: epoch  7, batch     6 | loss: 96.7776813CurrentTrain: epoch  7, batch     7 | loss: 98.8782886CurrentTrain: epoch  7, batch     8 | loss: 95.7728079CurrentTrain: epoch  7, batch     9 | loss: 123.7713695CurrentTrain: epoch  7, batch    10 | loss: 131.4810486CurrentTrain: epoch  7, batch    11 | loss: 170.2870963CurrentTrain: epoch  7, batch    12 | loss: 120.1788172CurrentTrain: epoch  7, batch    13 | loss: 172.9860785CurrentTrain: epoch  7, batch    14 | loss: 124.5419378CurrentTrain: epoch  7, batch    15 | loss: 94.9280773CurrentTrain: epoch  7, batch    16 | loss: 95.9829741CurrentTrain: epoch  7, batch    17 | loss: 92.4768965CurrentTrain: epoch  7, batch    18 | loss: 97.3767545CurrentTrain: epoch  7, batch    19 | loss: 79.3250694CurrentTrain: epoch  7, batch    20 | loss: 74.6738371CurrentTrain: epoch  7, batch    21 | loss: 168.0836394CurrentTrain: epoch  7, batch    22 | loss: 99.2607753CurrentTrain: epoch  7, batch    23 | loss: 102.3913585CurrentTrain: epoch  7, batch    24 | loss: 98.4428569CurrentTrain: epoch  7, batch    25 | loss: 77.0655830CurrentTrain: epoch  7, batch    26 | loss: 98.7720199CurrentTrain: epoch  7, batch    27 | loss: 96.7462779CurrentTrain: epoch  7, batch    28 | loss: 122.7337724CurrentTrain: epoch  7, batch    29 | loss: 95.1885002CurrentTrain: epoch  7, batch    30 | loss: 96.8961908CurrentTrain: epoch  7, batch    31 | loss: 125.5573661CurrentTrain: epoch  7, batch    32 | loss: 93.7320508CurrentTrain: epoch  7, batch    33 | loss: 124.6575707CurrentTrain: epoch  7, batch    34 | loss: 95.1783723CurrentTrain: epoch  7, batch    35 | loss: 96.8275463CurrentTrain: epoch  7, batch    36 | loss: 128.8396241CurrentTrain: epoch  7, batch    37 | loss: 102.5097350CurrentTrain: epoch  7, batch    38 | loss: 93.1493879CurrentTrain: epoch  7, batch    39 | loss: 81.6407513CurrentTrain: epoch  7, batch    40 | loss: 137.1263173CurrentTrain: epoch  7, batch    41 | loss: 127.5063318CurrentTrain: epoch  7, batch    42 | loss: 68.4433107CurrentTrain: epoch  7, batch    43 | loss: 81.0853188CurrentTrain: epoch  7, batch    44 | loss: 98.6461890CurrentTrain: epoch  7, batch    45 | loss: 100.8350535CurrentTrain: epoch  7, batch    46 | loss: 66.1536926CurrentTrain: epoch  7, batch    47 | loss: 65.4411502CurrentTrain: epoch  7, batch    48 | loss: 75.8723349CurrentTrain: epoch  7, batch    49 | loss: 131.7449965CurrentTrain: epoch  7, batch    50 | loss: 118.7098392CurrentTrain: epoch  7, batch    51 | loss: 128.8428927CurrentTrain: epoch  7, batch    52 | loss: 81.8876986CurrentTrain: epoch  7, batch    53 | loss: 177.1365614CurrentTrain: epoch  7, batch    54 | loss: 169.0281554CurrentTrain: epoch  7, batch    55 | loss: 79.9192953CurrentTrain: epoch  7, batch    56 | loss: 76.7194010CurrentTrain: epoch  7, batch    57 | loss: 173.0049577CurrentTrain: epoch  7, batch    58 | loss: 129.5510329CurrentTrain: epoch  7, batch    59 | loss: 175.6103439CurrentTrain: epoch  7, batch    60 | loss: 80.4464772CurrentTrain: epoch  7, batch    61 | loss: 99.5520383CurrentTrain: epoch  7, batch    62 | loss: 126.3349114CurrentTrain: epoch  7, batch    63 | loss: 73.6449046CurrentTrain: epoch  7, batch    64 | loss: 78.6652865CurrentTrain: epoch  7, batch    65 | loss: 123.6556053CurrentTrain: epoch  7, batch    66 | loss: 62.6804244CurrentTrain: epoch  7, batch    67 | loss: 126.7615407CurrentTrain: epoch  7, batch    68 | loss: 97.4307897CurrentTrain: epoch  7, batch    69 | loss: 129.8558173CurrentTrain: epoch  7, batch    70 | loss: 100.2788784CurrentTrain: epoch  7, batch    71 | loss: 77.3197374CurrentTrain: epoch  7, batch    72 | loss: 80.3718648CurrentTrain: epoch  7, batch    73 | loss: 123.9576378CurrentTrain: epoch  7, batch    74 | loss: 127.4174665CurrentTrain: epoch  7, batch    75 | loss: 119.6846699CurrentTrain: epoch  7, batch    76 | loss: 78.6268929CurrentTrain: epoch  7, batch    77 | loss: 98.9891496CurrentTrain: epoch  7, batch    78 | loss: 98.5564925CurrentTrain: epoch  7, batch    79 | loss: 172.8430178CurrentTrain: epoch  7, batch    80 | loss: 79.4451707CurrentTrain: epoch  7, batch    81 | loss: 167.6344424CurrentTrain: epoch  7, batch    82 | loss: 96.8404817CurrentTrain: epoch  7, batch    83 | loss: 59.9505706CurrentTrain: epoch  7, batch    84 | loss: 133.4820575CurrentTrain: epoch  7, batch    85 | loss: 127.0004047CurrentTrain: epoch  7, batch    86 | loss: 124.9060806CurrentTrain: epoch  7, batch    87 | loss: 126.2246930CurrentTrain: epoch  7, batch    88 | loss: 66.9946887CurrentTrain: epoch  7, batch    89 | loss: 79.0109400CurrentTrain: epoch  7, batch    90 | loss: 123.0563798CurrentTrain: epoch  7, batch    91 | loss: 81.4229236CurrentTrain: epoch  7, batch    92 | loss: 167.9784522CurrentTrain: epoch  7, batch    93 | loss: 94.4873557CurrentTrain: epoch  7, batch    94 | loss: 62.0884911CurrentTrain: epoch  7, batch    95 | loss: 81.7168371CurrentTrain: epoch  8, batch     0 | loss: 129.0814723CurrentTrain: epoch  8, batch     1 | loss: 170.5897809CurrentTrain: epoch  8, batch     2 | loss: 172.8494349CurrentTrain: epoch  8, batch     3 | loss: 122.8175306CurrentTrain: epoch  8, batch     4 | loss: 176.0182796CurrentTrain: epoch  8, batch     5 | loss: 96.8029652CurrentTrain: epoch  8, batch     6 | loss: 62.0955337CurrentTrain: epoch  8, batch     7 | loss: 168.9925639CurrentTrain: epoch  8, batch     8 | loss: 74.0170218CurrentTrain: epoch  8, batch     9 | loss: 97.7279977CurrentTrain: epoch  8, batch    10 | loss: 98.8543041CurrentTrain: epoch  8, batch    11 | loss: 124.5197799CurrentTrain: epoch  8, batch    12 | loss: 171.4620235CurrentTrain: epoch  8, batch    13 | loss: 96.2289029CurrentTrain: epoch  8, batch    14 | loss: 173.8297934CurrentTrain: epoch  8, batch    15 | loss: 127.0835928CurrentTrain: epoch  8, batch    16 | loss: 130.6474986CurrentTrain: epoch  8, batch    17 | loss: 126.3609500CurrentTrain: epoch  8, batch    18 | loss: 65.2217899CurrentTrain: epoch  8, batch    19 | loss: 97.2235433CurrentTrain: epoch  8, batch    20 | loss: 95.1736638CurrentTrain: epoch  8, batch    21 | loss: 124.9446745CurrentTrain: epoch  8, batch    22 | loss: 94.8400434CurrentTrain: epoch  8, batch    23 | loss: 78.1714758CurrentTrain: epoch  8, batch    24 | loss: 98.0235842CurrentTrain: epoch  8, batch    25 | loss: 126.8990000CurrentTrain: epoch  8, batch    26 | loss: 100.4127840CurrentTrain: epoch  8, batch    27 | loss: 128.4995016CurrentTrain: epoch  8, batch    28 | loss: 98.8544681CurrentTrain: epoch  8, batch    29 | loss: 166.8287771CurrentTrain: epoch  8, batch    30 | loss: 91.3203280CurrentTrain: epoch  8, batch    31 | loss: 96.7291053CurrentTrain: epoch  8, batch    32 | loss: 119.9123256CurrentTrain: epoch  8, batch    33 | loss: 96.4882908CurrentTrain: epoch  8, batch    34 | loss: 91.0342196CurrentTrain: epoch  8, batch    35 | loss: 100.8291120CurrentTrain: epoch  8, batch    36 | loss: 78.8236695CurrentTrain: epoch  8, batch    37 | loss: 66.8739793CurrentTrain: epoch  8, batch    38 | loss: 97.0797388CurrentTrain: epoch  8, batch    39 | loss: 100.4230340CurrentTrain: epoch  8, batch    40 | loss: 128.8831451CurrentTrain: epoch  8, batch    41 | loss: 98.4838252CurrentTrain: epoch  8, batch    42 | loss: 123.8755526CurrentTrain: epoch  8, batch    43 | loss: 97.0021699CurrentTrain: epoch  8, batch    44 | loss: 96.7256980CurrentTrain: epoch  8, batch    45 | loss: 61.9034831CurrentTrain: epoch  8, batch    46 | loss: 76.0893384CurrentTrain: epoch  8, batch    47 | loss: 176.2229590CurrentTrain: epoch  8, batch    48 | loss: 77.0904512CurrentTrain: epoch  8, batch    49 | loss: 81.4448453CurrentTrain: epoch  8, batch    50 | loss: 64.7906880CurrentTrain: epoch  8, batch    51 | loss: 99.5947487CurrentTrain: epoch  8, batch    52 | loss: 126.5177658CurrentTrain: epoch  8, batch    53 | loss: 125.6119912CurrentTrain: epoch  8, batch    54 | loss: 172.9286763CurrentTrain: epoch  8, batch    55 | loss: 77.3183261CurrentTrain: epoch  8, batch    56 | loss: 121.5546392CurrentTrain: epoch  8, batch    57 | loss: 78.9835464CurrentTrain: epoch  8, batch    58 | loss: 98.8513335CurrentTrain: epoch  8, batch    59 | loss: 83.2411947CurrentTrain: epoch  8, batch    60 | loss: 95.3757315CurrentTrain: epoch  8, batch    61 | loss: 98.0068085CurrentTrain: epoch  8, batch    62 | loss: 91.6186796CurrentTrain: epoch  8, batch    63 | loss: 80.4159468CurrentTrain: epoch  8, batch    64 | loss: 122.8471846CurrentTrain: epoch  8, batch    65 | loss: 95.7080745CurrentTrain: epoch  8, batch    66 | loss: 64.9677785CurrentTrain: epoch  8, batch    67 | loss: 99.2153611CurrentTrain: epoch  8, batch    68 | loss: 66.8786924CurrentTrain: epoch  8, batch    69 | loss: 173.5878939CurrentTrain: epoch  8, batch    70 | loss: 123.6976223CurrentTrain: epoch  8, batch    71 | loss: 128.8535129CurrentTrain: epoch  8, batch    72 | loss: 82.7784132CurrentTrain: epoch  8, batch    73 | loss: 120.0479789CurrentTrain: epoch  8, batch    74 | loss: 123.0186484CurrentTrain: epoch  8, batch    75 | loss: 96.9529581CurrentTrain: epoch  8, batch    76 | loss: 77.5502697CurrentTrain: epoch  8, batch    77 | loss: 176.2667569CurrentTrain: epoch  8, batch    78 | loss: 81.8085998CurrentTrain: epoch  8, batch    79 | loss: 97.3088466CurrentTrain: epoch  8, batch    80 | loss: 66.2826033CurrentTrain: epoch  8, batch    81 | loss: 170.2556981CurrentTrain: epoch  8, batch    82 | loss: 128.6176331CurrentTrain: epoch  8, batch    83 | loss: 74.5256311CurrentTrain: epoch  8, batch    84 | loss: 78.5894756CurrentTrain: epoch  8, batch    85 | loss: 90.6116078CurrentTrain: epoch  8, batch    86 | loss: 74.7334490CurrentTrain: epoch  8, batch    87 | loss: 96.5773160CurrentTrain: epoch  8, batch    88 | loss: 96.1146393CurrentTrain: epoch  8, batch    89 | loss: 127.7783251CurrentTrain: epoch  8, batch    90 | loss: 126.2742916CurrentTrain: epoch  8, batch    91 | loss: 96.3871180CurrentTrain: epoch  8, batch    92 | loss: 77.5581930CurrentTrain: epoch  8, batch    93 | loss: 99.2632199CurrentTrain: epoch  8, batch    94 | loss: 93.0363067CurrentTrain: epoch  8, batch    95 | loss: 82.2480551CurrentTrain: epoch  9, batch     0 | loss: 121.8330348CurrentTrain: epoch  9, batch     1 | loss: 120.0357091CurrentTrain: epoch  9, batch     2 | loss: 79.6475609CurrentTrain: epoch  9, batch     3 | loss: 66.0301767CurrentTrain: epoch  9, batch     4 | loss: 128.8344766CurrentTrain: epoch  9, batch     5 | loss: 64.4054426CurrentTrain: epoch  9, batch     6 | loss: 100.4738745CurrentTrain: epoch  9, batch     7 | loss: 80.7073607CurrentTrain: epoch  9, batch     8 | loss: 129.3563636CurrentTrain: epoch  9, batch     9 | loss: 273.8679458CurrentTrain: epoch  9, batch    10 | loss: 95.9432065CurrentTrain: epoch  9, batch    11 | loss: 262.4582427CurrentTrain: epoch  9, batch    12 | loss: 98.2025622CurrentTrain: epoch  9, batch    13 | loss: 95.9822799CurrentTrain: epoch  9, batch    14 | loss: 94.6188995CurrentTrain: epoch  9, batch    15 | loss: 98.6133379CurrentTrain: epoch  9, batch    16 | loss: 124.1485135CurrentTrain: epoch  9, batch    17 | loss: 76.3279096CurrentTrain: epoch  9, batch    18 | loss: 98.5101585CurrentTrain: epoch  9, batch    19 | loss: 76.1400380CurrentTrain: epoch  9, batch    20 | loss: 75.8263961CurrentTrain: epoch  9, batch    21 | loss: 96.9306334CurrentTrain: epoch  9, batch    22 | loss: 73.5072781CurrentTrain: epoch  9, batch    23 | loss: 65.1311925CurrentTrain: epoch  9, batch    24 | loss: 72.6316046CurrentTrain: epoch  9, batch    25 | loss: 125.0454801CurrentTrain: epoch  9, batch    26 | loss: 92.2791522CurrentTrain: epoch  9, batch    27 | loss: 76.9117283CurrentTrain: epoch  9, batch    28 | loss: 93.6778366CurrentTrain: epoch  9, batch    29 | loss: 96.1207960CurrentTrain: epoch  9, batch    30 | loss: 124.8683183CurrentTrain: epoch  9, batch    31 | loss: 100.6067606CurrentTrain: epoch  9, batch    32 | loss: 75.6902893CurrentTrain: epoch  9, batch    33 | loss: 77.1450618CurrentTrain: epoch  9, batch    34 | loss: 74.3420824CurrentTrain: epoch  9, batch    35 | loss: 81.0788815CurrentTrain: epoch  9, batch    36 | loss: 76.4678438CurrentTrain: epoch  9, batch    37 | loss: 117.8813510CurrentTrain: epoch  9, batch    38 | loss: 272.7038397CurrentTrain: epoch  9, batch    39 | loss: 100.4748578CurrentTrain: epoch  9, batch    40 | loss: 126.4005917CurrentTrain: epoch  9, batch    41 | loss: 98.7562227CurrentTrain: epoch  9, batch    42 | loss: 65.6992665CurrentTrain: epoch  9, batch    43 | loss: 126.2198488CurrentTrain: epoch  9, batch    44 | loss: 96.7214059CurrentTrain: epoch  9, batch    45 | loss: 79.7846879CurrentTrain: epoch  9, batch    46 | loss: 62.5925602CurrentTrain: epoch  9, batch    47 | loss: 174.1608822CurrentTrain: epoch  9, batch    48 | loss: 99.2664587CurrentTrain: epoch  9, batch    49 | loss: 172.7795095CurrentTrain: epoch  9, batch    50 | loss: 94.4710700CurrentTrain: epoch  9, batch    51 | loss: 77.7667264CurrentTrain: epoch  9, batch    52 | loss: 124.8983520CurrentTrain: epoch  9, batch    53 | loss: 121.8712672CurrentTrain: epoch  9, batch    54 | loss: 119.6335186CurrentTrain: epoch  9, batch    55 | loss: 79.7407715CurrentTrain: epoch  9, batch    56 | loss: 81.3643743CurrentTrain: epoch  9, batch    57 | loss: 176.2877523CurrentTrain: epoch  9, batch    58 | loss: 176.3548659CurrentTrain: epoch  9, batch    59 | loss: 63.0096367CurrentTrain: epoch  9, batch    60 | loss: 76.6382701CurrentTrain: epoch  9, batch    61 | loss: 170.1802507CurrentTrain: epoch  9, batch    62 | loss: 126.9046142CurrentTrain: epoch  9, batch    63 | loss: 100.8176147CurrentTrain: epoch  9, batch    64 | loss: 84.4794816CurrentTrain: epoch  9, batch    65 | loss: 128.0831669CurrentTrain: epoch  9, batch    66 | loss: 67.3183210CurrentTrain: epoch  9, batch    67 | loss: 82.3015480CurrentTrain: epoch  9, batch    68 | loss: 98.6787811CurrentTrain: epoch  9, batch    69 | loss: 170.5311459CurrentTrain: epoch  9, batch    70 | loss: 80.6786349CurrentTrain: epoch  9, batch    71 | loss: 96.6664858CurrentTrain: epoch  9, batch    72 | loss: 78.0363895CurrentTrain: epoch  9, batch    73 | loss: 126.7659553CurrentTrain: epoch  9, batch    74 | loss: 72.8098492CurrentTrain: epoch  9, batch    75 | loss: 81.6024360CurrentTrain: epoch  9, batch    76 | loss: 100.8044875CurrentTrain: epoch  9, batch    77 | loss: 125.2054464CurrentTrain: epoch  9, batch    78 | loss: 76.7397157CurrentTrain: epoch  9, batch    79 | loss: 100.1602033CurrentTrain: epoch  9, batch    80 | loss: 100.3298901CurrentTrain: epoch  9, batch    81 | loss: 78.2102230CurrentTrain: epoch  9, batch    82 | loss: 76.4757349CurrentTrain: epoch  9, batch    83 | loss: 77.6446239CurrentTrain: epoch  9, batch    84 | loss: 95.4850353CurrentTrain: epoch  9, batch    85 | loss: 100.5661790CurrentTrain: epoch  9, batch    86 | loss: 128.5922902CurrentTrain: epoch  9, batch    87 | loss: 95.2405933CurrentTrain: epoch  9, batch    88 | loss: 272.5315587CurrentTrain: epoch  9, batch    89 | loss: 94.9078513CurrentTrain: epoch  9, batch    90 | loss: 93.8725047CurrentTrain: epoch  9, batch    91 | loss: 101.8076469CurrentTrain: epoch  9, batch    92 | loss: 96.4841727CurrentTrain: epoch  9, batch    93 | loss: 122.2013400CurrentTrain: epoch  9, batch    94 | loss: 74.8400767CurrentTrain: epoch  9, batch    95 | loss: 78.4868508

F1 score per class: {32: 0.5962732919254659, 6: 0.9021739130434783, 19: 0.4166666666666667, 24: 0.75, 26: 0.9247311827956989, 29: 0.89}
Micro-average F1 score: 0.808870116156283
Weighted-average F1 score: 0.8209374387495004
F1 score per class: {32: 0.6881720430107527, 6: 0.918918918918919, 19: 0.42424242424242425, 24: 0.7384615384615385, 26: 0.9746192893401016, 29: 0.8704663212435233}
Micro-average F1 score: 0.8250758341759353
Weighted-average F1 score: 0.8265389071468215
F1 score per class: {32: 0.6810810810810811, 6: 0.9139784946236559, 19: 0.4375, 24: 0.7384615384615385, 26: 0.9795918367346939, 29: 0.8762886597938144}
Micro-average F1 score: 0.8259109311740891
Weighted-average F1 score: 0.8280528487883438

F1 score per class: {32: 0.5962732919254659, 6: 0.9021739130434783, 19: 0.4166666666666667, 24: 0.75, 26: 0.9247311827956989, 29: 0.89}
Micro-average F1 score: 0.808870116156283
Weighted-average F1 score: 0.8209374387495004
F1 score per class: {32: 0.6881720430107527, 6: 0.918918918918919, 19: 0.42424242424242425, 24: 0.7384615384615385, 26: 0.9746192893401016, 29: 0.8704663212435233}
Micro-average F1 score: 0.8250758341759353
Weighted-average F1 score: 0.8265389071468215
F1 score per class: {32: 0.6810810810810811, 6: 0.9139784946236559, 19: 0.4375, 24: 0.7384615384615385, 26: 0.9795918367346939, 29: 0.8762886597938144}
Micro-average F1 score: 0.8259109311740891
Weighted-average F1 score: 0.8280528487883438

F1 score per class: {32: 0.44036697247706424, 6: 0.6561264822134387, 19: 0.19607843137254902, 24: 0.6824644549763034, 26: 0.91005291005291, 29: 0.7841409691629956}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6579325029271361
F1 score per class: {32: 0.46886446886446886, 6: 0.6273062730627307, 19: 0.112, 24: 0.631578947368421, 26: 0.927536231884058, 29: 0.7533632286995515}
Micro-average F1 score: 0.6149208741522231
Weighted-average F1 score: 0.584819635367215
F1 score per class: {32: 0.46494464944649444, 6: 0.625, 19: 0.12727272727272726, 24: 0.631578947368421, 26: 0.9365853658536586, 29: 0.74235807860262}
Micro-average F1 score: 0.620532319391635
Weighted-average F1 score: 0.5943066333525363

F1 score per class: {32: 0.44036697247706424, 6: 0.6561264822134387, 19: 0.19607843137254902, 24: 0.6824644549763034, 26: 0.91005291005291, 29: 0.7841409691629956}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6579325029271361
F1 score per class: {32: 0.46886446886446886, 6: 0.6273062730627307, 19: 0.112, 24: 0.631578947368421, 26: 0.927536231884058, 29: 0.7533632286995515}
Micro-average F1 score: 0.6149208741522231
Weighted-average F1 score: 0.584819635367215
F1 score per class: {32: 0.46494464944649444, 6: 0.625, 19: 0.12727272727272726, 24: 0.631578947368421, 26: 0.9365853658536586, 29: 0.74235807860262}
Micro-average F1 score: 0.620532319391635
Weighted-average F1 score: 0.5943066333525363
cur_acc_wo_na:  ['0.8089']
his_acc_wo_na:  ['0.8089']
cur_acc des_wo_na:  ['0.8251']
his_acc des_wo_na:  ['0.8251']
cur_acc rrf_wo_na:  ['0.8259']
his_acc rrf_wo_na:  ['0.8259']
cur_acc_w_na:  ['0.6667']
his_acc_w_na:  ['0.6667']
cur_acc des_w_na:  ['0.6149']
his_acc des_w_na:  ['0.6149']
cur_acc rrf_w_na:  ['0.6205']
his_acc rrf_w_na:  ['0.6205']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 296.8382811CurrentTrain: epoch  0, batch     1 | loss: 179.9789901CurrentTrain: epoch  0, batch     2 | loss: 114.8597179CurrentTrain: epoch  0, batch     3 | loss: 116.0605815CurrentTrain: epoch  0, batch     4 | loss: 89.4224729CurrentTrain: epoch  1, batch     0 | loss: 92.1172742CurrentTrain: epoch  1, batch     1 | loss: 134.6921797CurrentTrain: epoch  1, batch     2 | loss: 136.7429106CurrentTrain: epoch  1, batch     3 | loss: 179.8793490CurrentTrain: epoch  1, batch     4 | loss: 69.9982258CurrentTrain: epoch  2, batch     0 | loss: 87.8741057CurrentTrain: epoch  2, batch     1 | loss: 268.5654208CurrentTrain: epoch  2, batch     2 | loss: 91.6048488CurrentTrain: epoch  2, batch     3 | loss: 107.2567621CurrentTrain: epoch  2, batch     4 | loss: 176.8957552CurrentTrain: epoch  3, batch     0 | loss: 132.0871444CurrentTrain: epoch  3, batch     1 | loss: 86.0597542CurrentTrain: epoch  3, batch     2 | loss: 104.4086632CurrentTrain: epoch  3, batch     3 | loss: 274.2953041CurrentTrain: epoch  3, batch     4 | loss: 64.3121022CurrentTrain: epoch  4, batch     0 | loss: 179.3291681CurrentTrain: epoch  4, batch     1 | loss: 175.7313477CurrentTrain: epoch  4, batch     2 | loss: 80.3247110CurrentTrain: epoch  4, batch     3 | loss: 135.1171452CurrentTrain: epoch  4, batch     4 | loss: 63.3593538CurrentTrain: epoch  5, batch     0 | loss: 100.6746898CurrentTrain: epoch  5, batch     1 | loss: 131.6618903CurrentTrain: epoch  5, batch     2 | loss: 80.6049373CurrentTrain: epoch  5, batch     3 | loss: 178.1537308CurrentTrain: epoch  5, batch     4 | loss: 81.3041541CurrentTrain: epoch  6, batch     0 | loss: 177.7273497CurrentTrain: epoch  6, batch     1 | loss: 125.3375424CurrentTrain: epoch  6, batch     2 | loss: 125.6067775CurrentTrain: epoch  6, batch     3 | loss: 102.2085641CurrentTrain: epoch  6, batch     4 | loss: 58.1156719CurrentTrain: epoch  7, batch     0 | loss: 178.4869726CurrentTrain: epoch  7, batch     1 | loss: 98.8013149CurrentTrain: epoch  7, batch     2 | loss: 81.8648012CurrentTrain: epoch  7, batch     3 | loss: 79.0896665CurrentTrain: epoch  7, batch     4 | loss: 111.8197649CurrentTrain: epoch  8, batch     0 | loss: 100.3024143CurrentTrain: epoch  8, batch     1 | loss: 102.1613896CurrentTrain: epoch  8, batch     2 | loss: 91.5745720CurrentTrain: epoch  8, batch     3 | loss: 176.8238819CurrentTrain: epoch  8, batch     4 | loss: 79.3984682CurrentTrain: epoch  9, batch     0 | loss: 127.8767896CurrentTrain: epoch  9, batch     1 | loss: 174.9579831CurrentTrain: epoch  9, batch     2 | loss: 80.6436465CurrentTrain: epoch  9, batch     3 | loss: 77.6792843CurrentTrain: epoch  9, batch     4 | loss: 108.2592195
MemoryTrain:  epoch  0, batch     0 | loss: 1.1709156MemoryTrain:  epoch  1, batch     0 | loss: 1.0043953MemoryTrain:  epoch  2, batch     0 | loss: 0.7738361MemoryTrain:  epoch  3, batch     0 | loss: 0.4933096MemoryTrain:  epoch  4, batch     0 | loss: 0.4283065MemoryTrain:  epoch  5, batch     0 | loss: 0.3719724MemoryTrain:  epoch  6, batch     0 | loss: 0.3089896MemoryTrain:  epoch  7, batch     0 | loss: 0.1785558MemoryTrain:  epoch  8, batch     0 | loss: 0.1225703MemoryTrain:  epoch  9, batch     0 | loss: 0.1219505

F1 score per class: {5: 0.9690721649484536, 6: 0.0, 10: 0.7261146496815286, 16: 0.8301886792452831, 17: 0.0, 18: 0.5769230769230769}
Micro-average F1 score: 0.791578947368421
Weighted-average F1 score: 0.8051733885099973
F1 score per class: {5: 0.9949748743718593, 6: 0.0, 10: 0.8255813953488372, 16: 0.9310344827586207, 17: 0.18181818181818182, 18: 0.8787878787878788}
Micro-average F1 score: 0.8764478764478765
Weighted-average F1 score: 0.8671976173291593
F1 score per class: {5: 0.9847715736040609, 6: 0.0, 10: 0.8457142857142858, 16: 0.9310344827586207, 17: 0.18181818181818182, 18: 0.7241379310344828}
Micro-average F1 score: 0.8661417322834646
Weighted-average F1 score: 0.8646160021584792

F1 score per class: {32: 0.9641025641025641, 5: 0.5333333333333333, 6: 0.6951219512195121, 10: 0.8301886792452831, 16: 0.0, 17: 0.5660377358490566, 18: 0.907103825136612, 19: 0.4166666666666667, 24: 0.7446808510638298, 26: 0.9361702127659575, 29: 0.9045226130653267}
Micro-average F1 score: 0.7921896792189679
Weighted-average F1 score: 0.8033553255117388
F1 score per class: {32: 0.9753694581280788, 5: 0.6137566137566137, 6: 0.8208092485549133, 10: 0.9152542372881356, 16: 0.06451612903225806, 17: 0.8529411764705882, 18: 0.9417989417989417, 19: 0.6470588235294118, 24: 0.7419354838709677, 26: 0.9587628865979382, 29: 0.9015544041450777}
Micro-average F1 score: 0.8347597103357473
Weighted-average F1 score: 0.8294581264577535
F1 score per class: {32: 0.97, 5: 0.5888888888888889, 6: 0.8409090909090909, 10: 0.9152542372881356, 16: 0.058823529411764705, 17: 0.7, 18: 0.9368421052631579, 19: 0.5625, 24: 0.7486631016042781, 26: 0.9479166666666666, 29: 0.9015544041450777}
Micro-average F1 score: 0.8236859614105123
Weighted-average F1 score: 0.8187736747093569

F1 score per class: {32: 0.9170731707317074, 5: 0.0, 6: 0.49137931034482757, 10: 0.5057471264367817, 16: 0.0, 17: 0.3125, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5222222222222223
Weighted-average F1 score: 0.46941404977306994
F1 score per class: {32: 0.5577464788732395, 5: 0.0, 6: 0.5378787878787878, 10: 0.47368421052631576, 16: 0.08, 17: 0.29743589743589743, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.398595258999122
Weighted-average F1 score: 0.36906352120959396
F1 score per class: {32: 0.6576271186440678, 5: 0.0, 6: 0.5323741007194245, 10: 0.43902439024390244, 16: 0.06060606060606061, 17: 0.27631578947368424, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.43137254901960786
Weighted-average F1 score: 0.3992742037639151

F1 score per class: {32: 0.912621359223301, 5: 0.37446808510638296, 6: 0.38127090301003347, 10: 0.4583333333333333, 16: 0.0, 17: 0.2777777777777778, 18: 0.6036363636363636, 19: 0.16393442622950818, 24: 0.639269406392694, 26: 0.8712871287128713, 29: 0.7407407407407407}
Micro-average F1 score: 0.5743174924165824
Weighted-average F1 score: 0.5546385167802808
F1 score per class: {32: 0.5103092783505154, 5: 0.3682539682539683, 6: 0.3858695652173913, 10: 0.391304347826087, 16: 0.02857142857142857, 17: 0.2396694214876033, 18: 0.5114942528735632, 19: 0.11578947368421053, 24: 0.6, 26: 0.8266666666666667, 29: 0.6641221374045801}
Micro-average F1 score: 0.45677233429394815
Weighted-average F1 score: 0.43308787119578657
F1 score per class: {32: 0.6381578947368421, 5: 0.38405797101449274, 6: 0.38341968911917096, 10: 0.3698630136986301, 16: 0.02127659574468085, 17: 0.22950819672131148, 18: 0.489010989010989, 19: 0.16666666666666666, 24: 0.6167400881057269, 26: 0.8666666666666667, 29: 0.6718146718146718}
Micro-average F1 score: 0.48416112631990615
Weighted-average F1 score: 0.4595768689176372
cur_acc_wo_na:  ['0.8089', '0.7916']
his_acc_wo_na:  ['0.8089', '0.7922']
cur_acc des_wo_na:  ['0.8251', '0.8764']
his_acc des_wo_na:  ['0.8251', '0.8348']
cur_acc rrf_wo_na:  ['0.8259', '0.8661']
his_acc rrf_wo_na:  ['0.8259', '0.8237']
cur_acc_w_na:  ['0.6667', '0.5222']
his_acc_w_na:  ['0.6667', '0.5743']
cur_acc des_w_na:  ['0.6149', '0.3986']
his_acc des_w_na:  ['0.6149', '0.4568']
cur_acc rrf_w_na:  ['0.6205', '0.4314']
his_acc rrf_w_na:  ['0.6205', '0.4842']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 97.2141099CurrentTrain: epoch  0, batch     1 | loss: 106.5257103CurrentTrain: epoch  0, batch     2 | loss: 116.6901448CurrentTrain: epoch  0, batch     3 | loss: 16.6028752CurrentTrain: epoch  1, batch     0 | loss: 90.2212905CurrentTrain: epoch  1, batch     1 | loss: 109.4710736CurrentTrain: epoch  1, batch     2 | loss: 110.6940247CurrentTrain: epoch  1, batch     3 | loss: 24.3553895CurrentTrain: epoch  2, batch     0 | loss: 103.6149751CurrentTrain: epoch  2, batch     1 | loss: 89.2571120CurrentTrain: epoch  2, batch     2 | loss: 82.3743223CurrentTrain: epoch  2, batch     3 | loss: 41.4094229CurrentTrain: epoch  3, batch     0 | loss: 105.7005967CurrentTrain: epoch  3, batch     1 | loss: 98.1400076CurrentTrain: epoch  3, batch     2 | loss: 128.4417633CurrentTrain: epoch  3, batch     3 | loss: 17.3334835CurrentTrain: epoch  4, batch     0 | loss: 99.6257481CurrentTrain: epoch  4, batch     1 | loss: 133.0625051CurrentTrain: epoch  4, batch     2 | loss: 78.7658250CurrentTrain: epoch  4, batch     3 | loss: 4.8568567CurrentTrain: epoch  5, batch     0 | loss: 81.8321910CurrentTrain: epoch  5, batch     1 | loss: 124.6391547CurrentTrain: epoch  5, batch     2 | loss: 96.3333344CurrentTrain: epoch  5, batch     3 | loss: 9.0750037CurrentTrain: epoch  6, batch     0 | loss: 124.2557034CurrentTrain: epoch  6, batch     1 | loss: 92.2566887CurrentTrain: epoch  6, batch     2 | loss: 83.8006644CurrentTrain: epoch  6, batch     3 | loss: 9.6921527CurrentTrain: epoch  7, batch     0 | loss: 96.4682812CurrentTrain: epoch  7, batch     1 | loss: 76.3159538CurrentTrain: epoch  7, batch     2 | loss: 97.9318532CurrentTrain: epoch  7, batch     3 | loss: 17.3559582CurrentTrain: epoch  8, batch     0 | loss: 79.3449530CurrentTrain: epoch  8, batch     1 | loss: 96.8822351CurrentTrain: epoch  8, batch     2 | loss: 95.2248247CurrentTrain: epoch  8, batch     3 | loss: 14.4709512CurrentTrain: epoch  9, batch     0 | loss: 73.0985227CurrentTrain: epoch  9, batch     1 | loss: 80.3343477CurrentTrain: epoch  9, batch     2 | loss: 99.5137608CurrentTrain: epoch  9, batch     3 | loss: 41.1362555
MemoryTrain:  epoch  0, batch     0 | loss: 1.0071394MemoryTrain:  epoch  1, batch     0 | loss: 0.7429166MemoryTrain:  epoch  2, batch     0 | loss: 0.6620129MemoryTrain:  epoch  3, batch     0 | loss: 0.4852191MemoryTrain:  epoch  4, batch     0 | loss: 0.4367469MemoryTrain:  epoch  5, batch     0 | loss: 0.2844002MemoryTrain:  epoch  6, batch     0 | loss: 0.2310392MemoryTrain:  epoch  7, batch     0 | loss: 0.2079987MemoryTrain:  epoch  8, batch     0 | loss: 0.1740003MemoryTrain:  epoch  9, batch     0 | loss: 0.1402371

F1 score per class: {7: 0.8888888888888888, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 26: 0.7272727272727273, 27: 0.0, 31: 0.5531914893617021}
Micro-average F1 score: 0.5753424657534246
Weighted-average F1 score: 0.46660754996174275
F1 score per class: {7: 0.8888888888888888, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.8, 27: 1.0, 31: 0.8376068376068376}
Micro-average F1 score: 0.8
Weighted-average F1 score: 0.7270999506293624
F1 score per class: {7: 0.8888888888888888, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.8, 27: 1.0, 31: 0.8376068376068376}
Micro-average F1 score: 0.8
Weighted-average F1 score: 0.7270999506293624

F1 score per class: {32: 0.9644670050761421, 5: 0.14545454545454545, 6: 0.08695652173913043, 7: 0.9803921568627451, 40: 0.6206896551724138, 10: 0.76, 9: 0.0, 16: 0.5769230769230769, 17: 0.6915887850467289, 18: 0.2, 19: 0.7283236994219653, 24: 0.5517241379310345, 26: 0.88268156424581, 27: 0.0, 29: 0.8969072164948454, 31: 0.5}
Micro-average F1 score: 0.6785934608266502
Weighted-average F1 score: 0.6902457315320765
F1 score per class: {32: 0.9519230769230769, 5: 0.2833333333333333, 6: 0.06837606837606838, 7: 0.9433962264150944, 40: 0.7051282051282052, 9: 0.9310344827586207, 10: 0.0, 16: 0.8615384615384616, 17: 0.774869109947644, 18: 0.30303030303030304, 19: 0.7582417582417582, 24: 0.625, 26: 0.9312169312169312, 27: 0.6666666666666666, 29: 0.8723404255319149, 31: 0.7153284671532847}
Micro-average F1 score: 0.7266475644699141
Weighted-average F1 score: 0.7073612444743811
F1 score per class: {32: 0.9383886255924171, 5: 0.2542372881355932, 6: 0.07920792079207921, 7: 0.9803921568627451, 40: 0.7710843373493976, 10: 0.9666666666666667, 9: 0.0, 16: 0.8064516129032258, 17: 0.7708333333333334, 18: 0.25806451612903225, 19: 0.7582417582417582, 24: 0.5882352941176471, 26: 0.9247311827956989, 27: 0.8, 29: 0.8631578947368421, 31: 0.7153284671532847}
Micro-average F1 score: 0.7334484743811168
Weighted-average F1 score: 0.7221629152489861

F1 score per class: {32: 0.0, 5: 0.0, 6: 0.8, 7: 0.9433962264150944, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 19: 0.5714285714285714, 26: 0.0, 27: 0.0, 31: 0.3851851851851852}
Micro-average F1 score: 0.4271186440677966
Weighted-average F1 score: 0.35276538172399735
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.8, 7: 0.5747126436781609, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.5128205128205128, 26: 0.2857142857142857, 27: 0.0, 31: 0.4279475982532751}
Micro-average F1 score: 0.38626609442060084
Weighted-average F1 score: 0.35431516859389167
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.8888888888888888, 7: 0.6578947368421053, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.46511627906976744, 26: 0.2857142857142857, 27: 0.0, 31: 0.4188034188034188}
Micro-average F1 score: 0.3956043956043956
Weighted-average F1 score: 0.36174111354748617

F1 score per class: {32: 0.7335907335907336, 5: 0.11594202898550725, 6: 0.049079754601226995, 7: 0.9259259259259259, 40: 0.38626609442060084, 10: 0.4634146341463415, 9: 0.0, 16: 0.26785714285714285, 17: 0.5156794425087108, 18: 0.13333333333333333, 19: 0.6528497409326425, 24: 0.20253164556962025, 26: 0.8272251308900523, 27: 0.0, 29: 0.7802690582959642, 31: 0.3132530120481928}
Micro-average F1 score: 0.492831541218638
Weighted-average F1 score: 0.47156422558059813
F1 score per class: {32: 0.31629392971246006, 5: 0.21518987341772153, 6: 0.029411764705882353, 7: 0.42016806722689076, 40: 0.3767123287671233, 9: 0.38571428571428573, 10: 0.0, 16: 0.21052631578947367, 17: 0.524822695035461, 18: 0.10309278350515463, 19: 0.6301369863013698, 24: 0.14705882352941177, 26: 0.8421052631578947, 27: 0.08163265306122448, 29: 0.6074074074074074, 31: 0.24378109452736318}
Micro-average F1 score: 0.354586129753915
Weighted-average F1 score: 0.32451012228262
F1 score per class: {32: 0.3198707592891761, 5: 0.19736842105263158, 6: 0.03375527426160337, 7: 0.5617977528089888, 40: 0.39628482972136225, 10: 0.42962962962962964, 9: 0.0, 16: 0.21929824561403508, 17: 0.5285714285714286, 18: 0.125, 19: 0.641860465116279, 24: 0.13245033112582782, 26: 0.8557213930348259, 27: 0.09523809523809523, 29: 0.6482213438735178, 31: 0.24438902743142144}
Micro-average F1 score: 0.37175372045520866
Weighted-average F1 score: 0.34086975373270323
cur_acc_wo_na:  ['0.8089', '0.7916', '0.5753']
his_acc_wo_na:  ['0.8089', '0.7922', '0.6786']
cur_acc des_wo_na:  ['0.8251', '0.8764', '0.8000']
his_acc des_wo_na:  ['0.8251', '0.8348', '0.7266']
cur_acc rrf_wo_na:  ['0.8259', '0.8661', '0.8000']
his_acc rrf_wo_na:  ['0.8259', '0.8237', '0.7334']
cur_acc_w_na:  ['0.6667', '0.5222', '0.4271']
his_acc_w_na:  ['0.6667', '0.5743', '0.4928']
cur_acc des_w_na:  ['0.6149', '0.3986', '0.3863']
his_acc des_w_na:  ['0.6149', '0.4568', '0.3546']
cur_acc rrf_w_na:  ['0.6205', '0.4314', '0.3956']
his_acc rrf_w_na:  ['0.6205', '0.4842', '0.3718']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 119.1195893CurrentTrain: epoch  0, batch     1 | loss: 120.3647765CurrentTrain: epoch  0, batch     2 | loss: 123.8173186CurrentTrain: epoch  0, batch     3 | loss: 99.9845839CurrentTrain: epoch  1, batch     0 | loss: 91.0666091CurrentTrain: epoch  1, batch     1 | loss: 92.9354653CurrentTrain: epoch  1, batch     2 | loss: 189.4525398CurrentTrain: epoch  1, batch     3 | loss: 88.6227149CurrentTrain: epoch  2, batch     0 | loss: 109.6673087CurrentTrain: epoch  2, batch     1 | loss: 104.3294939CurrentTrain: epoch  2, batch     2 | loss: 127.6724426CurrentTrain: epoch  2, batch     3 | loss: 89.1324865CurrentTrain: epoch  3, batch     0 | loss: 85.7871068CurrentTrain: epoch  3, batch     1 | loss: 101.8529738CurrentTrain: epoch  3, batch     2 | loss: 132.0495343CurrentTrain: epoch  3, batch     3 | loss: 92.2137748CurrentTrain: epoch  4, batch     0 | loss: 173.4724547CurrentTrain: epoch  4, batch     1 | loss: 106.5252935CurrentTrain: epoch  4, batch     2 | loss: 83.9247625CurrentTrain: epoch  4, batch     3 | loss: 101.0544850CurrentTrain: epoch  5, batch     0 | loss: 104.8362143CurrentTrain: epoch  5, batch     1 | loss: 101.7307706CurrentTrain: epoch  5, batch     2 | loss: 169.6076446CurrentTrain: epoch  5, batch     3 | loss: 67.2810249CurrentTrain: epoch  6, batch     0 | loss: 99.9987575CurrentTrain: epoch  6, batch     1 | loss: 94.4398346CurrentTrain: epoch  6, batch     2 | loss: 134.8005575CurrentTrain: epoch  6, batch     3 | loss: 80.7066964CurrentTrain: epoch  7, batch     0 | loss: 86.8580110CurrentTrain: epoch  7, batch     1 | loss: 98.7659388CurrentTrain: epoch  7, batch     2 | loss: 126.0187209CurrentTrain: epoch  7, batch     3 | loss: 76.7105177CurrentTrain: epoch  8, batch     0 | loss: 78.9165757CurrentTrain: epoch  8, batch     1 | loss: 124.3976504CurrentTrain: epoch  8, batch     2 | loss: 126.3540926CurrentTrain: epoch  8, batch     3 | loss: 102.2275404CurrentTrain: epoch  9, batch     0 | loss: 96.2856205CurrentTrain: epoch  9, batch     1 | loss: 129.6602714CurrentTrain: epoch  9, batch     2 | loss: 122.7097374CurrentTrain: epoch  9, batch     3 | loss: 78.4084077
MemoryTrain:  epoch  0, batch     0 | loss: 1.1257083MemoryTrain:  epoch  1, batch     0 | loss: 0.8756318MemoryTrain:  epoch  2, batch     0 | loss: 0.6528476MemoryTrain:  epoch  3, batch     0 | loss: 0.5937292MemoryTrain:  epoch  4, batch     0 | loss: 0.4440153MemoryTrain:  epoch  5, batch     0 | loss: 0.3239025MemoryTrain:  epoch  6, batch     0 | loss: 0.2721112MemoryTrain:  epoch  7, batch     0 | loss: 0.2464920MemoryTrain:  epoch  8, batch     0 | loss: 0.3401602MemoryTrain:  epoch  9, batch     0 | loss: 0.1914987

F1 score per class: {0: 0.9577464788732394, 32: 0.9528795811518325, 4: 0.5714285714285714, 13: 0.4878048780487805, 21: 0.7792207792207793, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0}
Micro-average F1 score: 0.8370927318295739
Weighted-average F1 score: 0.8305751478574034
F1 score per class: {0: 0.9577464788732394, 32: 0.98989898989899, 4: 0.0, 9: 0.5714285714285714, 13: 0.5909090909090909, 21: 0.825, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8571428571428571
Weighted-average F1 score: 0.8281157830823324
F1 score per class: {0: 0.9577464788732394, 32: 0.98989898989899, 4: 0.5714285714285714, 13: 0.6222222222222222, 21: 0.7948717948717948, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8564593301435407
Weighted-average F1 score: 0.8286286625595204

F1 score per class: {0: 0.9577464788732394, 4: 0.9528795811518325, 5: 0.964824120603015, 6: 0.16216216216216217, 7: 0.07142857142857142, 9: 0.9803921568627451, 10: 0.5815602836879432, 13: 0.07692307692307693, 16: 0.9090909090909091, 17: 0.0, 18: 0.6037735849056604, 19: 0.700507614213198, 21: 0.425531914893617, 23: 0.759493670886076, 24: 0.10526315789473684, 26: 0.6853932584269663, 27: 0.45161290322580644, 29: 0.8839779005524862, 31: 0.0, 32: 0.8156424581005587, 40: 0.6434782608695652}
Micro-average F1 score: 0.6943765281173594
Weighted-average F1 score: 0.6909644710369997
F1 score per class: {0: 0.9444444444444444, 4: 0.98, 5: 0.9705882352941176, 6: 0.3609022556390977, 7: 0.0759493670886076, 9: 0.9615384615384616, 10: 0.5074626865671642, 13: 0.06060606060606061, 16: 0.9473684210526315, 17: 0.0, 18: 0.7666666666666667, 19: 0.7849462365591398, 21: 0.49056603773584906, 23: 0.8048780487804879, 24: 0.08695652173913043, 26: 0.6984126984126984, 27: 0.45161290322580644, 29: 0.9319371727748691, 31: 0.6666666666666666, 32: 0.8783068783068783, 40: 0.7647058823529411}
Micro-average F1 score: 0.7316620241411328
Weighted-average F1 score: 0.714934803932137
F1 score per class: {0: 0.9444444444444444, 4: 0.9849246231155779, 5: 0.9705882352941176, 6: 0.2992125984251969, 7: 0.07692307692307693, 9: 0.9803921568627451, 10: 0.5255474452554745, 13: 0.05970149253731343, 16: 0.9473684210526315, 17: 0.0, 18: 0.7457627118644068, 19: 0.7567567567567568, 21: 0.5, 23: 0.775, 24: 0.08695652173913043, 26: 0.6984126984126984, 27: 0.4827586206896552, 29: 0.925531914893617, 31: 0.6666666666666666, 32: 0.8663101604278075, 40: 0.7681159420289855}
Micro-average F1 score: 0.7254901960784313
Weighted-average F1 score: 0.710856236242171

F1 score per class: {0: 0.8717948717948718, 4: 0.9528795811518325, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.09302325581395349, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.4, 23: 0.6896551724137931, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.5632377740303541
Weighted-average F1 score: 0.432109174921953
F1 score per class: {0: 0.6415094339622641, 4: 0.92018779342723, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.05128205128205128, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3333333333333333, 23: 0.5689655172413793, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.3951701427003293
Weighted-average F1 score: 0.2994417492048984
F1 score per class: {0: 0.6601941747572816, 4: 0.9655172413793104, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.05333333333333334, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.35443037974683544, 23: 0.5794392523364486, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4251781472684085
Weighted-average F1 score: 0.3191855917964918

F1 score per class: {0: 0.4533333333333333, 4: 0.9381443298969072, 5: 0.6857142857142857, 6: 0.13636363636363635, 7: 0.03636363636363636, 9: 0.8620689655172413, 10: 0.25705329153605017, 13: 0.022099447513812154, 16: 0.4672897196261682, 17: 0.0, 18: 0.3137254901960784, 19: 0.5207547169811321, 21: 0.2, 23: 0.6185567010309279, 24: 0.09090909090909091, 26: 0.6039603960396039, 27: 0.17073170731707318, 29: 0.8, 31: 0.0, 32: 0.7121951219512195, 40: 0.3854166666666667}
Micro-average F1 score: 0.45717965228589824
Weighted-average F1 score: 0.41665271559971434
F1 score per class: {0: 0.2905982905982906, 4: 0.8828828828828829, 5: 0.2986425339366516, 6: 0.2376237623762376, 7: 0.030303030303030304, 9: 0.49504950495049505, 10: 0.24727272727272728, 13: 0.011560693641618497, 16: 0.375, 17: 0.0, 18: 0.20175438596491227, 19: 0.4866666666666667, 21: 0.15294117647058825, 23: 0.43137254901960786, 24: 0.07142857142857142, 26: 0.6055045871559633, 27: 0.1308411214953271, 29: 0.7982062780269058, 31: 0.04878048780487805, 32: 0.6148148148148148, 40: 0.3151515151515151}
Micro-average F1 score: 0.3463736263736264
Weighted-average F1 score: 0.311736037627079
F1 score per class: {0: 0.2821576763485477, 4: 0.9423076923076923, 5: 0.3613138686131387, 6: 0.21714285714285714, 7: 0.032432432432432434, 9: 0.6756756756756757, 10: 0.2360655737704918, 13: 0.01238390092879257, 16: 0.39705882352941174, 17: 0.0, 18: 0.19555555555555557, 19: 0.5035971223021583, 21: 0.15217391304347827, 23: 0.43661971830985913, 24: 0.07142857142857142, 26: 0.6055045871559633, 27: 0.13592233009708737, 29: 0.8130841121495327, 31: 0.08695652173913043, 32: 0.6694214876033058, 40: 0.363013698630137}
Micro-average F1 score: 0.36633663366336633
Weighted-average F1 score: 0.3271122026918956
cur_acc_wo_na:  ['0.8089', '0.7916', '0.5753', '0.8371']
his_acc_wo_na:  ['0.8089', '0.7922', '0.6786', '0.6944']
cur_acc des_wo_na:  ['0.8251', '0.8764', '0.8000', '0.8571']
his_acc des_wo_na:  ['0.8251', '0.8348', '0.7266', '0.7317']
cur_acc rrf_wo_na:  ['0.8259', '0.8661', '0.8000', '0.8565']
his_acc rrf_wo_na:  ['0.8259', '0.8237', '0.7334', '0.7255']
cur_acc_w_na:  ['0.6667', '0.5222', '0.4271', '0.5632']
his_acc_w_na:  ['0.6667', '0.5743', '0.4928', '0.4572']
cur_acc des_w_na:  ['0.6149', '0.3986', '0.3863', '0.3952']
his_acc des_w_na:  ['0.6149', '0.4568', '0.3546', '0.3464']
cur_acc rrf_w_na:  ['0.6205', '0.4314', '0.3956', '0.4252']
his_acc rrf_w_na:  ['0.6205', '0.4842', '0.3718', '0.3663']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 115.6968663CurrentTrain: epoch  0, batch     1 | loss: 96.6752053CurrentTrain: epoch  0, batch     2 | loss: 116.0683904CurrentTrain: epoch  0, batch     3 | loss: 92.6547767CurrentTrain: epoch  1, batch     0 | loss: 128.6505578CurrentTrain: epoch  1, batch     1 | loss: 112.3633160CurrentTrain: epoch  1, batch     2 | loss: 90.2298776CurrentTrain: epoch  1, batch     3 | loss: 108.2593828CurrentTrain: epoch  2, batch     0 | loss: 105.0083588CurrentTrain: epoch  2, batch     1 | loss: 105.9029136CurrentTrain: epoch  2, batch     2 | loss: 278.2745566CurrentTrain: epoch  2, batch     3 | loss: 46.3818615CurrentTrain: epoch  3, batch     0 | loss: 123.4372913CurrentTrain: epoch  3, batch     1 | loss: 103.4711220CurrentTrain: epoch  3, batch     2 | loss: 130.8568207CurrentTrain: epoch  3, batch     3 | loss: 51.8441092CurrentTrain: epoch  4, batch     0 | loss: 85.2618021CurrentTrain: epoch  4, batch     1 | loss: 179.2044547CurrentTrain: epoch  4, batch     2 | loss: 98.2682123CurrentTrain: epoch  4, batch     3 | loss: 47.0548076CurrentTrain: epoch  5, batch     0 | loss: 97.5816034CurrentTrain: epoch  5, batch     1 | loss: 77.6854233CurrentTrain: epoch  5, batch     2 | loss: 179.6863050CurrentTrain: epoch  5, batch     3 | loss: 74.6417533CurrentTrain: epoch  6, batch     0 | loss: 128.1055125CurrentTrain: epoch  6, batch     1 | loss: 79.8178092CurrentTrain: epoch  6, batch     2 | loss: 79.1657409CurrentTrain: epoch  6, batch     3 | loss: 48.4685501CurrentTrain: epoch  7, batch     0 | loss: 93.6906623CurrentTrain: epoch  7, batch     1 | loss: 173.6244081CurrentTrain: epoch  7, batch     2 | loss: 78.1366588CurrentTrain: epoch  7, batch     3 | loss: 76.4628565CurrentTrain: epoch  8, batch     0 | loss: 101.4376172CurrentTrain: epoch  8, batch     1 | loss: 74.6776421CurrentTrain: epoch  8, batch     2 | loss: 76.6669208CurrentTrain: epoch  8, batch     3 | loss: 106.8623271CurrentTrain: epoch  9, batch     0 | loss: 127.1050137CurrentTrain: epoch  9, batch     1 | loss: 80.6958878CurrentTrain: epoch  9, batch     2 | loss: 93.0565886CurrentTrain: epoch  9, batch     3 | loss: 55.6401335
MemoryTrain:  epoch  0, batch     0 | loss: 0.6407378MemoryTrain:  epoch  1, batch     0 | loss: 0.5510773MemoryTrain:  epoch  2, batch     0 | loss: 0.4678694MemoryTrain:  epoch  3, batch     0 | loss: 0.3545148MemoryTrain:  epoch  4, batch     0 | loss: 0.2766551MemoryTrain:  epoch  5, batch     0 | loss: 0.2173966MemoryTrain:  epoch  6, batch     0 | loss: 0.1739938MemoryTrain:  epoch  7, batch     0 | loss: 0.1398634MemoryTrain:  epoch  8, batch     0 | loss: 0.1184118MemoryTrain:  epoch  9, batch     0 | loss: 0.0957777

F1 score per class: {33: 0.0, 36: 0.13793103448275862, 5: 0.0, 8: 0.0, 10: 0.0, 13: 0.0, 16: 0.8260869565217391, 18: 0.0, 20: 0.0, 21: 0.0, 26: 0.8823529411764706, 29: 0.4, 30: 0.693069306930693}
Micro-average F1 score: 0.5574712643678161
Weighted-average F1 score: 0.6095009437174698
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.0, 6: 0.704, 7: 0.0, 8: 0.0, 13: 0.0, 16: 0.0, 17: 0.8979591836734694, 18: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.972972972972973, 29: 0.625, 30: 0.921875}
Micro-average F1 score: 0.794392523364486
Weighted-average F1 score: 0.7526952669804374
F1 score per class: {33: 0.0, 36: 0.0, 5: 0.5663716814159292, 7: 0.0, 8: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.9090909090909091, 18: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.972972972972973, 29: 0.625, 30: 0.9291338582677166}
Micro-average F1 score: 0.7571428571428571
Weighted-average F1 score: 0.721421500435158

F1 score per class: {0: 0.927536231884058, 4: 0.88268156424581, 5: 0.9658536585365853, 6: 0.12962962962962962, 7: 0.08823529411764706, 8: 0.13793103448275862, 9: 0.9803921568627451, 10: 0.2033898305084746, 13: 0.14285714285714285, 16: 0.7843137254901961, 17: 0.0, 18: 0.5283018867924528, 19: 0.7052631578947368, 20: 0.8, 21: 0.36363636363636365, 23: 0.7692307692307693, 24: 0.10526315789473684, 26: 0.6961325966850829, 27: 0.41379310344827586, 29: 0.8023255813953488, 30: 0.8823529411764706, 31: 0.6666666666666666, 32: 0.8156424581005587, 33: 0.18181818181818182, 36: 0.693069306930693, 40: 0.6491228070175439}
Micro-average F1 score: 0.6481078729882558
Weighted-average F1 score: 0.6880990139621495
F1 score per class: {0: 0.9722222222222222, 4: 0.9690721649484536, 5: 0.8839285714285714, 6: 0.3181818181818182, 7: 0.0821917808219178, 8: 0.4861878453038674, 9: 0.9615384615384616, 10: 0.2857142857142857, 13: 0.16666666666666666, 16: 0.9310344827586207, 17: 0.0, 18: 0.75, 19: 0.7853403141361257, 20: 0.88, 21: 0.6101694915254238, 23: 0.7804878048780488, 24: 0.09090909090909091, 26: 0.7046632124352331, 27: 0.6470588235294118, 29: 0.8695652173913043, 30: 0.8780487804878049, 31: 1.0, 32: 0.8432432432432433, 33: 0.4, 36: 0.855072463768116, 40: 0.7368421052631579}
Micro-average F1 score: 0.7175337186897881
Weighted-average F1 score: 0.7248875977632674
F1 score per class: {0: 0.9722222222222222, 4: 0.9473684210526315, 5: 0.9041095890410958, 6: 0.3230769230769231, 7: 0.08108108108108109, 8: 0.44755244755244755, 9: 0.9803921568627451, 10: 0.34108527131782945, 13: 0.1, 16: 0.9310344827586207, 17: 0.0, 18: 0.7096774193548387, 19: 0.774869109947644, 20: 0.8737864077669902, 21: 0.6333333333333333, 23: 0.7654320987654321, 24: 0.09090909090909091, 26: 0.7046632124352331, 27: 0.6285714285714286, 29: 0.8491620111731844, 30: 0.9, 31: 1.0, 32: 0.8369565217391305, 33: 0.3125, 36: 0.8613138686131386, 40: 0.7424242424242424}
Micro-average F1 score: 0.7099533437013997
Weighted-average F1 score: 0.7111230495046198

F1 score per class: {0: 0.0, 5: 0.0, 7: 0.0, 8: 0.13793103448275862, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.608, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8333333333333334, 31: 0.0, 32: 0.0, 33: 0.23076923076923078, 36: 0.5833333333333334, 40: 0.0}
Micro-average F1 score: 0.36059479553903345
Weighted-average F1 score: 0.303334825195771
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.42105263157894735, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.4607329842931937, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8571428571428571, 31: 0.0, 32: 0.0, 33: 0.4166666666666667, 36: 0.5784313725490197, 40: 0.0}
Micro-average F1 score: 0.31894934333958724
Weighted-average F1 score: 0.2658319011772334
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.41025641025641024, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.46632124352331605, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8372093023255814, 31: 0.0, 32: 0.0, 33: 0.3125, 36: 0.5756097560975609, 40: 0.0}
Micro-average F1 score: 0.3151635282457879
Weighted-average F1 score: 0.259965505105844

F1 score per class: {0: 0.5423728813559322, 4: 0.8681318681318682, 5: 0.5673352435530086, 6: 0.112, 7: 0.043795620437956206, 8: 0.12244897959183673, 9: 0.7936507936507936, 10: 0.1437125748502994, 13: 0.04395604395604396, 16: 0.37383177570093457, 17: 0.0, 18: 0.27722772277227725, 19: 0.5296442687747036, 20: 0.37254901960784315, 21: 0.21333333333333335, 23: 0.5714285714285714, 24: 0.09090909090909091, 26: 0.6, 27: 0.12371134020618557, 29: 0.711340206185567, 30: 0.8333333333333334, 31: 0.06666666666666667, 32: 0.7336683417085427, 33: 0.07317073170731707, 36: 0.45751633986928103, 40: 0.4180790960451977}
Micro-average F1 score: 0.43952802359882004
Weighted-average F1 score: 0.42277439279456575
F1 score per class: {0: 0.24561403508771928, 4: 0.9038461538461539, 5: 0.18521983161833488, 6: 0.18421052631578946, 7: 0.03529411764705882, 8: 0.14308943089430895, 9: 0.4065040650406504, 10: 0.13934426229508196, 13: 0.0547945205479452, 16: 0.3253012048192771, 17: 0.0, 18: 0.2553191489361702, 19: 0.46875, 20: 0.24581005586592178, 21: 0.140625, 23: 0.37209302325581395, 24: 0.08, 26: 0.5551020408163265, 27: 0.18181818181818182, 29: 0.7272727272727273, 30: 0.4931506849315068, 31: 0.03305785123966942, 32: 0.5931558935361216, 33: 0.11494252873563218, 36: 0.3224043715846995, 40: 0.3356164383561644}
Micro-average F1 score: 0.29369085173501575
Weighted-average F1 score: 0.2693596507435283
F1 score per class: {0: 0.2422145328719723, 4: 0.9230769230769231, 5: 0.20754716981132076, 6: 0.19534883720930232, 7: 0.03488372093023256, 8: 0.1509433962264151, 9: 0.6172839506172839, 10: 0.15547703180212014, 13: 0.028368794326241134, 16: 0.3333333333333333, 17: 0.0, 18: 0.2913907284768212, 19: 0.4820846905537459, 20: 0.23622047244094488, 21: 0.1386861313868613, 23: 0.3974358974358974, 24: 0.08, 26: 0.5811965811965812, 27: 0.1774193548387097, 29: 0.7378640776699029, 30: 0.5806451612903226, 31: 0.043010752688172046, 32: 0.6062992125984252, 33: 0.07751937984496124, 36: 0.3172043010752688, 40: 0.34146341463414637}
Micro-average F1 score: 0.3037258815701929
Weighted-average F1 score: 0.275772187698285
cur_acc_wo_na:  ['0.8089', '0.7916', '0.5753', '0.8371', '0.5575']
his_acc_wo_na:  ['0.8089', '0.7922', '0.6786', '0.6944', '0.6481']
cur_acc des_wo_na:  ['0.8251', '0.8764', '0.8000', '0.8571', '0.7944']
his_acc des_wo_na:  ['0.8251', '0.8348', '0.7266', '0.7317', '0.7175']
cur_acc rrf_wo_na:  ['0.8259', '0.8661', '0.8000', '0.8565', '0.7571']
his_acc rrf_wo_na:  ['0.8259', '0.8237', '0.7334', '0.7255', '0.7100']
cur_acc_w_na:  ['0.6667', '0.5222', '0.4271', '0.5632', '0.3606']
his_acc_w_na:  ['0.6667', '0.5743', '0.4928', '0.4572', '0.4395']
cur_acc des_w_na:  ['0.6149', '0.3986', '0.3863', '0.3952', '0.3189']
his_acc des_w_na:  ['0.6149', '0.4568', '0.3546', '0.3464', '0.2937']
cur_acc rrf_w_na:  ['0.6205', '0.4314', '0.3956', '0.4252', '0.3152']
his_acc rrf_w_na:  ['0.6205', '0.4842', '0.3718', '0.3663', '0.3037']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 126.4649272CurrentTrain: epoch  0, batch     1 | loss: 122.3173558CurrentTrain: epoch  0, batch     2 | loss: 185.3305895CurrentTrain: epoch  0, batch     3 | loss: 117.1547906CurrentTrain: epoch  0, batch     4 | loss: 39.3918130CurrentTrain: epoch  1, batch     0 | loss: 145.6360444CurrentTrain: epoch  1, batch     1 | loss: 143.9182846CurrentTrain: epoch  1, batch     2 | loss: 110.7342358CurrentTrain: epoch  1, batch     3 | loss: 141.9078734CurrentTrain: epoch  1, batch     4 | loss: 21.5049252CurrentTrain: epoch  2, batch     0 | loss: 108.4991883CurrentTrain: epoch  2, batch     1 | loss: 107.6895108CurrentTrain: epoch  2, batch     2 | loss: 89.2432611CurrentTrain: epoch  2, batch     3 | loss: 176.2975886CurrentTrain: epoch  2, batch     4 | loss: 42.9417315CurrentTrain: epoch  3, batch     0 | loss: 100.8530514CurrentTrain: epoch  3, batch     1 | loss: 178.2454636CurrentTrain: epoch  3, batch     2 | loss: 174.4847458CurrentTrain: epoch  3, batch     3 | loss: 135.8896713CurrentTrain: epoch  3, batch     4 | loss: 22.9582937CurrentTrain: epoch  4, batch     0 | loss: 106.8843072CurrentTrain: epoch  4, batch     1 | loss: 129.7208436CurrentTrain: epoch  4, batch     2 | loss: 101.3152518CurrentTrain: epoch  4, batch     3 | loss: 101.7216789CurrentTrain: epoch  4, batch     4 | loss: 18.9492126CurrentTrain: epoch  5, batch     0 | loss: 128.7077136CurrentTrain: epoch  5, batch     1 | loss: 82.8731954CurrentTrain: epoch  5, batch     2 | loss: 101.6468286CurrentTrain: epoch  5, batch     3 | loss: 83.8385171CurrentTrain: epoch  5, batch     4 | loss: 41.8663984CurrentTrain: epoch  6, batch     0 | loss: 106.3841236CurrentTrain: epoch  6, batch     1 | loss: 130.6475516CurrentTrain: epoch  6, batch     2 | loss: 84.2326618CurrentTrain: epoch  6, batch     3 | loss: 77.0683892CurrentTrain: epoch  6, batch     4 | loss: 26.5668827CurrentTrain: epoch  7, batch     0 | loss: 98.0846409CurrentTrain: epoch  7, batch     1 | loss: 276.3177254CurrentTrain: epoch  7, batch     2 | loss: 81.9108396CurrentTrain: epoch  7, batch     3 | loss: 124.0555583CurrentTrain: epoch  7, batch     4 | loss: 15.8997773CurrentTrain: epoch  8, batch     0 | loss: 96.8579975CurrentTrain: epoch  8, batch     1 | loss: 174.3817448CurrentTrain: epoch  8, batch     2 | loss: 95.2177984CurrentTrain: epoch  8, batch     3 | loss: 97.8105839CurrentTrain: epoch  8, batch     4 | loss: 40.5602901CurrentTrain: epoch  9, batch     0 | loss: 97.4522953CurrentTrain: epoch  9, batch     1 | loss: 123.1053138CurrentTrain: epoch  9, batch     2 | loss: 167.8359554CurrentTrain: epoch  9, batch     3 | loss: 96.7880114CurrentTrain: epoch  9, batch     4 | loss: 41.1242103
MemoryTrain:  epoch  0, batch     0 | loss: 0.7024001MemoryTrain:  epoch  1, batch     0 | loss: 0.6225902MemoryTrain:  epoch  2, batch     0 | loss: 0.4808455MemoryTrain:  epoch  3, batch     0 | loss: 0.3608081MemoryTrain:  epoch  4, batch     0 | loss: 0.3403633MemoryTrain:  epoch  5, batch     0 | loss: 0.3074744MemoryTrain:  epoch  6, batch     0 | loss: 0.2456012MemoryTrain:  epoch  7, batch     0 | loss: 0.2162911MemoryTrain:  epoch  8, batch     0 | loss: 0.2195911MemoryTrain:  epoch  9, batch     0 | loss: 0.1730639

F1 score per class: {0: 0.0, 33: 0.875, 2: 0.0, 36: 0.6461538461538462, 5: 0.7682926829268293, 39: 0.0, 40: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.5333333333333333, 19: 0.0, 20: 0.0, 26: 0.13333333333333333, 28: 0.0}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6537943932761007
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8187919463087249, 12: 0.75, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.8, 33: 0.0, 36: 0.0, 39: 0.35294117647058826, 40: 0.0}
Micro-average F1 score: 0.6936708860759494
Weighted-average F1 score: 0.6121030241718408
F1 score per class: {0: 0.0, 33: 0.875, 2: 0.0, 36: 0.0, 5: 0.0, 6: 0.8266666666666667, 39: 0.7682926829268293, 40: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 18: 0.0, 19: 0.6666666666666666, 21: 0.0, 23: 0.0, 26: 0.25, 28: 0.0}
Micro-average F1 score: 0.712468193384224
Weighted-average F1 score: 0.6535386879730867

F1 score per class: {0: 0.9295774647887324, 2: 0.6363636363636364, 4: 0.8505747126436781, 5: 0.9705882352941176, 6: 0.22608695652173913, 7: 0.07692307692307693, 8: 0.07142857142857142, 9: 0.9803921568627451, 10: 0.14414414414414414, 11: 0.5153374233128835, 12: 0.5316455696202531, 13: 0.23529411764705882, 16: 0.7346938775510204, 17: 0.0, 18: 0.13953488372093023, 19: 0.6885245901639344, 20: 0.5897435897435898, 21: 0.37209302325581395, 23: 0.8735632183908046, 24: 0.10526315789473684, 26: 0.6900584795321637, 27: 0.4666666666666667, 28: 0.23529411764705882, 29: 0.7831325301204819, 30: 0.8823529411764706, 31: 0.0, 32: 0.8415300546448088, 33: 0.17391304347826086, 36: 0.4835164835164835, 39: 0.1111111111111111, 40: 0.6194690265486725}
Micro-average F1 score: 0.6015538290788013
Weighted-average F1 score: 0.6393848758038306
F1 score per class: {0: 0.972972972972973, 2: 0.4827586206896552, 4: 0.8888888888888888, 5: 0.8918918918918919, 6: 0.4966442953020134, 7: 0.08695652173913043, 8: 0.47058823529411764, 9: 0.9803921568627451, 10: 0.2857142857142857, 11: 0.6703296703296703, 12: 0.5970149253731343, 13: 0.15384615384615385, 16: 0.7083333333333334, 17: 0.0, 18: 0.1276595744680851, 19: 0.7614213197969543, 20: 0.7586206896551724, 21: 0.6363636363636364, 23: 0.8636363636363636, 24: 0.09523809523809523, 26: 0.6850828729281768, 27: 0.5161290322580645, 28: 0.25, 29: 0.8208092485549133, 30: 0.9, 31: 1.0, 32: 0.875, 33: 0.34782608695652173, 36: 0.8244274809160306, 39: 0.24, 40: 0.6811594202898551}
Micro-average F1 score: 0.6740146960587843
Weighted-average F1 score: 0.6771958505221498
F1 score per class: {0: 0.972972972972973, 2: 0.4827586206896552, 4: 0.8571428571428571, 5: 0.9124423963133641, 6: 0.36363636363636365, 7: 0.08571428571428572, 8: 0.2692307692307692, 9: 0.9803921568627451, 10: 0.2809917355371901, 11: 0.6631016042780749, 12: 0.5550660792951542, 13: 0.1111111111111111, 16: 0.7083333333333334, 17: 0.0, 18: 0.13043478260869565, 19: 0.7157894736842105, 20: 0.7727272727272727, 21: 0.6461538461538462, 23: 0.8604651162790697, 24: 0.09523809523809523, 26: 0.6850828729281768, 27: 0.5625, 28: 0.1875, 29: 0.8187134502923976, 30: 0.9230769230769231, 31: 1.0, 32: 0.8795811518324608, 33: 0.25, 36: 0.8130081300813008, 39: 0.13793103448275862, 40: 0.6870229007633588}
Micro-average F1 score: 0.6497117667005765
Weighted-average F1 score: 0.6555632478835846

F1 score per class: {0: 0.0, 2: 0.3684210526315789, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.5121951219512195, 12: 0.6206896551724138, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 28: 0.27586206896551724, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.07692307692307693, 40: 0.0}
Micro-average F1 score: 0.385502471169687
Weighted-average F1 score: 0.3099439552802059
F1 score per class: {0: 0.0, 2: 0.2641509433962264, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.5545454545454546, 12: 0.5853658536585366, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.11538461538461539, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.1935483870967742, 40: 0.0}
Micro-average F1 score: 0.2784552845528455
Weighted-average F1 score: 0.21000431607993272
F1 score per class: {0: 0.0, 2: 0.28, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.5610859728506787, 12: 0.586046511627907, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.1111111111111111, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.10526315789473684, 40: 0.0}
Micro-average F1 score: 0.3066812705366922
Weighted-average F1 score: 0.24083171488030886

F1 score per class: {0: 0.5, 2: 0.2, 4: 0.8268156424581006, 5: 0.6092307692307692, 6: 0.16560509554140126, 7: 0.03468208092485549, 8: 0.06521739130434782, 9: 0.78125, 10: 0.128, 11: 0.2616822429906542, 12: 0.16981132075471697, 13: 0.05555555555555555, 16: 0.3870967741935484, 17: 0.0, 18: 0.07692307692307693, 19: 0.5478260869565217, 20: 0.26900584795321636, 21: 0.2, 23: 0.5891472868217055, 24: 0.08695652173913043, 26: 0.5989847715736041, 27: 0.14432989690721648, 28: 0.0963855421686747, 29: 0.6914893617021277, 30: 0.8333333333333334, 31: 0.0, 32: 0.7403846153846154, 33: 0.08695652173913043, 36: 0.3464566929133858, 39: 0.05714285714285714, 40: 0.42168674698795183}
Micro-average F1 score: 0.3621380846325167
Weighted-average F1 score: 0.3339946147143382
F1 score per class: {0: 0.2823529411764706, 2: 0.11290322580645161, 4: 0.8602150537634409, 5: 0.22943221320973348, 6: 0.24422442244224424, 7: 0.03614457831325301, 8: 0.17518248175182483, 9: 0.4065040650406504, 10: 0.1552511415525114, 11: 0.2623655913978495, 12: 0.17118402282453637, 13: 0.07142857142857142, 16: 0.33663366336633666, 17: 0.0, 18: 0.06, 19: 0.5, 20: 0.2727272727272727, 21: 0.15272727272727274, 23: 0.36538461538461536, 24: 0.07692307692307693, 26: 0.5794392523364486, 27: 0.1391304347826087, 28: 0.05357142857142857, 29: 0.7171717171717171, 30: 0.48, 31: 0.0449438202247191, 32: 0.5544554455445545, 33: 0.11428571428571428, 36: 0.38434163701067614, 39: 0.05825242718446602, 40: 0.33098591549295775}
Micro-average F1 score: 0.2850685125017658
Weighted-average F1 score: 0.26273088865831173
F1 score per class: {0: 0.3564356435643564, 2: 0.1206896551724138, 4: 0.8426966292134831, 5: 0.3065015479876161, 6: 0.20869565217391303, 7: 0.03409090909090909, 8: 0.1407035175879397, 9: 0.6024096385542169, 10: 0.16346153846153846, 11: 0.2577962577962578, 12: 0.15613382899628253, 13: 0.0425531914893617, 16: 0.3469387755102041, 17: 0.0, 18: 0.06382978723404255, 19: 0.4981684981684982, 20: 0.2463768115942029, 21: 0.16091954022988506, 23: 0.4431137724550898, 24: 0.08, 26: 0.5821596244131455, 27: 0.15126050420168066, 28: 0.045112781954887216, 29: 0.717948717948718, 30: 0.6792452830188679, 31: 0.056338028169014086, 32: 0.5874125874125874, 33: 0.0821917808219178, 36: 0.398406374501992, 39: 0.03669724770642202, 40: 0.3629032258064516}
Micro-average F1 score: 0.2959530429409947
Weighted-average F1 score: 0.27046856259263835
cur_acc_wo_na:  ['0.8089', '0.7916', '0.5753', '0.8371', '0.5575', '0.6667']
his_acc_wo_na:  ['0.8089', '0.7922', '0.6786', '0.6944', '0.6481', '0.6016']
cur_acc des_wo_na:  ['0.8251', '0.8764', '0.8000', '0.8571', '0.7944', '0.6937']
his_acc des_wo_na:  ['0.8251', '0.8348', '0.7266', '0.7317', '0.7175', '0.6740']
cur_acc rrf_wo_na:  ['0.8259', '0.8661', '0.8000', '0.8565', '0.7571', '0.7125']
his_acc rrf_wo_na:  ['0.8259', '0.8237', '0.7334', '0.7255', '0.7100', '0.6497']
cur_acc_w_na:  ['0.6667', '0.5222', '0.4271', '0.5632', '0.3606', '0.3855']
his_acc_w_na:  ['0.6667', '0.5743', '0.4928', '0.4572', '0.4395', '0.3621']
cur_acc des_w_na:  ['0.6149', '0.3986', '0.3863', '0.3952', '0.3189', '0.2785']
his_acc des_w_na:  ['0.6149', '0.4568', '0.3546', '0.3464', '0.2937', '0.2851']
cur_acc rrf_w_na:  ['0.6205', '0.4314', '0.3956', '0.4252', '0.3152', '0.3067']
his_acc rrf_w_na:  ['0.6205', '0.4842', '0.3718', '0.3663', '0.3037', '0.2960']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 119.3711796CurrentTrain: epoch  0, batch     1 | loss: 146.5216850CurrentTrain: epoch  0, batch     2 | loss: 99.0203229CurrentTrain: epoch  0, batch     3 | loss: 143.5972737CurrentTrain: epoch  0, batch     4 | loss: 67.5685894CurrentTrain: epoch  1, batch     0 | loss: 145.5309521CurrentTrain: epoch  1, batch     1 | loss: 110.4721145CurrentTrain: epoch  1, batch     2 | loss: 181.6268225CurrentTrain: epoch  1, batch     3 | loss: 126.6677835CurrentTrain: epoch  1, batch     4 | loss: 158.3776245CurrentTrain: epoch  2, batch     0 | loss: 87.5191810CurrentTrain: epoch  2, batch     1 | loss: 132.1281229CurrentTrain: epoch  2, batch     2 | loss: 178.6134728CurrentTrain: epoch  2, batch     3 | loss: 103.3672043CurrentTrain: epoch  2, batch     4 | loss: 77.1835106CurrentTrain: epoch  3, batch     0 | loss: 276.1011841CurrentTrain: epoch  3, batch     1 | loss: 82.4082854CurrentTrain: epoch  3, batch     2 | loss: 264.0433387CurrentTrain: epoch  3, batch     3 | loss: 83.0775396CurrentTrain: epoch  3, batch     4 | loss: 70.7367070CurrentTrain: epoch  4, batch     0 | loss: 102.5439039CurrentTrain: epoch  4, batch     1 | loss: 131.6149135CurrentTrain: epoch  4, batch     2 | loss: 100.0754498CurrentTrain: epoch  4, batch     3 | loss: 83.9575193CurrentTrain: epoch  4, batch     4 | loss: 70.4914608CurrentTrain: epoch  5, batch     0 | loss: 101.2776781CurrentTrain: epoch  5, batch     1 | loss: 126.7187795CurrentTrain: epoch  5, batch     2 | loss: 127.4335449CurrentTrain: epoch  5, batch     3 | loss: 131.1107277CurrentTrain: epoch  5, batch     4 | loss: 43.4151157CurrentTrain: epoch  6, batch     0 | loss: 100.9497599CurrentTrain: epoch  6, batch     1 | loss: 95.1127215CurrentTrain: epoch  6, batch     2 | loss: 273.9282151CurrentTrain: epoch  6, batch     3 | loss: 128.2498895CurrentTrain: epoch  6, batch     4 | loss: 89.3276023CurrentTrain: epoch  7, batch     0 | loss: 127.1469474CurrentTrain: epoch  7, batch     1 | loss: 130.2912475CurrentTrain: epoch  7, batch     2 | loss: 99.9282509CurrentTrain: epoch  7, batch     3 | loss: 96.3051436CurrentTrain: epoch  7, batch     4 | loss: 67.6159415CurrentTrain: epoch  8, batch     0 | loss: 99.5732674CurrentTrain: epoch  8, batch     1 | loss: 101.9659214CurrentTrain: epoch  8, batch     2 | loss: 101.0689597CurrentTrain: epoch  8, batch     3 | loss: 79.9949421CurrentTrain: epoch  8, batch     4 | loss: 67.5920016CurrentTrain: epoch  9, batch     0 | loss: 121.7131575CurrentTrain: epoch  9, batch     1 | loss: 100.6968010CurrentTrain: epoch  9, batch     2 | loss: 177.7007850CurrentTrain: epoch  9, batch     3 | loss: 95.8478924CurrentTrain: epoch  9, batch     4 | loss: 69.0783456
MemoryTrain:  epoch  0, batch     0 | loss: 0.9677099MemoryTrain:  epoch  1, batch     0 | loss: 0.8120987MemoryTrain:  epoch  2, batch     0 | loss: 0.6441714MemoryTrain:  epoch  3, batch     0 | loss: 0.5336829MemoryTrain:  epoch  4, batch     0 | loss: 0.4518811MemoryTrain:  epoch  5, batch     0 | loss: 0.3742499MemoryTrain:  epoch  6, batch     0 | loss: 0.2889760MemoryTrain:  epoch  7, batch     0 | loss: 0.2562423MemoryTrain:  epoch  8, batch     0 | loss: 0.2344060MemoryTrain:  epoch  9, batch     0 | loss: 0.1925342

F1 score per class: {32: 0.36220472440944884, 1: 0.6333333333333333, 34: 0.0, 3: 0.10126582278481013, 11: 0.0, 14: 0.0, 18: 0.6871165644171779, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.25396825396825395}
Micro-average F1 score: 0.4417808219178082
Weighted-average F1 score: 0.41979778991709926
F1 score per class: {32: 0.384, 1: 0.9487179487179487, 34: 0.0, 3: 0.0, 9: 0.0, 10: 0.18181818181818182, 11: 0.0, 14: 0.0, 18: 0.7558139534883721, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.735632183908046}
Micro-average F1 score: 0.6114457831325302
Weighted-average F1 score: 0.5731938799645521
F1 score per class: {32: 0.375, 1: 0.9419354838709677, 34: 0.0, 3: 0.0, 10: 0.17391304347826086, 11: 0.0, 14: 0.0, 18: 0.7305389221556886, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.4931506849315068}
Micro-average F1 score: 0.5609756097560976
Weighted-average F1 score: 0.5217300375083922

F1 score per class: {0: 0.8484848484848485, 1: 0.3129251700680272, 2: 0.5384615384615384, 3: 0.6333333333333333, 4: 0.7804878048780488, 5: 0.9696969696969697, 6: 0.19469026548672566, 7: 0.08571428571428572, 8: 0.0, 9: 0.9803921568627451, 10: 0.1565217391304348, 11: 0.40476190476190477, 12: 0.3508771929824561, 13: 0.1111111111111111, 14: 0.09523809523809523, 16: 0.723404255319149, 17: 0.0, 18: 0.0, 19: 0.36496350364963503, 20: 0.13793103448275862, 21: 0.20512820512820512, 22: 0.6666666666666666, 23: 0.8192771084337349, 24: 0.08333333333333333, 26: 0.6896551724137931, 27: 0.0, 28: 0.3333333333333333, 29: 0.7295597484276729, 30: 0.8823529411764706, 31: 0.0, 32: 0.797752808988764, 33: 0.19047619047619047, 34: 0.1568627450980392, 36: 0.42857142857142855, 39: 0.09523809523809523, 40: 0.3218390804597701}
Micro-average F1 score: 0.49323889246619446
Weighted-average F1 score: 0.5356953396060171
F1 score per class: {0: 0.9295774647887324, 1: 0.3221476510067114, 2: 0.3888888888888889, 3: 0.9135802469135802, 4: 0.8505747126436781, 5: 0.9166666666666666, 6: 0.31007751937984496, 7: 0.08695652173913043, 8: 0.45390070921985815, 9: 0.8928571428571429, 10: 0.2975206611570248, 11: 0.6, 12: 0.5628140703517588, 13: 0.1111111111111111, 14: 0.16842105263157894, 16: 0.8235294117647058, 17: 0.0, 18: 0.125, 19: 0.6153846153846154, 20: 0.6746987951807228, 21: 0.2553191489361702, 22: 0.7222222222222222, 23: 0.8095238095238095, 24: 0.07142857142857142, 26: 0.7046632124352331, 27: 0.1, 28: 0.22641509433962265, 29: 0.7484662576687117, 30: 0.8780487804878049, 31: 0.6666666666666666, 32: 0.8216216216216217, 33: 0.38095238095238093, 34: 0.43243243243243246, 36: 0.7115384615384616, 39: 0.21052631578947367, 40: 0.7142857142857143}
Micro-average F1 score: 0.6069235064209939
Weighted-average F1 score: 0.6063869318370343
F1 score per class: {0: 0.9295774647887324, 1: 0.3137254901960784, 2: 0.3888888888888889, 3: 0.9012345679012346, 4: 0.8165680473372781, 5: 0.9252336448598131, 6: 0.30158730158730157, 7: 0.09230769230769231, 8: 0.15555555555555556, 9: 0.9615384615384616, 10: 0.3384615384615385, 11: 0.5792349726775956, 12: 0.5658536585365853, 13: 0.08695652173913043, 14: 0.1568627450980392, 16: 0.8, 17: 0.2, 18: 0.13043478260869565, 19: 0.5786163522012578, 20: 0.6666666666666666, 21: 0.22727272727272727, 22: 0.7011494252873564, 23: 0.7951807228915663, 24: 0.07407407407407407, 26: 0.7046632124352331, 27: 0.1111111111111111, 28: 0.17142857142857143, 29: 0.7530864197530864, 30: 0.9230769230769231, 31: 0.6666666666666666, 32: 0.8216216216216217, 33: 0.36363636363636365, 34: 0.28346456692913385, 36: 0.7358490566037735, 39: 0.16, 40: 0.656}
Micro-average F1 score: 0.5853382394565525
Weighted-average F1 score: 0.5852675192186151

F1 score per class: {0: 0.0, 1: 0.14556962025316456, 2: 0.0, 3: 0.4720496894409938, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.06779661016949153, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.48068669527896996, 23: 0.0, 24: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.24242424242424243, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.23390752493200362
Weighted-average F1 score: 0.1937914072144217
F1 score per class: {0: 0.0, 1: 0.1322314049586777, 2: 0.0, 3: 0.46105919003115264, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.09876543209876543, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5138339920948617, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5925925925925926, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.22886133032694475
Weighted-average F1 score: 0.19026940321946992
F1 score per class: {0: 0.0, 1: 0.1293800539083558, 2: 0.0, 3: 0.4694533762057878, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.08648648648648649, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5213675213675214, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.41379310344827586, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2160892542571932
Weighted-average F1 score: 0.17908423955190317

F1 score per class: {0: 0.49557522123893805, 1: 0.10623556581986143, 2: 0.22950819672131148, 3: 0.3438914027149321, 4: 0.7710843373493976, 5: 0.7164179104477612, 6: 0.1301775147928994, 7: 0.04285714285714286, 8: 0.0, 9: 0.7142857142857143, 10: 0.13953488372093023, 11: 0.21316614420062696, 12: 0.16997167138810199, 13: 0.03389830508474576, 14: 0.047337278106508875, 16: 0.4927536231884058, 17: 0.0, 18: 0.0, 19: 0.2857142857142857, 20: 0.11940298507462686, 21: 0.14035087719298245, 22: 0.4375, 23: 0.6415094339622641, 24: 0.058823529411764705, 26: 0.5882352941176471, 27: 0.0, 28: 0.11363636363636363, 29: 0.6338797814207651, 30: 0.8333333333333334, 31: 0.0, 32: 0.6016949152542372, 33: 0.125, 34: 0.09248554913294797, 36: 0.33962264150943394, 39: 0.03571428571428571, 40: 0.25}
Micro-average F1 score: 0.3149023638232271
Weighted-average F1 score: 0.295769241904996
F1 score per class: {0: 0.358695652173913, 1: 0.08921933085501858, 2: 0.12389380530973451, 3: 0.2596491228070175, 4: 0.8222222222222222, 5: 0.31629392971246006, 6: 0.1702127659574468, 7: 0.03773584905660377, 8: 0.23529411764705882, 9: 0.3067484662576687, 10: 0.1791044776119403, 11: 0.20987654320987653, 12: 0.19047619047619047, 13: 0.0273972602739726, 14: 0.05947955390334572, 16: 0.4883720930232558, 17: 0.0, 18: 0.05555555555555555, 19: 0.41201716738197425, 20: 0.30434782608695654, 21: 0.08955223880597014, 22: 0.4234527687296417, 23: 0.35789473684210527, 24: 0.04081632653061224, 26: 0.5811965811965812, 27: 0.05405405405405406, 28: 0.04316546762589928, 29: 0.6288659793814433, 30: 0.43902439024390244, 31: 0.03773584905660377, 32: 0.4578313253012048, 33: 0.14545454545454545, 34: 0.14953271028037382, 36: 0.37948717948717947, 39: 0.05063291139240506, 40: 0.36231884057971014}
Micro-average F1 score: 0.2647990255785627
Weighted-average F1 score: 0.24543756664010505
F1 score per class: {0: 0.3548387096774194, 1: 0.08540925266903915, 2: 0.11965811965811966, 3: 0.2547993019197208, 4: 0.8023255813953488, 5: 0.3815028901734104, 6: 0.17117117117117117, 7: 0.0392156862745098, 8: 0.12173913043478261, 9: 0.5319148936170213, 10: 0.19642857142857142, 11: 0.19813084112149532, 12: 0.18210361067503925, 13: 0.022222222222222223, 14: 0.05194805194805195, 16: 0.4819277108433735, 17: 0.1, 18: 0.061224489795918366, 19: 0.3817427385892116, 20: 0.3027027027027027, 21: 0.08928571428571429, 22: 0.4420289855072464, 23: 0.4489795918367347, 24: 0.044444444444444446, 26: 0.5887445887445888, 27: 0.06451612903225806, 28: 0.041666666666666664, 29: 0.6321243523316062, 30: 0.5625, 31: 0.046511627906976744, 32: 0.4720496894409938, 33: 0.12903225806451613, 34: 0.10465116279069768, 36: 0.3880597014925373, 39: 0.03636363636363636, 40: 0.3813953488372093}
Micro-average F1 score: 0.2645177794832438
Weighted-average F1 score: 0.24243573103663751
cur_acc_wo_na:  ['0.8089', '0.7916', '0.5753', '0.8371', '0.5575', '0.6667', '0.4418']
his_acc_wo_na:  ['0.8089', '0.7922', '0.6786', '0.6944', '0.6481', '0.6016', '0.4932']
cur_acc des_wo_na:  ['0.8251', '0.8764', '0.8000', '0.8571', '0.7944', '0.6937', '0.6114']
his_acc des_wo_na:  ['0.8251', '0.8348', '0.7266', '0.7317', '0.7175', '0.6740', '0.6069']
cur_acc rrf_wo_na:  ['0.8259', '0.8661', '0.8000', '0.8565', '0.7571', '0.7125', '0.5610']
his_acc rrf_wo_na:  ['0.8259', '0.8237', '0.7334', '0.7255', '0.7100', '0.6497', '0.5853']
cur_acc_w_na:  ['0.6667', '0.5222', '0.4271', '0.5632', '0.3606', '0.3855', '0.2339']
his_acc_w_na:  ['0.6667', '0.5743', '0.4928', '0.4572', '0.4395', '0.3621', '0.3149']
cur_acc des_w_na:  ['0.6149', '0.3986', '0.3863', '0.3952', '0.3189', '0.2785', '0.2289']
his_acc des_w_na:  ['0.6149', '0.4568', '0.3546', '0.3464', '0.2937', '0.2851', '0.2648']
cur_acc rrf_w_na:  ['0.6205', '0.4314', '0.3956', '0.4252', '0.3152', '0.3067', '0.2161']
his_acc rrf_w_na:  ['0.6205', '0.4842', '0.3718', '0.3663', '0.3037', '0.2960', '0.2645']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 116.8348751CurrentTrain: epoch  0, batch     1 | loss: 110.9773565CurrentTrain: epoch  0, batch     2 | loss: 96.0619832CurrentTrain: epoch  0, batch     3 | loss: 133.4328042CurrentTrain: epoch  1, batch     0 | loss: 135.9393064CurrentTrain: epoch  1, batch     1 | loss: 131.9011024CurrentTrain: epoch  1, batch     2 | loss: 120.5206915CurrentTrain: epoch  1, batch     3 | loss: 130.3065408CurrentTrain: epoch  2, batch     0 | loss: 100.5496062CurrentTrain: epoch  2, batch     1 | loss: 134.1452551CurrentTrain: epoch  2, batch     2 | loss: 81.8161190CurrentTrain: epoch  2, batch     3 | loss: 69.4573963CurrentTrain: epoch  3, batch     0 | loss: 81.8761390CurrentTrain: epoch  3, batch     1 | loss: 79.3472755CurrentTrain: epoch  3, batch     2 | loss: 178.2836634CurrentTrain: epoch  3, batch     3 | loss: 53.3546793CurrentTrain: epoch  4, batch     0 | loss: 98.0607518CurrentTrain: epoch  4, batch     1 | loss: 130.7068155CurrentTrain: epoch  4, batch     2 | loss: 77.3502929CurrentTrain: epoch  4, batch     3 | loss: 86.1711890CurrentTrain: epoch  5, batch     0 | loss: 82.9929034CurrentTrain: epoch  5, batch     1 | loss: 77.7753790CurrentTrain: epoch  5, batch     2 | loss: 124.7518798CurrentTrain: epoch  5, batch     3 | loss: 66.2321868CurrentTrain: epoch  6, batch     0 | loss: 100.1345432CurrentTrain: epoch  6, batch     1 | loss: 76.5160135CurrentTrain: epoch  6, batch     2 | loss: 76.8708672CurrentTrain: epoch  6, batch     3 | loss: 121.0322105CurrentTrain: epoch  7, batch     0 | loss: 100.8599900CurrentTrain: epoch  7, batch     1 | loss: 82.8375932CurrentTrain: epoch  7, batch     2 | loss: 75.0973327CurrentTrain: epoch  7, batch     3 | loss: 66.0462054CurrentTrain: epoch  8, batch     0 | loss: 95.3656608CurrentTrain: epoch  8, batch     1 | loss: 76.7521630CurrentTrain: epoch  8, batch     2 | loss: 98.3824842CurrentTrain: epoch  8, batch     3 | loss: 85.1767576CurrentTrain: epoch  9, batch     0 | loss: 100.4862057CurrentTrain: epoch  9, batch     1 | loss: 124.0984872CurrentTrain: epoch  9, batch     2 | loss: 120.0322199CurrentTrain: epoch  9, batch     3 | loss: 49.2657353
MemoryTrain:  epoch  0, batch     0 | loss: 0.7734529MemoryTrain:  epoch  1, batch     0 | loss: 0.6535048MemoryTrain:  epoch  2, batch     0 | loss: 0.5800968MemoryTrain:  epoch  3, batch     0 | loss: 0.4693441MemoryTrain:  epoch  4, batch     0 | loss: 0.3781740MemoryTrain:  epoch  5, batch     0 | loss: 0.2951463MemoryTrain:  epoch  6, batch     0 | loss: 0.2474061MemoryTrain:  epoch  7, batch     0 | loss: 0.2279603MemoryTrain:  epoch  8, batch     0 | loss: 0.2049079MemoryTrain:  epoch  9, batch     0 | loss: 0.1739012

F1 score per class: {1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 36: 0.8235294117647058, 38: 0.0, 10: 0.6133333333333333, 11: 0.0, 13: 0.6923076923076923, 15: 0.0, 23: 0.5542168674698795, 25: 0.35294117647058826}
Micro-average F1 score: 0.5180722891566265
Weighted-average F1 score: 0.4078497347477714
F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.75, 37: 0.0, 36: 0.0, 8: 0.0, 33: 0.631578947368421, 38: 0.0, 13: 0.0, 15: 0.0, 18: 0.8314606741573034, 22: 0.0, 23: 0.651685393258427, 25: 0.782608695652174}
Micro-average F1 score: 0.59375
Weighted-average F1 score: 0.460323679131438
F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.75, 37: 0.0, 36: 0.0, 8: 0.0, 33: 0.631578947368421, 38: 0.0, 13: 0.0, 15: 0.0, 18: 0.8314606741573034, 22: 0.0, 23: 0.651685393258427, 25: 0.6976744186046512}
Micro-average F1 score: 0.5826771653543307
Weighted-average F1 score: 0.4478744057687989

F1 score per class: {0: 0.8823529411764706, 1: 0.3157894736842105, 2: 0.56, 3: 0.45217391304347826, 4: 0.7951807228915663, 5: 0.8818181818181818, 6: 0.21052631578947367, 7: 0.09090909090909091, 8: 0.0, 9: 0.9803921568627451, 10: 0.18803418803418803, 11: 0.4028776978417266, 12: 0.375, 13: 0.09090909090909091, 14: 0.07228915662650602, 15: 0.5185185185185185, 16: 0.7083333333333334, 17: 0.0, 18: 0.0, 19: 0.4768211920529801, 20: 0.03636363636363636, 21: 0.21052631578947367, 22: 0.6783625730994152, 23: 0.813953488372093, 24: 0.08695652173913043, 25: 0.6133333333333333, 26: 0.7017543859649122, 27: 0.0, 28: 0.2962962962962963, 29: 0.7831325301204819, 30: 0.9142857142857143, 31: 0.0, 32: 0.7251461988304093, 33: 0.2857142857142857, 34: 0.14953271028037382, 35: 0.43902439024390244, 36: 0.27848101265822783, 37: 0.41818181818181815, 38: 0.2926829268292683, 39: 0.10526315789473684, 40: 0.46938775510204084}
Micro-average F1 score: 0.48929489009420496
Weighted-average F1 score: 0.5302181984013237
F1 score per class: {0: 0.9295774647887324, 1: 0.3157894736842105, 2: 0.358974358974359, 3: 0.5112781954887218, 4: 0.8304093567251462, 5: 0.8354430379746836, 6: 0.37410071942446044, 7: 0.1, 8: 0.45714285714285713, 9: 0.9259259259259259, 10: 0.31496062992125984, 11: 0.6390532544378699, 12: 0.57, 13: 0.13333333333333333, 14: 0.1188118811881188, 15: 0.5, 16: 0.7755102040816326, 17: 0.0, 18: 0.23076923076923078, 19: 0.5503355704697986, 20: 0.3125, 21: 0.2857142857142857, 22: 0.7005649717514124, 23: 0.813953488372093, 24: 0.07142857142857142, 25: 0.631578947368421, 26: 0.723404255319149, 27: 0.1111111111111111, 28: 0.2926829268292683, 29: 0.7857142857142857, 30: 0.926829268292683, 31: 0.6666666666666666, 32: 0.8191489361702128, 33: 0.3157894736842105, 34: 0.15, 35: 0.5174825174825175, 36: 0.6060606060606061, 37: 0.42028985507246375, 38: 0.43902439024390244, 39: 0.13333333333333333, 40: 0.7034482758620689}
Micro-average F1 score: 0.562452877607439
Weighted-average F1 score: 0.562897009472446
F1 score per class: {0: 0.9295774647887324, 1: 0.3137254901960784, 2: 0.3783783783783784, 3: 0.5074626865671642, 4: 0.8304093567251462, 5: 0.8461538461538461, 6: 0.35294117647058826, 7: 0.1, 8: 0.23529411764705882, 9: 0.9615384615384616, 10: 0.272, 11: 0.631578947368421, 12: 0.5576923076923077, 13: 0.1111111111111111, 14: 0.11764705882352941, 15: 0.42857142857142855, 16: 0.7755102040816326, 17: 0.0, 18: 0.08695652173913043, 19: 0.5822784810126582, 20: 0.3125, 21: 0.21739130434782608, 22: 0.6857142857142857, 23: 0.813953488372093, 24: 0.07142857142857142, 25: 0.631578947368421, 26: 0.7272727272727273, 27: 0.11764705882352941, 28: 0.21818181818181817, 29: 0.7904191616766467, 30: 0.9743589743589743, 31: 0.6666666666666666, 32: 0.8085106382978723, 33: 0.3157894736842105, 34: 0.16, 35: 0.5174825174825175, 36: 0.5106382978723404, 37: 0.4142857142857143, 38: 0.36585365853658536, 39: 0.11764705882352941, 40: 0.6615384615384615}
Micro-average F1 score: 0.5449936628643853
Weighted-average F1 score: 0.5482558358509225

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.56, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.6133333333333333, 26: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.5242718446601942, 36: 0.0, 37: 0.5111111111111111, 38: 0.32432432432432434, 40: 0.0}
Micro-average F1 score: 0.3481781376518219
Weighted-average F1 score: 0.23215377078924576
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.5217391304347826, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5714285714285714, 26: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.5, 36: 0.0, 37: 0.4715447154471545, 38: 0.5714285714285714, 40: 0.0}
Micro-average F1 score: 0.26480836236933797
Weighted-average F1 score: 0.18678127280990484
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.5217391304347826, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5581395348837209, 26: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.4900662251655629, 36: 0.0, 37: 0.47540983606557374, 38: 0.5084745762711864, 40: 0.0}
Micro-average F1 score: 0.2687651331719128
Weighted-average F1 score: 0.1920065553579637

F1 score per class: {0: 0.4580152671755725, 1: 0.09393346379647749, 2: 0.22950819672131148, 3: 0.24528301886792453, 4: 0.7810650887573964, 5: 0.6445182724252492, 6: 0.147239263803681, 7: 0.04411764705882353, 8: 0.0, 9: 0.704225352112676, 10: 0.1506849315068493, 11: 0.2222222222222222, 12: 0.15789473684210525, 13: 0.020833333333333332, 14: 0.03680981595092025, 15: 0.15053763440860216, 16: 0.4473684210526316, 17: 0.0, 18: 0.0, 19: 0.34615384615384615, 20: 0.03278688524590164, 21: 0.15384615384615385, 22: 0.45849802371541504, 23: 0.6730769230769231, 24: 0.0625, 25: 0.6133333333333333, 26: 0.5970149253731343, 27: 0.0, 28: 0.08247422680412371, 29: 0.6632653061224489, 30: 0.8648648648648649, 31: 0.0, 32: 0.5367965367965368, 33: 0.1935483870967742, 34: 0.10062893081761007, 35: 0.17088607594936708, 36: 0.22, 37: 0.24598930481283424, 38: 0.1188118811881188, 39: 0.05714285714285714, 40: 0.3382352941176471}
Micro-average F1 score: 0.2951101928374656
Weighted-average F1 score: 0.27291369485546696
F1 score per class: {0: 0.3251231527093596, 1: 0.08304498269896193, 2: 0.10606060606060606, 3: 0.13991769547325103, 4: 0.8022598870056498, 5: 0.3113207547169811, 6: 0.18181818181818182, 7: 0.041666666666666664, 8: 0.2140468227424749, 9: 0.31645569620253167, 10: 0.17316017316017315, 11: 0.2072936660268714, 12: 0.16332378223495703, 13: 0.0425531914893617, 14: 0.0316622691292876, 15: 0.15, 16: 0.4578313253012048, 17: 0.0, 18: 0.08955223880597014, 19: 0.34893617021276596, 20: 0.21505376344086022, 21: 0.10218978102189781, 22: 0.45925925925925926, 23: 0.4093567251461988, 24: 0.0392156862745098, 25: 0.5581395348837209, 26: 0.5738396624472574, 27: 0.07142857142857142, 28: 0.05454545454545454, 29: 0.6666666666666666, 30: 0.5352112676056338, 31: 0.04081632653061224, 32: 0.4476744186046512, 33: 0.11764705882352941, 34: 0.05421686746987952, 35: 0.11526479750778816, 36: 0.37037037037037035, 37: 0.14392059553349876, 38: 0.1180327868852459, 39: 0.06896551724137931, 40: 0.3434343434343434}
Micro-average F1 score: 0.23091209244737929
Weighted-average F1 score: 0.20952434382640145
F1 score per class: {0: 0.3251231527093596, 1: 0.0817717206132879, 2: 0.109375, 3: 0.14255765199161424, 4: 0.8114285714285714, 5: 0.36464088397790057, 6: 0.18604651162790697, 7: 0.039735099337748346, 8: 0.17266187050359713, 9: 0.5050505050505051, 10: 0.14977973568281938, 11: 0.2011173184357542, 12: 0.15405046480743692, 13: 0.03278688524590164, 14: 0.032432432432432434, 15: 0.11009174311926606, 16: 0.4691358024691358, 17: 0.0, 18: 0.041666666666666664, 19: 0.38016528925619836, 20: 0.2127659574468085, 21: 0.08196721311475409, 22: 0.46332046332046334, 23: 0.5263157894736842, 24: 0.0425531914893617, 25: 0.5454545454545454, 26: 0.6071428571428571, 27: 0.07407407407407407, 28: 0.045283018867924525, 29: 0.6700507614213198, 30: 0.6551724137931034, 31: 0.047619047619047616, 32: 0.44970414201183434, 33: 0.10344827586206896, 34: 0.05649717514124294, 35: 0.11708860759493671, 36: 0.31788079470198677, 37: 0.13908872901678657, 38: 0.09803921568627451, 39: 0.05405405405405406, 40: 0.3659574468085106}
Micro-average F1 score: 0.2304641440668882
Weighted-average F1 score: 0.20597548983226358
cur_acc_wo_na:  ['0.8089', '0.7916', '0.5753', '0.8371', '0.5575', '0.6667', '0.4418', '0.5181']
his_acc_wo_na:  ['0.8089', '0.7922', '0.6786', '0.6944', '0.6481', '0.6016', '0.4932', '0.4893']
cur_acc des_wo_na:  ['0.8251', '0.8764', '0.8000', '0.8571', '0.7944', '0.6937', '0.6114', '0.5938']
his_acc des_wo_na:  ['0.8251', '0.8348', '0.7266', '0.7317', '0.7175', '0.6740', '0.6069', '0.5625']
cur_acc rrf_wo_na:  ['0.8259', '0.8661', '0.8000', '0.8565', '0.7571', '0.7125', '0.5610', '0.5827']
his_acc rrf_wo_na:  ['0.8259', '0.8237', '0.7334', '0.7255', '0.7100', '0.6497', '0.5853', '0.5450']
cur_acc_w_na:  ['0.6667', '0.5222', '0.4271', '0.5632', '0.3606', '0.3855', '0.2339', '0.3482']
his_acc_w_na:  ['0.6667', '0.5743', '0.4928', '0.4572', '0.4395', '0.3621', '0.3149', '0.2951']
cur_acc des_w_na:  ['0.6149', '0.3986', '0.3863', '0.3952', '0.3189', '0.2785', '0.2289', '0.2648']
his_acc des_w_na:  ['0.6149', '0.4568', '0.3546', '0.3464', '0.2937', '0.2851', '0.2648', '0.2309']
cur_acc rrf_w_na:  ['0.6205', '0.4314', '0.3956', '0.4252', '0.3152', '0.3067', '0.2161', '0.2688']
his_acc rrf_w_na:  ['0.6205', '0.4842', '0.3718', '0.3663', '0.3037', '0.2960', '0.2645', '0.2305']
----------END
his_acc mean_wo_na:  [0.8188 0.7479 0.6741 0.6478 0.5897 0.579  0.5252 0.5142]
his_acc des mean_wo_na:  [0.8432 0.8134 0.7475 0.7153 0.6686 0.6437 0.6043 0.5855]
his_acc rrf mean_wo_na:  [0.844  0.8029 0.7406 0.7067 0.6569 0.6296 0.593  0.5752]
his_acc mean_w_na:  [0.6825 0.5628 0.4778 0.4314 0.3731 0.3525 0.3205 0.31  ]
his_acc des mean_w_na:  [0.6368 0.4843 0.3923 0.3324 0.292  0.2705 0.2484 0.2415]
his_acc rrf mean_w_na:  [0.6398 0.4973 0.4041 0.3456 0.2966 0.2803 0.2544 0.2494]
