#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 127.4410230CurrentTrain: epoch  0, batch     1 | loss: 91.0480700CurrentTrain: epoch  0, batch     2 | loss: 78.5039682CurrentTrain: epoch  0, batch     3 | loss: 88.2306910CurrentTrain: epoch  0, batch     4 | loss: 87.3597513CurrentTrain: epoch  0, batch     5 | loss: 87.4418091CurrentTrain: epoch  0, batch     6 | loss: 101.8835287CurrentTrain: epoch  0, batch     7 | loss: 100.6150611CurrentTrain: epoch  0, batch     8 | loss: 86.3544628CurrentTrain: epoch  0, batch     9 | loss: 86.4705513CurrentTrain: epoch  0, batch    10 | loss: 77.2608740CurrentTrain: epoch  0, batch    11 | loss: 100.9790722CurrentTrain: epoch  0, batch    12 | loss: 99.3914943CurrentTrain: epoch  0, batch    13 | loss: 193.1445742CurrentTrain: epoch  0, batch    14 | loss: 100.3338364CurrentTrain: epoch  0, batch    15 | loss: 86.2700722CurrentTrain: epoch  0, batch    16 | loss: 86.6955689CurrentTrain: epoch  0, batch    17 | loss: 118.3794258CurrentTrain: epoch  0, batch    18 | loss: 99.9756678CurrentTrain: epoch  0, batch    19 | loss: 86.3320767CurrentTrain: epoch  0, batch    20 | loss: 146.9150817CurrentTrain: epoch  0, batch    21 | loss: 146.1959342CurrentTrain: epoch  0, batch    22 | loss: 193.1894195CurrentTrain: epoch  0, batch    23 | loss: 145.9490238CurrentTrain: epoch  0, batch    24 | loss: 99.6026530CurrentTrain: epoch  0, batch    25 | loss: 192.7196244CurrentTrain: epoch  0, batch    26 | loss: 86.0092776CurrentTrain: epoch  0, batch    27 | loss: 117.8975299CurrentTrain: epoch  0, batch    28 | loss: 145.6390192CurrentTrain: epoch  0, batch    29 | loss: 98.7139077CurrentTrain: epoch  0, batch    30 | loss: 145.6671623CurrentTrain: epoch  0, batch    31 | loss: 145.4222380CurrentTrain: epoch  0, batch    32 | loss: 117.6100639CurrentTrain: epoch  0, batch    33 | loss: 85.5126644CurrentTrain: epoch  0, batch    34 | loss: 146.5690565CurrentTrain: epoch  0, batch    35 | loss: 84.9825595CurrentTrain: epoch  0, batch    36 | loss: 84.7571527CurrentTrain: epoch  0, batch    37 | loss: 98.5992710CurrentTrain: epoch  0, batch    38 | loss: 98.1671784CurrentTrain: epoch  0, batch    39 | loss: 85.1203110CurrentTrain: epoch  0, batch    40 | loss: 117.5720920CurrentTrain: epoch  0, batch    41 | loss: 85.5919577CurrentTrain: epoch  0, batch    42 | loss: 84.6573954CurrentTrain: epoch  0, batch    43 | loss: 74.3712639CurrentTrain: epoch  0, batch    44 | loss: 98.0891050CurrentTrain: epoch  0, batch    45 | loss: 82.7940821CurrentTrain: epoch  0, batch    46 | loss: 115.0650167CurrentTrain: epoch  0, batch    47 | loss: 83.8726932CurrentTrain: epoch  0, batch    48 | loss: 97.4309193CurrentTrain: epoch  0, batch    49 | loss: 97.1130939CurrentTrain: epoch  0, batch    50 | loss: 96.9793131CurrentTrain: epoch  0, batch    51 | loss: 97.8810438CurrentTrain: epoch  0, batch    52 | loss: 96.9075258CurrentTrain: epoch  0, batch    53 | loss: 142.9276334CurrentTrain: epoch  0, batch    54 | loss: 116.0490714CurrentTrain: epoch  0, batch    55 | loss: 82.5294288CurrentTrain: epoch  0, batch    56 | loss: 83.6886005CurrentTrain: epoch  0, batch    57 | loss: 95.5699823CurrentTrain: epoch  0, batch    58 | loss: 94.7212865CurrentTrain: epoch  0, batch    59 | loss: 95.2854462CurrentTrain: epoch  0, batch    60 | loss: 93.6369534CurrentTrain: epoch  0, batch    61 | loss: 95.9668130CurrentTrain: epoch  0, batch    62 | loss: 83.0474007CurrentTrain: epoch  0, batch    63 | loss: 111.9088133CurrentTrain: epoch  0, batch    64 | loss: 95.4962377CurrentTrain: epoch  0, batch    65 | loss: 71.6006318CurrentTrain: epoch  0, batch    66 | loss: 91.0208193CurrentTrain: epoch  0, batch    67 | loss: 94.6509443CurrentTrain: epoch  0, batch    68 | loss: 82.4460175CurrentTrain: epoch  0, batch    69 | loss: 141.7931879CurrentTrain: epoch  0, batch    70 | loss: 117.3976795CurrentTrain: epoch  0, batch    71 | loss: 82.6429037CurrentTrain: epoch  0, batch    72 | loss: 96.9215286CurrentTrain: epoch  0, batch    73 | loss: 112.8935665CurrentTrain: epoch  0, batch    74 | loss: 142.3137850CurrentTrain: epoch  0, batch    75 | loss: 95.0093906CurrentTrain: epoch  0, batch    76 | loss: 110.2806831CurrentTrain: epoch  0, batch    77 | loss: 92.8881405CurrentTrain: epoch  0, batch    78 | loss: 112.6259929CurrentTrain: epoch  0, batch    79 | loss: 111.0747890CurrentTrain: epoch  0, batch    80 | loss: 79.2325549CurrentTrain: epoch  0, batch    81 | loss: 92.2656294CurrentTrain: epoch  0, batch    82 | loss: 111.3827890CurrentTrain: epoch  0, batch    83 | loss: 93.8524872CurrentTrain: epoch  0, batch    84 | loss: 107.6148636CurrentTrain: epoch  0, batch    85 | loss: 79.7293064CurrentTrain: epoch  0, batch    86 | loss: 137.4133676CurrentTrain: epoch  0, batch    87 | loss: 81.6693868CurrentTrain: epoch  0, batch    88 | loss: 80.0372843CurrentTrain: epoch  0, batch    89 | loss: 66.0136528CurrentTrain: epoch  0, batch    90 | loss: 93.0794947CurrentTrain: epoch  0, batch    91 | loss: 80.1684487CurrentTrain: epoch  0, batch    92 | loss: 90.4279159CurrentTrain: epoch  0, batch    93 | loss: 135.2179701CurrentTrain: epoch  0, batch    94 | loss: 80.2209610CurrentTrain: epoch  0, batch    95 | loss: 93.3847682CurrentTrain: epoch  1, batch     0 | loss: 79.2731747CurrentTrain: epoch  1, batch     1 | loss: 93.0305624CurrentTrain: epoch  1, batch     2 | loss: 78.2924860CurrentTrain: epoch  1, batch     3 | loss: 79.3196894CurrentTrain: epoch  1, batch     4 | loss: 88.3923489CurrentTrain: epoch  1, batch     5 | loss: 75.7498863CurrentTrain: epoch  1, batch     6 | loss: 94.6736979CurrentTrain: epoch  1, batch     7 | loss: 137.9167703CurrentTrain: epoch  1, batch     8 | loss: 69.9167885CurrentTrain: epoch  1, batch     9 | loss: 107.1252431CurrentTrain: epoch  1, batch    10 | loss: 90.3771372CurrentTrain: epoch  1, batch    11 | loss: 77.4832794CurrentTrain: epoch  1, batch    12 | loss: 75.7594618CurrentTrain: epoch  1, batch    13 | loss: 109.9826618CurrentTrain: epoch  1, batch    14 | loss: 105.4590849CurrentTrain: epoch  1, batch    15 | loss: 93.1283803CurrentTrain: epoch  1, batch    16 | loss: 135.2773398CurrentTrain: epoch  1, batch    17 | loss: 76.2209645CurrentTrain: epoch  1, batch    18 | loss: 86.6363578CurrentTrain: epoch  1, batch    19 | loss: 110.0455275CurrentTrain: epoch  1, batch    20 | loss: 140.5807215CurrentTrain: epoch  1, batch    21 | loss: 79.2359260CurrentTrain: epoch  1, batch    22 | loss: 75.2222260CurrentTrain: epoch  1, batch    23 | loss: 89.5570312CurrentTrain: epoch  1, batch    24 | loss: 135.0164095CurrentTrain: epoch  1, batch    25 | loss: 91.0068620CurrentTrain: epoch  1, batch    26 | loss: 65.5059333CurrentTrain: epoch  1, batch    27 | loss: 75.5025897CurrentTrain: epoch  1, batch    28 | loss: 65.5450608CurrentTrain: epoch  1, batch    29 | loss: 66.7410771CurrentTrain: epoch  1, batch    30 | loss: 92.2867289CurrentTrain: epoch  1, batch    31 | loss: 89.8156154CurrentTrain: epoch  1, batch    32 | loss: 110.5913209CurrentTrain: epoch  1, batch    33 | loss: 93.6476509CurrentTrain: epoch  1, batch    34 | loss: 102.9301412CurrentTrain: epoch  1, batch    35 | loss: 137.7852367CurrentTrain: epoch  1, batch    36 | loss: 77.8870520CurrentTrain: epoch  1, batch    37 | loss: 88.9539746CurrentTrain: epoch  1, batch    38 | loss: 93.0017128CurrentTrain: epoch  1, batch    39 | loss: 65.6789405CurrentTrain: epoch  1, batch    40 | loss: 72.7117838CurrentTrain: epoch  1, batch    41 | loss: 74.2114984CurrentTrain: epoch  1, batch    42 | loss: 80.7905707CurrentTrain: epoch  1, batch    43 | loss: 89.1067885CurrentTrain: epoch  1, batch    44 | loss: 135.1759597CurrentTrain: epoch  1, batch    45 | loss: 89.8770850CurrentTrain: epoch  1, batch    46 | loss: 109.1201833CurrentTrain: epoch  1, batch    47 | loss: 72.6396539CurrentTrain: epoch  1, batch    48 | loss: 90.9544480CurrentTrain: epoch  1, batch    49 | loss: 91.6906317CurrentTrain: epoch  1, batch    50 | loss: 88.0424145CurrentTrain: epoch  1, batch    51 | loss: 113.0778987CurrentTrain: epoch  1, batch    52 | loss: 111.5028630CurrentTrain: epoch  1, batch    53 | loss: 135.5812074CurrentTrain: epoch  1, batch    54 | loss: 78.8930255CurrentTrain: epoch  1, batch    55 | loss: 90.9670416CurrentTrain: epoch  1, batch    56 | loss: 88.6030334CurrentTrain: epoch  1, batch    57 | loss: 107.3992212CurrentTrain: epoch  1, batch    58 | loss: 91.3898529CurrentTrain: epoch  1, batch    59 | loss: 74.9852433CurrentTrain: epoch  1, batch    60 | loss: 87.2601028CurrentTrain: epoch  1, batch    61 | loss: 133.8989591CurrentTrain: epoch  1, batch    62 | loss: 140.6456160CurrentTrain: epoch  1, batch    63 | loss: 73.6870411CurrentTrain: epoch  1, batch    64 | loss: 88.9374005CurrentTrain: epoch  1, batch    65 | loss: 109.4748078CurrentTrain: epoch  1, batch    66 | loss: 66.5595051CurrentTrain: epoch  1, batch    67 | loss: 136.0609080CurrentTrain: epoch  1, batch    68 | loss: 72.8028531CurrentTrain: epoch  1, batch    69 | loss: 81.0227116CurrentTrain: epoch  1, batch    70 | loss: 72.4734712CurrentTrain: epoch  1, batch    71 | loss: 87.2175823CurrentTrain: epoch  1, batch    72 | loss: 108.0751850CurrentTrain: epoch  1, batch    73 | loss: 100.9566707CurrentTrain: epoch  1, batch    74 | loss: 105.3580134CurrentTrain: epoch  1, batch    75 | loss: 86.4147624CurrentTrain: epoch  1, batch    76 | loss: 92.6486245CurrentTrain: epoch  1, batch    77 | loss: 91.1797280CurrentTrain: epoch  1, batch    78 | loss: 86.0920149CurrentTrain: epoch  1, batch    79 | loss: 89.7242693CurrentTrain: epoch  1, batch    80 | loss: 67.1529600CurrentTrain: epoch  1, batch    81 | loss: 103.4795334CurrentTrain: epoch  1, batch    82 | loss: 75.6835877CurrentTrain: epoch  1, batch    83 | loss: 184.3876974CurrentTrain: epoch  1, batch    84 | loss: 106.1153472CurrentTrain: epoch  1, batch    85 | loss: 72.3428410CurrentTrain: epoch  1, batch    86 | loss: 66.0492672CurrentTrain: epoch  1, batch    87 | loss: 77.2135334CurrentTrain: epoch  1, batch    88 | loss: 109.0449549CurrentTrain: epoch  1, batch    89 | loss: 87.2057957CurrentTrain: epoch  1, batch    90 | loss: 75.9403251CurrentTrain: epoch  1, batch    91 | loss: 88.2388825CurrentTrain: epoch  1, batch    92 | loss: 88.8474997CurrentTrain: epoch  1, batch    93 | loss: 136.8480256CurrentTrain: epoch  1, batch    94 | loss: 92.4356289CurrentTrain: epoch  1, batch    95 | loss: 69.4548275CurrentTrain: epoch  2, batch     0 | loss: 63.4179919CurrentTrain: epoch  2, batch     1 | loss: 105.3412329CurrentTrain: epoch  2, batch     2 | loss: 88.3243163CurrentTrain: epoch  2, batch     3 | loss: 91.0623958CurrentTrain: epoch  2, batch     4 | loss: 137.3153779CurrentTrain: epoch  2, batch     5 | loss: 72.6298831CurrentTrain: epoch  2, batch     6 | loss: 130.1664093CurrentTrain: epoch  2, batch     7 | loss: 83.5210488CurrentTrain: epoch  2, batch     8 | loss: 107.4258483CurrentTrain: epoch  2, batch     9 | loss: 62.5708276CurrentTrain: epoch  2, batch    10 | loss: 101.7330300CurrentTrain: epoch  2, batch    11 | loss: 61.9300909CurrentTrain: epoch  2, batch    12 | loss: 65.8920099CurrentTrain: epoch  2, batch    13 | loss: 103.9706789CurrentTrain: epoch  2, batch    14 | loss: 107.3455627CurrentTrain: epoch  2, batch    15 | loss: 109.2078491CurrentTrain: epoch  2, batch    16 | loss: 82.9385119CurrentTrain: epoch  2, batch    17 | loss: 89.2849051CurrentTrain: epoch  2, batch    18 | loss: 85.0826932CurrentTrain: epoch  2, batch    19 | loss: 104.1198872CurrentTrain: epoch  2, batch    20 | loss: 74.7663970CurrentTrain: epoch  2, batch    21 | loss: 103.3830726CurrentTrain: epoch  2, batch    22 | loss: 75.5686216CurrentTrain: epoch  2, batch    23 | loss: 85.4081056CurrentTrain: epoch  2, batch    24 | loss: 83.1861537CurrentTrain: epoch  2, batch    25 | loss: 84.1530047CurrentTrain: epoch  2, batch    26 | loss: 88.6894539CurrentTrain: epoch  2, batch    27 | loss: 80.8255949CurrentTrain: epoch  2, batch    28 | loss: 106.1210026CurrentTrain: epoch  2, batch    29 | loss: 102.5406908CurrentTrain: epoch  2, batch    30 | loss: 88.8561999CurrentTrain: epoch  2, batch    31 | loss: 85.4906884CurrentTrain: epoch  2, batch    32 | loss: 77.2799920CurrentTrain: epoch  2, batch    33 | loss: 109.4484495CurrentTrain: epoch  2, batch    34 | loss: 106.6219284CurrentTrain: epoch  2, batch    35 | loss: 103.6409224CurrentTrain: epoch  2, batch    36 | loss: 87.7884528CurrentTrain: epoch  2, batch    37 | loss: 102.3755579CurrentTrain: epoch  2, batch    38 | loss: 73.4799887CurrentTrain: epoch  2, batch    39 | loss: 106.1769631CurrentTrain: epoch  2, batch    40 | loss: 61.8865371CurrentTrain: epoch  2, batch    41 | loss: 108.1907111CurrentTrain: epoch  2, batch    42 | loss: 103.3865278CurrentTrain: epoch  2, batch    43 | loss: 75.6592408CurrentTrain: epoch  2, batch    44 | loss: 84.4646692CurrentTrain: epoch  2, batch    45 | loss: 74.6728297CurrentTrain: epoch  2, batch    46 | loss: 89.0068021CurrentTrain: epoch  2, batch    47 | loss: 76.2878906CurrentTrain: epoch  2, batch    48 | loss: 133.6648888CurrentTrain: epoch  2, batch    49 | loss: 134.4517165CurrentTrain: epoch  2, batch    50 | loss: 70.4502967CurrentTrain: epoch  2, batch    51 | loss: 137.5350594CurrentTrain: epoch  2, batch    52 | loss: 73.9044927CurrentTrain: epoch  2, batch    53 | loss: 89.7699520CurrentTrain: epoch  2, batch    54 | loss: 107.3909924CurrentTrain: epoch  2, batch    55 | loss: 86.7622580CurrentTrain: epoch  2, batch    56 | loss: 84.9023193CurrentTrain: epoch  2, batch    57 | loss: 70.3326004CurrentTrain: epoch  2, batch    58 | loss: 76.3343635CurrentTrain: epoch  2, batch    59 | loss: 73.5174947CurrentTrain: epoch  2, batch    60 | loss: 86.2920110CurrentTrain: epoch  2, batch    61 | loss: 69.7017907CurrentTrain: epoch  2, batch    62 | loss: 84.0696971CurrentTrain: epoch  2, batch    63 | loss: 71.4451823CurrentTrain: epoch  2, batch    64 | loss: 70.5004314CurrentTrain: epoch  2, batch    65 | loss: 85.4816932CurrentTrain: epoch  2, batch    66 | loss: 136.1844781CurrentTrain: epoch  2, batch    67 | loss: 106.6325354CurrentTrain: epoch  2, batch    68 | loss: 72.3551157CurrentTrain: epoch  2, batch    69 | loss: 70.4267502CurrentTrain: epoch  2, batch    70 | loss: 89.8599383CurrentTrain: epoch  2, batch    71 | loss: 72.9918836CurrentTrain: epoch  2, batch    72 | loss: 183.4414795CurrentTrain: epoch  2, batch    73 | loss: 86.6637856CurrentTrain: epoch  2, batch    74 | loss: 84.8372033CurrentTrain: epoch  2, batch    75 | loss: 84.5903745CurrentTrain: epoch  2, batch    76 | loss: 103.2660769CurrentTrain: epoch  2, batch    77 | loss: 63.5318322CurrentTrain: epoch  2, batch    78 | loss: 89.7297141CurrentTrain: epoch  2, batch    79 | loss: 84.7980599CurrentTrain: epoch  2, batch    80 | loss: 70.9325428CurrentTrain: epoch  2, batch    81 | loss: 87.9199839CurrentTrain: epoch  2, batch    82 | loss: 107.5052893CurrentTrain: epoch  2, batch    83 | loss: 107.0566054CurrentTrain: epoch  2, batch    84 | loss: 74.9873386CurrentTrain: epoch  2, batch    85 | loss: 135.4106780CurrentTrain: epoch  2, batch    86 | loss: 62.3271610CurrentTrain: epoch  2, batch    87 | loss: 63.3485352CurrentTrain: epoch  2, batch    88 | loss: 65.4891245CurrentTrain: epoch  2, batch    89 | loss: 100.9759242CurrentTrain: epoch  2, batch    90 | loss: 85.8036974CurrentTrain: epoch  2, batch    91 | loss: 75.8567062CurrentTrain: epoch  2, batch    92 | loss: 74.4626134CurrentTrain: epoch  2, batch    93 | loss: 87.9881775CurrentTrain: epoch  2, batch    94 | loss: 86.3258672CurrentTrain: epoch  2, batch    95 | loss: 72.6346423CurrentTrain: epoch  3, batch     0 | loss: 85.4345908CurrentTrain: epoch  3, batch     1 | loss: 102.4067955CurrentTrain: epoch  3, batch     2 | loss: 86.6896994CurrentTrain: epoch  3, batch     3 | loss: 89.0392953CurrentTrain: epoch  3, batch     4 | loss: 82.2052503CurrentTrain: epoch  3, batch     5 | loss: 100.2888207CurrentTrain: epoch  3, batch     6 | loss: 81.8916888CurrentTrain: epoch  3, batch     7 | loss: 62.7705242CurrentTrain: epoch  3, batch     8 | loss: 73.7004653CurrentTrain: epoch  3, batch     9 | loss: 126.1661371CurrentTrain: epoch  3, batch    10 | loss: 75.3918465CurrentTrain: epoch  3, batch    11 | loss: 81.2288000CurrentTrain: epoch  3, batch    12 | loss: 104.1574157