#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 425.2052931CurrentTrain: epoch  0, batch     1 | loss: 328.2788324CurrentTrain: epoch  0, batch     2 | loss: 318.7701013CurrentTrain: epoch  0, batch     3 | loss: 323.8244292CurrentTrain: epoch  0, batch     4 | loss: 337.3337573CurrentTrain: epoch  0, batch     5 | loss: 310.9351895CurrentTrain: epoch  0, batch     6 | loss: 378.8255536CurrentTrain: epoch  0, batch     7 | loss: 376.7739787CurrentTrain: epoch  0, batch     8 | loss: 310.7953375CurrentTrain: epoch  0, batch     9 | loss: 311.8241101CurrentTrain: epoch  0, batch    10 | loss: 235.8362789CurrentTrain: epoch  0, batch    11 | loss: 375.7288266CurrentTrain: epoch  0, batch    12 | loss: 337.1124071CurrentTrain: epoch  0, batch    13 | loss: 532.5268395CurrentTrain: epoch  0, batch    14 | loss: 359.5673968CurrentTrain: epoch  0, batch    15 | loss: 359.9207845CurrentTrain: epoch  0, batch    16 | loss: 319.3118732CurrentTrain: epoch  0, batch    17 | loss: 419.2464782CurrentTrain: epoch  0, batch    18 | loss: 345.9937526CurrentTrain: epoch  0, batch    19 | loss: 331.8639801CurrentTrain: epoch  0, batch    20 | loss: 468.7983555CurrentTrain: epoch  0, batch    21 | loss: 452.2567947CurrentTrain: epoch  0, batch    22 | loss: 544.0954121CurrentTrain: epoch  0, batch    23 | loss: 437.0141370CurrentTrain: epoch  0, batch    24 | loss: 345.3455292CurrentTrain: epoch  0, batch    25 | loss: 529.7332407CurrentTrain: epoch  0, batch    26 | loss: 320.0898678CurrentTrain: epoch  0, batch    27 | loss: 408.5067426CurrentTrain: epoch  0, batch    28 | loss: 437.5279502CurrentTrain: epoch  0, batch    29 | loss: 311.1625685CurrentTrain: epoch  0, batch    30 | loss: 465.4374283CurrentTrain: epoch  0, batch    31 | loss: 436.8385098CurrentTrain: epoch  0, batch    32 | loss: 404.0467984CurrentTrain: epoch  0, batch    33 | loss: 318.4961941CurrentTrain: epoch  0, batch    34 | loss: 451.1645999CurrentTrain: epoch  0, batch    35 | loss: 359.2364049CurrentTrain: epoch  0, batch    36 | loss: 345.9598853CurrentTrain: epoch  0, batch    37 | loss: 357.4139440CurrentTrain: epoch  0, batch    38 | loss: 343.5910564CurrentTrain: epoch  0, batch    39 | loss: 330.4029888CurrentTrain: epoch  0, batch    40 | loss: 418.9049504CurrentTrain: epoch  0, batch    41 | loss: 343.8347649CurrentTrain: epoch  0, batch    42 | loss: 330.4146778CurrentTrain: epoch  0, batch    43 | loss: 261.1657840CurrentTrain: epoch  0, batch    44 | loss: 356.9992300CurrentTrain: epoch  0, batch    45 | loss: 281.0386582CurrentTrain: epoch  0, batch    46 | loss: 389.0367202CurrentTrain: epoch  0, batch    47 | loss: 317.8395743CurrentTrain: epoch  0, batch    48 | loss: 385.1991904CurrentTrain: epoch  0, batch    49 | loss: 343.5663780CurrentTrain: epoch  0, batch    50 | loss: 383.7554193CurrentTrain: epoch  0, batch    51 | loss: 344.1942096CurrentTrain: epoch  0, batch    52 | loss: 384.7016304CurrentTrain: epoch  0, batch    53 | loss: 451.5892509CurrentTrain: epoch  0, batch    54 | loss: 419.3823467CurrentTrain: epoch  0, batch    55 | loss: 282.8862531CurrentTrain: epoch  0, batch    56 | loss: 330.3338599CurrentTrain: epoch  0, batch    57 | loss: 308.1809261CurrentTrain: epoch  0, batch    58 | loss: 304.8722746CurrentTrain: epoch  0, batch    59 | loss: 355.4214732CurrentTrain: epoch  0, batch    60 | loss: 307.5013009CurrentTrain: epoch  0, batch    61 | loss: 355.8494750CurrentTrain: epoch  0, batch    62 | loss: 329.0888251CurrentTrain: epoch  0, batch    63 | loss: 364.2033974CurrentTrain: epoch  0, batch    64 | loss: 343.4188676CurrentTrain: epoch  0, batch    65 | loss: 281.6357684CurrentTrain: epoch  0, batch    66 | loss: 328.6372492CurrentTrain: epoch  0, batch    67 | loss: 355.9282376CurrentTrain: epoch  0, batch    68 | loss: 329.9273698CurrentTrain: epoch  0, batch    69 | loss: 464.8392165CurrentTrain: epoch  0, batch    70 | loss: 419.1563706CurrentTrain: epoch  0, batch    71 | loss: 316.1617927CurrentTrain: epoch  0, batch    72 | loss: 370.0788896CurrentTrain: epoch  0, batch    73 | loss: 403.2863521CurrentTrain: epoch  0, batch    74 | loss: 436.0611580CurrentTrain: epoch  0, batch    75 | loss: 385.0711263CurrentTrain: epoch  0, batch    76 | loss: 387.1414742CurrentTrain: epoch  0, batch    77 | loss: 354.3093101CurrentTrain: epoch  0, batch    78 | loss: 402.0612711CurrentTrain: epoch  0, batch    79 | loss: 388.9691140CurrentTrain: epoch  0, batch    80 | loss: 327.9941052CurrentTrain: epoch  0, batch    81 | loss: 369.9324188CurrentTrain: epoch  0, batch    82 | loss: 402.4549839CurrentTrain: epoch  0, batch    83 | loss: 328.5876762CurrentTrain: epoch  0, batch    84 | loss: 373.2074339CurrentTrain: epoch  0, batch    85 | loss: 280.8129219CurrentTrain: epoch  0, batch    86 | loss: 404.7522352CurrentTrain: epoch  0, batch    87 | loss: 341.3037971CurrentTrain: epoch  0, batch    88 | loss: 343.0891780CurrentTrain: epoch  0, batch    89 | loss: 269.7652529CurrentTrain: epoch  0, batch    90 | loss: 369.6763912CurrentTrain: epoch  0, batch    91 | loss: 289.7841067CurrentTrain: epoch  0, batch    92 | loss: 315.5196867CurrentTrain: epoch  0, batch    93 | loss: 405.4887209CurrentTrain: epoch  0, batch    94 | loss: 327.7462935CurrentTrain: epoch  0, batch    95 | loss: 333.9830497CurrentTrain: epoch  1, batch     0 | loss: 326.5583253CurrentTrain: epoch  1, batch     1 | loss: 368.8738953CurrentTrain: epoch  1, batch     2 | loss: 341.9756037CurrentTrain: epoch  1, batch     3 | loss: 328.6545830CurrentTrain: epoch  1, batch     4 | loss: 326.4003897CurrentTrain: epoch  1, batch     5 | loss: 288.3176658CurrentTrain: epoch  1, batch     6 | loss: 368.9940052CurrentTrain: epoch  1, batch     7 | loss: 431.6174623CurrentTrain: epoch  1, batch     8 | loss: 330.6995754CurrentTrain: epoch  1, batch     9 | loss: 416.7326171CurrentTrain: epoch  1, batch    10 | loss: 325.6753453CurrentTrain: epoch  1, batch    11 | loss: 313.8004121CurrentTrain: epoch  1, batch    12 | loss: 287.7279651CurrentTrain: epoch  1, batch    13 | loss: 387.7243703CurrentTrain: epoch  1, batch    14 | loss: 344.3467475CurrentTrain: epoch  1, batch    15 | loss: 369.0750713CurrentTrain: epoch  1, batch    16 | loss: 447.3693065CurrentTrain: epoch  1, batch    17 | loss: 287.6946398CurrentTrain: epoch  1, batch    18 | loss: 311.9428264CurrentTrain: epoch  1, batch    19 | loss: 401.5404749CurrentTrain: epoch  1, batch    20 | loss: 462.6698382CurrentTrain: epoch  1, batch    21 | loss: 325.0978406CurrentTrain: epoch  1, batch    22 | loss: 326.2711276CurrentTrain: epoch  1, batch    23 | loss: 351.7321156CurrentTrain: epoch  1, batch    24 | loss: 431.5851924CurrentTrain: epoch  1, batch    25 | loss: 353.5851978CurrentTrain: epoch  1, batch    26 | loss: 290.7577898CurrentTrain: epoch  1, batch    27 | loss: 325.0779085CurrentTrain: epoch  1, batch    28 | loss: 281.1934888CurrentTrain: epoch  1, batch    29 | loss: 331.8088693CurrentTrain: epoch  1, batch    30 | loss: 381.4171194CurrentTrain: epoch  1, batch    31 | loss: 326.1099787CurrentTrain: epoch  1, batch    32 | loss: 415.1743149CurrentTrain: epoch  1, batch    33 | loss: 366.7676903CurrentTrain: epoch  1, batch    34 | loss: 326.6005207CurrentTrain: epoch  1, batch    35 | loss: 463.0381173CurrentTrain: epoch  1, batch    36 | loss: 353.4740330CurrentTrain: epoch  1, batch    37 | loss: 350.1040478CurrentTrain: epoch  1, batch    38 | loss: 338.1620663CurrentTrain: epoch  1, batch    39 | loss: 301.6386360CurrentTrain: epoch  1, batch    40 | loss: 261.7301632CurrentTrain: epoch  1, batch    41 | loss: 299.6522897CurrentTrain: epoch  1, batch    42 | loss: 353.9951187CurrentTrain: epoch  1, batch    43 | loss: 366.5751991CurrentTrain: epoch  1, batch    44 | loss: 434.9122814CurrentTrain: epoch  1, batch    45 | loss: 350.5552313CurrentTrain: epoch  1, batch    46 | loss: 415.7807403CurrentTrain: epoch  1, batch    47 | loss: 296.8481614CurrentTrain: epoch  1, batch    48 | loss: 364.5642022CurrentTrain: epoch  1, batch    49 | loss: 350.2679484CurrentTrain: epoch  1, batch    50 | loss: 337.9576457CurrentTrain: epoch  1, batch    51 | loss: 416.6916353CurrentTrain: epoch  1, batch    52 | loss: 402.0108740CurrentTrain: epoch  1, batch    53 | loss: 429.8664279CurrentTrain: epoch  1, batch    54 | loss: 338.9265902CurrentTrain: epoch  1, batch    55 | loss: 363.7724763CurrentTrain: epoch  1, batch    56 | loss: 337.1564655CurrentTrain: epoch  1, batch    57 | loss: 397.9998615CurrentTrain: epoch  1, batch    58 | loss: 353.1006221CurrentTrain: epoch  1, batch    59 | loss: 284.3715314CurrentTrain: epoch  1, batch    60 | loss: 325.3069564CurrentTrain: epoch  1, batch    61 | loss: 457.9218259CurrentTrain: epoch  1, batch    62 | loss: 463.0247302CurrentTrain: epoch  1, batch    63 | loss: 312.7554132CurrentTrain: epoch  1, batch    64 | loss: 322.2590778CurrentTrain: epoch  1, batch    65 | loss: 396.8045235CurrentTrain: epoch  1, batch    66 | loss: 299.3482437CurrentTrain: epoch  1, batch    67 | loss: 446.3635992CurrentTrain: epoch  1, batch    68 | loss: 282.7456607CurrentTrain: epoch  1, batch    69 | loss: 341.3824605CurrentTrain: epoch  1, batch    70 | loss: 282.0310748CurrentTrain: epoch  1, batch    71 | loss: 349.7756181CurrentTrain: epoch  1, batch    72 | loss: 399.2662945CurrentTrain: epoch  1, batch    73 | loss: 340.1554352CurrentTrain: epoch  1, batch    74 | loss: 367.2722808CurrentTrain: epoch  1, batch    75 | loss: 333.5572150CurrentTrain: epoch  1, batch    76 | loss: 378.9101953CurrentTrain: epoch  1, batch    77 | loss: 368.2451871CurrentTrain: epoch  1, batch    78 | loss: 322.7440875CurrentTrain: epoch  1, batch    79 | loss: 321.4109403CurrentTrain: epoch  1, batch    80 | loss: 273.2391805CurrentTrain: epoch  1, batch    81 | loss: 353.0533653CurrentTrain: epoch  1, batch    82 | loss: 269.2054028CurrentTrain: epoch  1, batch    83 | loss: 543.2411438CurrentTrain: epoch  1, batch    84 | loss: 370.1785036CurrentTrain: epoch  1, batch    85 | loss: 320.2585866CurrentTrain: epoch  1, batch    86 | loss: 284.7423049CurrentTrain: epoch  1, batch    87 | loss: 335.9769522CurrentTrain: epoch  1, batch    88 | loss: 398.6698559CurrentTrain: epoch  1, batch    89 | loss: 381.2531674CurrentTrain: epoch  1, batch    90 | loss: 281.5203362CurrentTrain: epoch  1, batch    91 | loss: 378.3769542CurrentTrain: epoch  1, batch    92 | loss: 335.1685051CurrentTrain: epoch  1, batch    93 | loss: 461.7329652CurrentTrain: epoch  1, batch    94 | loss: 336.4610708CurrentTrain: epoch  1, batch    95 | loss: 256.3851260CurrentTrain: epoch  2, batch     0 | loss: 297.9842023CurrentTrain: epoch  2, batch     1 | loss: 399.9503346CurrentTrain: epoch  2, batch     2 | loss: 347.6543113CurrentTrain: epoch  2, batch     3 | loss: 362.5301323CurrentTrain: epoch  2, batch     4 | loss: 427.3488113CurrentTrain: epoch  2, batch     5 | loss: 293.7830906CurrentTrain: epoch  2, batch     6 | loss: 432.3993465CurrentTrain: epoch  2, batch     7 | loss: 307.3105691CurrentTrain: epoch  2, batch     8 | loss: 367.4636407CurrentTrain: epoch  2, batch     9 | loss: 271.5338167CurrentTrain: epoch  2, batch    10 | loss: 365.5773813CurrentTrain: epoch  2, batch    11 | loss: 259.6699812CurrentTrain: epoch  2, batch    12 | loss: 270.7508888CurrentTrain: epoch  2, batch    13 | loss: 396.0519371CurrentTrain: epoch  2, batch    14 | loss: 396.3151638CurrentTrain: epoch  2, batch    15 | loss: 395.4921402CurrentTrain: epoch  2, batch    16 | loss: 347.3030674CurrentTrain: epoch  2, batch    17 | loss: 358.7720684CurrentTrain: epoch  2, batch    18 | loss: 318.2643613CurrentTrain: epoch  2, batch    19 | loss: 367.9610082CurrentTrain: epoch  2, batch    20 | loss: 293.5519853CurrentTrain: epoch  2, batch    21 | loss: 363.0859765CurrentTrain: epoch  2, batch    22 | loss: 318.8603842CurrentTrain: epoch  2, batch    23 | loss: 345.3836806CurrentTrain: epoch  2, batch    24 | loss: 327.4857204CurrentTrain: epoch  2, batch    25 | loss: 303.9000319CurrentTrain: epoch  2, batch    26 | loss: 351.1547664CurrentTrain: epoch  2, batch    27 | loss: 305.6032141CurrentTrain: epoch  2, batch    28 | loss: 395.1679958CurrentTrain: epoch  2, batch    29 | loss: 382.3560410CurrentTrain: epoch  2, batch    30 | loss: 363.7199616CurrentTrain: epoch  2, batch    31 | loss: 330.1594659CurrentTrain: epoch  2, batch    32 | loss: 351.3043070CurrentTrain: epoch  2, batch    33 | loss: 420.6213612CurrentTrain: epoch  2, batch    34 | loss: 410.7733310CurrentTrain: epoch  2, batch    35 | loss: 367.8244273CurrentTrain: epoch  2, batch    36 | loss: 346.6003287CurrentTrain: epoch  2, batch    37 | loss: 362.1994362CurrentTrain: epoch  2, batch    38 | loss: 320.6798999CurrentTrain: epoch  2, batch    39 | loss: 395.0046812CurrentTrain: epoch  2, batch    40 | loss: 269.8373484CurrentTrain: epoch  2, batch    41 | loss: 411.5887528CurrentTrain: epoch  2, batch    42 | loss: 363.2197040CurrentTrain: epoch  2, batch    43 | loss: 307.8859241CurrentTrain: epoch  2, batch    44 | loss: 331.4192510CurrentTrain: epoch  2, batch    45 | loss: 322.5484890CurrentTrain: epoch  2, batch    46 | loss: 375.0422067CurrentTrain: epoch  2, batch    47 | loss: 308.2115318CurrentTrain: epoch  2, batch    48 | loss: 456.7149186CurrentTrain: epoch  2, batch    49 | loss: 446.0848342CurrentTrain: epoch  2, batch    50 | loss: 279.6434040CurrentTrain: epoch  2, batch    51 | loss: 462.2611146CurrentTrain: epoch  2, batch    52 | loss: 333.2535636CurrentTrain: epoch  2, batch    53 | loss: 361.8190248CurrentTrain: epoch  2, batch    54 | loss: 392.3474165CurrentTrain: epoch  2, batch    55 | loss: 358.9290515CurrentTrain: epoch  2, batch    56 | loss: 316.6239949CurrentTrain: epoch  2, batch    57 | loss: 279.9249028CurrentTrain: epoch  2, batch    58 | loss: 335.3957739CurrentTrain: epoch  2, batch    59 | loss: 309.6412109CurrentTrain: epoch  2, batch    60 | loss: 320.8662554CurrentTrain: epoch  2, batch    61 | loss: 288.1174282CurrentTrain: epoch  2, batch    62 | loss: 358.0114882CurrentTrain: epoch  2, batch    63 | loss: 292.4888164CurrentTrain: epoch  2, batch    64 | loss: 319.5301351CurrentTrain: epoch  2, batch    65 | loss: 329.8221278CurrentTrain: epoch  2, batch    66 | loss: 457.5285236CurrentTrain: epoch  2, batch    67 | loss: 384.0045189CurrentTrain: epoch  2, batch    68 | loss: 320.8446181CurrentTrain: epoch  2, batch    69 | loss: 288.7704558CurrentTrain: epoch  2, batch    70 | loss: 361.4589656CurrentTrain: epoch  2, batch    71 | loss: 318.6574432CurrentTrain: epoch  2, batch    72 | loss: 537.8192140CurrentTrain: epoch  2, batch    73 | loss: 348.3992055CurrentTrain: epoch  2, batch    74 | loss: 343.8323849CurrentTrain: epoch  2, batch    75 | loss: 319.3560531CurrentTrain: epoch  2, batch    76 | loss: 378.6254691CurrentTrain: epoch  2, batch    77 | loss: 281.4635853CurrentTrain: epoch  2, batch    78 | loss: 345.2048543CurrentTrain: epoch  2, batch    79 | loss: 344.8869283CurrentTrain: epoch  2, batch    80 | loss: 302.8055408CurrentTrain: epoch  2, batch    81 | loss: 358.6681253CurrentTrain: epoch  2, batch    82 | loss: 382.7931459CurrentTrain: epoch  2, batch    83 | loss: 411.3566699CurrentTrain: epoch  2, batch    84 | loss: 294.1310367CurrentTrain: epoch  2, batch    85 | loss: 410.7965897CurrentTrain: epoch  2, batch    86 | loss: 283.5339195CurrentTrain: epoch  2, batch    87 | loss: 296.1082558CurrentTrain: epoch  2, batch    88 | loss: 326.4047266CurrentTrain: epoch  2, batch    89 | loss: 322.0706367CurrentTrain: epoch  2, batch    90 | loss: 374.8306791CurrentTrain: epoch  2, batch    91 | loss: 351.6638373CurrentTrain: epoch  2, batch    92 | loss: 337.9472216CurrentTrain: epoch  2, batch    93 | loss: 347.8920892CurrentTrain: epoch  2, batch    94 | loss: 345.3110260CurrentTrain: epoch  2, batch    95 | loss: 255.7851592CurrentTrain: epoch  3, batch     0 | loss: 359.2507598CurrentTrain: epoch  3, batch     1 | loss: 363.5929900CurrentTrain: epoch  3, batch     2 | loss: 362.5455754CurrentTrain: epoch  3, batch     3 | loss: 359.7931088CurrentTrain: epoch  3, batch     4 | loss: 343.9859280CurrentTrain: epoch  3, batch     5 | loss: 378.0083565CurrentTrain: epoch  3, batch     6 | loss: 314.6038157CurrentTrain: epoch  3, batch     7 | loss: 298.6738977CurrentTrain: epoch  3, batch     8 | loss: 319.3180374CurrentTrain: epoch  3, batch     9 | loss: 407.5965085CurrentTrain: epoch  3, batch    10 | loss: 347.2189980CurrentTrain: epoch  3, batch    11 | loss: 326.6026436CurrentTrain: epoch  3, batch    12 | loss: 376.1294186CurrentTrain: epoch  3, batch    13 | loss: 375.5246241CurrentTrain: epoch  3, batch    14 | loss: 360.2075999CurrentTrain: epoch  3, batch    15 | loss: 379.5462514CurrentTrain: epoch  3, batch    16 | loss: 365.1574706CurrentTrain: epoch  3, batch    17 | loss: 288.5956926CurrentTrain: epoch  3, batch    18 | loss: 303.7599632CurrentTrain: epoch  3, batch    19 | loss: 345.8251181CurrentTrain: epoch  3, batch    20 | loss: 379.4756196CurrentTrain: epoch  3, batch    21 | loss: 315.3411598CurrentTrain: epoch  3, batch    22 | loss: 374.8484770CurrentTrain: epoch  3, batch    23 | loss: 329.2127400CurrentTrain: epoch  3, batch    24 | loss: 317.3481092CurrentTrain: epoch  3, batch    25 | loss: 346.2410084CurrentTrain: epoch  3, batch    26 | loss: 393.9290268CurrentTrain: epoch  3, batch    27 | loss: 413.4254102CurrentTrain: epoch  3, batch    28 | loss: 375.0946649CurrentTrain: epoch  3, batch    29 | loss: 332.0115710CurrentTrain: epoch  3, batch    30 | loss: 376.6615750CurrentTrain: epoch  3, batch    31 | loss: 325.5166010CurrentTrain: epoch  3, batch    32 | loss: 330.4352969CurrentTrain: epoch  3, batch    33 | loss: 438.4900371CurrentTrain: epoch  3, batch    34 | loss: 362.5924761CurrentTrain: epoch  3, batch    35 | loss: 422.2108699CurrentTrain: epoch  3, batch    36 | loss: 266.0534058CurrentTrain: epoch  3, batch    37 | loss: 424.1892527CurrentTrain: epoch  3, batch    38 | loss: 248.7234190CurrentTrain: epoch  3, batch    39 | loss: 344.2906716CurrentTrain: epoch  3, batch    40 | loss: 328.7841168CurrentTrain: epoch  3, batch    41 | loss: 300.1001297CurrentTrain: epoch  3, batch    42 | loss: 328.4353462CurrentTrain: epoch  3, batch    43 | loss: 287.5036643CurrentTrain: epoch  3, batch    44 | loss: 311.0468991CurrentTrain: epoch  3, batch    45 | loss: 333.0282726CurrentTrain: epoch  3, batch    46 | loss: 392.3375256CurrentTrain: epoch  3, batch    47 | loss: 358.8351910CurrentTrain: epoch  3, batch    48 | loss: 440.6582550CurrentTrain: epoch  3, batch    49 | loss: 310.4955501CurrentTrain: epoch  3, batch    50 | loss: 346.5998018CurrentTrain: epoch  3, batch    51 | loss: 332.2835007CurrentTrain: epoch  3, batch    52 | loss: 441.3974920CurrentTrain: epoch  3, batch    53 | loss: 318.8556913CurrentTrain: epoch  3, batch    54 | loss: 390.3429990CurrentTrain: epoch  3, batch    55 | loss: 315.7700414CurrentTrain: epoch  3, batch    56 | loss: 340.4842081CurrentTrain: epoch  3, batch    57 | loss: 336.4335668CurrentTrain: epoch  3, batch    58 | loss: 347.9043113CurrentTrain: epoch  3, batch    59 | loss: 420.7676778CurrentTrain: epoch  3, batch    60 | loss: 358.3889247CurrentTrain: epoch  3, batch    61 | loss: 307.4379618CurrentTrain: epoch  3, batch    62 | loss: 301.0888258CurrentTrain: epoch  3, batch    63 | loss: 351.0675335CurrentTrain: epoch  3, batch    64 | loss: 302.8551441CurrentTrain: epoch  3, batch    65 | loss: 379.8832541CurrentTrain: epoch  3, batch    66 | loss: 393.0000380CurrentTrain: epoch  3, batch    67 | loss: 344.7354306CurrentTrain: epoch  3, batch    68 | loss: 344.1776173CurrentTrain: epoch  3, batch    69 | loss: 317.7517006CurrentTrain: epoch  3, batch    70 | loss: 345.8269250CurrentTrain: epoch  3, batch    71 | loss: 244.1550713CurrentTrain: epoch  3, batch    72 | loss: 346.4155652CurrentTrain: epoch  3, batch    73 | loss: 243.2779941CurrentTrain: epoch  3, batch    74 | loss: 360.0814463CurrentTrain: epoch  3, batch    75 | loss: 392.7146043CurrentTrain: epoch  3, batch    76 | loss: 344.4907328CurrentTrain: epoch  3, batch    77 | loss: 361.9237672CurrentTrain: epoch  3, batch    78 | loss: 404.4336261CurrentTrain: epoch  3, batch    79 | loss: 301.0171378CurrentTrain: epoch  3, batch    80 | loss: 364.8669232CurrentTrain: epoch  3, batch    81 | loss: 244.4492565CurrentTrain: epoch  3, batch    82 | loss: 414.0299334CurrentTrain: epoch  3, batch    83 | loss: 318.0948504CurrentTrain: epoch  3, batch    84 | loss: 316.7459333CurrentTrain: epoch  3, batch    85 | loss: 380.6762076CurrentTrain: epoch  3, batch    86 | loss: 333.3821855CurrentTrain: epoch  3, batch    87 | loss: 375.9591944CurrentTrain: epoch  3, batch    88 | loss: 287.6200959CurrentTrain: epoch  3, batch    89 | loss: 277.3490066CurrentTrain: epoch  3, batch    90 | loss: 347.3596990CurrentTrain: epoch  3, batch    91 | loss: 365.3071185CurrentTrain: epoch  3, batch    92 | loss: 318.9696051CurrentTrain: epoch  3, batch    93 | loss: 346.6882586CurrentTrain: epoch  3, batch    94 | loss: 409.8031118CurrentTrain: epoch  3, batch    95 | loss: 225.2716092CurrentTrain: epoch  4, batch     0 | loss: 329.0489269CurrentTrain: epoch  4, batch     1 | loss: 408.8241882CurrentTrain: epoch  4, batch     2 | loss: 359.0400413CurrentTrain: epoch  4, batch     3 | loss: 296.7793620CurrentTrain: epoch  4, batch     4 | loss: 459.4476953CurrentTrain: epoch  4, batch     5 | loss: 359.7625653CurrentTrain: epoch  4, batch     6 | loss: 329.9589322CurrentTrain: epoch  4, batch     7 | loss: 272.1030646CurrentTrain: epoch  4, batch     8 | loss: 408.6583145CurrentTrain: epoch  4, batch     9 | loss: 426.4066989CurrentTrain: epoch  4, batch    10 | loss: 408.1761019CurrentTrain: epoch  4, batch    11 | loss: 331.1479815CurrentTrain: epoch  4, batch    12 | loss: 332.3029118CurrentTrain: epoch  4, batch    13 | loss: 304.9985943CurrentTrain: epoch  4, batch    14 | loss: 297.2777848CurrentTrain: epoch  4, batch    15 | loss: 391.7777641CurrentTrain: epoch  4, batch    16 | loss: 359.3937377CurrentTrain: epoch  4, batch    17 | loss: 456.6302238CurrentTrain: epoch  4, batch    18 | loss: 329.8550709CurrentTrain: epoch  4, batch    19 | loss: 327.9020030CurrentTrain: epoch  4, batch    20 | loss: 326.7791058CurrentTrain: epoch  4, batch    21 | loss: 348.1489513CurrentTrain: epoch  4, batch    22 | loss: 393.1113345CurrentTrain: epoch  4, batch    23 | loss: 286.0512884CurrentTrain: epoch  4, batch    24 | loss: 343.0966145CurrentTrain: epoch  4, batch    25 | loss: 345.9732764CurrentTrain: epoch  4, batch    26 | loss: 357.6452093CurrentTrain: epoch  4, batch    27 | loss: 322.7829900CurrentTrain: epoch  4, batch    28 | loss: 376.6696038CurrentTrain: epoch  4, batch    29 | loss: 327.9277690CurrentTrain: epoch  4, batch    30 | loss: 333.2541147CurrentTrain: epoch  4, batch    31 | loss: 315.7869148CurrentTrain: epoch  4, batch    32 | loss: 315.4034198CurrentTrain: epoch  4, batch    33 | loss: 263.8044934CurrentTrain: epoch  4, batch    34 | loss: 375.5121972CurrentTrain: epoch  4, batch    35 | loss: 373.7952895CurrentTrain: epoch  4, batch    36 | loss: 328.8078193CurrentTrain: epoch  4, batch    37 | loss: 284.2220912CurrentTrain: epoch  4, batch    38 | loss: 311.5160852CurrentTrain: epoch  4, batch    39 | loss: 333.2640506CurrentTrain: epoch  4, batch    40 | loss: 343.9184944CurrentTrain: epoch  4, batch    41 | loss: 316.1796287CurrentTrain: epoch  4, batch    42 | loss: 360.5264623CurrentTrain: epoch  4, batch    43 | loss: 267.1659465CurrentTrain: epoch  4, batch    44 | loss: 285.8009218CurrentTrain: epoch  4, batch    45 | loss: 358.0399201CurrentTrain: epoch  4, batch    46 | loss: 334.7573316CurrentTrain: epoch  4, batch    47 | loss: 347.5633838CurrentTrain: epoch  4, batch    48 | loss: 346.5923226CurrentTrain: epoch  4, batch    49 | loss: 345.0957543CurrentTrain: epoch  4, batch    50 | loss: 314.8810039CurrentTrain: epoch  4, batch    51 | loss: 332.9125775CurrentTrain: epoch  4, batch    52 | loss: 360.5103032CurrentTrain: epoch  4, batch    53 | loss: 420.6930903CurrentTrain: epoch  4, batch    54 | loss: 344.0502147CurrentTrain: epoch  4, batch    55 | loss: 378.2636516CurrentTrain: epoch  4, batch    56 | loss: 456.3409178CurrentTrain: epoch  4, batch    57 | loss: 303.0063269CurrentTrain: epoch  4, batch    58 | loss: 421.4209391CurrentTrain: epoch  4, batch    59 | loss: 379.7405321CurrentTrain: epoch  4, batch    60 | loss: 374.9914546CurrentTrain: epoch  4, batch    61 | loss: 278.9500853CurrentTrain: epoch  4, batch    62 | loss: 251.7353434CurrentTrain: epoch  4, batch    63 | loss: 314.5066445CurrentTrain: epoch  4, batch    64 | loss: 357.8366148CurrentTrain: epoch  4, batch    65 | loss: 438.8529511CurrentTrain: epoch  4, batch    66 | loss: 317.6457515CurrentTrain: epoch  4, batch    67 | loss: 300.3788121CurrentTrain: epoch  4, batch    68 | loss: 380.2996517CurrentTrain: epoch  4, batch    69 | loss: 300.7377036CurrentTrain: epoch  4, batch    70 | loss: 313.8514807CurrentTrain: epoch  4, batch    71 | loss: 366.8828935CurrentTrain: epoch  4, batch    72 | loss: 257.7468287CurrentTrain: epoch  4, batch    73 | loss: 334.7269551CurrentTrain: epoch  4, batch    74 | loss: 457.9116234CurrentTrain: epoch  4, batch    75 | loss: 342.4071943CurrentTrain: epoch  4, batch    76 | loss: 457.3508987CurrentTrain: epoch  4, batch    77 | loss: 359.6081615CurrentTrain: epoch  4, batch    78 | loss: 289.6046052CurrentTrain: epoch  4, batch    79 | loss: 358.1996934CurrentTrain: epoch  4, batch    80 | loss: 290.3121489CurrentTrain: epoch  4, batch    81 | loss: 437.5661868CurrentTrain: epoch  4, batch    82 | loss: 297.9016790CurrentTrain: epoch  4, batch    83 | loss: 300.2785315CurrentTrain: epoch  4, batch    84 | loss: 251.4429823CurrentTrain: epoch  4, batch    85 | loss: 340.6330264CurrentTrain: epoch  4, batch    86 | loss: 409.3674193CurrentTrain: epoch  4, batch    87 | loss: 423.4612493CurrentTrain: epoch  4, batch    88 | loss: 392.8744476CurrentTrain: epoch  4, batch    89 | loss: 308.4733890CurrentTrain: epoch  4, batch    90 | loss: 391.1939877CurrentTrain: epoch  4, batch    91 | loss: 345.1050026CurrentTrain: epoch  4, batch    92 | loss: 359.2970284CurrentTrain: epoch  4, batch    93 | loss: 313.9516386CurrentTrain: epoch  4, batch    94 | loss: 377.4630221CurrentTrain: epoch  4, batch    95 | loss: 281.3460870CurrentTrain: epoch  5, batch     0 | loss: 325.5087662CurrentTrain: epoch  5, batch     1 | loss: 311.0516094CurrentTrain: epoch  5, batch     2 | loss: 374.6338103CurrentTrain: epoch  5, batch     3 | loss: 343.5320276CurrentTrain: epoch  5, batch     4 | loss: 312.7609322CurrentTrain: epoch  5, batch     5 | loss: 300.3078965CurrentTrain: epoch  5, batch     6 | loss: 461.9327697CurrentTrain: epoch  5, batch     7 | loss: 299.7918162CurrentTrain: epoch  5, batch     8 | loss: 326.2785803CurrentTrain: epoch  5, batch     9 | loss: 311.7224648CurrentTrain: epoch  5, batch    10 | loss: 316.1799531CurrentTrain: epoch  5, batch    11 | loss: 391.3494321CurrentTrain: epoch  5, batch    12 | loss: 455.3848429CurrentTrain: epoch  5, batch    13 | loss: 314.8493849CurrentTrain: epoch  5, batch    14 | loss: 373.7731227CurrentTrain: epoch  5, batch    15 | loss: 287.5929796CurrentTrain: epoch  5, batch    16 | loss: 341.8297389CurrentTrain: epoch  5, batch    17 | loss: 260.6604448CurrentTrain: epoch  5, batch    18 | loss: 315.9206453CurrentTrain: epoch  5, batch    19 | loss: 373.1987817CurrentTrain: epoch  5, batch    20 | loss: 341.3808007CurrentTrain: epoch  5, batch    21 | loss: 290.9269116CurrentTrain: epoch  5, batch    22 | loss: 356.5330739CurrentTrain: epoch  5, batch    23 | loss: 358.7014819CurrentTrain: epoch  5, batch    24 | loss: 306.4306995CurrentTrain: epoch  5, batch    25 | loss: 341.3891893CurrentTrain: epoch  5, batch    26 | loss: 407.3841210CurrentTrain: epoch  5, batch    27 | loss: 345.0928144CurrentTrain: epoch  5, batch    28 | loss: 457.1710390CurrentTrain: epoch  5, batch    29 | loss: 744.7894208CurrentTrain: epoch  5, batch    30 | loss: 316.4150001CurrentTrain: epoch  5, batch    31 | loss: 515.3325757CurrentTrain: epoch  5, batch    32 | loss: 277.3244309CurrentTrain: epoch  5, batch    33 | loss: 269.5092772CurrentTrain: epoch  5, batch    34 | loss: 327.7594981CurrentTrain: epoch  5, batch    35 | loss: 249.9245104CurrentTrain: epoch  5, batch    36 | loss: 438.4482603CurrentTrain: epoch  5, batch    37 | loss: 328.5685292CurrentTrain: epoch  5, batch    38 | loss: 326.6252449CurrentTrain: epoch  5, batch    39 | loss: 409.9997333CurrentTrain: epoch  5, batch    40 | loss: 324.4858551CurrentTrain: epoch  5, batch    41 | loss: 408.1934077CurrentTrain: epoch  5, batch    42 | loss: 458.6243085CurrentTrain: epoch  5, batch    43 | loss: 328.9737398CurrentTrain: epoch  5, batch    44 | loss: 314.1186934CurrentTrain: epoch  5, batch    45 | loss: 355.9029400CurrentTrain: epoch  5, batch    46 | loss: 356.8646685CurrentTrain: epoch  5, batch    47 | loss: 288.4215902CurrentTrain: epoch  5, batch    48 | loss: 285.9044181CurrentTrain: epoch  5, batch    49 | loss: 330.2224946CurrentTrain: epoch  5, batch    50 | loss: 292.4488521CurrentTrain: epoch  5, batch    51 | loss: 298.6682059CurrentTrain: epoch  5, batch    52 | loss: 376.6373838CurrentTrain: epoch  5, batch    53 | loss: 296.3919824CurrentTrain: epoch  5, batch    54 | loss: 318.2109417CurrentTrain: epoch  5, batch    55 | loss: 407.2892714CurrentTrain: epoch  5, batch    56 | loss: 455.4678808CurrentTrain: epoch  5, batch    57 | loss: 358.6444499CurrentTrain: epoch  5, batch    58 | loss: 357.3240369CurrentTrain: epoch  5, batch    59 | loss: 440.1093421CurrentTrain: epoch  5, batch    60 | loss: 393.7916915CurrentTrain: epoch  5, batch    61 | loss: 344.7578606CurrentTrain: epoch  5, batch    62 | loss: 413.3852002CurrentTrain: epoch  5, batch    63 | loss: 329.3234466CurrentTrain: epoch  5, batch    64 | loss: 264.7463642CurrentTrain: epoch  5, batch    65 | loss: 288.0232852CurrentTrain: epoch  5, batch    66 | loss: 325.4154515CurrentTrain: epoch  5, batch    67 | loss: 313.9304924CurrentTrain: epoch  5, batch    68 | loss: 276.3486784CurrentTrain: epoch  5, batch    69 | loss: 328.4150921CurrentTrain: epoch  5, batch    70 | loss: 391.0647001CurrentTrain: epoch  5, batch    71 | loss: 395.6385061CurrentTrain: epoch  5, batch    72 | loss: 377.1550309CurrentTrain: epoch  5, batch    73 | loss: 250.1413566CurrentTrain: epoch  5, batch    74 | loss: 272.8329930CurrentTrain: epoch  5, batch    75 | loss: 300.1104335CurrentTrain: epoch  5, batch    76 | loss: 423.9172196CurrentTrain: epoch  5, batch    77 | loss: 356.9556298CurrentTrain: epoch  5, batch    78 | loss: 311.6472506CurrentTrain: epoch  5, batch    79 | loss: 326.9470480CurrentTrain: epoch  5, batch    80 | loss: 344.1903025CurrentTrain: epoch  5, batch    81 | loss: 390.3833100CurrentTrain: epoch  5, batch    82 | loss: 341.7689513CurrentTrain: epoch  5, batch    83 | loss: 390.9110534CurrentTrain: epoch  5, batch    84 | loss: 374.0253916CurrentTrain: epoch  5, batch    85 | loss: 342.3266289CurrentTrain: epoch  5, batch    86 | loss: 390.4388134CurrentTrain: epoch  5, batch    87 | loss: 286.3396201CurrentTrain: epoch  5, batch    88 | loss: 377.0116668CurrentTrain: epoch  5, batch    89 | loss: 330.1968241CurrentTrain: epoch  5, batch    90 | loss: 341.0232647CurrentTrain: epoch  5, batch    91 | loss: 373.4970035CurrentTrain: epoch  5, batch    92 | loss: 312.2370063CurrentTrain: epoch  5, batch    93 | loss: 287.0525257CurrentTrain: epoch  5, batch    94 | loss: 438.0069342CurrentTrain: epoch  5, batch    95 | loss: 281.9927824CurrentTrain: epoch  6, batch     0 | loss: 266.9207403CurrentTrain: epoch  6, batch     1 | loss: 341.7735863CurrentTrain: epoch  6, batch     2 | loss: 309.6739328CurrentTrain: epoch  6, batch     3 | loss: 407.0827583CurrentTrain: epoch  6, batch     4 | loss: 456.8992142CurrentTrain: epoch  6, batch     5 | loss: 536.5852858CurrentTrain: epoch  6, batch     6 | loss: 357.6799066CurrentTrain: epoch  6, batch     7 | loss: 359.6927812CurrentTrain: epoch  6, batch     8 | loss: 391.7653226CurrentTrain: epoch  6, batch     9 | loss: 373.4139004CurrentTrain: epoch  6, batch    10 | loss: 403.2636840CurrentTrain: epoch  6, batch    11 | loss: 358.8611863CurrentTrain: epoch  6, batch    12 | loss: 407.1351603CurrentTrain: epoch  6, batch    13 | loss: 377.1191015CurrentTrain: epoch  6, batch    14 | loss: 280.0148136CurrentTrain: epoch  6, batch    15 | loss: 287.8149570CurrentTrain: epoch  6, batch    16 | loss: 298.7494577CurrentTrain: epoch  6, batch    17 | loss: 340.7697113CurrentTrain: epoch  6, batch    18 | loss: 298.5406277CurrentTrain: epoch  6, batch    19 | loss: 286.1415445CurrentTrain: epoch  6, batch    20 | loss: 286.8150086CurrentTrain: epoch  6, batch    21 | loss: 356.2973201CurrentTrain: epoch  6, batch    22 | loss: 373.6396881CurrentTrain: epoch  6, batch    23 | loss: 361.8053229CurrentTrain: epoch  6, batch    24 | loss: 391.6386556CurrentTrain: epoch  6, batch    25 | loss: 284.7076285CurrentTrain: epoch  6, batch    26 | loss: 314.2907671CurrentTrain: epoch  6, batch    27 | loss: 285.3086001CurrentTrain: epoch  6, batch    28 | loss: 348.6590350CurrentTrain: epoch  6, batch    29 | loss: 357.2472547CurrentTrain: epoch  6, batch    30 | loss: 390.2130999CurrentTrain: epoch  6, batch    31 | loss: 262.7214433CurrentTrain: epoch  6, batch    32 | loss: 358.6561223CurrentTrain: epoch  6, batch    33 | loss: 372.6840138CurrentTrain: epoch  6, batch    34 | loss: 358.1021680CurrentTrain: epoch  6, batch    35 | loss: 298.6382269CurrentTrain: epoch  6, batch    36 | loss: 313.2505070CurrentTrain: epoch  6, batch    37 | loss: 345.7143702CurrentTrain: epoch  6, batch    38 | loss: 374.6561280CurrentTrain: epoch  6, batch    39 | loss: 326.2936278CurrentTrain: epoch  6, batch    40 | loss: 287.3188118CurrentTrain: epoch  6, batch    41 | loss: 347.5129370CurrentTrain: epoch  6, batch    42 | loss: 311.7239475CurrentTrain: epoch  6, batch    43 | loss: 291.9374752CurrentTrain: epoch  6, batch    44 | loss: 248.9382636CurrentTrain: epoch  6, batch    45 | loss: 341.7309947CurrentTrain: epoch  6, batch    46 | loss: 325.3713331CurrentTrain: epoch  6, batch    47 | loss: 455.4913993CurrentTrain: epoch  6, batch    48 | loss: 329.3455480CurrentTrain: epoch  6, batch    49 | loss: 300.8790197CurrentTrain: epoch  6, batch    50 | loss: 408.8337715CurrentTrain: epoch  6, batch    51 | loss: 361.7936684CurrentTrain: epoch  6, batch    52 | loss: 455.7360424CurrentTrain: epoch  6, batch    53 | loss: 409.9391211CurrentTrain: epoch  6, batch    54 | loss: 359.0661503CurrentTrain: epoch  6, batch    55 | loss: 355.5280750CurrentTrain: epoch  6, batch    56 | loss: 376.0471202CurrentTrain: epoch  6, batch    57 | loss: 245.9569221CurrentTrain: epoch  6, batch    58 | loss: 392.1424392CurrentTrain: epoch  6, batch    59 | loss: 290.6433148CurrentTrain: epoch  6, batch    60 | loss: 324.7246825CurrentTrain: epoch  6, batch    61 | loss: 375.2393586CurrentTrain: epoch  6, batch    62 | loss: 358.2524093CurrentTrain: epoch  6, batch    63 | loss: 300.6541483CurrentTrain: epoch  6, batch    64 | loss: 325.2057912CurrentTrain: epoch  6, batch    65 | loss: 390.9305368CurrentTrain: epoch  6, batch    66 | loss: 358.4142205CurrentTrain: epoch  6, batch    67 | loss: 341.4677091CurrentTrain: epoch  6, batch    68 | loss: 373.4348201CurrentTrain: epoch  6, batch    69 | loss: 324.6838361CurrentTrain: epoch  6, batch    70 | loss: 393.2315678CurrentTrain: epoch  6, batch    71 | loss: 290.9407890CurrentTrain: epoch  6, batch    72 | loss: 289.9117806CurrentTrain: epoch  6, batch    73 | loss: 260.7069745CurrentTrain: epoch  6, batch    74 | loss: 407.2029042CurrentTrain: epoch  6, batch    75 | loss: 374.2765829CurrentTrain: epoch  6, batch    76 | loss: 252.0842975CurrentTrain: epoch  6, batch    77 | loss: 290.9109072CurrentTrain: epoch  6, batch    78 | loss: 390.1212920CurrentTrain: epoch  6, batch    79 | loss: 379.0502441CurrentTrain: epoch  6, batch    80 | loss: 325.2590131CurrentTrain: epoch  6, batch    81 | loss: 278.2748411CurrentTrain: epoch  6, batch    82 | loss: 315.4329802CurrentTrain: epoch  6, batch    83 | loss: 341.4118110CurrentTrain: epoch  6, batch    84 | loss: 342.9256723CurrentTrain: epoch  6, batch    85 | loss: 277.7404293CurrentTrain: epoch  6, batch    86 | loss: 310.6541715CurrentTrain: epoch  6, batch    87 | loss: 374.3947800CurrentTrain: epoch  6, batch    88 | loss: 373.3892832CurrentTrain: epoch  6, batch    89 | loss: 409.8596554CurrentTrain: epoch  6, batch    90 | loss: 374.0076692CurrentTrain: epoch  6, batch    91 | loss: 276.3606531CurrentTrain: epoch  6, batch    92 | loss: 342.6605180CurrentTrain: epoch  6, batch    93 | loss: 312.3004431CurrentTrain: epoch  6, batch    94 | loss: 318.7636395CurrentTrain: epoch  6, batch    95 | loss: 329.8888987CurrentTrain: epoch  7, batch     0 | loss: 355.6343796CurrentTrain: epoch  7, batch     1 | loss: 373.3094150CurrentTrain: epoch  7, batch     2 | loss: 358.2115454CurrentTrain: epoch  7, batch     3 | loss: 343.2745750CurrentTrain: epoch  7, batch     4 | loss: 325.2220811CurrentTrain: epoch  7, batch     5 | loss: 373.0122967CurrentTrain: epoch  7, batch     6 | loss: 356.2017059CurrentTrain: epoch  7, batch     7 | loss: 325.3185559CurrentTrain: epoch  7, batch     8 | loss: 298.2304729CurrentTrain: epoch  7, batch     9 | loss: 286.6385372CurrentTrain: epoch  7, batch    10 | loss: 314.4747699CurrentTrain: epoch  7, batch    11 | loss: 313.3377059CurrentTrain: epoch  7, batch    12 | loss: 398.4443763CurrentTrain: epoch  7, batch    13 | loss: 355.6653621CurrentTrain: epoch  7, batch    14 | loss: 359.6738473CurrentTrain: epoch  7, batch    15 | loss: 344.6844882CurrentTrain: epoch  7, batch    16 | loss: 390.1714671CurrentTrain: epoch  7, batch    17 | loss: 255.6152535CurrentTrain: epoch  7, batch    18 | loss: 313.7701534CurrentTrain: epoch  7, batch    19 | loss: 271.7786808CurrentTrain: epoch  7, batch    20 | loss: 298.7018339CurrentTrain: epoch  7, batch    21 | loss: 311.6081228CurrentTrain: epoch  7, batch    22 | loss: 389.8970198CurrentTrain: epoch  7, batch    23 | loss: 373.2522226CurrentTrain: epoch  7, batch    24 | loss: 317.2450291CurrentTrain: epoch  7, batch    25 | loss: 276.6828593CurrentTrain: epoch  7, batch    26 | loss: 341.7365021CurrentTrain: epoch  7, batch    27 | loss: 407.4226890CurrentTrain: epoch  7, batch    28 | loss: 314.5248393CurrentTrain: epoch  7, batch    29 | loss: 410.2380666CurrentTrain: epoch  7, batch    30 | loss: 325.4731698CurrentTrain: epoch  7, batch    31 | loss: 372.7821827CurrentTrain: epoch  7, batch    32 | loss: 402.6361988CurrentTrain: epoch  7, batch    33 | loss: 355.9427858CurrentTrain: epoch  7, batch    34 | loss: 372.5135255CurrentTrain: epoch  7, batch    35 | loss: 408.1776996CurrentTrain: epoch  7, batch    36 | loss: 356.4183246CurrentTrain: epoch  7, batch    37 | loss: 373.2673735CurrentTrain: epoch  7, batch    38 | loss: 355.7835025CurrentTrain: epoch  7, batch    39 | loss: 393.1704741CurrentTrain: epoch  7, batch    40 | loss: 342.1338207CurrentTrain: epoch  7, batch    41 | loss: 270.8261147CurrentTrain: epoch  7, batch    42 | loss: 361.7609851CurrentTrain: epoch  7, batch    43 | loss: 302.3630707CurrentTrain: epoch  7, batch    44 | loss: 311.4119049CurrentTrain: epoch  7, batch    45 | loss: 390.1249801CurrentTrain: epoch  7, batch    46 | loss: 330.4826180CurrentTrain: epoch  7, batch    47 | loss: 282.2827012CurrentTrain: epoch  7, batch    48 | loss: 406.7766443CurrentTrain: epoch  7, batch    49 | loss: 306.2504997CurrentTrain: epoch  7, batch    50 | loss: 356.6856972CurrentTrain: epoch  7, batch    51 | loss: 295.3959365CurrentTrain: epoch  7, batch    52 | loss: 298.9369292CurrentTrain: epoch  7, batch    53 | loss: 374.0318909CurrentTrain: epoch  7, batch    54 | loss: 299.6598525CurrentTrain: epoch  7, batch    55 | loss: 407.1997836CurrentTrain: epoch  7, batch    56 | loss: 374.2976173CurrentTrain: epoch  7, batch    57 | loss: 341.2087832CurrentTrain: epoch  7, batch    58 | loss: 325.5320452CurrentTrain: epoch  7, batch    59 | loss: 455.4041605CurrentTrain: epoch  7, batch    60 | loss: 287.0060926CurrentTrain: epoch  7, batch    61 | loss: 328.3696726CurrentTrain: epoch  7, batch    62 | loss: 419.0925244CurrentTrain: epoch  7, batch    63 | loss: 331.1195043CurrentTrain: epoch  7, batch    64 | loss: 257.1298394CurrentTrain: epoch  7, batch    65 | loss: 296.8570084CurrentTrain: epoch  7, batch    66 | loss: 339.7587967CurrentTrain: epoch  7, batch    67 | loss: 373.3492516CurrentTrain: epoch  7, batch    68 | loss: 285.4426683CurrentTrain: epoch  7, batch    69 | loss: 340.6377252CurrentTrain: epoch  7, batch    70 | loss: 329.0186591CurrentTrain: epoch  7, batch    71 | loss: 408.3589192CurrentTrain: epoch  7, batch    72 | loss: 284.4502308CurrentTrain: epoch  7, batch    73 | loss: 339.7460509CurrentTrain: epoch  7, batch    74 | loss: 250.9541822CurrentTrain: epoch  7, batch    75 | loss: 270.5242589CurrentTrain: epoch  7, batch    76 | loss: 301.6732208CurrentTrain: epoch  7, batch    77 | loss: 393.6904424CurrentTrain: epoch  7, batch    78 | loss: 357.3026719CurrentTrain: epoch  7, batch    79 | loss: 456.3324946CurrentTrain: epoch  7, batch    80 | loss: 328.7726839CurrentTrain: epoch  7, batch    81 | loss: 297.9451776CurrentTrain: epoch  7, batch    82 | loss: 340.3627387CurrentTrain: epoch  7, batch    83 | loss: 270.1392964CurrentTrain: epoch  7, batch    84 | loss: 324.6672118CurrentTrain: epoch  7, batch    85 | loss: 355.9636179CurrentTrain: epoch  7, batch    86 | loss: 303.0433922CurrentTrain: epoch  7, batch    87 | loss: 376.7807618CurrentTrain: epoch  7, batch    88 | loss: 341.4750540CurrentTrain: epoch  7, batch    89 | loss: 413.0253222CurrentTrain: epoch  7, batch    90 | loss: 355.7629070CurrentTrain: epoch  7, batch    91 | loss: 326.1301890CurrentTrain: epoch  7, batch    92 | loss: 374.7324511CurrentTrain: epoch  7, batch    93 | loss: 305.3374519CurrentTrain: epoch  7, batch    94 | loss: 321.1145188CurrentTrain: epoch  7, batch    95 | loss: 321.4111135CurrentTrain: epoch  8, batch     0 | loss: 299.3391149CurrentTrain: epoch  8, batch     1 | loss: 298.6006006CurrentTrain: epoch  8, batch     2 | loss: 390.4154036CurrentTrain: epoch  8, batch     3 | loss: 455.0992295CurrentTrain: epoch  8, batch     4 | loss: 341.3430340CurrentTrain: epoch  8, batch     5 | loss: 344.4642580CurrentTrain: epoch  8, batch     6 | loss: 372.7823332CurrentTrain: epoch  8, batch     7 | loss: 324.7194346CurrentTrain: epoch  8, batch     8 | loss: 331.0472878CurrentTrain: epoch  8, batch     9 | loss: 394.6659111CurrentTrain: epoch  8, batch    10 | loss: 270.9359254CurrentTrain: epoch  8, batch    11 | loss: 310.5097691CurrentTrain: epoch  8, batch    12 | loss: 339.7942478CurrentTrain: epoch  8, batch    13 | loss: 342.4070714CurrentTrain: epoch  8, batch    14 | loss: 324.5646538CurrentTrain: epoch  8, batch    15 | loss: 355.9165719CurrentTrain: epoch  8, batch    16 | loss: 373.0081141CurrentTrain: epoch  8, batch    17 | loss: 339.5801359CurrentTrain: epoch  8, batch    18 | loss: 390.9554663CurrentTrain: epoch  8, batch    19 | loss: 313.9063864CurrentTrain: epoch  8, batch    20 | loss: 296.7110432CurrentTrain: epoch  8, batch    21 | loss: 309.7365990CurrentTrain: epoch  8, batch    22 | loss: 341.7121592CurrentTrain: epoch  8, batch    23 | loss: 328.3633783CurrentTrain: epoch  8, batch    24 | loss: 287.1977207CurrentTrain: epoch  8, batch    25 | loss: 372.7404288CurrentTrain: epoch  8, batch    26 | loss: 270.9568592CurrentTrain: epoch  8, batch    27 | loss: 325.8477819CurrentTrain: epoch  8, batch    28 | loss: 299.2813449CurrentTrain: epoch  8, batch    29 | loss: 312.8411698CurrentTrain: epoch  8, batch    30 | loss: 408.3511080CurrentTrain: epoch  8, batch    31 | loss: 262.5628756CurrentTrain: epoch  8, batch    32 | loss: 515.2875911CurrentTrain: epoch  8, batch    33 | loss: 389.4819177CurrentTrain: epoch  8, batch    34 | loss: 359.1437535CurrentTrain: epoch  8, batch    35 | loss: 283.5129619CurrentTrain: epoch  8, batch    36 | loss: 341.7192854CurrentTrain: epoch  8, batch    37 | loss: 329.3303712CurrentTrain: epoch  8, batch    38 | loss: 324.3637756CurrentTrain: epoch  8, batch    39 | loss: 373.0019481CurrentTrain: epoch  8, batch    40 | loss: 356.7045668CurrentTrain: epoch  8, batch    41 | loss: 407.5250870CurrentTrain: epoch  8, batch    42 | loss: 355.7189701CurrentTrain: epoch  8, batch    43 | loss: 407.7155341CurrentTrain: epoch  8, batch    44 | loss: 437.2372226CurrentTrain: epoch  8, batch    45 | loss: 355.9175693CurrentTrain: epoch  8, batch    46 | loss: 283.3350142CurrentTrain: epoch  8, batch    47 | loss: 356.3740594CurrentTrain: epoch  8, batch    48 | loss: 297.4978113CurrentTrain: epoch  8, batch    49 | loss: 325.9422192CurrentTrain: epoch  8, batch    50 | loss: 373.0928100CurrentTrain: epoch  8, batch    51 | loss: 298.0347187CurrentTrain: epoch  8, batch    52 | loss: 355.4587148CurrentTrain: epoch  8, batch    53 | loss: 372.3821047CurrentTrain: epoch  8, batch    54 | loss: 373.6411009CurrentTrain: epoch  8, batch    55 | loss: 356.9787011CurrentTrain: epoch  8, batch    56 | loss: 313.1350726CurrentTrain: epoch  8, batch    57 | loss: 288.6623492CurrentTrain: epoch  8, batch    58 | loss: 339.6472074CurrentTrain: epoch  8, batch    59 | loss: 226.2126552CurrentTrain: epoch  8, batch    60 | loss: 436.8397959CurrentTrain: epoch  8, batch    61 | loss: 389.8644448CurrentTrain: epoch  8, batch    62 | loss: 263.3820021CurrentTrain: epoch  8, batch    63 | loss: 355.7851569CurrentTrain: epoch  8, batch    64 | loss: 342.0402482CurrentTrain: epoch  8, batch    65 | loss: 324.2969036CurrentTrain: epoch  8, batch    66 | loss: 271.2026711CurrentTrain: epoch  8, batch    67 | loss: 343.0974924CurrentTrain: epoch  8, batch    68 | loss: 341.7340118CurrentTrain: epoch  8, batch    69 | loss: 359.1635946CurrentTrain: epoch  8, batch    70 | loss: 372.5044849CurrentTrain: epoch  8, batch    71 | loss: 403.5283170CurrentTrain: epoch  8, batch    72 | loss: 326.6529886CurrentTrain: epoch  8, batch    73 | loss: 323.7861941CurrentTrain: epoch  8, batch    74 | loss: 389.5589739CurrentTrain: epoch  8, batch    75 | loss: 339.6820085CurrentTrain: epoch  8, batch    76 | loss: 276.2612197CurrentTrain: epoch  8, batch    77 | loss: 420.2450324CurrentTrain: epoch  8, batch    78 | loss: 250.8495173CurrentTrain: epoch  8, batch    79 | loss: 329.9813717CurrentTrain: epoch  8, batch    80 | loss: 357.3553537CurrentTrain: epoch  8, batch    81 | loss: 356.9651358CurrentTrain: epoch  8, batch    82 | loss: 344.6021003CurrentTrain: epoch  8, batch    83 | loss: 298.9583023CurrentTrain: epoch  8, batch    84 | loss: 437.5845397CurrentTrain: epoch  8, batch    85 | loss: 401.8207575CurrentTrain: epoch  8, batch    86 | loss: 390.0808946CurrentTrain: epoch  8, batch    87 | loss: 341.1841861CurrentTrain: epoch  8, batch    88 | loss: 211.9047009CurrentTrain: epoch  8, batch    89 | loss: 326.2009825CurrentTrain: epoch  8, batch    90 | loss: 324.4765710CurrentTrain: epoch  8, batch    91 | loss: 373.0417431CurrentTrain: epoch  8, batch    92 | loss: 339.9650293CurrentTrain: epoch  8, batch    93 | loss: 339.6862468CurrentTrain: epoch  8, batch    94 | loss: 357.8825879CurrentTrain: epoch  8, batch    95 | loss: 361.8360106CurrentTrain: epoch  9, batch     0 | loss: 328.8805505CurrentTrain: epoch  9, batch     1 | loss: 402.0709447CurrentTrain: epoch  9, batch     2 | loss: 313.0567103CurrentTrain: epoch  9, batch     3 | loss: 314.6317571CurrentTrain: epoch  9, batch     4 | loss: 341.1210126CurrentTrain: epoch  9, batch     5 | loss: 248.5725421CurrentTrain: epoch  9, batch     6 | loss: 372.6248994CurrentTrain: epoch  9, batch     7 | loss: 324.4650009CurrentTrain: epoch  9, batch     8 | loss: 372.5277271CurrentTrain: epoch  9, batch     9 | loss: 298.6372723CurrentTrain: epoch  9, batch    10 | loss: 408.1321262CurrentTrain: epoch  9, batch    11 | loss: 328.2535822CurrentTrain: epoch  9, batch    12 | loss: 375.5392485CurrentTrain: epoch  9, batch    13 | loss: 389.5456243CurrentTrain: epoch  9, batch    14 | loss: 359.1048307CurrentTrain: epoch  9, batch    15 | loss: 309.1051671CurrentTrain: epoch  9, batch    16 | loss: 455.1110478CurrentTrain: epoch  9, batch    17 | loss: 356.0011189CurrentTrain: epoch  9, batch    18 | loss: 325.5380398CurrentTrain: epoch  9, batch    19 | loss: 378.3103749CurrentTrain: epoch  9, batch    20 | loss: 271.7011713CurrentTrain: epoch  9, batch    21 | loss: 390.0668355CurrentTrain: epoch  9, batch    22 | loss: 328.2265178CurrentTrain: epoch  9, batch    23 | loss: 314.7092355CurrentTrain: epoch  9, batch    24 | loss: 436.5545298CurrentTrain: epoch  9, batch    25 | loss: 372.8976403CurrentTrain: epoch  9, batch    26 | loss: 244.7765948CurrentTrain: epoch  9, batch    27 | loss: 401.4615917CurrentTrain: epoch  9, batch    28 | loss: 299.0627265CurrentTrain: epoch  9, batch    29 | loss: 314.0199897CurrentTrain: epoch  9, batch    30 | loss: 297.7676407CurrentTrain: epoch  9, batch    31 | loss: 324.8232660CurrentTrain: epoch  9, batch    32 | loss: 389.6688869CurrentTrain: epoch  9, batch    33 | loss: 331.9821915CurrentTrain: epoch  9, batch    34 | loss: 320.8683809CurrentTrain: epoch  9, batch    35 | loss: 422.0709889CurrentTrain: epoch  9, batch    36 | loss: 324.2571482CurrentTrain: epoch  9, batch    37 | loss: 356.2305085CurrentTrain: epoch  9, batch    38 | loss: 339.3877355CurrentTrain: epoch  9, batch    39 | loss: 328.2914833CurrentTrain: epoch  9, batch    40 | loss: 271.5263177CurrentTrain: epoch  9, batch    41 | loss: 372.2020429CurrentTrain: epoch  9, batch    42 | loss: 283.1783542CurrentTrain: epoch  9, batch    43 | loss: 275.1282247CurrentTrain: epoch  9, batch    44 | loss: 419.0032244CurrentTrain: epoch  9, batch    45 | loss: 344.3009433CurrentTrain: epoch  9, batch    46 | loss: 356.6965329CurrentTrain: epoch  9, batch    47 | loss: 389.7635280CurrentTrain: epoch  9, batch    48 | loss: 323.7802686CurrentTrain: epoch  9, batch    49 | loss: 328.2076547CurrentTrain: epoch  9, batch    50 | loss: 309.6632218CurrentTrain: epoch  9, batch    51 | loss: 356.9538240CurrentTrain: epoch  9, batch    52 | loss: 315.4472064CurrentTrain: epoch  9, batch    53 | loss: 355.8049830CurrentTrain: epoch  9, batch    54 | loss: 313.4232768CurrentTrain: epoch  9, batch    55 | loss: 299.0626655CurrentTrain: epoch  9, batch    56 | loss: 356.0696021CurrentTrain: epoch  9, batch    57 | loss: 250.6634883CurrentTrain: epoch  9, batch    58 | loss: 341.0143814CurrentTrain: epoch  9, batch    59 | loss: 389.6502020CurrentTrain: epoch  9, batch    60 | loss: 295.7081531CurrentTrain: epoch  9, batch    61 | loss: 277.4055633CurrentTrain: epoch  9, batch    62 | loss: 373.5265332CurrentTrain: epoch  9, batch    63 | loss: 257.7517964CurrentTrain: epoch  9, batch    64 | loss: 279.0871483CurrentTrain: epoch  9, batch    65 | loss: 389.8710850CurrentTrain: epoch  9, batch    66 | loss: 374.6290364CurrentTrain: epoch  9, batch    67 | loss: 325.1488535CurrentTrain: epoch  9, batch    68 | loss: 455.0761364CurrentTrain: epoch  9, batch    69 | loss: 389.9339242CurrentTrain: epoch  9, batch    70 | loss: 339.9475387CurrentTrain: epoch  9, batch    71 | loss: 373.9290834CurrentTrain: epoch  9, batch    72 | loss: 284.4316668CurrentTrain: epoch  9, batch    73 | loss: 390.5282919CurrentTrain: epoch  9, batch    74 | loss: 455.3973803CurrentTrain: epoch  9, batch    75 | loss: 396.0671149CurrentTrain: epoch  9, batch    76 | loss: 372.6085178CurrentTrain: epoch  9, batch    77 | loss: 312.4792412CurrentTrain: epoch  9, batch    78 | loss: 402.8376578CurrentTrain: epoch  9, batch    79 | loss: 324.5295491CurrentTrain: epoch  9, batch    80 | loss: 290.4405565CurrentTrain: epoch  9, batch    81 | loss: 249.4613879CurrentTrain: epoch  9, batch    82 | loss: 407.3772594CurrentTrain: epoch  9, batch    83 | loss: 455.2269336CurrentTrain: epoch  9, batch    84 | loss: 455.2562277CurrentTrain: epoch  9, batch    85 | loss: 312.7008136CurrentTrain: epoch  9, batch    86 | loss: 309.8723989CurrentTrain: epoch  9, batch    87 | loss: 300.4077772CurrentTrain: epoch  9, batch    88 | loss: 455.0401719CurrentTrain: epoch  9, batch    89 | loss: 290.3388339CurrentTrain: epoch  9, batch    90 | loss: 340.6946626CurrentTrain: epoch  9, batch    91 | loss: 324.6977476CurrentTrain: epoch  9, batch    92 | loss: 248.2197144CurrentTrain: epoch  9, batch    93 | loss: 286.8969161CurrentTrain: epoch  9, batch    94 | loss: 340.6095114CurrentTrain: epoch  9, batch    95 | loss: 260.9234675

F1 score per class: {32: 0.6432748538011696, 6: 0.8700564971751412, 19: 0.42857142857142855, 24: 0.7668393782383419, 26: 0.88268156424581, 29: 0.9025641025641026}
Micro-average F1 score: 0.8038176033934252
Weighted-average F1 score: 0.8088095874900182
F1 score per class: {32: 0.717948717948718, 6: 0.9081081081081082, 19: 0.3870967741935484, 24: 0.7540983606557377, 26: 0.9583333333333334, 29: 0.8911917098445595}
Micro-average F1 score: 0.8314606741573034
Weighted-average F1 score: 0.8331111389923438
F1 score per class: {32: 0.711340206185567, 6: 0.9081081081081082, 19: 0.3870967741935484, 24: 0.75, 26: 0.9473684210526315, 29: 0.8911917098445595}
Micro-average F1 score: 0.827021494370522
Weighted-average F1 score: 0.8283908791839422

F1 score per class: {32: 0.6432748538011696, 6: 0.8700564971751412, 19: 0.42857142857142855, 24: 0.7668393782383419, 26: 0.88268156424581, 29: 0.9025641025641026}
Micro-average F1 score: 0.8038176033934252
Weighted-average F1 score: 0.8088095874900182
F1 score per class: {32: 0.717948717948718, 6: 0.9081081081081082, 19: 0.3870967741935484, 24: 0.7540983606557377, 26: 0.9583333333333334, 29: 0.8911917098445595}
Micro-average F1 score: 0.8314606741573034
Weighted-average F1 score: 0.8331111389923438
F1 score per class: {32: 0.711340206185567, 6: 0.9081081081081082, 19: 0.3870967741935484, 24: 0.75, 26: 0.9473684210526315, 29: 0.8911917098445595}
Micro-average F1 score: 0.827021494370522
Weighted-average F1 score: 0.8283908791839422
cur_acc:  ['0.8038']
his_acc:  ['0.8038']
cur_acc des:  ['0.8315']
his_acc des:  ['0.8315']
cur_acc rrf:  ['0.8270']
his_acc rrf:  ['0.8270']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 343.8245588CurrentTrain: epoch  0, batch     1 | loss: 374.7058546CurrentTrain: epoch  0, batch     2 | loss: 419.3387192CurrentTrain: epoch  0, batch     3 | loss: 327.3187900CurrentTrain: epoch  0, batch     4 | loss: 57.1127326CurrentTrain: epoch  1, batch     0 | loss: 401.7021613CurrentTrain: epoch  1, batch     1 | loss: 360.4803517CurrentTrain: epoch  1, batch     2 | loss: 387.4236314CurrentTrain: epoch  1, batch     3 | loss: 320.8946702CurrentTrain: epoch  1, batch     4 | loss: 60.0518968CurrentTrain: epoch  2, batch     0 | loss: 296.7171631CurrentTrain: epoch  2, batch     1 | loss: 446.7553921CurrentTrain: epoch  2, batch     2 | loss: 400.6134596CurrentTrain: epoch  2, batch     3 | loss: 381.1734696CurrentTrain: epoch  2, batch     4 | loss: 45.8700108CurrentTrain: epoch  3, batch     0 | loss: 415.4981759CurrentTrain: epoch  3, batch     1 | loss: 321.8809688CurrentTrain: epoch  3, batch     2 | loss: 412.5114884CurrentTrain: epoch  3, batch     3 | loss: 294.0440643CurrentTrain: epoch  3, batch     4 | loss: 78.8962692CurrentTrain: epoch  4, batch     0 | loss: 407.0605417CurrentTrain: epoch  4, batch     1 | loss: 335.2818646CurrentTrain: epoch  4, batch     2 | loss: 348.7039633CurrentTrain: epoch  4, batch     3 | loss: 345.6193888CurrentTrain: epoch  4, batch     4 | loss: 55.5253352CurrentTrain: epoch  5, batch     0 | loss: 315.4510109CurrentTrain: epoch  5, batch     1 | loss: 318.7092826CurrentTrain: epoch  5, batch     2 | loss: 358.5879344CurrentTrain: epoch  5, batch     3 | loss: 410.9812495CurrentTrain: epoch  5, batch     4 | loss: 76.8937178CurrentTrain: epoch  6, batch     0 | loss: 360.4989388CurrentTrain: epoch  6, batch     1 | loss: 409.1702350CurrentTrain: epoch  6, batch     2 | loss: 302.8206717CurrentTrain: epoch  6, batch     3 | loss: 375.5917768CurrentTrain: epoch  6, batch     4 | loss: 33.6016724CurrentTrain: epoch  7, batch     0 | loss: 343.0749863CurrentTrain: epoch  7, batch     1 | loss: 328.3296122CurrentTrain: epoch  7, batch     2 | loss: 325.9544653CurrentTrain: epoch  7, batch     3 | loss: 458.5066846CurrentTrain: epoch  7, batch     4 | loss: 45.5528808CurrentTrain: epoch  8, batch     0 | loss: 326.0880144CurrentTrain: epoch  8, batch     1 | loss: 392.2480713CurrentTrain: epoch  8, batch     2 | loss: 341.6584980CurrentTrain: epoch  8, batch     3 | loss: 343.7209928CurrentTrain: epoch  8, batch     4 | loss: 75.4517586CurrentTrain: epoch  9, batch     0 | loss: 419.4695116CurrentTrain: epoch  9, batch     1 | loss: 314.6314297CurrentTrain: epoch  9, batch     2 | loss: 392.1352793CurrentTrain: epoch  9, batch     3 | loss: 356.3884141CurrentTrain: epoch  9, batch     4 | loss: 45.3028164
MemoryTrain:  epoch  0, batch     0 | loss: 2.1284669MemoryTrain:  epoch  1, batch     0 | loss: 2.0259698MemoryTrain:  epoch  2, batch     0 | loss: 1.3826138MemoryTrain:  epoch  3, batch     0 | loss: 1.3328023MemoryTrain:  epoch  4, batch     0 | loss: 0.8756391MemoryTrain:  epoch  5, batch     0 | loss: 0.6745570MemoryTrain:  epoch  6, batch     0 | loss: 0.5707645MemoryTrain:  epoch  7, batch     0 | loss: 0.4005665MemoryTrain:  epoch  8, batch     0 | loss: 0.3889988MemoryTrain:  epoch  9, batch     0 | loss: 0.3226548

F1 score per class: {2: 0.8, 39: 0.5084745762711864, 11: 0.27586206896551724, 12: 0.0, 19: 0.5714285714285714, 28: 0.25}
Micro-average F1 score: 0.40559440559440557
Weighted-average F1 score: 0.42382987791276555
F1 score per class: {2: 0.9411764705882353, 6: 0.0, 39: 0.6865671641791045, 11: 0.6842105263157895, 12: 0.0, 19: 0.0, 24: 0.5263157894736842, 28: 0.5263157894736842}
Micro-average F1 score: 0.655367231638418
Weighted-average F1 score: 0.6156407504958866
F1 score per class: {2: 0.9411764705882353, 39: 0.6865671641791045, 11: 0.6842105263157895, 12: 0.0, 19: 0.5, 28: 0.5263157894736842}
Micro-average F1 score: 0.6628571428571428
Weighted-average F1 score: 0.6360460124636196

F1 score per class: {32: 0.8, 2: 0.6473988439306358, 6: 0.4918032786885246, 39: 0.2689075630252101, 11: 0.8044692737430168, 12: 0.35714285714285715, 19: 0.7624309392265194, 24: 0.2962962962962963, 26: 0.93048128342246, 28: 0.8969072164948454, 29: 0.2222222222222222}
Micro-average F1 score: 0.6983105390185036
Weighted-average F1 score: 0.7402208430456287
F1 score per class: {32: 0.9411764705882353, 2: 0.7623762376237624, 6: 0.6764705882352942, 39: 0.65, 11: 0.8494623655913979, 12: 0.45714285714285713, 19: 0.7582417582417582, 24: 0.38461538461538464, 26: 0.9473684210526315, 28: 0.8923076923076924, 29: 0.5}
Micro-average F1 score: 0.7798369162342476
Weighted-average F1 score: 0.7838543431581393
F1 score per class: {32: 0.9411764705882353, 2: 0.7096774193548387, 6: 0.6764705882352942, 39: 0.6540880503144654, 11: 0.851063829787234, 12: 0.4, 19: 0.7513812154696132, 24: 0.2857142857142857, 26: 0.9417989417989417, 28: 0.8923076923076924, 29: 0.4166666666666667}
Micro-average F1 score: 0.764179104477612
Weighted-average F1 score: 0.763929083710902
cur_acc:  ['0.8038', '0.4056']
his_acc:  ['0.8038', '0.6983']
cur_acc des:  ['0.8315', '0.6554']
his_acc des:  ['0.8315', '0.7798']
cur_acc rrf:  ['0.8270', '0.6629']
his_acc rrf:  ['0.8270', '0.7642']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 324.3009397CurrentTrain: epoch  0, batch     1 | loss: 330.2737609CurrentTrain: epoch  0, batch     2 | loss: 318.9816374CurrentTrain: epoch  0, batch     3 | loss: 38.8473800CurrentTrain: epoch  1, batch     0 | loss: 345.4210082CurrentTrain: epoch  1, batch     1 | loss: 288.8750822CurrentTrain: epoch  1, batch     2 | loss: 335.3739263CurrentTrain: epoch  1, batch     3 | loss: 30.1038479CurrentTrain: epoch  2, batch     0 | loss: 371.0039096CurrentTrain: epoch  2, batch     1 | loss: 393.5830811CurrentTrain: epoch  2, batch     2 | loss: 223.2714871CurrentTrain: epoch  2, batch     3 | loss: 55.0281483CurrentTrain: epoch  3, batch     0 | loss: 361.5142390CurrentTrain: epoch  3, batch     1 | loss: 307.3063804CurrentTrain: epoch  3, batch     2 | loss: 253.5880879CurrentTrain: epoch  3, batch     3 | loss: 54.9291735CurrentTrain: epoch  4, batch     0 | loss: 378.7623187CurrentTrain: epoch  4, batch     1 | loss: 263.8249424CurrentTrain: epoch  4, batch     2 | loss: 350.0651108CurrentTrain: epoch  4, batch     3 | loss: 18.2736065CurrentTrain: epoch  5, batch     0 | loss: 303.0291548CurrentTrain: epoch  5, batch     1 | loss: 288.9688859CurrentTrain: epoch  5, batch     2 | loss: 330.5437630CurrentTrain: epoch  5, batch     3 | loss: 33.1705525CurrentTrain: epoch  6, batch     0 | loss: 273.5723841CurrentTrain: epoch  6, batch     1 | loss: 343.1242691CurrentTrain: epoch  6, batch     2 | loss: 304.6428393CurrentTrain: epoch  6, batch     3 | loss: 30.3211997CurrentTrain: epoch  7, batch     0 | loss: 423.8378264CurrentTrain: epoch  7, batch     1 | loss: 326.9789513CurrentTrain: epoch  7, batch     2 | loss: 224.0070160CurrentTrain: epoch  7, batch     3 | loss: 55.0252841CurrentTrain: epoch  8, batch     0 | loss: 422.6267067CurrentTrain: epoch  8, batch     1 | loss: 259.6075740CurrentTrain: epoch  8, batch     2 | loss: 299.7425297CurrentTrain: epoch  8, batch     3 | loss: 17.9706973CurrentTrain: epoch  9, batch     0 | loss: 272.1416072CurrentTrain: epoch  9, batch     1 | loss: 342.6866219CurrentTrain: epoch  9, batch     2 | loss: 325.6998772CurrentTrain: epoch  9, batch     3 | loss: 29.7520028
MemoryTrain:  epoch  0, batch     0 | loss: 1.3678072MemoryTrain:  epoch  1, batch     0 | loss: 1.1597370MemoryTrain:  epoch  2, batch     0 | loss: 0.8800497MemoryTrain:  epoch  3, batch     0 | loss: 0.6718471MemoryTrain:  epoch  4, batch     0 | loss: 0.4816184MemoryTrain:  epoch  5, batch     0 | loss: 0.3749425MemoryTrain:  epoch  6, batch     0 | loss: 0.2879222MemoryTrain:  epoch  7, batch     0 | loss: 0.2606669MemoryTrain:  epoch  8, batch     0 | loss: 0.2036619MemoryTrain:  epoch  9, batch     0 | loss: 0.1336838

F1 score per class: {7: 0.3333333333333333, 40: 0.96, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.35294117647058826, 27: 0.0, 28: 0.6666666666666666, 31: 0.16216216216216217}
Micro-average F1 score: 0.33653846153846156
Weighted-average F1 score: 0.2811142306261205
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9615384615384616, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.5714285714285714, 27: 0.0, 28: 0.8, 31: 0.4186046511627907}
Micro-average F1 score: 0.4663677130044843
Weighted-average F1 score: 0.3655013360155242
F1 score per class: {6: 0.0, 7: 0.3333333333333333, 40: 0.9615384615384616, 9: 0.0, 11: 0.0, 19: 0.0, 26: 0.5714285714285714, 27: 0.0, 28: 0.8, 31: 0.4}
Micro-average F1 score: 0.45739910313901344
Weighted-average F1 score: 0.3587592835299257

F1 score per class: {32: 0.7142857142857143, 2: 0.288135593220339, 6: 0.030303030303030304, 7: 0.96, 40: 0.6616541353383458, 39: 0.18018018018018017, 11: 0.6857142857142857, 12: 0.2727272727272727, 9: 0.7597765363128491, 19: 0.2727272727272727, 24: 0.38095238095238093, 26: 0.918918918918919, 27: 0.6666666666666666, 28: 0.8969072164948454, 29: 0.13333333333333333, 31: 0.15}
Micro-average F1 score: 0.607681755829904
Weighted-average F1 score: 0.6562914529222944
F1 score per class: {32: 0.875, 2: 0.4492753623188406, 6: 0.034482758620689655, 7: 0.9615384615384616, 39: 0.8211920529801324, 40: 0.5974025974025974, 11: 0.6721991701244814, 12: 0.4, 9: 0.7666666666666667, 19: 0.42857142857142855, 24: 0.25, 26: 0.9197860962566845, 27: 0.8, 28: 0.8923076923076924, 29: 0.3333333333333333, 31: 0.3302752293577982}
Micro-average F1 score: 0.6700125470514429
Weighted-average F1 score: 0.6587175184421576
F1 score per class: {32: 0.8, 2: 0.4090909090909091, 6: 0.03225806451612903, 7: 0.9615384615384616, 40: 0.8157894736842105, 39: 0.5405405405405406, 11: 0.6666666666666666, 12: 0.3448275862068966, 9: 0.7666666666666667, 19: 0.4, 24: 0.24242424242424243, 26: 0.9247311827956989, 27: 0.8, 28: 0.8923076923076924, 29: 0.25, 31: 0.32075471698113206}
Micro-average F1 score: 0.6565656565656566
Weighted-average F1 score: 0.6488284569448245
cur_acc:  ['0.8038', '0.4056', '0.3365']
his_acc:  ['0.8038', '0.6983', '0.6077']
cur_acc des:  ['0.8315', '0.6554', '0.4664']
his_acc des:  ['0.8315', '0.7798', '0.6700']
cur_acc rrf:  ['0.8270', '0.6629', '0.4574']
his_acc rrf:  ['0.8270', '0.7642', '0.6566']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 351.2193219CurrentTrain: epoch  0, batch     1 | loss: 362.5170948CurrentTrain: epoch  0, batch     2 | loss: 349.7841384CurrentTrain: epoch  0, batch     3 | loss: 236.8084529CurrentTrain: epoch  1, batch     0 | loss: 301.1895720CurrentTrain: epoch  1, batch     1 | loss: 475.2815966CurrentTrain: epoch  1, batch     2 | loss: 316.2501080CurrentTrain: epoch  1, batch     3 | loss: 230.3672168CurrentTrain: epoch  2, batch     0 | loss: 371.7566474CurrentTrain: epoch  2, batch     1 | loss: 310.6637770CurrentTrain: epoch  2, batch     2 | loss: 370.8087573CurrentTrain: epoch  2, batch     3 | loss: 222.3234954CurrentTrain: epoch  3, batch     0 | loss: 280.4322790CurrentTrain: epoch  3, batch     1 | loss: 413.1836919CurrentTrain: epoch  3, batch     2 | loss: 380.0606488CurrentTrain: epoch  3, batch     3 | loss: 265.3159584CurrentTrain: epoch  4, batch     0 | loss: 381.7459835CurrentTrain: epoch  4, batch     1 | loss: 293.4219679CurrentTrain: epoch  4, batch     2 | loss: 322.1305498CurrentTrain: epoch  4, batch     3 | loss: 231.1514271CurrentTrain: epoch  5, batch     0 | loss: 335.1396563CurrentTrain: epoch  5, batch     1 | loss: 423.3626595CurrentTrain: epoch  5, batch     2 | loss: 297.1439293CurrentTrain: epoch  5, batch     3 | loss: 229.7261328CurrentTrain: epoch  6, batch     0 | loss: 279.0893177CurrentTrain: epoch  6, batch     1 | loss: 316.1497440CurrentTrain: epoch  6, batch     2 | loss: 382.5549429CurrentTrain: epoch  6, batch     3 | loss: 351.5506656CurrentTrain: epoch  7, batch     0 | loss: 303.8185399CurrentTrain: epoch  7, batch     1 | loss: 364.0809672CurrentTrain: epoch  7, batch     2 | loss: 330.8226993CurrentTrain: epoch  7, batch     3 | loss: 260.0784030CurrentTrain: epoch  8, batch     0 | loss: 328.1361112CurrentTrain: epoch  8, batch     1 | loss: 316.9139430CurrentTrain: epoch  8, batch     2 | loss: 316.0441628CurrentTrain: epoch  8, batch     3 | loss: 280.8228550CurrentTrain: epoch  9, batch     0 | loss: 360.2983701CurrentTrain: epoch  9, batch     1 | loss: 328.9610914CurrentTrain: epoch  9, batch     2 | loss: 345.9286100CurrentTrain: epoch  9, batch     3 | loss: 215.4982428
MemoryTrain:  epoch  0, batch     0 | loss: 1.8813537MemoryTrain:  epoch  1, batch     0 | loss: 1.4684369MemoryTrain:  epoch  2, batch     0 | loss: 1.1475982MemoryTrain:  epoch  3, batch     0 | loss: 1.0108301MemoryTrain:  epoch  4, batch     0 | loss: 0.7914084MemoryTrain:  epoch  5, batch     0 | loss: 0.6533854MemoryTrain:  epoch  6, batch     0 | loss: 0.5958403MemoryTrain:  epoch  7, batch     0 | loss: 0.4489361MemoryTrain:  epoch  8, batch     0 | loss: 0.3691140MemoryTrain:  epoch  9, batch     0 | loss: 0.3340571

F1 score per class: {32: 0.0, 35: 0.8888888888888888, 37: 0.375, 38: 0.0, 11: 0.0, 15: 0.0, 25: 0.47761194029850745, 26: 0.651685393258427, 27: 0.6976744186046512}
Micro-average F1 score: 0.5460750853242321
Weighted-average F1 score: 0.5284445091244715
F1 score per class: {32: 0.0, 35: 0.0, 37: 0.8235294117647058, 38: 0.0, 6: 0.9166666666666666, 40: 0.0, 11: 0.0, 15: 0.0, 24: 0.0, 25: 0.96, 26: 0.8118811881188119, 27: 0.9056603773584906, 28: 0.0}
Micro-average F1 score: 0.8431876606683805
Weighted-average F1 score: 0.7929078583112756
F1 score per class: {32: 0.0, 35: 0.8888888888888888, 37: 0.8181818181818182, 38: 0.0, 40: 0.0, 11: 0.0, 15: 0.0, 25: 0.9166666666666666, 26: 0.8118811881188119, 27: 0.9090909090909091, 28: 0.0}
Micro-average F1 score: 0.8083989501312336
Weighted-average F1 score: 0.7536013323554578

F1 score per class: {2: 0.8, 6: 0.3, 7: 0.03125, 9: 0.9803921568627451, 11: 0.5648854961832062, 12: 0.17699115044247787, 15: 0.8421052631578947, 19: 0.6448979591836734, 24: 0.2222222222222222, 25: 0.375, 26: 0.7540983606557377, 27: 0.38461538461538464, 28: 0.7142857142857143, 29: 0.9247311827956989, 31: 0.0, 32: 0.7978142076502732, 35: 0.47058823529411764, 37: 0.6170212765957447, 38: 0.6122448979591837, 39: 0.13333333333333333, 40: 0.14814814814814814}
Micro-average F1 score: 0.576
Weighted-average F1 score: 0.620129128022139
F1 score per class: {2: 0.875, 6: 0.5534591194968553, 7: 0.037037037037037035, 9: 0.9803921568627451, 11: 0.64, 12: 0.6987951807228916, 15: 0.6666666666666666, 19: 0.6666666666666666, 24: 0.3870967741935484, 25: 0.9166666666666666, 26: 0.7540983606557377, 27: 0.4666666666666667, 28: 0.4444444444444444, 29: 0.9312169312169312, 31: 0.8, 32: 0.8762886597938144, 35: 0.8727272727272727, 37: 0.5815602836879432, 38: 0.7272727272727273, 39: 0.0, 40: 0.36363636363636365}
Micro-average F1 score: 0.6899563318777293
Weighted-average F1 score: 0.6809318259353595
F1 score per class: {2: 0.875, 6: 0.527027027027027, 7: 0.03508771929824561, 9: 0.9615384615384616, 11: 0.631578947368421, 12: 0.5211267605633803, 15: 0.8, 19: 0.6694214876033058, 24: 0.2857142857142857, 25: 0.8181818181818182, 26: 0.75, 27: 0.4375, 28: 0.4444444444444444, 29: 0.93048128342246, 31: 0.8, 32: 0.8673469387755102, 35: 0.8301886792452831, 37: 0.5774647887323944, 38: 0.6756756756756757, 39: 0.0, 40: 0.3418803418803419}
Micro-average F1 score: 0.6623952686052242
Weighted-average F1 score: 0.6582851636845618
cur_acc:  ['0.8038', '0.4056', '0.3365', '0.5461']
his_acc:  ['0.8038', '0.6983', '0.6077', '0.5760']
cur_acc des:  ['0.8315', '0.6554', '0.4664', '0.8432']
his_acc des:  ['0.8315', '0.7798', '0.6700', '0.6900']
cur_acc rrf:  ['0.8270', '0.6629', '0.4574', '0.8084']
his_acc rrf:  ['0.8270', '0.7642', '0.6566', '0.6624']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 443.4683412CurrentTrain: epoch  0, batch     1 | loss: 388.8353055CurrentTrain: epoch  0, batch     2 | loss: 381.6139337CurrentTrain: epoch  0, batch     3 | loss: 338.8001758CurrentTrain: epoch  0, batch     4 | loss: 189.2268357CurrentTrain: epoch  1, batch     0 | loss: 363.9717841CurrentTrain: epoch  1, batch     1 | loss: 324.9814457CurrentTrain: epoch  1, batch     2 | loss: 332.7149189CurrentTrain: epoch  1, batch     3 | loss: 444.3560601CurrentTrain: epoch  1, batch     4 | loss: 255.4185320CurrentTrain: epoch  2, batch     0 | loss: 442.6698675CurrentTrain: epoch  2, batch     1 | loss: 332.6602839CurrentTrain: epoch  2, batch     2 | loss: 445.6748855CurrentTrain: epoch  2, batch     3 | loss: 298.7411414CurrentTrain: epoch  2, batch     4 | loss: 220.0503390CurrentTrain: epoch  3, batch     0 | loss: 360.7347896CurrentTrain: epoch  3, batch     1 | loss: 362.1662270CurrentTrain: epoch  3, batch     2 | loss: 349.4711439CurrentTrain: epoch  3, batch     3 | loss: 358.1389190CurrentTrain: epoch  3, batch     4 | loss: 221.4128596CurrentTrain: epoch  4, batch     0 | loss: 290.9467278CurrentTrain: epoch  4, batch     1 | loss: 440.8425655CurrentTrain: epoch  4, batch     2 | loss: 363.0444401CurrentTrain: epoch  4, batch     3 | loss: 317.1795374CurrentTrain: epoch  4, batch     4 | loss: 427.9288814CurrentTrain: epoch  5, batch     0 | loss: 362.6797400CurrentTrain: epoch  5, batch     1 | loss: 342.2447116CurrentTrain: epoch  5, batch     2 | loss: 394.5113639CurrentTrain: epoch  5, batch     3 | loss: 314.3519642CurrentTrain: epoch  5, batch     4 | loss: 305.3589246CurrentTrain: epoch  6, batch     0 | loss: 405.9739835CurrentTrain: epoch  6, batch     1 | loss: 374.4772787CurrentTrain: epoch  6, batch     2 | loss: 375.9358403CurrentTrain: epoch  6, batch     3 | loss: 360.9045316CurrentTrain: epoch  6, batch     4 | loss: 207.1248807CurrentTrain: epoch  7, batch     0 | loss: 261.6728779CurrentTrain: epoch  7, batch     1 | loss: 342.5983636CurrentTrain: epoch  7, batch     2 | loss: 438.3735422CurrentTrain: epoch  7, batch     3 | loss: 391.0895373CurrentTrain: epoch  7, batch     4 | loss: 302.3099982CurrentTrain: epoch  8, batch     0 | loss: 419.6782807CurrentTrain: epoch  8, batch     1 | loss: 341.5733240CurrentTrain: epoch  8, batch     2 | loss: 341.2446850CurrentTrain: epoch  8, batch     3 | loss: 358.8154754CurrentTrain: epoch  8, batch     4 | loss: 233.4990482CurrentTrain: epoch  9, batch     0 | loss: 325.0112926CurrentTrain: epoch  9, batch     1 | loss: 374.4831242CurrentTrain: epoch  9, batch     2 | loss: 300.6203225CurrentTrain: epoch  9, batch     3 | loss: 391.0326969CurrentTrain: epoch  9, batch     4 | loss: 300.9278567
MemoryTrain:  epoch  0, batch     0 | loss: 1.7619782MemoryTrain:  epoch  1, batch     0 | loss: 1.6312901MemoryTrain:  epoch  2, batch     0 | loss: 1.3999190MemoryTrain:  epoch  3, batch     0 | loss: 0.9984105MemoryTrain:  epoch  4, batch     0 | loss: 0.8260703MemoryTrain:  epoch  5, batch     0 | loss: 0.7215146MemoryTrain:  epoch  6, batch     0 | loss: 0.5196466MemoryTrain:  epoch  7, batch     0 | loss: 0.4050471MemoryTrain:  epoch  8, batch     0 | loss: 0.3550708MemoryTrain:  epoch  9, batch     0 | loss: 0.3053777

F1 score per class: {32: 0.27586206896551724, 1: 0.11494252873563218, 34: 0.0, 35: 0.02857142857142857, 3: 0.6590909090909091, 37: 0.0, 11: 0.0, 14: 0.0, 22: 0.0, 24: 0.3582089552238806, 26: 0.0, 27: 0.0}
Micro-average F1 score: 0.32280701754385965
Weighted-average F1 score: 0.3306377515037995
F1 score per class: {32: 0.40310077519379844, 1: 0.4954128440366973, 34: 0.0, 35: 0.0, 3: 0.0, 37: 0.05263157894736842, 38: 0.6463414634146342, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.865979381443299, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.45385779122541603
Weighted-average F1 score: 0.39658346687069806
F1 score per class: {32: 0.38095238095238093, 1: 0.3434343434343434, 34: 0.0, 35: 0.05128205128205128, 3: 0.6666666666666666, 37: 0.0, 38: 0.0, 11: 0.0, 14: 0.625, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.3786259541984733
Weighted-average F1 score: 0.3168439911228373

F1 score per class: {1: 0.25196850393700787, 2: 0.8, 3: 0.11235955056179775, 6: 0.32786885245901637, 7: 0.04081632653061224, 9: 0.9803921568627451, 11: 0.39344262295081966, 12: 0.14678899082568808, 14: 0.02631578947368421, 15: 0.7058823529411765, 19: 0.5463917525773195, 22: 0.6270270270270271, 24: 0.07142857142857142, 25: 0.375, 26: 0.7526881720430108, 27: 0.0, 28: 0.625, 29: 0.8764044943820225, 31: 0.0, 32: 0.632258064516129, 34: 0.24, 35: 0.20454545454545456, 37: 0.4583333333333333, 38: 0.56, 39: 0.0, 40: 0.13333333333333333}
Micro-average F1 score: 0.449034575662326
Weighted-average F1 score: 0.49385256869705174
F1 score per class: {1: 0.35135135135135137, 2: 0.8, 3: 0.4090909090909091, 6: 0.52, 7: 0.0, 9: 0.9615384615384616, 11: 0.3302752293577982, 12: 0.5786163522012578, 14: 0.05063291139240506, 15: 0.631578947368421, 19: 0.6206896551724138, 22: 0.6235294117647059, 24: 0.04878048780487805, 25: 0.7906976744186046, 26: 0.7567567567567568, 27: 0.0, 28: 0.37037037037037035, 29: 0.9130434782608695, 31: 0.0, 32: 0.8306010928961749, 34: 0.4692737430167598, 35: 0.4305555555555556, 37: 0.6274509803921569, 38: 0.5945945945945946, 39: 0.125, 40: 0.5042016806722689}
Micro-average F1 score: 0.5598484848484848
Weighted-average F1 score: 0.5549357906473918
F1 score per class: {1: 0.3333333333333333, 2: 0.8, 3: 0.2982456140350877, 6: 0.46808510638297873, 7: 0.0, 9: 0.9803921568627451, 11: 0.42748091603053434, 12: 0.25, 14: 0.047619047619047616, 15: 0.631578947368421, 19: 0.5953488372093023, 22: 0.6436781609195402, 24: 0.05128205128205128, 25: 0.7469879518072289, 26: 0.7608695652173914, 27: 0.0, 28: 0.4, 29: 0.9130434782608695, 31: 0.0, 32: 0.8387096774193549, 34: 0.352112676056338, 35: 0.4161073825503356, 37: 0.5161290322580645, 38: 0.5679012345679012, 39: 0.125, 40: 0.4339622641509434}
Micro-average F1 score: 0.5244890088700347
Weighted-average F1 score: 0.5280838804260944
cur_acc:  ['0.8038', '0.4056', '0.3365', '0.5461', '0.3228']
his_acc:  ['0.8038', '0.6983', '0.6077', '0.5760', '0.4490']
cur_acc des:  ['0.8315', '0.6554', '0.4664', '0.8432', '0.4539']
his_acc des:  ['0.8315', '0.7798', '0.6700', '0.6900', '0.5598']
cur_acc rrf:  ['0.8270', '0.6629', '0.4574', '0.8084', '0.3786']
his_acc rrf:  ['0.8270', '0.7642', '0.6566', '0.6624', '0.5245']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 382.9031108CurrentTrain: epoch  0, batch     1 | loss: 339.9242031CurrentTrain: epoch  0, batch     2 | loss: 331.7879059CurrentTrain: epoch  0, batch     3 | loss: 386.8412305CurrentTrain: epoch  1, batch     0 | loss: 421.0232927CurrentTrain: epoch  1, batch     1 | loss: 365.1404458CurrentTrain: epoch  1, batch     2 | loss: 296.3842079CurrentTrain: epoch  1, batch     3 | loss: 254.4374802CurrentTrain: epoch  2, batch     0 | loss: 331.4507120CurrentTrain: epoch  2, batch     1 | loss: 327.7744764CurrentTrain: epoch  2, batch     2 | loss: 354.1790304CurrentTrain: epoch  2, batch     3 | loss: 301.9148602CurrentTrain: epoch  3, batch     0 | loss: 368.2078838CurrentTrain: epoch  3, batch     1 | loss: 302.9966896CurrentTrain: epoch  3, batch     2 | loss: 380.3062694CurrentTrain: epoch  3, batch     3 | loss: 246.1717401CurrentTrain: epoch  4, batch     0 | loss: 362.2147202CurrentTrain: epoch  4, batch     1 | loss: 335.9741689CurrentTrain: epoch  4, batch     2 | loss: 348.4281872CurrentTrain: epoch  4, batch     3 | loss: 269.3700238CurrentTrain: epoch  5, batch     0 | loss: 340.1969760CurrentTrain: epoch  5, batch     1 | loss: 360.0606138CurrentTrain: epoch  5, batch     2 | loss: 289.5932066CurrentTrain: epoch  5, batch     3 | loss: 438.8682939CurrentTrain: epoch  6, batch     0 | loss: 390.7535304CurrentTrain: epoch  6, batch     1 | loss: 267.9838935CurrentTrain: epoch  6, batch     2 | loss: 423.8526419CurrentTrain: epoch  6, batch     3 | loss: 283.7419065CurrentTrain: epoch  7, batch     0 | loss: 341.2795053CurrentTrain: epoch  7, batch     1 | loss: 311.8740688CurrentTrain: epoch  7, batch     2 | loss: 368.7062857CurrentTrain: epoch  7, batch     3 | loss: 281.4410820CurrentTrain: epoch  8, batch     0 | loss: 359.2355136CurrentTrain: epoch  8, batch     1 | loss: 331.2539056CurrentTrain: epoch  8, batch     2 | loss: 300.5664575CurrentTrain: epoch  8, batch     3 | loss: 294.1607813CurrentTrain: epoch  9, batch     0 | loss: 314.6878769CurrentTrain: epoch  9, batch     1 | loss: 333.6051747CurrentTrain: epoch  9, batch     2 | loss: 340.5446741CurrentTrain: epoch  9, batch     3 | loss: 294.2668443
MemoryTrain:  epoch  0, batch     0 | loss: 1.6832030MemoryTrain:  epoch  1, batch     0 | loss: 1.3271591MemoryTrain:  epoch  2, batch     0 | loss: 0.9451859MemoryTrain:  epoch  3, batch     0 | loss: 0.7061322MemoryTrain:  epoch  4, batch     0 | loss: 0.5652882MemoryTrain:  epoch  5, batch     0 | loss: 0.5114915MemoryTrain:  epoch  6, batch     0 | loss: 0.4213532MemoryTrain:  epoch  7, batch     0 | loss: 0.3926437MemoryTrain:  epoch  8, batch     0 | loss: 0.3437584MemoryTrain:  epoch  9, batch     0 | loss: 0.2756668

F1 score per class: {0: 0.9722222222222222, 1: 0.0, 34: 0.0, 2: 0.7261146496815286, 4: 0.3333333333333333, 37: 0.41025641025641024, 13: 0.0, 21: 0.8941176470588236, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7091836734693877
Weighted-average F1 score: 0.6570242326351443
F1 score per class: {0: 1.0, 1: 0.0, 34: 0.0, 2: 0.8636363636363636, 4: 0.0, 37: 0.5714285714285714, 38: 0.0, 35: 0.6521739130434783, 40: 0.0, 9: 0.8674698795180723, 13: 0.0, 14: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.772093023255814
Weighted-average F1 score: 0.6894797387501123
F1 score per class: {0: 1.0, 1: 0.0, 34: 0.0, 2: 0.8095238095238095, 4: 0.5714285714285714, 37: 0.0, 38: 0.7083333333333334, 40: 0.0, 13: 0.8674698795180723, 14: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7511737089201878
Weighted-average F1 score: 0.6624323229116187

F1 score per class: {0: 0.958904109589041, 1: 0.2753623188405797, 2: 0.6086956521739131, 3: 0.15384615384615385, 4: 0.7261146496815286, 6: 0.3, 7: 0.044444444444444446, 9: 0.9803921568627451, 11: 0.29357798165137616, 12: 0.07476635514018691, 13: 0.16666666666666666, 14: 0.0547945205479452, 15: 0.75, 19: 0.579185520361991, 21: 0.24242424242424243, 22: 0.6369426751592356, 23: 0.8539325842696629, 24: 0.09090909090909091, 25: 0.44776119402985076, 26: 0.7513812154696132, 27: 0.0, 28: 0.4, 29: 0.8961748633879781, 31: 0.6666666666666666, 32: 0.5906040268456376, 34: 0.25806451612903225, 35: 0.26262626262626265, 37: 0.5106382978723404, 38: 0.43478260869565216, 39: 0.0, 40: 0.08108108108108109}
Micro-average F1 score: 0.48404255319148937
Weighted-average F1 score: 0.5275316433509627
F1 score per class: {0: 0.9736842105263158, 1: 0.3246753246753247, 2: 0.4827586206896552, 3: 0.45925925925925926, 4: 0.8636363636363636, 6: 0.6075949367088608, 7: 0.0, 9: 0.9090909090909091, 11: 0.39655172413793105, 12: 0.46357615894039733, 13: 0.19047619047619047, 14: 0.044444444444444446, 15: 0.75, 19: 0.6375545851528385, 21: 0.3333333333333333, 22: 0.6875, 23: 0.8, 24: 0.0, 25: 0.8444444444444444, 26: 0.7329842931937173, 27: 0.0, 28: 0.42857142857142855, 29: 0.8839779005524862, 31: 0.8, 32: 0.768361581920904, 34: 0.43820224719101125, 35: 0.5, 37: 0.5523809523809524, 38: 0.6153846153846154, 39: 0.13333333333333333, 40: 0.4}
Micro-average F1 score: 0.5841902313624678
Weighted-average F1 score: 0.5781630038019615
F1 score per class: {0: 0.9736842105263158, 1: 0.33766233766233766, 2: 0.5, 3: 0.43333333333333335, 4: 0.8095238095238095, 6: 0.5342465753424658, 7: 0.0, 9: 0.9433962264150944, 11: 0.4098360655737705, 12: 0.35555555555555557, 13: 0.19047619047619047, 14: 0.0449438202247191, 15: 0.75, 19: 0.6410256410256411, 21: 0.37362637362637363, 22: 0.6792452830188679, 23: 0.8275862068965517, 24: 0.0, 25: 0.7469879518072289, 26: 0.7291666666666666, 27: 0.0, 28: 0.4, 29: 0.9081081081081082, 31: 1.0, 32: 0.8216216216216217, 34: 0.37209302325581395, 35: 0.4794520547945205, 37: 0.5555555555555556, 38: 0.611764705882353, 39: 0.13333333333333333, 40: 0.34951456310679613}
Micro-average F1 score: 0.5704065040650407
Weighted-average F1 score: 0.5670309537945815
cur_acc:  ['0.8038', '0.4056', '0.3365', '0.5461', '0.3228', '0.7092']
his_acc:  ['0.8038', '0.6983', '0.6077', '0.5760', '0.4490', '0.4840']
cur_acc des:  ['0.8315', '0.6554', '0.4664', '0.8432', '0.4539', '0.7721']
his_acc des:  ['0.8315', '0.7798', '0.6700', '0.6900', '0.5598', '0.5842']
cur_acc rrf:  ['0.8270', '0.6629', '0.4574', '0.8084', '0.3786', '0.7512']
his_acc rrf:  ['0.8270', '0.7642', '0.6566', '0.6624', '0.5245', '0.5704']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 374.0651742CurrentTrain: epoch  0, batch     1 | loss: 356.4759559CurrentTrain: epoch  0, batch     2 | loss: 331.4477122CurrentTrain: epoch  0, batch     3 | loss: 214.2846143CurrentTrain: epoch  1, batch     0 | loss: 354.4742718CurrentTrain: epoch  1, batch     1 | loss: 323.6142225CurrentTrain: epoch  1, batch     2 | loss: 298.1782506CurrentTrain: epoch  1, batch     3 | loss: 245.2929603CurrentTrain: epoch  2, batch     0 | loss: 420.6281610CurrentTrain: epoch  2, batch     1 | loss: 303.3564278CurrentTrain: epoch  2, batch     2 | loss: 331.8795631CurrentTrain: epoch  2, batch     3 | loss: 182.6245522CurrentTrain: epoch  3, batch     0 | loss: 331.2233126CurrentTrain: epoch  3, batch     1 | loss: 362.0877786CurrentTrain: epoch  3, batch     2 | loss: 345.5842897CurrentTrain: epoch  3, batch     3 | loss: 174.4204182CurrentTrain: epoch  4, batch     0 | loss: 302.5875715CurrentTrain: epoch  4, batch     1 | loss: 345.0135268CurrentTrain: epoch  4, batch     2 | loss: 316.6998994CurrentTrain: epoch  4, batch     3 | loss: 272.8221360CurrentTrain: epoch  5, batch     0 | loss: 262.2660530CurrentTrain: epoch  5, batch     1 | loss: 391.6799869CurrentTrain: epoch  5, batch     2 | loss: 420.8060155CurrentTrain: epoch  5, batch     3 | loss: 196.8362264CurrentTrain: epoch  6, batch     0 | loss: 301.1438735CurrentTrain: epoch  6, batch     1 | loss: 375.3996456CurrentTrain: epoch  6, batch     2 | loss: 375.1007916CurrentTrain: epoch  6, batch     3 | loss: 172.7619707CurrentTrain: epoch  7, batch     0 | loss: 246.1849835CurrentTrain: epoch  7, batch     1 | loss: 326.8081480CurrentTrain: epoch  7, batch     2 | loss: 340.7292581CurrentTrain: epoch  7, batch     3 | loss: 463.3958083CurrentTrain: epoch  8, batch     0 | loss: 328.9367167CurrentTrain: epoch  8, batch     1 | loss: 419.9864678CurrentTrain: epoch  8, batch     2 | loss: 285.4143585CurrentTrain: epoch  8, batch     3 | loss: 183.6387386CurrentTrain: epoch  9, batch     0 | loss: 269.7195759CurrentTrain: epoch  9, batch     1 | loss: 340.0965017CurrentTrain: epoch  9, batch     2 | loss: 371.2226577CurrentTrain: epoch  9, batch     3 | loss: 308.4077967
MemoryTrain:  epoch  0, batch     0 | loss: 0.9423414MemoryTrain:  epoch  1, batch     0 | loss: 0.7201159MemoryTrain:  epoch  2, batch     0 | loss: 0.6032154MemoryTrain:  epoch  3, batch     0 | loss: 0.4522037MemoryTrain:  epoch  4, batch     0 | loss: 0.4021959MemoryTrain:  epoch  5, batch     0 | loss: 0.3292826MemoryTrain:  epoch  6, batch     0 | loss: 0.3054963MemoryTrain:  epoch  7, batch     0 | loss: 0.2411732MemoryTrain:  epoch  8, batch     0 | loss: 0.1971132MemoryTrain:  epoch  9, batch     0 | loss: 0.1826143

F1 score per class: {33: 0.5535714285714286, 34: 0.0, 36: 0.0, 37: 0.92, 38: 0.0, 8: 0.0, 13: 0.0, 14: 0.9142857142857143, 20: 0.42857142857142855, 26: 0.0, 28: 0.1917808219178082, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5802816901408451
Weighted-average F1 score: 0.6188505302695537
F1 score per class: {0: 0.0, 2: 0.0, 3: 0.0, 7: 0.0, 8: 0.7633587786259542, 12: 0.0, 13: 0.0, 20: 0.8888888888888888, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.42857142857142855, 34: 0.0, 36: 0.847457627118644, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.7597254004576659
Weighted-average F1 score: 0.6918516981902014
F1 score per class: {33: 0.0, 2: 0.0, 34: 0.7538461538461538, 36: 0.0, 37: 0.0, 38: 0.9215686274509803, 7: 0.0, 8: 0.0, 40: 0.0, 12: 1.0, 13: 0.42857142857142855, 20: 0.0, 26: 0.8141592920353983, 28: 0.0, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.7610208816705336
Weighted-average F1 score: 0.6986247833374329

F1 score per class: {0: 0.958904109589041, 1: 0.2714285714285714, 2: 0.6666666666666666, 3: 0.11363636363636363, 4: 0.9010989010989011, 6: 0.18803418803418803, 7: 0.0, 8: 0.5040650406504065, 9: 0.9803921568627451, 11: 0.20408163265306123, 12: 0.0380952380952381, 13: 0.14285714285714285, 14: 0.05194805194805195, 15: 0.75, 19: 0.5959595959595959, 20: 0.7022900763358778, 21: 0.15384615384615385, 22: 0.6013071895424836, 23: 0.7951807228915663, 24: 0.0, 25: 0.375, 26: 0.7431693989071039, 27: 0.0, 28: 0.36363636363636365, 29: 0.8913043478260869, 30: 0.9142857142857143, 31: 0.6666666666666666, 32: 0.5285714285714286, 33: 0.25, 34: 0.17647058823529413, 35: 0.2553191489361702, 36: 0.18181818181818182, 37: 0.4318181818181818, 38: 0.2631578947368421, 39: 0.0, 40: 0.10810810810810811}
Micro-average F1 score: 0.47928791509756935
Weighted-average F1 score: 0.5520981425657032
F1 score per class: {0: 0.9473684210526315, 1: 0.34210526315789475, 2: 0.5833333333333334, 3: 0.5112781954887218, 4: 0.93048128342246, 6: 0.3511450381679389, 7: 0.0, 8: 0.5208333333333334, 9: 0.9433962264150944, 11: 0.4793388429752066, 12: 0.42758620689655175, 13: 0.26666666666666666, 14: 0.05194805194805195, 15: 0.75, 19: 0.6232558139534884, 20: 0.7154471544715447, 21: 0.36585365853658536, 22: 0.703030303030303, 23: 0.8314606741573034, 24: 0.0, 25: 0.5205479452054794, 26: 0.7195767195767195, 27: 0.0, 28: 0.3333333333333333, 29: 0.8983957219251337, 30: 0.8444444444444444, 31: 1.0, 32: 0.8260869565217391, 33: 0.20689655172413793, 34: 0.44755244755244755, 35: 0.4520547945205479, 36: 0.704225352112676, 37: 0.4835164835164835, 38: 0.5897435897435898, 39: 0.23529411764705882, 40: 0.45045045045045046}
Micro-average F1 score: 0.591180654338549
Weighted-average F1 score: 0.5986249549974554
F1 score per class: {0: 0.935064935064935, 1: 0.3466666666666667, 2: 0.5, 3: 0.4462809917355372, 4: 0.9247311827956989, 6: 0.32558139534883723, 7: 0.0, 8: 0.5268817204301075, 9: 0.9615384615384616, 11: 0.4838709677419355, 12: 0.4166666666666667, 13: 0.26666666666666666, 14: 0.04878048780487805, 15: 0.75, 19: 0.6301369863013698, 20: 0.7121212121212122, 21: 0.35714285714285715, 22: 0.6790123456790124, 23: 0.8275862068965517, 24: 0.0, 25: 0.44776119402985076, 26: 0.723404255319149, 27: 0.0, 28: 0.3333333333333333, 29: 0.9032258064516129, 30: 0.8444444444444444, 31: 1.0, 32: 0.8404255319148937, 33: 0.1875, 34: 0.3247863247863248, 35: 0.44285714285714284, 36: 0.6618705035971223, 37: 0.43137254901960786, 38: 0.5789473684210527, 39: 0.125, 40: 0.45454545454545453}
Micro-average F1 score: 0.5779948290721058
Weighted-average F1 score: 0.5879241058731846
cur_acc:  ['0.8038', '0.4056', '0.3365', '0.5461', '0.3228', '0.7092', '0.5803']
his_acc:  ['0.8038', '0.6983', '0.6077', '0.5760', '0.4490', '0.4840', '0.4793']
cur_acc des:  ['0.8315', '0.6554', '0.4664', '0.8432', '0.4539', '0.7721', '0.7597']
his_acc des:  ['0.8315', '0.7798', '0.6700', '0.6900', '0.5598', '0.5842', '0.5912']
cur_acc rrf:  ['0.8270', '0.6629', '0.4574', '0.8084', '0.3786', '0.7512', '0.7610']
his_acc rrf:  ['0.8270', '0.7642', '0.6566', '0.6624', '0.5245', '0.5704', '0.5780']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 345.4727561CurrentTrain: epoch  0, batch     1 | loss: 407.4305707CurrentTrain: epoch  0, batch     2 | loss: 404.3930201CurrentTrain: epoch  0, batch     3 | loss: 398.5531565CurrentTrain: epoch  0, batch     4 | loss: 195.6237673CurrentTrain: epoch  1, batch     0 | loss: 331.9774776CurrentTrain: epoch  1, batch     1 | loss: 348.0704462CurrentTrain: epoch  1, batch     2 | loss: 351.7061465CurrentTrain: epoch  1, batch     3 | loss: 443.2556101CurrentTrain: epoch  1, batch     4 | loss: 271.8768044CurrentTrain: epoch  2, batch     0 | loss: 459.0110805CurrentTrain: epoch  2, batch     1 | loss: 302.3108154CurrentTrain: epoch  2, batch     2 | loss: 345.8452314CurrentTrain: epoch  2, batch     3 | loss: 440.6900524CurrentTrain: epoch  2, batch     4 | loss: 239.3155841CurrentTrain: epoch  3, batch     0 | loss: 541.7133014CurrentTrain: epoch  3, batch     1 | loss: 331.0975870CurrentTrain: epoch  3, batch     2 | loss: 331.9589651CurrentTrain: epoch  3, batch     3 | loss: 320.5793070CurrentTrain: epoch  3, batch     4 | loss: 310.1346160CurrentTrain: epoch  4, batch     0 | loss: 376.2073002CurrentTrain: epoch  4, batch     1 | loss: 390.8715386CurrentTrain: epoch  4, batch     2 | loss: 329.9217965CurrentTrain: epoch  4, batch     3 | loss: 343.2658935CurrentTrain: epoch  4, batch     4 | loss: 248.6464768CurrentTrain: epoch  5, batch     0 | loss: 374.3577862CurrentTrain: epoch  5, batch     1 | loss: 326.3717767CurrentTrain: epoch  5, batch     2 | loss: 312.7439400CurrentTrain: epoch  5, batch     3 | loss: 455.6227291CurrentTrain: epoch  5, batch     4 | loss: 251.8663595CurrentTrain: epoch  6, batch     0 | loss: 407.9397789CurrentTrain: epoch  6, batch     1 | loss: 343.2154779CurrentTrain: epoch  6, batch     2 | loss: 327.9693069CurrentTrain: epoch  6, batch     3 | loss: 331.9481855CurrentTrain: epoch  6, batch     4 | loss: 249.1187423CurrentTrain: epoch  7, batch     0 | loss: 437.0986953CurrentTrain: epoch  7, batch     1 | loss: 340.5562203CurrentTrain: epoch  7, batch     2 | loss: 391.4514820CurrentTrain: epoch  7, batch     3 | loss: 314.2742411CurrentTrain: epoch  7, batch     4 | loss: 208.6168080CurrentTrain: epoch  8, batch     0 | loss: 310.7919325CurrentTrain: epoch  8, batch     1 | loss: 408.4734702CurrentTrain: epoch  8, batch     2 | loss: 373.8641409CurrentTrain: epoch  8, batch     3 | loss: 407.2917145CurrentTrain: epoch  8, batch     4 | loss: 195.0664395CurrentTrain: epoch  9, batch     0 | loss: 372.9363301CurrentTrain: epoch  9, batch     1 | loss: 341.7703947CurrentTrain: epoch  9, batch     2 | loss: 455.8175125CurrentTrain: epoch  9, batch     3 | loss: 340.2499110CurrentTrain: epoch  9, batch     4 | loss: 207.6174505
MemoryTrain:  epoch  0, batch     0 | loss: 1.3660238MemoryTrain:  epoch  1, batch     0 | loss: 0.9311809MemoryTrain:  epoch  2, batch     0 | loss: 0.7538390MemoryTrain:  epoch  3, batch     0 | loss: 0.5461907MemoryTrain:  epoch  4, batch     0 | loss: 0.4031574MemoryTrain:  epoch  5, batch     0 | loss: 0.3267488MemoryTrain:  epoch  6, batch     0 | loss: 0.2699183MemoryTrain:  epoch  7, batch     0 | loss: 0.2229581MemoryTrain:  epoch  8, batch     0 | loss: 0.2016305MemoryTrain:  epoch  9, batch     0 | loss: 0.2033364

F1 score per class: {5: 0.9795918367346939, 38: 0.0, 7: 0.0, 8: 0.3870967741935484, 10: 0.0, 11: 0.0, 13: 0.8679245283018868, 16: 0.2, 17: 0.7666666666666667, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.7121535181236673
Weighted-average F1 score: 0.7312266134272151
F1 score per class: {5: 0.9900990099009901, 38: 0.0, 6: 0.0, 8: 0.5507246376811594, 10: 0.0, 39: 0.0, 11: 0.9655172413793104, 13: 0.7142857142857143, 16: 0.9577464788732394, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7522935779816514
Weighted-average F1 score: 0.688055490161801
F1 score per class: {34: 0.9950248756218906, 5: 0.0, 38: 0.0, 7: 0.0, 8: 0.6258503401360545, 10: 0.0, 6: 0.0, 39: 0.9655172413793104, 13: 0.5, 11: 0.9428571428571428, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.7734806629834254
Weighted-average F1 score: 0.7099672168431158

F1 score per class: {0: 0.8064516129032258, 1: 0.3013698630136986, 2: 0.5882352941176471, 3: 0.11494252873563218, 4: 0.8235294117647058, 5: 0.8930232558139535, 6: 0.24347826086956523, 7: 0.047619047619047616, 8: 0.2268041237113402, 9: 0.9803921568627451, 10: 0.3779527559055118, 11: 0.21153846153846154, 12: 0.0380952380952381, 13: 0.11764705882352941, 14: 0.0547945205479452, 15: 0.75, 16: 0.7796610169491526, 17: 0.18181818181818182, 18: 0.4742268041237113, 19: 0.5628140703517588, 20: 0.624, 21: 0.1702127659574468, 22: 0.5960264900662252, 23: 0.75, 24: 0.0, 25: 0.3225806451612903, 26: 0.7282608695652174, 27: 0.0, 28: 0.5, 29: 0.8852459016393442, 30: 0.8823529411764706, 31: 0.6666666666666666, 32: 0.5555555555555556, 33: 0.21428571428571427, 34: 0.10810810810810811, 35: 0.16091954022988506, 36: 0.27848101265822783, 37: 0.42857142857142855, 38: 0.14285714285714285, 39: 0.125, 40: 0.10666666666666667}
Micro-average F1 score: 0.48154761904761906
Weighted-average F1 score: 0.5543686261867729
F1 score per class: {0: 0.9333333333333333, 1: 0.33548387096774196, 2: 0.5714285714285714, 3: 0.5671641791044776, 4: 0.9130434782608695, 5: 0.8130081300813008, 6: 0.42758620689655175, 7: 0.04878048780487805, 8: 0.5555555555555556, 9: 0.8928571428571429, 10: 0.5135135135135135, 11: 0.23076923076923078, 12: 0.44285714285714284, 13: 0.25, 14: 0.07894736842105263, 15: 0.75, 16: 0.875, 17: 0.4166666666666667, 18: 0.32075471698113206, 19: 0.6, 20: 0.8073394495412844, 21: 0.3287671232876712, 22: 0.718562874251497, 23: 0.7912087912087912, 24: 0.0, 25: 0.56, 26: 0.7195767195767195, 27: 0.0, 28: 0.4, 29: 0.8983957219251337, 30: 0.926829268292683, 31: 1.0, 32: 0.8064516129032258, 33: 0.2857142857142857, 34: 0.2857142857142857, 35: 0.46715328467153283, 36: 0.5283018867924528, 37: 0.4883720930232558, 38: 0.42696629213483145, 39: 0.11764705882352941, 40: 0.41818181818181815}
Micro-average F1 score: 0.578895945285784
Weighted-average F1 score: 0.5819886435853848
F1 score per class: {0: 0.9210526315789473, 1: 0.33548387096774196, 2: 0.6, 3: 0.42276422764227645, 4: 0.8950276243093923, 5: 0.8130081300813008, 6: 0.41134751773049644, 7: 0.0425531914893617, 8: 0.5405405405405406, 9: 0.9433962264150944, 10: 0.5644171779141104, 11: 0.25688073394495414, 12: 0.373134328358209, 13: 0.21052631578947367, 14: 0.075, 15: 0.75, 16: 0.8484848484848485, 17: 0.2727272727272727, 18: 0.3707865168539326, 19: 0.5945945945945946, 20: 0.7457627118644068, 21: 0.35443037974683544, 22: 0.6875, 23: 0.7906976744186046, 24: 0.0, 25: 0.43478260869565216, 26: 0.723404255319149, 27: 0.0, 28: 0.42857142857142855, 29: 0.9032258064516129, 30: 0.926829268292683, 31: 1.0, 32: 0.8021390374331551, 33: 0.1951219512195122, 34: 0.26666666666666666, 35: 0.3559322033898305, 36: 0.5688073394495413, 37: 0.5168539325842697, 38: 0.2988505747126437, 39: 0.11764705882352941, 40: 0.41818181818181815}
Micro-average F1 score: 0.5629335976214074
Weighted-average F1 score: 0.5665174592967452
cur_acc:  ['0.8038', '0.4056', '0.3365', '0.5461', '0.3228', '0.7092', '0.5803', '0.7122']
his_acc:  ['0.8038', '0.6983', '0.6077', '0.5760', '0.4490', '0.4840', '0.4793', '0.4815']
cur_acc des:  ['0.8315', '0.6554', '0.4664', '0.8432', '0.4539', '0.7721', '0.7597', '0.7523']
his_acc des:  ['0.8315', '0.7798', '0.6700', '0.6900', '0.5598', '0.5842', '0.5912', '0.5789']
cur_acc rrf:  ['0.8270', '0.6629', '0.4574', '0.8084', '0.3786', '0.7512', '0.7610', '0.7735']
his_acc rrf:  ['0.8270', '0.7642', '0.6566', '0.6624', '0.5245', '0.5704', '0.5780', '0.5629']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 405.8941922CurrentTrain: epoch  0, batch     1 | loss: 373.9334020CurrentTrain: epoch  0, batch     2 | loss: 318.5869524CurrentTrain: epoch  0, batch     3 | loss: 367.3145428CurrentTrain: epoch  0, batch     4 | loss: 368.4734962CurrentTrain: epoch  0, batch     5 | loss: 378.5800737CurrentTrain: epoch  0, batch     6 | loss: 378.1976798CurrentTrain: epoch  0, batch     7 | loss: 365.3038042CurrentTrain: epoch  0, batch     8 | loss: 392.5784507CurrentTrain: epoch  0, batch     9 | loss: 378.1124022CurrentTrain: epoch  0, batch    10 | loss: 398.8759850CurrentTrain: epoch  0, batch    11 | loss: 469.9237708CurrentTrain: epoch  0, batch    12 | loss: 322.2257449CurrentTrain: epoch  0, batch    13 | loss: 361.6467547CurrentTrain: epoch  0, batch    14 | loss: 313.9875764CurrentTrain: epoch  0, batch    15 | loss: 381.8995068CurrentTrain: epoch  0, batch    16 | loss: 348.6419282CurrentTrain: epoch  0, batch    17 | loss: 347.8509861CurrentTrain: epoch  0, batch    18 | loss: 337.6271567CurrentTrain: epoch  0, batch    19 | loss: 392.0999187CurrentTrain: epoch  0, batch    20 | loss: 407.4921209CurrentTrain: epoch  0, batch    21 | loss: 392.1326382CurrentTrain: epoch  0, batch    22 | loss: 308.7804435CurrentTrain: epoch  0, batch    23 | loss: 321.0904005CurrentTrain: epoch  0, batch    24 | loss: 253.4384560CurrentTrain: epoch  0, batch    25 | loss: 331.8907358CurrentTrain: epoch  0, batch    26 | loss: 294.0866831CurrentTrain: epoch  0, batch    27 | loss: 322.2420310CurrentTrain: epoch  0, batch    28 | loss: 387.5843614CurrentTrain: epoch  0, batch    29 | loss: 359.3360497CurrentTrain: epoch  0, batch    30 | loss: 436.7028538CurrentTrain: epoch  0, batch    31 | loss: 358.2557759CurrentTrain: epoch  0, batch    32 | loss: 452.2974341CurrentTrain: epoch  0, batch    33 | loss: 321.1108214CurrentTrain: epoch  0, batch    34 | loss: 419.2091240CurrentTrain: epoch  0, batch    35 | loss: 419.8413061CurrentTrain: epoch  0, batch    36 | loss: 299.9488220CurrentTrain: epoch  0, batch    37 | loss: 308.3566752CurrentTrain: epoch  0, batch    38 | loss: 371.6371865CurrentTrain: epoch  0, batch    39 | loss: 466.7924052CurrentTrain: epoch  0, batch    40 | loss: 371.2066750CurrentTrain: epoch  0, batch    41 | loss: 332.4198415CurrentTrain: epoch  0, batch    42 | loss: 359.7215392CurrentTrain: epoch  0, batch    43 | loss: 358.9117170CurrentTrain: epoch  0, batch    44 | loss: 357.7017932CurrentTrain: epoch  0, batch    45 | loss: 332.0849374CurrentTrain: epoch  0, batch    46 | loss: 317.2738683CurrentTrain: epoch  0, batch    47 | loss: 372.3110897CurrentTrain: epoch  0, batch    48 | loss: 466.3425684CurrentTrain: epoch  0, batch    49 | loss: 378.0170622CurrentTrain: epoch  0, batch    50 | loss: 319.5726858CurrentTrain: epoch  0, batch    51 | loss: 344.5845503CurrentTrain: epoch  0, batch    52 | loss: 332.9644819CurrentTrain: epoch  0, batch    53 | loss: 317.8647126CurrentTrain: epoch  0, batch    54 | loss: 317.9770516CurrentTrain: epoch  0, batch    55 | loss: 363.4310455CurrentTrain: epoch  0, batch    56 | loss: 357.5142340CurrentTrain: epoch  0, batch    57 | loss: 343.9637701CurrentTrain: epoch  0, batch    58 | loss: 339.4276242CurrentTrain: epoch  0, batch    59 | loss: 391.4259854CurrentTrain: epoch  0, batch    60 | loss: 307.9141568CurrentTrain: epoch  0, batch    61 | loss: 357.6662125CurrentTrain: epoch  0, batch    62 | loss: 294.9879128CurrentTrain: epoch  0, batch    63 | loss: 329.8661296CurrentTrain: epoch  0, batch    64 | loss: 466.4171891CurrentTrain: epoch  0, batch    65 | loss: 345.2697583CurrentTrain: epoch  0, batch    66 | loss: 329.9688276CurrentTrain: epoch  0, batch    67 | loss: 261.6280439CurrentTrain: epoch  0, batch    68 | loss: 370.8182651CurrentTrain: epoch  0, batch    69 | loss: 450.4218092CurrentTrain: epoch  0, batch    70 | loss: 316.1661609CurrentTrain: epoch  0, batch    71 | loss: 356.3860906CurrentTrain: epoch  0, batch    72 | loss: 421.6437023CurrentTrain: epoch  0, batch    73 | loss: 316.8141734CurrentTrain: epoch  0, batch    74 | loss: 449.8282378CurrentTrain: epoch  0, batch    75 | loss: 329.1242029CurrentTrain: epoch  0, batch    76 | loss: 342.6953394CurrentTrain: epoch  0, batch    77 | loss: 344.7950245CurrentTrain: epoch  0, batch    78 | loss: 543.5828515CurrentTrain: epoch  0, batch    79 | loss: 356.0216734CurrentTrain: epoch  0, batch    80 | loss: 418.8503680CurrentTrain: epoch  0, batch    81 | loss: 343.1504398CurrentTrain: epoch  0, batch    82 | loss: 334.9432505CurrentTrain: epoch  0, batch    83 | loss: 451.0469040CurrentTrain: epoch  0, batch    84 | loss: 390.4645830CurrentTrain: epoch  0, batch    85 | loss: 383.9779826CurrentTrain: epoch  0, batch    86 | loss: 389.1504567CurrentTrain: epoch  0, batch    87 | loss: 390.3736611CurrentTrain: epoch  0, batch    88 | loss: 355.8873140CurrentTrain: epoch  0, batch    89 | loss: 404.0949194CurrentTrain: epoch  0, batch    90 | loss: 417.8845780CurrentTrain: epoch  0, batch    91 | loss: 362.4230209CurrentTrain: epoch  0, batch    92 | loss: 331.1562798CurrentTrain: epoch  0, batch    93 | loss: 282.6767892CurrentTrain: epoch  0, batch    94 | loss: 343.2288253CurrentTrain: epoch  0, batch    95 | loss: 279.3288843CurrentTrain: epoch  1, batch     0 | loss: 303.2922900CurrentTrain: epoch  1, batch     1 | loss: 449.2452655CurrentTrain: epoch  1, batch     2 | loss: 340.7389141CurrentTrain: epoch  1, batch     3 | loss: 356.3927569CurrentTrain: epoch  1, batch     4 | loss: 329.1096037CurrentTrain: epoch  1, batch     5 | loss: 332.1115603CurrentTrain: epoch  1, batch     6 | loss: 387.7919754CurrentTrain: epoch  1, batch     7 | loss: 436.0657404CurrentTrain: epoch  1, batch     8 | loss: 361.3684578CurrentTrain: epoch  1, batch     9 | loss: 315.2215962CurrentTrain: epoch  1, batch    10 | loss: 294.4954027CurrentTrain: epoch  1, batch    11 | loss: 368.9432668CurrentTrain: epoch  1, batch    12 | loss: 435.9890834CurrentTrain: epoch  1, batch    13 | loss: 448.3369904CurrentTrain: epoch  1, batch    14 | loss: 343.1874139CurrentTrain: epoch  1, batch    15 | loss: 281.0079855CurrentTrain: epoch  1, batch    16 | loss: 367.9887665CurrentTrain: epoch  1, batch    17 | loss: 353.5098775CurrentTrain: epoch  1, batch    18 | loss: 402.6522222CurrentTrain: epoch  1, batch    19 | loss: 465.0284030CurrentTrain: epoch  1, batch    20 | loss: 316.6389179CurrentTrain: epoch  1, batch    21 | loss: 382.7577422CurrentTrain: epoch  1, batch    22 | loss: 302.2182796CurrentTrain: epoch  1, batch    23 | loss: 303.0451071CurrentTrain: epoch  1, batch    24 | loss: 402.3801237CurrentTrain: epoch  1, batch    25 | loss: 416.2159770CurrentTrain: epoch  1, batch    26 | loss: 387.1144556CurrentTrain: epoch  1, batch    27 | loss: 292.9507166CurrentTrain: epoch  1, batch    28 | loss: 373.6479472CurrentTrain: epoch  1, batch    29 | loss: 369.0617085CurrentTrain: epoch  1, batch    30 | loss: 368.5877783CurrentTrain: epoch  1, batch    31 | loss: 385.2705689CurrentTrain: epoch  1, batch    32 | loss: 386.6532619CurrentTrain: epoch  1, batch    33 | loss: 294.3517450CurrentTrain: epoch  1, batch    34 | loss: 402.2424441CurrentTrain: epoch  1, batch    35 | loss: 343.0962444CurrentTrain: epoch  1, batch    36 | loss: 367.4795701CurrentTrain: epoch  1, batch    37 | loss: 369.0920509CurrentTrain: epoch  1, batch    38 | loss: 331.8885356CurrentTrain: epoch  1, batch    39 | loss: 402.0124509CurrentTrain: epoch  1, batch    40 | loss: 315.6703612CurrentTrain: epoch  1, batch    41 | loss: 339.9658898CurrentTrain: epoch  1, batch    42 | loss: 280.6508865CurrentTrain: epoch  1, batch    43 | loss: 356.3987264CurrentTrain: epoch  1, batch    44 | loss: 339.1042887CurrentTrain: epoch  1, batch    45 | loss: 327.3399582CurrentTrain: epoch  1, batch    46 | loss: 302.0081498CurrentTrain: epoch  1, batch    47 | loss: 314.6171879CurrentTrain: epoch  1, batch    48 | loss: 284.2922157CurrentTrain: epoch  1, batch    49 | loss: 327.3100383CurrentTrain: epoch  1, batch    50 | loss: 525.7619829CurrentTrain: epoch  1, batch    51 | loss: 373.2916759CurrentTrain: epoch  1, batch    52 | loss: 415.8509883CurrentTrain: epoch  1, batch    53 | loss: 326.7620528CurrentTrain: epoch  1, batch    54 | loss: 314.0506530CurrentTrain: epoch  1, batch    55 | loss: 327.9851951CurrentTrain: epoch  1, batch    56 | loss: 400.7285905CurrentTrain: epoch  1, batch    57 | loss: 433.7863717CurrentTrain: epoch  1, batch    58 | loss: 462.5758612CurrentTrain: epoch  1, batch    59 | loss: 445.4761827CurrentTrain: epoch  1, batch    60 | loss: 367.8705659CurrentTrain: epoch  1, batch    61 | loss: 328.0463173CurrentTrain: epoch  1, batch    62 | loss: 288.0784807CurrentTrain: epoch  1, batch    63 | loss: 367.4980886CurrentTrain: epoch  1, batch    64 | loss: 381.6853811CurrentTrain: epoch  1, batch    65 | loss: 337.1100278CurrentTrain: epoch  1, batch    66 | loss: 297.9483533CurrentTrain: epoch  1, batch    67 | loss: 298.5417682CurrentTrain: epoch  1, batch    68 | loss: 336.9998061CurrentTrain: epoch  1, batch    69 | loss: 466.2093695CurrentTrain: epoch  1, batch    70 | loss: 327.1287649CurrentTrain: epoch  1, batch    71 | loss: 350.9518549CurrentTrain: epoch  1, batch    72 | loss: 326.5864158CurrentTrain: epoch  1, batch    73 | loss: 373.3598238CurrentTrain: epoch  1, batch    74 | loss: 365.3449454CurrentTrain: epoch  1, batch    75 | loss: 282.9857444CurrentTrain: epoch  1, batch    76 | loss: 312.5249818CurrentTrain: epoch  1, batch    77 | loss: 360.2013833CurrentTrain: epoch  1, batch    78 | loss: 388.7640163CurrentTrain: epoch  1, batch    79 | loss: 401.3790652CurrentTrain: epoch  1, batch    80 | loss: 279.0747506CurrentTrain: epoch  1, batch    81 | loss: 339.2874328CurrentTrain: epoch  1, batch    82 | loss: 372.0409477CurrentTrain: epoch  1, batch    83 | loss: 357.6896177CurrentTrain: epoch  1, batch    84 | loss: 432.2561202CurrentTrain: epoch  1, batch    85 | loss: 339.9977469CurrentTrain: epoch  1, batch    86 | loss: 353.4571977CurrentTrain: epoch  1, batch    87 | loss: 368.8619084CurrentTrain: epoch  1, batch    88 | loss: 403.6576954CurrentTrain: epoch  1, batch    89 | loss: 304.5511153CurrentTrain: epoch  1, batch    90 | loss: 444.8978861CurrentTrain: epoch  1, batch    91 | loss: 338.2278672CurrentTrain: epoch  1, batch    92 | loss: 240.0583861CurrentTrain: epoch  1, batch    93 | loss: 336.6120456CurrentTrain: epoch  1, batch    94 | loss: 334.9271852CurrentTrain: epoch  1, batch    95 | loss: 272.7767159CurrentTrain: epoch  2, batch     0 | loss: 354.6436094CurrentTrain: epoch  2, batch     1 | loss: 365.3016523CurrentTrain: epoch  2, batch     2 | loss: 312.7294378CurrentTrain: epoch  2, batch     3 | loss: 400.6852767CurrentTrain: epoch  2, batch     4 | loss: 383.3232776CurrentTrain: epoch  2, batch     5 | loss: 296.2497349CurrentTrain: epoch  2, batch     6 | loss: 384.7946963CurrentTrain: epoch  2, batch     7 | loss: 274.5286398CurrentTrain: epoch  2, batch     8 | loss: 368.5731340CurrentTrain: epoch  2, batch     9 | loss: 322.1977683CurrentTrain: epoch  2, batch    10 | loss: 369.1415001CurrentTrain: epoch  2, batch    11 | loss: 337.2176145CurrentTrain: epoch  2, batch    12 | loss: 353.0078919CurrentTrain: epoch  2, batch    13 | loss: 379.8328293CurrentTrain: epoch  2, batch    14 | loss: 543.9396723CurrentTrain: epoch  2, batch    15 | loss: 332.2243384CurrentTrain: epoch  2, batch    16 | loss: 365.0827664CurrentTrain: epoch  2, batch    17 | loss: 335.0073932CurrentTrain: epoch  2, batch    18 | loss: 370.3119897CurrentTrain: epoch  2, batch    19 | loss: 299.5784262CurrentTrain: epoch  2, batch    20 | loss: 324.6532337CurrentTrain: epoch  2, batch    21 | loss: 349.0373505CurrentTrain: epoch  2, batch    22 | loss: 324.6122082CurrentTrain: epoch  2, batch    23 | loss: 442.6281721CurrentTrain: epoch  2, batch    24 | loss: 432.0045222CurrentTrain: epoch  2, batch    25 | loss: 446.0037556CurrentTrain: epoch  2, batch    26 | loss: 354.4068276CurrentTrain: epoch  2, batch    27 | loss: 379.6535056CurrentTrain: epoch  2, batch    28 | loss: 381.4288522CurrentTrain: epoch  2, batch    29 | loss: 361.9946270CurrentTrain: epoch  2, batch    30 | loss: 300.0362936CurrentTrain: epoch  2, batch    31 | loss: 443.4826449CurrentTrain: epoch  2, batch    32 | loss: 338.1647837CurrentTrain: epoch  2, batch    33 | loss: 352.4636585CurrentTrain: epoch  2, batch    34 | loss: 337.4402235CurrentTrain: epoch  2, batch    35 | loss: 305.1418442CurrentTrain: epoch  2, batch    36 | loss: 394.0873449CurrentTrain: epoch  2, batch    37 | loss: 271.2518484CurrentTrain: epoch  2, batch    38 | loss: 381.2656415CurrentTrain: epoch  2, batch    39 | loss: 397.9872025CurrentTrain: epoch  2, batch    40 | loss: 346.6925180CurrentTrain: epoch  2, batch    41 | loss: 321.7972066CurrentTrain: epoch  2, batch    42 | loss: 411.4116343CurrentTrain: epoch  2, batch    43 | loss: 321.0029069CurrentTrain: epoch  2, batch    44 | loss: 379.2416959CurrentTrain: epoch  2, batch    45 | loss: 444.2569154CurrentTrain: epoch  2, batch    46 | loss: 306.4603738CurrentTrain: epoch  2, batch    47 | loss: 287.1009500CurrentTrain: epoch  2, batch    48 | loss: 322.7739969CurrentTrain: epoch  2, batch    49 | loss: 415.4012156CurrentTrain: epoch  2, batch    50 | loss: 364.6244153CurrentTrain: epoch  2, batch    51 | loss: 320.6643494CurrentTrain: epoch  2, batch    52 | loss: 330.4064816CurrentTrain: epoch  2, batch    53 | loss: 377.0704389CurrentTrain: epoch  2, batch    54 | loss: 368.2932632CurrentTrain: epoch  2, batch    55 | loss: 379.8550206CurrentTrain: epoch  2, batch    56 | loss: 308.1409079CurrentTrain: epoch  2, batch    57 | loss: 378.7394297CurrentTrain: epoch  2, batch    58 | loss: 294.3097372CurrentTrain: epoch  2, batch    59 | loss: 334.6194651CurrentTrain: epoch  2, batch    60 | loss: 332.1323275CurrentTrain: epoch  2, batch    61 | loss: 308.8847311CurrentTrain: epoch  2, batch    62 | loss: 258.2935847CurrentTrain: epoch  2, batch    63 | loss: 260.3819542CurrentTrain: epoch  2, batch    64 | loss: 377.4337099CurrentTrain: epoch  2, batch    65 | loss: 322.0674075CurrentTrain: epoch  2, batch    66 | loss: 306.0220895CurrentTrain: epoch  2, batch    67 | loss: 365.6556129CurrentTrain: epoch  2, batch    68 | loss: 423.9230193CurrentTrain: epoch  2, batch    69 | loss: 305.1465989CurrentTrain: epoch  2, batch    70 | loss: 310.1196313CurrentTrain: epoch  2, batch    71 | loss: 330.8894456CurrentTrain: epoch  2, batch    72 | loss: 336.1015923CurrentTrain: epoch  2, batch    73 | loss: 332.9817770CurrentTrain: epoch  2, batch    74 | loss: 335.0238635CurrentTrain: epoch  2, batch    75 | loss: 411.1510859CurrentTrain: epoch  2, batch    76 | loss: 292.5947066CurrentTrain: epoch  2, batch    77 | loss: 325.5376362CurrentTrain: epoch  2, batch    78 | loss: 269.6569956CurrentTrain: epoch  2, batch    79 | loss: 285.6198190CurrentTrain: epoch  2, batch    80 | loss: 379.2947333CurrentTrain: epoch  2, batch    81 | loss: 414.6508139CurrentTrain: epoch  2, batch    82 | loss: 295.4877328CurrentTrain: epoch  2, batch    83 | loss: 289.6806448CurrentTrain: epoch  2, batch    84 | loss: 318.6070687CurrentTrain: epoch  2, batch    85 | loss: 350.7867038CurrentTrain: epoch  2, batch    86 | loss: 362.7943626CurrentTrain: epoch  2, batch    87 | loss: 316.1826890CurrentTrain: epoch  2, batch    88 | loss: 412.1793814CurrentTrain: epoch  2, batch    89 | loss: 464.7917975CurrentTrain: epoch  2, batch    90 | loss: 378.9827593CurrentTrain: epoch  2, batch    91 | loss: 443.0381823CurrentTrain: epoch  2, batch    92 | loss: 458.1711724CurrentTrain: epoch  2, batch    93 | loss: 343.3929257CurrentTrain: epoch  2, batch    94 | loss: 347.9387026CurrentTrain: epoch  2, batch    95 | loss: 263.2000053CurrentTrain: epoch  3, batch     0 | loss: 358.8373777CurrentTrain: epoch  3, batch     1 | loss: 319.0656787CurrentTrain: epoch  3, batch     2 | loss: 410.8298665CurrentTrain: epoch  3, batch     3 | loss: 331.4699108CurrentTrain: epoch  3, batch     4 | loss: 306.5550489CurrentTrain: epoch  3, batch     5 | loss: 332.2649375CurrentTrain: epoch  3, batch     6 | loss: 381.0530956CurrentTrain: epoch  3, batch     7 | loss: 329.0440055CurrentTrain: epoch  3, batch     8 | loss: 290.1942812CurrentTrain: epoch  3, batch     9 | loss: 293.3542708CurrentTrain: epoch  3, batch    10 | loss: 316.8746842CurrentTrain: epoch  3, batch    11 | loss: 396.2669696CurrentTrain: epoch  3, batch    12 | loss: 343.3979080CurrentTrain: epoch  3, batch    13 | loss: 348.1688559CurrentTrain: epoch  3, batch    14 | loss: 363.9233086CurrentTrain: epoch  3, batch    15 | loss: 308.1363587CurrentTrain: epoch  3, batch    16 | loss: 364.0749718CurrentTrain: epoch  3, batch    17 | loss: 332.4714409CurrentTrain: epoch  3, batch    18 | loss: 362.3164137CurrentTrain: epoch  3, batch    19 | loss: 395.7945568CurrentTrain: epoch  3, batch    20 | loss: 290.9372554CurrentTrain: epoch  3, batch    21 | loss: 428.3235838CurrentTrain: epoch  3, batch    22 | loss: 321.1941650CurrentTrain: epoch  3, batch    23 | loss: 292.6685204CurrentTrain: epoch  3, batch    24 | loss: 381.5962035CurrentTrain: epoch  3, batch    25 | loss: 243.3775134CurrentTrain: epoch  3, batch    26 | loss: 395.3256447CurrentTrain: epoch  3, batch    27 | loss: 464.1743995CurrentTrain: epoch  3, batch    28 | loss: 360.3447443CurrentTrain: epoch  3, batch    29 | loss: 439.4650576CurrentTrain: epoch  3, batch    30 | loss: 346.6387957CurrentTrain: epoch  3, batch    31 | loss: 347.4032848CurrentTrain: epoch  3, batch    32 | loss: 348.0179448CurrentTrain: epoch  3, batch    33 | loss: 298.1690935CurrentTrain: epoch  3, batch    34 | loss: 374.2755899CurrentTrain: epoch  3, batch    35 | loss: 400.9554574CurrentTrain: epoch  3, batch    36 | loss: 302.5858857CurrentTrain: epoch  3, batch    37 | loss: 396.3717226CurrentTrain: epoch  3, batch    38 | loss: 349.8086958CurrentTrain: epoch  3, batch    39 | loss: 350.4069682CurrentTrain: epoch  3, batch    40 | loss: 318.9022352CurrentTrain: epoch  3, batch    41 | loss: 408.8388549CurrentTrain: epoch  3, batch    42 | loss: 408.9012558CurrentTrain: epoch  3, batch    43 | loss: 398.3721349CurrentTrain: epoch  3, batch    44 | loss: 334.1072477CurrentTrain: epoch  3, batch    45 | loss: 289.1941308CurrentTrain: epoch  3, batch    46 | loss: 410.3262659CurrentTrain: epoch  3, batch    47 | loss: 375.0981724CurrentTrain: epoch  3, batch    48 | loss: 324.8227240CurrentTrain: epoch  3, batch    49 | loss: 331.0788162CurrentTrain: epoch  3, batch    50 | loss: 412.4962138CurrentTrain: epoch  3, batch    51 | loss: 269.9222500CurrentTrain: epoch  3, batch    52 | loss: 392.8520137CurrentTrain: epoch  3, batch    53 | loss: 377.6726229CurrentTrain: epoch  3, batch    54 | loss: 376.3150060CurrentTrain: epoch  3, batch    55 | loss: 319.2038729CurrentTrain: epoch  3, batch    56 | loss: 410.6630239CurrentTrain: epoch  3, batch    57 | loss: 329.1710439CurrentTrain: epoch  3, batch    58 | loss: 277.3848866CurrentTrain: epoch  3, batch    59 | loss: 358.6788362CurrentTrain: epoch  3, batch    60 | loss: 317.7470094CurrentTrain: epoch  3, batch    61 | loss: 331.1001149CurrentTrain: epoch  3, batch    62 | loss: 365.7856530CurrentTrain: epoch  3, batch    63 | loss: 378.6443445CurrentTrain: epoch  3, batch    64 | loss: 365.7094151CurrentTrain: epoch  3, batch    65 | loss: 330.2711943CurrentTrain: epoch  3, batch    66 | loss: 293.3689657CurrentTrain: epoch  3, batch    67 | loss: 263.6910986CurrentTrain: epoch  3, batch    68 | loss: 392.0759547CurrentTrain: epoch  3, batch    69 | loss: 410.5606290CurrentTrain: epoch  3, batch    70 | loss: 365.2064463CurrentTrain: epoch  3, batch    71 | loss: 343.6381536CurrentTrain: epoch  3, batch    72 | loss: 292.5901945CurrentTrain: epoch  3, batch    73 | loss: 378.2664014CurrentTrain: epoch  3, batch    74 | loss: 359.8456679CurrentTrain: epoch  3, batch    75 | loss: 316.8024488CurrentTrain: epoch  3, batch    76 | loss: 322.9737253CurrentTrain: epoch  3, batch    77 | loss: 409.7815203CurrentTrain: epoch  3, batch    78 | loss: 360.8679215CurrentTrain: epoch  3, batch    79 | loss: 391.0906806CurrentTrain: epoch  3, batch    80 | loss: 274.1925602CurrentTrain: epoch  3, batch    81 | loss: 289.0974124CurrentTrain: epoch  3, batch    82 | loss: 394.6010700CurrentTrain: epoch  3, batch    83 | loss: 276.5798029CurrentTrain: epoch  3, batch    84 | loss: 392.0119598CurrentTrain: epoch  3, batch    85 | loss: 286.4835971CurrentTrain: epoch  3, batch    86 | loss: 392.3414029CurrentTrain: epoch  3, batch    87 | loss: 267.0105138CurrentTrain: epoch  3, batch    88 | loss: 320.3179526CurrentTrain: epoch  3, batch    89 | loss: 362.4639240CurrentTrain: epoch  3, batch    90 | loss: 347.2339697CurrentTrain: epoch  3, batch    91 | loss: 350.6601871CurrentTrain: epoch  3, batch    92 | loss: 377.3520690CurrentTrain: epoch  3, batch    93 | loss: 334.9272935CurrentTrain: epoch  3, batch    94 | loss: 297.6420334CurrentTrain: epoch  3, batch    95 | loss: 253.0367546CurrentTrain: epoch  4, batch     0 | loss: 427.0004109CurrentTrain: epoch  4, batch     1 | loss: 344.6242885CurrentTrain: epoch  4, batch     2 | loss: 277.7934497CurrentTrain: epoch  4, batch     3 | loss: 347.8651720CurrentTrain: epoch  4, batch     4 | loss: 342.4782834CurrentTrain: epoch  4, batch     5 | loss: 382.7492683CurrentTrain: epoch  4, batch     6 | loss: 343.1673554CurrentTrain: epoch  4, batch     7 | loss: 261.2850815CurrentTrain: epoch  4, batch     8 | loss: 398.4866294CurrentTrain: epoch  4, batch     9 | loss: 279.3713648CurrentTrain: epoch  4, batch    10 | loss: 363.9368059CurrentTrain: epoch  4, batch    11 | loss: 392.2959496CurrentTrain: epoch  4, batch    12 | loss: 282.9371892CurrentTrain: epoch  4, batch    13 | loss: 333.9181242CurrentTrain: epoch  4, batch    14 | loss: 375.8965299CurrentTrain: epoch  4, batch    15 | loss: 318.7323085CurrentTrain: epoch  4, batch    16 | loss: 347.4702557CurrentTrain: epoch  4, batch    17 | loss: 330.3538532CurrentTrain: epoch  4, batch    18 | loss: 300.2377065CurrentTrain: epoch  4, batch    19 | loss: 376.6188125CurrentTrain: epoch  4, batch    20 | loss: 361.2953004CurrentTrain: epoch  4, batch    21 | loss: 409.3784156CurrentTrain: epoch  4, batch    22 | loss: 374.6752965CurrentTrain: epoch  4, batch    23 | loss: 538.8415202CurrentTrain: epoch  4, batch    24 | loss: 537.6138930CurrentTrain: epoch  4, batch    25 | loss: 270.7734787CurrentTrain: epoch  4, batch    26 | loss: 380.1905577CurrentTrain: epoch  4, batch    27 | loss: 258.1029457CurrentTrain: epoch  4, batch    28 | loss: 342.4896222CurrentTrain: epoch  4, batch    29 | loss: 382.7988706CurrentTrain: epoch  4, batch    30 | loss: 268.9487905CurrentTrain: epoch  4, batch    31 | loss: 345.8013709CurrentTrain: epoch  4, batch    32 | loss: 295.1538002CurrentTrain: epoch  4, batch    33 | loss: 392.1075749CurrentTrain: epoch  4, batch    34 | loss: 332.9534882CurrentTrain: epoch  4, batch    35 | loss: 271.3277720CurrentTrain: epoch  4, batch    36 | loss: 358.6430451CurrentTrain: epoch  4, batch    37 | loss: 329.2718432CurrentTrain: epoch  4, batch    38 | loss: 345.0351283CurrentTrain: epoch  4, batch    39 | loss: 344.0244346CurrentTrain: epoch  4, batch    40 | loss: 376.3426785CurrentTrain: epoch  4, batch    41 | loss: 343.1948875CurrentTrain: epoch  4, batch    42 | loss: 391.5449597CurrentTrain: epoch  4, batch    43 | loss: 232.1310943CurrentTrain: epoch  4, batch    44 | loss: 319.9671508CurrentTrain: epoch  4, batch    45 | loss: 394.2709986CurrentTrain: epoch  4, batch    46 | loss: 304.0345129CurrentTrain: epoch  4, batch    47 | loss: 287.7527066CurrentTrain: epoch  4, batch    48 | loss: 374.2186108CurrentTrain: epoch  4, batch    49 | loss: 463.8888122CurrentTrain: epoch  4, batch    50 | loss: 348.6079439CurrentTrain: epoch  4, batch    51 | loss: 314.6088144CurrentTrain: epoch  4, batch    52 | loss: 343.3209951CurrentTrain: epoch  4, batch    53 | loss: 459.3061738CurrentTrain: epoch  4, batch    54 | loss: 329.9068064CurrentTrain: epoch  4, batch    55 | loss: 392.5928566CurrentTrain: epoch  4, batch    56 | loss: 327.5104072CurrentTrain: epoch  4, batch    57 | loss: 446.3758965CurrentTrain: epoch  4, batch    58 | loss: 345.6911124CurrentTrain: epoch  4, batch    59 | loss: 347.3157272CurrentTrain: epoch  4, batch    60 | loss: 447.0238453CurrentTrain: epoch  4, batch    61 | loss: 335.5260493CurrentTrain: epoch  4, batch    62 | loss: 304.9878659CurrentTrain: epoch  4, batch    63 | loss: 318.8486741CurrentTrain: epoch  4, batch    64 | loss: 288.1433194CurrentTrain: epoch  4, batch    65 | loss: 357.9904005CurrentTrain: epoch  4, batch    66 | loss: 404.1939170CurrentTrain: epoch  4, batch    67 | loss: 423.7733089CurrentTrain: epoch  4, batch    68 | loss: 274.8456951CurrentTrain: epoch  4, batch    69 | loss: 346.0583806CurrentTrain: epoch  4, batch    70 | loss: 443.5475344CurrentTrain: epoch  4, batch    71 | loss: 329.5316859CurrentTrain: epoch  4, batch    72 | loss: 301.5648179CurrentTrain: epoch  4, batch    73 | loss: 347.1843348CurrentTrain: epoch  4, batch    74 | loss: 344.5727708CurrentTrain: epoch  4, batch    75 | loss: 316.9977665CurrentTrain: epoch  4, batch    76 | loss: 317.6159078CurrentTrain: epoch  4, batch    77 | loss: 385.2227919CurrentTrain: epoch  4, batch    78 | loss: 312.9015813CurrentTrain: epoch  4, batch    79 | loss: 440.1461925CurrentTrain: epoch  4, batch    80 | loss: 345.4397469CurrentTrain: epoch  4, batch    81 | loss: 394.2388774CurrentTrain: epoch  4, batch    82 | loss: 379.6908138CurrentTrain: epoch  4, batch    83 | loss: 271.9338547CurrentTrain: epoch  4, batch    84 | loss: 537.0805549CurrentTrain: epoch  4, batch    85 | loss: 332.1257982CurrentTrain: epoch  4, batch    86 | loss: 362.8948790CurrentTrain: epoch  4, batch    87 | loss: 275.5367184CurrentTrain: epoch  4, batch    88 | loss: 318.5420954CurrentTrain: epoch  4, batch    89 | loss: 325.2822602CurrentTrain: epoch  4, batch    90 | loss: 287.5958880CurrentTrain: epoch  4, batch    91 | loss: 288.5448822CurrentTrain: epoch  4, batch    92 | loss: 292.1407169CurrentTrain: epoch  4, batch    93 | loss: 302.4013306CurrentTrain: epoch  4, batch    94 | loss: 329.9294547CurrentTrain: epoch  4, batch    95 | loss: 250.6895053CurrentTrain: epoch  5, batch     0 | loss: 303.1717151CurrentTrain: epoch  5, batch     1 | loss: 375.9536821CurrentTrain: epoch  5, batch     2 | loss: 360.7496120CurrentTrain: epoch  5, batch     3 | loss: 327.6986387CurrentTrain: epoch  5, batch     4 | loss: 376.3901819CurrentTrain: epoch  5, batch     5 | loss: 333.5816906CurrentTrain: epoch  5, batch     6 | loss: 330.1413067CurrentTrain: epoch  5, batch     7 | loss: 313.3349830CurrentTrain: epoch  5, batch     8 | loss: 255.7649846CurrentTrain: epoch  5, batch     9 | loss: 343.2125151CurrentTrain: epoch  5, batch    10 | loss: 442.2880380CurrentTrain: epoch  5, batch    11 | loss: 342.7973788CurrentTrain: epoch  5, batch    12 | loss: 422.9112320CurrentTrain: epoch  5, batch    13 | loss: 344.4929049CurrentTrain: epoch  5, batch    14 | loss: 394.1387899CurrentTrain: epoch  5, batch    15 | loss: 393.4360704CurrentTrain: epoch  5, batch    16 | loss: 359.8661128CurrentTrain: epoch  5, batch    17 | loss: 342.4799834CurrentTrain: epoch  5, batch    18 | loss: 248.2308564CurrentTrain: epoch  5, batch    19 | loss: 345.3029775CurrentTrain: epoch  5, batch    20 | loss: 332.1198026CurrentTrain: epoch  5, batch    21 | loss: 319.1522309CurrentTrain: epoch  5, batch    22 | loss: 437.2687782CurrentTrain: epoch  5, batch    23 | loss: 287.9062662CurrentTrain: epoch  5, batch    24 | loss: 315.7241773CurrentTrain: epoch  5, batch    25 | loss: 310.7213473CurrentTrain: epoch  5, batch    26 | loss: 342.9629821CurrentTrain: epoch  5, batch    27 | loss: 344.8613414CurrentTrain: epoch  5, batch    28 | loss: 345.1104428CurrentTrain: epoch  5, batch    29 | loss: 329.8843025CurrentTrain: epoch  5, batch    30 | loss: 274.6870616CurrentTrain: epoch  5, batch    31 | loss: 374.0774000CurrentTrain: epoch  5, batch    32 | loss: 299.7731606CurrentTrain: epoch  5, batch    33 | loss: 457.0906192CurrentTrain: epoch  5, batch    34 | loss: 293.9275998CurrentTrain: epoch  5, batch    35 | loss: 408.1492610CurrentTrain: epoch  5, batch    36 | loss: 376.6353385CurrentTrain: epoch  5, batch    37 | loss: 395.9655558CurrentTrain: epoch  5, batch    38 | loss: 332.4448057CurrentTrain: epoch  5, batch    39 | loss: 390.2177979CurrentTrain: epoch  5, batch    40 | loss: 436.8202186CurrentTrain: epoch  5, batch    41 | loss: 288.9785872CurrentTrain: epoch  5, batch    42 | loss: 313.7882748CurrentTrain: epoch  5, batch    43 | loss: 299.2385522CurrentTrain: epoch  5, batch    44 | loss: 437.9431551CurrentTrain: epoch  5, batch    45 | loss: 298.3503203CurrentTrain: epoch  5, batch    46 | loss: 291.7136417CurrentTrain: epoch  5, batch    47 | loss: 296.8394729CurrentTrain: epoch  5, batch    48 | loss: 248.9431365CurrentTrain: epoch  5, batch    49 | loss: 319.4025179CurrentTrain: epoch  5, batch    50 | loss: 314.8497985CurrentTrain: epoch  5, batch    51 | loss: 316.5035882CurrentTrain: epoch  5, batch    52 | loss: 342.3327972CurrentTrain: epoch  5, batch    53 | loss: 497.2688819CurrentTrain: epoch  5, batch    54 | loss: 407.8235554CurrentTrain: epoch  5, batch    55 | loss: 300.7366202CurrentTrain: epoch  5, batch    56 | loss: 438.6706769CurrentTrain: epoch  5, batch    57 | loss: 378.2392086CurrentTrain: epoch  5, batch    58 | loss: 346.8807404CurrentTrain: epoch  5, batch    59 | loss: 543.7250379CurrentTrain: epoch  5, batch    60 | loss: 347.9840679CurrentTrain: epoch  5, batch    61 | loss: 422.4235643CurrentTrain: epoch  5, batch    62 | loss: 358.5791691CurrentTrain: epoch  5, batch    63 | loss: 326.0868610CurrentTrain: epoch  5, batch    64 | loss: 360.4752119CurrentTrain: epoch  5, batch    65 | loss: 340.6428711CurrentTrain: epoch  5, batch    66 | loss: 379.6194338CurrentTrain: epoch  5, batch    67 | loss: 326.5594580CurrentTrain: epoch  5, batch    68 | loss: 329.3302689CurrentTrain: epoch  5, batch    69 | loss: 327.3048542CurrentTrain: epoch  5, batch    70 | loss: 408.7794393CurrentTrain: epoch  5, batch    71 | loss: 516.4560180CurrentTrain: epoch  5, batch    72 | loss: 301.1252863CurrentTrain: epoch  5, batch    73 | loss: 376.3343838CurrentTrain: epoch  5, batch    74 | loss: 341.7225740CurrentTrain: epoch  5, batch    75 | loss: 302.6131132CurrentTrain: epoch  5, batch    76 | loss: 328.4099140CurrentTrain: epoch  5, batch    77 | loss: 285.1060218CurrentTrain: epoch  5, batch    78 | loss: 413.4550686CurrentTrain: epoch  5, batch    79 | loss: 286.6592986CurrentTrain: epoch  5, batch    80 | loss: 404.5610821CurrentTrain: epoch  5, batch    81 | loss: 326.9655321CurrentTrain: epoch  5, batch    82 | loss: 330.5828543CurrentTrain: epoch  5, batch    83 | loss: 439.3677895CurrentTrain: epoch  5, batch    84 | loss: 303.1730946CurrentTrain: epoch  5, batch    85 | loss: 393.2835859CurrentTrain: epoch  5, batch    86 | loss: 377.5293088CurrentTrain: epoch  5, batch    87 | loss: 408.0937080CurrentTrain: epoch  5, batch    88 | loss: 300.2735024CurrentTrain: epoch  5, batch    89 | loss: 375.0221895CurrentTrain: epoch  5, batch    90 | loss: 344.7500821CurrentTrain: epoch  5, batch    91 | loss: 408.5277541CurrentTrain: epoch  5, batch    92 | loss: 311.8913551CurrentTrain: epoch  5, batch    93 | loss: 348.6017645CurrentTrain: epoch  5, batch    94 | loss: 295.2859332CurrentTrain: epoch  5, batch    95 | loss: 240.7656481CurrentTrain: epoch  6, batch     0 | loss: 374.4945016CurrentTrain: epoch  6, batch     1 | loss: 328.8588686CurrentTrain: epoch  6, batch     2 | loss: 436.9761965CurrentTrain: epoch  6, batch     3 | loss: 356.1663715CurrentTrain: epoch  6, batch     4 | loss: 325.2138262CurrentTrain: epoch  6, batch     5 | loss: 343.3690475CurrentTrain: epoch  6, batch     6 | loss: 440.3236313CurrentTrain: epoch  6, batch     7 | loss: 420.4107532CurrentTrain: epoch  6, batch     8 | loss: 274.1322987CurrentTrain: epoch  6, batch     9 | loss: 272.0157916CurrentTrain: epoch  6, batch    10 | loss: 357.4928910CurrentTrain: epoch  6, batch    11 | loss: 357.1740437CurrentTrain: epoch  6, batch    12 | loss: 299.8420553CurrentTrain: epoch  6, batch    13 | loss: 359.8828309CurrentTrain: epoch  6, batch    14 | loss: 408.0462135CurrentTrain: epoch  6, batch    15 | loss: 264.6761675CurrentTrain: epoch  6, batch    16 | loss: 372.9669012CurrentTrain: epoch  6, batch    17 | loss: 393.0741326CurrentTrain: epoch  6, batch    18 | loss: 311.5214794CurrentTrain: epoch  6, batch    19 | loss: 374.2559995CurrentTrain: epoch  6, batch    20 | loss: 343.2434257CurrentTrain: epoch  6, batch    21 | loss: 324.5954373CurrentTrain: epoch  6, batch    22 | loss: 284.6173627CurrentTrain: epoch  6, batch    23 | loss: 376.3248253CurrentTrain: epoch  6, batch    24 | loss: 325.9766250CurrentTrain: epoch  6, batch    25 | loss: 316.9742862CurrentTrain: epoch  6, batch    26 | loss: 360.9267747CurrentTrain: epoch  6, batch    27 | loss: 312.7550248CurrentTrain: epoch  6, batch    28 | loss: 374.1294840CurrentTrain: epoch  6, batch    29 | loss: 413.4934489CurrentTrain: epoch  6, batch    30 | loss: 292.2358734CurrentTrain: epoch  6, batch    31 | loss: 239.3044119CurrentTrain: epoch  6, batch    32 | loss: 317.6908872CurrentTrain: epoch  6, batch    33 | loss: 302.1347781CurrentTrain: epoch  6, batch    34 | loss: 437.2051166CurrentTrain: epoch  6, batch    35 | loss: 345.3542929CurrentTrain: epoch  6, batch    36 | loss: 407.6440864CurrentTrain: epoch  6, batch    37 | loss: 333.6817854CurrentTrain: epoch  6, batch    38 | loss: 422.2494314CurrentTrain: epoch  6, batch    39 | loss: 408.2970341CurrentTrain: epoch  6, batch    40 | loss: 286.6967426CurrentTrain: epoch  6, batch    41 | loss: 341.7280065CurrentTrain: epoch  6, batch    42 | loss: 373.3825046CurrentTrain: epoch  6, batch    43 | loss: 408.0582849CurrentTrain: epoch  6, batch    44 | loss: 407.7349952CurrentTrain: epoch  6, batch    45 | loss: 356.8190712CurrentTrain: epoch  6, batch    46 | loss: 517.7504280CurrentTrain: epoch  6, batch    47 | loss: 357.4808248CurrentTrain: epoch  6, batch    48 | loss: 422.1148854CurrentTrain: epoch  6, batch    49 | loss: 312.1252424CurrentTrain: epoch  6, batch    50 | loss: 456.0073599CurrentTrain: epoch  6, batch    51 | loss: 310.4258553CurrentTrain: epoch  6, batch    52 | loss: 327.6436450CurrentTrain: epoch  6, batch    53 | loss: 347.1164936CurrentTrain: epoch  6, batch    54 | loss: 273.6098318CurrentTrain: epoch  6, batch    55 | loss: 374.8640577CurrentTrain: epoch  6, batch    56 | loss: 344.1742886CurrentTrain: epoch  6, batch    57 | loss: 316.7921996CurrentTrain: epoch  6, batch    58 | loss: 393.6910813CurrentTrain: epoch  6, batch    59 | loss: 376.5146859CurrentTrain: epoch  6, batch    60 | loss: 248.4926896CurrentTrain: epoch  6, batch    61 | loss: 294.2911542CurrentTrain: epoch  6, batch    62 | loss: 277.5264126CurrentTrain: epoch  6, batch    63 | loss: 319.6477550CurrentTrain: epoch  6, batch    64 | loss: 346.2807647CurrentTrain: epoch  6, batch    65 | loss: 326.2193381CurrentTrain: epoch  6, batch    66 | loss: 301.7746595CurrentTrain: epoch  6, batch    67 | loss: 390.5014101CurrentTrain: epoch  6, batch    68 | loss: 341.6204803CurrentTrain: epoch  6, batch    69 | loss: 288.2010882CurrentTrain: epoch  6, batch    70 | loss: 277.6235431CurrentTrain: epoch  6, batch    71 | loss: 321.6895577CurrentTrain: epoch  6, batch    72 | loss: 332.0330253CurrentTrain: epoch  6, batch    73 | loss: 303.6855557CurrentTrain: epoch  6, batch    74 | loss: 315.1883823CurrentTrain: epoch  6, batch    75 | loss: 315.2387355CurrentTrain: epoch  6, batch    76 | loss: 303.7077461CurrentTrain: epoch  6, batch    77 | loss: 348.3682885CurrentTrain: epoch  6, batch    78 | loss: 363.1798144CurrentTrain: epoch  6, batch    79 | loss: 310.2233481CurrentTrain: epoch  6, batch    80 | loss: 456.6282968CurrentTrain: epoch  6, batch    81 | loss: 293.5709185CurrentTrain: epoch  6, batch    82 | loss: 356.9350199CurrentTrain: epoch  6, batch    83 | loss: 379.5728490CurrentTrain: epoch  6, batch    84 | loss: 341.7535073CurrentTrain: epoch  6, batch    85 | loss: 393.6523067CurrentTrain: epoch  6, batch    86 | loss: 374.7888116CurrentTrain: epoch  6, batch    87 | loss: 334.6822638CurrentTrain: epoch  6, batch    88 | loss: 299.9471925CurrentTrain: epoch  6, batch    89 | loss: 374.2887376CurrentTrain: epoch  6, batch    90 | loss: 289.4145331CurrentTrain: epoch  6, batch    91 | loss: 357.5121510CurrentTrain: epoch  6, batch    92 | loss: 313.6247368CurrentTrain: epoch  6, batch    93 | loss: 272.6396509CurrentTrain: epoch  6, batch    94 | loss: 375.0331968CurrentTrain: epoch  6, batch    95 | loss: 293.6788750CurrentTrain: epoch  7, batch     0 | loss: 341.5623669CurrentTrain: epoch  7, batch     1 | loss: 392.6845173CurrentTrain: epoch  7, batch     2 | loss: 306.2325011CurrentTrain: epoch  7, batch     3 | loss: 315.4985679CurrentTrain: epoch  7, batch     4 | loss: 237.2922387CurrentTrain: epoch  7, batch     5 | loss: 339.5949371CurrentTrain: epoch  7, batch     6 | loss: 375.2107582CurrentTrain: epoch  7, batch     7 | loss: 277.9403151CurrentTrain: epoch  7, batch     8 | loss: 304.3823658CurrentTrain: epoch  7, batch     9 | loss: 287.6426103CurrentTrain: epoch  7, batch    10 | loss: 357.2420579CurrentTrain: epoch  7, batch    11 | loss: 285.5051771CurrentTrain: epoch  7, batch    12 | loss: 307.4703533CurrentTrain: epoch  7, batch    13 | loss: 391.7892807CurrentTrain: epoch  7, batch    14 | loss: 341.5432507CurrentTrain: epoch  7, batch    15 | loss: 220.9365769CurrentTrain: epoch  7, batch    16 | loss: 326.3434500CurrentTrain: epoch  7, batch    17 | loss: 340.7144784CurrentTrain: epoch  7, batch    18 | loss: 357.2864874CurrentTrain: epoch  7, batch    19 | loss: 390.5398823CurrentTrain: epoch  7, batch    20 | loss: 300.6238752CurrentTrain: epoch  7, batch    21 | loss: 437.1734017CurrentTrain: epoch  7, batch    22 | loss: 361.1603482CurrentTrain: epoch  7, batch    23 | loss: 455.8922555CurrentTrain: epoch  7, batch    24 | loss: 325.6302688CurrentTrain: epoch  7, batch    25 | loss: 440.5856981CurrentTrain: epoch  7, batch    26 | loss: 411.0790890CurrentTrain: epoch  7, batch    27 | loss: 357.0805989CurrentTrain: epoch  7, batch    28 | loss: 300.2691962CurrentTrain: epoch  7, batch    29 | loss: 299.7051126CurrentTrain: epoch  7, batch    30 | loss: 250.6010921CurrentTrain: epoch  7, batch    31 | loss: 329.1873023CurrentTrain: epoch  7, batch    32 | loss: 360.8222937CurrentTrain: epoch  7, batch    33 | loss: 346.6219720CurrentTrain: epoch  7, batch    34 | loss: 356.8570224CurrentTrain: epoch  7, batch    35 | loss: 391.2187211CurrentTrain: epoch  7, batch    36 | loss: 329.6498432CurrentTrain: epoch  7, batch    37 | loss: 327.6457892CurrentTrain: epoch  7, batch    38 | loss: 315.8341022CurrentTrain: epoch  7, batch    39 | loss: 313.6248197CurrentTrain: epoch  7, batch    40 | loss: 314.0368372CurrentTrain: epoch  7, batch    41 | loss: 328.0597919CurrentTrain: epoch  7, batch    42 | loss: 356.9407961CurrentTrain: epoch  7, batch    43 | loss: 357.8065335CurrentTrain: epoch  7, batch    44 | loss: 299.4055879CurrentTrain: epoch  7, batch    45 | loss: 291.3604636CurrentTrain: epoch  7, batch    46 | loss: 314.4690920CurrentTrain: epoch  7, batch    47 | loss: 311.2845650CurrentTrain: epoch  7, batch    48 | loss: 373.9041417CurrentTrain: epoch  7, batch    49 | loss: 339.9126519CurrentTrain: epoch  7, batch    50 | loss: 374.3747080CurrentTrain: epoch  7, batch    51 | loss: 356.2826555CurrentTrain: epoch  7, batch    52 | loss: 357.7054452CurrentTrain: epoch  7, batch    53 | loss: 342.3427933CurrentTrain: epoch  7, batch    54 | loss: 340.3706666CurrentTrain: epoch  7, batch    55 | loss: 339.9555270CurrentTrain: epoch  7, batch    56 | loss: 326.4905362CurrentTrain: epoch  7, batch    57 | loss: 372.7915742CurrentTrain: epoch  7, batch    58 | loss: 404.4754509CurrentTrain: epoch  7, batch    59 | loss: 329.0080065CurrentTrain: epoch  7, batch    60 | loss: 409.3547270CurrentTrain: epoch  7, batch    61 | loss: 260.3725749CurrentTrain: epoch  7, batch    62 | loss: 324.6586265CurrentTrain: epoch  7, batch    63 | loss: 395.4625929CurrentTrain: epoch  7, batch    64 | loss: 391.9809063CurrentTrain: epoch  7, batch    65 | loss: 328.6323303CurrentTrain: epoch  7, batch    66 | loss: 314.1348582CurrentTrain: epoch  7, batch    67 | loss: 279.3690706CurrentTrain: epoch  7, batch    68 | loss: 437.0108195CurrentTrain: epoch  7, batch    69 | loss: 325.6778739CurrentTrain: epoch  7, batch    70 | loss: 378.7229147CurrentTrain: epoch  7, batch    71 | loss: 295.1969687CurrentTrain: epoch  7, batch    72 | loss: 407.4426913CurrentTrain: epoch  7, batch    73 | loss: 390.6232277CurrentTrain: epoch  7, batch    74 | loss: 407.5881622CurrentTrain: epoch  7, batch    75 | loss: 438.2989865CurrentTrain: epoch  7, batch    76 | loss: 283.6395121CurrentTrain: epoch  7, batch    77 | loss: 437.5428243CurrentTrain: epoch  7, batch    78 | loss: 312.6228975CurrentTrain: epoch  7, batch    79 | loss: 329.2051089CurrentTrain: epoch  7, batch    80 | loss: 310.0243898CurrentTrain: epoch  7, batch    81 | loss: 327.1484629CurrentTrain: epoch  7, batch    82 | loss: 390.1101400CurrentTrain: epoch  7, batch    83 | loss: 357.0042707CurrentTrain: epoch  7, batch    84 | loss: 342.3610991CurrentTrain: epoch  7, batch    85 | loss: 372.5547118CurrentTrain: epoch  7, batch    86 | loss: 271.8612289CurrentTrain: epoch  7, batch    87 | loss: 393.2202914CurrentTrain: epoch  7, batch    88 | loss: 300.0549290CurrentTrain: epoch  7, batch    89 | loss: 302.8374523CurrentTrain: epoch  7, batch    90 | loss: 373.0231881CurrentTrain: epoch  7, batch    91 | loss: 420.0847572CurrentTrain: epoch  7, batch    92 | loss: 276.8630266CurrentTrain: epoch  7, batch    93 | loss: 307.5748559CurrentTrain: epoch  7, batch    94 | loss: 291.6951587CurrentTrain: epoch  7, batch    95 | loss: 275.5391186CurrentTrain: epoch  8, batch     0 | loss: 372.8426494CurrentTrain: epoch  8, batch     1 | loss: 348.4577924CurrentTrain: epoch  8, batch     2 | loss: 314.0230856CurrentTrain: epoch  8, batch     3 | loss: 299.0730232CurrentTrain: epoch  8, batch     4 | loss: 282.6020913CurrentTrain: epoch  8, batch     5 | loss: 357.3677711CurrentTrain: epoch  8, batch     6 | loss: 236.9978379CurrentTrain: epoch  8, batch     7 | loss: 390.3999078CurrentTrain: epoch  8, batch     8 | loss: 251.8775950CurrentTrain: epoch  8, batch     9 | loss: 291.9749543CurrentTrain: epoch  8, batch    10 | loss: 455.1909060CurrentTrain: epoch  8, batch    11 | loss: 329.1818893CurrentTrain: epoch  8, batch    12 | loss: 323.9450155CurrentTrain: epoch  8, batch    13 | loss: 359.3862563CurrentTrain: epoch  8, batch    14 | loss: 373.3631267CurrentTrain: epoch  8, batch    15 | loss: 456.7565382CurrentTrain: epoch  8, batch    16 | loss: 295.7336714CurrentTrain: epoch  8, batch    17 | loss: 360.1831304CurrentTrain: epoch  8, batch    18 | loss: 342.4710052CurrentTrain: epoch  8, batch    19 | loss: 300.3286648CurrentTrain: epoch  8, batch    20 | loss: 313.0321421CurrentTrain: epoch  8, batch    21 | loss: 326.2580455CurrentTrain: epoch  8, batch    22 | loss: 315.3276205CurrentTrain: epoch  8, batch    23 | loss: 264.4004851CurrentTrain: epoch  8, batch    24 | loss: 515.7389346CurrentTrain: epoch  8, batch    25 | loss: 455.3076937CurrentTrain: epoch  8, batch    26 | loss: 326.6287820CurrentTrain: epoch  8, batch    27 | loss: 356.0903040CurrentTrain: epoch  8, batch    28 | loss: 372.3177664CurrentTrain: epoch  8, batch    29 | loss: 355.6592298CurrentTrain: epoch  8, batch    30 | loss: 373.0584073CurrentTrain: epoch  8, batch    31 | loss: 389.9899870CurrentTrain: epoch  8, batch    32 | loss: 389.6782548CurrentTrain: epoch  8, batch    33 | loss: 285.9708965CurrentTrain: epoch  8, batch    34 | loss: 313.7993055CurrentTrain: epoch  8, batch    35 | loss: 355.7504992CurrentTrain: epoch  8, batch    36 | loss: 277.0342029CurrentTrain: epoch  8, batch    37 | loss: 296.3996462CurrentTrain: epoch  8, batch    38 | loss: 536.6676307CurrentTrain: epoch  8, batch    39 | loss: 294.1832106CurrentTrain: epoch  8, batch    40 | loss: 256.4179637CurrentTrain: epoch  8, batch    41 | loss: 355.7110355CurrentTrain: epoch  8, batch    42 | loss: 340.4903950CurrentTrain: epoch  8, batch    43 | loss: 373.1690264CurrentTrain: epoch  8, batch    44 | loss: 355.6506787CurrentTrain: epoch  8, batch    45 | loss: 340.7252664CurrentTrain: epoch  8, batch    46 | loss: 407.0242039CurrentTrain: epoch  8, batch    47 | loss: 389.9111711CurrentTrain: epoch  8, batch    48 | loss: 357.0589661CurrentTrain: epoch  8, batch    49 | loss: 327.4652851CurrentTrain: epoch  8, batch    50 | loss: 339.7921702CurrentTrain: epoch  8, batch    51 | loss: 324.9752523CurrentTrain: epoch  8, batch    52 | loss: 357.0232655CurrentTrain: epoch  8, batch    53 | loss: 311.8030639CurrentTrain: epoch  8, batch    54 | loss: 301.7882463CurrentTrain: epoch  8, batch    55 | loss: 410.9116943CurrentTrain: epoch  8, batch    56 | loss: 311.4362008CurrentTrain: epoch  8, batch    57 | loss: 373.0739867CurrentTrain: epoch  8, batch    58 | loss: 326.3166375CurrentTrain: epoch  8, batch    59 | loss: 437.1042497CurrentTrain: epoch  8, batch    60 | loss: 325.8327358CurrentTrain: epoch  8, batch    61 | loss: 263.0379242CurrentTrain: epoch  8, batch    62 | loss: 372.5512133CurrentTrain: epoch  8, batch    63 | loss: 359.1426241CurrentTrain: epoch  8, batch    64 | loss: 287.0115147CurrentTrain: epoch  8, batch    65 | loss: 329.5875443CurrentTrain: epoch  8, batch    66 | loss: 339.5408847CurrentTrain: epoch  8, batch    67 | loss: 372.5303670CurrentTrain: epoch  8, batch    68 | loss: 357.0805069CurrentTrain: epoch  8, batch    69 | loss: 284.7783636CurrentTrain: epoch  8, batch    70 | loss: 373.4281289CurrentTrain: epoch  8, batch    71 | loss: 326.7763713CurrentTrain: epoch  8, batch    72 | loss: 298.1554413CurrentTrain: epoch  8, batch    73 | loss: 373.0192717CurrentTrain: epoch  8, batch    74 | loss: 344.8695385CurrentTrain: epoch  8, batch    75 | loss: 324.6655743CurrentTrain: epoch  8, batch    76 | loss: 420.1442183CurrentTrain: epoch  8, batch    77 | loss: 264.3074541CurrentTrain: epoch  8, batch    78 | loss: 315.6476646CurrentTrain: epoch  8, batch    79 | loss: 392.5591286CurrentTrain: epoch  8, batch    80 | loss: 356.7513950CurrentTrain: epoch  8, batch    81 | loss: 421.8928795CurrentTrain: epoch  8, batch    82 | loss: 455.2146757CurrentTrain: epoch  8, batch    83 | loss: 270.3268419CurrentTrain: epoch  8, batch    84 | loss: 265.5583841CurrentTrain: epoch  8, batch    85 | loss: 389.8545923CurrentTrain: epoch  8, batch    86 | loss: 341.6983652CurrentTrain: epoch  8, batch    87 | loss: 357.5324957CurrentTrain: epoch  8, batch    88 | loss: 327.5932305CurrentTrain: epoch  8, batch    89 | loss: 323.7151249CurrentTrain: epoch  8, batch    90 | loss: 356.1512799CurrentTrain: epoch  8, batch    91 | loss: 257.7338504CurrentTrain: epoch  8, batch    92 | loss: 258.7976441CurrentTrain: epoch  8, batch    93 | loss: 437.2384282CurrentTrain: epoch  8, batch    94 | loss: 341.2214138CurrentTrain: epoch  8, batch    95 | loss: 310.5240976CurrentTrain: epoch  9, batch     0 | loss: 301.9785433CurrentTrain: epoch  9, batch     1 | loss: 362.9126667CurrentTrain: epoch  9, batch     2 | loss: 312.7200503CurrentTrain: epoch  9, batch     3 | loss: 390.7226958CurrentTrain: epoch  9, batch     4 | loss: 257.1826497CurrentTrain: epoch  9, batch     5 | loss: 349.1703847CurrentTrain: epoch  9, batch     6 | loss: 358.4080355CurrentTrain: epoch  9, batch     7 | loss: 455.1727275CurrentTrain: epoch  9, batch     8 | loss: 374.8666670CurrentTrain: epoch  9, batch     9 | loss: 325.5519796CurrentTrain: epoch  9, batch    10 | loss: 373.0087372CurrentTrain: epoch  9, batch    11 | loss: 299.2339805CurrentTrain: epoch  9, batch    12 | loss: 340.6319078CurrentTrain: epoch  9, batch    13 | loss: 392.3958402CurrentTrain: epoch  9, batch    14 | loss: 322.6029595CurrentTrain: epoch  9, batch    15 | loss: 292.9793449CurrentTrain: epoch  9, batch    16 | loss: 437.5832094CurrentTrain: epoch  9, batch    17 | loss: 390.2386363CurrentTrain: epoch  9, batch    18 | loss: 372.9370394CurrentTrain: epoch  9, batch    19 | loss: 407.1752076CurrentTrain: epoch  9, batch    20 | loss: 355.8316748CurrentTrain: epoch  9, batch    21 | loss: 295.9245183CurrentTrain: epoch  9, batch    22 | loss: 361.2948846CurrentTrain: epoch  9, batch    23 | loss: 331.0224214CurrentTrain: epoch  9, batch    24 | loss: 324.6449027CurrentTrain: epoch  9, batch    25 | loss: 341.9542348CurrentTrain: epoch  9, batch    26 | loss: 340.2093617CurrentTrain: epoch  9, batch    27 | loss: 313.0006715CurrentTrain: epoch  9, batch    28 | loss: 341.8030669CurrentTrain: epoch  9, batch    29 | loss: 373.2264265CurrentTrain: epoch  9, batch    30 | loss: 340.8488666CurrentTrain: epoch  9, batch    31 | loss: 372.2258701CurrentTrain: epoch  9, batch    32 | loss: 407.2669365CurrentTrain: epoch  9, batch    33 | loss: 311.0712693CurrentTrain: epoch  9, batch    34 | loss: 302.2915863CurrentTrain: epoch  9, batch    35 | loss: 455.1799376CurrentTrain: epoch  9, batch    36 | loss: 313.3801257CurrentTrain: epoch  9, batch    37 | loss: 343.0260043CurrentTrain: epoch  9, batch    38 | loss: 347.5702569CurrentTrain: epoch  9, batch    39 | loss: 342.2509738CurrentTrain: epoch  9, batch    40 | loss: 277.0275676CurrentTrain: epoch  9, batch    41 | loss: 248.0537294CurrentTrain: epoch  9, batch    42 | loss: 357.8278827CurrentTrain: epoch  9, batch    43 | loss: 389.6502124CurrentTrain: epoch  9, batch    44 | loss: 245.7499248CurrentTrain: epoch  9, batch    45 | loss: 373.9997661CurrentTrain: epoch  9, batch    46 | loss: 340.0744248CurrentTrain: epoch  9, batch    47 | loss: 281.3423684CurrentTrain: epoch  9, batch    48 | loss: 390.9233253CurrentTrain: epoch  9, batch    49 | loss: 290.5930270CurrentTrain: epoch  9, batch    50 | loss: 357.6639347CurrentTrain: epoch  9, batch    51 | loss: 387.3717447CurrentTrain: epoch  9, batch    52 | loss: 391.8550049CurrentTrain: epoch  9, batch    53 | loss: 325.4735824CurrentTrain: epoch  9, batch    54 | loss: 340.6309228CurrentTrain: epoch  9, batch    55 | loss: 310.2038826CurrentTrain: epoch  9, batch    56 | loss: 357.0622515CurrentTrain: epoch  9, batch    57 | loss: 309.1685944CurrentTrain: epoch  9, batch    58 | loss: 401.7587336CurrentTrain: epoch  9, batch    59 | loss: 339.7228184CurrentTrain: epoch  9, batch    60 | loss: 341.0721962CurrentTrain: epoch  9, batch    61 | loss: 436.8106746CurrentTrain: epoch  9, batch    62 | loss: 340.9499631CurrentTrain: epoch  9, batch    63 | loss: 309.8408527CurrentTrain: epoch  9, batch    64 | loss: 313.4168114CurrentTrain: epoch  9, batch    65 | loss: 324.7451381CurrentTrain: epoch  9, batch    66 | loss: 276.2334972CurrentTrain: epoch  9, batch    67 | loss: 329.4610263CurrentTrain: epoch  9, batch    68 | loss: 328.0416789CurrentTrain: epoch  9, batch    69 | loss: 357.5728395CurrentTrain: epoch  9, batch    70 | loss: 372.7737966CurrentTrain: epoch  9, batch    71 | loss: 342.4416176CurrentTrain: epoch  9, batch    72 | loss: 325.4202348CurrentTrain: epoch  9, batch    73 | loss: 323.6734317CurrentTrain: epoch  9, batch    74 | loss: 296.8764101CurrentTrain: epoch  9, batch    75 | loss: 312.7803954CurrentTrain: epoch  9, batch    76 | loss: 313.9169778CurrentTrain: epoch  9, batch    77 | loss: 343.0939022CurrentTrain: epoch  9, batch    78 | loss: 310.0456926CurrentTrain: epoch  9, batch    79 | loss: 344.9576798CurrentTrain: epoch  9, batch    80 | loss: 372.6144735CurrentTrain: epoch  9, batch    81 | loss: 314.9302337CurrentTrain: epoch  9, batch    82 | loss: 389.5856614CurrentTrain: epoch  9, batch    83 | loss: 357.5224376CurrentTrain: epoch  9, batch    84 | loss: 261.3440392CurrentTrain: epoch  9, batch    85 | loss: 436.7388830CurrentTrain: epoch  9, batch    86 | loss: 340.9247951CurrentTrain: epoch  9, batch    87 | loss: 285.2008272CurrentTrain: epoch  9, batch    88 | loss: 373.4044404CurrentTrain: epoch  9, batch    89 | loss: 407.1092764CurrentTrain: epoch  9, batch    90 | loss: 339.7946646CurrentTrain: epoch  9, batch    91 | loss: 389.6452375CurrentTrain: epoch  9, batch    92 | loss: 333.9157902CurrentTrain: epoch  9, batch    93 | loss: 339.5231749CurrentTrain: epoch  9, batch    94 | loss: 356.3375307CurrentTrain: epoch  9, batch    95 | loss: 274.6323494

F1 score per class: {32: 0.7157894736842105, 6: 0.8950276243093923, 19: 0.5384615384615384, 24: 0.7640449438202247, 26: 0.9473684210526315, 29: 0.8979591836734694}
Micro-average F1 score: 0.8366285119667014
Weighted-average F1 score: 0.8403808750647844
F1 score per class: {32: 0.7562189054726368, 6: 0.9473684210526315, 19: 0.5806451612903226, 24: 0.7513227513227513, 26: 0.9746192893401016, 29: 0.8969072164948454}
Micro-average F1 score: 0.8562874251497006
Weighted-average F1 score: 0.8572411750588379
F1 score per class: {32: 0.7411167512690355, 6: 0.9473684210526315, 19: 0.5806451612903226, 24: 0.7513227513227513, 26: 0.9746192893401016, 29: 0.883248730964467}
Micro-average F1 score: 0.8511488511488512
Weighted-average F1 score: 0.8525482250556429

F1 score per class: {32: 0.7157894736842105, 6: 0.8950276243093923, 19: 0.5384615384615384, 24: 0.7640449438202247, 26: 0.9473684210526315, 29: 0.8979591836734694}
Micro-average F1 score: 0.8366285119667014
Weighted-average F1 score: 0.8403808750647844
F1 score per class: {32: 0.7562189054726368, 6: 0.9473684210526315, 19: 0.5806451612903226, 24: 0.7513227513227513, 26: 0.9746192893401016, 29: 0.8969072164948454}
Micro-average F1 score: 0.8562874251497006
Weighted-average F1 score: 0.8572411750588379
F1 score per class: {32: 0.7411167512690355, 6: 0.9473684210526315, 19: 0.5806451612903226, 24: 0.7513227513227513, 26: 0.9746192893401016, 29: 0.883248730964467}
Micro-average F1 score: 0.8511488511488512
Weighted-average F1 score: 0.8525482250556429
cur_acc:  ['0.8366']
his_acc:  ['0.8366']
cur_acc des:  ['0.8563']
his_acc des:  ['0.8563']
cur_acc rrf:  ['0.8511']
his_acc rrf:  ['0.8511']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 373.3125455CurrentTrain: epoch  0, batch     1 | loss: 308.1162838CurrentTrain: epoch  0, batch     2 | loss: 361.4449904CurrentTrain: epoch  0, batch     3 | loss: 209.7186351CurrentTrain: epoch  1, batch     0 | loss: 403.1367360CurrentTrain: epoch  1, batch     1 | loss: 336.0393982CurrentTrain: epoch  1, batch     2 | loss: 339.7165675CurrentTrain: epoch  1, batch     3 | loss: 198.8030813CurrentTrain: epoch  2, batch     0 | loss: 431.1790696CurrentTrain: epoch  2, batch     1 | loss: 338.8894201CurrentTrain: epoch  2, batch     2 | loss: 346.5769495CurrentTrain: epoch  2, batch     3 | loss: 165.4532466CurrentTrain: epoch  3, batch     0 | loss: 293.8395210CurrentTrain: epoch  3, batch     1 | loss: 363.4085817CurrentTrain: epoch  3, batch     2 | loss: 335.0967421CurrentTrain: epoch  3, batch     3 | loss: 214.6815544CurrentTrain: epoch  4, batch     0 | loss: 363.4543850CurrentTrain: epoch  4, batch     1 | loss: 331.5135964CurrentTrain: epoch  4, batch     2 | loss: 304.4542156CurrentTrain: epoch  4, batch     3 | loss: 204.1143506CurrentTrain: epoch  5, batch     0 | loss: 460.4959304CurrentTrain: epoch  5, batch     1 | loss: 303.3940823CurrentTrain: epoch  5, batch     2 | loss: 249.7487823CurrentTrain: epoch  5, batch     3 | loss: 258.9941510CurrentTrain: epoch  6, batch     0 | loss: 343.7254608CurrentTrain: epoch  6, batch     1 | loss: 327.9081719CurrentTrain: epoch  6, batch     2 | loss: 275.3097362CurrentTrain: epoch  6, batch     3 | loss: 271.9677212CurrentTrain: epoch  7, batch     0 | loss: 328.5458488CurrentTrain: epoch  7, batch     1 | loss: 327.4370185CurrentTrain: epoch  7, batch     2 | loss: 310.4498471CurrentTrain: epoch  7, batch     3 | loss: 238.1708502CurrentTrain: epoch  8, batch     0 | loss: 315.2513789CurrentTrain: epoch  8, batch     1 | loss: 270.7264248CurrentTrain: epoch  8, batch     2 | loss: 407.9066375CurrentTrain: epoch  8, batch     3 | loss: 197.5916688CurrentTrain: epoch  9, batch     0 | loss: 356.4400599CurrentTrain: epoch  9, batch     1 | loss: 329.0605425CurrentTrain: epoch  9, batch     2 | loss: 372.8075068CurrentTrain: epoch  9, batch     3 | loss: 130.2777974
MemoryTrain:  epoch  0, batch     0 | loss: 2.4748261MemoryTrain:  epoch  1, batch     0 | loss: 2.0651923MemoryTrain:  epoch  2, batch     0 | loss: 1.4472884MemoryTrain:  epoch  3, batch     0 | loss: 1.2362144MemoryTrain:  epoch  4, batch     0 | loss: 1.0440697MemoryTrain:  epoch  5, batch     0 | loss: 0.7484082MemoryTrain:  epoch  6, batch     0 | loss: 0.6418974MemoryTrain:  epoch  7, batch     0 | loss: 0.5307997MemoryTrain:  epoch  8, batch     0 | loss: 0.5044619MemoryTrain:  epoch  9, batch     0 | loss: 0.3862038

F1 score per class: {33: 0.6153846153846154, 36: 0.6829268292682927, 8: 0.0, 20: 0.0, 26: 0.9142857142857143, 29: 0.42857142857142855, 30: 0.7307692307692307}
Micro-average F1 score: 0.6836158192090396
Weighted-average F1 score: 0.6907261289176817
F1 score per class: {33: 0.8175182481751825, 36: 0.8387096774193549, 8: 0.0, 20: 0.0, 26: 0.8888888888888888, 29: 0.42857142857142855, 30: 0.921875}
Micro-average F1 score: 0.8439024390243902
Weighted-average F1 score: 0.8494065626479914
F1 score per class: {33: 0.8175182481751825, 36: 0.8387096774193549, 8: 0.0, 20: 0.0, 26: 0.8888888888888888, 29: 0.42857142857142855, 30: 0.921875}
Micro-average F1 score: 0.8439024390243902
Weighted-average F1 score: 0.8494065626479914

F1 score per class: {32: 0.4225352112676056, 33: 0.47058823529411764, 36: 0.88268156424581, 6: 0.6829268292682927, 8: 0.5, 19: 0.7624309392265194, 20: 0.9368421052631579, 24: 0.8888888888888888, 26: 0.8709677419354839, 29: 0.42857142857142855, 30: 0.7307692307692307}
Micro-average F1 score: 0.7358636715724245
Weighted-average F1 score: 0.7539199338245149
F1 score per class: {32: 0.6989247311827957, 33: 0.6871165644171779, 36: 0.907103825136612, 6: 0.8387096774193549, 8: 0.6, 19: 0.7640449438202247, 20: 0.9538461538461539, 24: 0.7272727272727273, 26: 0.8854166666666666, 29: 0.35294117647058826, 30: 0.921875}
Micro-average F1 score: 0.8176011355571328
Weighted-average F1 score: 0.8193565437529695
F1 score per class: {32: 0.7032967032967034, 33: 0.6746987951807228, 36: 0.9010989010989011, 6: 0.8387096774193549, 8: 0.5517241379310345, 19: 0.770949720670391, 20: 0.9484536082474226, 24: 0.7272727272727273, 26: 0.8795811518324608, 29: 0.35294117647058826, 30: 0.921875}
Micro-average F1 score: 0.8142348754448399
Weighted-average F1 score: 0.8159233952027614
cur_acc:  ['0.8366', '0.6836']
his_acc:  ['0.8366', '0.7359']
cur_acc des:  ['0.8563', '0.8439']
his_acc des:  ['0.8563', '0.8176']
cur_acc rrf:  ['0.8511', '0.8439']
his_acc rrf:  ['0.8511', '0.8142']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 361.3492593CurrentTrain: epoch  0, batch     1 | loss: 347.9941081CurrentTrain: epoch  0, batch     2 | loss: 370.1044698CurrentTrain: epoch  0, batch     3 | loss: 389.7693735CurrentTrain: epoch  0, batch     4 | loss: 84.5067395CurrentTrain: epoch  1, batch     0 | loss: 303.0893310CurrentTrain: epoch  1, batch     1 | loss: 518.9530206CurrentTrain: epoch  1, batch     2 | loss: 410.9776979CurrentTrain: epoch  1, batch     3 | loss: 313.2532793CurrentTrain: epoch  1, batch     4 | loss: 64.1996548CurrentTrain: epoch  2, batch     0 | loss: 356.2151772CurrentTrain: epoch  2, batch     1 | loss: 293.2381081CurrentTrain: epoch  2, batch     2 | loss: 463.9628858CurrentTrain: epoch  2, batch     3 | loss: 353.4481853CurrentTrain: epoch  2, batch     4 | loss: 76.9610879CurrentTrain: epoch  3, batch     0 | loss: 294.7822922CurrentTrain: epoch  3, batch     1 | loss: 516.8632807CurrentTrain: epoch  3, batch     2 | loss: 371.9918051CurrentTrain: epoch  3, batch     3 | loss: 364.0886950CurrentTrain: epoch  3, batch     4 | loss: 57.3487590CurrentTrain: epoch  4, batch     0 | loss: 413.1809069CurrentTrain: epoch  4, batch     1 | loss: 336.2369147CurrentTrain: epoch  4, batch     2 | loss: 318.4362897CurrentTrain: epoch  4, batch     3 | loss: 346.7370795CurrentTrain: epoch  4, batch     4 | loss: 76.6255301CurrentTrain: epoch  5, batch     0 | loss: 332.0946170CurrentTrain: epoch  5, batch     1 | loss: 541.0048005CurrentTrain: epoch  5, batch     2 | loss: 291.3569506CurrentTrain: epoch  5, batch     3 | loss: 359.9077625CurrentTrain: epoch  5, batch     4 | loss: 49.2622780CurrentTrain: epoch  6, batch     0 | loss: 410.9884495CurrentTrain: epoch  6, batch     1 | loss: 347.3771205CurrentTrain: epoch  6, batch     2 | loss: 359.3101583CurrentTrain: epoch  6, batch     3 | loss: 290.9473427CurrentTrain: epoch  6, batch     4 | loss: 55.7436133CurrentTrain: epoch  7, batch     0 | loss: 317.7724110CurrentTrain: epoch  7, batch     1 | loss: 316.0372250CurrentTrain: epoch  7, batch     2 | loss: 333.4106985CurrentTrain: epoch  7, batch     3 | loss: 393.0616836CurrentTrain: epoch  7, batch     4 | loss: 119.5739318CurrentTrain: epoch  8, batch     0 | loss: 359.7461120CurrentTrain: epoch  8, batch     1 | loss: 365.1039235CurrentTrain: epoch  8, batch     2 | loss: 392.6672110CurrentTrain: epoch  8, batch     3 | loss: 328.6464224CurrentTrain: epoch  8, batch     4 | loss: 46.1227497CurrentTrain: epoch  9, batch     0 | loss: 422.0931344CurrentTrain: epoch  9, batch     1 | loss: 375.3608770CurrentTrain: epoch  9, batch     2 | loss: 342.0828560CurrentTrain: epoch  9, batch     3 | loss: 298.2636357CurrentTrain: epoch  9, batch     4 | loss: 75.8193192
MemoryTrain:  epoch  0, batch     0 | loss: 2.2752035MemoryTrain:  epoch  1, batch     0 | loss: 1.6934280MemoryTrain:  epoch  2, batch     0 | loss: 1.4959526MemoryTrain:  epoch  3, batch     0 | loss: 1.3371004MemoryTrain:  epoch  4, batch     0 | loss: 0.9675116MemoryTrain:  epoch  5, batch     0 | loss: 0.7164345MemoryTrain:  epoch  6, batch     0 | loss: 0.5407633MemoryTrain:  epoch  7, batch     0 | loss: 0.5776879MemoryTrain:  epoch  8, batch     0 | loss: 0.4455206MemoryTrain:  epoch  9, batch     0 | loss: 0.3866781

F1 score per class: {2: 0.8, 39: 0.0, 8: 0.5806451612903226, 11: 0.5185185185185185, 12: 0.0, 19: 0.0, 26: 0.6666666666666666, 28: 0.13333333333333333}
Micro-average F1 score: 0.5220125786163522
Weighted-average F1 score: 0.4914280197434319
F1 score per class: {33: 0.9411764705882353, 2: 0.0, 36: 0.8266666666666667, 39: 0.788235294117647, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.5454545454545454, 24: 0.0, 26: 0.0, 28: 0.42105263157894735}
Micro-average F1 score: 0.72544080604534
Weighted-average F1 score: 0.6612406880928814
F1 score per class: {2: 0.9411764705882353, 36: 0.0, 39: 0.8187919463087249, 8: 0.8023255813953488, 11: 0.0, 12: 0.0, 19: 0.6153846153846154, 26: 0.0, 28: 0.35294117647058826}
Micro-average F1 score: 0.7379134860050891
Weighted-average F1 score: 0.6860856836571153

F1 score per class: {32: 0.8, 33: 0.5548387096774193, 2: 0.17391304347826086, 36: 0.5581395348837209, 6: 0.4697986577181208, 39: 0.8333333333333334, 8: 0.5, 11: 0.43478260869565216, 12: 0.7262569832402235, 19: 0.16666666666666666, 20: 0.8901098901098901, 24: 0.8888888888888888, 26: 0.8808290155440415, 28: 0.3076923076923077, 29: 0.6666666666666666, 30: 0.11764705882352941}
Micro-average F1 score: 0.6463262764632628
Weighted-average F1 score: 0.6739813825380906
F1 score per class: {32: 0.8, 33: 0.77, 2: 0.6442953020134228, 36: 0.7948717948717948, 6: 0.6536585365853659, 39: 0.8512820512820513, 8: 0.7058823529411765, 11: 0.6060606060606061, 12: 0.7513812154696132, 19: 0.2857142857142857, 20: 0.9021739130434783, 24: 0.8571428571428571, 26: 0.882051282051282, 28: 0.4, 29: 0.8524590163934426, 30: 0.34782608695652173}
Micro-average F1 score: 0.768893756845564
Weighted-average F1 score: 0.7697886199374292
F1 score per class: {32: 0.8888888888888888, 33: 0.7127659574468085, 2: 0.5633802816901409, 36: 0.7870967741935484, 6: 0.6540284360189573, 39: 0.8512820512820513, 8: 0.7058823529411765, 11: 0.5517241379310345, 12: 0.7444444444444445, 19: 0.27586206896551724, 20: 0.8901098901098901, 24: 0.8571428571428571, 26: 0.8762886597938144, 28: 0.42857142857142855, 29: 0.8524590163934426, 30: 0.2857142857142857}
Micro-average F1 score: 0.7515218594355285
Weighted-average F1 score: 0.7517185506188274
cur_acc:  ['0.8366', '0.6836', '0.5220']
his_acc:  ['0.8366', '0.7359', '0.6463']
cur_acc des:  ['0.8563', '0.8439', '0.7254']
his_acc des:  ['0.8563', '0.8176', '0.7689']
cur_acc rrf:  ['0.8511', '0.8439', '0.7379']
his_acc rrf:  ['0.8511', '0.8142', '0.7515']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 268.6006679CurrentTrain: epoch  0, batch     1 | loss: 458.4946204CurrentTrain: epoch  0, batch     2 | loss: 418.1021220CurrentTrain: epoch  0, batch     3 | loss: 469.3678015CurrentTrain: epoch  0, batch     4 | loss: 235.2997894CurrentTrain: epoch  1, batch     0 | loss: 405.1897843CurrentTrain: epoch  1, batch     1 | loss: 294.0927976CurrentTrain: epoch  1, batch     2 | loss: 446.9323812CurrentTrain: epoch  1, batch     3 | loss: 363.7495054CurrentTrain: epoch  1, batch     4 | loss: 254.8776659CurrentTrain: epoch  2, batch     0 | loss: 396.5708544CurrentTrain: epoch  2, batch     1 | loss: 362.8847470CurrentTrain: epoch  2, batch     2 | loss: 337.0785525CurrentTrain: epoch  2, batch     3 | loss: 380.7514451CurrentTrain: epoch  2, batch     4 | loss: 225.1588466CurrentTrain: epoch  3, batch     0 | loss: 412.6775546CurrentTrain: epoch  3, batch     1 | loss: 334.5209334CurrentTrain: epoch  3, batch     2 | loss: 396.2484311CurrentTrain: epoch  3, batch     3 | loss: 378.2808611CurrentTrain: epoch  3, batch     4 | loss: 197.8836226CurrentTrain: epoch  4, batch     0 | loss: 361.5964952CurrentTrain: epoch  4, batch     1 | loss: 375.0585909CurrentTrain: epoch  4, batch     2 | loss: 444.3220989CurrentTrain: epoch  4, batch     3 | loss: 286.4712393CurrentTrain: epoch  4, batch     4 | loss: 252.6407721CurrentTrain: epoch  5, batch     0 | loss: 409.1062321CurrentTrain: epoch  5, batch     1 | loss: 342.4884218CurrentTrain: epoch  5, batch     2 | loss: 316.0807518CurrentTrain: epoch  5, batch     3 | loss: 375.7637514CurrentTrain: epoch  5, batch     4 | loss: 235.1982794CurrentTrain: epoch  6, batch     0 | loss: 392.2035171CurrentTrain: epoch  6, batch     1 | loss: 438.4354355CurrentTrain: epoch  6, batch     2 | loss: 360.0958245CurrentTrain: epoch  6, batch     3 | loss: 286.8628176CurrentTrain: epoch  6, batch     4 | loss: 267.9612414CurrentTrain: epoch  7, batch     0 | loss: 390.1462459CurrentTrain: epoch  7, batch     1 | loss: 391.6370069CurrentTrain: epoch  7, batch     2 | loss: 360.0234762CurrentTrain: epoch  7, batch     3 | loss: 359.1905127CurrentTrain: epoch  7, batch     4 | loss: 220.3444274CurrentTrain: epoch  8, batch     0 | loss: 300.0544646CurrentTrain: epoch  8, batch     1 | loss: 437.1073555CurrentTrain: epoch  8, batch     2 | loss: 329.2952240CurrentTrain: epoch  8, batch     3 | loss: 325.9109529CurrentTrain: epoch  8, batch     4 | loss: 341.2876503CurrentTrain: epoch  9, batch     0 | loss: 356.7371309CurrentTrain: epoch  9, batch     1 | loss: 391.4850070CurrentTrain: epoch  9, batch     2 | loss: 295.7478062CurrentTrain: epoch  9, batch     3 | loss: 419.4008770CurrentTrain: epoch  9, batch     4 | loss: 248.0811541
MemoryTrain:  epoch  0, batch     0 | loss: 1.5584241MemoryTrain:  epoch  1, batch     0 | loss: 1.3059436MemoryTrain:  epoch  2, batch     0 | loss: 0.9414257MemoryTrain:  epoch  3, batch     0 | loss: 0.7762775MemoryTrain:  epoch  4, batch     0 | loss: 0.5799417MemoryTrain:  epoch  5, batch     0 | loss: 0.4930874MemoryTrain:  epoch  6, batch     0 | loss: 0.3785607MemoryTrain:  epoch  7, batch     0 | loss: 0.3517011MemoryTrain:  epoch  8, batch     0 | loss: 0.2775011MemoryTrain:  epoch  9, batch     0 | loss: 0.2015138

F1 score per class: {5: 0.9743589743589743, 6: 0.0, 10: 0.5774647887323944, 11: 0.0, 12: 0.0, 16: 0.8518518518518519, 17: 0.6153846153846154, 18: 0.27906976744186046, 28: 0.0}
Micro-average F1 score: 0.7115789473684211
Weighted-average F1 score: 0.7105428402085286
F1 score per class: {5: 0.9950248756218906, 6: 0.0, 39: 0.0, 8: 0.7215189873417721, 10: 0.0, 11: 0.0, 12: 0.8928571428571429, 16: 0.3333333333333333, 17: 0.5490196078431373, 18: 0.0, 28: 0.0}
Micro-average F1 score: 0.7542857142857143
Weighted-average F1 score: 0.6997838310350336
F1 score per class: {5: 0.9950248756218906, 6: 0.0, 39: 0.0, 8: 0.7453416149068323, 10: 0.0, 11: 0.0, 12: 0.8928571428571429, 16: 0.6153846153846154, 17: 0.4897959183673469, 18: 0.0, 20: 0.0, 28: 0.0}
Micro-average F1 score: 0.767175572519084
Weighted-average F1 score: 0.7157932505780648

F1 score per class: {2: 0.7142857142857143, 5: 0.9595959595959596, 6: 0.6071428571428571, 8: 0.09302325581395349, 10: 0.47953216374269003, 11: 0.4892086330935252, 12: 0.38571428571428573, 16: 0.7796610169491526, 17: 0.5, 18: 0.22641509433962265, 19: 0.7415730337078652, 20: 0.4117647058823529, 24: 0.5, 26: 0.7374301675977654, 28: 0.2631578947368421, 29: 0.8715083798882681, 30: 0.8888888888888888, 32: 0.8795811518324608, 33: 0.42857142857142855, 36: 0.40963855421686746, 39: 0.1111111111111111}
Micro-average F1 score: 0.6296296296296297
Weighted-average F1 score: 0.6794254507480534
F1 score per class: {2: 0.7058823529411765, 5: 0.9433962264150944, 6: 0.7641509433962265, 8: 0.5797101449275363, 10: 0.6404494382022472, 11: 0.6867469879518072, 12: 0.624390243902439, 16: 0.847457627118644, 17: 0.26666666666666666, 18: 0.35443037974683544, 19: 0.8229166666666666, 20: 0.5405405405405406, 24: 0.6666666666666666, 26: 0.7555555555555555, 28: 0.27586206896551724, 29: 0.8961748633879781, 30: 0.8181818181818182, 32: 0.8979591836734694, 33: 0.4, 36: 0.7407407407407407, 39: 0.17391304347826086}
Micro-average F1 score: 0.7302798982188295
Weighted-average F1 score: 0.7344296457598991
F1 score per class: {2: 0.75, 5: 0.9389671361502347, 6: 0.7106598984771574, 8: 0.37037037037037035, 10: 0.5970149253731343, 11: 0.7209302325581395, 12: 0.6108374384236454, 16: 0.8333333333333334, 17: 0.47058823529411764, 18: 0.3380281690140845, 19: 0.8167539267015707, 20: 0.5526315789473685, 24: 0.42857142857142855, 26: 0.7555555555555555, 28: 0.20512820512820512, 29: 0.8901098901098901, 30: 0.782608695652174, 32: 0.8923076923076924, 33: 0.4, 36: 0.7747747747747747, 39: 0.16666666666666666}
Micro-average F1 score: 0.7095948827292111
Weighted-average F1 score: 0.7165392430059024
cur_acc:  ['0.8366', '0.6836', '0.5220', '0.7116']
his_acc:  ['0.8366', '0.7359', '0.6463', '0.6296']
cur_acc des:  ['0.8563', '0.8439', '0.7254', '0.7543']
his_acc des:  ['0.8563', '0.8176', '0.7689', '0.7303']
cur_acc rrf:  ['0.8511', '0.8439', '0.7379', '0.7672']
his_acc rrf:  ['0.8511', '0.8142', '0.7515', '0.7096']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 418.2981902CurrentTrain: epoch  0, batch     1 | loss: 350.9301382CurrentTrain: epoch  0, batch     2 | loss: 329.2798657CurrentTrain: epoch  0, batch     3 | loss: 400.5951505CurrentTrain: epoch  0, batch     4 | loss: 307.1195218CurrentTrain: epoch  1, batch     0 | loss: 354.6970716CurrentTrain: epoch  1, batch     1 | loss: 465.7141536CurrentTrain: epoch  1, batch     2 | loss: 357.2568179CurrentTrain: epoch  1, batch     3 | loss: 327.3943948CurrentTrain: epoch  1, batch     4 | loss: 221.8669479CurrentTrain: epoch  2, batch     0 | loss: 539.4461527CurrentTrain: epoch  2, batch     1 | loss: 335.7289963CurrentTrain: epoch  2, batch     2 | loss: 283.5756658CurrentTrain: epoch  2, batch     3 | loss: 361.1881128CurrentTrain: epoch  2, batch     4 | loss: 250.4958485CurrentTrain: epoch  3, batch     0 | loss: 378.1373276CurrentTrain: epoch  3, batch     1 | loss: 314.1477242CurrentTrain: epoch  3, batch     2 | loss: 399.1015183CurrentTrain: epoch  3, batch     3 | loss: 379.2273280CurrentTrain: epoch  3, batch     4 | loss: 270.3299008CurrentTrain: epoch  4, batch     0 | loss: 342.5793718CurrentTrain: epoch  4, batch     1 | loss: 318.3151905CurrentTrain: epoch  4, batch     2 | loss: 347.3630064CurrentTrain: epoch  4, batch     3 | loss: 411.0178451CurrentTrain: epoch  4, batch     4 | loss: 206.2771678CurrentTrain: epoch  5, batch     0 | loss: 439.1868552CurrentTrain: epoch  5, batch     1 | loss: 316.6624625CurrentTrain: epoch  5, batch     2 | loss: 376.7881073CurrentTrain: epoch  5, batch     3 | loss: 360.7412291CurrentTrain: epoch  5, batch     4 | loss: 188.9469883CurrentTrain: epoch  6, batch     0 | loss: 301.3110748CurrentTrain: epoch  6, batch     1 | loss: 390.9900192CurrentTrain: epoch  6, batch     2 | loss: 344.3235719CurrentTrain: epoch  6, batch     3 | loss: 456.3390187CurrentTrain: epoch  6, batch     4 | loss: 178.8535388CurrentTrain: epoch  7, batch     0 | loss: 359.0283001CurrentTrain: epoch  7, batch     1 | loss: 316.4835726CurrentTrain: epoch  7, batch     2 | loss: 342.5893818CurrentTrain: epoch  7, batch     3 | loss: 372.7563284CurrentTrain: epoch  7, batch     4 | loss: 233.0849460CurrentTrain: epoch  8, batch     0 | loss: 288.2109914CurrentTrain: epoch  8, batch     1 | loss: 373.9535500CurrentTrain: epoch  8, batch     2 | loss: 341.9198096CurrentTrain: epoch  8, batch     3 | loss: 390.9708247CurrentTrain: epoch  8, batch     4 | loss: 302.0419611CurrentTrain: epoch  9, batch     0 | loss: 407.9711251CurrentTrain: epoch  9, batch     1 | loss: 325.2492452CurrentTrain: epoch  9, batch     2 | loss: 437.9691391CurrentTrain: epoch  9, batch     3 | loss: 358.3962723CurrentTrain: epoch  9, batch     4 | loss: 159.6151846
MemoryTrain:  epoch  0, batch     0 | loss: 1.5778262MemoryTrain:  epoch  1, batch     0 | loss: 1.2307663MemoryTrain:  epoch  2, batch     0 | loss: 0.9647481MemoryTrain:  epoch  3, batch     0 | loss: 0.8058174MemoryTrain:  epoch  4, batch     0 | loss: 0.6296188MemoryTrain:  epoch  5, batch     0 | loss: 0.5553209MemoryTrain:  epoch  6, batch     0 | loss: 0.4737395MemoryTrain:  epoch  7, batch     0 | loss: 0.4508926MemoryTrain:  epoch  8, batch     0 | loss: 0.2938315MemoryTrain:  epoch  9, batch     0 | loss: 0.2502579

F1 score per class: {32: 0.3548387096774194, 1: 0.7286821705426356, 34: 0.0, 3: 0.0, 10: 0.0, 11: 0.0, 14: 0.736318407960199, 18: 0.0, 22: 0.0, 24: 0.0, 26: 0.625}
Micro-average F1 score: 0.5499181669394435
Weighted-average F1 score: 0.56950864923014
F1 score per class: {32: 0.3937007874015748, 1: 0.9426751592356688, 34: 0.0, 3: 0.0, 5: 0.0, 10: 0.075, 11: 0.7362637362637363, 14: 0.0, 22: 0.0, 24: 0.0, 26: 0.9320388349514563}
Micro-average F1 score: 0.6448736998514116
Weighted-average F1 score: 0.6358766446918246
F1 score per class: {32: 0.3937007874015748, 1: 0.9419354838709677, 34: 0.0, 3: 0.0, 5: 0.0, 10: 0.07894736842105263, 11: 0.7379679144385026, 14: 0.0, 22: 0.0, 24: 0.0, 26: 0.9320388349514563}
Micro-average F1 score: 0.6497764530551415
Weighted-average F1 score: 0.6447707056970295

F1 score per class: {1: 0.3308270676691729, 2: 0.7142857142857143, 3: 0.7286821705426356, 5: 0.9644670050761421, 6: 0.5609756097560976, 8: 0.17777777777777778, 10: 0.47674418604651164, 11: 0.4126984126984127, 12: 0.22608695652173913, 14: 0.0, 16: 0.7796610169491526, 17: 0.35294117647058826, 18: 0.09090909090909091, 19: 0.5584415584415584, 20: 0.19672131147540983, 22: 0.7081339712918661, 24: 0.06896551724137931, 26: 0.7403314917127072, 28: 0.3125, 29: 0.8777777777777778, 30: 0.9142857142857143, 32: 0.8066298342541437, 33: 0.42857142857142855, 34: 0.352112676056338, 36: 0.2631578947368421, 39: 0.1111111111111111}
Micro-average F1 score: 0.5566932119833143
Weighted-average F1 score: 0.6094444233683914
F1 score per class: {1: 0.36231884057971014, 2: 0.6666666666666666, 3: 0.9079754601226994, 5: 0.9523809523809523, 6: 0.7669902912621359, 8: 0.5179856115107914, 10: 0.6732673267326733, 11: 0.5714285714285714, 12: 0.7046632124352331, 14: 0.07142857142857142, 16: 0.8666666666666667, 17: 0.5555555555555556, 18: 0.10909090909090909, 19: 0.7528089887640449, 20: 0.3384615384615385, 22: 0.708994708994709, 24: 0.05128205128205128, 26: 0.7540983606557377, 28: 0.26666666666666666, 29: 0.9139784946236559, 30: 0.972972972972973, 32: 0.8297872340425532, 33: 0.4, 34: 0.5423728813559322, 36: 0.7454545454545455, 39: 0.18181818181818182}
Micro-average F1 score: 0.6745484400656815
Weighted-average F1 score: 0.6820327569083593
F1 score per class: {1: 0.36231884057971014, 2: 0.7058823529411765, 3: 0.9125, 5: 0.9569377990430622, 6: 0.7171717171717171, 8: 0.4107142857142857, 10: 0.6787330316742082, 11: 0.5673758865248227, 12: 0.6551724137931034, 14: 0.075, 16: 0.8852459016393442, 17: 0.5882352941176471, 18: 0.11764705882352941, 19: 0.7540983606557377, 20: 0.35294117647058826, 22: 0.7040816326530612, 24: 0.05405405405405406, 26: 0.7540983606557377, 28: 0.20512820512820512, 29: 0.9021739130434783, 30: 0.972972972972973, 32: 0.8297872340425532, 33: 0.42857142857142855, 34: 0.5026178010471204, 36: 0.6464646464646465, 39: 0.09523809523809523}
Micro-average F1 score: 0.6591586618085459
Weighted-average F1 score: 0.6683558932494629
cur_acc:  ['0.8366', '0.6836', '0.5220', '0.7116', '0.5499']
his_acc:  ['0.8366', '0.7359', '0.6463', '0.6296', '0.5567']
cur_acc des:  ['0.8563', '0.8439', '0.7254', '0.7543', '0.6449']
his_acc des:  ['0.8563', '0.8176', '0.7689', '0.7303', '0.6745']
cur_acc rrf:  ['0.8511', '0.8439', '0.7379', '0.7672', '0.6498']
his_acc rrf:  ['0.8511', '0.8142', '0.7515', '0.7096', '0.6592']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 330.3701556CurrentTrain: epoch  0, batch     1 | loss: 351.4813798CurrentTrain: epoch  0, batch     2 | loss: 335.4208840CurrentTrain: epoch  0, batch     3 | loss: 29.9145198CurrentTrain: epoch  1, batch     0 | loss: 345.9942020CurrentTrain: epoch  1, batch     1 | loss: 276.4767055CurrentTrain: epoch  1, batch     2 | loss: 315.6665823CurrentTrain: epoch  1, batch     3 | loss: 55.5885092CurrentTrain: epoch  2, batch     0 | loss: 414.7607182CurrentTrain: epoch  2, batch     1 | loss: 286.4351899CurrentTrain: epoch  2, batch     2 | loss: 280.7782606CurrentTrain: epoch  2, batch     3 | loss: 20.7664224CurrentTrain: epoch  3, batch     0 | loss: 295.5112043CurrentTrain: epoch  3, batch     1 | loss: 345.6464157CurrentTrain: epoch  3, batch     2 | loss: 332.7966301CurrentTrain: epoch  3, batch     3 | loss: 11.4893298CurrentTrain: epoch  4, batch     0 | loss: 334.3076973CurrentTrain: epoch  4, batch     1 | loss: 277.4995604CurrentTrain: epoch  4, batch     2 | loss: 318.6196220CurrentTrain: epoch  4, batch     3 | loss: 55.0991402CurrentTrain: epoch  5, batch     0 | loss: 345.7748996CurrentTrain: epoch  5, batch     1 | loss: 290.3161244CurrentTrain: epoch  5, batch     2 | loss: 316.8046333CurrentTrain: epoch  5, batch     3 | loss: 31.9612541CurrentTrain: epoch  6, batch     0 | loss: 239.5818618CurrentTrain: epoch  6, batch     1 | loss: 331.9587932CurrentTrain: epoch  6, batch     2 | loss: 422.5492287CurrentTrain: epoch  6, batch     3 | loss: 19.3639155CurrentTrain: epoch  7, batch     0 | loss: 274.2347774CurrentTrain: epoch  7, batch     1 | loss: 373.8907788CurrentTrain: epoch  7, batch     2 | loss: 283.6893605CurrentTrain: epoch  7, batch     3 | loss: 54.8133093CurrentTrain: epoch  8, batch     0 | loss: 300.9768423CurrentTrain: epoch  8, batch     1 | loss: 314.8377603CurrentTrain: epoch  8, batch     2 | loss: 300.7395600CurrentTrain: epoch  8, batch     3 | loss: 30.3519782CurrentTrain: epoch  9, batch     0 | loss: 344.6402809CurrentTrain: epoch  9, batch     1 | loss: 284.7756309CurrentTrain: epoch  9, batch     2 | loss: 301.2557113CurrentTrain: epoch  9, batch     3 | loss: 23.2170417
MemoryTrain:  epoch  0, batch     0 | loss: 1.0068440MemoryTrain:  epoch  1, batch     0 | loss: 0.8084635MemoryTrain:  epoch  2, batch     0 | loss: 0.5903114MemoryTrain:  epoch  3, batch     0 | loss: 0.4455757MemoryTrain:  epoch  4, batch     0 | loss: 0.3603408MemoryTrain:  epoch  5, batch     0 | loss: 0.3013847MemoryTrain:  epoch  6, batch     0 | loss: 0.2583995MemoryTrain:  epoch  7, batch     0 | loss: 0.2048008MemoryTrain:  epoch  8, batch     0 | loss: 0.1777234MemoryTrain:  epoch  9, batch     0 | loss: 0.1498391

F1 score per class: {1: 0.0, 34: 0.8888888888888888, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 22: 0.23529411764705882, 26: 0.6666666666666666, 27: 0.0, 31: 0.6336633663366337}
Micro-average F1 score: 0.5871559633027523
Weighted-average F1 score: 0.4863855163796922
F1 score per class: {1: 0.0, 34: 0.0, 3: 0.0, 6: 0.75, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 22: 0.3157894736842105, 26: 1.0, 27: 0.0, 31: 0.7169811320754716}
Micro-average F1 score: 0.6283185840707964
Weighted-average F1 score: 0.5169627123441233
F1 score per class: {1: 0.0, 34: 0.0, 3: 0.0, 6: 0.75, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 22: 0.3157894736842105, 26: 1.0, 27: 0.0, 31: 0.7289719626168224}
Micro-average F1 score: 0.6371681415929203
Weighted-average F1 score: 0.5275397080868747

F1 score per class: {1: 0.2975206611570248, 2: 0.8, 3: 0.6984126984126984, 5: 0.9651741293532339, 6: 0.16216216216216217, 7: 0.07207207207207207, 8: 0.13636363636363635, 9: 0.9803921568627451, 10: 0.4342105263157895, 11: 0.4748201438848921, 12: 0.14035087719298245, 14: 0.13513513513513514, 16: 0.7796610169491526, 17: 0.0, 18: 0.22641509433962265, 19: 0.6082474226804123, 20: 0.5405405405405406, 22: 0.75, 24: 0.06060606060606061, 26: 0.7159090909090909, 27: 0.12903225806451613, 28: 0.27906976744186046, 29: 0.8700564971751412, 30: 0.9142857142857143, 31: 0.6666666666666666, 32: 0.7796610169491526, 33: 0.42857142857142855, 34: 0.34532374100719426, 36: 0.3076923076923077, 39: 0.1, 40: 0.47761194029850745}
Micro-average F1 score: 0.5258855585831063
Weighted-average F1 score: 0.545950682546568
F1 score per class: {1: 0.3357664233576642, 2: 0.5217391304347826, 3: 0.9325153374233128, 5: 0.9259259259259259, 6: 0.36220472440944884, 7: 0.05714285714285714, 8: 0.37037037037037035, 9: 0.819672131147541, 10: 0.6486486486486487, 11: 0.6557377049180327, 12: 0.573170731707317, 14: 0.0975609756097561, 16: 0.8524590163934426, 17: 0.14285714285714285, 18: 0.26229508196721313, 19: 0.6829268292682927, 20: 0.6075949367088608, 22: 0.7553191489361702, 24: 0.05555555555555555, 26: 0.75, 27: 0.2222222222222222, 28: 0.21818181818181817, 29: 0.8852459016393442, 30: 1.0, 31: 0.6666666666666666, 32: 0.8043478260869565, 33: 0.35294117647058826, 34: 0.42028985507246375, 36: 0.6122448979591837, 39: 0.09090909090909091, 40: 0.475}
Micro-average F1 score: 0.6060422960725076
Weighted-average F1 score: 0.5964034269626044
F1 score per class: {1: 0.36231884057971014, 2: 0.5217391304347826, 3: 0.925, 5: 0.9259259259259259, 6: 0.336, 7: 0.056074766355140186, 8: 0.2857142857142857, 9: 0.9433962264150944, 10: 0.6310160427807486, 11: 0.6480446927374302, 12: 0.55, 14: 0.12048192771084337, 16: 0.8571428571428571, 17: 0.16666666666666666, 18: 0.2711864406779661, 19: 0.6766169154228856, 20: 0.6, 22: 0.7643979057591623, 24: 0.05714285714285714, 26: 0.7431693989071039, 27: 0.1935483870967742, 28: 0.21212121212121213, 29: 0.8715083798882681, 30: 1.0, 31: 1.0, 32: 0.8043478260869565, 33: 0.375, 34: 0.3841059602649007, 36: 0.5111111111111111, 39: 0.09523809523809523, 40: 0.4588235294117647}
Micro-average F1 score: 0.5946109597335756
Weighted-average F1 score: 0.5851169696408721
cur_acc:  ['0.8366', '0.6836', '0.5220', '0.7116', '0.5499', '0.5872']
his_acc:  ['0.8366', '0.7359', '0.6463', '0.6296', '0.5567', '0.5259']
cur_acc des:  ['0.8563', '0.8439', '0.7254', '0.7543', '0.6449', '0.6283']
his_acc des:  ['0.8563', '0.8176', '0.7689', '0.7303', '0.6745', '0.6060']
cur_acc rrf:  ['0.8511', '0.8439', '0.7379', '0.7672', '0.6498', '0.6372']
his_acc rrf:  ['0.8511', '0.8142', '0.7515', '0.7096', '0.6592', '0.5946']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 371.5410571CurrentTrain: epoch  0, batch     1 | loss: 328.8945795CurrentTrain: epoch  0, batch     2 | loss: 316.8225543CurrentTrain: epoch  0, batch     3 | loss: 372.8335576CurrentTrain: epoch  1, batch     0 | loss: 290.2558080CurrentTrain: epoch  1, batch     1 | loss: 355.8752796CurrentTrain: epoch  1, batch     2 | loss: 317.3229762CurrentTrain: epoch  1, batch     3 | loss: 370.9149875CurrentTrain: epoch  2, batch     0 | loss: 340.8781971CurrentTrain: epoch  2, batch     1 | loss: 367.7939269CurrentTrain: epoch  2, batch     2 | loss: 361.2573656CurrentTrain: epoch  2, batch     3 | loss: 203.2652838CurrentTrain: epoch  3, batch     0 | loss: 392.2533500CurrentTrain: epoch  3, batch     1 | loss: 294.7245694CurrentTrain: epoch  3, batch     2 | loss: 398.0399028CurrentTrain: epoch  3, batch     3 | loss: 236.8495136CurrentTrain: epoch  4, batch     0 | loss: 319.9833005CurrentTrain: epoch  4, batch     1 | loss: 346.4995580CurrentTrain: epoch  4, batch     2 | loss: 300.5919144CurrentTrain: epoch  4, batch     3 | loss: 294.8496013CurrentTrain: epoch  5, batch     0 | loss: 379.7807139CurrentTrain: epoch  5, batch     1 | loss: 316.2678627CurrentTrain: epoch  5, batch     2 | loss: 392.8859072CurrentTrain: epoch  5, batch     3 | loss: 191.7255288CurrentTrain: epoch  6, batch     0 | loss: 408.5630948CurrentTrain: epoch  6, batch     1 | loss: 274.4099006CurrentTrain: epoch  6, batch     2 | loss: 346.3296654CurrentTrain: epoch  6, batch     3 | loss: 230.9882206CurrentTrain: epoch  7, batch     0 | loss: 300.5628237CurrentTrain: epoch  7, batch     1 | loss: 377.2484875CurrentTrain: epoch  7, batch     2 | loss: 317.0067155CurrentTrain: epoch  7, batch     3 | loss: 243.9099576CurrentTrain: epoch  8, batch     0 | loss: 357.4324379CurrentTrain: epoch  8, batch     1 | loss: 286.8343527CurrentTrain: epoch  8, batch     2 | loss: 312.5114597CurrentTrain: epoch  8, batch     3 | loss: 290.3045020CurrentTrain: epoch  9, batch     0 | loss: 297.3844802CurrentTrain: epoch  9, batch     1 | loss: 342.6731703CurrentTrain: epoch  9, batch     2 | loss: 375.4083954CurrentTrain: epoch  9, batch     3 | loss: 239.6833326
MemoryTrain:  epoch  0, batch     0 | loss: 1.2587679MemoryTrain:  epoch  1, batch     0 | loss: 1.1229549MemoryTrain:  epoch  2, batch     0 | loss: 0.8267688MemoryTrain:  epoch  3, batch     0 | loss: 0.7078777MemoryTrain:  epoch  4, batch     0 | loss: 0.5964122MemoryTrain:  epoch  5, batch     0 | loss: 0.5075593MemoryTrain:  epoch  6, batch     0 | loss: 0.4111474MemoryTrain:  epoch  7, batch     0 | loss: 0.3565179MemoryTrain:  epoch  8, batch     0 | loss: 0.3398698MemoryTrain:  epoch  9, batch     0 | loss: 0.2723681

F1 score per class: {34: 0.0, 35: 0.0, 5: 0.0, 37: 0.8888888888888888, 38: 0.0, 10: 0.5352112676056338, 11: 0.0, 15: 0.0, 18: 0.7407407407407407, 25: 0.6206896551724138, 28: 0.782608695652174}
Micro-average F1 score: 0.6017699115044248
Weighted-average F1 score: 0.509767569777063
F1 score per class: {1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 38: 0.0, 8: 0.75, 7: 0.0, 11: 0.6835443037974683, 36: 0.0, 10: 0.0, 15: 0.0, 18: 0.8275862068965517, 25: 0.0, 26: 0.6363636363636364, 28: 0.84}
Micro-average F1 score: 0.6020408163265306
Weighted-average F1 score: 0.46621456266486555
F1 score per class: {1: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 7: 0.0, 10: 0.0, 11: 0.631578947368421, 15: 0.0, 18: 0.0, 20: 0.0, 25: 0.8791208791208791, 26: 0.651685393258427, 28: 0.8627450980392157}
Micro-average F1 score: 0.622107969151671
Weighted-average F1 score: 0.4976848061759235

F1 score per class: {1: 0.2542372881355932, 2: 0.7058823529411765, 3: 0.36, 5: 0.8930232558139535, 6: 0.05825242718446602, 7: 0.06593406593406594, 8: 0.09302325581395349, 9: 0.96, 10: 0.35555555555555557, 11: 0.44776119402985076, 12: 0.07272727272727272, 14: 0.0, 15: 0.7619047619047619, 16: 0.7796610169491526, 17: 0.0, 18: 0.0, 19: 0.5641025641025641, 20: 0.43478260869565216, 22: 0.6946107784431138, 24: 0.07692307692307693, 25: 0.5352112676056338, 26: 0.7415730337078652, 27: 0.36363636363636365, 28: 0.24242424242424243, 29: 0.8505747126436781, 30: 0.9142857142857143, 31: 1.0, 32: 0.5960264900662252, 33: 0.42857142857142855, 34: 0.5298013245033113, 35: 0.48, 36: 0.0, 37: 0.4122137404580153, 38: 0.4090909090909091, 39: 0.11764705882352941, 40: 0.5132743362831859}
Micro-average F1 score: 0.48042593172564985
Weighted-average F1 score: 0.5354432163639002
F1 score per class: {1: 0.3356643356643357, 2: 0.56, 3: 0.7901234567901234, 5: 0.8547008547008547, 6: 0.4153846153846154, 7: 0.023809523809523808, 8: 0.48739495798319327, 9: 0.9259259259259259, 10: 0.6352941176470588, 11: 0.6410256410256411, 12: 0.5217391304347826, 14: 0.0963855421686747, 15: 0.631578947368421, 16: 0.8666666666666667, 17: 0.0, 18: 0.11764705882352941, 19: 0.63, 20: 0.5, 22: 0.7700534759358288, 24: 0.07142857142857142, 25: 0.6835443037974683, 26: 0.75, 27: 0.25, 28: 0.20588235294117646, 29: 0.8729281767955801, 30: 0.972972972972973, 31: 0.6666666666666666, 32: 0.7613636363636364, 33: 0.35294117647058826, 34: 0.5317919075144508, 35: 0.6605504587155964, 36: 0.5894736842105263, 37: 0.47863247863247865, 38: 0.5, 39: 0.0, 40: 0.4875}
Micro-average F1 score: 0.592391304347826
Weighted-average F1 score: 0.5857076216151698
F1 score per class: {1: 0.3597122302158273, 2: 0.6363636363636364, 3: 0.7222222222222222, 5: 0.8571428571428571, 6: 0.3089430894308943, 7: 0.039603960396039604, 8: 0.28865979381443296, 9: 0.9803921568627451, 10: 0.5662650602409639, 11: 0.6369426751592356, 12: 0.44, 14: 0.09523809523809523, 15: 0.5714285714285714, 16: 0.8524590163934426, 17: 0.0, 18: 0.09090909090909091, 19: 0.6331658291457286, 20: 0.5135135135135135, 22: 0.7724867724867724, 24: 0.07692307692307693, 25: 0.631578947368421, 26: 0.7431693989071039, 27: 0.24242424242424243, 28: 0.21875, 29: 0.88268156424581, 30: 0.972972972972973, 31: 0.8, 32: 0.7752808988764045, 33: 0.375, 34: 0.5168539325842697, 35: 0.6451612903225806, 36: 0.14084507042253522, 37: 0.40559440559440557, 38: 0.43564356435643564, 39: 0.0, 40: 0.4819277108433735}
Micro-average F1 score: 0.5594749794913864
Weighted-average F1 score: 0.5644628852242178
cur_acc:  ['0.8366', '0.6836', '0.5220', '0.7116', '0.5499', '0.5872', '0.6018']
his_acc:  ['0.8366', '0.7359', '0.6463', '0.6296', '0.5567', '0.5259', '0.4804']
cur_acc des:  ['0.8563', '0.8439', '0.7254', '0.7543', '0.6449', '0.6283', '0.6020']
his_acc des:  ['0.8563', '0.8176', '0.7689', '0.7303', '0.6745', '0.6060', '0.5924']
cur_acc rrf:  ['0.8511', '0.8439', '0.7379', '0.7672', '0.6498', '0.6372', '0.6221']
his_acc rrf:  ['0.8511', '0.8142', '0.7515', '0.7096', '0.6592', '0.5946', '0.5595']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 341.7987459CurrentTrain: epoch  0, batch     1 | loss: 368.8158428CurrentTrain: epoch  0, batch     2 | loss: 453.5283332CurrentTrain: epoch  0, batch     3 | loss: 253.8892605CurrentTrain: epoch  1, batch     0 | loss: 371.9633756CurrentTrain: epoch  1, batch     1 | loss: 317.8030746CurrentTrain: epoch  1, batch     2 | loss: 300.4266711CurrentTrain: epoch  1, batch     3 | loss: 329.5694050CurrentTrain: epoch  2, batch     0 | loss: 391.4403575CurrentTrain: epoch  2, batch     1 | loss: 315.6623070CurrentTrain: epoch  2, batch     2 | loss: 307.1285534CurrentTrain: epoch  2, batch     3 | loss: 314.6335947CurrentTrain: epoch  3, batch     0 | loss: 378.0420526CurrentTrain: epoch  3, batch     1 | loss: 322.0357061CurrentTrain: epoch  3, batch     2 | loss: 374.9166078CurrentTrain: epoch  3, batch     3 | loss: 224.1745386CurrentTrain: epoch  4, batch     0 | loss: 420.1137893CurrentTrain: epoch  4, batch     1 | loss: 323.9081600CurrentTrain: epoch  4, batch     2 | loss: 334.3034158CurrentTrain: epoch  4, batch     3 | loss: 267.7088706CurrentTrain: epoch  5, batch     0 | loss: 447.3299974CurrentTrain: epoch  5, batch     1 | loss: 301.4368544CurrentTrain: epoch  5, batch     2 | loss: 357.8377845CurrentTrain: epoch  5, batch     3 | loss: 241.8303382CurrentTrain: epoch  6, batch     0 | loss: 456.3837200CurrentTrain: epoch  6, batch     1 | loss: 374.6549609CurrentTrain: epoch  6, batch     2 | loss: 273.3985584CurrentTrain: epoch  6, batch     3 | loss: 243.2865457CurrentTrain: epoch  7, batch     0 | loss: 326.5037779CurrentTrain: epoch  7, batch     1 | loss: 374.0596768CurrentTrain: epoch  7, batch     2 | loss: 341.7729367CurrentTrain: epoch  7, batch     3 | loss: 255.8189452CurrentTrain: epoch  8, batch     0 | loss: 415.0186748CurrentTrain: epoch  8, batch     1 | loss: 341.0501499CurrentTrain: epoch  8, batch     2 | loss: 257.2311315CurrentTrain: epoch  8, batch     3 | loss: 280.0317826CurrentTrain: epoch  9, batch     0 | loss: 309.3474356CurrentTrain: epoch  9, batch     1 | loss: 311.3978250CurrentTrain: epoch  9, batch     2 | loss: 392.3564771CurrentTrain: epoch  9, batch     3 | loss: 350.4705838
MemoryTrain:  epoch  0, batch     0 | loss: 1.1564488MemoryTrain:  epoch  1, batch     0 | loss: 0.9613822MemoryTrain:  epoch  2, batch     0 | loss: 0.7368278MemoryTrain:  epoch  3, batch     0 | loss: 0.6526109MemoryTrain:  epoch  4, batch     0 | loss: 0.5015724MemoryTrain:  epoch  5, batch     0 | loss: 0.3992508MemoryTrain:  epoch  6, batch     0 | loss: 0.3224040MemoryTrain:  epoch  7, batch     0 | loss: 0.2710384MemoryTrain:  epoch  8, batch     0 | loss: 0.2431991MemoryTrain:  epoch  9, batch     0 | loss: 0.2586140

F1 score per class: {0: 0.9428571428571428, 32: 0.0, 2: 0.0, 1: 0.9010989010989011, 4: 0.0, 11: 0.75, 13: 0.0, 14: 0.0, 15: 0.41025641025641024, 21: 0.0, 22: 0.8809523809523809, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8049382716049382
Weighted-average F1 score: 0.7736847836847838
F1 score per class: {0: 0.9863013698630136, 1: 0.0, 2: 0.0, 4: 0.88268156424581, 5: 0.0, 11: 0.0, 13: 0.5714285714285714, 14: 0.0, 15: 0.0, 21: 0.7843137254901961, 23: 0.7792207792207793, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.7713625866050808
Weighted-average F1 score: 0.6828377836609011
F1 score per class: {0: 0.9863013698630136, 1: 0.0, 2: 0.0, 4: 0.8888888888888888, 5: 0.0, 11: 0.0, 13: 0.5714285714285714, 14: 0.0, 15: 0.0, 21: 0.7843137254901961, 23: 0.7631578947368421, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.7713625866050808
Weighted-average F1 score: 0.6834679390901836

F1 score per class: {0: 0.9295774647887324, 1: 0.296875, 2: 0.5384615384615384, 3: 0.34615384615384615, 4: 0.9010989010989011, 5: 0.8930232558139535, 6: 0.05825242718446602, 7: 0.07058823529411765, 8: 0.09302325581395349, 9: 0.96, 10: 0.25210084033613445, 11: 0.36065573770491804, 12: 0.037037037037037035, 13: 0.05660377358490566, 14: 0.029850746268656716, 15: 0.6, 16: 0.8333333333333334, 17: 0.0, 18: 0.0, 19: 0.5670103092783505, 20: 0.2857142857142857, 21: 0.20512820512820512, 22: 0.527027027027027, 23: 0.8314606741573034, 24: 0.0, 25: 0.4927536231884058, 26: 0.7272727272727273, 27: 0.24, 28: 0.2857142857142857, 29: 0.8390804597701149, 30: 0.9444444444444444, 31: 0.8, 32: 0.6790123456790124, 33: 0.42857142857142855, 34: 0.40625, 35: 0.45, 36: 0.0, 37: 0.3826086956521739, 38: 0.4489795918367347, 39: 0.11764705882352941, 40: 0.44}
Micro-average F1 score: 0.4804939657591917
Weighted-average F1 score: 0.5253011430346921
F1 score per class: {0: 0.96, 1: 0.3310344827586207, 2: 0.3783783783783784, 3: 0.8023952095808383, 4: 0.8729281767955801, 5: 0.8547008547008547, 6: 0.36507936507936506, 7: 0.047619047619047616, 8: 0.5210084033613446, 9: 0.9259259259259259, 10: 0.39416058394160586, 11: 0.5853658536585366, 12: 0.5063291139240507, 13: 0.0425531914893617, 14: 0.07228915662650602, 15: 0.6, 16: 0.8852459016393442, 17: 0.0, 18: 0.0851063829787234, 19: 0.6432160804020101, 20: 0.36363636363636365, 21: 0.33613445378151263, 22: 0.5, 23: 0.6741573033707865, 24: 0.0, 25: 0.6329113924050633, 26: 0.7040816326530612, 27: 0.20689655172413793, 28: 0.26666666666666666, 29: 0.8681318681318682, 30: 1.0, 31: 1.0, 32: 0.7428571428571429, 33: 0.3333333333333333, 34: 0.519774011299435, 35: 0.6605504587155964, 36: 0.5434782608695652, 37: 0.3925233644859813, 38: 0.6086956521739131, 39: 0.11764705882352941, 40: 0.5170068027210885}
Micro-average F1 score: 0.5699633699633699
Weighted-average F1 score: 0.5633302346748696
F1 score per class: {0: 0.972972972972973, 1: 0.3448275862068966, 2: 0.3684210526315789, 3: 0.7922077922077922, 4: 0.8839779005524862, 5: 0.8583690987124464, 6: 0.3114754098360656, 7: 0.06451612903225806, 8: 0.30303030303030304, 9: 0.9803921568627451, 10: 0.36496350364963503, 11: 0.5714285714285714, 12: 0.423841059602649, 13: 0.036036036036036036, 14: 0.0759493670886076, 15: 0.5714285714285714, 16: 0.8852459016393442, 17: 0.0, 18: 0.08888888888888889, 19: 0.6368159203980099, 20: 0.3880597014925373, 21: 0.32786885245901637, 22: 0.4927536231884058, 23: 0.6904761904761905, 24: 0.0, 25: 0.6133333333333333, 26: 0.7076923076923077, 27: 0.20689655172413793, 28: 0.32, 29: 0.8666666666666667, 30: 1.0, 31: 1.0, 32: 0.7777777777777778, 33: 0.375, 34: 0.4919786096256685, 35: 0.6782608695652174, 36: 0.1917808219178082, 37: 0.352, 38: 0.5142857142857142, 39: 0.11764705882352941, 40: 0.5131578947368421}
Micro-average F1 score: 0.5480462029982797
Weighted-average F1 score: 0.5469003118969367
cur_acc:  ['0.8366', '0.6836', '0.5220', '0.7116', '0.5499', '0.5872', '0.6018', '0.8049']
his_acc:  ['0.8366', '0.7359', '0.6463', '0.6296', '0.5567', '0.5259', '0.4804', '0.4805']
cur_acc des:  ['0.8563', '0.8439', '0.7254', '0.7543', '0.6449', '0.6283', '0.6020', '0.7714']
his_acc des:  ['0.8563', '0.8176', '0.7689', '0.7303', '0.6745', '0.6060', '0.5924', '0.5700']
cur_acc rrf:  ['0.8511', '0.8439', '0.7379', '0.7672', '0.6498', '0.6372', '0.6221', '0.7714']
his_acc rrf:  ['0.8511', '0.8142', '0.7515', '0.7096', '0.6592', '0.5946', '0.5595', '0.5480']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 372.1605347CurrentTrain: epoch  0, batch     1 | loss: 341.5314382CurrentTrain: epoch  0, batch     2 | loss: 408.4030013CurrentTrain: epoch  0, batch     3 | loss: 383.0783903CurrentTrain: epoch  0, batch     4 | loss: 455.6614427CurrentTrain: epoch  0, batch     5 | loss: 422.9562119CurrentTrain: epoch  0, batch     6 | loss: 412.1939512CurrentTrain: epoch  0, batch     7 | loss: 370.6756775CurrentTrain: epoch  0, batch     8 | loss: 394.4945495CurrentTrain: epoch  0, batch     9 | loss: 391.2467740CurrentTrain: epoch  0, batch    10 | loss: 465.1429159CurrentTrain: epoch  0, batch    11 | loss: 323.7374535CurrentTrain: epoch  0, batch    12 | loss: 297.4945566CurrentTrain: epoch  0, batch    13 | loss: 337.4145726CurrentTrain: epoch  0, batch    14 | loss: 373.5563482CurrentTrain: epoch  0, batch    15 | loss: 373.1053104CurrentTrain: epoch  0, batch    16 | loss: 332.8011725CurrentTrain: epoch  0, batch    17 | loss: 467.1135671CurrentTrain: epoch  0, batch    18 | loss: 528.0639251CurrentTrain: epoch  0, batch    19 | loss: 359.1430358CurrentTrain: epoch  0, batch    20 | loss: 436.6074740CurrentTrain: epoch  0, batch    21 | loss: 283.5256468CurrentTrain: epoch  0, batch    22 | loss: 344.9231405CurrentTrain: epoch  0, batch    23 | loss: 273.1948818CurrentTrain: epoch  0, batch    24 | loss: 340.2758360CurrentTrain: epoch  0, batch    25 | loss: 371.3588407CurrentTrain: epoch  0, batch    26 | loss: 344.7207076CurrentTrain: epoch  0, batch    27 | loss: 451.6722771CurrentTrain: epoch  0, batch    28 | loss: 386.3975753CurrentTrain: epoch  0, batch    29 | loss: 343.4077075CurrentTrain: epoch  0, batch    30 | loss: 372.3158427CurrentTrain: epoch  0, batch    31 | loss: 295.6888199CurrentTrain: epoch  0, batch    32 | loss: 357.5270562CurrentTrain: epoch  0, batch    33 | loss: 450.5151403CurrentTrain: epoch  0, batch    34 | loss: 449.8857761CurrentTrain: epoch  0, batch    35 | loss: 271.7882266CurrentTrain: epoch  0, batch    36 | loss: 330.0864211CurrentTrain: epoch  0, batch    37 | loss: 371.0015775CurrentTrain: epoch  0, batch    38 | loss: 305.8131531CurrentTrain: epoch  0, batch    39 | loss: 362.6162912CurrentTrain: epoch  0, batch    40 | loss: 249.9473650CurrentTrain: epoch  0, batch    41 | loss: 364.2282245CurrentTrain: epoch  0, batch    42 | loss: 330.4880154CurrentTrain: epoch  0, batch    43 | loss: 345.5622267CurrentTrain: epoch  0, batch    44 | loss: 281.0716660CurrentTrain: epoch  0, batch    45 | loss: 343.9795171CurrentTrain: epoch  0, batch    46 | loss: 370.8005676CurrentTrain: epoch  0, batch    47 | loss: 436.7108735CurrentTrain: epoch  0, batch    48 | loss: 318.0405682CurrentTrain: epoch  0, batch    49 | loss: 370.2376589CurrentTrain: epoch  0, batch    50 | loss: 279.2360693CurrentTrain: epoch  0, batch    51 | loss: 270.9198468CurrentTrain: epoch  0, batch    52 | loss: 418.5259194CurrentTrain: epoch  0, batch    53 | loss: 403.9652533CurrentTrain: epoch  0, batch    54 | loss: 343.4452720CurrentTrain: epoch  0, batch    55 | loss: 355.6434488CurrentTrain: epoch  0, batch    56 | loss: 449.5350106CurrentTrain: epoch  0, batch    57 | loss: 368.9113690CurrentTrain: epoch  0, batch    58 | loss: 402.7806981CurrentTrain: epoch  0, batch    59 | loss: 383.9003849CurrentTrain: epoch  0, batch    60 | loss: 371.2195476CurrentTrain: epoch  0, batch    61 | loss: 364.4808603CurrentTrain: epoch  0, batch    62 | loss: 328.6780706CurrentTrain: epoch  0, batch    63 | loss: 383.5845976CurrentTrain: epoch  0, batch    64 | loss: 294.1599310CurrentTrain: epoch  0, batch    65 | loss: 388.3658566CurrentTrain: epoch  0, batch    66 | loss: 354.7710147CurrentTrain: epoch  0, batch    67 | loss: 355.0899240CurrentTrain: epoch  0, batch    68 | loss: 329.7356609CurrentTrain: epoch  0, batch    69 | loss: 327.3865979CurrentTrain: epoch  0, batch    70 | loss: 317.0942247CurrentTrain: epoch  0, batch    71 | loss: 387.9336610CurrentTrain: epoch  0, batch    72 | loss: 328.0573165CurrentTrain: epoch  0, batch    73 | loss: 316.2403101CurrentTrain: epoch  0, batch    74 | loss: 327.9190196CurrentTrain: epoch  0, batch    75 | loss: 359.8627216CurrentTrain: epoch  0, batch    76 | loss: 356.1617463CurrentTrain: epoch  0, batch    77 | loss: 372.0790726CurrentTrain: epoch  0, batch    78 | loss: 416.1860485CurrentTrain: epoch  0, batch    79 | loss: 542.1039794CurrentTrain: epoch  0, batch    80 | loss: 312.5603125CurrentTrain: epoch  0, batch    81 | loss: 291.0982950CurrentTrain: epoch  0, batch    82 | loss: 314.6325809CurrentTrain: epoch  0, batch    83 | loss: 383.8390518CurrentTrain: epoch  0, batch    84 | loss: 342.0229766CurrentTrain: epoch  0, batch    85 | loss: 327.0537916CurrentTrain: epoch  0, batch    86 | loss: 385.9998731CurrentTrain: epoch  0, batch    87 | loss: 374.2284741CurrentTrain: epoch  0, batch    88 | loss: 380.2276007CurrentTrain: epoch  0, batch    89 | loss: 290.1216492CurrentTrain: epoch  0, batch    90 | loss: 336.5683492CurrentTrain: epoch  0, batch    91 | loss: 368.2422552CurrentTrain: epoch  0, batch    92 | loss: 367.9663606CurrentTrain: epoch  0, batch    93 | loss: 402.7797585CurrentTrain: epoch  0, batch    94 | loss: 326.5867451CurrentTrain: epoch  0, batch    95 | loss: 303.4207607CurrentTrain: epoch  1, batch     0 | loss: 430.3369140CurrentTrain: epoch  1, batch     1 | loss: 326.2902983CurrentTrain: epoch  1, batch     2 | loss: 351.9134284CurrentTrain: epoch  1, batch     3 | loss: 540.6693998CurrentTrain: epoch  1, batch     4 | loss: 324.3361725CurrentTrain: epoch  1, batch     5 | loss: 401.3247204CurrentTrain: epoch  1, batch     6 | loss: 401.6113956CurrentTrain: epoch  1, batch     7 | loss: 354.7249931CurrentTrain: epoch  1, batch     8 | loss: 297.7722011CurrentTrain: epoch  1, batch     9 | loss: 364.9485742CurrentTrain: epoch  1, batch    10 | loss: 350.2877751CurrentTrain: epoch  1, batch    11 | loss: 367.5192493CurrentTrain: epoch  1, batch    12 | loss: 355.4569948CurrentTrain: epoch  1, batch    13 | loss: 320.6476554CurrentTrain: epoch  1, batch    14 | loss: 352.5742394CurrentTrain: epoch  1, batch    15 | loss: 311.7899914CurrentTrain: epoch  1, batch    16 | loss: 414.9690233CurrentTrain: epoch  1, batch    17 | loss: 320.8090375CurrentTrain: epoch  1, batch    18 | loss: 369.6360788CurrentTrain: epoch  1, batch    19 | loss: 308.9473754CurrentTrain: epoch  1, batch    20 | loss: 365.3757334CurrentTrain: epoch  1, batch    21 | loss: 319.5349049CurrentTrain: epoch  1, batch    22 | loss: 297.3794637CurrentTrain: epoch  1, batch    23 | loss: 444.3739223CurrentTrain: epoch  1, batch    24 | loss: 288.5244242CurrentTrain: epoch  1, batch    25 | loss: 324.3505734CurrentTrain: epoch  1, batch    26 | loss: 350.0404455CurrentTrain: epoch  1, batch    27 | loss: 298.4846233CurrentTrain: epoch  1, batch    28 | loss: 458.3224396CurrentTrain: epoch  1, batch    29 | loss: 398.2822192CurrentTrain: epoch  1, batch    30 | loss: 332.5723133CurrentTrain: epoch  1, batch    31 | loss: 336.1109298CurrentTrain: epoch  1, batch    32 | loss: 380.7928179CurrentTrain: epoch  1, batch    33 | loss: 349.2683084CurrentTrain: epoch  1, batch    34 | loss: 381.6794904CurrentTrain: epoch  1, batch    35 | loss: 462.0641615CurrentTrain: epoch  1, batch    36 | loss: 395.2240819CurrentTrain: epoch  1, batch    37 | loss: 287.1194503CurrentTrain: epoch  1, batch    38 | loss: 463.1584925CurrentTrain: epoch  1, batch    39 | loss: 373.0951455CurrentTrain: epoch  1, batch    40 | loss: 443.9163144CurrentTrain: epoch  1, batch    41 | loss: 442.8009253CurrentTrain: epoch  1, batch    42 | loss: 262.3641204CurrentTrain: epoch  1, batch    43 | loss: 323.2669805CurrentTrain: epoch  1, batch    44 | loss: 350.4981205CurrentTrain: epoch  1, batch    45 | loss: 355.5516980CurrentTrain: epoch  1, batch    46 | loss: 316.2453629CurrentTrain: epoch  1, batch    47 | loss: 312.3145723CurrentTrain: epoch  1, batch    48 | loss: 337.0694055CurrentTrain: epoch  1, batch    49 | loss: 306.7224740CurrentTrain: epoch  1, batch    50 | loss: 229.3325462CurrentTrain: epoch  1, batch    51 | loss: 309.4821178CurrentTrain: epoch  1, batch    52 | loss: 385.0917667CurrentTrain: epoch  1, batch    53 | loss: 366.9938775CurrentTrain: epoch  1, batch    54 | loss: 273.4210626CurrentTrain: epoch  1, batch    55 | loss: 352.0454340CurrentTrain: epoch  1, batch    56 | loss: 321.7450424CurrentTrain: epoch  1, batch    57 | loss: 463.9105274CurrentTrain: epoch  1, batch    58 | loss: 432.0659462CurrentTrain: epoch  1, batch    59 | loss: 323.6553715CurrentTrain: epoch  1, batch    60 | loss: 286.0227963CurrentTrain: epoch  1, batch    61 | loss: 321.3137668CurrentTrain: epoch  1, batch    62 | loss: 308.1399973CurrentTrain: epoch  1, batch    63 | loss: 307.7360587CurrentTrain: epoch  1, batch    64 | loss: 318.6552217CurrentTrain: epoch  1, batch    65 | loss: 430.9564287CurrentTrain: epoch  1, batch    66 | loss: 376.0177569CurrentTrain: epoch  1, batch    67 | loss: 440.9643933CurrentTrain: epoch  1, batch    68 | loss: 381.2189348CurrentTrain: epoch  1, batch    69 | loss: 383.5325893CurrentTrain: epoch  1, batch    70 | loss: 379.1314122CurrentTrain: epoch  1, batch    71 | loss: 350.2309218CurrentTrain: epoch  1, batch    72 | loss: 348.2751347CurrentTrain: epoch  1, batch    73 | loss: 306.0634446CurrentTrain: epoch  1, batch    74 | loss: 441.1308715CurrentTrain: epoch  1, batch    75 | loss: 351.8393654CurrentTrain: epoch  1, batch    76 | loss: 353.3351484CurrentTrain: epoch  1, batch    77 | loss: 335.8870200CurrentTrain: epoch  1, batch    78 | loss: 366.2739619CurrentTrain: epoch  1, batch    79 | loss: 366.5829883CurrentTrain: epoch  1, batch    80 | loss: 327.8167606CurrentTrain: epoch  1, batch    81 | loss: 313.0701886CurrentTrain: epoch  1, batch    82 | loss: 294.4008212CurrentTrain: epoch  1, batch    83 | loss: 338.3665268CurrentTrain: epoch  1, batch    84 | loss: 317.2339495CurrentTrain: epoch  1, batch    85 | loss: 363.8856219CurrentTrain: epoch  1, batch    86 | loss: 412.1177021CurrentTrain: epoch  1, batch    87 | loss: 399.0172447CurrentTrain: epoch  1, batch    88 | loss: 322.3501910CurrentTrain: epoch  1, batch    89 | loss: 336.4801380CurrentTrain: epoch  1, batch    90 | loss: 307.4608784CurrentTrain: epoch  1, batch    91 | loss: 335.1888085CurrentTrain: epoch  1, batch    92 | loss: 320.0151231CurrentTrain: epoch  1, batch    93 | loss: 332.6257894CurrentTrain: epoch  1, batch    94 | loss: 363.1000741CurrentTrain: epoch  1, batch    95 | loss: 256.6896097CurrentTrain: epoch  2, batch     0 | loss: 330.4700003CurrentTrain: epoch  2, batch     1 | loss: 335.1004334CurrentTrain: epoch  2, batch     2 | loss: 328.6504754CurrentTrain: epoch  2, batch     3 | loss: 420.7125837CurrentTrain: epoch  2, batch     4 | loss: 395.3330351CurrentTrain: epoch  2, batch     5 | loss: 318.5503053CurrentTrain: epoch  2, batch     6 | loss: 411.3777382CurrentTrain: epoch  2, batch     7 | loss: 379.3006434CurrentTrain: epoch  2, batch     8 | loss: 303.6126525CurrentTrain: epoch  2, batch     9 | loss: 377.4995658CurrentTrain: epoch  2, batch    10 | loss: 352.5563822CurrentTrain: epoch  2, batch    11 | loss: 321.9050687CurrentTrain: epoch  2, batch    12 | loss: 321.5794287CurrentTrain: epoch  2, batch    13 | loss: 375.7325192CurrentTrain: epoch  2, batch    14 | loss: 359.9893703CurrentTrain: epoch  2, batch    15 | loss: 394.0834775CurrentTrain: epoch  2, batch    16 | loss: 376.1041394CurrentTrain: epoch  2, batch    17 | loss: 345.4012315CurrentTrain: epoch  2, batch    18 | loss: 379.2797526CurrentTrain: epoch  2, batch    19 | loss: 328.2290805CurrentTrain: epoch  2, batch    20 | loss: 374.9327698CurrentTrain: epoch  2, batch    21 | loss: 393.0176066CurrentTrain: epoch  2, batch    22 | loss: 363.4572364CurrentTrain: epoch  2, batch    23 | loss: 461.6451007CurrentTrain: epoch  2, batch    24 | loss: 363.1947157CurrentTrain: epoch  2, batch    25 | loss: 457.7767302CurrentTrain: epoch  2, batch    26 | loss: 364.9035774CurrentTrain: epoch  2, batch    27 | loss: 362.8300614CurrentTrain: epoch  2, batch    28 | loss: 423.2017001CurrentTrain: epoch  2, batch    29 | loss: 289.5718201CurrentTrain: epoch  2, batch    30 | loss: 443.8157001CurrentTrain: epoch  2, batch    31 | loss: 303.8667416CurrentTrain: epoch  2, batch    32 | loss: 409.6180842CurrentTrain: epoch  2, batch    33 | loss: 309.4710991CurrentTrain: epoch  2, batch    34 | loss: 344.5577121CurrentTrain: epoch  2, batch    35 | loss: 309.0314959CurrentTrain: epoch  2, batch    36 | loss: 233.1730942CurrentTrain: epoch  2, batch    37 | loss: 397.5263780CurrentTrain: epoch  2, batch    38 | loss: 343.5543883CurrentTrain: epoch  2, batch    39 | loss: 265.6018758CurrentTrain: epoch  2, batch    40 | loss: 394.2982152CurrentTrain: epoch  2, batch    41 | loss: 336.6696565CurrentTrain: epoch  2, batch    42 | loss: 414.7013568CurrentTrain: epoch  2, batch    43 | loss: 376.0897097CurrentTrain: epoch  2, batch    44 | loss: 364.1331255CurrentTrain: epoch  2, batch    45 | loss: 383.0633708CurrentTrain: epoch  2, batch    46 | loss: 405.8502845CurrentTrain: epoch  2, batch    47 | loss: 422.3498953CurrentTrain: epoch  2, batch    48 | loss: 319.0720364CurrentTrain: epoch  2, batch    49 | loss: 313.3670483CurrentTrain: epoch  2, batch    50 | loss: 305.2371467CurrentTrain: epoch  2, batch    51 | loss: 422.1042714CurrentTrain: epoch  2, batch    52 | loss: 443.5876486CurrentTrain: epoch  2, batch    53 | loss: 287.5594039CurrentTrain: epoch  2, batch    54 | loss: 393.0941164CurrentTrain: epoch  2, batch    55 | loss: 274.7688117CurrentTrain: epoch  2, batch    56 | loss: 270.1665507CurrentTrain: epoch  2, batch    57 | loss: 361.8032441CurrentTrain: epoch  2, batch    58 | loss: 276.5752598CurrentTrain: epoch  2, batch    59 | loss: 317.7973568CurrentTrain: epoch  2, batch    60 | loss: 375.0499656CurrentTrain: epoch  2, batch    61 | loss: 335.0444966CurrentTrain: epoch  2, batch    62 | loss: 361.4615715CurrentTrain: epoch  2, batch    63 | loss: 347.3460217CurrentTrain: epoch  2, batch    64 | loss: 359.1134560CurrentTrain: epoch  2, batch    65 | loss: 346.0051386CurrentTrain: epoch  2, batch    66 | loss: 348.8423582CurrentTrain: epoch  2, batch    67 | loss: 370.6467841CurrentTrain: epoch  2, batch    68 | loss: 395.0785138CurrentTrain: epoch  2, batch    69 | loss: 334.6602904CurrentTrain: epoch  2, batch    70 | loss: 390.5644450CurrentTrain: epoch  2, batch    71 | loss: 292.3043594CurrentTrain: epoch  2, batch    72 | loss: 363.7146229CurrentTrain: epoch  2, batch    73 | loss: 276.0756944CurrentTrain: epoch  2, batch    74 | loss: 345.0677748CurrentTrain: epoch  2, batch    75 | loss: 316.5157045CurrentTrain: epoch  2, batch    76 | loss: 282.6884208CurrentTrain: epoch  2, batch    77 | loss: 333.7133514CurrentTrain: epoch  2, batch    78 | loss: 318.0714090CurrentTrain: epoch  2, batch    79 | loss: 348.8165771CurrentTrain: epoch  2, batch    80 | loss: 425.1202368CurrentTrain: epoch  2, batch    81 | loss: 264.6604136CurrentTrain: epoch  2, batch    82 | loss: 379.8029458CurrentTrain: epoch  2, batch    83 | loss: 320.6425604CurrentTrain: epoch  2, batch    84 | loss: 358.2659534CurrentTrain: epoch  2, batch    85 | loss: 280.4907556CurrentTrain: epoch  2, batch    86 | loss: 294.5574170CurrentTrain: epoch  2, batch    87 | loss: 337.3720121CurrentTrain: epoch  2, batch    88 | loss: 414.1425350CurrentTrain: epoch  2, batch    89 | loss: 303.3867275CurrentTrain: epoch  2, batch    90 | loss: 276.7575561CurrentTrain: epoch  2, batch    91 | loss: 346.4512336CurrentTrain: epoch  2, batch    92 | loss: 362.3387588CurrentTrain: epoch  2, batch    93 | loss: 303.2161013CurrentTrain: epoch  2, batch    94 | loss: 297.4395457CurrentTrain: epoch  2, batch    95 | loss: 298.7074252CurrentTrain: epoch  3, batch     0 | loss: 441.6031697CurrentTrain: epoch  3, batch     1 | loss: 375.1034198CurrentTrain: epoch  3, batch     2 | loss: 378.9107154CurrentTrain: epoch  3, batch     3 | loss: 331.2320019CurrentTrain: epoch  3, batch     4 | loss: 378.2119794CurrentTrain: epoch  3, batch     5 | loss: 359.8780343CurrentTrain: epoch  3, batch     6 | loss: 333.9166286CurrentTrain: epoch  3, batch     7 | loss: 394.1284136CurrentTrain: epoch  3, batch     8 | loss: 290.9901411CurrentTrain: epoch  3, batch     9 | loss: 318.8186688CurrentTrain: epoch  3, batch    10 | loss: 262.3091661CurrentTrain: epoch  3, batch    11 | loss: 346.2873294CurrentTrain: epoch  3, batch    12 | loss: 392.6686195CurrentTrain: epoch  3, batch    13 | loss: 329.5126036CurrentTrain: epoch  3, batch    14 | loss: 305.3635472CurrentTrain: epoch  3, batch    15 | loss: 348.2982125CurrentTrain: epoch  3, batch    16 | loss: 376.7366794CurrentTrain: epoch  3, batch    17 | loss: 439.9728908CurrentTrain: epoch  3, batch    18 | loss: 284.7071645CurrentTrain: epoch  3, batch    19 | loss: 459.1246353CurrentTrain: epoch  3, batch    20 | loss: 420.6473972CurrentTrain: epoch  3, batch    21 | loss: 329.2729934CurrentTrain: epoch  3, batch    22 | loss: 336.1619068CurrentTrain: epoch  3, batch    23 | loss: 347.4132180CurrentTrain: epoch  3, batch    24 | loss: 377.7049343CurrentTrain: epoch  3, batch    25 | loss: 376.7557956CurrentTrain: epoch  3, batch    26 | loss: 289.4501289CurrentTrain: epoch  3, batch    27 | loss: 410.2502365CurrentTrain: epoch  3, batch    28 | loss: 360.9570780CurrentTrain: epoch  3, batch    29 | loss: 288.6941420CurrentTrain: epoch  3, batch    30 | loss: 329.4942700CurrentTrain: epoch  3, batch    31 | loss: 362.8544894CurrentTrain: epoch  3, batch    32 | loss: 327.6555846CurrentTrain: epoch  3, batch    33 | loss: 245.0784368CurrentTrain: epoch  3, batch    34 | loss: 426.6889350CurrentTrain: epoch  3, batch    35 | loss: 271.7418664CurrentTrain: epoch  3, batch    36 | loss: 359.0726468CurrentTrain: epoch  3, batch    37 | loss: 334.5935487CurrentTrain: epoch  3, batch    38 | loss: 271.1992334CurrentTrain: epoch  3, batch    39 | loss: 351.2348244CurrentTrain: epoch  3, batch    40 | loss: 344.0712439CurrentTrain: epoch  3, batch    41 | loss: 346.5817911CurrentTrain: epoch  3, batch    42 | loss: 357.8115896CurrentTrain: epoch  3, batch    43 | loss: 423.4530681CurrentTrain: epoch  3, batch    44 | loss: 302.5990351CurrentTrain: epoch  3, batch    45 | loss: 301.9472809CurrentTrain: epoch  3, batch    46 | loss: 275.3636963CurrentTrain: epoch  3, batch    47 | loss: 346.4215887CurrentTrain: epoch  3, batch    48 | loss: 312.4888519CurrentTrain: epoch  3, batch    49 | loss: 343.9856462CurrentTrain: epoch  3, batch    50 | loss: 318.8459875CurrentTrain: epoch  3, batch    51 | loss: 302.2131840CurrentTrain: epoch  3, batch    52 | loss: 380.8842673CurrentTrain: epoch  3, batch    53 | loss: 457.7251131CurrentTrain: epoch  3, batch    54 | loss: 262.3211254CurrentTrain: epoch  3, batch    55 | loss: 302.8115157CurrentTrain: epoch  3, batch    56 | loss: 411.9983692CurrentTrain: epoch  3, batch    57 | loss: 278.8340160CurrentTrain: epoch  3, batch    58 | loss: 343.3023759CurrentTrain: epoch  3, batch    59 | loss: 289.2373367CurrentTrain: epoch  3, batch    60 | loss: 292.1877157CurrentTrain: epoch  3, batch    61 | loss: 377.8232018CurrentTrain: epoch  3, batch    62 | loss: 318.3229648CurrentTrain: epoch  3, batch    63 | loss: 349.8678109CurrentTrain: epoch  3, batch    64 | loss: 392.7410979CurrentTrain: epoch  3, batch    65 | loss: 378.8942956CurrentTrain: epoch  3, batch    66 | loss: 391.9069406CurrentTrain: epoch  3, batch    67 | loss: 326.8893822CurrentTrain: epoch  3, batch    68 | loss: 373.9600781CurrentTrain: epoch  3, batch    69 | loss: 331.3240100CurrentTrain: epoch  3, batch    70 | loss: 254.7263947CurrentTrain: epoch  3, batch    71 | loss: 299.7335583CurrentTrain: epoch  3, batch    72 | loss: 342.0653483CurrentTrain: epoch  3, batch    73 | loss: 345.4331756CurrentTrain: epoch  3, batch    74 | loss: 396.0338487CurrentTrain: epoch  3, batch    75 | loss: 498.1442703CurrentTrain: epoch  3, batch    76 | loss: 413.4003966CurrentTrain: epoch  3, batch    77 | loss: 391.3293646CurrentTrain: epoch  3, batch    78 | loss: 359.5153540CurrentTrain: epoch  3, batch    79 | loss: 314.9141100CurrentTrain: epoch  3, batch    80 | loss: 260.9928327CurrentTrain: epoch  3, batch    81 | loss: 335.0482503CurrentTrain: epoch  3, batch    82 | loss: 342.1390628CurrentTrain: epoch  3, batch    83 | loss: 423.0993653CurrentTrain: epoch  3, batch    84 | loss: 537.9679185CurrentTrain: epoch  3, batch    85 | loss: 332.1088546CurrentTrain: epoch  3, batch    86 | loss: 380.1129133CurrentTrain: epoch  3, batch    87 | loss: 360.9499331CurrentTrain: epoch  3, batch    88 | loss: 391.8390870CurrentTrain: epoch  3, batch    89 | loss: 326.7117595CurrentTrain: epoch  3, batch    90 | loss: 350.7101145CurrentTrain: epoch  3, batch    91 | loss: 277.6423207CurrentTrain: epoch  3, batch    92 | loss: 422.9802635CurrentTrain: epoch  3, batch    93 | loss: 460.1545259CurrentTrain: epoch  3, batch    94 | loss: 266.3977563CurrentTrain: epoch  3, batch    95 | loss: 293.8262803CurrentTrain: epoch  4, batch     0 | loss: 318.0677279CurrentTrain: epoch  4, batch     1 | loss: 396.1135186CurrentTrain: epoch  4, batch     2 | loss: 315.1830860CurrentTrain: epoch  4, batch     3 | loss: 357.8494712CurrentTrain: epoch  4, batch     4 | loss: 279.7311686CurrentTrain: epoch  4, batch     5 | loss: 374.5313127CurrentTrain: epoch  4, batch     6 | loss: 423.1630450CurrentTrain: epoch  4, batch     7 | loss: 342.8336269CurrentTrain: epoch  4, batch     8 | loss: 340.8123487CurrentTrain: epoch  4, batch     9 | loss: 297.4132830CurrentTrain: epoch  4, batch    10 | loss: 419.3083439CurrentTrain: epoch  4, batch    11 | loss: 359.5592536CurrentTrain: epoch  4, batch    12 | loss: 439.7603751CurrentTrain: epoch  4, batch    13 | loss: 341.6220672CurrentTrain: epoch  4, batch    14 | loss: 346.5256751CurrentTrain: epoch  4, batch    15 | loss: 405.1158325CurrentTrain: epoch  4, batch    16 | loss: 745.3052807CurrentTrain: epoch  4, batch    17 | loss: 326.8094822CurrentTrain: epoch  4, batch    18 | loss: 343.1438679CurrentTrain: epoch  4, batch    19 | loss: 277.0883181CurrentTrain: epoch  4, batch    20 | loss: 265.9742766CurrentTrain: epoch  4, batch    21 | loss: 358.5927860CurrentTrain: epoch  4, batch    22 | loss: 273.1928710CurrentTrain: epoch  4, batch    23 | loss: 344.9440191CurrentTrain: epoch  4, batch    24 | loss: 361.3379144CurrentTrain: epoch  4, batch    25 | loss: 330.0058004CurrentTrain: epoch  4, batch    26 | loss: 374.4093419CurrentTrain: epoch  4, batch    27 | loss: 326.3756500CurrentTrain: epoch  4, batch    28 | loss: 340.9518582CurrentTrain: epoch  4, batch    29 | loss: 304.0087128CurrentTrain: epoch  4, batch    30 | loss: 343.0844702CurrentTrain: epoch  4, batch    31 | loss: 372.7579264CurrentTrain: epoch  4, batch    32 | loss: 260.3928032CurrentTrain: epoch  4, batch    33 | loss: 516.8200420CurrentTrain: epoch  4, batch    34 | loss: 329.0945595CurrentTrain: epoch  4, batch    35 | loss: 391.0700345CurrentTrain: epoch  4, batch    36 | loss: 261.7293390CurrentTrain: epoch  4, batch    37 | loss: 456.1795402CurrentTrain: epoch  4, batch    38 | loss: 345.9446854CurrentTrain: epoch  4, batch    39 | loss: 315.4438512CurrentTrain: epoch  4, batch    40 | loss: 540.1074805CurrentTrain: epoch  4, batch    41 | loss: 330.0640395CurrentTrain: epoch  4, batch    42 | loss: 325.0859052CurrentTrain: epoch  4, batch    43 | loss: 291.8572784CurrentTrain: epoch  4, batch    44 | loss: 298.3496588CurrentTrain: epoch  4, batch    45 | loss: 387.5509818CurrentTrain: epoch  4, batch    46 | loss: 274.3179878CurrentTrain: epoch  4, batch    47 | loss: 419.2642372CurrentTrain: epoch  4, batch    48 | loss: 263.2641281CurrentTrain: epoch  4, batch    49 | loss: 358.6928724CurrentTrain: epoch  4, batch    50 | loss: 357.6390245CurrentTrain: epoch  4, batch    51 | loss: 359.6176486CurrentTrain: epoch  4, batch    52 | loss: 327.9183671CurrentTrain: epoch  4, batch    53 | loss: 289.1201653CurrentTrain: epoch  4, batch    54 | loss: 437.4373444CurrentTrain: epoch  4, batch    55 | loss: 344.2842275CurrentTrain: epoch  4, batch    56 | loss: 360.8651810CurrentTrain: epoch  4, batch    57 | loss: 399.0391510CurrentTrain: epoch  4, batch    58 | loss: 328.1523827CurrentTrain: epoch  4, batch    59 | loss: 341.2465540CurrentTrain: epoch  4, batch    60 | loss: 288.3862131CurrentTrain: epoch  4, batch    61 | loss: 289.7298125CurrentTrain: epoch  4, batch    62 | loss: 331.3871007CurrentTrain: epoch  4, batch    63 | loss: 360.6516091CurrentTrain: epoch  4, batch    64 | loss: 375.0679197CurrentTrain: epoch  4, batch    65 | loss: 410.1690388CurrentTrain: epoch  4, batch    66 | loss: 343.1266485CurrentTrain: epoch  4, batch    67 | loss: 273.7841134CurrentTrain: epoch  4, batch    68 | loss: 356.6789740CurrentTrain: epoch  4, batch    69 | loss: 408.0587806CurrentTrain: epoch  4, batch    70 | loss: 260.6949325CurrentTrain: epoch  4, batch    71 | loss: 343.9646865CurrentTrain: epoch  4, batch    72 | loss: 301.2077783CurrentTrain: epoch  4, batch    73 | loss: 373.5261402CurrentTrain: epoch  4, batch    74 | loss: 358.4002832CurrentTrain: epoch  4, batch    75 | loss: 361.5121781CurrentTrain: epoch  4, batch    76 | loss: 356.9012010CurrentTrain: epoch  4, batch    77 | loss: 356.8341919CurrentTrain: epoch  4, batch    78 | loss: 343.0960857CurrentTrain: epoch  4, batch    79 | loss: 274.9137541CurrentTrain: epoch  4, batch    80 | loss: 457.0878945CurrentTrain: epoch  4, batch    81 | loss: 297.1053063CurrentTrain: epoch  4, batch    82 | loss: 315.5344060CurrentTrain: epoch  4, batch    83 | loss: 302.6893792CurrentTrain: epoch  4, batch    84 | loss: 409.8539779CurrentTrain: epoch  4, batch    85 | loss: 359.2604642CurrentTrain: epoch  4, batch    86 | loss: 300.2412226CurrentTrain: epoch  4, batch    87 | loss: 330.4852756CurrentTrain: epoch  4, batch    88 | loss: 391.8426863CurrentTrain: epoch  4, batch    89 | loss: 358.4115603CurrentTrain: epoch  4, batch    90 | loss: 285.5212450CurrentTrain: epoch  4, batch    91 | loss: 328.6705165CurrentTrain: epoch  4, batch    92 | loss: 329.2937619CurrentTrain: epoch  4, batch    93 | loss: 331.5023105CurrentTrain: epoch  4, batch    94 | loss: 392.6009011CurrentTrain: epoch  4, batch    95 | loss: 295.4021562CurrentTrain: epoch  5, batch     0 | loss: 310.4943359CurrentTrain: epoch  5, batch     1 | loss: 326.5661807CurrentTrain: epoch  5, batch     2 | loss: 325.6297778CurrentTrain: epoch  5, batch     3 | loss: 329.1230822CurrentTrain: epoch  5, batch     4 | loss: 360.9758767CurrentTrain: epoch  5, batch     5 | loss: 373.7610501CurrentTrain: epoch  5, batch     6 | loss: 286.7385881CurrentTrain: epoch  5, batch     7 | loss: 321.4505388CurrentTrain: epoch  5, batch     8 | loss: 344.5634078CurrentTrain: epoch  5, batch     9 | loss: 312.5436363CurrentTrain: epoch  5, batch    10 | loss: 356.9378741CurrentTrain: epoch  5, batch    11 | loss: 329.2621965CurrentTrain: epoch  5, batch    12 | loss: 358.9055899CurrentTrain: epoch  5, batch    13 | loss: 288.0683478CurrentTrain: epoch  5, batch    14 | loss: 273.3881989CurrentTrain: epoch  5, batch    15 | loss: 360.3077605CurrentTrain: epoch  5, batch    16 | loss: 286.1488067CurrentTrain: epoch  5, batch    17 | loss: 359.6959088CurrentTrain: epoch  5, batch    18 | loss: 325.5078962CurrentTrain: epoch  5, batch    19 | loss: 326.1983553CurrentTrain: epoch  5, batch    20 | loss: 265.9064277CurrentTrain: epoch  5, batch    21 | loss: 285.2464636CurrentTrain: epoch  5, batch    22 | loss: 374.4985562CurrentTrain: epoch  5, batch    23 | loss: 325.9210434CurrentTrain: epoch  5, batch    24 | loss: 330.2603793CurrentTrain: epoch  5, batch    25 | loss: 329.5188846CurrentTrain: epoch  5, batch    26 | loss: 437.4020960CurrentTrain: epoch  5, batch    27 | loss: 356.1460776CurrentTrain: epoch  5, batch    28 | loss: 340.2566379CurrentTrain: epoch  5, batch    29 | loss: 363.1575075CurrentTrain: epoch  5, batch    30 | loss: 302.0497593CurrentTrain: epoch  5, batch    31 | loss: 327.1015298CurrentTrain: epoch  5, batch    32 | loss: 284.7277320CurrentTrain: epoch  5, batch    33 | loss: 356.7331728CurrentTrain: epoch  5, batch    34 | loss: 378.2873047CurrentTrain: epoch  5, batch    35 | loss: 408.4325357CurrentTrain: epoch  5, batch    36 | loss: 373.5152022CurrentTrain: epoch  5, batch    37 | loss: 393.6173346CurrentTrain: epoch  5, batch    38 | loss: 341.9904816CurrentTrain: epoch  5, batch    39 | loss: 326.2876049CurrentTrain: epoch  5, batch    40 | loss: 357.0407080CurrentTrain: epoch  5, batch    41 | loss: 325.9057397CurrentTrain: epoch  5, batch    42 | loss: 310.7992080CurrentTrain: epoch  5, batch    43 | loss: 271.3104704CurrentTrain: epoch  5, batch    44 | loss: 456.0437301CurrentTrain: epoch  5, batch    45 | loss: 324.7325725CurrentTrain: epoch  5, batch    46 | loss: 341.6728196CurrentTrain: epoch  5, batch    47 | loss: 373.2143712CurrentTrain: epoch  5, batch    48 | loss: 270.7896131CurrentTrain: epoch  5, batch    49 | loss: 259.0948727CurrentTrain: epoch  5, batch    50 | loss: 456.1286310CurrentTrain: epoch  5, batch    51 | loss: 332.0469522CurrentTrain: epoch  5, batch    52 | loss: 345.1510079CurrentTrain: epoch  5, batch    53 | loss: 291.8537351CurrentTrain: epoch  5, batch    54 | loss: 358.6972567CurrentTrain: epoch  5, batch    55 | loss: 457.0745750CurrentTrain: epoch  5, batch    56 | loss: 373.6435684CurrentTrain: epoch  5, batch    57 | loss: 290.9314650CurrentTrain: epoch  5, batch    58 | loss: 395.6750488CurrentTrain: epoch  5, batch    59 | loss: 328.7926008CurrentTrain: epoch  5, batch    60 | loss: 419.6953600CurrentTrain: epoch  5, batch    61 | loss: 373.5708809CurrentTrain: epoch  5, batch    62 | loss: 372.8863783CurrentTrain: epoch  5, batch    63 | loss: 373.2197190CurrentTrain: epoch  5, batch    64 | loss: 438.0071321CurrentTrain: epoch  5, batch    65 | loss: 344.7009510CurrentTrain: epoch  5, batch    66 | loss: 359.3467525CurrentTrain: epoch  5, batch    67 | loss: 279.1155183CurrentTrain: epoch  5, batch    68 | loss: 408.2247575CurrentTrain: epoch  5, batch    69 | loss: 282.0301307CurrentTrain: epoch  5, batch    70 | loss: 345.5438131CurrentTrain: epoch  5, batch    71 | loss: 441.8056741CurrentTrain: epoch  5, batch    72 | loss: 359.3375416CurrentTrain: epoch  5, batch    73 | loss: 297.6487501CurrentTrain: epoch  5, batch    74 | loss: 311.5274963CurrentTrain: epoch  5, batch    75 | loss: 439.4563540CurrentTrain: epoch  5, batch    76 | loss: 265.8162283CurrentTrain: epoch  5, batch    77 | loss: 312.9290322CurrentTrain: epoch  5, batch    78 | loss: 304.0302698CurrentTrain: epoch  5, batch    79 | loss: 340.7689339CurrentTrain: epoch  5, batch    80 | loss: 373.3594933CurrentTrain: epoch  5, batch    81 | loss: 314.6663467CurrentTrain: epoch  5, batch    82 | loss: 420.1058775CurrentTrain: epoch  5, batch    83 | loss: 291.5002969CurrentTrain: epoch  5, batch    84 | loss: 258.8748811CurrentTrain: epoch  5, batch    85 | loss: 407.7050730CurrentTrain: epoch  5, batch    86 | loss: 252.2079029CurrentTrain: epoch  5, batch    87 | loss: 393.7294362CurrentTrain: epoch  5, batch    88 | loss: 348.9116444CurrentTrain: epoch  5, batch    89 | loss: 403.2932532CurrentTrain: epoch  5, batch    90 | loss: 440.0174456CurrentTrain: epoch  5, batch    91 | loss: 329.7088651CurrentTrain: epoch  5, batch    92 | loss: 314.8614283CurrentTrain: epoch  5, batch    93 | loss: 347.2970449CurrentTrain: epoch  5, batch    94 | loss: 375.7386074CurrentTrain: epoch  5, batch    95 | loss: 306.4756075CurrentTrain: epoch  6, batch     0 | loss: 357.2623507CurrentTrain: epoch  6, batch     1 | loss: 313.6203129CurrentTrain: epoch  6, batch     2 | loss: 331.5824275CurrentTrain: epoch  6, batch     3 | loss: 312.0144089CurrentTrain: epoch  6, batch     4 | loss: 387.5944410CurrentTrain: epoch  6, batch     5 | loss: 359.1607932CurrentTrain: epoch  6, batch     6 | loss: 373.3329313CurrentTrain: epoch  6, batch     7 | loss: 331.6707306CurrentTrain: epoch  6, batch     8 | loss: 232.9662419CurrentTrain: epoch  6, batch     9 | loss: 332.8695557CurrentTrain: epoch  6, batch    10 | loss: 261.2197598CurrentTrain: epoch  6, batch    11 | loss: 373.4227458CurrentTrain: epoch  6, batch    12 | loss: 326.3096624CurrentTrain: epoch  6, batch    13 | loss: 327.8364054CurrentTrain: epoch  6, batch    14 | loss: 278.7838229CurrentTrain: epoch  6, batch    15 | loss: 374.1081933CurrentTrain: epoch  6, batch    16 | loss: 271.7071801CurrentTrain: epoch  6, batch    17 | loss: 346.3311948CurrentTrain: epoch  6, batch    18 | loss: 284.2690814CurrentTrain: epoch  6, batch    19 | loss: 357.9917332CurrentTrain: epoch  6, batch    20 | loss: 423.4182517CurrentTrain: epoch  6, batch    21 | loss: 420.6519581CurrentTrain: epoch  6, batch    22 | loss: 298.3018269CurrentTrain: epoch  6, batch    23 | loss: 373.8260778CurrentTrain: epoch  6, batch    24 | loss: 407.4413074CurrentTrain: epoch  6, batch    25 | loss: 455.3249298CurrentTrain: epoch  6, batch    26 | loss: 373.5387775CurrentTrain: epoch  6, batch    27 | loss: 374.8133956CurrentTrain: epoch  6, batch    28 | loss: 374.6186838CurrentTrain: epoch  6, batch    29 | loss: 340.4842341CurrentTrain: epoch  6, batch    30 | loss: 356.6948018CurrentTrain: epoch  6, batch    31 | loss: 299.6345432CurrentTrain: epoch  6, batch    32 | loss: 301.4509856CurrentTrain: epoch  6, batch    33 | loss: 285.6007740CurrentTrain: epoch  6, batch    34 | loss: 274.8913344CurrentTrain: epoch  6, batch    35 | loss: 305.6634127CurrentTrain: epoch  6, batch    36 | loss: 357.0844851CurrentTrain: epoch  6, batch    37 | loss: 390.9564372CurrentTrain: epoch  6, batch    38 | loss: 372.9834804CurrentTrain: epoch  6, batch    39 | loss: 356.9914650CurrentTrain: epoch  6, batch    40 | loss: 419.4300252CurrentTrain: epoch  6, batch    41 | loss: 391.9096358CurrentTrain: epoch  6, batch    42 | loss: 340.3930055CurrentTrain: epoch  6, batch    43 | loss: 357.1507542CurrentTrain: epoch  6, batch    44 | loss: 310.4199056CurrentTrain: epoch  6, batch    45 | loss: 357.5948199CurrentTrain: epoch  6, batch    46 | loss: 314.0000005CurrentTrain: epoch  6, batch    47 | loss: 391.1538128CurrentTrain: epoch  6, batch    48 | loss: 390.7578875CurrentTrain: epoch  6, batch    49 | loss: 262.0995063CurrentTrain: epoch  6, batch    50 | loss: 285.1118883CurrentTrain: epoch  6, batch    51 | loss: 309.2036374CurrentTrain: epoch  6, batch    52 | loss: 299.1275798CurrentTrain: epoch  6, batch    53 | loss: 300.5642399CurrentTrain: epoch  6, batch    54 | loss: 373.3529317CurrentTrain: epoch  6, batch    55 | loss: 390.1037154CurrentTrain: epoch  6, batch    56 | loss: 360.1867600CurrentTrain: epoch  6, batch    57 | loss: 300.5012655CurrentTrain: epoch  6, batch    58 | loss: 341.0139936CurrentTrain: epoch  6, batch    59 | loss: 328.6025967CurrentTrain: epoch  6, batch    60 | loss: 373.1937782CurrentTrain: epoch  6, batch    61 | loss: 393.2602698CurrentTrain: epoch  6, batch    62 | loss: 419.1406260CurrentTrain: epoch  6, batch    63 | loss: 374.3907446CurrentTrain: epoch  6, batch    64 | loss: 389.9129824CurrentTrain: epoch  6, batch    65 | loss: 419.0318884CurrentTrain: epoch  6, batch    66 | loss: 356.9039160CurrentTrain: epoch  6, batch    67 | loss: 307.5472938CurrentTrain: epoch  6, batch    68 | loss: 344.3622797CurrentTrain: epoch  6, batch    69 | loss: 259.1935608CurrentTrain: epoch  6, batch    70 | loss: 373.2322000CurrentTrain: epoch  6, batch    71 | loss: 335.2529222CurrentTrain: epoch  6, batch    72 | loss: 315.8625466CurrentTrain: epoch  6, batch    73 | loss: 265.2166331CurrentTrain: epoch  6, batch    74 | loss: 340.2043689CurrentTrain: epoch  6, batch    75 | loss: 373.2644698CurrentTrain: epoch  6, batch    76 | loss: 333.9896349CurrentTrain: epoch  6, batch    77 | loss: 306.5799811CurrentTrain: epoch  6, batch    78 | loss: 378.5336505CurrentTrain: epoch  6, batch    79 | loss: 392.5071744CurrentTrain: epoch  6, batch    80 | loss: 237.8073885CurrentTrain: epoch  6, batch    81 | loss: 313.1400856CurrentTrain: epoch  6, batch    82 | loss: 298.1410464CurrentTrain: epoch  6, batch    83 | loss: 357.4604421CurrentTrain: epoch  6, batch    84 | loss: 438.8156286CurrentTrain: epoch  6, batch    85 | loss: 356.2295503CurrentTrain: epoch  6, batch    86 | loss: 455.1885602CurrentTrain: epoch  6, batch    87 | loss: 266.8964342CurrentTrain: epoch  6, batch    88 | loss: 355.9418987CurrentTrain: epoch  6, batch    89 | loss: 419.3474660CurrentTrain: epoch  6, batch    90 | loss: 324.3128653CurrentTrain: epoch  6, batch    91 | loss: 439.3038432CurrentTrain: epoch  6, batch    92 | loss: 298.7568914CurrentTrain: epoch  6, batch    93 | loss: 342.2001002CurrentTrain: epoch  6, batch    94 | loss: 329.2123256CurrentTrain: epoch  6, batch    95 | loss: 290.3335785CurrentTrain: epoch  7, batch     0 | loss: 436.9512686CurrentTrain: epoch  7, batch     1 | loss: 290.8317513CurrentTrain: epoch  7, batch     2 | loss: 339.8824366CurrentTrain: epoch  7, batch     3 | loss: 358.1049939CurrentTrain: epoch  7, batch     4 | loss: 360.3131889CurrentTrain: epoch  7, batch     5 | loss: 296.8905459CurrentTrain: epoch  7, batch     6 | loss: 313.3284239CurrentTrain: epoch  7, batch     7 | loss: 389.8840327CurrentTrain: epoch  7, batch     8 | loss: 329.5161889CurrentTrain: epoch  7, batch     9 | loss: 311.7012394CurrentTrain: epoch  7, batch    10 | loss: 357.9299486CurrentTrain: epoch  7, batch    11 | loss: 420.1672001CurrentTrain: epoch  7, batch    12 | loss: 298.8641190CurrentTrain: epoch  7, batch    13 | loss: 271.6798609CurrentTrain: epoch  7, batch    14 | loss: 251.4760518CurrentTrain: epoch  7, batch    15 | loss: 270.2733552CurrentTrain: epoch  7, batch    16 | loss: 391.1142451CurrentTrain: epoch  7, batch    17 | loss: 310.2413137CurrentTrain: epoch  7, batch    18 | loss: 271.1059916CurrentTrain: epoch  7, batch    19 | loss: 325.1063957CurrentTrain: epoch  7, batch    20 | loss: 419.2164306CurrentTrain: epoch  7, batch    21 | loss: 455.3001597CurrentTrain: epoch  7, batch    22 | loss: 341.4681047CurrentTrain: epoch  7, batch    23 | loss: 357.6808183CurrentTrain: epoch  7, batch    24 | loss: 302.6595701CurrentTrain: epoch  7, batch    25 | loss: 372.9144976CurrentTrain: epoch  7, batch    26 | loss: 358.8093063CurrentTrain: epoch  7, batch    27 | loss: 313.6744443CurrentTrain: epoch  7, batch    28 | loss: 342.2826095CurrentTrain: epoch  7, batch    29 | loss: 407.8262399CurrentTrain: epoch  7, batch    30 | loss: 356.1043237CurrentTrain: epoch  7, batch    31 | loss: 401.7491113CurrentTrain: epoch  7, batch    32 | loss: 373.2387086CurrentTrain: epoch  7, batch    33 | loss: 276.1209570CurrentTrain: epoch  7, batch    34 | loss: 263.8158318CurrentTrain: epoch  7, batch    35 | loss: 295.9428534CurrentTrain: epoch  7, batch    36 | loss: 438.6681470CurrentTrain: epoch  7, batch    37 | loss: 372.9996174CurrentTrain: epoch  7, batch    38 | loss: 341.2871868CurrentTrain: epoch  7, batch    39 | loss: 372.7250575CurrentTrain: epoch  7, batch    40 | loss: 346.6730146CurrentTrain: epoch  7, batch    41 | loss: 312.5454107CurrentTrain: epoch  7, batch    42 | loss: 298.8024114CurrentTrain: epoch  7, batch    43 | loss: 299.0441967CurrentTrain: epoch  7, batch    44 | loss: 389.8489706CurrentTrain: epoch  7, batch    45 | loss: 455.0942201CurrentTrain: epoch  7, batch    46 | loss: 374.1482043CurrentTrain: epoch  7, batch    47 | loss: 324.9252343CurrentTrain: epoch  7, batch    48 | loss: 340.4203438CurrentTrain: epoch  7, batch    49 | loss: 235.5080042CurrentTrain: epoch  7, batch    50 | loss: 356.2374617CurrentTrain: epoch  7, batch    51 | loss: 356.0661293CurrentTrain: epoch  7, batch    52 | loss: 297.0914254CurrentTrain: epoch  7, batch    53 | loss: 387.1525923CurrentTrain: epoch  7, batch    54 | loss: 355.5387378CurrentTrain: epoch  7, batch    55 | loss: 407.8195476CurrentTrain: epoch  7, batch    56 | loss: 339.9919454CurrentTrain: epoch  7, batch    57 | loss: 323.9396823CurrentTrain: epoch  7, batch    58 | loss: 270.0335573CurrentTrain: epoch  7, batch    59 | loss: 340.3372615CurrentTrain: epoch  7, batch    60 | loss: 391.1299541CurrentTrain: epoch  7, batch    61 | loss: 420.0609213CurrentTrain: epoch  7, batch    62 | loss: 298.2193752CurrentTrain: epoch  7, batch    63 | loss: 261.9195359CurrentTrain: epoch  7, batch    64 | loss: 456.3113969CurrentTrain: epoch  7, batch    65 | loss: 297.9207840CurrentTrain: epoch  7, batch    66 | loss: 391.2494487CurrentTrain: epoch  7, batch    67 | loss: 311.8853472CurrentTrain: epoch  7, batch    68 | loss: 326.1015814CurrentTrain: epoch  7, batch    69 | loss: 355.9083963CurrentTrain: epoch  7, batch    70 | loss: 258.3073250CurrentTrain: epoch  7, batch    71 | loss: 304.8957832CurrentTrain: epoch  7, batch    72 | loss: 393.4301942CurrentTrain: epoch  7, batch    73 | loss: 284.9289567CurrentTrain: epoch  7, batch    74 | loss: 372.4186542CurrentTrain: epoch  7, batch    75 | loss: 355.9499076CurrentTrain: epoch  7, batch    76 | loss: 392.0693022CurrentTrain: epoch  7, batch    77 | loss: 389.7517029CurrentTrain: epoch  7, batch    78 | loss: 372.7557373CurrentTrain: epoch  7, batch    79 | loss: 339.9132768CurrentTrain: epoch  7, batch    80 | loss: 328.6198628CurrentTrain: epoch  7, batch    81 | loss: 309.7623619CurrentTrain: epoch  7, batch    82 | loss: 372.6122328CurrentTrain: epoch  7, batch    83 | loss: 408.5583342CurrentTrain: epoch  7, batch    84 | loss: 340.8041743CurrentTrain: epoch  7, batch    85 | loss: 436.7239609CurrentTrain: epoch  7, batch    86 | loss: 374.7192998CurrentTrain: epoch  7, batch    87 | loss: 314.2713935CurrentTrain: epoch  7, batch    88 | loss: 325.0187017CurrentTrain: epoch  7, batch    89 | loss: 373.4897109CurrentTrain: epoch  7, batch    90 | loss: 340.6974700CurrentTrain: epoch  7, batch    91 | loss: 373.2339196CurrentTrain: epoch  7, batch    92 | loss: 264.4248117CurrentTrain: epoch  7, batch    93 | loss: 326.6832223CurrentTrain: epoch  7, batch    94 | loss: 297.8000303CurrentTrain: epoch  7, batch    95 | loss: 306.0615008CurrentTrain: epoch  8, batch     0 | loss: 283.3398218CurrentTrain: epoch  8, batch     1 | loss: 339.6601467CurrentTrain: epoch  8, batch     2 | loss: 270.1398430CurrentTrain: epoch  8, batch     3 | loss: 437.4452254CurrentTrain: epoch  8, batch     4 | loss: 340.0184439CurrentTrain: epoch  8, batch     5 | loss: 389.5364093CurrentTrain: epoch  8, batch     6 | loss: 290.3253123CurrentTrain: epoch  8, batch     7 | loss: 355.6683156CurrentTrain: epoch  8, batch     8 | loss: 355.6353491CurrentTrain: epoch  8, batch     9 | loss: 328.4029213CurrentTrain: epoch  8, batch    10 | loss: 286.9083820CurrentTrain: epoch  8, batch    11 | loss: 283.8980940CurrentTrain: epoch  8, batch    12 | loss: 328.2114325CurrentTrain: epoch  8, batch    13 | loss: 374.8502871CurrentTrain: epoch  8, batch    14 | loss: 358.5839153CurrentTrain: epoch  8, batch    15 | loss: 391.5713035CurrentTrain: epoch  8, batch    16 | loss: 358.0492393CurrentTrain: epoch  8, batch    17 | loss: 308.8156024CurrentTrain: epoch  8, batch    18 | loss: 391.0640188CurrentTrain: epoch  8, batch    19 | loss: 373.9260661CurrentTrain: epoch  8, batch    20 | loss: 243.2284776CurrentTrain: epoch  8, batch    21 | loss: 324.5146950CurrentTrain: epoch  8, batch    22 | loss: 340.2293340CurrentTrain: epoch  8, batch    23 | loss: 407.1337229CurrentTrain: epoch  8, batch    24 | loss: 372.9339106CurrentTrain: epoch  8, batch    25 | loss: 264.8857394CurrentTrain: epoch  8, batch    26 | loss: 355.4732146CurrentTrain: epoch  8, batch    27 | loss: 339.6929257CurrentTrain: epoch  8, batch    28 | loss: 345.0070299CurrentTrain: epoch  8, batch    29 | loss: 258.1487188CurrentTrain: epoch  8, batch    30 | loss: 355.8128283CurrentTrain: epoch  8, batch    31 | loss: 275.9840968CurrentTrain: epoch  8, batch    32 | loss: 280.2343853CurrentTrain: epoch  8, batch    33 | loss: 357.6247686CurrentTrain: epoch  8, batch    34 | loss: 389.5998613CurrentTrain: epoch  8, batch    35 | loss: 286.4227722CurrentTrain: epoch  8, batch    36 | loss: 356.0679324CurrentTrain: epoch  8, batch    37 | loss: 276.0716946CurrentTrain: epoch  8, batch    38 | loss: 436.9283782CurrentTrain: epoch  8, batch    39 | loss: 269.9908382CurrentTrain: epoch  8, batch    40 | loss: 328.2721761CurrentTrain: epoch  8, batch    41 | loss: 339.9900204CurrentTrain: epoch  8, batch    42 | loss: 341.2744955CurrentTrain: epoch  8, batch    43 | loss: 340.9897400CurrentTrain: epoch  8, batch    44 | loss: 356.4855736CurrentTrain: epoch  8, batch    45 | loss: 298.0582846CurrentTrain: epoch  8, batch    46 | loss: 342.4109680CurrentTrain: epoch  8, batch    47 | loss: 437.3847895CurrentTrain: epoch  8, batch    48 | loss: 309.3874977CurrentTrain: epoch  8, batch    49 | loss: 455.4813044CurrentTrain: epoch  8, batch    50 | loss: 294.2939019CurrentTrain: epoch  8, batch    51 | loss: 455.0737361CurrentTrain: epoch  8, batch    52 | loss: 269.8255775CurrentTrain: epoch  8, batch    53 | loss: 372.9128934CurrentTrain: epoch  8, batch    54 | loss: 344.3192390CurrentTrain: epoch  8, batch    55 | loss: 231.6657681CurrentTrain: epoch  8, batch    56 | loss: 355.3982784CurrentTrain: epoch  8, batch    57 | loss: 333.7011801CurrentTrain: epoch  8, batch    58 | loss: 256.7829931CurrentTrain: epoch  8, batch    59 | loss: 328.9205816CurrentTrain: epoch  8, batch    60 | loss: 339.3869607CurrentTrain: epoch  8, batch    61 | loss: 455.1500263CurrentTrain: epoch  8, batch    62 | loss: 407.2461616CurrentTrain: epoch  8, batch    63 | loss: 344.2572653CurrentTrain: epoch  8, batch    64 | loss: 389.6523284CurrentTrain: epoch  8, batch    65 | loss: 373.3811526CurrentTrain: epoch  8, batch    66 | loss: 357.6872433CurrentTrain: epoch  8, batch    67 | loss: 406.2580914CurrentTrain: epoch  8, batch    68 | loss: 320.2911744CurrentTrain: epoch  8, batch    69 | loss: 407.6414321CurrentTrain: epoch  8, batch    70 | loss: 357.2544034CurrentTrain: epoch  8, batch    71 | loss: 324.2292662CurrentTrain: epoch  8, batch    72 | loss: 313.8092724CurrentTrain: epoch  8, batch    73 | loss: 284.3987667CurrentTrain: epoch  8, batch    74 | loss: 345.7680801CurrentTrain: epoch  8, batch    75 | loss: 372.2209672CurrentTrain: epoch  8, batch    76 | loss: 292.2573560CurrentTrain: epoch  8, batch    77 | loss: 356.0235780CurrentTrain: epoch  8, batch    78 | loss: 344.6320942CurrentTrain: epoch  8, batch    79 | loss: 340.8637408CurrentTrain: epoch  8, batch    80 | loss: 407.2364188CurrentTrain: epoch  8, batch    81 | loss: 344.1468606CurrentTrain: epoch  8, batch    82 | loss: 375.3663596CurrentTrain: epoch  8, batch    83 | loss: 358.4077116CurrentTrain: epoch  8, batch    84 | loss: 305.5987283CurrentTrain: epoch  8, batch    85 | loss: 288.1768590CurrentTrain: epoch  8, batch    86 | loss: 284.5148966CurrentTrain: epoch  8, batch    87 | loss: 339.8008673CurrentTrain: epoch  8, batch    88 | loss: 339.3559491CurrentTrain: epoch  8, batch    89 | loss: 309.7856009CurrentTrain: epoch  8, batch    90 | loss: 356.4783584CurrentTrain: epoch  8, batch    91 | loss: 438.2393312CurrentTrain: epoch  8, batch    92 | loss: 276.0944505CurrentTrain: epoch  8, batch    93 | loss: 268.9976604CurrentTrain: epoch  8, batch    94 | loss: 374.2632890CurrentTrain: epoch  8, batch    95 | loss: 337.1101729CurrentTrain: epoch  9, batch     0 | loss: 373.6348520CurrentTrain: epoch  9, batch     1 | loss: 455.0440861CurrentTrain: epoch  9, batch     2 | loss: 324.0866119CurrentTrain: epoch  9, batch     3 | loss: 312.5987306CurrentTrain: epoch  9, batch     4 | loss: 357.3829210CurrentTrain: epoch  9, batch     5 | loss: 390.4229652CurrentTrain: epoch  9, batch     6 | loss: 339.3672337CurrentTrain: epoch  9, batch     7 | loss: 356.8779063CurrentTrain: epoch  9, batch     8 | loss: 372.4907098CurrentTrain: epoch  9, batch     9 | loss: 408.7801539CurrentTrain: epoch  9, batch    10 | loss: 496.4410353CurrentTrain: epoch  9, batch    11 | loss: 390.1929707CurrentTrain: epoch  9, batch    12 | loss: 298.4125757CurrentTrain: epoch  9, batch    13 | loss: 419.0199017CurrentTrain: epoch  9, batch    14 | loss: 323.7531544CurrentTrain: epoch  9, batch    15 | loss: 312.6941342CurrentTrain: epoch  9, batch    16 | loss: 328.8308030CurrentTrain: epoch  9, batch    17 | loss: 284.0814090CurrentTrain: epoch  9, batch    18 | loss: 298.5371399CurrentTrain: epoch  9, batch    19 | loss: 356.8097816CurrentTrain: epoch  9, batch    20 | loss: 390.1524927CurrentTrain: epoch  9, batch    21 | loss: 341.1866299CurrentTrain: epoch  9, batch    22 | loss: 356.7268986CurrentTrain: epoch  9, batch    23 | loss: 284.1758863CurrentTrain: epoch  9, batch    24 | loss: 270.6603951CurrentTrain: epoch  9, batch    25 | loss: 372.5803345CurrentTrain: epoch  9, batch    26 | loss: 304.8857663CurrentTrain: epoch  9, batch    27 | loss: 308.6383417CurrentTrain: epoch  9, batch    28 | loss: 295.9431449CurrentTrain: epoch  9, batch    29 | loss: 455.2892997CurrentTrain: epoch  9, batch    30 | loss: 324.4439553CurrentTrain: epoch  9, batch    31 | loss: 356.5630526CurrentTrain: epoch  9, batch    32 | loss: 357.1048359CurrentTrain: epoch  9, batch    33 | loss: 373.1080831CurrentTrain: epoch  9, batch    34 | loss: 297.6385551CurrentTrain: epoch  9, batch    35 | loss: 357.2158925CurrentTrain: epoch  9, batch    36 | loss: 340.1145152CurrentTrain: epoch  9, batch    37 | loss: 373.1099409CurrentTrain: epoch  9, batch    38 | loss: 372.5901342CurrentTrain: epoch  9, batch    39 | loss: 373.0676866CurrentTrain: epoch  9, batch    40 | loss: 309.2222583CurrentTrain: epoch  9, batch    41 | loss: 301.1616309CurrentTrain: epoch  9, batch    42 | loss: 438.5955039CurrentTrain: epoch  9, batch    43 | loss: 300.6402454CurrentTrain: epoch  9, batch    44 | loss: 373.4181599CurrentTrain: epoch  9, batch    45 | loss: 372.5160259CurrentTrain: epoch  9, batch    46 | loss: 339.1706054CurrentTrain: epoch  9, batch    47 | loss: 286.5058477CurrentTrain: epoch  9, batch    48 | loss: 212.7835409CurrentTrain: epoch  9, batch    49 | loss: 271.0234333CurrentTrain: epoch  9, batch    50 | loss: 407.0963110CurrentTrain: epoch  9, batch    51 | loss: 339.3626629CurrentTrain: epoch  9, batch    52 | loss: 325.4153007CurrentTrain: epoch  9, batch    53 | loss: 391.0234527CurrentTrain: epoch  9, batch    54 | loss: 264.3959642CurrentTrain: epoch  9, batch    55 | loss: 324.0810917CurrentTrain: epoch  9, batch    56 | loss: 373.1661328CurrentTrain: epoch  9, batch    57 | loss: 390.0683043CurrentTrain: epoch  9, batch    58 | loss: 455.2212706CurrentTrain: epoch  9, batch    59 | loss: 339.3894006CurrentTrain: epoch  9, batch    60 | loss: 356.7423920CurrentTrain: epoch  9, batch    61 | loss: 390.3770509CurrentTrain: epoch  9, batch    62 | loss: 455.0697734CurrentTrain: epoch  9, batch    63 | loss: 436.5885751CurrentTrain: epoch  9, batch    64 | loss: 328.0367550CurrentTrain: epoch  9, batch    65 | loss: 373.1713007CurrentTrain: epoch  9, batch    66 | loss: 270.1367833CurrentTrain: epoch  9, batch    67 | loss: 461.9623595CurrentTrain: epoch  9, batch    68 | loss: 309.3835794CurrentTrain: epoch  9, batch    69 | loss: 271.2747417CurrentTrain: epoch  9, batch    70 | loss: 340.7778879CurrentTrain: epoch  9, batch    71 | loss: 344.5566774CurrentTrain: epoch  9, batch    72 | loss: 372.5776283CurrentTrain: epoch  9, batch    73 | loss: 389.4788958CurrentTrain: epoch  9, batch    74 | loss: 302.1522292CurrentTrain: epoch  9, batch    75 | loss: 355.7512537CurrentTrain: epoch  9, batch    76 | loss: 272.5130727CurrentTrain: epoch  9, batch    77 | loss: 407.3072896CurrentTrain: epoch  9, batch    78 | loss: 355.9183264CurrentTrain: epoch  9, batch    79 | loss: 298.2156831CurrentTrain: epoch  9, batch    80 | loss: 373.0199325CurrentTrain: epoch  9, batch    81 | loss: 275.7522265CurrentTrain: epoch  9, batch    82 | loss: 355.5946438CurrentTrain: epoch  9, batch    83 | loss: 335.7295465CurrentTrain: epoch  9, batch    84 | loss: 373.0392300CurrentTrain: epoch  9, batch    85 | loss: 317.7771278CurrentTrain: epoch  9, batch    86 | loss: 277.2350612CurrentTrain: epoch  9, batch    87 | loss: 280.9919364CurrentTrain: epoch  9, batch    88 | loss: 273.3216411CurrentTrain: epoch  9, batch    89 | loss: 282.9981932CurrentTrain: epoch  9, batch    90 | loss: 313.2575659CurrentTrain: epoch  9, batch    91 | loss: 339.5814720CurrentTrain: epoch  9, batch    92 | loss: 283.9009780CurrentTrain: epoch  9, batch    93 | loss: 323.8580303CurrentTrain: epoch  9, batch    94 | loss: 297.6102949CurrentTrain: epoch  9, batch    95 | loss: 304.8767846

F1 score per class: {32: 0.6470588235294118, 6: 0.8095238095238095, 19: 0.43478260869565216, 24: 0.7741935483870968, 26: 0.9130434782608695, 29: 0.9090909090909091}
Micro-average F1 score: 0.8051668460710442
Weighted-average F1 score: 0.8148001366527216
F1 score per class: {32: 0.7722772277227723, 6: 0.8961748633879781, 19: 0.6666666666666666, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.91}
Micro-average F1 score: 0.8603238866396761
Weighted-average F1 score: 0.862108092692313
F1 score per class: {32: 0.7661691542288557, 6: 0.8961748633879781, 19: 0.6896551724137931, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.9154228855721394}
Micro-average F1 score: 0.8611955420466059
Weighted-average F1 score: 0.8632100836974765

F1 score per class: {32: 0.6470588235294118, 6: 0.8095238095238095, 19: 0.43478260869565216, 24: 0.7741935483870968, 26: 0.9130434782608695, 29: 0.9090909090909091}
Micro-average F1 score: 0.8051668460710442
Weighted-average F1 score: 0.8148001366527216
F1 score per class: {32: 0.7722772277227723, 6: 0.8961748633879781, 19: 0.6666666666666666, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.91}
Micro-average F1 score: 0.8603238866396761
Weighted-average F1 score: 0.862108092692313
F1 score per class: {32: 0.7661691542288557, 6: 0.8961748633879781, 19: 0.6896551724137931, 24: 0.7752808988764045, 26: 0.9743589743589743, 29: 0.9154228855721394}
Micro-average F1 score: 0.8611955420466059
Weighted-average F1 score: 0.8632100836974765
cur_acc:  ['0.8052']
his_acc:  ['0.8052']
cur_acc des:  ['0.8603']
his_acc des:  ['0.8603']
cur_acc rrf:  ['0.8612']
his_acc rrf:  ['0.8612']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 358.7816154CurrentTrain: epoch  0, batch     1 | loss: 385.7222764CurrentTrain: epoch  0, batch     2 | loss: 346.7430509CurrentTrain: epoch  0, batch     3 | loss: 420.0988128CurrentTrain: epoch  0, batch     4 | loss: 257.5165723CurrentTrain: epoch  1, batch     0 | loss: 323.8885966CurrentTrain: epoch  1, batch     1 | loss: 340.0016352CurrentTrain: epoch  1, batch     2 | loss: 544.8443047CurrentTrain: epoch  1, batch     3 | loss: 365.1735634CurrentTrain: epoch  1, batch     4 | loss: 214.7729238CurrentTrain: epoch  2, batch     0 | loss: 323.4549509CurrentTrain: epoch  2, batch     1 | loss: 397.1022990CurrentTrain: epoch  2, batch     2 | loss: 349.9045052CurrentTrain: epoch  2, batch     3 | loss: 366.6581157CurrentTrain: epoch  2, batch     4 | loss: 343.6302154CurrentTrain: epoch  3, batch     0 | loss: 308.3180600CurrentTrain: epoch  3, batch     1 | loss: 394.1679388CurrentTrain: epoch  3, batch     2 | loss: 412.1846007CurrentTrain: epoch  3, batch     3 | loss: 331.0620624CurrentTrain: epoch  3, batch     4 | loss: 250.9199900CurrentTrain: epoch  4, batch     0 | loss: 263.7306484CurrentTrain: epoch  4, batch     1 | loss: 378.4212802CurrentTrain: epoch  4, batch     2 | loss: 537.3064687CurrentTrain: epoch  4, batch     3 | loss: 458.6449330CurrentTrain: epoch  4, batch     4 | loss: 183.5714500CurrentTrain: epoch  5, batch     0 | loss: 316.2896997CurrentTrain: epoch  5, batch     1 | loss: 391.8500066CurrentTrain: epoch  5, batch     2 | loss: 456.7564471CurrentTrain: epoch  5, batch     3 | loss: 316.3897458CurrentTrain: epoch  5, batch     4 | loss: 220.1247492CurrentTrain: epoch  6, batch     0 | loss: 440.5912658CurrentTrain: epoch  6, batch     1 | loss: 376.2110463CurrentTrain: epoch  6, batch     2 | loss: 357.2596605CurrentTrain: epoch  6, batch     3 | loss: 298.4048314CurrentTrain: epoch  6, batch     4 | loss: 222.8633362CurrentTrain: epoch  7, batch     0 | loss: 344.9115014CurrentTrain: epoch  7, batch     1 | loss: 342.0686339CurrentTrain: epoch  7, batch     2 | loss: 340.4162634CurrentTrain: epoch  7, batch     3 | loss: 408.4150027CurrentTrain: epoch  7, batch     4 | loss: 305.2441993CurrentTrain: epoch  8, batch     0 | loss: 375.9765887CurrentTrain: epoch  8, batch     1 | loss: 439.6441891CurrentTrain: epoch  8, batch     2 | loss: 391.1309227CurrentTrain: epoch  8, batch     3 | loss: 298.8638676CurrentTrain: epoch  8, batch     4 | loss: 193.8839553CurrentTrain: epoch  9, batch     0 | loss: 537.1721798CurrentTrain: epoch  9, batch     1 | loss: 372.3556282CurrentTrain: epoch  9, batch     2 | loss: 284.6686741CurrentTrain: epoch  9, batch     3 | loss: 313.0526963CurrentTrain: epoch  9, batch     4 | loss: 222.5610146
MemoryTrain:  epoch  0, batch     0 | loss: 3.1249894MemoryTrain:  epoch  1, batch     0 | loss: 2.2582043MemoryTrain:  epoch  2, batch     0 | loss: 1.6404786MemoryTrain:  epoch  3, batch     0 | loss: 1.4009919MemoryTrain:  epoch  4, batch     0 | loss: 1.0946738MemoryTrain:  epoch  5, batch     0 | loss: 0.7728383MemoryTrain:  epoch  6, batch     0 | loss: 0.5332324MemoryTrain:  epoch  7, batch     0 | loss: 0.5828972MemoryTrain:  epoch  8, batch     0 | loss: 0.3325494MemoryTrain:  epoch  9, batch     0 | loss: 0.2448555

F1 score per class: {5: 0.9473684210526315, 6: 0.0, 10: 0.4, 16: 0.8, 17: 0.0, 18: 0.27906976744186046}
Micro-average F1 score: 0.6604215456674473
Weighted-average F1 score: 0.7479309701946225
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.6883116883116883, 16: 0.8518518518518519, 17: 0.0, 18: 0.8064516129032258}
Micro-average F1 score: 0.8139059304703476
Weighted-average F1 score: 0.8205852908260903
F1 score per class: {5: 0.98989898989899, 6: 0.0, 10: 0.6967741935483871, 16: 0.8518518518518519, 17: 0.0, 18: 0.7666666666666667}
Micro-average F1 score: 0.813141683778234
Weighted-average F1 score: 0.8218505186400002

F1 score per class: {32: 0.9375, 5: 0.6033519553072626, 6: 0.3968253968253968, 10: 0.8, 16: 0.0, 17: 0.27906976744186046, 18: 0.7804878048780488, 19: 0.2, 24: 0.7727272727272727, 26: 0.9010989010989011, 29: 0.9081632653061225}
Micro-average F1 score: 0.7374631268436578
Weighted-average F1 score: 0.7687881832568518
F1 score per class: {32: 0.9468599033816425, 5: 0.6990291262135923, 6: 0.6838709677419355, 10: 0.8518518518518519, 16: 0.0, 17: 0.7936507936507936, 18: 0.8636363636363636, 19: 0.7142857142857143, 24: 0.7771428571428571, 26: 0.93048128342246, 29: 0.93}
Micro-average F1 score: 0.8208955223880597
Weighted-average F1 score: 0.8222355471593193
F1 score per class: {32: 0.9514563106796117, 5: 0.6896551724137931, 6: 0.6878980891719745, 10: 0.8518518518518519, 16: 0.0, 17: 0.7666666666666667, 18: 0.8636363636363636, 19: 0.7142857142857143, 24: 0.7771428571428571, 26: 0.93048128342246, 29: 0.9253731343283582}
Micro-average F1 score: 0.819047619047619
Weighted-average F1 score: 0.8204975056820905
cur_acc:  ['0.8052', '0.6604']
his_acc:  ['0.8052', '0.7375']
cur_acc des:  ['0.8603', '0.8139']
his_acc des:  ['0.8603', '0.8209']
cur_acc rrf:  ['0.8612', '0.8131']
his_acc rrf:  ['0.8612', '0.8190']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 361.7441184CurrentTrain: epoch  0, batch     1 | loss: 396.6656449CurrentTrain: epoch  0, batch     2 | loss: 318.3687538CurrentTrain: epoch  0, batch     3 | loss: 360.5596607CurrentTrain: epoch  0, batch     4 | loss: 120.2232119CurrentTrain: epoch  1, batch     0 | loss: 389.9288531CurrentTrain: epoch  1, batch     1 | loss: 327.3527197CurrentTrain: epoch  1, batch     2 | loss: 281.3066472CurrentTrain: epoch  1, batch     3 | loss: 414.7927741CurrentTrain: epoch  1, batch     4 | loss: 120.1002648CurrentTrain: epoch  2, batch     0 | loss: 322.8273523CurrentTrain: epoch  2, batch     1 | loss: 365.0834506CurrentTrain: epoch  2, batch     2 | loss: 318.3271497CurrentTrain: epoch  2, batch     3 | loss: 445.5720856CurrentTrain: epoch  2, batch     4 | loss: 78.6264320CurrentTrain: epoch  3, batch     0 | loss: 382.7161241CurrentTrain: epoch  3, batch     1 | loss: 307.7594238CurrentTrain: epoch  3, batch     2 | loss: 361.8565739CurrentTrain: epoch  3, batch     3 | loss: 361.5030615CurrentTrain: epoch  3, batch     4 | loss: 79.6802103CurrentTrain: epoch  4, batch     0 | loss: 398.6045049CurrentTrain: epoch  4, batch     1 | loss: 346.7368588CurrentTrain: epoch  4, batch     2 | loss: 329.2176066CurrentTrain: epoch  4, batch     3 | loss: 361.8591479CurrentTrain: epoch  4, batch     4 | loss: 56.1512304CurrentTrain: epoch  5, batch     0 | loss: 345.1776993CurrentTrain: epoch  5, batch     1 | loss: 343.4046259CurrentTrain: epoch  5, batch     2 | loss: 330.8353094CurrentTrain: epoch  5, batch     3 | loss: 358.6259631CurrentTrain: epoch  5, batch     4 | loss: 76.4418677CurrentTrain: epoch  6, batch     0 | loss: 375.4497553CurrentTrain: epoch  6, batch     1 | loss: 391.4757935CurrentTrain: epoch  6, batch     2 | loss: 302.4800832CurrentTrain: epoch  6, batch     3 | loss: 316.7764530CurrentTrain: epoch  6, batch     4 | loss: 65.4407662CurrentTrain: epoch  7, batch     0 | loss: 328.3104069CurrentTrain: epoch  7, batch     1 | loss: 358.9154883CurrentTrain: epoch  7, batch     2 | loss: 358.8616420CurrentTrain: epoch  7, batch     3 | loss: 342.5706312CurrentTrain: epoch  7, batch     4 | loss: 119.8087348CurrentTrain: epoch  8, batch     0 | loss: 372.9208903CurrentTrain: epoch  8, batch     1 | loss: 299.4479396CurrentTrain: epoch  8, batch     2 | loss: 392.5717531CurrentTrain: epoch  8, batch     3 | loss: 326.6152492CurrentTrain: epoch  8, batch     4 | loss: 54.1585722CurrentTrain: epoch  9, batch     0 | loss: 390.3627671CurrentTrain: epoch  9, batch     1 | loss: 407.3764179CurrentTrain: epoch  9, batch     2 | loss: 299.5320072CurrentTrain: epoch  9, batch     3 | loss: 284.7957670CurrentTrain: epoch  9, batch     4 | loss: 75.4635548
MemoryTrain:  epoch  0, batch     0 | loss: 1.8328607MemoryTrain:  epoch  1, batch     0 | loss: 1.5891716MemoryTrain:  epoch  2, batch     0 | loss: 1.2305133MemoryTrain:  epoch  3, batch     0 | loss: 1.0167474MemoryTrain:  epoch  4, batch     0 | loss: 0.7706880MemoryTrain:  epoch  5, batch     0 | loss: 0.6984431MemoryTrain:  epoch  6, batch     0 | loss: 0.4612184MemoryTrain:  epoch  7, batch     0 | loss: 0.4104762MemoryTrain:  epoch  8, batch     0 | loss: 0.3110006MemoryTrain:  epoch  9, batch     0 | loss: 0.2376739

F1 score per class: {32: 0.8, 2: 0.0, 5: 0.0, 39: 0.35514018691588783, 10: 0.1981981981981982, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.5454545454545454, 24: 0.0, 28: 0.4444444444444444}
Micro-average F1 score: 0.30824372759856633
Weighted-average F1 score: 0.2856997524287244
F1 score per class: {2: 0.8235294117647058, 5: 0.0, 6: 0.0, 39: 0.0, 10: 0.6461538461538462, 11: 0.5815602836879432, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.625, 28: 0.7272727272727273}
Micro-average F1 score: 0.5522788203753352
Weighted-average F1 score: 0.44408344338939404
F1 score per class: {32: 0.8235294117647058, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.6666666666666666, 10: 0.5915492957746479, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.625, 24: 0.0, 28: 0.6666666666666666}
Micro-average F1 score: 0.5785123966942148
Weighted-average F1 score: 0.48912017858786705

F1 score per class: {32: 0.8, 2: 0.9595959595959596, 5: 0.6413043478260869, 6: 0.057692307692307696, 39: 0.31932773109243695, 10: 0.19642857142857142, 11: 0.7272727272727273, 12: 0.0, 16: 0.05128205128205128, 17: 0.7485380116959064, 18: 0.19047619047619047, 19: 0.7657142857142857, 24: 0.4, 26: 0.9130434782608695, 28: 0.9045226130653267, 29: 0.42105263157894735}
Micro-average F1 score: 0.6482504604051565
Weighted-average F1 score: 0.7561303184247398
F1 score per class: {32: 0.8235294117647058, 2: 0.937799043062201, 5: 0.6808510638297872, 6: 0.43609022556390975, 39: 0.5874125874125874, 10: 0.5359477124183006, 11: 0.8070175438596491, 12: 0.08333333333333333, 16: 0.5454545454545454, 17: 0.8167539267015707, 18: 0.5, 19: 0.7513812154696132, 24: 0.30303030303030304, 26: 0.907103825136612, 28: 0.9207920792079208, 29: 0.5714285714285714}
Micro-average F1 score: 0.7229437229437229
Weighted-average F1 score: 0.7349190796897824
F1 score per class: {32: 0.8235294117647058, 2: 0.9514563106796117, 5: 0.7021276595744681, 6: 0.45112781954887216, 39: 0.5906040268456376, 10: 0.5562913907284768, 11: 0.8070175438596491, 12: 0.08333333333333333, 16: 0.5172413793103449, 17: 0.8167539267015707, 18: 0.5625, 19: 0.7555555555555555, 24: 0.2857142857142857, 26: 0.907103825136612, 28: 0.916256157635468, 29: 0.5185185185185185}
Micro-average F1 score: 0.7295528898582334
Weighted-average F1 score: 0.7419968154450576
cur_acc:  ['0.8052', '0.6604', '0.3082']
his_acc:  ['0.8052', '0.7375', '0.6483']
cur_acc des:  ['0.8603', '0.8139', '0.5523']
his_acc des:  ['0.8603', '0.8209', '0.7229']
cur_acc rrf:  ['0.8612', '0.8131', '0.5785']
his_acc rrf:  ['0.8612', '0.8190', '0.7296']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 404.7759488CurrentTrain: epoch  0, batch     1 | loss: 314.2376295CurrentTrain: epoch  0, batch     2 | loss: 350.6502127CurrentTrain: epoch  0, batch     3 | loss: 330.8278509CurrentTrain: epoch  1, batch     0 | loss: 314.8834039CurrentTrain: epoch  1, batch     1 | loss: 407.6382692CurrentTrain: epoch  1, batch     2 | loss: 332.8594411CurrentTrain: epoch  1, batch     3 | loss: 309.2485580CurrentTrain: epoch  2, batch     0 | loss: 542.3372828CurrentTrain: epoch  2, batch     1 | loss: 321.7455269CurrentTrain: epoch  2, batch     2 | loss: 264.2587872CurrentTrain: epoch  2, batch     3 | loss: 319.8821517CurrentTrain: epoch  3, batch     0 | loss: 378.9800342CurrentTrain: epoch  3, batch     1 | loss: 365.4613043CurrentTrain: epoch  3, batch     2 | loss: 294.7376410CurrentTrain: epoch  3, batch     3 | loss: 309.0184367CurrentTrain: epoch  4, batch     0 | loss: 354.9337176CurrentTrain: epoch  4, batch     1 | loss: 335.6008599CurrentTrain: epoch  4, batch     2 | loss: 305.1977017CurrentTrain: epoch  4, batch     3 | loss: 295.5719686CurrentTrain: epoch  5, batch     0 | loss: 319.0840416CurrentTrain: epoch  5, batch     1 | loss: 332.1602993CurrentTrain: epoch  5, batch     2 | loss: 342.7692368CurrentTrain: epoch  5, batch     3 | loss: 305.1653580CurrentTrain: epoch  6, batch     0 | loss: 413.0381821CurrentTrain: epoch  6, batch     1 | loss: 306.8691890CurrentTrain: epoch  6, batch     2 | loss: 313.6194086CurrentTrain: epoch  6, batch     3 | loss: 269.7748610CurrentTrain: epoch  7, batch     0 | loss: 348.4956667CurrentTrain: epoch  7, batch     1 | loss: 361.2142411CurrentTrain: epoch  7, batch     2 | loss: 393.5918459CurrentTrain: epoch  7, batch     3 | loss: 226.3512674CurrentTrain: epoch  8, batch     0 | loss: 348.7751924CurrentTrain: epoch  8, batch     1 | loss: 316.6367667CurrentTrain: epoch  8, batch     2 | loss: 390.8143731CurrentTrain: epoch  8, batch     3 | loss: 244.7917431CurrentTrain: epoch  9, batch     0 | loss: 341.0227493CurrentTrain: epoch  9, batch     1 | loss: 306.6634974CurrentTrain: epoch  9, batch     2 | loss: 358.7802363CurrentTrain: epoch  9, batch     3 | loss: 281.2471206
MemoryTrain:  epoch  0, batch     0 | loss: 1.7424365MemoryTrain:  epoch  1, batch     0 | loss: 1.1718723MemoryTrain:  epoch  2, batch     0 | loss: 0.9837227MemoryTrain:  epoch  3, batch     0 | loss: 0.7191975MemoryTrain:  epoch  4, batch     0 | loss: 0.5917055MemoryTrain:  epoch  5, batch     0 | loss: 0.5264062MemoryTrain:  epoch  6, batch     0 | loss: 0.4799379MemoryTrain:  epoch  7, batch     0 | loss: 0.3877990MemoryTrain:  epoch  8, batch     0 | loss: 0.3330399MemoryTrain:  epoch  9, batch     0 | loss: 0.2305089

F1 score per class: {0: 0.9117647058823529, 32: 0.0, 2: 0.7730061349693251, 4: 0.0, 11: 0.0, 12: 0.5714285714285714, 13: 0.5581395348837209, 21: 0.7631578947368421, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7025641025641025
Weighted-average F1 score: 0.629036487613239
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 2: 0.8023952095808383, 4: 0.0, 5: 0.0, 11: 0.0, 12: 0.5714285714285714, 13: 0.0, 19: 0.8076923076923077, 21: 0.7901234567901234, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7314814814814815
Weighted-average F1 score: 0.6331883089256815
F1 score per class: {0: 0.9722222222222222, 32: 0.0, 2: 0.8372093023255814, 4: 0.0, 5: 0.0, 11: 0.0, 12: 0.5714285714285714, 13: 0.8301886792452831, 21: 0.810126582278481, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7616822429906542
Weighted-average F1 score: 0.6713360878095405

F1 score per class: {0: 0.8985507246376812, 2: 0.5, 4: 0.7730061349693251, 5: 0.9748743718592965, 6: 0.7128712871287128, 10: 0.17857142857142858, 11: 0.3333333333333333, 12: 0.25, 13: 0.07547169811320754, 16: 0.7931034482758621, 17: 0.0, 18: 0.13953488372093023, 19: 0.7849462365591398, 21: 0.38095238095238093, 23: 0.7532467532467533, 24: 0.1, 26: 0.7311827956989247, 28: 0.46153846153846156, 29: 0.9197860962566845, 32: 0.9045226130653267, 39: 0.25}
Micro-average F1 score: 0.6638537271448663
Weighted-average F1 score: 0.7115627910036036
F1 score per class: {0: 0.9473684210526315, 2: 0.2926829268292683, 4: 0.7976190476190477, 5: 0.9478672985781991, 6: 0.6944444444444444, 10: 0.6415094339622641, 11: 0.6777777777777778, 12: 0.6265060240963856, 13: 0.09090909090909091, 16: 0.8666666666666667, 17: 0.0, 18: 0.43636363636363634, 19: 0.8543689320388349, 21: 0.5121951219512195, 23: 0.7441860465116279, 24: 0.09523809523809523, 26: 0.6938775510204082, 28: 0.6153846153846154, 29: 0.9139784946236559, 32: 0.9029126213592233, 39: 0.6206896551724138}
Micro-average F1 score: 0.7371475953565506
Weighted-average F1 score: 0.7333151671625002
F1 score per class: {0: 0.9333333333333333, 2: 0.34285714285714286, 4: 0.8372093023255814, 5: 0.9569377990430622, 6: 0.7081339712918661, 10: 0.5637583892617449, 11: 0.632183908045977, 12: 0.6097560975609756, 13: 0.07017543859649122, 16: 0.8666666666666667, 17: 0.0, 18: 0.4074074074074074, 19: 0.84, 21: 0.5176470588235295, 23: 0.7710843373493976, 24: 0.09523809523809523, 26: 0.6974358974358974, 28: 0.5, 29: 0.9139784946236559, 32: 0.8985507246376812, 39: 0.5384615384615384}
Micro-average F1 score: 0.7278056951423786
Weighted-average F1 score: 0.7231229261091819
cur_acc:  ['0.8052', '0.6604', '0.3082', '0.7026']
his_acc:  ['0.8052', '0.7375', '0.6483', '0.6639']
cur_acc des:  ['0.8603', '0.8139', '0.5523', '0.7315']
his_acc des:  ['0.8603', '0.8209', '0.7229', '0.7371']
cur_acc rrf:  ['0.8612', '0.8131', '0.5785', '0.7617']
his_acc rrf:  ['0.8612', '0.8190', '0.7296', '0.7278']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 355.1634122CurrentTrain: epoch  0, batch     1 | loss: 376.5819988CurrentTrain: epoch  0, batch     2 | loss: 303.5171091CurrentTrain: epoch  0, batch     3 | loss: 308.3004680CurrentTrain: epoch  1, batch     0 | loss: 369.5751898CurrentTrain: epoch  1, batch     1 | loss: 385.2763939CurrentTrain: epoch  1, batch     2 | loss: 298.9024324CurrentTrain: epoch  1, batch     3 | loss: 201.3061577CurrentTrain: epoch  2, batch     0 | loss: 300.1264111CurrentTrain: epoch  2, batch     1 | loss: 363.7483996CurrentTrain: epoch  2, batch     2 | loss: 309.3538551CurrentTrain: epoch  2, batch     3 | loss: 312.1541607CurrentTrain: epoch  3, batch     0 | loss: 317.9135379CurrentTrain: epoch  3, batch     1 | loss: 307.0977266CurrentTrain: epoch  3, batch     2 | loss: 393.8008047CurrentTrain: epoch  3, batch     3 | loss: 227.9386440CurrentTrain: epoch  4, batch     0 | loss: 274.2953301CurrentTrain: epoch  4, batch     1 | loss: 315.3309722CurrentTrain: epoch  4, batch     2 | loss: 391.5798679CurrentTrain: epoch  4, batch     3 | loss: 293.8881578CurrentTrain: epoch  5, batch     0 | loss: 345.3205276CurrentTrain: epoch  5, batch     1 | loss: 316.2298582CurrentTrain: epoch  5, batch     2 | loss: 376.6393078CurrentTrain: epoch  5, batch     3 | loss: 216.1422573CurrentTrain: epoch  6, batch     0 | loss: 360.3114020CurrentTrain: epoch  6, batch     1 | loss: 286.4731024CurrentTrain: epoch  6, batch     2 | loss: 376.9597682CurrentTrain: epoch  6, batch     3 | loss: 242.3511562CurrentTrain: epoch  7, batch     0 | loss: 358.5975517CurrentTrain: epoch  7, batch     1 | loss: 315.2149915CurrentTrain: epoch  7, batch     2 | loss: 295.3034440CurrentTrain: epoch  7, batch     3 | loss: 254.9586344CurrentTrain: epoch  8, batch     0 | loss: 313.7059344CurrentTrain: epoch  8, batch     1 | loss: 297.9384517CurrentTrain: epoch  8, batch     2 | loss: 358.5101209CurrentTrain: epoch  8, batch     3 | loss: 289.9050255CurrentTrain: epoch  9, batch     0 | loss: 294.9731384CurrentTrain: epoch  9, batch     1 | loss: 286.1561308CurrentTrain: epoch  9, batch     2 | loss: 391.1675084CurrentTrain: epoch  9, batch     3 | loss: 270.3688333
MemoryTrain:  epoch  0, batch     0 | loss: 1.6020943MemoryTrain:  epoch  1, batch     0 | loss: 1.2648967MemoryTrain:  epoch  2, batch     0 | loss: 1.0125815MemoryTrain:  epoch  3, batch     0 | loss: 0.8776098MemoryTrain:  epoch  4, batch     0 | loss: 0.6688084MemoryTrain:  epoch  5, batch     0 | loss: 0.5839199MemoryTrain:  epoch  6, batch     0 | loss: 0.5003138MemoryTrain:  epoch  7, batch     0 | loss: 0.4201573MemoryTrain:  epoch  8, batch     0 | loss: 0.3592887MemoryTrain:  epoch  9, batch     0 | loss: 0.3277733

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.8235294117647058, 37: 0.0, 38: 0.0, 39: 0.4, 13: 0.0, 15: 0.7, 21: 0.5882352941176471, 23: 0.6341463414634146, 25: 0.0}
Micro-average F1 score: 0.5292307692307693
Weighted-average F1 score: 0.43721363447031053
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 10: 0.0, 11: 0.0, 13: 0.0, 15: 0.5753424657534246, 18: 0.0, 21: 0.9263157894736842, 23: 0.75, 25: 0.7272727272727273}
Micro-average F1 score: 0.640625
Weighted-average F1 score: 0.5244614762515173
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.0, 38: 0.7058823529411765, 10: 0.0, 11: 0.0, 13: 0.0, 15: 0.5555555555555556, 18: 0.0, 21: 0.9032258064516129, 23: 0.7628865979381443, 25: 0.8085106382978723}
Micro-average F1 score: 0.6492146596858639
Weighted-average F1 score: 0.5390954569283867

F1 score per class: {0: 0.8985507246376812, 2: 0.4444444444444444, 4: 0.7654320987654321, 5: 0.8571428571428571, 6: 0.6292134831460674, 10: 0.18018018018018017, 11: 0.25210084033613445, 12: 0.20689655172413793, 13: 0.08163265306122448, 15: 0.5384615384615384, 16: 0.8, 17: 0.0, 18: 0.0, 19: 0.6380368098159509, 21: 0.3333333333333333, 23: 0.717948717948718, 24: 0.10526315789473684, 25: 0.4, 26: 0.7351351351351352, 28: 0.5454545454545454, 29: 0.9139784946236559, 32: 0.8955223880597015, 35: 0.691358024691358, 37: 0.4716981132075472, 38: 0.3611111111111111, 39: 0.19047619047619047}
Micro-average F1 score: 0.605414273995078
Weighted-average F1 score: 0.654529831292269
F1 score per class: {0: 0.96, 2: 0.2857142857142857, 4: 0.7951807228915663, 5: 0.8658008658008658, 6: 0.7192118226600985, 10: 0.4507042253521127, 11: 0.5033112582781457, 12: 0.5359477124183006, 13: 0.125, 15: 0.48, 16: 0.819672131147541, 17: 0.0, 18: 0.35714285714285715, 19: 0.7191011235955056, 21: 0.36363636363636365, 23: 0.6881720430107527, 24: 0.08695652173913043, 25: 0.5753424657534246, 26: 0.7263157894736842, 28: 0.42857142857142855, 29: 0.925531914893617, 32: 0.8985507246376812, 35: 0.8979591836734694, 37: 0.4864864864864865, 38: 0.43243243243243246, 39: 0.4166666666666667}
Micro-average F1 score: 0.6645138141370649
Weighted-average F1 score: 0.6587570447507578
F1 score per class: {0: 0.96, 2: 0.3333333333333333, 4: 0.8235294117647058, 5: 0.8658008658008658, 6: 0.7192118226600985, 10: 0.4148148148148148, 11: 0.4966442953020134, 12: 0.5165562913907285, 13: 0.13114754098360656, 15: 0.42857142857142855, 16: 0.8064516129032258, 17: 0.0, 18: 0.32, 19: 0.7191011235955056, 21: 0.36, 23: 0.7560975609756098, 24: 0.09090909090909091, 25: 0.5555555555555556, 26: 0.723404255319149, 28: 0.375, 29: 0.925531914893617, 32: 0.8899521531100478, 35: 0.875, 37: 0.46540880503144655, 38: 0.4578313253012048, 39: 0.4166666666666667}
Micro-average F1 score: 0.662108672184239
Weighted-average F1 score: 0.6584282971494277
cur_acc:  ['0.8052', '0.6604', '0.3082', '0.7026', '0.5292']
his_acc:  ['0.8052', '0.7375', '0.6483', '0.6639', '0.6054']
cur_acc des:  ['0.8603', '0.8139', '0.5523', '0.7315', '0.6406']
his_acc des:  ['0.8603', '0.8209', '0.7229', '0.7371', '0.6645']
cur_acc rrf:  ['0.8612', '0.8131', '0.5785', '0.7617', '0.6492']
his_acc rrf:  ['0.8612', '0.8190', '0.7296', '0.7278', '0.6621']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 362.3919400CurrentTrain: epoch  0, batch     1 | loss: 381.5969210CurrentTrain: epoch  0, batch     2 | loss: 349.9180057CurrentTrain: epoch  0, batch     3 | loss: 200.8127142CurrentTrain: epoch  1, batch     0 | loss: 374.4773386CurrentTrain: epoch  1, batch     1 | loss: 420.8372072CurrentTrain: epoch  1, batch     2 | loss: 286.4699898CurrentTrain: epoch  1, batch     3 | loss: 221.5016023CurrentTrain: epoch  2, batch     0 | loss: 380.5284881CurrentTrain: epoch  2, batch     1 | loss: 336.5330498CurrentTrain: epoch  2, batch     2 | loss: 308.2983357CurrentTrain: epoch  2, batch     3 | loss: 203.3110335CurrentTrain: epoch  3, batch     0 | loss: 336.7453945CurrentTrain: epoch  3, batch     1 | loss: 347.6738178CurrentTrain: epoch  3, batch     2 | loss: 292.4346997CurrentTrain: epoch  3, batch     3 | loss: 227.9596724CurrentTrain: epoch  4, batch     0 | loss: 362.3619039CurrentTrain: epoch  4, batch     1 | loss: 364.1829826CurrentTrain: epoch  4, batch     2 | loss: 301.8732581CurrentTrain: epoch  4, batch     3 | loss: 198.9025329CurrentTrain: epoch  5, batch     0 | loss: 359.1068358CurrentTrain: epoch  5, batch     1 | loss: 392.3337857CurrentTrain: epoch  5, batch     2 | loss: 249.1609775CurrentTrain: epoch  5, batch     3 | loss: 229.4220315CurrentTrain: epoch  6, batch     0 | loss: 342.8533511CurrentTrain: epoch  6, batch     1 | loss: 344.7446984CurrentTrain: epoch  6, batch     2 | loss: 349.3372042CurrentTrain: epoch  6, batch     3 | loss: 209.8480483CurrentTrain: epoch  7, batch     0 | loss: 342.5363581CurrentTrain: epoch  7, batch     1 | loss: 302.6428472CurrentTrain: epoch  7, batch     2 | loss: 341.4070070CurrentTrain: epoch  7, batch     3 | loss: 197.8228924CurrentTrain: epoch  8, batch     0 | loss: 285.0797466CurrentTrain: epoch  8, batch     1 | loss: 438.1831936CurrentTrain: epoch  8, batch     2 | loss: 315.1098705CurrentTrain: epoch  8, batch     3 | loss: 185.1796839CurrentTrain: epoch  9, batch     0 | loss: 327.1612727CurrentTrain: epoch  9, batch     1 | loss: 357.0111313CurrentTrain: epoch  9, batch     2 | loss: 296.2130157CurrentTrain: epoch  9, batch     3 | loss: 257.6092318
MemoryTrain:  epoch  0, batch     0 | loss: 1.0560429MemoryTrain:  epoch  1, batch     0 | loss: 0.7364706MemoryTrain:  epoch  2, batch     0 | loss: 0.5714936MemoryTrain:  epoch  3, batch     0 | loss: 0.4344567MemoryTrain:  epoch  4, batch     0 | loss: 0.3646021MemoryTrain:  epoch  5, batch     0 | loss: 0.2824451MemoryTrain:  epoch  6, batch     0 | loss: 0.2275844MemoryTrain:  epoch  7, batch     0 | loss: 0.1738817MemoryTrain:  epoch  8, batch     0 | loss: 0.1699510MemoryTrain:  epoch  9, batch     0 | loss: 0.1370067

F1 score per class: {5: 0.0, 6: 0.0, 8: 0.5535714285714286, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.0, 20: 0.8775510204081632, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.972972972972973, 33: 0.42857142857142855, 35: 0.0, 36: 0.08695652173913043, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.536986301369863
Weighted-average F1 score: 0.5584556458814826
F1 score per class: {5: 0.0, 6: 0.0, 8: 0.8088235294117647, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.0, 20: 0.8979591836734694, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.42857142857142855, 35: 0.0, 36: 0.8376068376068376, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.7640449438202247
Weighted-average F1 score: 0.6869119682380501
F1 score per class: {5: 0.0, 6: 0.0, 8: 0.8175182481751825, 11: 0.0, 12: 0.0, 13: 0.0, 18: 0.0, 20: 0.9108910891089109, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 1.0, 33: 0.42857142857142855, 35: 0.0, 36: 0.5833333333333334, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.6909090909090909
Weighted-average F1 score: 0.6046823797686283

F1 score per class: {0: 0.8823529411764706, 2: 0.48, 4: 0.8439306358381503, 5: 0.8818181818181818, 6: 0.655367231638418, 8: 0.4397163120567376, 10: 0.21238938053097345, 11: 0.3484848484848485, 12: 0.05357142857142857, 13: 0.08, 15: 0.631578947368421, 16: 0.7796610169491526, 17: 0.0, 18: 0.20408163265306123, 19: 0.686046511627907, 20: 0.7747747747747747, 21: 0.35294117647058826, 23: 0.6933333333333334, 24: 0.1, 25: 0.36363636363636365, 26: 0.7157894736842105, 28: 0.3333333333333333, 29: 0.8666666666666667, 30: 0.9473684210526315, 32: 0.88, 33: 0.24, 35: 0.5142857142857142, 36: 0.08695652173913043, 37: 0.5233644859813084, 38: 0.2916666666666667, 39: 0.16666666666666666}
Micro-average F1 score: 0.5990650845019777
Weighted-average F1 score: 0.6651287206381807
F1 score per class: {0: 0.9459459459459459, 2: 0.375, 4: 0.8571428571428571, 5: 0.823045267489712, 6: 0.6989247311827957, 8: 0.5612244897959183, 10: 0.42424242424242425, 11: 0.6289308176100629, 12: 0.35555555555555557, 13: 0.17391304347826086, 15: 0.48, 16: 0.8666666666666667, 17: 0.0, 18: 0.45454545454545453, 19: 0.7182320441988951, 20: 0.8888888888888888, 21: 0.3829787234042553, 23: 0.6896551724137931, 24: 0.08695652173913043, 25: 0.5205479452054794, 26: 0.7157894736842105, 28: 0.3076923076923077, 29: 0.9148936170212766, 30: 0.8260869565217391, 32: 0.883495145631068, 33: 0.2, 35: 0.8712871287128713, 36: 0.6853146853146853, 37: 0.48214285714285715, 38: 0.5428571428571428, 39: 0.23809523809523808}
Micro-average F1 score: 0.6691519105312209
Weighted-average F1 score: 0.6749000601434627
F1 score per class: {0: 0.9459459459459459, 2: 0.4, 4: 0.8636363636363636, 5: 0.8403361344537815, 6: 0.6951871657754011, 8: 0.5685279187817259, 10: 0.3387096774193548, 11: 0.5555555555555556, 12: 0.35555555555555557, 13: 0.16666666666666666, 15: 0.46153846153846156, 16: 0.8524590163934426, 17: 0.0, 18: 0.4375, 19: 0.7182320441988951, 20: 0.8846153846153846, 21: 0.3917525773195876, 23: 0.7228915662650602, 24: 0.09523809523809523, 25: 0.5, 26: 0.7157894736842105, 28: 0.3333333333333333, 29: 0.9148936170212766, 30: 0.8444444444444444, 32: 0.8846153846153846, 33: 0.1935483870967742, 35: 0.8571428571428571, 36: 0.5137614678899083, 37: 0.45112781954887216, 38: 0.4533333333333333, 39: 0.23809523809523808}
Micro-average F1 score: 0.6541705716963448
Weighted-average F1 score: 0.6623659165459452
cur_acc:  ['0.8052', '0.6604', '0.3082', '0.7026', '0.5292', '0.5370']
his_acc:  ['0.8052', '0.7375', '0.6483', '0.6639', '0.6054', '0.5991']
cur_acc des:  ['0.8603', '0.8139', '0.5523', '0.7315', '0.6406', '0.7640']
his_acc des:  ['0.8603', '0.8209', '0.7229', '0.7371', '0.6645', '0.6692']
cur_acc rrf:  ['0.8612', '0.8131', '0.5785', '0.7617', '0.6492', '0.6909']
his_acc rrf:  ['0.8612', '0.8190', '0.7296', '0.7278', '0.6621', '0.6542']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 338.3914838CurrentTrain: epoch  0, batch     1 | loss: 313.4102838CurrentTrain: epoch  0, batch     2 | loss: 331.4862069CurrentTrain: epoch  0, batch     3 | loss: 25.3689718CurrentTrain: epoch  1, batch     0 | loss: 299.7253129CurrentTrain: epoch  1, batch     1 | loss: 378.1272928CurrentTrain: epoch  1, batch     2 | loss: 327.6434166CurrentTrain: epoch  1, batch     3 | loss: 19.7289426CurrentTrain: epoch  2, batch     0 | loss: 360.1783249CurrentTrain: epoch  2, batch     1 | loss: 336.5974508CurrentTrain: epoch  2, batch     2 | loss: 296.4574657CurrentTrain: epoch  2, batch     3 | loss: 31.3724938CurrentTrain: epoch  3, batch     0 | loss: 320.5695921CurrentTrain: epoch  3, batch     1 | loss: 337.2653772CurrentTrain: epoch  3, batch     2 | loss: 309.4657966CurrentTrain: epoch  3, batch     3 | loss: 18.1978381CurrentTrain: epoch  4, batch     0 | loss: 365.3268988CurrentTrain: epoch  4, batch     1 | loss: 292.4676843CurrentTrain: epoch  4, batch     2 | loss: 286.8936346CurrentTrain: epoch  4, batch     3 | loss: 30.3367625CurrentTrain: epoch  5, batch     0 | loss: 314.3222262CurrentTrain: epoch  5, batch     1 | loss: 313.6682475CurrentTrain: epoch  5, batch     2 | loss: 342.9537058CurrentTrain: epoch  5, batch     3 | loss: 11.1157869CurrentTrain: epoch  6, batch     0 | loss: 316.9585426CurrentTrain: epoch  6, batch     1 | loss: 361.2304166CurrentTrain: epoch  6, batch     2 | loss: 271.7878089CurrentTrain: epoch  6, batch     3 | loss: 29.5704655CurrentTrain: epoch  7, batch     0 | loss: 259.0098010CurrentTrain: epoch  7, batch     1 | loss: 344.2179441CurrentTrain: epoch  7, batch     2 | loss: 326.7914665CurrentTrain: epoch  7, batch     3 | loss: 29.9663917CurrentTrain: epoch  8, batch     0 | loss: 247.4908512CurrentTrain: epoch  8, batch     1 | loss: 341.5684267CurrentTrain: epoch  8, batch     2 | loss: 314.2309280CurrentTrain: epoch  8, batch     3 | loss: 55.0073865CurrentTrain: epoch  9, batch     0 | loss: 311.4194633CurrentTrain: epoch  9, batch     1 | loss: 245.3757295CurrentTrain: epoch  9, batch     2 | loss: 355.8547648CurrentTrain: epoch  9, batch     3 | loss: 54.8987653
MemoryTrain:  epoch  0, batch     0 | loss: 0.8939753MemoryTrain:  epoch  1, batch     0 | loss: 0.7017157MemoryTrain:  epoch  2, batch     0 | loss: 0.4679000MemoryTrain:  epoch  3, batch     0 | loss: 0.2989086MemoryTrain:  epoch  4, batch     0 | loss: 0.2516099MemoryTrain:  epoch  5, batch     0 | loss: 0.2227606MemoryTrain:  epoch  6, batch     0 | loss: 0.1770332MemoryTrain:  epoch  7, batch     0 | loss: 0.1490450MemoryTrain:  epoch  8, batch     0 | loss: 0.1449865MemoryTrain:  epoch  9, batch     0 | loss: 0.1220180

F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 21: 0.0, 26: 0.42105263157894735, 27: 0.0, 31: 0.4186046511627907}
Micro-average F1 score: 0.4688995215311005
Weighted-average F1 score: 0.37593537447189146
F1 score per class: {35: 0.0, 6: 0.75, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 21: 0.64, 26: 1.0, 27: 0.0, 31: 0.8870967741935484}
Micro-average F1 score: 0.8266666666666667
Weighted-average F1 score: 0.7758308403279941
F1 score per class: {35: 0.0, 6: 0.75, 7: 0.9803921568627451, 40: 0.0, 9: 0.0, 19: 0.0, 21: 0.64, 26: 1.0, 27: 0.0, 31: 0.859504132231405}
Micro-average F1 score: 0.8
Weighted-average F1 score: 0.738680386755253

F1 score per class: {0: 0.8823529411764706, 2: 0.5217391304347826, 4: 0.8165680473372781, 5: 0.8699551569506726, 6: 0.5228758169934641, 7: 0.07692307692307693, 8: 0.2391304347826087, 9: 0.9803921568627451, 10: 0.17857142857142858, 11: 0.34782608695652173, 12: 0.03305785123966942, 13: 0.08, 15: 0.6666666666666666, 16: 0.7586206896551724, 17: 0.0, 18: 0.20408163265306123, 19: 0.6296296296296297, 20: 0.7758620689655172, 21: 0.3, 23: 0.7105263157894737, 24: 0.1, 25: 0.36363636363636365, 26: 0.708994708994709, 27: 0.26666666666666666, 28: 0.3333333333333333, 29: 0.8187134502923976, 30: 0.9142857142857143, 31: 0.0, 32: 0.8089887640449438, 33: 0.23076923076923078, 35: 0.40625, 36: 0.058823529411764705, 37: 0.4827586206896552, 38: 0.15, 39: 0.08695652173913043, 40: 0.3673469387755102}
Micro-average F1 score: 0.5481682496607869
Weighted-average F1 score: 0.6085416743009796
F1 score per class: {0: 0.9295774647887324, 2: 0.375, 4: 0.8505747126436781, 5: 0.8298755186721992, 6: 0.48951048951048953, 7: 0.0821917808219178, 8: 0.5730994152046783, 9: 0.9433962264150944, 10: 0.3779527559055118, 11: 0.6235294117647059, 12: 0.3472222222222222, 13: 0.17391304347826086, 15: 0.48, 16: 0.8333333333333334, 17: 0.0, 18: 0.5070422535211268, 19: 0.7262569832402235, 20: 0.9108910891089109, 21: 0.5294117647058824, 23: 0.6904761904761905, 24: 0.09523809523809523, 25: 0.56, 26: 0.7120418848167539, 27: 0.43243243243243246, 28: 0.3076923076923077, 29: 0.8864864864864865, 30: 1.0, 31: 0.6666666666666666, 32: 0.88, 33: 0.15789473684210525, 35: 0.845360824742268, 36: 0.7441860465116279, 37: 0.5510204081632653, 38: 0.43037974683544306, 39: 0.29411764705882354, 40: 0.6395348837209303}
Micro-average F1 score: 0.6544289044289044
Weighted-average F1 score: 0.6525045703610206
F1 score per class: {0: 0.9295774647887324, 2: 0.4444444444444444, 4: 0.8700564971751412, 5: 0.8368200836820083, 6: 0.5034013605442177, 7: 0.08, 8: 0.5915492957746479, 9: 0.9803921568627451, 10: 0.45588235294117646, 11: 0.5909090909090909, 12: 0.36363636363636365, 13: 0.14814814814814814, 15: 0.4444444444444444, 16: 0.819672131147541, 17: 0.0, 18: 0.41935483870967744, 19: 0.7351351351351352, 20: 0.8785046728971962, 21: 0.5454545454545454, 23: 0.7, 24: 0.09523809523809523, 25: 0.5, 26: 0.7120418848167539, 27: 0.4, 28: 0.3076923076923077, 29: 0.8913043478260869, 30: 1.0, 31: 0.6666666666666666, 32: 0.88, 33: 0.13636363636363635, 35: 0.8571428571428571, 36: 0.5, 37: 0.47863247863247865, 38: 0.35714285714285715, 39: 0.25, 40: 0.6459627329192547}
Micro-average F1 score: 0.6430038134350249
Weighted-average F1 score: 0.6382610202537672
cur_acc:  ['0.8052', '0.6604', '0.3082', '0.7026', '0.5292', '0.5370', '0.4689']
his_acc:  ['0.8052', '0.7375', '0.6483', '0.6639', '0.6054', '0.5991', '0.5482']
cur_acc des:  ['0.8603', '0.8139', '0.5523', '0.7315', '0.6406', '0.7640', '0.8267']
his_acc des:  ['0.8603', '0.8209', '0.7229', '0.7371', '0.6645', '0.6692', '0.6544']
cur_acc rrf:  ['0.8612', '0.8131', '0.5785', '0.7617', '0.6492', '0.6909', '0.8000']
his_acc rrf:  ['0.8612', '0.8190', '0.7296', '0.7278', '0.6621', '0.6542', '0.6430']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 413.3638021CurrentTrain: epoch  0, batch     1 | loss: 351.1235711CurrentTrain: epoch  0, batch     2 | loss: 364.0613477CurrentTrain: epoch  0, batch     3 | loss: 349.2977112CurrentTrain: epoch  0, batch     4 | loss: 303.7540021CurrentTrain: epoch  1, batch     0 | loss: 333.5520549CurrentTrain: epoch  1, batch     1 | loss: 400.6359148CurrentTrain: epoch  1, batch     2 | loss: 403.1343771CurrentTrain: epoch  1, batch     3 | loss: 386.9228988CurrentTrain: epoch  1, batch     4 | loss: 211.8325565CurrentTrain: epoch  2, batch     0 | loss: 378.1080891CurrentTrain: epoch  2, batch     1 | loss: 296.0750665CurrentTrain: epoch  2, batch     2 | loss: 333.4608197CurrentTrain: epoch  2, batch     3 | loss: 362.3747311CurrentTrain: epoch  2, batch     4 | loss: 428.2983936CurrentTrain: epoch  3, batch     0 | loss: 400.6975824CurrentTrain: epoch  3, batch     1 | loss: 302.1254802CurrentTrain: epoch  3, batch     2 | loss: 462.0840916CurrentTrain: epoch  3, batch     3 | loss: 376.5362117CurrentTrain: epoch  3, batch     4 | loss: 181.8152780CurrentTrain: epoch  4, batch     0 | loss: 360.0416041CurrentTrain: epoch  4, batch     1 | loss: 331.4704831CurrentTrain: epoch  4, batch     2 | loss: 358.2662503CurrentTrain: epoch  4, batch     3 | loss: 392.7718718CurrentTrain: epoch  4, batch     4 | loss: 205.4847252CurrentTrain: epoch  5, batch     0 | loss: 456.4354194CurrentTrain: epoch  5, batch     1 | loss: 360.1378345CurrentTrain: epoch  5, batch     2 | loss: 315.6472678CurrentTrain: epoch  5, batch     3 | loss: 313.4661489CurrentTrain: epoch  5, batch     4 | loss: 303.3278245CurrentTrain: epoch  6, batch     0 | loss: 300.9133485CurrentTrain: epoch  6, batch     1 | loss: 328.2545177CurrentTrain: epoch  6, batch     2 | loss: 357.9844169CurrentTrain: epoch  6, batch     3 | loss: 408.9769593CurrentTrain: epoch  6, batch     4 | loss: 248.8895658CurrentTrain: epoch  7, batch     0 | loss: 408.6608372CurrentTrain: epoch  7, batch     1 | loss: 298.9651658CurrentTrain: epoch  7, batch     2 | loss: 376.3082504CurrentTrain: epoch  7, batch     3 | loss: 374.6235816CurrentTrain: epoch  7, batch     4 | loss: 190.7059745CurrentTrain: epoch  8, batch     0 | loss: 327.7253447CurrentTrain: epoch  8, batch     1 | loss: 330.1214236CurrentTrain: epoch  8, batch     2 | loss: 456.9440198CurrentTrain: epoch  8, batch     3 | loss: 297.4451031CurrentTrain: epoch  8, batch     4 | loss: 301.2160683CurrentTrain: epoch  9, batch     0 | loss: 390.7000691CurrentTrain: epoch  9, batch     1 | loss: 341.9205845CurrentTrain: epoch  9, batch     2 | loss: 356.5917440CurrentTrain: epoch  9, batch     3 | loss: 313.9628023CurrentTrain: epoch  9, batch     4 | loss: 233.2806654
MemoryTrain:  epoch  0, batch     0 | loss: 1.3272302MemoryTrain:  epoch  1, batch     0 | loss: 1.0773614MemoryTrain:  epoch  2, batch     0 | loss: 0.7917322MemoryTrain:  epoch  3, batch     0 | loss: 0.7222127MemoryTrain:  epoch  4, batch     0 | loss: 0.5869178MemoryTrain:  epoch  5, batch     0 | loss: 0.4751544MemoryTrain:  epoch  6, batch     0 | loss: 0.3991952MemoryTrain:  epoch  7, batch     0 | loss: 0.3212840MemoryTrain:  epoch  8, batch     0 | loss: 0.2643017MemoryTrain:  epoch  9, batch     0 | loss: 0.2107736

F1 score per class: {32: 0.34146341463414637, 1: 0.5225225225225225, 34: 0.0, 35: 0.029411764705882353, 3: 0.0, 37: 0.782608695652174, 40: 0.0, 11: 0.0, 14: 0.0, 18: 0.4507042253521127, 22: 0.0, 23: 0.0, 24: 0.0}
Micro-average F1 score: 0.46959459459459457
Weighted-average F1 score: 0.46287402224057195
F1 score per class: {32: 0.4065040650406504, 1: 0.8611111111111112, 34: 0.0, 3: 0.12345679012345678, 35: 0.0, 5: 0.8020833333333334, 38: 0.0, 37: 0.0, 40: 0.0, 14: 0.0, 18: 0.9320388349514563, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.6439169139465876
Weighted-average F1 score: 0.6234982470809141
F1 score per class: {32: 0.4065040650406504, 1: 0.8611111111111112, 34: 0.0, 3: 0.0, 35: 0.14634146341463414, 37: 0.0, 38: 0.806282722513089, 5: 0.0, 40: 0.0, 11: 0.0, 14: 0.0, 18: 0.8541666666666666, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.6289120715350224
Weighted-average F1 score: 0.6000673791994432

F1 score per class: {0: 0.7457627118644068, 1: 0.29577464788732394, 2: 0.6666666666666666, 3: 0.4603174603174603, 4: 0.7730061349693251, 5: 0.8899082568807339, 6: 0.4931506849315068, 7: 0.0784313725490196, 8: 0.32653061224489793, 9: 0.9803921568627451, 10: 0.1981981981981982, 11: 0.31007751937984496, 12: 0.01694915254237288, 13: 0.08333333333333333, 14: 0.025, 15: 0.6666666666666666, 16: 0.7719298245614035, 17: 0.0, 18: 0.3508771929824561, 19: 0.49411764705882355, 20: 0.7350427350427351, 21: 0.0, 22: 0.7236180904522613, 23: 0.7804878048780488, 24: 0.09523809523809523, 25: 0.3225806451612903, 26: 0.7391304347826086, 27: 0.0, 28: 0.3333333333333333, 29: 0.7904191616766467, 30: 0.9142857142857143, 31: 0.0, 32: 0.7796610169491526, 33: 0.24, 34: 0.25196850393700787, 35: 0.34146341463414637, 36: 0.16666666666666666, 37: 0.4380952380952381, 38: 0.16666666666666666, 39: 0.16, 40: 0.3870967741935484}
Micro-average F1 score: 0.5080805216898214
Weighted-average F1 score: 0.5509674552541287
F1 score per class: {0: 0.9428571428571428, 1: 0.3333333333333333, 2: 0.3870967741935484, 3: 0.7469879518072289, 4: 0.8505747126436781, 5: 0.8064516129032258, 6: 0.48226950354609927, 7: 0.09523809523809523, 8: 0.5494505494505495, 9: 0.9615384615384616, 10: 0.32786885245901637, 11: 0.3157894736842105, 12: 0.2835820895522388, 13: 0.17391304347826086, 14: 0.11627906976744186, 15: 0.631578947368421, 16: 0.8666666666666667, 17: 0.0, 18: 0.3448275862068966, 19: 0.6335403726708074, 20: 0.9019607843137255, 21: 0.17647058823529413, 22: 0.7264150943396226, 23: 0.7555555555555555, 24: 0.07692307692307693, 25: 0.5205479452054794, 26: 0.7120418848167539, 27: 0.0, 28: 0.3076923076923077, 29: 0.8791208791208791, 30: 1.0, 31: 0.8, 32: 0.8615384615384616, 33: 0.1935483870967742, 34: 0.4266666666666667, 35: 0.6666666666666666, 36: 0.7350427350427351, 37: 0.325, 38: 0.3466666666666667, 39: 0.2631578947368421, 40: 0.6904761904761905}
Micro-average F1 score: 0.5985794758755817
Weighted-average F1 score: 0.6085119566656287
F1 score per class: {0: 0.9428571428571428, 1: 0.3401360544217687, 2: 0.41379310344827586, 3: 0.7425149700598802, 4: 0.8571428571428571, 5: 0.8264462809917356, 6: 0.5068493150684932, 7: 0.06153846153846154, 8: 0.5205479452054794, 9: 0.9803921568627451, 10: 0.40310077519379844, 11: 0.4094488188976378, 12: 0.2537313432835821, 13: 0.16, 14: 0.12244897959183673, 15: 0.6, 16: 0.8524590163934426, 17: 0.0, 18: 0.3389830508474576, 19: 0.6629213483146067, 20: 0.8727272727272727, 21: 0.0625, 22: 0.7298578199052133, 23: 0.7469879518072289, 24: 0.07692307692307693, 25: 0.4788732394366197, 26: 0.7157894736842105, 27: 0.0, 28: 0.3076923076923077, 29: 0.8777777777777778, 30: 1.0, 31: 0.8, 32: 0.8615384615384616, 33: 0.1935483870967742, 34: 0.39805825242718446, 35: 0.6538461538461539, 36: 0.64, 37: 0.3516483516483517, 38: 0.26506024096385544, 39: 0.2, 40: 0.6901408450704225}
Micro-average F1 score: 0.5928853754940712
Weighted-average F1 score: 0.5961273709125832
cur_acc:  ['0.8052', '0.6604', '0.3082', '0.7026', '0.5292', '0.5370', '0.4689', '0.4696']
his_acc:  ['0.8052', '0.7375', '0.6483', '0.6639', '0.6054', '0.5991', '0.5482', '0.5081']
cur_acc des:  ['0.8603', '0.8139', '0.5523', '0.7315', '0.6406', '0.7640', '0.8267', '0.6439']
his_acc des:  ['0.8603', '0.8209', '0.7229', '0.7371', '0.6645', '0.6692', '0.6544', '0.5986']
cur_acc rrf:  ['0.8612', '0.8131', '0.5785', '0.7617', '0.6492', '0.6909', '0.8000', '0.6289']
his_acc rrf:  ['0.8612', '0.8190', '0.7296', '0.7278', '0.6621', '0.6542', '0.6430', '0.5929']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 319.6742873CurrentTrain: epoch  0, batch     1 | loss: 335.4828468CurrentTrain: epoch  0, batch     2 | loss: 365.0657312CurrentTrain: epoch  0, batch     3 | loss: 414.8787813CurrentTrain: epoch  0, batch     4 | loss: 365.6898304CurrentTrain: epoch  0, batch     5 | loss: 278.2599255CurrentTrain: epoch  0, batch     6 | loss: 396.0739862CurrentTrain: epoch  0, batch     7 | loss: 321.8066287CurrentTrain: epoch  0, batch     8 | loss: 409.2991502CurrentTrain: epoch  0, batch     9 | loss: 333.1767248CurrentTrain: epoch  0, batch    10 | loss: 359.6901807CurrentTrain: epoch  0, batch    11 | loss: 395.3016024CurrentTrain: epoch  0, batch    12 | loss: 468.5013388CurrentTrain: epoch  0, batch    13 | loss: 333.1924715CurrentTrain: epoch  0, batch    14 | loss: 332.4117287CurrentTrain: epoch  0, batch    15 | loss: 387.6481380CurrentTrain: epoch  0, batch    16 | loss: 332.7737935CurrentTrain: epoch  0, batch    17 | loss: 466.6426429CurrentTrain: epoch  0, batch    18 | loss: 308.2265218CurrentTrain: epoch  0, batch    19 | loss: 333.0051280CurrentTrain: epoch  0, batch    20 | loss: 309.2440450CurrentTrain: epoch  0, batch    21 | loss: 344.8096848CurrentTrain: epoch  0, batch    22 | loss: 309.0657993CurrentTrain: epoch  0, batch    23 | loss: 419.3084680CurrentTrain: epoch  0, batch    24 | loss: 347.4431119CurrentTrain: epoch  0, batch    25 | loss: 371.5744927CurrentTrain: epoch  0, batch    26 | loss: 466.5855332CurrentTrain: epoch  0, batch    27 | loss: 319.1617445CurrentTrain: epoch  0, batch    28 | loss: 306.0850726CurrentTrain: epoch  0, batch    29 | loss: 405.0057176CurrentTrain: epoch  0, batch    30 | loss: 545.4986506CurrentTrain: epoch  0, batch    31 | loss: 271.8117831CurrentTrain: epoch  0, batch    32 | loss: 343.9350712CurrentTrain: epoch  0, batch    33 | loss: 329.9093109CurrentTrain: epoch  0, batch    34 | loss: 343.3978777CurrentTrain: epoch  0, batch    35 | loss: 352.2712282CurrentTrain: epoch  0, batch    36 | loss: 318.3336042CurrentTrain: epoch  0, batch    37 | loss: 344.7479719CurrentTrain: epoch  0, batch    38 | loss: 330.8804553CurrentTrain: epoch  0, batch    39 | loss: 356.2080288CurrentTrain: epoch  0, batch    40 | loss: 284.2733474CurrentTrain: epoch  0, batch    41 | loss: 464.9564380CurrentTrain: epoch  0, batch    42 | loss: 343.4825521CurrentTrain: epoch  0, batch    43 | loss: 317.4479205CurrentTrain: epoch  0, batch    44 | loss: 357.2740132CurrentTrain: epoch  0, batch    45 | loss: 358.3764442CurrentTrain: epoch  0, batch    46 | loss: 435.9531008CurrentTrain: epoch  0, batch    47 | loss: 370.9274411CurrentTrain: epoch  0, batch    48 | loss: 309.0664143CurrentTrain: epoch  0, batch    49 | loss: 308.4446576CurrentTrain: epoch  0, batch    50 | loss: 343.0931342CurrentTrain: epoch  0, batch    51 | loss: 330.0728438CurrentTrain: epoch  0, batch    52 | loss: 292.7241243CurrentTrain: epoch  0, batch    53 | loss: 317.9115028CurrentTrain: epoch  0, batch    54 | loss: 302.6792183CurrentTrain: epoch  0, batch    55 | loss: 369.6794171CurrentTrain: epoch  0, batch    56 | loss: 371.3956734CurrentTrain: epoch  0, batch    57 | loss: 418.7336897CurrentTrain: epoch  0, batch    58 | loss: 388.9700728CurrentTrain: epoch  0, batch    59 | loss: 417.5338123CurrentTrain: epoch  0, batch    60 | loss: 342.4727456CurrentTrain: epoch  0, batch    61 | loss: 291.6718362CurrentTrain: epoch  0, batch    62 | loss: 744.9661243CurrentTrain: epoch  0, batch    63 | loss: 267.9643144CurrentTrain: epoch  0, batch    64 | loss: 328.2593924CurrentTrain: epoch  0, batch    65 | loss: 316.6230093CurrentTrain: epoch  0, batch    66 | loss: 303.5205647CurrentTrain: epoch  0, batch    67 | loss: 449.6933234CurrentTrain: epoch  0, batch    68 | loss: 302.8619324CurrentTrain: epoch  0, batch    69 | loss: 355.7440763CurrentTrain: epoch  0, batch    70 | loss: 448.5758561CurrentTrain: epoch  0, batch    71 | loss: 418.8224992CurrentTrain: epoch  0, batch    72 | loss: 383.4636924CurrentTrain: epoch  0, batch    73 | loss: 403.7158484CurrentTrain: epoch  0, batch    74 | loss: 355.5938556CurrentTrain: epoch  0, batch    75 | loss: 342.3798863CurrentTrain: epoch  0, batch    76 | loss: 354.3458896CurrentTrain: epoch  0, batch    77 | loss: 418.2864816CurrentTrain: epoch  0, batch    78 | loss: 368.5390316CurrentTrain: epoch  0, batch    79 | loss: 447.5814348CurrentTrain: epoch  0, batch    80 | loss: 368.4497424CurrentTrain: epoch  0, batch    81 | loss: 417.1405442CurrentTrain: epoch  0, batch    82 | loss: 402.0779835CurrentTrain: epoch  0, batch    83 | loss: 339.6967029CurrentTrain: epoch  0, batch    84 | loss: 415.9249873CurrentTrain: epoch  0, batch    85 | loss: 368.6622009CurrentTrain: epoch  0, batch    86 | loss: 390.2697472CurrentTrain: epoch  0, batch    87 | loss: 373.4201248CurrentTrain: epoch  0, batch    88 | loss: 354.2913519CurrentTrain: epoch  0, batch    89 | loss: 289.9350758CurrentTrain: epoch  0, batch    90 | loss: 356.3463168CurrentTrain: epoch  0, batch    91 | loss: 278.8974084CurrentTrain: epoch  0, batch    92 | loss: 302.0869572CurrentTrain: epoch  0, batch    93 | loss: 302.4389339CurrentTrain: epoch  0, batch    94 | loss: 298.7067515CurrentTrain: epoch  0, batch    95 | loss: 371.8931823CurrentTrain: epoch  1, batch     0 | loss: 326.5688283CurrentTrain: epoch  1, batch     1 | loss: 380.9753264CurrentTrain: epoch  1, batch     2 | loss: 368.8761106CurrentTrain: epoch  1, batch     3 | loss: 367.7794338CurrentTrain: epoch  1, batch     4 | loss: 338.5874042CurrentTrain: epoch  1, batch     5 | loss: 382.0825440CurrentTrain: epoch  1, batch     6 | loss: 296.8682890CurrentTrain: epoch  1, batch     7 | loss: 312.1791448CurrentTrain: epoch  1, batch     8 | loss: 349.9039935CurrentTrain: epoch  1, batch     9 | loss: 324.1332835CurrentTrain: epoch  1, batch    10 | loss: 542.2420510CurrentTrain: epoch  1, batch    11 | loss: 445.3105419CurrentTrain: epoch  1, batch    12 | loss: 325.0873537CurrentTrain: epoch  1, batch    13 | loss: 327.7122615CurrentTrain: epoch  1, batch    14 | loss: 326.7026241CurrentTrain: epoch  1, batch    15 | loss: 381.8660726CurrentTrain: epoch  1, batch    16 | loss: 368.7345056CurrentTrain: epoch  1, batch    17 | loss: 311.6138943CurrentTrain: epoch  1, batch    18 | loss: 335.0413749CurrentTrain: epoch  1, batch    19 | loss: 299.6342270CurrentTrain: epoch  1, batch    20 | loss: 307.5504827CurrentTrain: epoch  1, batch    21 | loss: 324.6520114CurrentTrain: epoch  1, batch    22 | loss: 337.5266853CurrentTrain: epoch  1, batch    23 | loss: 290.1192701CurrentTrain: epoch  1, batch    24 | loss: 356.0536249CurrentTrain: epoch  1, batch    25 | loss: 284.5608622CurrentTrain: epoch  1, batch    26 | loss: 528.1773230CurrentTrain: epoch  1, batch    27 | loss: 352.3452805CurrentTrain: epoch  1, batch    28 | loss: 341.4217763CurrentTrain: epoch  1, batch    29 | loss: 336.7925939CurrentTrain: epoch  1, batch    30 | loss: 295.8332672CurrentTrain: epoch  1, batch    31 | loss: 352.7176681CurrentTrain: epoch  1, batch    32 | loss: 351.0385815CurrentTrain: epoch  1, batch    33 | loss: 378.2264154CurrentTrain: epoch  1, batch    34 | loss: 505.2321858CurrentTrain: epoch  1, batch    35 | loss: 396.2083215CurrentTrain: epoch  1, batch    36 | loss: 411.2569431CurrentTrain: epoch  1, batch    37 | loss: 335.1427601CurrentTrain: epoch  1, batch    38 | loss: 282.0846601CurrentTrain: epoch  1, batch    39 | loss: 395.4946020CurrentTrain: epoch  1, batch    40 | loss: 313.7078932CurrentTrain: epoch  1, batch    41 | loss: 414.1571463CurrentTrain: epoch  1, batch    42 | loss: 367.2196314CurrentTrain: epoch  1, batch    43 | loss: 309.1459496CurrentTrain: epoch  1, batch    44 | loss: 365.0630034CurrentTrain: epoch  1, batch    45 | loss: 275.3185541CurrentTrain: epoch  1, batch    46 | loss: 355.2143556CurrentTrain: epoch  1, batch    47 | loss: 379.7515719CurrentTrain: epoch  1, batch    48 | loss: 440.6938284CurrentTrain: epoch  1, batch    49 | loss: 298.6967390CurrentTrain: epoch  1, batch    50 | loss: 429.5784627CurrentTrain: epoch  1, batch    51 | loss: 301.7417211CurrentTrain: epoch  1, batch    52 | loss: 312.9961839CurrentTrain: epoch  1, batch    53 | loss: 384.4739469CurrentTrain: epoch  1, batch    54 | loss: 248.8396432CurrentTrain: epoch  1, batch    55 | loss: 387.0276677CurrentTrain: epoch  1, batch    56 | loss: 283.3427463CurrentTrain: epoch  1, batch    57 | loss: 412.9837543CurrentTrain: epoch  1, batch    58 | loss: 368.9579454CurrentTrain: epoch  1, batch    59 | loss: 352.9419472CurrentTrain: epoch  1, batch    60 | loss: 333.1333576CurrentTrain: epoch  1, batch    61 | loss: 338.0735093CurrentTrain: epoch  1, batch    62 | loss: 364.1164424CurrentTrain: epoch  1, batch    63 | loss: 364.5698738CurrentTrain: epoch  1, batch    64 | loss: 333.7240295CurrentTrain: epoch  1, batch    65 | loss: 333.3861608CurrentTrain: epoch  1, batch    66 | loss: 291.5008982CurrentTrain: epoch  1, batch    67 | loss: 336.0860452CurrentTrain: epoch  1, batch    68 | loss: 364.3905628CurrentTrain: epoch  1, batch    69 | loss: 305.4677434CurrentTrain: epoch  1, batch    70 | loss: 321.2347806CurrentTrain: epoch  1, batch    71 | loss: 360.7669614CurrentTrain: epoch  1, batch    72 | loss: 333.1566373CurrentTrain: epoch  1, batch    73 | loss: 363.0590615CurrentTrain: epoch  1, batch    74 | loss: 348.5527387CurrentTrain: epoch  1, batch    75 | loss: 334.7215160CurrentTrain: epoch  1, batch    76 | loss: 334.2341169CurrentTrain: epoch  1, batch    77 | loss: 320.0508516CurrentTrain: epoch  1, batch    78 | loss: 348.9957669CurrentTrain: epoch  1, batch    79 | loss: 379.0699991CurrentTrain: epoch  1, batch    80 | loss: 445.8872145CurrentTrain: epoch  1, batch    81 | loss: 284.3447582CurrentTrain: epoch  1, batch    82 | loss: 316.8838785CurrentTrain: epoch  1, batch    83 | loss: 322.7136071CurrentTrain: epoch  1, batch    84 | loss: 365.2611052CurrentTrain: epoch  1, batch    85 | loss: 365.2546738CurrentTrain: epoch  1, batch    86 | loss: 395.9907523CurrentTrain: epoch  1, batch    87 | loss: 302.0132479CurrentTrain: epoch  1, batch    88 | loss: 334.4134199CurrentTrain: epoch  1, batch    89 | loss: 327.0210550CurrentTrain: epoch  1, batch    90 | loss: 415.6265476CurrentTrain: epoch  1, batch    91 | loss: 365.7130020CurrentTrain: epoch  1, batch    92 | loss: 429.8891919CurrentTrain: epoch  1, batch    93 | loss: 381.7831601CurrentTrain: epoch  1, batch    94 | loss: 334.5128332CurrentTrain: epoch  1, batch    95 | loss: 301.0410937CurrentTrain: epoch  2, batch     0 | loss: 394.1618193CurrentTrain: epoch  2, batch     1 | loss: 336.0323260CurrentTrain: epoch  2, batch     2 | loss: 381.9471009CurrentTrain: epoch  2, batch     3 | loss: 365.9523293CurrentTrain: epoch  2, batch     4 | loss: 241.7432664CurrentTrain: epoch  2, batch     5 | loss: 270.2912178CurrentTrain: epoch  2, batch     6 | loss: 283.3968698CurrentTrain: epoch  2, batch     7 | loss: 360.1708698CurrentTrain: epoch  2, batch     8 | loss: 362.3455005CurrentTrain: epoch  2, batch     9 | loss: 275.1862508CurrentTrain: epoch  2, batch    10 | loss: 319.5652283CurrentTrain: epoch  2, batch    11 | loss: 363.3432802CurrentTrain: epoch  2, batch    12 | loss: 350.4607451CurrentTrain: epoch  2, batch    13 | loss: 376.8891746CurrentTrain: epoch  2, batch    14 | loss: 406.9028220CurrentTrain: epoch  2, batch    15 | loss: 247.1193712CurrentTrain: epoch  2, batch    16 | loss: 457.8435629CurrentTrain: epoch  2, batch    17 | loss: 348.1429793CurrentTrain: epoch  2, batch    18 | loss: 410.8140242CurrentTrain: epoch  2, batch    19 | loss: 304.9249858CurrentTrain: epoch  2, batch    20 | loss: 410.5752391CurrentTrain: epoch  2, batch    21 | loss: 333.6840234CurrentTrain: epoch  2, batch    22 | loss: 422.9168341CurrentTrain: epoch  2, batch    23 | loss: 429.9497678CurrentTrain: epoch  2, batch    24 | loss: 332.8179622CurrentTrain: epoch  2, batch    25 | loss: 323.7368282CurrentTrain: epoch  2, batch    26 | loss: 392.4323538CurrentTrain: epoch  2, batch    27 | loss: 267.0894750CurrentTrain: epoch  2, batch    28 | loss: 538.8283595CurrentTrain: epoch  2, batch    29 | loss: 393.6285860CurrentTrain: epoch  2, batch    30 | loss: 421.3221492CurrentTrain: epoch  2, batch    31 | loss: 410.1431658CurrentTrain: epoch  2, batch    32 | loss: 375.9098997CurrentTrain: epoch  2, batch    33 | loss: 313.0726816CurrentTrain: epoch  2, batch    34 | loss: 231.7236827CurrentTrain: epoch  2, batch    35 | loss: 378.5670582CurrentTrain: epoch  2, batch    36 | loss: 412.3187477CurrentTrain: epoch  2, batch    37 | loss: 377.7321681CurrentTrain: epoch  2, batch    38 | loss: 332.6477088CurrentTrain: epoch  2, batch    39 | loss: 374.9744428CurrentTrain: epoch  2, batch    40 | loss: 361.3663359CurrentTrain: epoch  2, batch    41 | loss: 321.5664100CurrentTrain: epoch  2, batch    42 | loss: 318.5521914CurrentTrain: epoch  2, batch    43 | loss: 305.7122026CurrentTrain: epoch  2, batch    44 | loss: 290.4532070CurrentTrain: epoch  2, batch    45 | loss: 349.3600029CurrentTrain: epoch  2, batch    46 | loss: 330.9750608CurrentTrain: epoch  2, batch    47 | loss: 322.8915757CurrentTrain: epoch  2, batch    48 | loss: 359.5976901CurrentTrain: epoch  2, batch    49 | loss: 271.2661669CurrentTrain: epoch  2, batch    50 | loss: 359.6042177CurrentTrain: epoch  2, batch    51 | loss: 281.9451035CurrentTrain: epoch  2, batch    52 | loss: 290.4441787CurrentTrain: epoch  2, batch    53 | loss: 439.5401806CurrentTrain: epoch  2, batch    54 | loss: 288.5069903CurrentTrain: epoch  2, batch    55 | loss: 350.1359249CurrentTrain: epoch  2, batch    56 | loss: 423.8394249CurrentTrain: epoch  2, batch    57 | loss: 347.2679556CurrentTrain: epoch  2, batch    58 | loss: 337.3829228CurrentTrain: epoch  2, batch    59 | loss: 293.9544258CurrentTrain: epoch  2, batch    60 | loss: 393.9441419CurrentTrain: epoch  2, batch    61 | loss: 265.5833910CurrentTrain: epoch  2, batch    62 | loss: 304.7423820CurrentTrain: epoch  2, batch    63 | loss: 361.0490660CurrentTrain: epoch  2, batch    64 | loss: 397.3410341CurrentTrain: epoch  2, batch    65 | loss: 290.3624798CurrentTrain: epoch  2, batch    66 | loss: 358.4984073CurrentTrain: epoch  2, batch    67 | loss: 334.7481959CurrentTrain: epoch  2, batch    68 | loss: 399.5576680CurrentTrain: epoch  2, batch    69 | loss: 318.3451936CurrentTrain: epoch  2, batch    70 | loss: 312.9642970CurrentTrain: epoch  2, batch    71 | loss: 291.7879630CurrentTrain: epoch  2, batch    72 | loss: 395.8283321CurrentTrain: epoch  2, batch    73 | loss: 410.2989350CurrentTrain: epoch  2, batch    74 | loss: 397.8626560CurrentTrain: epoch  2, batch    75 | loss: 281.5259211CurrentTrain: epoch  2, batch    76 | loss: 409.0986002CurrentTrain: epoch  2, batch    77 | loss: 331.3266164CurrentTrain: epoch  2, batch    78 | loss: 333.3934078CurrentTrain: epoch  2, batch    79 | loss: 411.9960638CurrentTrain: epoch  2, batch    80 | loss: 344.6463819CurrentTrain: epoch  2, batch    81 | loss: 292.8007015CurrentTrain: epoch  2, batch    82 | loss: 328.3192886CurrentTrain: epoch  2, batch    83 | loss: 441.3618046CurrentTrain: epoch  2, batch    84 | loss: 439.3313377CurrentTrain: epoch  2, batch    85 | loss: 319.7436503CurrentTrain: epoch  2, batch    86 | loss: 361.5898577CurrentTrain: epoch  2, batch    87 | loss: 334.2593220CurrentTrain: epoch  2, batch    88 | loss: 344.6054885CurrentTrain: epoch  2, batch    89 | loss: 346.5252483CurrentTrain: epoch  2, batch    90 | loss: 315.7765793CurrentTrain: epoch  2, batch    91 | loss: 344.4137234CurrentTrain: epoch  2, batch    92 | loss: 306.5619224CurrentTrain: epoch  2, batch    93 | loss: 288.8721218CurrentTrain: epoch  2, batch    94 | loss: 460.1994007CurrentTrain: epoch  2, batch    95 | loss: 294.7300670CurrentTrain: epoch  3, batch     0 | loss: 310.6862674CurrentTrain: epoch  3, batch     1 | loss: 394.0126884CurrentTrain: epoch  3, batch     2 | loss: 311.9646615CurrentTrain: epoch  3, batch     3 | loss: 280.7011856CurrentTrain: epoch  3, batch     4 | loss: 344.7326548CurrentTrain: epoch  3, batch     5 | loss: 394.8656025CurrentTrain: epoch  3, batch     6 | loss: 290.5723236CurrentTrain: epoch  3, batch     7 | loss: 344.5296452CurrentTrain: epoch  3, batch     8 | loss: 457.6473611CurrentTrain: epoch  3, batch     9 | loss: 305.5821375CurrentTrain: epoch  3, batch    10 | loss: 330.6576195CurrentTrain: epoch  3, batch    11 | loss: 343.7525228CurrentTrain: epoch  3, batch    12 | loss: 362.8421828CurrentTrain: epoch  3, batch    13 | loss: 293.2732239CurrentTrain: epoch  3, batch    14 | loss: 273.9347650CurrentTrain: epoch  3, batch    15 | loss: 391.9880670CurrentTrain: epoch  3, batch    16 | loss: 416.7625725CurrentTrain: epoch  3, batch    17 | loss: 306.9785733CurrentTrain: epoch  3, batch    18 | loss: 294.7688417CurrentTrain: epoch  3, batch    19 | loss: 343.7781414CurrentTrain: epoch  3, batch    20 | loss: 305.1323363CurrentTrain: epoch  3, batch    21 | loss: 440.5185196CurrentTrain: epoch  3, batch    22 | loss: 348.2586017CurrentTrain: epoch  3, batch    23 | loss: 362.8492932CurrentTrain: epoch  3, batch    24 | loss: 343.9205687CurrentTrain: epoch  3, batch    25 | loss: 358.6626796CurrentTrain: epoch  3, batch    26 | loss: 298.3786372CurrentTrain: epoch  3, batch    27 | loss: 540.7252408CurrentTrain: epoch  3, batch    28 | loss: 318.1323963CurrentTrain: epoch  3, batch    29 | loss: 345.6226267CurrentTrain: epoch  3, batch    30 | loss: 330.4552201CurrentTrain: epoch  3, batch    31 | loss: 280.1682922CurrentTrain: epoch  3, batch    32 | loss: 326.8588450CurrentTrain: epoch  3, batch    33 | loss: 437.6086209CurrentTrain: epoch  3, batch    34 | loss: 318.2588114CurrentTrain: epoch  3, batch    35 | loss: 300.9560874CurrentTrain: epoch  3, batch    36 | loss: 335.9524175CurrentTrain: epoch  3, batch    37 | loss: 341.7599397CurrentTrain: epoch  3, batch    38 | loss: 375.9551724CurrentTrain: epoch  3, batch    39 | loss: 319.5376444CurrentTrain: epoch  3, batch    40 | loss: 392.2147982CurrentTrain: epoch  3, batch    41 | loss: 316.9814138CurrentTrain: epoch  3, batch    42 | loss: 362.4856639CurrentTrain: epoch  3, batch    43 | loss: 316.5530285CurrentTrain: epoch  3, batch    44 | loss: 282.4981770CurrentTrain: epoch  3, batch    45 | loss: 363.4022338CurrentTrain: epoch  3, batch    46 | loss: 375.6182140CurrentTrain: epoch  3, batch    47 | loss: 307.3444310CurrentTrain: epoch  3, batch    48 | loss: 307.6978484CurrentTrain: epoch  3, batch    49 | loss: 364.4101233CurrentTrain: epoch  3, batch    50 | loss: 393.6053681CurrentTrain: epoch  3, batch    51 | loss: 357.1958344CurrentTrain: epoch  3, batch    52 | loss: 405.6590813CurrentTrain: epoch  3, batch    53 | loss: 318.6669955CurrentTrain: epoch  3, batch    54 | loss: 319.4209969CurrentTrain: epoch  3, batch    55 | loss: 343.2909875CurrentTrain: epoch  3, batch    56 | loss: 376.2735695CurrentTrain: epoch  3, batch    57 | loss: 345.5372741CurrentTrain: epoch  3, batch    58 | loss: 317.9444149CurrentTrain: epoch  3, batch    59 | loss: 424.4582917CurrentTrain: epoch  3, batch    60 | loss: 343.8862824CurrentTrain: epoch  3, batch    61 | loss: 391.2134705CurrentTrain: epoch  3, batch    62 | loss: 375.8809062CurrentTrain: epoch  3, batch    63 | loss: 378.1692771CurrentTrain: epoch  3, batch    64 | loss: 273.9930635CurrentTrain: epoch  3, batch    65 | loss: 280.6910339CurrentTrain: epoch  3, batch    66 | loss: 351.8799971CurrentTrain: epoch  3, batch    67 | loss: 299.5388996CurrentTrain: epoch  3, batch    68 | loss: 307.7604544CurrentTrain: epoch  3, batch    69 | loss: 412.8514609CurrentTrain: epoch  3, batch    70 | loss: 315.9239356CurrentTrain: epoch  3, batch    71 | loss: 342.2268406CurrentTrain: epoch  3, batch    72 | loss: 395.6474486CurrentTrain: epoch  3, batch    73 | loss: 260.9465781CurrentTrain: epoch  3, batch    74 | loss: 360.4614901CurrentTrain: epoch  3, batch    75 | loss: 288.2444441CurrentTrain: epoch  3, batch    76 | loss: 343.6454199CurrentTrain: epoch  3, batch    77 | loss: 302.1487761CurrentTrain: epoch  3, batch    78 | loss: 422.3723039CurrentTrain: epoch  3, batch    79 | loss: 357.4131598CurrentTrain: epoch  3, batch    80 | loss: 320.1978363CurrentTrain: epoch  3, batch    81 | loss: 329.0038560CurrentTrain: epoch  3, batch    82 | loss: 302.5930581CurrentTrain: epoch  3, batch    83 | loss: 456.3231868CurrentTrain: epoch  3, batch    84 | loss: 330.8684954CurrentTrain: epoch  3, batch    85 | loss: 358.2319872CurrentTrain: epoch  3, batch    86 | loss: 326.3844183CurrentTrain: epoch  3, batch    87 | loss: 301.2018623CurrentTrain: epoch  3, batch    88 | loss: 393.9188951CurrentTrain: epoch  3, batch    89 | loss: 356.7483121CurrentTrain: epoch  3, batch    90 | loss: 314.8350945CurrentTrain: epoch  3, batch    91 | loss: 393.1679492CurrentTrain: epoch  3, batch    92 | loss: 334.8460255CurrentTrain: epoch  3, batch    93 | loss: 333.4777947CurrentTrain: epoch  3, batch    94 | loss: 285.7745361CurrentTrain: epoch  3, batch    95 | loss: 292.7101110CurrentTrain: epoch  4, batch     0 | loss: 376.5784003CurrentTrain: epoch  4, batch     1 | loss: 326.0270220CurrentTrain: epoch  4, batch     2 | loss: 302.4030774CurrentTrain: epoch  4, batch     3 | loss: 315.6142650CurrentTrain: epoch  4, batch     4 | loss: 302.5775417CurrentTrain: epoch  4, batch     5 | loss: 437.9723540CurrentTrain: epoch  4, batch     6 | loss: 313.2558561CurrentTrain: epoch  4, batch     7 | loss: 408.4195551CurrentTrain: epoch  4, batch     8 | loss: 444.6838407CurrentTrain: epoch  4, batch     9 | loss: 315.0345177CurrentTrain: epoch  4, batch    10 | loss: 342.6353054CurrentTrain: epoch  4, batch    11 | loss: 328.6339694CurrentTrain: epoch  4, batch    12 | loss: 436.7837145CurrentTrain: epoch  4, batch    13 | loss: 539.4080288CurrentTrain: epoch  4, batch    14 | loss: 279.1426265CurrentTrain: epoch  4, batch    15 | loss: 330.1152280CurrentTrain: epoch  4, batch    16 | loss: 315.5473061CurrentTrain: epoch  4, batch    17 | loss: 293.7345305CurrentTrain: epoch  4, batch    18 | loss: 379.7838212CurrentTrain: epoch  4, batch    19 | loss: 391.6576762CurrentTrain: epoch  4, batch    20 | loss: 329.0872324CurrentTrain: epoch  4, batch    21 | loss: 342.8041966CurrentTrain: epoch  4, batch    22 | loss: 356.7100980CurrentTrain: epoch  4, batch    23 | loss: 316.9305345CurrentTrain: epoch  4, batch    24 | loss: 296.5959508CurrentTrain: epoch  4, batch    25 | loss: 344.2669277CurrentTrain: epoch  4, batch    26 | loss: 394.0095755CurrentTrain: epoch  4, batch    27 | loss: 343.6225439CurrentTrain: epoch  4, batch    28 | loss: 345.4941380CurrentTrain: epoch  4, batch    29 | loss: 288.3347997CurrentTrain: epoch  4, batch    30 | loss: 326.6038900CurrentTrain: epoch  4, batch    31 | loss: 357.8689097CurrentTrain: epoch  4, batch    32 | loss: 312.1238598CurrentTrain: epoch  4, batch    33 | loss: 538.2158102CurrentTrain: epoch  4, batch    34 | loss: 359.7598378CurrentTrain: epoch  4, batch    35 | loss: 327.4401148CurrentTrain: epoch  4, batch    36 | loss: 342.0253827CurrentTrain: epoch  4, batch    37 | loss: 360.8725873CurrentTrain: epoch  4, batch    38 | loss: 344.7453259CurrentTrain: epoch  4, batch    39 | loss: 376.6740632CurrentTrain: epoch  4, batch    40 | loss: 346.8091126CurrentTrain: epoch  4, batch    41 | loss: 332.3688508CurrentTrain: epoch  4, batch    42 | loss: 288.3623716CurrentTrain: epoch  4, batch    43 | loss: 360.2103547CurrentTrain: epoch  4, batch    44 | loss: 373.7653623CurrentTrain: epoch  4, batch    45 | loss: 374.5405333CurrentTrain: epoch  4, batch    46 | loss: 312.7424093CurrentTrain: epoch  4, batch    47 | loss: 281.3672905CurrentTrain: epoch  4, batch    48 | loss: 299.8710613CurrentTrain: epoch  4, batch    49 | loss: 331.3815284CurrentTrain: epoch  4, batch    50 | loss: 314.7077090CurrentTrain: epoch  4, batch    51 | loss: 358.2625505CurrentTrain: epoch  4, batch    52 | loss: 301.9862457CurrentTrain: epoch  4, batch    53 | loss: 254.8866726CurrentTrain: epoch  4, batch    54 | loss: 392.4786112CurrentTrain: epoch  4, batch    55 | loss: 332.2606671CurrentTrain: epoch  4, batch    56 | loss: 327.9385568CurrentTrain: epoch  4, batch    57 | loss: 317.1640532CurrentTrain: epoch  4, batch    58 | loss: 343.7235219CurrentTrain: epoch  4, batch    59 | loss: 359.2185168CurrentTrain: epoch  4, batch    60 | loss: 343.3200665CurrentTrain: epoch  4, batch    61 | loss: 361.5437362CurrentTrain: epoch  4, batch    62 | loss: 375.5418950CurrentTrain: epoch  4, batch    63 | loss: 442.7729936CurrentTrain: epoch  4, batch    64 | loss: 299.8787091CurrentTrain: epoch  4, batch    65 | loss: 301.3485799CurrentTrain: epoch  4, batch    66 | loss: 334.1672343CurrentTrain: epoch  4, batch    67 | loss: 357.9648406CurrentTrain: epoch  4, batch    68 | loss: 393.3950063CurrentTrain: epoch  4, batch    69 | loss: 377.3453839CurrentTrain: epoch  4, batch    70 | loss: 391.0309410CurrentTrain: epoch  4, batch    71 | loss: 303.0112330CurrentTrain: epoch  4, batch    72 | loss: 317.1072152CurrentTrain: epoch  4, batch    73 | loss: 276.4380807CurrentTrain: epoch  4, batch    74 | loss: 286.6127638CurrentTrain: epoch  4, batch    75 | loss: 375.9099100CurrentTrain: epoch  4, batch    76 | loss: 357.5930289CurrentTrain: epoch  4, batch    77 | loss: 440.8612684CurrentTrain: epoch  4, batch    78 | loss: 342.5384345CurrentTrain: epoch  4, batch    79 | loss: 359.9067887CurrentTrain: epoch  4, batch    80 | loss: 301.6679723CurrentTrain: epoch  4, batch    81 | loss: 311.8223029CurrentTrain: epoch  4, batch    82 | loss: 407.6805690CurrentTrain: epoch  4, batch    83 | loss: 375.2180603CurrentTrain: epoch  4, batch    84 | loss: 346.7546170CurrentTrain: epoch  4, batch    85 | loss: 289.9140717CurrentTrain: epoch  4, batch    86 | loss: 375.9129389CurrentTrain: epoch  4, batch    87 | loss: 343.4494958CurrentTrain: epoch  4, batch    88 | loss: 358.1177106CurrentTrain: epoch  4, batch    89 | loss: 252.1504436CurrentTrain: epoch  4, batch    90 | loss: 304.1455675CurrentTrain: epoch  4, batch    91 | loss: 326.2090589CurrentTrain: epoch  4, batch    92 | loss: 391.3008494CurrentTrain: epoch  4, batch    93 | loss: 390.6780780CurrentTrain: epoch  4, batch    94 | loss: 303.8785438CurrentTrain: epoch  4, batch    95 | loss: 265.4108778CurrentTrain: epoch  5, batch     0 | loss: 345.8284038CurrentTrain: epoch  5, batch     1 | loss: 299.8767796CurrentTrain: epoch  5, batch     2 | loss: 342.8363049CurrentTrain: epoch  5, batch     3 | loss: 327.3991152CurrentTrain: epoch  5, batch     4 | loss: 286.2448466CurrentTrain: epoch  5, batch     5 | loss: 343.1363675CurrentTrain: epoch  5, batch     6 | loss: 375.0572034CurrentTrain: epoch  5, batch     7 | loss: 376.3045405CurrentTrain: epoch  5, batch     8 | loss: 330.2386169CurrentTrain: epoch  5, batch     9 | loss: 297.8692385CurrentTrain: epoch  5, batch    10 | loss: 271.4635829CurrentTrain: epoch  5, batch    11 | loss: 373.5577840CurrentTrain: epoch  5, batch    12 | loss: 332.0112653CurrentTrain: epoch  5, batch    13 | loss: 341.2306990CurrentTrain: epoch  5, batch    14 | loss: 298.4243546CurrentTrain: epoch  5, batch    15 | loss: 407.1272281CurrentTrain: epoch  5, batch    16 | loss: 345.1018088CurrentTrain: epoch  5, batch    17 | loss: 269.0789757CurrentTrain: epoch  5, batch    18 | loss: 375.4673526CurrentTrain: epoch  5, batch    19 | loss: 315.7121371CurrentTrain: epoch  5, batch    20 | loss: 323.1146127CurrentTrain: epoch  5, batch    21 | loss: 342.5463451CurrentTrain: epoch  5, batch    22 | loss: 421.0799708CurrentTrain: epoch  5, batch    23 | loss: 393.8673789CurrentTrain: epoch  5, batch    24 | loss: 390.9603832CurrentTrain: epoch  5, batch    25 | loss: 340.1912202CurrentTrain: epoch  5, batch    26 | loss: 269.4777247CurrentTrain: epoch  5, batch    27 | loss: 419.5784770CurrentTrain: epoch  5, batch    28 | loss: 390.6032846CurrentTrain: epoch  5, batch    29 | loss: 301.3060474CurrentTrain: epoch  5, batch    30 | loss: 341.4595665CurrentTrain: epoch  5, batch    31 | loss: 357.5112881CurrentTrain: epoch  5, batch    32 | loss: 329.8910553CurrentTrain: epoch  5, batch    33 | loss: 358.3493205CurrentTrain: epoch  5, batch    34 | loss: 390.8241226CurrentTrain: epoch  5, batch    35 | loss: 271.5706664CurrentTrain: epoch  5, batch    36 | loss: 439.0933808CurrentTrain: epoch  5, batch    37 | loss: 437.4213020CurrentTrain: epoch  5, batch    38 | loss: 456.3322879CurrentTrain: epoch  5, batch    39 | loss: 315.0330160CurrentTrain: epoch  5, batch    40 | loss: 314.0837265CurrentTrain: epoch  5, batch    41 | loss: 373.6724250CurrentTrain: epoch  5, batch    42 | loss: 301.6695672CurrentTrain: epoch  5, batch    43 | loss: 328.6704144CurrentTrain: epoch  5, batch    44 | loss: 339.8278556CurrentTrain: epoch  5, batch    45 | loss: 357.1261191CurrentTrain: epoch  5, batch    46 | loss: 291.1578071CurrentTrain: epoch  5, batch    47 | loss: 391.4532546CurrentTrain: epoch  5, batch    48 | loss: 311.6908989CurrentTrain: epoch  5, batch    49 | loss: 300.5551700CurrentTrain: epoch  5, batch    50 | loss: 343.6069335CurrentTrain: epoch  5, batch    51 | loss: 516.3104660CurrentTrain: epoch  5, batch    52 | loss: 310.2340406CurrentTrain: epoch  5, batch    53 | loss: 327.3178824CurrentTrain: epoch  5, batch    54 | loss: 328.5916403CurrentTrain: epoch  5, batch    55 | loss: 373.5725516CurrentTrain: epoch  5, batch    56 | loss: 300.1719255CurrentTrain: epoch  5, batch    57 | loss: 285.0200234CurrentTrain: epoch  5, batch    58 | loss: 537.3027125CurrentTrain: epoch  5, batch    59 | loss: 293.1381560CurrentTrain: epoch  5, batch    60 | loss: 324.6468037CurrentTrain: epoch  5, batch    61 | loss: 283.6063840CurrentTrain: epoch  5, batch    62 | loss: 300.6813088CurrentTrain: epoch  5, batch    63 | loss: 285.9381245CurrentTrain: epoch  5, batch    64 | loss: 264.0285765CurrentTrain: epoch  5, batch    65 | loss: 438.2418695CurrentTrain: epoch  5, batch    66 | loss: 373.5074638CurrentTrain: epoch  5, batch    67 | loss: 322.3117152CurrentTrain: epoch  5, batch    68 | loss: 297.2493741CurrentTrain: epoch  5, batch    69 | loss: 744.8086906CurrentTrain: epoch  5, batch    70 | loss: 407.6368105CurrentTrain: epoch  5, batch    71 | loss: 367.0931848CurrentTrain: epoch  5, batch    72 | loss: 290.7961581CurrentTrain: epoch  5, batch    73 | loss: 373.1935750CurrentTrain: epoch  5, batch    74 | loss: 373.5142871CurrentTrain: epoch  5, batch    75 | loss: 325.4731369CurrentTrain: epoch  5, batch    76 | loss: 300.2253884CurrentTrain: epoch  5, batch    77 | loss: 291.8781979CurrentTrain: epoch  5, batch    78 | loss: 345.3995278CurrentTrain: epoch  5, batch    79 | loss: 455.2888683CurrentTrain: epoch  5, batch    80 | loss: 301.7298716CurrentTrain: epoch  5, batch    81 | loss: 317.0821987CurrentTrain: epoch  5, batch    82 | loss: 276.8964958CurrentTrain: epoch  5, batch    83 | loss: 392.3872262CurrentTrain: epoch  5, batch    84 | loss: 313.3102431CurrentTrain: epoch  5, batch    85 | loss: 300.5283232CurrentTrain: epoch  5, batch    86 | loss: 407.8147856CurrentTrain: epoch  5, batch    87 | loss: 402.3162990CurrentTrain: epoch  5, batch    88 | loss: 316.5508482CurrentTrain: epoch  5, batch    89 | loss: 358.0306703CurrentTrain: epoch  5, batch    90 | loss: 261.0067918CurrentTrain: epoch  5, batch    91 | loss: 373.6042994CurrentTrain: epoch  5, batch    92 | loss: 313.9003109CurrentTrain: epoch  5, batch    93 | loss: 286.3439394CurrentTrain: epoch  5, batch    94 | loss: 341.9770947CurrentTrain: epoch  5, batch    95 | loss: 294.1257732CurrentTrain: epoch  6, batch     0 | loss: 373.3220117CurrentTrain: epoch  6, batch     1 | loss: 311.9614985CurrentTrain: epoch  6, batch     2 | loss: 390.0803885CurrentTrain: epoch  6, batch     3 | loss: 328.5829809CurrentTrain: epoch  6, batch     4 | loss: 343.2761388CurrentTrain: epoch  6, batch     5 | loss: 272.8322370CurrentTrain: epoch  6, batch     6 | loss: 391.4767083CurrentTrain: epoch  6, batch     7 | loss: 373.7174916CurrentTrain: epoch  6, batch     8 | loss: 301.0333845CurrentTrain: epoch  6, batch     9 | loss: 358.3771055CurrentTrain: epoch  6, batch    10 | loss: 311.2637209CurrentTrain: epoch  6, batch    11 | loss: 340.0384324CurrentTrain: epoch  6, batch    12 | loss: 455.5738204CurrentTrain: epoch  6, batch    13 | loss: 457.0925571CurrentTrain: epoch  6, batch    14 | loss: 326.2929631CurrentTrain: epoch  6, batch    15 | loss: 330.0242131CurrentTrain: epoch  6, batch    16 | loss: 392.9072508CurrentTrain: epoch  6, batch    17 | loss: 301.3975497CurrentTrain: epoch  6, batch    18 | loss: 361.0942858CurrentTrain: epoch  6, batch    19 | loss: 282.0711407CurrentTrain: epoch  6, batch    20 | loss: 378.5580170CurrentTrain: epoch  6, batch    21 | loss: 264.7142840CurrentTrain: epoch  6, batch    22 | loss: 312.6883840CurrentTrain: epoch  6, batch    23 | loss: 374.2947921CurrentTrain: epoch  6, batch    24 | loss: 328.7302236CurrentTrain: epoch  6, batch    25 | loss: 341.7291697CurrentTrain: epoch  6, batch    26 | loss: 295.3362545CurrentTrain: epoch  6, batch    27 | loss: 328.9727083CurrentTrain: epoch  6, batch    28 | loss: 340.5839522CurrentTrain: epoch  6, batch    29 | loss: 498.4327446CurrentTrain: epoch  6, batch    30 | loss: 359.2029137CurrentTrain: epoch  6, batch    31 | loss: 246.2937633CurrentTrain: epoch  6, batch    32 | loss: 298.8133456CurrentTrain: epoch  6, batch    33 | loss: 256.7741846CurrentTrain: epoch  6, batch    34 | loss: 407.2171422CurrentTrain: epoch  6, batch    35 | loss: 389.9698274CurrentTrain: epoch  6, batch    36 | loss: 372.8190048CurrentTrain: epoch  6, batch    37 | loss: 328.6793501CurrentTrain: epoch  6, batch    38 | loss: 324.6838220CurrentTrain: epoch  6, batch    39 | loss: 455.3990192CurrentTrain: epoch  6, batch    40 | loss: 280.3752948CurrentTrain: epoch  6, batch    41 | loss: 340.0424462CurrentTrain: epoch  6, batch    42 | loss: 326.6257308CurrentTrain: epoch  6, batch    43 | loss: 287.2518158CurrentTrain: epoch  6, batch    44 | loss: 276.6320901CurrentTrain: epoch  6, batch    45 | loss: 342.8923373CurrentTrain: epoch  6, batch    46 | loss: 341.3185371CurrentTrain: epoch  6, batch    47 | loss: 402.4874272CurrentTrain: epoch  6, batch    48 | loss: 277.2804327CurrentTrain: epoch  6, batch    49 | loss: 357.8227141CurrentTrain: epoch  6, batch    50 | loss: 537.1972677CurrentTrain: epoch  6, batch    51 | loss: 345.7176663CurrentTrain: epoch  6, batch    52 | loss: 376.3197964CurrentTrain: epoch  6, batch    53 | loss: 270.3914009CurrentTrain: epoch  6, batch    54 | loss: 287.7625573CurrentTrain: epoch  6, batch    55 | loss: 300.4578834CurrentTrain: epoch  6, batch    56 | loss: 252.5806815CurrentTrain: epoch  6, batch    57 | loss: 408.2172504CurrentTrain: epoch  6, batch    58 | loss: 328.5451786CurrentTrain: epoch  6, batch    59 | loss: 310.5512771CurrentTrain: epoch  6, batch    60 | loss: 536.8894444CurrentTrain: epoch  6, batch    61 | loss: 341.2109300CurrentTrain: epoch  6, batch    62 | loss: 410.1959008CurrentTrain: epoch  6, batch    63 | loss: 232.0680309CurrentTrain: epoch  6, batch    64 | loss: 348.2801919CurrentTrain: epoch  6, batch    65 | loss: 327.1291395CurrentTrain: epoch  6, batch    66 | loss: 357.2895430CurrentTrain: epoch  6, batch    67 | loss: 325.1798248CurrentTrain: epoch  6, batch    68 | loss: 305.9267231CurrentTrain: epoch  6, batch    69 | loss: 345.0428236CurrentTrain: epoch  6, batch    70 | loss: 276.4994573CurrentTrain: epoch  6, batch    71 | loss: 358.8265429CurrentTrain: epoch  6, batch    72 | loss: 287.0007434CurrentTrain: epoch  6, batch    73 | loss: 373.7958590CurrentTrain: epoch  6, batch    74 | loss: 440.7434735CurrentTrain: epoch  6, batch    75 | loss: 409.4772177CurrentTrain: epoch  6, batch    76 | loss: 312.7872149CurrentTrain: epoch  6, batch    77 | loss: 280.3963052CurrentTrain: epoch  6, batch    78 | loss: 327.4937127CurrentTrain: epoch  6, batch    79 | loss: 300.6105766CurrentTrain: epoch  6, batch    80 | loss: 536.4990212CurrentTrain: epoch  6, batch    81 | loss: 359.0472896CurrentTrain: epoch  6, batch    82 | loss: 391.0175692CurrentTrain: epoch  6, batch    83 | loss: 346.2048250CurrentTrain: epoch  6, batch    84 | loss: 342.3615598CurrentTrain: epoch  6, batch    85 | loss: 376.7365729CurrentTrain: epoch  6, batch    86 | loss: 456.0863591CurrentTrain: epoch  6, batch    87 | loss: 302.4703995CurrentTrain: epoch  6, batch    88 | loss: 340.6002967CurrentTrain: epoch  6, batch    89 | loss: 292.3611014CurrentTrain: epoch  6, batch    90 | loss: 420.6852940CurrentTrain: epoch  6, batch    91 | loss: 340.1422221CurrentTrain: epoch  6, batch    92 | loss: 340.6838097CurrentTrain: epoch  6, batch    93 | loss: 373.6570342CurrentTrain: epoch  6, batch    94 | loss: 309.8986199CurrentTrain: epoch  6, batch    95 | loss: 290.6201976CurrentTrain: epoch  7, batch     0 | loss: 299.3125349CurrentTrain: epoch  7, batch     1 | loss: 301.2470162CurrentTrain: epoch  7, batch     2 | loss: 341.0637857CurrentTrain: epoch  7, batch     3 | loss: 315.7623507CurrentTrain: epoch  7, batch     4 | loss: 344.2910837CurrentTrain: epoch  7, batch     5 | loss: 270.5531469CurrentTrain: epoch  7, batch     6 | loss: 356.5391859CurrentTrain: epoch  7, batch     7 | loss: 284.8819735CurrentTrain: epoch  7, batch     8 | loss: 256.9604437CurrentTrain: epoch  7, batch     9 | loss: 456.6428664CurrentTrain: epoch  7, batch    10 | loss: 305.6937654CurrentTrain: epoch  7, batch    11 | loss: 339.7603066CurrentTrain: epoch  7, batch    12 | loss: 385.6015063CurrentTrain: epoch  7, batch    13 | loss: 441.8157421CurrentTrain: epoch  7, batch    14 | loss: 314.7941544CurrentTrain: epoch  7, batch    15 | loss: 297.9105207CurrentTrain: epoch  7, batch    16 | loss: 309.6346650CurrentTrain: epoch  7, batch    17 | loss: 324.0556243CurrentTrain: epoch  7, batch    18 | loss: 374.4894621CurrentTrain: epoch  7, batch    19 | loss: 418.5275998CurrentTrain: epoch  7, batch    20 | loss: 324.8405688CurrentTrain: epoch  7, batch    21 | loss: 407.0760600CurrentTrain: epoch  7, batch    22 | loss: 329.6043990CurrentTrain: epoch  7, batch    23 | loss: 300.1126054CurrentTrain: epoch  7, batch    24 | loss: 326.0527138CurrentTrain: epoch  7, batch    25 | loss: 284.8964496CurrentTrain: epoch  7, batch    26 | loss: 515.3698942CurrentTrain: epoch  7, batch    27 | loss: 264.0998195CurrentTrain: epoch  7, batch    28 | loss: 373.9732890CurrentTrain: epoch  7, batch    29 | loss: 376.5415709CurrentTrain: epoch  7, batch    30 | loss: 373.4408565CurrentTrain: epoch  7, batch    31 | loss: 355.7501036CurrentTrain: epoch  7, batch    32 | loss: 314.0312003CurrentTrain: epoch  7, batch    33 | loss: 281.4701146CurrentTrain: epoch  7, batch    34 | loss: 339.5306523CurrentTrain: epoch  7, batch    35 | loss: 420.2506016CurrentTrain: epoch  7, batch    36 | loss: 298.6937990CurrentTrain: epoch  7, batch    37 | loss: 315.5063954CurrentTrain: epoch  7, batch    38 | loss: 326.7948851CurrentTrain: epoch  7, batch    39 | loss: 373.2408219CurrentTrain: epoch  7, batch    40 | loss: 314.0451953CurrentTrain: epoch  7, batch    41 | loss: 455.9742958CurrentTrain: epoch  7, batch    42 | loss: 356.5863637CurrentTrain: epoch  7, batch    43 | loss: 375.2747218CurrentTrain: epoch  7, batch    44 | loss: 313.6402256CurrentTrain: epoch  7, batch    45 | loss: 311.7233698CurrentTrain: epoch  7, batch    46 | loss: 342.4855025CurrentTrain: epoch  7, batch    47 | loss: 456.9922444CurrentTrain: epoch  7, batch    48 | loss: 235.9311870CurrentTrain: epoch  7, batch    49 | loss: 339.8633558CurrentTrain: epoch  7, batch    50 | loss: 407.8019417CurrentTrain: epoch  7, batch    51 | loss: 372.6480865CurrentTrain: epoch  7, batch    52 | loss: 355.5372777CurrentTrain: epoch  7, batch    53 | loss: 329.4602238CurrentTrain: epoch  7, batch    54 | loss: 418.9012892CurrentTrain: epoch  7, batch    55 | loss: 328.6676484CurrentTrain: epoch  7, batch    56 | loss: 373.2283066CurrentTrain: epoch  7, batch    57 | loss: 295.8372535CurrentTrain: epoch  7, batch    58 | loss: 328.8767831CurrentTrain: epoch  7, batch    59 | loss: 286.9289856CurrentTrain: epoch  7, batch    60 | loss: 373.1727009CurrentTrain: epoch  7, batch    61 | loss: 407.1607263CurrentTrain: epoch  7, batch    62 | loss: 360.5009096CurrentTrain: epoch  7, batch    63 | loss: 326.6436084CurrentTrain: epoch  7, batch    64 | loss: 455.1761369CurrentTrain: epoch  7, batch    65 | loss: 295.5869353CurrentTrain: epoch  7, batch    66 | loss: 408.4330964CurrentTrain: epoch  7, batch    67 | loss: 537.6139392CurrentTrain: epoch  7, batch    68 | loss: 258.1449809CurrentTrain: epoch  7, batch    69 | loss: 297.5485562CurrentTrain: epoch  7, batch    70 | loss: 324.9241445CurrentTrain: epoch  7, batch    71 | loss: 375.4553170CurrentTrain: epoch  7, batch    72 | loss: 290.4189814CurrentTrain: epoch  7, batch    73 | loss: 271.8482312CurrentTrain: epoch  7, batch    74 | loss: 342.6978993CurrentTrain: epoch  7, batch    75 | loss: 409.8028431CurrentTrain: epoch  7, batch    76 | loss: 291.0800488CurrentTrain: epoch  7, batch    77 | loss: 297.0579788CurrentTrain: epoch  7, batch    78 | loss: 325.9372095CurrentTrain: epoch  7, batch    79 | loss: 372.0563104CurrentTrain: epoch  7, batch    80 | loss: 309.9344469CurrentTrain: epoch  7, batch    81 | loss: 339.5076285CurrentTrain: epoch  7, batch    82 | loss: 356.4155732CurrentTrain: epoch  7, batch    83 | loss: 356.9252009CurrentTrain: epoch  7, batch    84 | loss: 372.6798272CurrentTrain: epoch  7, batch    85 | loss: 284.4022956CurrentTrain: epoch  7, batch    86 | loss: 373.1785650CurrentTrain: epoch  7, batch    87 | loss: 272.0239550CurrentTrain: epoch  7, batch    88 | loss: 339.7977015CurrentTrain: epoch  7, batch    89 | loss: 380.7259937CurrentTrain: epoch  7, batch    90 | loss: 326.3963648CurrentTrain: epoch  7, batch    91 | loss: 313.2581874CurrentTrain: epoch  7, batch    92 | loss: 357.8702181CurrentTrain: epoch  7, batch    93 | loss: 344.0714400CurrentTrain: epoch  7, batch    94 | loss: 414.2007033CurrentTrain: epoch  7, batch    95 | loss: 238.8500017CurrentTrain: epoch  8, batch     0 | loss: 284.8887999CurrentTrain: epoch  8, batch     1 | loss: 358.0910520CurrentTrain: epoch  8, batch     2 | loss: 277.2788373CurrentTrain: epoch  8, batch     3 | loss: 312.7928295CurrentTrain: epoch  8, batch     4 | loss: 310.4899819CurrentTrain: epoch  8, batch     5 | loss: 265.1595504CurrentTrain: epoch  8, batch     6 | loss: 294.8892164CurrentTrain: epoch  8, batch     7 | loss: 356.7060997CurrentTrain: epoch  8, batch     8 | loss: 515.3310441CurrentTrain: epoch  8, batch     9 | loss: 390.4727821CurrentTrain: epoch  8, batch    10 | loss: 310.5815149CurrentTrain: epoch  8, batch    11 | loss: 325.1714711CurrentTrain: epoch  8, batch    12 | loss: 436.5548897CurrentTrain: epoch  8, batch    13 | loss: 374.2837744CurrentTrain: epoch  8, batch    14 | loss: 287.2196977CurrentTrain: epoch  8, batch    15 | loss: 313.4634789CurrentTrain: epoch  8, batch    16 | loss: 340.0062145CurrentTrain: epoch  8, batch    17 | loss: 515.3562236CurrentTrain: epoch  8, batch    18 | loss: 284.4845137CurrentTrain: epoch  8, batch    19 | loss: 391.5019652CurrentTrain: epoch  8, batch    20 | loss: 292.8653511CurrentTrain: epoch  8, batch    21 | loss: 298.2945709CurrentTrain: epoch  8, batch    22 | loss: 455.3975538CurrentTrain: epoch  8, batch    23 | loss: 326.6046510CurrentTrain: epoch  8, batch    24 | loss: 357.1564711CurrentTrain: epoch  8, batch    25 | loss: 313.7233225CurrentTrain: epoch  8, batch    26 | loss: 318.7777320CurrentTrain: epoch  8, batch    27 | loss: 279.2731448CurrentTrain: epoch  8, batch    28 | loss: 297.9364052CurrentTrain: epoch  8, batch    29 | loss: 313.2473750CurrentTrain: epoch  8, batch    30 | loss: 373.3753015CurrentTrain: epoch  8, batch    31 | loss: 266.8742423CurrentTrain: epoch  8, batch    32 | loss: 332.1486281CurrentTrain: epoch  8, batch    33 | loss: 373.1201176CurrentTrain: epoch  8, batch    34 | loss: 324.1238866CurrentTrain: epoch  8, batch    35 | loss: 364.3120344CurrentTrain: epoch  8, batch    36 | loss: 326.6094008CurrentTrain: epoch  8, batch    37 | loss: 325.4761505CurrentTrain: epoch  8, batch    38 | loss: 313.2973404CurrentTrain: epoch  8, batch    39 | loss: 312.7971175CurrentTrain: epoch  8, batch    40 | loss: 339.4161669CurrentTrain: epoch  8, batch    41 | loss: 372.4405355CurrentTrain: epoch  8, batch    42 | loss: 297.5503239CurrentTrain: epoch  8, batch    43 | loss: 300.4279614CurrentTrain: epoch  8, batch    44 | loss: 310.1486817CurrentTrain: epoch  8, batch    45 | loss: 325.4793236CurrentTrain: epoch  8, batch    46 | loss: 340.2704044CurrentTrain: epoch  8, batch    47 | loss: 344.3603820CurrentTrain: epoch  8, batch    48 | loss: 374.8127669CurrentTrain: epoch  8, batch    49 | loss: 390.7793256CurrentTrain: epoch  8, batch    50 | loss: 339.8030715CurrentTrain: epoch  8, batch    51 | loss: 257.3398926CurrentTrain: epoch  8, batch    52 | loss: 455.2529516CurrentTrain: epoch  8, batch    53 | loss: 340.6345307CurrentTrain: epoch  8, batch    54 | loss: 340.6859973CurrentTrain: epoch  8, batch    55 | loss: 356.6468247CurrentTrain: epoch  8, batch    56 | loss: 341.7919875CurrentTrain: epoch  8, batch    57 | loss: 295.4054054CurrentTrain: epoch  8, batch    58 | loss: 344.9039571CurrentTrain: epoch  8, batch    59 | loss: 356.3295881CurrentTrain: epoch  8, batch    60 | loss: 389.7004880CurrentTrain: epoch  8, batch    61 | loss: 277.0747631CurrentTrain: epoch  8, batch    62 | loss: 310.2227867CurrentTrain: epoch  8, batch    63 | loss: 437.9912012CurrentTrain: epoch  8, batch    64 | loss: 378.3354980CurrentTrain: epoch  8, batch    65 | loss: 331.4435466CurrentTrain: epoch  8, batch    66 | loss: 389.6098479CurrentTrain: epoch  8, batch    67 | loss: 439.8327981CurrentTrain: epoch  8, batch    68 | loss: 325.5064777CurrentTrain: epoch  8, batch    69 | loss: 325.2116839CurrentTrain: epoch  8, batch    70 | loss: 408.9156441CurrentTrain: epoch  8, batch    71 | loss: 403.5653141CurrentTrain: epoch  8, batch    72 | loss: 311.7150866CurrentTrain: epoch  8, batch    73 | loss: 356.4383181CurrentTrain: epoch  8, batch    74 | loss: 329.5418865CurrentTrain: epoch  8, batch    75 | loss: 324.4647948CurrentTrain: epoch  8, batch    76 | loss: 324.7198094CurrentTrain: epoch  8, batch    77 | loss: 340.8732389CurrentTrain: epoch  8, batch    78 | loss: 329.6324762CurrentTrain: epoch  8, batch    79 | loss: 295.6555980CurrentTrain: epoch  8, batch    80 | loss: 298.2245606CurrentTrain: epoch  8, batch    81 | loss: 410.4665268CurrentTrain: epoch  8, batch    82 | loss: 243.9968355CurrentTrain: epoch  8, batch    83 | loss: 312.9826681CurrentTrain: epoch  8, batch    84 | loss: 284.0894563CurrentTrain: epoch  8, batch    85 | loss: 407.3442423CurrentTrain: epoch  8, batch    86 | loss: 297.8005867CurrentTrain: epoch  8, batch    87 | loss: 355.4401557CurrentTrain: epoch  8, batch    88 | loss: 340.3741573CurrentTrain: epoch  8, batch    89 | loss: 340.9687453CurrentTrain: epoch  8, batch    90 | loss: 372.1971668CurrentTrain: epoch  8, batch    91 | loss: 408.4377502CurrentTrain: epoch  8, batch    92 | loss: 324.0144668CurrentTrain: epoch  8, batch    93 | loss: 355.3437087CurrentTrain: epoch  8, batch    94 | loss: 390.5815034CurrentTrain: epoch  8, batch    95 | loss: 430.2193597CurrentTrain: epoch  9, batch     0 | loss: 342.9580462CurrentTrain: epoch  9, batch     1 | loss: 310.3738640CurrentTrain: epoch  9, batch     2 | loss: 295.9090320CurrentTrain: epoch  9, batch     3 | loss: 315.2134689CurrentTrain: epoch  9, batch     4 | loss: 310.8772909CurrentTrain: epoch  9, batch     5 | loss: 297.9315552CurrentTrain: epoch  9, batch     6 | loss: 313.2437108CurrentTrain: epoch  9, batch     7 | loss: 324.2044062CurrentTrain: epoch  9, batch     8 | loss: 298.5015165CurrentTrain: epoch  9, batch     9 | loss: 298.4448150CurrentTrain: epoch  9, batch    10 | loss: 406.0172633CurrentTrain: epoch  9, batch    11 | loss: 272.4139295CurrentTrain: epoch  9, batch    12 | loss: 356.3808389CurrentTrain: epoch  9, batch    13 | loss: 308.7932855CurrentTrain: epoch  9, batch    14 | loss: 358.1282617CurrentTrain: epoch  9, batch    15 | loss: 355.7982118CurrentTrain: epoch  9, batch    16 | loss: 328.4101910CurrentTrain: epoch  9, batch    17 | loss: 344.4120494CurrentTrain: epoch  9, batch    18 | loss: 340.2753935CurrentTrain: epoch  9, batch    19 | loss: 283.6371579CurrentTrain: epoch  9, batch    20 | loss: 262.4835134CurrentTrain: epoch  9, batch    21 | loss: 324.9656101CurrentTrain: epoch  9, batch    22 | loss: 373.0463007CurrentTrain: epoch  9, batch    23 | loss: 372.1611773CurrentTrain: epoch  9, batch    24 | loss: 324.3075812CurrentTrain: epoch  9, batch    25 | loss: 309.3890895CurrentTrain: epoch  9, batch    26 | loss: 326.5104885CurrentTrain: epoch  9, batch    27 | loss: 372.1456685CurrentTrain: epoch  9, batch    28 | loss: 344.5434751CurrentTrain: epoch  9, batch    29 | loss: 357.4903436CurrentTrain: epoch  9, batch    30 | loss: 455.1218560CurrentTrain: epoch  9, batch    31 | loss: 373.2815808CurrentTrain: epoch  9, batch    32 | loss: 407.0523579CurrentTrain: epoch  9, batch    33 | loss: 363.6372927CurrentTrain: epoch  9, batch    34 | loss: 339.4910754CurrentTrain: epoch  9, batch    35 | loss: 283.4161969CurrentTrain: epoch  9, batch    36 | loss: 314.0173145CurrentTrain: epoch  9, batch    37 | loss: 328.9004228CurrentTrain: epoch  9, batch    38 | loss: 309.3934697CurrentTrain: epoch  9, batch    39 | loss: 285.8623187CurrentTrain: epoch  9, batch    40 | loss: 437.3488914CurrentTrain: epoch  9, batch    41 | loss: 299.8701003CurrentTrain: epoch  9, batch    42 | loss: 328.6504351CurrentTrain: epoch  9, batch    43 | loss: 436.7298217CurrentTrain: epoch  9, batch    44 | loss: 339.3711484CurrentTrain: epoch  9, batch    45 | loss: 436.7585635CurrentTrain: epoch  9, batch    46 | loss: 355.9412920CurrentTrain: epoch  9, batch    47 | loss: 374.4195338CurrentTrain: epoch  9, batch    48 | loss: 312.9658813CurrentTrain: epoch  9, batch    49 | loss: 355.3158934CurrentTrain: epoch  9, batch    50 | loss: 408.8803019CurrentTrain: epoch  9, batch    51 | loss: 287.7705721CurrentTrain: epoch  9, batch    52 | loss: 374.0899508CurrentTrain: epoch  9, batch    53 | loss: 324.3743905CurrentTrain: epoch  9, batch    54 | loss: 249.6603514CurrentTrain: epoch  9, batch    55 | loss: 308.9573900CurrentTrain: epoch  9, batch    56 | loss: 257.4643887CurrentTrain: epoch  9, batch    57 | loss: 455.3103249CurrentTrain: epoch  9, batch    58 | loss: 339.9626887CurrentTrain: epoch  9, batch    59 | loss: 373.3833706CurrentTrain: epoch  9, batch    60 | loss: 324.2956326CurrentTrain: epoch  9, batch    61 | loss: 438.9646948CurrentTrain: epoch  9, batch    62 | loss: 407.3646516CurrentTrain: epoch  9, batch    63 | loss: 358.9285523CurrentTrain: epoch  9, batch    64 | loss: 266.1651390CurrentTrain: epoch  9, batch    65 | loss: 536.8310464CurrentTrain: epoch  9, batch    66 | loss: 329.1962006CurrentTrain: epoch  9, batch    67 | loss: 298.3831839CurrentTrain: epoch  9, batch    68 | loss: 251.5484726CurrentTrain: epoch  9, batch    69 | loss: 372.9312637CurrentTrain: epoch  9, batch    70 | loss: 372.9083905CurrentTrain: epoch  9, batch    71 | loss: 339.3456494CurrentTrain: epoch  9, batch    72 | loss: 312.7573963CurrentTrain: epoch  9, batch    73 | loss: 298.0550954CurrentTrain: epoch  9, batch    74 | loss: 356.6949647CurrentTrain: epoch  9, batch    75 | loss: 358.3779747CurrentTrain: epoch  9, batch    76 | loss: 389.6110422CurrentTrain: epoch  9, batch    77 | loss: 455.6948729CurrentTrain: epoch  9, batch    78 | loss: 329.7183367CurrentTrain: epoch  9, batch    79 | loss: 324.0893725CurrentTrain: epoch  9, batch    80 | loss: 317.4022418CurrentTrain: epoch  9, batch    81 | loss: 296.4298700CurrentTrain: epoch  9, batch    82 | loss: 340.0870140CurrentTrain: epoch  9, batch    83 | loss: 340.7158493CurrentTrain: epoch  9, batch    84 | loss: 391.7891317CurrentTrain: epoch  9, batch    85 | loss: 436.7949471CurrentTrain: epoch  9, batch    86 | loss: 324.3800323CurrentTrain: epoch  9, batch    87 | loss: 455.1133585CurrentTrain: epoch  9, batch    88 | loss: 536.3632278CurrentTrain: epoch  9, batch    89 | loss: 340.5563899CurrentTrain: epoch  9, batch    90 | loss: 262.5766002CurrentTrain: epoch  9, batch    91 | loss: 305.2999070CurrentTrain: epoch  9, batch    92 | loss: 372.6871706CurrentTrain: epoch  9, batch    93 | loss: 324.7778953CurrentTrain: epoch  9, batch    94 | loss: 325.3774227CurrentTrain: epoch  9, batch    95 | loss: 313.7236735

F1 score per class: {32: 0.6309523809523809, 6: 0.7654320987654321, 19: 0.48, 24: 0.7597765363128491, 26: 0.88268156424581, 29: 0.8900523560209425}
Micro-average F1 score: 0.7809734513274337
Weighted-average F1 score: 0.7887836952597372
F1 score per class: {32: 0.6956521739130435, 6: 0.8235294117647058, 19: 0.48484848484848486, 24: 0.7752808988764045, 26: 0.9637305699481865, 29: 0.8631578947368421}
Micro-average F1 score: 0.8143459915611815
Weighted-average F1 score: 0.8165991449972303
F1 score per class: {32: 0.6956521739130435, 6: 0.8235294117647058, 19: 0.48484848484848486, 24: 0.7752808988764045, 26: 0.9583333333333334, 29: 0.8631578947368421}
Micro-average F1 score: 0.8130939809926082
Weighted-average F1 score: 0.8150987319587282

F1 score per class: {32: 0.6309523809523809, 6: 0.7654320987654321, 19: 0.48, 24: 0.7597765363128491, 26: 0.88268156424581, 29: 0.8900523560209425}
Micro-average F1 score: 0.7809734513274337
Weighted-average F1 score: 0.7887836952597372
F1 score per class: {32: 0.6956521739130435, 6: 0.8235294117647058, 19: 0.48484848484848486, 24: 0.7752808988764045, 26: 0.9637305699481865, 29: 0.8631578947368421}
Micro-average F1 score: 0.8143459915611815
Weighted-average F1 score: 0.8165991449972303
F1 score per class: {32: 0.6956521739130435, 6: 0.8235294117647058, 19: 0.48484848484848486, 24: 0.7752808988764045, 26: 0.9583333333333334, 29: 0.8631578947368421}
Micro-average F1 score: 0.8130939809926082
Weighted-average F1 score: 0.8150987319587282
cur_acc:  ['0.7810']
his_acc:  ['0.7810']
cur_acc des:  ['0.8143']
his_acc des:  ['0.8143']
cur_acc rrf:  ['0.8131']
his_acc rrf:  ['0.8131']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 389.4861225CurrentTrain: epoch  0, batch     1 | loss: 308.5706111CurrentTrain: epoch  0, batch     2 | loss: 348.6294608CurrentTrain: epoch  0, batch     3 | loss: 36.2887122CurrentTrain: epoch  1, batch     0 | loss: 340.4550476CurrentTrain: epoch  1, batch     1 | loss: 254.1057722CurrentTrain: epoch  1, batch     2 | loss: 398.7637803CurrentTrain: epoch  1, batch     3 | loss: 29.5653865CurrentTrain: epoch  2, batch     0 | loss: 323.9100371CurrentTrain: epoch  2, batch     1 | loss: 297.5955143CurrentTrain: epoch  2, batch     2 | loss: 310.1658327CurrentTrain: epoch  2, batch     3 | loss: 32.3999217CurrentTrain: epoch  3, batch     0 | loss: 323.5227778CurrentTrain: epoch  3, batch     1 | loss: 297.0313323CurrentTrain: epoch  3, batch     2 | loss: 331.6858984CurrentTrain: epoch  3, batch     3 | loss: 20.7043146CurrentTrain: epoch  4, batch     0 | loss: 349.0312133CurrentTrain: epoch  4, batch     1 | loss: 277.3559742CurrentTrain: epoch  4, batch     2 | loss: 352.2167001CurrentTrain: epoch  4, batch     3 | loss: 30.4425416CurrentTrain: epoch  5, batch     0 | loss: 331.7902048CurrentTrain: epoch  5, batch     1 | loss: 306.6023290CurrentTrain: epoch  5, batch     2 | loss: 347.5336220CurrentTrain: epoch  5, batch     3 | loss: 29.4507426CurrentTrain: epoch  6, batch     0 | loss: 287.4260645CurrentTrain: epoch  6, batch     1 | loss: 303.1606148CurrentTrain: epoch  6, batch     2 | loss: 304.2499735CurrentTrain: epoch  6, batch     3 | loss: 54.8799759CurrentTrain: epoch  7, batch     0 | loss: 313.6556301CurrentTrain: epoch  7, batch     1 | loss: 329.0418852CurrentTrain: epoch  7, batch     2 | loss: 273.3335031CurrentTrain: epoch  7, batch     3 | loss: 54.7003563CurrentTrain: epoch  8, batch     0 | loss: 272.7735495CurrentTrain: epoch  8, batch     1 | loss: 358.4290627CurrentTrain: epoch  8, batch     2 | loss: 300.1179169CurrentTrain: epoch  8, batch     3 | loss: 18.1457179CurrentTrain: epoch  9, batch     0 | loss: 272.6100405CurrentTrain: epoch  9, batch     1 | loss: 299.2117352CurrentTrain: epoch  9, batch     2 | loss: 314.0506039CurrentTrain: epoch  9, batch     3 | loss: 54.7491180
MemoryTrain:  epoch  0, batch     0 | loss: 3.4987163MemoryTrain:  epoch  1, batch     0 | loss: 2.4329680MemoryTrain:  epoch  2, batch     0 | loss: 2.0158438MemoryTrain:  epoch  3, batch     0 | loss: 1.7000052MemoryTrain:  epoch  4, batch     0 | loss: 1.4164154MemoryTrain:  epoch  5, batch     0 | loss: 1.0006695MemoryTrain:  epoch  6, batch     0 | loss: 0.7544749MemoryTrain:  epoch  7, batch     0 | loss: 0.6531315MemoryTrain:  epoch  8, batch     0 | loss: 0.5210759MemoryTrain:  epoch  9, batch     0 | loss: 0.4177911

F1 score per class: {6: 0.0, 7: 0.75, 40: 0.96, 9: 0.0, 19: 0.0, 26: 0.5, 27: 0.6666666666666666, 31: 0.2564102564102564}
Micro-average F1 score: 0.39814814814814814
Weighted-average F1 score: 0.31392911010558067
F1 score per class: {6: 0.0, 7: 0.0, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6666666666666666, 27: 1.0, 31: 0.2564102564102564}
Micro-average F1 score: 0.4017857142857143
Weighted-average F1 score: 0.3309749074454957
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.72, 27: 1.0, 31: 0.2564102564102564}
Micro-average F1 score: 0.42857142857142855
Weighted-average F1 score: 0.3532433057138939

F1 score per class: {32: 0.5034013605442177, 6: 0.11320754716981132, 7: 0.96, 40: 0.6160714285714286, 9: 0.32, 19: 0.770949720670391, 24: 0.38461538461538464, 26: 0.8888888888888888, 27: 0.6666666666666666, 29: 0.8854166666666666, 31: 0.18181818181818182}
Micro-average F1 score: 0.6509671993271657
Weighted-average F1 score: 0.6419105673842405
F1 score per class: {32: 0.6557377049180327, 6: 0.0, 7: 0.9803921568627451, 40: 0.6752136752136753, 9: 0.3333333333333333, 19: 0.770949720670391, 24: 0.5333333333333333, 26: 0.9479166666666666, 27: 0.8, 29: 0.8677248677248677, 31: 0.19801980198019803}
Micro-average F1 score: 0.698464025869038
Weighted-average F1 score: 0.6921601000565594
F1 score per class: {32: 0.6703910614525139, 6: 0.0975609756097561, 7: 0.9803921568627451, 40: 0.6521739130434783, 9: 0.35294117647058826, 19: 0.770949720670391, 24: 0.5625, 26: 0.9417989417989417, 27: 0.8, 29: 0.8631578947368421, 31: 0.18691588785046728}
Micro-average F1 score: 0.6936135812449474
Weighted-average F1 score: 0.684626651773555
cur_acc:  ['0.7810', '0.3981']
his_acc:  ['0.7810', '0.6510']
cur_acc des:  ['0.8143', '0.4018']
his_acc des:  ['0.8143', '0.6985']
cur_acc rrf:  ['0.8131', '0.4286']
his_acc rrf:  ['0.8131', '0.6936']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 353.8359719CurrentTrain: epoch  0, batch     1 | loss: 423.6637624CurrentTrain: epoch  0, batch     2 | loss: 374.7294038CurrentTrain: epoch  0, batch     3 | loss: 262.1846139CurrentTrain: epoch  1, batch     0 | loss: 400.2806531CurrentTrain: epoch  1, batch     1 | loss: 394.4220136CurrentTrain: epoch  1, batch     2 | loss: 316.8037438CurrentTrain: epoch  1, batch     3 | loss: 264.6823487CurrentTrain: epoch  2, batch     0 | loss: 423.6183651CurrentTrain: epoch  2, batch     1 | loss: 338.3414019CurrentTrain: epoch  2, batch     2 | loss: 284.1229262CurrentTrain: epoch  2, batch     3 | loss: 265.5161677CurrentTrain: epoch  3, batch     0 | loss: 352.4786122CurrentTrain: epoch  3, batch     1 | loss: 328.9085587CurrentTrain: epoch  3, batch     2 | loss: 363.2584912CurrentTrain: epoch  3, batch     3 | loss: 347.3072065CurrentTrain: epoch  4, batch     0 | loss: 337.6293394CurrentTrain: epoch  4, batch     1 | loss: 298.7923219CurrentTrain: epoch  4, batch     2 | loss: 362.8048350CurrentTrain: epoch  4, batch     3 | loss: 313.3240703CurrentTrain: epoch  5, batch     0 | loss: 304.0140831CurrentTrain: epoch  5, batch     1 | loss: 519.8977764CurrentTrain: epoch  5, batch     2 | loss: 310.8314185CurrentTrain: epoch  5, batch     3 | loss: 270.1033682CurrentTrain: epoch  6, batch     0 | loss: 290.9907417CurrentTrain: epoch  6, batch     1 | loss: 342.7938188CurrentTrain: epoch  6, batch     2 | loss: 342.2687751CurrentTrain: epoch  6, batch     3 | loss: 370.6956229CurrentTrain: epoch  7, batch     0 | loss: 332.8148727CurrentTrain: epoch  7, batch     1 | loss: 317.5609956CurrentTrain: epoch  7, batch     2 | loss: 359.7886844CurrentTrain: epoch  7, batch     3 | loss: 282.7915723CurrentTrain: epoch  8, batch     0 | loss: 396.0112699CurrentTrain: epoch  8, batch     1 | loss: 333.1374447CurrentTrain: epoch  8, batch     2 | loss: 357.3567689CurrentTrain: epoch  8, batch     3 | loss: 233.6598948CurrentTrain: epoch  9, batch     0 | loss: 300.8310814CurrentTrain: epoch  9, batch     1 | loss: 437.8156323CurrentTrain: epoch  9, batch     2 | loss: 331.9951932CurrentTrain: epoch  9, batch     3 | loss: 256.3100987
MemoryTrain:  epoch  0, batch     0 | loss: 2.3948867MemoryTrain:  epoch  1, batch     0 | loss: 1.8579229MemoryTrain:  epoch  2, batch     0 | loss: 1.3749797MemoryTrain:  epoch  3, batch     0 | loss: 1.1349986MemoryTrain:  epoch  4, batch     0 | loss: 0.8866788MemoryTrain:  epoch  5, batch     0 | loss: 0.6634449MemoryTrain:  epoch  6, batch     0 | loss: 0.5269368MemoryTrain:  epoch  7, batch     0 | loss: 0.4110510MemoryTrain:  epoch  8, batch     0 | loss: 0.3369066MemoryTrain:  epoch  9, batch     0 | loss: 0.2997241

F1 score per class: {0: 0.927536231884058, 32: 0.8095238095238095, 4: 0.5714285714285714, 13: 0.6222222222222222, 21: 0.6944444444444444, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.75
Weighted-average F1 score: 0.7175888322627453
F1 score per class: {0: 0.9577464788732394, 32: 0.9637305699481865, 4: 0.5714285714285714, 40: 0.8301886792452831, 13: 0.8674698795180723, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8841607565011821
Weighted-average F1 score: 0.8513608228363323
F1 score per class: {0: 0.9577464788732394, 32: 0.9528795811518325, 4: 0.5714285714285714, 40: 0.8148148148148148, 13: 0.7792207792207793, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8523809523809524
Weighted-average F1 score: 0.8126782171336407

F1 score per class: {32: 0.927536231884058, 0: 0.8095238095238095, 4: 0.5034965034965035, 6: 0.0625, 7: 0.96, 40: 0.13793103448275862, 9: 0.638095238095238, 13: 0.5714285714285714, 19: 0.6944444444444444, 21: 0.09090909090909091, 23: 0.7379679144385026, 24: 0.5714285714285714, 26: 0.8961748633879781, 27: 0.6666666666666666, 29: 0.8736842105263158, 31: 0.3652173913043478}
Micro-average F1 score: 0.6889890534449453
Weighted-average F1 score: 0.682223287718834
F1 score per class: {32: 0.9577464788732394, 0: 0.9587628865979382, 4: 0.7103825136612022, 6: 0.0, 7: 0.9803921568627451, 40: 0.23529411764705882, 9: 0.6757990867579908, 13: 0.6984126984126984, 19: 0.8674698795180723, 21: 0.09090909090909091, 23: 0.7379679144385026, 24: 0.5454545454545454, 26: 0.9430051813471503, 27: 0.6666666666666666, 29: 0.8979591836734694, 31: 0.3275862068965517}
Micro-average F1 score: 0.7576668671076368
Weighted-average F1 score: 0.7530834659058983
F1 score per class: {32: 0.9577464788732394, 0: 0.9479166666666666, 4: 0.6627906976744186, 6: 0.0, 7: 0.9803921568627451, 40: 0.14285714285714285, 9: 0.647887323943662, 13: 0.6984126984126984, 19: 0.7792207792207793, 21: 0.09090909090909091, 23: 0.7379679144385026, 24: 0.48484848484848486, 26: 0.9368421052631579, 27: 0.6666666666666666, 29: 0.8979591836734694, 31: 0.31496062992125984}
Micro-average F1 score: 0.7330917874396136
Weighted-average F1 score: 0.7217142897281619
cur_acc:  ['0.7810', '0.3981', '0.7500']
his_acc:  ['0.7810', '0.6510', '0.6890']
cur_acc des:  ['0.8143', '0.4018', '0.8842']
his_acc des:  ['0.8143', '0.6985', '0.7577']
cur_acc rrf:  ['0.8131', '0.4286', '0.8524']
his_acc rrf:  ['0.8131', '0.6936', '0.7331']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 346.5935375CurrentTrain: epoch  0, batch     1 | loss: 331.9143778CurrentTrain: epoch  0, batch     2 | loss: 398.1439756CurrentTrain: epoch  0, batch     3 | loss: 400.8263824CurrentTrain: epoch  0, batch     4 | loss: 295.5641902CurrentTrain: epoch  1, batch     0 | loss: 443.5123094CurrentTrain: epoch  1, batch     1 | loss: 462.5139779CurrentTrain: epoch  1, batch     2 | loss: 350.3403367CurrentTrain: epoch  1, batch     3 | loss: 308.9893342CurrentTrain: epoch  1, batch     4 | loss: 204.2805686CurrentTrain: epoch  2, batch     0 | loss: 294.8005694CurrentTrain: epoch  2, batch     1 | loss: 397.0945910CurrentTrain: epoch  2, batch     2 | loss: 352.8261099CurrentTrain: epoch  2, batch     3 | loss: 376.1601967CurrentTrain: epoch  2, batch     4 | loss: 286.8410024CurrentTrain: epoch  3, batch     0 | loss: 334.1514796CurrentTrain: epoch  3, batch     1 | loss: 379.2947089CurrentTrain: epoch  3, batch     2 | loss: 359.4681763CurrentTrain: epoch  3, batch     3 | loss: 346.1604127CurrentTrain: epoch  3, batch     4 | loss: 326.7132660CurrentTrain: epoch  4, batch     0 | loss: 440.8250859CurrentTrain: epoch  4, batch     1 | loss: 424.7623762CurrentTrain: epoch  4, batch     2 | loss: 456.6318977CurrentTrain: epoch  4, batch     3 | loss: 290.2397274CurrentTrain: epoch  4, batch     4 | loss: 185.2251610CurrentTrain: epoch  5, batch     0 | loss: 376.1570091CurrentTrain: epoch  5, batch     1 | loss: 303.7405609CurrentTrain: epoch  5, batch     2 | loss: 376.9939420CurrentTrain: epoch  5, batch     3 | loss: 392.2153078CurrentTrain: epoch  5, batch     4 | loss: 237.9069376CurrentTrain: epoch  6, batch     0 | loss: 360.1653936CurrentTrain: epoch  6, batch     1 | loss: 358.2185051CurrentTrain: epoch  6, batch     2 | loss: 345.8577224CurrentTrain: epoch  6, batch     3 | loss: 327.1589907CurrentTrain: epoch  6, batch     4 | loss: 341.4271930CurrentTrain: epoch  7, batch     0 | loss: 407.8944563CurrentTrain: epoch  7, batch     1 | loss: 373.4372493CurrentTrain: epoch  7, batch     2 | loss: 314.9755260CurrentTrain: epoch  7, batch     3 | loss: 300.1228914CurrentTrain: epoch  7, batch     4 | loss: 271.0555903CurrentTrain: epoch  8, batch     0 | loss: 375.4430873CurrentTrain: epoch  8, batch     1 | loss: 314.1073448CurrentTrain: epoch  8, batch     2 | loss: 341.0085635CurrentTrain: epoch  8, batch     3 | loss: 373.1021348CurrentTrain: epoch  8, batch     4 | loss: 221.8906716CurrentTrain: epoch  9, batch     0 | loss: 345.4723306CurrentTrain: epoch  9, batch     1 | loss: 339.8137185CurrentTrain: epoch  9, batch     2 | loss: 355.9377091CurrentTrain: epoch  9, batch     3 | loss: 357.1822527CurrentTrain: epoch  9, batch     4 | loss: 221.4383922
MemoryTrain:  epoch  0, batch     0 | loss: 1.0827004MemoryTrain:  epoch  1, batch     0 | loss: 0.8968270MemoryTrain:  epoch  2, batch     0 | loss: 0.5219843MemoryTrain:  epoch  3, batch     0 | loss: 0.3910106MemoryTrain:  epoch  4, batch     0 | loss: 0.2871709MemoryTrain:  epoch  5, batch     0 | loss: 0.2574630MemoryTrain:  epoch  6, batch     0 | loss: 0.1846304MemoryTrain:  epoch  7, batch     0 | loss: 0.1558444MemoryTrain:  epoch  8, batch     0 | loss: 0.1288891MemoryTrain:  epoch  9, batch     0 | loss: 0.1055231

F1 score per class: {5: 0.9637305699481865, 6: 0.0, 10: 0.27586206896551724, 13: 0.0, 16: 0.7843137254901961, 17: 0.8, 18: 0.6037735849056604}
Micro-average F1 score: 0.6726057906458798
Weighted-average F1 score: 0.7223797786266722
F1 score per class: {5: 0.99, 6: 0.0, 40: 0.6754966887417219, 10: 0.0, 13: 0.9310344827586207, 16: 0.875, 17: 0.9577464788732394, 18: 0.0}
Micro-average F1 score: 0.8273244781783681
Weighted-average F1 score: 0.789672019620555
F1 score per class: {5: 0.99, 6: 0.0, 40: 0.6842105263157895, 10: 0.0, 13: 0.9310344827586207, 16: 0.875, 17: 0.8787878787878788, 18: 0.0}
Micro-average F1 score: 0.8214971209213052
Weighted-average F1 score: 0.7847255566143302

F1 score per class: {0: 0.8253968253968254, 4: 0.7577639751552795, 5: 0.9587628865979382, 6: 0.4172661870503597, 7: 0.0, 9: 0.96, 10: 0.27350427350427353, 13: 0.07692307692307693, 16: 0.7843137254901961, 17: 0.5454545454545454, 18: 0.6037735849056604, 19: 0.6355140186915887, 21: 0.5306122448979592, 23: 0.8536585365853658, 24: 0.10526315789473684, 26: 0.7555555555555555, 27: 0.5517241379310345, 29: 0.8961748633879781, 31: 0.4, 32: 0.8617021276595744, 40: 0.32323232323232326}
Micro-average F1 score: 0.6852138073158166
Weighted-average F1 score: 0.7125403549493056
F1 score per class: {0: 0.9428571428571428, 4: 0.93048128342246, 5: 0.9611650485436893, 6: 0.6938775510204082, 7: 0.0, 9: 0.9803921568627451, 10: 0.6710526315789473, 13: 0.12903225806451613, 16: 0.9, 17: 0.5, 18: 0.9066666666666666, 19: 0.6575342465753424, 21: 0.6666666666666666, 23: 0.9438202247191011, 24: 0.08333333333333333, 26: 0.7624309392265194, 27: 0.5555555555555556, 29: 0.9430051813471503, 31: 0.6666666666666666, 32: 0.8865979381443299, 40: 0.30357142857142855}
Micro-average F1 score: 0.7730398899587345
Weighted-average F1 score: 0.7726430672042767
F1 score per class: {0: 0.9428571428571428, 4: 0.9417989417989417, 5: 0.9565217391304348, 6: 0.7103825136612022, 7: 0.0, 9: 0.9803921568627451, 10: 0.6753246753246753, 13: 0.11764705882352941, 16: 0.9, 17: 0.4375, 18: 0.8787878787878788, 19: 0.6513761467889908, 21: 0.6779661016949152, 23: 0.9318181818181818, 24: 0.08333333333333333, 26: 0.7624309392265194, 27: 0.5294117647058824, 29: 0.9368421052631579, 31: 0.6666666666666666, 32: 0.8923076923076924, 40: 0.2956521739130435}
Micro-average F1 score: 0.7690179806362379
Weighted-average F1 score: 0.7648917287971471
cur_acc:  ['0.7810', '0.3981', '0.7500', '0.6726']
his_acc:  ['0.7810', '0.6510', '0.6890', '0.6852']
cur_acc des:  ['0.8143', '0.4018', '0.8842', '0.8273']
his_acc des:  ['0.8143', '0.6985', '0.7577', '0.7730']
cur_acc rrf:  ['0.8131', '0.4286', '0.8524', '0.8215']
his_acc rrf:  ['0.8131', '0.6936', '0.7331', '0.7690']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 316.9128876CurrentTrain: epoch  0, batch     1 | loss: 324.6617153CurrentTrain: epoch  0, batch     2 | loss: 401.6403453CurrentTrain: epoch  0, batch     3 | loss: 252.7988230CurrentTrain: epoch  1, batch     0 | loss: 321.2699459CurrentTrain: epoch  1, batch     1 | loss: 361.3762323CurrentTrain: epoch  1, batch     2 | loss: 357.3785210CurrentTrain: epoch  1, batch     3 | loss: 224.4808895CurrentTrain: epoch  2, batch     0 | loss: 381.3436809CurrentTrain: epoch  2, batch     1 | loss: 345.7099333CurrentTrain: epoch  2, batch     2 | loss: 264.6691018CurrentTrain: epoch  2, batch     3 | loss: 255.7640737CurrentTrain: epoch  3, batch     0 | loss: 397.4437269CurrentTrain: epoch  3, batch     1 | loss: 292.2226821CurrentTrain: epoch  3, batch     2 | loss: 438.1266232CurrentTrain: epoch  3, batch     3 | loss: 177.4855320CurrentTrain: epoch  4, batch     0 | loss: 326.3440691CurrentTrain: epoch  4, batch     1 | loss: 301.2608180CurrentTrain: epoch  4, batch     2 | loss: 327.3372407CurrentTrain: epoch  4, batch     3 | loss: 270.3564962CurrentTrain: epoch  5, batch     0 | loss: 314.3024535CurrentTrain: epoch  5, batch     1 | loss: 345.1037388CurrentTrain: epoch  5, batch     2 | loss: 315.4759875CurrentTrain: epoch  5, batch     3 | loss: 227.8936894CurrentTrain: epoch  6, batch     0 | loss: 295.8445662CurrentTrain: epoch  6, batch     1 | loss: 357.5578042CurrentTrain: epoch  6, batch     2 | loss: 299.8214762CurrentTrain: epoch  6, batch     3 | loss: 269.8154613CurrentTrain: epoch  7, batch     0 | loss: 390.9953932CurrentTrain: epoch  7, batch     1 | loss: 340.0567543CurrentTrain: epoch  7, batch     2 | loss: 287.5301640CurrentTrain: epoch  7, batch     3 | loss: 213.5045472CurrentTrain: epoch  8, batch     0 | loss: 267.8263445CurrentTrain: epoch  8, batch     1 | loss: 314.2725732CurrentTrain: epoch  8, batch     2 | loss: 437.0052543CurrentTrain: epoch  8, batch     3 | loss: 254.6298703CurrentTrain: epoch  9, batch     0 | loss: 309.7610875CurrentTrain: epoch  9, batch     1 | loss: 374.5536320CurrentTrain: epoch  9, batch     2 | loss: 389.8137212CurrentTrain: epoch  9, batch     3 | loss: 199.1400948
MemoryTrain:  epoch  0, batch     0 | loss: 1.1145791MemoryTrain:  epoch  1, batch     0 | loss: 0.8822285MemoryTrain:  epoch  2, batch     0 | loss: 0.6313135MemoryTrain:  epoch  3, batch     0 | loss: 0.4866542MemoryTrain:  epoch  4, batch     0 | loss: 0.3825278MemoryTrain:  epoch  5, batch     0 | loss: 0.3007756MemoryTrain:  epoch  6, batch     0 | loss: 0.2245619MemoryTrain:  epoch  7, batch     0 | loss: 0.1708720MemoryTrain:  epoch  8, batch     0 | loss: 0.1554839MemoryTrain:  epoch  9, batch     0 | loss: 0.1366578

F1 score per class: {32: 0.0, 35: 0.0, 5: 0.8235294117647058, 37: 0.0, 38: 0.0, 13: 0.44776119402985076, 15: 0.0, 18: 0.0, 23: 0.26666666666666666, 25: 0.5714285714285714, 27: 0.5263157894736842}
Micro-average F1 score: 0.43389830508474575
Weighted-average F1 score: 0.36019752555131124
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.75, 6: 0.0, 38: 0.0, 13: 0.0, 15: 0.6493506493506493, 18: 0.0, 21: 0.92, 23: 0.7755102040816326, 25: 0.88}
Micro-average F1 score: 0.726790450928382
Weighted-average F1 score: 0.6513815567549334
F1 score per class: {32: 0.0, 35: 0.0, 5: 0.0, 37: 0.75, 6: 0.0, 38: 0.0, 13: 0.0, 15: 0.5753424657534246, 18: 0.0, 21: 0.7586206896551724, 23: 0.7755102040816326, 25: 0.88}
Micro-average F1 score: 0.6469002695417789
Weighted-average F1 score: 0.5454230844912363

F1 score per class: {0: 0.8253968253968254, 4: 0.717948717948718, 5: 0.8691588785046729, 6: 0.43795620437956206, 7: 0.0, 9: 0.96, 10: 0.3333333333333333, 13: 0.12121212121212122, 15: 0.6363636363636364, 16: 0.8727272727272727, 17: 0.5, 18: 0.19047619047619047, 19: 0.4632768361581921, 21: 0.3, 23: 0.8470588235294118, 24: 0.10526315789473684, 25: 0.44776119402985076, 26: 0.7640449438202247, 27: 0.4444444444444444, 29: 0.8961748633879781, 31: 0.6666666666666666, 32: 0.7134502923976608, 35: 0.26666666666666666, 37: 0.41025641025641024, 38: 0.4444444444444444, 40: 0.2619047619047619}
Micro-average F1 score: 0.6058661778185152
Weighted-average F1 score: 0.6475922961900882
F1 score per class: {0: 0.9428571428571428, 4: 0.9010989010989011, 5: 0.8722466960352423, 6: 0.696969696969697, 7: 0.0, 9: 0.9803921568627451, 10: 0.5611510791366906, 13: 0.14285714285714285, 15: 0.6, 16: 0.9, 17: 0.5, 18: 0.5555555555555556, 19: 0.5578947368421052, 21: 0.5901639344262295, 23: 0.8863636363636364, 24: 0.08695652173913043, 25: 0.6493506493506493, 26: 0.7540983606557377, 27: 0.4827586206896552, 29: 0.9312169312169312, 31: 0.8, 32: 0.8673469387755102, 35: 0.8679245283018868, 37: 0.6129032258064516, 38: 0.5116279069767442, 40: 0.4247787610619469}
Micro-average F1 score: 0.7259786476868327
Weighted-average F1 score: 0.7293679554589828
F1 score per class: {0: 0.9295774647887324, 4: 0.8636363636363636, 5: 0.8646288209606987, 6: 0.6666666666666666, 7: 0.0, 9: 0.9803921568627451, 10: 0.49624060150375937, 13: 0.1111111111111111, 15: 0.5217391304347826, 16: 0.9122807017543859, 17: 0.48, 18: 0.5384615384615384, 19: 0.5578947368421052, 21: 0.5396825396825397, 23: 0.8470588235294118, 24: 0.09523809523809523, 25: 0.5753424657534246, 26: 0.7540983606557377, 27: 0.4827586206896552, 29: 0.9197860962566845, 31: 0.8, 32: 0.8349514563106796, 35: 0.7252747252747253, 37: 0.5671641791044776, 38: 0.5057471264367817, 40: 0.42857142857142855}
Micro-average F1 score: 0.6959297685554668
Weighted-average F1 score: 0.6971692138901163
cur_acc:  ['0.7810', '0.3981', '0.7500', '0.6726', '0.4339']
his_acc:  ['0.7810', '0.6510', '0.6890', '0.6852', '0.6059']
cur_acc des:  ['0.8143', '0.4018', '0.8842', '0.8273', '0.7268']
his_acc des:  ['0.8143', '0.6985', '0.7577', '0.7730', '0.7260']
cur_acc rrf:  ['0.8131', '0.4286', '0.8524', '0.8215', '0.6469']
his_acc rrf:  ['0.8131', '0.6936', '0.7331', '0.7690', '0.6959']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 420.3877527CurrentTrain: epoch  0, batch     1 | loss: 316.1667168CurrentTrain: epoch  0, batch     2 | loss: 378.2890693CurrentTrain: epoch  0, batch     3 | loss: 367.2796830CurrentTrain: epoch  0, batch     4 | loss: 87.1910463CurrentTrain: epoch  1, batch     0 | loss: 330.8130007CurrentTrain: epoch  1, batch     1 | loss: 392.2209774CurrentTrain: epoch  1, batch     2 | loss: 311.9351317CurrentTrain: epoch  1, batch     3 | loss: 374.0307158CurrentTrain: epoch  1, batch     4 | loss: 120.0857875CurrentTrain: epoch  2, batch     0 | loss: 368.7829183CurrentTrain: epoch  2, batch     1 | loss: 365.2491832CurrentTrain: epoch  2, batch     2 | loss: 262.7863014CurrentTrain: epoch  2, batch     3 | loss: 461.0466725CurrentTrain: epoch  2, batch     4 | loss: 81.4874663CurrentTrain: epoch  3, batch     0 | loss: 410.7888175CurrentTrain: epoch  3, batch     1 | loss: 448.0904481CurrentTrain: epoch  3, batch     2 | loss: 281.5474891CurrentTrain: epoch  3, batch     3 | loss: 348.9267599CurrentTrain: epoch  3, batch     4 | loss: 48.6835676CurrentTrain: epoch  4, batch     0 | loss: 302.5063200CurrentTrain: epoch  4, batch     1 | loss: 360.0880915CurrentTrain: epoch  4, batch     2 | loss: 378.9742366CurrentTrain: epoch  4, batch     3 | loss: 348.2340925CurrentTrain: epoch  4, batch     4 | loss: 120.0800112CurrentTrain: epoch  5, batch     0 | loss: 412.2103211CurrentTrain: epoch  5, batch     1 | loss: 318.3402580CurrentTrain: epoch  5, batch     2 | loss: 291.8666307CurrentTrain: epoch  5, batch     3 | loss: 360.9183604CurrentTrain: epoch  5, batch     4 | loss: 120.0256810CurrentTrain: epoch  6, batch     0 | loss: 290.4504960CurrentTrain: epoch  6, batch     1 | loss: 377.5790765CurrentTrain: epoch  6, batch     2 | loss: 359.0189557CurrentTrain: epoch  6, batch     3 | loss: 377.4484771CurrentTrain: epoch  6, batch     4 | loss: 76.3340650CurrentTrain: epoch  7, batch     0 | loss: 314.4913526CurrentTrain: epoch  7, batch     1 | loss: 332.2251843CurrentTrain: epoch  7, batch     2 | loss: 358.9114294CurrentTrain: epoch  7, batch     3 | loss: 359.1726486CurrentTrain: epoch  7, batch     4 | loss: 119.7999625CurrentTrain: epoch  8, batch     0 | loss: 301.6313808CurrentTrain: epoch  8, batch     1 | loss: 392.2696422CurrentTrain: epoch  8, batch     2 | loss: 409.1113048CurrentTrain: epoch  8, batch     3 | loss: 298.3817605CurrentTrain: epoch  8, batch     4 | loss: 75.8358270CurrentTrain: epoch  9, batch     0 | loss: 314.9952070CurrentTrain: epoch  9, batch     1 | loss: 375.6454816CurrentTrain: epoch  9, batch     2 | loss: 345.0165982CurrentTrain: epoch  9, batch     3 | loss: 357.9693632CurrentTrain: epoch  9, batch     4 | loss: 120.0231937
MemoryTrain:  epoch  0, batch     0 | loss: 1.1378352MemoryTrain:  epoch  1, batch     0 | loss: 1.0214732MemoryTrain:  epoch  2, batch     0 | loss: 0.7999732MemoryTrain:  epoch  3, batch     0 | loss: 0.6431849MemoryTrain:  epoch  4, batch     0 | loss: 0.6309174MemoryTrain:  epoch  5, batch     0 | loss: 0.4922115MemoryTrain:  epoch  6, batch     0 | loss: 0.4241870MemoryTrain:  epoch  7, batch     0 | loss: 0.3357204MemoryTrain:  epoch  8, batch     0 | loss: 0.2642898MemoryTrain:  epoch  9, batch     0 | loss: 0.2166921

F1 score per class: {0: 0.0, 2: 0.875, 6: 0.0, 39: 0.0, 40: 0.6201550387596899, 10: 0.2905982905982906, 11: 0.0, 12: 0.0, 16: 0.625, 19: 0.23529411764705882, 28: 0.0}
Micro-average F1 score: 0.45806451612903226
Weighted-average F1 score: 0.4637078009039495
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.9404761904761905, 12: 0.7295597484276729, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 27: 0.0, 28: 0.9333333333333333, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.8461538461538461, 40: 0.0}
Micro-average F1 score: 0.7677725118483413
Weighted-average F1 score: 0.6950458122291007
F1 score per class: {0: 0.0, 2: 0.875, 35: 0.0, 37: 0.0, 6: 0.9529411764705882, 38: 0.7530864197530864, 40: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 39: 0.0, 16: 0.9333333333333333, 18: 0.0, 19: 0.0, 21: 0.0, 27: 0.8, 28: 0.0}
Micro-average F1 score: 0.7867298578199052
Weighted-average F1 score: 0.7206446691064676

F1 score per class: {0: 0.9142857142857143, 2: 0.56, 4: 0.7261146496815286, 5: 0.8773584905660378, 6: 0.4697986577181208, 7: 0.0, 9: 0.96, 10: 0.21052631578947367, 11: 0.43478260869565216, 12: 0.25757575757575757, 13: 0.1111111111111111, 15: 0.6666666666666666, 16: 0.8135593220338984, 17: 0.0, 18: 0.0, 19: 0.5177664974619289, 21: 0.34146341463414637, 23: 0.8470588235294118, 24: 0.1, 25: 0.44776119402985076, 26: 0.7640449438202247, 27: 0.4444444444444444, 28: 0.2222222222222222, 29: 0.8651685393258427, 31: 0.0, 32: 0.8108108108108109, 35: 0.3225806451612903, 37: 0.5238095238095238, 38: 0.25, 39: 0.125, 40: 0.27586206896551724}
Micro-average F1 score: 0.5719799305287534
Weighted-average F1 score: 0.6135570638752621
F1 score per class: {0: 0.972972972972973, 2: 0.45161290322580644, 4: 0.8764044943820225, 5: 0.8658008658008658, 6: 0.708994708994709, 7: 0.0, 9: 0.9803921568627451, 10: 0.5, 11: 0.7085201793721974, 12: 0.6338797814207651, 13: 0.0, 15: 0.631578947368421, 16: 0.9, 17: 0.3076923076923077, 18: 0.17391304347826086, 19: 0.5687203791469194, 21: 0.5483870967741935, 23: 0.8695652173913043, 24: 0.08695652173913043, 25: 0.5945945945945946, 26: 0.7555555555555555, 27: 0.4827586206896552, 28: 0.27450980392156865, 29: 0.8901098901098901, 31: 1.0, 32: 0.8613861386138614, 35: 0.8737864077669902, 37: 0.6304347826086957, 38: 0.5128205128205128, 39: 0.5789473684210527, 40: 0.34782608695652173}
Micro-average F1 score: 0.6930758988015979
Weighted-average F1 score: 0.6979115937796773
F1 score per class: {0: 0.972972972972973, 2: 0.4, 4: 0.8304093567251462, 5: 0.8658008658008658, 6: 0.6285714285714286, 7: 0.0, 9: 0.9803921568627451, 10: 0.45112781954887216, 11: 0.7043478260869566, 12: 0.6421052631578947, 13: 0.0, 15: 0.5714285714285714, 16: 0.9, 17: 0.3076923076923077, 18: 0.13636363636363635, 19: 0.5714285714285714, 21: 0.5396825396825397, 23: 0.8470588235294118, 24: 0.08695652173913043, 25: 0.5555555555555556, 26: 0.7624309392265194, 27: 0.4827586206896552, 28: 0.23728813559322035, 29: 0.8715083798882681, 31: 1.0, 32: 0.8246445497630331, 35: 0.7191011235955056, 37: 0.6170212765957447, 38: 0.45569620253164556, 39: 0.45454545454545453, 40: 0.3826086956521739}
Micro-average F1 score: 0.6666666666666666
Weighted-average F1 score: 0.6691437920482776
cur_acc:  ['0.7810', '0.3981', '0.7500', '0.6726', '0.4339', '0.4581']
his_acc:  ['0.7810', '0.6510', '0.6890', '0.6852', '0.6059', '0.5720']
cur_acc des:  ['0.8143', '0.4018', '0.8842', '0.8273', '0.7268', '0.7678']
his_acc des:  ['0.8143', '0.6985', '0.7577', '0.7730', '0.7260', '0.6931']
cur_acc rrf:  ['0.8131', '0.4286', '0.8524', '0.8215', '0.6469', '0.7867']
his_acc rrf:  ['0.8131', '0.6936', '0.7331', '0.7690', '0.6959', '0.6667']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 335.8749088CurrentTrain: epoch  0, batch     1 | loss: 337.6281677CurrentTrain: epoch  0, batch     2 | loss: 548.3356039CurrentTrain: epoch  0, batch     3 | loss: 305.8157183CurrentTrain: epoch  0, batch     4 | loss: 307.8327633CurrentTrain: epoch  1, batch     0 | loss: 543.4632470CurrentTrain: epoch  1, batch     1 | loss: 357.5137067CurrentTrain: epoch  1, batch     2 | loss: 383.1685770CurrentTrain: epoch  1, batch     3 | loss: 327.9333161CurrentTrain: epoch  1, batch     4 | loss: 185.4833416CurrentTrain: epoch  2, batch     0 | loss: 337.1506600CurrentTrain: epoch  2, batch     1 | loss: 363.5054668CurrentTrain: epoch  2, batch     2 | loss: 395.9460688CurrentTrain: epoch  2, batch     3 | loss: 377.9895501CurrentTrain: epoch  2, batch     4 | loss: 230.4160685CurrentTrain: epoch  3, batch     0 | loss: 359.4262617CurrentTrain: epoch  3, batch     1 | loss: 321.8542725CurrentTrain: epoch  3, batch     2 | loss: 358.5034528CurrentTrain: epoch  3, batch     3 | loss: 399.9093262CurrentTrain: epoch  3, batch     4 | loss: 204.5486220CurrentTrain: epoch  4, batch     0 | loss: 359.5287222CurrentTrain: epoch  4, batch     1 | loss: 361.3532218CurrentTrain: epoch  4, batch     2 | loss: 330.7305599CurrentTrain: epoch  4, batch     3 | loss: 374.9136595CurrentTrain: epoch  4, batch     4 | loss: 202.9310226CurrentTrain: epoch  5, batch     0 | loss: 392.2211724CurrentTrain: epoch  5, batch     1 | loss: 437.9676063CurrentTrain: epoch  5, batch     2 | loss: 374.8861168CurrentTrain: epoch  5, batch     3 | loss: 343.4109460CurrentTrain: epoch  5, batch     4 | loss: 149.9951976CurrentTrain: epoch  6, batch     0 | loss: 318.2431314CurrentTrain: epoch  6, batch     1 | loss: 343.7591500CurrentTrain: epoch  6, batch     2 | loss: 359.2023259CurrentTrain: epoch  6, batch     3 | loss: 340.9687943CurrentTrain: epoch  6, batch     4 | loss: 301.2615926CurrentTrain: epoch  7, batch     0 | loss: 327.9826749CurrentTrain: epoch  7, batch     1 | loss: 342.7520316CurrentTrain: epoch  7, batch     2 | loss: 538.0530968CurrentTrain: epoch  7, batch     3 | loss: 341.6413695CurrentTrain: epoch  7, batch     4 | loss: 202.8732508CurrentTrain: epoch  8, batch     0 | loss: 346.8005136CurrentTrain: epoch  8, batch     1 | loss: 517.6956390CurrentTrain: epoch  8, batch     2 | loss: 375.3911557CurrentTrain: epoch  8, batch     3 | loss: 390.9086571CurrentTrain: epoch  8, batch     4 | loss: 130.8700899CurrentTrain: epoch  9, batch     0 | loss: 373.2107146CurrentTrain: epoch  9, batch     1 | loss: 259.9106731CurrentTrain: epoch  9, batch     2 | loss: 373.8218950CurrentTrain: epoch  9, batch     3 | loss: 390.9569403CurrentTrain: epoch  9, batch     4 | loss: 249.0254359
MemoryTrain:  epoch  0, batch     0 | loss: 1.4278069MemoryTrain:  epoch  1, batch     0 | loss: 1.1534903MemoryTrain:  epoch  2, batch     0 | loss: 0.9166403MemoryTrain:  epoch  3, batch     0 | loss: 0.8355378MemoryTrain:  epoch  4, batch     0 | loss: 0.6708278MemoryTrain:  epoch  5, batch     0 | loss: 0.5889693MemoryTrain:  epoch  6, batch     0 | loss: 0.5429239MemoryTrain:  epoch  7, batch     0 | loss: 0.3941669MemoryTrain:  epoch  8, batch     0 | loss: 0.3535897MemoryTrain:  epoch  9, batch     0 | loss: 0.2986147

F1 score per class: {32: 0.3620689655172414, 1: 0.5225225225225225, 34: 0.0, 35: 0.13793103448275862, 3: 0.0, 37: 0.7374301675977654, 11: 0.0, 14: 0.0, 18: 0.0, 22: 0.0, 23: 0.5714285714285714, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.46153846153846156
Weighted-average F1 score: 0.41102778058404105
F1 score per class: {32: 0.38016528925619836, 1: 0.71875, 34: 0.0, 35: 0.0, 3: 0.0, 37: 0.0, 9: 0.07317073170731707, 10: 0.0, 12: 0.0, 11: 0.7333333333333333, 14: 0.0, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.8421052631578947, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.5337331334332833
Weighted-average F1 score: 0.4819429601847409
F1 score per class: {32: 0.38016528925619836, 1: 0.7086614173228346, 34: 0.0, 35: 0.0, 3: 0.0, 37: 0.11627906976744186, 10: 0.0, 11: 0.7486033519553073, 12: 0.0, 14: 0.0, 18: 0.0, 22: 0.0, 23: 0.8172043010752689, 24: 0.0, 27: 0.0}
Micro-average F1 score: 0.5353383458646617
Weighted-average F1 score: 0.4787162017764066

F1 score per class: {0: 0.8484848484848485, 1: 0.31343283582089554, 2: 0.6363636363636364, 3: 0.4603174603174603, 4: 0.717948717948718, 5: 0.8708133971291866, 6: 0.4195804195804196, 7: 0.0, 9: 0.96, 10: 0.2689075630252101, 11: 0.4816753926701571, 12: 0.1732283464566929, 13: 0.1, 14: 0.11650485436893204, 15: 0.6666666666666666, 16: 0.8275862068965517, 17: 0.0, 18: 0.08695652173913043, 19: 0.3878787878787879, 21: 0.0625, 22: 0.6804123711340206, 23: 0.8409090909090909, 24: 0.08333333333333333, 25: 0.44776119402985076, 26: 0.7570621468926554, 27: 0.17391304347826086, 28: 0.29411764705882354, 29: 0.8390804597701149, 31: 0.0, 32: 0.5806451612903226, 34: 0.41904761904761906, 35: 0.26666666666666666, 37: 0.4146341463414634, 38: 0.3333333333333333, 39: 0.21428571428571427, 40: 0.29545454545454547}
Micro-average F1 score: 0.512241054613936
Weighted-average F1 score: 0.5399197406841301
F1 score per class: {0: 0.9444444444444444, 1: 0.31724137931034485, 2: 0.3783783783783784, 3: 0.5822784810126582, 4: 0.8235294117647058, 5: 0.8608695652173913, 6: 0.6590909090909091, 7: 0.0, 9: 0.9615384615384616, 10: 0.40601503759398494, 11: 0.6826923076923077, 12: 0.6057142857142858, 13: 0.0, 14: 0.07058823529411765, 15: 0.6666666666666666, 16: 0.9152542372881356, 17: 0.15384615384615385, 18: 0.16666666666666666, 19: 0.49142857142857144, 21: 0.11764705882352941, 22: 0.6633165829145728, 23: 0.9032258064516129, 24: 0.06666666666666667, 25: 0.5945945945945946, 26: 0.7624309392265194, 27: 0.08695652173913043, 28: 0.21428571428571427, 29: 0.8777777777777778, 31: 0.0, 32: 0.8290155440414507, 34: 0.5263157894736842, 35: 0.43859649122807015, 37: 0.2777777777777778, 38: 0.4, 39: 0.6, 40: 0.4925373134328358}
Micro-average F1 score: 0.6044077134986225
Weighted-average F1 score: 0.6111325804728789
F1 score per class: {0: 0.9444444444444444, 1: 0.32167832167832167, 2: 0.45161290322580644, 3: 0.5844155844155844, 4: 0.8023952095808383, 5: 0.8596491228070176, 6: 0.593939393939394, 7: 0.0, 9: 0.9803921568627451, 10: 0.40601503759398494, 11: 0.6513761467889908, 12: 0.5207100591715976, 13: 0.0, 14: 0.10309278350515463, 15: 0.631578947368421, 16: 0.9152542372881356, 17: 0.0, 18: 0.1276595744680851, 19: 0.4659090909090909, 21: 0.0625, 22: 0.6802030456852792, 23: 0.8505747126436781, 24: 0.07142857142857142, 25: 0.5352112676056338, 26: 0.7624309392265194, 27: 0.09523809523809523, 28: 0.19672131147540983, 29: 0.8651685393258427, 31: 0.0, 32: 0.8205128205128205, 34: 0.5170068027210885, 35: 0.38181818181818183, 37: 0.3116883116883117, 38: 0.37333333333333335, 39: 0.3870967741935484, 40: 0.464}
Micro-average F1 score: 0.5847953216374269
Weighted-average F1 score: 0.5915557372549668
cur_acc:  ['0.7810', '0.3981', '0.7500', '0.6726', '0.4339', '0.4581', '0.4615']
his_acc:  ['0.7810', '0.6510', '0.6890', '0.6852', '0.6059', '0.5720', '0.5122']
cur_acc des:  ['0.8143', '0.4018', '0.8842', '0.8273', '0.7268', '0.7678', '0.5337']
his_acc des:  ['0.8143', '0.6985', '0.7577', '0.7730', '0.7260', '0.6931', '0.6044']
cur_acc rrf:  ['0.8131', '0.4286', '0.8524', '0.8215', '0.6469', '0.7867', '0.5353']
his_acc rrf:  ['0.8131', '0.6936', '0.7331', '0.7690', '0.6959', '0.6667', '0.5848']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 292.0598246CurrentTrain: epoch  0, batch     1 | loss: 458.5105051CurrentTrain: epoch  0, batch     2 | loss: 331.0217209CurrentTrain: epoch  0, batch     3 | loss: 210.8257236CurrentTrain: epoch  1, batch     0 | loss: 353.2666984CurrentTrain: epoch  1, batch     1 | loss: 350.7030106CurrentTrain: epoch  1, batch     2 | loss: 338.5873093CurrentTrain: epoch  1, batch     3 | loss: 201.7394041CurrentTrain: epoch  2, batch     0 | loss: 424.5856853CurrentTrain: epoch  2, batch     1 | loss: 347.7078544CurrentTrain: epoch  2, batch     2 | loss: 317.2512648CurrentTrain: epoch  2, batch     3 | loss: 162.6940256CurrentTrain: epoch  3, batch     0 | loss: 411.1938410CurrentTrain: epoch  3, batch     1 | loss: 378.0201798CurrentTrain: epoch  3, batch     2 | loss: 261.8063908CurrentTrain: epoch  3, batch     3 | loss: 188.1649489CurrentTrain: epoch  4, batch     0 | loss: 299.2946766CurrentTrain: epoch  4, batch     1 | loss: 317.0237461CurrentTrain: epoch  4, batch     2 | loss: 455.7062419CurrentTrain: epoch  4, batch     3 | loss: 185.3471758CurrentTrain: epoch  5, batch     0 | loss: 298.4846888CurrentTrain: epoch  5, batch     1 | loss: 420.7655416CurrentTrain: epoch  5, batch     2 | loss: 295.6944918CurrentTrain: epoch  5, batch     3 | loss: 237.8486448CurrentTrain: epoch  6, batch     0 | loss: 285.6641346CurrentTrain: epoch  6, batch     1 | loss: 390.3881131CurrentTrain: epoch  6, batch     2 | loss: 284.0728511CurrentTrain: epoch  6, batch     3 | loss: 237.4169011CurrentTrain: epoch  7, batch     0 | loss: 298.7675247CurrentTrain: epoch  7, batch     1 | loss: 340.4618791CurrentTrain: epoch  7, batch     2 | loss: 340.9064744CurrentTrain: epoch  7, batch     3 | loss: 208.4328193CurrentTrain: epoch  8, batch     0 | loss: 267.7335892CurrentTrain: epoch  8, batch     1 | loss: 538.7642892CurrentTrain: epoch  8, batch     2 | loss: 310.5141152CurrentTrain: epoch  8, batch     3 | loss: 197.3174069CurrentTrain: epoch  9, batch     0 | loss: 280.5733752CurrentTrain: epoch  9, batch     1 | loss: 419.0335868CurrentTrain: epoch  9, batch     2 | loss: 312.9443732CurrentTrain: epoch  9, batch     3 | loss: 222.0961676
MemoryTrain:  epoch  0, batch     0 | loss: 0.6940824MemoryTrain:  epoch  1, batch     0 | loss: 0.6302768MemoryTrain:  epoch  2, batch     0 | loss: 0.4669782MemoryTrain:  epoch  3, batch     0 | loss: 0.3870277MemoryTrain:  epoch  4, batch     0 | loss: 0.3002125MemoryTrain:  epoch  5, batch     0 | loss: 0.2512109MemoryTrain:  epoch  6, batch     0 | loss: 0.2611698MemoryTrain:  epoch  7, batch     0 | loss: 0.1960247MemoryTrain:  epoch  8, batch     0 | loss: 0.1756311MemoryTrain:  epoch  9, batch     0 | loss: 0.1505227

F1 score per class: {33: 0.5, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.735632183908046, 8: 0.0, 40: 0.0, 11: 0.8823529411764706, 12: 0.42857142857142855, 18: 0.0, 20: 0.3291139240506329, 26: 0.0, 28: 0.0, 30: 0.0}
Micro-average F1 score: 0.5247813411078717
Weighted-average F1 score: 0.5051370668302241
F1 score per class: {2: 0.0, 5: 0.0, 8: 0.7244094488188977, 10: 0.0, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.8297872340425532, 23: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 30: 1.0, 33: 0.625, 34: 0.0, 35: 0.0, 36: 0.9, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.7528868360277137
Weighted-average F1 score: 0.6794025940958981
F1 score per class: {2: 0.0, 5: 0.0, 8: 0.6935483870967742, 10: 0.0, 11: 0.0, 12: 0.0, 18: 0.0, 20: 0.8333333333333334, 25: 0.0, 26: 0.0, 28: 0.0, 30: 1.0, 33: 0.5333333333333333, 34: 0.0, 35: 0.0, 36: 0.831858407079646, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.72
Weighted-average F1 score: 0.6446044284084432

F1 score per class: {0: 0.8307692307692308, 1: 0.30434782608695654, 2: 0.6666666666666666, 3: 0.43103448275862066, 4: 0.7261146496815286, 5: 0.8609865470852018, 6: 0.2923076923076923, 7: 0.0, 8: 0.36486486486486486, 9: 0.96, 10: 0.14814814814814814, 11: 0.4578313253012048, 12: 0.06557377049180328, 13: 0.2857142857142857, 14: 0.024390243902439025, 15: 0.6666666666666666, 16: 0.8421052631578947, 17: 0.0, 18: 0.1276595744680851, 19: 0.43529411764705883, 20: 0.7272727272727273, 21: 0.0625, 22: 0.7083333333333334, 23: 0.8505747126436781, 24: 0.08, 25: 0.44776119402985076, 26: 0.7597765363128491, 27: 0.2, 28: 0.4444444444444444, 29: 0.8187134502923976, 30: 0.8823529411764706, 31: 0.6666666666666666, 32: 0.6025641025641025, 33: 0.3157894736842105, 34: 0.4485981308411215, 35: 0.21052631578947367, 36: 0.325, 37: 0.5168539325842697, 38: 0.0625, 39: 0.1, 40: 0.32967032967032966}
Micro-average F1 score: 0.5084449621432732
Weighted-average F1 score: 0.5619438324548968
F1 score per class: {0: 0.9444444444444444, 1: 0.30985915492957744, 2: 0.4117647058823529, 3: 0.5072463768115942, 4: 0.8571428571428571, 5: 0.8284518828451883, 6: 0.5032258064516129, 7: 0.0, 8: 0.4742268041237113, 9: 0.9259259259259259, 10: 0.23529411764705882, 11: 0.6532663316582915, 12: 0.4810126582278481, 13: 0.0, 14: 0.11627906976744186, 15: 0.631578947368421, 16: 0.896551724137931, 17: 0.0, 18: 0.2, 19: 0.538860103626943, 20: 0.8041237113402062, 21: 0.12121212121212122, 22: 0.7281553398058253, 23: 0.8723404255319149, 24: 0.07142857142857142, 25: 0.56, 26: 0.7540983606557377, 27: 0.16, 28: 0.3076923076923077, 29: 0.8777777777777778, 30: 0.9743589743589743, 31: 0.8, 32: 0.8210526315789474, 33: 0.4166666666666667, 34: 0.6086956521739131, 35: 0.4580152671755725, 36: 0.8244274809160306, 37: 0.4235294117647059, 38: 0.4, 39: 0.17647058823529413, 40: 0.5112781954887218}
Micro-average F1 score: 0.6063011659637807
Weighted-average F1 score: 0.6217374550864402
F1 score per class: {0: 0.9444444444444444, 1: 0.31724137931034485, 2: 0.45161290322580644, 3: 0.5222929936305732, 4: 0.8505747126436781, 5: 0.8354430379746836, 6: 0.49673202614379086, 7: 0.0, 8: 0.4574468085106383, 9: 0.9803921568627451, 10: 0.2222222222222222, 11: 0.6467661691542289, 12: 0.4444444444444444, 13: 0.0, 14: 0.10526315789473684, 15: 0.631578947368421, 16: 0.8813559322033898, 17: 0.0, 18: 0.20408163265306123, 19: 0.53125, 20: 0.8, 21: 0.12121212121212122, 22: 0.7373737373737373, 23: 0.8314606741573034, 24: 0.07407407407407407, 25: 0.5405405405405406, 26: 0.7582417582417582, 27: 0.17391304347826086, 28: 0.3076923076923077, 29: 0.8715083798882681, 30: 0.95, 31: 0.8, 32: 0.8210526315789474, 33: 0.34782608695652173, 34: 0.5874125874125874, 35: 0.2962962962962963, 36: 0.7966101694915254, 37: 0.42696629213483145, 38: 0.3793103448275862, 39: 0.125, 40: 0.5}
Micro-average F1 score: 0.5941088367448827
Weighted-average F1 score: 0.6079445745546844
cur_acc:  ['0.7810', '0.3981', '0.7500', '0.6726', '0.4339', '0.4581', '0.4615', '0.5248']
his_acc:  ['0.7810', '0.6510', '0.6890', '0.6852', '0.6059', '0.5720', '0.5122', '0.5084']
cur_acc des:  ['0.8143', '0.4018', '0.8842', '0.8273', '0.7268', '0.7678', '0.5337', '0.7529']
his_acc des:  ['0.8143', '0.6985', '0.7577', '0.7730', '0.7260', '0.6931', '0.6044', '0.6063']
cur_acc rrf:  ['0.8131', '0.4286', '0.8524', '0.8215', '0.6469', '0.7867', '0.5353', '0.7200']
his_acc rrf:  ['0.8131', '0.6936', '0.7331', '0.7690', '0.6959', '0.6667', '0.5848', '0.5941']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 369.3547497CurrentTrain: epoch  0, batch     1 | loss: 382.2269103CurrentTrain: epoch  0, batch     2 | loss: 352.7690042CurrentTrain: epoch  0, batch     3 | loss: 348.7687864CurrentTrain: epoch  0, batch     4 | loss: 459.2882481CurrentTrain: epoch  0, batch     5 | loss: 377.5198165CurrentTrain: epoch  0, batch     6 | loss: 349.2106808CurrentTrain: epoch  0, batch     7 | loss: 355.1312061CurrentTrain: epoch  0, batch     8 | loss: 469.3875446CurrentTrain: epoch  0, batch     9 | loss: 362.5096115CurrentTrain: epoch  0, batch    10 | loss: 300.5089449CurrentTrain: epoch  0, batch    11 | loss: 453.1826601CurrentTrain: epoch  0, batch    12 | loss: 347.5261115CurrentTrain: epoch  0, batch    13 | loss: 285.5596773CurrentTrain: epoch  0, batch    14 | loss: 372.1501447CurrentTrain: epoch  0, batch    15 | loss: 333.6990718CurrentTrain: epoch  0, batch    16 | loss: 378.6063539CurrentTrain: epoch  0, batch    17 | loss: 406.2216791CurrentTrain: epoch  0, batch    18 | loss: 373.3733798CurrentTrain: epoch  0, batch    19 | loss: 420.1694009CurrentTrain: epoch  0, batch    20 | loss: 359.3717802CurrentTrain: epoch  0, batch    21 | loss: 405.4614273CurrentTrain: epoch  0, batch    22 | loss: 345.7689309CurrentTrain: epoch  0, batch    23 | loss: 346.1780303CurrentTrain: epoch  0, batch    24 | loss: 319.2267875CurrentTrain: epoch  0, batch    25 | loss: 318.0914154CurrentTrain: epoch  0, batch    26 | loss: 273.0919533CurrentTrain: epoch  0, batch    27 | loss: 465.0242916CurrentTrain: epoch  0, batch    28 | loss: 467.6820551CurrentTrain: epoch  0, batch    29 | loss: 308.3645015CurrentTrain: epoch  0, batch    30 | loss: 386.6436530CurrentTrain: epoch  0, batch    31 | loss: 307.8473442CurrentTrain: epoch  0, batch    32 | loss: 297.3584050CurrentTrain: epoch  0, batch    33 | loss: 376.9697265CurrentTrain: epoch  0, batch    34 | loss: 371.0478088CurrentTrain: epoch  0, batch    35 | loss: 304.8160316CurrentTrain: epoch  0, batch    36 | loss: 389.2073782CurrentTrain: epoch  0, batch    37 | loss: 343.9212459CurrentTrain: epoch  0, batch    38 | loss: 369.8234538CurrentTrain: epoch  0, batch    39 | loss: 403.6510864CurrentTrain: epoch  0, batch    40 | loss: 419.8865698CurrentTrain: epoch  0, batch    41 | loss: 369.3653852CurrentTrain: epoch  0, batch    42 | loss: 369.9851397CurrentTrain: epoch  0, batch    43 | loss: 365.3880695CurrentTrain: epoch  0, batch    44 | loss: 329.6645859CurrentTrain: epoch  0, batch    45 | loss: 280.2493268CurrentTrain: epoch  0, batch    46 | loss: 305.2055681CurrentTrain: epoch  0, batch    47 | loss: 370.2052188CurrentTrain: epoch  0, batch    48 | loss: 419.2182101CurrentTrain: epoch  0, batch    49 | loss: 434.5035817CurrentTrain: epoch  0, batch    50 | loss: 330.0250124CurrentTrain: epoch  0, batch    51 | loss: 372.2130428CurrentTrain: epoch  0, batch    52 | loss: 465.7874646CurrentTrain: epoch  0, batch    53 | loss: 499.0943252CurrentTrain: epoch  0, batch    54 | loss: 316.7112627CurrentTrain: epoch  0, batch    55 | loss: 464.2709332CurrentTrain: epoch  0, batch    56 | loss: 383.4055823CurrentTrain: epoch  0, batch    57 | loss: 293.9940716CurrentTrain: epoch  0, batch    58 | loss: 368.7834140CurrentTrain: epoch  0, batch    59 | loss: 436.6709695CurrentTrain: epoch  0, batch    60 | loss: 328.7538193CurrentTrain: epoch  0, batch    61 | loss: 401.5901420CurrentTrain: epoch  0, batch    62 | loss: 355.2684634CurrentTrain: epoch  0, batch    63 | loss: 369.6637979CurrentTrain: epoch  0, batch    64 | loss: 316.2579651CurrentTrain: epoch  0, batch    65 | loss: 418.5117283CurrentTrain: epoch  0, batch    66 | loss: 320.2094950CurrentTrain: epoch  0, batch    67 | loss: 360.8800429CurrentTrain: epoch  0, batch    68 | loss: 373.4611664CurrentTrain: epoch  0, batch    69 | loss: 362.5480029CurrentTrain: epoch  0, batch    70 | loss: 437.0052643CurrentTrain: epoch  0, batch    71 | loss: 316.4685153CurrentTrain: epoch  0, batch    72 | loss: 303.6492417CurrentTrain: epoch  0, batch    73 | loss: 270.3357158CurrentTrain: epoch  0, batch    74 | loss: 329.0798635CurrentTrain: epoch  0, batch    75 | loss: 292.8335148CurrentTrain: epoch  0, batch    76 | loss: 303.7014320CurrentTrain: epoch  0, batch    77 | loss: 354.9474017CurrentTrain: epoch  0, batch    78 | loss: 330.9660499CurrentTrain: epoch  0, batch    79 | loss: 313.4949313CurrentTrain: epoch  0, batch    80 | loss: 355.6748620CurrentTrain: epoch  0, batch    81 | loss: 314.7425962CurrentTrain: epoch  0, batch    82 | loss: 300.4650160CurrentTrain: epoch  0, batch    83 | loss: 384.2324326CurrentTrain: epoch  0, batch    84 | loss: 418.3708912CurrentTrain: epoch  0, batch    85 | loss: 290.4753734CurrentTrain: epoch  0, batch    86 | loss: 328.7150536CurrentTrain: epoch  0, batch    87 | loss: 350.9031359CurrentTrain: epoch  0, batch    88 | loss: 445.9521562CurrentTrain: epoch  0, batch    89 | loss: 373.1147106CurrentTrain: epoch  0, batch    90 | loss: 328.9248132CurrentTrain: epoch  0, batch    91 | loss: 313.2210775CurrentTrain: epoch  0, batch    92 | loss: 352.4410283CurrentTrain: epoch  0, batch    93 | loss: 312.7250182CurrentTrain: epoch  0, batch    94 | loss: 339.6781276CurrentTrain: epoch  0, batch    95 | loss: 264.2561323CurrentTrain: epoch  1, batch     0 | loss: 286.0272878CurrentTrain: epoch  1, batch     1 | loss: 313.9951114CurrentTrain: epoch  1, batch     2 | loss: 386.7176072CurrentTrain: epoch  1, batch     3 | loss: 383.8153579CurrentTrain: epoch  1, batch     4 | loss: 430.0501834CurrentTrain: epoch  1, batch     5 | loss: 303.2824546CurrentTrain: epoch  1, batch     6 | loss: 325.1799407CurrentTrain: epoch  1, batch     7 | loss: 270.4946412CurrentTrain: epoch  1, batch     8 | loss: 351.8141683CurrentTrain: epoch  1, batch     9 | loss: 354.3250088CurrentTrain: epoch  1, batch    10 | loss: 335.2465742CurrentTrain: epoch  1, batch    11 | loss: 352.6826516CurrentTrain: epoch  1, batch    12 | loss: 276.8298018CurrentTrain: epoch  1, batch    13 | loss: 366.1669379CurrentTrain: epoch  1, batch    14 | loss: 348.0081570CurrentTrain: epoch  1, batch    15 | loss: 400.5253685CurrentTrain: epoch  1, batch    16 | loss: 387.2076420CurrentTrain: epoch  1, batch    17 | loss: 352.8532000CurrentTrain: epoch  1, batch    18 | loss: 297.4612578CurrentTrain: epoch  1, batch    19 | loss: 330.0776060CurrentTrain: epoch  1, batch    20 | loss: 339.6707807CurrentTrain: epoch  1, batch    21 | loss: 322.3309968CurrentTrain: epoch  1, batch    22 | loss: 283.6516232CurrentTrain: epoch  1, batch    23 | loss: 311.7397052CurrentTrain: epoch  1, batch    24 | loss: 357.6403960CurrentTrain: epoch  1, batch    25 | loss: 335.7278402CurrentTrain: epoch  1, batch    26 | loss: 399.4229834CurrentTrain: epoch  1, batch    27 | loss: 399.4600360CurrentTrain: epoch  1, batch    28 | loss: 366.8585050CurrentTrain: epoch  1, batch    29 | loss: 338.3033970CurrentTrain: epoch  1, batch    30 | loss: 322.8628280CurrentTrain: epoch  1, batch    31 | loss: 463.8507458CurrentTrain: epoch  1, batch    32 | loss: 320.4862752CurrentTrain: epoch  1, batch    33 | loss: 353.1163725CurrentTrain: epoch  1, batch    34 | loss: 393.0465992CurrentTrain: epoch  1, batch    35 | loss: 381.1557943CurrentTrain: epoch  1, batch    36 | loss: 385.9528752CurrentTrain: epoch  1, batch    37 | loss: 383.4606376CurrentTrain: epoch  1, batch    38 | loss: 416.6570487CurrentTrain: epoch  1, batch    39 | loss: 349.9454254CurrentTrain: epoch  1, batch    40 | loss: 307.5728239CurrentTrain: epoch  1, batch    41 | loss: 338.5269102CurrentTrain: epoch  1, batch    42 | loss: 268.4207651CurrentTrain: epoch  1, batch    43 | loss: 529.9762082CurrentTrain: epoch  1, batch    44 | loss: 328.7041101CurrentTrain: epoch  1, batch    45 | loss: 338.6065477CurrentTrain: epoch  1, batch    46 | loss: 368.5018872CurrentTrain: epoch  1, batch    47 | loss: 365.1340717CurrentTrain: epoch  1, batch    48 | loss: 347.2273006CurrentTrain: epoch  1, batch    49 | loss: 416.2883335CurrentTrain: epoch  1, batch    50 | loss: 324.1443988CurrentTrain: epoch  1, batch    51 | loss: 335.2660015CurrentTrain: epoch  1, batch    52 | loss: 321.6716553CurrentTrain: epoch  1, batch    53 | loss: 380.3253660CurrentTrain: epoch  1, batch    54 | loss: 348.0424525CurrentTrain: epoch  1, batch    55 | loss: 333.5156537CurrentTrain: epoch  1, batch    56 | loss: 323.1559230CurrentTrain: epoch  1, batch    57 | loss: 352.2813124CurrentTrain: epoch  1, batch    58 | loss: 382.4139082CurrentTrain: epoch  1, batch    59 | loss: 335.6271183CurrentTrain: epoch  1, batch    60 | loss: 322.0383727CurrentTrain: epoch  1, batch    61 | loss: 381.1550838CurrentTrain: epoch  1, batch    62 | loss: 339.5511604CurrentTrain: epoch  1, batch    63 | loss: 543.4089180CurrentTrain: epoch  1, batch    64 | loss: 352.8111865CurrentTrain: epoch  1, batch    65 | loss: 379.7834751CurrentTrain: epoch  1, batch    66 | loss: 348.6718038CurrentTrain: epoch  1, batch    67 | loss: 306.0998721CurrentTrain: epoch  1, batch    68 | loss: 364.9178293CurrentTrain: epoch  1, batch    69 | loss: 308.8814116CurrentTrain: epoch  1, batch    70 | loss: 308.5908795CurrentTrain: epoch  1, batch    71 | loss: 323.9894886CurrentTrain: epoch  1, batch    72 | loss: 361.3894935CurrentTrain: epoch  1, batch    73 | loss: 326.7290092CurrentTrain: epoch  1, batch    74 | loss: 382.8153649CurrentTrain: epoch  1, batch    75 | loss: 304.6098909CurrentTrain: epoch  1, batch    76 | loss: 366.5155418CurrentTrain: epoch  1, batch    77 | loss: 337.4058711CurrentTrain: epoch  1, batch    78 | loss: 344.7383227CurrentTrain: epoch  1, batch    79 | loss: 304.9960917CurrentTrain: epoch  1, batch    80 | loss: 378.9681093CurrentTrain: epoch  1, batch    81 | loss: 279.7837982CurrentTrain: epoch  1, batch    82 | loss: 334.0660643CurrentTrain: epoch  1, batch    83 | loss: 365.2308206CurrentTrain: epoch  1, batch    84 | loss: 450.2169993CurrentTrain: epoch  1, batch    85 | loss: 365.9123648CurrentTrain: epoch  1, batch    86 | loss: 350.0838047CurrentTrain: epoch  1, batch    87 | loss: 346.9717636CurrentTrain: epoch  1, batch    88 | loss: 280.8348643CurrentTrain: epoch  1, batch    89 | loss: 292.7550034CurrentTrain: epoch  1, batch    90 | loss: 307.0821690CurrentTrain: epoch  1, batch    91 | loss: 462.0757241CurrentTrain: epoch  1, batch    92 | loss: 351.2019988CurrentTrain: epoch  1, batch    93 | loss: 304.5867435CurrentTrain: epoch  1, batch    94 | loss: 412.2328774CurrentTrain: epoch  1, batch    95 | loss: 283.4956804CurrentTrain: epoch  2, batch     0 | loss: 344.8582413CurrentTrain: epoch  2, batch     1 | loss: 396.9335471CurrentTrain: epoch  2, batch     2 | loss: 395.8494357CurrentTrain: epoch  2, batch     3 | loss: 282.2407852CurrentTrain: epoch  2, batch     4 | loss: 345.7580365CurrentTrain: epoch  2, batch     5 | loss: 303.0752074CurrentTrain: epoch  2, batch     6 | loss: 331.7094035CurrentTrain: epoch  2, batch     7 | loss: 363.5525973CurrentTrain: epoch  2, batch     8 | loss: 362.4178991CurrentTrain: epoch  2, batch     9 | loss: 360.8721590CurrentTrain: epoch  2, batch    10 | loss: 439.1848319CurrentTrain: epoch  2, batch    11 | loss: 293.8033930CurrentTrain: epoch  2, batch    12 | loss: 349.7473427CurrentTrain: epoch  2, batch    13 | loss: 394.9104343CurrentTrain: epoch  2, batch    14 | loss: 361.7450417CurrentTrain: epoch  2, batch    15 | loss: 278.5515937CurrentTrain: epoch  2, batch    16 | loss: 440.3616432CurrentTrain: epoch  2, batch    17 | loss: 304.1630048CurrentTrain: epoch  2, batch    18 | loss: 348.9278446CurrentTrain: epoch  2, batch    19 | loss: 292.3203041CurrentTrain: epoch  2, batch    20 | loss: 331.3489480CurrentTrain: epoch  2, batch    21 | loss: 317.2125794CurrentTrain: epoch  2, batch    22 | loss: 408.8125877CurrentTrain: epoch  2, batch    23 | loss: 348.7887104CurrentTrain: epoch  2, batch    24 | loss: 305.1514865CurrentTrain: epoch  2, batch    25 | loss: 399.5112973CurrentTrain: epoch  2, batch    26 | loss: 306.8247414CurrentTrain: epoch  2, batch    27 | loss: 379.1756656CurrentTrain: epoch  2, batch    28 | loss: 362.7917783CurrentTrain: epoch  2, batch    29 | loss: 463.1945569CurrentTrain: epoch  2, batch    30 | loss: 303.1638502CurrentTrain: epoch  2, batch    31 | loss: 303.2359346CurrentTrain: epoch  2, batch    32 | loss: 360.1854170CurrentTrain: epoch  2, batch    33 | loss: 422.7315893CurrentTrain: epoch  2, batch    34 | loss: 279.4393448CurrentTrain: epoch  2, batch    35 | loss: 304.6764229CurrentTrain: epoch  2, batch    36 | loss: 394.0352078CurrentTrain: epoch  2, batch    37 | loss: 361.4844730CurrentTrain: epoch  2, batch    38 | loss: 250.1628190CurrentTrain: epoch  2, batch    39 | loss: 375.7824715CurrentTrain: epoch  2, batch    40 | loss: 319.2127603CurrentTrain: epoch  2, batch    41 | loss: 248.7161305CurrentTrain: epoch  2, batch    42 | loss: 396.2793113CurrentTrain: epoch  2, batch    43 | loss: 333.2666051CurrentTrain: epoch  2, batch    44 | loss: 407.8145146CurrentTrain: epoch  2, batch    45 | loss: 293.4669120CurrentTrain: epoch  2, batch    46 | loss: 361.0760199CurrentTrain: epoch  2, batch    47 | loss: 275.4327373CurrentTrain: epoch  2, batch    48 | loss: 351.6594552CurrentTrain: epoch  2, batch    49 | loss: 321.1076090CurrentTrain: epoch  2, batch    50 | loss: 375.2263246CurrentTrain: epoch  2, batch    51 | loss: 332.4139671CurrentTrain: epoch  2, batch    52 | loss: 303.2461096CurrentTrain: epoch  2, batch    53 | loss: 329.2161174CurrentTrain: epoch  2, batch    54 | loss: 362.4720260CurrentTrain: epoch  2, batch    55 | loss: 352.7875412CurrentTrain: epoch  2, batch    56 | loss: 363.9984157CurrentTrain: epoch  2, batch    57 | loss: 409.8900950CurrentTrain: epoch  2, batch    58 | loss: 329.2963734CurrentTrain: epoch  2, batch    59 | loss: 376.8961628CurrentTrain: epoch  2, batch    60 | loss: 282.0100516CurrentTrain: epoch  2, batch    61 | loss: 378.4128131CurrentTrain: epoch  2, batch    62 | loss: 233.6389248CurrentTrain: epoch  2, batch    63 | loss: 294.6649854CurrentTrain: epoch  2, batch    64 | loss: 294.0634607CurrentTrain: epoch  2, batch    65 | loss: 412.5894635CurrentTrain: epoch  2, batch    66 | loss: 393.2248108CurrentTrain: epoch  2, batch    67 | loss: 397.7102915CurrentTrain: epoch  2, batch    68 | loss: 329.2648253CurrentTrain: epoch  2, batch    69 | loss: 369.6125586CurrentTrain: epoch  2, batch    70 | loss: 304.3848196CurrentTrain: epoch  2, batch    71 | loss: 394.0933283CurrentTrain: epoch  2, batch    72 | loss: 439.0239736CurrentTrain: epoch  2, batch    73 | loss: 414.7523784CurrentTrain: epoch  2, batch    74 | loss: 350.9054304CurrentTrain: epoch  2, batch    75 | loss: 364.8719007CurrentTrain: epoch  2, batch    76 | loss: 348.2462959CurrentTrain: epoch  2, batch    77 | loss: 345.1787122CurrentTrain: epoch  2, batch    78 | loss: 329.8851747CurrentTrain: epoch  2, batch    79 | loss: 279.5091486CurrentTrain: epoch  2, batch    80 | loss: 332.5204126CurrentTrain: epoch  2, batch    81 | loss: 357.8190736CurrentTrain: epoch  2, batch    82 | loss: 345.8049528CurrentTrain: epoch  2, batch    83 | loss: 392.5800101CurrentTrain: epoch  2, batch    84 | loss: 336.4826642CurrentTrain: epoch  2, batch    85 | loss: 314.5126815CurrentTrain: epoch  2, batch    86 | loss: 346.5043292CurrentTrain: epoch  2, batch    87 | loss: 363.4727364CurrentTrain: epoch  2, batch    88 | loss: 348.1222159CurrentTrain: epoch  2, batch    89 | loss: 322.1604217CurrentTrain: epoch  2, batch    90 | loss: 318.6549867CurrentTrain: epoch  2, batch    91 | loss: 376.3950620CurrentTrain: epoch  2, batch    92 | loss: 363.2814739CurrentTrain: epoch  2, batch    93 | loss: 362.0649843CurrentTrain: epoch  2, batch    94 | loss: 305.2772555CurrentTrain: epoch  2, batch    95 | loss: 357.6632108CurrentTrain: epoch  3, batch     0 | loss: 305.0711838CurrentTrain: epoch  3, batch     1 | loss: 375.2832828CurrentTrain: epoch  3, batch     2 | loss: 302.8933323CurrentTrain: epoch  3, batch     3 | loss: 310.5825295CurrentTrain: epoch  3, batch     4 | loss: 412.6200648CurrentTrain: epoch  3, batch     5 | loss: 346.2456730CurrentTrain: epoch  3, batch     6 | loss: 288.7998204CurrentTrain: epoch  3, batch     7 | loss: 374.2875115CurrentTrain: epoch  3, batch     8 | loss: 393.6933322CurrentTrain: epoch  3, batch     9 | loss: 374.3274860CurrentTrain: epoch  3, batch    10 | loss: 287.8237692CurrentTrain: epoch  3, batch    11 | loss: 361.8829664CurrentTrain: epoch  3, batch    12 | loss: 268.4065237CurrentTrain: epoch  3, batch    13 | loss: 329.4713696CurrentTrain: epoch  3, batch    14 | loss: 361.3924299CurrentTrain: epoch  3, batch    15 | loss: 360.2977492CurrentTrain: epoch  3, batch    16 | loss: 346.9146480CurrentTrain: epoch  3, batch    17 | loss: 458.6422241CurrentTrain: epoch  3, batch    18 | loss: 267.7924837CurrentTrain: epoch  3, batch    19 | loss: 330.7452753CurrentTrain: epoch  3, batch    20 | loss: 344.8259183CurrentTrain: epoch  3, batch    21 | loss: 312.6008943CurrentTrain: epoch  3, batch    22 | loss: 343.0585503CurrentTrain: epoch  3, batch    23 | loss: 361.0422026CurrentTrain: epoch  3, batch    24 | loss: 342.2236032CurrentTrain: epoch  3, batch    25 | loss: 405.8038254CurrentTrain: epoch  3, batch    26 | loss: 359.2558464CurrentTrain: epoch  3, batch    27 | loss: 362.3819109CurrentTrain: epoch  3, batch    28 | loss: 325.4742262CurrentTrain: epoch  3, batch    29 | loss: 360.7165430CurrentTrain: epoch  3, batch    30 | loss: 301.9432420CurrentTrain: epoch  3, batch    31 | loss: 456.1369977CurrentTrain: epoch  3, batch    32 | loss: 375.8826138CurrentTrain: epoch  3, batch    33 | loss: 331.6218788CurrentTrain: epoch  3, batch    34 | loss: 378.3963170CurrentTrain: epoch  3, batch    35 | loss: 319.3608877CurrentTrain: epoch  3, batch    36 | loss: 303.6151278CurrentTrain: epoch  3, batch    37 | loss: 359.5403400CurrentTrain: epoch  3, batch    38 | loss: 360.5212616CurrentTrain: epoch  3, batch    39 | loss: 332.0516570CurrentTrain: epoch  3, batch    40 | loss: 347.4762676CurrentTrain: epoch  3, batch    41 | loss: 375.7062159CurrentTrain: epoch  3, batch    42 | loss: 357.5448131CurrentTrain: epoch  3, batch    43 | loss: 364.6313771CurrentTrain: epoch  3, batch    44 | loss: 352.8989199CurrentTrain: epoch  3, batch    45 | loss: 331.8497108CurrentTrain: epoch  3, batch    46 | loss: 437.4452309CurrentTrain: epoch  3, batch    47 | loss: 304.6978901CurrentTrain: epoch  3, batch    48 | loss: 357.4537057CurrentTrain: epoch  3, batch    49 | loss: 297.8347149CurrentTrain: epoch  3, batch    50 | loss: 309.0576971CurrentTrain: epoch  3, batch    51 | loss: 327.5151023CurrentTrain: epoch  3, batch    52 | loss: 394.9349201CurrentTrain: epoch  3, batch    53 | loss: 342.9576068CurrentTrain: epoch  3, batch    54 | loss: 288.5967503CurrentTrain: epoch  3, batch    55 | loss: 326.9877854CurrentTrain: epoch  3, batch    56 | loss: 345.9384638CurrentTrain: epoch  3, batch    57 | loss: 289.5627633CurrentTrain: epoch  3, batch    58 | loss: 329.6528645CurrentTrain: epoch  3, batch    59 | loss: 304.3208056CurrentTrain: epoch  3, batch    60 | loss: 263.6711856CurrentTrain: epoch  3, batch    61 | loss: 344.1658714CurrentTrain: epoch  3, batch    62 | loss: 411.3463312CurrentTrain: epoch  3, batch    63 | loss: 334.7894698CurrentTrain: epoch  3, batch    64 | loss: 360.0380568CurrentTrain: epoch  3, batch    65 | loss: 320.1678059CurrentTrain: epoch  3, batch    66 | loss: 303.5869232CurrentTrain: epoch  3, batch    67 | loss: 394.9626037CurrentTrain: epoch  3, batch    68 | loss: 410.4830503CurrentTrain: epoch  3, batch    69 | loss: 344.5059672CurrentTrain: epoch  3, batch    70 | loss: 238.9122195CurrentTrain: epoch  3, batch    71 | loss: 312.4680207CurrentTrain: epoch  3, batch    72 | loss: 409.4416871CurrentTrain: epoch  3, batch    73 | loss: 376.5299335CurrentTrain: epoch  3, batch    74 | loss: 302.9255671CurrentTrain: epoch  3, batch    75 | loss: 429.1262591CurrentTrain: epoch  3, batch    76 | loss: 304.7229211CurrentTrain: epoch  3, batch    77 | loss: 288.3943471CurrentTrain: epoch  3, batch    78 | loss: 316.8370243CurrentTrain: epoch  3, batch    79 | loss: 360.5180644CurrentTrain: epoch  3, batch    80 | loss: 410.6321505CurrentTrain: epoch  3, batch    81 | loss: 408.0095115CurrentTrain: epoch  3, batch    82 | loss: 409.2316855CurrentTrain: epoch  3, batch    83 | loss: 457.7127140CurrentTrain: epoch  3, batch    84 | loss: 376.8472585CurrentTrain: epoch  3, batch    85 | loss: 336.0556180CurrentTrain: epoch  3, batch    86 | loss: 316.9802878CurrentTrain: epoch  3, batch    87 | loss: 313.1150099CurrentTrain: epoch  3, batch    88 | loss: 260.7474601CurrentTrain: epoch  3, batch    89 | loss: 301.3071519CurrentTrain: epoch  3, batch    90 | loss: 302.3220662CurrentTrain: epoch  3, batch    91 | loss: 391.0393084CurrentTrain: epoch  3, batch    92 | loss: 311.4951068CurrentTrain: epoch  3, batch    93 | loss: 317.8794731CurrentTrain: epoch  3, batch    94 | loss: 315.8236929CurrentTrain: epoch  3, batch    95 | loss: 291.9874947CurrentTrain: epoch  4, batch     0 | loss: 322.5888450CurrentTrain: epoch  4, batch     1 | loss: 345.7502347CurrentTrain: epoch  4, batch     2 | loss: 252.8876125CurrentTrain: epoch  4, batch     3 | loss: 392.4981300CurrentTrain: epoch  4, batch     4 | loss: 342.1847967CurrentTrain: epoch  4, batch     5 | loss: 341.4665774CurrentTrain: epoch  4, batch     6 | loss: 456.4695208CurrentTrain: epoch  4, batch     7 | loss: 393.6005116CurrentTrain: epoch  4, batch     8 | loss: 376.7625084CurrentTrain: epoch  4, batch     9 | loss: 455.4786858CurrentTrain: epoch  4, batch    10 | loss: 278.4492789CurrentTrain: epoch  4, batch    11 | loss: 345.9232871CurrentTrain: epoch  4, batch    12 | loss: 302.3721399CurrentTrain: epoch  4, batch    13 | loss: 328.3363329CurrentTrain: epoch  4, batch    14 | loss: 356.8770990CurrentTrain: epoch  4, batch    15 | loss: 325.8778191CurrentTrain: epoch  4, batch    16 | loss: 439.3220456CurrentTrain: epoch  4, batch    17 | loss: 316.1020850CurrentTrain: epoch  4, batch    18 | loss: 377.2454522CurrentTrain: epoch  4, batch    19 | loss: 304.1675312CurrentTrain: epoch  4, batch    20 | loss: 329.2792569CurrentTrain: epoch  4, batch    21 | loss: 337.2280512CurrentTrain: epoch  4, batch    22 | loss: 288.4915855CurrentTrain: epoch  4, batch    23 | loss: 406.8739007CurrentTrain: epoch  4, batch    24 | loss: 359.7164372CurrentTrain: epoch  4, batch    25 | loss: 438.4289469CurrentTrain: epoch  4, batch    26 | loss: 311.7835026CurrentTrain: epoch  4, batch    27 | loss: 287.1920805CurrentTrain: epoch  4, batch    28 | loss: 292.9067694CurrentTrain: epoch  4, batch    29 | loss: 517.0792692CurrentTrain: epoch  4, batch    30 | loss: 318.0025403CurrentTrain: epoch  4, batch    31 | loss: 300.7795810CurrentTrain: epoch  4, batch    32 | loss: 242.3299847CurrentTrain: epoch  4, batch    33 | loss: 359.0771807CurrentTrain: epoch  4, batch    34 | loss: 227.7295528CurrentTrain: epoch  4, batch    35 | loss: 266.0487839CurrentTrain: epoch  4, batch    36 | loss: 437.0053009CurrentTrain: epoch  4, batch    37 | loss: 359.7411799CurrentTrain: epoch  4, batch    38 | loss: 391.0496273CurrentTrain: epoch  4, batch    39 | loss: 392.6533587CurrentTrain: epoch  4, batch    40 | loss: 343.3915055CurrentTrain: epoch  4, batch    41 | loss: 341.4330570CurrentTrain: epoch  4, batch    42 | loss: 294.9489169CurrentTrain: epoch  4, batch    43 | loss: 374.0487797CurrentTrain: epoch  4, batch    44 | loss: 357.2698636CurrentTrain: epoch  4, batch    45 | loss: 317.8810235CurrentTrain: epoch  4, batch    46 | loss: 390.4615605CurrentTrain: epoch  4, batch    47 | loss: 314.8165511CurrentTrain: epoch  4, batch    48 | loss: 252.2905504CurrentTrain: epoch  4, batch    49 | loss: 356.5184180CurrentTrain: epoch  4, batch    50 | loss: 375.1880575CurrentTrain: epoch  4, batch    51 | loss: 409.4627090CurrentTrain: epoch  4, batch    52 | loss: 390.1145862CurrentTrain: epoch  4, batch    53 | loss: 341.0813111CurrentTrain: epoch  4, batch    54 | loss: 312.1162521CurrentTrain: epoch  4, batch    55 | loss: 316.5925077CurrentTrain: epoch  4, batch    56 | loss: 285.3618232CurrentTrain: epoch  4, batch    57 | loss: 326.3507027CurrentTrain: epoch  4, batch    58 | loss: 330.0186976CurrentTrain: epoch  4, batch    59 | loss: 344.9548736CurrentTrain: epoch  4, batch    60 | loss: 275.2923013CurrentTrain: epoch  4, batch    61 | loss: 456.4639841CurrentTrain: epoch  4, batch    62 | loss: 279.8759589CurrentTrain: epoch  4, batch    63 | loss: 374.7061465CurrentTrain: epoch  4, batch    64 | loss: 375.9378915CurrentTrain: epoch  4, batch    65 | loss: 287.8402329CurrentTrain: epoch  4, batch    66 | loss: 330.3431936CurrentTrain: epoch  4, batch    67 | loss: 374.4497735CurrentTrain: epoch  4, batch    68 | loss: 237.6772410CurrentTrain: epoch  4, batch    69 | loss: 329.3493159CurrentTrain: epoch  4, batch    70 | loss: 246.3302779CurrentTrain: epoch  4, batch    71 | loss: 357.5373765CurrentTrain: epoch  4, batch    72 | loss: 328.3911090CurrentTrain: epoch  4, batch    73 | loss: 397.8177024CurrentTrain: epoch  4, batch    74 | loss: 329.3934023CurrentTrain: epoch  4, batch    75 | loss: 343.2241569CurrentTrain: epoch  4, batch    76 | loss: 382.1571305CurrentTrain: epoch  4, batch    77 | loss: 420.4309321CurrentTrain: epoch  4, batch    78 | loss: 347.3594564CurrentTrain: epoch  4, batch    79 | loss: 329.0430148CurrentTrain: epoch  4, batch    80 | loss: 350.0493728CurrentTrain: epoch  4, batch    81 | loss: 290.4072064CurrentTrain: epoch  4, batch    82 | loss: 394.9160324CurrentTrain: epoch  4, batch    83 | loss: 374.0842043CurrentTrain: epoch  4, batch    84 | loss: 347.1157267CurrentTrain: epoch  4, batch    85 | loss: 341.5381587CurrentTrain: epoch  4, batch    86 | loss: 341.2190677CurrentTrain: epoch  4, batch    87 | loss: 328.1972880CurrentTrain: epoch  4, batch    88 | loss: 297.7020686CurrentTrain: epoch  4, batch    89 | loss: 343.1836729CurrentTrain: epoch  4, batch    90 | loss: 378.2866885CurrentTrain: epoch  4, batch    91 | loss: 294.2745570CurrentTrain: epoch  4, batch    92 | loss: 458.5193511CurrentTrain: epoch  4, batch    93 | loss: 408.6988568CurrentTrain: epoch  4, batch    94 | loss: 358.8280448CurrentTrain: epoch  4, batch    95 | loss: 267.3667845CurrentTrain: epoch  5, batch     0 | loss: 301.0889328CurrentTrain: epoch  5, batch     1 | loss: 288.4979605CurrentTrain: epoch  5, batch     2 | loss: 326.2523420CurrentTrain: epoch  5, batch     3 | loss: 279.3264513CurrentTrain: epoch  5, batch     4 | loss: 312.9768432CurrentTrain: epoch  5, batch     5 | loss: 421.5167682CurrentTrain: epoch  5, batch     6 | loss: 343.5825344CurrentTrain: epoch  5, batch     7 | loss: 329.3239943CurrentTrain: epoch  5, batch     8 | loss: 358.8021629CurrentTrain: epoch  5, batch     9 | loss: 312.5310629CurrentTrain: epoch  5, batch    10 | loss: 344.1208687CurrentTrain: epoch  5, batch    11 | loss: 356.6080445CurrentTrain: epoch  5, batch    12 | loss: 358.4414371CurrentTrain: epoch  5, batch    13 | loss: 373.6203835CurrentTrain: epoch  5, batch    14 | loss: 331.0567894CurrentTrain: epoch  5, batch    15 | loss: 373.7846007CurrentTrain: epoch  5, batch    16 | loss: 317.9693630CurrentTrain: epoch  5, batch    17 | loss: 315.5233929CurrentTrain: epoch  5, batch    18 | loss: 358.4985013CurrentTrain: epoch  5, batch    19 | loss: 356.2704229CurrentTrain: epoch  5, batch    20 | loss: 310.0875316CurrentTrain: epoch  5, batch    21 | loss: 343.3221497CurrentTrain: epoch  5, batch    22 | loss: 299.9816372CurrentTrain: epoch  5, batch    23 | loss: 292.6834481CurrentTrain: epoch  5, batch    24 | loss: 359.2063317CurrentTrain: epoch  5, batch    25 | loss: 455.7325846CurrentTrain: epoch  5, batch    26 | loss: 341.2431452CurrentTrain: epoch  5, batch    27 | loss: 298.8505541CurrentTrain: epoch  5, batch    28 | loss: 390.5488495CurrentTrain: epoch  5, batch    29 | loss: 341.7177714CurrentTrain: epoch  5, batch    30 | loss: 341.4658047CurrentTrain: epoch  5, batch    31 | loss: 356.8842200CurrentTrain: epoch  5, batch    32 | loss: 376.3176465CurrentTrain: epoch  5, batch    33 | loss: 313.7165657CurrentTrain: epoch  5, batch    34 | loss: 325.2246730CurrentTrain: epoch  5, batch    35 | loss: 309.4271300CurrentTrain: epoch  5, batch    36 | loss: 264.3495511CurrentTrain: epoch  5, batch    37 | loss: 359.0183153CurrentTrain: epoch  5, batch    38 | loss: 344.4413708CurrentTrain: epoch  5, batch    39 | loss: 344.9372287CurrentTrain: epoch  5, batch    40 | loss: 396.4460893CurrentTrain: epoch  5, batch    41 | loss: 297.9924525CurrentTrain: epoch  5, batch    42 | loss: 340.8666133CurrentTrain: epoch  5, batch    43 | loss: 421.2628387CurrentTrain: epoch  5, batch    44 | loss: 393.8116871CurrentTrain: epoch  5, batch    45 | loss: 287.7141336CurrentTrain: epoch  5, batch    46 | loss: 330.7400968CurrentTrain: epoch  5, batch    47 | loss: 357.2749547CurrentTrain: epoch  5, batch    48 | loss: 341.6598872CurrentTrain: epoch  5, batch    49 | loss: 316.2093755CurrentTrain: epoch  5, batch    50 | loss: 250.0260556CurrentTrain: epoch  5, batch    51 | loss: 393.0056905CurrentTrain: epoch  5, batch    52 | loss: 407.9854581CurrentTrain: epoch  5, batch    53 | loss: 313.0422521CurrentTrain: epoch  5, batch    54 | loss: 358.9892564CurrentTrain: epoch  5, batch    55 | loss: 325.5336638CurrentTrain: epoch  5, batch    56 | loss: 373.8080207CurrentTrain: epoch  5, batch    57 | loss: 312.5278186CurrentTrain: epoch  5, batch    58 | loss: 436.7087191CurrentTrain: epoch  5, batch    59 | loss: 377.3317756CurrentTrain: epoch  5, batch    60 | loss: 373.0306931CurrentTrain: epoch  5, batch    61 | loss: 310.4726551CurrentTrain: epoch  5, batch    62 | loss: 285.1823898CurrentTrain: epoch  5, batch    63 | loss: 359.1747107CurrentTrain: epoch  5, batch    64 | loss: 374.6522635CurrentTrain: epoch  5, batch    65 | loss: 315.7953065CurrentTrain: epoch  5, batch    66 | loss: 266.4843256CurrentTrain: epoch  5, batch    67 | loss: 372.9828577CurrentTrain: epoch  5, batch    68 | loss: 341.8020316CurrentTrain: epoch  5, batch    69 | loss: 357.5549965CurrentTrain: epoch  5, batch    70 | loss: 357.3467945CurrentTrain: epoch  5, batch    71 | loss: 409.2752834CurrentTrain: epoch  5, batch    72 | loss: 359.1352614CurrentTrain: epoch  5, batch    73 | loss: 359.7247864CurrentTrain: epoch  5, batch    74 | loss: 279.0176884CurrentTrain: epoch  5, batch    75 | loss: 360.1090120CurrentTrain: epoch  5, batch    76 | loss: 351.6126831CurrentTrain: epoch  5, batch    77 | loss: 273.4240309CurrentTrain: epoch  5, batch    78 | loss: 357.6923852CurrentTrain: epoch  5, batch    79 | loss: 303.0875091CurrentTrain: epoch  5, batch    80 | loss: 330.3357912CurrentTrain: epoch  5, batch    81 | loss: 393.1137915CurrentTrain: epoch  5, batch    82 | loss: 286.3145773CurrentTrain: epoch  5, batch    83 | loss: 289.1448454CurrentTrain: epoch  5, batch    84 | loss: 341.0321943CurrentTrain: epoch  5, batch    85 | loss: 300.6390866CurrentTrain: epoch  5, batch    86 | loss: 394.1727882CurrentTrain: epoch  5, batch    87 | loss: 299.5021526CurrentTrain: epoch  5, batch    88 | loss: 456.8056380CurrentTrain: epoch  5, batch    89 | loss: 390.6339626CurrentTrain: epoch  5, batch    90 | loss: 269.6364349CurrentTrain: epoch  5, batch    91 | loss: 375.0816885CurrentTrain: epoch  5, batch    92 | loss: 281.6721320CurrentTrain: epoch  5, batch    93 | loss: 421.2016524CurrentTrain: epoch  5, batch    94 | loss: 390.5157009CurrentTrain: epoch  5, batch    95 | loss: 269.6228829CurrentTrain: epoch  6, batch     0 | loss: 373.4463024CurrentTrain: epoch  6, batch     1 | loss: 343.7477238CurrentTrain: epoch  6, batch     2 | loss: 407.2322142CurrentTrain: epoch  6, batch     3 | loss: 373.7373363CurrentTrain: epoch  6, batch     4 | loss: 340.5428634CurrentTrain: epoch  6, batch     5 | loss: 330.6135997CurrentTrain: epoch  6, batch     6 | loss: 390.0395530CurrentTrain: epoch  6, batch     7 | loss: 357.5910249CurrentTrain: epoch  6, batch     8 | loss: 339.5348364CurrentTrain: epoch  6, batch     9 | loss: 328.6356337CurrentTrain: epoch  6, batch    10 | loss: 373.7804675CurrentTrain: epoch  6, batch    11 | loss: 325.4605113CurrentTrain: epoch  6, batch    12 | loss: 455.3053853CurrentTrain: epoch  6, batch    13 | loss: 389.5526210CurrentTrain: epoch  6, batch    14 | loss: 341.6413388CurrentTrain: epoch  6, batch    15 | loss: 419.4374668CurrentTrain: epoch  6, batch    16 | loss: 278.2568460CurrentTrain: epoch  6, batch    17 | loss: 389.8484291CurrentTrain: epoch  6, batch    18 | loss: 285.6585302CurrentTrain: epoch  6, batch    19 | loss: 250.8988388CurrentTrain: epoch  6, batch    20 | loss: 309.8101732CurrentTrain: epoch  6, batch    21 | loss: 314.6325192CurrentTrain: epoch  6, batch    22 | loss: 328.8316749CurrentTrain: epoch  6, batch    23 | loss: 536.7113363CurrentTrain: epoch  6, batch    24 | loss: 295.2774343CurrentTrain: epoch  6, batch    25 | loss: 356.7820724CurrentTrain: epoch  6, batch    26 | loss: 390.7790665CurrentTrain: epoch  6, batch    27 | loss: 536.6320805CurrentTrain: epoch  6, batch    28 | loss: 313.0999331CurrentTrain: epoch  6, batch    29 | loss: 361.4642474CurrentTrain: epoch  6, batch    30 | loss: 341.6039638CurrentTrain: epoch  6, batch    31 | loss: 298.9689357CurrentTrain: epoch  6, batch    32 | loss: 419.8579197CurrentTrain: epoch  6, batch    33 | loss: 356.7203261CurrentTrain: epoch  6, batch    34 | loss: 358.8906836CurrentTrain: epoch  6, batch    35 | loss: 537.2409732CurrentTrain: epoch  6, batch    36 | loss: 271.2354665CurrentTrain: epoch  6, batch    37 | loss: 373.9623209CurrentTrain: epoch  6, batch    38 | loss: 376.8917947CurrentTrain: epoch  6, batch    39 | loss: 345.6292354CurrentTrain: epoch  6, batch    40 | loss: 298.2362261CurrentTrain: epoch  6, batch    41 | loss: 259.9534035CurrentTrain: epoch  6, batch    42 | loss: 328.1245338CurrentTrain: epoch  6, batch    43 | loss: 341.3006179CurrentTrain: epoch  6, batch    44 | loss: 373.8767895CurrentTrain: epoch  6, batch    45 | loss: 374.0545847CurrentTrain: epoch  6, batch    46 | loss: 326.0796295CurrentTrain: epoch  6, batch    47 | loss: 311.8564434CurrentTrain: epoch  6, batch    48 | loss: 329.8726785CurrentTrain: epoch  6, batch    49 | loss: 344.6203145CurrentTrain: epoch  6, batch    50 | loss: 270.1109996CurrentTrain: epoch  6, batch    51 | loss: 420.4023494CurrentTrain: epoch  6, batch    52 | loss: 357.9745654CurrentTrain: epoch  6, batch    53 | loss: 296.7556993CurrentTrain: epoch  6, batch    54 | loss: 372.3655039CurrentTrain: epoch  6, batch    55 | loss: 344.6291457CurrentTrain: epoch  6, batch    56 | loss: 328.5085023CurrentTrain: epoch  6, batch    57 | loss: 289.1257431CurrentTrain: epoch  6, batch    58 | loss: 418.8853694CurrentTrain: epoch  6, batch    59 | loss: 331.0521673CurrentTrain: epoch  6, batch    60 | loss: 372.5647604CurrentTrain: epoch  6, batch    61 | loss: 299.9891019CurrentTrain: epoch  6, batch    62 | loss: 325.9535154CurrentTrain: epoch  6, batch    63 | loss: 377.5635239CurrentTrain: epoch  6, batch    64 | loss: 375.6452666CurrentTrain: epoch  6, batch    65 | loss: 316.7166231CurrentTrain: epoch  6, batch    66 | loss: 455.2842603CurrentTrain: epoch  6, batch    67 | loss: 283.3076100CurrentTrain: epoch  6, batch    68 | loss: 271.4857094CurrentTrain: epoch  6, batch    69 | loss: 314.3193016CurrentTrain: epoch  6, batch    70 | loss: 270.7217502CurrentTrain: epoch  6, batch    71 | loss: 326.3507518CurrentTrain: epoch  6, batch    72 | loss: 407.3048517CurrentTrain: epoch  6, batch    73 | loss: 402.7545947CurrentTrain: epoch  6, batch    74 | loss: 302.3379731CurrentTrain: epoch  6, batch    75 | loss: 313.7018379CurrentTrain: epoch  6, batch    76 | loss: 325.4924294CurrentTrain: epoch  6, batch    77 | loss: 405.3068370CurrentTrain: epoch  6, batch    78 | loss: 455.0960361CurrentTrain: epoch  6, batch    79 | loss: 356.0947608CurrentTrain: epoch  6, batch    80 | loss: 287.1441154CurrentTrain: epoch  6, batch    81 | loss: 380.5545534CurrentTrain: epoch  6, batch    82 | loss: 273.5957968CurrentTrain: epoch  6, batch    83 | loss: 326.9301290CurrentTrain: epoch  6, batch    84 | loss: 357.4135908CurrentTrain: epoch  6, batch    85 | loss: 314.3316082CurrentTrain: epoch  6, batch    86 | loss: 296.3146118CurrentTrain: epoch  6, batch    87 | loss: 259.1295684CurrentTrain: epoch  6, batch    88 | loss: 263.6537851CurrentTrain: epoch  6, batch    89 | loss: 421.4045277CurrentTrain: epoch  6, batch    90 | loss: 515.7235709CurrentTrain: epoch  6, batch    91 | loss: 250.8872586CurrentTrain: epoch  6, batch    92 | loss: 328.4048350CurrentTrain: epoch  6, batch    93 | loss: 340.1094012CurrentTrain: epoch  6, batch    94 | loss: 277.2376135CurrentTrain: epoch  6, batch    95 | loss: 238.8296283CurrentTrain: epoch  7, batch     0 | loss: 251.5042802CurrentTrain: epoch  7, batch     1 | loss: 496.5358892CurrentTrain: epoch  7, batch     2 | loss: 284.1489333CurrentTrain: epoch  7, batch     3 | loss: 325.7009984CurrentTrain: epoch  7, batch     4 | loss: 328.5852158CurrentTrain: epoch  7, batch     5 | loss: 515.7228487CurrentTrain: epoch  7, batch     6 | loss: 437.1783276CurrentTrain: epoch  7, batch     7 | loss: 355.9743944CurrentTrain: epoch  7, batch     8 | loss: 358.4655939CurrentTrain: epoch  7, batch     9 | loss: 355.7391405CurrentTrain: epoch  7, batch    10 | loss: 299.3624934CurrentTrain: epoch  7, batch    11 | loss: 390.1254429CurrentTrain: epoch  7, batch    12 | loss: 271.3307440CurrentTrain: epoch  7, batch    13 | loss: 437.2707396CurrentTrain: epoch  7, batch    14 | loss: 372.8640471CurrentTrain: epoch  7, batch    15 | loss: 421.2650179CurrentTrain: epoch  7, batch    16 | loss: 373.5336284CurrentTrain: epoch  7, batch    17 | loss: 356.2320351CurrentTrain: epoch  7, batch    18 | loss: 313.4203312CurrentTrain: epoch  7, batch    19 | loss: 303.2770668CurrentTrain: epoch  7, batch    20 | loss: 422.2069951CurrentTrain: epoch  7, batch    21 | loss: 390.1184197CurrentTrain: epoch  7, batch    22 | loss: 314.0028685CurrentTrain: epoch  7, batch    23 | loss: 344.8953428CurrentTrain: epoch  7, batch    24 | loss: 308.8394379CurrentTrain: epoch  7, batch    25 | loss: 283.5485062CurrentTrain: epoch  7, batch    26 | loss: 358.1336402CurrentTrain: epoch  7, batch    27 | loss: 284.2970636CurrentTrain: epoch  7, batch    28 | loss: 407.2716698CurrentTrain: epoch  7, batch    29 | loss: 325.1341094CurrentTrain: epoch  7, batch    30 | loss: 403.1318029CurrentTrain: epoch  7, batch    31 | loss: 342.8032939CurrentTrain: epoch  7, batch    32 | loss: 283.6916094CurrentTrain: epoch  7, batch    33 | loss: 341.2764470CurrentTrain: epoch  7, batch    34 | loss: 270.9510955CurrentTrain: epoch  7, batch    35 | loss: 324.3119796CurrentTrain: epoch  7, batch    36 | loss: 236.9163153CurrentTrain: epoch  7, batch    37 | loss: 298.2228701CurrentTrain: epoch  7, batch    38 | loss: 328.1961076CurrentTrain: epoch  7, batch    39 | loss: 298.9607369CurrentTrain: epoch  7, batch    40 | loss: 373.5387706CurrentTrain: epoch  7, batch    41 | loss: 341.2349366CurrentTrain: epoch  7, batch    42 | loss: 256.9484268CurrentTrain: epoch  7, batch    43 | loss: 461.7843831CurrentTrain: epoch  7, batch    44 | loss: 356.5915418CurrentTrain: epoch  7, batch    45 | loss: 375.3091061CurrentTrain: epoch  7, batch    46 | loss: 455.2046563CurrentTrain: epoch  7, batch    47 | loss: 264.0354086CurrentTrain: epoch  7, batch    48 | loss: 372.2849340CurrentTrain: epoch  7, batch    49 | loss: 363.7043499CurrentTrain: epoch  7, batch    50 | loss: 329.3064592CurrentTrain: epoch  7, batch    51 | loss: 324.9483731CurrentTrain: epoch  7, batch    52 | loss: 355.5514257CurrentTrain: epoch  7, batch    53 | loss: 355.7135909CurrentTrain: epoch  7, batch    54 | loss: 328.4898584CurrentTrain: epoch  7, batch    55 | loss: 389.9413628CurrentTrain: epoch  7, batch    56 | loss: 372.6538200CurrentTrain: epoch  7, batch    57 | loss: 355.9911881CurrentTrain: epoch  7, batch    58 | loss: 328.1224762CurrentTrain: epoch  7, batch    59 | loss: 328.5053726CurrentTrain: epoch  7, batch    60 | loss: 297.5274496CurrentTrain: epoch  7, batch    61 | loss: 310.4686164CurrentTrain: epoch  7, batch    62 | loss: 281.2542619CurrentTrain: epoch  7, batch    63 | loss: 420.8809651CurrentTrain: epoch  7, batch    64 | loss: 332.4322679CurrentTrain: epoch  7, batch    65 | loss: 330.7967547CurrentTrain: epoch  7, batch    66 | loss: 357.9499967CurrentTrain: epoch  7, batch    67 | loss: 298.5289541CurrentTrain: epoch  7, batch    68 | loss: 356.8604165CurrentTrain: epoch  7, batch    69 | loss: 275.5755228CurrentTrain: epoch  7, batch    70 | loss: 340.3927334CurrentTrain: epoch  7, batch    71 | loss: 375.3733613CurrentTrain: epoch  7, batch    72 | loss: 406.9987749CurrentTrain: epoch  7, batch    73 | loss: 373.3412651CurrentTrain: epoch  7, batch    74 | loss: 298.8961345CurrentTrain: epoch  7, batch    75 | loss: 340.1031235CurrentTrain: epoch  7, batch    76 | loss: 203.1036958CurrentTrain: epoch  7, batch    77 | loss: 294.8621459CurrentTrain: epoch  7, batch    78 | loss: 339.8293272CurrentTrain: epoch  7, batch    79 | loss: 373.5104460CurrentTrain: epoch  7, batch    80 | loss: 331.6517713CurrentTrain: epoch  7, batch    81 | loss: 297.7555223CurrentTrain: epoch  7, batch    82 | loss: 301.5993362CurrentTrain: epoch  7, batch    83 | loss: 456.0553012CurrentTrain: epoch  7, batch    84 | loss: 392.8325100CurrentTrain: epoch  7, batch    85 | loss: 313.4231448CurrentTrain: epoch  7, batch    86 | loss: 456.8315558CurrentTrain: epoch  7, batch    87 | loss: 314.1980531CurrentTrain: epoch  7, batch    88 | loss: 357.6456107CurrentTrain: epoch  7, batch    89 | loss: 373.9171761CurrentTrain: epoch  7, batch    90 | loss: 342.9448265CurrentTrain: epoch  7, batch    91 | loss: 408.7572117CurrentTrain: epoch  7, batch    92 | loss: 334.1204401CurrentTrain: epoch  7, batch    93 | loss: 270.5411367CurrentTrain: epoch  7, batch    94 | loss: 357.1144370CurrentTrain: epoch  7, batch    95 | loss: 336.9977706CurrentTrain: epoch  8, batch     0 | loss: 301.4804133CurrentTrain: epoch  8, batch     1 | loss: 389.6763402CurrentTrain: epoch  8, batch     2 | loss: 407.0460337CurrentTrain: epoch  8, batch     3 | loss: 308.5853619CurrentTrain: epoch  8, batch     4 | loss: 326.7271080CurrentTrain: epoch  8, batch     5 | loss: 313.1064145CurrentTrain: epoch  8, batch     6 | loss: 282.1481809CurrentTrain: epoch  8, batch     7 | loss: 455.0706414CurrentTrain: epoch  8, batch     8 | loss: 339.8133396CurrentTrain: epoch  8, batch     9 | loss: 340.5720251CurrentTrain: epoch  8, batch    10 | loss: 270.0683347CurrentTrain: epoch  8, batch    11 | loss: 270.0087170CurrentTrain: epoch  8, batch    12 | loss: 339.6124694CurrentTrain: epoch  8, batch    13 | loss: 373.7906016CurrentTrain: epoch  8, batch    14 | loss: 314.0679597CurrentTrain: epoch  8, batch    15 | loss: 356.1967534CurrentTrain: epoch  8, batch    16 | loss: 437.0453426CurrentTrain: epoch  8, batch    17 | loss: 390.8265063CurrentTrain: epoch  8, batch    18 | loss: 309.5721726CurrentTrain: epoch  8, batch    19 | loss: 410.4058973CurrentTrain: epoch  8, batch    20 | loss: 390.5258989CurrentTrain: epoch  8, batch    21 | loss: 340.6303597CurrentTrain: epoch  8, batch    22 | loss: 373.0959614CurrentTrain: epoch  8, batch    23 | loss: 339.8454369CurrentTrain: epoch  8, batch    24 | loss: 300.3437439CurrentTrain: epoch  8, batch    25 | loss: 373.5446644CurrentTrain: epoch  8, batch    26 | loss: 339.5303464CurrentTrain: epoch  8, batch    27 | loss: 325.1541765CurrentTrain: epoch  8, batch    28 | loss: 339.5597301CurrentTrain: epoch  8, batch    29 | loss: 298.5435817CurrentTrain: epoch  8, batch    30 | loss: 313.3671613CurrentTrain: epoch  8, batch    31 | loss: 297.5671328CurrentTrain: epoch  8, batch    32 | loss: 314.4903886CurrentTrain: epoch  8, batch    33 | loss: 340.0365764CurrentTrain: epoch  8, batch    34 | loss: 297.8565160CurrentTrain: epoch  8, batch    35 | loss: 357.9608620CurrentTrain: epoch  8, batch    36 | loss: 298.0886029CurrentTrain: epoch  8, batch    37 | loss: 294.5016714CurrentTrain: epoch  8, batch    38 | loss: 317.8751378CurrentTrain: epoch  8, batch    39 | loss: 328.6072523CurrentTrain: epoch  8, batch    40 | loss: 389.8983025CurrentTrain: epoch  8, batch    41 | loss: 355.8382029CurrentTrain: epoch  8, batch    42 | loss: 437.1461197CurrentTrain: epoch  8, batch    43 | loss: 299.4866089CurrentTrain: epoch  8, batch    44 | loss: 325.1616470CurrentTrain: epoch  8, batch    45 | loss: 357.1424145CurrentTrain: epoch  8, batch    46 | loss: 392.5889280CurrentTrain: epoch  8, batch    47 | loss: 312.7504134CurrentTrain: epoch  8, batch    48 | loss: 330.3676753CurrentTrain: epoch  8, batch    49 | loss: 339.1403848CurrentTrain: epoch  8, batch    50 | loss: 357.6416082CurrentTrain: epoch  8, batch    51 | loss: 355.7552095CurrentTrain: epoch  8, batch    52 | loss: 373.4256985CurrentTrain: epoch  8, batch    53 | loss: 328.0751151CurrentTrain: epoch  8, batch    54 | loss: 328.1431793CurrentTrain: epoch  8, batch    55 | loss: 359.2251630CurrentTrain: epoch  8, batch    56 | loss: 309.7189655CurrentTrain: epoch  8, batch    57 | loss: 308.9734292CurrentTrain: epoch  8, batch    58 | loss: 315.7761903CurrentTrain: epoch  8, batch    59 | loss: 372.5487224CurrentTrain: epoch  8, batch    60 | loss: 278.6968195CurrentTrain: epoch  8, batch    61 | loss: 329.2482722CurrentTrain: epoch  8, batch    62 | loss: 355.8862568CurrentTrain: epoch  8, batch    63 | loss: 298.2567171CurrentTrain: epoch  8, batch    64 | loss: 269.2400641CurrentTrain: epoch  8, batch    65 | loss: 340.8629718CurrentTrain: epoch  8, batch    66 | loss: 356.1330544CurrentTrain: epoch  8, batch    67 | loss: 436.7676331CurrentTrain: epoch  8, batch    68 | loss: 340.7753315CurrentTrain: epoch  8, batch    69 | loss: 299.1547269CurrentTrain: epoch  8, batch    70 | loss: 372.4122479CurrentTrain: epoch  8, batch    71 | loss: 307.4922822CurrentTrain: epoch  8, batch    72 | loss: 328.5105399CurrentTrain: epoch  8, batch    73 | loss: 276.2809041CurrentTrain: epoch  8, batch    74 | loss: 286.6721315CurrentTrain: epoch  8, batch    75 | loss: 389.8351829CurrentTrain: epoch  8, batch    76 | loss: 324.7473122CurrentTrain: epoch  8, batch    77 | loss: 313.0437061CurrentTrain: epoch  8, batch    78 | loss: 340.2117886CurrentTrain: epoch  8, batch    79 | loss: 313.6203310CurrentTrain: epoch  8, batch    80 | loss: 356.5376915CurrentTrain: epoch  8, batch    81 | loss: 436.8310652CurrentTrain: epoch  8, batch    82 | loss: 226.8888603CurrentTrain: epoch  8, batch    83 | loss: 250.9579318CurrentTrain: epoch  8, batch    84 | loss: 390.4747011CurrentTrain: epoch  8, batch    85 | loss: 390.0076465CurrentTrain: epoch  8, batch    86 | loss: 357.4078876CurrentTrain: epoch  8, batch    87 | loss: 390.7549336CurrentTrain: epoch  8, batch    88 | loss: 328.2210816CurrentTrain: epoch  8, batch    89 | loss: 345.0007531CurrentTrain: epoch  8, batch    90 | loss: 356.4193231CurrentTrain: epoch  8, batch    91 | loss: 323.9794675CurrentTrain: epoch  8, batch    92 | loss: 325.8902095CurrentTrain: epoch  8, batch    93 | loss: 376.8136482CurrentTrain: epoch  8, batch    94 | loss: 455.8723560CurrentTrain: epoch  8, batch    95 | loss: 260.6707023CurrentTrain: epoch  9, batch     0 | loss: 308.8856185CurrentTrain: epoch  9, batch     1 | loss: 263.8505604CurrentTrain: epoch  9, batch     2 | loss: 372.6143222CurrentTrain: epoch  9, batch     3 | loss: 340.5566421CurrentTrain: epoch  9, batch     4 | loss: 496.9788164CurrentTrain: epoch  9, batch     5 | loss: 276.4171988CurrentTrain: epoch  9, batch     6 | loss: 374.0074177CurrentTrain: epoch  9, batch     7 | loss: 325.2451460CurrentTrain: epoch  9, batch     8 | loss: 418.4969930CurrentTrain: epoch  9, batch     9 | loss: 339.3931433CurrentTrain: epoch  9, batch    10 | loss: 356.3240814CurrentTrain: epoch  9, batch    11 | loss: 310.2037533CurrentTrain: epoch  9, batch    12 | loss: 536.6495609CurrentTrain: epoch  9, batch    13 | loss: 536.6123288CurrentTrain: epoch  9, batch    14 | loss: 269.4425963CurrentTrain: epoch  9, batch    15 | loss: 436.6772638CurrentTrain: epoch  9, batch    16 | loss: 455.2238686CurrentTrain: epoch  9, batch    17 | loss: 270.5347143CurrentTrain: epoch  9, batch    18 | loss: 272.3080434CurrentTrain: epoch  9, batch    19 | loss: 436.5740040CurrentTrain: epoch  9, batch    20 | loss: 355.9429470CurrentTrain: epoch  9, batch    21 | loss: 340.3084918CurrentTrain: epoch  9, batch    22 | loss: 389.4836091CurrentTrain: epoch  9, batch    23 | loss: 328.9548096CurrentTrain: epoch  9, batch    24 | loss: 373.3767578CurrentTrain: epoch  9, batch    25 | loss: 438.1814250CurrentTrain: epoch  9, batch    26 | loss: 402.9913022CurrentTrain: epoch  9, batch    27 | loss: 298.0684490CurrentTrain: epoch  9, batch    28 | loss: 311.2791788CurrentTrain: epoch  9, batch    29 | loss: 455.0661744CurrentTrain: epoch  9, batch    30 | loss: 340.6844852CurrentTrain: epoch  9, batch    31 | loss: 389.5534120CurrentTrain: epoch  9, batch    32 | loss: 356.7907116CurrentTrain: epoch  9, batch    33 | loss: 372.8303541CurrentTrain: epoch  9, batch    34 | loss: 357.3056893CurrentTrain: epoch  9, batch    35 | loss: 298.9877853CurrentTrain: epoch  9, batch    36 | loss: 313.5468948CurrentTrain: epoch  9, batch    37 | loss: 372.4432654CurrentTrain: epoch  9, batch    38 | loss: 355.4208304CurrentTrain: epoch  9, batch    39 | loss: 312.4982219CurrentTrain: epoch  9, batch    40 | loss: 250.7584757CurrentTrain: epoch  9, batch    41 | loss: 389.8778572CurrentTrain: epoch  9, batch    42 | loss: 373.8404329CurrentTrain: epoch  9, batch    43 | loss: 324.1402529CurrentTrain: epoch  9, batch    44 | loss: 300.9262038CurrentTrain: epoch  9, batch    45 | loss: 270.3183808CurrentTrain: epoch  9, batch    46 | loss: 372.1566827CurrentTrain: epoch  9, batch    47 | loss: 378.2133564CurrentTrain: epoch  9, batch    48 | loss: 340.2231992CurrentTrain: epoch  9, batch    49 | loss: 324.0210405CurrentTrain: epoch  9, batch    50 | loss: 328.1827366CurrentTrain: epoch  9, batch    51 | loss: 298.7568555CurrentTrain: epoch  9, batch    52 | loss: 339.1478421CurrentTrain: epoch  9, batch    53 | loss: 328.5331184CurrentTrain: epoch  9, batch    54 | loss: 389.9049109CurrentTrain: epoch  9, batch    55 | loss: 328.6444222CurrentTrain: epoch  9, batch    56 | loss: 315.6639578CurrentTrain: epoch  9, batch    57 | loss: 276.7086134CurrentTrain: epoch  9, batch    58 | loss: 455.0942774CurrentTrain: epoch  9, batch    59 | loss: 376.4447263CurrentTrain: epoch  9, batch    60 | loss: 276.2812217CurrentTrain: epoch  9, batch    61 | loss: 277.1680617CurrentTrain: epoch  9, batch    62 | loss: 314.1013135CurrentTrain: epoch  9, batch    63 | loss: 284.6979886CurrentTrain: epoch  9, batch    64 | loss: 323.8960938CurrentTrain: epoch  9, batch    65 | loss: 285.0849514CurrentTrain: epoch  9, batch    66 | loss: 283.7521848CurrentTrain: epoch  9, batch    67 | loss: 313.8956972CurrentTrain: epoch  9, batch    68 | loss: 328.2360876CurrentTrain: epoch  9, batch    69 | loss: 256.3833499CurrentTrain: epoch  9, batch    70 | loss: 323.8549713CurrentTrain: epoch  9, batch    71 | loss: 341.0161533CurrentTrain: epoch  9, batch    72 | loss: 309.6374765CurrentTrain: epoch  9, batch    73 | loss: 287.1224085CurrentTrain: epoch  9, batch    74 | loss: 356.5349400CurrentTrain: epoch  9, batch    75 | loss: 390.1498258CurrentTrain: epoch  9, batch    76 | loss: 357.4536746CurrentTrain: epoch  9, batch    77 | loss: 298.0907816CurrentTrain: epoch  9, batch    78 | loss: 294.9765203CurrentTrain: epoch  9, batch    79 | loss: 356.2927926CurrentTrain: epoch  9, batch    80 | loss: 340.2259121CurrentTrain: epoch  9, batch    81 | loss: 270.2081713CurrentTrain: epoch  9, batch    82 | loss: 313.2267351CurrentTrain: epoch  9, batch    83 | loss: 373.6756824CurrentTrain: epoch  9, batch    84 | loss: 339.4947512CurrentTrain: epoch  9, batch    85 | loss: 744.2173199CurrentTrain: epoch  9, batch    86 | loss: 328.7630346CurrentTrain: epoch  9, batch    87 | loss: 329.2433658CurrentTrain: epoch  9, batch    88 | loss: 324.1969431CurrentTrain: epoch  9, batch    89 | loss: 407.9175016CurrentTrain: epoch  9, batch    90 | loss: 356.4929248CurrentTrain: epoch  9, batch    91 | loss: 355.8290203CurrentTrain: epoch  9, batch    92 | loss: 317.4235008CurrentTrain: epoch  9, batch    93 | loss: 356.8080789CurrentTrain: epoch  9, batch    94 | loss: 305.1524246CurrentTrain: epoch  9, batch    95 | loss: 237.5003075

F1 score per class: {32: 0.6071428571428571, 6: 0.8505747126436781, 19: 0.2727272727272727, 24: 0.7619047619047619, 26: 0.9130434782608695, 29: 0.8795811518324608}
Micro-average F1 score: 0.7931034482758621
Weighted-average F1 score: 0.8045517388108449
F1 score per class: {32: 0.7083333333333334, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7567567567567568, 26: 0.9637305699481865, 29: 0.9025641025641026}
Micro-average F1 score: 0.8365878725590956
Weighted-average F1 score: 0.8405361347740211
F1 score per class: {32: 0.7120418848167539, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7567567567567568, 26: 0.9637305699481865, 29: 0.8979591836734694}
Micro-average F1 score: 0.8365878725590956
Weighted-average F1 score: 0.8407331385997823

F1 score per class: {32: 0.6071428571428571, 6: 0.8505747126436781, 19: 0.2727272727272727, 24: 0.7619047619047619, 26: 0.9130434782608695, 29: 0.8795811518324608}
Micro-average F1 score: 0.7931034482758621
Weighted-average F1 score: 0.8045517388108449
F1 score per class: {32: 0.7083333333333334, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7567567567567568, 26: 0.9637305699481865, 29: 0.9025641025641026}
Micro-average F1 score: 0.8365878725590956
Weighted-average F1 score: 0.8405361347740211
F1 score per class: {32: 0.7120418848167539, 6: 0.9010989010989011, 19: 0.46153846153846156, 24: 0.7567567567567568, 26: 0.9637305699481865, 29: 0.8979591836734694}
Micro-average F1 score: 0.8365878725590956
Weighted-average F1 score: 0.8407331385997823
cur_acc:  ['0.7931']
his_acc:  ['0.7931']
cur_acc des:  ['0.8366']
his_acc des:  ['0.8366']
cur_acc rrf:  ['0.8366']
his_acc rrf:  ['0.8366']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 368.6992775CurrentTrain: epoch  0, batch     1 | loss: 314.0059458CurrentTrain: epoch  0, batch     2 | loss: 408.6841490CurrentTrain: epoch  0, batch     3 | loss: 232.9286088CurrentTrain: epoch  1, batch     0 | loss: 458.9450399CurrentTrain: epoch  1, batch     1 | loss: 301.3459103CurrentTrain: epoch  1, batch     2 | loss: 341.3529626CurrentTrain: epoch  1, batch     3 | loss: 193.3859812CurrentTrain: epoch  2, batch     0 | loss: 381.1131977CurrentTrain: epoch  2, batch     1 | loss: 286.8853412CurrentTrain: epoch  2, batch     2 | loss: 349.6099575CurrentTrain: epoch  2, batch     3 | loss: 262.2219680CurrentTrain: epoch  3, batch     0 | loss: 348.8626949CurrentTrain: epoch  3, batch     1 | loss: 294.5737715CurrentTrain: epoch  3, batch     2 | loss: 367.0809858CurrentTrain: epoch  3, batch     3 | loss: 282.4152703CurrentTrain: epoch  4, batch     0 | loss: 293.0281759CurrentTrain: epoch  4, batch     1 | loss: 380.7408431CurrentTrain: epoch  4, batch     2 | loss: 305.8632035CurrentTrain: epoch  4, batch     3 | loss: 298.8524731CurrentTrain: epoch  5, batch     0 | loss: 363.0201302CurrentTrain: epoch  5, batch     1 | loss: 279.1927090CurrentTrain: epoch  5, batch     2 | loss: 322.0599739CurrentTrain: epoch  5, batch     3 | loss: 310.4782182CurrentTrain: epoch  6, batch     0 | loss: 336.1009592CurrentTrain: epoch  6, batch     1 | loss: 438.1203027CurrentTrain: epoch  6, batch     2 | loss: 293.6274771CurrentTrain: epoch  6, batch     3 | loss: 203.3933735CurrentTrain: epoch  7, batch     0 | loss: 313.2226443CurrentTrain: epoch  7, batch     1 | loss: 343.3590634CurrentTrain: epoch  7, batch     2 | loss: 318.6247371CurrentTrain: epoch  7, batch     3 | loss: 281.1795316CurrentTrain: epoch  8, batch     0 | loss: 312.7587440CurrentTrain: epoch  8, batch     1 | loss: 328.9162900CurrentTrain: epoch  8, batch     2 | loss: 392.5639458CurrentTrain: epoch  8, batch     3 | loss: 215.6772189CurrentTrain: epoch  9, batch     0 | loss: 315.3401271CurrentTrain: epoch  9, batch     1 | loss: 455.9243081CurrentTrain: epoch  9, batch     2 | loss: 391.1284870CurrentTrain: epoch  9, batch     3 | loss: 140.9653533
MemoryTrain:  epoch  0, batch     0 | loss: 3.4200098MemoryTrain:  epoch  1, batch     0 | loss: 2.6486246MemoryTrain:  epoch  2, batch     0 | loss: 2.2578151MemoryTrain:  epoch  3, batch     0 | loss: 1.7997042MemoryTrain:  epoch  4, batch     0 | loss: 1.5556122MemoryTrain:  epoch  5, batch     0 | loss: 1.3699577MemoryTrain:  epoch  6, batch     0 | loss: 1.1020707MemoryTrain:  epoch  7, batch     0 | loss: 0.9423374MemoryTrain:  epoch  8, batch     0 | loss: 0.7475015MemoryTrain:  epoch  9, batch     0 | loss: 0.6145328

F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.0, 6: 0.44776119402985076, 38: 0.0, 15: 0.0, 24: 0.8222222222222222, 25: 0.6813186813186813, 26: 0.6976744186046512}
Micro-average F1 score: 0.660377358490566
Weighted-average F1 score: 0.6507161184565493
F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.0, 38: 0.9166666666666666, 6: 0.0, 15: 0.0, 24: 0.9052631578947369, 25: 0.8118811881188119, 26: 0.8571428571428571}
Micro-average F1 score: 0.8390501319261213
Weighted-average F1 score: 0.8045884655950899
F1 score per class: {32: 0.0, 35: 0.8235294117647058, 37: 0.0, 38: 0.8817204301075269, 6: 0.0, 15: 0.0, 24: 0.9052631578947369, 25: 0.8118811881188119, 26: 0.8679245283018868}
Micro-average F1 score: 0.8310991957104558
Weighted-average F1 score: 0.7949538627008629

F1 score per class: {32: 0.751269035532995, 35: 0.8235294117647058, 37: 0.8304093567251462, 6: 0.4, 38: 0.44776119402985076, 15: 0.7567567567567568, 19: 0.9417989417989417, 24: 0.8465608465608465, 25: 0.7872340425531915, 26: 0.6526315789473685, 29: 0.6976744186046512}
Micro-average F1 score: 0.7752545027407988
Weighted-average F1 score: 0.7881002068590448
F1 score per class: {32: 0.7663551401869159, 35: 0.8235294117647058, 37: 0.8700564971751412, 6: 0.42424242424242425, 38: 0.9166666666666666, 15: 0.7567567567567568, 19: 0.9690721649484536, 24: 0.8601036269430051, 25: 0.8113207547169812, 26: 0.7735849056603774, 29: 0.8135593220338984}
Micro-average F1 score: 0.8289855072463768
Weighted-average F1 score: 0.8285251787158208
F1 score per class: {32: 0.7699530516431925, 35: 0.8235294117647058, 37: 0.8636363636363636, 6: 0.42424242424242425, 38: 0.8817204301075269, 15: 0.7567567567567568, 19: 0.9637305699481865, 24: 0.8601036269430051, 25: 0.8037383177570093, 26: 0.7663551401869159, 29: 0.8363636363636363}
Micro-average F1 score: 0.8250728862973761
Weighted-average F1 score: 0.8246105643991473
cur_acc:  ['0.7931', '0.6604']
his_acc:  ['0.7931', '0.7753']
cur_acc des:  ['0.8366', '0.8391']
his_acc des:  ['0.8366', '0.8290']
cur_acc rrf:  ['0.8366', '0.8311']
his_acc rrf:  ['0.8366', '0.8251']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 358.0727884CurrentTrain: epoch  0, batch     1 | loss: 412.8082058CurrentTrain: epoch  0, batch     2 | loss: 340.7642457CurrentTrain: epoch  0, batch     3 | loss: 181.0021042CurrentTrain: epoch  1, batch     0 | loss: 356.3616499CurrentTrain: epoch  1, batch     1 | loss: 322.8839441CurrentTrain: epoch  1, batch     2 | loss: 298.0266431CurrentTrain: epoch  1, batch     3 | loss: 281.5629758CurrentTrain: epoch  2, batch     0 | loss: 365.7706840CurrentTrain: epoch  2, batch     1 | loss: 325.8440298CurrentTrain: epoch  2, batch     2 | loss: 380.3163714CurrentTrain: epoch  2, batch     3 | loss: 185.2140258CurrentTrain: epoch  3, batch     0 | loss: 322.7775221CurrentTrain: epoch  3, batch     1 | loss: 359.6415697CurrentTrain: epoch  3, batch     2 | loss: 409.8747013CurrentTrain: epoch  3, batch     3 | loss: 175.3307319CurrentTrain: epoch  4, batch     0 | loss: 291.3125417CurrentTrain: epoch  4, batch     1 | loss: 335.1183338CurrentTrain: epoch  4, batch     2 | loss: 330.0884722CurrentTrain: epoch  4, batch     3 | loss: 276.5680552CurrentTrain: epoch  5, batch     0 | loss: 375.0009761CurrentTrain: epoch  5, batch     1 | loss: 291.1147963CurrentTrain: epoch  5, batch     2 | loss: 277.0559954CurrentTrain: epoch  5, batch     3 | loss: 273.6094296CurrentTrain: epoch  6, batch     0 | loss: 330.5628650CurrentTrain: epoch  6, batch     1 | loss: 329.3599919CurrentTrain: epoch  6, batch     2 | loss: 345.2515110CurrentTrain: epoch  6, batch     3 | loss: 226.7929061CurrentTrain: epoch  7, batch     0 | loss: 343.9691318CurrentTrain: epoch  7, batch     1 | loss: 328.7490178CurrentTrain: epoch  7, batch     2 | loss: 302.8597591CurrentTrain: epoch  7, batch     3 | loss: 238.4212119CurrentTrain: epoch  8, batch     0 | loss: 273.0211842CurrentTrain: epoch  8, batch     1 | loss: 391.6333750CurrentTrain: epoch  8, batch     2 | loss: 342.4079860CurrentTrain: epoch  8, batch     3 | loss: 223.8837401CurrentTrain: epoch  9, batch     0 | loss: 316.1141576CurrentTrain: epoch  9, batch     1 | loss: 312.2665297CurrentTrain: epoch  9, batch     2 | loss: 312.9663630CurrentTrain: epoch  9, batch     3 | loss: 273.2709714
MemoryTrain:  epoch  0, batch     0 | loss: 1.7346985MemoryTrain:  epoch  1, batch     0 | loss: 1.3537824MemoryTrain:  epoch  2, batch     0 | loss: 1.1220733MemoryTrain:  epoch  3, batch     0 | loss: 0.8476106MemoryTrain:  epoch  4, batch     0 | loss: 0.6612205MemoryTrain:  epoch  5, batch     0 | loss: 0.5436753MemoryTrain:  epoch  6, batch     0 | loss: 0.3991366MemoryTrain:  epoch  7, batch     0 | loss: 0.3519635MemoryTrain:  epoch  8, batch     0 | loss: 0.2628281MemoryTrain:  epoch  9, batch     0 | loss: 0.2570901

F1 score per class: {33: 0.734375, 36: 0.8131868131868132, 38: 0.0, 8: 0.0, 20: 0.9142857142857143, 26: 0.42857142857142855, 29: 0.5333333333333333, 30: 0.0}
Micro-average F1 score: 0.703601108033241
Weighted-average F1 score: 0.7178294061707524
F1 score per class: {33: 0.8827586206896552, 35: 0.9215686274509803, 36: 0.0, 37: 0.0, 38: 0.0, 8: 0.972972972972973, 20: 0.5333333333333333, 25: 0.0, 26: 0.8943089430894309, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.8526077097505669
Weighted-average F1 score: 0.8159728687844257
F1 score per class: {33: 0.8904109589041096, 36: 0.0, 37: 0.9423076923076923, 38: 0.0, 8: 0.0, 19: 0.972972972972973, 20: 0.5333333333333333, 26: 0.8943089430894309, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.8681818181818182
Weighted-average F1 score: 0.840261021756441

F1 score per class: {32: 0.5033112582781457, 33: 0.5987261146496815, 35: 0.8235294117647058, 36: 0.8304093567251462, 37: 0.6851851851851852, 6: 0.4, 38: 0.4, 8: 0.7526881720430108, 15: 0.9312169312169312, 19: 0.9142857142857143, 20: 0.8432432432432433, 24: 0.42857142857142855, 25: 0.7294117647058823, 26: 0.5333333333333333, 29: 0.6363636363636364, 30: 0.55}
Micro-average F1 score: 0.7061021170610212
Weighted-average F1 score: 0.7288314969169948
F1 score per class: {32: 0.6885245901639344, 33: 0.7032967032967034, 35: 0.8235294117647058, 36: 0.8950276243093923, 37: 0.7401574803149606, 6: 0.4375, 38: 0.6153846153846154, 8: 0.7567567567567568, 15: 0.9543147208121827, 19: 0.8181818181818182, 20: 0.8645833333333334, 24: 0.47058823529411764, 25: 0.8155339805825242, 26: 0.8333333333333334, 29: 0.6464646464646465, 30: 0.8}
Micro-average F1 score: 0.7817982456140351
Weighted-average F1 score: 0.7855050680886438
F1 score per class: {32: 0.6703910614525139, 33: 0.6878306878306878, 35: 0.8235294117647058, 36: 0.8839779005524862, 37: 0.7313432835820896, 6: 0.45161290322580644, 38: 0.5142857142857142, 8: 0.7567567567567568, 15: 0.9538461538461539, 19: 0.8181818181818182, 20: 0.8762886597938144, 24: 0.4444444444444444, 25: 0.84, 26: 0.8270676691729323, 29: 0.6464646464646465, 30: 0.7450980392156863}
Micro-average F1 score: 0.7736263736263737
Weighted-average F1 score: 0.7793660996872015
cur_acc:  ['0.7931', '0.6604', '0.7036']
his_acc:  ['0.7931', '0.7753', '0.7061']
cur_acc des:  ['0.8366', '0.8391', '0.8526']
his_acc des:  ['0.8366', '0.8290', '0.7818']
cur_acc rrf:  ['0.8366', '0.8311', '0.8682']
his_acc rrf:  ['0.8366', '0.8251', '0.7736']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 415.4529073CurrentTrain: epoch  0, batch     1 | loss: 335.5974428CurrentTrain: epoch  0, batch     2 | loss: 437.6175273CurrentTrain: epoch  0, batch     3 | loss: 346.0395627CurrentTrain: epoch  0, batch     4 | loss: 204.4676919CurrentTrain: epoch  1, batch     0 | loss: 359.1684138CurrentTrain: epoch  1, batch     1 | loss: 400.2781442CurrentTrain: epoch  1, batch     2 | loss: 430.8399334CurrentTrain: epoch  1, batch     3 | loss: 351.0143442CurrentTrain: epoch  1, batch     4 | loss: 223.6940081CurrentTrain: epoch  2, batch     0 | loss: 367.2384044CurrentTrain: epoch  2, batch     1 | loss: 381.6151005CurrentTrain: epoch  2, batch     2 | loss: 320.7571384CurrentTrain: epoch  2, batch     3 | loss: 349.7673856CurrentTrain: epoch  2, batch     4 | loss: 305.5815598CurrentTrain: epoch  3, batch     0 | loss: 394.0028025CurrentTrain: epoch  3, batch     1 | loss: 381.6489348CurrentTrain: epoch  3, batch     2 | loss: 303.3990790CurrentTrain: epoch  3, batch     3 | loss: 393.4297048CurrentTrain: epoch  3, batch     4 | loss: 200.4075438CurrentTrain: epoch  4, batch     0 | loss: 502.7902219CurrentTrain: epoch  4, batch     1 | loss: 540.1163293CurrentTrain: epoch  4, batch     2 | loss: 315.1519986CurrentTrain: epoch  4, batch     3 | loss: 300.2383570CurrentTrain: epoch  4, batch     4 | loss: 207.0586545CurrentTrain: epoch  5, batch     0 | loss: 438.4592268CurrentTrain: epoch  5, batch     1 | loss: 303.7923796CurrentTrain: epoch  5, batch     2 | loss: 358.8656333CurrentTrain: epoch  5, batch     3 | loss: 315.6863751CurrentTrain: epoch  5, batch     4 | loss: 251.0653401CurrentTrain: epoch  6, batch     0 | loss: 331.1824417CurrentTrain: epoch  6, batch     1 | loss: 374.3436325CurrentTrain: epoch  6, batch     2 | loss: 360.7231636CurrentTrain: epoch  6, batch     3 | loss: 358.6911513CurrentTrain: epoch  6, batch     4 | loss: 234.1618814CurrentTrain: epoch  7, batch     0 | loss: 343.1765194CurrentTrain: epoch  7, batch     1 | loss: 374.7957795CurrentTrain: epoch  7, batch     2 | loss: 392.8860198CurrentTrain: epoch  7, batch     3 | loss: 327.1178208CurrentTrain: epoch  7, batch     4 | loss: 203.3672338CurrentTrain: epoch  8, batch     0 | loss: 391.8668498CurrentTrain: epoch  8, batch     1 | loss: 456.2034997CurrentTrain: epoch  8, batch     2 | loss: 312.1915421CurrentTrain: epoch  8, batch     3 | loss: 314.9672254CurrentTrain: epoch  8, batch     4 | loss: 203.3123838CurrentTrain: epoch  9, batch     0 | loss: 285.9744862CurrentTrain: epoch  9, batch     1 | loss: 374.4605685CurrentTrain: epoch  9, batch     2 | loss: 346.2290488CurrentTrain: epoch  9, batch     3 | loss: 391.7567039CurrentTrain: epoch  9, batch     4 | loss: 216.8795341
MemoryTrain:  epoch  0, batch     0 | loss: 2.3071003MemoryTrain:  epoch  1, batch     0 | loss: 1.8706037MemoryTrain:  epoch  2, batch     0 | loss: 1.6983222MemoryTrain:  epoch  3, batch     0 | loss: 1.4313512MemoryTrain:  epoch  4, batch     0 | loss: 1.1499313MemoryTrain:  epoch  5, batch     0 | loss: 0.8694083MemoryTrain:  epoch  6, batch     0 | loss: 0.7609809MemoryTrain:  epoch  7, batch     0 | loss: 0.6856587MemoryTrain:  epoch  8, batch     0 | loss: 0.5375807MemoryTrain:  epoch  9, batch     0 | loss: 0.4080728

F1 score per class: {32: 0.3247863247863248, 1: 0.5982905982905983, 34: 0.0, 3: 0.7692307692307693, 35: 0.0, 37: 0.0, 14: 0.0, 22: 0.9215686274509803, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.565008025682183
Weighted-average F1 score: 0.5751436472024707
F1 score per class: {32: 0.4, 1: 0.7851851851851852, 34: 0.0, 3: 0.05405405405405406, 35: 0.581081081081081, 37: 0.0, 36: 0.0, 8: 0.0, 14: 0.9423076923076923, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.5119760479041916
Weighted-average F1 score: 0.4514720993182531
F1 score per class: {32: 0.39669421487603307, 1: 0.8029197080291971, 34: 0.0, 3: 0.05405405405405406, 35: 0.6503067484662577, 37: 0.0, 36: 0.0, 8: 0.0, 14: 0.9523809523809523, 22: 0.0, 24: 0.0, 26: 0.0}
Micro-average F1 score: 0.5517241379310345
Weighted-average F1 score: 0.5059970984818855

F1 score per class: {1: 0.304, 3: 0.5109489051094891, 6: 0.4492753623188406, 8: 0.38181818181818183, 14: 0.0, 15: 0.8235294117647058, 19: 0.5815602836879432, 20: 0.7155963302752294, 22: 0.746268656716418, 24: 0.07407407407407407, 25: 0.42424242424242425, 26: 0.7589743589743589, 29: 0.9197860962566845, 30: 0.9142857142857143, 32: 0.718562874251497, 33: 0.42857142857142855, 34: 0.47, 35: 0.08823529411764706, 36: 0.2631578947368421, 37: 0.2318840579710145, 38: 0.43243243243243246}
Micro-average F1 score: 0.546617915904936
Weighted-average F1 score: 0.5879993648737556
F1 score per class: {1: 0.36363636363636365, 3: 0.6162790697674418, 6: 0.6704545454545454, 8: 0.6703296703296703, 14: 0.05263157894736842, 15: 0.8235294117647058, 19: 0.6928104575163399, 20: 0.6962962962962963, 22: 0.5657894736842105, 24: 0.11764705882352941, 25: 0.56, 26: 0.7539267015706806, 29: 0.9538461538461539, 30: 0.8, 32: 0.8, 33: 0.375, 34: 0.632258064516129, 35: 0.2727272727272727, 36: 0.8125, 37: 0.35, 38: 0.7407407407407407}
Micro-average F1 score: 0.6272617611580217
Weighted-average F1 score: 0.6296277684302741
F1 score per class: {1: 0.35555555555555557, 3: 0.632183908045977, 6: 0.5853658536585366, 8: 0.6451612903225806, 14: 0.05194805194805195, 15: 0.8235294117647058, 19: 0.7261146496815286, 20: 0.696969696969697, 22: 0.6309523809523809, 24: 0.125, 25: 0.4788732394366197, 26: 0.7539267015706806, 29: 0.9479166666666666, 30: 0.9473684210526315, 32: 0.8131868131868132, 33: 0.375, 34: 0.5405405405405406, 35: 0.2823529411764706, 36: 0.6019417475728155, 37: 0.3333333333333333, 38: 0.6530612244897959}
Micro-average F1 score: 0.6112012987012987
Weighted-average F1 score: 0.6148588037807444
cur_acc:  ['0.7931', '0.6604', '0.7036', '0.5650']
his_acc:  ['0.7931', '0.7753', '0.7061', '0.5466']
cur_acc des:  ['0.8366', '0.8391', '0.8526', '0.5120']
his_acc des:  ['0.8366', '0.8290', '0.7818', '0.6273']
cur_acc rrf:  ['0.8366', '0.8311', '0.8682', '0.5517']
his_acc rrf:  ['0.8366', '0.8251', '0.7736', '0.6112']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 359.6152272CurrentTrain: epoch  0, batch     1 | loss: 382.5713593CurrentTrain: epoch  0, batch     2 | loss: 382.6828601CurrentTrain: epoch  0, batch     3 | loss: 363.6371504CurrentTrain: epoch  0, batch     4 | loss: 256.3674710CurrentTrain: epoch  1, batch     0 | loss: 386.4516732CurrentTrain: epoch  1, batch     1 | loss: 413.3259544CurrentTrain: epoch  1, batch     2 | loss: 319.2466780CurrentTrain: epoch  1, batch     3 | loss: 368.0482906CurrentTrain: epoch  1, batch     4 | loss: 229.1431185CurrentTrain: epoch  2, batch     0 | loss: 444.0794541CurrentTrain: epoch  2, batch     1 | loss: 367.7825659CurrentTrain: epoch  2, batch     2 | loss: 377.2199317CurrentTrain: epoch  2, batch     3 | loss: 345.7608989CurrentTrain: epoch  2, batch     4 | loss: 202.0997046CurrentTrain: epoch  3, batch     0 | loss: 380.9350109CurrentTrain: epoch  3, batch     1 | loss: 320.9404598CurrentTrain: epoch  3, batch     2 | loss: 458.6478161CurrentTrain: epoch  3, batch     3 | loss: 335.2817468CurrentTrain: epoch  3, batch     4 | loss: 211.3522064CurrentTrain: epoch  4, batch     0 | loss: 316.6017514CurrentTrain: epoch  4, batch     1 | loss: 332.6497167CurrentTrain: epoch  4, batch     2 | loss: 364.2527068CurrentTrain: epoch  4, batch     3 | loss: 393.5026689CurrentTrain: epoch  4, batch     4 | loss: 342.3854665CurrentTrain: epoch  5, batch     0 | loss: 359.8391117CurrentTrain: epoch  5, batch     1 | loss: 439.0010595CurrentTrain: epoch  5, batch     2 | loss: 313.4111790CurrentTrain: epoch  5, batch     3 | loss: 360.4525484CurrentTrain: epoch  5, batch     4 | loss: 234.6764931CurrentTrain: epoch  6, batch     0 | loss: 395.0856936CurrentTrain: epoch  6, batch     1 | loss: 342.9187611CurrentTrain: epoch  6, batch     2 | loss: 391.7848368CurrentTrain: epoch  6, batch     3 | loss: 343.8827738CurrentTrain: epoch  6, batch     4 | loss: 233.7041424CurrentTrain: epoch  7, batch     0 | loss: 377.0662941CurrentTrain: epoch  7, batch     1 | loss: 341.7174454CurrentTrain: epoch  7, batch     2 | loss: 373.9509566CurrentTrain: epoch  7, batch     3 | loss: 358.2482924CurrentTrain: epoch  7, batch     4 | loss: 252.6524075CurrentTrain: epoch  8, batch     0 | loss: 373.8810962CurrentTrain: epoch  8, batch     1 | loss: 300.7645753CurrentTrain: epoch  8, batch     2 | loss: 374.1519433CurrentTrain: epoch  8, batch     3 | loss: 408.9178667CurrentTrain: epoch  8, batch     4 | loss: 219.3123102CurrentTrain: epoch  9, batch     0 | loss: 341.3234327CurrentTrain: epoch  9, batch     1 | loss: 325.9138432CurrentTrain: epoch  9, batch     2 | loss: 391.2099151CurrentTrain: epoch  9, batch     3 | loss: 329.5550411CurrentTrain: epoch  9, batch     4 | loss: 283.0275277
MemoryTrain:  epoch  0, batch     0 | loss: 1.7360797MemoryTrain:  epoch  1, batch     0 | loss: 1.4561492MemoryTrain:  epoch  2, batch     0 | loss: 1.1238253MemoryTrain:  epoch  3, batch     0 | loss: 1.0361738MemoryTrain:  epoch  4, batch     0 | loss: 0.8145400MemoryTrain:  epoch  5, batch     0 | loss: 0.6592255MemoryTrain:  epoch  6, batch     0 | loss: 0.5271678MemoryTrain:  epoch  7, batch     0 | loss: 0.4303321MemoryTrain:  epoch  8, batch     0 | loss: 0.3996738MemoryTrain:  epoch  9, batch     0 | loss: 0.3538608

F1 score per class: {34: 0.9690721649484536, 5: 0.0, 38: 0.0, 6: 0.37398373983739835, 8: 0.7755102040816326, 10: 0.36363636363636365, 16: 0.15, 17: 0.0, 18: 0.0, 20: 0.0}
Micro-average F1 score: 0.636568848758465
Weighted-average F1 score: 0.6922537492589135
F1 score per class: {34: 1.0, 5: 0.0, 38: 0.0, 6: 0.5972222222222222, 8: 0.0, 10: 0.9473684210526315, 14: 0.2, 16: 0.7868852459016393, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7235621521335807
Weighted-average F1 score: 0.6499694716647838
F1 score per class: {34: 1.0, 5: 0.0, 38: 0.0, 6: 0.6490066225165563, 8: 0.0, 10: 0.9473684210526315, 14: 0.2, 16: 0.7241379310344828, 17: 0.0, 18: 0.0}
Micro-average F1 score: 0.7388059701492538
Weighted-average F1 score: 0.6695583910326521

F1 score per class: {1: 0.3384615384615385, 3: 0.19801980198019803, 5: 0.8703703703703703, 6: 0.4861111111111111, 8: 0.02197802197802198, 10: 0.3511450381679389, 14: 0.0, 15: 0.8235294117647058, 16: 0.7755102040816326, 17: 0.2, 18: 0.12, 19: 0.5074626865671642, 20: 0.5833333333333334, 22: 0.7078651685393258, 24: 0.15384615384615385, 25: 0.375, 26: 0.7597765363128491, 29: 0.8715083798882681, 30: 0.9142857142857143, 32: 0.718562874251497, 33: 0.42857142857142855, 34: 0.5423728813559322, 35: 0.21176470588235294, 36: 0.1917808219178082, 37: 0.2465753424657534, 38: 0.18181818181818182}
Micro-average F1 score: 0.5180089032780251
Weighted-average F1 score: 0.5909541181698411
F1 score per class: {1: 0.3384615384615385, 3: 0.46258503401360546, 5: 0.8620689655172413, 6: 0.7157894736842105, 8: 0.5806451612903226, 10: 0.5584415584415584, 14: 0.02631578947368421, 15: 0.8235294117647058, 16: 0.8852459016393442, 17: 0.125, 18: 0.47058823529411764, 19: 0.7012987012987013, 20: 0.7289719626168224, 22: 0.691358024691358, 24: 0.14925373134328357, 25: 0.6987951807228916, 26: 0.7555555555555555, 29: 0.9090909090909091, 30: 0.8095238095238095, 32: 0.8, 33: 0.35294117647058826, 34: 0.5528455284552846, 35: 0.3247863247863248, 36: 0.7090909090909091, 37: 0.4146341463414634, 38: 0.45161290322580644}
Micro-average F1 score: 0.6246648793565683
Weighted-average F1 score: 0.6225276347992005
F1 score per class: {1: 0.34074074074074073, 3: 0.47297297297297297, 5: 0.8438818565400844, 6: 0.6918918918918919, 8: 0.5348837209302325, 10: 0.5868263473053892, 14: 0.02631578947368421, 15: 0.8235294117647058, 16: 0.9, 17: 0.1, 18: 0.4375, 19: 0.7012987012987013, 20: 0.6782608695652174, 22: 0.7222222222222222, 24: 0.09090909090909091, 25: 0.4927536231884058, 26: 0.7555555555555555, 29: 0.9139784946236559, 30: 0.8717948717948718, 32: 0.819672131147541, 33: 0.4, 34: 0.5333333333333333, 35: 0.3333333333333333, 36: 0.58, 37: 0.35443037974683544, 38: 0.39215686274509803}
Micro-average F1 score: 0.6134595874196821
Weighted-average F1 score: 0.6179406767053054
cur_acc:  ['0.7931', '0.6604', '0.7036', '0.5650', '0.6366']
his_acc:  ['0.7931', '0.7753', '0.7061', '0.5466', '0.5180']
cur_acc des:  ['0.8366', '0.8391', '0.8526', '0.5120', '0.7236']
his_acc des:  ['0.8366', '0.8290', '0.7818', '0.6273', '0.6247']
cur_acc rrf:  ['0.8366', '0.8311', '0.8682', '0.5517', '0.7388']
his_acc rrf:  ['0.8366', '0.8251', '0.7736', '0.6112', '0.6135']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 356.0598163CurrentTrain: epoch  0, batch     1 | loss: 335.1099165CurrentTrain: epoch  0, batch     2 | loss: 347.5124901CurrentTrain: epoch  0, batch     3 | loss: 320.5544097CurrentTrain: epoch  1, batch     0 | loss: 404.7628963CurrentTrain: epoch  1, batch     1 | loss: 339.3206695CurrentTrain: epoch  1, batch     2 | loss: 331.5816808CurrentTrain: epoch  1, batch     3 | loss: 285.1081170CurrentTrain: epoch  2, batch     0 | loss: 336.5780709CurrentTrain: epoch  2, batch     1 | loss: 383.0310193CurrentTrain: epoch  2, batch     2 | loss: 286.8008134CurrentTrain: epoch  2, batch     3 | loss: 329.3422830CurrentTrain: epoch  3, batch     0 | loss: 418.9321053CurrentTrain: epoch  3, batch     1 | loss: 279.0598078CurrentTrain: epoch  3, batch     2 | loss: 337.5071146CurrentTrain: epoch  3, batch     3 | loss: 261.5682178CurrentTrain: epoch  4, batch     0 | loss: 450.2910001CurrentTrain: epoch  4, batch     1 | loss: 364.7214409CurrentTrain: epoch  4, batch     2 | loss: 318.9337219CurrentTrain: epoch  4, batch     3 | loss: 242.8447336CurrentTrain: epoch  5, batch     0 | loss: 326.8476004CurrentTrain: epoch  5, batch     1 | loss: 265.0095682CurrentTrain: epoch  5, batch     2 | loss: 381.8963293CurrentTrain: epoch  5, batch     3 | loss: 368.4248370CurrentTrain: epoch  6, batch     0 | loss: 396.3592817CurrentTrain: epoch  6, batch     1 | loss: 343.1118378CurrentTrain: epoch  6, batch     2 | loss: 311.6227804CurrentTrain: epoch  6, batch     3 | loss: 247.2862610CurrentTrain: epoch  7, batch     0 | loss: 315.4911932CurrentTrain: epoch  7, batch     1 | loss: 456.4350077CurrentTrain: epoch  7, batch     2 | loss: 301.1341978CurrentTrain: epoch  7, batch     3 | loss: 253.4842106CurrentTrain: epoch  8, batch     0 | loss: 314.2712529CurrentTrain: epoch  8, batch     1 | loss: 344.1376663CurrentTrain: epoch  8, batch     2 | loss: 317.6377461CurrentTrain: epoch  8, batch     3 | loss: 294.7856638CurrentTrain: epoch  9, batch     0 | loss: 317.8322342CurrentTrain: epoch  9, batch     1 | loss: 357.5840335CurrentTrain: epoch  9, batch     2 | loss: 339.8929816CurrentTrain: epoch  9, batch     3 | loss: 242.9400023
MemoryTrain:  epoch  0, batch     0 | loss: 1.1134822MemoryTrain:  epoch  1, batch     0 | loss: 0.9636873MemoryTrain:  epoch  2, batch     0 | loss: 0.6991186MemoryTrain:  epoch  3, batch     0 | loss: 0.5219636MemoryTrain:  epoch  4, batch     0 | loss: 0.4120400MemoryTrain:  epoch  5, batch     0 | loss: 0.3427041MemoryTrain:  epoch  6, batch     0 | loss: 0.2642914MemoryTrain:  epoch  7, batch     0 | loss: 0.2469469MemoryTrain:  epoch  8, batch     0 | loss: 0.1668347MemoryTrain:  epoch  9, batch     0 | loss: 0.1500893

F1 score per class: {0: 0.9117647058823529, 32: 0.0, 34: 0.9130434782608695, 1: 0.75, 4: 0.0, 13: 0.3684210526315789, 14: 0.0, 21: 0.7948717948717948, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.7979539641943734
Weighted-average F1 score: 0.7861422869339922
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 34: 0.9361702127659575, 1: 0.8888888888888888, 4: 0.0, 37: 0.0, 13: 0.6521739130434783, 14: 0.0, 20: 0.8292682926829268, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8271028037383178
Weighted-average F1 score: 0.7704441467960718
F1 score per class: {0: 0.9863013698630136, 32: 0.0, 34: 0.9417989417989417, 1: 0.0, 4: 0.8888888888888888, 37: 0.0, 5: 0.0, 13: 0.6521739130434783, 14: 0.0, 20: 0.8148148148148148, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8271028037383178
Weighted-average F1 score: 0.7710039571638737

F1 score per class: {0: 0.9117647058823529, 1: 0.3181818181818182, 3: 0.3652173913043478, 4: 0.9130434782608695, 5: 0.8663594470046083, 6: 0.5477707006369427, 8: 0.1111111111111111, 10: 0.12727272727272726, 13: 0.06315789473684211, 14: 0.028169014084507043, 15: 0.75, 16: 0.8461538461538461, 17: 0.2, 18: 0.13636363636363635, 19: 0.7261146496815286, 20: 0.6153846153846154, 21: 0.20588235294117646, 22: 0.631578947368421, 23: 0.7654320987654321, 24: 0.0, 25: 0.4, 26: 0.7379679144385026, 29: 0.8961748633879781, 30: 0.9444444444444444, 32: 0.7337278106508875, 33: 0.42857142857142855, 34: 0.5789473684210527, 35: 0.24242424242424243, 36: 0.1917808219178082, 37: 0.425, 38: 0.18181818181818182}
Micro-average F1 score: 0.5557782231128925
Weighted-average F1 score: 0.590059868640022
F1 score per class: {0: 0.96, 1: 0.3582089552238806, 3: 0.5064935064935064, 4: 0.9361702127659575, 5: 0.8658008658008658, 6: 0.7357512953367875, 8: 0.6210526315789474, 10: 0.49295774647887325, 13: 0.21621621621621623, 14: 0.05333333333333334, 15: 0.75, 16: 0.9333333333333333, 17: 0.375, 18: 0.391304347826087, 19: 0.8235294117647058, 20: 0.7884615384615384, 21: 0.3191489361702128, 22: 0.6540880503144654, 23: 0.7906976744186046, 24: 0.0, 25: 0.65, 26: 0.71875, 29: 0.9326424870466321, 30: 0.926829268292683, 32: 0.8216216216216217, 33: 0.47058823529411764, 34: 0.5801526717557252, 35: 0.358974358974359, 36: 0.6727272727272727, 37: 0.5567010309278351, 38: 0.509090909090909}
Micro-average F1 score: 0.6612810155799193
Weighted-average F1 score: 0.6586265017909003
F1 score per class: {0: 0.972972972972973, 1: 0.35036496350364965, 3: 0.49673202614379086, 4: 0.9417989417989417, 5: 0.8547008547008547, 6: 0.7291666666666666, 8: 0.5421686746987951, 10: 0.40601503759398494, 13: 0.11764705882352941, 14: 0.05333333333333334, 15: 0.75, 16: 0.9333333333333333, 17: 0.42105263157894735, 18: 0.3076923076923077, 19: 0.8275862068965517, 20: 0.7454545454545455, 21: 0.3157894736842105, 22: 0.6540880503144654, 23: 0.7764705882352941, 24: 0.0, 25: 0.5, 26: 0.7225130890052356, 29: 0.9319371727748691, 30: 0.95, 32: 0.8279569892473119, 33: 0.375, 34: 0.5673758865248227, 35: 0.33043478260869563, 36: 0.6909090909090909, 37: 0.5416666666666666, 38: 0.4583333333333333}
Micro-average F1 score: 0.6415531729933353
Weighted-average F1 score: 0.6378379320643147
cur_acc:  ['0.7931', '0.6604', '0.7036', '0.5650', '0.6366', '0.7980']
his_acc:  ['0.7931', '0.7753', '0.7061', '0.5466', '0.5180', '0.5558']
cur_acc des:  ['0.8366', '0.8391', '0.8526', '0.5120', '0.7236', '0.8271']
his_acc des:  ['0.8366', '0.8290', '0.7818', '0.6273', '0.6247', '0.6613']
cur_acc rrf:  ['0.8366', '0.8311', '0.8682', '0.5517', '0.7388', '0.8271']
his_acc rrf:  ['0.8366', '0.8251', '0.7736', '0.6112', '0.6135', '0.6416']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 439.5079091CurrentTrain: epoch  0, batch     1 | loss: 362.1923799CurrentTrain: epoch  0, batch     2 | loss: 416.5045637CurrentTrain: epoch  0, batch     3 | loss: 279.7459520CurrentTrain: epoch  0, batch     4 | loss: 120.4860614CurrentTrain: epoch  1, batch     0 | loss: 382.2903161CurrentTrain: epoch  1, batch     1 | loss: 317.8769379CurrentTrain: epoch  1, batch     2 | loss: 344.5558747CurrentTrain: epoch  1, batch     3 | loss: 357.5342662CurrentTrain: epoch  1, batch     4 | loss: 119.8990688CurrentTrain: epoch  2, batch     0 | loss: 400.6274700CurrentTrain: epoch  2, batch     1 | loss: 430.7360602CurrentTrain: epoch  2, batch     2 | loss: 367.0501055CurrentTrain: epoch  2, batch     3 | loss: 288.7707616CurrentTrain: epoch  2, batch     4 | loss: 57.9568087CurrentTrain: epoch  3, batch     0 | loss: 367.5822971CurrentTrain: epoch  3, batch     1 | loss: 305.7539260CurrentTrain: epoch  3, batch     2 | loss: 335.5177974CurrentTrain: epoch  3, batch     3 | loss: 441.1859805CurrentTrain: epoch  3, batch     4 | loss: 78.4188891CurrentTrain: epoch  4, batch     0 | loss: 320.5429367CurrentTrain: epoch  4, batch     1 | loss: 541.0078228CurrentTrain: epoch  4, batch     2 | loss: 348.3309634CurrentTrain: epoch  4, batch     3 | loss: 289.0144208CurrentTrain: epoch  4, batch     4 | loss: 78.1626227CurrentTrain: epoch  5, batch     0 | loss: 364.2278560CurrentTrain: epoch  5, batch     1 | loss: 440.4363862CurrentTrain: epoch  5, batch     2 | loss: 362.9880694CurrentTrain: epoch  5, batch     3 | loss: 361.3240933CurrentTrain: epoch  5, batch     4 | loss: 25.7772847CurrentTrain: epoch  6, batch     0 | loss: 392.9005096CurrentTrain: epoch  6, batch     1 | loss: 317.1416634CurrentTrain: epoch  6, batch     2 | loss: 331.9291725CurrentTrain: epoch  6, batch     3 | loss: 329.0742909CurrentTrain: epoch  6, batch     4 | loss: 119.8309470CurrentTrain: epoch  7, batch     0 | loss: 409.1545348CurrentTrain: epoch  7, batch     1 | loss: 312.3428190CurrentTrain: epoch  7, batch     2 | loss: 328.3138734CurrentTrain: epoch  7, batch     3 | loss: 346.7880785CurrentTrain: epoch  7, batch     4 | loss: 75.9215498CurrentTrain: epoch  8, batch     0 | loss: 359.0412626CurrentTrain: epoch  8, batch     1 | loss: 341.3098344CurrentTrain: epoch  8, batch     2 | loss: 440.1103656CurrentTrain: epoch  8, batch     3 | loss: 299.1418876CurrentTrain: epoch  8, batch     4 | loss: 55.0425297CurrentTrain: epoch  9, batch     0 | loss: 298.6456128CurrentTrain: epoch  9, batch     1 | loss: 341.3757375CurrentTrain: epoch  9, batch     2 | loss: 375.0837863CurrentTrain: epoch  9, batch     3 | loss: 408.5353972CurrentTrain: epoch  9, batch     4 | loss: 55.2945411
MemoryTrain:  epoch  0, batch     0 | loss: 1.2184419MemoryTrain:  epoch  1, batch     0 | loss: 1.0287495MemoryTrain:  epoch  2, batch     0 | loss: 0.8218324MemoryTrain:  epoch  3, batch     0 | loss: 0.6977983MemoryTrain:  epoch  4, batch     0 | loss: 0.5393313MemoryTrain:  epoch  5, batch     0 | loss: 0.4413840MemoryTrain:  epoch  6, batch     0 | loss: 0.4183226MemoryTrain:  epoch  7, batch     0 | loss: 0.3347642MemoryTrain:  epoch  8, batch     0 | loss: 0.3007821MemoryTrain:  epoch  9, batch     0 | loss: 0.2956954

F1 score per class: {0: 0.0, 2: 0.8, 34: 0.0, 6: 0.0, 39: 0.6564885496183206, 8: 0.4925373134328358, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 18: 0.6153846153846154, 19: 0.0, 28: 0.5263157894736842}
Micro-average F1 score: 0.5465465465465466
Weighted-average F1 score: 0.4866998097771552
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8421052631578947, 12: 0.7577639751552795, 13: 0.0, 14: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 28: 0.9333333333333333, 33: 0.0, 34: 0.0, 38: 0.0, 39: 0.7272727272727273}
Micro-average F1 score: 0.7135922330097088
Weighted-average F1 score: 0.6144811112854531
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8571428571428571, 12: 0.7375, 13: 0.0, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 28: 0.8571428571428571, 33: 0.0, 34: 0.0, 38: 0.0, 39: 0.8333333333333334}
Micro-average F1 score: 0.729064039408867
Weighted-average F1 score: 0.6439712248535777

F1 score per class: {0: 0.8985507246376812, 1: 0.3053435114503817, 2: 0.6666666666666666, 3: 0.17142857142857143, 4: 0.8700564971751412, 5: 0.8663594470046083, 6: 0.6136363636363636, 8: 0.24, 10: 0.14678899082568808, 11: 0.39814814814814814, 12: 0.45517241379310347, 13: 0.14285714285714285, 14: 0.02857142857142857, 15: 0.75, 16: 0.7931034482758621, 17: 0.0, 18: 0.047619047619047616, 19: 0.7657142857142857, 20: 0.5581395348837209, 21: 0.12903225806451613, 22: 0.6308724832214765, 23: 0.8235294117647058, 24: 0.0, 25: 0.4, 26: 0.7415730337078652, 28: 0.17391304347826086, 29: 0.8961748633879781, 30: 0.972972972972973, 32: 0.7932960893854749, 33: 0.42857142857142855, 34: 0.40540540540540543, 35: 0.2619047619047619, 36: 0.08695652173913043, 37: 0.3013698630136986, 38: 0.125, 39: 0.2702702702702703}
Micro-average F1 score: 0.5398442180946674
Weighted-average F1 score: 0.5881820447896442
F1 score per class: {0: 0.972972972972973, 1: 0.3283582089552239, 2: 0.4375, 3: 0.6097560975609756, 4: 0.8571428571428571, 5: 0.8722466960352423, 6: 0.7632850241545893, 8: 0.5485714285714286, 10: 0.463768115942029, 11: 0.5423728813559322, 12: 0.5980392156862745, 13: 0.1, 14: 0.04878048780487805, 15: 0.75, 16: 0.8135593220338984, 17: 0.0, 18: 0.12307692307692308, 19: 0.7931034482758621, 20: 0.7070707070707071, 21: 0.2823529411764706, 22: 0.5771812080536913, 23: 0.8247422680412371, 24: 0.0425531914893617, 25: 0.56, 26: 0.7252747252747253, 28: 0.3684210526315789, 29: 0.8961748633879781, 30: 0.95, 32: 0.845360824742268, 33: 0.375, 34: 0.6326530612244898, 35: 0.37735849056603776, 36: 0.48936170212765956, 37: 0.5111111111111111, 38: 0.4090909090909091, 39: 0.36363636363636365}
Micro-average F1 score: 0.6212590299277606
Weighted-average F1 score: 0.6199420550541731
F1 score per class: {0: 0.972972972972973, 1: 0.32592592592592595, 2: 0.5384615384615384, 3: 0.588957055214724, 4: 0.8700564971751412, 5: 0.8761061946902655, 6: 0.7609756097560976, 8: 0.3548387096774194, 10: 0.3333333333333333, 11: 0.515625, 12: 0.5728155339805825, 13: 0.1111111111111111, 14: 0.05128205128205128, 15: 0.75, 16: 0.8135593220338984, 17: 0.0, 18: 0.1509433962264151, 19: 0.7865168539325843, 20: 0.7058823529411765, 21: 0.26373626373626374, 22: 0.631578947368421, 23: 0.8421052631578947, 24: 0.05555555555555555, 25: 0.4788732394366197, 26: 0.7292817679558011, 28: 0.19672131147540983, 29: 0.8961748633879781, 30: 0.95, 32: 0.845360824742268, 33: 0.4, 34: 0.631578947368421, 35: 0.36538461538461536, 36: 0.4235294117647059, 37: 0.4772727272727273, 38: 0.425531914893617, 39: 0.37037037037037035}
Micro-average F1 score: 0.605098855359001
Weighted-average F1 score: 0.6068063627791106
cur_acc:  ['0.7931', '0.6604', '0.7036', '0.5650', '0.6366', '0.7980', '0.5465']
his_acc:  ['0.7931', '0.7753', '0.7061', '0.5466', '0.5180', '0.5558', '0.5398']
cur_acc des:  ['0.8366', '0.8391', '0.8526', '0.5120', '0.7236', '0.8271', '0.7136']
his_acc des:  ['0.8366', '0.8290', '0.7818', '0.6273', '0.6247', '0.6613', '0.6213']
cur_acc rrf:  ['0.8366', '0.8311', '0.8682', '0.5517', '0.7388', '0.8271', '0.7291']
his_acc rrf:  ['0.8366', '0.8251', '0.7736', '0.6112', '0.6135', '0.6416', '0.6051']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 356.1347441CurrentTrain: epoch  0, batch     1 | loss: 358.0995288CurrentTrain: epoch  0, batch     2 | loss: 281.2672134CurrentTrain: epoch  0, batch     3 | loss: 31.5227481CurrentTrain: epoch  1, batch     0 | loss: 360.3359687CurrentTrain: epoch  1, batch     1 | loss: 287.3499632CurrentTrain: epoch  1, batch     2 | loss: 324.9973381CurrentTrain: epoch  1, batch     3 | loss: 25.6201234CurrentTrain: epoch  2, batch     0 | loss: 349.8815151CurrentTrain: epoch  2, batch     1 | loss: 306.1026917CurrentTrain: epoch  2, batch     2 | loss: 308.0862486CurrentTrain: epoch  2, batch     3 | loss: 55.2223008CurrentTrain: epoch  3, batch     0 | loss: 307.7384730CurrentTrain: epoch  3, batch     1 | loss: 334.4483758CurrentTrain: epoch  3, batch     2 | loss: 299.1491983CurrentTrain: epoch  3, batch     3 | loss: 54.9719994CurrentTrain: epoch  4, batch     0 | loss: 334.0081263CurrentTrain: epoch  4, batch     1 | loss: 440.2637523CurrentTrain: epoch  4, batch     2 | loss: 236.4300516CurrentTrain: epoch  4, batch     3 | loss: 23.5260622CurrentTrain: epoch  5, batch     0 | loss: 289.2770006CurrentTrain: epoch  5, batch     1 | loss: 376.9201960CurrentTrain: epoch  5, batch     2 | loss: 302.9523777CurrentTrain: epoch  5, batch     3 | loss: 29.8727171CurrentTrain: epoch  6, batch     0 | loss: 377.7234239CurrentTrain: epoch  6, batch     1 | loss: 329.1445838CurrentTrain: epoch  6, batch     2 | loss: 258.9269458CurrentTrain: epoch  6, batch     3 | loss: 17.8521670CurrentTrain: epoch  7, batch     0 | loss: 311.3760877CurrentTrain: epoch  7, batch     1 | loss: 315.2571365CurrentTrain: epoch  7, batch     2 | loss: 358.2815395CurrentTrain: epoch  7, batch     3 | loss: 15.5288442CurrentTrain: epoch  8, batch     0 | loss: 245.5689976CurrentTrain: epoch  8, batch     1 | loss: 408.8939999CurrentTrain: epoch  8, batch     2 | loss: 300.0027018CurrentTrain: epoch  8, batch     3 | loss: 17.7747828CurrentTrain: epoch  9, batch     0 | loss: 325.1469556CurrentTrain: epoch  9, batch     1 | loss: 233.3147526CurrentTrain: epoch  9, batch     2 | loss: 356.8365608CurrentTrain: epoch  9, batch     3 | loss: 54.9012427
MemoryTrain:  epoch  0, batch     0 | loss: 1.1614310MemoryTrain:  epoch  1, batch     0 | loss: 1.0143601MemoryTrain:  epoch  2, batch     0 | loss: 0.7979403MemoryTrain:  epoch  3, batch     0 | loss: 0.6151116MemoryTrain:  epoch  4, batch     0 | loss: 0.4681322MemoryTrain:  epoch  5, batch     0 | loss: 0.4358733MemoryTrain:  epoch  6, batch     0 | loss: 0.3628033MemoryTrain:  epoch  7, batch     0 | loss: 0.3899085MemoryTrain:  epoch  8, batch     0 | loss: 0.2951244MemoryTrain:  epoch  9, batch     0 | loss: 0.2723963

F1 score per class: {1: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 22: 0.0, 26: 0.42105263157894735, 27: 0.0, 31: 0.5833333333333334}
Micro-average F1 score: 0.5566037735849056
Weighted-average F1 score: 0.45420724680849783
F1 score per class: {3: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 12: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.5833333333333334, 27: 1.0, 31: 0.9206349206349206}
Micro-average F1 score: 0.8468468468468469
Weighted-average F1 score: 0.8098717882214614
F1 score per class: {3: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 12: 0.0, 19: 0.0, 22: 0.0, 24: 0.0, 26: 0.5833333333333334, 27: 1.0, 31: 0.9206349206349206}
Micro-average F1 score: 0.8468468468468469
Weighted-average F1 score: 0.8098717882214614

F1 score per class: {0: 0.8823529411764706, 1: 0.205607476635514, 2: 0.631578947368421, 3: 0.18947368421052632, 4: 0.8636363636363636, 5: 0.8571428571428571, 6: 0.4233576642335766, 7: 0.08333333333333333, 8: 0.11627906976744186, 9: 0.9615384615384616, 10: 0.09345794392523364, 11: 0.4716981132075472, 12: 0.25396825396825395, 13: 0.08695652173913043, 14: 0.030303030303030304, 15: 0.75, 16: 0.6923076923076923, 17: 0.0, 18: 0.047619047619047616, 19: 0.6767676767676768, 20: 0.5348837209302325, 21: 0.10714285714285714, 22: 0.6040268456375839, 23: 0.8, 24: 0.0, 25: 0.375, 26: 0.7333333333333333, 27: 0.23529411764705882, 28: 0.25, 29: 0.8715083798882681, 30: 0.918918918918919, 31: 0.0, 32: 0.6832298136645962, 33: 0.42857142857142855, 34: 0.4675324675324675, 35: 0.14925373134328357, 36: 0.08695652173913043, 37: 0.38961038961038963, 38: 0.125, 39: 0.14814814814814814, 40: 0.5283018867924528}
Micro-average F1 score: 0.5081967213114754
Weighted-average F1 score: 0.5702276156487356
F1 score per class: {0: 0.9444444444444444, 1: 0.2619047619047619, 2: 0.5, 3: 0.6823529411764706, 4: 0.8636363636363636, 5: 0.8646288209606987, 6: 0.52, 7: 0.06779661016949153, 8: 0.46616541353383456, 9: 0.9433962264150944, 10: 0.460431654676259, 11: 0.5531914893617021, 12: 0.5591397849462365, 13: 0.07692307692307693, 14: 0.0547945205479452, 15: 0.75, 16: 0.8666666666666667, 17: 0.0, 18: 0.16216216216216217, 19: 0.7, 20: 0.75, 21: 0.3333333333333333, 22: 0.6369426751592356, 23: 0.7912087912087912, 24: 0.044444444444444446, 25: 0.5205479452054794, 26: 0.7058823529411765, 27: 0.19444444444444445, 28: 0.26666666666666666, 29: 0.8651685393258427, 30: 0.9473684210526315, 31: 0.6666666666666666, 32: 0.7888888888888889, 33: 0.4, 34: 0.68, 35: 0.38095238095238093, 36: 0.4835164835164835, 37: 0.5333333333333333, 38: 0.37209302325581395, 39: 0.37209302325581395, 40: 0.6904761904761905}
Micro-average F1 score: 0.5995525727069351
Weighted-average F1 score: 0.5925373252290619
F1 score per class: {0: 0.9444444444444444, 1: 0.25287356321839083, 2: 0.56, 3: 0.6626506024096386, 4: 0.88268156424581, 5: 0.8646288209606987, 6: 0.4931506849315068, 7: 0.06557377049180328, 8: 0.3728813559322034, 9: 0.9803921568627451, 10: 0.3464566929133858, 11: 0.5363984674329502, 12: 0.5604395604395604, 13: 0.04878048780487805, 14: 0.05405405405405406, 15: 0.75, 16: 0.8813559322033898, 17: 0.0, 18: 0.1111111111111111, 19: 0.7317073170731707, 20: 0.7428571428571429, 21: 0.2988505747126437, 22: 0.6580645161290323, 23: 0.7912087912087912, 24: 0.0, 25: 0.45714285714285713, 26: 0.7058823529411765, 27: 0.1917808219178082, 28: 0.21818181818181817, 29: 0.8715083798882681, 30: 0.9743589743589743, 31: 0.6666666666666666, 32: 0.7955801104972375, 33: 0.42857142857142855, 34: 0.6875, 35: 0.3488372093023256, 36: 0.5168539325842697, 37: 0.49411764705882355, 38: 0.36363636363636365, 39: 0.3, 40: 0.7160493827160493}
Micro-average F1 score: 0.5915
Weighted-average F1 score: 0.587379379274132
cur_acc:  ['0.7931', '0.6604', '0.7036', '0.5650', '0.6366', '0.7980', '0.5465', '0.5566']
his_acc:  ['0.7931', '0.7753', '0.7061', '0.5466', '0.5180', '0.5558', '0.5398', '0.5082']
cur_acc des:  ['0.8366', '0.8391', '0.8526', '0.5120', '0.7236', '0.8271', '0.7136', '0.8468']
his_acc des:  ['0.8366', '0.8290', '0.7818', '0.6273', '0.6247', '0.6613', '0.6213', '0.5996']
cur_acc rrf:  ['0.8366', '0.8311', '0.8682', '0.5517', '0.7388', '0.8271', '0.7291', '0.8468']
his_acc rrf:  ['0.8366', '0.8251', '0.7736', '0.6112', '0.6135', '0.6416', '0.6051', '0.5915']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown'])
CurrentTrain: epoch  0, batch     0 | loss: 387.8351276CurrentTrain: epoch  0, batch     1 | loss: 357.1539782CurrentTrain: epoch  0, batch     2 | loss: 393.0297416CurrentTrain: epoch  0, batch     3 | loss: 429.4088076CurrentTrain: epoch  0, batch     4 | loss: 312.8853913CurrentTrain: epoch  0, batch     5 | loss: 428.3312336CurrentTrain: epoch  0, batch     6 | loss: 336.6349745CurrentTrain: epoch  0, batch     7 | loss: 361.9431353CurrentTrain: epoch  0, batch     8 | loss: 336.5173662CurrentTrain: epoch  0, batch     9 | loss: 346.7752845CurrentTrain: epoch  0, batch    10 | loss: 378.9305877CurrentTrain: epoch  0, batch    11 | loss: 374.0195699CurrentTrain: epoch  0, batch    12 | loss: 392.1456528CurrentTrain: epoch  0, batch    13 | loss: 424.6742757CurrentTrain: epoch  0, batch    14 | loss: 373.7187003CurrentTrain: epoch  0, batch    15 | loss: 322.9632573CurrentTrain: epoch  0, batch    16 | loss: 276.6937781CurrentTrain: epoch  0, batch    17 | loss: 294.7086366CurrentTrain: epoch  0, batch    18 | loss: 358.4254132CurrentTrain: epoch  0, batch    19 | loss: 334.8787685CurrentTrain: epoch  0, batch    20 | loss: 436.4432924CurrentTrain: epoch  0, batch    21 | loss: 332.2422575CurrentTrain: epoch  0, batch    22 | loss: 372.7847425CurrentTrain: epoch  0, batch    23 | loss: 371.7937707CurrentTrain: epoch  0, batch    24 | loss: 309.0736585CurrentTrain: epoch  0, batch    25 | loss: 377.7201208CurrentTrain: epoch  0, batch    26 | loss: 370.8736171CurrentTrain: epoch  0, batch    27 | loss: 318.4497648CurrentTrain: epoch  0, batch    28 | loss: 357.5095860CurrentTrain: epoch  0, batch    29 | loss: 306.1180860CurrentTrain: epoch  0, batch    30 | loss: 370.9969214CurrentTrain: epoch  0, batch    31 | loss: 318.2172968CurrentTrain: epoch  0, batch    32 | loss: 466.1383156CurrentTrain: epoch  0, batch    33 | loss: 370.9079466CurrentTrain: epoch  0, batch    34 | loss: 320.9819836CurrentTrain: epoch  0, batch    35 | loss: 404.7221062CurrentTrain: epoch  0, batch    36 | loss: 345.0793039CurrentTrain: epoch  0, batch    37 | loss: 330.1447134CurrentTrain: epoch  0, batch    38 | loss: 356.1511309CurrentTrain: epoch  0, batch    39 | loss: 316.3706501CurrentTrain: epoch  0, batch    40 | loss: 370.7731702CurrentTrain: epoch  0, batch    41 | loss: 295.1348200CurrentTrain: epoch  0, batch    42 | loss: 403.4074979CurrentTrain: epoch  0, batch    43 | loss: 418.9167801CurrentTrain: epoch  0, batch    44 | loss: 452.4835785CurrentTrain: epoch  0, batch    45 | loss: 316.6557877CurrentTrain: epoch  0, batch    46 | loss: 542.2450455CurrentTrain: epoch  0, batch    47 | loss: 316.6188649CurrentTrain: epoch  0, batch    48 | loss: 357.0514271CurrentTrain: epoch  0, batch    49 | loss: 329.7591441CurrentTrain: epoch  0, batch    50 | loss: 356.8121021CurrentTrain: epoch  0, batch    51 | loss: 338.7232366CurrentTrain: epoch  0, batch    52 | loss: 316.5072909CurrentTrain: epoch  0, batch    53 | loss: 315.7864736CurrentTrain: epoch  0, batch    54 | loss: 342.8404729CurrentTrain: epoch  0, batch    55 | loss: 290.6804038CurrentTrain: epoch  0, batch    56 | loss: 375.0009336CurrentTrain: epoch  0, batch    57 | loss: 329.5118006CurrentTrain: epoch  0, batch    58 | loss: 305.7100046CurrentTrain: epoch  0, batch    59 | loss: 303.9610325CurrentTrain: epoch  0, batch    60 | loss: 465.6377247CurrentTrain: epoch  0, batch    61 | loss: 385.5272764CurrentTrain: epoch  0, batch    62 | loss: 279.6822006CurrentTrain: epoch  0, batch    63 | loss: 417.1394010CurrentTrain: epoch  0, batch    64 | loss: 355.4042863CurrentTrain: epoch  0, batch    65 | loss: 355.1054817CurrentTrain: epoch  0, batch    66 | loss: 355.5627888CurrentTrain: epoch  0, batch    67 | loss: 355.5957297CurrentTrain: epoch  0, batch    68 | loss: 403.7490530CurrentTrain: epoch  0, batch    69 | loss: 448.1799718CurrentTrain: epoch  0, batch    70 | loss: 382.1363454CurrentTrain: epoch  0, batch    71 | loss: 304.7070799CurrentTrain: epoch  0, batch    72 | loss: 389.6803209CurrentTrain: epoch  0, batch    73 | loss: 448.0587463CurrentTrain: epoch  0, batch    74 | loss: 332.5375685CurrentTrain: epoch  0, batch    75 | loss: 368.4421082CurrentTrain: epoch  0, batch    76 | loss: 388.1736761CurrentTrain: epoch  0, batch    77 | loss: 342.1063556CurrentTrain: epoch  0, batch    78 | loss: 316.9325768CurrentTrain: epoch  0, batch    79 | loss: 369.5002508CurrentTrain: epoch  0, batch    80 | loss: 393.6175373CurrentTrain: epoch  0, batch    81 | loss: 387.5062959CurrentTrain: epoch  0, batch    82 | loss: 402.2814796CurrentTrain: epoch  0, batch    83 | loss: 373.4009147CurrentTrain: epoch  0, batch    84 | loss: 327.6954992CurrentTrain: epoch  0, batch    85 | loss: 326.4334000CurrentTrain: epoch  0, batch    86 | loss: 305.6389982CurrentTrain: epoch  0, batch    87 | loss: 289.9273243CurrentTrain: epoch  0, batch    88 | loss: 266.3551007CurrentTrain: epoch  0, batch    89 | loss: 355.2476522CurrentTrain: epoch  0, batch    90 | loss: 403.7338850CurrentTrain: epoch  0, batch    91 | loss: 328.1092751CurrentTrain: epoch  0, batch    92 | loss: 326.8363344CurrentTrain: epoch  0, batch    93 | loss: 373.5227194CurrentTrain: epoch  0, batch    94 | loss: 369.6546425CurrentTrain: epoch  0, batch    95 | loss: 253.8802956CurrentTrain: epoch  1, batch     0 | loss: 422.7818973CurrentTrain: epoch  1, batch     1 | loss: 325.8428601CurrentTrain: epoch  1, batch     2 | loss: 311.0603623CurrentTrain: epoch  1, batch     3 | loss: 463.2680217CurrentTrain: epoch  1, batch     4 | loss: 340.6213545CurrentTrain: epoch  1, batch     5 | loss: 381.9391643CurrentTrain: epoch  1, batch     6 | loss: 287.8001440CurrentTrain: epoch  1, batch     7 | loss: 464.2326012CurrentTrain: epoch  1, batch     8 | loss: 371.6785743CurrentTrain: epoch  1, batch     9 | loss: 301.7541249CurrentTrain: epoch  1, batch    10 | loss: 355.9934111CurrentTrain: epoch  1, batch    11 | loss: 382.2984829CurrentTrain: epoch  1, batch    12 | loss: 385.5411718CurrentTrain: epoch  1, batch    13 | loss: 372.7902713CurrentTrain: epoch  1, batch    14 | loss: 401.2349050CurrentTrain: epoch  1, batch    15 | loss: 326.1833541CurrentTrain: epoch  1, batch    16 | loss: 442.7656928CurrentTrain: epoch  1, batch    17 | loss: 340.0232498CurrentTrain: epoch  1, batch    18 | loss: 366.9036281CurrentTrain: epoch  1, batch    19 | loss: 449.6890316CurrentTrain: epoch  1, batch    20 | loss: 542.0903195CurrentTrain: epoch  1, batch    21 | loss: 339.8693866CurrentTrain: epoch  1, batch    22 | loss: 338.2120540CurrentTrain: epoch  1, batch    23 | loss: 380.2464986CurrentTrain: epoch  1, batch    24 | loss: 381.0670625CurrentTrain: epoch  1, batch    25 | loss: 414.4409024CurrentTrain: epoch  1, batch    26 | loss: 286.8099785CurrentTrain: epoch  1, batch    27 | loss: 364.7446040CurrentTrain: epoch  1, batch    28 | loss: 352.2106462CurrentTrain: epoch  1, batch    29 | loss: 260.5054382CurrentTrain: epoch  1, batch    30 | loss: 323.6735573CurrentTrain: epoch  1, batch    31 | loss: 311.6484735CurrentTrain: epoch  1, batch    32 | loss: 277.2405452CurrentTrain: epoch  1, batch    33 | loss: 316.4251959CurrentTrain: epoch  1, batch    34 | loss: 309.0357248CurrentTrain: epoch  1, batch    35 | loss: 260.2554165CurrentTrain: epoch  1, batch    36 | loss: 462.1088906CurrentTrain: epoch  1, batch    37 | loss: 338.3641445CurrentTrain: epoch  1, batch    38 | loss: 383.4076600CurrentTrain: epoch  1, batch    39 | loss: 379.8362341CurrentTrain: epoch  1, batch    40 | loss: 354.3779024CurrentTrain: epoch  1, batch    41 | loss: 526.1165899CurrentTrain: epoch  1, batch    42 | loss: 337.5276327CurrentTrain: epoch  1, batch    43 | loss: 431.7063884CurrentTrain: epoch  1, batch    44 | loss: 296.2708637CurrentTrain: epoch  1, batch    45 | loss: 308.9506340CurrentTrain: epoch  1, batch    46 | loss: 320.6276368CurrentTrain: epoch  1, batch    47 | loss: 543.6991075CurrentTrain: epoch  1, batch    48 | loss: 364.1696098CurrentTrain: epoch  1, batch    49 | loss: 350.8388483CurrentTrain: epoch  1, batch    50 | loss: 398.1369795CurrentTrain: epoch  1, batch    51 | loss: 349.9600091CurrentTrain: epoch  1, batch    52 | loss: 317.8028224CurrentTrain: epoch  1, batch    53 | loss: 364.8929943CurrentTrain: epoch  1, batch    54 | loss: 285.8202754CurrentTrain: epoch  1, batch    55 | loss: 446.5878713CurrentTrain: epoch  1, batch    56 | loss: 309.3408783CurrentTrain: epoch  1, batch    57 | loss: 380.6075144CurrentTrain: epoch  1, batch    58 | loss: 381.4530849CurrentTrain: epoch  1, batch    59 | loss: 344.7661027CurrentTrain: epoch  1, batch    60 | loss: 324.6916677CurrentTrain: epoch  1, batch    61 | loss: 283.9817484CurrentTrain: epoch  1, batch    62 | loss: 333.7030736CurrentTrain: epoch  1, batch    63 | loss: 321.7314461CurrentTrain: epoch  1, batch    64 | loss: 319.4141148CurrentTrain: epoch  1, batch    65 | loss: 351.1202407CurrentTrain: epoch  1, batch    66 | loss: 415.4408940CurrentTrain: epoch  1, batch    67 | loss: 350.4066601CurrentTrain: epoch  1, batch    68 | loss: 282.3705076CurrentTrain: epoch  1, batch    69 | loss: 376.3063255CurrentTrain: epoch  1, batch    70 | loss: 377.8214834CurrentTrain: epoch  1, batch    71 | loss: 247.6237988CurrentTrain: epoch  1, batch    72 | loss: 370.6334965CurrentTrain: epoch  1, batch    73 | loss: 307.6472551CurrentTrain: epoch  1, batch    74 | loss: 337.7948024CurrentTrain: epoch  1, batch    75 | loss: 308.4560141CurrentTrain: epoch  1, batch    76 | loss: 313.1503075CurrentTrain: epoch  1, batch    77 | loss: 333.2433659CurrentTrain: epoch  1, batch    78 | loss: 394.9607982CurrentTrain: epoch  1, batch    79 | loss: 321.1005484CurrentTrain: epoch  1, batch    80 | loss: 330.6625551CurrentTrain: epoch  1, batch    81 | loss: 325.4286028CurrentTrain: epoch  1, batch    82 | loss: 443.2278493CurrentTrain: epoch  1, batch    83 | loss: 305.3571122CurrentTrain: epoch  1, batch    84 | loss: 427.8728091CurrentTrain: epoch  1, batch    85 | loss: 410.6621640CurrentTrain: epoch  1, batch    86 | loss: 363.6363504CurrentTrain: epoch  1, batch    87 | loss: 281.7933150CurrentTrain: epoch  1, batch    88 | loss: 320.8387356CurrentTrain: epoch  1, batch    89 | loss: 318.2947131CurrentTrain: epoch  1, batch    90 | loss: 294.6082905CurrentTrain: epoch  1, batch    91 | loss: 383.0115200CurrentTrain: epoch  1, batch    92 | loss: 319.0908957CurrentTrain: epoch  1, batch    93 | loss: 338.8718199CurrentTrain: epoch  1, batch    94 | loss: 382.0081109CurrentTrain: epoch  1, batch    95 | loss: 257.4147536CurrentTrain: epoch  2, batch     0 | loss: 442.8574242CurrentTrain: epoch  2, batch     1 | loss: 377.8680384CurrentTrain: epoch  2, batch     2 | loss: 379.1181634CurrentTrain: epoch  2, batch     3 | loss: 377.1281397CurrentTrain: epoch  2, batch     4 | loss: 361.8770024CurrentTrain: epoch  2, batch     5 | loss: 254.7573680CurrentTrain: epoch  2, batch     6 | loss: 307.6648441CurrentTrain: epoch  2, batch     7 | loss: 291.6586754CurrentTrain: epoch  2, batch     8 | loss: 412.9927276CurrentTrain: epoch  2, batch     9 | loss: 347.7190976CurrentTrain: epoch  2, batch    10 | loss: 309.6112511CurrentTrain: epoch  2, batch    11 | loss: 300.6523162CurrentTrain: epoch  2, batch    12 | loss: 321.2391222CurrentTrain: epoch  2, batch    13 | loss: 391.8687348CurrentTrain: epoch  2, batch    14 | loss: 318.2493610CurrentTrain: epoch  2, batch    15 | loss: 376.6018454CurrentTrain: epoch  2, batch    16 | loss: 327.4680188CurrentTrain: epoch  2, batch    17 | loss: 252.6871865CurrentTrain: epoch  2, batch    18 | loss: 333.0135829CurrentTrain: epoch  2, batch    19 | loss: 440.4977065CurrentTrain: epoch  2, batch    20 | loss: 271.7239937CurrentTrain: epoch  2, batch    21 | loss: 294.2714487CurrentTrain: epoch  2, batch    22 | loss: 265.7766494CurrentTrain: epoch  2, batch    23 | loss: 441.4209769CurrentTrain: epoch  2, batch    24 | loss: 412.0554909CurrentTrain: epoch  2, batch    25 | loss: 331.0017420CurrentTrain: epoch  2, batch    26 | loss: 321.0667645CurrentTrain: epoch  2, batch    27 | loss: 393.7852082CurrentTrain: epoch  2, batch    28 | loss: 306.5340690CurrentTrain: epoch  2, batch    29 | loss: 360.9048740CurrentTrain: epoch  2, batch    30 | loss: 300.6746176CurrentTrain: epoch  2, batch    31 | loss: 417.9363130CurrentTrain: epoch  2, batch    32 | loss: 456.3328334CurrentTrain: epoch  2, batch    33 | loss: 394.1191799CurrentTrain: epoch  2, batch    34 | loss: 362.5460571CurrentTrain: epoch  2, batch    35 | loss: 458.4556242CurrentTrain: epoch  2, batch    36 | loss: 314.9767214CurrentTrain: epoch  2, batch    37 | loss: 345.1598827CurrentTrain: epoch  2, batch    38 | loss: 332.6629041CurrentTrain: epoch  2, batch    39 | loss: 374.0248206CurrentTrain: epoch  2, batch    40 | loss: 348.9626825CurrentTrain: epoch  2, batch    41 | loss: 338.7933002CurrentTrain: epoch  2, batch    42 | loss: 364.1217837CurrentTrain: epoch  2, batch    43 | loss: 361.5850108CurrentTrain: epoch  2, batch    44 | loss: 335.8004431CurrentTrain: epoch  2, batch    45 | loss: 346.6293514CurrentTrain: epoch  2, batch    46 | loss: 320.3500305CurrentTrain: epoch  2, batch    47 | loss: 396.2221616CurrentTrain: epoch  2, batch    48 | loss: 257.7924458CurrentTrain: epoch  2, batch    49 | loss: 387.9679887CurrentTrain: epoch  2, batch    50 | loss: 295.4485472CurrentTrain: epoch  2, batch    51 | loss: 343.0361908CurrentTrain: epoch  2, batch    52 | loss: 395.9535369CurrentTrain: epoch  2, batch    53 | loss: 379.9686897CurrentTrain: epoch  2, batch    54 | loss: 277.5382872CurrentTrain: epoch  2, batch    55 | loss: 244.7880687CurrentTrain: epoch  2, batch    56 | loss: 522.8310188CurrentTrain: epoch  2, batch    57 | loss: 380.1719234CurrentTrain: epoch  2, batch    58 | loss: 322.7913146CurrentTrain: epoch  2, batch    59 | loss: 396.0231670CurrentTrain: epoch  2, batch    60 | loss: 348.3395379CurrentTrain: epoch  2, batch    61 | loss: 316.4418891CurrentTrain: epoch  2, batch    62 | loss: 522.6352358CurrentTrain: epoch  2, batch    63 | loss: 377.1342388CurrentTrain: epoch  2, batch    64 | loss: 395.6606104CurrentTrain: epoch  2, batch    65 | loss: 439.9379372CurrentTrain: epoch  2, batch    66 | loss: 456.8342015CurrentTrain: epoch  2, batch    67 | loss: 361.0650945CurrentTrain: epoch  2, batch    68 | loss: 299.8426351CurrentTrain: epoch  2, batch    69 | loss: 399.4155612CurrentTrain: epoch  2, batch    70 | loss: 377.0230588CurrentTrain: epoch  2, batch    71 | loss: 358.6938704CurrentTrain: epoch  2, batch    72 | loss: 279.3883538CurrentTrain: epoch  2, batch    73 | loss: 345.7523556CurrentTrain: epoch  2, batch    74 | loss: 380.2478787CurrentTrain: epoch  2, batch    75 | loss: 304.8294884CurrentTrain: epoch  2, batch    76 | loss: 412.0888551CurrentTrain: epoch  2, batch    77 | loss: 389.0975642CurrentTrain: epoch  2, batch    78 | loss: 293.8530794CurrentTrain: epoch  2, batch    79 | loss: 329.5376309CurrentTrain: epoch  2, batch    80 | loss: 315.7658009CurrentTrain: epoch  2, batch    81 | loss: 297.4176863CurrentTrain: epoch  2, batch    82 | loss: 380.0518885CurrentTrain: epoch  2, batch    83 | loss: 393.4997322CurrentTrain: epoch  2, batch    84 | loss: 262.6788372CurrentTrain: epoch  2, batch    85 | loss: 383.7939185CurrentTrain: epoch  2, batch    86 | loss: 256.8766804CurrentTrain: epoch  2, batch    87 | loss: 359.8687881CurrentTrain: epoch  2, batch    88 | loss: 283.0387781CurrentTrain: epoch  2, batch    89 | loss: 305.7083242CurrentTrain: epoch  2, batch    90 | loss: 376.1402066CurrentTrain: epoch  2, batch    91 | loss: 383.7657158CurrentTrain: epoch  2, batch    92 | loss: 306.6587878CurrentTrain: epoch  2, batch    93 | loss: 346.1688189CurrentTrain: epoch  2, batch    94 | loss: 348.9395213CurrentTrain: epoch  2, batch    95 | loss: 277.8344255CurrentTrain: epoch  3, batch     0 | loss: 328.7922852CurrentTrain: epoch  3, batch     1 | loss: 331.6921957CurrentTrain: epoch  3, batch     2 | loss: 344.9012892CurrentTrain: epoch  3, batch     3 | loss: 319.1746215CurrentTrain: epoch  3, batch     4 | loss: 438.2973051CurrentTrain: epoch  3, batch     5 | loss: 313.8761728CurrentTrain: epoch  3, batch     6 | loss: 341.7015828CurrentTrain: epoch  3, batch     7 | loss: 375.8288574CurrentTrain: epoch  3, batch     8 | loss: 302.5889083CurrentTrain: epoch  3, batch     9 | loss: 397.5219112CurrentTrain: epoch  3, batch    10 | loss: 342.9795752CurrentTrain: epoch  3, batch    11 | loss: 248.5798223CurrentTrain: epoch  3, batch    12 | loss: 341.5653662CurrentTrain: epoch  3, batch    13 | loss: 346.7433213CurrentTrain: epoch  3, batch    14 | loss: 345.8981193CurrentTrain: epoch  3, batch    15 | loss: 398.3767153CurrentTrain: epoch  3, batch    16 | loss: 362.7968824CurrentTrain: epoch  3, batch    17 | loss: 345.9484477CurrentTrain: epoch  3, batch    18 | loss: 344.9277601CurrentTrain: epoch  3, batch    19 | loss: 374.5338646CurrentTrain: epoch  3, batch    20 | loss: 440.2559213CurrentTrain: epoch  3, batch    21 | loss: 273.5770926CurrentTrain: epoch  3, batch    22 | loss: 304.6662831CurrentTrain: epoch  3, batch    23 | loss: 359.0324156CurrentTrain: epoch  3, batch    24 | loss: 249.8477760CurrentTrain: epoch  3, batch    25 | loss: 344.2885928CurrentTrain: epoch  3, batch    26 | loss: 392.0756291CurrentTrain: epoch  3, batch    27 | loss: 270.7554370CurrentTrain: epoch  3, batch    28 | loss: 263.5559760CurrentTrain: epoch  3, batch    29 | loss: 316.2631914CurrentTrain: epoch  3, batch    30 | loss: 262.6827064CurrentTrain: epoch  3, batch    31 | loss: 344.9217966CurrentTrain: epoch  3, batch    32 | loss: 312.0702945CurrentTrain: epoch  3, batch    33 | loss: 317.8696157CurrentTrain: epoch  3, batch    34 | loss: 318.8428300CurrentTrain: epoch  3, batch    35 | loss: 281.2420325CurrentTrain: epoch  3, batch    36 | loss: 283.1848923CurrentTrain: epoch  3, batch    37 | loss: 292.5632086CurrentTrain: epoch  3, batch    38 | loss: 299.5245020CurrentTrain: epoch  3, batch    39 | loss: 302.8822650CurrentTrain: epoch  3, batch    40 | loss: 342.9257021CurrentTrain: epoch  3, batch    41 | loss: 335.9771239CurrentTrain: epoch  3, batch    42 | loss: 361.8837242CurrentTrain: epoch  3, batch    43 | loss: 391.9366278CurrentTrain: epoch  3, batch    44 | loss: 374.2826266CurrentTrain: epoch  3, batch    45 | loss: 331.1388245CurrentTrain: epoch  3, batch    46 | loss: 390.9949074CurrentTrain: epoch  3, batch    47 | loss: 299.1652540CurrentTrain: epoch  3, batch    48 | loss: 349.0618063CurrentTrain: epoch  3, batch    49 | loss: 394.0156436CurrentTrain: epoch  3, batch    50 | loss: 446.4198506CurrentTrain: epoch  3, batch    51 | loss: 393.1863764CurrentTrain: epoch  3, batch    52 | loss: 279.1203277CurrentTrain: epoch  3, batch    53 | loss: 331.4648778CurrentTrain: epoch  3, batch    54 | loss: 392.8232998CurrentTrain: epoch  3, batch    55 | loss: 261.7256184CurrentTrain: epoch  3, batch    56 | loss: 374.5813060CurrentTrain: epoch  3, batch    57 | loss: 231.5553432CurrentTrain: epoch  3, batch    58 | loss: 405.0212110CurrentTrain: epoch  3, batch    59 | loss: 301.2318575CurrentTrain: epoch  3, batch    60 | loss: 362.0673560CurrentTrain: epoch  3, batch    61 | loss: 440.3390740CurrentTrain: epoch  3, batch    62 | loss: 437.7848789CurrentTrain: epoch  3, batch    63 | loss: 264.5146369CurrentTrain: epoch  3, batch    64 | loss: 390.7600034CurrentTrain: epoch  3, batch    65 | loss: 407.7062497CurrentTrain: epoch  3, batch    66 | loss: 363.5718571CurrentTrain: epoch  3, batch    67 | loss: 319.3506842CurrentTrain: epoch  3, batch    68 | loss: 301.0152961CurrentTrain: epoch  3, batch    69 | loss: 375.0551269CurrentTrain: epoch  3, batch    70 | loss: 380.0902699CurrentTrain: epoch  3, batch    71 | loss: 391.5640774CurrentTrain: epoch  3, batch    72 | loss: 378.3272493CurrentTrain: epoch  3, batch    73 | loss: 407.7435789CurrentTrain: epoch  3, batch    74 | loss: 323.9424145CurrentTrain: epoch  3, batch    75 | loss: 330.8058579CurrentTrain: epoch  3, batch    76 | loss: 392.6047433CurrentTrain: epoch  3, batch    77 | loss: 353.6419270CurrentTrain: epoch  3, batch    78 | loss: 408.6163214CurrentTrain: epoch  3, batch    79 | loss: 336.7942413CurrentTrain: epoch  3, batch    80 | loss: 329.7245243CurrentTrain: epoch  3, batch    81 | loss: 459.2297961CurrentTrain: epoch  3, batch    82 | loss: 458.0992583CurrentTrain: epoch  3, batch    83 | loss: 376.2315646CurrentTrain: epoch  3, batch    84 | loss: 302.5757925CurrentTrain: epoch  3, batch    85 | loss: 333.1390813CurrentTrain: epoch  3, batch    86 | loss: 342.8612728CurrentTrain: epoch  3, batch    87 | loss: 393.4534801CurrentTrain: epoch  3, batch    88 | loss: 348.4604431CurrentTrain: epoch  3, batch    89 | loss: 391.9048905CurrentTrain: epoch  3, batch    90 | loss: 323.7021922CurrentTrain: epoch  3, batch    91 | loss: 317.2627146CurrentTrain: epoch  3, batch    92 | loss: 328.4882176CurrentTrain: epoch  3, batch    93 | loss: 409.6104084CurrentTrain: epoch  3, batch    94 | loss: 320.1221775CurrentTrain: epoch  3, batch    95 | loss: 322.7028931CurrentTrain: epoch  4, batch     0 | loss: 390.9217527CurrentTrain: epoch  4, batch     1 | loss: 330.6977205CurrentTrain: epoch  4, batch     2 | loss: 375.2171179CurrentTrain: epoch  4, batch     3 | loss: 360.7308712CurrentTrain: epoch  4, batch     4 | loss: 307.2885585CurrentTrain: epoch  4, batch     5 | loss: 393.2070832CurrentTrain: epoch  4, batch     6 | loss: 374.4451687CurrentTrain: epoch  4, batch     7 | loss: 537.1435488CurrentTrain: epoch  4, batch     8 | loss: 286.3398783CurrentTrain: epoch  4, batch     9 | loss: 375.4397430CurrentTrain: epoch  4, batch    10 | loss: 329.7348685CurrentTrain: epoch  4, batch    11 | loss: 288.3280056CurrentTrain: epoch  4, batch    12 | loss: 302.3856132CurrentTrain: epoch  4, batch    13 | loss: 331.1920255CurrentTrain: epoch  4, batch    14 | loss: 332.5218173CurrentTrain: epoch  4, batch    15 | loss: 249.3368997CurrentTrain: epoch  4, batch    16 | loss: 265.7013048CurrentTrain: epoch  4, batch    17 | loss: 333.6841494CurrentTrain: epoch  4, batch    18 | loss: 285.7567833CurrentTrain: epoch  4, batch    19 | loss: 356.1955444CurrentTrain: epoch  4, batch    20 | loss: 392.6104520CurrentTrain: epoch  4, batch    21 | loss: 289.3372315CurrentTrain: epoch  4, batch    22 | loss: 355.9664461CurrentTrain: epoch  4, batch    23 | loss: 439.6025912CurrentTrain: epoch  4, batch    24 | loss: 457.1308204CurrentTrain: epoch  4, batch    25 | loss: 327.9764376CurrentTrain: epoch  4, batch    26 | loss: 314.0053564CurrentTrain: epoch  4, batch    27 | loss: 303.4834920CurrentTrain: epoch  4, batch    28 | loss: 441.1482044CurrentTrain: epoch  4, batch    29 | loss: 372.9816165CurrentTrain: epoch  4, batch    30 | loss: 358.3118178CurrentTrain: epoch  4, batch    31 | loss: 423.6178488CurrentTrain: epoch  4, batch    32 | loss: 373.4725001CurrentTrain: epoch  4, batch    33 | loss: 300.5431564CurrentTrain: epoch  4, batch    34 | loss: 328.0597340CurrentTrain: epoch  4, batch    35 | loss: 408.6166124CurrentTrain: epoch  4, batch    36 | loss: 329.5559913CurrentTrain: epoch  4, batch    37 | loss: 299.8961472CurrentTrain: epoch  4, batch    38 | loss: 287.0903877CurrentTrain: epoch  4, batch    39 | loss: 312.3704221CurrentTrain: epoch  4, batch    40 | loss: 266.0933186CurrentTrain: epoch  4, batch    41 | loss: 515.8114220CurrentTrain: epoch  4, batch    42 | loss: 381.2440916CurrentTrain: epoch  4, batch    43 | loss: 275.6716888CurrentTrain: epoch  4, batch    44 | loss: 421.1858697CurrentTrain: epoch  4, batch    45 | loss: 332.0349954CurrentTrain: epoch  4, batch    46 | loss: 273.3134371CurrentTrain: epoch  4, batch    47 | loss: 438.6688939CurrentTrain: epoch  4, batch    48 | loss: 394.6820018CurrentTrain: epoch  4, batch    49 | loss: 289.2547666CurrentTrain: epoch  4, batch    50 | loss: 408.8313986CurrentTrain: epoch  4, batch    51 | loss: 347.5540880CurrentTrain: epoch  4, batch    52 | loss: 447.0029896CurrentTrain: epoch  4, batch    53 | loss: 395.4412222CurrentTrain: epoch  4, batch    54 | loss: 407.4383790CurrentTrain: epoch  4, batch    55 | loss: 299.6062386CurrentTrain: epoch  4, batch    56 | loss: 377.0742948CurrentTrain: epoch  4, batch    57 | loss: 282.3939708CurrentTrain: epoch  4, batch    58 | loss: 347.5315343CurrentTrain: epoch  4, batch    59 | loss: 410.9873149CurrentTrain: epoch  4, batch    60 | loss: 301.1361179CurrentTrain: epoch  4, batch    61 | loss: 360.1381512CurrentTrain: epoch  4, batch    62 | loss: 358.5704407CurrentTrain: epoch  4, batch    63 | loss: 374.2474624CurrentTrain: epoch  4, batch    64 | loss: 346.8185461CurrentTrain: epoch  4, batch    65 | loss: 373.9444445CurrentTrain: epoch  4, batch    66 | loss: 345.7926102CurrentTrain: epoch  4, batch    67 | loss: 268.1999316CurrentTrain: epoch  4, batch    68 | loss: 289.0466350CurrentTrain: epoch  4, batch    69 | loss: 266.0488770CurrentTrain: epoch  4, batch    70 | loss: 359.0824928CurrentTrain: epoch  4, batch    71 | loss: 346.6170551CurrentTrain: epoch  4, batch    72 | loss: 316.2348490CurrentTrain: epoch  4, batch    73 | loss: 374.6777874CurrentTrain: epoch  4, batch    74 | loss: 364.7107973CurrentTrain: epoch  4, batch    75 | loss: 393.7874003CurrentTrain: epoch  4, batch    76 | loss: 302.8124537CurrentTrain: epoch  4, batch    77 | loss: 405.2263968CurrentTrain: epoch  4, batch    78 | loss: 297.3980032CurrentTrain: epoch  4, batch    79 | loss: 358.3179292CurrentTrain: epoch  4, batch    80 | loss: 284.0610826CurrentTrain: epoch  4, batch    81 | loss: 286.5660496CurrentTrain: epoch  4, batch    82 | loss: 375.5114373CurrentTrain: epoch  4, batch    83 | loss: 327.3390384CurrentTrain: epoch  4, batch    84 | loss: 422.4007089CurrentTrain: epoch  4, batch    85 | loss: 311.1603496CurrentTrain: epoch  4, batch    86 | loss: 376.0654903CurrentTrain: epoch  4, batch    87 | loss: 373.7299275CurrentTrain: epoch  4, batch    88 | loss: 408.5674393CurrentTrain: epoch  4, batch    89 | loss: 287.0724327CurrentTrain: epoch  4, batch    90 | loss: 327.6189415CurrentTrain: epoch  4, batch    91 | loss: 342.3230579CurrentTrain: epoch  4, batch    92 | loss: 341.7472227CurrentTrain: epoch  4, batch    93 | loss: 437.9198682CurrentTrain: epoch  4, batch    94 | loss: 358.5561895CurrentTrain: epoch  4, batch    95 | loss: 226.7930062CurrentTrain: epoch  5, batch     0 | loss: 341.2403384CurrentTrain: epoch  5, batch     1 | loss: 314.4034071CurrentTrain: epoch  5, batch     2 | loss: 327.2571651CurrentTrain: epoch  5, batch     3 | loss: 358.9810101CurrentTrain: epoch  5, batch     4 | loss: 345.0632101CurrentTrain: epoch  5, batch     5 | loss: 327.9871307CurrentTrain: epoch  5, batch     6 | loss: 406.5117399CurrentTrain: epoch  5, batch     7 | loss: 358.4500457CurrentTrain: epoch  5, batch     8 | loss: 345.8631701CurrentTrain: epoch  5, batch     9 | loss: 356.7252298CurrentTrain: epoch  5, batch    10 | loss: 345.8875951CurrentTrain: epoch  5, batch    11 | loss: 344.7137169CurrentTrain: epoch  5, batch    12 | loss: 276.2207202CurrentTrain: epoch  5, batch    13 | loss: 272.4635481CurrentTrain: epoch  5, batch    14 | loss: 362.0004339CurrentTrain: epoch  5, batch    15 | loss: 327.1968685CurrentTrain: epoch  5, batch    16 | loss: 357.4411909CurrentTrain: epoch  5, batch    17 | loss: 226.6127413CurrentTrain: epoch  5, batch    18 | loss: 392.9133168CurrentTrain: epoch  5, batch    19 | loss: 537.1838755CurrentTrain: epoch  5, batch    20 | loss: 372.9162183CurrentTrain: epoch  5, batch    21 | loss: 359.4076198CurrentTrain: epoch  5, batch    22 | loss: 376.4153500CurrentTrain: epoch  5, batch    23 | loss: 358.7682776CurrentTrain: epoch  5, batch    24 | loss: 341.8506708CurrentTrain: epoch  5, batch    25 | loss: 340.8742088CurrentTrain: epoch  5, batch    26 | loss: 285.3884658CurrentTrain: epoch  5, batch    27 | loss: 357.3926913CurrentTrain: epoch  5, batch    28 | loss: 358.7350785CurrentTrain: epoch  5, batch    29 | loss: 421.3660742CurrentTrain: epoch  5, batch    30 | loss: 345.2211800CurrentTrain: epoch  5, batch    31 | loss: 437.1153437CurrentTrain: epoch  5, batch    32 | loss: 378.6158926CurrentTrain: epoch  5, batch    33 | loss: 390.0010930CurrentTrain: epoch  5, batch    34 | loss: 437.3450996CurrentTrain: epoch  5, batch    35 | loss: 325.1490723CurrentTrain: epoch  5, batch    36 | loss: 357.1315245CurrentTrain: epoch  5, batch    37 | loss: 439.0663447CurrentTrain: epoch  5, batch    38 | loss: 455.9440531CurrentTrain: epoch  5, batch    39 | loss: 455.2328390CurrentTrain: epoch  5, batch    40 | loss: 358.0403590CurrentTrain: epoch  5, batch    41 | loss: 261.3469708CurrentTrain: epoch  5, batch    42 | loss: 326.6299983CurrentTrain: epoch  5, batch    43 | loss: 316.9339602CurrentTrain: epoch  5, batch    44 | loss: 298.9474444CurrentTrain: epoch  5, batch    45 | loss: 327.4339988CurrentTrain: epoch  5, batch    46 | loss: 376.7316179CurrentTrain: epoch  5, batch    47 | loss: 325.0983061CurrentTrain: epoch  5, batch    48 | loss: 284.4844661CurrentTrain: epoch  5, batch    49 | loss: 344.0331711CurrentTrain: epoch  5, batch    50 | loss: 378.0975356CurrentTrain: epoch  5, batch    51 | loss: 309.4591509CurrentTrain: epoch  5, batch    52 | loss: 357.3382662CurrentTrain: epoch  5, batch    53 | loss: 373.0148498CurrentTrain: epoch  5, batch    54 | loss: 438.7895367CurrentTrain: epoch  5, batch    55 | loss: 252.4874014CurrentTrain: epoch  5, batch    56 | loss: 272.3015898CurrentTrain: epoch  5, batch    57 | loss: 357.8016855CurrentTrain: epoch  5, batch    58 | loss: 341.2071809CurrentTrain: epoch  5, batch    59 | loss: 271.0689934CurrentTrain: epoch  5, batch    60 | loss: 341.1452456CurrentTrain: epoch  5, batch    61 | loss: 379.0208438CurrentTrain: epoch  5, batch    62 | loss: 325.9950068CurrentTrain: epoch  5, batch    63 | loss: 463.7068711CurrentTrain: epoch  5, batch    64 | loss: 285.3873152CurrentTrain: epoch  5, batch    65 | loss: 287.5554759CurrentTrain: epoch  5, batch    66 | loss: 302.8674980CurrentTrain: epoch  5, batch    67 | loss: 271.3994635CurrentTrain: epoch  5, batch    68 | loss: 391.6758179CurrentTrain: epoch  5, batch    69 | loss: 297.4938753CurrentTrain: epoch  5, batch    70 | loss: 317.2030911CurrentTrain: epoch  5, batch    71 | loss: 326.0508993CurrentTrain: epoch  5, batch    72 | loss: 390.4399236CurrentTrain: epoch  5, batch    73 | loss: 372.9337407CurrentTrain: epoch  5, batch    74 | loss: 357.8348040CurrentTrain: epoch  5, batch    75 | loss: 316.7706014CurrentTrain: epoch  5, batch    76 | loss: 327.0327356CurrentTrain: epoch  5, batch    77 | loss: 456.4525801CurrentTrain: epoch  5, batch    78 | loss: 271.4396858CurrentTrain: epoch  5, batch    79 | loss: 348.7362550CurrentTrain: epoch  5, batch    80 | loss: 310.4893095CurrentTrain: epoch  5, batch    81 | loss: 340.3531760CurrentTrain: epoch  5, batch    82 | loss: 358.3277006CurrentTrain: epoch  5, batch    83 | loss: 357.4689379CurrentTrain: epoch  5, batch    84 | loss: 264.2274764CurrentTrain: epoch  5, batch    85 | loss: 357.2081342CurrentTrain: epoch  5, batch    86 | loss: 376.9737294CurrentTrain: epoch  5, batch    87 | loss: 391.1763367CurrentTrain: epoch  5, batch    88 | loss: 356.6925795CurrentTrain: epoch  5, batch    89 | loss: 286.2378204CurrentTrain: epoch  5, batch    90 | loss: 332.0802442CurrentTrain: epoch  5, batch    91 | loss: 299.1918105CurrentTrain: epoch  5, batch    92 | loss: 284.3573867CurrentTrain: epoch  5, batch    93 | loss: 392.8955143CurrentTrain: epoch  5, batch    94 | loss: 297.3091517CurrentTrain: epoch  5, batch    95 | loss: 260.9485693CurrentTrain: epoch  6, batch     0 | loss: 328.9087173CurrentTrain: epoch  6, batch     1 | loss: 329.8666393CurrentTrain: epoch  6, batch     2 | loss: 321.7256753CurrentTrain: epoch  6, batch     3 | loss: 358.8908438CurrentTrain: epoch  6, batch     4 | loss: 374.5287666CurrentTrain: epoch  6, batch     5 | loss: 216.5971376CurrentTrain: epoch  6, batch     6 | loss: 278.3902889CurrentTrain: epoch  6, batch     7 | loss: 393.5802381CurrentTrain: epoch  6, batch     8 | loss: 377.1016413CurrentTrain: epoch  6, batch     9 | loss: 277.3498928CurrentTrain: epoch  6, batch    10 | loss: 437.7549057CurrentTrain: epoch  6, batch    11 | loss: 390.1903030CurrentTrain: epoch  6, batch    12 | loss: 345.6782221CurrentTrain: epoch  6, batch    13 | loss: 270.7601010CurrentTrain: epoch  6, batch    14 | loss: 372.6031799CurrentTrain: epoch  6, batch    15 | loss: 342.0305870CurrentTrain: epoch  6, batch    16 | loss: 306.3466174CurrentTrain: epoch  6, batch    17 | loss: 232.6600826CurrentTrain: epoch  6, batch    18 | loss: 263.6973031CurrentTrain: epoch  6, batch    19 | loss: 244.0709818CurrentTrain: epoch  6, batch    20 | loss: 298.4385143CurrentTrain: epoch  6, batch    21 | loss: 344.2132351CurrentTrain: epoch  6, batch    22 | loss: 340.9061471CurrentTrain: epoch  6, batch    23 | loss: 408.1099242CurrentTrain: epoch  6, batch    24 | loss: 275.8679573CurrentTrain: epoch  6, batch    25 | loss: 438.5008741CurrentTrain: epoch  6, batch    26 | loss: 372.2937488CurrentTrain: epoch  6, batch    27 | loss: 357.3234596CurrentTrain: epoch  6, batch    28 | loss: 310.8266026CurrentTrain: epoch  6, batch    29 | loss: 357.1998886CurrentTrain: epoch  6, batch    30 | loss: 339.9899295CurrentTrain: epoch  6, batch    31 | loss: 324.7147776CurrentTrain: epoch  6, batch    32 | loss: 284.3119965CurrentTrain: epoch  6, batch    33 | loss: 359.4917307CurrentTrain: epoch  6, batch    34 | loss: 298.6258748CurrentTrain: epoch  6, batch    35 | loss: 347.6908565CurrentTrain: epoch  6, batch    36 | loss: 312.6801614CurrentTrain: epoch  6, batch    37 | loss: 373.0631457CurrentTrain: epoch  6, batch    38 | loss: 373.2250334CurrentTrain: epoch  6, batch    39 | loss: 356.9941182CurrentTrain: epoch  6, batch    40 | loss: 313.0677395CurrentTrain: epoch  6, batch    41 | loss: 327.4620551CurrentTrain: epoch  6, batch    42 | loss: 276.7691001CurrentTrain: epoch  6, batch    43 | loss: 325.0904171CurrentTrain: epoch  6, batch    44 | loss: 314.7125119CurrentTrain: epoch  6, batch    45 | loss: 340.8344246CurrentTrain: epoch  6, batch    46 | loss: 297.7603846CurrentTrain: epoch  6, batch    47 | loss: 372.9181377CurrentTrain: epoch  6, batch    48 | loss: 373.7188087CurrentTrain: epoch  6, batch    49 | loss: 355.6392539CurrentTrain: epoch  6, batch    50 | loss: 238.7793201CurrentTrain: epoch  6, batch    51 | loss: 312.3178830CurrentTrain: epoch  6, batch    52 | loss: 461.6677683CurrentTrain: epoch  6, batch    53 | loss: 437.8426356CurrentTrain: epoch  6, batch    54 | loss: 339.6924748CurrentTrain: epoch  6, batch    55 | loss: 306.4738129CurrentTrain: epoch  6, batch    56 | loss: 315.7613660CurrentTrain: epoch  6, batch    57 | loss: 348.2926632CurrentTrain: epoch  6, batch    58 | loss: 407.1831991CurrentTrain: epoch  6, batch    59 | loss: 355.6997220CurrentTrain: epoch  6, batch    60 | loss: 355.4500866CurrentTrain: epoch  6, batch    61 | loss: 391.7409057CurrentTrain: epoch  6, batch    62 | loss: 344.4307563CurrentTrain: epoch  6, batch    63 | loss: 356.1761092CurrentTrain: epoch  6, batch    64 | loss: 372.2424254CurrentTrain: epoch  6, batch    65 | loss: 325.7356168CurrentTrain: epoch  6, batch    66 | loss: 297.5007768CurrentTrain: epoch  6, batch    67 | loss: 316.8460481CurrentTrain: epoch  6, batch    68 | loss: 390.6989804CurrentTrain: epoch  6, batch    69 | loss: 373.2766992CurrentTrain: epoch  6, batch    70 | loss: 343.2131319CurrentTrain: epoch  6, batch    71 | loss: 318.1653744CurrentTrain: epoch  6, batch    72 | loss: 421.3654072CurrentTrain: epoch  6, batch    73 | loss: 284.4005528CurrentTrain: epoch  6, batch    74 | loss: 456.3639185CurrentTrain: epoch  6, batch    75 | loss: 340.5537922CurrentTrain: epoch  6, batch    76 | loss: 272.8578714CurrentTrain: epoch  6, batch    77 | loss: 340.7335268CurrentTrain: epoch  6, batch    78 | loss: 402.6883860CurrentTrain: epoch  6, batch    79 | loss: 390.6808344CurrentTrain: epoch  6, batch    80 | loss: 456.4847727CurrentTrain: epoch  6, batch    81 | loss: 287.9557308CurrentTrain: epoch  6, batch    82 | loss: 466.6466409CurrentTrain: epoch  6, batch    83 | loss: 279.7072262CurrentTrain: epoch  6, batch    84 | loss: 372.2735145CurrentTrain: epoch  6, batch    85 | loss: 356.1723492CurrentTrain: epoch  6, batch    86 | loss: 408.8107345CurrentTrain: epoch  6, batch    87 | loss: 342.7535551CurrentTrain: epoch  6, batch    88 | loss: 328.8584581CurrentTrain: epoch  6, batch    89 | loss: 390.3704627CurrentTrain: epoch  6, batch    90 | loss: 455.2390039CurrentTrain: epoch  6, batch    91 | loss: 418.8935272CurrentTrain: epoch  6, batch    92 | loss: 244.9650198CurrentTrain: epoch  6, batch    93 | loss: 359.2770802CurrentTrain: epoch  6, batch    94 | loss: 314.9491443CurrentTrain: epoch  6, batch    95 | loss: 276.2960780CurrentTrain: epoch  7, batch     0 | loss: 356.2009264CurrentTrain: epoch  7, batch     1 | loss: 422.6664692CurrentTrain: epoch  7, batch     2 | loss: 389.5608511CurrentTrain: epoch  7, batch     3 | loss: 357.0995973CurrentTrain: epoch  7, batch     4 | loss: 390.4166935CurrentTrain: epoch  7, batch     5 | loss: 355.5583512CurrentTrain: epoch  7, batch     6 | loss: 378.3553090CurrentTrain: epoch  7, batch     7 | loss: 299.3233784CurrentTrain: epoch  7, batch     8 | loss: 233.4098848CurrentTrain: epoch  7, batch     9 | loss: 357.9208265CurrentTrain: epoch  7, batch    10 | loss: 314.3403513CurrentTrain: epoch  7, batch    11 | loss: 258.6930581CurrentTrain: epoch  7, batch    12 | loss: 356.6066886CurrentTrain: epoch  7, batch    13 | loss: 455.6313864CurrentTrain: epoch  7, batch    14 | loss: 377.4132969CurrentTrain: epoch  7, batch    15 | loss: 237.5148322CurrentTrain: epoch  7, batch    16 | loss: 310.2920380CurrentTrain: epoch  7, batch    17 | loss: 313.4973980CurrentTrain: epoch  7, batch    18 | loss: 339.8166147CurrentTrain: epoch  7, batch    19 | loss: 372.6274757CurrentTrain: epoch  7, batch    20 | loss: 276.4982147CurrentTrain: epoch  7, batch    21 | loss: 373.4470337CurrentTrain: epoch  7, batch    22 | loss: 355.7096119CurrentTrain: epoch  7, batch    23 | loss: 391.9910803CurrentTrain: epoch  7, batch    24 | loss: 332.6669442CurrentTrain: epoch  7, batch    25 | loss: 341.6556425CurrentTrain: epoch  7, batch    26 | loss: 309.2233024CurrentTrain: epoch  7, batch    27 | loss: 325.8586306CurrentTrain: epoch  7, batch    28 | loss: 283.7782146CurrentTrain: epoch  7, batch    29 | loss: 309.0733988CurrentTrain: epoch  7, batch    30 | loss: 438.4819241CurrentTrain: epoch  7, batch    31 | loss: 314.0024752CurrentTrain: epoch  7, batch    32 | loss: 341.8815201CurrentTrain: epoch  7, batch    33 | loss: 360.0941583CurrentTrain: epoch  7, batch    34 | loss: 436.8435683CurrentTrain: epoch  7, batch    35 | loss: 375.3288252CurrentTrain: epoch  7, batch    36 | loss: 284.2697113CurrentTrain: epoch  7, batch    37 | loss: 374.8850741CurrentTrain: epoch  7, batch    38 | loss: 250.3779837CurrentTrain: epoch  7, batch    39 | loss: 339.6263105CurrentTrain: epoch  7, batch    40 | loss: 355.7685839CurrentTrain: epoch  7, batch    41 | loss: 328.9744095CurrentTrain: epoch  7, batch    42 | loss: 356.4739321CurrentTrain: epoch  7, batch    43 | loss: 314.2064862CurrentTrain: epoch  7, batch    44 | loss: 357.6549630CurrentTrain: epoch  7, batch    45 | loss: 298.4403379CurrentTrain: epoch  7, batch    46 | loss: 342.2680535CurrentTrain: epoch  7, batch    47 | loss: 286.5771691CurrentTrain: epoch  7, batch    48 | loss: 330.7997058CurrentTrain: epoch  7, batch    49 | loss: 436.9599176CurrentTrain: epoch  7, batch    50 | loss: 299.2203630CurrentTrain: epoch  7, batch    51 | loss: 372.9009760CurrentTrain: epoch  7, batch    52 | loss: 299.4612575CurrentTrain: epoch  7, batch    53 | loss: 249.6297087CurrentTrain: epoch  7, batch    54 | loss: 313.0241179CurrentTrain: epoch  7, batch    55 | loss: 341.2776043CurrentTrain: epoch  7, batch    56 | loss: 413.8565148CurrentTrain: epoch  7, batch    57 | loss: 298.9938789CurrentTrain: epoch  7, batch    58 | loss: 324.2215160CurrentTrain: epoch  7, batch    59 | loss: 373.9000422CurrentTrain: epoch  7, batch    60 | loss: 373.6238191CurrentTrain: epoch  7, batch    61 | loss: 324.3646138CurrentTrain: epoch  7, batch    62 | loss: 325.6110117CurrentTrain: epoch  7, batch    63 | loss: 314.2784464CurrentTrain: epoch  7, batch    64 | loss: 389.6232408CurrentTrain: epoch  7, batch    65 | loss: 279.0893553CurrentTrain: epoch  7, batch    66 | loss: 418.5894904CurrentTrain: epoch  7, batch    67 | loss: 455.2647272CurrentTrain: epoch  7, batch    68 | loss: 329.2848343CurrentTrain: epoch  7, batch    69 | loss: 329.8633182CurrentTrain: epoch  7, batch    70 | loss: 325.1881023CurrentTrain: epoch  7, batch    71 | loss: 324.9108460CurrentTrain: epoch  7, batch    72 | loss: 375.3446961CurrentTrain: epoch  7, batch    73 | loss: 359.1905862CurrentTrain: epoch  7, batch    74 | loss: 436.8569550CurrentTrain: epoch  7, batch    75 | loss: 342.3004508CurrentTrain: epoch  7, batch    76 | loss: 358.4596423CurrentTrain: epoch  7, batch    77 | loss: 271.0421109CurrentTrain: epoch  7, batch    78 | loss: 285.7877634CurrentTrain: epoch  7, batch    79 | loss: 325.8859469CurrentTrain: epoch  7, batch    80 | loss: 407.1557018CurrentTrain: epoch  7, batch    81 | loss: 314.3705182CurrentTrain: epoch  7, batch    82 | loss: 372.5182013CurrentTrain: epoch  7, batch    83 | loss: 407.1481885CurrentTrain: epoch  7, batch    84 | loss: 298.6671833CurrentTrain: epoch  7, batch    85 | loss: 339.4839863CurrentTrain: epoch  7, batch    86 | loss: 323.7387730CurrentTrain: epoch  7, batch    87 | loss: 407.5004699CurrentTrain: epoch  7, batch    88 | loss: 301.7285007CurrentTrain: epoch  7, batch    89 | loss: 328.7048950CurrentTrain: epoch  7, batch    90 | loss: 314.2297366CurrentTrain: epoch  7, batch    91 | loss: 313.8016353CurrentTrain: epoch  7, batch    92 | loss: 340.3246793CurrentTrain: epoch  7, batch    93 | loss: 314.6555125CurrentTrain: epoch  7, batch    94 | loss: 311.2929273CurrentTrain: epoch  7, batch    95 | loss: 306.6818627CurrentTrain: epoch  8, batch     0 | loss: 313.7820847CurrentTrain: epoch  8, batch     1 | loss: 324.1512062CurrentTrain: epoch  8, batch     2 | loss: 313.3904813CurrentTrain: epoch  8, batch     3 | loss: 437.0753055CurrentTrain: epoch  8, batch     4 | loss: 377.0215054CurrentTrain: epoch  8, batch     5 | loss: 358.6758064CurrentTrain: epoch  8, batch     6 | loss: 297.7529743CurrentTrain: epoch  8, batch     7 | loss: 389.5858960CurrentTrain: epoch  8, batch     8 | loss: 372.3756116CurrentTrain: epoch  8, batch     9 | loss: 328.5331352CurrentTrain: epoch  8, batch    10 | loss: 436.8051580CurrentTrain: epoch  8, batch    11 | loss: 296.7012422CurrentTrain: epoch  8, batch    12 | loss: 299.6760860CurrentTrain: epoch  8, batch    13 | loss: 328.4604020CurrentTrain: epoch  8, batch    14 | loss: 313.6290414CurrentTrain: epoch  8, batch    15 | loss: 356.0804939CurrentTrain: epoch  8, batch    16 | loss: 407.3296855CurrentTrain: epoch  8, batch    17 | loss: 298.4104727CurrentTrain: epoch  8, batch    18 | loss: 328.5496966CurrentTrain: epoch  8, batch    19 | loss: 373.0359570CurrentTrain: epoch  8, batch    20 | loss: 300.6647202CurrentTrain: epoch  8, batch    21 | loss: 355.6072138CurrentTrain: epoch  8, batch    22 | loss: 373.1240046CurrentTrain: epoch  8, batch    23 | loss: 339.7065544CurrentTrain: epoch  8, batch    24 | loss: 291.3181579CurrentTrain: epoch  8, batch    25 | loss: 269.9593399CurrentTrain: epoch  8, batch    26 | loss: 339.8159517CurrentTrain: epoch  8, batch    27 | loss: 309.4039042CurrentTrain: epoch  8, batch    28 | loss: 341.0119261CurrentTrain: epoch  8, batch    29 | loss: 407.6592603CurrentTrain: epoch  8, batch    30 | loss: 301.0272216CurrentTrain: epoch  8, batch    31 | loss: 378.0587031CurrentTrain: epoch  8, batch    32 | loss: 246.7038003CurrentTrain: epoch  8, batch    33 | loss: 237.5644039CurrentTrain: epoch  8, batch    34 | loss: 281.9036012CurrentTrain: epoch  8, batch    35 | loss: 313.3528900CurrentTrain: epoch  8, batch    36 | loss: 276.1958173CurrentTrain: epoch  8, batch    37 | loss: 298.3205466CurrentTrain: epoch  8, batch    38 | loss: 329.0014453CurrentTrain: epoch  8, batch    39 | loss: 324.2548643CurrentTrain: epoch  8, batch    40 | loss: 356.8275386CurrentTrain: epoch  8, batch    41 | loss: 299.9485586CurrentTrain: epoch  8, batch    42 | loss: 355.5717455CurrentTrain: epoch  8, batch    43 | loss: 270.8323396CurrentTrain: epoch  8, batch    44 | loss: 389.4942248CurrentTrain: epoch  8, batch    45 | loss: 407.2604621CurrentTrain: epoch  8, batch    46 | loss: 315.0609789CurrentTrain: epoch  8, batch    47 | loss: 455.0715388CurrentTrain: epoch  8, batch    48 | loss: 373.8899203CurrentTrain: epoch  8, batch    49 | loss: 393.9757937CurrentTrain: epoch  8, batch    50 | loss: 312.9778886CurrentTrain: epoch  8, batch    51 | loss: 327.2084553CurrentTrain: epoch  8, batch    52 | loss: 284.3739457CurrentTrain: epoch  8, batch    53 | loss: 236.8731162CurrentTrain: epoch  8, batch    54 | loss: 340.3469594CurrentTrain: epoch  8, batch    55 | loss: 342.3215228CurrentTrain: epoch  8, batch    56 | loss: 389.6221308CurrentTrain: epoch  8, batch    57 | loss: 536.8192250CurrentTrain: epoch  8, batch    58 | loss: 324.1151572CurrentTrain: epoch  8, batch    59 | loss: 390.4203521CurrentTrain: epoch  8, batch    60 | loss: 270.9438281CurrentTrain: epoch  8, batch    61 | loss: 436.7948410CurrentTrain: epoch  8, batch    62 | loss: 407.4707018CurrentTrain: epoch  8, batch    63 | loss: 313.0846519CurrentTrain: epoch  8, batch    64 | loss: 389.9396687CurrentTrain: epoch  8, batch    65 | loss: 290.6915979CurrentTrain: epoch  8, batch    66 | loss: 324.0440347CurrentTrain: epoch  8, batch    67 | loss: 359.1819656CurrentTrain: epoch  8, batch    68 | loss: 298.7501211CurrentTrain: epoch  8, batch    69 | loss: 356.1302884CurrentTrain: epoch  8, batch    70 | loss: 372.6364292CurrentTrain: epoch  8, batch    71 | loss: 314.7814703CurrentTrain: epoch  8, batch    72 | loss: 324.1646371CurrentTrain: epoch  8, batch    73 | loss: 341.1093511CurrentTrain: epoch  8, batch    74 | loss: 272.6368617CurrentTrain: epoch  8, batch    75 | loss: 315.1528262CurrentTrain: epoch  8, batch    76 | loss: 389.7792161CurrentTrain: epoch  8, batch    77 | loss: 458.7562530CurrentTrain: epoch  8, batch    78 | loss: 373.6454983CurrentTrain: epoch  8, batch    79 | loss: 313.2728352CurrentTrain: epoch  8, batch    80 | loss: 389.7504732CurrentTrain: epoch  8, batch    81 | loss: 373.0841521CurrentTrain: epoch  8, batch    82 | loss: 326.7620945CurrentTrain: epoch  8, batch    83 | loss: 374.4222624CurrentTrain: epoch  8, batch    84 | loss: 390.7253273CurrentTrain: epoch  8, batch    85 | loss: 310.2868333CurrentTrain: epoch  8, batch    86 | loss: 420.2044057CurrentTrain: epoch  8, batch    87 | loss: 409.0656789CurrentTrain: epoch  8, batch    88 | loss: 224.6771964CurrentTrain: epoch  8, batch    89 | loss: 376.5282208CurrentTrain: epoch  8, batch    90 | loss: 263.4850111CurrentTrain: epoch  8, batch    91 | loss: 455.1072726CurrentTrain: epoch  8, batch    92 | loss: 356.8215119CurrentTrain: epoch  8, batch    93 | loss: 324.7531230CurrentTrain: epoch  8, batch    94 | loss: 276.4878267CurrentTrain: epoch  8, batch    95 | loss: 351.1477494CurrentTrain: epoch  9, batch     0 | loss: 373.6255314CurrentTrain: epoch  9, batch     1 | loss: 340.8547733CurrentTrain: epoch  9, batch     2 | loss: 340.4006093CurrentTrain: epoch  9, batch     3 | loss: 341.6615992CurrentTrain: epoch  9, batch     4 | loss: 340.0266171CurrentTrain: epoch  9, batch     5 | loss: 373.2160442CurrentTrain: epoch  9, batch     6 | loss: 294.5942220CurrentTrain: epoch  9, batch     7 | loss: 373.5654240CurrentTrain: epoch  9, batch     8 | loss: 341.7297218CurrentTrain: epoch  9, batch     9 | loss: 284.6496476CurrentTrain: epoch  9, batch    10 | loss: 373.5781458CurrentTrain: epoch  9, batch    11 | loss: 403.0004580CurrentTrain: epoch  9, batch    12 | loss: 356.3816826CurrentTrain: epoch  9, batch    13 | loss: 342.2172588CurrentTrain: epoch  9, batch    14 | loss: 339.3435018CurrentTrain: epoch  9, batch    15 | loss: 356.0909173CurrentTrain: epoch  9, batch    16 | loss: 356.7319981CurrentTrain: epoch  9, batch    17 | loss: 313.5821276CurrentTrain: epoch  9, batch    18 | loss: 355.9349179CurrentTrain: epoch  9, batch    19 | loss: 327.4120917CurrentTrain: epoch  9, batch    20 | loss: 437.8251021CurrentTrain: epoch  9, batch    21 | loss: 283.5304973CurrentTrain: epoch  9, batch    22 | loss: 283.8720727CurrentTrain: epoch  9, batch    23 | loss: 270.4118173CurrentTrain: epoch  9, batch    24 | loss: 418.7829532CurrentTrain: epoch  9, batch    25 | loss: 309.3734941CurrentTrain: epoch  9, batch    26 | loss: 299.0386996CurrentTrain: epoch  9, batch    27 | loss: 269.7590311CurrentTrain: epoch  9, batch    28 | loss: 389.7441900CurrentTrain: epoch  9, batch    29 | loss: 262.2710065CurrentTrain: epoch  9, batch    30 | loss: 313.8311270CurrentTrain: epoch  9, batch    31 | loss: 328.2198383CurrentTrain: epoch  9, batch    32 | loss: 408.0817044CurrentTrain: epoch  9, batch    33 | loss: 372.3016308CurrentTrain: epoch  9, batch    34 | loss: 341.3424275CurrentTrain: epoch  9, batch    35 | loss: 328.5792510CurrentTrain: epoch  9, batch    36 | loss: 394.4247266CurrentTrain: epoch  9, batch    37 | loss: 339.6289180CurrentTrain: epoch  9, batch    38 | loss: 257.1808944CurrentTrain: epoch  9, batch    39 | loss: 356.6934640CurrentTrain: epoch  9, batch    40 | loss: 324.7493623CurrentTrain: epoch  9, batch    41 | loss: 355.6998492CurrentTrain: epoch  9, batch    42 | loss: 372.5626180CurrentTrain: epoch  9, batch    43 | loss: 343.1045022CurrentTrain: epoch  9, batch    44 | loss: 419.4289492CurrentTrain: epoch  9, batch    45 | loss: 309.2264289CurrentTrain: epoch  9, batch    46 | loss: 538.3114143CurrentTrain: epoch  9, batch    47 | loss: 324.0504052CurrentTrain: epoch  9, batch    48 | loss: 390.4961809CurrentTrain: epoch  9, batch    49 | loss: 324.5243349CurrentTrain: epoch  9, batch    50 | loss: 328.4209379CurrentTrain: epoch  9, batch    51 | loss: 298.2946382CurrentTrain: epoch  9, batch    52 | loss: 323.5536765CurrentTrain: epoch  9, batch    53 | loss: 330.0567919CurrentTrain: epoch  9, batch    54 | loss: 390.2341125CurrentTrain: epoch  9, batch    55 | loss: 515.3012272CurrentTrain: epoch  9, batch    56 | loss: 372.3326256CurrentTrain: epoch  9, batch    57 | loss: 313.4144480CurrentTrain: epoch  9, batch    58 | loss: 256.9018117CurrentTrain: epoch  9, batch    59 | loss: 355.5006343CurrentTrain: epoch  9, batch    60 | loss: 436.5766667CurrentTrain: epoch  9, batch    61 | loss: 256.8879895CurrentTrain: epoch  9, batch    62 | loss: 344.3760534CurrentTrain: epoch  9, batch    63 | loss: 389.5764974CurrentTrain: epoch  9, batch    64 | loss: 256.4385800CurrentTrain: epoch  9, batch    65 | loss: 356.4204086CurrentTrain: epoch  9, batch    66 | loss: 392.8947301CurrentTrain: epoch  9, batch    67 | loss: 326.8647371CurrentTrain: epoch  9, batch    68 | loss: 309.5538712CurrentTrain: epoch  9, batch    69 | loss: 285.0246322CurrentTrain: epoch  9, batch    70 | loss: 313.3146374CurrentTrain: epoch  9, batch    71 | loss: 436.6804561CurrentTrain: epoch  9, batch    72 | loss: 436.5793370CurrentTrain: epoch  9, batch    73 | loss: 375.0390831CurrentTrain: epoch  9, batch    74 | loss: 340.0234917CurrentTrain: epoch  9, batch    75 | loss: 328.8326209CurrentTrain: epoch  9, batch    76 | loss: 324.1429275CurrentTrain: epoch  9, batch    77 | loss: 312.5401487CurrentTrain: epoch  9, batch    78 | loss: 305.5319196CurrentTrain: epoch  9, batch    79 | loss: 355.4222760CurrentTrain: epoch  9, batch    80 | loss: 313.4006031CurrentTrain: epoch  9, batch    81 | loss: 236.0269439CurrentTrain: epoch  9, batch    82 | loss: 324.2653900CurrentTrain: epoch  9, batch    83 | loss: 328.9195270CurrentTrain: epoch  9, batch    84 | loss: 374.2415661CurrentTrain: epoch  9, batch    85 | loss: 339.4770846CurrentTrain: epoch  9, batch    86 | loss: 356.3636817CurrentTrain: epoch  9, batch    87 | loss: 313.0309006CurrentTrain: epoch  9, batch    88 | loss: 436.6831325CurrentTrain: epoch  9, batch    89 | loss: 256.1659875CurrentTrain: epoch  9, batch    90 | loss: 325.2331854CurrentTrain: epoch  9, batch    91 | loss: 324.5287772CurrentTrain: epoch  9, batch    92 | loss: 455.0764589CurrentTrain: epoch  9, batch    93 | loss: 280.4966531CurrentTrain: epoch  9, batch    94 | loss: 340.0302719CurrentTrain: epoch  9, batch    95 | loss: 337.0751809

F1 score per class: {32: 0.6951871657754011, 6: 0.8636363636363636, 19: 0.48, 24: 0.7619047619047619, 26: 0.93048128342246, 29: 0.9191919191919192}
Micro-average F1 score: 0.8253638253638254
Weighted-average F1 score: 0.8295492130786248
F1 score per class: {32: 0.7707317073170732, 6: 0.907103825136612, 19: 0.6428571428571429, 24: 0.7567567567567568, 26: 0.9797979797979798, 29: 0.9191919191919192}
Micro-average F1 score: 0.8605817452357071
Weighted-average F1 score: 0.8623598175543578
F1 score per class: {32: 0.7745098039215687, 6: 0.907103825136612, 19: 0.6428571428571429, 24: 0.7567567567567568, 26: 0.9797979797979798, 29: 0.9191919191919192}
Micro-average F1 score: 0.8614457831325302
Weighted-average F1 score: 0.8633735208118992

F1 score per class: {32: 0.6951871657754011, 6: 0.8636363636363636, 19: 0.48, 24: 0.7619047619047619, 26: 0.93048128342246, 29: 0.9191919191919192}
Micro-average F1 score: 0.8253638253638254
Weighted-average F1 score: 0.8295492130786248
F1 score per class: {32: 0.7707317073170732, 6: 0.907103825136612, 19: 0.6428571428571429, 24: 0.7567567567567568, 26: 0.9797979797979798, 29: 0.9191919191919192}
Micro-average F1 score: 0.8605817452357071
Weighted-average F1 score: 0.8623598175543578
F1 score per class: {32: 0.7745098039215687, 6: 0.907103825136612, 19: 0.6428571428571429, 24: 0.7567567567567568, 26: 0.9797979797979798, 29: 0.9191919191919192}
Micro-average F1 score: 0.8614457831325302
Weighted-average F1 score: 0.8633735208118992
cur_acc:  ['0.8254']
his_acc:  ['0.8254']
cur_acc des:  ['0.8606']
his_acc des:  ['0.8606']
cur_acc rrf:  ['0.8614']
his_acc rrf:  ['0.8614']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 310.1162004CurrentTrain: epoch  0, batch     1 | loss: 459.1354628CurrentTrain: epoch  0, batch     2 | loss: 423.8290016CurrentTrain: epoch  0, batch     3 | loss: 334.9673965CurrentTrain: epoch  0, batch     4 | loss: 276.2166925CurrentTrain: epoch  1, batch     0 | loss: 353.0620095CurrentTrain: epoch  1, batch     1 | loss: 351.8666345CurrentTrain: epoch  1, batch     2 | loss: 419.9262073CurrentTrain: epoch  1, batch     3 | loss: 337.3383173CurrentTrain: epoch  1, batch     4 | loss: 254.7575894CurrentTrain: epoch  2, batch     0 | loss: 395.8629693CurrentTrain: epoch  2, batch     1 | loss: 338.6005525CurrentTrain: epoch  2, batch     2 | loss: 363.7744928CurrentTrain: epoch  2, batch     3 | loss: 364.8319581CurrentTrain: epoch  2, batch     4 | loss: 228.0204306CurrentTrain: epoch  3, batch     0 | loss: 351.8141589CurrentTrain: epoch  3, batch     1 | loss: 368.1290556CurrentTrain: epoch  3, batch     2 | loss: 348.3117113CurrentTrain: epoch  3, batch     3 | loss: 424.7065329CurrentTrain: epoch  3, batch     4 | loss: 285.7611653CurrentTrain: epoch  4, batch     0 | loss: 360.6986356CurrentTrain: epoch  4, batch     1 | loss: 306.6256834CurrentTrain: epoch  4, batch     2 | loss: 379.4817756CurrentTrain: epoch  4, batch     3 | loss: 440.4370230CurrentTrain: epoch  4, batch     4 | loss: 237.1719002CurrentTrain: epoch  5, batch     0 | loss: 375.6270246CurrentTrain: epoch  5, batch     1 | loss: 392.2238496CurrentTrain: epoch  5, batch     2 | loss: 332.0372486CurrentTrain: epoch  5, batch     3 | loss: 332.9587432CurrentTrain: epoch  5, batch     4 | loss: 209.8956828CurrentTrain: epoch  6, batch     0 | loss: 344.8089426CurrentTrain: epoch  6, batch     1 | loss: 287.3452461CurrentTrain: epoch  6, batch     2 | loss: 357.6104880CurrentTrain: epoch  6, batch     3 | loss: 457.1018130CurrentTrain: epoch  6, batch     4 | loss: 267.8309686CurrentTrain: epoch  7, batch     0 | loss: 328.0333535CurrentTrain: epoch  7, batch     1 | loss: 391.0396910CurrentTrain: epoch  7, batch     2 | loss: 342.1516480CurrentTrain: epoch  7, batch     3 | loss: 408.6497151CurrentTrain: epoch  7, batch     4 | loss: 222.3964435CurrentTrain: epoch  8, batch     0 | loss: 357.4284185CurrentTrain: epoch  8, batch     1 | loss: 327.7797462CurrentTrain: epoch  8, batch     2 | loss: 419.4313075CurrentTrain: epoch  8, batch     3 | loss: 436.9014067CurrentTrain: epoch  8, batch     4 | loss: 223.7825656CurrentTrain: epoch  9, batch     0 | loss: 315.0759322CurrentTrain: epoch  9, batch     1 | loss: 408.3741830CurrentTrain: epoch  9, batch     2 | loss: 373.1464120CurrentTrain: epoch  9, batch     3 | loss: 310.4515216CurrentTrain: epoch  9, batch     4 | loss: 321.4576230
MemoryTrain:  epoch  0, batch     0 | loss: 2.4310101MemoryTrain:  epoch  1, batch     0 | loss: 1.8161777MemoryTrain:  epoch  2, batch     0 | loss: 1.4843219MemoryTrain:  epoch  3, batch     0 | loss: 1.0251403MemoryTrain:  epoch  4, batch     0 | loss: 0.9126126MemoryTrain:  epoch  5, batch     0 | loss: 0.6047170MemoryTrain:  epoch  6, batch     0 | loss: 0.4098880MemoryTrain:  epoch  7, batch     0 | loss: 0.5750638MemoryTrain:  epoch  8, batch     0 | loss: 0.2674561MemoryTrain:  epoch  9, batch     0 | loss: 0.2887926

F1 score per class: {5: 0.9690721649484536, 6: 0.0, 10: 0.6206896551724138, 16: 0.75, 17: 0.0, 18: 0.3181818181818182, 19: 0.0}
Micro-average F1 score: 0.7256637168141593
Weighted-average F1 score: 0.765631197245375
F1 score per class: {5: 0.9797979797979798, 6: 0.0, 10: 0.7484662576687117, 16: 0.8, 17: 0.0, 18: 0.8064516129032258, 19: 0.0}
Micro-average F1 score: 0.8152610441767069
Weighted-average F1 score: 0.8078147143059076
F1 score per class: {5: 0.9847715736040609, 6: 0.0, 10: 0.7560975609756098, 16: 0.8, 17: 0.0, 18: 0.7868852459016393, 19: 0.0}
Micro-average F1 score: 0.8185483870967742
Weighted-average F1 score: 0.812719692925783

F1 score per class: {32: 0.9641025641025641, 5: 0.6732673267326733, 6: 0.6206896551724138, 10: 0.75, 16: 0.0, 17: 0.3181818181818182, 18: 0.7976190476190477, 19: 0.5, 24: 0.7526881720430108, 26: 0.93048128342246, 29: 0.9137055837563451}
Micro-average F1 score: 0.7852062588904695
Weighted-average F1 score: 0.8095946232719301
F1 score per class: {32: 0.941747572815534, 5: 0.7387387387387387, 6: 0.7393939393939394, 10: 0.8, 16: 0.0, 17: 0.7692307692307693, 18: 0.8839779005524862, 19: 0.5454545454545454, 24: 0.7567567567567568, 26: 0.9424083769633508, 29: 0.9072164948453608}
Micro-average F1 score: 0.8287808127914723
Weighted-average F1 score: 0.8354968906841062
F1 score per class: {32: 0.9509803921568627, 5: 0.728110599078341, 6: 0.7469879518072289, 10: 0.8, 16: 0.0, 17: 0.7619047619047619, 18: 0.8901098901098901, 19: 0.5806451612903226, 24: 0.7526881720430108, 26: 0.9479166666666666, 29: 0.9025641025641026}
Micro-average F1 score: 0.8302139037433155
Weighted-average F1 score: 0.836783665902689
cur_acc:  ['0.8254', '0.7257']
his_acc:  ['0.8254', '0.7852']
cur_acc des:  ['0.8606', '0.8153']
his_acc des:  ['0.8606', '0.8288']
cur_acc rrf:  ['0.8614', '0.8185']
his_acc rrf:  ['0.8614', '0.8302']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 296.4812742CurrentTrain: epoch  0, batch     1 | loss: 370.4149075CurrentTrain: epoch  0, batch     2 | loss: 309.1696495CurrentTrain: epoch  0, batch     3 | loss: 55.2596131CurrentTrain: epoch  1, batch     0 | loss: 290.2997751CurrentTrain: epoch  1, batch     1 | loss: 324.4792571CurrentTrain: epoch  1, batch     2 | loss: 357.4279186CurrentTrain: epoch  1, batch     3 | loss: 30.0616167CurrentTrain: epoch  2, batch     0 | loss: 333.4475956CurrentTrain: epoch  2, batch     1 | loss: 310.4726613CurrentTrain: epoch  2, batch     2 | loss: 309.0089928CurrentTrain: epoch  2, batch     3 | loss: 30.1663872CurrentTrain: epoch  3, batch     0 | loss: 362.5386353CurrentTrain: epoch  3, batch     1 | loss: 357.8932460CurrentTrain: epoch  3, batch     2 | loss: 240.4227707CurrentTrain: epoch  3, batch     3 | loss: 30.9031489CurrentTrain: epoch  4, batch     0 | loss: 227.8508787CurrentTrain: epoch  4, batch     1 | loss: 319.1011941CurrentTrain: epoch  4, batch     2 | loss: 393.2478150CurrentTrain: epoch  4, batch     3 | loss: 54.7735023CurrentTrain: epoch  5, batch     0 | loss: 259.6560883CurrentTrain: epoch  5, batch     1 | loss: 361.7197686CurrentTrain: epoch  5, batch     2 | loss: 361.9982412CurrentTrain: epoch  5, batch     3 | loss: 10.4946708CurrentTrain: epoch  6, batch     0 | loss: 343.1288196CurrentTrain: epoch  6, batch     1 | loss: 234.1544101CurrentTrain: epoch  6, batch     2 | loss: 375.6342706CurrentTrain: epoch  6, batch     3 | loss: 29.8170651CurrentTrain: epoch  7, batch     0 | loss: 538.0312310CurrentTrain: epoch  7, batch     1 | loss: 233.3327592CurrentTrain: epoch  7, batch     2 | loss: 276.6468551CurrentTrain: epoch  7, batch     3 | loss: 29.8214927CurrentTrain: epoch  8, batch     0 | loss: 392.1345019CurrentTrain: epoch  8, batch     1 | loss: 285.7037486CurrentTrain: epoch  8, batch     2 | loss: 257.9400053CurrentTrain: epoch  8, batch     3 | loss: 29.8976032CurrentTrain: epoch  9, batch     0 | loss: 299.0981232CurrentTrain: epoch  9, batch     1 | loss: 295.8614573CurrentTrain: epoch  9, batch     2 | loss: 340.6047827CurrentTrain: epoch  9, batch     3 | loss: 29.8680036
MemoryTrain:  epoch  0, batch     0 | loss: 1.1011462MemoryTrain:  epoch  1, batch     0 | loss: 0.7809595MemoryTrain:  epoch  2, batch     0 | loss: 0.6072930MemoryTrain:  epoch  3, batch     0 | loss: 0.5365007MemoryTrain:  epoch  4, batch     0 | loss: 0.4250616MemoryTrain:  epoch  5, batch     0 | loss: 0.3036426MemoryTrain:  epoch  6, batch     0 | loss: 0.2720999MemoryTrain:  epoch  7, batch     0 | loss: 0.1846303MemoryTrain:  epoch  8, batch     0 | loss: 0.1472452MemoryTrain:  epoch  9, batch     0 | loss: 0.1446538

F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6, 27: 0.0, 31: 0.2564102564102564}
Micro-average F1 score: 0.40375586854460094
Weighted-average F1 score: 0.3274420568538215
F1 score per class: {6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.5833333333333334, 27: 1.0, 31: 0.5684210526315789}
Micro-average F1 score: 0.5625
Weighted-average F1 score: 0.44924269229222785
F1 score per class: {5: 0.0, 6: 0.0, 7: 0.5714285714285714, 40: 0.9803921568627451, 9: 0.0, 19: 0.0, 24: 0.0, 26: 0.6363636363636364, 27: 1.0, 31: 0.5833333333333334}
Micro-average F1 score: 0.5714285714285714
Weighted-average F1 score: 0.4519387920457439

F1 score per class: {32: 0.964824120603015, 5: 0.5135135135135135, 6: 0.045454545454545456, 7: 0.9803921568627451, 40: 0.5401459854014599, 10: 0.7755102040816326, 9: 0.0, 16: 0.6296296296296297, 17: 0.6666666666666666, 18: 0.34782608695652173, 19: 0.6415094339622641, 24: 0.46153846153846156, 26: 0.9130434782608695, 27: 0.0, 29: 0.8900523560209425, 31: 0.23529411764705882}
Micro-average F1 score: 0.6736068585425597
Weighted-average F1 score: 0.6710131086447725
F1 score per class: {32: 0.9252336448598131, 5: 0.535031847133758, 6: 0.04938271604938271, 7: 0.9803921568627451, 40: 0.6878980891719745, 10: 0.8076923076923077, 9: 0.0, 16: 0.8787878787878788, 17: 0.6634615384615384, 18: 0.45161290322580644, 19: 0.75, 24: 0.45161290322580644, 26: 0.9197860962566845, 27: 1.0, 29: 0.8923076923076924, 31: 0.432}
Micro-average F1 score: 0.7144495412844036
Weighted-average F1 score: 0.6946998549955202
F1 score per class: {32: 0.9339622641509434, 5: 0.5316455696202531, 6: 0.05128205128205128, 7: 0.9803921568627451, 40: 0.7044025157232704, 10: 0.8076923076923077, 9: 0.0, 16: 0.8253968253968254, 17: 0.6634615384615384, 18: 0.3448275862068966, 19: 0.75, 24: 0.45161290322580644, 26: 0.9197860962566845, 27: 1.0, 29: 0.8923076923076924, 31: 0.4409448818897638}
Micro-average F1 score: 0.714203565267395
Weighted-average F1 score: 0.695384185295433
cur_acc:  ['0.8254', '0.7257', '0.4038']
his_acc:  ['0.8254', '0.7852', '0.6736']
cur_acc des:  ['0.8606', '0.8153', '0.5625']
his_acc des:  ['0.8606', '0.8288', '0.7144']
cur_acc rrf:  ['0.8614', '0.8185', '0.5714']
his_acc rrf:  ['0.8614', '0.8302', '0.7142']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 401.0315634CurrentTrain: epoch  0, batch     1 | loss: 368.5311978CurrentTrain: epoch  0, batch     2 | loss: 339.0993599CurrentTrain: epoch  0, batch     3 | loss: 279.9744954CurrentTrain: epoch  1, batch     0 | loss: 358.0862505CurrentTrain: epoch  1, batch     1 | loss: 363.9670578CurrentTrain: epoch  1, batch     2 | loss: 345.7861097CurrentTrain: epoch  1, batch     3 | loss: 255.9337550CurrentTrain: epoch  2, batch     0 | loss: 296.6966846CurrentTrain: epoch  2, batch     1 | loss: 472.5851674CurrentTrain: epoch  2, batch     2 | loss: 356.0041263CurrentTrain: epoch  2, batch     3 | loss: 259.2206020CurrentTrain: epoch  3, batch     0 | loss: 344.9499550CurrentTrain: epoch  3, batch     1 | loss: 351.5245306CurrentTrain: epoch  3, batch     2 | loss: 295.9848211CurrentTrain: epoch  3, batch     3 | loss: 309.9619795CurrentTrain: epoch  4, batch     0 | loss: 497.9019820CurrentTrain: epoch  4, batch     1 | loss: 281.7825921CurrentTrain: epoch  4, batch     2 | loss: 288.4452573CurrentTrain: epoch  4, batch     3 | loss: 362.3426721CurrentTrain: epoch  5, batch     0 | loss: 344.3403616CurrentTrain: epoch  5, batch     1 | loss: 303.2250289CurrentTrain: epoch  5, batch     2 | loss: 398.7356811CurrentTrain: epoch  5, batch     3 | loss: 278.1977079CurrentTrain: epoch  6, batch     0 | loss: 358.6365293CurrentTrain: epoch  6, batch     1 | loss: 376.2461479CurrentTrain: epoch  6, batch     2 | loss: 317.4653818CurrentTrain: epoch  6, batch     3 | loss: 275.2524626CurrentTrain: epoch  7, batch     0 | loss: 361.1127743CurrentTrain: epoch  7, batch     1 | loss: 300.4748602CurrentTrain: epoch  7, batch     2 | loss: 332.5234641CurrentTrain: epoch  7, batch     3 | loss: 309.8946105CurrentTrain: epoch  8, batch     0 | loss: 330.0574525CurrentTrain: epoch  8, batch     1 | loss: 332.7676584CurrentTrain: epoch  8, batch     2 | loss: 361.9105091CurrentTrain: epoch  8, batch     3 | loss: 278.5532534CurrentTrain: epoch  9, batch     0 | loss: 330.5016967CurrentTrain: epoch  9, batch     1 | loss: 357.2080281CurrentTrain: epoch  9, batch     2 | loss: 313.5833444CurrentTrain: epoch  9, batch     3 | loss: 256.6734907
MemoryTrain:  epoch  0, batch     0 | loss: 1.4330513MemoryTrain:  epoch  1, batch     0 | loss: 1.1989762MemoryTrain:  epoch  2, batch     0 | loss: 0.7733865MemoryTrain:  epoch  3, batch     0 | loss: 0.6649205MemoryTrain:  epoch  4, batch     0 | loss: 0.5196768MemoryTrain:  epoch  5, batch     0 | loss: 0.4177701MemoryTrain:  epoch  6, batch     0 | loss: 0.3409756MemoryTrain:  epoch  7, batch     0 | loss: 0.2576879MemoryTrain:  epoch  8, batch     0 | loss: 0.2232839MemoryTrain:  epoch  9, batch     0 | loss: 0.1961071

F1 score per class: {0: 0.9428571428571428, 32: 0.9361702127659575, 4: 0.3333333333333333, 13: 0.41025641025641024, 21: 0.8536585365853658, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.8483290488431876
Weighted-average F1 score: 0.8697674428021441
F1 score per class: {0: 0.9722222222222222, 32: 0.9847715736040609, 4: 0.0, 5: 0.0, 40: 0.5714285714285714, 9: 0.0, 13: 0.7346938775510204, 18: 0.8536585365853658, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8758782201405152
Weighted-average F1 score: 0.8395942808533091
F1 score per class: {0: 0.9722222222222222, 32: 0.9949748743718593, 4: 0.0, 5: 0.0, 40: 0.5714285714285714, 9: 0.7346938775510204, 13: 0.810126582278481, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.8752941176470588
Weighted-average F1 score: 0.8430326239623354

F1 score per class: {0: 0.9428571428571428, 4: 0.9361702127659575, 5: 0.9637305699481865, 6: 0.4748201438848921, 7: 0.028169014084507043, 9: 0.96, 10: 0.3870967741935484, 13: 0.05, 16: 0.8, 17: 0.0, 18: 0.35555555555555557, 19: 0.5803108808290155, 21: 0.36363636363636365, 23: 0.8333333333333334, 24: 0.10526315789473684, 26: 0.6459627329192547, 27: 0.45454545454545453, 29: 0.9081081081081082, 31: 0.0, 32: 0.8089887640449438, 40: 0.2857142857142857}
Micro-average F1 score: 0.6663249615581753
Weighted-average F1 score: 0.6736540380312946
F1 score per class: {0: 0.9459459459459459, 4: 0.9748743718592965, 5: 0.9565217391304348, 6: 0.6012269938650306, 7: 0.08823529411764706, 9: 0.9615384615384616, 10: 0.6068965517241379, 13: 0.06779661016949153, 16: 0.9090909090909091, 17: 0.0, 18: 0.8484848484848485, 19: 0.6461538461538462, 21: 0.631578947368421, 23: 0.8536585365853658, 24: 0.1, 26: 0.6810810810810811, 27: 0.5625, 29: 0.9375, 31: 0.8, 32: 0.8969072164948454, 40: 0.484375}
Micro-average F1 score: 0.7367458866544789
Weighted-average F1 score: 0.716118957729337
F1 score per class: {0: 0.9459459459459459, 4: 0.9801980198019802, 5: 0.9611650485436893, 6: 0.6097560975609756, 7: 0.08823529411764706, 9: 0.9615384615384616, 10: 0.6068965517241379, 13: 0.06779661016949153, 16: 0.8888888888888888, 17: 0.0, 18: 0.7457627118644068, 19: 0.6666666666666666, 21: 0.631578947368421, 23: 0.8, 24: 0.1, 26: 0.6847826086956522, 27: 0.5714285714285714, 29: 0.9375, 31: 0.5, 32: 0.8969072164948454, 40: 0.5245901639344263}
Micro-average F1 score: 0.7379087977890373
Weighted-average F1 score: 0.7185935595202171
cur_acc:  ['0.8254', '0.7257', '0.4038', '0.8483']
his_acc:  ['0.8254', '0.7852', '0.6736', '0.6663']
cur_acc des:  ['0.8606', '0.8153', '0.5625', '0.8759']
his_acc des:  ['0.8606', '0.8288', '0.7144', '0.7367']
cur_acc rrf:  ['0.8614', '0.8185', '0.5714', '0.8753']
his_acc rrf:  ['0.8614', '0.8302', '0.7142', '0.7379']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 331.3840863CurrentTrain: epoch  0, batch     1 | loss: 356.3796749CurrentTrain: epoch  0, batch     2 | loss: 328.6515586CurrentTrain: epoch  0, batch     3 | loss: 269.6746183CurrentTrain: epoch  1, batch     0 | loss: 288.8479845CurrentTrain: epoch  1, batch     1 | loss: 355.3492730CurrentTrain: epoch  1, batch     2 | loss: 354.7899524CurrentTrain: epoch  1, batch     3 | loss: 329.5172767CurrentTrain: epoch  2, batch     0 | loss: 278.5914266CurrentTrain: epoch  2, batch     1 | loss: 399.2293705CurrentTrain: epoch  2, batch     2 | loss: 364.6216697CurrentTrain: epoch  2, batch     3 | loss: 192.7431572CurrentTrain: epoch  3, batch     0 | loss: 288.9367617CurrentTrain: epoch  3, batch     1 | loss: 361.1483580CurrentTrain: epoch  3, batch     2 | loss: 320.3750784CurrentTrain: epoch  3, batch     3 | loss: 230.9533348CurrentTrain: epoch  4, batch     0 | loss: 393.8523531CurrentTrain: epoch  4, batch     1 | loss: 345.3724322CurrentTrain: epoch  4, batch     2 | loss: 278.5765314CurrentTrain: epoch  4, batch     3 | loss: 198.8644022CurrentTrain: epoch  5, batch     0 | loss: 392.6510163CurrentTrain: epoch  5, batch     1 | loss: 300.3575572CurrentTrain: epoch  5, batch     2 | loss: 327.6779089CurrentTrain: epoch  5, batch     3 | loss: 209.7663971CurrentTrain: epoch  6, batch     0 | loss: 298.2886564CurrentTrain: epoch  6, batch     1 | loss: 298.7111801CurrentTrain: epoch  6, batch     2 | loss: 455.9507534CurrentTrain: epoch  6, batch     3 | loss: 199.3691364CurrentTrain: epoch  7, batch     0 | loss: 310.5605166CurrentTrain: epoch  7, batch     1 | loss: 285.5890328CurrentTrain: epoch  7, batch     2 | loss: 373.5929758CurrentTrain: epoch  7, batch     3 | loss: 238.4378035CurrentTrain: epoch  8, batch     0 | loss: 357.0869032CurrentTrain: epoch  8, batch     1 | loss: 325.3622363CurrentTrain: epoch  8, batch     2 | loss: 325.7556416CurrentTrain: epoch  8, batch     3 | loss: 208.8176313CurrentTrain: epoch  9, batch     0 | loss: 340.6374720CurrentTrain: epoch  9, batch     1 | loss: 342.0024763CurrentTrain: epoch  9, batch     2 | loss: 341.4040810CurrentTrain: epoch  9, batch     3 | loss: 208.6199979
MemoryTrain:  epoch  0, batch     0 | loss: 0.7875503MemoryTrain:  epoch  1, batch     0 | loss: 0.6699665MemoryTrain:  epoch  2, batch     0 | loss: 0.5877684MemoryTrain:  epoch  3, batch     0 | loss: 0.3890095MemoryTrain:  epoch  4, batch     0 | loss: 0.2909079MemoryTrain:  epoch  5, batch     0 | loss: 0.2375028MemoryTrain:  epoch  6, batch     0 | loss: 0.1816452MemoryTrain:  epoch  7, batch     0 | loss: 0.1748324MemoryTrain:  epoch  8, batch     0 | loss: 0.1367093MemoryTrain:  epoch  9, batch     0 | loss: 0.1246981

F1 score per class: {33: 0.0, 36: 0.1590909090909091, 5: 0.0, 8: 0.0, 10: 0.0, 13: 0.9090909090909091, 18: 0.0, 20: 0.0, 26: 0.8823529411764706, 29: 0.42857142857142855, 30: 0.625}
Micro-average F1 score: 0.5865102639296188
Weighted-average F1 score: 0.6844885061462601
F1 score per class: {33: 0.0, 36: 0.7244094488188977, 5: 0.0, 8: 0.0, 10: 0.0, 13: 0.9090909090909091, 18: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.972972972972973, 29: 0.5333333333333333, 30: 0.953125}
Micro-average F1 score: 0.8188235294117647
Weighted-average F1 score: 0.7885181567093147
F1 score per class: {33: 0.0, 36: 0.704, 5: 0.0, 8: 0.0, 10: 0.0, 13: 0.0, 18: 0.92, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.972972972972973, 29: 0.5333333333333333, 30: 0.9448818897637795}
Micro-average F1 score: 0.8210023866348448
Weighted-average F1 score: 0.8009608623533904

F1 score per class: {0: 0.9428571428571428, 4: 0.8764044943820225, 5: 0.9494949494949495, 6: 0.23529411764705882, 7: 0.03636363636363636, 8: 0.14285714285714285, 9: 0.96, 10: 0.20869565217391303, 13: 0.13333333333333333, 16: 0.7755102040816326, 17: 0.0, 18: 0.3829787234042553, 19: 0.5376344086021505, 20: 0.8823529411764706, 21: 0.32558139534883723, 23: 0.8148148148148148, 24: 0.10526315789473684, 26: 0.7283236994219653, 27: 0.5, 29: 0.861878453038674, 30: 0.8571428571428571, 31: 0.6666666666666666, 32: 0.8156424581005587, 33: 0.4, 36: 0.6060606060606061, 40: 0.2988505747126437}
Micro-average F1 score: 0.6367713004484304
Weighted-average F1 score: 0.6913777591304069
F1 score per class: {0: 0.9722222222222222, 4: 0.9473684210526315, 5: 0.9041095890410958, 6: 0.5660377358490566, 7: 0.08163265306122448, 8: 0.4946236559139785, 9: 0.9803921568627451, 10: 0.352, 13: 0.19047619047619047, 16: 0.8888888888888888, 17: 0.0, 18: 0.7419354838709677, 19: 0.6122448979591837, 20: 0.8737864077669902, 21: 0.6206896551724138, 23: 0.8148148148148148, 24: 0.1, 26: 0.7195767195767195, 27: 0.5454545454545454, 29: 0.9052631578947369, 30: 0.9, 31: 0.8, 32: 0.8795811518324608, 33: 0.47058823529411764, 36: 0.8652482269503546, 40: 0.453781512605042}
Micro-average F1 score: 0.7201550387596899
Weighted-average F1 score: 0.7263906857666477
F1 score per class: {0: 0.9722222222222222, 4: 0.9528795811518325, 5: 0.9383886255924171, 6: 0.5641025641025641, 7: 0.08, 8: 0.5028571428571429, 9: 0.9803921568627451, 10: 0.390625, 13: 0.16, 16: 0.8888888888888888, 17: 0.0, 18: 0.7333333333333333, 19: 0.6122448979591837, 20: 0.8679245283018868, 21: 0.6206896551724138, 23: 0.810126582278481, 24: 0.1, 26: 0.7195767195767195, 27: 0.5333333333333333, 29: 0.91005291005291, 30: 0.8780487804878049, 31: 0.8, 32: 0.8795811518324608, 33: 0.42105263157894735, 36: 0.8633093525179856, 40: 0.47863247863247865}
Micro-average F1 score: 0.7247169074580242
Weighted-average F1 score: 0.7293697919901897
cur_acc:  ['0.8254', '0.7257', '0.4038', '0.8483', '0.5865']
his_acc:  ['0.8254', '0.7852', '0.6736', '0.6663', '0.6368']
cur_acc des:  ['0.8606', '0.8153', '0.5625', '0.8759', '0.8188']
his_acc des:  ['0.8606', '0.8288', '0.7144', '0.7367', '0.7202']
cur_acc rrf:  ['0.8614', '0.8185', '0.5714', '0.8753', '0.8210']
his_acc rrf:  ['0.8614', '0.8302', '0.7142', '0.7379', '0.7247']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 371.0809897CurrentTrain: epoch  0, batch     1 | loss: 393.7518516CurrentTrain: epoch  0, batch     2 | loss: 339.3778421CurrentTrain: epoch  0, batch     3 | loss: 347.9237870CurrentTrain: epoch  0, batch     4 | loss: 120.5899818CurrentTrain: epoch  1, batch     0 | loss: 342.4674375CurrentTrain: epoch  1, batch     1 | loss: 451.0702651CurrentTrain: epoch  1, batch     2 | loss: 332.5209013CurrentTrain: epoch  1, batch     3 | loss: 327.6483480CurrentTrain: epoch  1, batch     4 | loss: 76.3587568CurrentTrain: epoch  2, batch     0 | loss: 311.6192635CurrentTrain: epoch  2, batch     1 | loss: 367.4128979CurrentTrain: epoch  2, batch     2 | loss: 462.6887586CurrentTrain: epoch  2, batch     3 | loss: 351.1892920CurrentTrain: epoch  2, batch     4 | loss: 79.1455533CurrentTrain: epoch  3, batch     0 | loss: 379.5790761CurrentTrain: epoch  3, batch     1 | loss: 458.9921514CurrentTrain: epoch  3, batch     2 | loss: 366.3781504CurrentTrain: epoch  3, batch     3 | loss: 317.4039055CurrentTrain: epoch  3, batch     4 | loss: 57.7713078CurrentTrain: epoch  4, batch     0 | loss: 394.7220932CurrentTrain: epoch  4, batch     1 | loss: 345.6800182CurrentTrain: epoch  4, batch     2 | loss: 332.9632788CurrentTrain: epoch  4, batch     3 | loss: 336.4761375CurrentTrain: epoch  4, batch     4 | loss: 119.9537625CurrentTrain: epoch  5, batch     0 | loss: 344.4818918CurrentTrain: epoch  5, batch     1 | loss: 290.6833666CurrentTrain: epoch  5, batch     2 | loss: 358.9690820CurrentTrain: epoch  5, batch     3 | loss: 395.6987368CurrentTrain: epoch  5, batch     4 | loss: 76.2243223CurrentTrain: epoch  6, batch     0 | loss: 332.5378269CurrentTrain: epoch  6, batch     1 | loss: 376.7488224CurrentTrain: epoch  6, batch     2 | loss: 287.9509265CurrentTrain: epoch  6, batch     3 | loss: 373.5446554CurrentTrain: epoch  6, batch     4 | loss: 76.5541143CurrentTrain: epoch  7, batch     0 | loss: 376.4754795CurrentTrain: epoch  7, batch     1 | loss: 455.6985280CurrentTrain: epoch  7, batch     2 | loss: 270.8848282CurrentTrain: epoch  7, batch     3 | loss: 359.4625236CurrentTrain: epoch  7, batch     4 | loss: 75.4807517CurrentTrain: epoch  8, batch     0 | loss: 299.8822802CurrentTrain: epoch  8, batch     1 | loss: 373.6404683CurrentTrain: epoch  8, batch     2 | loss: 297.6674893CurrentTrain: epoch  8, batch     3 | loss: 537.8135283CurrentTrain: epoch  8, batch     4 | loss: 65.5859525CurrentTrain: epoch  9, batch     0 | loss: 374.0528418CurrentTrain: epoch  9, batch     1 | loss: 374.0756031CurrentTrain: epoch  9, batch     2 | loss: 357.0646005CurrentTrain: epoch  9, batch     3 | loss: 329.3903659CurrentTrain: epoch  9, batch     4 | loss: 37.5086744
MemoryTrain:  epoch  0, batch     0 | loss: 1.5216415MemoryTrain:  epoch  1, batch     0 | loss: 1.0829093MemoryTrain:  epoch  2, batch     0 | loss: 0.8117548MemoryTrain:  epoch  3, batch     0 | loss: 0.6730648MemoryTrain:  epoch  4, batch     0 | loss: 0.5540739MemoryTrain:  epoch  5, batch     0 | loss: 0.4203851MemoryTrain:  epoch  6, batch     0 | loss: 0.3974243MemoryTrain:  epoch  7, batch     0 | loss: 0.3055538MemoryTrain:  epoch  8, batch     0 | loss: 0.2966907MemoryTrain:  epoch  9, batch     0 | loss: 0.2423979

F1 score per class: {0: 0.0, 33: 0.875, 2: 0.0, 6: 0.6865671641791045, 39: 0.4580152671755725, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.2222222222222222, 19: 0.0, 28: 0.35294117647058826}
Micro-average F1 score: 0.5420560747663551
Weighted-average F1 score: 0.5214373390815249
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.8266666666666667, 12: 0.7261146496815286, 13: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.4, 33: 0.0, 36: 0.0, 39: 0.4444444444444444, 40: 0.0}
Micro-average F1 score: 0.6616541353383458
Weighted-average F1 score: 0.5630258119053235
F1 score per class: {0: 0.0, 2: 0.875, 5: 0.0, 6: 0.0, 8: 0.0, 11: 0.8496732026143791, 12: 0.7375, 13: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.5454545454545454, 33: 0.0, 36: 0.0, 39: 0.4444444444444444, 40: 0.0}
Micro-average F1 score: 0.6952141057934509
Weighted-average F1 score: 0.6124601100228991

F1 score per class: {0: 0.9444444444444444, 2: 0.6363636363636364, 4: 0.8439306358381503, 5: 0.9538461538461539, 6: 0.2786885245901639, 7: 0.02857142857142857, 8: 0.23076923076923078, 9: 0.96, 10: 0.19298245614035087, 11: 0.467005076142132, 12: 0.3592814371257485, 13: 0.17391304347826086, 16: 0.5116279069767442, 17: 0.0, 18: 0.0, 19: 0.592964824120603, 20: 0.8, 21: 0.32558139534883723, 23: 0.8837209302325582, 24: 0.10526315789473684, 26: 0.6746987951807228, 27: 0.36363636363636365, 28: 0.1, 29: 0.8743169398907104, 30: 0.8823529411764706, 31: 0.0, 32: 0.8677248677248677, 33: 0.375, 36: 0.16666666666666666, 39: 0.1935483870967742, 40: 0.3218390804597701}
Micro-average F1 score: 0.5765901392547986
Weighted-average F1 score: 0.6187585246534525
F1 score per class: {0: 0.972972972972973, 2: 0.358974358974359, 4: 0.8439306358381503, 5: 0.9295774647887324, 6: 0.6257668711656442, 7: 0.03508771929824561, 8: 0.5245901639344263, 9: 0.9803921568627451, 10: 0.3140495867768595, 11: 0.5904761904761905, 12: 0.5560975609756098, 13: 0.16, 16: 0.46153846153846156, 17: 0.0, 18: 0.08888888888888889, 19: 0.6602870813397129, 20: 0.9, 21: 0.625, 23: 0.945054945054945, 24: 0.10526315789473684, 26: 0.7303370786516854, 27: 0.48484848484848486, 28: 0.14285714285714285, 29: 0.8924731182795699, 30: 0.9230769230769231, 31: 1.0, 32: 0.9081632653061225, 33: 0.3333333333333333, 36: 0.75, 39: 0.26666666666666666, 40: 0.49206349206349204}
Micro-average F1 score: 0.6684210526315789
Weighted-average F1 score: 0.6682726356375577
F1 score per class: {0: 0.972972972972973, 2: 0.4827586206896552, 4: 0.8764044943820225, 5: 0.9473684210526315, 6: 0.5822784810126582, 7: 0.03508771929824561, 8: 0.5066666666666667, 9: 0.9803921568627451, 10: 0.40310077519379844, 11: 0.5777777777777777, 12: 0.5488372093023256, 13: 0.11428571428571428, 16: 0.46153846153846156, 17: 0.0, 18: 0.09523809523809523, 19: 0.6571428571428571, 20: 0.8823529411764706, 21: 0.6349206349206349, 23: 0.945054945054945, 24: 0.10526315789473684, 26: 0.7303370786516854, 27: 0.5333333333333333, 28: 0.1935483870967742, 29: 0.8924731182795699, 30: 0.9, 31: 0.6666666666666666, 32: 0.9090909090909091, 33: 0.35294117647058826, 36: 0.62, 39: 0.25, 40: 0.49572649572649574}
Micro-average F1 score: 0.6668876367252238
Weighted-average F1 score: 0.666359722559008
cur_acc:  ['0.8254', '0.7257', '0.4038', '0.8483', '0.5865', '0.5421']
his_acc:  ['0.8254', '0.7852', '0.6736', '0.6663', '0.6368', '0.5766']
cur_acc des:  ['0.8606', '0.8153', '0.5625', '0.8759', '0.8188', '0.6617']
his_acc des:  ['0.8606', '0.8288', '0.7144', '0.7367', '0.7202', '0.6684']
cur_acc rrf:  ['0.8614', '0.8185', '0.5714', '0.8753', '0.8210', '0.6952']
his_acc rrf:  ['0.8614', '0.8302', '0.7142', '0.7379', '0.7247', '0.6669']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 324.4713546CurrentTrain: epoch  0, batch     1 | loss: 414.1179827CurrentTrain: epoch  0, batch     2 | loss: 450.5079085CurrentTrain: epoch  0, batch     3 | loss: 395.9392311CurrentTrain: epoch  0, batch     4 | loss: 227.4623333CurrentTrain: epoch  1, batch     0 | loss: 404.8251879CurrentTrain: epoch  1, batch     1 | loss: 402.4352840CurrentTrain: epoch  1, batch     2 | loss: 348.8913467CurrentTrain: epoch  1, batch     3 | loss: 300.6398491CurrentTrain: epoch  1, batch     4 | loss: 290.8770087CurrentTrain: epoch  2, batch     0 | loss: 347.5649889CurrentTrain: epoch  2, batch     1 | loss: 407.8737301CurrentTrain: epoch  2, batch     2 | loss: 415.2608591CurrentTrain: epoch  2, batch     3 | loss: 318.3264853CurrentTrain: epoch  2, batch     4 | loss: 200.4951862CurrentTrain: epoch  3, batch     0 | loss: 347.3896898CurrentTrain: epoch  3, batch     1 | loss: 360.5427755CurrentTrain: epoch  3, batch     2 | loss: 330.8745405CurrentTrain: epoch  3, batch     3 | loss: 343.7623767CurrentTrain: epoch  3, batch     4 | loss: 302.7589865CurrentTrain: epoch  4, batch     0 | loss: 374.9615088CurrentTrain: epoch  4, batch     1 | loss: 359.5101392CurrentTrain: epoch  4, batch     2 | loss: 275.5315951CurrentTrain: epoch  4, batch     3 | loss: 392.8678404CurrentTrain: epoch  4, batch     4 | loss: 249.6028985CurrentTrain: epoch  5, batch     0 | loss: 455.9461347CurrentTrain: epoch  5, batch     1 | loss: 286.6520555CurrentTrain: epoch  5, batch     2 | loss: 377.2979575CurrentTrain: epoch  5, batch     3 | loss: 342.1763377CurrentTrain: epoch  5, batch     4 | loss: 248.6030151CurrentTrain: epoch  6, batch     0 | loss: 390.7141197CurrentTrain: epoch  6, batch     1 | loss: 329.5980377CurrentTrain: epoch  6, batch     2 | loss: 301.4249725CurrentTrain: epoch  6, batch     3 | loss: 407.9174256CurrentTrain: epoch  6, batch     4 | loss: 216.1924227CurrentTrain: epoch  7, batch     0 | loss: 515.7981221CurrentTrain: epoch  7, batch     1 | loss: 315.3576064CurrentTrain: epoch  7, batch     2 | loss: 309.9788825CurrentTrain: epoch  7, batch     3 | loss: 391.2132103CurrentTrain: epoch  7, batch     4 | loss: 201.4257957CurrentTrain: epoch  8, batch     0 | loss: 356.9805722CurrentTrain: epoch  8, batch     1 | loss: 246.2157460CurrentTrain: epoch  8, batch     2 | loss: 419.3685902CurrentTrain: epoch  8, batch     3 | loss: 436.9940298CurrentTrain: epoch  8, batch     4 | loss: 301.2332641CurrentTrain: epoch  9, batch     0 | loss: 390.8748563CurrentTrain: epoch  9, batch     1 | loss: 401.8705878CurrentTrain: epoch  9, batch     2 | loss: 310.4236208CurrentTrain: epoch  9, batch     3 | loss: 437.0781994CurrentTrain: epoch  9, batch     4 | loss: 187.8831744
MemoryTrain:  epoch  0, batch     0 | loss: 1.7773791MemoryTrain:  epoch  1, batch     0 | loss: 1.2270741MemoryTrain:  epoch  2, batch     0 | loss: 1.0369533MemoryTrain:  epoch  3, batch     0 | loss: 0.8794596MemoryTrain:  epoch  4, batch     0 | loss: 0.7301652MemoryTrain:  epoch  5, batch     0 | loss: 0.6288998MemoryTrain:  epoch  6, batch     0 | loss: 0.5102402MemoryTrain:  epoch  7, batch     0 | loss: 0.4195634MemoryTrain:  epoch  8, batch     0 | loss: 0.3375443MemoryTrain:  epoch  9, batch     0 | loss: 0.3013545

F1 score per class: {32: 0.3492063492063492, 1: 0.6557377049180327, 34: 0.0, 3: 0.08450704225352113, 11: 0.0, 14: 0.0, 18: 0.7431693989071039, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 27: 0.19672131147540983}
Micro-average F1 score: 0.4656616415410385
Weighted-average F1 score: 0.46648968578766026
F1 score per class: {32: 0.38461538461538464, 1: 0.8918918918918919, 34: 0.0, 3: 0.0, 36: 0.0, 10: 0.07407407407407407, 11: 0.0, 12: 0.0, 14: 0.7272727272727273, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.6419753086419753, 27: 0.0}
Micro-average F1 score: 0.5644171779141104
Weighted-average F1 score: 0.5349944357495597
F1 score per class: {32: 0.38461538461538464, 1: 0.8918918918918919, 34: 0.0, 3: 0.0, 36: 0.12048192771084337, 10: 0.0, 11: 0.0, 14: 0.7528089887640449, 18: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.38235294117647056, 27: 0.0}
Micro-average F1 score: 0.5415384615384615
Weighted-average F1 score: 0.5151843575980628

F1 score per class: {0: 0.8656716417910447, 1: 0.31654676258992803, 2: 0.5217391304347826, 3: 0.6060606060606061, 4: 0.8095238095238095, 5: 0.9583333333333334, 6: 0.2564102564102564, 7: 0.03333333333333333, 8: 0.1875, 9: 0.9803921568627451, 10: 0.2542372881355932, 11: 0.37815126050420167, 12: 0.2153846153846154, 13: 0.06060606060606061, 14: 0.08108108108108109, 16: 0.4878048780487805, 17: 0.0, 18: 0.05128205128205128, 19: 0.3333333333333333, 20: 0.6153846153846154, 21: 0.06060606060606061, 22: 0.6834170854271356, 23: 0.8735632183908046, 24: 0.0, 26: 0.7167630057803468, 27: 0.125, 28: 0.0, 29: 0.8666666666666667, 30: 0.8571428571428571, 31: 0.0, 32: 0.7840909090909091, 33: 0.35294117647058826, 34: 0.13793103448275862, 36: 0.058823529411764705, 39: 0.16666666666666666, 40: 0.1095890410958904}
Micro-average F1 score: 0.5022194039315155
Weighted-average F1 score: 0.5520162876848902
F1 score per class: {0: 0.9295774647887324, 1: 0.3246753246753247, 2: 0.3333333333333333, 3: 0.8627450980392157, 4: 0.8095238095238095, 5: 0.9339622641509434, 6: 0.5897435897435898, 7: 0.03773584905660377, 8: 0.4942528735632184, 9: 0.9433962264150944, 10: 0.2975206611570248, 11: 0.47342995169082125, 12: 0.4731182795698925, 13: 0.07142857142857142, 14: 0.07228915662650602, 16: 0.46153846153846156, 17: 0.0, 18: 0.1276595744680851, 19: 0.5263157894736842, 20: 0.8333333333333334, 21: 0.30434782608695654, 22: 0.6772486772486772, 23: 0.8863636363636364, 24: 0.08333333333333333, 26: 0.7243243243243244, 27: 0.0, 28: 0.11428571428571428, 29: 0.861878453038674, 30: 0.9, 31: 0.6666666666666666, 32: 0.8586387434554974, 33: 0.3333333333333333, 34: 0.4642857142857143, 36: 0.7339449541284404, 39: 0.25, 40: 0.6231884057971014}
Micro-average F1 score: 0.6060606060606061
Weighted-average F1 score: 0.6038995798220369
F1 score per class: {0: 0.9295774647887324, 1: 0.32679738562091504, 2: 0.4375, 3: 0.8198757763975155, 4: 0.8636363636363636, 5: 0.9509803921568627, 6: 0.56, 7: 0.03636363636363636, 8: 0.5039370078740157, 9: 0.9615384615384616, 10: 0.3387096774193548, 11: 0.43902439024390244, 12: 0.4419889502762431, 13: 0.08163265306122448, 14: 0.11235955056179775, 16: 0.46153846153846156, 17: 0.0, 18: 0.13333333333333333, 19: 0.5348837209302325, 20: 0.8571428571428571, 21: 0.16216216216216217, 22: 0.6907216494845361, 23: 0.8863636363636364, 24: 0.08333333333333333, 26: 0.7243243243243244, 27: 0.0, 28: 0.11428571428571428, 29: 0.8666666666666667, 30: 0.972972972972973, 31: 0.0, 32: 0.8342245989304813, 33: 0.3157894736842105, 34: 0.2736842105263158, 36: 0.40476190476190477, 39: 0.2222222222222222, 40: 0.5714285714285714}
Micro-average F1 score: 0.5893611032929919
Weighted-average F1 score: 0.5892005736232073
cur_acc:  ['0.8254', '0.7257', '0.4038', '0.8483', '0.5865', '0.5421', '0.4657']
his_acc:  ['0.8254', '0.7852', '0.6736', '0.6663', '0.6368', '0.5766', '0.5022']
cur_acc des:  ['0.8606', '0.8153', '0.5625', '0.8759', '0.8188', '0.6617', '0.5644']
his_acc des:  ['0.8606', '0.8288', '0.7144', '0.7367', '0.7202', '0.6684', '0.6061']
cur_acc rrf:  ['0.8614', '0.8185', '0.5714', '0.8753', '0.8210', '0.6952', '0.5415']
his_acc rrf:  ['0.8614', '0.8302', '0.7142', '0.7379', '0.7247', '0.6669', '0.5894']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'NA or unknown', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 337.2121518CurrentTrain: epoch  0, batch     1 | loss: 301.3935290CurrentTrain: epoch  0, batch     2 | loss: 382.5751380CurrentTrain: epoch  0, batch     3 | loss: 256.9339039CurrentTrain: epoch  1, batch     0 | loss: 326.2717204CurrentTrain: epoch  1, batch     1 | loss: 312.1903240CurrentTrain: epoch  1, batch     2 | loss: 395.5656210CurrentTrain: epoch  1, batch     3 | loss: 232.4983242CurrentTrain: epoch  2, batch     0 | loss: 415.2604250CurrentTrain: epoch  2, batch     1 | loss: 302.7700390CurrentTrain: epoch  2, batch     2 | loss: 295.0214982CurrentTrain: epoch  2, batch     3 | loss: 280.1411794CurrentTrain: epoch  3, batch     0 | loss: 304.2613138CurrentTrain: epoch  3, batch     1 | loss: 361.6166362CurrentTrain: epoch  3, batch     2 | loss: 345.2906441CurrentTrain: epoch  3, batch     3 | loss: 243.4334958CurrentTrain: epoch  4, batch     0 | loss: 397.5187702CurrentTrain: epoch  4, batch     1 | loss: 332.8308538CurrentTrain: epoch  4, batch     2 | loss: 245.6119920CurrentTrain: epoch  4, batch     3 | loss: 272.1156786CurrentTrain: epoch  5, batch     0 | loss: 271.4033689CurrentTrain: epoch  5, batch     1 | loss: 343.2274825CurrentTrain: epoch  5, batch     2 | loss: 341.6395008CurrentTrain: epoch  5, batch     3 | loss: 309.6403420CurrentTrain: epoch  6, batch     0 | loss: 341.6675113CurrentTrain: epoch  6, batch     1 | loss: 325.3427865CurrentTrain: epoch  6, batch     2 | loss: 274.8159666CurrentTrain: epoch  6, batch     3 | loss: 349.0277146CurrentTrain: epoch  7, batch     0 | loss: 328.9707165CurrentTrain: epoch  7, batch     1 | loss: 325.2749049CurrentTrain: epoch  7, batch     2 | loss: 298.0642428CurrentTrain: epoch  7, batch     3 | loss: 269.6742202CurrentTrain: epoch  8, batch     0 | loss: 257.0042437CurrentTrain: epoch  8, batch     1 | loss: 455.3146469CurrentTrain: epoch  8, batch     2 | loss: 340.4580251CurrentTrain: epoch  8, batch     3 | loss: 239.7228688CurrentTrain: epoch  9, batch     0 | loss: 297.8462388CurrentTrain: epoch  9, batch     1 | loss: 437.2294950CurrentTrain: epoch  9, batch     2 | loss: 298.3816579CurrentTrain: epoch  9, batch     3 | loss: 239.5056021
MemoryTrain:  epoch  0, batch     0 | loss: 1.4658068MemoryTrain:  epoch  1, batch     0 | loss: 1.2442766MemoryTrain:  epoch  2, batch     0 | loss: 0.8795206MemoryTrain:  epoch  3, batch     0 | loss: 0.6848784MemoryTrain:  epoch  4, batch     0 | loss: 0.6171786MemoryTrain:  epoch  5, batch     0 | loss: 0.4549330MemoryTrain:  epoch  6, batch     0 | loss: 0.3796144MemoryTrain:  epoch  7, batch     0 | loss: 0.3459241MemoryTrain:  epoch  8, batch     0 | loss: 0.2654524MemoryTrain:  epoch  9, batch     0 | loss: 0.2451047

F1 score per class: {33: 0.0, 34: 0.0, 35: 0.0, 3: 0.0, 5: 0.0, 37: 0.0, 38: 0.75, 8: 0.0, 10: 0.0, 11: 0.4927536231884058, 39: 0.0, 13: 0.0, 15: 0.64, 20: 0.6363636363636364, 23: 0.5263157894736842, 25: 0.0}
Micro-average F1 score: 0.4709141274238227
Weighted-average F1 score: 0.3207384456695097
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 13: 0.0, 14: 0.0, 15: 0.75, 18: 0.0, 20: 0.0, 22: 0.0, 23: 0.0, 25: 0.5945945945945946, 33: 0.0, 34: 0.0, 35: 0.7560975609756098, 36: 0.0, 37: 0.6813186813186813, 38: 0.6341463414634146, 39: 0.0}
Micro-average F1 score: 0.5162907268170426
Weighted-average F1 score: 0.35547922743044696
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 13: 0.0, 15: 0.75, 18: 0.0, 20: 0.0, 22: 0.0, 23: 0.0, 25: 0.5945945945945946, 33: 0.0, 34: 0.0, 35: 0.7710843373493976, 37: 0.7096774193548387, 38: 0.6341463414634146, 39: 0.0}
Micro-average F1 score: 0.5353535353535354
Weighted-average F1 score: 0.37907198540510706

F1 score per class: {0: 0.8823529411764706, 1: 0.30985915492957744, 2: 0.5833333333333334, 3: 0.8279569892473119, 4: 0.8304093567251462, 5: 0.8847926267281107, 6: 0.17699115044247787, 7: 0.03508771929824561, 8: 0.27350427350427353, 9: 0.96, 10: 0.3464566929133858, 11: 0.44954128440366975, 12: 0.2, 13: 0.0975609756097561, 14: 0.0, 15: 0.6, 16: 0.5777777777777777, 17: 0.0, 18: 0.047619047619047616, 19: 0.5027322404371585, 20: 0.78, 21: 0.11764705882352941, 22: 0.7150259067357513, 23: 0.8275862068965517, 24: 0.0, 25: 0.4927536231884058, 26: 0.7134502923976608, 27: 0.125, 28: 0.17391304347826086, 29: 0.8729281767955801, 30: 0.8823529411764706, 31: 0.0, 32: 0.7228915662650602, 33: 0.35294117647058826, 34: 0.0547945205479452, 35: 0.5853658536585366, 36: 0.058823529411764705, 37: 0.3236994219653179, 38: 0.3448275862068966, 39: 0.15384615384615385, 40: 0.2891566265060241}
Micro-average F1 score: 0.5243243243243243
Weighted-average F1 score: 0.5639945480515985
F1 score per class: {0: 0.9459459459459459, 1: 0.3125, 2: 0.3684210526315789, 3: 0.8260869565217391, 4: 0.8165680473372781, 5: 0.8354430379746836, 6: 0.6075949367088608, 7: 0.04, 8: 0.5536723163841808, 9: 0.9433962264150944, 10: 0.4148148148148148, 11: 0.47, 12: 0.5136612021857924, 13: 0.22727272727272727, 14: 0.15841584158415842, 15: 0.5217391304347826, 16: 0.5365853658536586, 17: 0.0, 18: 0.38596491228070173, 19: 0.6728110599078341, 20: 0.8484848484848485, 21: 0.25, 22: 0.7165775401069518, 23: 0.8666666666666667, 24: 0.08695652173913043, 25: 0.5945945945945946, 26: 0.7403314917127072, 27: 0.0, 28: 0.12121212121212122, 29: 0.8864864864864865, 30: 0.9743589743589743, 31: 0.6666666666666666, 32: 0.8167539267015707, 33: 0.38095238095238093, 34: 0.416, 35: 0.6666666666666666, 36: 0.6909090909090909, 37: 0.5081967213114754, 38: 0.4126984126984127, 39: 0.375, 40: 0.5271317829457365}
Micro-average F1 score: 0.6136854741896759
Weighted-average F1 score: 0.6065526029331274
F1 score per class: {0: 0.9459459459459459, 1: 0.33540372670807456, 2: 0.4666666666666667, 3: 0.8167539267015707, 4: 0.8571428571428571, 5: 0.8608695652173913, 6: 0.5526315789473685, 7: 0.04, 8: 0.5490196078431373, 9: 0.9615384615384616, 10: 0.43283582089552236, 11: 0.45, 12: 0.4756756756756757, 13: 0.11764705882352941, 14: 0.13186813186813187, 15: 0.4444444444444444, 16: 0.5365853658536586, 17: 0.0, 18: 0.20408163265306123, 19: 0.6280193236714976, 20: 0.8712871287128713, 21: 0.10810810810810811, 22: 0.7225130890052356, 23: 0.8372093023255814, 24: 0.08695652173913043, 25: 0.5945945945945946, 26: 0.7403314917127072, 27: 0.0, 28: 0.12121212121212122, 29: 0.8913043478260869, 30: 0.9743589743589743, 31: 0.0, 32: 0.8229166666666666, 33: 0.4, 34: 0.2653061224489796, 35: 0.6736842105263158, 36: 0.2857142857142857, 37: 0.4230769230769231, 38: 0.36619718309859156, 39: 0.30303030303030304, 40: 0.5811965811965812}
Micro-average F1 score: 0.5923412506059137
Weighted-average F1 score: 0.591412948561889
cur_acc:  ['0.8254', '0.7257', '0.4038', '0.8483', '0.5865', '0.5421', '0.4657', '0.4709']
his_acc:  ['0.8254', '0.7852', '0.6736', '0.6663', '0.6368', '0.5766', '0.5022', '0.5243']
cur_acc des:  ['0.8606', '0.8153', '0.5625', '0.8759', '0.8188', '0.6617', '0.5644', '0.5163']
his_acc des:  ['0.8606', '0.8288', '0.7144', '0.7367', '0.7202', '0.6684', '0.6061', '0.6137']
cur_acc rrf:  ['0.8614', '0.8185', '0.5714', '0.8753', '0.8210', '0.6952', '0.5415', '0.5354']
his_acc rrf:  ['0.8614', '0.8302', '0.7142', '0.7379', '0.7247', '0.6669', '0.5894', '0.5923']
----------END
his_acc mean:  [0.8075 0.7305 0.6618 0.6279 0.562  0.5522 0.5104 0.5018]
his_acc des mean:  [0.8433 0.7958 0.736  0.7157 0.6616 0.647  0.6116 0.5945]
his_acc rrf mean:  [0.8417 0.7911 0.7264 0.703  0.6466 0.6324 0.5933 0.5803]
