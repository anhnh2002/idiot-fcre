#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 127.4410230CurrentTrain: epoch  0, batch     1 | loss: 91.0480700CurrentTrain: epoch  0, batch     2 | loss: 78.5039682CurrentTrain: epoch  0, batch     3 | loss: 88.2306910CurrentTrain: epoch  0, batch     4 | loss: 87.3597513CurrentTrain: epoch  0, batch     5 | loss: 87.4418091CurrentTrain: epoch  0, batch     6 | loss: 101.8835287CurrentTrain: epoch  0, batch     7 | loss: 100.6150611CurrentTrain: epoch  0, batch     8 | loss: 86.3544628CurrentTrain: epoch  0, batch     9 | loss: 86.4705513CurrentTrain: epoch  0, batch    10 | loss: 77.2608740CurrentTrain: epoch  0, batch    11 | loss: 100.9790722CurrentTrain: epoch  0, batch    12 | loss: 99.3914943CurrentTrain: epoch  0, batch    13 | loss: 193.1445742CurrentTrain: epoch  0, batch    14 | loss: 100.3338364CurrentTrain: epoch  0, batch    15 | loss: 86.2700722CurrentTrain: epoch  0, batch    16 | loss: 86.6955689CurrentTrain: epoch  0, batch    17 | loss: 118.3794258CurrentTrain: epoch  0, batch    18 | loss: 99.9756678CurrentTrain: epoch  0, batch    19 | loss: 86.3320767CurrentTrain: epoch  0, batch    20 | loss: 146.9150817CurrentTrain: epoch  0, batch    21 | loss: 146.1959342CurrentTrain: epoch  0, batch    22 | loss: 193.1894195CurrentTrain: epoch  0, batch    23 | loss: 145.9490238CurrentTrain: epoch  0, batch    24 | loss: 99.6026530CurrentTrain: epoch  0, batch    25 | loss: 192.7196244CurrentTrain: epoch  0, batch    26 | loss: 86.0092776CurrentTrain: epoch  0, batch    27 | loss: 117.8975299CurrentTrain: epoch  0, batch    28 | loss: 145.6390192CurrentTrain: epoch  0, batch    29 | loss: 98.7139077CurrentTrain: epoch  0, batch    30 | loss: 145.6671623CurrentTrain: epoch  0, batch    31 | loss: 145.4222380CurrentTrain: epoch  0, batch    32 | loss: 117.6100639CurrentTrain: epoch  0, batch    33 | loss: 85.5126644CurrentTrain: epoch  0, batch    34 | loss: 146.5690565CurrentTrain: epoch  0, batch    35 | loss: 84.9825595CurrentTrain: epoch  0, batch    36 | loss: 84.7571527CurrentTrain: epoch  0, batch    37 | loss: 98.5992710CurrentTrain: epoch  0, batch    38 | loss: 98.1671784CurrentTrain: epoch  0, batch    39 | loss: 85.1203110CurrentTrain: epoch  0, batch    40 | loss: 117.5720920CurrentTrain: epoch  0, batch    41 | loss: 85.5919577CurrentTrain: epoch  0, batch    42 | loss: 84.6573954CurrentTrain: epoch  0, batch    43 | loss: 74.3712639CurrentTrain: epoch  0, batch    44 | loss: 98.0891050CurrentTrain: epoch  0, batch    45 | loss: 82.7940821CurrentTrain: epoch  0, batch    46 | loss: 115.0650167CurrentTrain: epoch  0, batch    47 | loss: 83.8726932CurrentTrain: epoch  0, batch    48 | loss: 97.4309193CurrentTrain: epoch  0, batch    49 | loss: 97.1130939CurrentTrain: epoch  0, batch    50 | loss: 96.9793131CurrentTrain: epoch  0, batch    51 | loss: 97.8810438CurrentTrain: epoch  0, batch    52 | loss: 96.9075258CurrentTrain: epoch  0, batch    53 | loss: 142.9276334CurrentTrain: epoch  0, batch    54 | loss: 116.0490714CurrentTrain: epoch  0, batch    55 | loss: 82.5294288CurrentTrain: epoch  0, batch    56 | loss: 83.6886005CurrentTrain: epoch  0, batch    57 | loss: 95.5699823CurrentTrain: epoch  0, batch    58 | loss: 94.7212865CurrentTrain: epoch  0, batch    59 | loss: 95.2854462CurrentTrain: epoch  0, batch    60 | loss: 93.6369534CurrentTrain: epoch  0, batch    61 | loss: 95.9668130CurrentTrain: epoch  0, batch    62 | loss: 83.0474007CurrentTrain: epoch  0, batch    63 | loss: 111.9088133CurrentTrain: epoch  0, batch    64 | loss: 95.4962377CurrentTrain: epoch  0, batch    65 | loss: 71.6006318CurrentTrain: epoch  0, batch    66 | loss: 91.0208193CurrentTrain: epoch  0, batch    67 | loss: 94.6509443CurrentTrain: epoch  0, batch    68 | loss: 82.4460175CurrentTrain: epoch  0, batch    69 | loss: 141.7931879CurrentTrain: epoch  0, batch    70 | loss: 117.3976795CurrentTrain: epoch  0, batch    71 | loss: 82.6429037CurrentTrain: epoch  0, batch    72 | loss: 96.9215286CurrentTrain: epoch  0, batch    73 | loss: 112.8935665CurrentTrain: epoch  0, batch    74 | loss: 142.3137850CurrentTrain: epoch  0, batch    75 | loss: 95.0093906CurrentTrain: epoch  0, batch    76 | loss: 110.2806831CurrentTrain: epoch  0, batch    77 | loss: 92.8881405CurrentTrain: epoch  0, batch    78 | loss: 112.6259929CurrentTrain: epoch  0, batch    79 | loss: 111.0747890CurrentTrain: epoch  0, batch    80 | loss: 79.2325549CurrentTrain: epoch  0, batch    81 | loss: 92.2656294CurrentTrain: epoch  0, batch    82 | loss: 111.3827890CurrentTrain: epoch  0, batch    83 | loss: 93.8524872CurrentTrain: epoch  0, batch    84 | loss: 107.6148636CurrentTrain: epoch  0, batch    85 | loss: 79.7293064CurrentTrain: epoch  0, batch    86 | loss: 137.4133676CurrentTrain: epoch  0, batch    87 | loss: 81.6693868CurrentTrain: epoch  0, batch    88 | loss: 80.0372843CurrentTrain: epoch  0, batch    89 | loss: 66.0136528CurrentTrain: epoch  0, batch    90 | loss: 93.0794947CurrentTrain: epoch  0, batch    91 | loss: 80.1684487CurrentTrain: epoch  0, batch    92 | loss: 90.4279159CurrentTrain: epoch  0, batch    93 | loss: 135.2179701CurrentTrain: epoch  0, batch    94 | loss: 80.2209610CurrentTrain: epoch  0, batch    95 | loss: 93.3847682CurrentTrain: epoch  1, batch     0 | loss: 79.2731747CurrentTrain: epoch  1, batch     1 | loss: 93.0305624CurrentTrain: epoch  1, batch     2 | loss: 78.2924860CurrentTrain: epoch  1, batch     3 | loss: 79.3196894CurrentTrain: epoch  1, batch     4 | loss: 88.3923489CurrentTrain: epoch  1, batch     5 | loss: 75.7498863CurrentTrain: epoch  1, batch     6 | loss: 94.6736979CurrentTrain: epoch  1, batch     7 | loss: 137.9167703CurrentTrain: epoch  1, batch     8 | loss: 69.9167885CurrentTrain: epoch  1, batch     9 | loss: 107.1252431CurrentTrain: epoch  1, batch    10 | loss: 90.3771372CurrentTrain: epoch  1, batch    11 | loss: 77.4832794CurrentTrain: epoch  1, batch    12 | loss: 75.7594618CurrentTrain: epoch  1, batch    13 | loss: 109.9826618CurrentTrain: epoch  1, batch    14 | loss: 105.4590849CurrentTrain: epoch  1, batch    15 | loss: 93.1283803CurrentTrain: epoch  1, batch    16 | loss: 135.2773398CurrentTrain: epoch  1, batch    17 | loss: 76.2209645CurrentTrain: epoch  1, batch    18 | loss: 86.6363578CurrentTrain: epoch  1, batch    19 | loss: 110.0455275CurrentTrain: epoch  1, batch    20 | loss: 140.5807215CurrentTrain: epoch  1, batch    21 | loss: 79.2359260CurrentTrain: epoch  1, batch    22 | loss: 75.2222260CurrentTrain: epoch  1, batch    23 | loss: 89.5570312CurrentTrain: epoch  1, batch    24 | loss: 135.0164095CurrentTrain: epoch  1, batch    25 | loss: 91.0068620CurrentTrain: epoch  1, batch    26 | loss: 65.5059333CurrentTrain: epoch  1, batch    27 | loss: 75.5025897CurrentTrain: epoch  1, batch    28 | loss: 65.5450608CurrentTrain: epoch  1, batch    29 | loss: 66.7410771CurrentTrain: epoch  1, batch    30 | loss: 92.2867289CurrentTrain: epoch  1, batch    31 | loss: 89.8156154CurrentTrain: epoch  1, batch    32 | loss: 110.5913209CurrentTrain: epoch  1, batch    33 | loss: 93.6476509CurrentTrain: epoch  1, batch    34 | loss: 102.9301412CurrentTrain: epoch  1, batch    35 | loss: 137.7852367CurrentTrain: epoch  1, batch    36 | loss: 77.8870520CurrentTrain: epoch  1, batch    37 | loss: 88.9539746CurrentTrain: epoch  1, batch    38 | loss: 93.0017128CurrentTrain: epoch  1, batch    39 | loss: 65.6789405CurrentTrain: epoch  1, batch    40 | loss: 72.7117838CurrentTrain: epoch  1, batch    41 | loss: 74.2114984CurrentTrain: epoch  1, batch    42 | loss: 80.7905707CurrentTrain: epoch  1, batch    43 | loss: 89.1067885CurrentTrain: epoch  1, batch    44 | loss: 135.1759597CurrentTrain: epoch  1, batch    45 | loss: 89.8770850CurrentTrain: epoch  1, batch    46 | loss: 109.1201833CurrentTrain: epoch  1, batch    47 | loss: 72.6396539CurrentTrain: epoch  1, batch    48 | loss: 90.9544480CurrentTrain: epoch  1, batch    49 | loss: 91.6906317CurrentTrain: epoch  1, batch    50 | loss: 88.0424145CurrentTrain: epoch  1, batch    51 | loss: 113.0778987CurrentTrain: epoch  1, batch    52 | loss: 111.5028630CurrentTrain: epoch  1, batch    53 | loss: 135.5812074CurrentTrain: epoch  1, batch    54 | loss: 78.8930255CurrentTrain: epoch  1, batch    55 | loss: 90.9670416CurrentTrain: epoch  1, batch    56 | loss: 88.6030334CurrentTrain: epoch  1, batch    57 | loss: 107.3992212CurrentTrain: epoch  1, batch    58 | loss: 91.3898529CurrentTrain: epoch  1, batch    59 | loss: 74.9852433CurrentTrain: epoch  1, batch    60 | loss: 87.2601028CurrentTrain: epoch  1, batch    61 | loss: 133.8989591CurrentTrain: epoch  1, batch    62 | loss: 140.6456160CurrentTrain: epoch  1, batch    63 | loss: 73.6870411CurrentTrain: epoch  1, batch    64 | loss: 88.9374005CurrentTrain: epoch  1, batch    65 | loss: 109.4748078CurrentTrain: epoch  1, batch    66 | loss: 66.5595051CurrentTrain: epoch  1, batch    67 | loss: 136.0609080CurrentTrain: epoch  1, batch    68 | loss: 72.8028531CurrentTrain: epoch  1, batch    69 | loss: 81.0227116CurrentTrain: epoch  1, batch    70 | loss: 72.4734712CurrentTrain: epoch  1, batch    71 | loss: 87.2175823CurrentTrain: epoch  1, batch    72 | loss: 108.0751850CurrentTrain: epoch  1, batch    73 | loss: 100.9566707CurrentTrain: epoch  1, batch    74 | loss: 105.3580134CurrentTrain: epoch  1, batch    75 | loss: 86.4147624CurrentTrain: epoch  1, batch    76 | loss: 92.6486245CurrentTrain: epoch  1, batch    77 | loss: 91.1797280CurrentTrain: epoch  1, batch    78 | loss: 86.0920149CurrentTrain: epoch  1, batch    79 | loss: 89.7242693CurrentTrain: epoch  1, batch    80 | loss: 67.1529600CurrentTrain: epoch  1, batch    81 | loss: 103.4795334CurrentTrain: epoch  1, batch    82 | loss: 75.6835877CurrentTrain: epoch  1, batch    83 | loss: 184.3876974CurrentTrain: epoch  1, batch    84 | loss: 106.1153472CurrentTrain: epoch  1, batch    85 | loss: 72.3428410CurrentTrain: epoch  1, batch    86 | loss: 66.0492672CurrentTrain: epoch  1, batch    87 | loss: 77.2135334CurrentTrain: epoch  1, batch    88 | loss: 109.0449549CurrentTrain: epoch  1, batch    89 | loss: 87.2057957CurrentTrain: epoch  1, batch    90 | loss: 75.9403251CurrentTrain: epoch  1, batch    91 | loss: 88.2388825CurrentTrain: epoch  1, batch    92 | loss: 88.8474997CurrentTrain: epoch  1, batch    93 | loss: 136.8480256CurrentTrain: epoch  1, batch    94 | loss: 92.4356289CurrentTrain: epoch  1, batch    95 | loss: 69.4548275CurrentTrain: epoch  2, batch     0 | loss: 63.4179919CurrentTrain: epoch  2, batch     1 | loss: 105.3412329CurrentTrain: epoch  2, batch     2 | loss: 88.3243163CurrentTrain: epoch  2, batch     3 | loss: 91.0623958CurrentTrain: epoch  2, batch     4 | loss: 137.3153779CurrentTrain: epoch  2, batch     5 | loss: 72.6298831CurrentTrain: epoch  2, batch     6 | loss: 130.1664093CurrentTrain: epoch  2, batch     7 | loss: 83.5210488CurrentTrain: epoch  2, batch     8 | loss: 107.4258483CurrentTrain: epoch  2, batch     9 | loss: 62.5708276CurrentTrain: epoch  2, batch    10 | loss: 101.7330300CurrentTrain: epoch  2, batch    11 | loss: 61.9300909CurrentTrain: epoch  2, batch    12 | loss: 65.8920099CurrentTrain: epoch  2, batch    13 | loss: 103.9706789CurrentTrain: epoch  2, batch    14 | loss: 107.3455627CurrentTrain: epoch  2, batch    15 | loss: 109.2078491CurrentTrain: epoch  2, batch    16 | loss: 82.9385119CurrentTrain: epoch  2, batch    17 | loss: 89.2849051CurrentTrain: epoch  2, batch    18 | loss: 85.0826932CurrentTrain: epoch  2, batch    19 | loss: 104.1198872CurrentTrain: epoch  2, batch    20 | loss: 74.7663970CurrentTrain: epoch  2, batch    21 | loss: 103.3830726CurrentTrain: epoch  2, batch    22 | loss: 75.5686216CurrentTrain: epoch  2, batch    23 | loss: 85.4081050CurrentTrain: epoch  2, batch    24 | loss: 83.1861495CurrentTrain: epoch  2, batch    25 | loss: 84.1530034CurrentTrain: epoch  2, batch    26 | loss: 88.6894512CurrentTrain: epoch  2, batch    27 | loss: 80.8255948CurrentTrain: epoch  2, batch    28 | loss: 106.1210095CurrentTrain: epoch  2, batch    29 | loss: 102.5406903CurrentTrain: epoch  2, batch    30 | loss: 88.8562022CurrentTrain: epoch  2, batch    31 | loss: 85.4906868CurrentTrain: epoch  2, batch    32 | loss: 77.2799897CurrentTrain: epoch  2, batch    33 | loss: 109.4484462CurrentTrain: epoch  2, batch    34 | loss: 106.6219230CurrentTrain: epoch  2, batch    35 | loss: 103.6409236CurrentTrain: epoch  2, batch    36 | loss: 87.7884562CurrentTrain: epoch  2, batch    37 | loss: 102.3755549CurrentTrain: epoch  2, batch    38 | loss: 73.4799853CurrentTrain: epoch  2, batch    39 | loss: 106.1769630CurrentTrain: epoch  2, batch    40 | loss: 61.8865377CurrentTrain: epoch  2, batch    41 | loss: 108.1907035CurrentTrain: epoch  2, batch    42 | loss: 103.3865254CurrentTrain: epoch  2, batch    43 | loss: 75.6592371CurrentTrain: epoch  2, batch    44 | loss: 84.4646694CurrentTrain: epoch  2, batch    45 | loss: 74.6728364CurrentTrain: epoch  2, batch    46 | loss: 89.0068011CurrentTrain: epoch  2, batch    47 | loss: 76.2878937CurrentTrain: epoch  2, batch    48 | loss: 133.6648879CurrentTrain: epoch  2, batch    49 | loss: 134.4517204CurrentTrain: epoch  2, batch    50 | loss: 70.4502929CurrentTrain: epoch  2, batch    51 | loss: 137.5350647CurrentTrain: epoch  2, batch    52 | loss: 73.9044925CurrentTrain: epoch  2, batch    53 | loss: 89.7699511CurrentTrain: epoch  2, batch    54 | loss: 107.3909905CurrentTrain: epoch  2, batch    55 | loss: 86.7622512CurrentTrain: epoch  2, batch    56 | loss: 84.9023217CurrentTrain: epoch  2, batch    57 | loss: 70.3325964CurrentTrain: epoch  2, batch    58 | loss: 76.3343459CurrentTrain: epoch  2, batch    59 | loss: 73.5175022CurrentTrain: epoch  2, batch    60 | loss: 86.2919929CurrentTrain: epoch  2, batch    61 | loss: 69.7017649CurrentTrain: epoch  2, batch    62 | loss: 84.0696701CurrentTrain: epoch  2, batch    63 | loss: 71.4451878CurrentTrain: epoch  2, batch    64 | loss: 70.5003837CurrentTrain: epoch  2, batch    65 | loss: 85.4816817CurrentTrain: epoch  2, batch    66 | loss: 136.1844682CurrentTrain: epoch  2, batch    67 | loss: 106.6324220CurrentTrain: epoch  2, batch    68 | loss: 72.3551325CurrentTrain: epoch  2, batch    69 | loss: 70.4267303CurrentTrain: epoch  2, batch    70 | loss: 89.8599128CurrentTrain: epoch  2, batch    71 | loss: 72.9919474CurrentTrain: epoch  2, batch    72 | loss: 183.4413983CurrentTrain: epoch  2, batch    73 | loss: 86.6636909CurrentTrain: epoch  2, batch    74 | loss: 84.8363688CurrentTrain: epoch  2, batch    75 | loss: 84.5902250CurrentTrain: epoch  2, batch    76 | loss: 103.2658473CurrentTrain: epoch  2, batch    77 | loss: 63.5314926CurrentTrain: epoch  2, batch    78 | loss: 89.7298697CurrentTrain: epoch  2, batch    79 | loss: 84.7977622CurrentTrain: epoch  2, batch    80 | loss: 70.9316282CurrentTrain: epoch  2, batch    81 | loss: 87.9190464CurrentTrain: epoch  2, batch    82 | loss: 107.5059669CurrentTrain: epoch  2, batch    83 | loss: 107.0559783CurrentTrain: epoch  2, batch    84 | loss: 74.9884473CurrentTrain: epoch  2, batch    85 | loss: 135.4145300CurrentTrain: epoch  2, batch    86 | loss: 62.3272418CurrentTrain: epoch  2, batch    87 | loss: 63.3476455CurrentTrain: epoch  2, batch    88 | loss: 65.4904684CurrentTrain: epoch  2, batch    89 | loss: 100.9780323CurrentTrain: epoch  2, batch    90 | loss: 85.8031022CurrentTrain: epoch  2, batch    91 | loss: 75.8521428CurrentTrain: epoch  2, batch    92 | loss: 74.4622912CurrentTrain: epoch  2, batch    93 | loss: 87.9898049CurrentTrain: epoch  2, batch    94 | loss: 86.3269721CurrentTrain: epoch  2, batch    95 | loss: 72.6376446CurrentTrain: epoch  3, batch     0 | loss: 85.4346541CurrentTrain: epoch  3, batch     1 | loss: 102.4034963CurrentTrain: epoch  3, batch     2 | loss: 86.6899570CurrentTrain: epoch  3, batch     3 | loss: 89.0365677CurrentTrain: epoch  3, batch     4 | loss: 82.2054974CurrentTrain: epoch  3, batch     5 | loss: 100.2855408CurrentTrain: epoch  3, batch     6 | loss: 81.8948336CurrentTrain: epoch  3, batch     7 | loss: 62.7673200CurrentTrain: epoch  3, batch     8 | loss: 73.6985395CurrentTrain: epoch  3, batch     9 | loss: 126.1735471CurrentTrain: epoch  3, batch    10 | loss: 75.3936704CurrentTrain: epoch  3, batch    11 | loss: 81.2165392CurrentTrain: epoch  3, batch    12 | loss: 104.1522094CurrentTrain: epoch  3, batch    13 | loss: 104.1047331CurrentTrain: epoch  3, batch    14 | loss: 84.6802132CurrentTrain: epoch  3, batch    15 | loss: 104.5275814CurrentTrain: epoch  3, batch    16 | loss: 100.2351785CurrentTrain: epoch  3, batch    17 | loss: 68.4604342CurrentTrain: epoch  3, batch    18 | loss: 72.6690974CurrentTrain: epoch  3, batch    19 | loss: 74.2109650CurrentTrain: epoch  3, batch    20 | loss: 105.8434408CurrentTrain: epoch  3, batch    21 | loss: 81.2765529CurrentTrain: epoch  3, batch    22 | loss: 106.9010966CurrentTrain: epoch  3, batch    23 | loss: 87.0158982CurrentTrain: epoch  3, batch    24 | loss: 71.6707935CurrentTrain: epoch  3, batch    25 | loss: 86.1093908CurrentTrain: epoch  3, batch    26 | loss: 104.3349912CurrentTrain: epoch  3, batch    27 | loss: 106.8150587CurrentTrain: epoch  3, batch    28 | loss: 104.3844274CurrentTrain: epoch  3, batch    29 | loss: 74.4884645CurrentTrain: epoch  3, batch    30 | loss: 89.9939651CurrentTrain: epoch  3, batch    31 | loss: 82.6904485CurrentTrain: epoch  3, batch    32 | loss: 83.4403538CurrentTrain: epoch  3, batch    33 | loss: 131.5674986CurrentTrain: epoch  3, batch    34 | loss: 85.6250099CurrentTrain: epoch  3, batch    35 | loss: 126.1099988CurrentTrain: epoch  3, batch    36 | loss: 60.7332587CurrentTrain: epoch  3, batch    37 | loss: 129.7976532CurrentTrain: epoch  3, batch    38 | loss: 67.1867203CurrentTrain: epoch  3, batch    39 | loss: 87.6559596CurrentTrain: epoch  3, batch    40 | loss: 82.6557451CurrentTrain: epoch  3, batch    41 | loss: 82.2333486CurrentTrain: epoch  3, batch    42 | loss: 86.9087081CurrentTrain: epoch  3, batch    43 | loss: 70.3761649CurrentTrain: epoch  3, batch    44 | loss: 66.0293002CurrentTrain: epoch  3, batch    45 | loss: 72.1870707CurrentTrain: epoch  3, batch    46 | loss: 103.8320906CurrentTrain: epoch  3, batch    47 | loss: 87.8438238CurrentTrain: epoch  3, batch    48 | loss: 131.4546772CurrentTrain: epoch  3, batch    49 | loss: 62.5047247CurrentTrain: epoch  3, batch    50 | loss: 73.5781643CurrentTrain: epoch  3, batch    51 | loss: 75.9038365CurrentTrain: epoch  3, batch    52 | loss: 134.2776014CurrentTrain: epoch  3, batch    53 | loss: 74.4304647CurrentTrain: epoch  3, batch    54 | loss: 104.1705194CurrentTrain: epoch  3, batch    55 | loss: 79.5173929CurrentTrain: epoch  3, batch    56 | loss: 72.0621945CurrentTrain: epoch  3, batch    57 | loss: 74.3053430CurrentTrain: epoch  3, batch    58 | loss: 84.2206378CurrentTrain: epoch  3, batch    59 | loss: 128.8138664CurrentTrain: epoch  3, batch    60 | loss: 85.9800530CurrentTrain: epoch  3, batch    61 | loss: 70.6162665CurrentTrain: epoch  3, batch    62 | loss: 80.0838329CurrentTrain: epoch  3, batch    63 | loss: 103.3169515CurrentTrain: epoch  3, batch    64 | loss: 81.3783575CurrentTrain: epoch  3, batch    65 | loss: 103.9511769CurrentTrain: epoch  3, batch    66 | loss: 105.3683758CurrentTrain: epoch  3, batch    67 | loss: 71.5048635CurrentTrain: epoch  3, batch    68 | loss: 84.6099001CurrentTrain: epoch  3, batch    69 | loss: 68.6900475CurrentTrain: epoch  3, batch    70 | loss: 83.7144106CurrentTrain: epoch  3, batch    71 | loss: 59.7546727CurrentTrain: epoch  3, batch    72 | loss: 73.8701197CurrentTrain: epoch  3, batch    73 | loss: 57.9294724CurrentTrain: epoch  3, batch    74 | loss: 86.6658030CurrentTrain: epoch  3, batch    75 | loss: 107.1591767CurrentTrain: epoch  3, batch    76 | loss: 97.2414778CurrentTrain: epoch  3, batch    77 | loss: 85.4913013CurrentTrain: epoch  3, batch    78 | loss: 127.8766229CurrentTrain: epoch  3, batch    79 | loss: 80.8128194CurrentTrain: epoch  3, batch    80 | loss: 103.8418503CurrentTrain: epoch  3, batch    81 | loss: 61.9756248CurrentTrain: epoch  3, batch    82 | loss: 109.0201214CurrentTrain: epoch  3, batch    83 | loss: 81.6627533CurrentTrain: epoch  3, batch    84 | loss: 71.6048356CurrentTrain: epoch  3, batch    85 | loss: 108.9822924CurrentTrain: epoch  3, batch    86 | loss: 72.1632558CurrentTrain: epoch  3, batch    87 | loss: 85.2314206CurrentTrain: epoch  3, batch    88 | loss: 68.1882593CurrentTrain: epoch  3, batch    89 | loss: 70.0357821CurrentTrain: epoch  3, batch    90 | loss: 96.6964800CurrentTrain: epoch  3, batch    91 | loss: 100.5192026CurrentTrain: epoch  3, batch    92 | loss: 75.4359048CurrentTrain: epoch  3, batch    93 | loss: 84.8906207CurrentTrain: epoch  3, batch    94 | loss: 106.4097139CurrentTrain: epoch  3, batch    95 | loss: 54.9313660CurrentTrain: epoch  4, batch     0 | loss: 81.1925157CurrentTrain: epoch  4, batch     1 | loss: 104.3787064CurrentTrain: epoch  4, batch     2 | loss: 98.6428350CurrentTrain: epoch  4, batch     3 | loss: 76.2837498CurrentTrain: epoch  4, batch     4 | loss: 131.8924357CurrentTrain: epoch  4, batch     5 | loss: 97.0244505CurrentTrain: epoch  4, batch     6 | loss: 71.2075864CurrentTrain: epoch  4, batch     7 | loss: 66.9381776CurrentTrain: epoch  4, batch     8 | loss: 103.4600690CurrentTrain: epoch  4, batch     9 | loss: 130.4543729CurrentTrain: epoch  4, batch    10 | loss: 103.2540657CurrentTrain: epoch  4, batch    11 | loss: 70.2449217CurrentTrain: epoch  4, batch    12 | loss: 82.2115107CurrentTrain: epoch  4, batch    13 | loss: 71.6310113CurrentTrain: epoch  4, batch    14 | loss: 80.7929243CurrentTrain: epoch  4, batch    15 | loss: 102.3534805CurrentTrain: epoch  4, batch    16 | loss: 99.8323898CurrentTrain: epoch  4, batch    17 | loss: 128.7073855CurrentTrain: epoch  4, batch    18 | loss: 80.7205688CurrentTrain: epoch  4, batch    19 | loss: 81.4364864CurrentTrain: epoch  4, batch    20 | loss: 77.7514090CurrentTrain: epoch  4, batch    21 | loss: 70.8457955CurrentTrain: epoch  4, batch    22 | loss: 104.6074832CurrentTrain: epoch  4, batch    23 | loss: 74.4933746CurrentTrain: epoch  4, batch    24 | loss: 94.2783243CurrentTrain: epoch  4, batch    25 | loss: 104.1429598CurrentTrain: epoch  4, batch    26 | loss: 84.9297681CurrentTrain: epoch  4, batch    27 | loss: 63.3821319CurrentTrain: epoch  4, batch    28 | loss: 96.6560494CurrentTrain: epoch  4, batch    29 | loss: 83.5038535CurrentTrain: epoch  4, batch    30 | loss: 86.3454950CurrentTrain: epoch  4, batch    31 | loss: 72.5052351CurrentTrain: epoch  4, batch    32 | loss: 95.8136729CurrentTrain: epoch  4, batch    33 | loss: 57.8686551CurrentTrain: epoch  4, batch    34 | loss: 101.3968784CurrentTrain: epoch  4, batch    35 | loss: 83.9633156CurrentTrain: epoch  4, batch    36 | loss: 71.4138861CurrentTrain: epoch  4, batch    37 | loss: 61.9323310CurrentTrain: epoch  4, batch    38 | loss: 84.1530392CurrentTrain: epoch  4, batch    39 | loss: 72.7254932CurrentTrain: epoch  4, batch    40 | loss: 83.2335746CurrentTrain: epoch  4, batch    41 | loss: 70.2985642CurrentTrain: epoch  4, batch    42 | loss: 86.0387686CurrentTrain: epoch  4, batch    43 | loss: 60.3262014CurrentTrain: epoch  4, batch    44 | loss: 68.0434803CurrentTrain: epoch  4, batch    45 | loss: 84.6535878CurrentTrain: epoch  4, batch    46 | loss: 72.1181269CurrentTrain: epoch  4, batch    47 | loss: 99.6223688CurrentTrain: epoch  4, batch    48 | loss: 75.0443566CurrentTrain: epoch  4, batch    49 | loss: 83.5192579CurrentTrain: epoch  4, batch    50 | loss: 78.1933990CurrentTrain: epoch  4, batch    51 | loss: 72.3876646CurrentTrain: epoch  4, batch    52 | loss: 81.8095363CurrentTrain: epoch  4, batch    53 | loss: 126.2700385CurrentTrain: epoch  4, batch    54 | loss: 82.8447424CurrentTrain: epoch  4, batch    55 | loss: 103.1459397CurrentTrain: epoch  4, batch    56 | loss: 131.1097842CurrentTrain: epoch  4, batch    57 | loss: 72.1752033CurrentTrain: epoch  4, batch    58 | loss: 127.0904184CurrentTrain: epoch  4, batch    59 | loss: 101.0160464CurrentTrain: epoch  4, batch    60 | loss: 83.4471940CurrentTrain: epoch  4, batch    61 | loss: 59.7131681CurrentTrain: epoch  4, batch    62 | loss: 58.4153715CurrentTrain: epoch  4, batch    63 | loss: 69.3455335CurrentTrain: epoch  4, batch    64 | loss: 85.0272972CurrentTrain: epoch  4, batch    65 | loss: 126.1189119CurrentTrain: epoch  4, batch    66 | loss: 73.6771863CurrentTrain: epoch  4, batch    67 | loss: 71.3220187CurrentTrain: epoch  4, batch    68 | loss: 102.0806572CurrentTrain: epoch  4, batch    69 | loss: 70.9098329CurrentTrain: epoch  4, batch    70 | loss: 92.7670377CurrentTrain: epoch  4, batch    71 | loss: 103.2963046CurrentTrain: epoch  4, batch    72 | loss: 67.3147651CurrentTrain: epoch  4, batch    73 | loss: 74.2166769CurrentTrain: epoch  4, batch    74 | loss: 131.8244115CurrentTrain: epoch  4, batch    75 | loss: 82.8397650CurrentTrain: epoch  4, batch    76 | loss: 132.2306219CurrentTrain: epoch  4, batch    77 | loss: 100.6214646CurrentTrain: epoch  4, batch    78 | loss: 69.8300142CurrentTrain: epoch  4, batch    79 | loss: 98.7361017CurrentTrain: epoch  4, batch    80 | loss: 72.3268745CurrentTrain: epoch  4, batch    81 | loss: 131.2540161CurrentTrain: epoch  4, batch    82 | loss: 79.9228140CurrentTrain: epoch  4, batch    83 | loss: 71.7709219CurrentTrain: epoch  4, batch    84 | loss: 75.8890895CurrentTrain: epoch  4, batch    85 | loss: 84.2054499CurrentTrain: epoch  4, batch    86 | loss: 109.4163649CurrentTrain: epoch  4, batch    87 | loss: 125.0045133CurrentTrain: epoch  4, batch    88 | loss: 101.9408284CurrentTrain: epoch  4, batch    89 | loss: 71.2770392CurrentTrain: epoch  4, batch    90 | loss: 120.7855996CurrentTrain: epoch  4, batch    91 | loss: 96.4343043CurrentTrain: epoch  4, batch    92 | loss: 85.6023625CurrentTrain: epoch  4, batch    93 | loss: 80.0909842CurrentTrain: epoch  4, batch    94 | loss: 89.6994524CurrentTrain: epoch  4, batch    95 | loss: 59.8925185CurrentTrain: epoch  5, batch     0 | loss: 80.5966003CurrentTrain: epoch  5, batch     1 | loss: 82.4205823CurrentTrain: epoch  5, batch     2 | loss: 98.5674685CurrentTrain: epoch  5, batch     3 | loss: 84.1024254CurrentTrain: epoch  5, batch     4 | loss: 79.6472206CurrentTrain: epoch  5, batch     5 | loss: 68.1020415CurrentTrain: epoch  5, batch     6 | loss: 127.5205251CurrentTrain: epoch  5, batch     7 | loss: 79.5938142CurrentTrain: epoch  5, batch     8 | loss: 81.1310523CurrentTrain: epoch  5, batch     9 | loss: 79.1048799CurrentTrain: epoch  5, batch    10 | loss: 70.8473665CurrentTrain: epoch  5, batch    11 | loss: 102.6003275CurrentTrain: epoch  5, batch    12 | loss: 131.7212337CurrentTrain: epoch  5, batch    13 | loss: 67.2501293CurrentTrain: epoch  5, batch    14 | loss: 97.0861518CurrentTrain: epoch  5, batch    15 | loss: 65.9092995CurrentTrain: epoch  5, batch    16 | loss: 82.3551287CurrentTrain: epoch  5, batch    17 | loss: 67.4767199CurrentTrain: epoch  5, batch    18 | loss: 83.2738142CurrentTrain: epoch  5, batch    19 | loss: 99.2961102CurrentTrain: epoch  5, batch    20 | loss: 81.0365967CurrentTrain: epoch  5, batch    21 | loss: 65.7767380CurrentTrain: epoch  5, batch    22 | loss: 83.1492932CurrentTrain: epoch  5, batch    23 | loss: 95.1483461CurrentTrain: epoch  5, batch    24 | loss: 59.7823532CurrentTrain: epoch  5, batch    25 | loss: 83.3601883CurrentTrain: epoch  5, batch    26 | loss: 101.7092636CurrentTrain: epoch  5, batch    27 | loss: 95.7124455CurrentTrain: epoch  5, batch    28 | loss: 129.0960142CurrentTrain: epoch  5, batch    29 | loss: 271.6319033CurrentTrain: epoch  5, batch    30 | loss: 72.7203998CurrentTrain: epoch  5, batch    31 | loss: 168.9589176CurrentTrain: epoch  5, batch    32 | loss: 67.1434747CurrentTrain: epoch  5, batch    33 | loss: 75.2706331CurrentTrain: epoch  5, batch    34 | loss: 80.4702621CurrentTrain: epoch  5, batch    35 | loss: 62.8996400CurrentTrain: epoch  5, batch    36 | loss: 127.4575868CurrentTrain: epoch  5, batch    37 | loss: 80.8636340CurrentTrain: epoch  5, batch    38 | loss: 93.8283353CurrentTrain: epoch  5, batch    39 | loss: 101.2214563CurrentTrain: epoch  5, batch    40 | loss: 79.5770264CurrentTrain: epoch  5, batch    41 | loss: 102.4560972CurrentTrain: epoch  5, batch    42 | loss: 131.4703456CurrentTrain: epoch  5, batch    43 | loss: 71.2333321CurrentTrain: epoch  5, batch    44 | loss: 68.5021368CurrentTrain: epoch  5, batch    45 | loss: 83.4708248CurrentTrain: epoch  5, batch    46 | loss: 84.1538873CurrentTrain: epoch  5, batch    47 | loss: 67.7231565CurrentTrain: epoch  5, batch    48 | loss: 63.4383996CurrentTrain: epoch  5, batch    49 | loss: 72.4667632CurrentTrain: epoch  5, batch    50 | loss: 60.6082015CurrentTrain: epoch  5, batch    51 | loss: 81.4443219CurrentTrain: epoch  5, batch    52 | loss: 84.5161335CurrentTrain: epoch  5, batch    53 | loss: 75.8044405CurrentTrain: epoch  5, batch    54 | loss: 85.7379908CurrentTrain: epoch  5, batch    55 | loss: 103.2407996CurrentTrain: epoch  5, batch    56 | loss: 130.0235594CurrentTrain: epoch  5, batch    57 | loss: 83.7625513CurrentTrain: epoch  5, batch    58 | loss: 85.0186685CurrentTrain: epoch  5, batch    59 | loss: 128.9411005CurrentTrain: epoch  5, batch    60 | loss: 101.1383579CurrentTrain: epoch  5, batch    61 | loss: 80.7230477CurrentTrain: epoch  5, batch    62 | loss: 100.9514014CurrentTrain: epoch  5, batch    63 | loss: 84.9982294CurrentTrain: epoch  5, batch    64 | loss: 56.0775791CurrentTrain: epoch  5, batch    65 | loss: 77.1445326CurrentTrain: epoch  5, batch    66 | loss: 79.3955089CurrentTrain: epoch  5, batch    67 | loss: 80.5895903CurrentTrain: epoch  5, batch    68 | loss: 64.5130640CurrentTrain: epoch  5, batch    69 | loss: 80.7609168CurrentTrain: epoch  5, batch    70 | loss: 99.8350609CurrentTrain: epoch  5, batch    71 | loss: 101.6812270CurrentTrain: epoch  5, batch    72 | loss: 99.7269097CurrentTrain: epoch  5, batch    73 | loss: 79.8480792CurrentTrain: epoch  5, batch    74 | loss: 63.9505803CurrentTrain: epoch  5, batch    75 | loss: 68.6775772CurrentTrain: epoch  5, batch    76 | loss: 128.3159447CurrentTrain: epoch  5, batch    77 | loss: 84.3186951CurrentTrain: epoch  5, batch    78 | loss: 78.7282573CurrentTrain: epoch  5, batch    79 | loss: 81.4848158CurrentTrain: epoch  5, batch    80 | loss: 96.7537570CurrentTrain: epoch  5, batch    81 | loss: 99.8168078CurrentTrain: epoch  5, batch    82 | loss: 81.5710042CurrentTrain: epoch  5, batch    83 | loss: 102.0406270CurrentTrain: epoch  5, batch    84 | loss: 97.2162191CurrentTrain: epoch  5, batch    85 | loss: 84.4176718CurrentTrain: epoch  5, batch    86 | loss: 101.6673858CurrentTrain: epoch  5, batch    87 | loss: 66.3582953CurrentTrain: epoch  5, batch    88 | loss: 104.3301354CurrentTrain: epoch  5, batch    89 | loss: 79.5462126CurrentTrain: epoch  5, batch    90 | loss: 80.5540503CurrentTrain: epoch  5, batch    91 | loss: 86.2468114CurrentTrain: epoch  5, batch    92 | loss: 82.0794009CurrentTrain: epoch  5, batch    93 | loss: 69.0485457CurrentTrain: epoch  5, batch    94 | loss: 125.2713697CurrentTrain: epoch  5, batch    95 | loss: 60.8778001CurrentTrain: epoch  6, batch     0 | loss: 58.0283327CurrentTrain: epoch  6, batch     1 | loss: 80.9879624CurrentTrain: epoch  6, batch     2 | loss: 77.2207963CurrentTrain: epoch  6, batch     3 | loss: 100.4769427CurrentTrain: epoch  6, batch     4 | loss: 128.4869613CurrentTrain: epoch  6, batch     5 | loss: 173.6298729CurrentTrain: epoch  6, batch     6 | loss: 81.7397507CurrentTrain: epoch  6, batch     7 | loss: 84.1658001CurrentTrain: epoch  6, batch     8 | loss: 98.8058030CurrentTrain: epoch  6, batch     9 | loss: 83.3517394CurrentTrain: epoch  6, batch    10 | loss: 123.8727908CurrentTrain: epoch  6, batch    11 | loss: 82.3793326CurrentTrain: epoch  6, batch    12 | loss: 101.1016140CurrentTrain: epoch  6, batch    13 | loss: 99.5286563CurrentTrain: epoch  6, batch    14 | loss: 60.2063899CurrentTrain: epoch  6, batch    15 | loss: 77.2456175CurrentTrain: epoch  6, batch    16 | loss: 67.3385731CurrentTrain: epoch  6, batch    17 | loss: 82.1057935CurrentTrain: epoch  6, batch    18 | loss: 66.5025558CurrentTrain: epoch  6, batch    19 | loss: 66.0600100CurrentTrain: epoch  6, batch    20 | loss: 67.9114848CurrentTrain: epoch  6, batch    21 | loss: 82.6364220CurrentTrain: epoch  6, batch    22 | loss: 83.9926183CurrentTrain: epoch  6, batch    23 | loss: 96.2474965CurrentTrain: epoch  6, batch    24 | loss: 100.1549627CurrentTrain: epoch  6, batch    25 | loss: 67.2542082CurrentTrain: epoch  6, batch    26 | loss: 67.3861490CurrentTrain: epoch  6, batch    27 | loss: 69.4247437CurrentTrain: epoch  6, batch    28 | loss: 71.5442680CurrentTrain: epoch  6, batch    29 | loss: 94.3330497CurrentTrain: epoch  6, batch    30 | loss: 99.5891309CurrentTrain: epoch  6, batch    31 | loss: 56.9387874CurrentTrain: epoch  6, batch    32 | loss: 98.4598429CurrentTrain: epoch  6, batch    33 | loss: 84.0313433CurrentTrain: epoch  6, batch    34 | loss: 97.3688242CurrentTrain: epoch  6, batch    35 | loss: 66.5963597CurrentTrain: epoch  6, batch    36 | loss: 68.0612801CurrentTrain: epoch  6, batch    37 | loss: 71.9990121CurrentTrain: epoch  6, batch    38 | loss: 100.8632375CurrentTrain: epoch  6, batch    39 | loss: 78.8489600CurrentTrain: epoch  6, batch    40 | loss: 66.7381005CurrentTrain: epoch  6, batch    41 | loss: 97.0387021CurrentTrain: epoch  6, batch    42 | loss: 77.1481161CurrentTrain: epoch  6, batch    43 | loss: 58.7045695CurrentTrain: epoch  6, batch    44 | loss: 56.5464024CurrentTrain: epoch  6, batch    45 | loss: 81.4180694CurrentTrain: epoch  6, batch    46 | loss: 77.6669854CurrentTrain: epoch  6, batch    47 | loss: 131.1936837CurrentTrain: epoch  6, batch    48 | loss: 70.5489916CurrentTrain: epoch  6, batch    49 | loss: 67.1715008CurrentTrain: epoch  6, batch    50 | loss: 100.2561889CurrentTrain: epoch  6, batch    51 | loss: 96.5312105CurrentTrain: epoch  6, batch    52 | loss: 124.9058923CurrentTrain: epoch  6, batch    53 | loss: 100.7273867CurrentTrain: epoch  6, batch    54 | loss: 83.0760904CurrentTrain: epoch  6, batch    55 | loss: 81.8412858CurrentTrain: epoch  6, batch    56 | loss: 102.9064570CurrentTrain: epoch  6, batch    57 | loss: 62.4564163CurrentTrain: epoch  6, batch    58 | loss: 100.6488655CurrentTrain: epoch  6, batch    59 | loss: 58.4989762CurrentTrain: epoch  6, batch    60 | loss: 77.8470590CurrentTrain: epoch  6, batch    61 | loss: 83.5033851CurrentTrain: epoch  6, batch    62 | loss: 100.2748025CurrentTrain: epoch  6, batch    63 | loss: 81.0132620CurrentTrain: epoch  6, batch    64 | loss: 78.2577202CurrentTrain: epoch  6, batch    65 | loss: 101.2698840CurrentTrain: epoch  6, batch    66 | loss: 82.2403879CurrentTrain: epoch  6, batch    67 | loss: 79.8747678CurrentTrain: epoch  6, batch    68 | loss: 99.9560869CurrentTrain: epoch  6, batch    69 | loss: 78.7748803CurrentTrain: epoch  6, batch    70 | loss: 100.0490645CurrentTrain: epoch  6, batch    71 | loss: 56.4726161CurrentTrain: epoch  6, batch    72 | loss: 69.8882750CurrentTrain: epoch  6, batch    73 | loss: 63.4320737CurrentTrain: epoch  6, batch    74 | loss: 102.4201292CurrentTrain: epoch  6, batch    75 | loss: 100.6421962CurrentTrain: epoch  6, batch    76 | loss: 55.0849909CurrentTrain: epoch  6, batch    77 | loss: 58.4333371CurrentTrain: epoch  6, batch    78 | loss: 97.4035222CurrentTrain: epoch  6, batch    79 | loss: 96.9742505CurrentTrain: epoch  6, batch    80 | loss: 80.3147398CurrentTrain: epoch  6, batch    81 | loss: 59.5488601CurrentTrain: epoch  6, batch    82 | loss: 71.7031220CurrentTrain: epoch  6, batch    83 | loss: 96.0758771CurrentTrain: epoch  6, batch    84 | loss: 81.8855698CurrentTrain: epoch  6, batch    85 | loss: 59.8308118CurrentTrain: epoch  6, batch    86 | loss: 80.6573668CurrentTrain: epoch  6, batch    87 | loss: 97.9234290CurrentTrain: epoch  6, batch    88 | loss: 89.2239801CurrentTrain: epoch  6, batch    89 | loss: 102.2873144CurrentTrain: epoch  6, batch    90 | loss: 97.0976080CurrentTrain: epoch  6, batch    91 | loss: 60.1794104CurrentTrain: epoch  6, batch    92 | loss: 81.0200931CurrentTrain: epoch  6, batch    93 | loss: 79.8498574CurrentTrain: epoch  6, batch    94 | loss: 69.5990396CurrentTrain: epoch  6, batch    95 | loss: 101.4232935CurrentTrain: epoch  7, batch     0 | loss: 79.3539305CurrentTrain: epoch  7, batch     1 | loss: 97.1318668CurrentTrain: epoch  7, batch     2 | loss: 82.2508994CurrentTrain: epoch  7, batch     3 | loss: 79.2520262CurrentTrain: epoch  7, batch     4 | loss: 79.9066568CurrentTrain: epoch  7, batch     5 | loss: 96.4381151CurrentTrain: epoch  7, batch     6 | loss: 81.9498116CurrentTrain: epoch  7, batch     7 | loss: 80.8653621CurrentTrain: epoch  7, batch     8 | loss: 67.3406509CurrentTrain: epoch  7, batch     9 | loss: 64.5404223CurrentTrain: epoch  7, batch    10 | loss: 65.4414518CurrentTrain: epoch  7, batch    11 | loss: 69.1733550CurrentTrain: epoch  7, batch    12 | loss: 98.4021300CurrentTrain: epoch  7, batch    13 | loss: 80.8222327CurrentTrain: epoch  7, batch    14 | loss: 80.4265900CurrentTrain: epoch  7, batch    15 | loss: 68.1375272CurrentTrain: epoch  7, batch    16 | loss: 101.6645560CurrentTrain: epoch  7, batch    17 | loss: 74.3520428CurrentTrain: epoch  7, batch    18 | loss: 67.6826685CurrentTrain: epoch  7, batch    19 | loss: 63.9340917CurrentTrain: epoch  7, batch    20 | loss: 75.4377439CurrentTrain: epoch  7, batch    21 | loss: 76.4234360CurrentTrain: epoch  7, batch    22 | loss: 99.4695050CurrentTrain: epoch  7, batch    23 | loss: 95.8118466CurrentTrain: epoch  7, batch    24 | loss: 66.3005924CurrentTrain: epoch  7, batch    25 | loss: 56.8784669CurrentTrain: epoch  7, batch    26 | loss: 80.4671667CurrentTrain: epoch  7, batch    27 | loss: 98.8481693CurrentTrain: epoch  7, batch    28 | loss: 66.7940969CurrentTrain: epoch  7, batch    29 | loss: 102.0614883CurrentTrain: epoch  7, batch    30 | loss: 80.1201729CurrentTrain: epoch  7, batch    31 | loss: 83.2261393CurrentTrain: epoch  7, batch    32 | loss: 121.0673824CurrentTrain: epoch  7, batch    33 | loss: 82.5480052CurrentTrain: epoch  7, batch    34 | loss: 83.6835440CurrentTrain: epoch  7, batch    35 | loss: 97.7575281CurrentTrain: epoch  7, batch    36 | loss: 79.3788721CurrentTrain: epoch  7, batch    37 | loss: 82.6836352CurrentTrain: epoch  7, batch    38 | loss: 81.0910899CurrentTrain: epoch  7, batch    39 | loss: 102.6052265CurrentTrain: epoch  7, batch    40 | loss: 80.8881637CurrentTrain: epoch  7, batch    41 | loss: 64.0593875CurrentTrain: epoch  7, batch    42 | loss: 95.1771833CurrentTrain: epoch  7, batch    43 | loss: 65.6052315CurrentTrain: epoch  7, batch    44 | loss: 78.0356973CurrentTrain: epoch  7, batch    45 | loss: 98.1485984CurrentTrain: epoch  7, batch    46 | loss: 71.7165055CurrentTrain: epoch  7, batch    47 | loss: 75.7541394CurrentTrain: epoch  7, batch    48 | loss: 121.0509474CurrentTrain: epoch  7, batch    49 | loss: 57.4444081CurrentTrain: epoch  7, batch    50 | loss: 92.2139371CurrentTrain: epoch  7, batch    51 | loss: 75.2406979CurrentTrain: epoch  7, batch    52 | loss: 68.8551652CurrentTrain: epoch  7, batch    53 | loss: 98.7050812CurrentTrain: epoch  7, batch    54 | loss: 65.4921690CurrentTrain: epoch  7, batch    55 | loss: 100.2865134CurrentTrain: epoch  7, batch    56 | loss: 93.4819243CurrentTrain: epoch  7, batch    57 | loss: 81.6331656CurrentTrain: epoch  7, batch    58 | loss: 78.6644896CurrentTrain: epoch  7, batch    59 | loss: 126.3647629CurrentTrain: epoch  7, batch    60 | loss: 66.2172835CurrentTrain: epoch  7, batch    61 | loss: 67.4140902CurrentTrain: epoch  7, batch    62 | loss: 120.1889135CurrentTrain: epoch  7, batch    63 | loss: 80.6780922CurrentTrain: epoch  7, batch    64 | loss: 62.6310023CurrentTrain: epoch  7, batch    65 | loss: 74.3160076CurrentTrain: epoch  7, batch    66 | loss: 78.5188142CurrentTrain: epoch  7, batch    67 | loss: 80.7815207CurrentTrain: epoch  7, batch    68 | loss: 63.4580665CurrentTrain: epoch  7, batch    69 | loss: 79.5962943CurrentTrain: epoch  7, batch    70 | loss: 70.1964850CurrentTrain: epoch  7, batch    71 | loss: 97.8073029CurrentTrain: epoch  7, batch    72 | loss: 66.4740330CurrentTrain: epoch  7, batch    73 | loss: 78.1585405CurrentTrain: epoch  7, batch    74 | loss: 55.3784339CurrentTrain: epoch  7, batch    75 | loss: 64.9509142CurrentTrain: epoch  7, batch    76 | loss: 68.7736149CurrentTrain: epoch  7, batch    77 | loss: 98.1162825CurrentTrain: epoch  7, batch    78 | loss: 85.5149763CurrentTrain: epoch  7, batch    79 | loss: 126.8034485CurrentTrain: epoch  7, batch    80 | loss: 69.4305656CurrentTrain: epoch  7, batch    81 | loss: 66.6993700CurrentTrain: epoch  7, batch    82 | loss: 78.2715533CurrentTrain: epoch  7, batch    83 | loss: 64.8358333CurrentTrain: epoch  7, batch    84 | loss: 78.4339882CurrentTrain: epoch  7, batch    85 | loss: 83.3529062CurrentTrain: epoch  7, batch    86 | loss: 65.7365696CurrentTrain: epoch  7, batch    87 | loss: 99.3715544CurrentTrain: epoch  7, batch    88 | loss: 94.5228844CurrentTrain: epoch  7, batch    89 | loss: 126.6911650CurrentTrain: epoch  7, batch    90 | loss: 80.5774266CurrentTrain: epoch  7, batch    91 | loss: 81.6581555CurrentTrain: epoch  7, batch    92 | loss: 101.9159107CurrentTrain: epoch  7, batch    93 | loss: 57.7476524CurrentTrain: epoch  7, batch    94 | loss: 59.9537683CurrentTrain: epoch  7, batch    95 | loss: 83.0220926CurrentTrain: epoch  8, batch     0 | loss: 68.6429513CurrentTrain: epoch  8, batch     1 | loss: 64.1831303CurrentTrain: epoch  8, batch     2 | loss: 93.9567616CurrentTrain: epoch  8, batch     3 | loss: 127.5366862CurrentTrain: epoch  8, batch     4 | loss: 78.5519602CurrentTrain: epoch  8, batch     5 | loss: 68.7558935CurrentTrain: epoch  8, batch     6 | loss: 82.0887907CurrentTrain: epoch  8, batch     7 | loss: 76.4718326CurrentTrain: epoch  8, batch     8 | loss: 66.7060750CurrentTrain: epoch  8, batch     9 | loss: 96.9569041CurrentTrain: epoch  8, batch    10 | loss: 75.7853662CurrentTrain: epoch  8, batch    11 | loss: 77.8429937CurrentTrain: epoch  8, batch    12 | loss: 77.4746469CurrentTrain: epoch  8, batch    13 | loss: 91.7826128CurrentTrain: epoch  8, batch    14 | loss: 78.2033357CurrentTrain: epoch  8, batch    15 | loss: 78.8704016CurrentTrain: epoch  8, batch    16 | loss: 95.0697935CurrentTrain: epoch  8, batch    17 | loss: 78.8986369CurrentTrain: epoch  8, batch    18 | loss: 97.9099228CurrentTrain: epoch  8, batch    19 | loss: 65.7712156CurrentTrain: epoch  8, batch    20 | loss: 76.2392729CurrentTrain: epoch  8, batch    21 | loss: 75.2757527CurrentTrain: epoch  8, batch    22 | loss: 92.3826257CurrentTrain: epoch  8, batch    23 | loss: 66.8449452CurrentTrain: epoch  8, batch    24 | loss: 63.9017651CurrentTrain: epoch  8, batch    25 | loss: 94.5461905CurrentTrain: epoch  8, batch    26 | loss: 65.2237135CurrentTrain: epoch  8, batch    27 | loss: 76.1450419CurrentTrain: epoch  8, batch    28 | loss: 66.2522762CurrentTrain: epoch  8, batch    29 | loss: 68.2914763CurrentTrain: epoch  8, batch    30 | loss: 101.7659450CurrentTrain: epoch  8, batch    31 | loss: 54.2004824CurrentTrain: epoch  8, batch    32 | loss: 169.0480093CurrentTrain: epoch  8, batch    33 | loss: 95.5356772CurrentTrain: epoch  8, batch    34 | loss: 94.3875187CurrentTrain: epoch  8, batch    35 | loss: 65.5120452CurrentTrain: epoch  8, batch    36 | loss: 95.4392348CurrentTrain: epoch  8, batch    37 | loss: 67.3236024CurrentTrain: epoch  8, batch    38 | loss: 77.7978862CurrentTrain: epoch  8, batch    39 | loss: 80.6845611CurrentTrain: epoch  8, batch    40 | loss: 80.5662272CurrentTrain: epoch  8, batch    41 | loss: 97.1013060CurrentTrain: epoch  8, batch    42 | loss: 79.7708279CurrentTrain: epoch  8, batch    43 | loss: 102.4053648CurrentTrain: epoch  8, batch    44 | loss: 123.3007622CurrentTrain: epoch  8, batch    45 | loss: 81.8264171CurrentTrain: epoch  8, batch    46 | loss: 65.7287109CurrentTrain: epoch  8, batch    47 | loss: 81.0465801CurrentTrain: epoch  8, batch    48 | loss: 64.1936796CurrentTrain: epoch  8, batch    49 | loss: 75.9249068CurrentTrain: epoch  8, batch    50 | loss: 94.7304168CurrentTrain: epoch  8, batch    51 | loss: 62.4194882CurrentTrain: epoch  8, batch    52 | loss: 78.4739713CurrentTrain: epoch  8, batch    53 | loss: 83.6248174CurrentTrain: epoch  8, batch    54 | loss: 97.9729612CurrentTrain: epoch  8, batch    55 | loss: 94.6605357CurrentTrain: epoch  8, batch    56 | loss: 68.8368596CurrentTrain: epoch  8, batch    57 | loss: 65.0805656CurrentTrain: epoch  8, batch    58 | loss: 78.6367131CurrentTrain: epoch  8, batch    59 | loss: 54.7006272CurrentTrain: epoch  8, batch    60 | loss: 122.3512790CurrentTrain: epoch  8, batch    61 | loss: 95.9575936CurrentTrain: epoch  8, batch    62 | loss: 56.2356042CurrentTrain: epoch  8, batch    63 | loss: 80.4276721CurrentTrain: epoch  8, batch    64 | loss: 80.2258990CurrentTrain: epoch  8, batch    65 | loss: 79.8548151CurrentTrain: epoch  8, batch    66 | loss: 73.7879378CurrentTrain: epoch  8, batch    67 | loss: 91.9617351CurrentTrain: epoch  8, batch    68 | loss: 94.7916144CurrentTrain: epoch  8, batch    69 | loss: 82.3051605CurrentTrain: epoch  8, batch    70 | loss: 96.4293579CurrentTrain: epoch  8, batch    71 | loss: 122.1917015CurrentTrain: epoch  8, batch    72 | loss: 76.4376415CurrentTrain: epoch  8, batch    73 | loss: 77.8058608CurrentTrain: epoch  8, batch    74 | loss: 96.8104094CurrentTrain: epoch  8, batch    75 | loss: 78.4496677CurrentTrain: epoch  8, batch    76 | loss: 57.7578066CurrentTrain: epoch  8, batch    77 | loss: 121.0735997CurrentTrain: epoch  8, batch    78 | loss: 64.0317273CurrentTrain: epoch  8, batch    79 | loss: 69.6034165CurrentTrain: epoch  8, batch    80 | loss: 92.9017462CurrentTrain: epoch  8, batch    81 | loss: 78.5621308CurrentTrain: epoch  8, batch    82 | loss: 67.6421500CurrentTrain: epoch  8, batch    83 | loss: 64.8457372CurrentTrain: epoch  8, batch    84 | loss: 125.2434232CurrentTrain: epoch  8, batch    85 | loss: 119.0629876CurrentTrain: epoch  8, batch    86 | loss: 96.9240912CurrentTrain: epoch  8, batch    87 | loss: 78.6042970CurrentTrain: epoch  8, batch    88 | loss: 52.1580830CurrentTrain: epoch  8, batch    89 | loss: 76.7125847CurrentTrain: epoch  8, batch    90 | loss: 76.9393862CurrentTrain: epoch  8, batch    91 | loss: 83.5227832CurrentTrain: epoch  8, batch    92 | loss: 79.5162485CurrentTrain: epoch  8, batch    93 | loss: 77.4405654CurrentTrain: epoch  8, batch    94 | loss: 98.8682787CurrentTrain: epoch  8, batch    95 | loss: 102.1088155CurrentTrain: epoch  9, batch     0 | loss: 66.3059325CurrentTrain: epoch  9, batch     1 | loss: 117.1129574CurrentTrain: epoch  9, batch     2 | loss: 75.3157977CurrentTrain: epoch  9, batch     3 | loss: 66.2553450CurrentTrain: epoch  9, batch     4 | loss: 77.4599482CurrentTrain: epoch  9, batch     5 | loss: 53.0968986CurrentTrain: epoch  9, batch     6 | loss: 101.2700827CurrentTrain: epoch  9, batch     7 | loss: 75.1637981CurrentTrain: epoch  9, batch     8 | loss: 97.4368652CurrentTrain: epoch  9, batch     9 | loss: 64.4874213CurrentTrain: epoch  9, batch    10 | loss: 101.5379925CurrentTrain: epoch  9, batch    11 | loss: 67.6796917CurrentTrain: epoch  9, batch    12 | loss: 93.7909293CurrentTrain: epoch  9, batch    13 | loss: 97.4840072CurrentTrain: epoch  9, batch    14 | loss: 95.5277312CurrentTrain: epoch  9, batch    15 | loss: 74.3903567CurrentTrain: epoch  9, batch    16 | loss: 126.2412563CurrentTrain: epoch  9, batch    17 | loss: 78.4670722CurrentTrain: epoch  9, batch    18 | loss: 79.1315793CurrentTrain: epoch  9, batch    19 | loss: 80.5014506CurrentTrain: epoch  9, batch    20 | loss: 64.4208415CurrentTrain: epoch  9, batch    21 | loss: 95.9900464CurrentTrain: epoch  9, batch    22 | loss: 66.2912792CurrentTrain: epoch  9, batch    23 | loss: 66.7852087CurrentTrain: epoch  9, batch    24 | loss: 123.6100574CurrentTrain: epoch  9, batch    25 | loss: 93.5807705CurrentTrain: epoch  9, batch    26 | loss: 61.8912863CurrentTrain: epoch  9, batch    27 | loss: 120.0274327CurrentTrain: epoch  9, batch    28 | loss: 64.6761341CurrentTrain: epoch  9, batch    29 | loss: 67.6992334CurrentTrain: epoch  9, batch    30 | loss: 63.8019128CurrentTrain: epoch  9, batch    31 | loss: 82.0582400CurrentTrain: epoch  9, batch    32 | loss: 94.0152227CurrentTrain: epoch  9, batch    33 | loss: 69.6376774CurrentTrain: epoch  9, batch    34 | loss: 61.0737306CurrentTrain: epoch  9, batch    35 | loss: 120.0576422CurrentTrain: epoch  9, batch    36 | loss: 81.8861101CurrentTrain: epoch  9, batch    37 | loss: 79.2004942CurrentTrain: epoch  9, batch    38 | loss: 80.5772991CurrentTrain: epoch  9, batch    39 | loss: 66.1024813CurrentTrain: epoch  9, batch    40 | loss: 61.9738811CurrentTrain: epoch  9, batch    41 | loss: 80.0399837CurrentTrain: epoch  9, batch    42 | loss: 62.5148334CurrentTrain: epoch  9, batch    43 | loss: 63.2843894CurrentTrain: epoch  9, batch    44 | loss: 124.7518789CurrentTrain: epoch  9, batch    45 | loss: 67.6737210CurrentTrain: epoch  9, batch    46 | loss: 80.6307174CurrentTrain: epoch  9, batch    47 | loss: 95.7726862CurrentTrain: epoch  9, batch    48 | loss: 77.5838819CurrentTrain: epoch  9, batch    49 | loss: 65.1747135CurrentTrain: epoch  9, batch    50 | loss: 75.2149964CurrentTrain: epoch  9, batch    51 | loss: 78.1526424CurrentTrain: epoch  9, batch    52 | loss: 67.3154908CurrentTrain: epoch  9, batch    53 | loss: 80.6100437CurrentTrain: epoch  9, batch    54 | loss: 66.2671897CurrentTrain: epoch  9, batch    55 | loss: 63.7503375CurrentTrain: epoch  9, batch    56 | loss: 84.0439402CurrentTrain: epoch  9, batch    57 | loss: 53.1982222CurrentTrain: epoch  9, batch    58 | loss: 93.3911116CurrentTrain: epoch  9, batch    59 | loss: 96.0880535CurrentTrain: epoch  9, batch    60 | loss: 71.9684019CurrentTrain: epoch  9, batch    61 | loss: 55.4079889CurrentTrain: epoch  9, batch    62 | loss: 95.2490093CurrentTrain: epoch  9, batch    63 | loss: 63.7213692CurrentTrain: epoch  9, batch    64 | loss: 57.9612236CurrentTrain: epoch  9, batch    65 | loss: 93.8750001CurrentTrain: epoch  9, batch    66 | loss: 94.0402828CurrentTrain: epoch  9, batch    67 | loss: 74.9685111CurrentTrain: epoch  9, batch    68 | loss: 124.0524224CurrentTrain: epoch  9, batch    69 | loss: 97.6627017CurrentTrain: epoch  9, batch    70 | loss: 76.2832644CurrentTrain: epoch  9, batch    71 | loss: 94.6688434CurrentTrain: epoch  9, batch    72 | loss: 65.2861287CurrentTrain: epoch  9, batch    73 | loss: 95.7428481CurrentTrain: epoch  9, batch    74 | loss: 128.6629073CurrentTrain: epoch  9, batch    75 | loss: 100.6865859CurrentTrain: epoch  9, batch    76 | loss: 94.1151704CurrentTrain: epoch  9, batch    77 | loss: 64.5731023CurrentTrain: epoch  9, batch    78 | loss: 124.1530635CurrentTrain: epoch  9, batch    79 | loss: 79.1377053CurrentTrain: epoch  9, batch    80 | loss: 55.9293031CurrentTrain: epoch  9, batch    81 | loss: 55.1283029CurrentTrain: epoch  9, batch    82 | loss: 99.1955046CurrentTrain: epoch  9, batch    83 | loss: 124.2602564CurrentTrain: epoch  9, batch    84 | loss: 122.8081851CurrentTrain: epoch  9, batch    85 | loss: 66.4450693CurrentTrain: epoch  9, batch    86 | loss: 77.2979328CurrentTrain: epoch  9, batch    87 | loss: 65.4198537CurrentTrain: epoch  9, batch    88 | loss: 127.5075009CurrentTrain: epoch  9, batch    89 | loss: 57.5270692CurrentTrain: epoch  9, batch    90 | loss: 79.3325871CurrentTrain: epoch  9, batch    91 | loss: 78.1652247CurrentTrain: epoch  9, batch    92 | loss: 55.0479015CurrentTrain: epoch  9, batch    93 | loss: 66.9286628CurrentTrain: epoch  9, batch    94 | loss: 92.0563933CurrentTrain: epoch  9, batch    95 | loss: 65.3386420

F1 score per class: {32: 0.5698924731182796, 6: 0.8151658767772512, 19: 0.4375, 24: 0.7486033519553073, 26: 0.9183673469387755, 29: 0.8240740740740741}
Micro-average F1 score: 0.7686274509803922
Weighted-average F1 score: 0.7739336408638072
F1 score per class: {32: 0.6415094339622641, 6: 0.7982062780269058, 19: 0.2647058823529412, 24: 0.7076923076923077, 26: 0.93, 29: 0.8269230769230769}
Micro-average F1 score: 0.7486437613019892
Weighted-average F1 score: 0.7360581368319584
F1 score per class: {32: 0.6372549019607843, 6: 0.8, 19: 0.35555555555555557, 24: 0.7076923076923077, 26: 0.93, 29: 0.8169014084507042}
Micro-average F1 score: 0.7613741875580315
Weighted-average F1 score: 0.7592399609832208

F1 score per class: {32: 0.5698924731182796, 6: 0.8151658767772512, 19: 0.4375, 24: 0.7486033519553073, 26: 0.9183673469387755, 29: 0.8240740740740741}
Micro-average F1 score: 0.7686274509803922
Weighted-average F1 score: 0.7739336408638072
F1 score per class: {32: 0.6415094339622641, 6: 0.7982062780269058, 19: 0.2647058823529412, 24: 0.7076923076923077, 26: 0.93, 29: 0.8269230769230769}
Micro-average F1 score: 0.7486437613019892
Weighted-average F1 score: 0.7360581368319584
F1 score per class: {32: 0.6372549019607843, 6: 0.8, 19: 0.35555555555555557, 24: 0.7076923076923077, 26: 0.93, 29: 0.8169014084507042}
Micro-average F1 score: 0.7613741875580315
Weighted-average F1 score: 0.7592399609832208

F1 score per class: {32: 0.43089430894308944, 6: 0.7678571428571429, 19: 0.32558139534883723, 24: 0.6907216494845361, 26: 0.8333333333333334, 29: 0.624561403508772}
Micro-average F1 score: 0.6490066225165563
Weighted-average F1 score: 0.642612682711643
F1 score per class: {32: 0.4459016393442623, 6: 0.7385892116182573, 19: 0.1565217391304348, 24: 0.6301369863013698, 26: 0.8378378378378378, 29: 0.6417910447761194}
Micro-average F1 score: 0.6043795620437956
Weighted-average F1 score: 0.5818743388589994
F1 score per class: {32: 0.44368600682593856, 6: 0.7521367521367521, 19: 0.22535211267605634, 24: 0.6330275229357798, 26: 0.8416289592760181, 29: 0.6304347826086957}
Micro-average F1 score: 0.6245239908606245
Weighted-average F1 score: 0.6111349177276887

F1 score per class: {32: 0.43089430894308944, 6: 0.7678571428571429, 19: 0.32558139534883723, 24: 0.6907216494845361, 26: 0.8333333333333334, 29: 0.624561403508772}
Micro-average F1 score: 0.6490066225165563
Weighted-average F1 score: 0.642612682711643
F1 score per class: {32: 0.4459016393442623, 6: 0.7385892116182573, 19: 0.1565217391304348, 24: 0.6301369863013698, 26: 0.8378378378378378, 29: 0.6417910447761194}
Micro-average F1 score: 0.6043795620437956
Weighted-average F1 score: 0.5818743388589994
F1 score per class: {32: 0.44368600682593856, 6: 0.7521367521367521, 19: 0.22535211267605634, 24: 0.6330275229357798, 26: 0.8416289592760181, 29: 0.6304347826086957}
Micro-average F1 score: 0.6245239908606245
Weighted-average F1 score: 0.6111349177276887
cur_acc_wo_na:  ['0.7686']
his_acc_wo_na:  ['0.7686']
cur_acc des_wo_na:  ['0.7486']
his_acc des_wo_na:  ['0.7486']
cur_acc rrf_wo_na:  ['0.7614']
his_acc rrf_wo_na:  ['0.7614']
cur_acc_w_na:  ['0.6490']
his_acc_w_na:  ['0.6490']
cur_acc des_w_na:  ['0.6044']
his_acc des_w_na:  ['0.6044']
cur_acc rrf_w_na:  ['0.6245']
his_acc rrf_w_na:  ['0.6245']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 82.9382965CurrentTrain: epoch  0, batch     1 | loss: 149.5928401CurrentTrain: epoch  0, batch     2 | loss: 112.3778729CurrentTrain: epoch  0, batch     3 | loss: 113.5412951CurrentTrain: epoch  0, batch     4 | loss: 23.0901328CurrentTrain: epoch  1, batch     0 | loss: 111.2566488CurrentTrain: epoch  1, batch     1 | loss: 88.5494721CurrentTrain: epoch  1, batch     2 | loss: 109.5450935CurrentTrain: epoch  1, batch     3 | loss: 86.5788015CurrentTrain: epoch  1, batch     4 | loss: 27.0386901CurrentTrain: epoch  2, batch     0 | loss: 134.1622342CurrentTrain: epoch  2, batch     1 | loss: 75.8961551CurrentTrain: epoch  2, batch     2 | loss: 86.8769187CurrentTrain: epoch  2, batch     3 | loss: 85.3024523CurrentTrain: epoch  2, batch     4 | loss: 19.5960175CurrentTrain: epoch  3, batch     0 | loss: 85.2053168CurrentTrain: epoch  3, batch     1 | loss: 84.8775625CurrentTrain: epoch  3, batch     2 | loss: 84.7193757CurrentTrain: epoch  3, batch     3 | loss: 103.9163239CurrentTrain: epoch  3, batch     4 | loss: 25.9705590CurrentTrain: epoch  4, batch     0 | loss: 86.7587362CurrentTrain: epoch  4, batch     1 | loss: 102.5390549CurrentTrain: epoch  4, batch     2 | loss: 69.2603613CurrentTrain: epoch  4, batch     3 | loss: 99.0717061CurrentTrain: epoch  4, batch     4 | loss: 14.0911919CurrentTrain: epoch  5, batch     0 | loss: 80.3426895CurrentTrain: epoch  5, batch     1 | loss: 80.6538188CurrentTrain: epoch  5, batch     2 | loss: 78.6035615CurrentTrain: epoch  5, batch     3 | loss: 128.2763077CurrentTrain: epoch  5, batch     4 | loss: 16.1098959CurrentTrain: epoch  6, batch     0 | loss: 79.1359331CurrentTrain: epoch  6, batch     1 | loss: 100.4427420CurrentTrain: epoch  6, batch     2 | loss: 79.6435652CurrentTrain: epoch  6, batch     3 | loss: 96.1951807CurrentTrain: epoch  6, batch     4 | loss: 25.5409682CurrentTrain: epoch  7, batch     0 | loss: 119.8205663CurrentTrain: epoch  7, batch     1 | loss: 91.3571194CurrentTrain: epoch  7, batch     2 | loss: 97.9983821CurrentTrain: epoch  7, batch     3 | loss: 79.2629263CurrentTrain: epoch  7, batch     4 | loss: 15.6494367CurrentTrain: epoch  8, batch     0 | loss: 93.5120630CurrentTrain: epoch  8, batch     1 | loss: 65.8002483CurrentTrain: epoch  8, batch     2 | loss: 80.2043825CurrentTrain: epoch  8, batch     3 | loss: 120.9660330CurrentTrain: epoch  8, batch     4 | loss: 15.4175945CurrentTrain: epoch  9, batch     0 | loss: 80.4600441CurrentTrain: epoch  9, batch     1 | loss: 76.4122924CurrentTrain: epoch  9, batch     2 | loss: 76.3967261CurrentTrain: epoch  9, batch     3 | loss: 65.2197810CurrentTrain: epoch  9, batch     4 | loss: 40.4716344
MemoryTrain:  epoch  0, batch     0 | loss: 2.2915869MemoryTrain:  epoch  1, batch     0 | loss: 1.8886189MemoryTrain:  epoch  2, batch     0 | loss: 1.5704907MemoryTrain:  epoch  3, batch     0 | loss: 1.4097200MemoryTrain:  epoch  4, batch     0 | loss: 1.0744942MemoryTrain:  epoch  5, batch     0 | loss: 0.8191913MemoryTrain:  epoch  6, batch     0 | loss: 0.6166217MemoryTrain:  epoch  7, batch     0 | loss: 0.5786514MemoryTrain:  epoch  8, batch     0 | loss: 0.5547426MemoryTrain:  epoch  9, batch     0 | loss: 0.4572981

F1 score per class: {32: 0.6, 2: 0.0, 6: 0.5, 39: 0.6626506024096386, 11: 0.0, 12: 0.0, 19: 0.24390243902439024, 24: 0.0, 28: 0.0, 29: 0.3125}
Micro-average F1 score: 0.5047619047619047
Weighted-average F1 score: 0.4478045783424926
F1 score per class: {32: 0.6666666666666666, 2: 0.0, 6: 0.2777777777777778, 39: 0.6077348066298343, 11: 0.0, 12: 0.0, 19: 0.3333333333333333, 24: 0.0, 28: 0.0, 29: 0.3225806451612903}
Micro-average F1 score: 0.42409638554216866
Weighted-average F1 score: 0.3959146897635072
F1 score per class: {32: 0.6363636363636364, 2: 0.0, 6: 0.2777777777777778, 39: 0.5942857142857143, 11: 0.0, 12: 0.0, 19: 0.2702702702702703, 24: 0.0, 28: 0.0, 29: 0.34285714285714286}
Micro-average F1 score: 0.4239401496259352
Weighted-average F1 score: 0.40350301778873204

F1 score per class: {32: 0.5454545454545454, 2: 0.6176470588235294, 6: 0.41420118343195267, 39: 0.5365853658536586, 11: 0.803921568627451, 12: 0.3783783783783784, 19: 0.7283236994219653, 24: 0.12658227848101267, 26: 0.8900523560209425, 28: 0.8088888888888889, 29: 0.2631578947368421}
Micro-average F1 score: 0.6425339366515838
Weighted-average F1 score: 0.6204834958275051
F1 score per class: {32: 0.5925925925925926, 2: 0.6370967741935484, 6: 0.25210084033613445, 39: 0.40441176470588236, 11: 0.7922705314009661, 12: 0.3103448275862069, 19: 0.7472527472527473, 24: 0.2564102564102564, 26: 0.8854166666666666, 28: 0.7811158798283262, 29: 0.2777777777777778}
Micro-average F1 score: 0.6224426534407935
Weighted-average F1 score: 0.6166655005428658
F1 score per class: {32: 0.5833333333333334, 2: 0.6425339366515838, 6: 0.25, 39: 0.41935483870967744, 11: 0.8058252427184466, 12: 0.3333333333333333, 19: 0.7415730337078652, 24: 0.1724137931034483, 26: 0.8842105263157894, 28: 0.7878787878787878, 29: 0.27906976744186046}
Micro-average F1 score: 0.6239590006406149
Weighted-average F1 score: 0.6156869858164701

F1 score per class: {32: 0.375, 2: 0.0, 6: 0.40229885057471265, 39: 0.5612244897959183, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.11904761904761904, 26: 0.0, 28: 0.0, 29: 0.16666666666666666}
Micro-average F1 score: 0.351575456053068
Weighted-average F1 score: 0.29639202573989365
F1 score per class: {32: 0.4, 2: 0.0, 6: 0.23255813953488372, 39: 0.4954954954954955, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.16129032258064516, 26: 0.0, 28: 0.0, 29: 0.2127659574468085}
Micro-average F1 score: 0.29042904290429045
Weighted-average F1 score: 0.25353046043018135
F1 score per class: {32: 0.4, 2: 0.0, 6: 0.22900763358778625, 39: 0.5, 11: 0.0, 12: 0.0, 19: 0.0, 24: 0.13157894736842105, 26: 0.0, 28: 0.0, 29: 0.19672131147540983}
Micro-average F1 score: 0.29259896729776247
Weighted-average F1 score: 0.25536076879743563

F1 score per class: {32: 0.34285714285714286, 2: 0.4, 6: 0.32710280373831774, 39: 0.32934131736526945, 11: 0.7627906976744186, 12: 0.25925925925925924, 19: 0.6774193548387096, 24: 0.06756756756756757, 26: 0.8173076923076923, 28: 0.6006600660066007, 29: 0.12658227848101267}
Micro-average F1 score: 0.47537063605930174
Weighted-average F1 score: 0.44047117212710735
F1 score per class: {32: 0.32653061224489793, 2: 0.38256658595641646, 6: 0.2097902097902098, 39: 0.25287356321839083, 11: 0.7488584474885844, 12: 0.2, 19: 0.6634146341463415, 24: 0.12658227848101267, 26: 0.8056872037914692, 28: 0.5889967637540453, 29: 0.16393442622950818}
Micro-average F1 score: 0.45347786811201446
Weighted-average F1 score: 0.42752351116971665
F1 score per class: {32: 0.35, 2: 0.4, 6: 0.2054794520547945, 39: 0.26804123711340205, 11: 0.7614678899082569, 12: 0.2153846153846154, 19: 0.673469387755102, 24: 0.08928571428571429, 26: 0.8115942028985508, 28: 0.5909090909090909, 29: 0.14285714285714285}
Micro-average F1 score: 0.4596507786691836
Weighted-average F1 score: 0.43099017262628725
cur_acc_wo_na:  ['0.7686', '0.5048']
his_acc_wo_na:  ['0.7686', '0.6425']
cur_acc des_wo_na:  ['0.7486', '0.4241']
his_acc des_wo_na:  ['0.7486', '0.6224']
cur_acc rrf_wo_na:  ['0.7614', '0.4239']
his_acc rrf_wo_na:  ['0.7614', '0.6240']
cur_acc_w_na:  ['0.6490', '0.3516']
his_acc_w_na:  ['0.6490', '0.4754']
cur_acc des_w_na:  ['0.6044', '0.2904']
his_acc des_w_na:  ['0.6044', '0.4535']
cur_acc rrf_w_na:  ['0.6245', '0.2926']
his_acc rrf_w_na:  ['0.6245', '0.4597']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 117.1565926CurrentTrain: epoch  0, batch     1 | loss: 80.4925564CurrentTrain: epoch  0, batch     2 | loss: 116.5786131CurrentTrain: epoch  0, batch     3 | loss: 10.2169272CurrentTrain: epoch  1, batch     0 | loss: 75.0117994CurrentTrain: epoch  1, batch     1 | loss: 74.3150930CurrentTrain: epoch  1, batch     2 | loss: 86.0317501CurrentTrain: epoch  1, batch     3 | loss: 11.9682935CurrentTrain: epoch  2, batch     0 | loss: 69.7799746CurrentTrain: epoch  2, batch     1 | loss: 84.8028210CurrentTrain: epoch  2, batch     2 | loss: 73.5638597CurrentTrain: epoch  2, batch     3 | loss: 12.6631714CurrentTrain: epoch  3, batch     0 | loss: 124.4884102CurrentTrain: epoch  3, batch     1 | loss: 81.5071881CurrentTrain: epoch  3, batch     2 | loss: 64.8359713CurrentTrain: epoch  3, batch     3 | loss: 17.7775614CurrentTrain: epoch  4, batch     0 | loss: 126.5374333CurrentTrain: epoch  4, batch     1 | loss: 64.4134009CurrentTrain: epoch  4, batch     2 | loss: 66.4201411CurrentTrain: epoch  4, batch     3 | loss: 6.2165243CurrentTrain: epoch  5, batch     0 | loss: 65.1817480CurrentTrain: epoch  5, batch     1 | loss: 94.6379518CurrentTrain: epoch  5, batch     2 | loss: 77.2997138CurrentTrain: epoch  5, batch     3 | loss: 10.0077086CurrentTrain: epoch  6, batch     0 | loss: 67.1171146CurrentTrain: epoch  6, batch     1 | loss: 91.8723252CurrentTrain: epoch  6, batch     2 | loss: 75.3621660CurrentTrain: epoch  6, batch     3 | loss: 19.0560685CurrentTrain: epoch  7, batch     0 | loss: 71.4065944CurrentTrain: epoch  7, batch     1 | loss: 92.6070765CurrentTrain: epoch  7, batch     2 | loss: 73.7799757CurrentTrain: epoch  7, batch     3 | loss: 9.6478908CurrentTrain: epoch  8, batch     0 | loss: 96.2237196CurrentTrain: epoch  8, batch     1 | loss: 75.1208897CurrentTrain: epoch  8, batch     2 | loss: 58.4844128CurrentTrain: epoch  8, batch     3 | loss: 9.0527789CurrentTrain: epoch  9, batch     0 | loss: 63.5027439CurrentTrain: epoch  9, batch     1 | loss: 91.4393060CurrentTrain: epoch  9, batch     2 | loss: 60.8643795CurrentTrain: epoch  9, batch     3 | loss: 9.3283244
MemoryTrain:  epoch  0, batch     0 | loss: 1.3080035MemoryTrain:  epoch  1, batch     0 | loss: 1.0750788MemoryTrain:  epoch  2, batch     0 | loss: 0.8434558MemoryTrain:  epoch  3, batch     0 | loss: 0.7072558MemoryTrain:  epoch  4, batch     0 | loss: 0.6797307MemoryTrain:  epoch  5, batch     0 | loss: 0.5053691MemoryTrain:  epoch  6, batch     0 | loss: 0.4218104MemoryTrain:  epoch  7, batch     0 | loss: 0.3561533MemoryTrain:  epoch  8, batch     0 | loss: 0.3090732MemoryTrain:  epoch  9, batch     0 | loss: 0.2701922

F1 score per class: {32: 0.0, 6: 0.75, 7: 0.9019607843137255, 40: 0.0, 9: 0.0, 19: 0.2222222222222222, 26: 0.0, 27: 0.0, 31: 0.3548387096774194}
Micro-average F1 score: 0.3875968992248062
Weighted-average F1 score: 0.31792429486728985
F1 score per class: {32: 0.0, 6: 0.75, 7: 0.7352941176470589, 40: 0.0, 9: 0.0, 12: 0.0, 19: 0.6, 26: 0.0, 27: 0.0, 31: 0.39344262295081966}
Micro-average F1 score: 0.4070175438596491
Weighted-average F1 score: 0.342562892283747
F1 score per class: {32: 0.0, 6: 0.75, 7: 0.8, 40: 0.0, 9: 0.0, 19: 0.5714285714285714, 26: 0.0, 27: 0.0, 31: 0.3870967741935484}
Micro-average F1 score: 0.4175824175824176
Weighted-average F1 score: 0.35030721966205836

F1 score per class: {32: 0.5263157894736842, 2: 0.20512820512820512, 6: 0.06, 7: 0.9019607843137255, 40: 0.43356643356643354, 11: 0.20634920634920634, 12: 0.639344262295082, 39: 0.18181818181818182, 9: 0.7251461988304093, 19: 0.18181818181818182, 24: 0.1917808219178082, 26: 0.8663101604278075, 27: 0.0, 28: 0.7922705314009661, 29: 0.23529411764705882, 31: 0.19469026548672566}
Micro-average F1 score: 0.49019607843137253
Weighted-average F1 score: 0.47134746205402483
F1 score per class: {32: 0.48484848484848486, 2: 0.33093525179856115, 6: 0.05555555555555555, 7: 0.7142857142857143, 40: 0.336, 11: 0.28402366863905326, 12: 0.6283524904214559, 39: 0.26666666666666666, 9: 0.7362637362637363, 19: 0.42857142857142855, 24: 0.2127659574468085, 26: 0.8829787234042553, 27: 0.0, 28: 0.7884615384615384, 29: 0.27586206896551724, 31: 0.24}
Micro-average F1 score: 0.5032751091703057
Weighted-average F1 score: 0.48100101038917126
F1 score per class: {32: 0.5714285714285714, 2: 0.3333333333333333, 6: 0.061224489795918366, 7: 0.7741935483870968, 40: 0.33587786259541985, 11: 0.3, 12: 0.6328125, 39: 0.2962962962962963, 9: 0.7303370786516854, 19: 0.42857142857142855, 24: 0.1694915254237288, 26: 0.8783068783068783, 27: 0.0, 28: 0.7902439024390244, 29: 0.2727272727272727, 31: 0.22119815668202766}
Micro-average F1 score: 0.5067567567567568
Weighted-average F1 score: 0.48350380150043054

F1 score per class: {32: 0.0, 6: 0.6, 7: 0.8363636363636363, 40: 0.0, 9: 0.0, 19: 0.21052631578947367, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.3235294117647059}
Micro-average F1 score: 0.3460207612456747
Weighted-average F1 score: 0.2886725905673274
F1 score per class: {32: 0.0, 6: 0.6666666666666666, 7: 0.6578947368421053, 40: 0.0, 9: 0.0, 12: 0.0, 19: 0.0, 24: 0.5454545454545454, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.375}
Micro-average F1 score: 0.3591331269349845
Weighted-average F1 score: 0.3012942316644155
F1 score per class: {32: 0.0, 6: 0.6666666666666666, 7: 0.75, 40: 0.0, 9: 0.0, 12: 0.0, 19: 0.5, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.3582089552238806}
Micro-average F1 score: 0.3737704918032787
Weighted-average F1 score: 0.31575108749446484

F1 score per class: {32: 0.37037037037037035, 2: 0.15894039735099338, 6: 0.03550295857988166, 7: 0.8363636363636363, 40: 0.34831460674157305, 11: 0.18571428571428572, 12: 0.6070038910505836, 39: 0.15384615384615385, 9: 0.6702702702702703, 19: 0.14814814814814814, 24: 0.0979020979020979, 26: 0.7941176470588235, 27: 0.0, 28: 0.6142322097378277, 29: 0.12121212121212122, 31: 0.16236162361623616}
Micro-average F1 score: 0.3953488372093023
Weighted-average F1 score: 0.3640093981797757
F1 score per class: {32: 0.2909090909090909, 2: 0.2358974358974359, 6: 0.031088082901554404, 7: 0.6172839506172839, 40: 0.2727272727272727, 11: 0.2096069868995633, 12: 0.5899280575539568, 39: 0.18181818181818182, 9: 0.67, 19: 0.34285714285714286, 24: 0.09615384615384616, 26: 0.8137254901960784, 27: 0.0, 28: 0.6283524904214559, 29: 0.14285714285714285, 31: 0.20869565217391303}
Micro-average F1 score: 0.39267461669505965
Weighted-average F1 score: 0.36054478537574147
F1 score per class: {32: 0.375, 2: 0.23783783783783785, 6: 0.034482758620689655, 7: 0.7272727272727273, 40: 0.2716049382716049, 11: 0.24277456647398843, 12: 0.5955882352941176, 39: 0.2222222222222222, 9: 0.6666666666666666, 19: 0.3333333333333333, 24: 0.07936507936507936, 26: 0.8097560975609757, 27: 0.0, 28: 0.627906976744186, 29: 0.13043478260869565, 31: 0.18532818532818532}
Micro-average F1 score: 0.400355871886121
Weighted-average F1 score: 0.36663748399787754
cur_acc_wo_na:  ['0.7686', '0.5048', '0.3876']
his_acc_wo_na:  ['0.7686', '0.6425', '0.4902']
cur_acc des_wo_na:  ['0.7486', '0.4241', '0.4070']
his_acc des_wo_na:  ['0.7486', '0.6224', '0.5033']
cur_acc rrf_wo_na:  ['0.7614', '0.4239', '0.4176']
his_acc rrf_wo_na:  ['0.7614', '0.6240', '0.5068']
cur_acc_w_na:  ['0.6490', '0.3516', '0.3460']
his_acc_w_na:  ['0.6490', '0.4754', '0.3953']
cur_acc des_w_na:  ['0.6044', '0.2904', '0.3591']
his_acc des_w_na:  ['0.6044', '0.4535', '0.3927']
cur_acc rrf_w_na:  ['0.6245', '0.2926', '0.3738']
his_acc rrf_w_na:  ['0.6245', '0.4597', '0.4004']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 90.1855753CurrentTrain: epoch  0, batch     1 | loss: 93.6445563CurrentTrain: epoch  0, batch     2 | loss: 120.1720932CurrentTrain: epoch  0, batch     3 | loss: 131.1677790CurrentTrain: epoch  1, batch     0 | loss: 76.5536483CurrentTrain: epoch  1, batch     1 | loss: 108.6520478CurrentTrain: epoch  1, batch     2 | loss: 85.5830978CurrentTrain: epoch  1, batch     3 | loss: 74.9643887CurrentTrain: epoch  2, batch     0 | loss: 87.7487769CurrentTrain: epoch  2, batch     1 | loss: 73.4331532CurrentTrain: epoch  2, batch     2 | loss: 70.9110575CurrentTrain: epoch  2, batch     3 | loss: 89.8316703CurrentTrain: epoch  3, batch     0 | loss: 103.9234112CurrentTrain: epoch  3, batch     1 | loss: 82.9275322CurrentTrain: epoch  3, batch     2 | loss: 83.5323168CurrentTrain: epoch  3, batch     3 | loss: 57.6967866CurrentTrain: epoch  4, batch     0 | loss: 81.8954065CurrentTrain: epoch  4, batch     1 | loss: 98.3406694CurrentTrain: epoch  4, batch     2 | loss: 126.0638915CurrentTrain: epoch  4, batch     3 | loss: 51.5925462CurrentTrain: epoch  5, batch     0 | loss: 99.2915625CurrentTrain: epoch  5, batch     1 | loss: 76.7398507CurrentTrain: epoch  5, batch     2 | loss: 99.7987411CurrentTrain: epoch  5, batch     3 | loss: 44.4298909CurrentTrain: epoch  6, batch     0 | loss: 126.7514228CurrentTrain: epoch  6, batch     1 | loss: 88.8279224CurrentTrain: epoch  6, batch     2 | loss: 79.0363573CurrentTrain: epoch  6, batch     3 | loss: 63.0399826CurrentTrain: epoch  7, batch     0 | loss: 80.2784232CurrentTrain: epoch  7, batch     1 | loss: 64.3225154CurrentTrain: epoch  7, batch     2 | loss: 80.2762149CurrentTrain: epoch  7, batch     3 | loss: 52.4146137CurrentTrain: epoch  8, batch     0 | loss: 96.4734080CurrentTrain: epoch  8, batch     1 | loss: 66.1329402CurrentTrain: epoch  8, batch     2 | loss: 76.4050204CurrentTrain: epoch  8, batch     3 | loss: 63.9966388CurrentTrain: epoch  9, batch     0 | loss: 77.7923506CurrentTrain: epoch  9, batch     1 | loss: 93.3485878CurrentTrain: epoch  9, batch     2 | loss: 63.0492023CurrentTrain: epoch  9, batch     3 | loss: 80.6134348
MemoryTrain:  epoch  0, batch     0 | loss: 1.1058413MemoryTrain:  epoch  1, batch     0 | loss: 0.9614311MemoryTrain:  epoch  2, batch     0 | loss: 0.7690889MemoryTrain:  epoch  3, batch     0 | loss: 0.6412857MemoryTrain:  epoch  4, batch     0 | loss: 0.5204571MemoryTrain:  epoch  5, batch     0 | loss: 0.4270666MemoryTrain:  epoch  6, batch     0 | loss: 0.4040123MemoryTrain:  epoch  7, batch     0 | loss: 0.3130110MemoryTrain:  epoch  8, batch     0 | loss: 0.3217818MemoryTrain:  epoch  9, batch     0 | loss: 0.2600935

F1 score per class: {32: 0.0, 2: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.8, 7: 0.0, 11: 0.4857142857142857, 12: 0.0, 40: 0.0, 6: 0.0, 15: 0.78, 19: 0.5365853658536586, 25: 0.6046511627906976, 28: 0.0, 31: 0.0}
Micro-average F1 score: 0.5173210161662818
Weighted-average F1 score: 0.43895475273059525
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 15: 0.64, 19: 0.0, 24: 0.0, 25: 0.7415730337078652, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 35: 0.7924528301886793, 37: 0.5087719298245614, 38: 0.6666666666666666, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5048543689320388
Weighted-average F1 score: 0.39945899825295544
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.5333333333333333, 19: 0.0, 24: 0.0, 25: 0.6666666666666666, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 35: 0.780952380952381, 37: 0.5081967213114754, 38: 0.6530612244897959, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4939759036144578
Weighted-average F1 score: 0.39526569362634933

F1 score per class: {2: 0.5384615384615384, 6: 0.3333333333333333, 7: 0.05309734513274336, 9: 0.8928571428571429, 11: 0.40718562874251496, 12: 0.20155038759689922, 15: 0.5, 19: 0.5963636363636363, 24: 0.24, 25: 0.4857142857142857, 26: 0.7386363636363636, 27: 0.3333333333333333, 28: 0.1917808219178082, 29: 0.9015544041450777, 31: 0.15384615384615385, 32: 0.7639484978540773, 35: 0.4727272727272727, 37: 0.24719101123595505, 38: 0.37681159420289856, 39: 0.125, 40: 0.18556701030927836}
Micro-average F1 score: 0.4653225806451613
Weighted-average F1 score: 0.44236052387685254
F1 score per class: {2: 0.22580645161290322, 6: 0.3804347826086957, 7: 0.04878048780487805, 9: 0.5681818181818182, 11: 0.4722222222222222, 12: 0.3523809523809524, 15: 0.32653061224489793, 19: 0.5754385964912281, 24: 0.28, 25: 0.7415730337078652, 26: 0.7351351351351352, 27: 0.38095238095238093, 28: 0.13592233009708737, 29: 0.8877551020408163, 31: 0.1111111111111111, 32: 0.7398373983739838, 35: 0.5060240963855421, 37: 0.27751196172248804, 38: 0.375, 39: 0.11764705882352941, 40: 0.1588785046728972}
Micro-average F1 score: 0.461376404494382
Weighted-average F1 score: 0.42859662055354075
F1 score per class: {2: 0.3181818181818182, 6: 0.3780487804878049, 7: 0.05128205128205128, 9: 0.7352941176470589, 11: 0.4723618090452261, 12: 0.28205128205128205, 15: 0.26666666666666666, 19: 0.5774647887323944, 24: 0.2, 25: 0.6666666666666666, 26: 0.7472527472527473, 27: 0.3673469387755102, 28: 0.12612612612612611, 29: 0.8911917098445595, 31: 0.11764705882352941, 32: 0.7489711934156379, 35: 0.47126436781609193, 37: 0.2551440329218107, 38: 0.3855421686746988, 39: 0.11764705882352941, 40: 0.1459227467811159}
Micro-average F1 score: 0.4534206695778748
Weighted-average F1 score: 0.4192424287860673

F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.6666666666666666, 19: 0.0, 24: 0.0, 25: 0.4358974358974359, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6141732283464567, 37: 0.44594594594594594, 38: 0.5098039215686274, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3882149046793761
Weighted-average F1 score: 0.32529937622425276
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 15: 0.5, 19: 0.0, 24: 0.0, 25: 0.66, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.65625, 37: 0.4461538461538462, 38: 0.46153846153846156, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.36262203626220363
Weighted-average F1 score: 0.28588942307692305
F1 score per class: {2: 0.0, 6: 0.0, 7: 0.0, 11: 0.0, 12: 0.0, 15: 0.43243243243243246, 19: 0.0, 24: 0.0, 25: 0.5869565217391305, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6307692307692307, 37: 0.4397163120567376, 38: 0.45714285714285713, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3649851632047478
Weighted-average F1 score: 0.29556390497379675

F1 score per class: {2: 0.358974358974359, 6: 0.23834196891191708, 7: 0.027906976744186046, 9: 0.819672131147541, 11: 0.3383084577114428, 12: 0.16666666666666666, 15: 0.30303030303030304, 19: 0.5559322033898305, 24: 0.1935483870967742, 25: 0.4358974358974359, 26: 0.6701030927835051, 27: 0.22950819672131148, 28: 0.09271523178807947, 29: 0.7909090909090909, 31: 0.07692307692307693, 32: 0.5836065573770491, 35: 0.2932330827067669, 37: 0.14831460674157304, 38: 0.23636363636363636, 39: 0.1111111111111111, 40: 0.1469387755102041}
Micro-average F1 score: 0.341824644549763
Weighted-average F1 score: 0.3114938748847034
F1 score per class: {2: 0.14, 6: 0.24647887323943662, 7: 0.02553191489361702, 9: 0.47619047619047616, 11: 0.3541666666666667, 12: 0.22023809523809523, 15: 0.19753086419753085, 19: 0.5222929936305732, 24: 0.18421052631578946, 25: 0.66, 26: 0.6325581395348837, 27: 0.24242424242424243, 28: 0.07106598984771574, 29: 0.7699115044247787, 31: 0.05555555555555555, 32: 0.5705329153605015, 35: 0.328125, 37: 0.1657142857142857, 38: 0.21301775147928995, 39: 0.1111111111111111, 40: 0.12734082397003746}
Micro-average F1 score: 0.32540861812778604
Weighted-average F1 score: 0.296907152099204
F1 score per class: {2: 0.23333333333333334, 6: 0.2605042016806723, 7: 0.02643171806167401, 9: 0.6666666666666666, 11: 0.36153846153846153, 12: 0.2018348623853211, 15: 0.16666666666666666, 19: 0.5239616613418531, 24: 0.13953488372093023, 25: 0.5869565217391305, 26: 0.6699507389162561, 27: 0.23076923076923078, 28: 0.06635071090047394, 29: 0.7713004484304933, 31: 0.058823529411764705, 32: 0.5723270440251572, 35: 0.30036630036630035, 37: 0.15196078431372548, 38: 0.2119205298013245, 39: 0.1, 40: 0.11371237458193979}
Micro-average F1 score: 0.32447916666666665
Weighted-average F1 score: 0.29296276780824215
cur_acc_wo_na:  ['0.7686', '0.5048', '0.3876', '0.5173']
his_acc_wo_na:  ['0.7686', '0.6425', '0.4902', '0.4653']
cur_acc des_wo_na:  ['0.7486', '0.4241', '0.4070', '0.5049']
his_acc des_wo_na:  ['0.7486', '0.6224', '0.5033', '0.4614']
cur_acc rrf_wo_na:  ['0.7614', '0.4239', '0.4176', '0.4940']
his_acc rrf_wo_na:  ['0.7614', '0.6240', '0.5068', '0.4534']
cur_acc_w_na:  ['0.6490', '0.3516', '0.3460', '0.3882']
his_acc_w_na:  ['0.6490', '0.4754', '0.3953', '0.3418']
cur_acc des_w_na:  ['0.6044', '0.2904', '0.3591', '0.3626']
his_acc des_w_na:  ['0.6044', '0.4535', '0.3927', '0.3254']
cur_acc rrf_w_na:  ['0.6245', '0.2926', '0.3738', '0.3650']
his_acc rrf_w_na:  ['0.6245', '0.4597', '0.4004', '0.3245']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 142.8784472CurrentTrain: epoch  0, batch     1 | loss: 100.6805161CurrentTrain: epoch  0, batch     2 | loss: 102.2240911CurrentTrain: epoch  0, batch     3 | loss: 113.2297153CurrentTrain: epoch  0, batch     4 | loss: 79.6350488CurrentTrain: epoch  1, batch     0 | loss: 89.6628248CurrentTrain: epoch  1, batch     1 | loss: 112.4879741CurrentTrain: epoch  1, batch     2 | loss: 77.1676182CurrentTrain: epoch  1, batch     3 | loss: 106.6055352CurrentTrain: epoch  1, batch     4 | loss: 105.2749299CurrentTrain: epoch  2, batch     0 | loss: 87.6680999CurrentTrain: epoch  2, batch     1 | loss: 90.4045682CurrentTrain: epoch  2, batch     2 | loss: 76.2254531CurrentTrain: epoch  2, batch     3 | loss: 86.5705932CurrentTrain: epoch  2, batch     4 | loss: 101.7108276CurrentTrain: epoch  3, batch     0 | loss: 69.0240686CurrentTrain: epoch  3, batch     1 | loss: 103.1177397CurrentTrain: epoch  3, batch     2 | loss: 87.3910179CurrentTrain: epoch  3, batch     3 | loss: 104.6163832CurrentTrain: epoch  3, batch     4 | loss: 150.6722008CurrentTrain: epoch  4, batch     0 | loss: 81.3114803CurrentTrain: epoch  4, batch     1 | loss: 103.8861875CurrentTrain: epoch  4, batch     2 | loss: 84.2619526CurrentTrain: epoch  4, batch     3 | loss: 82.0522725CurrentTrain: epoch  4, batch     4 | loss: 70.7384049CurrentTrain: epoch  5, batch     0 | loss: 127.2279624CurrentTrain: epoch  5, batch     1 | loss: 67.3522748CurrentTrain: epoch  5, batch     2 | loss: 97.9524681CurrentTrain: epoch  5, batch     3 | loss: 97.3295471CurrentTrain: epoch  5, batch     4 | loss: 69.9988659CurrentTrain: epoch  6, batch     0 | loss: 98.2176696CurrentTrain: epoch  6, batch     1 | loss: 70.2182085CurrentTrain: epoch  6, batch     2 | loss: 99.1134283CurrentTrain: epoch  6, batch     3 | loss: 98.9151712CurrentTrain: epoch  6, batch     4 | loss: 36.3824128CurrentTrain: epoch  7, batch     0 | loss: 95.1223287CurrentTrain: epoch  7, batch     1 | loss: 80.2042124CurrentTrain: epoch  7, batch     2 | loss: 78.1202154CurrentTrain: epoch  7, batch     3 | loss: 97.8662650CurrentTrain: epoch  7, batch     4 | loss: 53.7141399CurrentTrain: epoch  8, batch     0 | loss: 95.6328623CurrentTrain: epoch  8, batch     1 | loss: 66.3405252CurrentTrain: epoch  8, batch     2 | loss: 95.8312390CurrentTrain: epoch  8, batch     3 | loss: 76.4781887CurrentTrain: epoch  8, batch     4 | loss: 52.4911409CurrentTrain: epoch  9, batch     0 | loss: 76.3980421CurrentTrain: epoch  9, batch     1 | loss: 63.3139135CurrentTrain: epoch  9, batch     2 | loss: 81.0130016CurrentTrain: epoch  9, batch     3 | loss: 120.7412811CurrentTrain: epoch  9, batch     4 | loss: 69.9687437
MemoryTrain:  epoch  0, batch     0 | loss: 1.5120168MemoryTrain:  epoch  1, batch     0 | loss: 1.2998295MemoryTrain:  epoch  2, batch     0 | loss: 1.0840005MemoryTrain:  epoch  3, batch     0 | loss: 0.8655443MemoryTrain:  epoch  4, batch     0 | loss: 0.7456074MemoryTrain:  epoch  5, batch     0 | loss: 0.7004153MemoryTrain:  epoch  6, batch     0 | loss: 0.5659426MemoryTrain:  epoch  7, batch     0 | loss: 0.4448861MemoryTrain:  epoch  8, batch     0 | loss: 0.3648046MemoryTrain:  epoch  9, batch     0 | loss: 0.3099382

F1 score per class: {1: 0.2, 3: 0.6753246753246753, 7: 0.0, 11: 0.0, 14: 0.11904761904761904, 19: 0.0, 22: 0.5426356589147286, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.7047619047619048, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.36639676113360325
Weighted-average F1 score: 0.3060727962620878
F1 score per class: {1: 0.21839080459770116, 2: 0.0, 3: 0.5818181818181818, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.05555555555555555, 15: 0.0, 19: 0.0, 22: 0.553030303030303, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5903614457831325, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.31893687707641194
Weighted-average F1 score: 0.27347776936476564
F1 score per class: {1: 0.21428571428571427, 2: 0.0, 3: 0.5680473372781065, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.06, 19: 0.0, 22: 0.5421245421245421, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.6818181818181818, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3263888888888889
Weighted-average F1 score: 0.27757613436609346

F1 score per class: {1: 0.16037735849056603, 2: 0.6363636363636364, 3: 0.4642857142857143, 6: 0.25396825396825395, 7: 0.047244094488188976, 9: 0.819672131147541, 11: 0.2275449101796407, 12: 0.05309734513274336, 14: 0.10309278350515463, 15: 0.6956521739130435, 19: 0.5362318840579711, 22: 0.46357615894039733, 24: 0.0, 25: 0.3939393939393939, 26: 0.7415730337078652, 27: 0.0, 28: 0.17721518987341772, 29: 0.8787878787878788, 31: 0.0, 32: 0.6160714285714286, 34: 0.39361702127659576, 35: 0.21311475409836064, 37: 0.14218009478672985, 38: 0.2898550724637681, 39: 0.2222222222222222, 40: 0.24864864864864866}
Micro-average F1 score: 0.3842925659472422
Weighted-average F1 score: 0.37651099883979966
F1 score per class: {1: 0.16740088105726872, 2: 0.2857142857142857, 3: 0.3794466403162055, 6: 0.35353535353535354, 7: 0.05504587155963303, 9: 0.5747126436781609, 11: 0.18333333333333332, 12: 0.25252525252525254, 14: 0.04, 15: 0.36363636363636365, 19: 0.54, 22: 0.4605678233438486, 24: 0.06896551724137931, 25: 0.735632183908046, 26: 0.7272727272727273, 27: 0.0, 28: 0.11023622047244094, 29: 0.8557213930348259, 31: 0.06451612903225806, 32: 0.5787545787545788, 34: 0.2076271186440678, 35: 0.21739130434782608, 37: 0.125, 38: 0.35714285714285715, 39: 0.125, 40: 0.2011173184357542}
Micro-average F1 score: 0.3559155621011291
Weighted-average F1 score: 0.33546306071300314
F1 score per class: {1: 0.16363636363636364, 2: 0.4, 3: 0.3595505617977528, 6: 0.38323353293413176, 7: 0.05217391304347826, 9: 0.78125, 11: 0.20833333333333334, 12: 0.23776223776223776, 14: 0.04411764705882353, 15: 0.42857142857142855, 19: 0.5346534653465347, 22: 0.43529411764705883, 24: 0.0, 25: 0.6829268292682927, 26: 0.7391304347826086, 27: 0.0, 28: 0.1076923076923077, 29: 0.86, 31: 0.09090909090909091, 32: 0.5914396887159533, 34: 0.2710843373493976, 35: 0.22093023255813954, 37: 0.10309278350515463, 38: 0.34782608695652173, 39: 0.23529411764705882, 40: 0.18556701030927836}
Micro-average F1 score: 0.3627476202727039
Weighted-average F1 score: 0.34151998031245917

F1 score per class: {1: 0.11604095563139932, 2: 0.0, 3: 0.5531914893617021, 6: 0.0, 7: 0.0, 11: 0.0, 14: 0.10869565217391304, 19: 0.0, 22: 0.41297935103244837, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5401459854014599, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2558303886925795
Weighted-average F1 score: 0.2180852622103089
F1 score per class: {1: 0.12582781456953643, 2: 0.0, 3: 0.41379310344827586, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.046511627906976744, 15: 0.0, 19: 0.0, 22: 0.4147727272727273, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.43171806167400884, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.20949263502454993
Weighted-average F1 score: 0.18552151317221882
F1 score per class: {1: 0.12162162162162163, 2: 0.0, 3: 0.40336134453781514, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.05217391304347826, 15: 0.0, 19: 0.0, 22: 0.3978494623655914, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.4712041884816754, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.2151029748283753
Weighted-average F1 score: 0.19120075386818566

F1 score per class: {1: 0.09090909090909091, 2: 0.3684210526315789, 3: 0.3291139240506329, 6: 0.18181818181818182, 7: 0.02531645569620253, 9: 0.7575757575757576, 11: 0.17194570135746606, 12: 0.04580152671755725, 14: 0.08264462809917356, 15: 0.5161290322580645, 19: 0.49169435215946844, 22: 0.33175355450236965, 24: 0.0, 25: 0.3611111111111111, 26: 0.6700507614213198, 27: 0.0, 28: 0.0880503144654088, 29: 0.7532467532467533, 31: 0.0, 32: 0.4791666666666667, 34: 0.22699386503067484, 35: 0.14772727272727273, 37: 0.09202453987730061, 38: 0.21739130434782608, 39: 0.2, 40: 0.215962441314554}
Micro-average F1 score: 0.27905964301262515
Weighted-average F1 score: 0.26211592667653927
F1 score per class: {1: 0.09336609336609336, 2: 0.16279069767441862, 3: 0.24120603015075376, 6: 0.21406727828746178, 7: 0.028846153846153848, 9: 0.43478260869565216, 11: 0.16666666666666666, 12: 0.1718213058419244, 14: 0.029850746268656716, 15: 0.2553191489361702, 19: 0.4879518072289157, 22: 0.31947483588621445, 24: 0.06060606060606061, 25: 0.6274509803921569, 26: 0.6267281105990783, 27: 0.0, 28: 0.06086956521739131, 29: 0.7350427350427351, 31: 0.027777777777777776, 32: 0.41688654353562005, 34: 0.13333333333333333, 35: 0.14336917562724014, 37: 0.08695652173913043, 38: 0.19900497512437812, 39: 0.125, 40: 0.17391304347826086}
Micro-average F1 score: 0.24601289446895147
Weighted-average F1 score: 0.22753051466436805
F1 score per class: {1: 0.08977556109725686, 2: 0.25925925925925924, 3: 0.22857142857142856, 6: 0.2549800796812749, 7: 0.02654867256637168, 9: 0.7142857142857143, 11: 0.17751479289940827, 12: 0.19101123595505617, 14: 0.03409090909090909, 15: 0.2857142857142857, 19: 0.4864864864864865, 22: 0.298989898989899, 24: 0.0, 25: 0.6086956521739131, 26: 0.6476190476190476, 27: 0.0, 28: 0.05785123966942149, 29: 0.7381974248927039, 31: 0.041666666666666664, 32: 0.4355300859598854, 34: 0.1618705035971223, 35: 0.14339622641509434, 37: 0.07168458781362007, 38: 0.19875776397515527, 39: 0.21052631578947367, 40: 0.15859030837004406}
Micro-average F1 score: 0.25391680172879527
Weighted-average F1 score: 0.2328965371543594
cur_acc_wo_na:  ['0.7686', '0.5048', '0.3876', '0.5173', '0.3664']
his_acc_wo_na:  ['0.7686', '0.6425', '0.4902', '0.4653', '0.3843']
cur_acc des_wo_na:  ['0.7486', '0.4241', '0.4070', '0.5049', '0.3189']
his_acc des_wo_na:  ['0.7486', '0.6224', '0.5033', '0.4614', '0.3559']
cur_acc rrf_wo_na:  ['0.7614', '0.4239', '0.4176', '0.4940', '0.3264']
his_acc rrf_wo_na:  ['0.7614', '0.6240', '0.5068', '0.4534', '0.3627']
cur_acc_w_na:  ['0.6490', '0.3516', '0.3460', '0.3882', '0.2558']
his_acc_w_na:  ['0.6490', '0.4754', '0.3953', '0.3418', '0.2791']
cur_acc des_w_na:  ['0.6044', '0.2904', '0.3591', '0.3626', '0.2095']
his_acc des_w_na:  ['0.6044', '0.4535', '0.3927', '0.3254', '0.2460']
cur_acc rrf_w_na:  ['0.6245', '0.2926', '0.3738', '0.3650', '0.2151']
his_acc rrf_w_na:  ['0.6245', '0.4597', '0.4004', '0.3245', '0.2539']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 86.6431380CurrentTrain: epoch  0, batch     1 | loss: 87.0274301CurrentTrain: epoch  0, batch     2 | loss: 145.8641002CurrentTrain: epoch  0, batch     3 | loss: 80.8001368CurrentTrain: epoch  1, batch     0 | loss: 130.6803219CurrentTrain: epoch  1, batch     1 | loss: 79.9393786CurrentTrain: epoch  1, batch     2 | loss: 88.2855907CurrentTrain: epoch  1, batch     3 | loss: 113.7751107CurrentTrain: epoch  2, batch     0 | loss: 107.3583253CurrentTrain: epoch  2, batch     1 | loss: 105.5257044CurrentTrain: epoch  2, batch     2 | loss: 72.3163057CurrentTrain: epoch  2, batch     3 | loss: 70.4608457CurrentTrain: epoch  3, batch     0 | loss: 85.0880791CurrentTrain: epoch  3, batch     1 | loss: 175.7788549CurrentTrain: epoch  3, batch     2 | loss: 70.3859757CurrentTrain: epoch  3, batch     3 | loss: 66.4170110CurrentTrain: epoch  4, batch     0 | loss: 99.9816775CurrentTrain: epoch  4, batch     1 | loss: 79.7851092CurrentTrain: epoch  4, batch     2 | loss: 84.1808031CurrentTrain: epoch  4, batch     3 | loss: 68.8812765CurrentTrain: epoch  5, batch     0 | loss: 78.8484628CurrentTrain: epoch  5, batch     1 | loss: 101.8307853CurrentTrain: epoch  5, batch     2 | loss: 81.7726021CurrentTrain: epoch  5, batch     3 | loss: 63.0126137CurrentTrain: epoch  6, batch     0 | loss: 123.7850918CurrentTrain: epoch  6, batch     1 | loss: 75.5679473CurrentTrain: epoch  6, batch     2 | loss: 80.8210070CurrentTrain: epoch  6, batch     3 | loss: 65.4686926CurrentTrain: epoch  7, batch     0 | loss: 67.5916613CurrentTrain: epoch  7, batch     1 | loss: 80.7947553CurrentTrain: epoch  7, batch     2 | loss: 79.8454104CurrentTrain: epoch  7, batch     3 | loss: 63.3375660CurrentTrain: epoch  8, batch     0 | loss: 66.7291794CurrentTrain: epoch  8, batch     1 | loss: 65.2036759CurrentTrain: epoch  8, batch     2 | loss: 77.0831579CurrentTrain: epoch  8, batch     3 | loss: 136.0911415CurrentTrain: epoch  9, batch     0 | loss: 63.8468452CurrentTrain: epoch  9, batch     1 | loss: 64.9745322CurrentTrain: epoch  9, batch     2 | loss: 79.5525884CurrentTrain: epoch  9, batch     3 | loss: 64.9677429
MemoryTrain:  epoch  0, batch     0 | loss: 1.2495854MemoryTrain:  epoch  1, batch     0 | loss: 1.0555230MemoryTrain:  epoch  2, batch     0 | loss: 0.8720126MemoryTrain:  epoch  3, batch     0 | loss: 0.6923714MemoryTrain:  epoch  4, batch     0 | loss: 0.6807621MemoryTrain:  epoch  5, batch     0 | loss: 0.4930664MemoryTrain:  epoch  6, batch     0 | loss: 0.4390084MemoryTrain:  epoch  7, batch     0 | loss: 0.4034267MemoryTrain:  epoch  8, batch     0 | loss: 0.3439752MemoryTrain:  epoch  9, batch     0 | loss: 0.3331260

F1 score per class: {0: 0.8780487804878049, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7607361963190185, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.37037037037037035, 14: 0.0, 19: 0.0, 21: 0.08888888888888889, 22: 0.0, 23: 0.8387096774193549, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.0}
Micro-average F1 score: 0.5373134328358209
Weighted-average F1 score: 0.4285183963415579
F1 score per class: {0: 0.7128712871287128, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.6987951807228916, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.6153846153846154, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.5205479452054794, 22: 0.0, 23: 0.7058823529411765, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.4342688330871492
Weighted-average F1 score: 0.318060972291948
F1 score per class: {0: 0.7346938775510204, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7204968944099379, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.4444444444444444, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.4927536231884058, 22: 0.0, 23: 0.7142857142857143, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.4517133956386293
Weighted-average F1 score: 0.3310370236424812

F1 score per class: {0: 0.4235294117647059, 1: 0.18181818181818182, 2: 0.27906976744186046, 3: 0.48, 4: 0.7607361963190185, 6: 0.1935483870967742, 7: 0.041666666666666664, 9: 0.819672131147541, 11: 0.22598870056497175, 12: 0.017241379310344827, 13: 0.07194244604316546, 14: 0.07228915662650602, 15: 0.6666666666666666, 19: 0.541095890410959, 21: 0.05333333333333334, 22: 0.4738675958188153, 23: 0.7647058823529411, 24: 0.0, 25: 0.4411764705882353, 26: 0.6982248520710059, 27: 0.0, 28: 0.19047619047619047, 29: 0.8442211055276382, 31: 0.125, 32: 0.5849802371541502, 34: 0.4, 35: 0.2033898305084746, 37: 0.36893203883495146, 38: 0.34615384615384615, 39: 0.125, 40: 0.24836601307189543}
Micro-average F1 score: 0.40105124835742445
Weighted-average F1 score: 0.3885852065847948
F1 score per class: {0: 0.277992277992278, 1: 0.16535433070866143, 2: 0.15384615384615385, 3: 0.3406113537117904, 4: 0.6863905325443787, 6: 0.3568075117370892, 7: 0.05504587155963303, 9: 0.49019607843137253, 11: 0.2074074074074074, 12: 0.24742268041237114, 13: 0.10666666666666667, 14: 0.038461538461538464, 15: 0.6, 19: 0.48823529411764705, 21: 0.20765027322404372, 22: 0.45136186770428016, 23: 0.631578947368421, 24: 0.0, 25: 0.7058823529411765, 26: 0.6842105263157895, 27: 0.0, 28: 0.10619469026548672, 29: 0.8177339901477833, 31: 0.13333333333333333, 32: 0.535031847133758, 34: 0.2569832402234637, 35: 0.26200873362445415, 37: 0.17391304347826086, 38: 0.45569620253164556, 39: 0.23529411764705882, 40: 0.22459893048128343}
Micro-average F1 score: 0.3580170089193113
Weighted-average F1 score: 0.3334414458613537
F1 score per class: {0: 0.288, 1: 0.17254901960784313, 2: 0.1891891891891892, 3: 0.32786885245901637, 4: 0.7204968944099379, 6: 0.3850267379679144, 7: 0.05217391304347826, 9: 0.7352941176470589, 11: 0.23255813953488372, 12: 0.18543046357615894, 13: 0.08163265306122448, 14: 0.07246376811594203, 15: 0.6, 19: 0.5045592705167173, 21: 0.17801047120418848, 22: 0.4280442804428044, 23: 0.6185567010309279, 24: 0.0, 25: 0.6987951807228916, 26: 0.6739130434782609, 27: 0.0, 28: 0.10416666666666667, 29: 0.8177339901477833, 31: 0.09090909090909091, 32: 0.543046357615894, 34: 0.37104072398190047, 35: 0.2153846153846154, 37: 0.2033898305084746, 38: 0.42857142857142855, 39: 0.23529411764705882, 40: 0.22105263157894736}
Micro-average F1 score: 0.36714129244249727
Weighted-average F1 score: 0.341264511467349

F1 score per class: {0: 0.8, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7294117647058823, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.18867924528301888, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.08333333333333333, 22: 0.0, 23: 0.7358490566037735, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.4027972027972028
Weighted-average F1 score: 0.297671087493507
F1 score per class: {0: 0.6206896551724138, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.6666666666666666, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.25806451612903225, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.3877551020408163, 22: 0.0, 23: 0.594059405940594, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.315450643776824
Weighted-average F1 score: 0.22912400740005004
F1 score per class: {0: 0.631578947368421, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.6863905325443787, 6: 0.0, 7: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.20512820512820512, 14: 0.0, 15: 0.0, 19: 0.0, 21: 0.3617021276595745, 22: 0.0, 23: 0.5882352941176471, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.327683615819209
Weighted-average F1 score: 0.2377549772726516

F1 score per class: {0: 0.2748091603053435, 1: 0.09821428571428571, 2: 0.18461538461538463, 3: 0.34782608695652173, 4: 0.7167630057803468, 6: 0.1348314606741573, 7: 0.023346303501945526, 9: 0.7352941176470589, 11: 0.17316017316017315, 12: 0.015625, 13: 0.034722222222222224, 14: 0.061224489795918366, 15: 0.5217391304347826, 19: 0.4952978056426332, 21: 0.041237113402061855, 22: 0.33170731707317075, 23: 0.65, 24: 0.0, 25: 0.410958904109589, 26: 0.6344086021505376, 27: 0.0, 28: 0.12903225806451613, 29: 0.7058823529411765, 31: 0.058823529411764705, 32: 0.4180790960451977, 34: 0.2719298245614035, 35: 0.14285714285714285, 37: 0.2585034013605442, 38: 0.23376623376623376, 39: 0.1111111111111111, 40: 0.2261904761904762}
Micro-average F1 score: 0.29284206486279024
Weighted-average F1 score: 0.2716424795396706
F1 score per class: {0: 0.19047619047619047, 1: 0.09012875536480687, 2: 0.10606060606060606, 3: 0.21666666666666667, 4: 0.6105263157894737, 6: 0.2111111111111111, 7: 0.030456852791878174, 9: 0.3816793893129771, 11: 0.17177914110429449, 12: 0.16666666666666666, 13: 0.049689440993788817, 14: 0.02926829268292683, 15: 0.46153846153846156, 19: 0.4256410256410256, 21: 0.14393939393939395, 22: 0.3178082191780822, 23: 0.5128205128205128, 24: 0.0, 25: 0.6185567010309279, 26: 0.6018518518518519, 27: 0.0, 28: 0.0625, 29: 0.6747967479674797, 31: 0.06557377049180328, 32: 0.3775280898876405, 34: 0.15807560137457044, 35: 0.17094017094017094, 37: 0.14414414414414414, 38: 0.2748091603053435, 39: 0.23529411764705882, 40: 0.19004524886877827}
Micro-average F1 score: 0.24981907656679694
Weighted-average F1 score: 0.22940463434183095
F1 score per class: {0: 0.19672131147540983, 1: 0.09341825902335456, 2: 0.1320754716981132, 3: 0.20833333333333334, 4: 0.6666666666666666, 6: 0.23920265780730898, 7: 0.0273972602739726, 9: 0.6578947368421053, 11: 0.18518518518518517, 12: 0.1414141414141414, 13: 0.03619909502262444, 14: 0.056179775280898875, 15: 0.46153846153846156, 19: 0.43915343915343913, 21: 0.12318840579710146, 22: 0.2966751918158568, 23: 0.49586776859504134, 24: 0.0, 25: 0.6236559139784946, 26: 0.6078431372549019, 27: 0.0, 28: 0.06896551724137931, 29: 0.680327868852459, 31: 0.043478260869565216, 32: 0.3840749414519906, 34: 0.2152230971128609, 35: 0.13636363636363635, 37: 0.15584415584415584, 38: 0.2777777777777778, 39: 0.23529411764705882, 40: 0.1875}
Micro-average F1 score: 0.2578064913090294
Weighted-average F1 score: 0.2347169964558222
cur_acc_wo_na:  ['0.7686', '0.5048', '0.3876', '0.5173', '0.3664', '0.5373']
his_acc_wo_na:  ['0.7686', '0.6425', '0.4902', '0.4653', '0.3843', '0.4011']
cur_acc des_wo_na:  ['0.7486', '0.4241', '0.4070', '0.5049', '0.3189', '0.4343']
his_acc des_wo_na:  ['0.7486', '0.6224', '0.5033', '0.4614', '0.3559', '0.3580']
cur_acc rrf_wo_na:  ['0.7614', '0.4239', '0.4176', '0.4940', '0.3264', '0.4517']
his_acc rrf_wo_na:  ['0.7614', '0.6240', '0.5068', '0.4534', '0.3627', '0.3671']
cur_acc_w_na:  ['0.6490', '0.3516', '0.3460', '0.3882', '0.2558', '0.4028']
his_acc_w_na:  ['0.6490', '0.4754', '0.3953', '0.3418', '0.2791', '0.2928']
cur_acc des_w_na:  ['0.6044', '0.2904', '0.3591', '0.3626', '0.2095', '0.3155']
his_acc des_w_na:  ['0.6044', '0.4535', '0.3927', '0.3254', '0.2460', '0.2498']
cur_acc rrf_w_na:  ['0.6245', '0.2926', '0.3738', '0.3650', '0.2151', '0.3277']
his_acc rrf_w_na:  ['0.6245', '0.4597', '0.4004', '0.3245', '0.2539', '0.2578']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 111.5020086CurrentTrain: epoch  0, batch     1 | loss: 93.8595942CurrentTrain: epoch  0, batch     2 | loss: 91.9889100CurrentTrain: epoch  0, batch     3 | loss: 85.6518058CurrentTrain: epoch  1, batch     0 | loss: 108.7031287CurrentTrain: epoch  1, batch     1 | loss: 89.9600691CurrentTrain: epoch  1, batch     2 | loss: 75.4679165CurrentTrain: epoch  1, batch     3 | loss: 51.3685267CurrentTrain: epoch  2, batch     0 | loss: 73.7367535CurrentTrain: epoch  2, batch     1 | loss: 83.2022808CurrentTrain: epoch  2, batch     2 | loss: 85.0714534CurrentTrain: epoch  2, batch     3 | loss: 59.8165115CurrentTrain: epoch  3, batch     0 | loss: 68.6441202CurrentTrain: epoch  3, batch     1 | loss: 83.1049543CurrentTrain: epoch  3, batch     2 | loss: 68.9774917CurrentTrain: epoch  3, batch     3 | loss: 59.5095571CurrentTrain: epoch  4, batch     0 | loss: 125.4054737CurrentTrain: epoch  4, batch     1 | loss: 81.9166247CurrentTrain: epoch  4, batch     2 | loss: 67.5619495CurrentTrain: epoch  4, batch     3 | loss: 47.4925415CurrentTrain: epoch  5, batch     0 | loss: 98.5926858CurrentTrain: epoch  5, batch     1 | loss: 70.1393206CurrentTrain: epoch  5, batch     2 | loss: 93.1394814CurrentTrain: epoch  5, batch     3 | loss: 45.3215329CurrentTrain: epoch  6, batch     0 | loss: 64.8834237CurrentTrain: epoch  6, batch     1 | loss: 100.4472416CurrentTrain: epoch  6, batch     2 | loss: 92.9030575CurrentTrain: epoch  6, batch     3 | loss: 55.7696539CurrentTrain: epoch  7, batch     0 | loss: 64.4272095CurrentTrain: epoch  7, batch     1 | loss: 75.6914910CurrentTrain: epoch  7, batch     2 | loss: 81.0329973CurrentTrain: epoch  7, batch     3 | loss: 46.7150014CurrentTrain: epoch  8, batch     0 | loss: 77.1766065CurrentTrain: epoch  8, batch     1 | loss: 75.3355937CurrentTrain: epoch  8, batch     2 | loss: 77.6334376CurrentTrain: epoch  8, batch     3 | loss: 56.6025368CurrentTrain: epoch  9, batch     0 | loss: 76.0738408CurrentTrain: epoch  9, batch     1 | loss: 74.2606103CurrentTrain: epoch  9, batch     2 | loss: 63.3294399CurrentTrain: epoch  9, batch     3 | loss: 72.6282165
MemoryTrain:  epoch  0, batch     0 | loss: 0.7148117MemoryTrain:  epoch  1, batch     0 | loss: 0.5699795MemoryTrain:  epoch  2, batch     0 | loss: 0.4939811MemoryTrain:  epoch  3, batch     0 | loss: 0.4088785MemoryTrain:  epoch  4, batch     0 | loss: 0.3443124MemoryTrain:  epoch  5, batch     0 | loss: 0.3100533MemoryTrain:  epoch  6, batch     0 | loss: 0.2881961MemoryTrain:  epoch  7, batch     0 | loss: 0.2140952MemoryTrain:  epoch  8, batch     0 | loss: 0.2028561MemoryTrain:  epoch  9, batch     0 | loss: 0.2159985

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 7: 0.0, 8: 0.6470588235294118, 11: 0.0, 13: 0.0, 20: 0.7796610169491526, 22: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8333333333333334, 32: 0.0, 33: 0.375, 36: 0.6788990825688074, 37: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.6081871345029239
Weighted-average F1 score: 0.5415633602694638
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.691358024691358, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.7543859649122807, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8780487804878049, 31: 0.0, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 35: 0.0, 36: 0.7175572519083969, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5475409836065573
Weighted-average F1 score: 0.4478832934801795
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.6589595375722543, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.7413793103448276, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8717948717948718, 32: 0.0, 33: 0.3157894736842105, 34: 0.0, 35: 0.0, 36: 0.72, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5699481865284974
Weighted-average F1 score: 0.48572416069941904

F1 score per class: {0: 0.4585987261146497, 1: 0.1752988047808765, 2: 0.35294117647058826, 3: 0.45918367346938777, 4: 0.7261146496815286, 6: 0.12389380530973451, 7: 0.042105263157894736, 8: 0.2669902912621359, 9: 0.7352941176470589, 11: 0.20125786163522014, 12: 0.01834862385321101, 13: 0.034482758620689655, 14: 0.04395604395604396, 15: 0.631578947368421, 19: 0.5163398692810458, 20: 0.44660194174757284, 21: 0.0761904761904762, 22: 0.5214007782101168, 23: 0.7474747474747475, 24: 0.0, 25: 0.3225806451612903, 26: 0.6627218934911243, 27: 0.0, 28: 0.36363636363636365, 29: 0.8217821782178217, 30: 0.8333333333333334, 31: 0.1, 32: 0.584, 33: 0.09090909090909091, 34: 0.26865671641791045, 35: 0.208955223880597, 36: 0.556390977443609, 37: 0.29545454545454547, 38: 0.43243243243243246, 39: 0.2222222222222222, 40: 0.24489795918367346}
Micro-average F1 score: 0.39195979899497485
Weighted-average F1 score: 0.38509869560894733
F1 score per class: {0: 0.21864951768488747, 1: 0.1908713692946058, 2: 0.1590909090909091, 3: 0.38022813688212925, 4: 0.7885714285714286, 6: 0.3391812865497076, 7: 0.04938271604938271, 8: 0.28717948717948716, 9: 0.4672897196261682, 11: 0.23333333333333334, 12: 0.24875621890547264, 13: 0.044444444444444446, 14: 0.04918032786885246, 15: 0.6, 19: 0.4398976982097187, 20: 0.4574468085106383, 21: 0.16049382716049382, 22: 0.45614035087719296, 23: 0.64, 24: 0.0, 25: 0.4057971014492754, 26: 0.6736842105263158, 27: 0.0, 28: 0.1276595744680851, 29: 0.8038277511961722, 30: 0.5373134328358209, 31: 0.046511627906976744, 32: 0.5151515151515151, 33: 0.07692307692307693, 34: 0.358974358974359, 35: 0.2938388625592417, 36: 0.5108695652173914, 37: 0.21428571428571427, 38: 0.5573770491803278, 39: 0.19047619047619047, 40: 0.20134228187919462}
Micro-average F1 score: 0.3708007989831124
Weighted-average F1 score: 0.353624866416469
F1 score per class: {0: 0.31390134529147984, 1: 0.19008264462809918, 2: 0.24561403508771928, 3: 0.39357429718875503, 4: 0.8439306358381503, 6: 0.27941176470588236, 7: 0.04597701149425287, 8: 0.2620689655172414, 9: 0.704225352112676, 11: 0.3037974683544304, 12: 0.2191780821917808, 13: 0.029411764705882353, 14: 0.046875, 15: 0.6, 19: 0.45901639344262296, 20: 0.4321608040201005, 21: 0.13872832369942195, 22: 0.45993031358885017, 23: 0.6597938144329897, 24: 0.0, 25: 0.36923076923076925, 26: 0.6631016042780749, 27: 0.0, 28: 0.42857142857142855, 29: 0.8, 30: 0.576271186440678, 31: 0.0625, 32: 0.5222929936305732, 33: 0.06896551724137931, 34: 0.432, 35: 0.23255813953488372, 36: 0.5232558139534884, 37: 0.2608695652173913, 38: 0.47619047619047616, 39: 0.21052631578947367, 40: 0.18633540372670807}
Micro-average F1 score: 0.3795591964111566
Weighted-average F1 score: 0.3606916953980277

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.4888888888888889, 11: 0.0, 12: 0.0, 13: 0.0, 19: 0.0, 20: 0.5822784810126582, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.7894736842105263, 31: 0.0, 32: 0.0, 33: 0.3333333333333333, 36: 0.5362318840579711, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4250681198910082
Weighted-average F1 score: 0.37156857059805903
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.5283018867924528, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.5657894736842105, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8181818181818182, 31: 0.0, 32: 0.0, 33: 0.2222222222222222, 34: 0.0, 35: 0.0, 36: 0.6064516129032258, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3694690265486726
Weighted-average F1 score: 0.30109838060824967
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.0, 8: 0.48717948717948717, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 19: 0.0, 20: 0.5548387096774193, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8292682926829268, 31: 0.0, 32: 0.0, 33: 0.2222222222222222, 34: 0.0, 35: 0.0, 36: 0.6122448979591837, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.38961038961038963
Weighted-average F1 score: 0.3278708862746018

F1 score per class: {0: 0.3130434782608696, 1: 0.09586056644880174, 2: 0.21428571428571427, 3: 0.3146853146853147, 4: 0.6909090909090909, 6: 0.09333333333333334, 7: 0.023121387283236993, 8: 0.14725568942436412, 9: 0.6578947368421053, 11: 0.1839080459770115, 12: 0.017241379310344827, 13: 0.015625, 14: 0.03773584905660377, 15: 0.5, 19: 0.45272206303724927, 20: 0.25136612021857924, 21: 0.05673758865248227, 22: 0.3817663817663818, 23: 0.6548672566371682, 24: 0.0, 25: 0.3076923076923077, 26: 0.5989304812834224, 27: 0.0, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.7692307692307693, 31: 0.045454545454545456, 32: 0.4112676056338028, 33: 0.06521739130434782, 34: 0.24324324324324326, 35: 0.14358974358974358, 36: 0.387434554973822, 37: 0.2549019607843137, 38: 0.3018867924528302, 39: 0.18181818181818182, 40: 0.20689655172413793}
Micro-average F1 score: 0.28071323409128085
Weighted-average F1 score: 0.2628538950131677
F1 score per class: {0: 0.14255765199161424, 1: 0.10244988864142539, 2: 0.1111111111111111, 3: 0.2364066193853428, 4: 0.7340425531914894, 6: 0.2116788321167883, 7: 0.028169014084507043, 8: 0.16470588235294117, 9: 0.38461538461538464, 11: 0.2153846153846154, 12: 0.1607717041800643, 13: 0.02040816326530612, 14: 0.0392156862745098, 15: 0.46153846153846156, 19: 0.3706896551724138, 20: 0.25903614457831325, 21: 0.11555555555555555, 22: 0.325, 23: 0.5039370078740157, 24: 0.0, 25: 0.3783783783783784, 26: 0.6009389671361502, 27: 0.0, 28: 0.07228915662650602, 29: 0.6412213740458015, 30: 0.375, 31: 0.02197802197802198, 32: 0.3601694915254237, 33: 0.048, 34: 0.21428571428571427, 35: 0.17971014492753623, 36: 0.3700787401574803, 37: 0.16551724137931034, 38: 0.34, 39: 0.13793103448275862, 40: 0.16759776536312848}
Micro-average F1 score: 0.2533813128179675
Weighted-average F1 score: 0.23731455040468255
F1 score per class: {0: 0.2017291066282421, 1: 0.10043668122270742, 2: 0.16279069767441862, 3: 0.2538860103626943, 4: 0.8021978021978022, 6: 0.18536585365853658, 7: 0.025974025974025976, 8: 0.14960629921259844, 9: 0.6097560975609756, 11: 0.22966507177033493, 12: 0.14883720930232558, 13: 0.015037593984962405, 14: 0.03773584905660377, 15: 0.4444444444444444, 19: 0.3916083916083916, 20: 0.24431818181818182, 21: 0.100418410041841, 22: 0.3375959079283887, 23: 0.5245901639344263, 24: 0.0, 25: 0.34782608695652173, 26: 0.5933014354066986, 27: 0.0, 28: 0.20689655172413793, 29: 0.6456692913385826, 30: 0.4, 31: 0.03076923076923077, 32: 0.37701149425287356, 33: 0.045112781954887216, 34: 0.31952662721893493, 35: 0.14336917562724014, 36: 0.3813559322033898, 37: 0.2112676056338028, 38: 0.3125, 39: 0.17391304347826086, 40: 0.15306122448979592}
Micro-average F1 score: 0.26397178513293545
Weighted-average F1 score: 0.24526181626565066
cur_acc_wo_na:  ['0.7686', '0.5048', '0.3876', '0.5173', '0.3664', '0.5373', '0.6082']
his_acc_wo_na:  ['0.7686', '0.6425', '0.4902', '0.4653', '0.3843', '0.4011', '0.3920']
cur_acc des_wo_na:  ['0.7486', '0.4241', '0.4070', '0.5049', '0.3189', '0.4343', '0.5475']
his_acc des_wo_na:  ['0.7486', '0.6224', '0.5033', '0.4614', '0.3559', '0.3580', '0.3708']
cur_acc rrf_wo_na:  ['0.7614', '0.4239', '0.4176', '0.4940', '0.3264', '0.4517', '0.5699']
his_acc rrf_wo_na:  ['0.7614', '0.6240', '0.5068', '0.4534', '0.3627', '0.3671', '0.3796']
cur_acc_w_na:  ['0.6490', '0.3516', '0.3460', '0.3882', '0.2558', '0.4028', '0.4251']
his_acc_w_na:  ['0.6490', '0.4754', '0.3953', '0.3418', '0.2791', '0.2928', '0.2807']
cur_acc des_w_na:  ['0.6044', '0.2904', '0.3591', '0.3626', '0.2095', '0.3155', '0.3695']
his_acc des_w_na:  ['0.6044', '0.4535', '0.3927', '0.3254', '0.2460', '0.2498', '0.2534']
cur_acc rrf_w_na:  ['0.6245', '0.2926', '0.3738', '0.3650', '0.2151', '0.3277', '0.3896']
his_acc rrf_w_na:  ['0.6245', '0.4597', '0.4004', '0.3245', '0.2539', '0.2578', '0.2640']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 124.8207863CurrentTrain: epoch  0, batch     1 | loss: 145.0326359CurrentTrain: epoch  0, batch     2 | loss: 109.2668584CurrentTrain: epoch  0, batch     3 | loss: 84.3749606CurrentTrain: epoch  0, batch     4 | loss: 63.5202050CurrentTrain: epoch  1, batch     0 | loss: 88.5160708CurrentTrain: epoch  1, batch     1 | loss: 90.4916908CurrentTrain: epoch  1, batch     2 | loss: 107.7321706CurrentTrain: epoch  1, batch     3 | loss: 131.9947799CurrentTrain: epoch  1, batch     4 | loss: 111.4190042CurrentTrain: epoch  2, batch     0 | loss: 110.6767608CurrentTrain: epoch  2, batch     1 | loss: 127.8808200CurrentTrain: epoch  2, batch     2 | loss: 101.6152771CurrentTrain: epoch  2, batch     3 | loss: 103.8845691CurrentTrain: epoch  2, batch     4 | loss: 52.6866341CurrentTrain: epoch  3, batch     0 | loss: 80.5861983CurrentTrain: epoch  3, batch     1 | loss: 103.9357996CurrentTrain: epoch  3, batch     2 | loss: 81.9274261CurrentTrain: epoch  3, batch     3 | loss: 102.0145997CurrentTrain: epoch  3, batch     4 | loss: 87.0548533CurrentTrain: epoch  4, batch     0 | loss: 82.9710282CurrentTrain: epoch  4, batch     1 | loss: 69.5585026CurrentTrain: epoch  4, batch     2 | loss: 103.3574267CurrentTrain: epoch  4, batch     3 | loss: 80.9030648CurrentTrain: epoch  4, batch     4 | loss: 169.5560388CurrentTrain: epoch  5, batch     0 | loss: 81.0717991CurrentTrain: epoch  5, batch     1 | loss: 82.2647086CurrentTrain: epoch  5, batch     2 | loss: 82.4506496CurrentTrain: epoch  5, batch     3 | loss: 100.9771764CurrentTrain: epoch  5, batch     4 | loss: 51.6199216CurrentTrain: epoch  6, batch     0 | loss: 79.4908292CurrentTrain: epoch  6, batch     1 | loss: 83.2557208CurrentTrain: epoch  6, batch     2 | loss: 67.2541656CurrentTrain: epoch  6, batch     3 | loss: 95.4302788CurrentTrain: epoch  6, batch     4 | loss: 79.8323241CurrentTrain: epoch  7, batch     0 | loss: 96.0890756CurrentTrain: epoch  7, batch     1 | loss: 78.1306112CurrentTrain: epoch  7, batch     2 | loss: 127.0697939CurrentTrain: epoch  7, batch     3 | loss: 65.9101814CurrentTrain: epoch  7, batch     4 | loss: 60.7006268CurrentTrain: epoch  8, batch     0 | loss: 95.7402622CurrentTrain: epoch  8, batch     1 | loss: 78.0450363CurrentTrain: epoch  8, batch     2 | loss: 95.6724064CurrentTrain: epoch  8, batch     3 | loss: 76.7758010CurrentTrain: epoch  8, batch     4 | loss: 49.8251890CurrentTrain: epoch  9, batch     0 | loss: 96.8616053CurrentTrain: epoch  9, batch     1 | loss: 92.2554370CurrentTrain: epoch  9, batch     2 | loss: 73.9372636CurrentTrain: epoch  9, batch     3 | loss: 97.2671738CurrentTrain: epoch  9, batch     4 | loss: 58.8899725
MemoryTrain:  epoch  0, batch     0 | loss: 0.8938609MemoryTrain:  epoch  1, batch     0 | loss: 0.7576269MemoryTrain:  epoch  2, batch     0 | loss: 0.5556735MemoryTrain:  epoch  3, batch     0 | loss: 0.4567721MemoryTrain:  epoch  4, batch     0 | loss: 0.3614983MemoryTrain:  epoch  5, batch     0 | loss: 0.3131677MemoryTrain:  epoch  6, batch     0 | loss: 0.3181773MemoryTrain:  epoch  7, batch     0 | loss: 0.2432357MemoryTrain:  epoch  8, batch     0 | loss: 0.2355643MemoryTrain:  epoch  9, batch     0 | loss: 0.2045386

F1 score per class: {0: 0.0, 2: 0.0, 3: 0.0, 5: 0.8899521531100478, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.5818181818181818, 11: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.8, 17: 0.0, 18: 0.304, 20: 0.0, 23: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 37: 0.0}
Micro-average F1 score: 0.5832012678288431
Weighted-average F1 score: 0.5227463845272592
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.6666666666666666, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.6075949367088608, 11: 0.0, 12: 0.0, 13: 0.0, 16: 0.75, 17: 0.8, 18: 0.2823529411764706, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4734982332155477
Weighted-average F1 score: 0.4091217815687921
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.7058823529411765, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.6049382716049383, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.7761194029850746, 17: 0.8, 18: 0.28735632183908044, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5043695380774033
Weighted-average F1 score: 0.4415279458053149

F1 score per class: {0: 0.49624060150375937, 1: 0.1640625, 2: 0.3448275862068966, 3: 0.425531914893617, 4: 0.7012987012987013, 5: 0.7782426778242678, 6: 0.03669724770642202, 7: 0.05, 8: 0.25757575757575757, 9: 0.704225352112676, 10: 0.26666666666666666, 11: 0.1984732824427481, 12: 0.018691588785046728, 13: 0.04878048780487805, 14: 0.02631578947368421, 15: 0.5555555555555556, 16: 0.6857142857142857, 17: 0.0, 18: 0.15447154471544716, 19: 0.512987012987013, 20: 0.44776119402985076, 21: 0.14, 22: 0.5134099616858238, 23: 0.7524752475247525, 24: 0.0, 25: 0.3225806451612903, 26: 0.6467065868263473, 27: 0.0, 28: 0.3076923076923077, 29: 0.8159203980099502, 30: 0.7777777777777778, 31: 0.14285714285714285, 32: 0.5418326693227091, 33: 0.12, 34: 0.07017543859649122, 35: 0.19672131147540983, 36: 0.3170731707317073, 37: 0.17073170731707318, 38: 0.13333333333333333, 39: 0.2, 40: 0.21374045801526717}
Micro-average F1 score: 0.38416908412764195
Weighted-average F1 score: 0.38865952548672955
F1 score per class: {0: 0.2517482517482518, 1: 0.1686746987951807, 2: 0.20588235294117646, 3: 0.3173076923076923, 4: 0.8135593220338984, 5: 0.4987012987012987, 6: 0.3487179487179487, 7: 0.06451612903225806, 8: 0.36220472440944884, 9: 0.373134328358209, 10: 0.3392226148409894, 11: 0.22950819672131148, 12: 0.32388663967611336, 13: 0.125, 14: 0.05084745762711865, 15: 0.6, 16: 0.5625, 17: 0.375, 18: 0.1509433962264151, 19: 0.41075794621026895, 20: 0.4083769633507853, 21: 0.1509433962264151, 22: 0.4281150159744409, 23: 0.5904761904761905, 24: 0.0, 25: 0.3888888888888889, 26: 0.6666666666666666, 27: 0.0, 28: 0.17391304347826086, 29: 0.783410138248848, 30: 0.4186046511627907, 31: 0.06896551724137931, 32: 0.4748603351955307, 33: 0.10526315789473684, 34: 0.4594594594594595, 35: 0.2764227642276423, 36: 0.549618320610687, 37: 0.25862068965517243, 38: 0.4, 39: 0.13333333333333333, 40: 0.14545454545454545}
Micro-average F1 score: 0.3680641183723798
Weighted-average F1 score: 0.3527177181971532
F1 score per class: {0: 0.3431372549019608, 1: 0.1774193548387097, 2: 0.2857142857142857, 3: 0.3222748815165877, 4: 0.8165680473372781, 5: 0.5663716814159292, 6: 0.16176470588235295, 7: 0.05357142857142857, 8: 0.31718061674008813, 9: 0.6578947368421053, 10: 0.32131147540983607, 11: 0.22900763358778625, 12: 0.2484472049689441, 13: 0.05128205128205128, 14: 0.05172413793103448, 15: 0.5714285714285714, 16: 0.5909090909090909, 17: 0.5714285714285714, 18: 0.13054830287206268, 19: 0.450402144772118, 20: 0.3561643835616438, 21: 0.16352201257861634, 22: 0.45307443365695793, 23: 0.6138613861386139, 24: 0.0, 25: 0.35294117647058826, 26: 0.6666666666666666, 27: 0.0, 28: 0.4444444444444444, 29: 0.8246445497630331, 30: 0.5151515151515151, 31: 0.08333333333333333, 32: 0.5128205128205128, 33: 0.11428571428571428, 34: 0.3380281690140845, 35: 0.2840909090909091, 36: 0.42857142857142855, 37: 0.2692307692307692, 38: 0.3783783783783784, 39: 0.16666666666666666, 40: 0.11904761904761904}
Micro-average F1 score: 0.3690840994211781
Weighted-average F1 score: 0.35484549195167897

F1 score per class: {0: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.7237354085603113, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.463768115942029, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.5052631578947369, 17: 0.0, 18: 0.2087912087912088, 20: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 37: 0.0, 39: 0.0}
Micro-average F1 score: 0.3978378378378378
Weighted-average F1 score: 0.3487927243261158
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.5052631578947369, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.5052631578947369, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.4778761061946903, 17: 0.5217391304347826, 18: 0.19672131147540983, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.30970724191063176
Weighted-average F1 score: 0.2687247989595976
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.5423728813559322, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.49246231155778897, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.49056603773584906, 17: 0.5714285714285714, 18: 0.1984126984126984, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.33416046319272125
Weighted-average F1 score: 0.2929425215352899

F1 score per class: {0: 0.34375, 1: 0.08955223880597014, 2: 0.2, 3: 0.29739776951672864, 4: 0.6708074534161491, 5: 0.5360230547550432, 6: 0.02877697841726619, 7: 0.026200873362445413, 8: 0.20606060606060606, 9: 0.625, 10: 0.17679558011049723, 11: 0.18571428571428572, 12: 0.01652892561983471, 13: 0.028985507246376812, 14: 0.02197802197802198, 15: 0.45454545454545453, 16: 0.3870967741935484, 17: 0.0, 18: 0.095, 19: 0.4450704225352113, 20: 0.2222222222222222, 21: 0.09929078014184398, 22: 0.3806818181818182, 23: 0.672566371681416, 24: 0.0, 25: 0.3076923076923077, 26: 0.5806451612903226, 27: 0.0, 28: 0.23529411764705882, 29: 0.6533864541832669, 30: 0.7, 31: 0.0625, 32: 0.39766081871345027, 33: 0.08571428571428572, 34: 0.06779661016949153, 35: 0.13872832369942195, 36: 0.2549019607843137, 37: 0.13725490196078433, 38: 0.12121212121212122, 39: 0.14285714285714285, 40: 0.1830065359477124}
Micro-average F1 score: 0.2771300448430493
Weighted-average F1 score: 0.2669178995678471
F1 score per class: {0: 0.16628175519630484, 1: 0.09190371991247265, 2: 0.13333333333333333, 3: 0.19584569732937684, 4: 0.7422680412371134, 5: 0.3018867924528302, 6: 0.20987654320987653, 7: 0.033707865168539325, 8: 0.23958333333333334, 9: 0.2857142857142857, 10: 0.21428571428571427, 11: 0.21374045801526717, 12: 0.1724137931034483, 13: 0.08333333333333333, 14: 0.0392156862745098, 15: 0.48, 16: 0.32926829268292684, 17: 0.18181818181818182, 18: 0.09486166007905138, 19: 0.3387096774193548, 20: 0.23353293413173654, 21: 0.11162790697674418, 22: 0.3004484304932735, 23: 0.44285714285714284, 24: 0.0, 25: 0.3684210526315789, 26: 0.5688888888888889, 27: 0.0, 28: 0.0963855421686747, 29: 0.5944055944055944, 30: 0.27692307692307694, 31: 0.031746031746031744, 32: 0.3373015873015873, 33: 0.064, 34: 0.288135593220339, 35: 0.17435897435897435, 36: 0.4067796610169492, 37: 0.18404907975460122, 38: 0.24175824175824176, 39: 0.06666666666666667, 40: 0.11594202898550725}
Micro-average F1 score: 0.2462363373891524
Weighted-average F1 score: 0.23402201361236674
F1 score per class: {0: 0.21604938271604937, 1: 0.09649122807017543, 2: 0.18666666666666668, 3: 0.2138364779874214, 4: 0.770949720670391, 5: 0.35887850467289717, 6: 0.10784313725490197, 7: 0.026905829596412557, 8: 0.21238938053097345, 9: 0.5617977528089888, 10: 0.2024793388429752, 11: 0.21739130434782608, 12: 0.16597510373443983, 13: 0.04081632653061224, 14: 0.04081632653061224, 15: 0.46153846153846156, 16: 0.33766233766233766, 17: 0.2553191489361702, 18: 0.08196721311475409, 19: 0.37250554323725055, 20: 0.20207253886010362, 21: 0.12037037037037036, 22: 0.3181818181818182, 23: 0.4626865671641791, 24: 0.0, 25: 0.3380281690140845, 26: 0.5904761904761905, 27: 0.0, 28: 0.19047619047619047, 29: 0.6444444444444445, 30: 0.34, 31: 0.04, 32: 0.36199095022624433, 33: 0.06896551724137931, 34: 0.2696629213483146, 35: 0.18115942028985507, 36: 0.30434782608695654, 37: 0.1958041958041958, 38: 0.27450980392156865, 39: 0.125, 40: 0.09433962264150944}
Micro-average F1 score: 0.25344867898059387
Weighted-average F1 score: 0.23923210646657123
cur_acc_wo_na:  ['0.7686', '0.5048', '0.3876', '0.5173', '0.3664', '0.5373', '0.6082', '0.5832']
his_acc_wo_na:  ['0.7686', '0.6425', '0.4902', '0.4653', '0.3843', '0.4011', '0.3920', '0.3842']
cur_acc des_wo_na:  ['0.7486', '0.4241', '0.4070', '0.5049', '0.3189', '0.4343', '0.5475', '0.4735']
his_acc des_wo_na:  ['0.7486', '0.6224', '0.5033', '0.4614', '0.3559', '0.3580', '0.3708', '0.3681']
cur_acc rrf_wo_na:  ['0.7614', '0.4239', '0.4176', '0.4940', '0.3264', '0.4517', '0.5699', '0.5044']
his_acc rrf_wo_na:  ['0.7614', '0.6240', '0.5068', '0.4534', '0.3627', '0.3671', '0.3796', '0.3691']
cur_acc_w_na:  ['0.6490', '0.3516', '0.3460', '0.3882', '0.2558', '0.4028', '0.4251', '0.3978']
his_acc_w_na:  ['0.6490', '0.4754', '0.3953', '0.3418', '0.2791', '0.2928', '0.2807', '0.2771']
cur_acc des_w_na:  ['0.6044', '0.2904', '0.3591', '0.3626', '0.2095', '0.3155', '0.3695', '0.3097']
his_acc des_w_na:  ['0.6044', '0.4535', '0.3927', '0.3254', '0.2460', '0.2498', '0.2534', '0.2462']
cur_acc rrf_w_na:  ['0.6245', '0.2926', '0.3738', '0.3650', '0.2151', '0.3277', '0.3896', '0.3342']
his_acc rrf_w_na:  ['0.6245', '0.4597', '0.4004', '0.3245', '0.2539', '0.2578', '0.2640', '0.2534']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 129.3004379CurrentTrain: epoch  0, batch     1 | loss: 89.8706462CurrentTrain: epoch  0, batch     2 | loss: 78.5453616CurrentTrain: epoch  0, batch     3 | loss: 120.6638838CurrentTrain: epoch  0, batch     4 | loss: 77.4670345CurrentTrain: epoch  0, batch     5 | loss: 148.6213970CurrentTrain: epoch  0, batch     6 | loss: 87.9486794CurrentTrain: epoch  0, batch     7 | loss: 100.2376841CurrentTrain: epoch  0, batch     8 | loss: 86.4261345CurrentTrain: epoch  0, batch     9 | loss: 119.0365575CurrentTrain: epoch  0, batch    10 | loss: 90.3385735CurrentTrain: epoch  0, batch    11 | loss: 100.4251385CurrentTrain: epoch  0, batch    12 | loss: 86.2470075CurrentTrain: epoch  0, batch    13 | loss: 146.7566809CurrentTrain: epoch  0, batch    14 | loss: 100.2443359CurrentTrain: epoch  0, batch    15 | loss: 100.5337085CurrentTrain: epoch  0, batch    16 | loss: 99.5033363CurrentTrain: epoch  0, batch    17 | loss: 76.0592658CurrentTrain: epoch  0, batch    18 | loss: 86.9618747CurrentTrain: epoch  0, batch    19 | loss: 99.0564395CurrentTrain: epoch  0, batch    20 | loss: 118.7343053CurrentTrain: epoch  0, batch    21 | loss: 119.2424026CurrentTrain: epoch  0, batch    22 | loss: 99.3444931CurrentTrain: epoch  0, batch    23 | loss: 99.0136827CurrentTrain: epoch  0, batch    24 | loss: 86.4898473CurrentTrain: epoch  0, batch    25 | loss: 117.5609955CurrentTrain: epoch  0, batch    26 | loss: 99.2943373CurrentTrain: epoch  0, batch    27 | loss: 98.8033229CurrentTrain: epoch  0, batch    28 | loss: 85.7582527CurrentTrain: epoch  0, batch    29 | loss: 98.4809561CurrentTrain: epoch  0, batch    30 | loss: 98.6522930CurrentTrain: epoch  0, batch    31 | loss: 117.4696908CurrentTrain: epoch  0, batch    32 | loss: 83.6037298CurrentTrain: epoch  0, batch    33 | loss: 98.0159560CurrentTrain: epoch  0, batch    34 | loss: 117.1272727CurrentTrain: epoch  0, batch    35 | loss: 99.1723931CurrentTrain: epoch  0, batch    36 | loss: 98.6011562CurrentTrain: epoch  0, batch    37 | loss: 97.5095871CurrentTrain: epoch  0, batch    38 | loss: 84.6423965CurrentTrain: epoch  0, batch    39 | loss: 97.5944142CurrentTrain: epoch  0, batch    40 | loss: 145.3938363CurrentTrain: epoch  0, batch    41 | loss: 98.8286182CurrentTrain: epoch  0, batch    42 | loss: 96.8195068CurrentTrain: epoch  0, batch    43 | loss: 96.4546012CurrentTrain: epoch  0, batch    44 | loss: 115.1203338CurrentTrain: epoch  0, batch    45 | loss: 115.4269532CurrentTrain: epoch  0, batch    46 | loss: 141.8206245CurrentTrain: epoch  0, batch    47 | loss: 93.1723204CurrentTrain: epoch  0, batch    48 | loss: 84.4601080CurrentTrain: epoch  0, batch    49 | loss: 96.5611830CurrentTrain: epoch  0, batch    50 | loss: 73.0093058CurrentTrain: epoch  0, batch    51 | loss: 115.6106827CurrentTrain: epoch  0, batch    52 | loss: 81.9611099CurrentTrain: epoch  0, batch    53 | loss: 80.8089023CurrentTrain: epoch  0, batch    54 | loss: 72.9600860CurrentTrain: epoch  0, batch    55 | loss: 191.9699392CurrentTrain: epoch  0, batch    56 | loss: 142.8494318CurrentTrain: epoch  0, batch    57 | loss: 191.8699336CurrentTrain: epoch  0, batch    58 | loss: 112.3183862CurrentTrain: epoch  0, batch    59 | loss: 116.4772629CurrentTrain: epoch  0, batch    60 | loss: 138.2212671CurrentTrain: epoch  0, batch    61 | loss: 71.1159820CurrentTrain: epoch  0, batch    62 | loss: 190.7740011CurrentTrain: epoch  0, batch    63 | loss: 113.3313255CurrentTrain: epoch  0, batch    64 | loss: 81.3652624CurrentTrain: epoch  0, batch    65 | loss: 94.7023699CurrentTrain: epoch  0, batch    66 | loss: 136.5005156CurrentTrain: epoch  0, batch    67 | loss: 94.2762559CurrentTrain: epoch  0, batch    68 | loss: 82.0914838CurrentTrain: epoch  0, batch    69 | loss: 142.0641545CurrentTrain: epoch  0, batch    70 | loss: 93.9045959CurrentTrain: epoch  0, batch    71 | loss: 78.5545837CurrentTrain: epoch  0, batch    72 | loss: 96.4817750CurrentTrain: epoch  0, batch    73 | loss: 94.5409598CurrentTrain: epoch  0, batch    74 | loss: 113.7949762CurrentTrain: epoch  0, batch    75 | loss: 111.8486622CurrentTrain: epoch  0, batch    76 | loss: 141.3080618CurrentTrain: epoch  0, batch    77 | loss: 110.9914759CurrentTrain: epoch  0, batch    78 | loss: 109.9924684CurrentTrain: epoch  0, batch    79 | loss: 91.3616353CurrentTrain: epoch  0, batch    80 | loss: 80.9103730CurrentTrain: epoch  0, batch    81 | loss: 78.3012493CurrentTrain: epoch  0, batch    82 | loss: 92.1452042CurrentTrain: epoch  0, batch    83 | loss: 81.9941384CurrentTrain: epoch  0, batch    84 | loss: 79.4437352CurrentTrain: epoch  0, batch    85 | loss: 110.6121851CurrentTrain: epoch  0, batch    86 | loss: 79.5295444CurrentTrain: epoch  0, batch    87 | loss: 110.6956448CurrentTrain: epoch  0, batch    88 | loss: 76.4673681CurrentTrain: epoch  0, batch    89 | loss: 113.0307923CurrentTrain: epoch  0, batch    90 | loss: 91.5237269CurrentTrain: epoch  0, batch    91 | loss: 91.5211326CurrentTrain: epoch  0, batch    92 | loss: 88.2537116CurrentTrain: epoch  0, batch    93 | loss: 95.7621248CurrentTrain: epoch  0, batch    94 | loss: 138.1011728CurrentTrain: epoch  0, batch    95 | loss: 113.1758902CurrentTrain: epoch  1, batch     0 | loss: 90.2264670CurrentTrain: epoch  1, batch     1 | loss: 76.3796583CurrentTrain: epoch  1, batch     2 | loss: 113.7622618CurrentTrain: epoch  1, batch     3 | loss: 75.4810897CurrentTrain: epoch  1, batch     4 | loss: 108.3512968CurrentTrain: epoch  1, batch     5 | loss: 109.1670140CurrentTrain: epoch  1, batch     6 | loss: 133.7274384CurrentTrain: epoch  1, batch     7 | loss: 136.4646792CurrentTrain: epoch  1, batch     8 | loss: 142.5120721CurrentTrain: epoch  1, batch     9 | loss: 111.6564861CurrentTrain: epoch  1, batch    10 | loss: 91.8540604CurrentTrain: epoch  1, batch    11 | loss: 74.4007000CurrentTrain: epoch  1, batch    12 | loss: 113.3687298CurrentTrain: epoch  1, batch    13 | loss: 81.0888030CurrentTrain: epoch  1, batch    14 | loss: 90.7781153CurrentTrain: epoch  1, batch    15 | loss: 91.2984803CurrentTrain: epoch  1, batch    16 | loss: 107.0828485CurrentTrain: epoch  1, batch    17 | loss: 88.1342857CurrentTrain: epoch  1, batch    18 | loss: 88.3760446CurrentTrain: epoch  1, batch    19 | loss: 76.4785052CurrentTrain: epoch  1, batch    20 | loss: 77.7830794CurrentTrain: epoch  1, batch    21 | loss: 134.6323517CurrentTrain: epoch  1, batch    22 | loss: 73.5187918CurrentTrain: epoch  1, batch    23 | loss: 89.1095380CurrentTrain: epoch  1, batch    24 | loss: 73.8056264CurrentTrain: epoch  1, batch    25 | loss: 86.0059061CurrentTrain: epoch  1, batch    26 | loss: 88.4342312CurrentTrain: epoch  1, batch    27 | loss: 106.8548475CurrentTrain: epoch  1, batch    28 | loss: 137.3628136CurrentTrain: epoch  1, batch    29 | loss: 80.4170216CurrentTrain: epoch  1, batch    30 | loss: 89.2745005CurrentTrain: epoch  1, batch    31 | loss: 90.1668493CurrentTrain: epoch  1, batch    32 | loss: 77.0651042CurrentTrain: epoch  1, batch    33 | loss: 67.1615725CurrentTrain: epoch  1, batch    34 | loss: 135.4153035CurrentTrain: epoch  1, batch    35 | loss: 110.4614351CurrentTrain: epoch  1, batch    36 | loss: 76.5403351CurrentTrain: epoch  1, batch    37 | loss: 110.6360595CurrentTrain: epoch  1, batch    38 | loss: 141.5616844CurrentTrain: epoch  1, batch    39 | loss: 89.7981442CurrentTrain: epoch  1, batch    40 | loss: 85.0211109CurrentTrain: epoch  1, batch    41 | loss: 89.2132711CurrentTrain: epoch  1, batch    42 | loss: 109.7466337CurrentTrain: epoch  1, batch    43 | loss: 78.4728102CurrentTrain: epoch  1, batch    44 | loss: 105.0461564CurrentTrain: epoch  1, batch    45 | loss: 108.2639216CurrentTrain: epoch  1, batch    46 | loss: 185.7390081CurrentTrain: epoch  1, batch    47 | loss: 89.8885307CurrentTrain: epoch  1, batch    48 | loss: 91.3722128CurrentTrain: epoch  1, batch    49 | loss: 87.9136083CurrentTrain: epoch  1, batch    50 | loss: 106.2313295CurrentTrain: epoch  1, batch    51 | loss: 92.0600450CurrentTrain: epoch  1, batch    52 | loss: 136.5386026CurrentTrain: epoch  1, batch    53 | loss: 85.7910255CurrentTrain: epoch  1, batch    54 | loss: 85.7362489CurrentTrain: epoch  1, batch    55 | loss: 90.7715437CurrentTrain: epoch  1, batch    56 | loss: 75.9386775CurrentTrain: epoch  1, batch    57 | loss: 72.5046705CurrentTrain: epoch  1, batch    58 | loss: 109.2216183CurrentTrain: epoch  1, batch    59 | loss: 105.9790368CurrentTrain: epoch  1, batch    60 | loss: 72.5416529CurrentTrain: epoch  1, batch    61 | loss: 64.5135529CurrentTrain: epoch  1, batch    62 | loss: 109.6193730CurrentTrain: epoch  1, batch    63 | loss: 114.0748398CurrentTrain: epoch  1, batch    64 | loss: 76.0562536CurrentTrain: epoch  1, batch    65 | loss: 86.7542653CurrentTrain: epoch  1, batch    66 | loss: 106.4016801CurrentTrain: epoch  1, batch    67 | loss: 71.8180240CurrentTrain: epoch  1, batch    68 | loss: 90.0369266CurrentTrain: epoch  1, batch    69 | loss: 76.4013310CurrentTrain: epoch  1, batch    70 | loss: 109.2830556CurrentTrain: epoch  1, batch    71 | loss: 74.5340393CurrentTrain: epoch  1, batch    72 | loss: 105.5263878CurrentTrain: epoch  1, batch    73 | loss: 83.1370147CurrentTrain: epoch  1, batch    74 | loss: 85.8828762CurrentTrain: epoch  1, batch    75 | loss: 75.1705366CurrentTrain: epoch  1, batch    76 | loss: 85.6596838CurrentTrain: epoch  1, batch    77 | loss: 71.1675651CurrentTrain: epoch  1, batch    78 | loss: 90.5048581CurrentTrain: epoch  1, batch    79 | loss: 105.1869456CurrentTrain: epoch  1, batch    80 | loss: 86.2476524CurrentTrain: epoch  1, batch    81 | loss: 106.4336603CurrentTrain: epoch  1, batch    82 | loss: 78.5109847CurrentTrain: epoch  1, batch    83 | loss: 86.5194117CurrentTrain: epoch  1, batch    84 | loss: 76.7980685CurrentTrain: epoch  1, batch    85 | loss: 77.7313755CurrentTrain: epoch  1, batch    86 | loss: 88.9326801CurrentTrain: epoch  1, batch    87 | loss: 75.2467286CurrentTrain: epoch  1, batch    88 | loss: 102.7282108CurrentTrain: epoch  1, batch    89 | loss: 87.4815638CurrentTrain: epoch  1, batch    90 | loss: 77.1397565CurrentTrain: epoch  1, batch    91 | loss: 89.5341773CurrentTrain: epoch  1, batch    92 | loss: 88.2154506CurrentTrain: epoch  1, batch    93 | loss: 85.2167563CurrentTrain: epoch  1, batch    94 | loss: 88.0854737CurrentTrain: epoch  1, batch    95 | loss: 72.2689326CurrentTrain: epoch  2, batch     0 | loss: 64.6717788CurrentTrain: epoch  2, batch     1 | loss: 64.0816504CurrentTrain: epoch  2, batch     2 | loss: 83.4691715CurrentTrain: epoch  2, batch     3 | loss: 74.3013617CurrentTrain: epoch  2, batch     4 | loss: 61.7689133CurrentTrain: epoch  2, batch     5 | loss: 105.5197729CurrentTrain: epoch  2, batch     6 | loss: 75.9130474CurrentTrain: epoch  2, batch     7 | loss: 87.5511162CurrentTrain: epoch  2, batch     8 | loss: 135.5037260CurrentTrain: epoch  2, batch     9 | loss: 86.1442483CurrentTrain: epoch  2, batch    10 | loss: 82.8477434CurrentTrain: epoch  2, batch    11 | loss: 74.7981035CurrentTrain: epoch  2, batch    12 | loss: 63.6464871CurrentTrain: epoch  2, batch    13 | loss: 103.4845902CurrentTrain: epoch  2, batch    14 | loss: 83.5545289CurrentTrain: epoch  2, batch    15 | loss: 104.6001943CurrentTrain: epoch  2, batch    16 | loss: 62.3755835CurrentTrain: epoch  2, batch    17 | loss: 102.7940314CurrentTrain: epoch  2, batch    18 | loss: 130.2534270CurrentTrain: epoch  2, batch    19 | loss: 86.1160144CurrentTrain: epoch  2, batch    20 | loss: 87.6990514CurrentTrain: epoch  2, batch    21 | loss: 103.8652026CurrentTrain: epoch  2, batch    22 | loss: 92.8201285CurrentTrain: epoch  2, batch    23 | loss: 62.1176086CurrentTrain: epoch  2, batch    24 | loss: 62.8326729CurrentTrain: epoch  2, batch    25 | loss: 86.6334511CurrentTrain: epoch  2, batch    26 | loss: 68.6555702CurrentTrain: epoch  2, batch    27 | loss: 83.6603796CurrentTrain: epoch  2, batch    28 | loss: 75.5237237CurrentTrain: epoch  2, batch    29 | loss: 87.4104609CurrentTrain: epoch  2, batch    30 | loss: 86.6366963CurrentTrain: epoch  2, batch    31 | loss: 85.7955076CurrentTrain: epoch  2, batch    32 | loss: 108.3265358CurrentTrain: epoch  2, batch    33 | loss: 130.5220712CurrentTrain: epoch  2, batch    34 | loss: 87.6356570CurrentTrain: epoch  2, batch    35 | loss: 89.0763176CurrentTrain: epoch  2, batch    36 | loss: 129.0388050CurrentTrain: epoch  2, batch    37 | loss: 102.0628886CurrentTrain: epoch  2, batch    38 | loss: 85.7410710CurrentTrain: epoch  2, batch    39 | loss: 106.7806686CurrentTrain: epoch  2, batch    40 | loss: 86.0481011CurrentTrain: epoch  2, batch    41 | loss: 71.0549482CurrentTrain: epoch  2, batch    42 | loss: 64.5095339CurrentTrain: epoch  2, batch    43 | loss: 69.5909368CurrentTrain: epoch  2, batch    44 | loss: 132.2145949CurrentTrain: epoch  2, batch    45 | loss: 90.7969603CurrentTrain: epoch  2, batch    46 | loss: 88.5974288CurrentTrain: epoch  2, batch    47 | loss: 84.0153515CurrentTrain: epoch  2, batch    48 | loss: 129.4489261CurrentTrain: epoch  2, batch    49 | loss: 64.3023183CurrentTrain: epoch  2, batch    50 | loss: 107.9064008CurrentTrain: epoch  2, batch    51 | loss: 87.2509801CurrentTrain: epoch  2, batch    52 | loss: 106.0710196CurrentTrain: epoch  2, batch    53 | loss: 63.0521394CurrentTrain: epoch  2, batch    54 | loss: 107.1718341CurrentTrain: epoch  2, batch    55 | loss: 85.9767733CurrentTrain: epoch  2, batch    56 | loss: 104.5894209CurrentTrain: epoch  2, batch    57 | loss: 84.2155033CurrentTrain: epoch  2, batch    58 | loss: 84.0519366CurrentTrain: epoch  2, batch    59 | loss: 103.8790705CurrentTrain: epoch  2, batch    60 | loss: 100.7336039CurrentTrain: epoch  2, batch    61 | loss: 108.0141274CurrentTrain: epoch  2, batch    62 | loss: 74.1945693CurrentTrain: epoch  2, batch    63 | loss: 87.1168211CurrentTrain: epoch  2, batch    64 | loss: 61.8763604CurrentTrain: epoch  2, batch    65 | loss: 103.9278370CurrentTrain: epoch  2, batch    66 | loss: 70.0366018CurrentTrain: epoch  2, batch    67 | loss: 89.9617222CurrentTrain: epoch  2, batch    68 | loss: 72.9659730CurrentTrain: epoch  2, batch    69 | loss: 76.3143978CurrentTrain: epoch  2, batch    70 | loss: 134.9421687CurrentTrain: epoch  2, batch    71 | loss: 70.8993727CurrentTrain: epoch  2, batch    72 | loss: 83.4336130CurrentTrain: epoch  2, batch    73 | loss: 133.4438201CurrentTrain: epoch  2, batch    74 | loss: 84.7010286CurrentTrain: epoch  2, batch    75 | loss: 74.9079208CurrentTrain: epoch  2, batch    76 | loss: 106.2952551CurrentTrain: epoch  2, batch    77 | loss: 88.1169638CurrentTrain: epoch  2, batch    78 | loss: 105.5212144CurrentTrain: epoch  2, batch    79 | loss: 74.5303599CurrentTrain: epoch  2, batch    80 | loss: 133.5915969CurrentTrain: epoch  2, batch    81 | loss: 73.0423697CurrentTrain: epoch  2, batch    82 | loss: 92.2550914CurrentTrain: epoch  2, batch    83 | loss: 77.0843040CurrentTrain: epoch  2, batch    84 | loss: 69.7560313CurrentTrain: epoch  2, batch    85 | loss: 87.5225454CurrentTrain: epoch  2, batch    86 | loss: 138.1871916CurrentTrain: epoch  2, batch    87 | loss: 107.8491156CurrentTrain: epoch  2, batch    88 | loss: 86.0140344CurrentTrain: epoch  2, batch    89 | loss: 84.9371351CurrentTrain: epoch  2, batch    90 | loss: 74.0434348CurrentTrain: epoch  2, batch    91 | loss: 87.5503139CurrentTrain: epoch  2, batch    92 | loss: 106.2782224CurrentTrain: epoch  2, batch    93 | loss: 136.0433089CurrentTrain: epoch  2, batch    94 | loss: 75.0874169CurrentTrain: epoch  2, batch    95 | loss: 108.4193530CurrentTrain: epoch  3, batch     0 | loss: 135.9928093CurrentTrain: epoch  3, batch     1 | loss: 69.3383647CurrentTrain: epoch  3, batch     2 | loss: 63.0448933CurrentTrain: epoch  3, batch     3 | loss: 70.6992503CurrentTrain: epoch  3, batch     4 | loss: 104.1720433CurrentTrain: epoch  3, batch     5 | loss: 72.3800283CurrentTrain: epoch  3, batch     6 | loss: 69.8931767CurrentTrain: epoch  3, batch     7 | loss: 128.7175178CurrentTrain: epoch  3, batch     8 | loss: 81.3650159CurrentTrain: epoch  3, batch     9 | loss: 61.2209590CurrentTrain: epoch  3, batch    10 | loss: 88.1085415CurrentTrain: epoch  3, batch    11 | loss: 99.2926436CurrentTrain: epoch  3, batch    12 | loss: 71.5217578CurrentTrain: epoch  3, batch    13 | loss: 104.4054951CurrentTrain: epoch  3, batch    14 | loss: 73.1116193CurrentTrain: epoch  3, batch    15 | loss: 71.9219093CurrentTrain: epoch  3, batch    16 | loss: 88.6683671CurrentTrain: epoch  3, batch    17 | loss: 72.9220059CurrentTrain: epoch  3, batch    18 | loss: 84.5796058CurrentTrain: epoch  3, batch    19 | loss: 104.8079200CurrentTrain: epoch  3, batch    20 | loss: 83.6933606CurrentTrain: epoch  3, batch    21 | loss: 89.3343617CurrentTrain: epoch  3, batch    22 | loss: 79.8067409CurrentTrain: epoch  3, batch    23 | loss: 128.9211046CurrentTrain: epoch  3, batch    24 | loss: 68.2607813CurrentTrain: epoch  3, batch    25 | loss: 103.0250205CurrentTrain: epoch  3, batch    26 | loss: 103.6976264CurrentTrain: epoch  3, batch    27 | loss: 59.6687595CurrentTrain: epoch  3, batch    28 | loss: 83.9710184CurrentTrain: epoch  3, batch    29 | loss: 71.1468374CurrentTrain: epoch  3, batch    30 | loss: 131.3671471CurrentTrain: epoch  3, batch    31 | loss: 102.9784382CurrentTrain: epoch  3, batch    32 | loss: 137.1640687CurrentTrain: epoch  3, batch    33 | loss: 101.2351730CurrentTrain: epoch  3, batch    34 | loss: 105.2916701CurrentTrain: epoch  3, batch    35 | loss: 83.6856199CurrentTrain: epoch  3, batch    36 | loss: 100.4631503CurrentTrain: epoch  3, batch    37 | loss: 70.1259257CurrentTrain: epoch  3, batch    38 | loss: 73.4931824CurrentTrain: epoch  3, batch    39 | loss: 86.2403738CurrentTrain: epoch  3, batch    40 | loss: 58.5708814CurrentTrain: epoch  3, batch    41 | loss: 61.6684269CurrentTrain: epoch  3, batch    42 | loss: 132.0474604CurrentTrain: epoch  3, batch    43 | loss: 90.8764900CurrentTrain: epoch  3, batch    44 | loss: 100.5636200CurrentTrain: epoch  3, batch    45 | loss: 57.8444545CurrentTrain: epoch  3, batch    46 | loss: 129.8114463CurrentTrain: epoch  3, batch    47 | loss: 100.6128866CurrentTrain: epoch  3, batch    48 | loss: 82.5303275CurrentTrain: epoch  3, batch    49 | loss: 81.8669289CurrentTrain: epoch  3, batch    50 | loss: 100.3184405CurrentTrain: epoch  3, batch    51 | loss: 70.1275154CurrentTrain: epoch  3, batch    52 | loss: 105.0879656CurrentTrain: epoch  3, batch    53 | loss: 70.7311263CurrentTrain: epoch  3, batch    54 | loss: 72.4667482CurrentTrain: epoch  3, batch    55 | loss: 102.8269588CurrentTrain: epoch  3, batch    56 | loss: 102.6183435CurrentTrain: epoch  3, batch    57 | loss: 84.0932787CurrentTrain: epoch  3, batch    58 | loss: 70.9247549CurrentTrain: epoch  3, batch    59 | loss: 102.1201267CurrentTrain: epoch  3, batch    60 | loss: 85.8905244CurrentTrain: epoch  3, batch    61 | loss: 87.5232059CurrentTrain: epoch  3, batch    62 | loss: 81.3152728CurrentTrain: epoch  3, batch    63 | loss: 84.2830449CurrentTrain: epoch  3, batch    64 | loss: 81.6922779CurrentTrain: epoch  3, batch    65 | loss: 64.0410126CurrentTrain: epoch  3, batch    66 | loss: 86.6140906CurrentTrain: epoch  3, batch    67 | loss: 70.4940041CurrentTrain: epoch  3, batch    68 | loss: 101.6171760CurrentTrain: epoch  3, batch    69 | loss: 62.2615279CurrentTrain: epoch  3, batch    70 | loss: 85.8266292CurrentTrain: epoch  3, batch    71 | loss: 95.6052888CurrentTrain: epoch  3, batch    72 | loss: 73.7002349CurrentTrain: epoch  3, batch    73 | loss: 85.2854829CurrentTrain: epoch  3, batch    74 | loss: 70.7727763CurrentTrain: epoch  3, batch    75 | loss: 81.0329086CurrentTrain: epoch  3, batch    76 | loss: 86.5504464CurrentTrain: epoch  3, batch    77 | loss: 91.5551312CurrentTrain: epoch  3, batch    78 | loss: 106.4220110CurrentTrain: epoch  3, batch    79 | loss: 101.1827311CurrentTrain: epoch  3, batch    80 | loss: 81.7087828CurrentTrain: epoch  3, batch    81 | loss: 101.2987503CurrentTrain: epoch  3, batch    82 | loss: 74.4234395CurrentTrain: epoch  3, batch    83 | loss: 133.2280224CurrentTrain: epoch  3, batch    84 | loss: 81.3520074CurrentTrain: epoch  3, batch    85 | loss: 85.3822737CurrentTrain: epoch  3, batch    86 | loss: 83.5257217CurrentTrain: epoch  3, batch    87 | loss: 106.9121499CurrentTrain: epoch  3, batch    88 | loss: 173.8234344CurrentTrain: epoch  3, batch    89 | loss: 87.4756367CurrentTrain: epoch  3, batch    90 | loss: 87.4317323CurrentTrain: epoch  3, batch    91 | loss: 102.9990645CurrentTrain: epoch  3, batch    92 | loss: 102.6556731CurrentTrain: epoch  3, batch    93 | loss: 97.9076566CurrentTrain: epoch  3, batch    94 | loss: 83.7977648CurrentTrain: epoch  3, batch    95 | loss: 73.3514418CurrentTrain: epoch  4, batch     0 | loss: 78.8701882CurrentTrain: epoch  4, batch     1 | loss: 98.3531231CurrentTrain: epoch  4, batch     2 | loss: 81.4298219CurrentTrain: epoch  4, batch     3 | loss: 69.7624659CurrentTrain: epoch  4, batch     4 | loss: 131.4382161CurrentTrain: epoch  4, batch     5 | loss: 128.0041906CurrentTrain: epoch  4, batch     6 | loss: 81.9741562CurrentTrain: epoch  4, batch     7 | loss: 83.4648149CurrentTrain: epoch  4, batch     8 | loss: 68.6219346CurrentTrain: epoch  4, batch     9 | loss: 103.3156677CurrentTrain: epoch  4, batch    10 | loss: 100.7248061CurrentTrain: epoch  4, batch    11 | loss: 101.1794962CurrentTrain: epoch  4, batch    12 | loss: 71.5287657CurrentTrain: epoch  4, batch    13 | loss: 76.4650283CurrentTrain: epoch  4, batch    14 | loss: 84.8488845CurrentTrain: epoch  4, batch    15 | loss: 57.9536831CurrentTrain: epoch  4, batch    16 | loss: 132.1542520CurrentTrain: epoch  4, batch    17 | loss: 71.6252563CurrentTrain: epoch  4, batch    18 | loss: 124.8424117CurrentTrain: epoch  4, batch    19 | loss: 98.2697891CurrentTrain: epoch  4, batch    20 | loss: 87.2051859CurrentTrain: epoch  4, batch    21 | loss: 82.2640342CurrentTrain: epoch  4, batch    22 | loss: 83.7329519CurrentTrain: epoch  4, batch    23 | loss: 78.9543566CurrentTrain: epoch  4, batch    24 | loss: 84.0835929CurrentTrain: epoch  4, batch    25 | loss: 84.3644738CurrentTrain: epoch  4, batch    26 | loss: 70.1016690CurrentTrain: epoch  4, batch    27 | loss: 81.0638786CurrentTrain: epoch  4, batch    28 | loss: 73.5847510CurrentTrain: epoch  4, batch    29 | loss: 86.9792017CurrentTrain: epoch  4, batch    30 | loss: 103.3009674CurrentTrain: epoch  4, batch    31 | loss: 68.0470201CurrentTrain: epoch  4, batch    32 | loss: 82.4312469CurrentTrain: epoch  4, batch    33 | loss: 82.3786344CurrentTrain: epoch  4, batch    34 | loss: 99.5505445CurrentTrain: epoch  4, batch    35 | loss: 65.5830598CurrentTrain: epoch  4, batch    36 | loss: 83.8034365CurrentTrain: epoch  4, batch    37 | loss: 97.4689443CurrentTrain: epoch  4, batch    38 | loss: 68.0486252CurrentTrain: epoch  4, batch    39 | loss: 70.7182079CurrentTrain: epoch  4, batch    40 | loss: 86.4477145CurrentTrain: epoch  4, batch    41 | loss: 81.5358344CurrentTrain: epoch  4, batch    42 | loss: 79.3321359CurrentTrain: epoch  4, batch    43 | loss: 82.0273873CurrentTrain: epoch  4, batch    44 | loss: 100.6417651CurrentTrain: epoch  4, batch    45 | loss: 72.6242223CurrentTrain: epoch  4, batch    46 | loss: 64.0169503CurrentTrain: epoch  4, batch    47 | loss: 80.7066374CurrentTrain: epoch  4, batch    48 | loss: 58.8680406CurrentTrain: epoch  4, batch    49 | loss: 81.9098247CurrentTrain: epoch  4, batch    50 | loss: 68.8004509CurrentTrain: epoch  4, batch    51 | loss: 85.5392926CurrentTrain: epoch  4, batch    52 | loss: 84.2966550CurrentTrain: epoch  4, batch    53 | loss: 85.1346727CurrentTrain: epoch  4, batch    54 | loss: 102.4445201CurrentTrain: epoch  4, batch    55 | loss: 101.2117548CurrentTrain: epoch  4, batch    56 | loss: 124.8991927CurrentTrain: epoch  4, batch    57 | loss: 68.3978311CurrentTrain: epoch  4, batch    58 | loss: 80.4176757CurrentTrain: epoch  4, batch    59 | loss: 72.3230194CurrentTrain: epoch  4, batch    60 | loss: 61.0429274CurrentTrain: epoch  4, batch    61 | loss: 87.5908394CurrentTrain: epoch  4, batch    62 | loss: 82.2631777CurrentTrain: epoch  4, batch    63 | loss: 71.4812851CurrentTrain: epoch  4, batch    64 | loss: 87.1835553CurrentTrain: epoch  4, batch    65 | loss: 82.5740246CurrentTrain: epoch  4, batch    66 | loss: 100.7835928CurrentTrain: epoch  4, batch    67 | loss: 71.4262649CurrentTrain: epoch  4, batch    68 | loss: 81.4026956CurrentTrain: epoch  4, batch    69 | loss: 107.0771153CurrentTrain: epoch  4, batch    70 | loss: 82.0234843CurrentTrain: epoch  4, batch    71 | loss: 136.7146464CurrentTrain: epoch  4, batch    72 | loss: 129.9631970CurrentTrain: epoch  4, batch    73 | loss: 101.0692447CurrentTrain: epoch  4, batch    74 | loss: 85.8152140CurrentTrain: epoch  4, batch    75 | loss: 84.5567318CurrentTrain: epoch  4, batch    76 | loss: 97.2583791CurrentTrain: epoch  4, batch    77 | loss: 98.6600689CurrentTrain: epoch  4, batch    78 | loss: 132.5843838CurrentTrain: epoch  4, batch    79 | loss: 126.6892239CurrentTrain: epoch  4, batch    80 | loss: 84.5993255CurrentTrain: epoch  4, batch    81 | loss: 79.6531435CurrentTrain: epoch  4, batch    82 | loss: 64.7146163CurrentTrain: epoch  4, batch    83 | loss: 100.0065485CurrentTrain: epoch  4, batch    84 | loss: 90.5630248CurrentTrain: epoch  4, batch    85 | loss: 102.8937995CurrentTrain: epoch  4, batch    86 | loss: 103.0995636CurrentTrain: epoch  4, batch    87 | loss: 71.2280367CurrentTrain: epoch  4, batch    88 | loss: 96.8719572CurrentTrain: epoch  4, batch    89 | loss: 103.0039880CurrentTrain: epoch  4, batch    90 | loss: 124.1115246CurrentTrain: epoch  4, batch    91 | loss: 82.6010964CurrentTrain: epoch  4, batch    92 | loss: 120.9595899CurrentTrain: epoch  4, batch    93 | loss: 101.6174398CurrentTrain: epoch  4, batch    94 | loss: 58.2427714CurrentTrain: epoch  4, batch    95 | loss: 69.2616363CurrentTrain: epoch  5, batch     0 | loss: 60.3929456CurrentTrain: epoch  5, batch     1 | loss: 100.4381935CurrentTrain: epoch  5, batch     2 | loss: 83.5607643CurrentTrain: epoch  5, batch     3 | loss: 99.1951600CurrentTrain: epoch  5, batch     4 | loss: 102.6112691CurrentTrain: epoch  5, batch     5 | loss: 81.8870860CurrentTrain: epoch  5, batch     6 | loss: 125.0609966CurrentTrain: epoch  5, batch     7 | loss: 79.7462683CurrentTrain: epoch  5, batch     8 | loss: 69.9766015CurrentTrain: epoch  5, batch     9 | loss: 55.5030108CurrentTrain: epoch  5, batch    10 | loss: 83.1496498CurrentTrain: epoch  5, batch    11 | loss: 125.3315178CurrentTrain: epoch  5, batch    12 | loss: 69.1232824CurrentTrain: epoch  5, batch    13 | loss: 103.0844196CurrentTrain: epoch  5, batch    14 | loss: 71.6104174CurrentTrain: epoch  5, batch    15 | loss: 98.5171658CurrentTrain: epoch  5, batch    16 | loss: 80.6199171CurrentTrain: epoch  5, batch    17 | loss: 72.8526466CurrentTrain: epoch  5, batch    18 | loss: 82.2356863CurrentTrain: epoch  5, batch    19 | loss: 83.8312902CurrentTrain: epoch  5, batch    20 | loss: 97.5511878CurrentTrain: epoch  5, batch    21 | loss: 70.4939897CurrentTrain: epoch  5, batch    22 | loss: 81.5023498CurrentTrain: epoch  5, batch    23 | loss: 98.9429736CurrentTrain: epoch  5, batch    24 | loss: 76.9739857CurrentTrain: epoch  5, batch    25 | loss: 63.4587721CurrentTrain: epoch  5, batch    26 | loss: 71.3611971CurrentTrain: epoch  5, batch    27 | loss: 103.5660723CurrentTrain: epoch  5, batch    28 | loss: 104.0537310CurrentTrain: epoch  5, batch    29 | loss: 80.2649415CurrentTrain: epoch  5, batch    30 | loss: 100.8069204CurrentTrain: epoch  5, batch    31 | loss: 69.7964969CurrentTrain: epoch  5, batch    32 | loss: 80.2661478CurrentTrain: epoch  5, batch    33 | loss: 70.1193493CurrentTrain: epoch  5, batch    34 | loss: 82.8387248CurrentTrain: epoch  5, batch    35 | loss: 78.9576269CurrentTrain: epoch  5, batch    36 | loss: 70.1625497CurrentTrain: epoch  5, batch    37 | loss: 102.9508565CurrentTrain: epoch  5, batch    38 | loss: 84.7996645CurrentTrain: epoch  5, batch    39 | loss: 95.3423309CurrentTrain: epoch  5, batch    40 | loss: 68.9124659CurrentTrain: epoch  5, batch    41 | loss: 103.2513082CurrentTrain: epoch  5, batch    42 | loss: 80.3859585CurrentTrain: epoch  5, batch    43 | loss: 78.0817289CurrentTrain: epoch  5, batch    44 | loss: 65.7539705CurrentTrain: epoch  5, batch    45 | loss: 64.9923505CurrentTrain: epoch  5, batch    46 | loss: 101.4170762CurrentTrain: epoch  5, batch    47 | loss: 130.6016217CurrentTrain: epoch  5, batch    48 | loss: 66.7524392CurrentTrain: epoch  5, batch    49 | loss: 70.4353061CurrentTrain: epoch  5, batch    50 | loss: 60.3366883CurrentTrain: epoch  5, batch    51 | loss: 100.6494753CurrentTrain: epoch  5, batch    52 | loss: 124.6134690CurrentTrain: epoch  5, batch    53 | loss: 70.2881795CurrentTrain: epoch  5, batch    54 | loss: 103.0136759CurrentTrain: epoch  5, batch    55 | loss: 95.8062117CurrentTrain: epoch  5, batch    56 | loss: 68.1304666CurrentTrain: epoch  5, batch    57 | loss: 100.4390568CurrentTrain: epoch  5, batch    58 | loss: 66.6819762CurrentTrain: epoch  5, batch    59 | loss: 71.7992843CurrentTrain: epoch  5, batch    60 | loss: 96.1144001CurrentTrain: epoch  5, batch    61 | loss: 62.6474669CurrentTrain: epoch  5, batch    62 | loss: 71.0582985CurrentTrain: epoch  5, batch    63 | loss: 82.3043344CurrentTrain: epoch  5, batch    64 | loss: 124.2871233CurrentTrain: epoch  5, batch    65 | loss: 67.0896836CurrentTrain: epoch  5, batch    66 | loss: 130.9973454CurrentTrain: epoch  5, batch    67 | loss: 69.7944301CurrentTrain: epoch  5, batch    68 | loss: 95.4857071CurrentTrain: epoch  5, batch    69 | loss: 78.1330791CurrentTrain: epoch  5, batch    70 | loss: 126.2896488CurrentTrain: epoch  5, batch    71 | loss: 96.1335261CurrentTrain: epoch  5, batch    72 | loss: 124.1454290CurrentTrain: epoch  5, batch    73 | loss: 72.0601304CurrentTrain: epoch  5, batch    74 | loss: 81.3061905CurrentTrain: epoch  5, batch    75 | loss: 71.1554254CurrentTrain: epoch  5, batch    76 | loss: 80.0078878CurrentTrain: epoch  5, batch    77 | loss: 66.5356210CurrentTrain: epoch  5, batch    78 | loss: 71.2156140CurrentTrain: epoch  5, batch    79 | loss: 69.3423578CurrentTrain: epoch  5, batch    80 | loss: 102.2899674CurrentTrain: epoch  5, batch    81 | loss: 84.4425764CurrentTrain: epoch  5, batch    82 | loss: 94.9535647CurrentTrain: epoch  5, batch    83 | loss: 81.4981229CurrentTrain: epoch  5, batch    84 | loss: 98.6403925CurrentTrain: epoch  5, batch    85 | loss: 124.3720741CurrentTrain: epoch  5, batch    86 | loss: 91.9941447CurrentTrain: epoch  5, batch    87 | loss: 84.2609020CurrentTrain: epoch  5, batch    88 | loss: 80.6250953CurrentTrain: epoch  5, batch    89 | loss: 82.1959635CurrentTrain: epoch  5, batch    90 | loss: 126.2755290CurrentTrain: epoch  5, batch    91 | loss: 96.8573942CurrentTrain: epoch  5, batch    92 | loss: 81.4568877CurrentTrain: epoch  5, batch    93 | loss: 99.7405488CurrentTrain: epoch  5, batch    94 | loss: 82.6888659CurrentTrain: epoch  5, batch    95 | loss: 84.7720533CurrentTrain: epoch  6, batch     0 | loss: 68.3638385CurrentTrain: epoch  6, batch     1 | loss: 69.5334381CurrentTrain: epoch  6, batch     2 | loss: 78.1191972CurrentTrain: epoch  6, batch     3 | loss: 69.4573315CurrentTrain: epoch  6, batch     4 | loss: 91.7997626CurrentTrain: epoch  6, batch     5 | loss: 66.7076470CurrentTrain: epoch  6, batch     6 | loss: 80.9268163CurrentTrain: epoch  6, batch     7 | loss: 79.2079796CurrentTrain: epoch  6, batch     8 | loss: 70.1219448CurrentTrain: epoch  6, batch     9 | loss: 99.2645854CurrentTrain: epoch  6, batch    10 | loss: 53.8754192CurrentTrain: epoch  6, batch    11 | loss: 128.5003094CurrentTrain: epoch  6, batch    12 | loss: 99.7091984CurrentTrain: epoch  6, batch    13 | loss: 67.2909967CurrentTrain: epoch  6, batch    14 | loss: 95.5653297CurrentTrain: epoch  6, batch    15 | loss: 65.0724392CurrentTrain: epoch  6, batch    16 | loss: 81.3897466CurrentTrain: epoch  6, batch    17 | loss: 104.3205638CurrentTrain: epoch  6, batch    18 | loss: 82.9109705CurrentTrain: epoch  6, batch    19 | loss: 81.0401509CurrentTrain: epoch  6, batch    20 | loss: 76.0792630CurrentTrain: epoch  6, batch    21 | loss: 83.6311691CurrentTrain: epoch  6, batch    22 | loss: 76.7114959CurrentTrain: epoch  6, batch    23 | loss: 78.8677687CurrentTrain: epoch  6, batch    24 | loss: 98.3521492CurrentTrain: epoch  6, batch    25 | loss: 80.3708480CurrentTrain: epoch  6, batch    26 | loss: 78.9127733CurrentTrain: epoch  6, batch    27 | loss: 128.1213470CurrentTrain: epoch  6, batch    28 | loss: 64.7584854CurrentTrain: epoch  6, batch    29 | loss: 81.5867098CurrentTrain: epoch  6, batch    30 | loss: 75.6299032CurrentTrain: epoch  6, batch    31 | loss: 66.2829348CurrentTrain: epoch  6, batch    32 | loss: 125.0782947CurrentTrain: epoch  6, batch    33 | loss: 79.3093655CurrentTrain: epoch  6, batch    34 | loss: 83.3298062CurrentTrain: epoch  6, batch    35 | loss: 99.0001065CurrentTrain: epoch  6, batch    36 | loss: 69.3468054CurrentTrain: epoch  6, batch    37 | loss: 91.2076392CurrentTrain: epoch  6, batch    38 | loss: 56.5160265CurrentTrain: epoch  6, batch    39 | loss: 103.3299518CurrentTrain: epoch  6, batch    40 | loss: 67.4572342CurrentTrain: epoch  6, batch    41 | loss: 60.9062601CurrentTrain: epoch  6, batch    42 | loss: 80.5722079CurrentTrain: epoch  6, batch    43 | loss: 58.5108510CurrentTrain: epoch  6, batch    44 | loss: 66.3039574CurrentTrain: epoch  6, batch    45 | loss: 99.6669287CurrentTrain: epoch  6, batch    46 | loss: 66.9450473CurrentTrain: epoch  6, batch    47 | loss: 101.7108606CurrentTrain: epoch  6, batch    48 | loss: 101.8305141CurrentTrain: epoch  6, batch    49 | loss: 99.6781619CurrentTrain: epoch  6, batch    50 | loss: 70.0389710CurrentTrain: epoch  6, batch    51 | loss: 69.2484762CurrentTrain: epoch  6, batch    52 | loss: 68.2053248CurrentTrain: epoch  6, batch    53 | loss: 127.2384283CurrentTrain: epoch  6, batch    54 | loss: 69.1955889CurrentTrain: epoch  6, batch    55 | loss: 94.9453566CurrentTrain: epoch  6, batch    56 | loss: 80.4198235CurrentTrain: epoch  6, batch    57 | loss: 81.5149706CurrentTrain: epoch  6, batch    58 | loss: 87.4396727CurrentTrain: epoch  6, batch    59 | loss: 79.6800873CurrentTrain: epoch  6, batch    60 | loss: 82.0941346CurrentTrain: epoch  6, batch    61 | loss: 68.1792109CurrentTrain: epoch  6, batch    62 | loss: 78.6234249CurrentTrain: epoch  6, batch    63 | loss: 83.8023632CurrentTrain: epoch  6, batch    64 | loss: 95.3687957CurrentTrain: epoch  6, batch    65 | loss: 64.4272147CurrentTrain: epoch  6, batch    66 | loss: 102.7875357CurrentTrain: epoch  6, batch    67 | loss: 97.0893793CurrentTrain: epoch  6, batch    68 | loss: 94.6900529CurrentTrain: epoch  6, batch    69 | loss: 94.1279750CurrentTrain: epoch  6, batch    70 | loss: 75.3537242CurrentTrain: epoch  6, batch    71 | loss: 65.9610454CurrentTrain: epoch  6, batch    72 | loss: 124.7683987CurrentTrain: epoch  6, batch    73 | loss: 94.7593842CurrentTrain: epoch  6, batch    74 | loss: 81.8876923CurrentTrain: epoch  6, batch    75 | loss: 100.4201179CurrentTrain: epoch  6, batch    76 | loss: 125.0723484CurrentTrain: epoch  6, batch    77 | loss: 83.8400576CurrentTrain: epoch  6, batch    78 | loss: 81.6479057CurrentTrain: epoch  6, batch    79 | loss: 80.5332158CurrentTrain: epoch  6, batch    80 | loss: 66.2365330CurrentTrain: epoch  6, batch    81 | loss: 102.5934065CurrentTrain: epoch  6, batch    82 | loss: 68.0243212CurrentTrain: epoch  6, batch    83 | loss: 98.1197180CurrentTrain: epoch  6, batch    84 | loss: 96.7993543CurrentTrain: epoch  6, batch    85 | loss: 77.0924059CurrentTrain: epoch  6, batch    86 | loss: 80.9630036CurrentTrain: epoch  6, batch    87 | loss: 103.0429380CurrentTrain: epoch  6, batch    88 | loss: 81.1881694CurrentTrain: epoch  6, batch    89 | loss: 61.3089040CurrentTrain: epoch  6, batch    90 | loss: 124.9142062CurrentTrain: epoch  6, batch    91 | loss: 100.5652727CurrentTrain: epoch  6, batch    92 | loss: 83.3570023CurrentTrain: epoch  6, batch    93 | loss: 77.4878581CurrentTrain: epoch  6, batch    94 | loss: 81.2766155CurrentTrain: epoch  6, batch    95 | loss: 67.6046675CurrentTrain: epoch  7, batch     0 | loss: 66.5015719CurrentTrain: epoch  7, batch     1 | loss: 76.5865271CurrentTrain: epoch  7, batch     2 | loss: 64.8132885CurrentTrain: epoch  7, batch     3 | loss: 81.5164215CurrentTrain: epoch  7, batch     4 | loss: 79.1440776CurrentTrain: epoch  7, batch     5 | loss: 99.2241126CurrentTrain: epoch  7, batch     6 | loss: 121.8351373CurrentTrain: epoch  7, batch     7 | loss: 75.5651696CurrentTrain: epoch  7, batch     8 | loss: 63.3169585CurrentTrain: epoch  7, batch     9 | loss: 66.4956222CurrentTrain: epoch  7, batch    10 | loss: 57.4993513CurrentTrain: epoch  7, batch    11 | loss: 79.2675148CurrentTrain: epoch  7, batch    12 | loss: 98.2996022CurrentTrain: epoch  7, batch    13 | loss: 56.2531767CurrentTrain: epoch  7, batch    14 | loss: 65.9819245CurrentTrain: epoch  7, batch    15 | loss: 76.5661897CurrentTrain: epoch  7, batch    16 | loss: 76.6837440CurrentTrain: epoch  7, batch    17 | loss: 67.5147180CurrentTrain: epoch  7, batch    18 | loss: 124.9321632CurrentTrain: epoch  7, batch    19 | loss: 93.9966865CurrentTrain: epoch  7, batch    20 | loss: 70.8191336CurrentTrain: epoch  7, batch    21 | loss: 69.1430677CurrentTrain: epoch  7, batch    22 | loss: 78.3885772CurrentTrain: epoch  7, batch    23 | loss: 99.2055875CurrentTrain: epoch  7, batch    24 | loss: 78.8722790CurrentTrain: epoch  7, batch    25 | loss: 80.0692375CurrentTrain: epoch  7, batch    26 | loss: 98.6945525CurrentTrain: epoch  7, batch    27 | loss: 120.6546844CurrentTrain: epoch  7, batch    28 | loss: 80.0811029CurrentTrain: epoch  7, batch    29 | loss: 92.1274596CurrentTrain: epoch  7, batch    30 | loss: 55.4327223CurrentTrain: epoch  7, batch    31 | loss: 82.4149119CurrentTrain: epoch  7, batch    32 | loss: 99.4254122CurrentTrain: epoch  7, batch    33 | loss: 79.2932287CurrentTrain: epoch  7, batch    34 | loss: 77.3829455CurrentTrain: epoch  7, batch    35 | loss: 124.0810124CurrentTrain: epoch  7, batch    36 | loss: 79.8814589CurrentTrain: epoch  7, batch    37 | loss: 79.2006898CurrentTrain: epoch  7, batch    38 | loss: 95.4111042CurrentTrain: epoch  7, batch    39 | loss: 70.1199818CurrentTrain: epoch  7, batch    40 | loss: 81.2286467CurrentTrain: epoch  7, batch    41 | loss: 118.4009940CurrentTrain: epoch  7, batch    42 | loss: 68.9618328CurrentTrain: epoch  7, batch    43 | loss: 97.6664494CurrentTrain: epoch  7, batch    44 | loss: 96.7293350CurrentTrain: epoch  7, batch    45 | loss: 79.0625371CurrentTrain: epoch  7, batch    46 | loss: 97.9276730CurrentTrain: epoch  7, batch    47 | loss: 99.8317139CurrentTrain: epoch  7, batch    48 | loss: 96.7955297CurrentTrain: epoch  7, batch    49 | loss: 70.0552485CurrentTrain: epoch  7, batch    50 | loss: 57.0084116CurrentTrain: epoch  7, batch    51 | loss: 95.3648703CurrentTrain: epoch  7, batch    52 | loss: 76.0130839CurrentTrain: epoch  7, batch    53 | loss: 82.9972956CurrentTrain: epoch  7, batch    54 | loss: 63.6646231CurrentTrain: epoch  7, batch    55 | loss: 75.9254750CurrentTrain: epoch  7, batch    56 | loss: 77.2512324CurrentTrain: epoch  7, batch    57 | loss: 96.6918203CurrentTrain: epoch  7, batch    58 | loss: 97.9700361CurrentTrain: epoch  7, batch    59 | loss: 75.4792556CurrentTrain: epoch  7, batch    60 | loss: 75.5806115CurrentTrain: epoch  7, batch    61 | loss: 80.3566443CurrentTrain: epoch  7, batch    62 | loss: 77.5448623CurrentTrain: epoch  7, batch    63 | loss: 101.2362388CurrentTrain: epoch  7, batch    64 | loss: 68.3766393CurrentTrain: epoch  7, batch    65 | loss: 76.7561942CurrentTrain: epoch  7, batch    66 | loss: 97.3968964CurrentTrain: epoch  7, batch    67 | loss: 99.0392556CurrentTrain: epoch  7, batch    68 | loss: 77.6922904CurrentTrain: epoch  7, batch    69 | loss: 65.3714589CurrentTrain: epoch  7, batch    70 | loss: 65.7428003CurrentTrain: epoch  7, batch    71 | loss: 59.9730770CurrentTrain: epoch  7, batch    72 | loss: 101.3084075CurrentTrain: epoch  7, batch    73 | loss: 77.3689812CurrentTrain: epoch  7, batch    74 | loss: 83.8753939CurrentTrain: epoch  7, batch    75 | loss: 76.4039737CurrentTrain: epoch  7, batch    76 | loss: 129.3197569CurrentTrain: epoch  7, batch    77 | loss: 67.4684777CurrentTrain: epoch  7, batch    78 | loss: 123.0020733CurrentTrain: epoch  7, batch    79 | loss: 81.0807680CurrentTrain: epoch  7, batch    80 | loss: 128.6027764CurrentTrain: epoch  7, batch    81 | loss: 56.8421961CurrentTrain: epoch  7, batch    82 | loss: 66.8324978CurrentTrain: epoch  7, batch    83 | loss: 56.8456727CurrentTrain: epoch  7, batch    84 | loss: 66.5825277CurrentTrain: epoch  7, batch    85 | loss: 125.4677332CurrentTrain: epoch  7, batch    86 | loss: 59.3033168CurrentTrain: epoch  7, batch    87 | loss: 126.4409148CurrentTrain: epoch  7, batch    88 | loss: 78.8559800CurrentTrain: epoch  7, batch    89 | loss: 79.6257348CurrentTrain: epoch  7, batch    90 | loss: 127.7604463CurrentTrain: epoch  7, batch    91 | loss: 80.0114791CurrentTrain: epoch  7, batch    92 | loss: 54.2576242CurrentTrain: epoch  7, batch    93 | loss: 90.6347005CurrentTrain: epoch  7, batch    94 | loss: 77.6705080CurrentTrain: epoch  7, batch    95 | loss: 141.9430435CurrentTrain: epoch  8, batch     0 | loss: 77.1272703CurrentTrain: epoch  8, batch     1 | loss: 66.8610754CurrentTrain: epoch  8, batch     2 | loss: 60.2154748CurrentTrain: epoch  8, batch     3 | loss: 78.2879947CurrentTrain: epoch  8, batch     4 | loss: 65.2549088CurrentTrain: epoch  8, batch     5 | loss: 77.4166697CurrentTrain: epoch  8, batch     6 | loss: 61.0836428CurrentTrain: epoch  8, batch     7 | loss: 81.5966477CurrentTrain: epoch  8, batch     8 | loss: 77.7413576CurrentTrain: epoch  8, batch     9 | loss: 57.2034042CurrentTrain: epoch  8, batch    10 | loss: 79.2566155CurrentTrain: epoch  8, batch    11 | loss: 61.8974290CurrentTrain: epoch  8, batch    12 | loss: 79.7298891CurrentTrain: epoch  8, batch    13 | loss: 95.5962280CurrentTrain: epoch  8, batch    14 | loss: 99.3975801CurrentTrain: epoch  8, batch    15 | loss: 64.9497816CurrentTrain: epoch  8, batch    16 | loss: 123.9514534CurrentTrain: epoch  8, batch    17 | loss: 80.7443374CurrentTrain: epoch  8, batch    18 | loss: 80.7825129CurrentTrain: epoch  8, batch    19 | loss: 115.9322938CurrentTrain: epoch  8, batch    20 | loss: 78.2862182CurrentTrain: epoch  8, batch    21 | loss: 81.5441410CurrentTrain: epoch  8, batch    22 | loss: 76.8169326CurrentTrain: epoch  8, batch    23 | loss: 94.1797709CurrentTrain: epoch  8, batch    24 | loss: 78.1909361CurrentTrain: epoch  8, batch    25 | loss: 78.5452370CurrentTrain: epoch  8, batch    26 | loss: 98.7165617CurrentTrain: epoch  8, batch    27 | loss: 78.5884450CurrentTrain: epoch  8, batch    28 | loss: 98.7404723CurrentTrain: epoch  8, batch    29 | loss: 78.5590152CurrentTrain: epoch  8, batch    30 | loss: 80.0420750CurrentTrain: epoch  8, batch    31 | loss: 58.5104371CurrentTrain: epoch  8, batch    32 | loss: 62.7564112CurrentTrain: epoch  8, batch    33 | loss: 118.6808945CurrentTrain: epoch  8, batch    34 | loss: 122.4015391CurrentTrain: epoch  8, batch    35 | loss: 93.7617527CurrentTrain: epoch  8, batch    36 | loss: 90.6567981CurrentTrain: epoch  8, batch    37 | loss: 78.7354408CurrentTrain: epoch  8, batch    38 | loss: 79.3035079CurrentTrain: epoch  8, batch    39 | loss: 80.6628683CurrentTrain: epoch  8, batch    40 | loss: 80.0924799CurrentTrain: epoch  8, batch    41 | loss: 78.5012335CurrentTrain: epoch  8, batch    42 | loss: 66.5203265CurrentTrain: epoch  8, batch    43 | loss: 127.9140898CurrentTrain: epoch  8, batch    44 | loss: 124.3476442CurrentTrain: epoch  8, batch    45 | loss: 61.4523589CurrentTrain: epoch  8, batch    46 | loss: 58.6722110CurrentTrain: epoch  8, batch    47 | loss: 79.7623738CurrentTrain: epoch  8, batch    48 | loss: 83.0232943CurrentTrain: epoch  8, batch    49 | loss: 70.2172217CurrentTrain: epoch  8, batch    50 | loss: 54.8356361CurrentTrain: epoch  8, batch    51 | loss: 93.3308942CurrentTrain: epoch  8, batch    52 | loss: 121.9906577CurrentTrain: epoch  8, batch    53 | loss: 75.8983801CurrentTrain: epoch  8, batch    54 | loss: 70.3910422CurrentTrain: epoch  8, batch    55 | loss: 66.6408072CurrentTrain: epoch  8, batch    56 | loss: 65.8503245CurrentTrain: epoch  8, batch    57 | loss: 77.6372946CurrentTrain: epoch  8, batch    58 | loss: 62.1438032CurrentTrain: epoch  8, batch    59 | loss: 85.2129727CurrentTrain: epoch  8, batch    60 | loss: 98.9770608CurrentTrain: epoch  8, batch    61 | loss: 67.0226789CurrentTrain: epoch  8, batch    62 | loss: 120.8995096CurrentTrain: epoch  8, batch    63 | loss: 68.9373579CurrentTrain: epoch  8, batch    64 | loss: 97.5268719CurrentTrain: epoch  8, batch    65 | loss: 56.6729920CurrentTrain: epoch  8, batch    66 | loss: 93.9549469CurrentTrain: epoch  8, batch    67 | loss: 97.1495565CurrentTrain: epoch  8, batch    68 | loss: 96.2250708CurrentTrain: epoch  8, batch    69 | loss: 97.3052563CurrentTrain: epoch  8, batch    70 | loss: 61.2410157CurrentTrain: epoch  8, batch    71 | loss: 52.9727980CurrentTrain: epoch  8, batch    72 | loss: 91.1991289CurrentTrain: epoch  8, batch    73 | loss: 78.0337191CurrentTrain: epoch  8, batch    74 | loss: 82.2656267CurrentTrain: epoch  8, batch    75 | loss: 97.4486083CurrentTrain: epoch  8, batch    76 | loss: 79.0269241CurrentTrain: epoch  8, batch    77 | loss: 96.7620366CurrentTrain: epoch  8, batch    78 | loss: 70.6489356CurrentTrain: epoch  8, batch    79 | loss: 74.9774073CurrentTrain: epoch  8, batch    80 | loss: 74.8191578CurrentTrain: epoch  8, batch    81 | loss: 80.6146519CurrentTrain: epoch  8, batch    82 | loss: 65.7225041CurrentTrain: epoch  8, batch    83 | loss: 166.3514389CurrentTrain: epoch  8, batch    84 | loss: 67.4988641CurrentTrain: epoch  8, batch    85 | loss: 78.5458862CurrentTrain: epoch  8, batch    86 | loss: 66.9222743CurrentTrain: epoch  8, batch    87 | loss: 65.3915984CurrentTrain: epoch  8, batch    88 | loss: 82.9272646CurrentTrain: epoch  8, batch    89 | loss: 91.7433465CurrentTrain: epoch  8, batch    90 | loss: 62.1215522CurrentTrain: epoch  8, batch    91 | loss: 82.4571711CurrentTrain: epoch  8, batch    92 | loss: 67.5144084CurrentTrain: epoch  8, batch    93 | loss: 104.4086794CurrentTrain: epoch  8, batch    94 | loss: 65.5258322CurrentTrain: epoch  8, batch    95 | loss: 78.6291624CurrentTrain: epoch  9, batch     0 | loss: 93.0849629CurrentTrain: epoch  9, batch     1 | loss: 63.0553894CurrentTrain: epoch  9, batch     2 | loss: 96.0534544CurrentTrain: epoch  9, batch     3 | loss: 56.1570541CurrentTrain: epoch  9, batch     4 | loss: 94.4501334CurrentTrain: epoch  9, batch     5 | loss: 62.8900921CurrentTrain: epoch  9, batch     6 | loss: 60.8378155CurrentTrain: epoch  9, batch     7 | loss: 95.1477987CurrentTrain: epoch  9, batch     8 | loss: 91.3779488CurrentTrain: epoch  9, batch     9 | loss: 79.9420518CurrentTrain: epoch  9, batch    10 | loss: 97.9670321CurrentTrain: epoch  9, batch    11 | loss: 126.0282773CurrentTrain: epoch  9, batch    12 | loss: 95.5243647CurrentTrain: epoch  9, batch    13 | loss: 94.0605029CurrentTrain: epoch  9, batch    14 | loss: 93.7543618CurrentTrain: epoch  9, batch    15 | loss: 65.0323943CurrentTrain: epoch  9, batch    16 | loss: 76.7302180CurrentTrain: epoch  9, batch    17 | loss: 129.3965655CurrentTrain: epoch  9, batch    18 | loss: 80.0854794CurrentTrain: epoch  9, batch    19 | loss: 94.3231075CurrentTrain: epoch  9, batch    20 | loss: 88.3880956CurrentTrain: epoch  9, batch    21 | loss: 60.0156538CurrentTrain: epoch  9, batch    22 | loss: 76.6006677CurrentTrain: epoch  9, batch    23 | loss: 98.2901078CurrentTrain: epoch  9, batch    24 | loss: 97.7555443CurrentTrain: epoch  9, batch    25 | loss: 96.1430001CurrentTrain: epoch  9, batch    26 | loss: 77.5664730CurrentTrain: epoch  9, batch    27 | loss: 66.8544032CurrentTrain: epoch  9, batch    28 | loss: 64.4963039CurrentTrain: epoch  9, batch    29 | loss: 79.7414365CurrentTrain: epoch  9, batch    30 | loss: 58.6998068CurrentTrain: epoch  9, batch    31 | loss: 95.2232803CurrentTrain: epoch  9, batch    32 | loss: 65.9872984CurrentTrain: epoch  9, batch    33 | loss: 75.9276709CurrentTrain: epoch  9, batch    34 | loss: 118.0587283CurrentTrain: epoch  9, batch    35 | loss: 78.1441363CurrentTrain: epoch  9, batch    36 | loss: 64.0619180CurrentTrain: epoch  9, batch    37 | loss: 168.0334979CurrentTrain: epoch  9, batch    38 | loss: 80.3409925CurrentTrain: epoch  9, batch    39 | loss: 73.5526021CurrentTrain: epoch  9, batch    40 | loss: 100.2495574CurrentTrain: epoch  9, batch    41 | loss: 66.6214010CurrentTrain: epoch  9, batch    42 | loss: 62.2384000CurrentTrain: epoch  9, batch    43 | loss: 56.4687249CurrentTrain: epoch  9, batch    44 | loss: 75.1566872CurrentTrain: epoch  9, batch    45 | loss: 66.2480347CurrentTrain: epoch  9, batch    46 | loss: 120.9841754CurrentTrain: epoch  9, batch    47 | loss: 126.1740215CurrentTrain: epoch  9, batch    48 | loss: 79.4820493CurrentTrain: epoch  9, batch    49 | loss: 67.4447910CurrentTrain: epoch  9, batch    50 | loss: 75.9382144CurrentTrain: epoch  9, batch    51 | loss: 69.3059541CurrentTrain: epoch  9, batch    52 | loss: 71.0009665CurrentTrain: epoch  9, batch    53 | loss: 92.0597384CurrentTrain: epoch  9, batch    54 | loss: 63.9372792CurrentTrain: epoch  9, batch    55 | loss: 78.4838579CurrentTrain: epoch  9, batch    56 | loss: 64.9284920CurrentTrain: epoch  9, batch    57 | loss: 77.1368083CurrentTrain: epoch  9, batch    58 | loss: 64.8532583CurrentTrain: epoch  9, batch    59 | loss: 73.3191592CurrentTrain: epoch  9, batch    60 | loss: 94.2128805CurrentTrain: epoch  9, batch    61 | loss: 96.4941603CurrentTrain: epoch  9, batch    62 | loss: 73.0826198CurrentTrain: epoch  9, batch    63 | loss: 94.0586621CurrentTrain: epoch  9, batch    64 | loss: 162.9259971CurrentTrain: epoch  9, batch    65 | loss: 77.4701425CurrentTrain: epoch  9, batch    66 | loss: 97.5640351CurrentTrain: epoch  9, batch    67 | loss: 78.0358801CurrentTrain: epoch  9, batch    68 | loss: 76.0459178CurrentTrain: epoch  9, batch    69 | loss: 67.4695801CurrentTrain: epoch  9, batch    70 | loss: 75.2203989CurrentTrain: epoch  9, batch    71 | loss: 74.1603806CurrentTrain: epoch  9, batch    72 | loss: 65.2001192CurrentTrain: epoch  9, batch    73 | loss: 61.6715292CurrentTrain: epoch  9, batch    74 | loss: 79.0468172CurrentTrain: epoch  9, batch    75 | loss: 57.9354840CurrentTrain: epoch  9, batch    76 | loss: 74.4450661CurrentTrain: epoch  9, batch    77 | loss: 65.9409009CurrentTrain: epoch  9, batch    78 | loss: 97.7197860CurrentTrain: epoch  9, batch    79 | loss: 76.8449426CurrentTrain: epoch  9, batch    80 | loss: 120.2448827CurrentTrain: epoch  9, batch    81 | loss: 74.1116560CurrentTrain: epoch  9, batch    82 | loss: 96.1419907CurrentTrain: epoch  9, batch    83 | loss: 63.2621909CurrentTrain: epoch  9, batch    84 | loss: 98.4842766CurrentTrain: epoch  9, batch    85 | loss: 66.9482489CurrentTrain: epoch  9, batch    86 | loss: 74.6009676CurrentTrain: epoch  9, batch    87 | loss: 82.0426431CurrentTrain: epoch  9, batch    88 | loss: 66.7895783CurrentTrain: epoch  9, batch    89 | loss: 174.6323432CurrentTrain: epoch  9, batch    90 | loss: 66.4426207CurrentTrain: epoch  9, batch    91 | loss: 75.8755917CurrentTrain: epoch  9, batch    92 | loss: 64.5033321CurrentTrain: epoch  9, batch    93 | loss: 79.1776029CurrentTrain: epoch  9, batch    94 | loss: 64.1749809CurrentTrain: epoch  9, batch    95 | loss: 81.8901997

F1 score per class: {32: 0.5668449197860963, 6: 0.8288288288288288, 19: 0.41025641025641024, 24: 0.7486033519553073, 26: 0.9128205128205128, 29: 0.8207547169811321}
Micro-average F1 score: 0.7659574468085106
Weighted-average F1 score: 0.7690506813530947
F1 score per class: {32: 0.6511627906976745, 6: 0.7965367965367965, 19: 0.21505376344086022, 24: 0.7582417582417582, 26: 0.9405940594059405, 29: 0.8380952380952381}
Micro-average F1 score: 0.7484554280670785
Weighted-average F1 score: 0.7246601105045914
F1 score per class: {32: 0.660377358490566, 6: 0.8, 19: 0.32727272727272727, 24: 0.7666666666666667, 26: 0.9346733668341709, 29: 0.8202764976958525}
Micro-average F1 score: 0.7721866422689845
Weighted-average F1 score: 0.7650603512006352

F1 score per class: {32: 0.5668449197860963, 6: 0.8288288288288288, 19: 0.41025641025641024, 24: 0.7486033519553073, 26: 0.9128205128205128, 29: 0.8207547169811321}
Micro-average F1 score: 0.7659574468085106
Weighted-average F1 score: 0.7690506813530947
F1 score per class: {32: 0.6511627906976745, 6: 0.7965367965367965, 19: 0.21505376344086022, 24: 0.7582417582417582, 26: 0.9405940594059405, 29: 0.8380952380952381}
Micro-average F1 score: 0.7484554280670785
Weighted-average F1 score: 0.7246601105045914
F1 score per class: {32: 0.660377358490566, 6: 0.8, 19: 0.32727272727272727, 24: 0.7666666666666667, 26: 0.9346733668341709, 29: 0.8202764976958525}
Micro-average F1 score: 0.7721866422689845
Weighted-average F1 score: 0.7650603512006352

F1 score per class: {32: 0.4274193548387097, 6: 0.773109243697479, 19: 0.24615384615384617, 24: 0.6979166666666666, 26: 0.8516746411483254, 29: 0.6397058823529411}
Micro-average F1 score: 0.6470588235294118
Weighted-average F1 score: 0.635391858213624
F1 score per class: {32: 0.48109965635738833, 6: 0.736, 19: 0.12195121951219512, 24: 0.6934673366834171, 26: 0.8444444444444444, 29: 0.6567164179104478}
Micro-average F1 score: 0.6070150322118826
Weighted-average F1 score: 0.5740979436964855
F1 score per class: {32: 0.4878048780487805, 6: 0.736, 19: 0.1651376146788991, 24: 0.700507614213198, 26: 0.8493150684931506, 29: 0.6472727272727272}
Micro-average F1 score: 0.631264023934181
Weighted-average F1 score: 0.6092032895396754

F1 score per class: {32: 0.4274193548387097, 6: 0.773109243697479, 19: 0.24615384615384617, 24: 0.6979166666666666, 26: 0.8516746411483254, 29: 0.6397058823529411}
Micro-average F1 score: 0.6470588235294118
Weighted-average F1 score: 0.635391858213624
F1 score per class: {32: 0.48109965635738833, 6: 0.736, 19: 0.12195121951219512, 24: 0.6934673366834171, 26: 0.8444444444444444, 29: 0.6567164179104478}
Micro-average F1 score: 0.6070150322118826
Weighted-average F1 score: 0.5740979436964855
F1 score per class: {32: 0.4878048780487805, 6: 0.736, 19: 0.1651376146788991, 24: 0.700507614213198, 26: 0.8493150684931506, 29: 0.6472727272727272}
Micro-average F1 score: 0.631264023934181
Weighted-average F1 score: 0.6092032895396754
cur_acc_wo_na:  ['0.7660']
his_acc_wo_na:  ['0.7660']
cur_acc des_wo_na:  ['0.7485']
his_acc des_wo_na:  ['0.7485']
cur_acc rrf_wo_na:  ['0.7722']
his_acc rrf_wo_na:  ['0.7722']
cur_acc_w_na:  ['0.6471']
his_acc_w_na:  ['0.6471']
cur_acc des_w_na:  ['0.6070']
his_acc des_w_na:  ['0.6070']
cur_acc rrf_w_na:  ['0.6313']
his_acc rrf_w_na:  ['0.6313']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 83.5936512CurrentTrain: epoch  0, batch     1 | loss: 94.2364671CurrentTrain: epoch  0, batch     2 | loss: 94.4286126CurrentTrain: epoch  0, batch     3 | loss: 70.1457869CurrentTrain: epoch  1, batch     0 | loss: 136.4377139CurrentTrain: epoch  1, batch     1 | loss: 71.7649662CurrentTrain: epoch  1, batch     2 | loss: 109.7520394CurrentTrain: epoch  1, batch     3 | loss: 63.6651780CurrentTrain: epoch  2, batch     0 | loss: 103.0998385CurrentTrain: epoch  2, batch     1 | loss: 87.7351471CurrentTrain: epoch  2, batch     2 | loss: 84.4936453CurrentTrain: epoch  2, batch     3 | loss: 60.6930029CurrentTrain: epoch  3, batch     0 | loss: 101.9069424CurrentTrain: epoch  3, batch     1 | loss: 68.2490503CurrentTrain: epoch  3, batch     2 | loss: 101.1648301CurrentTrain: epoch  3, batch     3 | loss: 59.6375353CurrentTrain: epoch  4, batch     0 | loss: 95.4077914CurrentTrain: epoch  4, batch     1 | loss: 96.6672391CurrentTrain: epoch  4, batch     2 | loss: 67.9087439CurrentTrain: epoch  4, batch     3 | loss: 78.7688236CurrentTrain: epoch  5, batch     0 | loss: 125.0308602CurrentTrain: epoch  5, batch     1 | loss: 79.5396395CurrentTrain: epoch  5, batch     2 | loss: 67.6928224CurrentTrain: epoch  5, batch     3 | loss: 44.4838353CurrentTrain: epoch  6, batch     0 | loss: 67.8060044CurrentTrain: epoch  6, batch     1 | loss: 67.0035472CurrentTrain: epoch  6, batch     2 | loss: 64.7863931CurrentTrain: epoch  6, batch     3 | loss: 101.4297715CurrentTrain: epoch  7, batch     0 | loss: 118.4039066CurrentTrain: epoch  7, batch     1 | loss: 76.5806552CurrentTrain: epoch  7, batch     2 | loss: 77.2509274CurrentTrain: epoch  7, batch     3 | loss: 46.8214465CurrentTrain: epoch  8, batch     0 | loss: 90.9336322CurrentTrain: epoch  8, batch     1 | loss: 115.3558274CurrentTrain: epoch  8, batch     2 | loss: 93.1162377CurrentTrain: epoch  8, batch     3 | loss: 37.2637934CurrentTrain: epoch  9, batch     0 | loss: 64.2413611CurrentTrain: epoch  9, batch     1 | loss: 63.4368128CurrentTrain: epoch  9, batch     2 | loss: 96.7825869CurrentTrain: epoch  9, batch     3 | loss: 53.3791213
MemoryTrain:  epoch  0, batch     0 | loss: 1.2402422MemoryTrain:  epoch  1, batch     0 | loss: 1.0401750MemoryTrain:  epoch  2, batch     0 | loss: 0.9059738MemoryTrain:  epoch  3, batch     0 | loss: 0.7788987MemoryTrain:  epoch  4, batch     0 | loss: 0.5971008MemoryTrain:  epoch  5, batch     0 | loss: 0.4416568MemoryTrain:  epoch  6, batch     0 | loss: 0.3786986MemoryTrain:  epoch  7, batch     0 | loss: 0.3209977MemoryTrain:  epoch  8, batch     0 | loss: 0.2625702MemoryTrain:  epoch  9, batch     0 | loss: 0.2493255

F1 score per class: {32: 0.0, 33: 0.5228758169934641, 36: 0.0, 6: 0.7045454545454546, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.8888888888888888, 26: 0.0, 29: 0.47058823529411764, 30: 0.660377358490566}
Micro-average F1 score: 0.5833333333333334
Weighted-average F1 score: 0.5271210902516087
F1 score per class: {32: 0.0, 33: 0.5972222222222222, 36: 0.0, 6: 0.7111111111111111, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.6538461538461539, 26: 0.0, 29: 0.4444444444444444, 30: 0.6814814814814815}
Micro-average F1 score: 0.5772357723577236
Weighted-average F1 score: 0.5169933195795264
F1 score per class: {32: 0.0, 33: 0.5906040268456376, 36: 0.0, 6: 0.7045454545454546, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.7804878048780488, 26: 0.0, 29: 0.4444444444444444, 30: 0.688}
Micro-average F1 score: 0.5910064239828694
Weighted-average F1 score: 0.5296163648240552

F1 score per class: {32: 0.45121951219512196, 33: 0.35714285714285715, 36: 0.788135593220339, 6: 0.6813186813186813, 8: 0.3829787234042553, 19: 0.7528089887640449, 20: 0.9064039408866995, 24: 0.8888888888888888, 26: 0.8113207547169812, 29: 0.38095238095238093, 30: 0.625}
Micro-average F1 score: 0.6692913385826772
Weighted-average F1 score: 0.6633140112274316
F1 score per class: {32: 0.6462882096069869, 33: 0.5029239766081871, 36: 0.7410358565737052, 6: 0.6736842105263158, 8: 0.22784810126582278, 19: 0.7272727272727273, 20: 0.8985507246376812, 24: 0.4473684210526316, 26: 0.7837837837837838, 29: 0.21052631578947367, 30: 0.6013071895424836}
Micro-average F1 score: 0.6627634660421545
Weighted-average F1 score: 0.6471196564468763
F1 score per class: {32: 0.5463917525773195, 33: 0.42105263157894735, 36: 0.7469879518072289, 6: 0.6739130434782609, 8: 0.2898550724637681, 19: 0.7472527472527473, 20: 0.9029126213592233, 24: 0.6808510638297872, 26: 0.7909090909090909, 29: 0.25, 30: 0.6417910447761194}
Micro-average F1 score: 0.6634026927784578
Weighted-average F1 score: 0.6517471702049497

F1 score per class: {32: 0.0, 33: 0.4166666666666667, 36: 0.0, 6: 0.5535714285714286, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.8421052631578947, 26: 0.0, 29: 0.38095238095238093, 30: 0.5982905982905983}
Micro-average F1 score: 0.4540540540540541
Weighted-average F1 score: 0.39715891104779993
F1 score per class: {32: 0.0, 33: 0.47513812154696133, 36: 0.0, 6: 0.5765765765765766, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.5666666666666667, 26: 0.0, 29: 0.23529411764705882, 30: 0.5476190476190477}
Micro-average F1 score: 0.42388059701492536
Weighted-average F1 score: 0.37558295652829055
F1 score per class: {32: 0.0, 33: 0.45595854922279794, 36: 0.0, 6: 0.5688073394495413, 8: 0.0, 19: 0.0, 20: 0.0, 24: 0.7111111111111111, 26: 0.0, 29: 0.2857142857142857, 30: 0.5548387096774193}
Micro-average F1 score: 0.43879173290937995
Weighted-average F1 score: 0.3896443168090457

F1 score per class: {32: 0.34579439252336447, 33: 0.25806451612903225, 36: 0.6992481203007519, 6: 0.4732824427480916, 8: 0.21176470588235294, 19: 0.6802030456852792, 20: 0.7965367965367965, 24: 0.8421052631578947, 26: 0.6254545454545455, 29: 0.25806451612903225, 30: 0.546875}
Micro-average F1 score: 0.5351521511017838
Weighted-average F1 score: 0.5186444320240069
F1 score per class: {32: 0.4265129682997118, 33: 0.3659574468085106, 36: 0.6480836236933798, 6: 0.48120300751879697, 8: 0.1276595744680851, 19: 0.6384976525821596, 20: 0.775, 24: 0.33663366336633666, 26: 0.5938566552901023, 29: 0.12903225806451613, 30: 0.4816753926701571}
Micro-average F1 score: 0.5046812304948729
Weighted-average F1 score: 0.4861268753123389
F1 score per class: {32: 0.39552238805970147, 33: 0.28664495114006516, 36: 0.6480836236933798, 6: 0.47692307692307695, 8: 0.15625, 19: 0.6570048309178744, 20: 0.7848101265822784, 24: 0.5818181818181818, 26: 0.6062717770034843, 29: 0.16, 30: 0.5088757396449705}
Micro-average F1 score: 0.5101176470588236
Weighted-average F1 score: 0.4917483967347553
cur_acc_wo_na:  ['0.7660', '0.5833']
his_acc_wo_na:  ['0.7660', '0.6693']
cur_acc des_wo_na:  ['0.7485', '0.5772']
his_acc des_wo_na:  ['0.7485', '0.6628']
cur_acc rrf_wo_na:  ['0.7722', '0.5910']
his_acc rrf_wo_na:  ['0.7722', '0.6634']
cur_acc_w_na:  ['0.6471', '0.4541']
his_acc_w_na:  ['0.6471', '0.5352']
cur_acc des_w_na:  ['0.6070', '0.4239']
his_acc des_w_na:  ['0.6070', '0.5047']
cur_acc rrf_w_na:  ['0.6313', '0.4388']
his_acc rrf_w_na:  ['0.6313', '0.5101']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 103.0043604CurrentTrain: epoch  0, batch     1 | loss: 96.4309152CurrentTrain: epoch  0, batch     2 | loss: 90.9551962CurrentTrain: epoch  0, batch     3 | loss: 114.4905825CurrentTrain: epoch  0, batch     4 | loss: 21.3077374CurrentTrain: epoch  1, batch     0 | loss: 107.3471843CurrentTrain: epoch  1, batch     1 | loss: 73.3105687CurrentTrain: epoch  1, batch     2 | loss: 90.1917044CurrentTrain: epoch  1, batch     3 | loss: 84.8562745CurrentTrain: epoch  1, batch     4 | loss: 21.6665884CurrentTrain: epoch  2, batch     0 | loss: 86.0273832CurrentTrain: epoch  2, batch     1 | loss: 87.5410412CurrentTrain: epoch  2, batch     2 | loss: 81.9325608CurrentTrain: epoch  2, batch     3 | loss: 125.4547725CurrentTrain: epoch  2, batch     4 | loss: 43.5468283CurrentTrain: epoch  3, batch     0 | loss: 71.0396483CurrentTrain: epoch  3, batch     1 | loss: 68.6397587CurrentTrain: epoch  3, batch     2 | loss: 100.4216126CurrentTrain: epoch  3, batch     3 | loss: 100.0173969CurrentTrain: epoch  3, batch     4 | loss: 17.4904394CurrentTrain: epoch  4, batch     0 | loss: 85.5881887CurrentTrain: epoch  4, batch     1 | loss: 98.1766484CurrentTrain: epoch  4, batch     2 | loss: 80.9162019CurrentTrain: epoch  4, batch     3 | loss: 76.0554080CurrentTrain: epoch  4, batch     4 | loss: 23.4035016CurrentTrain: epoch  5, batch     0 | loss: 67.0033851CurrentTrain: epoch  5, batch     1 | loss: 96.7723522CurrentTrain: epoch  5, batch     2 | loss: 84.3330671CurrentTrain: epoch  5, batch     3 | loss: 69.3900873CurrentTrain: epoch  5, batch     4 | loss: 12.4667641CurrentTrain: epoch  6, batch     0 | loss: 94.2155482CurrentTrain: epoch  6, batch     1 | loss: 61.8486868CurrentTrain: epoch  6, batch     2 | loss: 79.3648404CurrentTrain: epoch  6, batch     3 | loss: 123.8621275CurrentTrain: epoch  6, batch     4 | loss: 24.2635646CurrentTrain: epoch  7, batch     0 | loss: 119.9651761CurrentTrain: epoch  7, batch     1 | loss: 122.7043268CurrentTrain: epoch  7, batch     2 | loss: 64.1127625CurrentTrain: epoch  7, batch     3 | loss: 65.0377995CurrentTrain: epoch  7, batch     4 | loss: 18.1110503CurrentTrain: epoch  8, batch     0 | loss: 75.5932859CurrentTrain: epoch  8, batch     1 | loss: 120.9593057CurrentTrain: epoch  8, batch     2 | loss: 62.4236712CurrentTrain: epoch  8, batch     3 | loss: 95.5279883CurrentTrain: epoch  8, batch     4 | loss: 23.4816853CurrentTrain: epoch  9, batch     0 | loss: 65.7769158CurrentTrain: epoch  9, batch     1 | loss: 118.8324809CurrentTrain: epoch  9, batch     2 | loss: 94.9636289CurrentTrain: epoch  9, batch     3 | loss: 76.9697392CurrentTrain: epoch  9, batch     4 | loss: 8.4702305
MemoryTrain:  epoch  0, batch     0 | loss: 1.3305254MemoryTrain:  epoch  1, batch     0 | loss: 1.1668278MemoryTrain:  epoch  2, batch     0 | loss: 0.9835095MemoryTrain:  epoch  3, batch     0 | loss: 0.7711208MemoryTrain:  epoch  4, batch     0 | loss: 0.6029179MemoryTrain:  epoch  5, batch     0 | loss: 0.5773766MemoryTrain:  epoch  6, batch     0 | loss: 0.4891247MemoryTrain:  epoch  7, batch     0 | loss: 0.4357771MemoryTrain:  epoch  8, batch     0 | loss: 0.4519111MemoryTrain:  epoch  9, batch     0 | loss: 0.3702665

F1 score per class: {32: 0.5714285714285714, 33: 0.0, 2: 0.0, 6: 0.6351351351351351, 39: 0.5521472392638037, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.25, 20: 0.0, 24: 0.0, 28: 0.6}
Micro-average F1 score: 0.5
Weighted-average F1 score: 0.4179451054491416
F1 score per class: {32: 0.5185185185185185, 33: 0.0, 2: 0.0, 36: 0.5882352941176471, 6: 0.4819277108433735, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.24, 19: 0.0, 20: 0.0, 24: 0.0, 28: 0.0, 30: 0.5217391304347826}
Micro-average F1 score: 0.39263803680981596
Weighted-average F1 score: 0.2894500316316892
F1 score per class: {32: 0.5384615384615384, 33: 0.0, 2: 0.0, 6: 0.5882352941176471, 39: 0.4727272727272727, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.23076923076923078, 20: 0.0, 24: 0.0, 28: 0.5454545454545454}
Micro-average F1 score: 0.4025423728813559
Weighted-average F1 score: 0.3029007727914959

F1 score per class: {32: 0.48, 33: 0.56, 2: 0.3274336283185841, 36: 0.4253393665158371, 6: 0.35294117647058826, 39: 0.8113207547169812, 8: 0.6373626373626373, 11: 0.35555555555555557, 12: 0.75, 19: 0.13043478260869565, 20: 0.7954545454545454, 24: 0.8648648648648649, 26: 0.7416666666666667, 28: 0.23529411764705882, 29: 0.3037974683544304, 30: 0.375}
Micro-average F1 score: 0.5562591329761325
Weighted-average F1 score: 0.5404675909728373
F1 score per class: {32: 0.3333333333333333, 33: 0.6184738955823293, 2: 0.3744292237442922, 36: 0.4819277108433735, 6: 0.28169014084507044, 39: 0.7428571428571429, 8: 0.5918367346938775, 11: 0.21333333333333335, 12: 0.7513812154696132, 19: 0.13043478260869565, 20: 0.8241758241758241, 24: 0.4146341463414634, 26: 0.7114624505928854, 28: 0.20689655172413793, 29: 0.5454545454545454, 30: 0.2857142857142857}
Micro-average F1 score: 0.5427830596369922
Weighted-average F1 score: 0.5202533094995666
F1 score per class: {32: 0.42424242424242425, 33: 0.6019417475728155, 2: 0.33201581027667987, 36: 0.43956043956043955, 6: 0.2899628252788104, 39: 0.7583333333333333, 8: 0.5894736842105263, 11: 0.2903225806451613, 12: 0.7555555555555555, 19: 0.12, 20: 0.8241758241758241, 24: 0.6938775510204082, 26: 0.7171314741035857, 28: 0.25, 29: 0.5544554455445545, 30: 0.3076923076923077}
Micro-average F1 score: 0.5487364620938628
Weighted-average F1 score: 0.5264778303558446

F1 score per class: {32: 0.34285714285714286, 33: 0.0, 2: 0.0, 6: 0.5340909090909091, 39: 0.47619047619047616, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.0, 20: 0.13636363636363635, 24: 0.0, 26: 0.0, 28: 0.4444444444444444}
Micro-average F1 score: 0.35785953177257523
Weighted-average F1 score: 0.2875517329343186
F1 score per class: {32: 0.3181818181818182, 33: 0.0, 2: 0.0, 36: 0.5128205128205128, 6: 0.3883495145631068, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.13043478260869565, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 30: 0.34285714285714286}
Micro-average F1 score: 0.270042194092827
Weighted-average F1 score: 0.20188969266458998
F1 score per class: {32: 0.3333333333333333, 33: 0.0, 2: 0.0, 36: 0.5, 6: 0.39, 39: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 19: 0.125, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 30: 0.375}
Micro-average F1 score: 0.28023598820059
Weighted-average F1 score: 0.21296296296296297

F1 score per class: {32: 0.2608695652173913, 33: 0.3843137254901961, 2: 0.2138728323699422, 36: 0.3333333333333333, 6: 0.22113022113022113, 39: 0.7510917030567685, 8: 0.42962962962962964, 11: 0.21333333333333335, 12: 0.6839378238341969, 19: 0.07228915662650602, 20: 0.7368421052631579, 24: 0.8, 26: 0.541033434650456, 28: 0.2222222222222222, 29: 0.2857142857142857, 30: 0.21052631578947367}
Micro-average F1 score: 0.41242325749368003
Weighted-average F1 score: 0.3853930987449157
F1 score per class: {32: 0.2, 33: 0.37745098039215685, 2: 0.24404761904761904, 36: 0.39215686274509803, 6: 0.16913319238900634, 39: 0.6642335766423357, 8: 0.4, 11: 0.125, 12: 0.6634146341463415, 19: 0.07317073170731707, 20: 0.7614213197969543, 24: 0.3063063063063063, 26: 0.5217391304347826, 28: 0.15384615384615385, 29: 0.4489795918367347, 30: 0.1348314606741573}
Micro-average F1 score: 0.3861051337227175
Weighted-average F1 score: 0.3591751722883885
F1 score per class: {32: 0.25, 33: 0.4025974025974026, 2: 0.21158690176322417, 36: 0.35555555555555557, 6: 0.17333333333333334, 39: 0.6842105263157895, 8: 0.39436619718309857, 11: 0.16071428571428573, 12: 0.68, 19: 0.06315789473684211, 20: 0.7614213197969543, 24: 0.6071428571428571, 26: 0.5247813411078717, 28: 0.1935483870967742, 29: 0.4628099173553719, 30: 0.14457831325301204}
Micro-average F1 score: 0.3945489941596366
Weighted-average F1 score: 0.36507094954103125
cur_acc_wo_na:  ['0.7660', '0.5833', '0.5000']
his_acc_wo_na:  ['0.7660', '0.6693', '0.5563']
cur_acc des_wo_na:  ['0.7485', '0.5772', '0.3926']
his_acc des_wo_na:  ['0.7485', '0.6628', '0.5428']
cur_acc rrf_wo_na:  ['0.7722', '0.5910', '0.4025']
his_acc rrf_wo_na:  ['0.7722', '0.6634', '0.5487']
cur_acc_w_na:  ['0.6471', '0.4541', '0.3579']
his_acc_w_na:  ['0.6471', '0.5352', '0.4124']
cur_acc des_w_na:  ['0.6070', '0.4239', '0.2700']
his_acc des_w_na:  ['0.6070', '0.5047', '0.3861']
cur_acc rrf_w_na:  ['0.6313', '0.4388', '0.2802']
his_acc rrf_w_na:  ['0.6313', '0.5101', '0.3945']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 87.5538496CurrentTrain: epoch  0, batch     1 | loss: 114.4100504CurrentTrain: epoch  0, batch     2 | loss: 117.5829372CurrentTrain: epoch  0, batch     3 | loss: 142.2159868CurrentTrain: epoch  0, batch     4 | loss: 86.8831758CurrentTrain: epoch  1, batch     0 | loss: 86.5297210CurrentTrain: epoch  1, batch     1 | loss: 94.5307695CurrentTrain: epoch  1, batch     2 | loss: 102.6933072CurrentTrain: epoch  1, batch     3 | loss: 134.1459214CurrentTrain: epoch  1, batch     4 | loss: 66.9490727CurrentTrain: epoch  2, batch     0 | loss: 85.9549831CurrentTrain: epoch  2, batch     1 | loss: 105.5551435CurrentTrain: epoch  2, batch     2 | loss: 84.9120011CurrentTrain: epoch  2, batch     3 | loss: 104.3188612CurrentTrain: epoch  2, batch     4 | loss: 84.9020527CurrentTrain: epoch  3, batch     0 | loss: 72.1928254CurrentTrain: epoch  3, batch     1 | loss: 83.5432185CurrentTrain: epoch  3, batch     2 | loss: 105.4010520CurrentTrain: epoch  3, batch     3 | loss: 85.1653028CurrentTrain: epoch  3, batch     4 | loss: 54.4807455CurrentTrain: epoch  4, batch     0 | loss: 100.1270101CurrentTrain: epoch  4, batch     1 | loss: 98.8616460CurrentTrain: epoch  4, batch     2 | loss: 70.9591429CurrentTrain: epoch  4, batch     3 | loss: 84.6133867CurrentTrain: epoch  4, batch     4 | loss: 64.6047431CurrentTrain: epoch  5, batch     0 | loss: 127.9852459CurrentTrain: epoch  5, batch     1 | loss: 82.2124995CurrentTrain: epoch  5, batch     2 | loss: 82.0223086CurrentTrain: epoch  5, batch     3 | loss: 97.9019064CurrentTrain: epoch  5, batch     4 | loss: 50.7619630CurrentTrain: epoch  6, batch     0 | loss: 77.4533890CurrentTrain: epoch  6, batch     1 | loss: 99.5894257CurrentTrain: epoch  6, batch     2 | loss: 97.6682965CurrentTrain: epoch  6, batch     3 | loss: 78.5628734CurrentTrain: epoch  6, batch     4 | loss: 105.0541534CurrentTrain: epoch  7, batch     0 | loss: 63.2214383CurrentTrain: epoch  7, batch     1 | loss: 98.9024183CurrentTrain: epoch  7, batch     2 | loss: 93.8379759CurrentTrain: epoch  7, batch     3 | loss: 97.8328928CurrentTrain: epoch  7, batch     4 | loss: 107.6830381CurrentTrain: epoch  8, batch     0 | loss: 62.6412404CurrentTrain: epoch  8, batch     1 | loss: 99.4095862CurrentTrain: epoch  8, batch     2 | loss: 66.6926387CurrentTrain: epoch  8, batch     3 | loss: 124.2139781CurrentTrain: epoch  8, batch     4 | loss: 76.0810993CurrentTrain: epoch  9, batch     0 | loss: 76.6437866CurrentTrain: epoch  9, batch     1 | loss: 78.1336386CurrentTrain: epoch  9, batch     2 | loss: 78.6237583CurrentTrain: epoch  9, batch     3 | loss: 120.4517660CurrentTrain: epoch  9, batch     4 | loss: 47.9315771
MemoryTrain:  epoch  0, batch     0 | loss: 1.1662331MemoryTrain:  epoch  1, batch     0 | loss: 1.1084409MemoryTrain:  epoch  2, batch     0 | loss: 0.8237410MemoryTrain:  epoch  3, batch     0 | loss: 0.7017817MemoryTrain:  epoch  4, batch     0 | loss: 0.5539364MemoryTrain:  epoch  5, batch     0 | loss: 0.5077944MemoryTrain:  epoch  6, batch     0 | loss: 0.4725729MemoryTrain:  epoch  7, batch     0 | loss: 0.4011673MemoryTrain:  epoch  8, batch     0 | loss: 0.3524735MemoryTrain:  epoch  9, batch     0 | loss: 0.2639906

F1 score per class: {2: 0.0, 5: 0.8622222222222222, 6: 0.0, 8: 0.0, 10: 0.5503355704697986, 11: 0.0, 12: 0.0, 16: 0.64, 17: 0.2, 18: 0.05, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 39: 0.0}
Micro-average F1 score: 0.5693430656934306
Weighted-average F1 score: 0.543728752686757
F1 score per class: {2: 0.0, 5: 0.751937984496124, 6: 0.0, 8: 0.0, 10: 0.4689655172413793, 11: 0.0, 12: 0.0, 16: 0.6666666666666666, 17: 0.7142857142857143, 18: 0.4, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.5234493192133132
Weighted-average F1 score: 0.474496307526437
F1 score per class: {2: 0.0, 5: 0.7619047619047619, 6: 0.0, 8: 0.0, 10: 0.4794520547945205, 11: 0.0, 12: 0.0, 16: 0.7272727272727273, 17: 0.36363636363636365, 18: 0.3, 19: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.5242718446601942
Weighted-average F1 score: 0.4785772199975516

F1 score per class: {2: 0.4, 5: 0.776, 6: 0.48, 8: 0.23880597014925373, 10: 0.3445378151260504, 11: 0.20987654320987653, 12: 0.32456140350877194, 16: 0.5925925925925926, 17: 0.125, 18: 0.03636363636363636, 19: 0.7904761904761904, 20: 0.5301204819277109, 24: 0.22857142857142856, 26: 0.7015706806282722, 28: 0.08955223880597014, 29: 0.7586206896551724, 30: 0.8421052631578947, 32: 0.6987951807228916, 33: 0.14285714285714285, 36: 0.13513513513513514, 39: 0.2702702702702703}
Micro-average F1 score: 0.5037863690713431
Weighted-average F1 score: 0.514744514907125
F1 score per class: {2: 0.4666666666666667, 5: 0.6198083067092651, 6: 0.5071428571428571, 8: 0.24817518248175183, 10: 0.34, 11: 0.18791946308724833, 12: 0.2857142857142857, 16: 0.5411764705882353, 17: 0.29411764705882354, 18: 0.18064516129032257, 19: 0.7389558232931727, 20: 0.5263157894736842, 24: 0.24242424242424243, 26: 0.7106598984771574, 28: 0.11538461538461539, 29: 0.8152173913043478, 30: 0.38636363636363635, 32: 0.6923076923076923, 33: 0.16216216216216217, 36: 0.4778761061946903, 39: 0.20833333333333334}
Micro-average F1 score: 0.4825106243870546
Weighted-average F1 score: 0.4745592835234175
F1 score per class: {2: 0.4666666666666667, 5: 0.6442953020134228, 6: 0.5445544554455446, 8: 0.33766233766233766, 10: 0.3211009174311927, 11: 0.2413793103448276, 12: 0.291970802919708, 16: 0.625, 17: 0.16, 18: 0.16666666666666666, 19: 0.7520661157024794, 20: 0.5168539325842697, 24: 0.2641509433962264, 26: 0.7046632124352331, 28: 0.09523809523809523, 29: 0.8021978021978022, 30: 0.6938775510204082, 32: 0.7, 33: 0.21428571428571427, 36: 0.4158415841584158, 39: 0.18604651162790697}
Micro-average F1 score: 0.49964912280701756
Weighted-average F1 score: 0.49174887777798054

F1 score per class: {2: 0.0, 5: 0.6554054054054054, 6: 0.0, 8: 0.0, 10: 0.44324324324324327, 11: 0.0, 12: 0.0, 16: 0.43243243243243246, 17: 0.2, 18: 0.045454545454545456, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 39: 0.0}
Micro-average F1 score: 0.3851851851851852
Weighted-average F1 score: 0.34771599996319097
F1 score per class: {2: 0.0, 5: 0.5739644970414202, 6: 0.0, 8: 0.0, 10: 0.3953488372093023, 11: 0.0, 12: 0.0, 16: 0.42592592592592593, 17: 0.5555555555555556, 18: 0.2413793103448276, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.33690360272638753
Weighted-average F1 score: 0.29608502571823087
F1 score per class: {2: 0.0, 5: 0.5835866261398176, 6: 0.0, 8: 0.0, 10: 0.39325842696629215, 11: 0.0, 12: 0.0, 16: 0.43478260869565216, 17: 0.26666666666666666, 18: 0.19148936170212766, 19: 0.0, 20: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.0}
Micro-average F1 score: 0.3364485981308411
Weighted-average F1 score: 0.2966330568348482

F1 score per class: {2: 0.24390243902439024, 5: 0.5118733509234829, 6: 0.2896551724137931, 8: 0.2064516129032258, 10: 0.22969187675070027, 11: 0.17525773195876287, 12: 0.18092909535452323, 16: 0.3950617283950617, 17: 0.08, 18: 0.02666666666666667, 19: 0.7186147186147186, 20: 0.3826086956521739, 24: 0.13333333333333333, 26: 0.6291079812206573, 28: 0.04918032786885246, 29: 0.6839378238341969, 30: 0.7804878048780488, 32: 0.5132743362831859, 33: 0.125, 36: 0.12987012987012986, 39: 0.13333333333333333}
Micro-average F1 score: 0.3623853211009174
Weighted-average F1 score: 0.35218181248407193
F1 score per class: {2: 0.2641509433962264, 5: 0.41541755888650966, 6: 0.2892057026476578, 8: 0.19318181818181818, 10: 0.24372759856630824, 11: 0.1686746987951807, 12: 0.1680327868852459, 16: 0.3150684931506849, 17: 0.17857142857142858, 18: 0.10486891385767791, 19: 0.647887323943662, 20: 0.3448275862068966, 24: 0.13675213675213677, 26: 0.5957446808510638, 28: 0.06666666666666667, 29: 0.7109004739336493, 30: 0.2857142857142857, 32: 0.5113636363636364, 33: 0.11320754716981132, 36: 0.3698630136986301, 39: 0.10869565217391304}
Micro-average F1 score: 0.33295736521542973
Weighted-average F1 score: 0.318514300137514
F1 score per class: {2: 0.2641509433962264, 5: 0.43537414965986393, 6: 0.3151862464183381, 8: 0.26262626262626265, 10: 0.2280130293159609, 11: 0.2028985507246377, 12: 0.16632016632016633, 16: 0.34782608695652173, 17: 0.09302325581395349, 18: 0.0972972972972973, 19: 0.6715867158671587, 20: 0.3382352941176471, 24: 0.14285714285714285, 26: 0.6098654708520179, 28: 0.05172413793103448, 29: 0.7121951219512195, 30: 0.6415094339622641, 32: 0.5155807365439093, 33: 0.16666666666666666, 36: 0.34710743801652894, 39: 0.09302325581395349}
Micro-average F1 score: 0.3492764287466274
Weighted-average F1 score: 0.3327664417729338
cur_acc_wo_na:  ['0.7660', '0.5833', '0.5000', '0.5693']
his_acc_wo_na:  ['0.7660', '0.6693', '0.5563', '0.5038']
cur_acc des_wo_na:  ['0.7485', '0.5772', '0.3926', '0.5234']
his_acc des_wo_na:  ['0.7485', '0.6628', '0.5428', '0.4825']
cur_acc rrf_wo_na:  ['0.7722', '0.5910', '0.4025', '0.5243']
his_acc rrf_wo_na:  ['0.7722', '0.6634', '0.5487', '0.4996']
cur_acc_w_na:  ['0.6471', '0.4541', '0.3579', '0.3852']
his_acc_w_na:  ['0.6471', '0.5352', '0.4124', '0.3624']
cur_acc des_w_na:  ['0.6070', '0.4239', '0.2700', '0.3369']
his_acc des_w_na:  ['0.6070', '0.5047', '0.3861', '0.3330']
cur_acc rrf_w_na:  ['0.6313', '0.4388', '0.2802', '0.3364']
his_acc rrf_w_na:  ['0.6313', '0.5101', '0.3945', '0.3493']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 90.0489490CurrentTrain: epoch  0, batch     1 | loss: 102.1740090CurrentTrain: epoch  0, batch     2 | loss: 83.9139384CurrentTrain: epoch  0, batch     3 | loss: 144.7352210CurrentTrain: epoch  0, batch     4 | loss: 70.5166743CurrentTrain: epoch  1, batch     0 | loss: 81.8279683CurrentTrain: epoch  1, batch     1 | loss: 106.0799640CurrentTrain: epoch  1, batch     2 | loss: 179.3956727CurrentTrain: epoch  1, batch     3 | loss: 95.3625825CurrentTrain: epoch  1, batch     4 | loss: 60.5678387CurrentTrain: epoch  2, batch     0 | loss: 88.6108617CurrentTrain: epoch  2, batch     1 | loss: 90.1508414CurrentTrain: epoch  2, batch     2 | loss: 87.7429604CurrentTrain: epoch  2, batch     3 | loss: 104.6736319CurrentTrain: epoch  2, batch     4 | loss: 77.0232172CurrentTrain: epoch  3, batch     0 | loss: 68.5675766CurrentTrain: epoch  3, batch     1 | loss: 71.1174990CurrentTrain: epoch  3, batch     2 | loss: 88.4698059CurrentTrain: epoch  3, batch     3 | loss: 87.1550001CurrentTrain: epoch  3, batch     4 | loss: 148.3269470CurrentTrain: epoch  4, batch     0 | loss: 82.5664593CurrentTrain: epoch  4, batch     1 | loss: 85.3438898CurrentTrain: epoch  4, batch     2 | loss: 84.3879979CurrentTrain: epoch  4, batch     3 | loss: 83.3095762CurrentTrain: epoch  4, batch     4 | loss: 70.0274720CurrentTrain: epoch  5, batch     0 | loss: 84.0693461CurrentTrain: epoch  5, batch     1 | loss: 127.0153967CurrentTrain: epoch  5, batch     2 | loss: 98.3141844CurrentTrain: epoch  5, batch     3 | loss: 66.9595490CurrentTrain: epoch  5, batch     4 | loss: 48.2252427CurrentTrain: epoch  6, batch     0 | loss: 79.6549414CurrentTrain: epoch  6, batch     1 | loss: 82.5688596CurrentTrain: epoch  6, batch     2 | loss: 68.5068710CurrentTrain: epoch  6, batch     3 | loss: 81.8532835CurrentTrain: epoch  6, batch     4 | loss: 54.8596414CurrentTrain: epoch  7, batch     0 | loss: 78.0065066CurrentTrain: epoch  7, batch     1 | loss: 80.0868254CurrentTrain: epoch  7, batch     2 | loss: 82.0607642CurrentTrain: epoch  7, batch     3 | loss: 69.1435506CurrentTrain: epoch  7, batch     4 | loss: 68.5528601CurrentTrain: epoch  8, batch     0 | loss: 93.7698209CurrentTrain: epoch  8, batch     1 | loss: 123.4987475CurrentTrain: epoch  8, batch     2 | loss: 120.2613439CurrentTrain: epoch  8, batch     3 | loss: 65.9788634CurrentTrain: epoch  8, batch     4 | loss: 50.4659349CurrentTrain: epoch  9, batch     0 | loss: 67.2519629CurrentTrain: epoch  9, batch     1 | loss: 81.5577616CurrentTrain: epoch  9, batch     2 | loss: 76.6953070CurrentTrain: epoch  9, batch     3 | loss: 94.7186001CurrentTrain: epoch  9, batch     4 | loss: 69.0353100
MemoryTrain:  epoch  0, batch     0 | loss: 1.2344124MemoryTrain:  epoch  1, batch     0 | loss: 1.0607998MemoryTrain:  epoch  2, batch     0 | loss: 0.8924742MemoryTrain:  epoch  3, batch     0 | loss: 0.8438601MemoryTrain:  epoch  4, batch     0 | loss: 0.5952736MemoryTrain:  epoch  5, batch     0 | loss: 0.4830563MemoryTrain:  epoch  6, batch     0 | loss: 0.4165760MemoryTrain:  epoch  7, batch     0 | loss: 0.3616222MemoryTrain:  epoch  8, batch     0 | loss: 0.2969565MemoryTrain:  epoch  9, batch     0 | loss: 0.2556961

F1 score per class: {1: 0.2517482517482518, 3: 0.6971428571428572, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.04878048780487805, 18: 0.0, 19: 0.0, 22: 0.4186046511627907, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.6614173228346457, 39: 0.0}
Micro-average F1 score: 0.39235412474849096
Weighted-average F1 score: 0.3694232788345248
F1 score per class: {1: 0.23170731707317074, 3: 0.6517857142857143, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.07476635514018691, 16: 0.0, 18: 0.0, 19: 0.0, 22: 0.4596774193548387, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.6891891891891891}
Micro-average F1 score: 0.3763837638376384
Weighted-average F1 score: 0.34527636723046023
F1 score per class: {1: 0.23076923076923078, 2: 0.0, 3: 0.6893203883495146, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.06, 18: 0.0, 19: 0.0, 22: 0.45692883895131087, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.6712328767123288, 39: 0.0}
Micro-average F1 score: 0.384395813510942
Weighted-average F1 score: 0.3545073303491306

F1 score per class: {1: 0.21686746987951808, 2: 0.47619047619047616, 3: 0.5622119815668203, 5: 0.852017937219731, 6: 0.4946236559139785, 8: 0.20869565217391303, 10: 0.3858267716535433, 11: 0.06779661016949153, 12: 0.20618556701030927, 14: 0.03225806451612903, 16: 0.5660377358490566, 17: 0.14285714285714285, 18: 0.03508771929824561, 19: 0.6721991701244814, 20: 0.525, 22: 0.314410480349345, 24: 0.0, 26: 0.7340425531914894, 28: 0.1016949152542373, 29: 0.6904761904761905, 30: 0.8333333333333334, 32: 0.6062992125984252, 33: 0.0, 34: 0.20588235294117646, 36: 0.16, 39: 0.3333333333333333}
Micro-average F1 score: 0.4117181314330958
Weighted-average F1 score: 0.403487715925221
F1 score per class: {1: 0.1890547263681592, 2: 0.4666666666666667, 3: 0.46645367412140576, 5: 0.697841726618705, 6: 0.5307692307692308, 8: 0.1951219512195122, 10: 0.40930232558139534, 11: 0.03418803418803419, 12: 0.24896265560165975, 14: 0.04878048780487805, 16: 0.475, 17: 0.16666666666666666, 18: 0.03333333333333333, 19: 0.6385964912280702, 20: 0.49382716049382713, 22: 0.3864406779661017, 24: 0.10810810810810811, 26: 0.7120418848167539, 28: 0.14285714285714285, 29: 0.7582417582417582, 30: 0.7083333333333334, 32: 0.5490196078431373, 33: 0.12903225806451613, 34: 0.19615384615384615, 36: 0.5984251968503937, 39: 0.1568627450980392}
Micro-average F1 score: 0.41056533827618164
Weighted-average F1 score: 0.40021100966329365
F1 score per class: {1: 0.1945945945945946, 2: 0.38461538461538464, 3: 0.5017667844522968, 5: 0.7376425855513308, 6: 0.5221674876847291, 8: 0.23255813953488372, 10: 0.39655172413793105, 11: 0.034482758620689655, 12: 0.24034334763948498, 14: 0.0375, 16: 0.5901639344262295, 17: 0.1, 18: 0.03389830508474576, 19: 0.6417910447761194, 20: 0.5301204819277109, 22: 0.3719512195121951, 24: 0.07407407407407407, 26: 0.723404255319149, 28: 0.15625, 29: 0.7513812154696132, 30: 0.8421052631578947, 32: 0.5782312925170068, 33: 0.0, 34: 0.1798165137614679, 36: 0.4166666666666667, 39: 0.2692307692307692}
Micro-average F1 score: 0.407514450867052
Weighted-average F1 score: 0.3937856452339358

F1 score per class: {1: 0.13636363636363635, 2: 0.0, 3: 0.5520361990950227, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.04819277108433735, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.3037974683544304, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.5121951219512195, 39: 0.0}
Micro-average F1 score: 0.26952315134761573
Weighted-average F1 score: 0.25108206641686515
F1 score per class: {1: 0.13240418118466898, 2: 0.0, 3: 0.4506172839506173, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.07017543859649122, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.3352941176470588, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.49514563106796117, 39: 0.0}
Micro-average F1 score: 0.24862888482632542
Weighted-average F1 score: 0.23120978434321862
F1 score per class: {1: 0.13138686131386862, 2: 0.0, 3: 0.49477351916376305, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.05660377358490566, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.33516483516483514, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.4803921568627451, 39: 0.0}
Micro-average F1 score: 0.25748884639898023
Weighted-average F1 score: 0.23996742530551593

F1 score per class: {1: 0.11285266457680251, 2: 0.24390243902439024, 3: 0.3696969696969697, 5: 0.6168831168831169, 6: 0.30564784053156147, 8: 0.1875, 10: 0.2692307692307692, 11: 0.06299212598425197, 12: 0.1282051282051282, 14: 0.028985507246376812, 16: 0.4, 17: 0.09523809523809523, 18: 0.02702702702702703, 19: 0.5955882352941176, 20: 0.3925233644859813, 22: 0.21333333333333335, 24: 0.0, 26: 0.6540284360189573, 28: 0.05309734513274336, 29: 0.5979381443298969, 30: 0.7894736842105263, 32: 0.4502923976608187, 33: 0.0, 34: 0.11764705882352941, 36: 0.14634146341463414, 39: 0.15384615384615385}
Micro-average F1 score: 0.2879822780136607
Weighted-average F1 score: 0.27366068050460474
F1 score per class: {1: 0.10526315789473684, 2: 0.25925925925925924, 3: 0.26887661141804786, 5: 0.470873786407767, 6: 0.2967741935483871, 8: 0.17266187050359713, 10: 0.28852459016393445, 11: 0.03125, 12: 0.1366742596810934, 14: 0.0427807486631016, 16: 0.2835820895522388, 17: 0.0975609756097561, 18: 0.022988505747126436, 19: 0.5498489425981873, 20: 0.3669724770642202, 22: 0.2638888888888889, 24: 0.08163265306122448, 26: 0.6071428571428571, 28: 0.07207207207207207, 29: 0.6540284360189573, 30: 0.6296296296296297, 32: 0.38443935926773454, 33: 0.08163265306122448, 34: 0.11245865490628446, 36: 0.4367816091954023, 39: 0.08421052631578947}
Micro-average F1 score: 0.2735412164248225
Weighted-average F1 score: 0.26113850138390704
F1 score per class: {1: 0.10714285714285714, 2: 0.2127659574468085, 3: 0.3008474576271186, 5: 0.49238578680203043, 6: 0.3037249283667622, 8: 0.20689655172413793, 10: 0.2754491017964072, 11: 0.03225806451612903, 12: 0.13053613053613053, 14: 0.03260869565217391, 16: 0.35294117647058826, 17: 0.06451612903225806, 18: 0.023809523809523808, 19: 0.5620915032679739, 20: 0.38596491228070173, 22: 0.2557651991614256, 24: 0.05555555555555555, 26: 0.6415094339622641, 28: 0.08264462809917356, 29: 0.6538461538461539, 30: 0.8, 32: 0.4116222760290557, 33: 0.0, 34: 0.10348468848996832, 36: 0.32, 39: 0.1414141414141414}
Micro-average F1 score: 0.27521145087833443
Weighted-average F1 score: 0.2604351364389129
cur_acc_wo_na:  ['0.7660', '0.5833', '0.5000', '0.5693', '0.3924']
his_acc_wo_na:  ['0.7660', '0.6693', '0.5563', '0.5038', '0.4117']
cur_acc des_wo_na:  ['0.7485', '0.5772', '0.3926', '0.5234', '0.3764']
his_acc des_wo_na:  ['0.7485', '0.6628', '0.5428', '0.4825', '0.4106']
cur_acc rrf_wo_na:  ['0.7722', '0.5910', '0.4025', '0.5243', '0.3844']
his_acc rrf_wo_na:  ['0.7722', '0.6634', '0.5487', '0.4996', '0.4075']
cur_acc_w_na:  ['0.6471', '0.4541', '0.3579', '0.3852', '0.2695']
his_acc_w_na:  ['0.6471', '0.5352', '0.4124', '0.3624', '0.2880']
cur_acc des_w_na:  ['0.6070', '0.4239', '0.2700', '0.3369', '0.2486']
his_acc des_w_na:  ['0.6070', '0.5047', '0.3861', '0.3330', '0.2735']
cur_acc rrf_w_na:  ['0.6313', '0.4388', '0.2802', '0.3364', '0.2575']
his_acc rrf_w_na:  ['0.6313', '0.5101', '0.3945', '0.3493', '0.2752']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 84.0479078CurrentTrain: epoch  0, batch     1 | loss: 101.3230456CurrentTrain: epoch  0, batch     2 | loss: 77.8004751CurrentTrain: epoch  0, batch     3 | loss: 17.9088123CurrentTrain: epoch  1, batch     0 | loss: 79.6968893CurrentTrain: epoch  1, batch     1 | loss: 73.6260314CurrentTrain: epoch  1, batch     2 | loss: 101.7359407CurrentTrain: epoch  1, batch     3 | loss: 12.3222629CurrentTrain: epoch  2, batch     0 | loss: 71.3939829CurrentTrain: epoch  2, batch     1 | loss: 67.6100657CurrentTrain: epoch  2, batch     2 | loss: 84.9884761CurrentTrain: epoch  2, batch     3 | loss: 11.8443959CurrentTrain: epoch  3, batch     0 | loss: 79.6385303CurrentTrain: epoch  3, batch     1 | loss: 67.1820462CurrentTrain: epoch  3, batch     2 | loss: 98.3171973CurrentTrain: epoch  3, batch     3 | loss: 20.9537384CurrentTrain: epoch  4, batch     0 | loss: 82.5908975CurrentTrain: epoch  4, batch     1 | loss: 65.8908680CurrentTrain: epoch  4, batch     2 | loss: 62.8552986CurrentTrain: epoch  4, batch     3 | loss: 6.7543928CurrentTrain: epoch  5, batch     0 | loss: 62.5137430CurrentTrain: epoch  5, batch     1 | loss: 78.5160287CurrentTrain: epoch  5, batch     2 | loss: 75.2030230CurrentTrain: epoch  5, batch     3 | loss: 22.0723493CurrentTrain: epoch  6, batch     0 | loss: 64.7730247CurrentTrain: epoch  6, batch     1 | loss: 67.3453974CurrentTrain: epoch  6, batch     2 | loss: 63.7405289CurrentTrain: epoch  6, batch     3 | loss: 4.6491028CurrentTrain: epoch  7, batch     0 | loss: 62.7154554CurrentTrain: epoch  7, batch     1 | loss: 65.4606888CurrentTrain: epoch  7, batch     2 | loss: 64.3532816CurrentTrain: epoch  7, batch     3 | loss: 19.6534008CurrentTrain: epoch  8, batch     0 | loss: 62.4978894CurrentTrain: epoch  8, batch     1 | loss: 61.5248823CurrentTrain: epoch  8, batch     2 | loss: 75.6637517CurrentTrain: epoch  8, batch     3 | loss: 20.5564927CurrentTrain: epoch  9, batch     0 | loss: 62.5240352CurrentTrain: epoch  9, batch     1 | loss: 62.6924670CurrentTrain: epoch  9, batch     2 | loss: 61.2810482CurrentTrain: epoch  9, batch     3 | loss: 15.7583180
MemoryTrain:  epoch  0, batch     0 | loss: 0.7946314MemoryTrain:  epoch  1, batch     0 | loss: 0.6680496MemoryTrain:  epoch  2, batch     0 | loss: 0.5768219MemoryTrain:  epoch  3, batch     0 | loss: 0.4571665MemoryTrain:  epoch  4, batch     0 | loss: 0.3537475MemoryTrain:  epoch  5, batch     0 | loss: 0.3172685MemoryTrain:  epoch  6, batch     0 | loss: 0.2695245MemoryTrain:  epoch  7, batch     0 | loss: 0.2359241MemoryTrain:  epoch  8, batch     0 | loss: 0.2125772MemoryTrain:  epoch  9, batch     0 | loss: 0.1997141

F1 score per class: {32: 0.0, 1: 0.0, 34: 0.8, 3: 0.96, 7: 0.0, 40: 0.0, 9: 0.0, 10: 0.0, 12: 0.0, 16: 0.0, 19: 0.0, 22: 0.6666666666666666, 26: 0.0, 27: 0.0, 31: 0.41379310344827586}
Micro-average F1 score: 0.39114391143911437
Weighted-average F1 score: 0.30298557727505676
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.6666666666666666, 9: 0.6944444444444444, 10: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 31: 0.3333333333333333, 32: 0.0, 34: 0.0, 40: 0.6376811594202898}
Micro-average F1 score: 0.4582043343653251
Weighted-average F1 score: 0.3951297875783002
F1 score per class: {32: 0.0, 1: 0.0, 34: 0.6666666666666666, 3: 0.9230769230769231, 7: 0.0, 40: 0.0, 9: 0.0, 10: 0.0, 12: 0.0, 16: 0.0, 19: 0.0, 22: 0.3333333333333333, 26: 0.0, 27: 0.0, 31: 0.6153846153846154}
Micro-average F1 score: 0.48184818481848185
Weighted-average F1 score: 0.4029304029304029

F1 score per class: {1: 0.182648401826484, 2: 0.47619047619047616, 3: 0.6122448979591837, 5: 0.7901234567901234, 6: 0.019417475728155338, 7: 0.047619047619047616, 8: 0.14432989690721648, 9: 0.96, 10: 0.29896907216494845, 11: 0.08403361344537816, 12: 0.14130434782608695, 14: 0.01834862385321101, 16: 0.56, 17: 0.0, 18: 0.03636363636363636, 19: 0.5612648221343873, 20: 0.5783132530120482, 22: 0.4296875, 24: 0.0, 26: 0.7078651685393258, 27: 0.0, 28: 0.2, 29: 0.6583850931677019, 30: 0.8333333333333334, 31: 0.2, 32: 0.6186440677966102, 33: 0.0, 34: 0.19558359621451105, 36: 0.14084507042253522, 39: 0.125, 40: 0.14201183431952663}
Micro-average F1 score: 0.3589481746234363
Weighted-average F1 score: 0.3505500016796072
F1 score per class: {1: 0.17857142857142858, 2: 0.4, 3: 0.5551020408163265, 5: 0.6378737541528239, 6: 0.32051282051282054, 7: 0.06153846153846154, 8: 0.14814814814814814, 9: 0.6172839506172839, 10: 0.34523809523809523, 11: 0.05309734513274336, 12: 0.18095238095238095, 14: 0.056338028169014086, 16: 0.4864864864864865, 17: 0.23529411764705882, 18: 0.03225806451612903, 19: 0.5714285714285714, 20: 0.5555555555555556, 22: 0.41042345276872966, 24: 0.13333333333333333, 26: 0.717948717948718, 27: 0.0, 28: 0.10638297872340426, 29: 0.6904761904761905, 30: 0.7804878048780488, 31: 0.07142857142857142, 32: 0.6070038910505836, 33: 0.12121212121212122, 34: 0.189873417721519, 36: 0.5423728813559322, 39: 0.09523809523809523, 40: 0.2875816993464052}
Micro-average F1 score: 0.37176366452755033
Weighted-average F1 score: 0.35618355341692176
F1 score per class: {1: 0.18421052631578946, 2: 0.42857142857142855, 3: 0.5569620253164557, 5: 0.6981818181818182, 6: 0.26153846153846155, 7: 0.05228758169934641, 8: 0.13725490196078433, 9: 0.8888888888888888, 10: 0.3315508021390374, 11: 0.056074766355140186, 12: 0.18269230769230768, 14: 0.031746031746031744, 16: 0.576271186440678, 17: 0.0, 18: 0.03278688524590164, 19: 0.5882352941176471, 20: 0.5747126436781609, 22: 0.4080267558528428, 24: 0.14814814814814814, 26: 0.7195767195767195, 27: 0.0, 28: 0.12987012987012986, 29: 0.6904761904761905, 30: 0.8205128205128205, 31: 0.10526315789473684, 32: 0.6015625, 33: 0.10526315789473684, 34: 0.1875, 36: 0.3617021276595745, 39: 0.1111111111111111, 40: 0.25071225071225073}
Micro-average F1 score: 0.36743119266055047
Weighted-average F1 score: 0.35043295937649876

F1 score per class: {32: 0.0, 1: 0.0, 34: 0.0, 3: 0.7272727272727273, 5: 0.9411764705882353, 7: 0.0, 40: 0.0, 9: 0.0, 12: 0.0, 10: 0.0, 16: 0.0, 19: 0.0, 22: 0.6666666666666666, 26: 0.0, 27: 0.0, 31: 0.39344262295081966}
Micro-average F1 score: 0.33865814696485624
Weighted-average F1 score: 0.25027947956961394
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.6153846153846154, 9: 0.6410256410256411, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.2857142857142857, 32: 0.0, 34: 0.0, 36: 0.0, 40: 0.6111111111111112}
Micro-average F1 score: 0.37092731829573933
Weighted-average F1 score: 0.30220851273482857
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 7: 0.6153846153846154, 9: 0.8888888888888888, 10: 0.0, 11: 0.0, 12: 0.0, 16: 0.0, 19: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 31: 0.2857142857142857, 32: 0.0, 34: 0.0, 36: 0.0, 40: 0.5906040268456376}
Micro-average F1 score: 0.3945945945945946
Weighted-average F1 score: 0.3089041539649761

F1 score per class: {1: 0.10443864229765012, 2: 0.2631578947368421, 3: 0.4181184668989547, 5: 0.5800604229607251, 6: 0.017857142857142856, 7: 0.023391812865497075, 8: 0.13592233009708737, 9: 0.9230769230769231, 10: 0.232, 11: 0.08, 12: 0.09774436090225563, 14: 0.016260162601626018, 16: 0.42424242424242425, 17: 0.0, 18: 0.025, 19: 0.5239852398523985, 20: 0.41739130434782606, 22: 0.3179190751445087, 24: 0.0, 26: 0.6395939086294417, 27: 0.0, 28: 0.1111111111111111, 29: 0.5988700564971752, 30: 0.7894736842105263, 31: 0.11764705882352941, 32: 0.4694533762057878, 33: 0.0, 34: 0.1167608286252354, 36: 0.13513513513513514, 39: 0.0625, 40: 0.1188118811881188}
Micro-average F1 score: 0.26523297491039427
Weighted-average F1 score: 0.24922690267484207
F1 score per class: {1: 0.10335917312661498, 2: 0.21875, 3: 0.3460559796437659, 5: 0.4343891402714932, 6: 0.2336448598130841, 7: 0.028985507246376812, 8: 0.13675213675213677, 9: 0.5050505050505051, 10: 0.2543859649122807, 11: 0.05084745762711865, 12: 0.11550151975683891, 14: 0.049079754601226995, 16: 0.28346456692913385, 17: 0.13333333333333333, 18: 0.0196078431372549, 19: 0.5190839694656488, 20: 0.37037037037037035, 22: 0.2889908256880734, 24: 0.11428571428571428, 26: 0.6167400881057269, 27: 0.0, 28: 0.056818181818181816, 29: 0.6270270270270271, 30: 0.6956521739130435, 31: 0.03571428571428571, 32: 0.4394366197183099, 33: 0.08695652173913043, 34: 0.11194029850746269, 36: 0.39751552795031053, 39: 0.05, 40: 0.24242424242424243}
Micro-average F1 score: 0.25881990448313047
Weighted-average F1 score: 0.2429127605212598
F1 score per class: {1: 0.10632911392405063, 2: 0.24489795918367346, 3: 0.35106382978723405, 5: 0.47642679900744417, 6: 0.2, 7: 0.02572347266881029, 8: 0.12844036697247707, 9: 0.8275862068965517, 10: 0.24701195219123506, 11: 0.05263157894736842, 12: 0.11692307692307692, 14: 0.02702702702702703, 16: 0.3617021276595745, 17: 0.0, 18: 0.0196078431372549, 19: 0.5462184873949579, 20: 0.3787878787878788, 22: 0.2890995260663507, 24: 0.125, 26: 0.6445497630331753, 27: 0.0, 28: 0.07042253521126761, 29: 0.6270270270270271, 30: 0.7619047619047619, 31: 0.04878048780487805, 32: 0.44126074498567336, 33: 0.1, 34: 0.11278195488721804, 36: 0.288135593220339, 39: 0.05333333333333334, 40: 0.2100238663484487}
Micro-average F1 score: 0.2590556274256145
Weighted-average F1 score: 0.24054320880998667
cur_acc_wo_na:  ['0.7660', '0.5833', '0.5000', '0.5693', '0.3924', '0.3911']
his_acc_wo_na:  ['0.7660', '0.6693', '0.5563', '0.5038', '0.4117', '0.3589']
cur_acc des_wo_na:  ['0.7485', '0.5772', '0.3926', '0.5234', '0.3764', '0.4582']
his_acc des_wo_na:  ['0.7485', '0.6628', '0.5428', '0.4825', '0.4106', '0.3718']
cur_acc rrf_wo_na:  ['0.7722', '0.5910', '0.4025', '0.5243', '0.3844', '0.4818']
his_acc rrf_wo_na:  ['0.7722', '0.6634', '0.5487', '0.4996', '0.4075', '0.3674']
cur_acc_w_na:  ['0.6471', '0.4541', '0.3579', '0.3852', '0.2695', '0.3387']
his_acc_w_na:  ['0.6471', '0.5352', '0.4124', '0.3624', '0.2880', '0.2652']
cur_acc des_w_na:  ['0.6070', '0.4239', '0.2700', '0.3369', '0.2486', '0.3709']
his_acc des_w_na:  ['0.6070', '0.5047', '0.3861', '0.3330', '0.2735', '0.2588']
cur_acc rrf_w_na:  ['0.6313', '0.4388', '0.2802', '0.3364', '0.2575', '0.3946']
his_acc rrf_w_na:  ['0.6313', '0.5101', '0.3945', '0.3493', '0.2752', '0.2591']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 99.5180329CurrentTrain: epoch  0, batch     1 | loss: 86.5593254CurrentTrain: epoch  0, batch     2 | loss: 86.3185609CurrentTrain: epoch  0, batch     3 | loss: 72.8611793CurrentTrain: epoch  1, batch     0 | loss: 77.5537371CurrentTrain: epoch  1, batch     1 | loss: 91.1822856CurrentTrain: epoch  1, batch     2 | loss: 88.9553600CurrentTrain: epoch  1, batch     3 | loss: 69.4699198CurrentTrain: epoch  2, batch     0 | loss: 69.8412813CurrentTrain: epoch  2, batch     1 | loss: 71.8338982CurrentTrain: epoch  2, batch     2 | loss: 83.9940782CurrentTrain: epoch  2, batch     3 | loss: 88.0233257CurrentTrain: epoch  3, batch     0 | loss: 80.4232935CurrentTrain: epoch  3, batch     1 | loss: 67.8494054CurrentTrain: epoch  3, batch     2 | loss: 103.0974756CurrentTrain: epoch  3, batch     3 | loss: 47.1980270CurrentTrain: epoch  4, batch     0 | loss: 75.4615251CurrentTrain: epoch  4, batch     1 | loss: 76.9497207CurrentTrain: epoch  4, batch     2 | loss: 97.4331185CurrentTrain: epoch  4, batch     3 | loss: 57.8333754CurrentTrain: epoch  5, batch     0 | loss: 64.2691340CurrentTrain: epoch  5, batch     1 | loss: 98.1286230CurrentTrain: epoch  5, batch     2 | loss: 78.9334331CurrentTrain: epoch  5, batch     3 | loss: 85.4860089CurrentTrain: epoch  6, batch     0 | loss: 63.2804955CurrentTrain: epoch  6, batch     1 | loss: 97.9937585CurrentTrain: epoch  6, batch     2 | loss: 81.0806973CurrentTrain: epoch  6, batch     3 | loss: 43.9907780CurrentTrain: epoch  7, batch     0 | loss: 77.6280167CurrentTrain: epoch  7, batch     1 | loss: 75.6421251CurrentTrain: epoch  7, batch     2 | loss: 65.1469863CurrentTrain: epoch  7, batch     3 | loss: 63.6521419CurrentTrain: epoch  8, batch     0 | loss: 118.0741833CurrentTrain: epoch  8, batch     1 | loss: 77.5806408CurrentTrain: epoch  8, batch     2 | loss: 66.8266611CurrentTrain: epoch  8, batch     3 | loss: 66.4747798CurrentTrain: epoch  9, batch     0 | loss: 62.6949223CurrentTrain: epoch  9, batch     1 | loss: 74.0130365CurrentTrain: epoch  9, batch     2 | loss: 80.4457670CurrentTrain: epoch  9, batch     3 | loss: 53.7788671
MemoryTrain:  epoch  0, batch     0 | loss: 0.8905822MemoryTrain:  epoch  1, batch     0 | loss: 0.7936475MemoryTrain:  epoch  2, batch     0 | loss: 0.6367514MemoryTrain:  epoch  3, batch     0 | loss: 0.5651292MemoryTrain:  epoch  4, batch     0 | loss: 0.4484940MemoryTrain:  epoch  5, batch     0 | loss: 0.4178083MemoryTrain:  epoch  6, batch     0 | loss: 0.3624021MemoryTrain:  epoch  7, batch     0 | loss: 0.3493737MemoryTrain:  epoch  8, batch     0 | loss: 0.2799769MemoryTrain:  epoch  9, batch     0 | loss: 0.2450983

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.8421052631578947, 19: 0.0, 20: 0.0, 25: 0.375, 28: 0.0, 32: 0.0, 34: 0.0, 35: 0.8380952380952381, 37: 0.6346153846153846, 38: 0.5714285714285714, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5105386416861827
Weighted-average F1 score: 0.412651182073632
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.7619047619047619, 16: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.0, 25: 0.4931506849315068, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.8076923076923077, 37: 0.6111111111111112, 38: 0.6666666666666666, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4474187380497132
Weighted-average F1 score: 0.323638123412649
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.7619047619047619, 19: 0.0, 20: 0.0, 24: 0.0, 25: 0.4788732394366197, 26: 0.0, 27: 0.0, 28: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.8301886792452831, 37: 0.6111111111111112, 38: 0.6808510638297872, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.47105788423153694
Weighted-average F1 score: 0.35136474944946006

F1 score per class: {1: 0.17959183673469387, 2: 0.38095238095238093, 3: 0.32061068702290074, 5: 0.7413127413127413, 6: 0.1391304347826087, 7: 0.049586776859504134, 8: 0.06451612903225806, 9: 0.8888888888888888, 10: 0.23333333333333334, 11: 0.05555555555555555, 12: 0.06557377049180328, 14: 0.015503875968992248, 15: 0.5, 16: 0.6, 17: 0.0, 18: 0.047619047619047616, 19: 0.5363636363636364, 20: 0.3283582089552239, 22: 0.4066390041493776, 24: 0.0, 25: 0.36923076923076925, 26: 0.7005649717514124, 27: 0.0, 28: 0.1935483870967742, 29: 0.6707317073170732, 30: 0.8108108108108109, 31: 0.125, 32: 0.5303030303030303, 33: 0.0, 34: 0.19428571428571428, 35: 0.30662020905923343, 36: 0.029850746268656716, 37: 0.2214765100671141, 38: 0.2926829268292683, 39: 0.18461538461538463, 40: 0.2433234421364985}
Micro-average F1 score: 0.3274276743664499
Weighted-average F1 score: 0.3276277712313093
F1 score per class: {1: 0.1685823754789272, 2: 0.42424242424242425, 3: 0.3761467889908257, 5: 0.5623188405797102, 6: 0.35294117647058826, 7: 0.05660377358490566, 8: 0.1652892561983471, 9: 0.5813953488372093, 10: 0.357487922705314, 11: 0.018867924528301886, 12: 0.12440191387559808, 14: 0.05434782608695652, 15: 0.3902439024390244, 16: 0.5274725274725275, 17: 0.0, 18: 0.04081632653061224, 19: 0.5136986301369864, 20: 0.5, 22: 0.37722419928825623, 24: 0.11428571428571428, 25: 0.47368421052631576, 26: 0.711340206185567, 27: 0.0, 28: 0.13636363636363635, 29: 0.7555555555555555, 30: 0.6938775510204082, 31: 0.046511627906976744, 32: 0.5126353790613718, 33: 0.21621621621621623, 34: 0.16774193548387098, 35: 0.2916666666666667, 36: 0.27586206896551724, 37: 0.2043343653250774, 38: 0.2711864406779661, 39: 0.0625, 40: 0.24666666666666667}
Micro-average F1 score: 0.3340790454884415
Weighted-average F1 score: 0.32289464494153836
F1 score per class: {1: 0.16793893129770993, 2: 0.42857142857142855, 3: 0.4371584699453552, 5: 0.5907692307692308, 6: 0.2753623188405797, 7: 0.047244094488188976, 8: 0.10416666666666667, 9: 0.8135593220338984, 10: 0.36097560975609755, 11: 0.01818181818181818, 12: 0.12560386473429952, 14: 0.04597701149425287, 15: 0.37209302325581395, 16: 0.6153846153846154, 17: 0.0, 18: 0.043478260869565216, 19: 0.5236363636363637, 20: 0.45, 22: 0.39097744360902253, 24: 0.07692307692307693, 25: 0.4657534246575342, 26: 0.71875, 27: 0.0, 28: 0.1348314606741573, 29: 0.7597765363128491, 30: 0.7692307692307693, 31: 0.08, 32: 0.5179856115107914, 33: 0.1, 34: 0.15737704918032788, 35: 0.292358803986711, 36: 0.08695652173913043, 37: 0.2, 38: 0.2782608695652174, 39: 0.04878048780487805, 40: 0.22356495468277945}
Micro-average F1 score: 0.3300933125972006
Weighted-average F1 score: 0.31912868747777534

F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.5925925925925926, 19: 0.0, 20: 0.0, 22: 0.0, 25: 0.36363636363636365, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.7213114754098361, 37: 0.5546218487394958, 38: 0.48, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3732876712328767
Weighted-average F1 score: 0.2863205926754877
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.5517241379310345, 16: 0.0, 17: 0.0, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.0, 25: 0.46153846153846156, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7058823529411765, 37: 0.49624060150375937, 38: 0.47761194029850745, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3132530120481928
Weighted-average F1 score: 0.22955162674378365
F1 score per class: {1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.5714285714285714, 19: 0.0, 20: 0.0, 22: 0.0, 24: 0.0, 25: 0.4533333333333333, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.7096774193548387, 37: 0.49624060150375937, 38: 0.48484848484848486, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3323943661971831
Weighted-average F1 score: 0.24984015500845

F1 score per class: {1: 0.10161662817551963, 2: 0.20512820512820512, 3: 0.24, 5: 0.5597667638483965, 6: 0.1038961038961039, 7: 0.02575107296137339, 8: 0.0625, 9: 0.8275862068965517, 10: 0.18834080717488788, 11: 0.05357142857142857, 12: 0.04477611940298507, 14: 0.013333333333333334, 15: 0.2962962962962963, 16: 0.44776119402985076, 17: 0.0, 18: 0.041666666666666664, 19: 0.4978902953586498, 20: 0.23655913978494625, 22: 0.30434782608695654, 24: 0.0, 25: 0.3582089552238806, 26: 0.6231155778894473, 27: 0.0, 28: 0.1, 29: 0.5945945945945946, 30: 0.7692307692307693, 31: 0.06896551724137931, 32: 0.40816326530612246, 33: 0.0, 34: 0.11003236245954692, 35: 0.18604651162790697, 36: 0.029850746268656716, 37: 0.13692946058091288, 38: 0.15789473684210525, 39: 0.10084033613445378, 40: 0.19759036144578312}
Micro-average F1 score: 0.23624595469255663
Weighted-average F1 score: 0.2242583587845159
F1 score per class: {1: 0.09361702127659574, 2: 0.2413793103448276, 3: 0.24624624624624625, 5: 0.40082644628099173, 6: 0.23076923076923078, 7: 0.027649769585253458, 8: 0.14388489208633093, 9: 0.42735042735042733, 10: 0.24584717607973422, 11: 0.01818181818181818, 12: 0.08024691358024691, 14: 0.04484304932735426, 15: 0.27586206896551724, 16: 0.3116883116883117, 17: 0.0, 18: 0.031746031746031744, 19: 0.4573170731707317, 20: 0.3333333333333333, 22: 0.2630272952853598, 24: 0.08695652173913043, 25: 0.4186046511627907, 26: 0.6, 27: 0.0, 28: 0.07017543859649122, 29: 0.6634146341463415, 30: 0.6181818181818182, 31: 0.02564102564102564, 32: 0.3717277486910995, 33: 0.13793103448275862, 34: 0.10420841683366733, 35: 0.17536534446764093, 36: 0.23300970873786409, 37: 0.12199630314232902, 38: 0.1509433962264151, 39: 0.0392156862745098, 40: 0.19072164948453607}
Micro-average F1 score: 0.2295964125560538
Weighted-average F1 score: 0.21739867062921686
F1 score per class: {1: 0.09381663113006397, 2: 0.24, 3: 0.2952029520295203, 5: 0.42761692650334077, 6: 0.18627450980392157, 7: 0.023715415019762844, 8: 0.09803921568627451, 9: 0.7384615384615385, 10: 0.2534246575342466, 11: 0.018018018018018018, 12: 0.07878787878787878, 14: 0.037914691943127965, 15: 0.25396825396825395, 16: 0.38095238095238093, 17: 0.0, 18: 0.03389830508474576, 19: 0.47840531561461797, 20: 0.3103448275862069, 22: 0.268733850129199, 24: 0.06666666666666667, 25: 0.41975308641975306, 26: 0.6216216216216216, 27: 0.0, 28: 0.0759493670886076, 29: 0.6732673267326733, 30: 0.7142857142857143, 31: 0.038461538461538464, 32: 0.38095238095238093, 33: 0.07407407407407407, 34: 0.0973630831643002, 35: 0.17425742574257425, 36: 0.08450704225352113, 37: 0.11870503597122302, 38: 0.1568627450980392, 39: 0.02702702702702703, 40: 0.17411764705882352}
Micro-average F1 score: 0.2292735619767756
Weighted-average F1 score: 0.21543777162960354
cur_acc_wo_na:  ['0.7660', '0.5833', '0.5000', '0.5693', '0.3924', '0.3911', '0.5105']
his_acc_wo_na:  ['0.7660', '0.6693', '0.5563', '0.5038', '0.4117', '0.3589', '0.3274']
cur_acc des_wo_na:  ['0.7485', '0.5772', '0.3926', '0.5234', '0.3764', '0.4582', '0.4474']
his_acc des_wo_na:  ['0.7485', '0.6628', '0.5428', '0.4825', '0.4106', '0.3718', '0.3341']
cur_acc rrf_wo_na:  ['0.7722', '0.5910', '0.4025', '0.5243', '0.3844', '0.4818', '0.4711']
his_acc rrf_wo_na:  ['0.7722', '0.6634', '0.5487', '0.4996', '0.4075', '0.3674', '0.3301']
cur_acc_w_na:  ['0.6471', '0.4541', '0.3579', '0.3852', '0.2695', '0.3387', '0.3733']
his_acc_w_na:  ['0.6471', '0.5352', '0.4124', '0.3624', '0.2880', '0.2652', '0.2362']
cur_acc des_w_na:  ['0.6070', '0.4239', '0.2700', '0.3369', '0.2486', '0.3709', '0.3133']
his_acc des_w_na:  ['0.6070', '0.5047', '0.3861', '0.3330', '0.2735', '0.2588', '0.2296']
cur_acc rrf_w_na:  ['0.6313', '0.4388', '0.2802', '0.3364', '0.2575', '0.3946', '0.3324']
his_acc rrf_w_na:  ['0.6313', '0.5101', '0.3945', '0.3493', '0.2752', '0.2591', '0.2293']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 96.4236955CurrentTrain: epoch  0, batch     1 | loss: 150.5565498CurrentTrain: epoch  0, batch     2 | loss: 84.4080738CurrentTrain: epoch  0, batch     3 | loss: 83.8976761CurrentTrain: epoch  1, batch     0 | loss: 92.6463141CurrentTrain: epoch  1, batch     1 | loss: 81.5230876CurrentTrain: epoch  1, batch     2 | loss: 77.6680931CurrentTrain: epoch  1, batch     3 | loss: 72.5075135CurrentTrain: epoch  2, batch     0 | loss: 109.8779490CurrentTrain: epoch  2, batch     1 | loss: 76.9599575CurrentTrain: epoch  2, batch     2 | loss: 83.0280217CurrentTrain: epoch  2, batch     3 | loss: 83.7669439CurrentTrain: epoch  3, batch     0 | loss: 105.3286817CurrentTrain: epoch  3, batch     1 | loss: 88.1734709CurrentTrain: epoch  3, batch     2 | loss: 78.4984413CurrentTrain: epoch  3, batch     3 | loss: 64.5248370CurrentTrain: epoch  4, batch     0 | loss: 80.3645265CurrentTrain: epoch  4, batch     1 | loss: 98.6973931CurrentTrain: epoch  4, batch     2 | loss: 130.6382568CurrentTrain: epoch  4, batch     3 | loss: 65.0785286CurrentTrain: epoch  5, batch     0 | loss: 77.1191524CurrentTrain: epoch  5, batch     1 | loss: 80.1043658CurrentTrain: epoch  5, batch     2 | loss: 86.0265168CurrentTrain: epoch  5, batch     3 | loss: 75.5323029CurrentTrain: epoch  6, batch     0 | loss: 102.2368286CurrentTrain: epoch  6, batch     1 | loss: 67.9798731CurrentTrain: epoch  6, batch     2 | loss: 77.5472166CurrentTrain: epoch  6, batch     3 | loss: 65.6808332CurrentTrain: epoch  7, batch     0 | loss: 81.5086071CurrentTrain: epoch  7, batch     1 | loss: 66.9732952CurrentTrain: epoch  7, batch     2 | loss: 75.9235368CurrentTrain: epoch  7, batch     3 | loss: 79.4072841CurrentTrain: epoch  8, batch     0 | loss: 100.6812075CurrentTrain: epoch  8, batch     1 | loss: 76.8071274CurrentTrain: epoch  8, batch     2 | loss: 79.4019122CurrentTrain: epoch  8, batch     3 | loss: 59.5698335CurrentTrain: epoch  9, batch     0 | loss: 66.0944829CurrentTrain: epoch  9, batch     1 | loss: 74.9145677CurrentTrain: epoch  9, batch     2 | loss: 97.5791831CurrentTrain: epoch  9, batch     3 | loss: 75.7052561
MemoryTrain:  epoch  0, batch     0 | loss: 0.8173753MemoryTrain:  epoch  1, batch     0 | loss: 0.7128504MemoryTrain:  epoch  2, batch     0 | loss: 0.6501970MemoryTrain:  epoch  3, batch     0 | loss: 0.5006100MemoryTrain:  epoch  4, batch     0 | loss: 0.4174130MemoryTrain:  epoch  5, batch     0 | loss: 0.3823682MemoryTrain:  epoch  6, batch     0 | loss: 0.3539739MemoryTrain:  epoch  7, batch     0 | loss: 0.3445678MemoryTrain:  epoch  8, batch     0 | loss: 0.2696418MemoryTrain:  epoch  9, batch     0 | loss: 0.2565654

F1 score per class: {0: 0.8888888888888888, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.847457627118644, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.2222222222222222, 14: 0.0, 15: 0.0, 18: 0.0, 19: 0.0, 21: 0.36363636363636365, 23: 0.7727272727272727, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.59765625
Weighted-average F1 score: 0.4782990691685839
F1 score per class: {0: 0.7692307692307693, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7878787878787878, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.3333333333333333, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.4507042253521127, 22: 0.0, 23: 0.6588235294117647, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4791666666666667
Weighted-average F1 score: 0.3720074761408133
F1 score per class: {0: 0.8192771084337349, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8210526315789474, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.3333333333333333, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.47058823529411764, 22: 0.0, 23: 0.7126436781609196, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.5182829888712241
Weighted-average F1 score: 0.40056804155430203

F1 score per class: {0: 0.4025157232704403, 1: 0.18095238095238095, 2: 0.3870967741935484, 3: 0.35036496350364965, 4: 0.847457627118644, 5: 0.8245614035087719, 6: 0.1652892561983471, 7: 0.05555555555555555, 8: 0.045454545454545456, 9: 0.9230769230769231, 10: 0.30057803468208094, 11: 0.10619469026548672, 12: 0.047337278106508875, 13: 0.023529411764705882, 14: 0.019801980198019802, 15: 0.5, 16: 0.56, 17: 0.0, 18: 0.047619047619047616, 19: 0.5384615384615384, 20: 0.44155844155844154, 21: 0.15151515151515152, 22: 0.24096385542168675, 23: 0.7083333333333334, 24: 0.0, 25: 0.375, 26: 0.7052023121387283, 27: 0.0, 28: 0.22857142857142856, 29: 0.5769230769230769, 30: 0.8108108108108109, 31: 0.10526315789473684, 32: 0.5124555160142349, 33: 0.0, 34: 0.203125, 35: 0.3391304347826087, 36: 0.0, 37: 0.26609442060085836, 38: 0.2553191489361702, 39: 0.1111111111111111, 40: 0.32653061224489793}
Micro-average F1 score: 0.3536429484448232
Weighted-average F1 score: 0.3470751015640658
F1 score per class: {0: 0.2356902356902357, 1: 0.1825726141078838, 2: 0.23529411764705882, 3: 0.3548387096774194, 4: 0.7609756097560976, 5: 0.6193548387096774, 6: 0.3595505617977528, 7: 0.05063291139240506, 8: 0.1346153846153846, 9: 0.6097560975609756, 10: 0.35555555555555557, 11: 0.019417475728155338, 12: 0.10810810810810811, 13: 0.04597701149425287, 14: 0.03007518796992481, 15: 0.5, 16: 0.5714285714285714, 17: 0.0, 18: 0.0425531914893617, 19: 0.5277777777777778, 20: 0.45, 21: 0.18823529411764706, 22: 0.3696682464454976, 23: 0.5833333333333334, 24: 0.0, 25: 0.48, 26: 0.6567164179104478, 27: 0.0, 28: 0.16393442622950818, 29: 0.7222222222222222, 30: 0.6956521739130435, 31: 0.047619047619047616, 32: 0.49517684887459806, 33: 0.2222222222222222, 34: 0.22340425531914893, 35: 0.2857142857142857, 36: 0.1728395061728395, 37: 0.21518987341772153, 38: 0.24793388429752067, 39: 0.07142857142857142, 40: 0.33476394849785407}
Micro-average F1 score: 0.34088279054389153
Weighted-average F1 score: 0.3260824745395239
F1 score per class: {0: 0.29694323144104806, 1: 0.19130434782608696, 2: 0.27906976744186046, 3: 0.4039408866995074, 4: 0.8082901554404145, 5: 0.7007299270072993, 6: 0.31788079470198677, 7: 0.05333333333333334, 8: 0.08421052631578947, 9: 0.8421052631578947, 10: 0.3611111111111111, 11: 0.01904761904761905, 12: 0.10138248847926268, 13: 0.04145077720207254, 14: 0.03076923076923077, 15: 0.5, 16: 0.6153846153846154, 17: 0.0, 18: 0.045454545454545456, 19: 0.5691699604743083, 20: 0.45, 21: 0.1951219512195122, 22: 0.37, 23: 0.62, 24: 0.0, 25: 0.463768115942029, 26: 0.6770833333333334, 27: 0.0, 28: 0.1724137931034483, 29: 0.7344632768361582, 30: 0.8, 31: 0.07142857142857142, 32: 0.4887459807073955, 33: 0.09090909090909091, 34: 0.22580645161290322, 35: 0.3271375464684015, 36: 0.05714285714285714, 37: 0.20754716981132076, 38: 0.2682926829268293, 39: 0.11594202898550725, 40: 0.32}
Micro-average F1 score: 0.35327635327635326
Weighted-average F1 score: 0.3378959285209903

F1 score per class: {0: 0.8421052631578947, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8108108108108109, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.10256410256410256, 14: 0.0, 15: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.25, 23: 0.6868686868686869, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4519940915805022
Weighted-average F1 score: 0.33526340347139616
F1 score per class: {0: 0.6730769230769231, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7428571428571429, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.1702127659574468, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3368421052631579, 22: 0.0, 23: 0.56, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.343649946638207
Weighted-average F1 score: 0.25829465895048037
F1 score per class: {0: 0.7391304347826086, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.7722772277227723, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.16, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3368421052631579, 22: 0.0, 23: 0.6078431372549019, 26: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3742824339839265
Weighted-average F1 score: 0.27897652600096456

F1 score per class: {0: 0.2807017543859649, 1: 0.09595959595959595, 2: 0.22641509433962265, 3: 0.25668449197860965, 4: 0.7894736842105263, 5: 0.6787003610108303, 6: 0.125, 7: 0.028708133971291867, 8: 0.0449438202247191, 9: 0.8888888888888888, 10: 0.22127659574468084, 11: 0.10344827586206896, 12: 0.03508771929824561, 13: 0.011299435028248588, 14: 0.018018018018018018, 15: 0.3111111111111111, 16: 0.417910447761194, 17: 0.0, 18: 0.0425531914893617, 19: 0.5022421524663677, 20: 0.3008849557522124, 21: 0.10204081632653061, 22: 0.19607843137254902, 23: 0.6017699115044248, 24: 0.0, 25: 0.36363636363636365, 26: 0.6354166666666666, 27: 0.0, 28: 0.13559322033898305, 29: 0.4972375690607735, 30: 0.75, 31: 0.05555555555555555, 32: 0.36363636363636365, 33: 0.0, 34: 0.15028901734104047, 35: 0.20911528150134048, 36: 0.0, 37: 0.15656565656565657, 38: 0.1643835616438356, 39: 0.06451612903225806, 40: 0.2711864406779661}
Micro-average F1 score: 0.25784405094749924
Weighted-average F1 score: 0.23845583715661287
F1 score per class: {0: 0.15350877192982457, 1: 0.1004566210045662, 2: 0.14457831325301204, 3: 0.22506393861892582, 4: 0.6724137931034483, 5: 0.44036697247706424, 6: 0.24334600760456274, 7: 0.024691358024691357, 8: 0.12173913043478261, 9: 0.5050505050505051, 10: 0.24024024024024024, 11: 0.019230769230769232, 12: 0.06936416184971098, 13: 0.022222222222222223, 14: 0.026143790849673203, 15: 0.3783783783783784, 16: 0.3137254901960784, 17: 0.0, 18: 0.03333333333333333, 19: 0.47058823529411764, 20: 0.2975206611570248, 21: 0.1350210970464135, 22: 0.26440677966101694, 23: 0.47058823529411764, 24: 0.0, 25: 0.43902439024390244, 26: 0.5546218487394958, 27: 0.0, 28: 0.08064516129032258, 29: 0.6103286384976526, 30: 0.6153846153846154, 31: 0.022727272727272728, 32: 0.34375, 33: 0.1568627450980392, 34: 0.13043478260869565, 35: 0.1732283464566929, 36: 0.15217391304347827, 37: 0.13102119460500963, 38: 0.12875536480686695, 39: 0.04081632653061224, 40: 0.254071661237785}
Micro-average F1 score: 0.2310881990361986
Weighted-average F1 score: 0.2154915827450619
F1 score per class: {0: 0.19540229885057472, 1: 0.10232558139534884, 2: 0.17391304347826086, 3: 0.281786941580756, 4: 0.7090909090909091, 5: 0.493573264781491, 6: 0.21621621621621623, 7: 0.02666666666666667, 8: 0.07920792079207921, 9: 0.8, 10: 0.2524271844660194, 11: 0.018867924528301886, 12: 0.06451612903225806, 13: 0.019559902200488997, 14: 0.0273972602739726, 15: 0.34146341463414637, 16: 0.35398230088495575, 17: 0.0, 18: 0.03636363636363636, 19: 0.5217391304347826, 20: 0.27692307692307694, 21: 0.13559322033898305, 22: 0.27205882352941174, 23: 0.4881889763779528, 24: 0.0, 25: 0.4155844155844156, 26: 0.5882352941176471, 27: 0.0, 28: 0.09090909090909091, 29: 0.625, 30: 0.7441860465116279, 31: 0.04081632653061224, 32: 0.3377777777777778, 33: 0.06896551724137931, 34: 0.13291139240506328, 35: 0.19642857142857142, 36: 0.05405405405405406, 37: 0.125, 38: 0.1506849315068493, 39: 0.06722689075630252, 40: 0.24390243902439024}
Micro-average F1 score: 0.24195121951219511
Weighted-average F1 score: 0.22438074860741697
cur_acc_wo_na:  ['0.7660', '0.5833', '0.5000', '0.5693', '0.3924', '0.3911', '0.5105', '0.5977']
his_acc_wo_na:  ['0.7660', '0.6693', '0.5563', '0.5038', '0.4117', '0.3589', '0.3274', '0.3536']
cur_acc des_wo_na:  ['0.7485', '0.5772', '0.3926', '0.5234', '0.3764', '0.4582', '0.4474', '0.4792']
his_acc des_wo_na:  ['0.7485', '0.6628', '0.5428', '0.4825', '0.4106', '0.3718', '0.3341', '0.3409']
cur_acc rrf_wo_na:  ['0.7722', '0.5910', '0.4025', '0.5243', '0.3844', '0.4818', '0.4711', '0.5183']
his_acc rrf_wo_na:  ['0.7722', '0.6634', '0.5487', '0.4996', '0.4075', '0.3674', '0.3301', '0.3533']
cur_acc_w_na:  ['0.6471', '0.4541', '0.3579', '0.3852', '0.2695', '0.3387', '0.3733', '0.4520']
his_acc_w_na:  ['0.6471', '0.5352', '0.4124', '0.3624', '0.2880', '0.2652', '0.2362', '0.2578']
cur_acc des_w_na:  ['0.6070', '0.4239', '0.2700', '0.3369', '0.2486', '0.3709', '0.3133', '0.3436']
his_acc des_w_na:  ['0.6070', '0.5047', '0.3861', '0.3330', '0.2735', '0.2588', '0.2296', '0.2311']
cur_acc rrf_w_na:  ['0.6313', '0.4388', '0.2802', '0.3364', '0.2575', '0.3946', '0.3324', '0.3743']
his_acc rrf_w_na:  ['0.6313', '0.5101', '0.3945', '0.3493', '0.2752', '0.2591', '0.2293', '0.2420']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 85.0288738CurrentTrain: epoch  0, batch     1 | loss: 79.6946741CurrentTrain: epoch  0, batch     2 | loss: 103.3027814CurrentTrain: epoch  0, batch     3 | loss: 102.3563452CurrentTrain: epoch  0, batch     4 | loss: 87.4592133CurrentTrain: epoch  0, batch     5 | loss: 119.8583115CurrentTrain: epoch  0, batch     6 | loss: 100.5131064CurrentTrain: epoch  0, batch     7 | loss: 119.6340728CurrentTrain: epoch  0, batch     8 | loss: 86.3221264CurrentTrain: epoch  0, batch     9 | loss: 119.2527409CurrentTrain: epoch  0, batch    10 | loss: 86.2030121CurrentTrain: epoch  0, batch    11 | loss: 86.7449064CurrentTrain: epoch  0, batch    12 | loss: 86.6850817CurrentTrain: epoch  0, batch    13 | loss: 119.7271519CurrentTrain: epoch  0, batch    14 | loss: 146.7858934CurrentTrain: epoch  0, batch    15 | loss: 118.2604807CurrentTrain: epoch  0, batch    16 | loss: 118.7160441CurrentTrain: epoch  0, batch    17 | loss: 99.9569012CurrentTrain: epoch  0, batch    18 | loss: 145.5499185CurrentTrain: epoch  0, batch    19 | loss: 100.0270752CurrentTrain: epoch  0, batch    20 | loss: 76.5529471CurrentTrain: epoch  0, batch    21 | loss: 87.9547629CurrentTrain: epoch  0, batch    22 | loss: 87.2594707CurrentTrain: epoch  0, batch    23 | loss: 117.7437771CurrentTrain: epoch  0, batch    24 | loss: 117.1905593CurrentTrain: epoch  0, batch    25 | loss: 117.5541939CurrentTrain: epoch  0, batch    26 | loss: 85.3659621CurrentTrain: epoch  0, batch    27 | loss: 192.0667674CurrentTrain: epoch  0, batch    28 | loss: 99.4957667CurrentTrain: epoch  0, batch    29 | loss: 99.3405510CurrentTrain: epoch  0, batch    30 | loss: 98.7421684CurrentTrain: epoch  0, batch    31 | loss: 85.6879731CurrentTrain: epoch  0, batch    32 | loss: 118.1489569CurrentTrain: epoch  0, batch    33 | loss: 85.0877774CurrentTrain: epoch  0, batch    34 | loss: 98.7328978CurrentTrain: epoch  0, batch    35 | loss: 98.5556618CurrentTrain: epoch  0, batch    36 | loss: 117.4436057CurrentTrain: epoch  0, batch    37 | loss: 97.2943023CurrentTrain: epoch  0, batch    38 | loss: 97.7042101CurrentTrain: epoch  0, batch    39 | loss: 116.7541951CurrentTrain: epoch  0, batch    40 | loss: 84.8775530CurrentTrain: epoch  0, batch    41 | loss: 98.9208011CurrentTrain: epoch  0, batch    42 | loss: 146.2763896CurrentTrain: epoch  0, batch    43 | loss: 116.8553222CurrentTrain: epoch  0, batch    44 | loss: 98.1616155CurrentTrain: epoch  0, batch    45 | loss: 97.8332339CurrentTrain: epoch  0, batch    46 | loss: 84.3434360CurrentTrain: epoch  0, batch    47 | loss: 96.8757676CurrentTrain: epoch  0, batch    48 | loss: 97.2704239CurrentTrain: epoch  0, batch    49 | loss: 97.4427591CurrentTrain: epoch  0, batch    50 | loss: 84.7077073CurrentTrain: epoch  0, batch    51 | loss: 116.8555338CurrentTrain: epoch  0, batch    52 | loss: 72.9153218CurrentTrain: epoch  0, batch    53 | loss: 116.7349267CurrentTrain: epoch  0, batch    54 | loss: 95.5628530CurrentTrain: epoch  0, batch    55 | loss: 114.4984071CurrentTrain: epoch  0, batch    56 | loss: 143.7784193CurrentTrain: epoch  0, batch    57 | loss: 71.1843012CurrentTrain: epoch  0, batch    58 | loss: 114.9239670CurrentTrain: epoch  0, batch    59 | loss: 79.9185911CurrentTrain: epoch  0, batch    60 | loss: 113.3616759CurrentTrain: epoch  0, batch    61 | loss: 96.5851887CurrentTrain: epoch  0, batch    62 | loss: 95.8116797CurrentTrain: epoch  0, batch    63 | loss: 70.8683289CurrentTrain: epoch  0, batch    64 | loss: 114.5722709CurrentTrain: epoch  0, batch    65 | loss: 92.3313521CurrentTrain: epoch  0, batch    66 | loss: 95.6384489CurrentTrain: epoch  0, batch    67 | loss: 138.5288797CurrentTrain: epoch  0, batch    68 | loss: 110.3784742CurrentTrain: epoch  0, batch    69 | loss: 80.9522074CurrentTrain: epoch  0, batch    70 | loss: 77.7194145CurrentTrain: epoch  0, batch    71 | loss: 80.5629103CurrentTrain: epoch  0, batch    72 | loss: 115.3387224CurrentTrain: epoch  0, batch    73 | loss: 95.7141663CurrentTrain: epoch  0, batch    74 | loss: 77.3383434CurrentTrain: epoch  0, batch    75 | loss: 92.6872527CurrentTrain: epoch  0, batch    76 | loss: 70.6337917CurrentTrain: epoch  0, batch    77 | loss: 94.2085604CurrentTrain: epoch  0, batch    78 | loss: 81.3372436CurrentTrain: epoch  0, batch    79 | loss: 114.3428057CurrentTrain: epoch  0, batch    80 | loss: 93.9670718CurrentTrain: epoch  0, batch    81 | loss: 94.0890120CurrentTrain: epoch  0, batch    82 | loss: 81.6610464CurrentTrain: epoch  0, batch    83 | loss: 89.9698610CurrentTrain: epoch  0, batch    84 | loss: 110.9700359CurrentTrain: epoch  0, batch    85 | loss: 92.3889012CurrentTrain: epoch  0, batch    86 | loss: 90.8366630CurrentTrain: epoch  0, batch    87 | loss: 110.7015340CurrentTrain: epoch  0, batch    88 | loss: 93.8773084CurrentTrain: epoch  0, batch    89 | loss: 110.3744359CurrentTrain: epoch  0, batch    90 | loss: 92.2852475CurrentTrain: epoch  0, batch    91 | loss: 78.2317009CurrentTrain: epoch  0, batch    92 | loss: 79.8553420CurrentTrain: epoch  0, batch    93 | loss: 94.4087650CurrentTrain: epoch  0, batch    94 | loss: 91.6822661CurrentTrain: epoch  0, batch    95 | loss: 117.4938758CurrentTrain: epoch  1, batch     0 | loss: 79.0053670CurrentTrain: epoch  1, batch     1 | loss: 78.3722652CurrentTrain: epoch  1, batch     2 | loss: 110.6205248CurrentTrain: epoch  1, batch     3 | loss: 80.4223659CurrentTrain: epoch  1, batch     4 | loss: 76.1336695CurrentTrain: epoch  1, batch     5 | loss: 106.3970768CurrentTrain: epoch  1, batch     6 | loss: 93.4530612CurrentTrain: epoch  1, batch     7 | loss: 76.1796658CurrentTrain: epoch  1, batch     8 | loss: 80.6576060CurrentTrain: epoch  1, batch     9 | loss: 94.4336138CurrentTrain: epoch  1, batch    10 | loss: 108.2304770CurrentTrain: epoch  1, batch    11 | loss: 112.4656872CurrentTrain: epoch  1, batch    12 | loss: 108.4333931CurrentTrain: epoch  1, batch    13 | loss: 110.8537982CurrentTrain: epoch  1, batch    14 | loss: 91.7678383CurrentTrain: epoch  1, batch    15 | loss: 142.8893603CurrentTrain: epoch  1, batch    16 | loss: 89.9369066CurrentTrain: epoch  1, batch    17 | loss: 102.6410316CurrentTrain: epoch  1, batch    18 | loss: 106.5681169CurrentTrain: epoch  1, batch    19 | loss: 88.4769898CurrentTrain: epoch  1, batch    20 | loss: 187.0725889CurrentTrain: epoch  1, batch    21 | loss: 108.9051636CurrentTrain: epoch  1, batch    22 | loss: 93.0882702CurrentTrain: epoch  1, batch    23 | loss: 104.2550885CurrentTrain: epoch  1, batch    24 | loss: 107.7929511CurrentTrain: epoch  1, batch    25 | loss: 89.1444035CurrentTrain: epoch  1, batch    26 | loss: 81.2923446CurrentTrain: epoch  1, batch    27 | loss: 66.6404692CurrentTrain: epoch  1, batch    28 | loss: 109.2324004CurrentTrain: epoch  1, batch    29 | loss: 75.5721893CurrentTrain: epoch  1, batch    30 | loss: 76.9813425CurrentTrain: epoch  1, batch    31 | loss: 89.2787246CurrentTrain: epoch  1, batch    32 | loss: 67.7718024CurrentTrain: epoch  1, batch    33 | loss: 76.9504248CurrentTrain: epoch  1, batch    34 | loss: 89.4290569CurrentTrain: epoch  1, batch    35 | loss: 92.1831589CurrentTrain: epoch  1, batch    36 | loss: 89.8244538CurrentTrain: epoch  1, batch    37 | loss: 77.9524953CurrentTrain: epoch  1, batch    38 | loss: 67.7175453CurrentTrain: epoch  1, batch    39 | loss: 73.7045312CurrentTrain: epoch  1, batch    40 | loss: 84.5539818CurrentTrain: epoch  1, batch    41 | loss: 109.9992402CurrentTrain: epoch  1, batch    42 | loss: 82.9438244CurrentTrain: epoch  1, batch    43 | loss: 108.7967050CurrentTrain: epoch  1, batch    44 | loss: 74.9477891CurrentTrain: epoch  1, batch    45 | loss: 108.7224877CurrentTrain: epoch  1, batch    46 | loss: 68.2934385CurrentTrain: epoch  1, batch    47 | loss: 75.8773831CurrentTrain: epoch  1, batch    48 | loss: 91.9373770CurrentTrain: epoch  1, batch    49 | loss: 107.7954121CurrentTrain: epoch  1, batch    50 | loss: 71.8369377CurrentTrain: epoch  1, batch    51 | loss: 74.4510018CurrentTrain: epoch  1, batch    52 | loss: 109.4857164CurrentTrain: epoch  1, batch    53 | loss: 107.4276451CurrentTrain: epoch  1, batch    54 | loss: 86.6425780CurrentTrain: epoch  1, batch    55 | loss: 138.1706594CurrentTrain: epoch  1, batch    56 | loss: 91.6081206CurrentTrain: epoch  1, batch    57 | loss: 111.3095358CurrentTrain: epoch  1, batch    58 | loss: 183.7776394CurrentTrain: epoch  1, batch    59 | loss: 90.8103351CurrentTrain: epoch  1, batch    60 | loss: 109.5544008CurrentTrain: epoch  1, batch    61 | loss: 64.7341447CurrentTrain: epoch  1, batch    62 | loss: 77.4726593CurrentTrain: epoch  1, batch    63 | loss: 106.4885165CurrentTrain: epoch  1, batch    64 | loss: 90.4452478CurrentTrain: epoch  1, batch    65 | loss: 102.5780008CurrentTrain: epoch  1, batch    66 | loss: 72.4996601CurrentTrain: epoch  1, batch    67 | loss: 84.3128117CurrentTrain: epoch  1, batch    68 | loss: 133.8385893CurrentTrain: epoch  1, batch    69 | loss: 92.9394843CurrentTrain: epoch  1, batch    70 | loss: 89.6870941CurrentTrain: epoch  1, batch    71 | loss: 99.3769816CurrentTrain: epoch  1, batch    72 | loss: 86.7234863CurrentTrain: epoch  1, batch    73 | loss: 75.4423987CurrentTrain: epoch  1, batch    74 | loss: 106.8738733CurrentTrain: epoch  1, batch    75 | loss: 87.0832700CurrentTrain: epoch  1, batch    76 | loss: 108.4637458CurrentTrain: epoch  1, batch    77 | loss: 101.5742255CurrentTrain: epoch  1, batch    78 | loss: 78.2477008CurrentTrain: epoch  1, batch    79 | loss: 90.9907188CurrentTrain: epoch  1, batch    80 | loss: 85.3577697CurrentTrain: epoch  1, batch    81 | loss: 109.5349805CurrentTrain: epoch  1, batch    82 | loss: 68.2249982CurrentTrain: epoch  1, batch    83 | loss: 105.9647395CurrentTrain: epoch  1, batch    84 | loss: 89.7285639CurrentTrain: epoch  1, batch    85 | loss: 137.7648082CurrentTrain: epoch  1, batch    86 | loss: 72.6326505CurrentTrain: epoch  1, batch    87 | loss: 62.9436675CurrentTrain: epoch  1, batch    88 | loss: 106.5543547CurrentTrain: epoch  1, batch    89 | loss: 136.5264330CurrentTrain: epoch  1, batch    90 | loss: 87.9186855CurrentTrain: epoch  1, batch    91 | loss: 88.1329393CurrentTrain: epoch  1, batch    92 | loss: 76.2712668CurrentTrain: epoch  1, batch    93 | loss: 74.9567203CurrentTrain: epoch  1, batch    94 | loss: 72.9737304CurrentTrain: epoch  1, batch    95 | loss: 75.1447534CurrentTrain: epoch  2, batch     0 | loss: 86.2702810CurrentTrain: epoch  2, batch     1 | loss: 88.8379289CurrentTrain: epoch  2, batch     2 | loss: 80.3738823CurrentTrain: epoch  2, batch     3 | loss: 62.5304436CurrentTrain: epoch  2, batch     4 | loss: 131.3183792CurrentTrain: epoch  2, batch     5 | loss: 71.7703091CurrentTrain: epoch  2, batch     6 | loss: 61.4298396CurrentTrain: epoch  2, batch     7 | loss: 102.5244415CurrentTrain: epoch  2, batch     8 | loss: 87.1470252CurrentTrain: epoch  2, batch     9 | loss: 107.4227586CurrentTrain: epoch  2, batch    10 | loss: 82.7092163CurrentTrain: epoch  2, batch    11 | loss: 106.9143063CurrentTrain: epoch  2, batch    12 | loss: 138.0496032CurrentTrain: epoch  2, batch    13 | loss: 134.4510350CurrentTrain: epoch  2, batch    14 | loss: 86.6161743CurrentTrain: epoch  2, batch    15 | loss: 102.8294235CurrentTrain: epoch  2, batch    16 | loss: 102.9146686CurrentTrain: epoch  2, batch    17 | loss: 73.7806418CurrentTrain: epoch  2, batch    18 | loss: 86.1572756CurrentTrain: epoch  2, batch    19 | loss: 104.9798665CurrentTrain: epoch  2, batch    20 | loss: 178.2281737CurrentTrain: epoch  2, batch    21 | loss: 101.9209426CurrentTrain: epoch  2, batch    22 | loss: 106.4075943CurrentTrain: epoch  2, batch    23 | loss: 138.0198895CurrentTrain: epoch  2, batch    24 | loss: 103.6152057CurrentTrain: epoch  2, batch    25 | loss: 65.1615415CurrentTrain: epoch  2, batch    26 | loss: 89.0096625CurrentTrain: epoch  2, batch    27 | loss: 72.2188446CurrentTrain: epoch  2, batch    28 | loss: 91.2984263CurrentTrain: epoch  2, batch    29 | loss: 82.1715285CurrentTrain: epoch  2, batch    30 | loss: 132.4559529CurrentTrain: epoch  2, batch    31 | loss: 81.4578444CurrentTrain: epoch  2, batch    32 | loss: 87.6786328CurrentTrain: epoch  2, batch    33 | loss: 72.9268083CurrentTrain: epoch  2, batch    34 | loss: 89.2275527CurrentTrain: epoch  2, batch    35 | loss: 74.1226044CurrentTrain: epoch  2, batch    36 | loss: 73.5518383CurrentTrain: epoch  2, batch    37 | loss: 87.0562754CurrentTrain: epoch  2, batch    38 | loss: 87.2719716CurrentTrain: epoch  2, batch    39 | loss: 136.7822591CurrentTrain: epoch  2, batch    40 | loss: 104.6987894CurrentTrain: epoch  2, batch    41 | loss: 106.8165373CurrentTrain: epoch  2, batch    42 | loss: 88.0449175CurrentTrain: epoch  2, batch    43 | loss: 71.3542378CurrentTrain: epoch  2, batch    44 | loss: 103.8663900CurrentTrain: epoch  2, batch    45 | loss: 132.1349832CurrentTrain: epoch  2, batch    46 | loss: 86.1816777CurrentTrain: epoch  2, batch    47 | loss: 82.3750586CurrentTrain: epoch  2, batch    48 | loss: 85.8010105CurrentTrain: epoch  2, batch    49 | loss: 179.0412825CurrentTrain: epoch  2, batch    50 | loss: 101.7044498CurrentTrain: epoch  2, batch    51 | loss: 104.3651576CurrentTrain: epoch  2, batch    52 | loss: 88.9227043CurrentTrain: epoch  2, batch    53 | loss: 87.4990426CurrentTrain: epoch  2, batch    54 | loss: 72.3423168CurrentTrain: epoch  2, batch    55 | loss: 85.6749269CurrentTrain: epoch  2, batch    56 | loss: 66.5235816CurrentTrain: epoch  2, batch    57 | loss: 135.1428628CurrentTrain: epoch  2, batch    58 | loss: 100.6833038CurrentTrain: epoch  2, batch    59 | loss: 75.0835365CurrentTrain: epoch  2, batch    60 | loss: 88.9699084CurrentTrain: epoch  2, batch    61 | loss: 84.1684613CurrentTrain: epoch  2, batch    62 | loss: 74.6624069CurrentTrain: epoch  2, batch    63 | loss: 85.6193614CurrentTrain: epoch  2, batch    64 | loss: 103.1225551CurrentTrain: epoch  2, batch    65 | loss: 86.5794352CurrentTrain: epoch  2, batch    66 | loss: 70.5379537CurrentTrain: epoch  2, batch    67 | loss: 84.2355504CurrentTrain: epoch  2, batch    68 | loss: 94.6403945CurrentTrain: epoch  2, batch    69 | loss: 108.9270145CurrentTrain: epoch  2, batch    70 | loss: 84.7592272CurrentTrain: epoch  2, batch    71 | loss: 136.4284228CurrentTrain: epoch  2, batch    72 | loss: 131.6334914CurrentTrain: epoch  2, batch    73 | loss: 84.5319196CurrentTrain: epoch  2, batch    74 | loss: 91.2385737CurrentTrain: epoch  2, batch    75 | loss: 91.8136208CurrentTrain: epoch  2, batch    76 | loss: 74.2103055CurrentTrain: epoch  2, batch    77 | loss: 87.5041900CurrentTrain: epoch  2, batch    78 | loss: 85.0718013CurrentTrain: epoch  2, batch    79 | loss: 88.2891830CurrentTrain: epoch  2, batch    80 | loss: 81.2420885CurrentTrain: epoch  2, batch    81 | loss: 71.2916301CurrentTrain: epoch  2, batch    82 | loss: 103.9894035CurrentTrain: epoch  2, batch    83 | loss: 73.9397212CurrentTrain: epoch  2, batch    84 | loss: 86.5639049CurrentTrain: epoch  2, batch    85 | loss: 72.6862376CurrentTrain: epoch  2, batch    86 | loss: 84.9370104CurrentTrain: epoch  2, batch    87 | loss: 83.7261567CurrentTrain: epoch  2, batch    88 | loss: 101.6412346CurrentTrain: epoch  2, batch    89 | loss: 58.0611316CurrentTrain: epoch  2, batch    90 | loss: 104.1985246CurrentTrain: epoch  2, batch    91 | loss: 70.2040209CurrentTrain: epoch  2, batch    92 | loss: 83.3419980CurrentTrain: epoch  2, batch    93 | loss: 80.9696751CurrentTrain: epoch  2, batch    94 | loss: 180.4756385CurrentTrain: epoch  2, batch    95 | loss: 60.4241956CurrentTrain: epoch  3, batch     0 | loss: 86.6665546CurrentTrain: epoch  3, batch     1 | loss: 71.4566520CurrentTrain: epoch  3, batch     2 | loss: 136.5602692CurrentTrain: epoch  3, batch     3 | loss: 59.4320430CurrentTrain: epoch  3, batch     4 | loss: 63.7756152CurrentTrain: epoch  3, batch     5 | loss: 106.3040882CurrentTrain: epoch  3, batch     6 | loss: 71.2959312CurrentTrain: epoch  3, batch     7 | loss: 102.2854113CurrentTrain: epoch  3, batch     8 | loss: 104.6899314CurrentTrain: epoch  3, batch     9 | loss: 102.4395246CurrentTrain: epoch  3, batch    10 | loss: 84.6065862CurrentTrain: epoch  3, batch    11 | loss: 85.5371183CurrentTrain: epoch  3, batch    12 | loss: 63.0333137CurrentTrain: epoch  3, batch    13 | loss: 132.0849483CurrentTrain: epoch  3, batch    14 | loss: 132.4595356CurrentTrain: epoch  3, batch    15 | loss: 71.8014425CurrentTrain: epoch  3, batch    16 | loss: 101.1106987CurrentTrain: epoch  3, batch    17 | loss: 105.5612878CurrentTrain: epoch  3, batch    18 | loss: 85.2088536CurrentTrain: epoch  3, batch    19 | loss: 100.0404300CurrentTrain: epoch  3, batch    20 | loss: 70.3804971CurrentTrain: epoch  3, batch    21 | loss: 99.3403818CurrentTrain: epoch  3, batch    22 | loss: 103.2234667CurrentTrain: epoch  3, batch    23 | loss: 102.8817587CurrentTrain: epoch  3, batch    24 | loss: 103.1283862CurrentTrain: epoch  3, batch    25 | loss: 90.1516489CurrentTrain: epoch  3, batch    26 | loss: 83.1678757CurrentTrain: epoch  3, batch    27 | loss: 89.0616195CurrentTrain: epoch  3, batch    28 | loss: 66.8416746CurrentTrain: epoch  3, batch    29 | loss: 103.3786124CurrentTrain: epoch  3, batch    30 | loss: 130.7829818CurrentTrain: epoch  3, batch    31 | loss: 62.4688043CurrentTrain: epoch  3, batch    32 | loss: 103.5310068CurrentTrain: epoch  3, batch    33 | loss: 81.6699798CurrentTrain: epoch  3, batch    34 | loss: 100.3922277CurrentTrain: epoch  3, batch    35 | loss: 76.6187310CurrentTrain: epoch  3, batch    36 | loss: 70.2499107CurrentTrain: epoch  3, batch    37 | loss: 71.1258994CurrentTrain: epoch  3, batch    38 | loss: 101.0753392CurrentTrain: epoch  3, batch    39 | loss: 58.9721532CurrentTrain: epoch  3, batch    40 | loss: 71.0045450CurrentTrain: epoch  3, batch    41 | loss: 84.9318074CurrentTrain: epoch  3, batch    42 | loss: 70.3463288CurrentTrain: epoch  3, batch    43 | loss: 106.2500881CurrentTrain: epoch  3, batch    44 | loss: 99.9325098CurrentTrain: epoch  3, batch    45 | loss: 106.9484162CurrentTrain: epoch  3, batch    46 | loss: 60.7654730CurrentTrain: epoch  3, batch    47 | loss: 64.1857637CurrentTrain: epoch  3, batch    48 | loss: 74.9968975CurrentTrain: epoch  3, batch    49 | loss: 68.9016242CurrentTrain: epoch  3, batch    50 | loss: 62.6359410CurrentTrain: epoch  3, batch    51 | loss: 72.2818414CurrentTrain: epoch  3, batch    52 | loss: 67.0615443CurrentTrain: epoch  3, batch    53 | loss: 82.3288010CurrentTrain: epoch  3, batch    54 | loss: 84.8881360CurrentTrain: epoch  3, batch    55 | loss: 60.7797513CurrentTrain: epoch  3, batch    56 | loss: 81.8977744CurrentTrain: epoch  3, batch    57 | loss: 97.3773571CurrentTrain: epoch  3, batch    58 | loss: 104.9099129CurrentTrain: epoch  3, batch    59 | loss: 89.0616845CurrentTrain: epoch  3, batch    60 | loss: 81.1586550CurrentTrain: epoch  3, batch    61 | loss: 68.7756862CurrentTrain: epoch  3, batch    62 | loss: 72.7775235CurrentTrain: epoch  3, batch    63 | loss: 138.8211490CurrentTrain: epoch  3, batch    64 | loss: 70.0540880CurrentTrain: epoch  3, batch    65 | loss: 128.6129488CurrentTrain: epoch  3, batch    66 | loss: 127.7049915CurrentTrain: epoch  3, batch    67 | loss: 85.8209024CurrentTrain: epoch  3, batch    68 | loss: 73.5997525CurrentTrain: epoch  3, batch    69 | loss: 86.8466730CurrentTrain: epoch  3, batch    70 | loss: 101.9539275CurrentTrain: epoch  3, batch    71 | loss: 100.7685042CurrentTrain: epoch  3, batch    72 | loss: 80.8858019CurrentTrain: epoch  3, batch    73 | loss: 70.3579378CurrentTrain: epoch  3, batch    74 | loss: 87.1787553CurrentTrain: epoch  3, batch    75 | loss: 103.0801750CurrentTrain: epoch  3, batch    76 | loss: 69.8028822CurrentTrain: epoch  3, batch    77 | loss: 69.4867996CurrentTrain: epoch  3, batch    78 | loss: 104.2278489CurrentTrain: epoch  3, batch    79 | loss: 85.2293630CurrentTrain: epoch  3, batch    80 | loss: 104.9998589CurrentTrain: epoch  3, batch    81 | loss: 72.6164472CurrentTrain: epoch  3, batch    82 | loss: 73.9792322CurrentTrain: epoch  3, batch    83 | loss: 74.7531766CurrentTrain: epoch  3, batch    84 | loss: 70.4984663CurrentTrain: epoch  3, batch    85 | loss: 105.1608742CurrentTrain: epoch  3, batch    86 | loss: 63.6612736CurrentTrain: epoch  3, batch    87 | loss: 87.4361587CurrentTrain: epoch  3, batch    88 | loss: 126.7850551CurrentTrain: epoch  3, batch    89 | loss: 86.3208527CurrentTrain: epoch  3, batch    90 | loss: 70.7598738CurrentTrain: epoch  3, batch    91 | loss: 71.4082794CurrentTrain: epoch  3, batch    92 | loss: 72.4161301CurrentTrain: epoch  3, batch    93 | loss: 69.8936294CurrentTrain: epoch  3, batch    94 | loss: 106.4821960CurrentTrain: epoch  3, batch    95 | loss: 67.3550111CurrentTrain: epoch  4, batch     0 | loss: 67.2890912CurrentTrain: epoch  4, batch     1 | loss: 102.5631964CurrentTrain: epoch  4, batch     2 | loss: 68.9875862CurrentTrain: epoch  4, batch     3 | loss: 85.4721510CurrentTrain: epoch  4, batch     4 | loss: 61.1849109CurrentTrain: epoch  4, batch     5 | loss: 69.1511784CurrentTrain: epoch  4, batch     6 | loss: 85.4117719CurrentTrain: epoch  4, batch     7 | loss: 80.8285760CurrentTrain: epoch  4, batch     8 | loss: 69.8294620CurrentTrain: epoch  4, batch     9 | loss: 75.4908868CurrentTrain: epoch  4, batch    10 | loss: 73.7001059CurrentTrain: epoch  4, batch    11 | loss: 78.2222260CurrentTrain: epoch  4, batch    12 | loss: 73.9461437CurrentTrain: epoch  4, batch    13 | loss: 127.3360821CurrentTrain: epoch  4, batch    14 | loss: 102.7588471CurrentTrain: epoch  4, batch    15 | loss: 85.8917086CurrentTrain: epoch  4, batch    16 | loss: 80.7961729CurrentTrain: epoch  4, batch    17 | loss: 85.1869729CurrentTrain: epoch  4, batch    18 | loss: 99.6698422CurrentTrain: epoch  4, batch    19 | loss: 127.7945708CurrentTrain: epoch  4, batch    20 | loss: 80.1193658CurrentTrain: epoch  4, batch    21 | loss: 126.7650174CurrentTrain: epoch  4, batch    22 | loss: 71.6381830CurrentTrain: epoch  4, batch    23 | loss: 87.1097912CurrentTrain: epoch  4, batch    24 | loss: 82.4475765CurrentTrain: epoch  4, batch    25 | loss: 69.1778049CurrentTrain: epoch  4, batch    26 | loss: 76.1275773CurrentTrain: epoch  4, batch    27 | loss: 83.4456702CurrentTrain: epoch  4, batch    28 | loss: 104.3041939CurrentTrain: epoch  4, batch    29 | loss: 67.4798585CurrentTrain: epoch  4, batch    30 | loss: 93.4785112CurrentTrain: epoch  4, batch    31 | loss: 72.5681091CurrentTrain: epoch  4, batch    32 | loss: 100.4605401CurrentTrain: epoch  4, batch    33 | loss: 62.5352518CurrentTrain: epoch  4, batch    34 | loss: 96.9200461CurrentTrain: epoch  4, batch    35 | loss: 88.9930740CurrentTrain: epoch  4, batch    36 | loss: 128.1523605CurrentTrain: epoch  4, batch    37 | loss: 104.5802465CurrentTrain: epoch  4, batch    38 | loss: 103.0415854CurrentTrain: epoch  4, batch    39 | loss: 99.8123503CurrentTrain: epoch  4, batch    40 | loss: 85.1367328CurrentTrain: epoch  4, batch    41 | loss: 82.0680637CurrentTrain: epoch  4, batch    42 | loss: 96.9512360CurrentTrain: epoch  4, batch    43 | loss: 103.5814246CurrentTrain: epoch  4, batch    44 | loss: 133.1692981CurrentTrain: epoch  4, batch    45 | loss: 83.0718746CurrentTrain: epoch  4, batch    46 | loss: 82.3332220CurrentTrain: epoch  4, batch    47 | loss: 104.9570640CurrentTrain: epoch  4, batch    48 | loss: 128.8628297CurrentTrain: epoch  4, batch    49 | loss: 82.3658639CurrentTrain: epoch  4, batch    50 | loss: 91.8277350CurrentTrain: epoch  4, batch    51 | loss: 125.9852946CurrentTrain: epoch  4, batch    52 | loss: 83.7485030CurrentTrain: epoch  4, batch    53 | loss: 69.1998873CurrentTrain: epoch  4, batch    54 | loss: 62.8425531CurrentTrain: epoch  4, batch    55 | loss: 101.9403457CurrentTrain: epoch  4, batch    56 | loss: 80.3327632CurrentTrain: epoch  4, batch    57 | loss: 74.3337359CurrentTrain: epoch  4, batch    58 | loss: 67.5230644CurrentTrain: epoch  4, batch    59 | loss: 101.2061494CurrentTrain: epoch  4, batch    60 | loss: 73.2895088CurrentTrain: epoch  4, batch    61 | loss: 83.7400859CurrentTrain: epoch  4, batch    62 | loss: 82.0670872CurrentTrain: epoch  4, batch    63 | loss: 80.5146882CurrentTrain: epoch  4, batch    64 | loss: 70.6565707CurrentTrain: epoch  4, batch    65 | loss: 69.4019834CurrentTrain: epoch  4, batch    66 | loss: 100.2968932CurrentTrain: epoch  4, batch    67 | loss: 82.4102507CurrentTrain: epoch  4, batch    68 | loss: 60.1380096CurrentTrain: epoch  4, batch    69 | loss: 127.9452162CurrentTrain: epoch  4, batch    70 | loss: 80.7927219CurrentTrain: epoch  4, batch    71 | loss: 128.0987147CurrentTrain: epoch  4, batch    72 | loss: 59.1739710CurrentTrain: epoch  4, batch    73 | loss: 126.5683422CurrentTrain: epoch  4, batch    74 | loss: 61.3306719CurrentTrain: epoch  4, batch    75 | loss: 72.4374481CurrentTrain: epoch  4, batch    76 | loss: 68.8385626CurrentTrain: epoch  4, batch    77 | loss: 107.0423328CurrentTrain: epoch  4, batch    78 | loss: 104.2796205CurrentTrain: epoch  4, batch    79 | loss: 83.1254444CurrentTrain: epoch  4, batch    80 | loss: 75.3114419CurrentTrain: epoch  4, batch    81 | loss: 103.9554145CurrentTrain: epoch  4, batch    82 | loss: 78.0651282CurrentTrain: epoch  4, batch    83 | loss: 102.4438501CurrentTrain: epoch  4, batch    84 | loss: 70.5457858CurrentTrain: epoch  4, batch    85 | loss: 99.3026772CurrentTrain: epoch  4, batch    86 | loss: 104.0072823CurrentTrain: epoch  4, batch    87 | loss: 99.3655963CurrentTrain: epoch  4, batch    88 | loss: 100.7223597CurrentTrain: epoch  4, batch    89 | loss: 99.9856808CurrentTrain: epoch  4, batch    90 | loss: 99.0600185CurrentTrain: epoch  4, batch    91 | loss: 85.4273364CurrentTrain: epoch  4, batch    92 | loss: 101.2875118CurrentTrain: epoch  4, batch    93 | loss: 67.3656527CurrentTrain: epoch  4, batch    94 | loss: 71.3769323CurrentTrain: epoch  4, batch    95 | loss: 66.7886422CurrentTrain: epoch  5, batch     0 | loss: 100.2533108CurrentTrain: epoch  5, batch     1 | loss: 68.9809231CurrentTrain: epoch  5, batch     2 | loss: 97.2080092CurrentTrain: epoch  5, batch     3 | loss: 120.4768424CurrentTrain: epoch  5, batch     4 | loss: 69.4144226CurrentTrain: epoch  5, batch     5 | loss: 95.0617876CurrentTrain: epoch  5, batch     6 | loss: 100.0994989CurrentTrain: epoch  5, batch     7 | loss: 82.6328974CurrentTrain: epoch  5, batch     8 | loss: 65.8541484CurrentTrain: epoch  5, batch     9 | loss: 58.3985795CurrentTrain: epoch  5, batch    10 | loss: 100.6337274CurrentTrain: epoch  5, batch    11 | loss: 99.0232214CurrentTrain: epoch  5, batch    12 | loss: 65.7823397CurrentTrain: epoch  5, batch    13 | loss: 95.5728666CurrentTrain: epoch  5, batch    14 | loss: 103.3794278CurrentTrain: epoch  5, batch    15 | loss: 68.0689234CurrentTrain: epoch  5, batch    16 | loss: 127.6132578CurrentTrain: epoch  5, batch    17 | loss: 78.6318146CurrentTrain: epoch  5, batch    18 | loss: 96.2739523CurrentTrain: epoch  5, batch    19 | loss: 104.2874185CurrentTrain: epoch  5, batch    20 | loss: 84.3354943CurrentTrain: epoch  5, batch    21 | loss: 78.9650123CurrentTrain: epoch  5, batch    22 | loss: 83.9155117CurrentTrain: epoch  5, batch    23 | loss: 81.0670670CurrentTrain: epoch  5, batch    24 | loss: 67.7302648CurrentTrain: epoch  5, batch    25 | loss: 99.8961526CurrentTrain: epoch  5, batch    26 | loss: 78.3980238CurrentTrain: epoch  5, batch    27 | loss: 66.4806225CurrentTrain: epoch  5, batch    28 | loss: 127.6220743CurrentTrain: epoch  5, batch    29 | loss: 71.2660521CurrentTrain: epoch  5, batch    30 | loss: 92.2779444CurrentTrain: epoch  5, batch    31 | loss: 80.8967546CurrentTrain: epoch  5, batch    32 | loss: 71.2453864CurrentTrain: epoch  5, batch    33 | loss: 91.1761557CurrentTrain: epoch  5, batch    34 | loss: 98.0871902CurrentTrain: epoch  5, batch    35 | loss: 56.8701572CurrentTrain: epoch  5, batch    36 | loss: 68.4339111CurrentTrain: epoch  5, batch    37 | loss: 127.5555253CurrentTrain: epoch  5, batch    38 | loss: 80.0152094CurrentTrain: epoch  5, batch    39 | loss: 64.2239010CurrentTrain: epoch  5, batch    40 | loss: 130.5235417CurrentTrain: epoch  5, batch    41 | loss: 81.0010028CurrentTrain: epoch  5, batch    42 | loss: 82.7832192CurrentTrain: epoch  5, batch    43 | loss: 65.8423722CurrentTrain: epoch  5, batch    44 | loss: 99.5216020CurrentTrain: epoch  5, batch    45 | loss: 78.8242591CurrentTrain: epoch  5, batch    46 | loss: 70.4434021CurrentTrain: epoch  5, batch    47 | loss: 69.4645843CurrentTrain: epoch  5, batch    48 | loss: 105.0485857CurrentTrain: epoch  5, batch    49 | loss: 55.2166395CurrentTrain: epoch  5, batch    50 | loss: 95.5487721CurrentTrain: epoch  5, batch    51 | loss: 72.9315140CurrentTrain: epoch  5, batch    52 | loss: 83.3865940CurrentTrain: epoch  5, batch    53 | loss: 83.9243255CurrentTrain: epoch  5, batch    54 | loss: 83.7454911CurrentTrain: epoch  5, batch    55 | loss: 123.6134006CurrentTrain: epoch  5, batch    56 | loss: 177.3019833CurrentTrain: epoch  5, batch    57 | loss: 68.7650721CurrentTrain: epoch  5, batch    58 | loss: 82.2920778CurrentTrain: epoch  5, batch    59 | loss: 84.4842679CurrentTrain: epoch  5, batch    60 | loss: 64.8982171CurrentTrain: epoch  5, batch    61 | loss: 98.4352854CurrentTrain: epoch  5, batch    62 | loss: 98.1471206CurrentTrain: epoch  5, batch    63 | loss: 67.9124739CurrentTrain: epoch  5, batch    64 | loss: 79.6915680CurrentTrain: epoch  5, batch    65 | loss: 170.9738249CurrentTrain: epoch  5, batch    66 | loss: 81.2276120CurrentTrain: epoch  5, batch    67 | loss: 100.9987052CurrentTrain: epoch  5, batch    68 | loss: 67.3192428CurrentTrain: epoch  5, batch    69 | loss: 81.4853730CurrentTrain: epoch  5, batch    70 | loss: 74.1276060CurrentTrain: epoch  5, batch    71 | loss: 173.0829093CurrentTrain: epoch  5, batch    72 | loss: 58.3171328CurrentTrain: epoch  5, batch    73 | loss: 99.3403555CurrentTrain: epoch  5, batch    74 | loss: 71.0269730CurrentTrain: epoch  5, batch    75 | loss: 71.0890332CurrentTrain: epoch  5, batch    76 | loss: 69.7751293CurrentTrain: epoch  5, batch    77 | loss: 67.2433244CurrentTrain: epoch  5, batch    78 | loss: 71.0750705CurrentTrain: epoch  5, batch    79 | loss: 59.1055199CurrentTrain: epoch  5, batch    80 | loss: 93.9267479CurrentTrain: epoch  5, batch    81 | loss: 71.9137540CurrentTrain: epoch  5, batch    82 | loss: 78.0550230CurrentTrain: epoch  5, batch    83 | loss: 71.2666420CurrentTrain: epoch  5, batch    84 | loss: 97.8041944CurrentTrain: epoch  5, batch    85 | loss: 102.6321382CurrentTrain: epoch  5, batch    86 | loss: 84.2061081CurrentTrain: epoch  5, batch    87 | loss: 85.7715516CurrentTrain: epoch  5, batch    88 | loss: 83.4305486CurrentTrain: epoch  5, batch    89 | loss: 80.3101830CurrentTrain: epoch  5, batch    90 | loss: 81.9031317CurrentTrain: epoch  5, batch    91 | loss: 101.0176431CurrentTrain: epoch  5, batch    92 | loss: 84.6014400CurrentTrain: epoch  5, batch    93 | loss: 71.8018021CurrentTrain: epoch  5, batch    94 | loss: 99.0123692CurrentTrain: epoch  5, batch    95 | loss: 69.2437417CurrentTrain: epoch  6, batch     0 | loss: 100.7565753CurrentTrain: epoch  6, batch     1 | loss: 75.6660245CurrentTrain: epoch  6, batch     2 | loss: 82.6388024CurrentTrain: epoch  6, batch     3 | loss: 77.8838447CurrentTrain: epoch  6, batch     4 | loss: 79.7528197CurrentTrain: epoch  6, batch     5 | loss: 98.0020209CurrentTrain: epoch  6, batch     6 | loss: 84.1925780CurrentTrain: epoch  6, batch     7 | loss: 80.4714180CurrentTrain: epoch  6, batch     8 | loss: 99.0793001CurrentTrain: epoch  6, batch     9 | loss: 120.1766422CurrentTrain: epoch  6, batch    10 | loss: 79.3612059CurrentTrain: epoch  6, batch    11 | loss: 103.1976343CurrentTrain: epoch  6, batch    12 | loss: 80.4940740CurrentTrain: epoch  6, batch    13 | loss: 80.6078580CurrentTrain: epoch  6, batch    14 | loss: 63.9196748CurrentTrain: epoch  6, batch    15 | loss: 121.7296790CurrentTrain: epoch  6, batch    16 | loss: 94.7741956CurrentTrain: epoch  6, batch    17 | loss: 120.8223777CurrentTrain: epoch  6, batch    18 | loss: 130.7331635CurrentTrain: epoch  6, batch    19 | loss: 69.6401306CurrentTrain: epoch  6, batch    20 | loss: 175.5472821CurrentTrain: epoch  6, batch    21 | loss: 82.5690173CurrentTrain: epoch  6, batch    22 | loss: 77.6750443CurrentTrain: epoch  6, batch    23 | loss: 100.3849785CurrentTrain: epoch  6, batch    24 | loss: 65.3163554CurrentTrain: epoch  6, batch    25 | loss: 100.3463328CurrentTrain: epoch  6, batch    26 | loss: 66.5080362CurrentTrain: epoch  6, batch    27 | loss: 62.9698128CurrentTrain: epoch  6, batch    28 | loss: 78.8460755CurrentTrain: epoch  6, batch    29 | loss: 125.2560092CurrentTrain: epoch  6, batch    30 | loss: 70.6314203CurrentTrain: epoch  6, batch    31 | loss: 68.0245821CurrentTrain: epoch  6, batch    32 | loss: 68.5188651CurrentTrain: epoch  6, batch    33 | loss: 126.1927487CurrentTrain: epoch  6, batch    34 | loss: 83.0519134CurrentTrain: epoch  6, batch    35 | loss: 66.7365909CurrentTrain: epoch  6, batch    36 | loss: 99.9288198CurrentTrain: epoch  6, batch    37 | loss: 122.8190246CurrentTrain: epoch  6, batch    38 | loss: 99.6694934CurrentTrain: epoch  6, batch    39 | loss: 69.0684808CurrentTrain: epoch  6, batch    40 | loss: 97.3826982CurrentTrain: epoch  6, batch    41 | loss: 100.6527228CurrentTrain: epoch  6, batch    42 | loss: 58.5489298CurrentTrain: epoch  6, batch    43 | loss: 77.8657166CurrentTrain: epoch  6, batch    44 | loss: 78.4523463CurrentTrain: epoch  6, batch    45 | loss: 56.4183131CurrentTrain: epoch  6, batch    46 | loss: 69.7959752CurrentTrain: epoch  6, batch    47 | loss: 77.2837879CurrentTrain: epoch  6, batch    48 | loss: 67.3098835CurrentTrain: epoch  6, batch    49 | loss: 123.8240521CurrentTrain: epoch  6, batch    50 | loss: 119.6951578CurrentTrain: epoch  6, batch    51 | loss: 59.9093993CurrentTrain: epoch  6, batch    52 | loss: 65.4177564CurrentTrain: epoch  6, batch    53 | loss: 67.5736274CurrentTrain: epoch  6, batch    54 | loss: 82.0597184CurrentTrain: epoch  6, batch    55 | loss: 124.7232547CurrentTrain: epoch  6, batch    56 | loss: 124.2572627CurrentTrain: epoch  6, batch    57 | loss: 82.3570000CurrentTrain: epoch  6, batch    58 | loss: 96.3917443CurrentTrain: epoch  6, batch    59 | loss: 98.3684540CurrentTrain: epoch  6, batch    60 | loss: 101.9206608CurrentTrain: epoch  6, batch    61 | loss: 68.0450718CurrentTrain: epoch  6, batch    62 | loss: 76.8227574CurrentTrain: epoch  6, batch    63 | loss: 80.0562350CurrentTrain: epoch  6, batch    64 | loss: 77.2344155CurrentTrain: epoch  6, batch    65 | loss: 103.1838876CurrentTrain: epoch  6, batch    66 | loss: 78.9228988CurrentTrain: epoch  6, batch    67 | loss: 95.5935316CurrentTrain: epoch  6, batch    68 | loss: 84.3779148CurrentTrain: epoch  6, batch    69 | loss: 95.6699157CurrentTrain: epoch  6, batch    70 | loss: 98.4526391CurrentTrain: epoch  6, batch    71 | loss: 94.1889948CurrentTrain: epoch  6, batch    72 | loss: 54.8564943CurrentTrain: epoch  6, batch    73 | loss: 83.2945335CurrentTrain: epoch  6, batch    74 | loss: 101.2304025CurrentTrain: epoch  6, batch    75 | loss: 71.8414987CurrentTrain: epoch  6, batch    76 | loss: 79.9288184CurrentTrain: epoch  6, batch    77 | loss: 103.3305973CurrentTrain: epoch  6, batch    78 | loss: 99.0238474CurrentTrain: epoch  6, batch    79 | loss: 97.1159364CurrentTrain: epoch  6, batch    80 | loss: 96.9098018CurrentTrain: epoch  6, batch    81 | loss: 71.6321102CurrentTrain: epoch  6, batch    82 | loss: 78.9519018CurrentTrain: epoch  6, batch    83 | loss: 66.3413918CurrentTrain: epoch  6, batch    84 | loss: 95.9280987CurrentTrain: epoch  6, batch    85 | loss: 74.8223013CurrentTrain: epoch  6, batch    86 | loss: 96.7476043CurrentTrain: epoch  6, batch    87 | loss: 97.7617199CurrentTrain: epoch  6, batch    88 | loss: 119.7832108CurrentTrain: epoch  6, batch    89 | loss: 73.4880596CurrentTrain: epoch  6, batch    90 | loss: 64.0765198CurrentTrain: epoch  6, batch    91 | loss: 123.5395710CurrentTrain: epoch  6, batch    92 | loss: 65.4437578CurrentTrain: epoch  6, batch    93 | loss: 64.1666974CurrentTrain: epoch  6, batch    94 | loss: 64.3368942CurrentTrain: epoch  6, batch    95 | loss: 107.8231892CurrentTrain: epoch  7, batch     0 | loss: 62.4283672CurrentTrain: epoch  7, batch     1 | loss: 67.2681484CurrentTrain: epoch  7, batch     2 | loss: 124.7197041CurrentTrain: epoch  7, batch     3 | loss: 96.8864450CurrentTrain: epoch  7, batch     4 | loss: 121.2655589CurrentTrain: epoch  7, batch     5 | loss: 80.0127202CurrentTrain: epoch  7, batch     6 | loss: 77.1969340CurrentTrain: epoch  7, batch     7 | loss: 94.8746380CurrentTrain: epoch  7, batch     8 | loss: 54.9149708CurrentTrain: epoch  7, batch     9 | loss: 64.9493976CurrentTrain: epoch  7, batch    10 | loss: 118.6367957CurrentTrain: epoch  7, batch    11 | loss: 126.0643702CurrentTrain: epoch  7, batch    12 | loss: 77.8193299CurrentTrain: epoch  7, batch    13 | loss: 82.1470724CurrentTrain: epoch  7, batch    14 | loss: 54.5826041CurrentTrain: epoch  7, batch    15 | loss: 97.4144531CurrentTrain: epoch  7, batch    16 | loss: 65.5979207CurrentTrain: epoch  7, batch    17 | loss: 102.3773756CurrentTrain: epoch  7, batch    18 | loss: 66.7068756CurrentTrain: epoch  7, batch    19 | loss: 93.5546918CurrentTrain: epoch  7, batch    20 | loss: 97.4145590CurrentTrain: epoch  7, batch    21 | loss: 83.9944720CurrentTrain: epoch  7, batch    22 | loss: 94.3078500CurrentTrain: epoch  7, batch    23 | loss: 83.3153436CurrentTrain: epoch  7, batch    24 | loss: 123.4045797CurrentTrain: epoch  7, batch    25 | loss: 76.1974109CurrentTrain: epoch  7, batch    26 | loss: 83.0242211CurrentTrain: epoch  7, batch    27 | loss: 57.2614735CurrentTrain: epoch  7, batch    28 | loss: 56.2314357CurrentTrain: epoch  7, batch    29 | loss: 64.0729804CurrentTrain: epoch  7, batch    30 | loss: 98.6536710CurrentTrain: epoch  7, batch    31 | loss: 67.4919614CurrentTrain: epoch  7, batch    32 | loss: 95.8496841CurrentTrain: epoch  7, batch    33 | loss: 95.3573571CurrentTrain: epoch  7, batch    34 | loss: 97.8451090CurrentTrain: epoch  7, batch    35 | loss: 81.6918583CurrentTrain: epoch  7, batch    36 | loss: 77.3133601CurrentTrain: epoch  7, batch    37 | loss: 63.4929811CurrentTrain: epoch  7, batch    38 | loss: 76.7423445CurrentTrain: epoch  7, batch    39 | loss: 76.6489561CurrentTrain: epoch  7, batch    40 | loss: 129.1652753CurrentTrain: epoch  7, batch    41 | loss: 78.5376882CurrentTrain: epoch  7, batch    42 | loss: 78.1695162CurrentTrain: epoch  7, batch    43 | loss: 124.4454683CurrentTrain: epoch  7, batch    44 | loss: 57.0212498CurrentTrain: epoch  7, batch    45 | loss: 102.5268934CurrentTrain: epoch  7, batch    46 | loss: 65.5990882CurrentTrain: epoch  7, batch    47 | loss: 79.6969965CurrentTrain: epoch  7, batch    48 | loss: 80.9582347CurrentTrain: epoch  7, batch    49 | loss: 125.2790487CurrentTrain: epoch  7, batch    50 | loss: 100.2422923CurrentTrain: epoch  7, batch    51 | loss: 101.7198929CurrentTrain: epoch  7, batch    52 | loss: 76.1740838CurrentTrain: epoch  7, batch    53 | loss: 123.2036328CurrentTrain: epoch  7, batch    54 | loss: 79.9811758CurrentTrain: epoch  7, batch    55 | loss: 81.2858412CurrentTrain: epoch  7, batch    56 | loss: 64.9550122CurrentTrain: epoch  7, batch    57 | loss: 125.7105847CurrentTrain: epoch  7, batch    58 | loss: 69.7914605CurrentTrain: epoch  7, batch    59 | loss: 82.0841968CurrentTrain: epoch  7, batch    60 | loss: 94.4473299CurrentTrain: epoch  7, batch    61 | loss: 78.9590815CurrentTrain: epoch  7, batch    62 | loss: 98.2084797CurrentTrain: epoch  7, batch    63 | loss: 80.8412841CurrentTrain: epoch  7, batch    64 | loss: 55.9262189CurrentTrain: epoch  7, batch    65 | loss: 77.4999913CurrentTrain: epoch  7, batch    66 | loss: 67.4514481CurrentTrain: epoch  7, batch    67 | loss: 81.2574684CurrentTrain: epoch  7, batch    68 | loss: 79.3267601CurrentTrain: epoch  7, batch    69 | loss: 99.3808497CurrentTrain: epoch  7, batch    70 | loss: 79.8051895CurrentTrain: epoch  7, batch    71 | loss: 56.4185975CurrentTrain: epoch  7, batch    72 | loss: 77.6556798CurrentTrain: epoch  7, batch    73 | loss: 78.4358541CurrentTrain: epoch  7, batch    74 | loss: 122.9495708CurrentTrain: epoch  7, batch    75 | loss: 95.0021384CurrentTrain: epoch  7, batch    76 | loss: 79.5093729CurrentTrain: epoch  7, batch    77 | loss: 57.2293505CurrentTrain: epoch  7, batch    78 | loss: 261.1011518CurrentTrain: epoch  7, batch    79 | loss: 78.3179866CurrentTrain: epoch  7, batch    80 | loss: 78.6933894CurrentTrain: epoch  7, batch    81 | loss: 63.5053311CurrentTrain: epoch  7, batch    82 | loss: 103.2503710CurrentTrain: epoch  7, batch    83 | loss: 77.8893302CurrentTrain: epoch  7, batch    84 | loss: 81.2999095CurrentTrain: epoch  7, batch    85 | loss: 79.0875691CurrentTrain: epoch  7, batch    86 | loss: 126.0095401CurrentTrain: epoch  7, batch    87 | loss: 99.8755173CurrentTrain: epoch  7, batch    88 | loss: 122.5048565CurrentTrain: epoch  7, batch    89 | loss: 78.3308078CurrentTrain: epoch  7, batch    90 | loss: 80.0793183CurrentTrain: epoch  7, batch    91 | loss: 125.3253451CurrentTrain: epoch  7, batch    92 | loss: 126.2886468CurrentTrain: epoch  7, batch    93 | loss: 64.6714854CurrentTrain: epoch  7, batch    94 | loss: 52.0077974CurrentTrain: epoch  7, batch    95 | loss: 100.9229659CurrentTrain: epoch  8, batch     0 | loss: 100.0615739CurrentTrain: epoch  8, batch     1 | loss: 97.1833524CurrentTrain: epoch  8, batch     2 | loss: 79.8571285CurrentTrain: epoch  8, batch     3 | loss: 64.1382749CurrentTrain: epoch  8, batch     4 | loss: 67.0290212CurrentTrain: epoch  8, batch     5 | loss: 92.9122021CurrentTrain: epoch  8, batch     6 | loss: 77.7663854CurrentTrain: epoch  8, batch     7 | loss: 127.2476534CurrentTrain: epoch  8, batch     8 | loss: 67.3515754CurrentTrain: epoch  8, batch     9 | loss: 63.3694834CurrentTrain: epoch  8, batch    10 | loss: 60.1428685CurrentTrain: epoch  8, batch    11 | loss: 63.5456487CurrentTrain: epoch  8, batch    12 | loss: 64.9025033CurrentTrain: epoch  8, batch    13 | loss: 172.0488126CurrentTrain: epoch  8, batch    14 | loss: 78.6671253CurrentTrain: epoch  8, batch    15 | loss: 78.5669880CurrentTrain: epoch  8, batch    16 | loss: 124.6645025CurrentTrain: epoch  8, batch    17 | loss: 98.4430345CurrentTrain: epoch  8, batch    18 | loss: 97.8008476CurrentTrain: epoch  8, batch    19 | loss: 66.5037621CurrentTrain: epoch  8, batch    20 | loss: 54.6189076CurrentTrain: epoch  8, batch    21 | loss: 63.3789030CurrentTrain: epoch  8, batch    22 | loss: 124.6753773CurrentTrain: epoch  8, batch    23 | loss: 76.5246426CurrentTrain: epoch  8, batch    24 | loss: 97.1735192CurrentTrain: epoch  8, batch    25 | loss: 65.2461978CurrentTrain: epoch  8, batch    26 | loss: 95.2612771CurrentTrain: epoch  8, batch    27 | loss: 120.1184290CurrentTrain: epoch  8, batch    28 | loss: 81.8063722CurrentTrain: epoch  8, batch    29 | loss: 91.8087189CurrentTrain: epoch  8, batch    30 | loss: 69.4189247CurrentTrain: epoch  8, batch    31 | loss: 125.7390436CurrentTrain: epoch  8, batch    32 | loss: 122.5396094CurrentTrain: epoch  8, batch    33 | loss: 91.9697660CurrentTrain: epoch  8, batch    34 | loss: 126.4548503CurrentTrain: epoch  8, batch    35 | loss: 120.4075738CurrentTrain: epoch  8, batch    36 | loss: 76.7081084CurrentTrain: epoch  8, batch    37 | loss: 94.5693662CurrentTrain: epoch  8, batch    38 | loss: 125.2315165CurrentTrain: epoch  8, batch    39 | loss: 79.0110082CurrentTrain: epoch  8, batch    40 | loss: 76.7836403CurrentTrain: epoch  8, batch    41 | loss: 62.0777251CurrentTrain: epoch  8, batch    42 | loss: 167.3236370CurrentTrain: epoch  8, batch    43 | loss: 94.4964829CurrentTrain: epoch  8, batch    44 | loss: 94.3205920CurrentTrain: epoch  8, batch    45 | loss: 78.9221875CurrentTrain: epoch  8, batch    46 | loss: 81.7469235CurrentTrain: epoch  8, batch    47 | loss: 123.3137104CurrentTrain: epoch  8, batch    48 | loss: 76.6810293CurrentTrain: epoch  8, batch    49 | loss: 64.7436188CurrentTrain: epoch  8, batch    50 | loss: 67.4050041CurrentTrain: epoch  8, batch    51 | loss: 67.0402766CurrentTrain: epoch  8, batch    52 | loss: 63.8092329CurrentTrain: epoch  8, batch    53 | loss: 105.4526326CurrentTrain: epoch  8, batch    54 | loss: 96.7005262CurrentTrain: epoch  8, batch    55 | loss: 80.4839537CurrentTrain: epoch  8, batch    56 | loss: 80.2040674CurrentTrain: epoch  8, batch    57 | loss: 89.7969896CurrentTrain: epoch  8, batch    58 | loss: 93.6415351CurrentTrain: epoch  8, batch    59 | loss: 122.3991733CurrentTrain: epoch  8, batch    60 | loss: 78.7821089CurrentTrain: epoch  8, batch    61 | loss: 101.0908529CurrentTrain: epoch  8, batch    62 | loss: 63.6045195CurrentTrain: epoch  8, batch    63 | loss: 67.3604346CurrentTrain: epoch  8, batch    64 | loss: 76.1523654CurrentTrain: epoch  8, batch    65 | loss: 96.4252056CurrentTrain: epoch  8, batch    66 | loss: 65.1227810CurrentTrain: epoch  8, batch    67 | loss: 77.6612374CurrentTrain: epoch  8, batch    68 | loss: 77.7738072CurrentTrain: epoch  8, batch    69 | loss: 67.3585805CurrentTrain: epoch  8, batch    70 | loss: 69.1996273CurrentTrain: epoch  8, batch    71 | loss: 66.6526256CurrentTrain: epoch  8, batch    72 | loss: 126.4967175CurrentTrain: epoch  8, batch    73 | loss: 67.5130442CurrentTrain: epoch  8, batch    74 | loss: 75.0024184CurrentTrain: epoch  8, batch    75 | loss: 100.7331809CurrentTrain: epoch  8, batch    76 | loss: 79.5794816CurrentTrain: epoch  8, batch    77 | loss: 67.0331788CurrentTrain: epoch  8, batch    78 | loss: 63.6998275CurrentTrain: epoch  8, batch    79 | loss: 54.2715270CurrentTrain: epoch  8, batch    80 | loss: 77.2033788CurrentTrain: epoch  8, batch    81 | loss: 64.7996626CurrentTrain: epoch  8, batch    82 | loss: 64.8578596CurrentTrain: epoch  8, batch    83 | loss: 63.7117754CurrentTrain: epoch  8, batch    84 | loss: 98.1995150CurrentTrain: epoch  8, batch    85 | loss: 78.5717465CurrentTrain: epoch  8, batch    86 | loss: 56.8170746CurrentTrain: epoch  8, batch    87 | loss: 76.4158776CurrentTrain: epoch  8, batch    88 | loss: 79.5380041CurrentTrain: epoch  8, batch    89 | loss: 66.3655888CurrentTrain: epoch  8, batch    90 | loss: 124.8572751CurrentTrain: epoch  8, batch    91 | loss: 64.6058989CurrentTrain: epoch  8, batch    92 | loss: 67.5904061CurrentTrain: epoch  8, batch    93 | loss: 98.3090122CurrentTrain: epoch  8, batch    94 | loss: 97.1797952CurrentTrain: epoch  8, batch    95 | loss: 83.2621446CurrentTrain: epoch  9, batch     0 | loss: 120.0224060CurrentTrain: epoch  9, batch     1 | loss: 78.4063522CurrentTrain: epoch  9, batch     2 | loss: 66.8439300CurrentTrain: epoch  9, batch     3 | loss: 121.5635072CurrentTrain: epoch  9, batch     4 | loss: 163.5662749CurrentTrain: epoch  9, batch     5 | loss: 98.3783665CurrentTrain: epoch  9, batch     6 | loss: 102.2740770CurrentTrain: epoch  9, batch     7 | loss: 64.7563613CurrentTrain: epoch  9, batch     8 | loss: 79.0982914CurrentTrain: epoch  9, batch     9 | loss: 81.6496005CurrentTrain: epoch  9, batch    10 | loss: 66.0508890CurrentTrain: epoch  9, batch    11 | loss: 79.2399161CurrentTrain: epoch  9, batch    12 | loss: 65.9148654CurrentTrain: epoch  9, batch    13 | loss: 77.6836125CurrentTrain: epoch  9, batch    14 | loss: 92.6553538CurrentTrain: epoch  9, batch    15 | loss: 57.0916061CurrentTrain: epoch  9, batch    16 | loss: 124.4907767CurrentTrain: epoch  9, batch    17 | loss: 122.6240934CurrentTrain: epoch  9, batch    18 | loss: 72.9053679CurrentTrain: epoch  9, batch    19 | loss: 97.3283444CurrentTrain: epoch  9, batch    20 | loss: 77.3146172CurrentTrain: epoch  9, batch    21 | loss: 95.6976136CurrentTrain: epoch  9, batch    22 | loss: 74.4521329CurrentTrain: epoch  9, batch    23 | loss: 91.3205908CurrentTrain: epoch  9, batch    24 | loss: 94.5592420CurrentTrain: epoch  9, batch    25 | loss: 97.9058586CurrentTrain: epoch  9, batch    26 | loss: 79.9879678CurrentTrain: epoch  9, batch    27 | loss: 169.4247136CurrentTrain: epoch  9, batch    28 | loss: 67.5336754CurrentTrain: epoch  9, batch    29 | loss: 95.7673249CurrentTrain: epoch  9, batch    30 | loss: 75.7055758CurrentTrain: epoch  9, batch    31 | loss: 73.5942860CurrentTrain: epoch  9, batch    32 | loss: 75.1373392CurrentTrain: epoch  9, batch    33 | loss: 76.9527985CurrentTrain: epoch  9, batch    34 | loss: 65.8933316CurrentTrain: epoch  9, batch    35 | loss: 67.0209316CurrentTrain: epoch  9, batch    36 | loss: 77.4954701CurrentTrain: epoch  9, batch    37 | loss: 66.1296331CurrentTrain: epoch  9, batch    38 | loss: 73.6658627CurrentTrain: epoch  9, batch    39 | loss: 96.7687432CurrentTrain: epoch  9, batch    40 | loss: 78.4337565CurrentTrain: epoch  9, batch    41 | loss: 73.9652038CurrentTrain: epoch  9, batch    42 | loss: 75.3984108CurrentTrain: epoch  9, batch    43 | loss: 67.7790653CurrentTrain: epoch  9, batch    44 | loss: 52.8234584CurrentTrain: epoch  9, batch    45 | loss: 65.7582666CurrentTrain: epoch  9, batch    46 | loss: 80.8999959CurrentTrain: epoch  9, batch    47 | loss: 57.4802733CurrentTrain: epoch  9, batch    48 | loss: 76.1682517CurrentTrain: epoch  9, batch    49 | loss: 72.3623789CurrentTrain: epoch  9, batch    50 | loss: 68.1003686CurrentTrain: epoch  9, batch    51 | loss: 68.7593014CurrentTrain: epoch  9, batch    52 | loss: 95.0318277CurrentTrain: epoch  9, batch    53 | loss: 99.7054803CurrentTrain: epoch  9, batch    54 | loss: 74.3218692CurrentTrain: epoch  9, batch    55 | loss: 76.6538293CurrentTrain: epoch  9, batch    56 | loss: 127.6940745CurrentTrain: epoch  9, batch    57 | loss: 76.3052953CurrentTrain: epoch  9, batch    58 | loss: 96.9051422CurrentTrain: epoch  9, batch    59 | loss: 73.5643669CurrentTrain: epoch  9, batch    60 | loss: 52.2808651CurrentTrain: epoch  9, batch    61 | loss: 78.2626806CurrentTrain: epoch  9, batch    62 | loss: 76.2778966CurrentTrain: epoch  9, batch    63 | loss: 97.6654569CurrentTrain: epoch  9, batch    64 | loss: 119.3354010CurrentTrain: epoch  9, batch    65 | loss: 124.8338577CurrentTrain: epoch  9, batch    66 | loss: 121.4480808CurrentTrain: epoch  9, batch    67 | loss: 78.3522374CurrentTrain: epoch  9, batch    68 | loss: 75.9729156CurrentTrain: epoch  9, batch    69 | loss: 62.0832532CurrentTrain: epoch  9, batch    70 | loss: 75.5262416CurrentTrain: epoch  9, batch    71 | loss: 66.2346461CurrentTrain: epoch  9, batch    72 | loss: 78.0224676CurrentTrain: epoch  9, batch    73 | loss: 116.7931281CurrentTrain: epoch  9, batch    74 | loss: 56.5894458CurrentTrain: epoch  9, batch    75 | loss: 64.0734371CurrentTrain: epoch  9, batch    76 | loss: 76.6076389CurrentTrain: epoch  9, batch    77 | loss: 62.1968281CurrentTrain: epoch  9, batch    78 | loss: 68.4223824CurrentTrain: epoch  9, batch    79 | loss: 57.1050187CurrentTrain: epoch  9, batch    80 | loss: 92.6340475CurrentTrain: epoch  9, batch    81 | loss: 99.6279352CurrentTrain: epoch  9, batch    82 | loss: 68.4437061CurrentTrain: epoch  9, batch    83 | loss: 66.1482129CurrentTrain: epoch  9, batch    84 | loss: 74.8138524CurrentTrain: epoch  9, batch    85 | loss: 81.1094278CurrentTrain: epoch  9, batch    86 | loss: 58.5210822CurrentTrain: epoch  9, batch    87 | loss: 122.2136139CurrentTrain: epoch  9, batch    88 | loss: 53.0094201CurrentTrain: epoch  9, batch    89 | loss: 57.4489504CurrentTrain: epoch  9, batch    90 | loss: 65.6720604CurrentTrain: epoch  9, batch    91 | loss: 119.2589170CurrentTrain: epoch  9, batch    92 | loss: 124.9384779CurrentTrain: epoch  9, batch    93 | loss: 61.8894856CurrentTrain: epoch  9, batch    94 | loss: 80.2767084CurrentTrain: epoch  9, batch    95 | loss: 109.9426508

F1 score per class: {32: 0.5876288659793815, 6: 0.7894736842105263, 19: 0.391304347826087, 24: 0.7613636363636364, 26: 0.9183673469387755, 29: 0.8229665071770335}
Micro-average F1 score: 0.7607244995233555
Weighted-average F1 score: 0.758901163224473
F1 score per class: {32: 0.5729166666666666, 6: 0.7777777777777778, 19: 0.24, 24: 0.7542857142857143, 26: 0.94, 29: 0.8181818181818182}
Micro-average F1 score: 0.7390510948905109
Weighted-average F1 score: 0.7255429105688621
F1 score per class: {32: 0.5699481865284974, 6: 0.7758620689655172, 19: 0.3157894736842105, 24: 0.7542857142857143, 26: 0.9346733668341709, 29: 0.827906976744186}
Micro-average F1 score: 0.7507002801120448
Weighted-average F1 score: 0.7451142100142416

F1 score per class: {32: 0.5876288659793815, 6: 0.7894736842105263, 19: 0.391304347826087, 24: 0.7613636363636364, 26: 0.9183673469387755, 29: 0.8229665071770335}
Micro-average F1 score: 0.7607244995233555
Weighted-average F1 score: 0.758901163224473
F1 score per class: {32: 0.5729166666666666, 6: 0.7777777777777778, 19: 0.24, 24: 0.7542857142857143, 26: 0.94, 29: 0.8181818181818182}
Micro-average F1 score: 0.7390510948905109
Weighted-average F1 score: 0.7255429105688621
F1 score per class: {32: 0.5699481865284974, 6: 0.7758620689655172, 19: 0.3157894736842105, 24: 0.7542857142857143, 26: 0.9346733668341709, 29: 0.827906976744186}
Micro-average F1 score: 0.7507002801120448
Weighted-average F1 score: 0.7451142100142416

F1 score per class: {32: 0.4367816091954023, 6: 0.728744939271255, 19: 0.20930232558139536, 24: 0.708994708994709, 26: 0.8490566037735849, 29: 0.6590038314176245}
Micro-average F1 score: 0.6353503184713376
Weighted-average F1 score: 0.6178514754394002
F1 score per class: {32: 0.44176706827309237, 6: 0.7109375, 19: 0.13636363636363635, 24: 0.7021276595744681, 26: 0.8703703703703703, 29: 0.631578947368421}
Micro-average F1 score: 0.6108597285067874
Weighted-average F1 score: 0.5839941831520041
F1 score per class: {32: 0.44, 6: 0.7114624505928854, 19: 0.16981132075471697, 24: 0.7021276595744681, 26: 0.8651162790697674, 29: 0.6425992779783394}
Micro-average F1 score: 0.6237393328161366
Weighted-average F1 score: 0.6028700771788186

F1 score per class: {32: 0.4367816091954023, 6: 0.728744939271255, 19: 0.20930232558139536, 24: 0.708994708994709, 26: 0.8490566037735849, 29: 0.6590038314176245}
Micro-average F1 score: 0.6353503184713376
Weighted-average F1 score: 0.6178514754394002
F1 score per class: {32: 0.44176706827309237, 6: 0.7109375, 19: 0.13636363636363635, 24: 0.7021276595744681, 26: 0.8703703703703703, 29: 0.631578947368421}
Micro-average F1 score: 0.6108597285067874
Weighted-average F1 score: 0.5839941831520041
F1 score per class: {32: 0.44, 6: 0.7114624505928854, 19: 0.16981132075471697, 24: 0.7021276595744681, 26: 0.8651162790697674, 29: 0.6425992779783394}
Micro-average F1 score: 0.6237393328161366
Weighted-average F1 score: 0.6028700771788186
cur_acc_wo_na:  ['0.7607']
his_acc_wo_na:  ['0.7607']
cur_acc des_wo_na:  ['0.7391']
his_acc des_wo_na:  ['0.7391']
cur_acc rrf_wo_na:  ['0.7507']
his_acc rrf_wo_na:  ['0.7507']
cur_acc_w_na:  ['0.6354']
his_acc_w_na:  ['0.6354']
cur_acc des_w_na:  ['0.6109']
his_acc des_w_na:  ['0.6109']
cur_acc rrf_w_na:  ['0.6237']
his_acc rrf_w_na:  ['0.6237']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 93.2946798CurrentTrain: epoch  0, batch     1 | loss: 120.2649716CurrentTrain: epoch  0, batch     2 | loss: 116.3710878CurrentTrain: epoch  0, batch     3 | loss: 116.7776215CurrentTrain: epoch  0, batch     4 | loss: 57.6965929CurrentTrain: epoch  1, batch     0 | loss: 91.1497659CurrentTrain: epoch  1, batch     1 | loss: 106.7729715CurrentTrain: epoch  1, batch     2 | loss: 110.1351851CurrentTrain: epoch  1, batch     3 | loss: 86.6006326CurrentTrain: epoch  1, batch     4 | loss: 88.8442895CurrentTrain: epoch  2, batch     0 | loss: 86.2718625CurrentTrain: epoch  2, batch     1 | loss: 104.5939264CurrentTrain: epoch  2, batch     2 | loss: 106.9216365CurrentTrain: epoch  2, batch     3 | loss: 83.7817849CurrentTrain: epoch  2, batch     4 | loss: 84.6100998CurrentTrain: epoch  3, batch     0 | loss: 266.2736442CurrentTrain: epoch  3, batch     1 | loss: 83.3645617CurrentTrain: epoch  3, batch     2 | loss: 82.6536710CurrentTrain: epoch  3, batch     3 | loss: 104.3824618CurrentTrain: epoch  3, batch     4 | loss: 53.9088959CurrentTrain: epoch  4, batch     0 | loss: 72.1516096CurrentTrain: epoch  4, batch     1 | loss: 97.4895735CurrentTrain: epoch  4, batch     2 | loss: 127.8641456CurrentTrain: epoch  4, batch     3 | loss: 83.9948957CurrentTrain: epoch  4, batch     4 | loss: 82.4052516CurrentTrain: epoch  5, batch     0 | loss: 103.4615178CurrentTrain: epoch  5, batch     1 | loss: 82.8675669CurrentTrain: epoch  5, batch     2 | loss: 124.7346627CurrentTrain: epoch  5, batch     3 | loss: 78.5475156CurrentTrain: epoch  5, batch     4 | loss: 77.6616047CurrentTrain: epoch  6, batch     0 | loss: 79.1483780CurrentTrain: epoch  6, batch     1 | loss: 122.8873438CurrentTrain: epoch  6, batch     2 | loss: 79.4610815CurrentTrain: epoch  6, batch     3 | loss: 123.9250429CurrentTrain: epoch  6, batch     4 | loss: 63.5206154CurrentTrain: epoch  7, batch     0 | loss: 68.4649657CurrentTrain: epoch  7, batch     1 | loss: 98.3489285CurrentTrain: epoch  7, batch     2 | loss: 64.5586534CurrentTrain: epoch  7, batch     3 | loss: 81.7754420CurrentTrain: epoch  7, batch     4 | loss: 161.7563244CurrentTrain: epoch  8, batch     0 | loss: 66.2807722CurrentTrain: epoch  8, batch     1 | loss: 124.9106608CurrentTrain: epoch  8, batch     2 | loss: 67.6218072CurrentTrain: epoch  8, batch     3 | loss: 79.5522742CurrentTrain: epoch  8, batch     4 | loss: 72.7062997CurrentTrain: epoch  9, batch     0 | loss: 63.5991322CurrentTrain: epoch  9, batch     1 | loss: 80.7945897CurrentTrain: epoch  9, batch     2 | loss: 77.1731528CurrentTrain: epoch  9, batch     3 | loss: 96.5846938CurrentTrain: epoch  9, batch     4 | loss: 76.3410908
MemoryTrain:  epoch  0, batch     0 | loss: 1.5395724MemoryTrain:  epoch  1, batch     0 | loss: 1.3482368MemoryTrain:  epoch  2, batch     0 | loss: 1.1591323MemoryTrain:  epoch  3, batch     0 | loss: 0.7956124MemoryTrain:  epoch  4, batch     0 | loss: 0.5972796MemoryTrain:  epoch  5, batch     0 | loss: 0.5678363MemoryTrain:  epoch  6, batch     0 | loss: 0.4172274MemoryTrain:  epoch  7, batch     0 | loss: 0.3588225MemoryTrain:  epoch  8, batch     0 | loss: 0.3125552MemoryTrain:  epoch  9, batch     0 | loss: 0.2562108

F1 score per class: {32: 0.8518518518518519, 5: 0.0, 6: 0.1308411214953271, 10: 0.5882352941176471, 16: 0.0, 17: 0.42857142857142855, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5191146881287726
Weighted-average F1 score: 0.571160637939411
F1 score per class: {32: 0.6934306569343066, 5: 0.0, 6: 0.5384615384615384, 10: 0.6349206349206349, 16: 0.0, 17: 0.4, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5230312035661219
Weighted-average F1 score: 0.491091086682107
F1 score per class: {32: 0.7450980392156863, 5: 0.0, 6: 0.4117647058823529, 10: 0.6451612903225806, 16: 0.0, 17: 0.4634146341463415, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5337726523887973
Weighted-average F1 score: 0.5190711338885318

F1 score per class: {32: 0.832579185520362, 5: 0.5622119815668203, 6: 0.12844036697247707, 10: 0.5660377358490566, 16: 0.0, 17: 0.39473684210526316, 18: 0.7868852459016393, 19: 0.3125, 24: 0.7539267015706806, 26: 0.898989898989899, 29: 0.7692307692307693}
Micro-average F1 score: 0.676980198019802
Weighted-average F1 score: 0.7099296712790375
F1 score per class: {32: 0.6484641638225256, 5: 0.5872340425531914, 6: 0.48554913294797686, 10: 0.6060606060606061, 16: 0.0, 17: 0.36538461538461536, 18: 0.6555183946488294, 19: 0.20833333333333334, 24: 0.7395833333333334, 26: 0.8878048780487805, 29: 0.7654320987654321}
Micro-average F1 score: 0.6343244653103808
Weighted-average F1 score: 0.6265274680649546
F1 score per class: {32: 0.6985294117647058, 5: 0.5714285714285714, 6: 0.3708609271523179, 10: 0.6153846153846154, 16: 0.0, 17: 0.4318181818181818, 18: 0.7101449275362319, 19: 0.25, 24: 0.7461139896373057, 26: 0.896551724137931, 29: 0.7603305785123967}
Micro-average F1 score: 0.6530386740331492
Weighted-average F1 score: 0.6571507635770105

F1 score per class: {32: 0.6865671641791045, 5: 0.0, 6: 0.12612612612612611, 10: 0.35294117647058826, 16: 0.0, 17: 0.3409090909090909, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.3891402714932127
Weighted-average F1 score: 0.3967152436995425
F1 score per class: {32: 0.5026455026455027, 5: 0.0, 6: 0.4221105527638191, 10: 0.39215686274509803, 16: 0.0, 17: 0.31666666666666665, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.36666666666666664
Weighted-average F1 score: 0.3450921372953921
F1 score per class: {32: 0.5491329479768786, 5: 0.0, 6: 0.35668789808917195, 10: 0.39603960396039606, 16: 0.0, 17: 0.3584905660377358, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.37894736842105264
Weighted-average F1 score: 0.35971083995023634

F1 score per class: {32: 0.647887323943662, 5: 0.3630952380952381, 6: 0.11864406779661017, 10: 0.3333333333333333, 16: 0.0, 17: 0.2830188679245283, 18: 0.6832740213523132, 19: 0.1694915254237288, 24: 0.6728971962616822, 26: 0.7807017543859649, 29: 0.5769230769230769}
Micro-average F1 score: 0.520952380952381
Weighted-average F1 score: 0.5253715057835456
F1 score per class: {32: 0.4367816091954023, 5: 0.3791208791208791, 6: 0.3514644351464435, 10: 0.35398230088495575, 16: 0.0, 17: 0.2676056338028169, 18: 0.5311653116531165, 19: 0.11834319526627218, 24: 0.6454545454545455, 26: 0.7459016393442623, 29: 0.5758513931888545}
Micro-average F1 score: 0.4616552771450266
Weighted-average F1 score: 0.44933421940087176
F1 score per class: {32: 0.48717948717948717, 5: 0.36666666666666664, 6: 0.2931937172774869, 10: 0.37037037037037035, 16: 0.0, 17: 0.3064516129032258, 18: 0.5903614457831325, 19: 0.1342281879194631, 24: 0.6545454545454545, 26: 0.7647058823529411, 29: 0.5714285714285714}
Micro-average F1 score: 0.48284313725490197
Weighted-average F1 score: 0.4741994856744529
cur_acc_wo_na:  ['0.7607', '0.5191']
his_acc_wo_na:  ['0.7607', '0.6770']
cur_acc des_wo_na:  ['0.7391', '0.5230']
his_acc des_wo_na:  ['0.7391', '0.6343']
cur_acc rrf_wo_na:  ['0.7507', '0.5338']
his_acc rrf_wo_na:  ['0.7507', '0.6530']
cur_acc_w_na:  ['0.6354', '0.3891']
his_acc_w_na:  ['0.6354', '0.5210']
cur_acc des_w_na:  ['0.6109', '0.3667']
his_acc des_w_na:  ['0.6109', '0.4617']
cur_acc rrf_w_na:  ['0.6237', '0.3789']
his_acc rrf_w_na:  ['0.6237', '0.4828']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 81.7649767CurrentTrain: epoch  0, batch     1 | loss: 97.3623616CurrentTrain: epoch  0, batch     2 | loss: 113.5551015CurrentTrain: epoch  0, batch     3 | loss: 79.1549004CurrentTrain: epoch  0, batch     4 | loss: 40.7224617CurrentTrain: epoch  1, batch     0 | loss: 91.9246735CurrentTrain: epoch  1, batch     1 | loss: 78.9280875CurrentTrain: epoch  1, batch     2 | loss: 91.9590483CurrentTrain: epoch  1, batch     3 | loss: 85.4891336CurrentTrain: epoch  1, batch     4 | loss: 38.8690871CurrentTrain: epoch  2, batch     0 | loss: 101.4535359CurrentTrain: epoch  2, batch     1 | loss: 89.5283179CurrentTrain: epoch  2, batch     2 | loss: 103.4075109CurrentTrain: epoch  2, batch     3 | loss: 68.1653391CurrentTrain: epoch  2, batch     4 | loss: 24.6853618CurrentTrain: epoch  3, batch     0 | loss: 84.9008943CurrentTrain: epoch  3, batch     1 | loss: 71.4934752CurrentTrain: epoch  3, batch     2 | loss: 100.8746554CurrentTrain: epoch  3, batch     3 | loss: 82.7682893CurrentTrain: epoch  3, batch     4 | loss: 13.3349791CurrentTrain: epoch  4, batch     0 | loss: 100.2683326CurrentTrain: epoch  4, batch     1 | loss: 81.3958423CurrentTrain: epoch  4, batch     2 | loss: 66.3689185CurrentTrain: epoch  4, batch     3 | loss: 81.6102270CurrentTrain: epoch  4, batch     4 | loss: 40.3193939CurrentTrain: epoch  5, batch     0 | loss: 80.2473175CurrentTrain: epoch  5, batch     1 | loss: 95.6377015CurrentTrain: epoch  5, batch     2 | loss: 95.3571740CurrentTrain: epoch  5, batch     3 | loss: 78.9975112CurrentTrain: epoch  5, batch     4 | loss: 40.7555589CurrentTrain: epoch  6, batch     0 | loss: 65.8227882CurrentTrain: epoch  6, batch     1 | loss: 80.3065702CurrentTrain: epoch  6, batch     2 | loss: 77.7059898CurrentTrain: epoch  6, batch     3 | loss: 99.3516302CurrentTrain: epoch  6, batch     4 | loss: 15.4849834CurrentTrain: epoch  7, batch     0 | loss: 95.5255856CurrentTrain: epoch  7, batch     1 | loss: 96.2714113CurrentTrain: epoch  7, batch     2 | loss: 70.7180019CurrentTrain: epoch  7, batch     3 | loss: 124.0729863CurrentTrain: epoch  7, batch     4 | loss: 24.2337302CurrentTrain: epoch  8, batch     0 | loss: 95.8580075CurrentTrain: epoch  8, batch     1 | loss: 91.1604391CurrentTrain: epoch  8, batch     2 | loss: 65.5897501CurrentTrain: epoch  8, batch     3 | loss: 65.7295614CurrentTrain: epoch  8, batch     4 | loss: 24.8522108CurrentTrain: epoch  9, batch     0 | loss: 76.8173144CurrentTrain: epoch  9, batch     1 | loss: 74.8995049CurrentTrain: epoch  9, batch     2 | loss: 75.2340471CurrentTrain: epoch  9, batch     3 | loss: 78.2159030CurrentTrain: epoch  9, batch     4 | loss: 24.8680273
MemoryTrain:  epoch  0, batch     0 | loss: 1.1119565MemoryTrain:  epoch  1, batch     0 | loss: 0.9654546MemoryTrain:  epoch  2, batch     0 | loss: 0.7266065MemoryTrain:  epoch  3, batch     0 | loss: 0.6101525MemoryTrain:  epoch  4, batch     0 | loss: 0.5024471MemoryTrain:  epoch  5, batch     0 | loss: 0.4119794MemoryTrain:  epoch  6, batch     0 | loss: 0.3645252MemoryTrain:  epoch  7, batch     0 | loss: 0.3263771MemoryTrain:  epoch  8, batch     0 | loss: 0.2880495MemoryTrain:  epoch  9, batch     0 | loss: 0.2439540

F1 score per class: {32: 0.5555555555555556, 2: 0.0, 5: 0.0, 6: 0.5771812080536913, 39: 0.6390532544378699, 11: 0.0, 12: 0.0, 16: 0.0, 18: 0.0, 19: 0.22727272727272727, 24: 0.0, 28: 0.0, 29: 0.26666666666666666}
Micro-average F1 score: 0.4879120879120879
Weighted-average F1 score: 0.409962767355818
F1 score per class: {32: 0.5833333333333334, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.46616541353383456, 11: 0.5888888888888889, 12: 0.0, 10: 0.0, 18: 0.0, 19: 0.21428571428571427, 24: 0.0, 28: 0.0, 29: 0.26666666666666666}
Micro-average F1 score: 0.38996138996138996
Weighted-average F1 score: 0.30565530325255724
F1 score per class: {32: 0.5263157894736842, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.5285714285714286, 11: 0.6057142857142858, 12: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.2222222222222222, 24: 0.0, 28: 0.0, 29: 0.26666666666666666}
Micro-average F1 score: 0.4329896907216495
Weighted-average F1 score: 0.34836967104477984

F1 score per class: {32: 0.5, 2: 0.8955223880597015, 5: 0.5308056872037915, 6: 0.07407407407407407, 39: 0.3739130434782609, 11: 0.48, 12: 0.4186046511627907, 10: 0.0, 16: 0.0, 17: 0.7863247863247863, 18: 0.4166666666666667, 19: 0.7513227513227513, 24: 0.11235955056179775, 26: 0.8526315789473684, 28: 0.7295081967213115, 29: 0.23529411764705882}
Micro-average F1 score: 0.5766698024459078
Weighted-average F1 score: 0.5878935959767274
F1 score per class: {32: 0.45161290322580644, 2: 0.6643598615916955, 5: 0.5378151260504201, 6: 0.4594594594594595, 39: 0.34831460674157305, 11: 0.37857142857142856, 12: 0.5882352941176471, 10: 0.0, 16: 0.35135135135135137, 17: 0.6832740213523132, 18: 0.2702702702702703, 19: 0.73, 24: 0.11320754716981132, 26: 0.8421052631578947, 28: 0.6618705035971223, 29: 0.20512820512820512}
Micro-average F1 score: 0.5466342254663422
Weighted-average F1 score: 0.5356151776752431
F1 score per class: {32: 0.45454545454545453, 2: 0.7899159663865546, 5: 0.5625, 6: 0.30158730158730157, 39: 0.3627450980392157, 11: 0.4061302681992337, 12: 0.5882352941176471, 10: 0.0, 16: 0.1111111111111111, 17: 0.7384615384615385, 18: 0.3333333333333333, 19: 0.7448979591836735, 24: 0.10619469026548672, 26: 0.8465608465608465, 28: 0.6865671641791045, 29: 0.20512820512820512}
Micro-average F1 score: 0.5617977528089888
Weighted-average F1 score: 0.5571451925420872

F1 score per class: {32: 0.4, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.4777777777777778, 11: 0.5242718446601942, 12: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.12987012987012986, 26: 0.0, 28: 0.0, 29: 0.18181818181818182}
Micro-average F1 score: 0.3518225039619651
Weighted-average F1 score: 0.2920969793813606
F1 score per class: {32: 0.4117647058823529, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.4189189189189189, 11: 0.4669603524229075, 12: 0.0, 10: 0.0, 18: 0.0, 19: 0.0, 24: 0.12, 26: 0.0, 28: 0.0, 29: 0.16666666666666666}
Micro-average F1 score: 0.26613965744400525
Weighted-average F1 score: 0.20637015593809235
F1 score per class: {32: 0.38461538461538464, 2: 0.0, 5: 0.0, 6: 0.0, 39: 0.4484848484848485, 11: 0.48623853211009177, 12: 0.0, 10: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 24: 0.12244897959183673, 26: 0.0, 28: 0.0, 29: 0.16666666666666666}
Micro-average F1 score: 0.29745042492917845
Weighted-average F1 score: 0.2364168716441841

F1 score per class: {32: 0.3448275862068966, 2: 0.7792207792207793, 5: 0.3209169054441261, 6: 0.06896551724137931, 39: 0.2529411764705882, 11: 0.2488479262672811, 12: 0.3, 10: 0.0, 16: 0.0, 17: 0.7330677290836654, 18: 0.24096385542168675, 19: 0.6666666666666666, 24: 0.06172839506172839, 26: 0.7641509433962265, 28: 0.55625, 29: 0.12307692307692308}
Micro-average F1 score: 0.4158751696065129
Weighted-average F1 score: 0.39859164236772743
F1 score per class: {32: 0.2978723404255319, 2: 0.4648910411622276, 5: 0.30403800475059384, 6: 0.36363636363636365, 39: 0.27555555555555555, 11: 0.1948529411764706, 12: 0.375, 10: 0.0, 16: 0.18181818181818182, 17: 0.6018808777429467, 18: 0.16, 19: 0.6347826086956522, 24: 0.06382978723404255, 26: 0.7511737089201878, 28: 0.48936170212765956, 29: 0.10666666666666667}
Micro-average F1 score: 0.3743404609830603
Weighted-average F1 score: 0.3546164184186989
F1 score per class: {32: 0.29411764705882354, 2: 0.5714285714285714, 5: 0.3247422680412371, 6: 0.2602739726027397, 39: 0.2596491228070175, 11: 0.20784313725490197, 12: 0.38961038961038963, 10: 0.0, 16: 0.06976744186046512, 17: 0.6597938144329897, 18: 0.19047619047619047, 19: 0.6488888888888888, 24: 0.05714285714285714, 26: 0.7547169811320755, 28: 0.5082872928176796, 29: 0.1038961038961039}
Micro-average F1 score: 0.3884075291305647
Weighted-average F1 score: 0.3690215762348484
cur_acc_wo_na:  ['0.7607', '0.5191', '0.4879']
his_acc_wo_na:  ['0.7607', '0.6770', '0.5767']
cur_acc des_wo_na:  ['0.7391', '0.5230', '0.3900']
his_acc des_wo_na:  ['0.7391', '0.6343', '0.5466']
cur_acc rrf_wo_na:  ['0.7507', '0.5338', '0.4330']
his_acc rrf_wo_na:  ['0.7507', '0.6530', '0.5618']
cur_acc_w_na:  ['0.6354', '0.3891', '0.3518']
his_acc_w_na:  ['0.6354', '0.5210', '0.4159']
cur_acc des_w_na:  ['0.6109', '0.3667', '0.2661']
his_acc des_w_na:  ['0.6109', '0.4617', '0.3743']
cur_acc rrf_w_na:  ['0.6237', '0.3789', '0.2975']
his_acc rrf_w_na:  ['0.6237', '0.4828', '0.3884']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 112.8031618CurrentTrain: epoch  0, batch     1 | loss: 122.1250418CurrentTrain: epoch  0, batch     2 | loss: 99.2930199CurrentTrain: epoch  0, batch     3 | loss: 101.9336271CurrentTrain: epoch  1, batch     0 | loss: 91.2090183CurrentTrain: epoch  1, batch     1 | loss: 82.9750114CurrentTrain: epoch  1, batch     2 | loss: 108.3109986CurrentTrain: epoch  1, batch     3 | loss: 79.7936124CurrentTrain: epoch  2, batch     0 | loss: 107.0396425CurrentTrain: epoch  2, batch     1 | loss: 85.4194920CurrentTrain: epoch  2, batch     2 | loss: 90.5416830CurrentTrain: epoch  2, batch     3 | loss: 84.1125984CurrentTrain: epoch  3, batch     0 | loss: 104.8952622CurrentTrain: epoch  3, batch     1 | loss: 103.3340285CurrentTrain: epoch  3, batch     2 | loss: 87.8831633CurrentTrain: epoch  3, batch     3 | loss: 78.3406888CurrentTrain: epoch  4, batch     0 | loss: 101.2843176CurrentTrain: epoch  4, batch     1 | loss: 86.8450999CurrentTrain: epoch  4, batch     2 | loss: 83.2003064CurrentTrain: epoch  4, batch     3 | loss: 68.3309935CurrentTrain: epoch  5, batch     0 | loss: 80.1400643CurrentTrain: epoch  5, batch     1 | loss: 122.3573457CurrentTrain: epoch  5, batch     2 | loss: 98.6798527CurrentTrain: epoch  5, batch     3 | loss: 59.2515207CurrentTrain: epoch  6, batch     0 | loss: 65.4359045CurrentTrain: epoch  6, batch     1 | loss: 83.2822577CurrentTrain: epoch  6, batch     2 | loss: 78.6527359CurrentTrain: epoch  6, batch     3 | loss: 142.0830263CurrentTrain: epoch  7, batch     0 | loss: 100.9026238CurrentTrain: epoch  7, batch     1 | loss: 95.9670946CurrentTrain: epoch  7, batch     2 | loss: 78.6715641CurrentTrain: epoch  7, batch     3 | loss: 77.8058948CurrentTrain: epoch  8, batch     0 | loss: 67.0358608CurrentTrain: epoch  8, batch     1 | loss: 67.2789036CurrentTrain: epoch  8, batch     2 | loss: 81.7101874CurrentTrain: epoch  8, batch     3 | loss: 79.4842898CurrentTrain: epoch  9, batch     0 | loss: 65.2473318CurrentTrain: epoch  9, batch     1 | loss: 63.0586662CurrentTrain: epoch  9, batch     2 | loss: 168.7090288CurrentTrain: epoch  9, batch     3 | loss: 101.1573469
MemoryTrain:  epoch  0, batch     0 | loss: 0.8629147MemoryTrain:  epoch  1, batch     0 | loss: 0.7981316MemoryTrain:  epoch  2, batch     0 | loss: 0.6796496MemoryTrain:  epoch  3, batch     0 | loss: 0.5919747MemoryTrain:  epoch  4, batch     0 | loss: 0.5214820MemoryTrain:  epoch  5, batch     0 | loss: 0.4201733MemoryTrain:  epoch  6, batch     0 | loss: 0.3678336MemoryTrain:  epoch  7, batch     0 | loss: 0.3922395MemoryTrain:  epoch  8, batch     0 | loss: 0.2666919MemoryTrain:  epoch  9, batch     0 | loss: 0.2591128

F1 score per class: {0: 0.9041095890410958, 32: 0.0, 2: 0.7757575757575758, 4: 0.0, 5: 0.0, 6: 0.0, 39: 0.0, 11: 0.0, 10: 0.2857142857142857, 13: 0.0, 12: 0.0, 18: 0.5428571428571428, 19: 0.851063829787234, 21: 0.0, 23: 0.0, 29: 0.0}
Micro-average F1 score: 0.6475409836065574
Weighted-average F1 score: 0.5474441969126083
F1 score per class: {0: 0.825, 2: 0.0, 4: 0.7727272727272727, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.26666666666666666, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.45652173913043476, 23: 0.6341463414634146, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.48859934853420195
Weighted-average F1 score: 0.3728728107414097
F1 score per class: {0: 0.8648648648648649, 2: 0.0, 4: 0.8, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.23529411764705882, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.45652173913043476, 23: 0.6419753086419753, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.5209790209790209
Weighted-average F1 score: 0.399143511342467

F1 score per class: {0: 0.673469387755102, 2: 0.36363636363636365, 4: 0.7757575757575758, 5: 0.8598130841121495, 6: 0.5407725321888412, 10: 0.12389380530973451, 11: 0.30927835051546393, 12: 0.3657142857142857, 13: 0.03773584905660377, 16: 0.5, 17: 0.0, 18: 0.0, 19: 0.7381974248927039, 21: 0.40860215053763443, 23: 0.8080808080808081, 24: 0.37037037037037035, 26: 0.745945945945946, 28: 0.15384615384615385, 29: 0.8367346938775511, 32: 0.6926070038910506, 39: 0.2564102564102564}
Micro-average F1 score: 0.5715393563396666
Weighted-average F1 score: 0.5745367399606703
F1 score per class: {0: 0.515625, 2: 0.2222222222222222, 4: 0.7555555555555555, 5: 0.6336633663366337, 6: 0.49808429118773945, 10: 0.3088235294117647, 11: 0.34408602150537637, 12: 0.373015873015873, 13: 0.03389830508474576, 16: 0.5901639344262295, 17: 0.0, 18: 0.19672131147540983, 19: 0.6690140845070423, 21: 0.23595505617977527, 23: 0.5909090909090909, 24: 0.47368421052631576, 26: 0.6826923076923077, 28: 0.14545454545454545, 29: 0.8307692307692308, 32: 0.6334519572953736, 39: 0.17777777777777778}
Micro-average F1 score: 0.5079872204472844
Weighted-average F1 score: 0.4890437598267131
F1 score per class: {0: 0.5378151260504201, 2: 0.3333333333333333, 4: 0.7906976744186046, 5: 0.7380952380952381, 6: 0.512, 10: 0.22950819672131148, 11: 0.3316582914572864, 12: 0.3826086956521739, 13: 0.03278688524590164, 16: 0.6206896551724138, 17: 0.0, 18: 0.14545454545454545, 19: 0.7121212121212122, 21: 0.23333333333333334, 23: 0.6046511627906976, 24: 0.4864864864864865, 26: 0.6829268292682927, 28: 0.16, 29: 0.8426395939086294, 32: 0.6428571428571429, 39: 0.13043478260869565}
Micro-average F1 score: 0.5236974789915967
Weighted-average F1 score: 0.5056763653402055

F1 score per class: {0: 0.8148148148148148, 2: 0.0, 4: 0.7441860465116279, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.16, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.41304347826086957, 23: 0.7547169811320755, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.4884080370942813
Weighted-average F1 score: 0.3802276376145071
F1 score per class: {0: 0.7333333333333333, 2: 0.0, 4: 0.7272727272727273, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.12903225806451613, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3559322033898305, 23: 0.5652173913043478, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3546099290780142
Weighted-average F1 score: 0.25861839768799083
F1 score per class: {0: 0.7529411764705882, 2: 0.0, 4: 0.7555555555555555, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.125, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.35294117647058826, 23: 0.5714285714285714, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 39: 0.0}
Micro-average F1 score: 0.3786531130876747
Weighted-average F1 score: 0.2754185213179922

F1 score per class: {0: 0.528, 2: 0.23076923076923078, 4: 0.7191011235955056, 5: 0.7131782945736435, 6: 0.32225063938618925, 10: 0.11475409836065574, 11: 0.20477815699658702, 12: 0.2006269592476489, 13: 0.018518518518518517, 16: 0.3380281690140845, 17: 0.0, 18: 0.0, 19: 0.6640926640926641, 21: 0.2714285714285714, 23: 0.6666666666666666, 24: 0.22727272727272727, 26: 0.6699029126213593, 28: 0.10526315789473684, 29: 0.70995670995671, 32: 0.4903581267217631, 39: 0.14084507042253522}
Micro-average F1 score: 0.4148606811145511
Weighted-average F1 score: 0.39528385525911497
F1 score per class: {0: 0.4125, 2: 0.15053763440860216, 4: 0.6699507389162561, 5: 0.4129032258064516, 6: 0.2771855010660981, 10: 0.23595505617977527, 11: 0.24615384615384617, 12: 0.1934156378600823, 13: 0.01990049751243781, 16: 0.3564356435643564, 17: 0.0, 18: 0.11320754716981132, 19: 0.5846153846153846, 21: 0.1693548387096774, 23: 0.48598130841121495, 24: 0.21686746987951808, 26: 0.5991561181434599, 28: 0.0761904761904762, 29: 0.7074235807860262, 32: 0.44836272040302266, 39: 0.09876543209876543}
Micro-average F1 score: 0.3497580290365156
Weighted-average F1 score: 0.3289635072741587
F1 score per class: {0: 0.42105263157894735, 2: 0.21212121212121213, 4: 0.7120418848167539, 5: 0.5054347826086957, 6: 0.2929061784897025, 10: 0.19858156028368795, 11: 0.2357142857142857, 12: 0.19599109131403117, 13: 0.017777777777777778, 16: 0.3870967741935484, 17: 0.0, 18: 0.08695652173913043, 19: 0.6245847176079734, 21: 0.1640625, 23: 0.48598130841121495, 24: 0.26865671641791045, 26: 0.6060606060606061, 28: 0.07920792079207921, 29: 0.7094017094017094, 32: 0.46035805626598464, 39: 0.07228915662650602}
Micro-average F1 score: 0.36444444444444446
Weighted-average F1 score: 0.3418492015206167
cur_acc_wo_na:  ['0.7607', '0.5191', '0.4879', '0.6475']
his_acc_wo_na:  ['0.7607', '0.6770', '0.5767', '0.5715']
cur_acc des_wo_na:  ['0.7391', '0.5230', '0.3900', '0.4886']
his_acc des_wo_na:  ['0.7391', '0.6343', '0.5466', '0.5080']
cur_acc rrf_wo_na:  ['0.7507', '0.5338', '0.4330', '0.5210']
his_acc rrf_wo_na:  ['0.7507', '0.6530', '0.5618', '0.5237']
cur_acc_w_na:  ['0.6354', '0.3891', '0.3518', '0.4884']
his_acc_w_na:  ['0.6354', '0.5210', '0.4159', '0.4149']
cur_acc des_w_na:  ['0.6109', '0.3667', '0.2661', '0.3546']
his_acc des_w_na:  ['0.6109', '0.4617', '0.3743', '0.3498']
cur_acc rrf_w_na:  ['0.6237', '0.3789', '0.2975', '0.3787']
his_acc rrf_w_na:  ['0.6237', '0.4828', '0.3884', '0.3644']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 77.3252772CurrentTrain: epoch  0, batch     1 | loss: 119.7041387CurrentTrain: epoch  0, batch     2 | loss: 95.2584140CurrentTrain: epoch  0, batch     3 | loss: 78.5165375CurrentTrain: epoch  1, batch     0 | loss: 91.6063674CurrentTrain: epoch  1, batch     1 | loss: 73.6462112CurrentTrain: epoch  1, batch     2 | loss: 135.5382609CurrentTrain: epoch  1, batch     3 | loss: 56.1112211CurrentTrain: epoch  2, batch     0 | loss: 101.0119031CurrentTrain: epoch  2, batch     1 | loss: 80.4642687CurrentTrain: epoch  2, batch     2 | loss: 71.2403012CurrentTrain: epoch  2, batch     3 | loss: 73.0060331CurrentTrain: epoch  3, batch     0 | loss: 84.0266105CurrentTrain: epoch  3, batch     1 | loss: 78.9379029CurrentTrain: epoch  3, batch     2 | loss: 82.1903385CurrentTrain: epoch  3, batch     3 | loss: 87.3192970CurrentTrain: epoch  4, batch     0 | loss: 81.5344301CurrentTrain: epoch  4, batch     1 | loss: 85.2469828CurrentTrain: epoch  4, batch     2 | loss: 67.9530907CurrentTrain: epoch  4, batch     3 | loss: 52.3135409CurrentTrain: epoch  5, batch     0 | loss: 80.0304977CurrentTrain: epoch  5, batch     1 | loss: 75.4197451CurrentTrain: epoch  5, batch     2 | loss: 81.8990357CurrentTrain: epoch  5, batch     3 | loss: 68.5814108CurrentTrain: epoch  6, batch     0 | loss: 73.0482544CurrentTrain: epoch  6, batch     1 | loss: 68.2866862CurrentTrain: epoch  6, batch     2 | loss: 100.2712951CurrentTrain: epoch  6, batch     3 | loss: 53.7921599CurrentTrain: epoch  7, batch     0 | loss: 76.8025299CurrentTrain: epoch  7, batch     1 | loss: 78.7239874CurrentTrain: epoch  7, batch     2 | loss: 93.9124302CurrentTrain: epoch  7, batch     3 | loss: 62.5062273CurrentTrain: epoch  8, batch     0 | loss: 96.5712816CurrentTrain: epoch  8, batch     1 | loss: 64.7778563CurrentTrain: epoch  8, batch     2 | loss: 74.8125810CurrentTrain: epoch  8, batch     3 | loss: 64.1295652CurrentTrain: epoch  9, batch     0 | loss: 76.5735460CurrentTrain: epoch  9, batch     1 | loss: 77.1581527CurrentTrain: epoch  9, batch     2 | loss: 79.5227390CurrentTrain: epoch  9, batch     3 | loss: 46.5353131
MemoryTrain:  epoch  0, batch     0 | loss: 0.8947203MemoryTrain:  epoch  1, batch     0 | loss: 0.8377508MemoryTrain:  epoch  2, batch     0 | loss: 0.6856259MemoryTrain:  epoch  3, batch     0 | loss: 0.5560164MemoryTrain:  epoch  4, batch     0 | loss: 0.4453944MemoryTrain:  epoch  5, batch     0 | loss: 0.3890123MemoryTrain:  epoch  6, batch     0 | loss: 0.3385137MemoryTrain:  epoch  7, batch     0 | loss: 0.2842316MemoryTrain:  epoch  8, batch     0 | loss: 0.2426504MemoryTrain:  epoch  9, batch     0 | loss: 0.2290270

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.8571428571428571, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.47058823529411764, 32: 0.0, 35: 0.75, 37: 0.5925925925925926, 38: 0.6909090909090909}
Micro-average F1 score: 0.5209302325581395
Weighted-average F1 score: 0.42710468599243323
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.6153846153846154, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.8380952380952381, 37: 0.706766917293233, 38: 0.7586206896551724, 39: 0.0}
Micro-average F1 score: 0.5218978102189781
Weighted-average F1 score: 0.4061760283646611
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.7368421052631579, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5789473684210527, 26: 0.0, 29: 0.0, 32: 0.0, 35: 0.8571428571428571, 37: 0.6776859504132231, 38: 0.7241379310344828}
Micro-average F1 score: 0.5418326693227091
Weighted-average F1 score: 0.43147844179876765

F1 score per class: {0: 0.75, 2: 0.358974358974359, 4: 0.7012987012987013, 5: 0.8017241379310345, 6: 0.4953271028037383, 10: 0.09259259259259259, 11: 0.12244897959183673, 12: 0.40609137055837563, 13: 0.038834951456310676, 15: 0.2465753424657534, 16: 0.5614035087719298, 17: 0.0, 18: 0.08888888888888889, 19: 0.70042194092827, 21: 0.11594202898550725, 23: 0.7912087912087912, 24: 0.3225806451612903, 25: 0.47058823529411764, 26: 0.7419354838709677, 28: 0.25, 29: 0.8442211055276382, 32: 0.6973180076628352, 35: 0.5179856115107914, 37: 0.17204301075268819, 38: 0.2331288343558282, 39: 0.0}
Micro-average F1 score: 0.48182637428657255
Weighted-average F1 score: 0.46175949540158934
F1 score per class: {0: 0.5483870967741935, 2: 0.19718309859154928, 4: 0.7100591715976331, 5: 0.5359116022099447, 6: 0.46332046332046334, 10: 0.16393442622950818, 11: 0.11864406779661017, 12: 0.373134328358209, 13: 0.03076923076923077, 15: 0.34285714285714286, 16: 0.5671641791044776, 17: 0.0, 18: 0.0, 19: 0.6618705035971223, 21: 0.18604651162790697, 23: 0.6458333333333334, 24: 0.2857142857142857, 25: 0.6153846153846154, 26: 0.6857142857142857, 28: 0.17391304347826086, 29: 0.8097560975609757, 32: 0.6896551724137931, 35: 0.5028571428571429, 37: 0.19381443298969073, 38: 0.2619047619047619, 39: 0.1}
Micro-average F1 score: 0.4399801587301587
Weighted-average F1 score: 0.4198990453787297
F1 score per class: {0: 0.5765765765765766, 2: 0.2641509433962264, 4: 0.7654320987654321, 5: 0.6737588652482269, 6: 0.4918032786885246, 10: 0.13675213675213677, 11: 0.10294117647058823, 12: 0.37398373983739835, 13: 0.032520325203252036, 15: 0.2692307692307692, 16: 0.5806451612903226, 17: 0.0, 18: 0.0, 19: 0.6816479400749064, 21: 0.2328767123287671, 23: 0.6666666666666666, 24: 0.34146341463414637, 25: 0.5789473684210527, 26: 0.6990291262135923, 28: 0.16666666666666666, 29: 0.8177339901477833, 32: 0.6818181818181818, 35: 0.5325443786982249, 37: 0.17903930131004367, 38: 0.2441860465116279, 39: 0.125}
Micro-average F1 score: 0.4575645756457565
Weighted-average F1 score: 0.4356497901145473

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6666666666666666, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4266666666666667, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.631578947368421, 37: 0.49230769230769234, 38: 0.5}
Micro-average F1 score: 0.3714759535655058
Weighted-average F1 score: 0.2970091512746658
F1 score per class: {0: 0.0, 2: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.5217391304347826, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5333333333333333, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.6984126984126984, 37: 0.573170731707317, 38: 0.5238095238095238, 39: 0.0}
Micro-average F1 score: 0.35396039603960394
Weighted-average F1 score: 0.27738602499862525
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.5384615384615384, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.5176470588235295, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 35: 0.7086614173228346, 37: 0.5394736842105263, 38: 0.5121951219512195}
Micro-average F1 score: 0.37158469945355194
Weighted-average F1 score: 0.295378758579719

F1 score per class: {0: 0.5892857142857143, 2: 0.2413793103448276, 4: 0.6708074534161491, 5: 0.6619217081850534, 6: 0.2896174863387978, 10: 0.08620689655172414, 11: 0.09424083769633508, 12: 0.20151133501259447, 13: 0.021164021164021163, 15: 0.13138686131386862, 16: 0.36363636363636365, 17: 0.0, 18: 0.056338028169014086, 19: 0.6171003717472119, 21: 0.09090909090909091, 23: 0.6666666666666666, 24: 0.20833333333333334, 25: 0.4266666666666667, 26: 0.6571428571428571, 28: 0.13333333333333333, 29: 0.672, 32: 0.5214899713467048, 35: 0.34285714285714286, 37: 0.1038961038961039, 38: 0.12063492063492064, 39: 0.0}
Micro-average F1 score: 0.3349342242639382
Weighted-average F1 score: 0.30738766606096224
F1 score per class: {0: 0.4276729559748428, 2: 0.13333333333333333, 4: 0.6417112299465241, 5: 0.3527272727272727, 6: 0.26490066225165565, 10: 0.14184397163120568, 11: 0.11290322580645161, 12: 0.18083182640144665, 13: 0.017094017094017096, 15: 0.19672131147540983, 16: 0.3392857142857143, 17: 0.0, 18: 0.0, 19: 0.5609756097560976, 21: 0.1285140562248996, 23: 0.5210084033613446, 24: 0.13953488372093023, 25: 0.5106382978723404, 26: 0.582995951417004, 28: 0.10256410256410256, 29: 0.640926640926641, 32: 0.5172413793103449, 35: 0.3333333333333333, 37: 0.11519607843137254, 38: 0.13924050632911392, 39: 0.08}
Micro-average F1 score: 0.29443983402489626
Weighted-average F1 score: 0.2742554445418089
F1 score per class: {0: 0.4740740740740741, 2: 0.18181818181818182, 4: 0.7294117647058823, 5: 0.4973821989528796, 6: 0.2891566265060241, 10: 0.11940298507462686, 11: 0.09722222222222222, 12: 0.18253968253968253, 13: 0.017937219730941704, 15: 0.16666666666666666, 16: 0.34285714285714286, 17: 0.0, 18: 0.0, 19: 0.5833333333333334, 21: 0.15813953488372093, 23: 0.5217391304347826, 24: 0.18421052631578946, 25: 0.5057471264367817, 26: 0.6127659574468085, 28: 0.08888888888888889, 29: 0.6484375, 32: 0.5056179775280899, 35: 0.3515625, 37: 0.10366624525916561, 38: 0.12844036697247707, 39: 0.1111111111111111}
Micro-average F1 score: 0.3110553664217882
Weighted-average F1 score: 0.2870434305497706
cur_acc_wo_na:  ['0.7607', '0.5191', '0.4879', '0.6475', '0.5209']
his_acc_wo_na:  ['0.7607', '0.6770', '0.5767', '0.5715', '0.4818']
cur_acc des_wo_na:  ['0.7391', '0.5230', '0.3900', '0.4886', '0.5219']
his_acc des_wo_na:  ['0.7391', '0.6343', '0.5466', '0.5080', '0.4400']
cur_acc rrf_wo_na:  ['0.7507', '0.5338', '0.4330', '0.5210', '0.5418']
his_acc rrf_wo_na:  ['0.7507', '0.6530', '0.5618', '0.5237', '0.4576']
cur_acc_w_na:  ['0.6354', '0.3891', '0.3518', '0.4884', '0.3715']
his_acc_w_na:  ['0.6354', '0.5210', '0.4159', '0.4149', '0.3349']
cur_acc des_w_na:  ['0.6109', '0.3667', '0.2661', '0.3546', '0.3540']
his_acc des_w_na:  ['0.6109', '0.4617', '0.3743', '0.3498', '0.2944']
cur_acc rrf_w_na:  ['0.6237', '0.3789', '0.2975', '0.3787', '0.3716']
his_acc rrf_w_na:  ['0.6237', '0.4828', '0.3884', '0.3644', '0.3111']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 96.7951030CurrentTrain: epoch  0, batch     1 | loss: 99.0919662CurrentTrain: epoch  0, batch     2 | loss: 78.6834220CurrentTrain: epoch  0, batch     3 | loss: 58.8432198CurrentTrain: epoch  1, batch     0 | loss: 76.6143300CurrentTrain: epoch  1, batch     1 | loss: 75.4612248CurrentTrain: epoch  1, batch     2 | loss: 84.3195153CurrentTrain: epoch  1, batch     3 | loss: 54.7293227CurrentTrain: epoch  2, batch     0 | loss: 71.2301249CurrentTrain: epoch  2, batch     1 | loss: 103.5944991CurrentTrain: epoch  2, batch     2 | loss: 86.5681086CurrentTrain: epoch  2, batch     3 | loss: 51.5979988CurrentTrain: epoch  3, batch     0 | loss: 70.0507327CurrentTrain: epoch  3, batch     1 | loss: 101.2432274CurrentTrain: epoch  3, batch     2 | loss: 83.1719387CurrentTrain: epoch  3, batch     3 | loss: 48.6003038CurrentTrain: epoch  4, batch     0 | loss: 68.4756096CurrentTrain: epoch  4, batch     1 | loss: 81.1727631CurrentTrain: epoch  4, batch     2 | loss: 68.2075775CurrentTrain: epoch  4, batch     3 | loss: 105.6373854CurrentTrain: epoch  5, batch     0 | loss: 92.5458873CurrentTrain: epoch  5, batch     1 | loss: 80.3141078CurrentTrain: epoch  5, batch     2 | loss: 98.6801494CurrentTrain: epoch  5, batch     3 | loss: 46.4746718CurrentTrain: epoch  6, batch     0 | loss: 79.2506726CurrentTrain: epoch  6, batch     1 | loss: 95.1586953CurrentTrain: epoch  6, batch     2 | loss: 67.0046122CurrentTrain: epoch  6, batch     3 | loss: 52.9723162CurrentTrain: epoch  7, batch     0 | loss: 78.3290650CurrentTrain: epoch  7, batch     1 | loss: 81.0111694CurrentTrain: epoch  7, batch     2 | loss: 61.2334351CurrentTrain: epoch  7, batch     3 | loss: 59.4122671CurrentTrain: epoch  8, batch     0 | loss: 92.3907320CurrentTrain: epoch  8, batch     1 | loss: 91.6856821CurrentTrain: epoch  8, batch     2 | loss: 89.5446887CurrentTrain: epoch  8, batch     3 | loss: 46.9442193CurrentTrain: epoch  9, batch     0 | loss: 75.1030300CurrentTrain: epoch  9, batch     1 | loss: 98.1920678CurrentTrain: epoch  9, batch     2 | loss: 91.1620503CurrentTrain: epoch  9, batch     3 | loss: 34.7962510
MemoryTrain:  epoch  0, batch     0 | loss: 0.8245130MemoryTrain:  epoch  1, batch     0 | loss: 0.8005906MemoryTrain:  epoch  2, batch     0 | loss: 0.6179877MemoryTrain:  epoch  3, batch     0 | loss: 0.5008301MemoryTrain:  epoch  4, batch     0 | loss: 0.4247217MemoryTrain:  epoch  5, batch     0 | loss: 0.3612462MemoryTrain:  epoch  6, batch     0 | loss: 0.3713868MemoryTrain:  epoch  7, batch     0 | loss: 0.2961778MemoryTrain:  epoch  8, batch     0 | loss: 0.2666467MemoryTrain:  epoch  9, batch     0 | loss: 0.2401196

F1 score per class: {0: 0.0, 2: 0.0, 6: 0.0, 8: 0.6299212598425197, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 20: 0.875, 21: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.918918918918919, 32: 0.0, 33: 0.375, 36: 0.4666666666666667, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5615550755939525
Weighted-average F1 score: 0.47130137281593304
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.717948717948718, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 19: 0.0, 20: 0.7920792079207921, 21: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.7659574468085106, 32: 0.0, 33: 0.375, 35: 0.0, 36: 0.7397260273972602, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.5463258785942492
Weighted-average F1 score: 0.43942725904012714
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.7152317880794702, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.8333333333333334, 21: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8372093023255814, 32: 0.0, 33: 0.5, 35: 0.0, 36: 0.7352941176470589, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5836177474402731
Weighted-average F1 score: 0.476422075089214

F1 score per class: {0: 0.7021276595744681, 2: 0.4375, 4: 0.7577639751552795, 5: 0.8355555555555556, 6: 0.45454545454545453, 8: 0.43010752688172044, 10: 0.07692307692307693, 11: 0.12658227848101267, 12: 0.32432432432432434, 13: 0.06779661016949153, 15: 0.2857142857142857, 16: 0.5, 17: 0.0, 18: 0.0, 19: 0.7706422018348624, 20: 0.43946188340807174, 21: 0.10714285714285714, 23: 0.6666666666666666, 24: 0.2857142857142857, 25: 0.4, 26: 0.6979166666666666, 28: 0.16, 29: 0.8349514563106796, 30: 0.918918918918919, 32: 0.624561403508772, 33: 0.18181818181818182, 35: 0.47706422018348627, 36: 0.42, 37: 0.3163841807909605, 38: 0.19047619047619047, 39: 0.0}
Micro-average F1 score: 0.5043427909669947
Weighted-average F1 score: 0.5191484680828818
F1 score per class: {0: 0.5354330708661418, 2: 0.20588235294117646, 4: 0.7602339181286549, 5: 0.5395095367847411, 6: 0.4819277108433735, 8: 0.36245954692556637, 10: 0.10619469026548672, 11: 0.1780821917808219, 12: 0.329004329004329, 13: 0.06896551724137931, 15: 0.24, 16: 0.5142857142857142, 17: 0.0, 18: 0.04878048780487805, 19: 0.6691729323308271, 20: 0.47337278106508873, 21: 0.1830065359477124, 23: 0.6373626373626373, 24: 0.26865671641791045, 25: 0.4225352112676056, 26: 0.6507177033492823, 28: 0.23076923076923078, 29: 0.8165137614678899, 30: 0.46153846153846156, 32: 0.6844106463878327, 33: 0.12, 35: 0.4942528735632184, 36: 0.4235294117647059, 37: 0.20987654320987653, 38: 0.23668639053254437, 39: 0.08}
Micro-average F1 score: 0.4432900432900433
Weighted-average F1 score: 0.43455997177784755
F1 score per class: {0: 0.5862068965517241, 2: 0.30434782608695654, 4: 0.7928994082840237, 5: 0.7032967032967034, 6: 0.47580645161290325, 8: 0.37630662020905925, 10: 0.09174311926605505, 11: 0.1794871794871795, 12: 0.33488372093023255, 13: 0.06153846153846154, 15: 0.2545454545454545, 16: 0.5573770491803278, 17: 0.0, 18: 0.09302325581395349, 19: 0.7250996015936255, 20: 0.47368421052631576, 21: 0.24427480916030533, 23: 0.6363636363636364, 24: 0.3902439024390244, 25: 0.4411764705882353, 26: 0.6601941747572816, 28: 0.2222222222222222, 29: 0.8207547169811321, 30: 0.5625, 32: 0.6666666666666666, 33: 0.17391304347826086, 35: 0.525, 36: 0.43478260869565216, 37: 0.21993127147766323, 38: 0.2206896551724138, 39: 0.1111111111111111}
Micro-average F1 score: 0.47261710556979725
Weighted-average F1 score: 0.4658563943387265

F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5755395683453237, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 20: 0.593939393939394, 21: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.8717948717948718, 32: 0.0, 33: 0.375, 35: 0.0, 36: 0.4158415841584158, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.40625
Weighted-average F1 score: 0.3256086077573191
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.5925925925925926, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.5673758865248227, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.6545454545454545, 32: 0.0, 33: 0.3157894736842105, 35: 0.0, 36: 0.5775401069518716, 37: 0.0, 38: 0.0, 39: 0.0}
Micro-average F1 score: 0.35662148070907196
Weighted-average F1 score: 0.2875467132169589
F1 score per class: {0: 0.0, 2: 0.0, 5: 0.0, 6: 0.0, 8: 0.574468085106383, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.5921052631578947, 21: 0.0, 24: 0.0, 26: 0.0, 28: 0.0, 29: 0.0, 30: 0.75, 32: 0.0, 33: 0.42105263157894735, 35: 0.0, 36: 0.5586592178770949, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3842696629213483
Weighted-average F1 score: 0.3152371981314112

F1 score per class: {0: 0.5454545454545454, 2: 0.27450980392156865, 4: 0.7176470588235294, 5: 0.706766917293233, 6: 0.26881720430107525, 8: 0.29739776951672864, 10: 0.07547169811320754, 11: 0.10638297872340426, 12: 0.2, 13: 0.04, 15: 0.15384615384615385, 16: 0.36363636363636365, 17: 0.0, 18: 0.0, 19: 0.7, 20: 0.2020618556701031, 21: 0.0821917808219178, 23: 0.6021505376344086, 24: 0.21621621621621623, 25: 0.38235294117647056, 26: 0.6146788990825688, 28: 0.1111111111111111, 29: 0.6490566037735849, 30: 0.85, 32: 0.46596858638743455, 33: 0.125, 35: 0.33548387096774196, 36: 0.33070866141732286, 37: 0.19858156028368795, 38: 0.14634146341463414, 39: 0.0}
Micro-average F1 score: 0.3664282709297434
Weighted-average F1 score: 0.3543253560477141
F1 score per class: {0: 0.4121212121212121, 2: 0.1414141414141414, 4: 0.7142857142857143, 5: 0.3384615384615385, 6: 0.27972027972027974, 8: 0.2174757281553398, 10: 0.09090909090909091, 11: 0.15028901734104047, 12: 0.18581907090464547, 13: 0.0380952380952381, 15: 0.16, 16: 0.3185840707964602, 17: 0.0, 18: 0.03636363636363636, 19: 0.5993265993265994, 20: 0.23668639053254437, 21: 0.1308411214953271, 23: 0.4915254237288136, 24: 0.13740458015267176, 25: 0.4, 26: 0.544, 28: 0.15, 29: 0.6116838487972509, 30: 0.34615384615384615, 32: 0.5070422535211268, 33: 0.0821917808219178, 35: 0.3161764705882353, 36: 0.2975206611570248, 37: 0.11643835616438356, 38: 0.136986301369863, 39: 0.0392156862745098}
Micro-average F1 score: 0.2968976514931864
Weighted-average F1 score: 0.2831634009150086
F1 score per class: {0: 0.4503311258278146, 2: 0.2028985507246377, 4: 0.7403314917127072, 5: 0.5393258426966292, 6: 0.2757009345794392, 8: 0.22453222453222454, 10: 0.08333333333333333, 11: 0.14814814814814814, 12: 0.18461538461538463, 13: 0.03571428571428571, 15: 0.15053763440860216, 16: 0.3469387755102041, 17: 0.0, 18: 0.07017543859649122, 19: 0.6546762589928058, 20: 0.23316062176165803, 21: 0.16326530612244897, 23: 0.5384615384615384, 24: 0.2222222222222222, 25: 0.4166666666666667, 26: 0.5596707818930041, 28: 0.15, 29: 0.6236559139784946, 30: 0.43373493975903615, 32: 0.4945054945054945, 33: 0.1038961038961039, 35: 0.3442622950819672, 36: 0.29239766081871343, 37: 0.12549019607843137, 38: 0.12075471698113208, 39: 0.1111111111111111}
Micro-average F1 score: 0.32149651236525045
Weighted-average F1 score: 0.30572594619220916
cur_acc_wo_na:  ['0.7607', '0.5191', '0.4879', '0.6475', '0.5209', '0.5616']
his_acc_wo_na:  ['0.7607', '0.6770', '0.5767', '0.5715', '0.4818', '0.5043']
cur_acc des_wo_na:  ['0.7391', '0.5230', '0.3900', '0.4886', '0.5219', '0.5463']
his_acc des_wo_na:  ['0.7391', '0.6343', '0.5466', '0.5080', '0.4400', '0.4433']
cur_acc rrf_wo_na:  ['0.7507', '0.5338', '0.4330', '0.5210', '0.5418', '0.5836']
his_acc rrf_wo_na:  ['0.7507', '0.6530', '0.5618', '0.5237', '0.4576', '0.4726']
cur_acc_w_na:  ['0.6354', '0.3891', '0.3518', '0.4884', '0.3715', '0.4062']
his_acc_w_na:  ['0.6354', '0.5210', '0.4159', '0.4149', '0.3349', '0.3664']
cur_acc des_w_na:  ['0.6109', '0.3667', '0.2661', '0.3546', '0.3540', '0.3566']
his_acc des_w_na:  ['0.6109', '0.4617', '0.3743', '0.3498', '0.2944', '0.2969']
cur_acc rrf_w_na:  ['0.6237', '0.3789', '0.2975', '0.3787', '0.3716', '0.3843']
his_acc rrf_w_na:  ['0.6237', '0.4828', '0.3884', '0.3644', '0.3111', '0.3215']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 103.7520339CurrentTrain: epoch  0, batch     1 | loss: 83.4708703CurrentTrain: epoch  0, batch     2 | loss: 92.9898634CurrentTrain: epoch  0, batch     3 | loss: 12.8620128CurrentTrain: epoch  1, batch     0 | loss: 72.4846251CurrentTrain: epoch  1, batch     1 | loss: 76.3690392CurrentTrain: epoch  1, batch     2 | loss: 111.4403490CurrentTrain: epoch  1, batch     3 | loss: 26.0066744CurrentTrain: epoch  2, batch     0 | loss: 72.4387312CurrentTrain: epoch  2, batch     1 | loss: 80.6833999CurrentTrain: epoch  2, batch     2 | loss: 70.9438656CurrentTrain: epoch  2, batch     3 | loss: 19.9124254CurrentTrain: epoch  3, batch     0 | loss: 81.1793298CurrentTrain: epoch  3, batch     1 | loss: 79.0949870CurrentTrain: epoch  3, batch     2 | loss: 93.7747221CurrentTrain: epoch  3, batch     3 | loss: 20.4582685CurrentTrain: epoch  4, batch     0 | loss: 76.7550607CurrentTrain: epoch  4, batch     1 | loss: 70.8725986CurrentTrain: epoch  4, batch     2 | loss: 66.8926681CurrentTrain: epoch  4, batch     3 | loss: 5.6003802CurrentTrain: epoch  5, batch     0 | loss: 66.0772645CurrentTrain: epoch  5, batch     1 | loss: 75.6601775CurrentTrain: epoch  5, batch     2 | loss: 76.7121777CurrentTrain: epoch  5, batch     3 | loss: 18.2852789CurrentTrain: epoch  6, batch     0 | loss: 68.3217027CurrentTrain: epoch  6, batch     1 | loss: 62.2957066CurrentTrain: epoch  6, batch     2 | loss: 78.7973859CurrentTrain: epoch  6, batch     3 | loss: 4.0547974CurrentTrain: epoch  7, batch     0 | loss: 75.7991739CurrentTrain: epoch  7, batch     1 | loss: 79.8297926CurrentTrain: epoch  7, batch     2 | loss: 60.8575977CurrentTrain: epoch  7, batch     3 | loss: 17.3730651CurrentTrain: epoch  8, batch     0 | loss: 59.9127992CurrentTrain: epoch  8, batch     1 | loss: 93.9262574CurrentTrain: epoch  8, batch     2 | loss: 76.0243168CurrentTrain: epoch  8, batch     3 | loss: 5.4407992CurrentTrain: epoch  9, batch     0 | loss: 61.3624999CurrentTrain: epoch  9, batch     1 | loss: 65.3626289CurrentTrain: epoch  9, batch     2 | loss: 60.8756743CurrentTrain: epoch  9, batch     3 | loss: 5.7304184
MemoryTrain:  epoch  0, batch     0 | loss: 0.7673539MemoryTrain:  epoch  1, batch     0 | loss: 0.7101525MemoryTrain:  epoch  2, batch     0 | loss: 0.5486977MemoryTrain:  epoch  3, batch     0 | loss: 0.4571332MemoryTrain:  epoch  4, batch     0 | loss: 0.3729998MemoryTrain:  epoch  5, batch     0 | loss: 0.3245618MemoryTrain:  epoch  6, batch     0 | loss: 0.2632777MemoryTrain:  epoch  7, batch     0 | loss: 0.2320486MemoryTrain:  epoch  8, batch     0 | loss: 0.2167824MemoryTrain:  epoch  9, batch     0 | loss: 0.1778007

F1 score per class: {0: 0.0, 32: 0.0, 35: 0.6, 6: 0.0, 7: 0.8620689655172413, 40: 0.0, 9: 0.0, 8: 0.0, 16: 0.5454545454545454, 19: 0.6666666666666666, 26: 0.0, 27: 0.0, 31: 0.256}
Micro-average F1 score: 0.3612040133779264
Weighted-average F1 score: 0.306778186336807
F1 score per class: {0: 0.0, 6: 0.0, 7: 0.6, 8: 0.0, 9: 0.7352941176470589, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.37037037037037035, 30: 0.0, 31: 0.4, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.5034013605442177}
Micro-average F1 score: 0.4251497005988024
Weighted-average F1 score: 0.3638011164061585
F1 score per class: {0: 0.0, 6: 0.0, 7: 0.6, 8: 0.0, 9: 0.8333333333333334, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.48484848484848486, 30: 0.0, 31: 0.5, 32: 0.0, 35: 0.0, 37: 0.0, 40: 0.5068493150684932}
Micro-average F1 score: 0.4668769716088328
Weighted-average F1 score: 0.40179581086524874

F1 score per class: {0: 0.6976744186046512, 2: 0.45161290322580644, 4: 0.7133757961783439, 5: 0.8122270742358079, 6: 0.2677165354330709, 7: 0.04477611940298507, 8: 0.36923076923076925, 9: 0.8333333333333334, 10: 0.0761904761904762, 11: 0.136986301369863, 12: 0.2647058823529412, 13: 0.06557377049180328, 15: 0.45714285714285713, 16: 0.46808510638297873, 17: 0.0, 18: 0.045454545454545456, 19: 0.6, 20: 0.3983739837398374, 21: 0.05263157894736842, 23: 0.65, 24: 0.10526315789473684, 25: 0.4, 26: 0.6631578947368421, 27: 0.32142857142857145, 28: 0.25, 29: 0.8223350253807107, 30: 0.8484848484848485, 31: 0.2857142857142857, 32: 0.7090909090909091, 33: 0.1875, 35: 0.35443037974683544, 36: 0.5272727272727272, 37: 0.3068181818181818, 38: 0.1694915254237288, 39: 0.0, 40: 0.10158730158730159}
Micro-average F1 score: 0.4360821990926074
Weighted-average F1 score: 0.4281015780512963
F1 score per class: {0: 0.5517241379310345, 2: 0.23333333333333334, 4: 0.7108433734939759, 5: 0.5052083333333334, 6: 0.33783783783783783, 7: 0.047619047619047616, 8: 0.42424242424242425, 9: 0.6756756756756757, 10: 0.09345794392523364, 11: 0.2054794520547945, 12: 0.25766871165644173, 13: 0.05, 15: 0.2926829268292683, 16: 0.5161290322580645, 17: 0.0, 18: 0.1509433962264151, 19: 0.584, 20: 0.43349753694581283, 21: 0.23529411764705882, 23: 0.6136363636363636, 24: 0.21875, 25: 0.4166666666666667, 26: 0.6602870813397129, 27: 0.21739130434782608, 28: 0.13953488372093023, 29: 0.8115942028985508, 30: 0.6521739130434783, 31: 0.09523809523809523, 32: 0.7203791469194313, 33: 0.1276595744680851, 35: 0.5081967213114754, 36: 0.5714285714285714, 37: 0.21700879765395895, 38: 0.3181818181818182, 39: 0.125, 40: 0.24183006535947713}
Micro-average F1 score: 0.4186046511627907
Weighted-average F1 score: 0.40185280046498023
F1 score per class: {0: 0.6037735849056604, 2: 0.3333333333333333, 4: 0.7530864197530864, 5: 0.6596491228070176, 6: 0.3356643356643357, 7: 0.047244094488188976, 8: 0.4245810055865922, 9: 0.746268656716418, 10: 0.07547169811320754, 11: 0.2, 12: 0.25, 13: 0.05063291139240506, 15: 0.3111111111111111, 16: 0.5185185185185185, 17: 0.0, 18: 0.07692307692307693, 19: 0.6266094420600858, 20: 0.4186046511627907, 21: 0.1839080459770115, 23: 0.5925925925925926, 24: 0.14814814814814814, 25: 0.42857142857142855, 26: 0.6731707317073171, 27: 0.2711864406779661, 28: 0.14634146341463414, 29: 0.8316831683168316, 30: 0.7368421052631579, 31: 0.2222222222222222, 32: 0.7203791469194313, 33: 0.1276595744680851, 35: 0.4423076923076923, 36: 0.5490196078431373, 37: 0.25384615384615383, 38: 0.2222222222222222, 39: 0.125, 40: 0.2215568862275449}
Micro-average F1 score: 0.4333648393194707
Weighted-average F1 score: 0.417438232377153

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.5, 8: 0.0, 9: 0.78125, 10: 0.0, 12: 0.0, 16: 0.0, 19: 0.0, 26: 0.0, 27: 0.47368421052631576, 28: 0.0, 29: 0.0, 31: 0.5, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.2119205298013245}
Micro-average F1 score: 0.2918918918918919
Weighted-average F1 score: 0.24851175483073368
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.46153846153846156, 8: 0.0, 9: 0.6493506493506493, 10: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.35714285714285715, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.2857142857142857, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.4228571428571429}
Micro-average F1 score: 0.3242009132420091
Weighted-average F1 score: 0.2741994425327759
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.46153846153846156, 8: 0.0, 9: 0.7352941176470589, 10: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 26: 0.0, 27: 0.41025641025641024, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.4, 32: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.4228571428571429}
Micro-average F1 score: 0.3583535108958838
Weighted-average F1 score: 0.3066456240329937

F1 score per class: {0: 0.5128205128205128, 2: 0.28, 4: 0.6746987951807228, 5: 0.6326530612244898, 6: 0.19101123595505617, 7: 0.02459016393442623, 8: 0.2823529411764706, 9: 0.7575757575757576, 10: 0.07339449541284404, 11: 0.11560693641618497, 12: 0.1572052401746725, 13: 0.0380952380952381, 15: 0.2807017543859649, 16: 0.3333333333333333, 17: 0.0, 18: 0.03508771929824561, 19: 0.5555555555555556, 20: 0.175, 21: 0.047619047619047616, 23: 0.5777777777777777, 24: 0.1, 25: 0.38235294117647056, 26: 0.5887850467289719, 27: 0.2222222222222222, 28: 0.14634146341463414, 29: 0.6532258064516129, 30: 0.8, 31: 0.15384615384615385, 32: 0.527027027027027, 33: 0.14285714285714285, 35: 0.2616822429906542, 36: 0.36477987421383645, 37: 0.19494584837545126, 38: 0.12195121951219512, 39: 0.0, 40: 0.07511737089201878}
Micro-average F1 score: 0.3157487922705314
Weighted-average F1 score: 0.29489013054805735
F1 score per class: {0: 0.42953020134228187, 2: 0.1414141414141414, 4: 0.6742857142857143, 5: 0.3104, 6: 0.22321428571428573, 7: 0.024793388429752067, 8: 0.2906574394463668, 9: 0.5681818181818182, 10: 0.08849557522123894, 11: 0.1744186046511628, 12: 0.15162454873646208, 13: 0.029411764705882353, 15: 0.20689655172413793, 16: 0.3076923076923077, 17: 0.0, 18: 0.09195402298850575, 19: 0.5309090909090909, 20: 0.21674876847290642, 21: 0.17721518987341772, 23: 0.47368421052631576, 24: 0.12727272727272726, 25: 0.38461538461538464, 26: 0.5609756097560976, 27: 0.18181818181818182, 28: 0.061855670103092786, 29: 0.6199261992619927, 30: 0.5882352941176471, 31: 0.05714285714285714, 32: 0.5409252669039146, 33: 0.08333333333333333, 35: 0.36046511627906974, 36: 0.41025641025641024, 37: 0.12780656303972365, 38: 0.19310344827586207, 39: 0.08, 40: 0.18592964824120603}
Micro-average F1 score: 0.28962406015037595
Weighted-average F1 score: 0.2711976849256225
F1 score per class: {0: 0.460431654676259, 2: 0.2153846153846154, 4: 0.7052023121387283, 5: 0.44549763033175355, 6: 0.22966507177033493, 7: 0.024390243902439025, 8: 0.2980392156862745, 9: 0.6329113924050633, 10: 0.07142857142857142, 11: 0.16393442622950818, 12: 0.14671814671814673, 13: 0.02962962962962963, 15: 0.208955223880597, 16: 0.35, 17: 0.0, 18: 0.04938271604938271, 19: 0.5793650793650794, 20: 0.1978021978021978, 21: 0.12598425196850394, 23: 0.48484848484848486, 24: 0.11764705882352941, 25: 0.39473684210526316, 26: 0.5774058577405857, 27: 0.19753086419753085, 28: 0.06315789473684211, 29: 0.6412213740458015, 30: 0.6829268292682927, 31: 0.1111111111111111, 32: 0.5352112676056338, 33: 0.08955223880597014, 35: 0.3129251700680272, 36: 0.3835616438356164, 37: 0.15529411764705883, 38: 0.13333333333333333, 39: 0.1111111111111111, 40: 0.16972477064220184}
Micro-average F1 score: 0.3043983402489627
Weighted-average F1 score: 0.2843728935935644
cur_acc_wo_na:  ['0.7607', '0.5191', '0.4879', '0.6475', '0.5209', '0.5616', '0.3612']
his_acc_wo_na:  ['0.7607', '0.6770', '0.5767', '0.5715', '0.4818', '0.5043', '0.4361']
cur_acc des_wo_na:  ['0.7391', '0.5230', '0.3900', '0.4886', '0.5219', '0.5463', '0.4251']
his_acc des_wo_na:  ['0.7391', '0.6343', '0.5466', '0.5080', '0.4400', '0.4433', '0.4186']
cur_acc rrf_wo_na:  ['0.7507', '0.5338', '0.4330', '0.5210', '0.5418', '0.5836', '0.4669']
his_acc rrf_wo_na:  ['0.7507', '0.6530', '0.5618', '0.5237', '0.4576', '0.4726', '0.4334']
cur_acc_w_na:  ['0.6354', '0.3891', '0.3518', '0.4884', '0.3715', '0.4062', '0.2919']
his_acc_w_na:  ['0.6354', '0.5210', '0.4159', '0.4149', '0.3349', '0.3664', '0.3157']
cur_acc des_w_na:  ['0.6109', '0.3667', '0.2661', '0.3546', '0.3540', '0.3566', '0.3242']
his_acc des_w_na:  ['0.6109', '0.4617', '0.3743', '0.3498', '0.2944', '0.2969', '0.2896']
cur_acc rrf_w_na:  ['0.6237', '0.3789', '0.2975', '0.3787', '0.3716', '0.3843', '0.3584']
his_acc rrf_w_na:  ['0.6237', '0.4828', '0.3884', '0.3644', '0.3111', '0.3215', '0.3044']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 103.1852566CurrentTrain: epoch  0, batch     1 | loss: 100.7073479CurrentTrain: epoch  0, batch     2 | loss: 86.5574487CurrentTrain: epoch  0, batch     3 | loss: 118.9680097CurrentTrain: epoch  0, batch     4 | loss: 60.9900500CurrentTrain: epoch  1, batch     0 | loss: 91.9570585CurrentTrain: epoch  1, batch     1 | loss: 87.3932849CurrentTrain: epoch  1, batch     2 | loss: 77.5733583CurrentTrain: epoch  1, batch     3 | loss: 135.3022055CurrentTrain: epoch  1, batch     4 | loss: 82.5032018CurrentTrain: epoch  2, batch     0 | loss: 86.3483646CurrentTrain: epoch  2, batch     1 | loss: 105.0392909CurrentTrain: epoch  2, batch     2 | loss: 74.3792730CurrentTrain: epoch  2, batch     3 | loss: 103.8464831CurrentTrain: epoch  2, batch     4 | loss: 50.0066848CurrentTrain: epoch  3, batch     0 | loss: 105.5684104CurrentTrain: epoch  3, batch     1 | loss: 86.0158769CurrentTrain: epoch  3, batch     2 | loss: 73.6322979CurrentTrain: epoch  3, batch     3 | loss: 68.8205311CurrentTrain: epoch  3, batch     4 | loss: 71.3604531CurrentTrain: epoch  4, batch     0 | loss: 68.7636621CurrentTrain: epoch  4, batch     1 | loss: 99.2021978CurrentTrain: epoch  4, batch     2 | loss: 100.8584659CurrentTrain: epoch  4, batch     3 | loss: 126.2556511CurrentTrain: epoch  4, batch     4 | loss: 39.8433373CurrentTrain: epoch  5, batch     0 | loss: 79.8752147CurrentTrain: epoch  5, batch     1 | loss: 101.9688155CurrentTrain: epoch  5, batch     2 | loss: 79.4252148CurrentTrain: epoch  5, batch     3 | loss: 81.0747850CurrentTrain: epoch  5, batch     4 | loss: 54.7988489CurrentTrain: epoch  6, batch     0 | loss: 82.7287415CurrentTrain: epoch  6, batch     1 | loss: 69.1627348CurrentTrain: epoch  6, batch     2 | loss: 68.7138403CurrentTrain: epoch  6, batch     3 | loss: 79.8230636CurrentTrain: epoch  6, batch     4 | loss: 95.3219039CurrentTrain: epoch  7, batch     0 | loss: 76.8682325CurrentTrain: epoch  7, batch     1 | loss: 80.7127276CurrentTrain: epoch  7, batch     2 | loss: 94.7200822CurrentTrain: epoch  7, batch     3 | loss: 93.2906718CurrentTrain: epoch  7, batch     4 | loss: 94.5354211CurrentTrain: epoch  8, batch     0 | loss: 97.5930510CurrentTrain: epoch  8, batch     1 | loss: 95.7903703CurrentTrain: epoch  8, batch     2 | loss: 120.9046888CurrentTrain: epoch  8, batch     3 | loss: 90.7234278CurrentTrain: epoch  8, batch     4 | loss: 42.8103021CurrentTrain: epoch  9, batch     0 | loss: 94.1070778CurrentTrain: epoch  9, batch     1 | loss: 78.7517285CurrentTrain: epoch  9, batch     2 | loss: 65.3749254CurrentTrain: epoch  9, batch     3 | loss: 95.0337301CurrentTrain: epoch  9, batch     4 | loss: 67.2093500
MemoryTrain:  epoch  0, batch     0 | loss: 1.0211397MemoryTrain:  epoch  1, batch     0 | loss: 0.8909100MemoryTrain:  epoch  2, batch     0 | loss: 0.7361609MemoryTrain:  epoch  3, batch     0 | loss: 0.6081082MemoryTrain:  epoch  4, batch     0 | loss: 0.4463525MemoryTrain:  epoch  5, batch     0 | loss: 0.4347524MemoryTrain:  epoch  6, batch     0 | loss: 0.3510502MemoryTrain:  epoch  7, batch     0 | loss: 0.3201654MemoryTrain:  epoch  8, batch     0 | loss: 0.2762089MemoryTrain:  epoch  9, batch     0 | loss: 0.2658429

F1 score per class: {0: 0.0, 1: 0.2122905027932961, 3: 0.7209302325581395, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 14: 0.06349206349206349, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.5966386554621849, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5225225225225225, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.3666997026759167
Weighted-average F1 score: 0.31411985973853207
F1 score per class: {0: 0.0, 1: 0.21428571428571427, 2: 0.0, 3: 0.5578231292517006, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 12: 0.0, 13: 0.0, 14: 0.07476635514018691, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.5594405594405595, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.300597779675491
Weighted-average F1 score: 0.2521139637802435
F1 score per class: {0: 0.0, 1: 0.21428571428571427, 3: 0.5962732919254659, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.08196721311475409, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.5652173913043478, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.32321428571428573
Weighted-average F1 score: 0.27615612907334725

F1 score per class: {0: 0.6451612903225806, 1: 0.16521739130434782, 2: 0.46153846153846156, 3: 0.43508771929824563, 4: 0.6577181208053692, 5: 0.8288288288288288, 6: 0.27692307692307694, 7: 0.05263157894736842, 8: 0.3898305084745763, 9: 0.8333333333333334, 10: 0.07692307692307693, 11: 0.12903225806451613, 12: 0.2553191489361702, 13: 0.06060606060606061, 14: 0.03292181069958848, 15: 0.5714285714285714, 16: 0.45454545454545453, 17: 0.0, 18: 0.0, 19: 0.5668016194331984, 20: 0.38461538461538464, 21: 0.0, 22: 0.4797297297297297, 23: 0.6511627906976745, 24: 0.0, 25: 0.3225806451612903, 26: 0.6632124352331606, 27: 0.0, 28: 0.26666666666666666, 29: 0.7920792079207921, 30: 0.8484848484848485, 31: 0.16666666666666666, 32: 0.5892116182572614, 33: 0.20689655172413793, 34: 0.1157684630738523, 35: 0.029850746268656716, 36: 0.45871559633027525, 37: 0.40707964601769914, 38: 0.11940298507462686, 39: 0.0, 40: 0.18518518518518517}
Micro-average F1 score: 0.3706609139046872
Weighted-average F1 score: 0.3514479748307593
F1 score per class: {0: 0.49206349206349204, 1: 0.16, 2: 0.28, 3: 0.3474576271186441, 4: 0.6751592356687898, 5: 0.5065274151436031, 6: 0.33986928104575165, 7: 0.05, 8: 0.3979591836734694, 9: 0.5102040816326531, 10: 0.057692307692307696, 11: 0.11864406779661017, 12: 0.225, 13: 0.02666666666666667, 14: 0.041666666666666664, 15: 0.3076923076923077, 16: 0.5283018867924528, 17: 0.0, 18: 0.1111111111111111, 19: 0.5232974910394266, 20: 0.43010752688172044, 21: 0.047619047619047616, 22: 0.4444444444444444, 23: 0.6593406593406593, 24: 0.12121212121212122, 25: 0.4383561643835616, 26: 0.6761904761904762, 27: 0.0, 28: 0.08695652173913043, 29: 0.8, 30: 0.7391304347826086, 31: 0.044444444444444446, 32: 0.5415162454873647, 33: 0.15789473684210525, 34: 0.0912863070539419, 35: 0.4430379746835443, 36: 0.5172413793103449, 37: 0.3333333333333333, 38: 0.19469026548672566, 39: 0.125, 40: 0.22570532915360503}
Micro-average F1 score: 0.3479954552832332
Weighted-average F1 score: 0.3281240128128827
F1 score per class: {0: 0.543859649122807, 1: 0.16143497757847533, 2: 0.375, 3: 0.3380281690140845, 4: 0.6883116883116883, 5: 0.6988847583643123, 6: 0.3448275862068966, 7: 0.05128205128205128, 8: 0.4155844155844156, 9: 0.704225352112676, 10: 0.0761904761904762, 11: 0.11475409836065574, 12: 0.23529411764705882, 13: 0.024691358024691357, 14: 0.043859649122807015, 15: 0.34285714285714286, 16: 0.46808510638297873, 17: 0.0, 18: 0.0, 19: 0.5758754863813229, 20: 0.4, 21: 0.0, 22: 0.4394366197183099, 23: 0.6206896551724138, 24: 0.0, 25: 0.43478260869565216, 26: 0.6796116504854369, 27: 0.0, 28: 0.1016949152542373, 29: 0.8, 30: 0.7777777777777778, 31: 0.1111111111111111, 32: 0.5670498084291188, 33: 0.16216216216216217, 34: 0.08876560332871013, 35: 0.35294117647058826, 36: 0.5070422535211268, 37: 0.3584905660377358, 38: 0.13043478260869565, 39: 0.125, 40: 0.208955223880597}
Micro-average F1 score: 0.35081685296646603
Weighted-average F1 score: 0.32495146628362204

F1 score per class: {0: 0.0, 1: 0.11987381703470032, 2: 0.0, 3: 0.5232067510548524, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.049079754601226995, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.46864686468646866, 23: 0.0, 24: 0.0, 25: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.3625, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.24552090245520902
Weighted-average F1 score: 0.21647548729385943
F1 score per class: {0: 0.0, 1: 0.12040133779264214, 2: 0.0, 3: 0.4079601990049751, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 12: 0.0, 13: 0.0, 14: 0.0625, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.4444444444444444, 23: 0.0, 24: 0.0, 25: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.33, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.19764177428411006
Weighted-average F1 score: 0.17161346340352665
F1 score per class: {0: 0.0, 1: 0.11842105263157894, 2: 0.0, 3: 0.41201716738197425, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.06756756756756757, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.43454038997214484, 23: 0.0, 24: 0.0, 25: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.3282051282051282, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 40: 0.0}
Micro-average F1 score: 0.21157218001168906
Weighted-average F1 score: 0.18809178049795536

F1 score per class: {0: 0.46875, 1: 0.08796296296296297, 2: 0.26666666666666666, 3: 0.2743362831858407, 4: 0.6282051282051282, 5: 0.6764705882352942, 6: 0.18556701030927836, 7: 0.0297029702970297, 8: 0.31724137931034485, 9: 0.78125, 10: 0.07547169811320754, 11: 0.125, 12: 0.15254237288135594, 13: 0.032, 14: 0.023809523809523808, 15: 0.375, 16: 0.37735849056603776, 17: 0.0, 18: 0.0, 19: 0.5283018867924528, 20: 0.16791044776119404, 21: 0.0, 22: 0.3397129186602871, 23: 0.5544554455445545, 24: 0.0, 25: 0.30303030303030304, 26: 0.5871559633027523, 27: 0.0, 28: 0.14814814814814814, 29: 0.64, 30: 0.8, 31: 0.08695652173913043, 32: 0.4251497005988024, 33: 0.14285714285714285, 34: 0.06954436450839328, 35: 0.02564102564102564, 36: 0.2976190476190476, 37: 0.2948717948717949, 38: 0.08, 39: 0.0, 40: 0.14084507042253522}
Micro-average F1 score: 0.25947281713344317
Weighted-average F1 score: 0.2368981269416549
F1 score per class: {0: 0.36046511627906974, 1: 0.08674698795180723, 2: 0.1686746987951807, 3: 0.20603015075376885, 4: 0.6424242424242425, 5: 0.3108974358974359, 6: 0.21666666666666667, 7: 0.026200873362445413, 8: 0.2653061224489796, 9: 0.4032258064516129, 10: 0.05555555555555555, 11: 0.11570247933884298, 12: 0.1267605633802817, 13: 0.015873015873015872, 14: 0.03389830508474576, 15: 0.21428571428571427, 16: 0.35, 17: 0.0, 18: 0.075, 19: 0.4694533762057878, 20: 0.21164021164021163, 21: 0.0425531914893617, 22: 0.311284046692607, 23: 0.47244094488188976, 24: 0.08163265306122448, 25: 0.41025641025641024, 26: 0.570281124497992, 27: 0.0, 28: 0.043795620437956206, 29: 0.6199261992619927, 30: 0.6181818181818182, 31: 0.023529411764705882, 32: 0.38961038961038963, 33: 0.09523809523809523, 34: 0.05602716468590832, 35: 0.29535864978902954, 36: 0.3237410071942446, 37: 0.23170731707317074, 38: 0.11702127659574468, 39: 0.09090909090909091, 40: 0.1643835616438356}
Micro-average F1 score: 0.23457330415754923
Weighted-average F1 score: 0.21903317367301522
F1 score per class: {0: 0.3875, 1: 0.08633093525179857, 2: 0.2222222222222222, 3: 0.20041753653444677, 4: 0.6503067484662577, 5: 0.4895833333333333, 6: 0.21929824561403508, 7: 0.027149321266968326, 8: 0.3033175355450237, 9: 0.5952380952380952, 10: 0.07339449541284404, 11: 0.112, 12: 0.1323529411764706, 13: 0.013888888888888888, 14: 0.034013605442176874, 15: 0.21818181818181817, 16: 0.34375, 17: 0.0, 18: 0.0, 19: 0.5304659498207885, 20: 0.19310344827586207, 21: 0.0, 22: 0.30174081237911027, 23: 0.5046728971962616, 24: 0.0, 25: 0.40540540540540543, 26: 0.5833333333333334, 27: 0.0, 28: 0.048, 29: 0.6212121212121212, 30: 0.717948717948718, 31: 0.05128205128205128, 32: 0.4065934065934066, 33: 0.11320754716981132, 34: 0.05391743892165122, 35: 0.25301204819277107, 36: 0.29508196721311475, 37: 0.2550335570469799, 38: 0.07692307692307693, 39: 0.1111111111111111, 40: 0.15317286652078774}
Micro-average F1 score: 0.23845704266510812
Weighted-average F1 score: 0.21801950548495183
cur_acc_wo_na:  ['0.7607', '0.5191', '0.4879', '0.6475', '0.5209', '0.5616', '0.3612', '0.3667']
his_acc_wo_na:  ['0.7607', '0.6770', '0.5767', '0.5715', '0.4818', '0.5043', '0.4361', '0.3707']
cur_acc des_wo_na:  ['0.7391', '0.5230', '0.3900', '0.4886', '0.5219', '0.5463', '0.4251', '0.3006']
his_acc des_wo_na:  ['0.7391', '0.6343', '0.5466', '0.5080', '0.4400', '0.4433', '0.4186', '0.3480']
cur_acc rrf_wo_na:  ['0.7507', '0.5338', '0.4330', '0.5210', '0.5418', '0.5836', '0.4669', '0.3232']
his_acc rrf_wo_na:  ['0.7507', '0.6530', '0.5618', '0.5237', '0.4576', '0.4726', '0.4334', '0.3508']
cur_acc_w_na:  ['0.6354', '0.3891', '0.3518', '0.4884', '0.3715', '0.4062', '0.2919', '0.2455']
his_acc_w_na:  ['0.6354', '0.5210', '0.4159', '0.4149', '0.3349', '0.3664', '0.3157', '0.2595']
cur_acc des_w_na:  ['0.6109', '0.3667', '0.2661', '0.3546', '0.3540', '0.3566', '0.3242', '0.1976']
his_acc des_w_na:  ['0.6109', '0.4617', '0.3743', '0.3498', '0.2944', '0.2969', '0.2896', '0.2346']
cur_acc rrf_w_na:  ['0.6237', '0.3789', '0.2975', '0.3787', '0.3716', '0.3843', '0.3584', '0.2116']
his_acc rrf_w_na:  ['0.6237', '0.4828', '0.3884', '0.3644', '0.3111', '0.3215', '0.3044', '0.2385']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 110.3891770CurrentTrain: epoch  0, batch     1 | loss: 200.4824042CurrentTrain: epoch  0, batch     2 | loss: 121.2992533CurrentTrain: epoch  0, batch     3 | loss: 102.0438969CurrentTrain: epoch  0, batch     4 | loss: 87.1438242CurrentTrain: epoch  0, batch     5 | loss: 89.2813015CurrentTrain: epoch  0, batch     6 | loss: 77.7941401CurrentTrain: epoch  0, batch     7 | loss: 119.6123361CurrentTrain: epoch  0, batch     8 | loss: 117.9500123CurrentTrain: epoch  0, batch     9 | loss: 119.0262606CurrentTrain: epoch  0, batch    10 | loss: 147.4794548CurrentTrain: epoch  0, batch    11 | loss: 118.7724413CurrentTrain: epoch  0, batch    12 | loss: 100.5762973CurrentTrain: epoch  0, batch    13 | loss: 147.0847761CurrentTrain: epoch  0, batch    14 | loss: 118.1986003CurrentTrain: epoch  0, batch    15 | loss: 86.8730513CurrentTrain: epoch  0, batch    16 | loss: 99.8807584CurrentTrain: epoch  0, batch    17 | loss: 86.7138612CurrentTrain: epoch  0, batch    18 | loss: 100.1218458CurrentTrain: epoch  0, batch    19 | loss: 118.1528812CurrentTrain: epoch  0, batch    20 | loss: 86.2135669CurrentTrain: epoch  0, batch    21 | loss: 145.8380795CurrentTrain: epoch  0, batch    22 | loss: 75.8535603CurrentTrain: epoch  0, batch    23 | loss: 99.3860393CurrentTrain: epoch  0, batch    24 | loss: 117.9291572CurrentTrain: epoch  0, batch    25 | loss: 98.9385726CurrentTrain: epoch  0, batch    26 | loss: 86.6218414CurrentTrain: epoch  0, batch    27 | loss: 85.7736180CurrentTrain: epoch  0, batch    28 | loss: 117.6874032CurrentTrain: epoch  0, batch    29 | loss: 98.6777602CurrentTrain: epoch  0, batch    30 | loss: 117.9775338CurrentTrain: epoch  0, batch    31 | loss: 75.8767745CurrentTrain: epoch  0, batch    32 | loss: 116.4086892CurrentTrain: epoch  0, batch    33 | loss: 117.7494210CurrentTrain: epoch  0, batch    34 | loss: 84.6478113CurrentTrain: epoch  0, batch    35 | loss: 144.9540378CurrentTrain: epoch  0, batch    36 | loss: 98.3568474CurrentTrain: epoch  0, batch    37 | loss: 98.4065607CurrentTrain: epoch  0, batch    38 | loss: 84.2370106CurrentTrain: epoch  0, batch    39 | loss: 98.0293127CurrentTrain: epoch  0, batch    40 | loss: 97.2392520CurrentTrain: epoch  0, batch    41 | loss: 97.3926635CurrentTrain: epoch  0, batch    42 | loss: 116.0971367CurrentTrain: epoch  0, batch    43 | loss: 145.1257000CurrentTrain: epoch  0, batch    44 | loss: 97.8674896CurrentTrain: epoch  0, batch    45 | loss: 116.9155627CurrentTrain: epoch  0, batch    46 | loss: 114.6775391CurrentTrain: epoch  0, batch    47 | loss: 95.5493921CurrentTrain: epoch  0, batch    48 | loss: 117.3094301CurrentTrain: epoch  0, batch    49 | loss: 96.8774527CurrentTrain: epoch  0, batch    50 | loss: 81.8449548CurrentTrain: epoch  0, batch    51 | loss: 97.6687440CurrentTrain: epoch  0, batch    52 | loss: 72.2345537CurrentTrain: epoch  0, batch    53 | loss: 84.1668416CurrentTrain: epoch  0, batch    54 | loss: 141.4854763CurrentTrain: epoch  0, batch    55 | loss: 83.5453605CurrentTrain: epoch  0, batch    56 | loss: 95.4137907CurrentTrain: epoch  0, batch    57 | loss: 115.9319410CurrentTrain: epoch  0, batch    58 | loss: 188.8595594CurrentTrain: epoch  0, batch    59 | loss: 141.6871651CurrentTrain: epoch  0, batch    60 | loss: 94.9369231CurrentTrain: epoch  0, batch    61 | loss: 92.6132674CurrentTrain: epoch  0, batch    62 | loss: 191.2423584CurrentTrain: epoch  0, batch    63 | loss: 114.1762169CurrentTrain: epoch  0, batch    64 | loss: 81.1261688CurrentTrain: epoch  0, batch    65 | loss: 81.1339419CurrentTrain: epoch  0, batch    66 | loss: 113.1260141CurrentTrain: epoch  0, batch    67 | loss: 112.7952519CurrentTrain: epoch  0, batch    68 | loss: 94.3871104CurrentTrain: epoch  0, batch    69 | loss: 140.2768102CurrentTrain: epoch  0, batch    70 | loss: 90.3420361CurrentTrain: epoch  0, batch    71 | loss: 69.6265610CurrentTrain: epoch  0, batch    72 | loss: 115.4183020CurrentTrain: epoch  0, batch    73 | loss: 92.9402758CurrentTrain: epoch  0, batch    74 | loss: 95.7657671CurrentTrain: epoch  0, batch    75 | loss: 96.4428886CurrentTrain: epoch  0, batch    76 | loss: 79.4838312CurrentTrain: epoch  0, batch    77 | loss: 77.8597908CurrentTrain: epoch  0, batch    78 | loss: 71.8118911CurrentTrain: epoch  0, batch    79 | loss: 94.5497846CurrentTrain: epoch  0, batch    80 | loss: 79.6508798CurrentTrain: epoch  0, batch    81 | loss: 79.4320910CurrentTrain: epoch  0, batch    82 | loss: 80.7597956CurrentTrain: epoch  0, batch    83 | loss: 67.6264357CurrentTrain: epoch  0, batch    84 | loss: 95.5861545CurrentTrain: epoch  0, batch    85 | loss: 109.6302156CurrentTrain: epoch  0, batch    86 | loss: 80.8415905CurrentTrain: epoch  0, batch    87 | loss: 94.8370477CurrentTrain: epoch  0, batch    88 | loss: 92.1947611CurrentTrain: epoch  0, batch    89 | loss: 137.7837158CurrentTrain: epoch  0, batch    90 | loss: 141.1540291CurrentTrain: epoch  0, batch    91 | loss: 111.2322901CurrentTrain: epoch  0, batch    92 | loss: 77.7995842CurrentTrain: epoch  0, batch    93 | loss: 80.2411881CurrentTrain: epoch  0, batch    94 | loss: 86.5957904CurrentTrain: epoch  0, batch    95 | loss: 95.1257721CurrentTrain: epoch  1, batch     0 | loss: 67.7760297CurrentTrain: epoch  1, batch     1 | loss: 80.6505844CurrentTrain: epoch  1, batch     2 | loss: 66.8090001CurrentTrain: epoch  1, batch     3 | loss: 109.4498191CurrentTrain: epoch  1, batch     4 | loss: 75.9257347CurrentTrain: epoch  1, batch     5 | loss: 88.8092971CurrentTrain: epoch  1, batch     6 | loss: 107.8154840CurrentTrain: epoch  1, batch     7 | loss: 89.0345852CurrentTrain: epoch  1, batch     8 | loss: 65.9582690CurrentTrain: epoch  1, batch     9 | loss: 78.1284655CurrentTrain: epoch  1, batch    10 | loss: 86.7636974CurrentTrain: epoch  1, batch    11 | loss: 140.8591398CurrentTrain: epoch  1, batch    12 | loss: 78.5098199CurrentTrain: epoch  1, batch    13 | loss: 110.5064388CurrentTrain: epoch  1, batch    14 | loss: 78.2033245CurrentTrain: epoch  1, batch    15 | loss: 92.5522333CurrentTrain: epoch  1, batch    16 | loss: 89.7105680CurrentTrain: epoch  1, batch    17 | loss: 90.2932908CurrentTrain: epoch  1, batch    18 | loss: 92.0452734CurrentTrain: epoch  1, batch    19 | loss: 109.1266927CurrentTrain: epoch  1, batch    20 | loss: 70.3450506CurrentTrain: epoch  1, batch    21 | loss: 74.5180434CurrentTrain: epoch  1, batch    22 | loss: 141.3013909CurrentTrain: epoch  1, batch    23 | loss: 137.0954003CurrentTrain: epoch  1, batch    24 | loss: 76.1858536CurrentTrain: epoch  1, batch    25 | loss: 79.7752064CurrentTrain: epoch  1, batch    26 | loss: 109.5725516CurrentTrain: epoch  1, batch    27 | loss: 75.7587559CurrentTrain: epoch  1, batch    28 | loss: 79.0593824CurrentTrain: epoch  1, batch    29 | loss: 135.8627768CurrentTrain: epoch  1, batch    30 | loss: 78.3239502CurrentTrain: epoch  1, batch    31 | loss: 77.1218723CurrentTrain: epoch  1, batch    32 | loss: 93.0202300CurrentTrain: epoch  1, batch    33 | loss: 73.8709592CurrentTrain: epoch  1, batch    34 | loss: 90.1552864CurrentTrain: epoch  1, batch    35 | loss: 135.7141006CurrentTrain: epoch  1, batch    36 | loss: 138.7312809CurrentTrain: epoch  1, batch    37 | loss: 192.3422326CurrentTrain: epoch  1, batch    38 | loss: 88.1782630CurrentTrain: epoch  1, batch    39 | loss: 77.6068412CurrentTrain: epoch  1, batch    40 | loss: 90.8187340CurrentTrain: epoch  1, batch    41 | loss: 79.0253807CurrentTrain: epoch  1, batch    42 | loss: 88.7071330CurrentTrain: epoch  1, batch    43 | loss: 67.9079478CurrentTrain: epoch  1, batch    44 | loss: 138.1064549CurrentTrain: epoch  1, batch    45 | loss: 109.0522826CurrentTrain: epoch  1, batch    46 | loss: 136.4719661CurrentTrain: epoch  1, batch    47 | loss: 75.9461568CurrentTrain: epoch  1, batch    48 | loss: 73.4610335CurrentTrain: epoch  1, batch    49 | loss: 107.0221752CurrentTrain: epoch  1, batch    50 | loss: 74.2075837CurrentTrain: epoch  1, batch    51 | loss: 78.9834940CurrentTrain: epoch  1, batch    52 | loss: 136.0459330CurrentTrain: epoch  1, batch    53 | loss: 90.8349913CurrentTrain: epoch  1, batch    54 | loss: 92.5466304CurrentTrain: epoch  1, batch    55 | loss: 133.3175069CurrentTrain: epoch  1, batch    56 | loss: 103.8435076CurrentTrain: epoch  1, batch    57 | loss: 66.3104774CurrentTrain: epoch  1, batch    58 | loss: 106.4580752CurrentTrain: epoch  1, batch    59 | loss: 83.2739963CurrentTrain: epoch  1, batch    60 | loss: 65.4071128CurrentTrain: epoch  1, batch    61 | loss: 67.6256014CurrentTrain: epoch  1, batch    62 | loss: 90.4917901CurrentTrain: epoch  1, batch    63 | loss: 76.2569623CurrentTrain: epoch  1, batch    64 | loss: 77.6431161CurrentTrain: epoch  1, batch    65 | loss: 92.1683151CurrentTrain: epoch  1, batch    66 | loss: 65.6251303CurrentTrain: epoch  1, batch    67 | loss: 108.2036044CurrentTrain: epoch  1, batch    68 | loss: 62.2299388CurrentTrain: epoch  1, batch    69 | loss: 109.7829306CurrentTrain: epoch  1, batch    70 | loss: 140.2091838CurrentTrain: epoch  1, batch    71 | loss: 69.5084456CurrentTrain: epoch  1, batch    72 | loss: 140.8998256CurrentTrain: epoch  1, batch    73 | loss: 138.5782492CurrentTrain: epoch  1, batch    74 | loss: 67.5427811CurrentTrain: epoch  1, batch    75 | loss: 66.5180170CurrentTrain: epoch  1, batch    76 | loss: 76.4287086CurrentTrain: epoch  1, batch    77 | loss: 78.7524178CurrentTrain: epoch  1, batch    78 | loss: 109.4764055CurrentTrain: epoch  1, batch    79 | loss: 87.3229766CurrentTrain: epoch  1, batch    80 | loss: 70.5725680CurrentTrain: epoch  1, batch    81 | loss: 135.2059470CurrentTrain: epoch  1, batch    82 | loss: 76.1626101CurrentTrain: epoch  1, batch    83 | loss: 104.4875165CurrentTrain: epoch  1, batch    84 | loss: 70.7488665CurrentTrain: epoch  1, batch    85 | loss: 89.0777984CurrentTrain: epoch  1, batch    86 | loss: 90.3136955CurrentTrain: epoch  1, batch    87 | loss: 88.1547409CurrentTrain: epoch  1, batch    88 | loss: 98.8676265CurrentTrain: epoch  1, batch    89 | loss: 86.0747318CurrentTrain: epoch  1, batch    90 | loss: 104.2237058CurrentTrain: epoch  1, batch    91 | loss: 89.6844833CurrentTrain: epoch  1, batch    92 | loss: 86.5189925CurrentTrain: epoch  1, batch    93 | loss: 105.2290842CurrentTrain: epoch  1, batch    94 | loss: 84.6975743CurrentTrain: epoch  1, batch    95 | loss: 62.1400102CurrentTrain: epoch  2, batch     0 | loss: 98.3082039CurrentTrain: epoch  2, batch     1 | loss: 75.4307518CurrentTrain: epoch  2, batch     2 | loss: 69.9175511CurrentTrain: epoch  2, batch     3 | loss: 61.4085160CurrentTrain: epoch  2, batch     4 | loss: 99.8616875CurrentTrain: epoch  2, batch     5 | loss: 107.6627221CurrentTrain: epoch  2, batch     6 | loss: 86.1030692CurrentTrain: epoch  2, batch     7 | loss: 72.5556342CurrentTrain: epoch  2, batch     8 | loss: 72.7860188CurrentTrain: epoch  2, batch     9 | loss: 103.5411013CurrentTrain: epoch  2, batch    10 | loss: 73.8491688CurrentTrain: epoch  2, batch    11 | loss: 90.4392407CurrentTrain: epoch  2, batch    12 | loss: 70.9562067CurrentTrain: epoch  2, batch    13 | loss: 73.4952879CurrentTrain: epoch  2, batch    14 | loss: 106.7874731CurrentTrain: epoch  2, batch    15 | loss: 64.3731276CurrentTrain: epoch  2, batch    16 | loss: 65.1650926CurrentTrain: epoch  2, batch    17 | loss: 177.0536018CurrentTrain: epoch  2, batch    18 | loss: 83.7558112CurrentTrain: epoch  2, batch    19 | loss: 108.8835263CurrentTrain: epoch  2, batch    20 | loss: 86.5656508CurrentTrain: epoch  2, batch    21 | loss: 99.3137074CurrentTrain: epoch  2, batch    22 | loss: 69.8834803CurrentTrain: epoch  2, batch    23 | loss: 73.8589077CurrentTrain: epoch  2, batch    24 | loss: 71.2738609CurrentTrain: epoch  2, batch    25 | loss: 75.0475251CurrentTrain: epoch  2, batch    26 | loss: 104.1574187CurrentTrain: epoch  2, batch    27 | loss: 62.9174943CurrentTrain: epoch  2, batch    28 | loss: 106.2463203CurrentTrain: epoch  2, batch    29 | loss: 71.3970239CurrentTrain: epoch  2, batch    30 | loss: 102.6699883CurrentTrain: epoch  2, batch    31 | loss: 107.8173699CurrentTrain: epoch  2, batch    32 | loss: 81.1061828CurrentTrain: epoch  2, batch    33 | loss: 75.4001535CurrentTrain: epoch  2, batch    34 | loss: 89.0250079CurrentTrain: epoch  2, batch    35 | loss: 89.2018146CurrentTrain: epoch  2, batch    36 | loss: 84.4458626CurrentTrain: epoch  2, batch    37 | loss: 133.9832349CurrentTrain: epoch  2, batch    38 | loss: 72.2588050CurrentTrain: epoch  2, batch    39 | loss: 103.5975943CurrentTrain: epoch  2, batch    40 | loss: 86.4951298CurrentTrain: epoch  2, batch    41 | loss: 75.8948195CurrentTrain: epoch  2, batch    42 | loss: 73.8343289CurrentTrain: epoch  2, batch    43 | loss: 71.6160227CurrentTrain: epoch  2, batch    44 | loss: 110.7872321CurrentTrain: epoch  2, batch    45 | loss: 84.8707027CurrentTrain: epoch  2, batch    46 | loss: 110.0042221CurrentTrain: epoch  2, batch    47 | loss: 107.9175136CurrentTrain: epoch  2, batch    48 | loss: 104.3004463CurrentTrain: epoch  2, batch    49 | loss: 106.7424966CurrentTrain: epoch  2, batch    50 | loss: 78.1513100CurrentTrain: epoch  2, batch    51 | loss: 74.3642668CurrentTrain: epoch  2, batch    52 | loss: 88.7953557CurrentTrain: epoch  2, batch    53 | loss: 83.1955476CurrentTrain: epoch  2, batch    54 | loss: 106.0911108CurrentTrain: epoch  2, batch    55 | loss: 87.0814416CurrentTrain: epoch  2, batch    56 | loss: 133.8917781CurrentTrain: epoch  2, batch    57 | loss: 104.8871759CurrentTrain: epoch  2, batch    58 | loss: 87.6510779CurrentTrain: epoch  2, batch    59 | loss: 88.5677859CurrentTrain: epoch  2, batch    60 | loss: 100.8924333CurrentTrain: epoch  2, batch    61 | loss: 122.6016597CurrentTrain: epoch  2, batch    62 | loss: 86.9450670CurrentTrain: epoch  2, batch    63 | loss: 87.9772338CurrentTrain: epoch  2, batch    64 | loss: 88.5112047CurrentTrain: epoch  2, batch    65 | loss: 88.1324551CurrentTrain: epoch  2, batch    66 | loss: 107.0870834CurrentTrain: epoch  2, batch    67 | loss: 104.0381805CurrentTrain: epoch  2, batch    68 | loss: 134.6202636CurrentTrain: epoch  2, batch    69 | loss: 108.2767415CurrentTrain: epoch  2, batch    70 | loss: 64.7671820CurrentTrain: epoch  2, batch    71 | loss: 100.3403531CurrentTrain: epoch  2, batch    72 | loss: 179.6049294CurrentTrain: epoch  2, batch    73 | loss: 88.5398827CurrentTrain: epoch  2, batch    74 | loss: 85.1080742CurrentTrain: epoch  2, batch    75 | loss: 72.5056201CurrentTrain: epoch  2, batch    76 | loss: 101.1914036CurrentTrain: epoch  2, batch    77 | loss: 82.6636069CurrentTrain: epoch  2, batch    78 | loss: 72.6219963CurrentTrain: epoch  2, batch    79 | loss: 105.9150790CurrentTrain: epoch  2, batch    80 | loss: 181.6332184CurrentTrain: epoch  2, batch    81 | loss: 86.1916256CurrentTrain: epoch  2, batch    82 | loss: 88.0292984CurrentTrain: epoch  2, batch    83 | loss: 59.5062646CurrentTrain: epoch  2, batch    84 | loss: 87.4356885CurrentTrain: epoch  2, batch    85 | loss: 61.4508364CurrentTrain: epoch  2, batch    86 | loss: 82.2421593CurrentTrain: epoch  2, batch    87 | loss: 99.9956145CurrentTrain: epoch  2, batch    88 | loss: 126.1579127CurrentTrain: epoch  2, batch    89 | loss: 135.0743183CurrentTrain: epoch  2, batch    90 | loss: 130.1573328CurrentTrain: epoch  2, batch    91 | loss: 109.2762116CurrentTrain: epoch  2, batch    92 | loss: 109.1951108CurrentTrain: epoch  2, batch    93 | loss: 109.5950561CurrentTrain: epoch  2, batch    94 | loss: 73.7647676CurrentTrain: epoch  2, batch    95 | loss: 71.8903173CurrentTrain: epoch  3, batch     0 | loss: 60.3711124CurrentTrain: epoch  3, batch     1 | loss: 63.2382380CurrentTrain: epoch  3, batch     2 | loss: 71.3207293CurrentTrain: epoch  3, batch     3 | loss: 86.1689682CurrentTrain: epoch  3, batch     4 | loss: 69.8773127CurrentTrain: epoch  3, batch     5 | loss: 84.1059026CurrentTrain: epoch  3, batch     6 | loss: 106.9233002CurrentTrain: epoch  3, batch     7 | loss: 81.3621998CurrentTrain: epoch  3, batch     8 | loss: 61.2599287CurrentTrain: epoch  3, batch     9 | loss: 109.2040654CurrentTrain: epoch  3, batch    10 | loss: 84.7506396CurrentTrain: epoch  3, batch    11 | loss: 99.2501517CurrentTrain: epoch  3, batch    12 | loss: 88.2841762CurrentTrain: epoch  3, batch    13 | loss: 107.6026067CurrentTrain: epoch  3, batch    14 | loss: 87.3497744CurrentTrain: epoch  3, batch    15 | loss: 105.3512592CurrentTrain: epoch  3, batch    16 | loss: 104.2437147CurrentTrain: epoch  3, batch    17 | loss: 73.1893536CurrentTrain: epoch  3, batch    18 | loss: 83.7706305CurrentTrain: epoch  3, batch    19 | loss: 86.8304123CurrentTrain: epoch  3, batch    20 | loss: 74.3023161CurrentTrain: epoch  3, batch    21 | loss: 69.0171660CurrentTrain: epoch  3, batch    22 | loss: 101.9685430CurrentTrain: epoch  3, batch    23 | loss: 178.0018814CurrentTrain: epoch  3, batch    24 | loss: 83.3033987CurrentTrain: epoch  3, batch    25 | loss: 105.0998201CurrentTrain: epoch  3, batch    26 | loss: 99.3145327CurrentTrain: epoch  3, batch    27 | loss: 132.0712844CurrentTrain: epoch  3, batch    28 | loss: 68.3935250CurrentTrain: epoch  3, batch    29 | loss: 101.8276677CurrentTrain: epoch  3, batch    30 | loss: 86.0812957CurrentTrain: epoch  3, batch    31 | loss: 67.8132191CurrentTrain: epoch  3, batch    32 | loss: 103.5203549CurrentTrain: epoch  3, batch    33 | loss: 85.7297235CurrentTrain: epoch  3, batch    34 | loss: 72.1726039CurrentTrain: epoch  3, batch    35 | loss: 63.9069524CurrentTrain: epoch  3, batch    36 | loss: 88.4996448CurrentTrain: epoch  3, batch    37 | loss: 67.5363362CurrentTrain: epoch  3, batch    38 | loss: 105.3254878CurrentTrain: epoch  3, batch    39 | loss: 83.6763872CurrentTrain: epoch  3, batch    40 | loss: 101.0503251CurrentTrain: epoch  3, batch    41 | loss: 82.9510750CurrentTrain: epoch  3, batch    42 | loss: 84.5757862CurrentTrain: epoch  3, batch    43 | loss: 85.5265219CurrentTrain: epoch  3, batch    44 | loss: 72.7827098CurrentTrain: epoch  3, batch    45 | loss: 84.8612850CurrentTrain: epoch  3, batch    46 | loss: 68.1903086CurrentTrain: epoch  3, batch    47 | loss: 80.7629213CurrentTrain: epoch  3, batch    48 | loss: 75.6461238CurrentTrain: epoch  3, batch    49 | loss: 105.4945830CurrentTrain: epoch  3, batch    50 | loss: 83.8121630CurrentTrain: epoch  3, batch    51 | loss: 86.2915999CurrentTrain: epoch  3, batch    52 | loss: 104.0466033CurrentTrain: epoch  3, batch    53 | loss: 60.3965166CurrentTrain: epoch  3, batch    54 | loss: 68.1032248CurrentTrain: epoch  3, batch    55 | loss: 81.4547194CurrentTrain: epoch  3, batch    56 | loss: 72.6120297CurrentTrain: epoch  3, batch    57 | loss: 105.2288257CurrentTrain: epoch  3, batch    58 | loss: 173.0353275CurrentTrain: epoch  3, batch    59 | loss: 81.8843315CurrentTrain: epoch  3, batch    60 | loss: 102.4923599CurrentTrain: epoch  3, batch    61 | loss: 101.2021639CurrentTrain: epoch  3, batch    62 | loss: 134.4289250CurrentTrain: epoch  3, batch    63 | loss: 85.3027054CurrentTrain: epoch  3, batch    64 | loss: 68.7839587CurrentTrain: epoch  3, batch    65 | loss: 74.1029092CurrentTrain: epoch  3, batch    66 | loss: 68.9511265CurrentTrain: epoch  3, batch    67 | loss: 130.8511569CurrentTrain: epoch  3, batch    68 | loss: 102.3773630CurrentTrain: epoch  3, batch    69 | loss: 106.7560985CurrentTrain: epoch  3, batch    70 | loss: 69.0444162CurrentTrain: epoch  3, batch    71 | loss: 73.6268331CurrentTrain: epoch  3, batch    72 | loss: 84.2113061CurrentTrain: epoch  3, batch    73 | loss: 101.3639073CurrentTrain: epoch  3, batch    74 | loss: 60.9831869CurrentTrain: epoch  3, batch    75 | loss: 72.6604903CurrentTrain: epoch  3, batch    76 | loss: 128.0154445CurrentTrain: epoch  3, batch    77 | loss: 70.2133380CurrentTrain: epoch  3, batch    78 | loss: 78.9342945CurrentTrain: epoch  3, batch    79 | loss: 83.7303764CurrentTrain: epoch  3, batch    80 | loss: 87.7965850CurrentTrain: epoch  3, batch    81 | loss: 74.5014656CurrentTrain: epoch  3, batch    82 | loss: 57.8023013CurrentTrain: epoch  3, batch    83 | loss: 84.5609392CurrentTrain: epoch  3, batch    84 | loss: 105.8980950CurrentTrain: epoch  3, batch    85 | loss: 79.5919396CurrentTrain: epoch  3, batch    86 | loss: 85.4057031CurrentTrain: epoch  3, batch    87 | loss: 88.3859452CurrentTrain: epoch  3, batch    88 | loss: 85.8330935CurrentTrain: epoch  3, batch    89 | loss: 131.9399346CurrentTrain: epoch  3, batch    90 | loss: 64.0668082CurrentTrain: epoch  3, batch    91 | loss: 104.8699039CurrentTrain: epoch  3, batch    92 | loss: 73.7870112CurrentTrain: epoch  3, batch    93 | loss: 66.4059447CurrentTrain: epoch  3, batch    94 | loss: 73.0470279CurrentTrain: epoch  3, batch    95 | loss: 75.1158308CurrentTrain: epoch  4, batch     0 | loss: 85.3062769CurrentTrain: epoch  4, batch     1 | loss: 86.8989505CurrentTrain: epoch  4, batch     2 | loss: 60.9630047CurrentTrain: epoch  4, batch     3 | loss: 133.9672377CurrentTrain: epoch  4, batch     4 | loss: 82.8840877CurrentTrain: epoch  4, batch     5 | loss: 83.5906893CurrentTrain: epoch  4, batch     6 | loss: 74.1163734CurrentTrain: epoch  4, batch     7 | loss: 68.0294228CurrentTrain: epoch  4, batch     8 | loss: 88.1115184CurrentTrain: epoch  4, batch     9 | loss: 65.4080047CurrentTrain: epoch  4, batch    10 | loss: 84.9157758CurrentTrain: epoch  4, batch    11 | loss: 82.7683427CurrentTrain: epoch  4, batch    12 | loss: 68.9940254CurrentTrain: epoch  4, batch    13 | loss: 124.1808499CurrentTrain: epoch  4, batch    14 | loss: 83.7246347CurrentTrain: epoch  4, batch    15 | loss: 76.1393986CurrentTrain: epoch  4, batch    16 | loss: 101.3942203CurrentTrain: epoch  4, batch    17 | loss: 84.3139343CurrentTrain: epoch  4, batch    18 | loss: 60.5979740CurrentTrain: epoch  4, batch    19 | loss: 70.6837158CurrentTrain: epoch  4, batch    20 | loss: 67.9224400CurrentTrain: epoch  4, batch    21 | loss: 57.3449074CurrentTrain: epoch  4, batch    22 | loss: 82.4325061CurrentTrain: epoch  4, batch    23 | loss: 92.1804724CurrentTrain: epoch  4, batch    24 | loss: 131.3146492CurrentTrain: epoch  4, batch    25 | loss: 70.6389876CurrentTrain: epoch  4, batch    26 | loss: 95.8532724CurrentTrain: epoch  4, batch    27 | loss: 79.3777785CurrentTrain: epoch  4, batch    28 | loss: 84.3742547CurrentTrain: epoch  4, batch    29 | loss: 127.4189373CurrentTrain: epoch  4, batch    30 | loss: 82.8220348CurrentTrain: epoch  4, batch    31 | loss: 101.1314542CurrentTrain: epoch  4, batch    32 | loss: 95.3702507CurrentTrain: epoch  4, batch    33 | loss: 100.5777669CurrentTrain: epoch  4, batch    34 | loss: 103.6959569CurrentTrain: epoch  4, batch    35 | loss: 82.1037518CurrentTrain: epoch  4, batch    36 | loss: 86.2115861CurrentTrain: epoch  4, batch    37 | loss: 82.3176161CurrentTrain: epoch  4, batch    38 | loss: 127.9263860CurrentTrain: epoch  4, batch    39 | loss: 87.7305594CurrentTrain: epoch  4, batch    40 | loss: 103.6055468CurrentTrain: epoch  4, batch    41 | loss: 84.4849621CurrentTrain: epoch  4, batch    42 | loss: 77.0117722CurrentTrain: epoch  4, batch    43 | loss: 129.8532401CurrentTrain: epoch  4, batch    44 | loss: 83.2336894CurrentTrain: epoch  4, batch    45 | loss: 100.0054273CurrentTrain: epoch  4, batch    46 | loss: 94.3673038CurrentTrain: epoch  4, batch    47 | loss: 71.2323701CurrentTrain: epoch  4, batch    48 | loss: 80.6526245CurrentTrain: epoch  4, batch    49 | loss: 82.1107655CurrentTrain: epoch  4, batch    50 | loss: 84.2780947CurrentTrain: epoch  4, batch    51 | loss: 101.8341204CurrentTrain: epoch  4, batch    52 | loss: 77.1902884CurrentTrain: epoch  4, batch    53 | loss: 70.3354601CurrentTrain: epoch  4, batch    54 | loss: 103.9096592CurrentTrain: epoch  4, batch    55 | loss: 80.8637013CurrentTrain: epoch  4, batch    56 | loss: 82.1181735CurrentTrain: epoch  4, batch    57 | loss: 87.0442362CurrentTrain: epoch  4, batch    58 | loss: 64.0218368CurrentTrain: epoch  4, batch    59 | loss: 80.0656620CurrentTrain: epoch  4, batch    60 | loss: 110.1810885CurrentTrain: epoch  4, batch    61 | loss: 71.3036687CurrentTrain: epoch  4, batch    62 | loss: 83.0116123CurrentTrain: epoch  4, batch    63 | loss: 83.2043043CurrentTrain: epoch  4, batch    64 | loss: 98.1525216CurrentTrain: epoch  4, batch    65 | loss: 70.1694677CurrentTrain: epoch  4, batch    66 | loss: 82.2472185CurrentTrain: epoch  4, batch    67 | loss: 78.9794643CurrentTrain: epoch  4, batch    68 | loss: 84.7817266CurrentTrain: epoch  4, batch    69 | loss: 88.3186440CurrentTrain: epoch  4, batch    70 | loss: 70.7525690CurrentTrain: epoch  4, batch    71 | loss: 97.9130649CurrentTrain: epoch  4, batch    72 | loss: 81.6232122CurrentTrain: epoch  4, batch    73 | loss: 71.4170134CurrentTrain: epoch  4, batch    74 | loss: 86.4484463CurrentTrain: epoch  4, batch    75 | loss: 66.9377377CurrentTrain: epoch  4, batch    76 | loss: 104.3874442CurrentTrain: epoch  4, batch    77 | loss: 71.2830649CurrentTrain: epoch  4, batch    78 | loss: 128.4745112CurrentTrain: epoch  4, batch    79 | loss: 71.1694645CurrentTrain: epoch  4, batch    80 | loss: 83.8951379CurrentTrain: epoch  4, batch    81 | loss: 77.8390809CurrentTrain: epoch  4, batch    82 | loss: 65.3530778CurrentTrain: epoch  4, batch    83 | loss: 82.2166316CurrentTrain: epoch  4, batch    84 | loss: 100.5063854CurrentTrain: epoch  4, batch    85 | loss: 67.6055521CurrentTrain: epoch  4, batch    86 | loss: 174.1751541CurrentTrain: epoch  4, batch    87 | loss: 101.2651904CurrentTrain: epoch  4, batch    88 | loss: 80.9822847CurrentTrain: epoch  4, batch    89 | loss: 70.5997577CurrentTrain: epoch  4, batch    90 | loss: 85.2910995CurrentTrain: epoch  4, batch    91 | loss: 70.0340881CurrentTrain: epoch  4, batch    92 | loss: 97.9555689CurrentTrain: epoch  4, batch    93 | loss: 80.0621885CurrentTrain: epoch  4, batch    94 | loss: 84.7538789CurrentTrain: epoch  4, batch    95 | loss: 84.5162965CurrentTrain: epoch  5, batch     0 | loss: 64.6610457CurrentTrain: epoch  5, batch     1 | loss: 79.8420899CurrentTrain: epoch  5, batch     2 | loss: 69.9009755CurrentTrain: epoch  5, batch     3 | loss: 81.9155818CurrentTrain: epoch  5, batch     4 | loss: 101.0683886CurrentTrain: epoch  5, batch     5 | loss: 166.1201162CurrentTrain: epoch  5, batch     6 | loss: 130.3295202CurrentTrain: epoch  5, batch     7 | loss: 96.9447202CurrentTrain: epoch  5, batch     8 | loss: 82.6357515CurrentTrain: epoch  5, batch     9 | loss: 170.0794939CurrentTrain: epoch  5, batch    10 | loss: 98.2238981CurrentTrain: epoch  5, batch    11 | loss: 66.1369765CurrentTrain: epoch  5, batch    12 | loss: 99.6005154CurrentTrain: epoch  5, batch    13 | loss: 80.2699587CurrentTrain: epoch  5, batch    14 | loss: 83.4423560CurrentTrain: epoch  5, batch    15 | loss: 79.8548156CurrentTrain: epoch  5, batch    16 | loss: 86.7613833CurrentTrain: epoch  5, batch    17 | loss: 83.7868442CurrentTrain: epoch  5, batch    18 | loss: 82.8667940CurrentTrain: epoch  5, batch    19 | loss: 76.6581232CurrentTrain: epoch  5, batch    20 | loss: 84.3171847CurrentTrain: epoch  5, batch    21 | loss: 101.1446540CurrentTrain: epoch  5, batch    22 | loss: 97.8513070CurrentTrain: epoch  5, batch    23 | loss: 82.0876868CurrentTrain: epoch  5, batch    24 | loss: 103.1298107CurrentTrain: epoch  5, batch    25 | loss: 58.3399267CurrentTrain: epoch  5, batch    26 | loss: 70.5400717CurrentTrain: epoch  5, batch    27 | loss: 82.8969510CurrentTrain: epoch  5, batch    28 | loss: 64.7966228CurrentTrain: epoch  5, batch    29 | loss: 65.6318223CurrentTrain: epoch  5, batch    30 | loss: 57.0604329CurrentTrain: epoch  5, batch    31 | loss: 67.3933555CurrentTrain: epoch  5, batch    32 | loss: 72.8870362CurrentTrain: epoch  5, batch    33 | loss: 81.7252934CurrentTrain: epoch  5, batch    34 | loss: 77.7262329CurrentTrain: epoch  5, batch    35 | loss: 79.5401589CurrentTrain: epoch  5, batch    36 | loss: 92.8865457CurrentTrain: epoch  5, batch    37 | loss: 99.0720186CurrentTrain: epoch  5, batch    38 | loss: 83.4957602CurrentTrain: epoch  5, batch    39 | loss: 70.1626811CurrentTrain: epoch  5, batch    40 | loss: 58.8488275CurrentTrain: epoch  5, batch    41 | loss: 121.2630487CurrentTrain: epoch  5, batch    42 | loss: 88.0062610CurrentTrain: epoch  5, batch    43 | loss: 81.9242947CurrentTrain: epoch  5, batch    44 | loss: 95.3054218CurrentTrain: epoch  5, batch    45 | loss: 99.4095864CurrentTrain: epoch  5, batch    46 | loss: 82.6187465CurrentTrain: epoch  5, batch    47 | loss: 56.4943034CurrentTrain: epoch  5, batch    48 | loss: 82.1305368CurrentTrain: epoch  5, batch    49 | loss: 97.3449762CurrentTrain: epoch  5, batch    50 | loss: 83.9633652CurrentTrain: epoch  5, batch    51 | loss: 84.0931216CurrentTrain: epoch  5, batch    52 | loss: 97.7218681CurrentTrain: epoch  5, batch    53 | loss: 125.8092421CurrentTrain: epoch  5, batch    54 | loss: 83.0389067CurrentTrain: epoch  5, batch    55 | loss: 80.4177021CurrentTrain: epoch  5, batch    56 | loss: 83.6718753CurrentTrain: epoch  5, batch    57 | loss: 69.4850232CurrentTrain: epoch  5, batch    58 | loss: 100.6229539CurrentTrain: epoch  5, batch    59 | loss: 128.6679709CurrentTrain: epoch  5, batch    60 | loss: 80.4604768CurrentTrain: epoch  5, batch    61 | loss: 90.5466708CurrentTrain: epoch  5, batch    62 | loss: 98.8805734CurrentTrain: epoch  5, batch    63 | loss: 69.9831556CurrentTrain: epoch  5, batch    64 | loss: 85.1795048CurrentTrain: epoch  5, batch    65 | loss: 77.3974188CurrentTrain: epoch  5, batch    66 | loss: 69.6043747CurrentTrain: epoch  5, batch    67 | loss: 82.3056019CurrentTrain: epoch  5, batch    68 | loss: 71.7531760CurrentTrain: epoch  5, batch    69 | loss: 77.0594135CurrentTrain: epoch  5, batch    70 | loss: 99.7704673CurrentTrain: epoch  5, batch    71 | loss: 83.4737583CurrentTrain: epoch  5, batch    72 | loss: 70.6783901CurrentTrain: epoch  5, batch    73 | loss: 100.4094468CurrentTrain: epoch  5, batch    74 | loss: 77.2156719CurrentTrain: epoch  5, batch    75 | loss: 124.5055863CurrentTrain: epoch  5, batch    76 | loss: 61.8342084CurrentTrain: epoch  5, batch    77 | loss: 123.5764967CurrentTrain: epoch  5, batch    78 | loss: 98.9806860CurrentTrain: epoch  5, batch    79 | loss: 78.5796173CurrentTrain: epoch  5, batch    80 | loss: 80.0381638CurrentTrain: epoch  5, batch    81 | loss: 84.9453462CurrentTrain: epoch  5, batch    82 | loss: 69.5868314CurrentTrain: epoch  5, batch    83 | loss: 98.4076014CurrentTrain: epoch  5, batch    84 | loss: 81.3944868CurrentTrain: epoch  5, batch    85 | loss: 67.5757064CurrentTrain: epoch  5, batch    86 | loss: 83.4349979CurrentTrain: epoch  5, batch    87 | loss: 98.3392848CurrentTrain: epoch  5, batch    88 | loss: 130.1296760CurrentTrain: epoch  5, batch    89 | loss: 65.3391892CurrentTrain: epoch  5, batch    90 | loss: 79.1808706CurrentTrain: epoch  5, batch    91 | loss: 68.3507829CurrentTrain: epoch  5, batch    92 | loss: 102.0752813CurrentTrain: epoch  5, batch    93 | loss: 56.4549351CurrentTrain: epoch  5, batch    94 | loss: 70.5986463CurrentTrain: epoch  5, batch    95 | loss: 84.1230983CurrentTrain: epoch  6, batch     0 | loss: 78.3342016CurrentTrain: epoch  6, batch     1 | loss: 81.0923730CurrentTrain: epoch  6, batch     2 | loss: 97.8129421CurrentTrain: epoch  6, batch     3 | loss: 68.5510428CurrentTrain: epoch  6, batch     4 | loss: 56.8192693CurrentTrain: epoch  6, batch     5 | loss: 76.9109352CurrentTrain: epoch  6, batch     6 | loss: 82.6632916CurrentTrain: epoch  6, batch     7 | loss: 80.4439471CurrentTrain: epoch  6, batch     8 | loss: 82.8059467CurrentTrain: epoch  6, batch     9 | loss: 98.3831815CurrentTrain: epoch  6, batch    10 | loss: 68.4823046CurrentTrain: epoch  6, batch    11 | loss: 59.9089083CurrentTrain: epoch  6, batch    12 | loss: 120.1191839CurrentTrain: epoch  6, batch    13 | loss: 80.4712074CurrentTrain: epoch  6, batch    14 | loss: 96.9726674CurrentTrain: epoch  6, batch    15 | loss: 69.7496047CurrentTrain: epoch  6, batch    16 | loss: 55.8170303CurrentTrain: epoch  6, batch    17 | loss: 79.3307444CurrentTrain: epoch  6, batch    18 | loss: 66.6547065CurrentTrain: epoch  6, batch    19 | loss: 98.5074577CurrentTrain: epoch  6, batch    20 | loss: 62.8049981CurrentTrain: epoch  6, batch    21 | loss: 78.6812435CurrentTrain: epoch  6, batch    22 | loss: 68.0326447CurrentTrain: epoch  6, batch    23 | loss: 56.7524789CurrentTrain: epoch  6, batch    24 | loss: 83.3352415CurrentTrain: epoch  6, batch    25 | loss: 59.6678387CurrentTrain: epoch  6, batch    26 | loss: 57.0771109CurrentTrain: epoch  6, batch    27 | loss: 81.0440688CurrentTrain: epoch  6, batch    28 | loss: 64.8949751CurrentTrain: epoch  6, batch    29 | loss: 94.1101808CurrentTrain: epoch  6, batch    30 | loss: 80.4749459CurrentTrain: epoch  6, batch    31 | loss: 66.7549571CurrentTrain: epoch  6, batch    32 | loss: 129.1024323CurrentTrain: epoch  6, batch    33 | loss: 78.2013316CurrentTrain: epoch  6, batch    34 | loss: 100.5124762CurrentTrain: epoch  6, batch    35 | loss: 57.2179825CurrentTrain: epoch  6, batch    36 | loss: 82.4629445CurrentTrain: epoch  6, batch    37 | loss: 61.1391790CurrentTrain: epoch  6, batch    38 | loss: 79.7479997CurrentTrain: epoch  6, batch    39 | loss: 97.7937761CurrentTrain: epoch  6, batch    40 | loss: 83.6830292CurrentTrain: epoch  6, batch    41 | loss: 79.5258185CurrentTrain: epoch  6, batch    42 | loss: 105.0964489CurrentTrain: epoch  6, batch    43 | loss: 78.5953736CurrentTrain: epoch  6, batch    44 | loss: 100.0423582CurrentTrain: epoch  6, batch    45 | loss: 123.6772000CurrentTrain: epoch  6, batch    46 | loss: 82.1070276CurrentTrain: epoch  6, batch    47 | loss: 173.2027372CurrentTrain: epoch  6, batch    48 | loss: 175.0776604CurrentTrain: epoch  6, batch    49 | loss: 175.3349634CurrentTrain: epoch  6, batch    50 | loss: 57.7866188CurrentTrain: epoch  6, batch    51 | loss: 83.2412417CurrentTrain: epoch  6, batch    52 | loss: 95.3780718CurrentTrain: epoch  6, batch    53 | loss: 99.6787878CurrentTrain: epoch  6, batch    54 | loss: 67.3266179CurrentTrain: epoch  6, batch    55 | loss: 60.1483290CurrentTrain: epoch  6, batch    56 | loss: 78.5586432CurrentTrain: epoch  6, batch    57 | loss: 82.9616544CurrentTrain: epoch  6, batch    58 | loss: 63.2879253CurrentTrain: epoch  6, batch    59 | loss: 99.7289108CurrentTrain: epoch  6, batch    60 | loss: 70.7538157CurrentTrain: epoch  6, batch    61 | loss: 80.5635938CurrentTrain: epoch  6, batch    62 | loss: 101.2239264CurrentTrain: epoch  6, batch    63 | loss: 57.4187766CurrentTrain: epoch  6, batch    64 | loss: 82.2954907CurrentTrain: epoch  6, batch    65 | loss: 57.3495079CurrentTrain: epoch  6, batch    66 | loss: 70.0286227CurrentTrain: epoch  6, batch    67 | loss: 68.1797191CurrentTrain: epoch  6, batch    68 | loss: 66.7119118CurrentTrain: epoch  6, batch    69 | loss: 64.2461274CurrentTrain: epoch  6, batch    70 | loss: 79.0847830CurrentTrain: epoch  6, batch    71 | loss: 82.5213956CurrentTrain: epoch  6, batch    72 | loss: 101.4489788CurrentTrain: epoch  6, batch    73 | loss: 55.4535887CurrentTrain: epoch  6, batch    74 | loss: 129.1461051CurrentTrain: epoch  6, batch    75 | loss: 66.9726210CurrentTrain: epoch  6, batch    76 | loss: 85.4750200CurrentTrain: epoch  6, batch    77 | loss: 97.3148720CurrentTrain: epoch  6, batch    78 | loss: 83.9058322CurrentTrain: epoch  6, batch    79 | loss: 78.6909113CurrentTrain: epoch  6, batch    80 | loss: 66.3644512CurrentTrain: epoch  6, batch    81 | loss: 101.7964455CurrentTrain: epoch  6, batch    82 | loss: 75.7629771CurrentTrain: epoch  6, batch    83 | loss: 177.9140702CurrentTrain: epoch  6, batch    84 | loss: 81.7366656CurrentTrain: epoch  6, batch    85 | loss: 81.5887555CurrentTrain: epoch  6, batch    86 | loss: 124.0571142CurrentTrain: epoch  6, batch    87 | loss: 80.3918163CurrentTrain: epoch  6, batch    88 | loss: 55.0557046CurrentTrain: epoch  6, batch    89 | loss: 68.8330086CurrentTrain: epoch  6, batch    90 | loss: 75.7580257CurrentTrain: epoch  6, batch    91 | loss: 97.2719861CurrentTrain: epoch  6, batch    92 | loss: 80.2311670CurrentTrain: epoch  6, batch    93 | loss: 99.2865975CurrentTrain: epoch  6, batch    94 | loss: 53.1808862CurrentTrain: epoch  6, batch    95 | loss: 82.0412869CurrentTrain: epoch  7, batch     0 | loss: 66.3918118CurrentTrain: epoch  7, batch     1 | loss: 56.3731127CurrentTrain: epoch  7, batch     2 | loss: 100.2599039CurrentTrain: epoch  7, batch     3 | loss: 94.9816754CurrentTrain: epoch  7, batch     4 | loss: 78.1706684CurrentTrain: epoch  7, batch     5 | loss: 77.4294205CurrentTrain: epoch  7, batch     6 | loss: 72.7718438CurrentTrain: epoch  7, batch     7 | loss: 95.9666362CurrentTrain: epoch  7, batch     8 | loss: 120.4806388CurrentTrain: epoch  7, batch     9 | loss: 80.3094718CurrentTrain: epoch  7, batch    10 | loss: 76.3808326CurrentTrain: epoch  7, batch    11 | loss: 98.2202422CurrentTrain: epoch  7, batch    12 | loss: 56.2853126CurrentTrain: epoch  7, batch    13 | loss: 79.0366578CurrentTrain: epoch  7, batch    14 | loss: 65.5988097CurrentTrain: epoch  7, batch    15 | loss: 80.5899142CurrentTrain: epoch  7, batch    16 | loss: 77.5775330CurrentTrain: epoch  7, batch    17 | loss: 99.0280623CurrentTrain: epoch  7, batch    18 | loss: 75.8145739CurrentTrain: epoch  7, batch    19 | loss: 94.4637316CurrentTrain: epoch  7, batch    20 | loss: 99.7836053CurrentTrain: epoch  7, batch    21 | loss: 126.7223567CurrentTrain: epoch  7, batch    22 | loss: 97.4388356CurrentTrain: epoch  7, batch    23 | loss: 68.0281304CurrentTrain: epoch  7, batch    24 | loss: 59.5693889CurrentTrain: epoch  7, batch    25 | loss: 85.3558206CurrentTrain: epoch  7, batch    26 | loss: 97.2923869CurrentTrain: epoch  7, batch    27 | loss: 98.8319496CurrentTrain: epoch  7, batch    28 | loss: 67.2136910CurrentTrain: epoch  7, batch    29 | loss: 97.9361874CurrentTrain: epoch  7, batch    30 | loss: 67.0866344CurrentTrain: epoch  7, batch    31 | loss: 57.4547407CurrentTrain: epoch  7, batch    32 | loss: 127.6036458CurrentTrain: epoch  7, batch    33 | loss: 65.5571075CurrentTrain: epoch  7, batch    34 | loss: 62.8490288CurrentTrain: epoch  7, batch    35 | loss: 123.8468330CurrentTrain: epoch  7, batch    36 | loss: 99.5228371CurrentTrain: epoch  7, batch    37 | loss: 60.4008290CurrentTrain: epoch  7, batch    38 | loss: 55.8513981CurrentTrain: epoch  7, batch    39 | loss: 94.3290044CurrentTrain: epoch  7, batch    40 | loss: 80.6453713CurrentTrain: epoch  7, batch    41 | loss: 66.1890639CurrentTrain: epoch  7, batch    42 | loss: 94.0868523CurrentTrain: epoch  7, batch    43 | loss: 100.5877763CurrentTrain: epoch  7, batch    44 | loss: 101.6615220CurrentTrain: epoch  7, batch    45 | loss: 64.7973997CurrentTrain: epoch  7, batch    46 | loss: 66.4336952CurrentTrain: epoch  7, batch    47 | loss: 97.5581324CurrentTrain: epoch  7, batch    48 | loss: 64.3180615CurrentTrain: epoch  7, batch    49 | loss: 69.2110766CurrentTrain: epoch  7, batch    50 | loss: 100.7904761CurrentTrain: epoch  7, batch    51 | loss: 100.2557521CurrentTrain: epoch  7, batch    52 | loss: 64.8228655CurrentTrain: epoch  7, batch    53 | loss: 124.2210109CurrentTrain: epoch  7, batch    54 | loss: 59.6894260CurrentTrain: epoch  7, batch    55 | loss: 56.3678584CurrentTrain: epoch  7, batch    56 | loss: 59.6893174CurrentTrain: epoch  7, batch    57 | loss: 78.6750556CurrentTrain: epoch  7, batch    58 | loss: 66.3887999CurrentTrain: epoch  7, batch    59 | loss: 80.8294215CurrentTrain: epoch  7, batch    60 | loss: 79.3511275CurrentTrain: epoch  7, batch    61 | loss: 77.8519084CurrentTrain: epoch  7, batch    62 | loss: 59.8858706CurrentTrain: epoch  7, batch    63 | loss: 79.2495135CurrentTrain: epoch  7, batch    64 | loss: 99.4115683CurrentTrain: epoch  7, batch    65 | loss: 98.6386659CurrentTrain: epoch  7, batch    66 | loss: 75.3062448CurrentTrain: epoch  7, batch    67 | loss: 93.4796885CurrentTrain: epoch  7, batch    68 | loss: 98.3855288CurrentTrain: epoch  7, batch    69 | loss: 97.5366304CurrentTrain: epoch  7, batch    70 | loss: 66.9478752CurrentTrain: epoch  7, batch    71 | loss: 98.7181432CurrentTrain: epoch  7, batch    72 | loss: 78.6350513CurrentTrain: epoch  7, batch    73 | loss: 78.6386296CurrentTrain: epoch  7, batch    74 | loss: 92.0771748CurrentTrain: epoch  7, batch    75 | loss: 67.8698020CurrentTrain: epoch  7, batch    76 | loss: 77.8151750CurrentTrain: epoch  7, batch    77 | loss: 97.8476650CurrentTrain: epoch  7, batch    78 | loss: 96.9038292CurrentTrain: epoch  7, batch    79 | loss: 84.0399279CurrentTrain: epoch  7, batch    80 | loss: 64.7857988CurrentTrain: epoch  7, batch    81 | loss: 82.2986060CurrentTrain: epoch  7, batch    82 | loss: 99.7306935CurrentTrain: epoch  7, batch    83 | loss: 66.0543598CurrentTrain: epoch  7, batch    84 | loss: 64.7061091CurrentTrain: epoch  7, batch    85 | loss: 99.1824461CurrentTrain: epoch  7, batch    86 | loss: 79.3430127CurrentTrain: epoch  7, batch    87 | loss: 96.6593249CurrentTrain: epoch  7, batch    88 | loss: 100.6732670CurrentTrain: epoch  7, batch    89 | loss: 93.0742127CurrentTrain: epoch  7, batch    90 | loss: 78.9586749CurrentTrain: epoch  7, batch    91 | loss: 99.2956044CurrentTrain: epoch  7, batch    92 | loss: 77.4167549CurrentTrain: epoch  7, batch    93 | loss: 80.5084367CurrentTrain: epoch  7, batch    94 | loss: 94.7720807CurrentTrain: epoch  7, batch    95 | loss: 61.9574125CurrentTrain: epoch  8, batch     0 | loss: 79.8011473CurrentTrain: epoch  8, batch     1 | loss: 66.9040135CurrentTrain: epoch  8, batch     2 | loss: 68.3009325CurrentTrain: epoch  8, batch     3 | loss: 127.2233971CurrentTrain: epoch  8, batch     4 | loss: 77.8682226CurrentTrain: epoch  8, batch     5 | loss: 75.2925077CurrentTrain: epoch  8, batch     6 | loss: 61.9226912CurrentTrain: epoch  8, batch     7 | loss: 56.6336313CurrentTrain: epoch  8, batch     8 | loss: 58.1961731CurrentTrain: epoch  8, batch     9 | loss: 67.9563418CurrentTrain: epoch  8, batch    10 | loss: 97.9998259CurrentTrain: epoch  8, batch    11 | loss: 66.1798843CurrentTrain: epoch  8, batch    12 | loss: 81.3702865CurrentTrain: epoch  8, batch    13 | loss: 80.4255578CurrentTrain: epoch  8, batch    14 | loss: 80.5580236CurrentTrain: epoch  8, batch    15 | loss: 119.2573222CurrentTrain: epoch  8, batch    16 | loss: 122.6455957CurrentTrain: epoch  8, batch    17 | loss: 60.1274347CurrentTrain: epoch  8, batch    18 | loss: 65.0129468CurrentTrain: epoch  8, batch    19 | loss: 55.7941310CurrentTrain: epoch  8, batch    20 | loss: 65.3063853CurrentTrain: epoch  8, batch    21 | loss: 94.5453697CurrentTrain: epoch  8, batch    22 | loss: 77.4218533CurrentTrain: epoch  8, batch    23 | loss: 94.9206732CurrentTrain: epoch  8, batch    24 | loss: 125.0398864CurrentTrain: epoch  8, batch    25 | loss: 62.2547636CurrentTrain: epoch  8, batch    26 | loss: 75.9678300CurrentTrain: epoch  8, batch    27 | loss: 52.3336819CurrentTrain: epoch  8, batch    28 | loss: 78.1165715CurrentTrain: epoch  8, batch    29 | loss: 65.9761354CurrentTrain: epoch  8, batch    30 | loss: 124.9066607CurrentTrain: epoch  8, batch    31 | loss: 94.1162508CurrentTrain: epoch  8, batch    32 | loss: 78.3175154CurrentTrain: epoch  8, batch    33 | loss: 78.5616071CurrentTrain: epoch  8, batch    34 | loss: 77.1193939CurrentTrain: epoch  8, batch    35 | loss: 62.5892769CurrentTrain: epoch  8, batch    36 | loss: 75.7997973CurrentTrain: epoch  8, batch    37 | loss: 69.4848016CurrentTrain: epoch  8, batch    38 | loss: 72.7357673CurrentTrain: epoch  8, batch    39 | loss: 102.8342521CurrentTrain: epoch  8, batch    40 | loss: 68.3560407CurrentTrain: epoch  8, batch    41 | loss: 118.9820853CurrentTrain: epoch  8, batch    42 | loss: 81.5845151CurrentTrain: epoch  8, batch    43 | loss: 56.8084841CurrentTrain: epoch  8, batch    44 | loss: 55.9897171CurrentTrain: epoch  8, batch    45 | loss: 81.4304674CurrentTrain: epoch  8, batch    46 | loss: 124.3597665CurrentTrain: epoch  8, batch    47 | loss: 100.3613783CurrentTrain: epoch  8, batch    48 | loss: 54.8933097CurrentTrain: epoch  8, batch    49 | loss: 65.7862500CurrentTrain: epoch  8, batch    50 | loss: 60.1287017CurrentTrain: epoch  8, batch    51 | loss: 68.1499786CurrentTrain: epoch  8, batch    52 | loss: 77.6828624CurrentTrain: epoch  8, batch    53 | loss: 64.8465268CurrentTrain: epoch  8, batch    54 | loss: 92.4665651CurrentTrain: epoch  8, batch    55 | loss: 264.2149448CurrentTrain: epoch  8, batch    56 | loss: 77.6276652CurrentTrain: epoch  8, batch    57 | loss: 117.8494021CurrentTrain: epoch  8, batch    58 | loss: 76.0198245CurrentTrain: epoch  8, batch    59 | loss: 65.1105886CurrentTrain: epoch  8, batch    60 | loss: 82.1134952CurrentTrain: epoch  8, batch    61 | loss: 65.9928138CurrentTrain: epoch  8, batch    62 | loss: 78.7410316CurrentTrain: epoch  8, batch    63 | loss: 98.7428774CurrentTrain: epoch  8, batch    64 | loss: 77.2882935CurrentTrain: epoch  8, batch    65 | loss: 123.4829608CurrentTrain: epoch  8, batch    66 | loss: 81.1846869CurrentTrain: epoch  8, batch    67 | loss: 75.9289941CurrentTrain: epoch  8, batch    68 | loss: 126.5149506CurrentTrain: epoch  8, batch    69 | loss: 59.2242013CurrentTrain: epoch  8, batch    70 | loss: 127.6405185CurrentTrain: epoch  8, batch    71 | loss: 75.3085826CurrentTrain: epoch  8, batch    72 | loss: 63.8179116CurrentTrain: epoch  8, batch    73 | loss: 92.3728852CurrentTrain: epoch  8, batch    74 | loss: 78.4874386CurrentTrain: epoch  8, batch    75 | loss: 97.7907935CurrentTrain: epoch  8, batch    76 | loss: 65.1206191CurrentTrain: epoch  8, batch    77 | loss: 97.8369945CurrentTrain: epoch  8, batch    78 | loss: 168.0241139CurrentTrain: epoch  8, batch    79 | loss: 76.2285920CurrentTrain: epoch  8, batch    80 | loss: 124.2865115CurrentTrain: epoch  8, batch    81 | loss: 73.5353358CurrentTrain: epoch  8, batch    82 | loss: 67.7554179CurrentTrain: epoch  8, batch    83 | loss: 73.4606326CurrentTrain: epoch  8, batch    84 | loss: 96.3748266CurrentTrain: epoch  8, batch    85 | loss: 77.8281825CurrentTrain: epoch  8, batch    86 | loss: 124.1803475CurrentTrain: epoch  8, batch    87 | loss: 77.5320064CurrentTrain: epoch  8, batch    88 | loss: 70.0953673CurrentTrain: epoch  8, batch    89 | loss: 80.0331509CurrentTrain: epoch  8, batch    90 | loss: 94.8790581CurrentTrain: epoch  8, batch    91 | loss: 80.0361971CurrentTrain: epoch  8, batch    92 | loss: 78.0252736CurrentTrain: epoch  8, batch    93 | loss: 97.3802017CurrentTrain: epoch  8, batch    94 | loss: 77.9798456CurrentTrain: epoch  8, batch    95 | loss: 52.0836841CurrentTrain: epoch  9, batch     0 | loss: 78.3348037CurrentTrain: epoch  9, batch     1 | loss: 81.8847408CurrentTrain: epoch  9, batch     2 | loss: 97.1091848CurrentTrain: epoch  9, batch     3 | loss: 123.7663504CurrentTrain: epoch  9, batch     4 | loss: 66.6556061CurrentTrain: epoch  9, batch     5 | loss: 63.9225381CurrentTrain: epoch  9, batch     6 | loss: 66.4000447CurrentTrain: epoch  9, batch     7 | loss: 57.3135761CurrentTrain: epoch  9, batch     8 | loss: 63.3348703CurrentTrain: epoch  9, batch     9 | loss: 96.5490521CurrentTrain: epoch  9, batch    10 | loss: 92.1008609CurrentTrain: epoch  9, batch    11 | loss: 55.5551161CurrentTrain: epoch  9, batch    12 | loss: 98.3184014CurrentTrain: epoch  9, batch    13 | loss: 63.3689878CurrentTrain: epoch  9, batch    14 | loss: 76.4831376CurrentTrain: epoch  9, batch    15 | loss: 167.2585407CurrentTrain: epoch  9, batch    16 | loss: 68.5673122CurrentTrain: epoch  9, batch    17 | loss: 75.7190886CurrentTrain: epoch  9, batch    18 | loss: 63.3881275CurrentTrain: epoch  9, batch    19 | loss: 163.9762609CurrentTrain: epoch  9, batch    20 | loss: 66.3462732CurrentTrain: epoch  9, batch    21 | loss: 94.3179476CurrentTrain: epoch  9, batch    22 | loss: 58.0898028CurrentTrain: epoch  9, batch    23 | loss: 89.8454346CurrentTrain: epoch  9, batch    24 | loss: 77.1497974CurrentTrain: epoch  9, batch    25 | loss: 81.1213551CurrentTrain: epoch  9, batch    26 | loss: 67.8884051CurrentTrain: epoch  9, batch    27 | loss: 125.6617610CurrentTrain: epoch  9, batch    28 | loss: 75.3475440CurrentTrain: epoch  9, batch    29 | loss: 94.1847857CurrentTrain: epoch  9, batch    30 | loss: 77.6218908CurrentTrain: epoch  9, batch    31 | loss: 61.9031759CurrentTrain: epoch  9, batch    32 | loss: 101.0762470CurrentTrain: epoch  9, batch    33 | loss: 98.0847720CurrentTrain: epoch  9, batch    34 | loss: 72.2912790CurrentTrain: epoch  9, batch    35 | loss: 125.1742113CurrentTrain: epoch  9, batch    36 | loss: 62.0125653CurrentTrain: epoch  9, batch    37 | loss: 78.1974652CurrentTrain: epoch  9, batch    38 | loss: 69.6694349CurrentTrain: epoch  9, batch    39 | loss: 78.0020153CurrentTrain: epoch  9, batch    40 | loss: 90.9510369CurrentTrain: epoch  9, batch    41 | loss: 60.2049401CurrentTrain: epoch  9, batch    42 | loss: 92.0769987CurrentTrain: epoch  9, batch    43 | loss: 92.7849971CurrentTrain: epoch  9, batch    44 | loss: 121.1093834CurrentTrain: epoch  9, batch    45 | loss: 75.0495641CurrentTrain: epoch  9, batch    46 | loss: 81.6599890CurrentTrain: epoch  9, batch    47 | loss: 98.0256272CurrentTrain: epoch  9, batch    48 | loss: 125.0675891CurrentTrain: epoch  9, batch    49 | loss: 76.3347024CurrentTrain: epoch  9, batch    50 | loss: 78.0651993CurrentTrain: epoch  9, batch    51 | loss: 57.1532696CurrentTrain: epoch  9, batch    52 | loss: 64.8548818CurrentTrain: epoch  9, batch    53 | loss: 69.7358369CurrentTrain: epoch  9, batch    54 | loss: 99.0279242CurrentTrain: epoch  9, batch    55 | loss: 56.9706712CurrentTrain: epoch  9, batch    56 | loss: 97.5518898CurrentTrain: epoch  9, batch    57 | loss: 61.8127373CurrentTrain: epoch  9, batch    58 | loss: 69.5603718CurrentTrain: epoch  9, batch    59 | loss: 89.9899240CurrentTrain: epoch  9, batch    60 | loss: 62.6205771CurrentTrain: epoch  9, batch    61 | loss: 76.1839936CurrentTrain: epoch  9, batch    62 | loss: 75.5267314CurrentTrain: epoch  9, batch    63 | loss: 75.2190240CurrentTrain: epoch  9, batch    64 | loss: 64.2686510CurrentTrain: epoch  9, batch    65 | loss: 54.0026447CurrentTrain: epoch  9, batch    66 | loss: 93.1256813CurrentTrain: epoch  9, batch    67 | loss: 75.6732484CurrentTrain: epoch  9, batch    68 | loss: 77.7024193CurrentTrain: epoch  9, batch    69 | loss: 77.6239165CurrentTrain: epoch  9, batch    70 | loss: 66.1902657CurrentTrain: epoch  9, batch    71 | loss: 125.3122380CurrentTrain: epoch  9, batch    72 | loss: 121.8590596CurrentTrain: epoch  9, batch    73 | loss: 96.7274943CurrentTrain: epoch  9, batch    74 | loss: 76.6132442CurrentTrain: epoch  9, batch    75 | loss: 91.0829408CurrentTrain: epoch  9, batch    76 | loss: 77.5172161CurrentTrain: epoch  9, batch    77 | loss: 57.4310896CurrentTrain: epoch  9, batch    78 | loss: 74.7636909CurrentTrain: epoch  9, batch    79 | loss: 69.4786594CurrentTrain: epoch  9, batch    80 | loss: 77.6704719CurrentTrain: epoch  9, batch    81 | loss: 65.3229458CurrentTrain: epoch  9, batch    82 | loss: 67.2617366CurrentTrain: epoch  9, batch    83 | loss: 75.4022298CurrentTrain: epoch  9, batch    84 | loss: 96.6044482CurrentTrain: epoch  9, batch    85 | loss: 101.1602536CurrentTrain: epoch  9, batch    86 | loss: 66.2047579CurrentTrain: epoch  9, batch    87 | loss: 63.6135567CurrentTrain: epoch  9, batch    88 | loss: 93.0207471CurrentTrain: epoch  9, batch    89 | loss: 78.5938013CurrentTrain: epoch  9, batch    90 | loss: 77.3174442CurrentTrain: epoch  9, batch    91 | loss: 63.6741327CurrentTrain: epoch  9, batch    92 | loss: 75.7720555CurrentTrain: epoch  9, batch    93 | loss: 78.0619052CurrentTrain: epoch  9, batch    94 | loss: 81.1689665CurrentTrain: epoch  9, batch    95 | loss: 81.6283215

F1 score per class: {32: 0.6108374384236454, 6: 0.7813953488372093, 19: 0.41025641025641024, 24: 0.7486631016042781, 26: 0.9238578680203046, 29: 0.8198198198198198}
Micro-average F1 score: 0.7638758231420508
Weighted-average F1 score: 0.7635927099905668
F1 score per class: {32: 0.6161137440758294, 6: 0.7947598253275109, 19: 0.2857142857142857, 24: 0.7419354838709677, 26: 0.9458128078817734, 29: 0.8241206030150754}
Micro-average F1 score: 0.7613741875580315
Weighted-average F1 score: 0.7559622476565782
F1 score per class: {32: 0.6132075471698113, 6: 0.8, 19: 0.2916666666666667, 24: 0.7419354838709677, 26: 0.945273631840796, 29: 0.83}
Micro-average F1 score: 0.7630597014925373
Weighted-average F1 score: 0.7575962702379828

F1 score per class: {32: 0.6108374384236454, 6: 0.7813953488372093, 19: 0.41025641025641024, 24: 0.7486631016042781, 26: 0.9238578680203046, 29: 0.8198198198198198}
Micro-average F1 score: 0.7638758231420508
Weighted-average F1 score: 0.7635927099905668
F1 score per class: {32: 0.6161137440758294, 6: 0.7947598253275109, 19: 0.2857142857142857, 24: 0.7419354838709677, 26: 0.9458128078817734, 29: 0.8241206030150754}
Micro-average F1 score: 0.7613741875580315
Weighted-average F1 score: 0.7559622476565782
F1 score per class: {32: 0.6132075471698113, 6: 0.8, 19: 0.2916666666666667, 24: 0.7419354838709677, 26: 0.945273631840796, 29: 0.83}
Micro-average F1 score: 0.7630597014925373
Weighted-average F1 score: 0.7575962702379828

F1 score per class: {32: 0.4305555555555556, 6: 0.7272727272727273, 19: 0.2318840579710145, 24: 0.6698564593301436, 26: 0.8544600938967136, 29: 0.6066666666666667}
Micro-average F1 score: 0.6198473282442748
Weighted-average F1 score: 0.6047284554095217
F1 score per class: {32: 0.42071197411003236, 6: 0.7250996015936255, 19: 0.1728395061728395, 24: 0.6798029556650246, 26: 0.8571428571428571, 29: 0.6639676113360324}
Micro-average F1 score: 0.623574144486692
Weighted-average F1 score: 0.6050393838194877
F1 score per class: {32: 0.41935483870967744, 6: 0.7346938775510204, 19: 0.1794871794871795, 24: 0.6731707317073171, 26: 0.867579908675799, 29: 0.6666666666666666}
Micro-average F1 score: 0.6263399693721287
Weighted-average F1 score: 0.6073891223834805

F1 score per class: {32: 0.4305555555555556, 6: 0.7272727272727273, 19: 0.2318840579710145, 24: 0.6698564593301436, 26: 0.8544600938967136, 29: 0.6066666666666667}
Micro-average F1 score: 0.6198473282442748
Weighted-average F1 score: 0.6047284554095217
F1 score per class: {32: 0.42071197411003236, 6: 0.7250996015936255, 19: 0.1728395061728395, 24: 0.6798029556650246, 26: 0.8571428571428571, 29: 0.6639676113360324}
Micro-average F1 score: 0.623574144486692
Weighted-average F1 score: 0.6050393838194877
F1 score per class: {32: 0.41935483870967744, 6: 0.7346938775510204, 19: 0.1794871794871795, 24: 0.6731707317073171, 26: 0.867579908675799, 29: 0.6666666666666666}
Micro-average F1 score: 0.6263399693721287
Weighted-average F1 score: 0.6073891223834805
cur_acc_wo_na:  ['0.7639']
his_acc_wo_na:  ['0.7639']
cur_acc des_wo_na:  ['0.7614']
his_acc des_wo_na:  ['0.7614']
cur_acc rrf_wo_na:  ['0.7631']
his_acc rrf_wo_na:  ['0.7631']
cur_acc_w_na:  ['0.6198']
his_acc_w_na:  ['0.6198']
cur_acc des_w_na:  ['0.6236']
his_acc des_w_na:  ['0.6236']
cur_acc rrf_w_na:  ['0.6263']
his_acc rrf_w_na:  ['0.6263']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 82.3452399CurrentTrain: epoch  0, batch     1 | loss: 146.6718991CurrentTrain: epoch  0, batch     2 | loss: 78.0283619CurrentTrain: epoch  0, batch     3 | loss: 14.0564505CurrentTrain: epoch  1, batch     0 | loss: 87.7291069CurrentTrain: epoch  1, batch     1 | loss: 83.4359909CurrentTrain: epoch  1, batch     2 | loss: 85.6856533CurrentTrain: epoch  1, batch     3 | loss: 14.8933725CurrentTrain: epoch  2, batch     0 | loss: 71.9777464CurrentTrain: epoch  2, batch     1 | loss: 81.1554490CurrentTrain: epoch  2, batch     2 | loss: 70.9444290CurrentTrain: epoch  2, batch     3 | loss: 10.3614078CurrentTrain: epoch  3, batch     0 | loss: 80.5544914CurrentTrain: epoch  3, batch     1 | loss: 85.6882728CurrentTrain: epoch  3, batch     2 | loss: 65.5771799CurrentTrain: epoch  3, batch     3 | loss: 22.0231225CurrentTrain: epoch  4, batch     0 | loss: 80.3719817CurrentTrain: epoch  4, batch     1 | loss: 79.8884467CurrentTrain: epoch  4, batch     2 | loss: 76.7900785CurrentTrain: epoch  4, batch     3 | loss: 17.8813197CurrentTrain: epoch  5, batch     0 | loss: 78.3179943CurrentTrain: epoch  5, batch     1 | loss: 65.4379273CurrentTrain: epoch  5, batch     2 | loss: 65.8749563CurrentTrain: epoch  5, batch     3 | loss: 10.0039658CurrentTrain: epoch  6, batch     0 | loss: 66.2135940CurrentTrain: epoch  6, batch     1 | loss: 63.1531599CurrentTrain: epoch  6, batch     2 | loss: 65.3147121CurrentTrain: epoch  6, batch     3 | loss: 16.8699771CurrentTrain: epoch  7, batch     0 | loss: 64.9495283CurrentTrain: epoch  7, batch     1 | loss: 74.2337116CurrentTrain: epoch  7, batch     2 | loss: 91.7228797CurrentTrain: epoch  7, batch     3 | loss: 18.9300134CurrentTrain: epoch  8, batch     0 | loss: 60.1925391CurrentTrain: epoch  8, batch     1 | loss: 119.6167481CurrentTrain: epoch  8, batch     2 | loss: 63.4266796CurrentTrain: epoch  8, batch     3 | loss: 8.9050609CurrentTrain: epoch  9, batch     0 | loss: 62.6953164CurrentTrain: epoch  9, batch     1 | loss: 62.2137656CurrentTrain: epoch  9, batch     2 | loss: 76.7971836CurrentTrain: epoch  9, batch     3 | loss: 5.3315195
MemoryTrain:  epoch  0, batch     0 | loss: 2.2693174MemoryTrain:  epoch  1, batch     0 | loss: 2.2962070MemoryTrain:  epoch  2, batch     0 | loss: 1.6813204MemoryTrain:  epoch  3, batch     0 | loss: 1.4652972MemoryTrain:  epoch  4, batch     0 | loss: 1.2480565MemoryTrain:  epoch  5, batch     0 | loss: 1.0434285MemoryTrain:  epoch  6, batch     0 | loss: 0.8828078MemoryTrain:  epoch  7, batch     0 | loss: 0.7885316MemoryTrain:  epoch  8, batch     0 | loss: 0.6394886MemoryTrain:  epoch  9, batch     0 | loss: 0.5875903

F1 score per class: {32: 0.0, 6: 0.6666666666666666, 7: 0.8, 40: 0.0, 9: 0.0, 19: 0.35714285714285715, 26: 0.0, 27: 0.5, 29: 0.0, 31: 0.21138211382113822}
Micro-average F1 score: 0.3205574912891986
Weighted-average F1 score: 0.2765885883466597
F1 score per class: {32: 0.0, 6: 0.6, 7: 0.7246376811594203, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.4166666666666667, 26: 0.0, 27: 0.18181818181818182, 29: 0.0, 31: 0.34285714285714286}
Micro-average F1 score: 0.36619718309859156
Weighted-average F1 score: 0.3121460679133313
F1 score per class: {32: 0.0, 6: 0.6, 7: 0.7692307692307693, 40: 0.0, 9: 0.0, 19: 0.46153846153846156, 26: 0.0, 27: 0.18181818181818182, 29: 0.0, 31: 0.32075471698113206}
Micro-average F1 score: 0.37545126353790614
Weighted-average F1 score: 0.32408001767881384

F1 score per class: {32: 0.38461538461538464, 6: 0.06382978723404255, 7: 0.8, 40: 0.6456692913385826, 9: 0.18181818181818182, 19: 0.7403314917127072, 24: 0.2702702702702703, 26: 0.9292929292929293, 27: 0.18181818181818182, 29: 0.835820895522388, 31: 0.06753246753246753}
Micro-average F1 score: 0.5040650406504065
Weighted-average F1 score: 0.43388393678753434
F1 score per class: {32: 0.35714285714285715, 6: 0.05555555555555555, 7: 0.6944444444444444, 40: 0.6332046332046332, 9: 0.23529411764705882, 19: 0.7368421052631579, 24: 0.3225806451612903, 26: 0.9253731343283582, 27: 0.08333333333333333, 29: 0.8102564102564103, 31: 0.20689655172413793}
Micro-average F1 score: 0.5672268907563025
Weighted-average F1 score: 0.5310556255759712
F1 score per class: {32: 0.3546099290780142, 6: 0.056074766355140186, 7: 0.7692307692307693, 40: 0.6381322957198443, 9: 0.23076923076923078, 19: 0.7486631016042781, 24: 0.36363636363636365, 26: 0.9353233830845771, 27: 0.08, 29: 0.8020304568527918, 31: 0.192090395480226}
Micro-average F1 score: 0.5720338983050848
Weighted-average F1 score: 0.5353832190822697

F1 score per class: {32: 0.0, 6: 0.5, 7: 0.75, 40: 0.0, 9: 0.0, 19: 0.3333333333333333, 26: 0.0, 27: 0.3333333333333333, 29: 0.0, 31: 0.19402985074626866}
Micro-average F1 score: 0.2902208201892745
Weighted-average F1 score: 0.2552018233953386
F1 score per class: {32: 0.0, 6: 0.42857142857142855, 7: 0.6756756756756757, 40: 0.0, 9: 0.0, 19: 0.0, 24: 0.38461538461538464, 26: 0.0, 27: 0.13333333333333333, 29: 0.0, 31: 0.3302752293577982}
Micro-average F1 score: 0.3291139240506329
Weighted-average F1 score: 0.2800207575573186
F1 score per class: {32: 0.0, 6: 0.46153846153846156, 7: 0.7246376811594203, 40: 0.0, 9: 0.0, 19: 0.42857142857142855, 26: 0.0, 27: 0.125, 29: 0.0, 31: 0.3063063063063063}
Micro-average F1 score: 0.34098360655737703
Weighted-average F1 score: 0.2942485370780386

F1 score per class: {32: 0.26548672566371684, 6: 0.0410958904109589, 7: 0.75, 40: 0.6074074074074074, 9: 0.16, 19: 0.6836734693877551, 24: 0.2222222222222222, 26: 0.8440366972477065, 27: 0.06896551724137931, 29: 0.6774193548387096, 31: 0.053830227743271224}
Micro-average F1 score: 0.41333333333333333
Weighted-average F1 score: 0.35578518944119997
F1 score per class: {32: 0.26881720430107525, 6: 0.0335195530726257, 7: 0.6410256410256411, 40: 0.5899280575539568, 9: 0.14035087719298245, 19: 0.6572769953051644, 24: 0.2631578947368421, 26: 0.8378378378378378, 27: 0.05, 29: 0.6810344827586207, 31: 0.18}
Micro-average F1 score: 0.4701102727800348
Weighted-average F1 score: 0.43207049438244916
F1 score per class: {32: 0.25906735751295334, 6: 0.03529411764705882, 7: 0.7246376811594203, 40: 0.5963636363636363, 9: 0.17142857142857143, 19: 0.6763285024154589, 24: 0.3, 26: 0.8468468468468469, 27: 0.04, 29: 0.6723404255319149, 31: 0.1650485436893204}
Micro-average F1 score: 0.47591069330199764
Weighted-average F1 score: 0.43700865194201
cur_acc_wo_na:  ['0.7639', '0.3206']
his_acc_wo_na:  ['0.7639', '0.5041']
cur_acc des_wo_na:  ['0.7614', '0.3662']
his_acc des_wo_na:  ['0.7614', '0.5672']
cur_acc rrf_wo_na:  ['0.7631', '0.3755']
his_acc rrf_wo_na:  ['0.7631', '0.5720']
cur_acc_w_na:  ['0.6198', '0.2902']
his_acc_w_na:  ['0.6198', '0.4133']
cur_acc des_w_na:  ['0.6236', '0.3291']
his_acc des_w_na:  ['0.6236', '0.4701']
cur_acc rrf_w_na:  ['0.6263', '0.3410']
his_acc rrf_w_na:  ['0.6263', '0.4759']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 103.7877731CurrentTrain: epoch  0, batch     1 | loss: 104.8870957CurrentTrain: epoch  0, batch     2 | loss: 89.0600183CurrentTrain: epoch  0, batch     3 | loss: 96.2668735CurrentTrain: epoch  1, batch     0 | loss: 84.6299288CurrentTrain: epoch  1, batch     1 | loss: 107.7414504CurrentTrain: epoch  1, batch     2 | loss: 77.9819346CurrentTrain: epoch  1, batch     3 | loss: 144.9211991CurrentTrain: epoch  2, batch     0 | loss: 94.0539165CurrentTrain: epoch  2, batch     1 | loss: 129.4409189CurrentTrain: epoch  2, batch     2 | loss: 89.0707049CurrentTrain: epoch  2, batch     3 | loss: 68.9644872CurrentTrain: epoch  3, batch     0 | loss: 74.0939927CurrentTrain: epoch  3, batch     1 | loss: 133.5967135CurrentTrain: epoch  3, batch     2 | loss: 72.8691723CurrentTrain: epoch  3, batch     3 | loss: 59.1103574CurrentTrain: epoch  4, batch     0 | loss: 83.7712298CurrentTrain: epoch  4, batch     1 | loss: 86.6446055CurrentTrain: epoch  4, batch     2 | loss: 71.0663700CurrentTrain: epoch  4, batch     3 | loss: 86.8426184CurrentTrain: epoch  5, batch     0 | loss: 83.3121483CurrentTrain: epoch  5, batch     1 | loss: 80.1561819CurrentTrain: epoch  5, batch     2 | loss: 84.5793563CurrentTrain: epoch  5, batch     3 | loss: 79.2291873CurrentTrain: epoch  6, batch     0 | loss: 82.8464191CurrentTrain: epoch  6, batch     1 | loss: 96.6424955CurrentTrain: epoch  6, batch     2 | loss: 96.0864511CurrentTrain: epoch  6, batch     3 | loss: 66.1626103CurrentTrain: epoch  7, batch     0 | loss: 69.5607031CurrentTrain: epoch  7, batch     1 | loss: 99.2063321CurrentTrain: epoch  7, batch     2 | loss: 82.2905936CurrentTrain: epoch  7, batch     3 | loss: 61.9839598CurrentTrain: epoch  8, batch     0 | loss: 64.9645613CurrentTrain: epoch  8, batch     1 | loss: 95.8161352CurrentTrain: epoch  8, batch     2 | loss: 100.9775968CurrentTrain: epoch  8, batch     3 | loss: 76.7845050CurrentTrain: epoch  9, batch     0 | loss: 77.1722545CurrentTrain: epoch  9, batch     1 | loss: 122.1406131CurrentTrain: epoch  9, batch     2 | loss: 96.0375954CurrentTrain: epoch  9, batch     3 | loss: 52.8430896
MemoryTrain:  epoch  0, batch     0 | loss: 1.8382345MemoryTrain:  epoch  1, batch     0 | loss: 1.4604825MemoryTrain:  epoch  2, batch     0 | loss: 1.2839787MemoryTrain:  epoch  3, batch     0 | loss: 0.9634521MemoryTrain:  epoch  4, batch     0 | loss: 0.8112990MemoryTrain:  epoch  5, batch     0 | loss: 0.7210658MemoryTrain:  epoch  6, batch     0 | loss: 0.5701075MemoryTrain:  epoch  7, batch     0 | loss: 0.4451061MemoryTrain:  epoch  8, batch     0 | loss: 0.4023412MemoryTrain:  epoch  9, batch     0 | loss: 0.3796558

F1 score per class: {0: 0.8831168831168831, 32: 0.8950276243093923, 4: 0.0, 6: 0.0, 7: 0.25, 40: 0.0, 13: 0.5, 19: 0.5526315789473685, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5977859778597786
Weighted-average F1 score: 0.4940938778981671
F1 score per class: {0: 0.7674418604651163, 32: 0.8811881188118812, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.13333333333333333, 9: 0.0, 13: 0.4166666666666667, 19: 0.5813953488372093, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5531914893617021
Weighted-average F1 score: 0.4618335666315101
F1 score per class: {0: 0.8048780487804879, 32: 0.9319371727748691, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.11764705882352941, 9: 0.0, 13: 0.46808510638297873, 19: 0.5681818181818182, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.5796610169491525
Weighted-average F1 score: 0.4789793942462151

F1 score per class: {32: 0.6666666666666666, 0: 0.8950276243093923, 4: 0.4049079754601227, 6: 0.06818181818181818, 7: 0.8, 40: 0.1111111111111111, 9: 0.6554621848739496, 13: 0.28378378378378377, 19: 0.5121951219512195, 21: 0.09523809523809523, 23: 0.700507614213198, 24: 0.32432432432432434, 26: 0.9117647058823529, 27: 0.11764705882352941, 29: 0.8110599078341014, 31: 0.199203187250996}
Micro-average F1 score: 0.5562977099236641
Weighted-average F1 score: 0.5057731403692408
F1 score per class: {32: 0.5454545454545454, 0: 0.8768472906403941, 4: 0.4083769633507853, 6: 0.06593406593406594, 7: 0.6756756756756757, 40: 0.06779661016949153, 9: 0.589041095890411, 13: 0.2, 19: 0.5263157894736842, 21: 0.2727272727272727, 23: 0.6796116504854369, 24: 0.2222222222222222, 26: 0.9090909090909091, 27: 0.08333333333333333, 29: 0.7685185185185185, 31: 0.11976047904191617}
Micro-average F1 score: 0.524546660769571
Weighted-average F1 score: 0.4807580673723854
F1 score per class: {32: 0.532258064516129, 0: 0.9319371727748691, 4: 0.430939226519337, 6: 0.06976744186046512, 7: 0.746268656716418, 40: 0.05970149253731343, 9: 0.6058394160583942, 13: 0.21890547263681592, 19: 0.5154639175257731, 21: 0.12903225806451613, 23: 0.693069306930693, 24: 0.3111111111111111, 26: 0.9090909090909091, 27: 0.06666666666666667, 29: 0.7636363636363637, 31: 0.11827956989247312}
Micro-average F1 score: 0.5345997286295794
Weighted-average F1 score: 0.48804167727310593

F1 score per class: {0: 0.8395061728395061, 32: 0.8526315789473684, 4: 0.0, 6: 0.0, 7: 0.11363636363636363, 40: 0.0, 13: 0.375, 19: 0.5121951219512195, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.4544179523141655
Weighted-average F1 score: 0.34768511391544077
F1 score per class: {0: 0.7333333333333333, 32: 0.827906976744186, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.06896551724137931, 9: 0.0, 13: 0.31007751937984496, 19: 0.4854368932038835, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.4188351920693928
Weighted-average F1 score: 0.33271414706078956
F1 score per class: {0: 0.75, 32: 0.8811881188118812, 4: 0.0, 6: 0.0, 7: 0.0, 40: 0.06451612903225806, 9: 0.0, 13: 0.34108527131782945, 19: 0.4716981132075472, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0}
Micro-average F1 score: 0.44129032258064516
Weighted-average F1 score: 0.3478651400757534

F1 score per class: {32: 0.5, 0: 0.8481675392670157, 4: 0.2773109243697479, 6: 0.043795620437956206, 7: 0.75, 40: 0.04830917874396135, 9: 0.624, 13: 0.1917808219178082, 19: 0.47191011235955055, 21: 0.07692307692307693, 23: 0.6272727272727273, 24: 0.2553191489361702, 26: 0.7982832618025751, 27: 0.07407407407407407, 29: 0.6027397260273972, 31: 0.17006802721088435}
Micro-average F1 score: 0.43670411985018726
Weighted-average F1 score: 0.3879100874865342
F1 score per class: {32: 0.44, 0: 0.8202764976958525, 4: 0.2736842105263158, 6: 0.03896103896103896, 7: 0.5681818181818182, 40: 0.031007751937984496, 9: 0.5425867507886435, 13: 0.13029315960912052, 19: 0.43478260869565216, 21: 0.15789473684210525, 23: 0.6060606060606061, 24: 0.1724137931034483, 26: 0.7786885245901639, 27: 0.04878048780487805, 29: 0.5570469798657718, 31: 0.09950248756218906}
Micro-average F1 score: 0.40176151761517614
Weighted-average F1 score: 0.3620676126476774
F1 score per class: {32: 0.41509433962264153, 0: 0.8768472906403941, 4: 0.2857142857142857, 6: 0.04195804195804196, 7: 0.684931506849315, 40: 0.029411764705882353, 9: 0.5665529010238908, 13: 0.138801261829653, 19: 0.42735042735042733, 21: 0.08888888888888889, 23: 0.6222222222222222, 24: 0.2413793103448276, 26: 0.7883817427385892, 27: 0.038461538461538464, 29: 0.5694915254237288, 31: 0.09649122807017543}
Micro-average F1 score: 0.41357592722183345
Weighted-average F1 score: 0.3697860805702305
cur_acc_wo_na:  ['0.7639', '0.3206', '0.5978']
his_acc_wo_na:  ['0.7639', '0.5041', '0.5563']
cur_acc des_wo_na:  ['0.7614', '0.3662', '0.5532']
his_acc des_wo_na:  ['0.7614', '0.5672', '0.5245']
cur_acc rrf_wo_na:  ['0.7631', '0.3755', '0.5797']
his_acc rrf_wo_na:  ['0.7631', '0.5720', '0.5346']
cur_acc_w_na:  ['0.6198', '0.2902', '0.4544']
his_acc_w_na:  ['0.6198', '0.4133', '0.4367']
cur_acc des_w_na:  ['0.6236', '0.3291', '0.4188']
his_acc des_w_na:  ['0.6236', '0.4701', '0.4018']
cur_acc rrf_w_na:  ['0.6263', '0.3410', '0.4413']
his_acc rrf_w_na:  ['0.6263', '0.4759', '0.4136']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 112.5498933CurrentTrain: epoch  0, batch     1 | loss: 115.4252003CurrentTrain: epoch  0, batch     2 | loss: 143.7471037CurrentTrain: epoch  0, batch     3 | loss: 139.1674632CurrentTrain: epoch  0, batch     4 | loss: 51.4605898CurrentTrain: epoch  1, batch     0 | loss: 113.3132762CurrentTrain: epoch  1, batch     1 | loss: 134.8610319CurrentTrain: epoch  1, batch     2 | loss: 90.4119167CurrentTrain: epoch  1, batch     3 | loss: 86.8219355CurrentTrain: epoch  1, batch     4 | loss: 81.4081023CurrentTrain: epoch  2, batch     0 | loss: 132.7175793CurrentTrain: epoch  2, batch     1 | loss: 83.2972150CurrentTrain: epoch  2, batch     2 | loss: 86.9877880CurrentTrain: epoch  2, batch     3 | loss: 87.6983793CurrentTrain: epoch  2, batch     4 | loss: 64.6848774CurrentTrain: epoch  3, batch     0 | loss: 84.5501858CurrentTrain: epoch  3, batch     1 | loss: 82.3741285CurrentTrain: epoch  3, batch     2 | loss: 133.0487482CurrentTrain: epoch  3, batch     3 | loss: 80.9137828CurrentTrain: epoch  3, batch     4 | loss: 114.9804728CurrentTrain: epoch  4, batch     0 | loss: 129.0334762CurrentTrain: epoch  4, batch     1 | loss: 79.6025914CurrentTrain: epoch  4, batch     2 | loss: 99.6799373CurrentTrain: epoch  4, batch     3 | loss: 128.1092038CurrentTrain: epoch  4, batch     4 | loss: 61.7417975CurrentTrain: epoch  5, batch     0 | loss: 105.1895359CurrentTrain: epoch  5, batch     1 | loss: 96.6845794CurrentTrain: epoch  5, batch     2 | loss: 96.7545592CurrentTrain: epoch  5, batch     3 | loss: 100.1306700CurrentTrain: epoch  5, batch     4 | loss: 105.3004599CurrentTrain: epoch  6, batch     0 | loss: 79.2719500CurrentTrain: epoch  6, batch     1 | loss: 100.8201881CurrentTrain: epoch  6, batch     2 | loss: 125.7260111CurrentTrain: epoch  6, batch     3 | loss: 78.6799721CurrentTrain: epoch  6, batch     4 | loss: 61.4348113CurrentTrain: epoch  7, batch     0 | loss: 82.5210031CurrentTrain: epoch  7, batch     1 | loss: 66.2274704CurrentTrain: epoch  7, batch     2 | loss: 81.4860369CurrentTrain: epoch  7, batch     3 | loss: 81.7420704CurrentTrain: epoch  7, batch     4 | loss: 59.2740564CurrentTrain: epoch  8, batch     0 | loss: 123.3461027CurrentTrain: epoch  8, batch     1 | loss: 90.9404176CurrentTrain: epoch  8, batch     2 | loss: 80.0953359CurrentTrain: epoch  8, batch     3 | loss: 65.7993834CurrentTrain: epoch  8, batch     4 | loss: 109.3149043CurrentTrain: epoch  9, batch     0 | loss: 95.2823663CurrentTrain: epoch  9, batch     1 | loss: 168.3501410CurrentTrain: epoch  9, batch     2 | loss: 73.3798136CurrentTrain: epoch  9, batch     3 | loss: 95.4095407CurrentTrain: epoch  9, batch     4 | loss: 48.1125909
MemoryTrain:  epoch  0, batch     0 | loss: 1.0194239MemoryTrain:  epoch  1, batch     0 | loss: 0.9525044MemoryTrain:  epoch  2, batch     0 | loss: 0.8108599MemoryTrain:  epoch  3, batch     0 | loss: 0.6635398MemoryTrain:  epoch  4, batch     0 | loss: 0.4780919MemoryTrain:  epoch  5, batch     0 | loss: 0.4432413MemoryTrain:  epoch  6, batch     0 | loss: 0.3599453MemoryTrain:  epoch  7, batch     0 | loss: 0.3043999MemoryTrain:  epoch  8, batch     0 | loss: 0.2925927MemoryTrain:  epoch  9, batch     0 | loss: 0.2610637

F1 score per class: {32: 0.8545454545454545, 5: 0.0, 6: 0.0, 7: 0.19642857142857142, 40: 0.0, 10: 0.7, 13: 0.2, 16: 0.2692307692307692, 17: 0.0, 18: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5037593984962406
Weighted-average F1 score: 0.5083635114885114
F1 score per class: {0: 0.0, 5: 0.6209150326797386, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.5594405594405595, 13: 0.0, 16: 0.6571428571428571, 17: 0.46153846153846156, 18: 0.5272727272727272, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.5163043478260869
Weighted-average F1 score: 0.4751896043711901
F1 score per class: {0: 0.0, 5: 0.6643356643356644, 6: 0.0, 7: 0.0, 10: 0.4626865671641791, 13: 0.0, 16: 0.6865671641791045, 17: 0.3333333333333333, 18: 0.4878048780487805, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.5089285714285714
Weighted-average F1 score: 0.47386863999201423

F1 score per class: {0: 0.75, 4: 0.9010989010989011, 5: 0.8138528138528138, 6: 0.3870967741935484, 7: 0.075, 9: 0.7692307692307693, 10: 0.18333333333333332, 13: 0.07272727272727272, 16: 0.6, 17: 0.13333333333333333, 18: 0.25925925925925924, 19: 0.6446280991735537, 21: 0.29411764705882354, 23: 0.6190476190476191, 24: 0.08333333333333333, 26: 0.6974358974358974, 27: 0.35714285714285715, 29: 0.8855721393034826, 31: 0.13333333333333333, 32: 0.7685185185185185, 40: 0.19166666666666668}
Micro-average F1 score: 0.561014263074485
Weighted-average F1 score: 0.5393333675185152
F1 score per class: {0: 0.591304347826087, 4: 0.8955223880597015, 5: 0.547550432276657, 6: 0.4111111111111111, 7: 0.07407407407407407, 9: 0.6097560975609756, 10: 0.47904191616766467, 13: 0.12903225806451613, 16: 0.5287356321839081, 17: 0.1276595744680851, 18: 0.4233576642335766, 19: 0.5754385964912281, 21: 0.1308411214953271, 23: 0.5684210526315789, 24: 0.2, 26: 0.6764705882352942, 27: 0.3125, 29: 0.8666666666666667, 31: 0.05, 32: 0.7354260089686099, 40: 0.14130434782608695}
Micro-average F1 score: 0.5081859855926654
Weighted-average F1 score: 0.47440886787688347
F1 score per class: {0: 0.6285714285714286, 4: 0.9361702127659575, 5: 0.6089743589743589, 6: 0.4222222222222222, 7: 0.075, 9: 0.6944444444444444, 10: 0.40789473684210525, 13: 0.10810810810810811, 16: 0.575, 17: 0.09523809523809523, 18: 0.4444444444444444, 19: 0.6029411764705882, 21: 0.1643835616438356, 23: 0.5934065934065934, 24: 0.11428571428571428, 26: 0.68, 27: 0.31746031746031744, 29: 0.875, 31: 0.06666666666666667, 32: 0.7321428571428571, 40: 0.125}
Micro-average F1 score: 0.5221606648199446
Weighted-average F1 score: 0.48686486175274757

F1 score per class: {0: 0.0, 4: 0.0, 5: 0.6937269372693727, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.1896551724137931, 13: 0.0, 16: 0.44680851063829785, 17: 0.07692307692307693, 18: 0.2028985507246377, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.3701657458563536
Weighted-average F1 score: 0.35280799027548043
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.43577981651376146, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.45714285714285713, 13: 0.0, 16: 0.3709677419354839, 17: 0.23076923076923078, 18: 0.3240223463687151, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.3301476976542137
Weighted-average F1 score: 0.30344337198138066
F1 score per class: {0: 0.0, 4: 0.0, 5: 0.47738693467336685, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.39490445859872614, 13: 0.0, 16: 0.3865546218487395, 17: 0.14814814814814814, 18: 0.27586206896551724, 19: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.32571428571428573
Weighted-average F1 score: 0.2992687236851047

F1 score per class: {0: 0.6111111111111112, 4: 0.8497409326424871, 5: 0.6308724832214765, 6: 0.2777777777777778, 7: 0.043795620437956206, 9: 0.704225352112676, 10: 0.15942028985507245, 13: 0.044444444444444446, 16: 0.3620689655172414, 17: 0.045454545454545456, 18: 0.18421052631578946, 19: 0.6023166023166023, 21: 0.1932367149758454, 23: 0.5473684210526316, 24: 0.06451612903225806, 26: 0.6153846153846154, 27: 0.2247191011235955, 29: 0.7478991596638656, 31: 0.09090909090909091, 32: 0.5724137931034483, 40: 0.15384615384615385}
Micro-average F1 score: 0.43730697961704756
Weighted-average F1 score: 0.40972780352154115
F1 score per class: {0: 0.46258503401360546, 4: 0.8294930875576036, 5: 0.3321678321678322, 6: 0.27106227106227104, 7: 0.037267080745341616, 9: 0.45871559633027525, 10: 0.35874439461883406, 13: 0.07692307692307693, 16: 0.2857142857142857, 17: 0.06741573033707865, 18: 0.2457627118644068, 19: 0.5030674846625767, 21: 0.08722741433021806, 23: 0.453781512605042, 24: 0.12121212121212122, 26: 0.5872340425531914, 27: 0.20202020202020202, 29: 0.6816479400749064, 31: 0.027777777777777776, 32: 0.5484949832775919, 40: 0.10699588477366255}
Micro-average F1 score: 0.3592592592592593
Weighted-average F1 score: 0.3310629905710282
F1 score per class: {0: 0.4888888888888889, 4: 0.88, 5: 0.3869653767820774, 6: 0.2835820895522388, 7: 0.03896103896103896, 9: 0.625, 10: 0.30845771144278605, 13: 0.057971014492753624, 16: 0.3006535947712418, 17: 0.05128205128205128, 18: 0.23809523809523808, 19: 0.5256410256410257, 21: 0.10588235294117647, 23: 0.47368421052631576, 24: 0.0784313725490196, 26: 0.591304347826087, 27: 0.20618556701030927, 29: 0.7193675889328063, 31: 0.03773584905660377, 32: 0.5521885521885522, 40: 0.09252669039145907}
Micro-average F1 score: 0.3746583850931677
Weighted-average F1 score: 0.3431584870332838
cur_acc_wo_na:  ['0.7639', '0.3206', '0.5978', '0.5038']
his_acc_wo_na:  ['0.7639', '0.5041', '0.5563', '0.5610']
cur_acc des_wo_na:  ['0.7614', '0.3662', '0.5532', '0.5163']
his_acc des_wo_na:  ['0.7614', '0.5672', '0.5245', '0.5082']
cur_acc rrf_wo_na:  ['0.7631', '0.3755', '0.5797', '0.5089']
his_acc rrf_wo_na:  ['0.7631', '0.5720', '0.5346', '0.5222']
cur_acc_w_na:  ['0.6198', '0.2902', '0.4544', '0.3702']
his_acc_w_na:  ['0.6198', '0.4133', '0.4367', '0.4373']
cur_acc des_w_na:  ['0.6236', '0.3291', '0.4188', '0.3301']
his_acc des_w_na:  ['0.6236', '0.4701', '0.4018', '0.3593']
cur_acc rrf_w_na:  ['0.6263', '0.3410', '0.4413', '0.3257']
his_acc rrf_w_na:  ['0.6263', '0.4759', '0.4136', '0.3747']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 75.5984497CurrentTrain: epoch  0, batch     1 | loss: 95.2789432CurrentTrain: epoch  0, batch     2 | loss: 96.2684253CurrentTrain: epoch  0, batch     3 | loss: 61.6844086CurrentTrain: epoch  1, batch     0 | loss: 84.0032390CurrentTrain: epoch  1, batch     1 | loss: 89.2673116CurrentTrain: epoch  1, batch     2 | loss: 74.7355564CurrentTrain: epoch  1, batch     3 | loss: 50.5145257CurrentTrain: epoch  2, batch     0 | loss: 70.6132354CurrentTrain: epoch  2, batch     1 | loss: 69.0944938CurrentTrain: epoch  2, batch     2 | loss: 100.1780928CurrentTrain: epoch  2, batch     3 | loss: 121.2357590CurrentTrain: epoch  3, batch     0 | loss: 66.8645508CurrentTrain: epoch  3, batch     1 | loss: 78.0184380CurrentTrain: epoch  3, batch     2 | loss: 72.1042034CurrentTrain: epoch  3, batch     3 | loss: 89.1613740CurrentTrain: epoch  4, batch     0 | loss: 68.5812784CurrentTrain: epoch  4, batch     1 | loss: 82.9733954CurrentTrain: epoch  4, batch     2 | loss: 68.7194699CurrentTrain: epoch  4, batch     3 | loss: 43.9348996CurrentTrain: epoch  5, batch     0 | loss: 67.4517971CurrentTrain: epoch  5, batch     1 | loss: 81.2449555CurrentTrain: epoch  5, batch     2 | loss: 94.9092056CurrentTrain: epoch  5, batch     3 | loss: 52.0884339CurrentTrain: epoch  6, batch     0 | loss: 78.4716931CurrentTrain: epoch  6, batch     1 | loss: 79.0749184CurrentTrain: epoch  6, batch     2 | loss: 65.3114445CurrentTrain: epoch  6, batch     3 | loss: 66.6049668CurrentTrain: epoch  7, batch     0 | loss: 78.0193798CurrentTrain: epoch  7, batch     1 | loss: 94.5505478CurrentTrain: epoch  7, batch     2 | loss: 92.2581928CurrentTrain: epoch  7, batch     3 | loss: 52.1603636CurrentTrain: epoch  8, batch     0 | loss: 62.3628869CurrentTrain: epoch  8, batch     1 | loss: 79.0075766CurrentTrain: epoch  8, batch     2 | loss: 63.4483864CurrentTrain: epoch  8, batch     3 | loss: 65.9260321CurrentTrain: epoch  9, batch     0 | loss: 97.2076066CurrentTrain: epoch  9, batch     1 | loss: 61.5724613CurrentTrain: epoch  9, batch     2 | loss: 65.5244044CurrentTrain: epoch  9, batch     3 | loss: 58.8539043
MemoryTrain:  epoch  0, batch     0 | loss: 0.8216194MemoryTrain:  epoch  1, batch     0 | loss: 0.7746159MemoryTrain:  epoch  2, batch     0 | loss: 0.6211116MemoryTrain:  epoch  3, batch     0 | loss: 0.5055215MemoryTrain:  epoch  4, batch     0 | loss: 0.4771674MemoryTrain:  epoch  5, batch     0 | loss: 0.3825370MemoryTrain:  epoch  6, batch     0 | loss: 0.3045730MemoryTrain:  epoch  7, batch     0 | loss: 0.2598431MemoryTrain:  epoch  8, batch     0 | loss: 0.2136347MemoryTrain:  epoch  9, batch     0 | loss: 0.2063071

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.6363636363636364, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.44776119402985076, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.72, 37: 0.6615384615384615, 38: 0.5238095238095238, 40: 0.0}
Micro-average F1 score: 0.4890829694323144
Weighted-average F1 score: 0.40184360773477446
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.0, 15: 0.5714285714285714, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.4864864864864865, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.7592592592592593, 37: 0.6929133858267716, 38: 0.6428571428571429, 40: 0.0}
Micro-average F1 score: 0.4721189591078067
Weighted-average F1 score: 0.3700041294260786
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.5714285714285714, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.4931506849315068, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 35: 0.7027027027027027, 37: 0.6470588235294118, 38: 0.6538461538461539, 40: 0.0}
Micro-average F1 score: 0.4796905222437137
Weighted-average F1 score: 0.3913696807341173

F1 score per class: {0: 0.7142857142857143, 4: 0.8372093023255814, 5: 0.7654320987654321, 6: 0.367816091954023, 7: 0.0625, 9: 0.7692307692307693, 10: 0.171875, 13: 0.12658227848101267, 15: 0.208955223880597, 16: 0.6764705882352942, 17: 0.08, 18: 0.0, 19: 0.5907172995780591, 21: 0.1111111111111111, 23: 0.6410256410256411, 24: 0.0, 25: 0.44776119402985076, 26: 0.6767676767676768, 27: 0.32727272727272727, 29: 0.8571428571428571, 31: 0.2222222222222222, 32: 0.7393364928909952, 35: 0.43373493975903615, 37: 0.21393034825870647, 38: 0.2972972972972973, 40: 0.2541436464088398}
Micro-average F1 score: 0.4716981132075472
Weighted-average F1 score: 0.4378002292724301
F1 score per class: {0: 0.5789473684210527, 4: 0.883248730964467, 5: 0.5203252032520326, 6: 0.4219409282700422, 7: 0.07142857142857142, 9: 0.5434782608695652, 10: 0.2875816993464052, 13: 0.14285714285714285, 15: 0.3, 16: 0.5274725274725275, 17: 0.0, 18: 0.417910447761194, 19: 0.5703422053231939, 21: 0.15028901734104047, 23: 0.5567010309278351, 24: 0.2727272727272727, 25: 0.4864864864864865, 26: 0.6575342465753424, 27: 0.2857142857142857, 29: 0.8599033816425121, 31: 0.08695652173913043, 32: 0.7522123893805309, 35: 0.4019607843137255, 37: 0.25142857142857145, 38: 0.288, 40: 0.26143790849673204}
Micro-average F1 score: 0.4731471535982814
Weighted-average F1 score: 0.446528332412846
F1 score per class: {0: 0.6597938144329897, 4: 0.9139784946236559, 5: 0.6109324758842444, 6: 0.44339622641509435, 7: 0.07058823529411765, 9: 0.7352941176470589, 10: 0.29333333333333333, 13: 0.12698412698412698, 15: 0.21818181818181817, 16: 0.6052631578947368, 17: 0.0, 18: 0.18867924528301888, 19: 0.5758754863813229, 21: 0.1477832512315271, 23: 0.6341463414634146, 24: 0.07407407407407407, 25: 0.4931506849315068, 26: 0.6728971962616822, 27: 0.3103448275862069, 29: 0.8543689320388349, 31: 0.1111111111111111, 32: 0.7443946188340808, 35: 0.3786407766990291, 37: 0.21674876847290642, 38: 0.3063063063063063, 40: 0.2484472049689441}
Micro-average F1 score: 0.4728682170542636
Weighted-average F1 score: 0.43882478038648515

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.4117647058823529, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 25: 0.4166666666666667, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5714285714285714, 37: 0.46994535519125685, 38: 0.4489795918367347, 40: 0.0}
Micro-average F1 score: 0.3393939393939394
Weighted-average F1 score: 0.27953089160947425
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.0, 15: 0.41379310344827586, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.4444444444444444, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.6165413533834586, 37: 0.5534591194968553, 38: 0.4931506849315068, 40: 0.0}
Micro-average F1 score: 0.33159268929503916
Weighted-average F1 score: 0.26246297784771017
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 13: 0.0, 15: 0.4, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 25: 0.45, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.5735294117647058, 37: 0.4888888888888889, 38: 0.4857142857142857, 40: 0.0}
Micro-average F1 score: 0.3392612859097127
Weighted-average F1 score: 0.28003144654088047

F1 score per class: {0: 0.5882352941176471, 4: 0.7912087912087912, 5: 0.58125, 6: 0.24242424242424243, 7: 0.03296703296703297, 9: 0.7142857142857143, 10: 0.13496932515337423, 13: 0.08130081300813008, 15: 0.10294117647058823, 16: 0.37398373983739835, 17: 0.045454545454545456, 18: 0.0, 19: 0.546875, 21: 0.07407407407407407, 23: 0.5813953488372093, 24: 0.0, 25: 0.4166666666666667, 26: 0.5851528384279476, 27: 0.21176470588235294, 29: 0.6850393700787402, 31: 0.15384615384615385, 32: 0.582089552238806, 35: 0.2517482517482518, 37: 0.11605937921727395, 38: 0.16793893129770993, 40: 0.2100456621004566}
Micro-average F1 score: 0.3343399482312338
Weighted-average F1 score: 0.2988800432296326
F1 score per class: {0: 0.4583333333333333, 4: 0.8285714285714286, 5: 0.3368421052631579, 6: 0.25839793281653745, 7: 0.03896103896103896, 9: 0.4065040650406504, 10: 0.21256038647342995, 13: 0.09411764705882353, 15: 0.21428571428571427, 16: 0.294478527607362, 17: 0.0, 18: 0.23728813559322035, 19: 0.5244755244755245, 21: 0.09885931558935361, 23: 0.4426229508196721, 24: 0.1518987341772152, 25: 0.43373493975903615, 26: 0.5373134328358209, 27: 0.18823529411764706, 29: 0.6793893129770993, 31: 0.05128205128205128, 32: 0.5743243243243243, 35: 0.24550898203592814, 37: 0.15714285714285714, 38: 0.15384615384615385, 40: 0.2222222222222222}
Micro-average F1 score: 0.3306436479639707
Weighted-average F1 score: 0.30644714534299894
F1 score per class: {0: 0.5245901639344263, 4: 0.8585858585858586, 5: 0.4367816091954023, 6: 0.283987915407855, 7: 0.037037037037037035, 9: 0.6578947368421053, 10: 0.2222222222222222, 13: 0.08247422680412371, 15: 0.13333333333333333, 16: 0.3262411347517731, 17: 0.0, 18: 0.14285714285714285, 19: 0.5285714285714286, 21: 0.09584664536741214, 23: 0.5591397849462365, 24: 0.05555555555555555, 25: 0.4444444444444444, 26: 0.5691699604743083, 27: 0.19148936170212766, 29: 0.6821705426356589, 31: 0.07407407407407407, 32: 0.564625850340136, 35: 0.2314540059347181, 37: 0.12941176470588237, 38: 0.1588785046728972, 40: 0.21052631578947367}
Micro-average F1 score: 0.3353622619281367
Weighted-average F1 score: 0.3043402867700954
cur_acc_wo_na:  ['0.7639', '0.3206', '0.5978', '0.5038', '0.4891']
his_acc_wo_na:  ['0.7639', '0.5041', '0.5563', '0.5610', '0.4717']
cur_acc des_wo_na:  ['0.7614', '0.3662', '0.5532', '0.5163', '0.4721']
his_acc des_wo_na:  ['0.7614', '0.5672', '0.5245', '0.5082', '0.4731']
cur_acc rrf_wo_na:  ['0.7631', '0.3755', '0.5797', '0.5089', '0.4797']
his_acc rrf_wo_na:  ['0.7631', '0.5720', '0.5346', '0.5222', '0.4729']
cur_acc_w_na:  ['0.6198', '0.2902', '0.4544', '0.3702', '0.3394']
his_acc_w_na:  ['0.6198', '0.4133', '0.4367', '0.4373', '0.3343']
cur_acc des_w_na:  ['0.6236', '0.3291', '0.4188', '0.3301', '0.3316']
his_acc des_w_na:  ['0.6236', '0.4701', '0.4018', '0.3593', '0.3306']
cur_acc rrf_w_na:  ['0.6263', '0.3410', '0.4413', '0.3257', '0.3393']
his_acc rrf_w_na:  ['0.6263', '0.4759', '0.4136', '0.3747', '0.3354']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 146.7843382CurrentTrain: epoch  0, batch     1 | loss: 79.4909859CurrentTrain: epoch  0, batch     2 | loss: 92.1819669CurrentTrain: epoch  0, batch     3 | loss: 111.8377712CurrentTrain: epoch  0, batch     4 | loss: 22.6023572CurrentTrain: epoch  1, batch     0 | loss: 135.9699787CurrentTrain: epoch  1, batch     1 | loss: 102.7166455CurrentTrain: epoch  1, batch     2 | loss: 75.0824565CurrentTrain: epoch  1, batch     3 | loss: 89.7358692CurrentTrain: epoch  1, batch     4 | loss: 28.5651706CurrentTrain: epoch  2, batch     0 | loss: 105.2119965CurrentTrain: epoch  2, batch     1 | loss: 83.4677583CurrentTrain: epoch  2, batch     2 | loss: 128.5782060CurrentTrain: epoch  2, batch     3 | loss: 71.3794300CurrentTrain: epoch  2, batch     4 | loss: 18.7914960CurrentTrain: epoch  3, batch     0 | loss: 83.1995317CurrentTrain: epoch  3, batch     1 | loss: 82.2402464CurrentTrain: epoch  3, batch     2 | loss: 100.3295188CurrentTrain: epoch  3, batch     3 | loss: 69.1002533CurrentTrain: epoch  3, batch     4 | loss: 39.4946789CurrentTrain: epoch  4, batch     0 | loss: 69.7845919CurrentTrain: epoch  4, batch     1 | loss: 97.4858918CurrentTrain: epoch  4, batch     2 | loss: 67.6252805CurrentTrain: epoch  4, batch     3 | loss: 97.6004545CurrentTrain: epoch  4, batch     4 | loss: 26.3223582CurrentTrain: epoch  5, batch     0 | loss: 85.1980522CurrentTrain: epoch  5, batch     1 | loss: 96.3935674CurrentTrain: epoch  5, batch     2 | loss: 77.7993319CurrentTrain: epoch  5, batch     3 | loss: 62.6827035CurrentTrain: epoch  5, batch     4 | loss: 39.6357126CurrentTrain: epoch  6, batch     0 | loss: 94.1960968CurrentTrain: epoch  6, batch     1 | loss: 95.8125764CurrentTrain: epoch  6, batch     2 | loss: 77.9869409CurrentTrain: epoch  6, batch     3 | loss: 77.6112666CurrentTrain: epoch  6, batch     4 | loss: 22.7444100CurrentTrain: epoch  7, batch     0 | loss: 66.8281442CurrentTrain: epoch  7, batch     1 | loss: 79.8847305CurrentTrain: epoch  7, batch     2 | loss: 77.0678429CurrentTrain: epoch  7, batch     3 | loss: 92.8679030CurrentTrain: epoch  7, batch     4 | loss: 23.2412305CurrentTrain: epoch  8, batch     0 | loss: 78.7462685CurrentTrain: epoch  8, batch     1 | loss: 76.7019145CurrentTrain: epoch  8, batch     2 | loss: 76.5606313CurrentTrain: epoch  8, batch     3 | loss: 62.7033337CurrentTrain: epoch  8, batch     4 | loss: 39.3731217CurrentTrain: epoch  9, batch     0 | loss: 75.9194536CurrentTrain: epoch  9, batch     1 | loss: 77.4297330CurrentTrain: epoch  9, batch     2 | loss: 91.8129109CurrentTrain: epoch  9, batch     3 | loss: 76.5343288CurrentTrain: epoch  9, batch     4 | loss: 38.4622139
MemoryTrain:  epoch  0, batch     0 | loss: 0.9845265MemoryTrain:  epoch  1, batch     0 | loss: 0.9109858MemoryTrain:  epoch  2, batch     0 | loss: 0.6890521MemoryTrain:  epoch  3, batch     0 | loss: 0.6076157MemoryTrain:  epoch  4, batch     0 | loss: 0.5071771MemoryTrain:  epoch  5, batch     0 | loss: 0.4462615MemoryTrain:  epoch  6, batch     0 | loss: 0.3940756MemoryTrain:  epoch  7, batch     0 | loss: 0.3650175MemoryTrain:  epoch  8, batch     0 | loss: 0.2781268MemoryTrain:  epoch  9, batch     0 | loss: 0.2797801

F1 score per class: {0: 0.0, 2: 0.6363636363636364, 4: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.48226950354609927, 12: 0.47368421052631576, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 27: 0.0, 28: 0.21875, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.2222222222222222, 40: 0.0}
Micro-average F1 score: 0.3302411873840445
Weighted-average F1 score: 0.24251087124039952
F1 score per class: {0: 0.0, 2: 0.45161290322580644, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.42748091603053434, 12: 0.47058823529411764, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 24: 0.0, 27: 0.0, 28: 0.22950819672131148, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.35294117647058826, 40: 0.0}
Micro-average F1 score: 0.30656934306569344
Weighted-average F1 score: 0.22030728780094294
F1 score per class: {0: 0.0, 2: 0.56, 4: 0.0, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.4626865671641791, 12: 0.48366013071895425, 13: 0.0, 15: 0.0, 16: 0.0, 21: 0.0, 27: 0.0, 28: 0.2028985507246377, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.34146341463414637, 40: 0.0}
Micro-average F1 score: 0.3290203327171904
Weighted-average F1 score: 0.24060215157447723

F1 score per class: {0: 0.6966292134831461, 2: 0.2692307692307692, 4: 0.7976190476190477, 5: 0.8, 6: 0.22727272727272727, 7: 0.0625, 9: 0.7575757575757576, 10: 0.12612612612612611, 11: 0.25, 12: 0.3302752293577982, 13: 0.08163265306122448, 15: 0.21621621621621623, 16: 0.6666666666666666, 17: 0.0, 18: 0.0, 19: 0.6160337552742616, 21: 0.1761006289308176, 23: 0.6133333333333333, 24: 0.09523809523809523, 25: 0.47058823529411764, 26: 0.6836734693877551, 27: 0.2727272727272727, 28: 0.07909604519774012, 29: 0.8442211055276382, 31: 0.0, 32: 0.7466666666666667, 35: 0.46153846153846156, 37: 0.25882352941176473, 38: 0.21951219512195122, 39: 0.05291005291005291, 40: 0.28402366863905326}
Micro-average F1 score: 0.4286835906830673
Weighted-average F1 score: 0.39512344317443465
F1 score per class: {0: 0.5255474452554745, 2: 0.12280701754385964, 4: 0.8306010928961749, 5: 0.6440677966101694, 6: 0.36036036036036034, 7: 0.07228915662650602, 9: 0.5555555555555556, 10: 0.25, 11: 0.25339366515837103, 12: 0.26666666666666666, 13: 0.0, 15: 0.35294117647058826, 16: 0.6024096385542169, 17: 0.0, 18: 0.0, 19: 0.6, 21: 0.21052631578947367, 23: 0.5979381443298969, 24: 0.22857142857142856, 25: 0.4864864864864865, 26: 0.6857142857142857, 27: 0.32653061224489793, 28: 0.08974358974358974, 29: 0.8290155440414507, 31: 0.08695652173913043, 32: 0.7131147540983607, 35: 0.4251207729468599, 37: 0.26277372262773724, 38: 0.3409090909090909, 39: 0.09523809523809523, 40: 0.28125}
Micro-average F1 score: 0.4353939393939394
Weighted-average F1 score: 0.4121692183485236
F1 score per class: {0: 0.66, 2: 0.21212121212121213, 4: 0.8539325842696629, 5: 0.7224334600760456, 6: 0.3870967741935484, 7: 0.0821917808219178, 9: 0.7142857142857143, 10: 0.18181818181818182, 11: 0.26956521739130435, 12: 0.29838709677419356, 13: 0.06896551724137931, 15: 0.2553191489361702, 16: 0.684931506849315, 17: 0.0, 18: 0.0, 19: 0.6030534351145038, 21: 0.18579234972677597, 23: 0.6436781609195402, 24: 0.08, 25: 0.5142857142857142, 26: 0.6893203883495146, 27: 0.2857142857142857, 28: 0.07909604519774012, 29: 0.841025641025641, 31: 0.11764705882352941, 32: 0.7350427350427351, 35: 0.425531914893617, 37: 0.267515923566879, 38: 0.32608695652173914, 39: 0.09395973154362416, 40: 0.2714285714285714}
Micro-average F1 score: 0.4466313398940197
Weighted-average F1 score: 0.4185822152552445

F1 score per class: {0: 0.0, 2: 0.3888888888888889, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.3756906077348066, 12: 0.39344262295081966, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 28: 0.10526315789473684, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.11764705882352941, 40: 0.0}
Micro-average F1 score: 0.21654501216545013
Weighted-average F1 score: 0.1651837481551704
F1 score per class: {0: 0.0, 2: 0.25, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.345679012345679, 12: 0.4044943820224719, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 27: 0.0, 28: 0.12962962962962962, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.17647058823529413, 40: 0.0}
Micro-average F1 score: 0.20363636363636364
Weighted-average F1 score: 0.15078083075742887
F1 score per class: {0: 0.0, 2: 0.2978723404255319, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.37575757575757573, 12: 0.4157303370786517, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 28: 0.11864406779661017, 29: 0.0, 31: 0.0, 32: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.16470588235294117, 40: 0.0}
Micro-average F1 score: 0.21948212083847102
Weighted-average F1 score: 0.16456826632825053

F1 score per class: {0: 0.5688073394495413, 2: 0.16279069767441862, 4: 0.7486033519553073, 5: 0.6433566433566433, 6: 0.16393442622950818, 7: 0.038461538461538464, 9: 0.704225352112676, 10: 0.11570247933884298, 11: 0.16152019002375298, 12: 0.16400911161731208, 13: 0.057971014492753624, 15: 0.12121212121212122, 16: 0.36666666666666664, 17: 0.0, 18: 0.0, 19: 0.5793650793650794, 21: 0.11428571428571428, 23: 0.5411764705882353, 24: 0.07692307692307693, 25: 0.4444444444444444, 26: 0.5982142857142857, 27: 0.2033898305084746, 28: 0.03482587064676617, 29: 0.7058823529411765, 31: 0.0, 32: 0.56, 35: 0.2845849802371542, 37: 0.16356877323420074, 38: 0.13138686131386862, 39: 0.02631578947368421, 40: 0.23880597014925373}
Micro-average F1 score: 0.2958815028901734
Weighted-average F1 score: 0.26112180220958825
F1 score per class: {0: 0.4044943820224719, 2: 0.07368421052631578, 4: 0.7875647668393783, 5: 0.4470588235294118, 6: 0.22160664819944598, 7: 0.039735099337748346, 9: 0.4166666666666667, 10: 0.2073170731707317, 11: 0.16816816816816818, 12: 0.14906832298136646, 13: 0.0, 15: 0.21818181818181817, 16: 0.33112582781456956, 17: 0.0, 18: 0.0, 19: 0.5625, 21: 0.14746543778801843, 23: 0.48333333333333334, 24: 0.12903225806451613, 25: 0.45569620253164556, 26: 0.5925925925925926, 27: 0.24242424242424243, 28: 0.05090909090909091, 29: 0.7048458149779736, 31: 0.05263157894736842, 32: 0.5304878048780488, 35: 0.2603550295857988, 37: 0.2033898305084746, 38: 0.22058823529411764, 39: 0.048, 40: 0.24324324324324326}
Micro-average F1 score: 0.30497537782305995
Weighted-average F1 score: 0.2801839501701718
F1 score per class: {0: 0.528, 2: 0.11475409836065574, 4: 0.8128342245989305, 5: 0.5177111716621253, 6: 0.24742268041237114, 7: 0.0425531914893617, 9: 0.6493506493506493, 10: 0.15602836879432624, 11: 0.1781609195402299, 12: 0.164079822616408, 13: 0.046511627906976744, 15: 0.14814814814814814, 16: 0.3875968992248062, 17: 0.0, 18: 0.0, 19: 0.5642857142857143, 21: 0.12408759124087591, 23: 0.5656565656565656, 24: 0.058823529411764705, 25: 0.4864864864864865, 26: 0.6042553191489362, 27: 0.2028985507246377, 28: 0.0438871473354232, 29: 0.7161572052401747, 31: 0.06666666666666667, 32: 0.5408805031446541, 35: 0.26143790849673204, 37: 0.20192307692307693, 38: 0.2097902097902098, 39: 0.04666666666666667, 40: 0.23170731707317074}
Micro-average F1 score: 0.31344076500796886
Weighted-average F1 score: 0.2838025471180534
cur_acc_wo_na:  ['0.7639', '0.3206', '0.5978', '0.5038', '0.4891', '0.3302']
his_acc_wo_na:  ['0.7639', '0.5041', '0.5563', '0.5610', '0.4717', '0.4287']
cur_acc des_wo_na:  ['0.7614', '0.3662', '0.5532', '0.5163', '0.4721', '0.3066']
his_acc des_wo_na:  ['0.7614', '0.5672', '0.5245', '0.5082', '0.4731', '0.4354']
cur_acc rrf_wo_na:  ['0.7631', '0.3755', '0.5797', '0.5089', '0.4797', '0.3290']
his_acc rrf_wo_na:  ['0.7631', '0.5720', '0.5346', '0.5222', '0.4729', '0.4466']
cur_acc_w_na:  ['0.6198', '0.2902', '0.4544', '0.3702', '0.3394', '0.2165']
his_acc_w_na:  ['0.6198', '0.4133', '0.4367', '0.4373', '0.3343', '0.2959']
cur_acc des_w_na:  ['0.6236', '0.3291', '0.4188', '0.3301', '0.3316', '0.2036']
his_acc des_w_na:  ['0.6236', '0.4701', '0.4018', '0.3593', '0.3306', '0.3050']
cur_acc rrf_w_na:  ['0.6263', '0.3410', '0.4413', '0.3257', '0.3393', '0.2195']
his_acc rrf_w_na:  ['0.6263', '0.4759', '0.4136', '0.3747', '0.3354', '0.3134']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 94.0121722CurrentTrain: epoch  0, batch     1 | loss: 97.3948672CurrentTrain: epoch  0, batch     2 | loss: 204.2359142CurrentTrain: epoch  0, batch     3 | loss: 91.1396904CurrentTrain: epoch  0, batch     4 | loss: 82.7752771CurrentTrain: epoch  1, batch     0 | loss: 76.8249382CurrentTrain: epoch  1, batch     1 | loss: 97.9883943CurrentTrain: epoch  1, batch     2 | loss: 104.5973763CurrentTrain: epoch  1, batch     3 | loss: 106.8100798CurrentTrain: epoch  1, batch     4 | loss: 51.2682379CurrentTrain: epoch  2, batch     0 | loss: 106.6598241CurrentTrain: epoch  2, batch     1 | loss: 73.8377923CurrentTrain: epoch  2, batch     2 | loss: 71.1487911CurrentTrain: epoch  2, batch     3 | loss: 88.8531607CurrentTrain: epoch  2, batch     4 | loss: 60.0818876CurrentTrain: epoch  3, batch     0 | loss: 109.3976367CurrentTrain: epoch  3, batch     1 | loss: 85.2668251CurrentTrain: epoch  3, batch     2 | loss: 97.7601990CurrentTrain: epoch  3, batch     3 | loss: 100.9064099CurrentTrain: epoch  3, batch     4 | loss: 98.7653790CurrentTrain: epoch  4, batch     0 | loss: 100.0284610CurrentTrain: epoch  4, batch     1 | loss: 124.6631120CurrentTrain: epoch  4, batch     2 | loss: 82.4416862CurrentTrain: epoch  4, batch     3 | loss: 70.8792688CurrentTrain: epoch  4, batch     4 | loss: 55.1553079CurrentTrain: epoch  5, batch     0 | loss: 80.2537735CurrentTrain: epoch  5, batch     1 | loss: 79.0550949CurrentTrain: epoch  5, batch     2 | loss: 82.6264754CurrentTrain: epoch  5, batch     3 | loss: 124.8499665CurrentTrain: epoch  5, batch     4 | loss: 56.0540176CurrentTrain: epoch  6, batch     0 | loss: 81.1390491CurrentTrain: epoch  6, batch     1 | loss: 97.6853146CurrentTrain: epoch  6, batch     2 | loss: 167.1127219CurrentTrain: epoch  6, batch     3 | loss: 68.4140726CurrentTrain: epoch  6, batch     4 | loss: 49.5309068CurrentTrain: epoch  7, batch     0 | loss: 77.9296959CurrentTrain: epoch  7, batch     1 | loss: 96.0699506CurrentTrain: epoch  7, batch     2 | loss: 97.7985543CurrentTrain: epoch  7, batch     3 | loss: 80.8911654CurrentTrain: epoch  7, batch     4 | loss: 54.2347170CurrentTrain: epoch  8, batch     0 | loss: 94.8050983CurrentTrain: epoch  8, batch     1 | loss: 68.1757553CurrentTrain: epoch  8, batch     2 | loss: 96.1429466CurrentTrain: epoch  8, batch     3 | loss: 93.5604695CurrentTrain: epoch  8, batch     4 | loss: 66.9992187CurrentTrain: epoch  9, batch     0 | loss: 79.3799433CurrentTrain: epoch  9, batch     1 | loss: 78.8752570CurrentTrain: epoch  9, batch     2 | loss: 96.5475169CurrentTrain: epoch  9, batch     3 | loss: 77.5407049CurrentTrain: epoch  9, batch     4 | loss: 41.5747527
MemoryTrain:  epoch  0, batch     0 | loss: 1.3329321MemoryTrain:  epoch  1, batch     0 | loss: 1.1149694MemoryTrain:  epoch  2, batch     0 | loss: 0.8375164MemoryTrain:  epoch  3, batch     0 | loss: 0.6791996MemoryTrain:  epoch  4, batch     0 | loss: 0.6309172MemoryTrain:  epoch  5, batch     0 | loss: 0.5027248MemoryTrain:  epoch  6, batch     0 | loss: 0.4171479MemoryTrain:  epoch  7, batch     0 | loss: 0.3942250MemoryTrain:  epoch  8, batch     0 | loss: 0.3313822MemoryTrain:  epoch  9, batch     0 | loss: 0.2864721

F1 score per class: {0: 0.0, 1: 0.24050632911392406, 2: 0.0, 3: 0.5466666666666666, 6: 0.0, 11: 0.0, 12: 0.0, 14: 0.08450704225352113, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5460992907801419, 23: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.6947368421052632, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.35126903553299493
Weighted-average F1 score: 0.29989277700747463
F1 score per class: {0: 0.0, 1: 0.26285714285714284, 2: 0.0, 3: 0.5586592178770949, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.08791208791208792, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5440613026819924, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.7559055118110236, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3388072601555748
Weighted-average F1 score: 0.2864775761425171
F1 score per class: {0: 0.0, 1: 0.27058823529411763, 2: 0.0, 3: 0.5714285714285714, 5: 0.0, 6: 0.0, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.024691358024691357, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.56, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.7521367521367521, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.35747021081576535
Weighted-average F1 score: 0.3106130871990157

F1 score per class: {0: 0.7733333333333333, 1: 0.1919191919191919, 2: 0.32558139534883723, 3: 0.3215686274509804, 4: 0.7831325301204819, 5: 0.832579185520362, 6: 0.2676056338028169, 7: 0.0392156862745098, 9: 0.7692307692307693, 10: 0.21052631578947367, 11: 0.19469026548672566, 12: 0.20930232558139536, 13: 0.19047619047619047, 14: 0.06896551724137931, 15: 0.4444444444444444, 16: 0.6428571428571429, 17: 0.0, 18: 0.0, 19: 0.5421245421245421, 21: 0.01818181818181818, 22: 0.41397849462365593, 23: 0.6666666666666666, 24: 0.0, 25: 0.4, 26: 0.6926829268292682, 27: 0.0, 28: 0.06060606060606061, 29: 0.8235294117647058, 31: 0.0, 32: 0.6729857819905213, 34: 0.29333333333333333, 35: 0.22535211267605634, 37: 0.15384615384615385, 38: 0.2558139534883721, 39: 0.078125, 40: 0.275}
Micro-average F1 score: 0.38819474364498063
Weighted-average F1 score: 0.366236279412067
F1 score per class: {0: 0.5555555555555556, 1: 0.19327731092436976, 2: 0.1728395061728395, 3: 0.31347962382445144, 4: 0.8295454545454546, 5: 0.6188925081433225, 6: 0.36666666666666664, 7: 0.06557377049180328, 9: 0.5, 10: 0.18461538461538463, 11: 0.1732283464566929, 12: 0.25084745762711863, 13: 0.16666666666666666, 14: 0.06153846153846154, 15: 0.5, 16: 0.6486486486486487, 17: 0.0, 18: 0.03076923076923077, 19: 0.5152542372881356, 21: 0.018867924528301886, 22: 0.41887905604719766, 23: 0.6274509803921569, 24: 0.08695652173913043, 25: 0.4864864864864865, 26: 0.6824644549763034, 27: 0.0, 28: 0.10071942446043165, 29: 0.7916666666666666, 31: 0.0, 32: 0.583941605839416, 34: 0.22588235294117648, 35: 0.18072289156626506, 37: 0.06896551724137931, 38: 0.2571428571428571, 39: 0.1111111111111111, 40: 0.25333333333333335}
Micro-average F1 score: 0.36590742101396034
Weighted-average F1 score: 0.349867572351341
F1 score per class: {0: 0.6881720430107527, 1: 0.2100456621004566, 2: 0.23333333333333334, 3: 0.31347962382445144, 4: 0.8372093023255814, 5: 0.753968253968254, 6: 0.35714285714285715, 7: 0.06779661016949153, 9: 0.7142857142857143, 10: 0.18333333333333332, 11: 0.20512820512820512, 12: 0.2682926829268293, 13: 0.14814814814814814, 14: 0.018867924528301886, 15: 0.4444444444444444, 16: 0.6666666666666666, 17: 0.0, 18: 0.0, 19: 0.5347222222222222, 21: 0.016666666666666666, 22: 0.41397849462365593, 23: 0.6236559139784946, 24: 0.0, 25: 0.4507042253521127, 26: 0.6923076923076923, 27: 0.0, 28: 0.09271523178807947, 29: 0.7979274611398963, 31: 0.0, 32: 0.6086956521739131, 34: 0.25507246376811593, 35: 0.19047619047619047, 37: 0.11764705882352941, 38: 0.272, 39: 0.0847457627118644, 40: 0.2331288343558282}
Micro-average F1 score: 0.38026238496181713
Weighted-average F1 score: 0.3613033825762683

F1 score per class: {0: 0.0, 1: 0.13058419243986255, 2: 0.0, 3: 0.39805825242718446, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0821917808219178, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.4230769230769231, 23: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.6, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.236986301369863
Weighted-average F1 score: 0.20341086491939633
F1 score per class: {0: 0.0, 1: 0.1419753086419753, 2: 0.0, 3: 0.390625, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.08080808080808081, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.4329268292682927, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.5925925925925926, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2238720731010851
Weighted-average F1 score: 0.19376879253567483
F1 score per class: {0: 0.0, 1: 0.14743589743589744, 2: 0.0, 3: 0.3937007874015748, 5: 0.0, 6: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.022988505747126436, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.43874643874643876, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 34: 0.6068965517241379, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2360774818401937
Weighted-average F1 score: 0.20821246323154932

F1 score per class: {0: 0.5631067961165048, 1: 0.09921671018276762, 2: 0.2028985507246377, 3: 0.20603015075376885, 4: 0.7428571428571429, 5: 0.6789667896678967, 6: 0.17272727272727273, 7: 0.02127659574468085, 9: 0.6944444444444444, 10: 0.17777777777777778, 11: 0.12464589235127478, 12: 0.12244897959183673, 13: 0.10526315789473684, 14: 0.061855670103092786, 15: 0.26666666666666666, 16: 0.42857142857142855, 17: 0.0, 18: 0.0, 19: 0.5, 21: 0.014598540145985401, 22: 0.28051001821493626, 23: 0.56, 24: 0.0, 25: 0.38235294117647056, 26: 0.6016949152542372, 27: 0.0, 28: 0.030837004405286344, 29: 0.680161943319838, 31: 0.0, 32: 0.5017667844522968, 34: 0.20689655172413793, 35: 0.15609756097560976, 37: 0.1, 38: 0.15942028985507245, 39: 0.04081632653061224, 40: 0.23036649214659685}
Micro-average F1 score: 0.27204106280193235
Weighted-average F1 score: 0.24724906609787872
F1 score per class: {0: 0.4046242774566474, 1: 0.10087719298245613, 2: 0.1111111111111111, 3: 0.19646365422396855, 4: 0.7849462365591398, 5: 0.42035398230088494, 6: 0.21674876847290642, 7: 0.03418803418803419, 9: 0.3472222222222222, 10: 0.14634146341463414, 11: 0.13924050632911392, 12: 0.1295971978984238, 13: 0.09876543209876543, 14: 0.05128205128205128, 15: 0.32432432432432434, 16: 0.38095238095238093, 17: 0.0, 18: 0.020618556701030927, 19: 0.4735202492211838, 21: 0.01639344262295082, 22: 0.2874493927125506, 23: 0.48120300751879697, 24: 0.07407407407407407, 25: 0.45, 26: 0.5669291338582677, 27: 0.0, 28: 0.056451612903225805, 29: 0.6696035242290749, 31: 0.0, 32: 0.42328042328042326, 34: 0.1411764705882353, 35: 0.12, 37: 0.05517241379310345, 38: 0.15789473684210525, 39: 0.055865921787709494, 40: 0.21348314606741572}
Micro-average F1 score: 0.2492804404955575
Weighted-average F1 score: 0.23242927635999397
F1 score per class: {0: 0.512, 1: 0.10874704491725769, 2: 0.14583333333333334, 3: 0.19455252918287938, 4: 0.7912087912087912, 5: 0.5507246376811594, 6: 0.21671826625386997, 7: 0.037383177570093455, 9: 0.6172839506172839, 10: 0.14285714285714285, 11: 0.15458937198067632, 12: 0.14042553191489363, 13: 0.08888888888888889, 14: 0.016, 15: 0.25, 16: 0.38095238095238093, 17: 0.0, 18: 0.0, 19: 0.49044585987261147, 21: 0.013888888888888888, 22: 0.28308823529411764, 23: 0.5272727272727272, 24: 0.0, 25: 0.4155844155844156, 26: 0.5975103734439834, 27: 0.0, 28: 0.05128205128205128, 29: 0.6754385964912281, 31: 0.0, 32: 0.44126074498567336, 34: 0.15331010452961671, 35: 0.12727272727272726, 37: 0.08974358974358974, 38: 0.17346938775510204, 39: 0.041666666666666664, 40: 0.19487179487179487}
Micro-average F1 score: 0.2611267984402313
Weighted-average F1 score: 0.24100251842673112
cur_acc_wo_na:  ['0.7639', '0.3206', '0.5978', '0.5038', '0.4891', '0.3302', '0.3513']
his_acc_wo_na:  ['0.7639', '0.5041', '0.5563', '0.5610', '0.4717', '0.4287', '0.3882']
cur_acc des_wo_na:  ['0.7614', '0.3662', '0.5532', '0.5163', '0.4721', '0.3066', '0.3388']
his_acc des_wo_na:  ['0.7614', '0.5672', '0.5245', '0.5082', '0.4731', '0.4354', '0.3659']
cur_acc rrf_wo_na:  ['0.7631', '0.3755', '0.5797', '0.5089', '0.4797', '0.3290', '0.3575']
his_acc rrf_wo_na:  ['0.7631', '0.5720', '0.5346', '0.5222', '0.4729', '0.4466', '0.3803']
cur_acc_w_na:  ['0.6198', '0.2902', '0.4544', '0.3702', '0.3394', '0.2165', '0.2370']
his_acc_w_na:  ['0.6198', '0.4133', '0.4367', '0.4373', '0.3343', '0.2959', '0.2720']
cur_acc des_w_na:  ['0.6236', '0.3291', '0.4188', '0.3301', '0.3316', '0.2036', '0.2239']
his_acc des_w_na:  ['0.6236', '0.4701', '0.4018', '0.3593', '0.3306', '0.3050', '0.2493']
cur_acc rrf_w_na:  ['0.6263', '0.3410', '0.4413', '0.3257', '0.3393', '0.2195', '0.2361']
his_acc rrf_w_na:  ['0.6263', '0.4759', '0.4136', '0.3747', '0.3354', '0.3134', '0.2611']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 94.8522665CurrentTrain: epoch  0, batch     1 | loss: 84.1047589CurrentTrain: epoch  0, batch     2 | loss: 84.0981426CurrentTrain: epoch  0, batch     3 | loss: 57.9969906CurrentTrain: epoch  1, batch     0 | loss: 86.8779818CurrentTrain: epoch  1, batch     1 | loss: 87.3448806CurrentTrain: epoch  1, batch     2 | loss: 108.0364315CurrentTrain: epoch  1, batch     3 | loss: 54.4374404CurrentTrain: epoch  2, batch     0 | loss: 85.9167136CurrentTrain: epoch  2, batch     1 | loss: 81.1866951CurrentTrain: epoch  2, batch     2 | loss: 102.0916223CurrentTrain: epoch  2, batch     3 | loss: 49.0896531CurrentTrain: epoch  3, batch     0 | loss: 66.9476694CurrentTrain: epoch  3, batch     1 | loss: 70.6792190CurrentTrain: epoch  3, batch     2 | loss: 128.6024453CurrentTrain: epoch  3, batch     3 | loss: 48.4169570CurrentTrain: epoch  4, batch     0 | loss: 83.5424907CurrentTrain: epoch  4, batch     1 | loss: 77.8948538CurrentTrain: epoch  4, batch     2 | loss: 65.5439800CurrentTrain: epoch  4, batch     3 | loss: 58.5010358CurrentTrain: epoch  5, batch     0 | loss: 77.2820301CurrentTrain: epoch  5, batch     1 | loss: 81.3226117CurrentTrain: epoch  5, batch     2 | loss: 62.0269962CurrentTrain: epoch  5, batch     3 | loss: 62.3703012CurrentTrain: epoch  6, batch     0 | loss: 64.6159849CurrentTrain: epoch  6, batch     1 | loss: 95.6249718CurrentTrain: epoch  6, batch     2 | loss: 67.7710097CurrentTrain: epoch  6, batch     3 | loss: 46.3073992CurrentTrain: epoch  7, batch     0 | loss: 61.2254309CurrentTrain: epoch  7, batch     1 | loss: 92.6052703CurrentTrain: epoch  7, batch     2 | loss: 94.1426466CurrentTrain: epoch  7, batch     3 | loss: 57.6886575CurrentTrain: epoch  8, batch     0 | loss: 77.3459759CurrentTrain: epoch  8, batch     1 | loss: 94.5263778CurrentTrain: epoch  8, batch     2 | loss: 59.6931526CurrentTrain: epoch  8, batch     3 | loss: 72.4939162CurrentTrain: epoch  9, batch     0 | loss: 77.0637846CurrentTrain: epoch  9, batch     1 | loss: 71.9214362CurrentTrain: epoch  9, batch     2 | loss: 71.9555407CurrentTrain: epoch  9, batch     3 | loss: 102.2865535
MemoryTrain:  epoch  0, batch     0 | loss: 0.7732572MemoryTrain:  epoch  1, batch     0 | loss: 0.6759460MemoryTrain:  epoch  2, batch     0 | loss: 0.5292940MemoryTrain:  epoch  3, batch     0 | loss: 0.4164500MemoryTrain:  epoch  4, batch     0 | loss: 0.3755929MemoryTrain:  epoch  5, batch     0 | loss: 0.3233376MemoryTrain:  epoch  6, batch     0 | loss: 0.3035160MemoryTrain:  epoch  7, batch     0 | loss: 0.2486718MemoryTrain:  epoch  8, batch     0 | loss: 0.2185098MemoryTrain:  epoch  9, batch     0 | loss: 0.1894065

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5084745762711864, 11: 0.0, 12: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.8849557522123894, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8333333333333334, 32: 0.0, 33: 0.35294117647058826, 34: 0.0, 36: 0.5490196078431373, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4921875
Weighted-average F1 score: 0.38104970928970827
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5777777777777777, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.7920792079207921, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.85, 31: 0.0, 32: 0.0, 33: 0.4444444444444444, 34: 0.0, 35: 0.0, 36: 0.723404255319149, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4967105263157895
Weighted-average F1 score: 0.3810189735605425
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5354330708661418, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 20: 0.7920792079207921, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8717948717948718, 31: 0.0, 32: 0.0, 33: 0.4444444444444444, 34: 0.0, 35: 0.0, 36: 0.671875, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.4876325088339223
Weighted-average F1 score: 0.37033105247500986

F1 score per class: {0: 0.6095238095238096, 1: 0.18253968253968253, 2: 0.42424242424242425, 3: 0.3, 4: 0.8522727272727273, 5: 0.808695652173913, 6: 0.3, 7: 0.03773584905660377, 8: 0.31088082901554404, 9: 0.7352941176470589, 10: 0.0392156862745098, 11: 0.1452991452991453, 12: 0.17073170731707318, 13: 0.16666666666666666, 14: 0.06451612903225806, 15: 0.36363636363636365, 16: 0.5454545454545454, 17: 0.0, 18: 0.0, 19: 0.5116279069767442, 20: 0.4366812227074236, 21: 0.05333333333333334, 22: 0.4476744186046512, 23: 0.6265060240963856, 24: 0.0, 25: 0.3225806451612903, 26: 0.6730769230769231, 27: 0.0, 28: 0.07692307692307693, 29: 0.784688995215311, 30: 0.8333333333333334, 31: 0.058823529411764705, 32: 0.5882352941176471, 33: 0.08571428571428572, 34: 0.2556390977443609, 35: 0.23225806451612904, 36: 0.3111111111111111, 37: 0.19047619047619047, 38: 0.14285714285714285, 39: 0.06557377049180328, 40: 0.2033898305084746}
Micro-average F1 score: 0.37929050814956855
Weighted-average F1 score: 0.37300011401236305
F1 score per class: {0: 0.39759036144578314, 1: 0.1696113074204947, 2: 0.125, 3: 0.26877470355731226, 4: 0.8268156424581006, 5: 0.5791044776119403, 6: 0.34554973821989526, 7: 0.06451612903225806, 8: 0.3058823529411765, 9: 0.46296296296296297, 10: 0.11864406779661017, 11: 0.17886178861788618, 12: 0.22556390977443608, 13: 0.09523809523809523, 14: 0.05263157894736842, 15: 0.48, 16: 0.6133333333333333, 17: 0.0, 18: 0.03508771929824561, 19: 0.44562334217506633, 20: 0.547945205479452, 21: 0.023255813953488372, 22: 0.34545454545454546, 23: 0.5769230769230769, 24: 0.0, 25: 0.4533333333333333, 26: 0.6545454545454545, 27: 0.0, 28: 0.15789473684210525, 29: 0.8019323671497585, 30: 0.4722222222222222, 31: 0.03333333333333333, 32: 0.539568345323741, 33: 0.13559322033898305, 34: 0.2558139534883721, 35: 0.22608695652173913, 36: 0.37777777777777777, 37: 0.09009009009009009, 38: 0.35443037974683544, 39: 0.16666666666666666, 40: 0.16058394160583941}
Micro-average F1 score: 0.35698234349919744
Weighted-average F1 score: 0.34989282625618523
F1 score per class: {0: 0.4714285714285714, 1: 0.18045112781954886, 2: 0.18421052631578946, 3: 0.2619047619047619, 4: 0.8636363636363636, 5: 0.7003610108303249, 6: 0.34782608695652173, 7: 0.03508771929824561, 8: 0.2857142857142857, 9: 0.6756756756756757, 10: 0.10810810810810811, 11: 0.19047619047619047, 12: 0.23214285714285715, 13: 0.08695652173913043, 14: 0.032520325203252036, 15: 0.3870967741935484, 16: 0.6176470588235294, 17: 0.0, 18: 0.0392156862745098, 19: 0.47191011235955055, 20: 0.4878048780487805, 21: 0.017699115044247787, 22: 0.37593984962406013, 23: 0.6373626373626373, 24: 0.0, 25: 0.43478260869565216, 26: 0.6604651162790698, 27: 0.0, 28: 0.1590909090909091, 29: 0.7980769230769231, 30: 0.6181818181818182, 31: 0.0392156862745098, 32: 0.5535055350553506, 33: 0.12903225806451613, 34: 0.3203463203463203, 35: 0.21782178217821782, 36: 0.344, 37: 0.11666666666666667, 38: 0.30985915492957744, 39: 0.12903225806451613, 40: 0.1509433962264151}
Micro-average F1 score: 0.3706763035622096
Weighted-average F1 score: 0.36247936566849887

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.47244094488188976, 9: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6535947712418301, 21: 0.0, 22: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.7894736842105263, 31: 0.0, 32: 0.0, 33: 0.2608695652173913, 34: 0.0, 36: 0.4057971014492754, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3423913043478261
Weighted-average F1 score: 0.26492275624646466
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.52, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6060606060606061, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.7906976744186046, 31: 0.0, 32: 0.0, 33: 0.36363636363636365, 34: 0.0, 35: 0.0, 36: 0.5454545454545454, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3289760348583878
Weighted-average F1 score: 0.2505523602019997
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.4927536231884058, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6015037593984962, 21: 0.0, 22: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.8095238095238095, 31: 0.0, 32: 0.0, 33: 0.34782608695652173, 34: 0.0, 35: 0.0, 36: 0.49142857142857144, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.3250883392226148
Weighted-average F1 score: 0.2458971334344021

F1 score per class: {0: 0.44755244755244755, 1: 0.09766454352441614, 2: 0.23333333333333334, 3: 0.20496894409937888, 4: 0.7978723404255319, 5: 0.6348122866894198, 6: 0.1875, 7: 0.020202020202020204, 8: 0.22388059701492538, 9: 0.6410256410256411, 10: 0.038461538461538464, 11: 0.10759493670886076, 12: 0.10894941634241245, 13: 0.08695652173913043, 14: 0.056074766355140186, 15: 0.20689655172413793, 16: 0.38461538461538464, 17: 0.0, 18: 0.0, 19: 0.456973293768546, 20: 0.21929824561403508, 21: 0.041237113402061855, 22: 0.3201663201663202, 23: 0.5360824742268041, 24: 0.0, 25: 0.3125, 26: 0.5737704918032787, 27: 0.0, 28: 0.039603960396039604, 29: 0.6188679245283019, 30: 0.75, 31: 0.03389830508474576, 32: 0.4297994269340974, 33: 0.05454545454545454, 34: 0.17801047120418848, 35: 0.16143497757847533, 36: 0.20437956204379562, 37: 0.1509433962264151, 38: 0.1016949152542373, 39: 0.037037037037037035, 40: 0.16363636363636364}
Micro-average F1 score: 0.26940888041405614
Weighted-average F1 score: 0.25570224781378753
F1 score per class: {0: 0.2894736842105263, 1: 0.09177820267686425, 2: 0.08536585365853659, 3: 0.17258883248730963, 4: 0.7872340425531915, 5: 0.3559633027522936, 6: 0.20307692307692307, 7: 0.03418803418803419, 8: 0.19746835443037974, 9: 0.3401360544217687, 10: 0.109375, 11: 0.17054263565891473, 12: 0.12048192771084337, 13: 0.044444444444444446, 14: 0.043795620437956206, 15: 0.375, 16: 0.37398373983739835, 17: 0.0, 18: 0.02531645569620253, 19: 0.3952941176470588, 20: 0.2846975088967972, 21: 0.019801980198019802, 22: 0.24203821656050956, 23: 0.4225352112676056, 24: 0.0, 25: 0.425, 26: 0.5393258426966292, 27: 0.0, 28: 0.08333333333333333, 29: 0.6264150943396226, 30: 0.35789473684210527, 31: 0.01904761904761905, 32: 0.3978779840848806, 33: 0.0851063829787234, 34: 0.15602836879432624, 35: 0.14285714285714285, 36: 0.24170616113744076, 37: 0.06622516556291391, 38: 0.21705426356589147, 39: 0.09174311926605505, 40: 0.1317365269461078}
Micro-average F1 score: 0.24252998909487458
Weighted-average F1 score: 0.23258519590884144
F1 score per class: {0: 0.34554973821989526, 1: 0.096579476861167, 2: 0.12727272727272726, 3: 0.17142857142857143, 4: 0.8172043010752689, 5: 0.48866498740554154, 6: 0.20711974110032363, 7: 0.01834862385321101, 8: 0.1894150417827298, 9: 0.5681818181818182, 10: 0.09917355371900827, 11: 0.16470588235294117, 12: 0.12351543942992874, 13: 0.04, 14: 0.0273972602739726, 15: 0.22641509433962265, 16: 0.3652173913043478, 17: 0.0, 18: 0.024390243902439025, 19: 0.42, 20: 0.25316455696202533, 21: 0.015267175572519083, 22: 0.2568493150684932, 23: 0.5178571428571429, 24: 0.0, 25: 0.410958904109589, 26: 0.5612648221343873, 27: 0.0, 28: 0.08536585365853659, 29: 0.6264150943396226, 30: 0.4927536231884058, 31: 0.022222222222222223, 32: 0.4076086956521739, 33: 0.0761904761904762, 34: 0.1942257217847769, 35: 0.13707165109034267, 36: 0.2193877551020408, 37: 0.08860759493670886, 38: 0.1896551724137931, 39: 0.07142857142857142, 40: 0.12435233160621761}
Micro-average F1 score: 0.2547604967474867
Weighted-average F1 score: 0.24273511757918176
cur_acc_wo_na:  ['0.7639', '0.3206', '0.5978', '0.5038', '0.4891', '0.3302', '0.3513', '0.4922']
his_acc_wo_na:  ['0.7639', '0.5041', '0.5563', '0.5610', '0.4717', '0.4287', '0.3882', '0.3793']
cur_acc des_wo_na:  ['0.7614', '0.3662', '0.5532', '0.5163', '0.4721', '0.3066', '0.3388', '0.4967']
his_acc des_wo_na:  ['0.7614', '0.5672', '0.5245', '0.5082', '0.4731', '0.4354', '0.3659', '0.3570']
cur_acc rrf_wo_na:  ['0.7631', '0.3755', '0.5797', '0.5089', '0.4797', '0.3290', '0.3575', '0.4876']
his_acc rrf_wo_na:  ['0.7631', '0.5720', '0.5346', '0.5222', '0.4729', '0.4466', '0.3803', '0.3707']
cur_acc_w_na:  ['0.6198', '0.2902', '0.4544', '0.3702', '0.3394', '0.2165', '0.2370', '0.3424']
his_acc_w_na:  ['0.6198', '0.4133', '0.4367', '0.4373', '0.3343', '0.2959', '0.2720', '0.2694']
cur_acc des_w_na:  ['0.6236', '0.3291', '0.4188', '0.3301', '0.3316', '0.2036', '0.2239', '0.3290']
his_acc des_w_na:  ['0.6236', '0.4701', '0.4018', '0.3593', '0.3306', '0.3050', '0.2493', '0.2425']
cur_acc rrf_w_na:  ['0.6263', '0.3410', '0.4413', '0.3257', '0.3393', '0.2195', '0.2361', '0.3251']
his_acc rrf_w_na:  ['0.6263', '0.4759', '0.4136', '0.3747', '0.3354', '0.3134', '0.2611', '0.2548']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 128.5957515CurrentTrain: epoch  0, batch     1 | loss: 81.1061273CurrentTrain: epoch  0, batch     2 | loss: 122.6673997CurrentTrain: epoch  0, batch     3 | loss: 101.5094963CurrentTrain: epoch  0, batch     4 | loss: 101.2465668CurrentTrain: epoch  0, batch     5 | loss: 87.2371363CurrentTrain: epoch  0, batch     6 | loss: 118.5034698CurrentTrain: epoch  0, batch     7 | loss: 99.9375571CurrentTrain: epoch  0, batch     8 | loss: 146.7485509CurrentTrain: epoch  0, batch     9 | loss: 100.1369365CurrentTrain: epoch  0, batch    10 | loss: 118.0586945CurrentTrain: epoch  0, batch    11 | loss: 118.6537816CurrentTrain: epoch  0, batch    12 | loss: 100.5065029CurrentTrain: epoch  0, batch    13 | loss: 119.3034641CurrentTrain: epoch  0, batch    14 | loss: 99.9371605CurrentTrain: epoch  0, batch    15 | loss: 192.8493944CurrentTrain: epoch  0, batch    16 | loss: 118.8880254CurrentTrain: epoch  0, batch    17 | loss: 118.2655134CurrentTrain: epoch  0, batch    18 | loss: 117.6319162CurrentTrain: epoch  0, batch    19 | loss: 99.5577795CurrentTrain: epoch  0, batch    20 | loss: 86.5459397CurrentTrain: epoch  0, batch    21 | loss: 87.5489414CurrentTrain: epoch  0, batch    22 | loss: 99.0582159CurrentTrain: epoch  0, batch    23 | loss: 98.4303701CurrentTrain: epoch  0, batch    24 | loss: 85.2954262CurrentTrain: epoch  0, batch    25 | loss: 116.7952007CurrentTrain: epoch  0, batch    26 | loss: 98.2672903CurrentTrain: epoch  0, batch    27 | loss: 75.9294346CurrentTrain: epoch  0, batch    28 | loss: 117.6486611CurrentTrain: epoch  0, batch    29 | loss: 85.5170540CurrentTrain: epoch  0, batch    30 | loss: 117.2767527CurrentTrain: epoch  0, batch    31 | loss: 84.8510291CurrentTrain: epoch  0, batch    32 | loss: 74.9021276CurrentTrain: epoch  0, batch    33 | loss: 191.3920540CurrentTrain: epoch  0, batch    34 | loss: 84.0210334CurrentTrain: epoch  0, batch    35 | loss: 85.2829436CurrentTrain: epoch  0, batch    36 | loss: 116.7911202CurrentTrain: epoch  0, batch    37 | loss: 74.6431758CurrentTrain: epoch  0, batch    38 | loss: 96.2826127CurrentTrain: epoch  0, batch    39 | loss: 96.7836048CurrentTrain: epoch  0, batch    40 | loss: 97.3341083CurrentTrain: epoch  0, batch    41 | loss: 73.7195572CurrentTrain: epoch  0, batch    42 | loss: 190.2322667CurrentTrain: epoch  0, batch    43 | loss: 73.6614997CurrentTrain: epoch  0, batch    44 | loss: 115.1039617CurrentTrain: epoch  0, batch    45 | loss: 82.2421807CurrentTrain: epoch  0, batch    46 | loss: 96.6747077CurrentTrain: epoch  0, batch    47 | loss: 114.9526676CurrentTrain: epoch  0, batch    48 | loss: 82.9243595CurrentTrain: epoch  0, batch    49 | loss: 95.5564505CurrentTrain: epoch  0, batch    50 | loss: 94.7368178CurrentTrain: epoch  0, batch    51 | loss: 115.4177689CurrentTrain: epoch  0, batch    52 | loss: 82.9030239CurrentTrain: epoch  0, batch    53 | loss: 83.3459701CurrentTrain: epoch  0, batch    54 | loss: 97.3042326CurrentTrain: epoch  0, batch    55 | loss: 81.4522479CurrentTrain: epoch  0, batch    56 | loss: 81.5102470CurrentTrain: epoch  0, batch    57 | loss: 111.0403335CurrentTrain: epoch  0, batch    58 | loss: 112.5133405CurrentTrain: epoch  0, batch    59 | loss: 142.6869510CurrentTrain: epoch  0, batch    60 | loss: 93.4189479CurrentTrain: epoch  0, batch    61 | loss: 94.4025340CurrentTrain: epoch  0, batch    62 | loss: 93.3553502CurrentTrain: epoch  0, batch    63 | loss: 78.7496544CurrentTrain: epoch  0, batch    64 | loss: 95.1705875CurrentTrain: epoch  0, batch    65 | loss: 90.2014941CurrentTrain: epoch  0, batch    66 | loss: 80.9206649CurrentTrain: epoch  0, batch    67 | loss: 70.3252268CurrentTrain: epoch  0, batch    68 | loss: 78.5670058CurrentTrain: epoch  0, batch    69 | loss: 96.9289395CurrentTrain: epoch  0, batch    70 | loss: 112.8771144CurrentTrain: epoch  0, batch    71 | loss: 112.2472379CurrentTrain: epoch  0, batch    72 | loss: 76.6453543CurrentTrain: epoch  0, batch    73 | loss: 106.7125012CurrentTrain: epoch  0, batch    74 | loss: 75.3268115CurrentTrain: epoch  0, batch    75 | loss: 94.3784568CurrentTrain: epoch  0, batch    76 | loss: 141.2199318CurrentTrain: epoch  0, batch    77 | loss: 186.6789119CurrentTrain: epoch  0, batch    78 | loss: 79.8456082CurrentTrain: epoch  0, batch    79 | loss: 80.2955564CurrentTrain: epoch  0, batch    80 | loss: 138.5439279CurrentTrain: epoch  0, batch    81 | loss: 81.5055469CurrentTrain: epoch  0, batch    82 | loss: 143.0149163CurrentTrain: epoch  0, batch    83 | loss: 186.8479843CurrentTrain: epoch  0, batch    84 | loss: 82.5442100CurrentTrain: epoch  0, batch    85 | loss: 68.2358811CurrentTrain: epoch  0, batch    86 | loss: 89.6606804CurrentTrain: epoch  0, batch    87 | loss: 67.1352395CurrentTrain: epoch  0, batch    88 | loss: 113.8509717CurrentTrain: epoch  0, batch    89 | loss: 78.1941962CurrentTrain: epoch  0, batch    90 | loss: 91.8595313CurrentTrain: epoch  0, batch    91 | loss: 111.1730073CurrentTrain: epoch  0, batch    92 | loss: 93.9465174CurrentTrain: epoch  0, batch    93 | loss: 112.9088933CurrentTrain: epoch  0, batch    94 | loss: 87.2871602CurrentTrain: epoch  0, batch    95 | loss: 76.5005541CurrentTrain: epoch  1, batch     0 | loss: 81.3191887CurrentTrain: epoch  1, batch     1 | loss: 90.9917163CurrentTrain: epoch  1, batch     2 | loss: 68.9217282CurrentTrain: epoch  1, batch     3 | loss: 139.1103359CurrentTrain: epoch  1, batch     4 | loss: 110.7194745CurrentTrain: epoch  1, batch     5 | loss: 91.1055312CurrentTrain: epoch  1, batch     6 | loss: 138.2654626CurrentTrain: epoch  1, batch     7 | loss: 74.9458800CurrentTrain: epoch  1, batch     8 | loss: 89.6815004CurrentTrain: epoch  1, batch     9 | loss: 90.9742088CurrentTrain: epoch  1, batch    10 | loss: 87.5010772CurrentTrain: epoch  1, batch    11 | loss: 90.0973671CurrentTrain: epoch  1, batch    12 | loss: 184.5891169CurrentTrain: epoch  1, batch    13 | loss: 78.9418325CurrentTrain: epoch  1, batch    14 | loss: 73.8520234CurrentTrain: epoch  1, batch    15 | loss: 91.6743901CurrentTrain: epoch  1, batch    16 | loss: 66.6258101CurrentTrain: epoch  1, batch    17 | loss: 78.1676961CurrentTrain: epoch  1, batch    18 | loss: 62.6435195CurrentTrain: epoch  1, batch    19 | loss: 108.8880583CurrentTrain: epoch  1, batch    20 | loss: 84.7915417CurrentTrain: epoch  1, batch    21 | loss: 91.5391812CurrentTrain: epoch  1, batch    22 | loss: 74.4863191CurrentTrain: epoch  1, batch    23 | loss: 109.7281797CurrentTrain: epoch  1, batch    24 | loss: 93.0535442CurrentTrain: epoch  1, batch    25 | loss: 102.9942749CurrentTrain: epoch  1, batch    26 | loss: 101.7695104CurrentTrain: epoch  1, batch    27 | loss: 89.6264479CurrentTrain: epoch  1, batch    28 | loss: 90.8907992CurrentTrain: epoch  1, batch    29 | loss: 89.5796905CurrentTrain: epoch  1, batch    30 | loss: 88.3663415CurrentTrain: epoch  1, batch    31 | loss: 106.9653089CurrentTrain: epoch  1, batch    32 | loss: 75.8137335CurrentTrain: epoch  1, batch    33 | loss: 106.0969837CurrentTrain: epoch  1, batch    34 | loss: 88.1548925CurrentTrain: epoch  1, batch    35 | loss: 87.3361692CurrentTrain: epoch  1, batch    36 | loss: 78.7861572CurrentTrain: epoch  1, batch    37 | loss: 85.4981180CurrentTrain: epoch  1, batch    38 | loss: 66.5374139CurrentTrain: epoch  1, batch    39 | loss: 108.6984021CurrentTrain: epoch  1, batch    40 | loss: 106.7459343CurrentTrain: epoch  1, batch    41 | loss: 107.0169904CurrentTrain: epoch  1, batch    42 | loss: 89.2363909CurrentTrain: epoch  1, batch    43 | loss: 86.9009737CurrentTrain: epoch  1, batch    44 | loss: 90.5665278CurrentTrain: epoch  1, batch    45 | loss: 134.5937696CurrentTrain: epoch  1, batch    46 | loss: 83.0442019CurrentTrain: epoch  1, batch    47 | loss: 103.1420432CurrentTrain: epoch  1, batch    48 | loss: 108.1763052CurrentTrain: epoch  1, batch    49 | loss: 108.2581145CurrentTrain: epoch  1, batch    50 | loss: 78.6655758CurrentTrain: epoch  1, batch    51 | loss: 111.6429825CurrentTrain: epoch  1, batch    52 | loss: 86.8999618CurrentTrain: epoch  1, batch    53 | loss: 92.3448138CurrentTrain: epoch  1, batch    54 | loss: 140.3761017CurrentTrain: epoch  1, batch    55 | loss: 83.4494595CurrentTrain: epoch  1, batch    56 | loss: 71.3386468CurrentTrain: epoch  1, batch    57 | loss: 106.9181270CurrentTrain: epoch  1, batch    58 | loss: 88.0392710CurrentTrain: epoch  1, batch    59 | loss: 85.4170374CurrentTrain: epoch  1, batch    60 | loss: 71.1704987CurrentTrain: epoch  1, batch    61 | loss: 90.0445491CurrentTrain: epoch  1, batch    62 | loss: 103.9736929CurrentTrain: epoch  1, batch    63 | loss: 86.1257574CurrentTrain: epoch  1, batch    64 | loss: 86.8335679CurrentTrain: epoch  1, batch    65 | loss: 77.3989620CurrentTrain: epoch  1, batch    66 | loss: 88.9840404CurrentTrain: epoch  1, batch    67 | loss: 74.7397070CurrentTrain: epoch  1, batch    68 | loss: 76.0390872CurrentTrain: epoch  1, batch    69 | loss: 107.4013429CurrentTrain: epoch  1, batch    70 | loss: 105.3505859CurrentTrain: epoch  1, batch    71 | loss: 74.7975818CurrentTrain: epoch  1, batch    72 | loss: 182.1555739CurrentTrain: epoch  1, batch    73 | loss: 64.6409418CurrentTrain: epoch  1, batch    74 | loss: 75.8706597CurrentTrain: epoch  1, batch    75 | loss: 106.8738376CurrentTrain: epoch  1, batch    76 | loss: 107.4878434CurrentTrain: epoch  1, batch    77 | loss: 106.7784787CurrentTrain: epoch  1, batch    78 | loss: 104.8448900CurrentTrain: epoch  1, batch    79 | loss: 73.7467752CurrentTrain: epoch  1, batch    80 | loss: 91.1591368CurrentTrain: epoch  1, batch    81 | loss: 63.9984720CurrentTrain: epoch  1, batch    82 | loss: 77.7334473CurrentTrain: epoch  1, batch    83 | loss: 74.2260856CurrentTrain: epoch  1, batch    84 | loss: 83.6795504CurrentTrain: epoch  1, batch    85 | loss: 107.2197605CurrentTrain: epoch  1, batch    86 | loss: 92.0134692CurrentTrain: epoch  1, batch    87 | loss: 90.0968348CurrentTrain: epoch  1, batch    88 | loss: 109.3482819CurrentTrain: epoch  1, batch    89 | loss: 182.6260937CurrentTrain: epoch  1, batch    90 | loss: 87.1585135CurrentTrain: epoch  1, batch    91 | loss: 138.4195233CurrentTrain: epoch  1, batch    92 | loss: 109.3571640CurrentTrain: epoch  1, batch    93 | loss: 72.3395536CurrentTrain: epoch  1, batch    94 | loss: 104.5511391CurrentTrain: epoch  1, batch    95 | loss: 74.9610078CurrentTrain: epoch  2, batch     0 | loss: 105.4671712CurrentTrain: epoch  2, batch     1 | loss: 130.7394548CurrentTrain: epoch  2, batch     2 | loss: 87.5906103CurrentTrain: epoch  2, batch     3 | loss: 110.1220182CurrentTrain: epoch  2, batch     4 | loss: 86.8613919CurrentTrain: epoch  2, batch     5 | loss: 90.8324736CurrentTrain: epoch  2, batch     6 | loss: 86.7064739CurrentTrain: epoch  2, batch     7 | loss: 87.9577274CurrentTrain: epoch  2, batch     8 | loss: 87.4091689CurrentTrain: epoch  2, batch     9 | loss: 69.3420297CurrentTrain: epoch  2, batch    10 | loss: 103.0952454CurrentTrain: epoch  2, batch    11 | loss: 65.1639230CurrentTrain: epoch  2, batch    12 | loss: 134.2333970CurrentTrain: epoch  2, batch    13 | loss: 72.5726962CurrentTrain: epoch  2, batch    14 | loss: 72.4429175CurrentTrain: epoch  2, batch    15 | loss: 71.8033429CurrentTrain: epoch  2, batch    16 | loss: 104.7307378CurrentTrain: epoch  2, batch    17 | loss: 88.2176099CurrentTrain: epoch  2, batch    18 | loss: 107.1742574CurrentTrain: epoch  2, batch    19 | loss: 84.9080070CurrentTrain: epoch  2, batch    20 | loss: 104.5458804CurrentTrain: epoch  2, batch    21 | loss: 87.3432051CurrentTrain: epoch  2, batch    22 | loss: 138.8434556CurrentTrain: epoch  2, batch    23 | loss: 102.4930286CurrentTrain: epoch  2, batch    24 | loss: 88.7712548CurrentTrain: epoch  2, batch    25 | loss: 104.9831763CurrentTrain: epoch  2, batch    26 | loss: 105.9137289CurrentTrain: epoch  2, batch    27 | loss: 89.3274554CurrentTrain: epoch  2, batch    28 | loss: 92.5038449CurrentTrain: epoch  2, batch    29 | loss: 108.1936331CurrentTrain: epoch  2, batch    30 | loss: 73.6187851CurrentTrain: epoch  2, batch    31 | loss: 77.6288584CurrentTrain: epoch  2, batch    32 | loss: 86.7289433CurrentTrain: epoch  2, batch    33 | loss: 75.1872561CurrentTrain: epoch  2, batch    34 | loss: 88.8535205CurrentTrain: epoch  2, batch    35 | loss: 105.5862146CurrentTrain: epoch  2, batch    36 | loss: 84.2270714CurrentTrain: epoch  2, batch    37 | loss: 70.8468016CurrentTrain: epoch  2, batch    38 | loss: 64.1107508CurrentTrain: epoch  2, batch    39 | loss: 71.7485882CurrentTrain: epoch  2, batch    40 | loss: 84.8182624CurrentTrain: epoch  2, batch    41 | loss: 87.2666762CurrentTrain: epoch  2, batch    42 | loss: 85.9568554CurrentTrain: epoch  2, batch    43 | loss: 87.0854095CurrentTrain: epoch  2, batch    44 | loss: 88.3752758CurrentTrain: epoch  2, batch    45 | loss: 85.0381584CurrentTrain: epoch  2, batch    46 | loss: 84.1488729CurrentTrain: epoch  2, batch    47 | loss: 131.2606784CurrentTrain: epoch  2, batch    48 | loss: 133.7073976CurrentTrain: epoch  2, batch    49 | loss: 68.2066287CurrentTrain: epoch  2, batch    50 | loss: 62.6471212CurrentTrain: epoch  2, batch    51 | loss: 73.1306348CurrentTrain: epoch  2, batch    52 | loss: 72.8808986CurrentTrain: epoch  2, batch    53 | loss: 69.5818452CurrentTrain: epoch  2, batch    54 | loss: 82.7419934CurrentTrain: epoch  2, batch    55 | loss: 102.9704002CurrentTrain: epoch  2, batch    56 | loss: 71.9825144CurrentTrain: epoch  2, batch    57 | loss: 102.9500013CurrentTrain: epoch  2, batch    58 | loss: 132.4467645CurrentTrain: epoch  2, batch    59 | loss: 71.1371962CurrentTrain: epoch  2, batch    60 | loss: 88.4416002CurrentTrain: epoch  2, batch    61 | loss: 88.2580946CurrentTrain: epoch  2, batch    62 | loss: 71.8927221CurrentTrain: epoch  2, batch    63 | loss: 75.2766902CurrentTrain: epoch  2, batch    64 | loss: 80.0088147CurrentTrain: epoch  2, batch    65 | loss: 60.1575427CurrentTrain: epoch  2, batch    66 | loss: 103.8141315CurrentTrain: epoch  2, batch    67 | loss: 71.5610867CurrentTrain: epoch  2, batch    68 | loss: 88.3916671CurrentTrain: epoch  2, batch    69 | loss: 87.4533292CurrentTrain: epoch  2, batch    70 | loss: 90.9783313CurrentTrain: epoch  2, batch    71 | loss: 129.1676754CurrentTrain: epoch  2, batch    72 | loss: 60.6973864CurrentTrain: epoch  2, batch    73 | loss: 103.9523423CurrentTrain: epoch  2, batch    74 | loss: 61.3777242CurrentTrain: epoch  2, batch    75 | loss: 85.7803861CurrentTrain: epoch  2, batch    76 | loss: 69.2125187CurrentTrain: epoch  2, batch    77 | loss: 64.8081795CurrentTrain: epoch  2, batch    78 | loss: 133.3533445CurrentTrain: epoch  2, batch    79 | loss: 104.8664587CurrentTrain: epoch  2, batch    80 | loss: 64.0514196CurrentTrain: epoch  2, batch    81 | loss: 77.6717962CurrentTrain: epoch  2, batch    82 | loss: 63.2364439CurrentTrain: epoch  2, batch    83 | loss: 85.2957588CurrentTrain: epoch  2, batch    84 | loss: 103.8134270CurrentTrain: epoch  2, batch    85 | loss: 70.3036235CurrentTrain: epoch  2, batch    86 | loss: 64.9508220CurrentTrain: epoch  2, batch    87 | loss: 84.3476144CurrentTrain: epoch  2, batch    88 | loss: 136.0244509CurrentTrain: epoch  2, batch    89 | loss: 74.5383113CurrentTrain: epoch  2, batch    90 | loss: 105.3529835CurrentTrain: epoch  2, batch    91 | loss: 107.8432707CurrentTrain: epoch  2, batch    92 | loss: 83.0984322CurrentTrain: epoch  2, batch    93 | loss: 88.8477485CurrentTrain: epoch  2, batch    94 | loss: 106.2345875CurrentTrain: epoch  2, batch    95 | loss: 72.5066197CurrentTrain: epoch  3, batch     0 | loss: 104.6531147CurrentTrain: epoch  3, batch     1 | loss: 82.5922956CurrentTrain: epoch  3, batch     2 | loss: 84.9100761CurrentTrain: epoch  3, batch     3 | loss: 73.4302966CurrentTrain: epoch  3, batch     4 | loss: 72.3991648CurrentTrain: epoch  3, batch     5 | loss: 82.7819505CurrentTrain: epoch  3, batch     6 | loss: 87.0443005CurrentTrain: epoch  3, batch     7 | loss: 125.5326421CurrentTrain: epoch  3, batch     8 | loss: 60.0715984CurrentTrain: epoch  3, batch     9 | loss: 106.6450416CurrentTrain: epoch  3, batch    10 | loss: 77.5049798CurrentTrain: epoch  3, batch    11 | loss: 69.0070626CurrentTrain: epoch  3, batch    12 | loss: 126.7326396CurrentTrain: epoch  3, batch    13 | loss: 72.1907521CurrentTrain: epoch  3, batch    14 | loss: 175.8593547CurrentTrain: epoch  3, batch    15 | loss: 132.2265564CurrentTrain: epoch  3, batch    16 | loss: 100.1129566CurrentTrain: epoch  3, batch    17 | loss: 85.5572604CurrentTrain: epoch  3, batch    18 | loss: 69.0313754CurrentTrain: epoch  3, batch    19 | loss: 71.9233083CurrentTrain: epoch  3, batch    20 | loss: 126.2102859CurrentTrain: epoch  3, batch    21 | loss: 69.3134378CurrentTrain: epoch  3, batch    22 | loss: 70.5957592CurrentTrain: epoch  3, batch    23 | loss: 73.9320059CurrentTrain: epoch  3, batch    24 | loss: 83.6549477CurrentTrain: epoch  3, batch    25 | loss: 89.1708664CurrentTrain: epoch  3, batch    26 | loss: 65.4141203CurrentTrain: epoch  3, batch    27 | loss: 65.8700528CurrentTrain: epoch  3, batch    28 | loss: 85.9906757CurrentTrain: epoch  3, batch    29 | loss: 67.6882904CurrentTrain: epoch  3, batch    30 | loss: 72.3118320CurrentTrain: epoch  3, batch    31 | loss: 83.3515469CurrentTrain: epoch  3, batch    32 | loss: 86.4487449CurrentTrain: epoch  3, batch    33 | loss: 81.0366237CurrentTrain: epoch  3, batch    34 | loss: 85.2091225CurrentTrain: epoch  3, batch    35 | loss: 82.9582723CurrentTrain: epoch  3, batch    36 | loss: 81.9209744CurrentTrain: epoch  3, batch    37 | loss: 101.8012702CurrentTrain: epoch  3, batch    38 | loss: 111.3419389CurrentTrain: epoch  3, batch    39 | loss: 86.5142230CurrentTrain: epoch  3, batch    40 | loss: 62.4237397CurrentTrain: epoch  3, batch    41 | loss: 103.1322335CurrentTrain: epoch  3, batch    42 | loss: 88.9839880CurrentTrain: epoch  3, batch    43 | loss: 72.4985865CurrentTrain: epoch  3, batch    44 | loss: 86.5186193CurrentTrain: epoch  3, batch    45 | loss: 85.8893504CurrentTrain: epoch  3, batch    46 | loss: 72.9597901CurrentTrain: epoch  3, batch    47 | loss: 80.9081113CurrentTrain: epoch  3, batch    48 | loss: 66.4588441CurrentTrain: epoch  3, batch    49 | loss: 75.5959886CurrentTrain: epoch  3, batch    50 | loss: 69.9818384CurrentTrain: epoch  3, batch    51 | loss: 80.6205225CurrentTrain: epoch  3, batch    52 | loss: 86.9510962CurrentTrain: epoch  3, batch    53 | loss: 83.1353249CurrentTrain: epoch  3, batch    54 | loss: 73.6354738CurrentTrain: epoch  3, batch    55 | loss: 135.6931697CurrentTrain: epoch  3, batch    56 | loss: 81.0942192CurrentTrain: epoch  3, batch    57 | loss: 98.3188332CurrentTrain: epoch  3, batch    58 | loss: 60.6082548CurrentTrain: epoch  3, batch    59 | loss: 100.9424060CurrentTrain: epoch  3, batch    60 | loss: 84.4624127CurrentTrain: epoch  3, batch    61 | loss: 84.5394723CurrentTrain: epoch  3, batch    62 | loss: 182.6895454CurrentTrain: epoch  3, batch    63 | loss: 85.0228520CurrentTrain: epoch  3, batch    64 | loss: 83.5836217CurrentTrain: epoch  3, batch    65 | loss: 105.1453245CurrentTrain: epoch  3, batch    66 | loss: 78.6027940CurrentTrain: epoch  3, batch    67 | loss: 68.2980939CurrentTrain: epoch  3, batch    68 | loss: 72.3321772CurrentTrain: epoch  3, batch    69 | loss: 71.3742916CurrentTrain: epoch  3, batch    70 | loss: 57.8327365CurrentTrain: epoch  3, batch    71 | loss: 80.0284632CurrentTrain: epoch  3, batch    72 | loss: 79.0298386CurrentTrain: epoch  3, batch    73 | loss: 86.2757373CurrentTrain: epoch  3, batch    74 | loss: 107.7229913CurrentTrain: epoch  3, batch    75 | loss: 85.1750852CurrentTrain: epoch  3, batch    76 | loss: 87.0297675CurrentTrain: epoch  3, batch    77 | loss: 86.9439136CurrentTrain: epoch  3, batch    78 | loss: 73.3381367CurrentTrain: epoch  3, batch    79 | loss: 103.8505422CurrentTrain: epoch  3, batch    80 | loss: 71.2446088CurrentTrain: epoch  3, batch    81 | loss: 100.1323114CurrentTrain: epoch  3, batch    82 | loss: 101.7428856CurrentTrain: epoch  3, batch    83 | loss: 64.5980087CurrentTrain: epoch  3, batch    84 | loss: 88.2950686CurrentTrain: epoch  3, batch    85 | loss: 128.9753872CurrentTrain: epoch  3, batch    86 | loss: 72.3600834CurrentTrain: epoch  3, batch    87 | loss: 101.3126164CurrentTrain: epoch  3, batch    88 | loss: 105.7211898CurrentTrain: epoch  3, batch    89 | loss: 80.3764092CurrentTrain: epoch  3, batch    90 | loss: 81.2318410CurrentTrain: epoch  3, batch    91 | loss: 85.2638232CurrentTrain: epoch  3, batch    92 | loss: 102.5869365CurrentTrain: epoch  3, batch    93 | loss: 74.9023503CurrentTrain: epoch  3, batch    94 | loss: 68.3132802CurrentTrain: epoch  3, batch    95 | loss: 75.8460064CurrentTrain: epoch  4, batch     0 | loss: 82.0769249CurrentTrain: epoch  4, batch     1 | loss: 61.6173443CurrentTrain: epoch  4, batch     2 | loss: 66.4093576CurrentTrain: epoch  4, batch     3 | loss: 169.2610289CurrentTrain: epoch  4, batch     4 | loss: 98.0778257CurrentTrain: epoch  4, batch     5 | loss: 76.8775141CurrentTrain: epoch  4, batch     6 | loss: 73.9378273CurrentTrain: epoch  4, batch     7 | loss: 80.8176837CurrentTrain: epoch  4, batch     8 | loss: 98.3552596CurrentTrain: epoch  4, batch     9 | loss: 70.8907166CurrentTrain: epoch  4, batch    10 | loss: 102.1255480CurrentTrain: epoch  4, batch    11 | loss: 71.0126507CurrentTrain: epoch  4, batch    12 | loss: 83.9251783CurrentTrain: epoch  4, batch    13 | loss: 70.0060420CurrentTrain: epoch  4, batch    14 | loss: 100.0063733CurrentTrain: epoch  4, batch    15 | loss: 104.2345430CurrentTrain: epoch  4, batch    16 | loss: 133.0051954CurrentTrain: epoch  4, batch    17 | loss: 125.2975863CurrentTrain: epoch  4, batch    18 | loss: 84.1600534CurrentTrain: epoch  4, batch    19 | loss: 69.7848528CurrentTrain: epoch  4, batch    20 | loss: 69.0437951CurrentTrain: epoch  4, batch    21 | loss: 126.9534668CurrentTrain: epoch  4, batch    22 | loss: 104.5391911CurrentTrain: epoch  4, batch    23 | loss: 81.4918007CurrentTrain: epoch  4, batch    24 | loss: 101.1021232CurrentTrain: epoch  4, batch    25 | loss: 65.9699377CurrentTrain: epoch  4, batch    26 | loss: 85.9901374CurrentTrain: epoch  4, batch    27 | loss: 101.0089044CurrentTrain: epoch  4, batch    28 | loss: 125.4849557CurrentTrain: epoch  4, batch    29 | loss: 78.5863560CurrentTrain: epoch  4, batch    30 | loss: 83.9173101CurrentTrain: epoch  4, batch    31 | loss: 68.1153483CurrentTrain: epoch  4, batch    32 | loss: 87.0929857CurrentTrain: epoch  4, batch    33 | loss: 61.8112974CurrentTrain: epoch  4, batch    34 | loss: 103.1396700CurrentTrain: epoch  4, batch    35 | loss: 84.0773870CurrentTrain: epoch  4, batch    36 | loss: 105.4455452CurrentTrain: epoch  4, batch    37 | loss: 75.3774760CurrentTrain: epoch  4, batch    38 | loss: 86.0508324CurrentTrain: epoch  4, batch    39 | loss: 65.7405132CurrentTrain: epoch  4, batch    40 | loss: 71.0413069CurrentTrain: epoch  4, batch    41 | loss: 83.6442568CurrentTrain: epoch  4, batch    42 | loss: 61.4050734CurrentTrain: epoch  4, batch    43 | loss: 103.3194966CurrentTrain: epoch  4, batch    44 | loss: 81.1826098CurrentTrain: epoch  4, batch    45 | loss: 97.8431585CurrentTrain: epoch  4, batch    46 | loss: 86.3292311CurrentTrain: epoch  4, batch    47 | loss: 126.4043631CurrentTrain: epoch  4, batch    48 | loss: 100.0229069CurrentTrain: epoch  4, batch    49 | loss: 68.0369064CurrentTrain: epoch  4, batch    50 | loss: 86.2470366CurrentTrain: epoch  4, batch    51 | loss: 129.5760083CurrentTrain: epoch  4, batch    52 | loss: 74.5360130CurrentTrain: epoch  4, batch    53 | loss: 84.5768094CurrentTrain: epoch  4, batch    54 | loss: 100.8592672CurrentTrain: epoch  4, batch    55 | loss: 97.8923991CurrentTrain: epoch  4, batch    56 | loss: 78.4892236CurrentTrain: epoch  4, batch    57 | loss: 86.8933408CurrentTrain: epoch  4, batch    58 | loss: 84.4201112CurrentTrain: epoch  4, batch    59 | loss: 69.9634210CurrentTrain: epoch  4, batch    60 | loss: 81.7305238CurrentTrain: epoch  4, batch    61 | loss: 63.5568416CurrentTrain: epoch  4, batch    62 | loss: 66.0082033CurrentTrain: epoch  4, batch    63 | loss: 86.3890527CurrentTrain: epoch  4, batch    64 | loss: 101.0522014CurrentTrain: epoch  4, batch    65 | loss: 67.5833850CurrentTrain: epoch  4, batch    66 | loss: 102.0883017CurrentTrain: epoch  4, batch    67 | loss: 69.1392933CurrentTrain: epoch  4, batch    68 | loss: 84.8685122CurrentTrain: epoch  4, batch    69 | loss: 76.9149086CurrentTrain: epoch  4, batch    70 | loss: 60.9217432CurrentTrain: epoch  4, batch    71 | loss: 103.7415555CurrentTrain: epoch  4, batch    72 | loss: 176.9437578CurrentTrain: epoch  4, batch    73 | loss: 68.2114905CurrentTrain: epoch  4, batch    74 | loss: 102.4032868CurrentTrain: epoch  4, batch    75 | loss: 74.8074170CurrentTrain: epoch  4, batch    76 | loss: 59.3516101CurrentTrain: epoch  4, batch    77 | loss: 84.3014780CurrentTrain: epoch  4, batch    78 | loss: 63.2647340CurrentTrain: epoch  4, batch    79 | loss: 83.9432910CurrentTrain: epoch  4, batch    80 | loss: 128.5342457CurrentTrain: epoch  4, batch    81 | loss: 84.2964749CurrentTrain: epoch  4, batch    82 | loss: 71.4633835CurrentTrain: epoch  4, batch    83 | loss: 128.9796966CurrentTrain: epoch  4, batch    84 | loss: 77.9729743CurrentTrain: epoch  4, batch    85 | loss: 103.9506446CurrentTrain: epoch  4, batch    86 | loss: 102.2817054CurrentTrain: epoch  4, batch    87 | loss: 83.3341528CurrentTrain: epoch  4, batch    88 | loss: 82.0102639CurrentTrain: epoch  4, batch    89 | loss: 70.7930619CurrentTrain: epoch  4, batch    90 | loss: 71.2705121CurrentTrain: epoch  4, batch    91 | loss: 135.1172109CurrentTrain: epoch  4, batch    92 | loss: 70.8885216CurrentTrain: epoch  4, batch    93 | loss: 70.4742754CurrentTrain: epoch  4, batch    94 | loss: 103.1622515CurrentTrain: epoch  4, batch    95 | loss: 46.3045957CurrentTrain: epoch  5, batch     0 | loss: 79.3169652CurrentTrain: epoch  5, batch     1 | loss: 63.1016069CurrentTrain: epoch  5, batch     2 | loss: 67.8382945CurrentTrain: epoch  5, batch     3 | loss: 125.7027986CurrentTrain: epoch  5, batch     4 | loss: 93.8411641CurrentTrain: epoch  5, batch     5 | loss: 94.1701547CurrentTrain: epoch  5, batch     6 | loss: 99.3827973CurrentTrain: epoch  5, batch     7 | loss: 78.2023614CurrentTrain: epoch  5, batch     8 | loss: 56.6872608CurrentTrain: epoch  5, batch     9 | loss: 103.2894720CurrentTrain: epoch  5, batch    10 | loss: 128.7767624CurrentTrain: epoch  5, batch    11 | loss: 79.2098174CurrentTrain: epoch  5, batch    12 | loss: 99.3141363CurrentTrain: epoch  5, batch    13 | loss: 72.3777850CurrentTrain: epoch  5, batch    14 | loss: 79.5853901CurrentTrain: epoch  5, batch    15 | loss: 81.2390313CurrentTrain: epoch  5, batch    16 | loss: 79.7036765CurrentTrain: epoch  5, batch    17 | loss: 80.6153285CurrentTrain: epoch  5, batch    18 | loss: 84.4893912CurrentTrain: epoch  5, batch    19 | loss: 124.1648444CurrentTrain: epoch  5, batch    20 | loss: 80.4395109CurrentTrain: epoch  5, batch    21 | loss: 100.2684499CurrentTrain: epoch  5, batch    22 | loss: 80.1772226CurrentTrain: epoch  5, batch    23 | loss: 92.7710869CurrentTrain: epoch  5, batch    24 | loss: 104.2511830CurrentTrain: epoch  5, batch    25 | loss: 81.1634769CurrentTrain: epoch  5, batch    26 | loss: 67.1554258CurrentTrain: epoch  5, batch    27 | loss: 96.3205113CurrentTrain: epoch  5, batch    28 | loss: 103.8561183CurrentTrain: epoch  5, batch    29 | loss: 71.0865837CurrentTrain: epoch  5, batch    30 | loss: 104.3858179CurrentTrain: epoch  5, batch    31 | loss: 81.5174993CurrentTrain: epoch  5, batch    32 | loss: 69.5361090CurrentTrain: epoch  5, batch    33 | loss: 85.7445237CurrentTrain: epoch  5, batch    34 | loss: 79.4067398CurrentTrain: epoch  5, batch    35 | loss: 79.3979490CurrentTrain: epoch  5, batch    36 | loss: 68.2588443CurrentTrain: epoch  5, batch    37 | loss: 70.6166604CurrentTrain: epoch  5, batch    38 | loss: 100.9713139CurrentTrain: epoch  5, batch    39 | loss: 56.8121913CurrentTrain: epoch  5, batch    40 | loss: 81.0037404CurrentTrain: epoch  5, batch    41 | loss: 58.4905888CurrentTrain: epoch  5, batch    42 | loss: 71.8368342CurrentTrain: epoch  5, batch    43 | loss: 56.1031832CurrentTrain: epoch  5, batch    44 | loss: 58.8061206CurrentTrain: epoch  5, batch    45 | loss: 85.5122099CurrentTrain: epoch  5, batch    46 | loss: 99.0289407CurrentTrain: epoch  5, batch    47 | loss: 102.5230850CurrentTrain: epoch  5, batch    48 | loss: 124.7874009CurrentTrain: epoch  5, batch    49 | loss: 99.6913153CurrentTrain: epoch  5, batch    50 | loss: 82.2731571CurrentTrain: epoch  5, batch    51 | loss: 82.9691437CurrentTrain: epoch  5, batch    52 | loss: 126.1973503CurrentTrain: epoch  5, batch    53 | loss: 87.4806371CurrentTrain: epoch  5, batch    54 | loss: 66.9495799CurrentTrain: epoch  5, batch    55 | loss: 101.9952346CurrentTrain: epoch  5, batch    56 | loss: 59.8514727CurrentTrain: epoch  5, batch    57 | loss: 96.7072153CurrentTrain: epoch  5, batch    58 | loss: 127.9054017CurrentTrain: epoch  5, batch    59 | loss: 94.9378836CurrentTrain: epoch  5, batch    60 | loss: 67.9726470CurrentTrain: epoch  5, batch    61 | loss: 71.4252894CurrentTrain: epoch  5, batch    62 | loss: 85.0458884CurrentTrain: epoch  5, batch    63 | loss: 63.9766915CurrentTrain: epoch  5, batch    64 | loss: 87.2580709CurrentTrain: epoch  5, batch    65 | loss: 77.4928901CurrentTrain: epoch  5, batch    66 | loss: 82.9242274CurrentTrain: epoch  5, batch    67 | loss: 83.6290282CurrentTrain: epoch  5, batch    68 | loss: 99.9178673CurrentTrain: epoch  5, batch    69 | loss: 65.0303699CurrentTrain: epoch  5, batch    70 | loss: 78.7065790CurrentTrain: epoch  5, batch    71 | loss: 126.0726438CurrentTrain: epoch  5, batch    72 | loss: 57.0133896CurrentTrain: epoch  5, batch    73 | loss: 59.9944983CurrentTrain: epoch  5, batch    74 | loss: 72.4724888CurrentTrain: epoch  5, batch    75 | loss: 64.3134744CurrentTrain: epoch  5, batch    76 | loss: 71.7726728CurrentTrain: epoch  5, batch    77 | loss: 72.7622105CurrentTrain: epoch  5, batch    78 | loss: 57.8404028CurrentTrain: epoch  5, batch    79 | loss: 68.2792039CurrentTrain: epoch  5, batch    80 | loss: 80.0902149CurrentTrain: epoch  5, batch    81 | loss: 100.7100946CurrentTrain: epoch  5, batch    82 | loss: 75.8896106CurrentTrain: epoch  5, batch    83 | loss: 124.4379375CurrentTrain: epoch  5, batch    84 | loss: 55.3757412CurrentTrain: epoch  5, batch    85 | loss: 99.0788566CurrentTrain: epoch  5, batch    86 | loss: 85.9235349CurrentTrain: epoch  5, batch    87 | loss: 128.3497479CurrentTrain: epoch  5, batch    88 | loss: 67.2806494CurrentTrain: epoch  5, batch    89 | loss: 100.8799148CurrentTrain: epoch  5, batch    90 | loss: 130.2101948CurrentTrain: epoch  5, batch    91 | loss: 99.2647145CurrentTrain: epoch  5, batch    92 | loss: 81.5993696CurrentTrain: epoch  5, batch    93 | loss: 72.2882249CurrentTrain: epoch  5, batch    94 | loss: 58.8403969CurrentTrain: epoch  5, batch    95 | loss: 80.6355734CurrentTrain: epoch  6, batch     0 | loss: 80.1999013CurrentTrain: epoch  6, batch     1 | loss: 81.3006158CurrentTrain: epoch  6, batch     2 | loss: 78.2854515CurrentTrain: epoch  6, batch     3 | loss: 79.8450331CurrentTrain: epoch  6, batch     4 | loss: 102.6742402CurrentTrain: epoch  6, batch     5 | loss: 79.7845719CurrentTrain: epoch  6, batch     6 | loss: 74.8292955CurrentTrain: epoch  6, batch     7 | loss: 81.9936311CurrentTrain: epoch  6, batch     8 | loss: 64.6914756CurrentTrain: epoch  6, batch     9 | loss: 97.5474933CurrentTrain: epoch  6, batch    10 | loss: 66.3555168CurrentTrain: epoch  6, batch    11 | loss: 66.5875669CurrentTrain: epoch  6, batch    12 | loss: 65.8492077CurrentTrain: epoch  6, batch    13 | loss: 100.5267460CurrentTrain: epoch  6, batch    14 | loss: 69.1958849CurrentTrain: epoch  6, batch    15 | loss: 69.1398002CurrentTrain: epoch  6, batch    16 | loss: 89.2299110CurrentTrain: epoch  6, batch    17 | loss: 99.0616611CurrentTrain: epoch  6, batch    18 | loss: 65.7992484CurrentTrain: epoch  6, batch    19 | loss: 126.3596360CurrentTrain: epoch  6, batch    20 | loss: 67.8107581CurrentTrain: epoch  6, batch    21 | loss: 55.1425629CurrentTrain: epoch  6, batch    22 | loss: 66.7757904CurrentTrain: epoch  6, batch    23 | loss: 69.3356574CurrentTrain: epoch  6, batch    24 | loss: 76.5674655CurrentTrain: epoch  6, batch    25 | loss: 98.1507842CurrentTrain: epoch  6, batch    26 | loss: 93.1604112CurrentTrain: epoch  6, batch    27 | loss: 77.4536517CurrentTrain: epoch  6, batch    28 | loss: 98.3010619CurrentTrain: epoch  6, batch    29 | loss: 80.3896626CurrentTrain: epoch  6, batch    30 | loss: 82.0021096CurrentTrain: epoch  6, batch    31 | loss: 101.8859116CurrentTrain: epoch  6, batch    32 | loss: 97.7917029CurrentTrain: epoch  6, batch    33 | loss: 81.8732830CurrentTrain: epoch  6, batch    34 | loss: 99.7698349CurrentTrain: epoch  6, batch    35 | loss: 84.0090176CurrentTrain: epoch  6, batch    36 | loss: 119.1502742CurrentTrain: epoch  6, batch    37 | loss: 101.5980354CurrentTrain: epoch  6, batch    38 | loss: 95.2334339CurrentTrain: epoch  6, batch    39 | loss: 58.4376574CurrentTrain: epoch  6, batch    40 | loss: 77.6250462CurrentTrain: epoch  6, batch    41 | loss: 79.4763520CurrentTrain: epoch  6, batch    42 | loss: 76.2592413CurrentTrain: epoch  6, batch    43 | loss: 120.6592209CurrentTrain: epoch  6, batch    44 | loss: 126.7851617CurrentTrain: epoch  6, batch    45 | loss: 98.3658866CurrentTrain: epoch  6, batch    46 | loss: 92.2342776CurrentTrain: epoch  6, batch    47 | loss: 64.0556044CurrentTrain: epoch  6, batch    48 | loss: 99.4106492CurrentTrain: epoch  6, batch    49 | loss: 97.0700435CurrentTrain: epoch  6, batch    50 | loss: 64.7202104CurrentTrain: epoch  6, batch    51 | loss: 83.9392044CurrentTrain: epoch  6, batch    52 | loss: 68.7340147CurrentTrain: epoch  6, batch    53 | loss: 85.3343353CurrentTrain: epoch  6, batch    54 | loss: 100.8042507CurrentTrain: epoch  6, batch    55 | loss: 81.7612521CurrentTrain: epoch  6, batch    56 | loss: 95.0727104CurrentTrain: epoch  6, batch    57 | loss: 97.6363093CurrentTrain: epoch  6, batch    58 | loss: 99.7266854CurrentTrain: epoch  6, batch    59 | loss: 65.1060391CurrentTrain: epoch  6, batch    60 | loss: 79.7242676CurrentTrain: epoch  6, batch    61 | loss: 126.9394450CurrentTrain: epoch  6, batch    62 | loss: 83.9948639CurrentTrain: epoch  6, batch    63 | loss: 66.4196657CurrentTrain: epoch  6, batch    64 | loss: 56.7851100CurrentTrain: epoch  6, batch    65 | loss: 67.6782422CurrentTrain: epoch  6, batch    66 | loss: 100.8581818CurrentTrain: epoch  6, batch    67 | loss: 81.3431488CurrentTrain: epoch  6, batch    68 | loss: 78.1071544CurrentTrain: epoch  6, batch    69 | loss: 60.7467558CurrentTrain: epoch  6, batch    70 | loss: 82.1076956CurrentTrain: epoch  6, batch    71 | loss: 99.2031742CurrentTrain: epoch  6, batch    72 | loss: 99.3117446CurrentTrain: epoch  6, batch    73 | loss: 101.2106218CurrentTrain: epoch  6, batch    74 | loss: 69.6055859CurrentTrain: epoch  6, batch    75 | loss: 71.0929690CurrentTrain: epoch  6, batch    76 | loss: 61.0099762CurrentTrain: epoch  6, batch    77 | loss: 98.9927995CurrentTrain: epoch  6, batch    78 | loss: 79.9676121CurrentTrain: epoch  6, batch    79 | loss: 58.3889729CurrentTrain: epoch  6, batch    80 | loss: 83.6790550CurrentTrain: epoch  6, batch    81 | loss: 101.9936708CurrentTrain: epoch  6, batch    82 | loss: 71.6626333CurrentTrain: epoch  6, batch    83 | loss: 96.9185849CurrentTrain: epoch  6, batch    84 | loss: 82.0677563CurrentTrain: epoch  6, batch    85 | loss: 63.3612116CurrentTrain: epoch  6, batch    86 | loss: 84.0241980CurrentTrain: epoch  6, batch    87 | loss: 94.5064548CurrentTrain: epoch  6, batch    88 | loss: 97.2250245CurrentTrain: epoch  6, batch    89 | loss: 104.4036271CurrentTrain: epoch  6, batch    90 | loss: 102.0097377CurrentTrain: epoch  6, batch    91 | loss: 101.8553818CurrentTrain: epoch  6, batch    92 | loss: 79.1658394CurrentTrain: epoch  6, batch    93 | loss: 79.4498231CurrentTrain: epoch  6, batch    94 | loss: 81.5931617CurrentTrain: epoch  6, batch    95 | loss: 71.4013916CurrentTrain: epoch  7, batch     0 | loss: 63.3974306CurrentTrain: epoch  7, batch     1 | loss: 122.5049944CurrentTrain: epoch  7, batch     2 | loss: 97.3601877CurrentTrain: epoch  7, batch     3 | loss: 67.7687959CurrentTrain: epoch  7, batch     4 | loss: 79.9316414CurrentTrain: epoch  7, batch     5 | loss: 103.2687358CurrentTrain: epoch  7, batch     6 | loss: 75.0486897CurrentTrain: epoch  7, batch     7 | loss: 54.4373252CurrentTrain: epoch  7, batch     8 | loss: 68.1209760CurrentTrain: epoch  7, batch     9 | loss: 67.3012747CurrentTrain: epoch  7, batch    10 | loss: 102.7820997CurrentTrain: epoch  7, batch    11 | loss: 79.1842513CurrentTrain: epoch  7, batch    12 | loss: 97.1008249CurrentTrain: epoch  7, batch    13 | loss: 77.0484632CurrentTrain: epoch  7, batch    14 | loss: 96.1237820CurrentTrain: epoch  7, batch    15 | loss: 68.8178793CurrentTrain: epoch  7, batch    16 | loss: 91.5298048CurrentTrain: epoch  7, batch    17 | loss: 95.9283189CurrentTrain: epoch  7, batch    18 | loss: 66.7488869CurrentTrain: epoch  7, batch    19 | loss: 66.6042156CurrentTrain: epoch  7, batch    20 | loss: 64.5580843CurrentTrain: epoch  7, batch    21 | loss: 121.9494524CurrentTrain: epoch  7, batch    22 | loss: 65.1690960CurrentTrain: epoch  7, batch    23 | loss: 69.5232076CurrentTrain: epoch  7, batch    24 | loss: 65.5784500CurrentTrain: epoch  7, batch    25 | loss: 70.0655023CurrentTrain: epoch  7, batch    26 | loss: 99.3805314CurrentTrain: epoch  7, batch    27 | loss: 97.1629814CurrentTrain: epoch  7, batch    28 | loss: 78.7877440CurrentTrain: epoch  7, batch    29 | loss: 97.5748511CurrentTrain: epoch  7, batch    30 | loss: 82.1329424CurrentTrain: epoch  7, batch    31 | loss: 68.2121969CurrentTrain: epoch  7, batch    32 | loss: 78.3058573CurrentTrain: epoch  7, batch    33 | loss: 97.4615083CurrentTrain: epoch  7, batch    34 | loss: 76.9869596CurrentTrain: epoch  7, batch    35 | loss: 80.4575591CurrentTrain: epoch  7, batch    36 | loss: 73.2195266CurrentTrain: epoch  7, batch    37 | loss: 122.4805419CurrentTrain: epoch  7, batch    38 | loss: 80.6916138CurrentTrain: epoch  7, batch    39 | loss: 65.3312547CurrentTrain: epoch  7, batch    40 | loss: 65.0498894CurrentTrain: epoch  7, batch    41 | loss: 83.1272407CurrentTrain: epoch  7, batch    42 | loss: 121.5866994CurrentTrain: epoch  7, batch    43 | loss: 77.9186107CurrentTrain: epoch  7, batch    44 | loss: 66.0992155CurrentTrain: epoch  7, batch    45 | loss: 99.9395895CurrentTrain: epoch  7, batch    46 | loss: 81.0656620CurrentTrain: epoch  7, batch    47 | loss: 65.1349235CurrentTrain: epoch  7, batch    48 | loss: 96.6028157CurrentTrain: epoch  7, batch    49 | loss: 99.6507614CurrentTrain: epoch  7, batch    50 | loss: 68.7775616CurrentTrain: epoch  7, batch    51 | loss: 94.3732340CurrentTrain: epoch  7, batch    52 | loss: 116.7627694CurrentTrain: epoch  7, batch    53 | loss: 64.8049457CurrentTrain: epoch  7, batch    54 | loss: 66.0908953CurrentTrain: epoch  7, batch    55 | loss: 67.1880548CurrentTrain: epoch  7, batch    56 | loss: 61.9247403CurrentTrain: epoch  7, batch    57 | loss: 127.6555343CurrentTrain: epoch  7, batch    58 | loss: 60.6301506CurrentTrain: epoch  7, batch    59 | loss: 64.1838838CurrentTrain: epoch  7, batch    60 | loss: 76.4768916CurrentTrain: epoch  7, batch    61 | loss: 100.5190842CurrentTrain: epoch  7, batch    62 | loss: 69.0797335CurrentTrain: epoch  7, batch    63 | loss: 131.0437392CurrentTrain: epoch  7, batch    64 | loss: 64.0915935CurrentTrain: epoch  7, batch    65 | loss: 84.8667860CurrentTrain: epoch  7, batch    66 | loss: 130.3796311CurrentTrain: epoch  7, batch    67 | loss: 81.1707870CurrentTrain: epoch  7, batch    68 | loss: 121.3586391CurrentTrain: epoch  7, batch    69 | loss: 102.4916885CurrentTrain: epoch  7, batch    70 | loss: 95.3025978CurrentTrain: epoch  7, batch    71 | loss: 83.8364814CurrentTrain: epoch  7, batch    72 | loss: 78.9108668CurrentTrain: epoch  7, batch    73 | loss: 69.2813009CurrentTrain: epoch  7, batch    74 | loss: 122.5606821CurrentTrain: epoch  7, batch    75 | loss: 99.0303592CurrentTrain: epoch  7, batch    76 | loss: 78.3787946CurrentTrain: epoch  7, batch    77 | loss: 54.7955147CurrentTrain: epoch  7, batch    78 | loss: 70.8223326CurrentTrain: epoch  7, batch    79 | loss: 80.7048172CurrentTrain: epoch  7, batch    80 | loss: 68.1407186CurrentTrain: epoch  7, batch    81 | loss: 100.3801888CurrentTrain: epoch  7, batch    82 | loss: 70.5915382CurrentTrain: epoch  7, batch    83 | loss: 95.1416090CurrentTrain: epoch  7, batch    84 | loss: 81.3792824CurrentTrain: epoch  7, batch    85 | loss: 122.3624120CurrentTrain: epoch  7, batch    86 | loss: 79.7098650CurrentTrain: epoch  7, batch    87 | loss: 57.6846253CurrentTrain: epoch  7, batch    88 | loss: 61.5165037CurrentTrain: epoch  7, batch    89 | loss: 68.8724042CurrentTrain: epoch  7, batch    90 | loss: 78.5853598CurrentTrain: epoch  7, batch    91 | loss: 99.4293037CurrentTrain: epoch  7, batch    92 | loss: 65.9067887CurrentTrain: epoch  7, batch    93 | loss: 118.5031243CurrentTrain: epoch  7, batch    94 | loss: 75.7834416CurrentTrain: epoch  7, batch    95 | loss: 77.8182784CurrentTrain: epoch  8, batch     0 | loss: 68.8643358CurrentTrain: epoch  8, batch     1 | loss: 125.1691761CurrentTrain: epoch  8, batch     2 | loss: 64.9671784CurrentTrain: epoch  8, batch     3 | loss: 54.7012610CurrentTrain: epoch  8, batch     4 | loss: 96.3553831CurrentTrain: epoch  8, batch     5 | loss: 95.3042300CurrentTrain: epoch  8, batch     6 | loss: 94.9459648CurrentTrain: epoch  8, batch     7 | loss: 66.4382787CurrentTrain: epoch  8, batch     8 | loss: 80.0434914CurrentTrain: epoch  8, batch     9 | loss: 55.2489910CurrentTrain: epoch  8, batch    10 | loss: 75.7235308CurrentTrain: epoch  8, batch    11 | loss: 78.1623433CurrentTrain: epoch  8, batch    12 | loss: 80.5943456CurrentTrain: epoch  8, batch    13 | loss: 53.6895253CurrentTrain: epoch  8, batch    14 | loss: 122.1371870CurrentTrain: epoch  8, batch    15 | loss: 67.5407633CurrentTrain: epoch  8, batch    16 | loss: 100.4281833CurrentTrain: epoch  8, batch    17 | loss: 92.8514867CurrentTrain: epoch  8, batch    18 | loss: 65.3680816CurrentTrain: epoch  8, batch    19 | loss: 95.2905461CurrentTrain: epoch  8, batch    20 | loss: 78.5447837CurrentTrain: epoch  8, batch    21 | loss: 97.9276792CurrentTrain: epoch  8, batch    22 | loss: 168.1021900CurrentTrain: epoch  8, batch    23 | loss: 78.0331886CurrentTrain: epoch  8, batch    24 | loss: 80.9532308CurrentTrain: epoch  8, batch    25 | loss: 118.1541337CurrentTrain: epoch  8, batch    26 | loss: 77.0986971CurrentTrain: epoch  8, batch    27 | loss: 82.0434193CurrentTrain: epoch  8, batch    28 | loss: 81.4069452CurrentTrain: epoch  8, batch    29 | loss: 66.1787934CurrentTrain: epoch  8, batch    30 | loss: 77.5861345CurrentTrain: epoch  8, batch    31 | loss: 120.7393086CurrentTrain: epoch  8, batch    32 | loss: 75.7555968CurrentTrain: epoch  8, batch    33 | loss: 73.6145942CurrentTrain: epoch  8, batch    34 | loss: 62.1148362CurrentTrain: epoch  8, batch    35 | loss: 76.7244607CurrentTrain: epoch  8, batch    36 | loss: 76.4463003CurrentTrain: epoch  8, batch    37 | loss: 123.7997973CurrentTrain: epoch  8, batch    38 | loss: 99.1238783CurrentTrain: epoch  8, batch    39 | loss: 124.6096191CurrentTrain: epoch  8, batch    40 | loss: 56.2998869CurrentTrain: epoch  8, batch    41 | loss: 92.0392154CurrentTrain: epoch  8, batch    42 | loss: 98.6450480CurrentTrain: epoch  8, batch    43 | loss: 74.6320107CurrentTrain: epoch  8, batch    44 | loss: 56.5215020CurrentTrain: epoch  8, batch    45 | loss: 56.1062553CurrentTrain: epoch  8, batch    46 | loss: 81.5742847CurrentTrain: epoch  8, batch    47 | loss: 74.6894087CurrentTrain: epoch  8, batch    48 | loss: 83.7104337CurrentTrain: epoch  8, batch    49 | loss: 77.1920986CurrentTrain: epoch  8, batch    50 | loss: 78.9938510CurrentTrain: epoch  8, batch    51 | loss: 78.2563845CurrentTrain: epoch  8, batch    52 | loss: 75.1324667CurrentTrain: epoch  8, batch    53 | loss: 74.7536651CurrentTrain: epoch  8, batch    54 | loss: 119.8271041CurrentTrain: epoch  8, batch    55 | loss: 99.3197576CurrentTrain: epoch  8, batch    56 | loss: 58.1078940CurrentTrain: epoch  8, batch    57 | loss: 97.7380508CurrentTrain: epoch  8, batch    58 | loss: 75.2756225CurrentTrain: epoch  8, batch    59 | loss: 123.1511732CurrentTrain: epoch  8, batch    60 | loss: 67.1462183CurrentTrain: epoch  8, batch    61 | loss: 74.2031696CurrentTrain: epoch  8, batch    62 | loss: 169.5821843CurrentTrain: epoch  8, batch    63 | loss: 64.6319025CurrentTrain: epoch  8, batch    64 | loss: 79.8572112CurrentTrain: epoch  8, batch    65 | loss: 68.3653340CurrentTrain: epoch  8, batch    66 | loss: 74.0991334CurrentTrain: epoch  8, batch    67 | loss: 95.5856248CurrentTrain: epoch  8, batch    68 | loss: 100.0358203CurrentTrain: epoch  8, batch    69 | loss: 64.7348692CurrentTrain: epoch  8, batch    70 | loss: 68.2576020CurrentTrain: epoch  8, batch    71 | loss: 78.5619885CurrentTrain: epoch  8, batch    72 | loss: 78.0397133CurrentTrain: epoch  8, batch    73 | loss: 122.0091674CurrentTrain: epoch  8, batch    74 | loss: 75.9336835CurrentTrain: epoch  8, batch    75 | loss: 64.2663140CurrentTrain: epoch  8, batch    76 | loss: 95.8945269CurrentTrain: epoch  8, batch    77 | loss: 64.8338429CurrentTrain: epoch  8, batch    78 | loss: 95.9259150CurrentTrain: epoch  8, batch    79 | loss: 60.6459834CurrentTrain: epoch  8, batch    80 | loss: 95.9280561CurrentTrain: epoch  8, batch    81 | loss: 80.6146157CurrentTrain: epoch  8, batch    82 | loss: 63.0567035CurrentTrain: epoch  8, batch    83 | loss: 67.4930409CurrentTrain: epoch  8, batch    84 | loss: 78.5689655CurrentTrain: epoch  8, batch    85 | loss: 80.1366914CurrentTrain: epoch  8, batch    86 | loss: 95.2663574CurrentTrain: epoch  8, batch    87 | loss: 55.4603893CurrentTrain: epoch  8, batch    88 | loss: 161.9932874CurrentTrain: epoch  8, batch    89 | loss: 63.0585299CurrentTrain: epoch  8, batch    90 | loss: 98.1114288CurrentTrain: epoch  8, batch    91 | loss: 64.7975256CurrentTrain: epoch  8, batch    92 | loss: 62.7795633CurrentTrain: epoch  8, batch    93 | loss: 68.6288273CurrentTrain: epoch  8, batch    94 | loss: 60.3130305CurrentTrain: epoch  8, batch    95 | loss: 83.1221599CurrentTrain: epoch  9, batch     0 | loss: 75.9091622CurrentTrain: epoch  9, batch     1 | loss: 63.6274973CurrentTrain: epoch  9, batch     2 | loss: 121.6866472CurrentTrain: epoch  9, batch     3 | loss: 79.5963637CurrentTrain: epoch  9, batch     4 | loss: 73.3059185CurrentTrain: epoch  9, batch     5 | loss: 80.6014958CurrentTrain: epoch  9, batch     6 | loss: 65.0246835CurrentTrain: epoch  9, batch     7 | loss: 66.8629577CurrentTrain: epoch  9, batch     8 | loss: 76.3520455CurrentTrain: epoch  9, batch     9 | loss: 78.7033322CurrentTrain: epoch  9, batch    10 | loss: 76.0741996CurrentTrain: epoch  9, batch    11 | loss: 78.1248455CurrentTrain: epoch  9, batch    12 | loss: 97.4648350CurrentTrain: epoch  9, batch    13 | loss: 76.8741231CurrentTrain: epoch  9, batch    14 | loss: 80.2535038CurrentTrain: epoch  9, batch    15 | loss: 63.6829987CurrentTrain: epoch  9, batch    16 | loss: 79.1644248CurrentTrain: epoch  9, batch    17 | loss: 75.7109040CurrentTrain: epoch  9, batch    18 | loss: 125.6293822CurrentTrain: epoch  9, batch    19 | loss: 62.5474453CurrentTrain: epoch  9, batch    20 | loss: 96.1116467CurrentTrain: epoch  9, batch    21 | loss: 117.6973607CurrentTrain: epoch  9, batch    22 | loss: 93.5452197CurrentTrain: epoch  9, batch    23 | loss: 76.6043562CurrentTrain: epoch  9, batch    24 | loss: 96.6165659CurrentTrain: epoch  9, batch    25 | loss: 61.4694372CurrentTrain: epoch  9, batch    26 | loss: 67.2397976CurrentTrain: epoch  9, batch    27 | loss: 78.0244743CurrentTrain: epoch  9, batch    28 | loss: 120.2209796CurrentTrain: epoch  9, batch    29 | loss: 54.1389599CurrentTrain: epoch  9, batch    30 | loss: 77.4921726CurrentTrain: epoch  9, batch    31 | loss: 96.6701012CurrentTrain: epoch  9, batch    32 | loss: 74.2461615CurrentTrain: epoch  9, batch    33 | loss: 77.6960419CurrentTrain: epoch  9, batch    34 | loss: 123.4906671CurrentTrain: epoch  9, batch    35 | loss: 66.9490744CurrentTrain: epoch  9, batch    36 | loss: 74.0162384CurrentTrain: epoch  9, batch    37 | loss: 99.0305431CurrentTrain: epoch  9, batch    38 | loss: 94.8591200CurrentTrain: epoch  9, batch    39 | loss: 80.5195450CurrentTrain: epoch  9, batch    40 | loss: 55.9105364CurrentTrain: epoch  9, batch    41 | loss: 95.7190480CurrentTrain: epoch  9, batch    42 | loss: 66.8790409CurrentTrain: epoch  9, batch    43 | loss: 75.0224896CurrentTrain: epoch  9, batch    44 | loss: 64.3810424CurrentTrain: epoch  9, batch    45 | loss: 95.2750960CurrentTrain: epoch  9, batch    46 | loss: 77.9191781CurrentTrain: epoch  9, batch    47 | loss: 97.5525240CurrentTrain: epoch  9, batch    48 | loss: 79.4158418CurrentTrain: epoch  9, batch    49 | loss: 77.7785903CurrentTrain: epoch  9, batch    50 | loss: 61.0976742CurrentTrain: epoch  9, batch    51 | loss: 67.9193091CurrentTrain: epoch  9, batch    52 | loss: 98.9068435CurrentTrain: epoch  9, batch    53 | loss: 79.6112231CurrentTrain: epoch  9, batch    54 | loss: 96.1489515CurrentTrain: epoch  9, batch    55 | loss: 74.8804009CurrentTrain: epoch  9, batch    56 | loss: 77.1337506CurrentTrain: epoch  9, batch    57 | loss: 58.9569668CurrentTrain: epoch  9, batch    58 | loss: 61.7176392CurrentTrain: epoch  9, batch    59 | loss: 123.4019833CurrentTrain: epoch  9, batch    60 | loss: 99.0277072CurrentTrain: epoch  9, batch    61 | loss: 79.8847467CurrentTrain: epoch  9, batch    62 | loss: 66.4022989CurrentTrain: epoch  9, batch    63 | loss: 120.7231619CurrentTrain: epoch  9, batch    64 | loss: 75.3255127CurrentTrain: epoch  9, batch    65 | loss: 79.9762848CurrentTrain: epoch  9, batch    66 | loss: 64.6406393CurrentTrain: epoch  9, batch    67 | loss: 91.8510455CurrentTrain: epoch  9, batch    68 | loss: 73.5291760CurrentTrain: epoch  9, batch    69 | loss: 66.2039453CurrentTrain: epoch  9, batch    70 | loss: 67.5989015CurrentTrain: epoch  9, batch    71 | loss: 68.6725121CurrentTrain: epoch  9, batch    72 | loss: 94.6512913CurrentTrain: epoch  9, batch    73 | loss: 73.8784478CurrentTrain: epoch  9, batch    74 | loss: 78.9669275CurrentTrain: epoch  9, batch    75 | loss: 66.7138489CurrentTrain: epoch  9, batch    76 | loss: 97.5487420CurrentTrain: epoch  9, batch    77 | loss: 80.0044704CurrentTrain: epoch  9, batch    78 | loss: 55.3827715CurrentTrain: epoch  9, batch    79 | loss: 123.6235764CurrentTrain: epoch  9, batch    80 | loss: 78.4348746CurrentTrain: epoch  9, batch    81 | loss: 76.3298983CurrentTrain: epoch  9, batch    82 | loss: 94.9435685CurrentTrain: epoch  9, batch    83 | loss: 63.6503365CurrentTrain: epoch  9, batch    84 | loss: 75.7213219CurrentTrain: epoch  9, batch    85 | loss: 58.3991908CurrentTrain: epoch  9, batch    86 | loss: 77.3585389CurrentTrain: epoch  9, batch    87 | loss: 94.6985295CurrentTrain: epoch  9, batch    88 | loss: 77.4313692CurrentTrain: epoch  9, batch    89 | loss: 75.3110331CurrentTrain: epoch  9, batch    90 | loss: 65.1789469CurrentTrain: epoch  9, batch    91 | loss: 92.7261261CurrentTrain: epoch  9, batch    92 | loss: 73.5380949CurrentTrain: epoch  9, batch    93 | loss: 67.7718256CurrentTrain: epoch  9, batch    94 | loss: 167.3849966CurrentTrain: epoch  9, batch    95 | loss: 66.2991321

F1 score per class: {32: 0.6325581395348837, 6: 0.7873303167420814, 19: 0.3888888888888889, 24: 0.7333333333333333, 26: 0.9278350515463918, 29: 0.8504672897196262}
Micro-average F1 score: 0.7716981132075472
Weighted-average F1 score: 0.7709365072183918
F1 score per class: {32: 0.6370967741935484, 6: 0.7913043478260869, 19: 0.24615384615384617, 24: 0.7272727272727273, 26: 0.95, 29: 0.8240740740740741}
Micro-average F1 score: 0.7504363001745201
Weighted-average F1 score: 0.7366153478225909
F1 score per class: {32: 0.6610878661087866, 6: 0.7982456140350878, 19: 0.3076923076923077, 24: 0.7351351351351352, 26: 0.9547738693467337, 29: 0.8240740740740741}
Micro-average F1 score: 0.7685433422698839
Weighted-average F1 score: 0.7606154452438549

F1 score per class: {32: 0.6325581395348837, 6: 0.7873303167420814, 19: 0.3888888888888889, 24: 0.7333333333333333, 26: 0.9278350515463918, 29: 0.8504672897196262}
Micro-average F1 score: 0.7716981132075472
Weighted-average F1 score: 0.7709365072183918
F1 score per class: {32: 0.6370967741935484, 6: 0.7913043478260869, 19: 0.24615384615384617, 24: 0.7272727272727273, 26: 0.95, 29: 0.8240740740740741}
Micro-average F1 score: 0.7504363001745201
Weighted-average F1 score: 0.7366153478225909
F1 score per class: {32: 0.6610878661087866, 6: 0.7982456140350878, 19: 0.3076923076923077, 24: 0.7351351351351352, 26: 0.9547738693467337, 29: 0.8240740740740741}
Micro-average F1 score: 0.7685433422698839
Weighted-average F1 score: 0.7606154452438549

F1 score per class: {32: 0.46258503401360546, 6: 0.7404255319148936, 19: 0.25, 24: 0.673469387755102, 26: 0.8490566037735849, 29: 0.6546762589928058}
Micro-average F1 score: 0.6435877261998426
Weighted-average F1 score: 0.6314458411088989
F1 score per class: {32: 0.42473118279569894, 6: 0.7338709677419355, 19: 0.13675213675213677, 24: 0.6538461538461539, 26: 0.852017937219731, 29: 0.6402877697841727}
Micro-average F1 score: 0.5947441217150761
Weighted-average F1 score: 0.5679558840513926
F1 score per class: {32: 0.4438202247191011, 6: 0.7428571428571429, 19: 0.17391304347826086, 24: 0.6601941747572816, 26: 0.8597285067873304, 29: 0.6402877697841727}
Micro-average F1 score: 0.6151645207439199
Weighted-average F1 score: 0.593387253780555

F1 score per class: {32: 0.46258503401360546, 6: 0.7404255319148936, 19: 0.25, 24: 0.673469387755102, 26: 0.8490566037735849, 29: 0.6546762589928058}
Micro-average F1 score: 0.6435877261998426
Weighted-average F1 score: 0.6314458411088989
F1 score per class: {32: 0.42473118279569894, 6: 0.7338709677419355, 19: 0.13675213675213677, 24: 0.6538461538461539, 26: 0.852017937219731, 29: 0.6402877697841727}
Micro-average F1 score: 0.5947441217150761
Weighted-average F1 score: 0.5679558840513926
F1 score per class: {32: 0.4438202247191011, 6: 0.7428571428571429, 19: 0.17391304347826086, 24: 0.6601941747572816, 26: 0.8597285067873304, 29: 0.6402877697841727}
Micro-average F1 score: 0.6151645207439199
Weighted-average F1 score: 0.593387253780555
cur_acc_wo_na:  ['0.7717']
his_acc_wo_na:  ['0.7717']
cur_acc des_wo_na:  ['0.7504']
his_acc des_wo_na:  ['0.7504']
cur_acc rrf_wo_na:  ['0.7685']
his_acc rrf_wo_na:  ['0.7685']
cur_acc_w_na:  ['0.6436']
his_acc_w_na:  ['0.6436']
cur_acc des_w_na:  ['0.5947']
his_acc des_w_na:  ['0.5947']
cur_acc rrf_w_na:  ['0.6152']
his_acc rrf_w_na:  ['0.6152']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 94.0704562CurrentTrain: epoch  0, batch     1 | loss: 90.7068426CurrentTrain: epoch  0, batch     2 | loss: 142.9054676CurrentTrain: epoch  0, batch     3 | loss: 53.5285862CurrentTrain: epoch  1, batch     0 | loss: 74.7843520CurrentTrain: epoch  1, batch     1 | loss: 87.7714351CurrentTrain: epoch  1, batch     2 | loss: 103.8873864CurrentTrain: epoch  1, batch     3 | loss: 72.7155699CurrentTrain: epoch  2, batch     0 | loss: 106.7235519CurrentTrain: epoch  2, batch     1 | loss: 82.4254828CurrentTrain: epoch  2, batch     2 | loss: 83.3898774CurrentTrain: epoch  2, batch     3 | loss: 85.9434117CurrentTrain: epoch  3, batch     0 | loss: 81.7887475CurrentTrain: epoch  3, batch     1 | loss: 70.6960172CurrentTrain: epoch  3, batch     2 | loss: 99.0872559CurrentTrain: epoch  3, batch     3 | loss: 85.9372012CurrentTrain: epoch  4, batch     0 | loss: 98.7667115CurrentTrain: epoch  4, batch     1 | loss: 69.1404072CurrentTrain: epoch  4, batch     2 | loss: 68.5705811CurrentTrain: epoch  4, batch     3 | loss: 56.9930608CurrentTrain: epoch  5, batch     0 | loss: 81.2084677CurrentTrain: epoch  5, batch     1 | loss: 71.2507040CurrentTrain: epoch  5, batch     2 | loss: 65.5774073CurrentTrain: epoch  5, batch     3 | loss: 54.4868590CurrentTrain: epoch  6, batch     0 | loss: 79.2012399CurrentTrain: epoch  6, batch     1 | loss: 75.3215354CurrentTrain: epoch  6, batch     2 | loss: 65.3593667CurrentTrain: epoch  6, batch     3 | loss: 85.9639817CurrentTrain: epoch  7, batch     0 | loss: 98.0725815CurrentTrain: epoch  7, batch     1 | loss: 63.6670271CurrentTrain: epoch  7, batch     2 | loss: 77.3707890CurrentTrain: epoch  7, batch     3 | loss: 82.6425459CurrentTrain: epoch  8, batch     0 | loss: 64.1707032CurrentTrain: epoch  8, batch     1 | loss: 77.8225911CurrentTrain: epoch  8, batch     2 | loss: 79.2525687CurrentTrain: epoch  8, batch     3 | loss: 63.3651156CurrentTrain: epoch  9, batch     0 | loss: 64.3204293CurrentTrain: epoch  9, batch     1 | loss: 75.9190035CurrentTrain: epoch  9, batch     2 | loss: 80.3842598CurrentTrain: epoch  9, batch     3 | loss: 43.2775155
MemoryTrain:  epoch  0, batch     0 | loss: 2.7589404MemoryTrain:  epoch  1, batch     0 | loss: 2.3419682MemoryTrain:  epoch  2, batch     0 | loss: 1.9962927MemoryTrain:  epoch  3, batch     0 | loss: 1.5327938MemoryTrain:  epoch  4, batch     0 | loss: 1.3877745MemoryTrain:  epoch  5, batch     0 | loss: 1.1139993MemoryTrain:  epoch  6, batch     0 | loss: 1.0026613MemoryTrain:  epoch  7, batch     0 | loss: 0.8007884MemoryTrain:  epoch  8, batch     0 | loss: 0.6673503MemoryTrain:  epoch  9, batch     0 | loss: 0.5814076

F1 score per class: {32: 0.0, 35: 0.5517241379310345, 37: 0.0, 38: 0.0, 6: 0.5866666666666667, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.7272727272727273, 26: 0.6111111111111112, 29: 0.46153846153846156}
Micro-average F1 score: 0.5621621621621622
Weighted-average F1 score: 0.5047067412625462
F1 score per class: {32: 0.0, 35: 0.7, 37: 0.0, 38: 0.0, 6: 0.7368421052631579, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.7428571428571429, 26: 0.6851851851851852, 29: 0.5194805194805194}
Micro-average F1 score: 0.6013071895424836
Weighted-average F1 score: 0.5315578704807591
F1 score per class: {32: 0.0, 35: 0.6363636363636364, 37: 0.0, 38: 0.0, 6: 0.7294117647058823, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.7692307692307693, 26: 0.6260869565217392, 29: 0.7058823529411765}
Micro-average F1 score: 0.616822429906542
Weighted-average F1 score: 0.5424945346574064

F1 score per class: {32: 0.6410256410256411, 35: 0.2857142857142857, 37: 0.8056872037914692, 6: 0.3125, 38: 0.5866666666666667, 15: 0.7597765363128491, 19: 0.9072164948453608, 24: 0.7870370370370371, 25: 0.6095238095238096, 26: 0.4342105263157895, 29: 0.46153846153846156}
Micro-average F1 score: 0.6831882116543871
Weighted-average F1 score: 0.6729657317664292
F1 score per class: {32: 0.5917602996254682, 35: 0.45161290322580644, 37: 0.7543859649122807, 6: 0.2903225806451613, 38: 0.7368421052631579, 15: 0.6766169154228856, 19: 0.92, 24: 0.7729468599033816, 25: 0.5306122448979592, 26: 0.5, 29: 0.37383177570093457}
Micro-average F1 score: 0.6520968694624926
Weighted-average F1 score: 0.6333087852634713
F1 score per class: {32: 0.5714285714285714, 35: 0.3684210526315789, 37: 0.7644444444444445, 6: 0.32558139534883723, 38: 0.7294117647058823, 15: 0.6938775510204082, 19: 0.9246231155778895, 24: 0.7677725118483413, 25: 0.5714285714285714, 26: 0.4472049689440994, 29: 0.631578947368421}
Micro-average F1 score: 0.6687230104873535
Weighted-average F1 score: 0.6548247531650941

F1 score per class: {32: 0.0, 35: 0.38095238095238093, 37: 0.0, 38: 0.0, 6: 0.5365853658536586, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.5925925925925926, 26: 0.5546218487394958, 29: 0.35294117647058826}
Micro-average F1 score: 0.4482758620689655
Weighted-average F1 score: 0.3912781409821958
F1 score per class: {32: 0.0, 35: 0.5384615384615384, 37: 0.0, 38: 0.0, 6: 0.660377358490566, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6, 26: 0.6166666666666667, 29: 0.3448275862068966}
Micro-average F1 score: 0.4623115577889447
Weighted-average F1 score: 0.4015671453537426
F1 score per class: {32: 0.0, 35: 0.45161290322580644, 37: 0.0, 38: 0.0, 6: 0.6526315789473685, 15: 0.0, 19: 0.0, 24: 0.0, 25: 0.6299212598425197, 26: 0.5581395348837209, 29: 0.4675324675324675}
Micro-average F1 score: 0.4835164835164835
Weighted-average F1 score: 0.4256282997871824

F1 score per class: {32: 0.44642857142857145, 35: 0.1568627450980392, 37: 0.748898678414097, 6: 0.21739130434782608, 38: 0.5365853658536586, 15: 0.6699507389162561, 19: 0.7822222222222223, 24: 0.6343283582089553, 25: 0.42105263157894735, 26: 0.3188405797101449, 29: 0.3050847457627119}
Micro-average F1 score: 0.5348715259570005
Weighted-average F1 score: 0.51286745896706
F1 score per class: {32: 0.37708830548926014, 35: 0.34146341463414637, 37: 0.6963562753036437, 6: 0.14634146341463414, 38: 0.6542056074766355, 15: 0.5836909871244635, 19: 0.7829787234042553, 24: 0.6374501992031872, 25: 0.3406113537117904, 26: 0.37755102040816324, 29: 0.2094240837696335}
Micro-average F1 score: 0.4859154929577465
Weighted-average F1 score: 0.4573650767943259
F1 score per class: {32: 0.3645083932853717, 35: 0.23333333333333334, 37: 0.7107438016528925, 6: 0.17721518987341772, 38: 0.6458333333333334, 15: 0.6153846153846154, 19: 0.8, 24: 0.6328125, 25: 0.37383177570093457, 26: 0.3333333333333333, 29: 0.3302752293577982}
Micro-average F1 score: 0.5065420560747663
Weighted-average F1 score: 0.48151682047892885
cur_acc_wo_na:  ['0.7717', '0.5622']
his_acc_wo_na:  ['0.7717', '0.6832']
cur_acc des_wo_na:  ['0.7504', '0.6013']
his_acc des_wo_na:  ['0.7504', '0.6521']
cur_acc rrf_wo_na:  ['0.7685', '0.6168']
his_acc rrf_wo_na:  ['0.7685', '0.6687']
cur_acc_w_na:  ['0.6436', '0.4483']
his_acc_w_na:  ['0.6436', '0.5349']
cur_acc des_w_na:  ['0.5947', '0.4623']
his_acc des_w_na:  ['0.5947', '0.4859']
cur_acc rrf_w_na:  ['0.6152', '0.4835']
his_acc rrf_w_na:  ['0.6152', '0.5065']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 119.6638578CurrentTrain: epoch  0, batch     1 | loss: 86.4669708CurrentTrain: epoch  0, batch     2 | loss: 81.0083237CurrentTrain: epoch  0, batch     3 | loss: 51.3544630CurrentTrain: epoch  1, batch     0 | loss: 80.9170624CurrentTrain: epoch  1, batch     1 | loss: 72.8442971CurrentTrain: epoch  1, batch     2 | loss: 85.5564002CurrentTrain: epoch  1, batch     3 | loss: 86.1936910CurrentTrain: epoch  2, batch     0 | loss: 82.4491249CurrentTrain: epoch  2, batch     1 | loss: 82.6845758CurrentTrain: epoch  2, batch     2 | loss: 86.8044579CurrentTrain: epoch  2, batch     3 | loss: 50.2401744CurrentTrain: epoch  3, batch     0 | loss: 101.8863235CurrentTrain: epoch  3, batch     1 | loss: 86.6758893CurrentTrain: epoch  3, batch     2 | loss: 80.1115564CurrentTrain: epoch  3, batch     3 | loss: 58.0252150CurrentTrain: epoch  4, batch     0 | loss: 80.7896281CurrentTrain: epoch  4, batch     1 | loss: 98.8620679CurrentTrain: epoch  4, batch     2 | loss: 81.2550484CurrentTrain: epoch  4, batch     3 | loss: 56.6886425CurrentTrain: epoch  5, batch     0 | loss: 64.6954809CurrentTrain: epoch  5, batch     1 | loss: 82.1998007CurrentTrain: epoch  5, batch     2 | loss: 76.9775106CurrentTrain: epoch  5, batch     3 | loss: 50.2866379CurrentTrain: epoch  6, batch     0 | loss: 78.6481674CurrentTrain: epoch  6, batch     1 | loss: 67.1275892CurrentTrain: epoch  6, batch     2 | loss: 79.4932021CurrentTrain: epoch  6, batch     3 | loss: 58.1426052CurrentTrain: epoch  7, batch     0 | loss: 80.6344264CurrentTrain: epoch  7, batch     1 | loss: 73.8822296CurrentTrain: epoch  7, batch     2 | loss: 64.9381043CurrentTrain: epoch  7, batch     3 | loss: 71.4159913CurrentTrain: epoch  8, batch     0 | loss: 93.6844096CurrentTrain: epoch  8, batch     1 | loss: 93.5190328CurrentTrain: epoch  8, batch     2 | loss: 75.5155972CurrentTrain: epoch  8, batch     3 | loss: 53.8025198CurrentTrain: epoch  9, batch     0 | loss: 76.4381115CurrentTrain: epoch  9, batch     1 | loss: 97.2276230CurrentTrain: epoch  9, batch     2 | loss: 60.7736674CurrentTrain: epoch  9, batch     3 | loss: 45.5400785
MemoryTrain:  epoch  0, batch     0 | loss: 1.3207543MemoryTrain:  epoch  1, batch     0 | loss: 1.3311616MemoryTrain:  epoch  2, batch     0 | loss: 0.9747569MemoryTrain:  epoch  3, batch     0 | loss: 0.7597667MemoryTrain:  epoch  4, batch     0 | loss: 0.6409308MemoryTrain:  epoch  5, batch     0 | loss: 0.6128009MemoryTrain:  epoch  6, batch     0 | loss: 0.4642689MemoryTrain:  epoch  7, batch     0 | loss: 0.3970092MemoryTrain:  epoch  8, batch     0 | loss: 0.3661101MemoryTrain:  epoch  9, batch     0 | loss: 0.3075380

F1 score per class: {32: 0.0, 33: 0.45528455284552843, 35: 0.0, 36: 0.7415730337078652, 37: 0.0, 6: 0.0, 38: 0.9142857142857143, 8: 0.0, 15: 0.35294117647058826, 20: 0.0, 26: 0.5333333333333333, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5161290322580645
Weighted-average F1 score: 0.433855964457456
F1 score per class: {32: 0.0, 33: 0.6225165562913907, 35: 0.0, 36: 0.0, 37: 0.7311827956989247, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.0, 19: 0.8372093023255814, 20: 0.0, 24: 0.46153846153846156, 25: 0.0, 26: 0.7022900763358778, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.571969696969697
Weighted-average F1 score: 0.4873981249608698
F1 score per class: {32: 0.0, 33: 0.5594405594405595, 35: 0.0, 36: 0.0, 37: 0.75, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.8780487804878049, 19: 0.0, 20: 0.3333333333333333, 24: 0.0, 26: 0.6842105263157895, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.5649484536082474
Weighted-average F1 score: 0.4829835687107781

F1 score per class: {32: 0.5777777777777777, 33: 0.4028776978417266, 35: 0.5333333333333333, 36: 0.7850467289719626, 37: 0.5116279069767442, 6: 0.25, 38: 0.34375, 8: 0.7150259067357513, 15: 0.9090909090909091, 19: 0.8421052631578947, 20: 0.7614678899082569, 24: 0.2727272727272727, 25: 0.5116279069767442, 26: 0.4752475247524752, 29: 0.2716049382716049, 30: 0.0}
Micro-average F1 score: 0.6122222222222222
Weighted-average F1 score: 0.642863633800592
F1 score per class: {32: 0.6046511627906976, 33: 0.4973544973544973, 35: 0.5833333333333334, 36: 0.7350427350427351, 37: 0.5, 6: 0.25925925925925924, 38: 0.46153846153846156, 8: 0.6540284360189573, 15: 0.8899521531100478, 19: 0.5070422535211268, 20: 0.7636363636363637, 24: 0.1935483870967742, 25: 0.6165413533834586, 26: 0.48677248677248675, 29: 0.2882882882882883, 30: 0.48148148148148145}
Micro-average F1 score: 0.593819973130318
Weighted-average F1 score: 0.5871948508582387
F1 score per class: {32: 0.6007905138339921, 33: 0.43956043956043955, 35: 0.4827586206896552, 36: 0.7319148936170212, 37: 0.5217391304347826, 6: 0.26666666666666666, 38: 0.4444444444444444, 8: 0.6798029556650246, 15: 0.9054726368159204, 19: 0.631578947368421, 20: 0.7567567567567568, 24: 0.14035087719298245, 25: 0.5882352941176471, 26: 0.5131578947368421, 29: 0.30927835051546393, 30: 0.34146341463414637}
Micro-average F1 score: 0.5981930575368521
Weighted-average F1 score: 0.5971199141568159

F1 score per class: {32: 0.0, 33: 0.39436619718309857, 35: 0.0, 36: 0.0, 37: 0.5546218487394958, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.8648648648648649, 19: 0.0, 20: 0.3, 24: 0.0, 26: 0.44036697247706424, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.3880597014925373
Weighted-average F1 score: 0.31904952649284446
F1 score per class: {32: 0.0, 33: 0.47959183673469385, 35: 0.0, 36: 0.0, 37: 0.5619834710743802, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.0, 19: 0.7346938775510204, 20: 0.0, 24: 0.3157894736842105, 25: 0.0, 26: 0.5476190476190477, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.39425587467362927
Weighted-average F1 score: 0.33500977338246807
F1 score per class: {32: 0.0, 33: 0.4419889502762431, 35: 0.0, 36: 0.0, 37: 0.5853658536585366, 6: 0.0, 38: 0.0, 8: 0.0, 15: 0.8, 19: 0.0, 20: 0.22857142857142856, 24: 0.0, 26: 0.5379310344827586, 29: 0.0, 30: 0.0}
Micro-average F1 score: 0.397677793904209
Weighted-average F1 score: 0.3348720631875886

F1 score per class: {32: 0.3939393939393939, 33: 0.33532934131736525, 35: 0.3076923076923077, 36: 0.7148936170212766, 37: 0.3127962085308057, 6: 0.1702127659574468, 38: 0.3283582089552239, 8: 0.6216216216216216, 15: 0.7692307692307693, 19: 0.7272727272727273, 20: 0.5865724381625441, 24: 0.1875, 25: 0.38596491228070173, 26: 0.3779527559055118, 29: 0.24719101123595505, 30: 0.0}
Micro-average F1 score: 0.4814329401485365
Weighted-average F1 score: 0.4860081624024062
F1 score per class: {32: 0.3929471032745592, 33: 0.32752613240418116, 35: 0.4117647058823529, 36: 0.6590038314176245, 37: 0.3177570093457944, 6: 0.14, 38: 0.42857142857142855, 8: 0.5609756097560976, 15: 0.7265625, 19: 0.37894736842105264, 20: 0.5915492957746479, 24: 0.11764705882352941, 25: 0.42487046632124353, 26: 0.3665338645418327, 29: 0.2119205298013245, 30: 0.29213483146067415}
Micro-average F1 score: 0.435611038107753
Weighted-average F1 score: 0.42265308302081006
F1 score per class: {32: 0.39276485788113696, 33: 0.3053435114503817, 35: 0.32558139534883723, 36: 0.6590038314176245, 37: 0.3380281690140845, 6: 0.15584415584415584, 38: 0.4155844155844156, 8: 0.5847457627118644, 15: 0.7398373983739838, 19: 0.4864864864864865, 20: 0.5894736842105263, 24: 0.1, 25: 0.4093567251461988, 26: 0.38613861386138615, 29: 0.24390243902439024, 30: 0.2028985507246377}
Micro-average F1 score: 0.4483250178189594
Weighted-average F1 score: 0.43843685997568316
cur_acc_wo_na:  ['0.7717', '0.5622', '0.5161']
his_acc_wo_na:  ['0.7717', '0.6832', '0.6122']
cur_acc des_wo_na:  ['0.7504', '0.6013', '0.5720']
his_acc des_wo_na:  ['0.7504', '0.6521', '0.5938']
cur_acc rrf_wo_na:  ['0.7685', '0.6168', '0.5649']
his_acc rrf_wo_na:  ['0.7685', '0.6687', '0.5982']
cur_acc_w_na:  ['0.6436', '0.4483', '0.3881']
his_acc_w_na:  ['0.6436', '0.5349', '0.4814']
cur_acc des_w_na:  ['0.5947', '0.4623', '0.3943']
his_acc des_w_na:  ['0.5947', '0.4859', '0.4356']
cur_acc rrf_w_na:  ['0.6152', '0.4835', '0.3977']
his_acc rrf_w_na:  ['0.6152', '0.5065', '0.4483']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 101.1077053CurrentTrain: epoch  0, batch     1 | loss: 141.8137334CurrentTrain: epoch  0, batch     2 | loss: 93.4341657CurrentTrain: epoch  0, batch     3 | loss: 85.7943989CurrentTrain: epoch  0, batch     4 | loss: 65.3390035CurrentTrain: epoch  1, batch     0 | loss: 75.8113461CurrentTrain: epoch  1, batch     1 | loss: 108.3628606CurrentTrain: epoch  1, batch     2 | loss: 92.1721001CurrentTrain: epoch  1, batch     3 | loss: 74.3303757CurrentTrain: epoch  1, batch     4 | loss: 111.2942706CurrentTrain: epoch  2, batch     0 | loss: 107.2366038CurrentTrain: epoch  2, batch     1 | loss: 76.4555805CurrentTrain: epoch  2, batch     2 | loss: 86.0728418CurrentTrain: epoch  2, batch     3 | loss: 87.1735172CurrentTrain: epoch  2, batch     4 | loss: 47.4459651CurrentTrain: epoch  3, batch     0 | loss: 129.6564347CurrentTrain: epoch  3, batch     1 | loss: 99.4396081CurrentTrain: epoch  3, batch     2 | loss: 83.3213754CurrentTrain: epoch  3, batch     3 | loss: 85.3495235CurrentTrain: epoch  3, batch     4 | loss: 57.4333439CurrentTrain: epoch  4, batch     0 | loss: 87.4858329CurrentTrain: epoch  4, batch     1 | loss: 84.2192655CurrentTrain: epoch  4, batch     2 | loss: 79.2418917CurrentTrain: epoch  4, batch     3 | loss: 98.6390455CurrentTrain: epoch  4, batch     4 | loss: 56.3113747CurrentTrain: epoch  5, batch     0 | loss: 66.4300297CurrentTrain: epoch  5, batch     1 | loss: 80.3120055CurrentTrain: epoch  5, batch     2 | loss: 123.5099671CurrentTrain: epoch  5, batch     3 | loss: 105.9329656CurrentTrain: epoch  5, batch     4 | loss: 68.2542713CurrentTrain: epoch  6, batch     0 | loss: 70.5353995CurrentTrain: epoch  6, batch     1 | loss: 81.0734077CurrentTrain: epoch  6, batch     2 | loss: 96.9189081CurrentTrain: epoch  6, batch     3 | loss: 81.1451696CurrentTrain: epoch  6, batch     4 | loss: 69.9776012CurrentTrain: epoch  7, batch     0 | loss: 78.8298055CurrentTrain: epoch  7, batch     1 | loss: 95.8944681CurrentTrain: epoch  7, batch     2 | loss: 80.8396885CurrentTrain: epoch  7, batch     3 | loss: 98.6395778CurrentTrain: epoch  7, batch     4 | loss: 37.6916904CurrentTrain: epoch  8, batch     0 | loss: 80.9104184CurrentTrain: epoch  8, batch     1 | loss: 65.7420430CurrentTrain: epoch  8, batch     2 | loss: 67.3559975CurrentTrain: epoch  8, batch     3 | loss: 96.5259171CurrentTrain: epoch  8, batch     4 | loss: 68.6639297CurrentTrain: epoch  9, batch     0 | loss: 123.7967419CurrentTrain: epoch  9, batch     1 | loss: 93.5677083CurrentTrain: epoch  9, batch     2 | loss: 96.7737309CurrentTrain: epoch  9, batch     3 | loss: 64.5513236CurrentTrain: epoch  9, batch     4 | loss: 51.9364418
MemoryTrain:  epoch  0, batch     0 | loss: 1.7107451MemoryTrain:  epoch  1, batch     0 | loss: 1.2255757MemoryTrain:  epoch  2, batch     0 | loss: 1.1423941MemoryTrain:  epoch  3, batch     0 | loss: 1.0167389MemoryTrain:  epoch  4, batch     0 | loss: 0.7379675MemoryTrain:  epoch  5, batch     0 | loss: 0.6899189MemoryTrain:  epoch  6, batch     0 | loss: 0.5680914MemoryTrain:  epoch  7, batch     0 | loss: 0.4653198MemoryTrain:  epoch  8, batch     0 | loss: 0.3840376MemoryTrain:  epoch  9, batch     0 | loss: 0.3238425

F1 score per class: {32: 0.21649484536082475, 1: 0.691358024691358, 34: 0.0, 3: 0.042105263157894736, 35: 0.0, 37: 0.576271186440678, 33: 0.0, 6: 0.0, 14: 0.0, 19: 0.0, 22: 0.0, 24: 0.6504065040650406, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.391705069124424
Weighted-average F1 score: 0.3386149830399959
F1 score per class: {1: 0.23783783783783785, 3: 0.5957446808510638, 6: 0.0, 8: 0.0, 14: 0.07207207207207207, 19: 0.0, 20: 0.0, 22: 0.5395348837209303, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.6115702479338843, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.33146067415730335
Weighted-average F1 score: 0.27948379424610037
F1 score per class: {1: 0.23404255319148937, 3: 0.6229508196721312, 6: 0.0, 8: 0.0, 14: 0.04838709677419355, 19: 0.0, 22: 0.5346534653465347, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.6615384615384615, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.3527093596059113
Weighted-average F1 score: 0.303229462094034

F1 score per class: {1: 0.17142857142857143, 3: 0.4609053497942387, 6: 0.543859649122807, 8: 0.35398230088495575, 14: 0.037037037037037035, 15: 0.56, 19: 0.6666666666666666, 20: 0.4793388429752066, 22: 0.5573770491803278, 24: 0.07692307692307693, 25: 0.3225806451612903, 26: 0.7052631578947368, 29: 0.87, 30: 0.8823529411764706, 32: 0.6027397260273972, 33: 0.21428571428571427, 34: 0.2768166089965398, 35: 0.07894736842105263, 36: 0.42857142857142855, 37: 0.16216216216216217, 38: 0.125}
Micro-average F1 score: 0.4559139784946237
Weighted-average F1 score: 0.4486117489137991
F1 score per class: {1: 0.19469026548672566, 3: 0.3822525597269625, 6: 0.5777777777777777, 8: 0.4973544973544973, 14: 0.05970149253731343, 15: 0.56, 19: 0.6335877862595419, 20: 0.48484848484848486, 22: 0.48945147679324896, 24: 0.05555555555555555, 25: 0.47368421052631576, 26: 0.6538461538461539, 29: 0.8532110091743119, 30: 0.6, 32: 0.5769230769230769, 33: 0.12903225806451613, 34: 0.33636363636363636, 35: 0.11650485436893204, 36: 0.5365853658536586, 37: 0.20512820512820512, 38: 0.41509433962264153}
Micro-average F1 score: 0.45844424726412303
Weighted-average F1 score: 0.44712764177028486
F1 score per class: {1: 0.1864406779661017, 3: 0.3958333333333333, 6: 0.5977011494252874, 8: 0.4533333333333333, 14: 0.0392156862745098, 15: 0.5384615384615384, 19: 0.6451612903225806, 20: 0.49230769230769234, 22: 0.4976958525345622, 24: 0.0851063829787234, 25: 0.3880597014925373, 26: 0.6766169154228856, 29: 0.875, 30: 0.8292682926829268, 32: 0.592, 33: 0.16666666666666666, 34: 0.3028169014084507, 35: 0.0851063829787234, 36: 0.5555555555555556, 37: 0.225, 38: 0.32558139534883723}
Micro-average F1 score: 0.4573934837092732
Weighted-average F1 score: 0.44245466120907034

F1 score per class: {1: 0.12316715542521994, 3: 0.5333333333333333, 6: 0.0, 14: 0.03418803418803419, 15: 0.0, 19: 0.0, 20: 0.0, 22: 0.5230769230769231, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.48484848484848486, 35: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.275750202757502
Weighted-average F1 score: 0.23567785925161783
F1 score per class: {1: 0.1345565749235474, 3: 0.417910447761194, 6: 0.0, 8: 0.0, 14: 0.057971014492753624, 15: 0.0, 19: 0.0, 20: 0.0, 22: 0.46586345381526106, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.4431137724550898, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.22461928934010153
Weighted-average F1 score: 0.19404626781982093
F1 score per class: {1: 0.13253012048192772, 3: 0.42696629213483145, 6: 0.0, 8: 0.0, 14: 0.03680981595092025, 15: 0.0, 19: 0.0, 20: 0.0, 22: 0.463519313304721, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.49142857142857144, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.24026845637583893
Weighted-average F1 score: 0.21054466207126277

F1 score per class: {1: 0.09292035398230089, 3: 0.3209169054441261, 6: 0.3583815028901734, 8: 0.31007751937984496, 14: 0.0273972602739726, 15: 0.3684210526315789, 19: 0.5882352941176471, 20: 0.30526315789473685, 22: 0.49038461538461536, 24: 0.06666666666666667, 25: 0.3076923076923077, 26: 0.6175115207373272, 29: 0.7280334728033473, 30: 0.8333333333333334, 32: 0.4664310954063604, 33: 0.18181818181818182, 34: 0.19607843137254902, 35: 0.061855670103092786, 36: 0.34951456310679613, 37: 0.14814814814814814, 38: 0.08}
Micro-average F1 score: 0.3402889245585875
Weighted-average F1 score: 0.3216957470335416
F1 score per class: {1: 0.10551558752997602, 3: 0.24724061810154527, 6: 0.36879432624113473, 8: 0.3657587548638132, 14: 0.04597701149425287, 15: 0.3888888888888889, 19: 0.5460526315789473, 20: 0.3018867924528302, 22: 0.38666666666666666, 24: 0.034482758620689655, 25: 0.43902439024390244, 26: 0.5506072874493927, 29: 0.6714801444043321, 30: 0.45569620253164556, 32: 0.43478260869565216, 33: 0.0851063829787234, 34: 0.22629969418960244, 35: 0.07317073170731707, 36: 0.3876651982378855, 37: 0.1568627450980392, 38: 0.2391304347826087}
Micro-average F1 score: 0.3243356350700983
Weighted-average F1 score: 0.31143475070369125
F1 score per class: {1: 0.10022779043280182, 3: 0.24945295404814005, 6: 0.38141809290953543, 8: 0.37777777777777777, 14: 0.02857142857142857, 15: 0.358974358974359, 19: 0.5594405594405595, 20: 0.29906542056074764, 22: 0.41064638783269963, 24: 0.06349206349206349, 25: 0.37142857142857144, 26: 0.5811965811965812, 29: 0.7137254901960784, 30: 0.7727272727272727, 32: 0.4484848484848485, 33: 0.12195121951219512, 34: 0.20476190476190476, 35: 0.057971014492753624, 36: 0.43478260869565216, 37: 0.19148936170212766, 38: 0.18666666666666668}
Micro-average F1 score: 0.32882882882882886
Weighted-average F1 score: 0.31058862491712386
cur_acc_wo_na:  ['0.7717', '0.5622', '0.5161', '0.3917']
his_acc_wo_na:  ['0.7717', '0.6832', '0.6122', '0.4559']
cur_acc des_wo_na:  ['0.7504', '0.6013', '0.5720', '0.3315']
his_acc des_wo_na:  ['0.7504', '0.6521', '0.5938', '0.4584']
cur_acc rrf_wo_na:  ['0.7685', '0.6168', '0.5649', '0.3527']
his_acc rrf_wo_na:  ['0.7685', '0.6687', '0.5982', '0.4574']
cur_acc_w_na:  ['0.6436', '0.4483', '0.3881', '0.2758']
his_acc_w_na:  ['0.6436', '0.5349', '0.4814', '0.3403']
cur_acc des_w_na:  ['0.5947', '0.4623', '0.3943', '0.2246']
his_acc des_w_na:  ['0.5947', '0.4859', '0.4356', '0.3243']
cur_acc rrf_w_na:  ['0.6152', '0.4835', '0.3977', '0.2403']
his_acc rrf_w_na:  ['0.6152', '0.5065', '0.4483', '0.3288']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 151.6965275CurrentTrain: epoch  0, batch     1 | loss: 99.4418773CurrentTrain: epoch  0, batch     2 | loss: 88.2167214CurrentTrain: epoch  0, batch     3 | loss: 92.5352380CurrentTrain: epoch  0, batch     4 | loss: 121.3372453CurrentTrain: epoch  1, batch     0 | loss: 91.5156312CurrentTrain: epoch  1, batch     1 | loss: 106.9122506CurrentTrain: epoch  1, batch     2 | loss: 95.6644865CurrentTrain: epoch  1, batch     3 | loss: 103.6797566CurrentTrain: epoch  1, batch     4 | loss: 66.0022042CurrentTrain: epoch  2, batch     0 | loss: 90.5467369CurrentTrain: epoch  2, batch     1 | loss: 68.9068617CurrentTrain: epoch  2, batch     2 | loss: 106.6529380CurrentTrain: epoch  2, batch     3 | loss: 102.1218420CurrentTrain: epoch  2, batch     4 | loss: 117.1865820CurrentTrain: epoch  3, batch     0 | loss: 71.7036110CurrentTrain: epoch  3, batch     1 | loss: 98.5708747CurrentTrain: epoch  3, batch     2 | loss: 128.9643363CurrentTrain: epoch  3, batch     3 | loss: 86.6924792CurrentTrain: epoch  3, batch     4 | loss: 81.3899179CurrentTrain: epoch  4, batch     0 | loss: 82.1205965CurrentTrain: epoch  4, batch     1 | loss: 97.1539502CurrentTrain: epoch  4, batch     2 | loss: 123.2691681CurrentTrain: epoch  4, batch     3 | loss: 86.9672122CurrentTrain: epoch  4, batch     4 | loss: 53.6011361CurrentTrain: epoch  5, batch     0 | loss: 126.3560786CurrentTrain: epoch  5, batch     1 | loss: 84.6570700CurrentTrain: epoch  5, batch     2 | loss: 68.5783536CurrentTrain: epoch  5, batch     3 | loss: 99.8443036CurrentTrain: epoch  5, batch     4 | loss: 49.0041577CurrentTrain: epoch  6, batch     0 | loss: 83.6685119CurrentTrain: epoch  6, batch     1 | loss: 79.1225656CurrentTrain: epoch  6, batch     2 | loss: 84.1957301CurrentTrain: epoch  6, batch     3 | loss: 79.5829392CurrentTrain: epoch  6, batch     4 | loss: 63.6240490CurrentTrain: epoch  7, batch     0 | loss: 126.4063680CurrentTrain: epoch  7, batch     1 | loss: 98.5834840CurrentTrain: epoch  7, batch     2 | loss: 77.8208816CurrentTrain: epoch  7, batch     3 | loss: 80.6389868CurrentTrain: epoch  7, batch     4 | loss: 60.9941550CurrentTrain: epoch  8, batch     0 | loss: 97.4021536CurrentTrain: epoch  8, batch     1 | loss: 67.5304954CurrentTrain: epoch  8, batch     2 | loss: 122.9077361CurrentTrain: epoch  8, batch     3 | loss: 80.1517781CurrentTrain: epoch  8, batch     4 | loss: 40.4559572CurrentTrain: epoch  9, batch     0 | loss: 65.2302019CurrentTrain: epoch  9, batch     1 | loss: 95.6627548CurrentTrain: epoch  9, batch     2 | loss: 74.9144763CurrentTrain: epoch  9, batch     3 | loss: 96.0983463CurrentTrain: epoch  9, batch     4 | loss: 108.8703744
MemoryTrain:  epoch  0, batch     0 | loss: 1.0334061MemoryTrain:  epoch  1, batch     0 | loss: 0.8277471MemoryTrain:  epoch  2, batch     0 | loss: 0.6641004MemoryTrain:  epoch  3, batch     0 | loss: 0.5577616MemoryTrain:  epoch  4, batch     0 | loss: 0.4557621MemoryTrain:  epoch  5, batch     0 | loss: 0.3678903MemoryTrain:  epoch  6, batch     0 | loss: 0.3337553MemoryTrain:  epoch  7, batch     0 | loss: 0.2676822MemoryTrain:  epoch  8, batch     0 | loss: 0.2422668MemoryTrain:  epoch  9, batch     0 | loss: 0.2137016

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.8837209302325582, 6: 0.0, 10: 0.4, 14: 0.0, 15: 0.0, 16: 0.7575757575757576, 17: 0.36363636363636365, 18: 0.28169014084507044, 19: 0.0, 20: 0.0, 26: 0.0, 29: 0.0, 34: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5317725752508361
Weighted-average F1 score: 0.475793080017008
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.776, 6: 0.0, 8: 0.0, 10: 0.4861111111111111, 14: 0.0, 15: 0.0, 16: 0.746268656716418, 17: 0.5, 18: 0.21621621621621623, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.4583901773533424
Weighted-average F1 score: 0.3827151623356594
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.8398268398268398, 6: 0.0, 8: 0.0, 10: 0.5, 14: 0.0, 15: 0.0, 16: 0.7058823529411765, 17: 0.5, 18: 0.28205128205128205, 19: 0.0, 20: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.49927431059506533
Weighted-average F1 score: 0.42107735591569806

F1 score per class: {1: 0.1796875, 3: 0.3855421686746988, 5: 0.753968253968254, 6: 0.4577114427860697, 8: 0.09195402298850575, 10: 0.2903225806451613, 14: 0.016, 15: 0.5333333333333333, 16: 0.5747126436781609, 17: 0.23529411764705882, 18: 0.16806722689075632, 19: 0.6756756756756757, 20: 0.46616541353383456, 22: 0.5875706214689266, 24: 0.07692307692307693, 25: 0.3225806451612903, 26: 0.6865671641791045, 29: 0.8341708542713567, 30: 0.8333333333333334, 32: 0.5872340425531914, 33: 0.2727272727272727, 34: 0.20875420875420875, 35: 0.13953488372093023, 36: 0.14084507042253522, 37: 0.1794871794871795, 38: 0.12121212121212122}
Micro-average F1 score: 0.4300822561692127
Weighted-average F1 score: 0.4324710221770588
F1 score per class: {1: 0.18565400843881857, 3: 0.35714285714285715, 5: 0.5969230769230769, 6: 0.5468164794007491, 8: 0.4305555555555556, 10: 0.358974358974359, 14: 0.04819277108433735, 15: 0.45161290322580644, 16: 0.5813953488372093, 17: 0.2727272727272727, 18: 0.14545454545454545, 19: 0.5992779783393501, 20: 0.5081967213114754, 22: 0.5178571428571429, 24: 0.0967741935483871, 25: 0.42105263157894735, 26: 0.6666666666666666, 29: 0.8504672897196262, 30: 0.4857142857142857, 32: 0.5655172413793104, 33: 0.125, 34: 0.22591362126245848, 35: 0.18604651162790697, 36: 0.5217391304347826, 37: 0.1610738255033557, 38: 0.38596491228070173}
Micro-average F1 score: 0.4343629343629344
Weighted-average F1 score: 0.42180017993219426
F1 score per class: {1: 0.1862348178137652, 3: 0.37209302325581395, 5: 0.6736111111111112, 6: 0.5491803278688525, 8: 0.2909090909090909, 10: 0.3394495412844037, 14: 0.07777777777777778, 15: 0.4375, 16: 0.5217391304347826, 17: 0.2857142857142857, 18: 0.16666666666666666, 19: 0.6374501992031872, 20: 0.496, 22: 0.5208333333333334, 24: 0.0851063829787234, 25: 0.4057971014492754, 26: 0.6792452830188679, 29: 0.8571428571428571, 30: 0.7906976744186046, 32: 0.5735294117647058, 33: 0.1935483870967742, 34: 0.21686746987951808, 35: 0.19130434782608696, 36: 0.3076923076923077, 37: 0.2222222222222222, 38: 0.10526315789473684}
Micro-average F1 score: 0.4317771472788238
Weighted-average F1 score: 0.4203883999193712

F1 score per class: {1: 0.0, 3: 0.0, 5: 0.7089552238805971, 6: 0.0, 8: 0.0, 10: 0.3776223776223776, 14: 0.0, 15: 0.0, 16: 0.47619047619047616, 17: 0.3333333333333333, 18: 0.2247191011235955, 19: 0.0, 20: 0.0, 26: 0.0, 29: 0.0, 33: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.375
Weighted-average F1 score: 0.32122713080841825
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.5878787878787879, 6: 0.0, 8: 0.0, 10: 0.4430379746835443, 14: 0.0, 15: 0.0, 16: 0.47619047619047616, 17: 0.375, 18: 0.17582417582417584, 19: 0.0, 20: 0.0, 22: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3035230352303523
Weighted-average F1 score: 0.25119388080933575
F1 score per class: {1: 0.0, 3: 0.0, 5: 0.6402640264026402, 6: 0.0, 8: 0.0, 10: 0.4431137724550898, 14: 0.0, 15: 0.0, 16: 0.4485981308411215, 17: 0.4, 18: 0.22, 19: 0.0, 20: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.3359375
Weighted-average F1 score: 0.2813693531938413

F1 score per class: {1: 0.09583333333333334, 3: 0.2962962962962963, 5: 0.5352112676056338, 6: 0.3006535947712418, 8: 0.08791208791208792, 10: 0.24107142857142858, 14: 0.012903225806451613, 15: 0.34782608695652173, 16: 0.3067484662576687, 17: 0.16666666666666666, 18: 0.10752688172043011, 19: 0.5813953488372093, 20: 0.2672413793103448, 22: 0.5252525252525253, 24: 0.06666666666666667, 25: 0.3076923076923077, 26: 0.5822784810126582, 29: 0.6831275720164609, 30: 0.7894736842105263, 32: 0.4394904458598726, 33: 0.2, 34: 0.13507625272331156, 35: 0.10909090909090909, 36: 0.12987012987012986, 37: 0.1686746987951807, 38: 0.0975609756097561}
Micro-average F1 score: 0.31409568762068224
Weighted-average F1 score: 0.3031408148383292
F1 score per class: {1: 0.0967032967032967, 3: 0.23668639053254437, 5: 0.383399209486166, 6: 0.32516703786191536, 8: 0.34831460674157305, 10: 0.2681992337164751, 14: 0.03669724770642202, 15: 0.2978723404255319, 16: 0.3424657534246575, 17: 0.11764705882352941, 18: 0.09696969696969697, 19: 0.47564469914040114, 20: 0.28703703703703703, 22: 0.4496124031007752, 24: 0.05714285714285714, 25: 0.4050632911392405, 26: 0.5509433962264151, 29: 0.6476868327402135, 30: 0.38636363636363635, 32: 0.4194373401534527, 33: 0.06896551724137931, 34: 0.14225941422594143, 35: 0.11428571428571428, 36: 0.3973509933774834, 37: 0.10480349344978165, 38: 0.23157894736842105}
Micro-average F1 score: 0.2966869952200429
Weighted-average F1 score: 0.28403686696199704
F1 score per class: {1: 0.09871244635193133, 3: 0.26229508196721313, 5: 0.4311111111111111, 6: 0.33668341708542715, 8: 0.24806201550387597, 10: 0.2534246575342466, 14: 0.058823529411764705, 15: 0.2857142857142857, 16: 0.294478527607362, 17: 0.13953488372093023, 18: 0.11, 19: 0.5161290322580645, 20: 0.2767857142857143, 22: 0.44642857142857145, 24: 0.06349206349206349, 25: 0.3888888888888889, 26: 0.5691699604743083, 29: 0.6744186046511628, 30: 0.68, 32: 0.4250681198910082, 33: 0.125, 34: 0.13432835820895522, 35: 0.11956521739130435, 36: 0.24742268041237114, 37: 0.18518518518518517, 38: 0.07017543859649122}
Micro-average F1 score: 0.29978510028653294
Weighted-average F1 score: 0.2860895141852134
cur_acc_wo_na:  ['0.7717', '0.5622', '0.5161', '0.3917', '0.5318']
his_acc_wo_na:  ['0.7717', '0.6832', '0.6122', '0.4559', '0.4301']
cur_acc des_wo_na:  ['0.7504', '0.6013', '0.5720', '0.3315', '0.4584']
his_acc des_wo_na:  ['0.7504', '0.6521', '0.5938', '0.4584', '0.4344']
cur_acc rrf_wo_na:  ['0.7685', '0.6168', '0.5649', '0.3527', '0.4993']
his_acc rrf_wo_na:  ['0.7685', '0.6687', '0.5982', '0.4574', '0.4318']
cur_acc_w_na:  ['0.6436', '0.4483', '0.3881', '0.2758', '0.3750']
his_acc_w_na:  ['0.6436', '0.5349', '0.4814', '0.3403', '0.3141']
cur_acc des_w_na:  ['0.5947', '0.4623', '0.3943', '0.2246', '0.3035']
his_acc des_w_na:  ['0.5947', '0.4859', '0.4356', '0.3243', '0.2967']
cur_acc rrf_w_na:  ['0.6152', '0.4835', '0.3977', '0.2403', '0.3359']
his_acc rrf_w_na:  ['0.6152', '0.5065', '0.4483', '0.3288', '0.2998']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 105.9853912CurrentTrain: epoch  0, batch     1 | loss: 143.8408427CurrentTrain: epoch  0, batch     2 | loss: 112.7021370CurrentTrain: epoch  0, batch     3 | loss: 95.7510684CurrentTrain: epoch  1, batch     0 | loss: 144.4676236CurrentTrain: epoch  1, batch     1 | loss: 77.9398539CurrentTrain: epoch  1, batch     2 | loss: 103.9447918CurrentTrain: epoch  1, batch     3 | loss: 73.1984714CurrentTrain: epoch  2, batch     0 | loss: 104.9088567CurrentTrain: epoch  2, batch     1 | loss: 74.9694580CurrentTrain: epoch  2, batch     2 | loss: 102.4581316CurrentTrain: epoch  2, batch     3 | loss: 59.6636201CurrentTrain: epoch  3, batch     0 | loss: 70.7757821CurrentTrain: epoch  3, batch     1 | loss: 86.1017688CurrentTrain: epoch  3, batch     2 | loss: 72.0855894CurrentTrain: epoch  3, batch     3 | loss: 71.6167794CurrentTrain: epoch  4, batch     0 | loss: 83.9232472CurrentTrain: epoch  4, batch     1 | loss: 84.1768529CurrentTrain: epoch  4, batch     2 | loss: 86.2400787CurrentTrain: epoch  4, batch     3 | loss: 62.4740429CurrentTrain: epoch  5, batch     0 | loss: 98.5100338CurrentTrain: epoch  5, batch     1 | loss: 85.5377285CurrentTrain: epoch  5, batch     2 | loss: 122.9147606CurrentTrain: epoch  5, batch     3 | loss: 61.1425703CurrentTrain: epoch  6, batch     0 | loss: 84.8655377CurrentTrain: epoch  6, batch     1 | loss: 98.1016718CurrentTrain: epoch  6, batch     2 | loss: 98.3759787CurrentTrain: epoch  6, batch     3 | loss: 51.0899986CurrentTrain: epoch  7, batch     0 | loss: 64.6729829CurrentTrain: epoch  7, batch     1 | loss: 94.4273545CurrentTrain: epoch  7, batch     2 | loss: 97.8735346CurrentTrain: epoch  7, batch     3 | loss: 83.7447478CurrentTrain: epoch  8, batch     0 | loss: 75.2548950CurrentTrain: epoch  8, batch     1 | loss: 76.1189514CurrentTrain: epoch  8, batch     2 | loss: 71.7939844CurrentTrain: epoch  8, batch     3 | loss: 99.1510980CurrentTrain: epoch  9, batch     0 | loss: 64.8142465CurrentTrain: epoch  9, batch     1 | loss: 67.0662742CurrentTrain: epoch  9, batch     2 | loss: 93.4716198CurrentTrain: epoch  9, batch     3 | loss: 77.4470644
MemoryTrain:  epoch  0, batch     0 | loss: 0.9025866MemoryTrain:  epoch  1, batch     0 | loss: 0.6809347MemoryTrain:  epoch  2, batch     0 | loss: 0.5790903MemoryTrain:  epoch  3, batch     0 | loss: 0.5398957MemoryTrain:  epoch  4, batch     0 | loss: 0.4509027MemoryTrain:  epoch  5, batch     0 | loss: 0.3848948MemoryTrain:  epoch  6, batch     0 | loss: 0.3139807MemoryTrain:  epoch  7, batch     0 | loss: 0.2765183MemoryTrain:  epoch  8, batch     0 | loss: 0.2502521MemoryTrain:  epoch  9, batch     0 | loss: 0.2162413

F1 score per class: {0: 0.8095238095238095, 1: 0.0, 3: 0.0, 4: 0.88268156424581, 5: 0.0, 6: 0.0, 10: 0.0, 13: 0.19047619047619047, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.5454545454545454, 23: 0.7640449438202247, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0}
Micro-average F1 score: 0.6626262626262627
Weighted-average F1 score: 0.5673013760073295
F1 score per class: {0: 0.7058823529411765, 1: 0.0, 3: 0.0, 4: 0.8497409326424871, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.4, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.5, 22: 0.0, 23: 0.6506024096385542, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.5478547854785478
Weighted-average F1 score: 0.4477459751914915
F1 score per class: {0: 0.7741935483870968, 1: 0.0, 3: 0.0, 4: 0.8804347826086957, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.2222222222222222, 14: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.5194805194805194, 23: 0.6428571428571429, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0}
Micro-average F1 score: 0.5855379188712522
Weighted-average F1 score: 0.483809992851628

F1 score per class: {0: 0.40718562874251496, 1: 0.1825726141078838, 3: 0.4093567251461988, 4: 0.88268156424581, 5: 0.7723577235772358, 6: 0.45794392523364486, 8: 0.07142857142857142, 10: 0.3502824858757062, 13: 0.032520325203252036, 14: 0.0273972602739726, 15: 0.5454545454545454, 16: 0.647887323943662, 17: 0.3, 18: 0.11904761904761904, 19: 0.6666666666666666, 20: 0.535031847133758, 21: 0.15306122448979592, 22: 0.5859872611464968, 23: 0.6732673267326733, 24: 0.0, 25: 0.3225806451612903, 26: 0.6700507614213198, 29: 0.7938144329896907, 30: 0.8648648648648649, 32: 0.5783132530120482, 33: 0.2608695652173913, 34: 0.1647940074906367, 35: 0.1308411214953271, 36: 0.08695652173913043, 37: 0.14285714285714285, 38: 0.12121212121212122}
Micro-average F1 score: 0.43784051510648836
Weighted-average F1 score: 0.4273381793877925
F1 score per class: {0: 0.26865671641791045, 1: 0.19574468085106383, 3: 0.25296442687747034, 4: 0.8497409326424871, 5: 0.6217948717948718, 6: 0.5220588235294118, 8: 0.4217687074829932, 10: 0.3192488262910798, 13: 0.07692307692307693, 14: 0.045454545454545456, 15: 0.631578947368421, 16: 0.5454545454545454, 17: 0.2857142857142857, 18: 0.14814814814814814, 19: 0.608058608058608, 20: 0.5151515151515151, 21: 0.15019762845849802, 22: 0.5024630541871922, 23: 0.5684210526315789, 24: 0.09090909090909091, 25: 0.40540540540540543, 26: 0.6238532110091743, 29: 0.8262910798122066, 30: 0.44155844155844154, 32: 0.570446735395189, 33: 0.2222222222222222, 34: 0.20253164556962025, 35: 0.19653179190751446, 36: 0.509090909090909, 37: 0.1981981981981982, 38: 0.36363636363636365}
Micro-average F1 score: 0.42419255297263936
Weighted-average F1 score: 0.40385161802313285
F1 score per class: {0: 0.3050847457627119, 1: 0.19591836734693877, 3: 0.29535864978902954, 4: 0.8804347826086957, 5: 0.6830985915492958, 6: 0.536, 8: 0.17204301075268819, 10: 0.29310344827586204, 13: 0.044444444444444446, 14: 0.03636363636363636, 15: 0.631578947368421, 16: 0.5517241379310345, 17: 0.3, 18: 0.15217391304347827, 19: 0.6142322097378277, 20: 0.46258503401360546, 21: 0.14705882352941177, 22: 0.5402298850574713, 23: 0.5625, 24: 0.09523809523809523, 25: 0.4411764705882353, 26: 0.6296296296296297, 29: 0.8415841584158416, 30: 0.8292682926829268, 32: 0.5857142857142857, 33: 0.20689655172413793, 34: 0.19157088122605365, 35: 0.1891891891891892, 36: 0.23376623376623376, 37: 0.2, 38: 0.11764705882352941}
Micro-average F1 score: 0.4202961672473868
Weighted-average F1 score: 0.40252874582493403

F1 score per class: {0: 0.7391304347826086, 1: 0.0, 3: 0.0, 4: 0.8586956521739131, 5: 0.0, 6: 0.0, 10: 0.0, 13: 0.1111111111111111, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.38961038961038963, 23: 0.6355140186915887, 24: 0.0, 26: 0.0, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0}
Micro-average F1 score: 0.4992389649923896
Weighted-average F1 score: 0.39423334595805465
F1 score per class: {0: 0.6, 1: 0.0, 3: 0.0, 4: 0.8241206030150754, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.26666666666666666, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.3877551020408163, 22: 0.0, 23: 0.574468085106383, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0}
Micro-average F1 score: 0.4019370460048426
Weighted-average F1 score: 0.3086372907622011
F1 score per class: {0: 0.6666666666666666, 1: 0.0, 3: 0.0, 4: 0.8526315789473684, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 13: 0.14814814814814814, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.40404040404040403, 22: 0.0, 23: 0.5454545454545454, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0}
Micro-average F1 score: 0.4289405684754522
Weighted-average F1 score: 0.3306278312928512

F1 score per class: {0: 0.2698412698412698, 1: 0.09606986899563319, 3: 0.29914529914529914, 4: 0.8449197860962567, 5: 0.5571847507331378, 6: 0.2916666666666667, 8: 0.07058823529411765, 10: 0.25203252032520324, 13: 0.016877637130801686, 14: 0.02631578947368421, 15: 0.41379310344827586, 16: 0.37398373983739835, 17: 0.18181818181818182, 18: 0.07633587786259542, 19: 0.5779467680608364, 20: 0.2876712328767123, 21: 0.09836065573770492, 22: 0.5609756097560976, 23: 0.5190839694656488, 24: 0.0, 25: 0.3076923076923077, 26: 0.5892857142857143, 29: 0.6553191489361702, 30: 0.8, 32: 0.43373493975903615, 33: 0.1935483870967742, 34: 0.1111111111111111, 35: 0.09210526315789473, 36: 0.08333333333333333, 37: 0.13157894736842105, 38: 0.10526315789473684}
Micro-average F1 score: 0.3152639087018545
Weighted-average F1 score: 0.2938571497062124
F1 score per class: {0: 0.17307692307692307, 1: 0.10267857142857142, 3: 0.16161616161616163, 4: 0.8, 5: 0.4226579520697168, 6: 0.3148558758314856, 8: 0.32978723404255317, 10: 0.21656050955414013, 13: 0.0380952380952381, 14: 0.03773584905660377, 15: 0.5217391304347826, 16: 0.31788079470198677, 17: 0.13043478260869565, 18: 0.1, 19: 0.4811594202898551, 20: 0.2821576763485477, 21: 0.09743589743589744, 22: 0.43037974683544306, 23: 0.4778761061946903, 24: 0.07692307692307693, 25: 0.38961038961038963, 26: 0.525096525096525, 29: 0.6308243727598566, 30: 0.3541666666666667, 32: 0.41604010025062654, 33: 0.13333333333333333, 34: 0.12972972972972974, 35: 0.11929824561403508, 36: 0.40875912408759124, 37: 0.15172413793103448, 38: 0.21052631578947367}
Micro-average F1 score: 0.29293933797414407
Weighted-average F1 score: 0.27417994322329814
F1 score per class: {0: 0.1935483870967742, 1: 0.10412147505422993, 3: 0.19021739130434784, 4: 0.8307692307692308, 5: 0.46190476190476193, 6: 0.3213429256594724, 8: 0.16, 10: 0.19373219373219372, 13: 0.023668639053254437, 14: 0.031496062992125984, 15: 0.5217391304347826, 16: 0.31788079470198677, 17: 0.17142857142857143, 18: 0.1044776119402985, 19: 0.4939759036144578, 20: 0.2518518518518518, 21: 0.0954653937947494, 22: 0.48205128205128206, 23: 0.43902439024390244, 24: 0.08, 25: 0.4225352112676056, 26: 0.5418326693227091, 29: 0.6772908366533864, 30: 0.68, 32: 0.42487046632124353, 33: 0.12, 34: 0.12376237623762376, 35: 0.1222707423580786, 36: 0.1956521739130435, 37: 0.18604651162790697, 38: 0.0851063829787234}
Micro-average F1 score: 0.29224712295578437
Weighted-average F1 score: 0.2721375104188334
cur_acc_wo_na:  ['0.7717', '0.5622', '0.5161', '0.3917', '0.5318', '0.6626']
his_acc_wo_na:  ['0.7717', '0.6832', '0.6122', '0.4559', '0.4301', '0.4378']
cur_acc des_wo_na:  ['0.7504', '0.6013', '0.5720', '0.3315', '0.4584', '0.5479']
his_acc des_wo_na:  ['0.7504', '0.6521', '0.5938', '0.4584', '0.4344', '0.4242']
cur_acc rrf_wo_na:  ['0.7685', '0.6168', '0.5649', '0.3527', '0.4993', '0.5855']
his_acc rrf_wo_na:  ['0.7685', '0.6687', '0.5982', '0.4574', '0.4318', '0.4203']
cur_acc_w_na:  ['0.6436', '0.4483', '0.3881', '0.2758', '0.3750', '0.4992']
his_acc_w_na:  ['0.6436', '0.5349', '0.4814', '0.3403', '0.3141', '0.3153']
cur_acc des_w_na:  ['0.5947', '0.4623', '0.3943', '0.2246', '0.3035', '0.4019']
his_acc des_w_na:  ['0.5947', '0.4859', '0.4356', '0.3243', '0.2967', '0.2929']
cur_acc rrf_w_na:  ['0.6152', '0.4835', '0.3977', '0.2403', '0.3359', '0.4289']
his_acc rrf_w_na:  ['0.6152', '0.5065', '0.4483', '0.3288', '0.2998', '0.2922']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 91.9293380CurrentTrain: epoch  0, batch     1 | loss: 118.6465483CurrentTrain: epoch  0, batch     2 | loss: 112.5209713CurrentTrain: epoch  0, batch     3 | loss: 116.4367558CurrentTrain: epoch  0, batch     4 | loss: 28.9031404CurrentTrain: epoch  1, batch     0 | loss: 92.9735522CurrentTrain: epoch  1, batch     1 | loss: 135.5570545CurrentTrain: epoch  1, batch     2 | loss: 74.1591554CurrentTrain: epoch  1, batch     3 | loss: 183.5152949CurrentTrain: epoch  1, batch     4 | loss: 28.7002170CurrentTrain: epoch  2, batch     0 | loss: 109.3696820CurrentTrain: epoch  2, batch     1 | loss: 103.9543231CurrentTrain: epoch  2, batch     2 | loss: 97.8164753CurrentTrain: epoch  2, batch     3 | loss: 85.4718380CurrentTrain: epoch  2, batch     4 | loss: 15.5427519CurrentTrain: epoch  3, batch     0 | loss: 84.0932890CurrentTrain: epoch  3, batch     1 | loss: 83.7708261CurrentTrain: epoch  3, batch     2 | loss: 82.4693778CurrentTrain: epoch  3, batch     3 | loss: 81.8487121CurrentTrain: epoch  3, batch     4 | loss: 27.2540067CurrentTrain: epoch  4, batch     0 | loss: 70.5630720CurrentTrain: epoch  4, batch     1 | loss: 64.6865148CurrentTrain: epoch  4, batch     2 | loss: 86.3528346CurrentTrain: epoch  4, batch     3 | loss: 83.4833159CurrentTrain: epoch  4, batch     4 | loss: 42.4437143CurrentTrain: epoch  5, batch     0 | loss: 68.2196116CurrentTrain: epoch  5, batch     1 | loss: 123.2187481CurrentTrain: epoch  5, batch     2 | loss: 81.9858772CurrentTrain: epoch  5, batch     3 | loss: 78.6862832CurrentTrain: epoch  5, batch     4 | loss: 25.7280750CurrentTrain: epoch  6, batch     0 | loss: 79.6719706CurrentTrain: epoch  6, batch     1 | loss: 66.7056785CurrentTrain: epoch  6, batch     2 | loss: 77.2665180CurrentTrain: epoch  6, batch     3 | loss: 97.9906557CurrentTrain: epoch  6, batch     4 | loss: 40.2743365CurrentTrain: epoch  7, batch     0 | loss: 79.6908923CurrentTrain: epoch  7, batch     1 | loss: 67.7359386CurrentTrain: epoch  7, batch     2 | loss: 78.7702424CurrentTrain: epoch  7, batch     3 | loss: 77.9012353CurrentTrain: epoch  7, batch     4 | loss: 11.8904694CurrentTrain: epoch  8, batch     0 | loss: 65.2436336CurrentTrain: epoch  8, batch     1 | loss: 94.1198382CurrentTrain: epoch  8, batch     2 | loss: 80.6082579CurrentTrain: epoch  8, batch     3 | loss: 64.6011942CurrentTrain: epoch  8, batch     4 | loss: 24.7202583CurrentTrain: epoch  9, batch     0 | loss: 79.7929225CurrentTrain: epoch  9, batch     1 | loss: 91.7350484CurrentTrain: epoch  9, batch     2 | loss: 77.4688761CurrentTrain: epoch  9, batch     3 | loss: 75.7656799CurrentTrain: epoch  9, batch     4 | loss: 24.2859457
MemoryTrain:  epoch  0, batch     0 | loss: 0.8519881MemoryTrain:  epoch  1, batch     0 | loss: 0.8137141MemoryTrain:  epoch  2, batch     0 | loss: 0.6460978MemoryTrain:  epoch  3, batch     0 | loss: 0.5996029MemoryTrain:  epoch  4, batch     0 | loss: 0.4719860MemoryTrain:  epoch  5, batch     0 | loss: 0.4249083MemoryTrain:  epoch  6, batch     0 | loss: 0.3439234MemoryTrain:  epoch  7, batch     0 | loss: 0.3273952MemoryTrain:  epoch  8, batch     0 | loss: 0.2912050MemoryTrain:  epoch  9, batch     0 | loss: 0.2608144

F1 score per class: {0: 0.0, 2: 0.5833333333333334, 3: 0.0, 5: 0.0, 6: 0.0, 10: 0.0, 11: 0.43410852713178294, 12: 0.72, 13: 0.0, 15: 0.0, 16: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 28: 0.29411764705882354, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 38: 0.0, 39: 0.21052631578947367}
Micro-average F1 score: 0.4449152542372881
Weighted-average F1 score: 0.35275944669912995
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.4666666666666667, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.352, 12: 0.6458333333333334, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 28: 0.38095238095238093, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.4444444444444444}
Micro-average F1 score: 0.3482758620689655
Weighted-average F1 score: 0.25753691245658006
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.5384615384615384, 3: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.4122137404580153, 12: 0.6524064171122995, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 28: 0.37037037037037035, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.4}
Micro-average F1 score: 0.3860294117647059
Weighted-average F1 score: 0.29253979175183975

F1 score per class: {0: 0.5, 1: 0.18181818181818182, 2: 0.2857142857142857, 3: 0.4720496894409938, 4: 0.8439306358381503, 5: 0.8034188034188035, 6: 0.5196850393700787, 8: 0.07058823529411765, 10: 0.41916167664670656, 11: 0.1830065359477124, 12: 0.35795454545454547, 13: 0.020618556701030927, 14: 0.0, 15: 0.3333333333333333, 16: 0.5373134328358209, 17: 0.0, 18: 0.0, 19: 0.6929824561403509, 20: 0.52, 21: 0.19900497512437812, 22: 0.49645390070921985, 23: 0.7115384615384616, 24: 0.0, 25: 0.3492063492063492, 26: 0.6844919786096256, 28: 0.05917159763313609, 29: 0.7578947368421053, 30: 0.8205128205128205, 32: 0.6046511627906976, 33: 0.0, 34: 0.16071428571428573, 35: 0.14583333333333334, 36: 0.11428571428571428, 37: 0.15151515151515152, 38: 0.17142857142857143, 39: 0.1}
Micro-average F1 score: 0.41770519982810483
Weighted-average F1 score: 0.4065593694593696
F1 score per class: {0: 0.2891566265060241, 1: 0.18867924528301888, 2: 0.15217391304347827, 3: 0.32857142857142857, 4: 0.8342245989304813, 5: 0.6488294314381271, 6: 0.47126436781609193, 8: 0.4431818181818182, 10: 0.4069264069264069, 11: 0.15120274914089346, 12: 0.26666666666666666, 13: 0.0392156862745098, 14: 0.034482758620689655, 15: 0.34285714285714286, 16: 0.5352112676056338, 17: 0.0, 18: 0.037037037037037035, 19: 0.6021505376344086, 20: 0.4745762711864407, 21: 0.12389380530973451, 22: 0.4397163120567376, 23: 0.6542056074766355, 24: 0.08333333333333333, 25: 0.3888888888888889, 26: 0.6567164179104478, 28: 0.14035087719298245, 29: 0.7817258883248731, 30: 0.6101694915254238, 32: 0.5283018867924528, 33: 0.24, 34: 0.16666666666666666, 35: 0.15503875968992248, 36: 0.42857142857142855, 37: 0.2222222222222222, 38: 0.3870967741935484, 39: 0.19672131147540983}
Micro-average F1 score: 0.38788092302272326
Weighted-average F1 score: 0.3676921632379446
F1 score per class: {0: 0.37305699481865284, 1: 0.18604651162790697, 2: 0.20588235294117646, 3: 0.368, 4: 0.8603351955307262, 5: 0.7265917602996255, 6: 0.5187713310580204, 8: 0.17475728155339806, 10: 0.4392523364485981, 11: 0.17088607594936708, 12: 0.28438228438228436, 13: 0.031746031746031744, 14: 0.019801980198019802, 15: 0.3157894736842105, 16: 0.5205479452054794, 17: 0.0, 18: 0.03636363636363636, 19: 0.6058394160583942, 20: 0.48854961832061067, 21: 0.1308411214953271, 22: 0.45255474452554745, 23: 0.64, 24: 0.08695652173913043, 25: 0.3939393939393939, 26: 0.6666666666666666, 28: 0.078125, 29: 0.7794871794871795, 30: 0.7391304347826086, 32: 0.5478547854785478, 33: 0.25, 34: 0.16666666666666666, 35: 0.16666666666666666, 36: 0.1794871794871795, 37: 0.21621621621621623, 38: 0.25, 39: 0.14705882352941177}
Micro-average F1 score: 0.39118975903614456
Weighted-average F1 score: 0.37335891741625865

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.3888888888888889, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.35668789808917195, 12: 0.5833333333333334, 13: 0.0, 15: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 24: 0.0, 26: 0.0, 28: 0.13157894736842105, 29: 0.0, 32: 0.0, 34: 0.0, 35: 0.0, 37: 0.0, 38: 0.0, 39: 0.16}
Micro-average F1 score: 0.2995720399429387
Weighted-average F1 score: 0.23544709555160204
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.27450980392156865, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.28205128205128205, 12: 0.4881889763779528, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.1702127659574468, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.3076923076923077}
Micro-average F1 score: 0.22197802197802197
Weighted-average F1 score: 0.1739806177583877
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.30434782608695654, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 8: 0.0, 10: 0.0, 11: 0.3312883435582822, 12: 0.5062240663900415, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.14925373134328357, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.2777777777777778}
Micro-average F1 score: 0.2527075812274368
Weighted-average F1 score: 0.20000354567557066

F1 score per class: {0: 0.33497536945812806, 1: 0.09259259259259259, 2: 0.17721518987341772, 3: 0.36363636363636365, 4: 0.8021978021978022, 5: 0.6287625418060201, 6: 0.29398663697104677, 8: 0.06741573033707865, 10: 0.2978723404255319, 11: 0.1222707423580786, 12: 0.1431818181818182, 13: 0.009852216748768473, 14: 0.0, 15: 0.23076923076923078, 16: 0.3025210084033613, 17: 0.0, 18: 0.0, 19: 0.6245059288537549, 20: 0.2582781456953642, 21: 0.132013201320132, 22: 0.4861111111111111, 23: 0.5736434108527132, 24: 0.0, 25: 0.3333333333333333, 26: 0.5953488372093023, 28: 0.03184713375796178, 29: 0.6343612334801763, 30: 0.7619047619047619, 32: 0.4444444444444444, 33: 0.0, 34: 0.12857142857142856, 35: 0.1044776119402985, 36: 0.10666666666666667, 37: 0.14285714285714285, 38: 0.13636363636363635, 39: 0.06060606060606061}
Micro-average F1 score: 0.27923010629129563
Weighted-average F1 score: 0.25507580012431624
F1 score per class: {0: 0.1925133689839572, 1: 0.0966183574879227, 2: 0.07608695652173914, 3: 0.21100917431192662, 4: 0.78, 5: 0.43891402714932126, 6: 0.26366559485530544, 8: 0.30708661417322836, 10: 0.26628895184135976, 11: 0.10352941176470588, 12: 0.11061552185548618, 13: 0.022222222222222223, 14: 0.0273972602739726, 15: 0.2727272727272727, 16: 0.336283185840708, 17: 0.0, 18: 0.024691358024691357, 19: 0.509090909090909, 20: 0.25925925925925924, 21: 0.08187134502923976, 22: 0.4189189189189189, 23: 0.4666666666666667, 24: 0.06896551724137931, 25: 0.36363636363636365, 26: 0.559322033898305, 28: 0.06666666666666667, 29: 0.6553191489361702, 30: 0.5070422535211268, 32: 0.375, 33: 0.17647058823529413, 34: 0.11904761904761904, 35: 0.09852216748768473, 36: 0.3333333333333333, 37: 0.19130434782608696, 38: 0.24242424242424243, 39: 0.10619469026548672}
Micro-average F1 score: 0.2513698630136986
Weighted-average F1 score: 0.23124197504447466
F1 score per class: {0: 0.25263157894736843, 1: 0.09456264775413711, 2: 0.10687022900763359, 3: 0.2440318302387268, 4: 0.8148148148148148, 5: 0.5052083333333334, 6: 0.30218687872763417, 8: 0.15126050420168066, 10: 0.2919254658385093, 11: 0.11688311688311688, 12: 0.11607992388201713, 13: 0.017857142857142856, 14: 0.016666666666666666, 15: 0.2222222222222222, 16: 0.31932773109243695, 17: 0.0, 18: 0.023255813953488372, 19: 0.515527950310559, 20: 0.26229508196721313, 21: 0.08641975308641975, 22: 0.44285714285714284, 23: 0.460431654676259, 24: 0.07142857142857142, 25: 0.37142857142857144, 26: 0.5764192139737991, 28: 0.038910505836575876, 29: 0.6551724137931034, 30: 0.6415094339622641, 32: 0.39243498817966904, 33: 0.20689655172413793, 34: 0.125, 35: 0.11612903225806452, 36: 0.15217391304347827, 37: 0.19753086419753085, 38: 0.17647058823529413, 39: 0.08547008547008547}
Micro-average F1 score: 0.2570827662996412
Weighted-average F1 score: 0.23559572651743751
cur_acc_wo_na:  ['0.7717', '0.5622', '0.5161', '0.3917', '0.5318', '0.6626', '0.4449']
his_acc_wo_na:  ['0.7717', '0.6832', '0.6122', '0.4559', '0.4301', '0.4378', '0.4177']
cur_acc des_wo_na:  ['0.7504', '0.6013', '0.5720', '0.3315', '0.4584', '0.5479', '0.3483']
his_acc des_wo_na:  ['0.7504', '0.6521', '0.5938', '0.4584', '0.4344', '0.4242', '0.3879']
cur_acc rrf_wo_na:  ['0.7685', '0.6168', '0.5649', '0.3527', '0.4993', '0.5855', '0.3860']
his_acc rrf_wo_na:  ['0.7685', '0.6687', '0.5982', '0.4574', '0.4318', '0.4203', '0.3912']
cur_acc_w_na:  ['0.6436', '0.4483', '0.3881', '0.2758', '0.3750', '0.4992', '0.2996']
his_acc_w_na:  ['0.6436', '0.5349', '0.4814', '0.3403', '0.3141', '0.3153', '0.2792']
cur_acc des_w_na:  ['0.5947', '0.4623', '0.3943', '0.2246', '0.3035', '0.4019', '0.2220']
his_acc des_w_na:  ['0.5947', '0.4859', '0.4356', '0.3243', '0.2967', '0.2929', '0.2514']
cur_acc rrf_w_na:  ['0.6152', '0.4835', '0.3977', '0.2403', '0.3359', '0.4289', '0.2527']
his_acc rrf_w_na:  ['0.6152', '0.5065', '0.4483', '0.3288', '0.2998', '0.2922', '0.2571']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 84.4300054CurrentTrain: epoch  0, batch     1 | loss: 101.9597482CurrentTrain: epoch  0, batch     2 | loss: 85.4040831CurrentTrain: epoch  0, batch     3 | loss: 25.3032082CurrentTrain: epoch  1, batch     0 | loss: 106.7098756CurrentTrain: epoch  1, batch     1 | loss: 73.8600627CurrentTrain: epoch  1, batch     2 | loss: 89.0854084CurrentTrain: epoch  1, batch     3 | loss: 14.0610607CurrentTrain: epoch  2, batch     0 | loss: 80.0240373CurrentTrain: epoch  2, batch     1 | loss: 69.2257420CurrentTrain: epoch  2, batch     2 | loss: 107.8685491CurrentTrain: epoch  2, batch     3 | loss: 6.4113239CurrentTrain: epoch  3, batch     0 | loss: 63.5714795CurrentTrain: epoch  3, batch     1 | loss: 97.5340415CurrentTrain: epoch  3, batch     2 | loss: 86.4742686CurrentTrain: epoch  3, batch     3 | loss: 17.6442290CurrentTrain: epoch  4, batch     0 | loss: 77.8978200CurrentTrain: epoch  4, batch     1 | loss: 68.4160297CurrentTrain: epoch  4, batch     2 | loss: 77.2776201CurrentTrain: epoch  4, batch     3 | loss: 16.6647654CurrentTrain: epoch  5, batch     0 | loss: 74.0705133CurrentTrain: epoch  5, batch     1 | loss: 81.8377023CurrentTrain: epoch  5, batch     2 | loss: 67.8590695CurrentTrain: epoch  5, batch     3 | loss: 7.6047726CurrentTrain: epoch  6, batch     0 | loss: 64.7331383CurrentTrain: epoch  6, batch     1 | loss: 78.4765780CurrentTrain: epoch  6, batch     2 | loss: 73.7081634CurrentTrain: epoch  6, batch     3 | loss: 11.9493163CurrentTrain: epoch  7, batch     0 | loss: 64.1273303CurrentTrain: epoch  7, batch     1 | loss: 61.6620235CurrentTrain: epoch  7, batch     2 | loss: 77.2600816CurrentTrain: epoch  7, batch     3 | loss: 9.2281031CurrentTrain: epoch  8, batch     0 | loss: 76.3719162CurrentTrain: epoch  8, batch     1 | loss: 62.9139087CurrentTrain: epoch  8, batch     2 | loss: 73.7060248CurrentTrain: epoch  8, batch     3 | loss: 4.0375669CurrentTrain: epoch  9, batch     0 | loss: 63.0944584CurrentTrain: epoch  9, batch     1 | loss: 72.4771441CurrentTrain: epoch  9, batch     2 | loss: 74.8435596CurrentTrain: epoch  9, batch     3 | loss: 9.2582969
MemoryTrain:  epoch  0, batch     0 | loss: 0.7677654MemoryTrain:  epoch  1, batch     0 | loss: 0.6453742MemoryTrain:  epoch  2, batch     0 | loss: 0.5617145MemoryTrain:  epoch  3, batch     0 | loss: 0.4806868MemoryTrain:  epoch  4, batch     0 | loss: 0.3363463MemoryTrain:  epoch  5, batch     0 | loss: 0.2871350MemoryTrain:  epoch  6, batch     0 | loss: 0.2671005MemoryTrain:  epoch  7, batch     0 | loss: 0.2687408MemoryTrain:  epoch  8, batch     0 | loss: 0.2102570MemoryTrain:  epoch  9, batch     0 | loss: 0.2189307

F1 score per class: {0: 0.0, 1: 0.0, 34: 0.0, 3: 0.0, 37: 0.6, 6: 0.8771929824561403, 7: 0.0, 40: 0.0, 9: 0.0, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.18181818181818182, 26: 0.0, 27: 0.0, 31: 0.4672897196261682}
Micro-average F1 score: 0.38028169014084506
Weighted-average F1 score: 0.2944332068113209
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 6: 0.0, 7: 0.6, 9: 0.684931506849315, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 30: 0.0, 31: 0.16666666666666666, 34: 0.0, 35: 0.0, 37: 0.0, 40: 0.5950413223140496}
Micro-average F1 score: 0.4166666666666667
Weighted-average F1 score: 0.3454325129860056
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 6: 0.0, 7: 0.6, 9: 0.7936507936507936, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 31: 0.16666666666666666, 34: 0.0, 37: 0.0, 40: 0.6}
Micro-average F1 score: 0.436241610738255
Weighted-average F1 score: 0.35452035886818495

F1 score per class: {0: 0.4217687074829932, 1: 0.13793103448275862, 2: 0.34285714285714286, 3: 0.3595505617977528, 4: 0.7577639751552795, 5: 0.749003984063745, 6: 0.272108843537415, 7: 0.0425531914893617, 8: 0.07058823529411765, 9: 0.8620689655172413, 10: 0.272108843537415, 11: 0.17073170731707318, 12: 0.3474903474903475, 13: 0.025974025974025976, 14: 0.0, 15: 0.42857142857142855, 16: 0.5666666666666667, 17: 0.0, 18: 0.0, 19: 0.5528455284552846, 20: 0.5274725274725275, 21: 0.15028901734104047, 22: 0.547945205479452, 23: 0.74, 24: 0.0, 25: 0.4, 26: 0.6850828729281768, 27: 0.0, 28: 0.07575757575757576, 29: 0.7213114754098361, 30: 0.8571428571428571, 31: 0.06896551724137931, 32: 0.5636363636363636, 33: 0.10526315789473684, 34: 0.16806722689075632, 35: 0.08823529411764706, 36: 0.029850746268656716, 37: 0.1643835616438356, 38: 0.11764705882352941, 39: 0.17142857142857143, 40: 0.2604166666666667}
Micro-average F1 score: 0.36759220886862826
Weighted-average F1 score: 0.35212774375306477
F1 score per class: {0: 0.37209302325581395, 1: 0.16363636363636364, 2: 0.1917808219178082, 3: 0.3786407766990291, 4: 0.8044692737430168, 5: 0.5969230769230769, 6: 0.2857142857142857, 7: 0.03333333333333333, 8: 0.390625, 9: 0.4854368932038835, 10: 0.31645569620253167, 11: 0.15447154471544716, 12: 0.25925925925925924, 13: 0.03333333333333333, 14: 0.04, 15: 0.46153846153846156, 16: 0.6, 17: 0.26666666666666666, 18: 0.038461538461538464, 19: 0.5367647058823529, 20: 0.4755244755244755, 21: 0.16071428571428573, 22: 0.5135135135135135, 23: 0.6262626262626263, 24: 0.0, 25: 0.4166666666666667, 26: 0.6597938144329897, 27: 0.0, 28: 0.10526315789473684, 29: 0.7252747252747253, 30: 0.6956521739130435, 31: 0.038461538461538464, 32: 0.5846153846153846, 33: 0.25, 34: 0.2318840579710145, 35: 0.15789473684210525, 36: 0.29545454545454547, 37: 0.1935483870967742, 38: 0.35294117647058826, 39: 0.2, 40: 0.34951456310679613}
Micro-average F1 score: 0.3718914845516202
Weighted-average F1 score: 0.3508650166228135
F1 score per class: {0: 0.3699421965317919, 1: 0.15246636771300448, 2: 0.2857142857142857, 3: 0.37373737373737376, 4: 0.8187134502923976, 5: 0.6339869281045751, 6: 0.2913907284768212, 7: 0.03409090909090909, 8: 0.14285714285714285, 9: 0.704225352112676, 10: 0.32894736842105265, 11: 0.17054263565891473, 12: 0.26053639846743293, 13: 0.030303030303030304, 14: 0.02127659574468085, 15: 0.46153846153846156, 16: 0.6268656716417911, 17: 0.16666666666666666, 18: 0.038461538461538464, 19: 0.584, 20: 0.46153846153846156, 21: 0.16, 22: 0.5, 23: 0.6391752577319587, 24: 0.0, 25: 0.43478260869565216, 26: 0.6632124352331606, 27: 0.0, 28: 0.0963855421686747, 29: 0.7252747252747253, 30: 0.8421052631578947, 31: 0.047619047619047616, 32: 0.5691056910569106, 33: 0.23076923076923078, 34: 0.2318840579710145, 35: 0.136986301369863, 36: 0.15584415584415584, 37: 0.1794871794871795, 38: 0.1111111111111111, 39: 0.125, 40: 0.34951456310679613}
Micro-average F1 score: 0.3676672552481852
Weighted-average F1 score: 0.34912053718919794

F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5, 9: 0.8064516129032258, 12: 0.0, 13: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 31: 0.13333333333333333, 32: 0.0, 34: 0.0, 37: 0.0, 40: 0.43478260869565216}
Micro-average F1 score: 0.3050847457627119
Weighted-average F1 score: 0.22791822502727133
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5, 9: 0.6097560975609756, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 30: 0.0, 31: 0.125, 32: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 40: 0.5373134328358209}
Micro-average F1 score: 0.32663316582914576
Weighted-average F1 score: 0.26358812686823524
F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.5, 9: 0.704225352112676, 10: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 16: 0.0, 19: 0.0, 21: 0.0, 26: 0.0, 27: 0.0, 31: 0.125, 32: 0.0, 34: 0.0, 36: 0.0, 37: 0.0, 40: 0.5538461538461539}
Micro-average F1 score: 0.3439153439153439
Weighted-average F1 score: 0.2699947059982271

F1 score per class: {0: 0.3147208121827411, 1: 0.07567567567567568, 2: 0.20689655172413793, 3: 0.2612244897959184, 4: 0.7176470588235294, 5: 0.5280898876404494, 6: 0.17467248908296942, 7: 0.023529411764705882, 8: 0.06741573033707865, 9: 0.7692307692307693, 10: 0.23668639053254437, 11: 0.11475409836065574, 12: 0.14446227929373998, 13: 0.014492753623188406, 14: 0.0, 15: 0.36363636363636365, 16: 0.33663366336633666, 17: 0.0, 18: 0.0, 19: 0.5271317829457365, 20: 0.26666666666666666, 21: 0.09737827715355805, 22: 0.5405405405405406, 23: 0.6166666666666667, 24: 0.0, 25: 0.37681159420289856, 26: 0.5990338164251208, 27: 0.0, 28: 0.044444444444444446, 29: 0.6346153846153846, 30: 0.7894736842105263, 31: 0.03278688524590164, 32: 0.42758620689655175, 33: 0.08333333333333333, 34: 0.1282051282051282, 35: 0.07058823529411765, 36: 0.028985507246376812, 37: 0.1518987341772152, 38: 0.0975609756097561, 39: 0.10344827586206896, 40: 0.199203187250996}
Micro-average F1 score: 0.2581490104772992
Weighted-average F1 score: 0.23399450777368236
F1 score per class: {0: 0.256, 1: 0.0906801007556675, 2: 0.12173913043478261, 3: 0.2775800711743772, 4: 0.7659574468085106, 5: 0.37093690248565964, 6: 0.20276497695852536, 7: 0.016901408450704224, 8: 0.3067484662576687, 9: 0.3597122302158273, 10: 0.23809523809523808, 11: 0.10079575596816977, 12: 0.12027491408934708, 13: 0.018867924528301886, 14: 0.034482758620689655, 15: 0.375, 16: 0.3652173913043478, 17: 0.14285714285714285, 18: 0.029411764705882353, 19: 0.49491525423728816, 20: 0.25092250922509224, 21: 0.10682492581602374, 22: 0.5033112582781457, 23: 0.496, 24: 0.0, 25: 0.39473684210526316, 26: 0.5688888888888889, 27: 0.0, 28: 0.061855670103092786, 29: 0.6407766990291263, 30: 0.6274509803921569, 31: 0.0196078431372549, 32: 0.4418604651162791, 33: 0.2222222222222222, 34: 0.17777777777777778, 35: 0.1276595744680851, 36: 0.24299065420560748, 37: 0.17307692307692307, 38: 0.20869565217391303, 39: 0.11494252873563218, 40: 0.27586206896551724}
Micro-average F1 score: 0.25925925925925924
Weighted-average F1 score: 0.2357820005033206
F1 score per class: {0: 0.2591093117408907, 1: 0.08436724565756824, 2: 0.175, 3: 0.27715355805243447, 4: 0.7777777777777778, 5: 0.39835728952772076, 6: 0.20276497695852536, 7: 0.017341040462427744, 8: 0.12612612612612611, 9: 0.5882352941176471, 10: 0.25252525252525254, 11: 0.1111111111111111, 12: 0.11764705882352941, 13: 0.01652892561983471, 14: 0.019417475728155338, 15: 0.36363636363636365, 16: 0.3684210526315789, 17: 0.1, 18: 0.029411764705882353, 19: 0.553030303030303, 20: 0.24324324324324326, 21: 0.10495626822157435, 22: 0.4897959183673469, 23: 0.5166666666666667, 24: 0.0, 25: 0.410958904109589, 26: 0.5765765765765766, 27: 0.0, 28: 0.05517241379310345, 29: 0.6407766990291263, 30: 0.7804878048780488, 31: 0.02247191011235955, 32: 0.4268292682926829, 33: 0.20689655172413793, 34: 0.1797752808988764, 35: 0.10989010989010989, 36: 0.14285714285714285, 37: 0.16470588235294117, 38: 0.07017543859649122, 39: 0.07792207792207792, 40: 0.2748091603053435}
Micro-average F1 score: 0.2567826801863524
Weighted-average F1 score: 0.23247165443280954
cur_acc_wo_na:  ['0.7717', '0.5622', '0.5161', '0.3917', '0.5318', '0.6626', '0.4449', '0.3803']
his_acc_wo_na:  ['0.7717', '0.6832', '0.6122', '0.4559', '0.4301', '0.4378', '0.4177', '0.3676']
cur_acc des_wo_na:  ['0.7504', '0.6013', '0.5720', '0.3315', '0.4584', '0.5479', '0.3483', '0.4167']
his_acc des_wo_na:  ['0.7504', '0.6521', '0.5938', '0.4584', '0.4344', '0.4242', '0.3879', '0.3719']
cur_acc rrf_wo_na:  ['0.7685', '0.6168', '0.5649', '0.3527', '0.4993', '0.5855', '0.3860', '0.4362']
his_acc rrf_wo_na:  ['0.7685', '0.6687', '0.5982', '0.4574', '0.4318', '0.4203', '0.3912', '0.3677']
cur_acc_w_na:  ['0.6436', '0.4483', '0.3881', '0.2758', '0.3750', '0.4992', '0.2996', '0.3051']
his_acc_w_na:  ['0.6436', '0.5349', '0.4814', '0.3403', '0.3141', '0.3153', '0.2792', '0.2581']
cur_acc des_w_na:  ['0.5947', '0.4623', '0.3943', '0.2246', '0.3035', '0.4019', '0.2220', '0.3266']
his_acc des_w_na:  ['0.5947', '0.4859', '0.4356', '0.3243', '0.2967', '0.2929', '0.2514', '0.2593']
cur_acc rrf_w_na:  ['0.6152', '0.4835', '0.3977', '0.2403', '0.3359', '0.4289', '0.2527', '0.3439']
his_acc rrf_w_na:  ['0.6152', '0.5065', '0.4483', '0.3288', '0.2998', '0.2922', '0.2571', '0.2568']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/na_test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA'])
CurrentTrain: epoch  0, batch     0 | loss: 108.3715807CurrentTrain: epoch  0, batch     1 | loss: 80.2098559CurrentTrain: epoch  0, batch     2 | loss: 119.8339094CurrentTrain: epoch  0, batch     3 | loss: 87.6112970CurrentTrain: epoch  0, batch     4 | loss: 88.0646976CurrentTrain: epoch  0, batch     5 | loss: 119.2571298CurrentTrain: epoch  0, batch     6 | loss: 101.9607267CurrentTrain: epoch  0, batch     7 | loss: 100.1073973CurrentTrain: epoch  0, batch     8 | loss: 86.9180096CurrentTrain: epoch  0, batch     9 | loss: 99.9724455CurrentTrain: epoch  0, batch    10 | loss: 100.7428697CurrentTrain: epoch  0, batch    11 | loss: 118.5609691CurrentTrain: epoch  0, batch    12 | loss: 86.0865641CurrentTrain: epoch  0, batch    13 | loss: 85.9262768CurrentTrain: epoch  0, batch    14 | loss: 99.5912475CurrentTrain: epoch  0, batch    15 | loss: 86.5486839CurrentTrain: epoch  0, batch    16 | loss: 99.0119190CurrentTrain: epoch  0, batch    17 | loss: 99.9471986CurrentTrain: epoch  0, batch    18 | loss: 99.3149330CurrentTrain: epoch  0, batch    19 | loss: 86.5812927CurrentTrain: epoch  0, batch    20 | loss: 145.8908131CurrentTrain: epoch  0, batch    21 | loss: 118.0145333CurrentTrain: epoch  0, batch    22 | loss: 117.8115921CurrentTrain: epoch  0, batch    23 | loss: 117.6856791CurrentTrain: epoch  0, batch    24 | loss: 75.9031402CurrentTrain: epoch  0, batch    25 | loss: 98.9844342CurrentTrain: epoch  0, batch    26 | loss: 192.1062380CurrentTrain: epoch  0, batch    27 | loss: 98.6989398CurrentTrain: epoch  0, batch    28 | loss: 191.5686191CurrentTrain: epoch  0, batch    29 | loss: 145.6489388CurrentTrain: epoch  0, batch    30 | loss: 117.5900480CurrentTrain: epoch  0, batch    31 | loss: 84.4084249CurrentTrain: epoch  0, batch    32 | loss: 116.4580532CurrentTrain: epoch  0, batch    33 | loss: 117.4461742CurrentTrain: epoch  0, batch    34 | loss: 117.1089417CurrentTrain: epoch  0, batch    35 | loss: 145.8849554CurrentTrain: epoch  0, batch    36 | loss: 96.8672341CurrentTrain: epoch  0, batch    37 | loss: 96.6838260CurrentTrain: epoch  0, batch    38 | loss: 82.4288147CurrentTrain: epoch  0, batch    39 | loss: 98.4319243CurrentTrain: epoch  0, batch    40 | loss: 84.8501754CurrentTrain: epoch  0, batch    41 | loss: 84.7924372CurrentTrain: epoch  0, batch    42 | loss: 83.7895841CurrentTrain: epoch  0, batch    43 | loss: 115.0981767CurrentTrain: epoch  0, batch    44 | loss: 96.6130433CurrentTrain: epoch  0, batch    45 | loss: 96.5302020CurrentTrain: epoch  0, batch    46 | loss: 94.5584013CurrentTrain: epoch  0, batch    47 | loss: 93.1750455CurrentTrain: epoch  0, batch    48 | loss: 82.5621911CurrentTrain: epoch  0, batch    49 | loss: 117.1500621CurrentTrain: epoch  0, batch    50 | loss: 96.1811470CurrentTrain: epoch  0, batch    51 | loss: 83.3693607CurrentTrain: epoch  0, batch    52 | loss: 81.3995441CurrentTrain: epoch  0, batch    53 | loss: 97.6341713CurrentTrain: epoch  0, batch    54 | loss: 95.1561703CurrentTrain: epoch  0, batch    55 | loss: 80.6322863CurrentTrain: epoch  0, batch    56 | loss: 112.0677491CurrentTrain: epoch  0, batch    57 | loss: 140.8397585CurrentTrain: epoch  0, batch    58 | loss: 93.2294885CurrentTrain: epoch  0, batch    59 | loss: 95.4372793CurrentTrain: epoch  0, batch    60 | loss: 141.7555589CurrentTrain: epoch  0, batch    61 | loss: 94.1387967CurrentTrain: epoch  0, batch    62 | loss: 94.4421497CurrentTrain: epoch  0, batch    63 | loss: 94.7316259CurrentTrain: epoch  0, batch    64 | loss: 80.4366770CurrentTrain: epoch  0, batch    65 | loss: 81.4287250CurrentTrain: epoch  0, batch    66 | loss: 92.7940622CurrentTrain: epoch  0, batch    67 | loss: 79.4185997CurrentTrain: epoch  0, batch    68 | loss: 93.9165281CurrentTrain: epoch  0, batch    69 | loss: 76.3197690CurrentTrain: epoch  0, batch    70 | loss: 112.2930631CurrentTrain: epoch  0, batch    71 | loss: 81.3403128CurrentTrain: epoch  0, batch    72 | loss: 90.6622209CurrentTrain: epoch  0, batch    73 | loss: 92.2713804CurrentTrain: epoch  0, batch    74 | loss: 79.3641180CurrentTrain: epoch  0, batch    75 | loss: 77.1429024CurrentTrain: epoch  0, batch    76 | loss: 95.2851587CurrentTrain: epoch  0, batch    77 | loss: 90.7663540CurrentTrain: epoch  0, batch    78 | loss: 140.2376785CurrentTrain: epoch  0, batch    79 | loss: 77.3302738CurrentTrain: epoch  0, batch    80 | loss: 78.1110483CurrentTrain: epoch  0, batch    81 | loss: 112.4911400CurrentTrain: epoch  0, batch    82 | loss: 111.7974291CurrentTrain: epoch  0, batch    83 | loss: 107.4440213CurrentTrain: epoch  0, batch    84 | loss: 94.8416938CurrentTrain: epoch  0, batch    85 | loss: 128.7632877CurrentTrain: epoch  0, batch    86 | loss: 95.9285757CurrentTrain: epoch  0, batch    87 | loss: 92.1911188CurrentTrain: epoch  0, batch    88 | loss: 91.7599615CurrentTrain: epoch  0, batch    89 | loss: 112.2557337CurrentTrain: epoch  0, batch    90 | loss: 78.9493469CurrentTrain: epoch  0, batch    91 | loss: 109.4801142CurrentTrain: epoch  0, batch    92 | loss: 91.0312506CurrentTrain: epoch  0, batch    93 | loss: 91.4928848CurrentTrain: epoch  0, batch    94 | loss: 114.2737266CurrentTrain: epoch  0, batch    95 | loss: 87.1641068CurrentTrain: epoch  1, batch     0 | loss: 88.5895153CurrentTrain: epoch  1, batch     1 | loss: 92.3637549CurrentTrain: epoch  1, batch     2 | loss: 78.7953586CurrentTrain: epoch  1, batch     3 | loss: 109.9295026CurrentTrain: epoch  1, batch     4 | loss: 91.3835894CurrentTrain: epoch  1, batch     5 | loss: 110.6560564CurrentTrain: epoch  1, batch     6 | loss: 68.7920215CurrentTrain: epoch  1, batch     7 | loss: 88.0616090CurrentTrain: epoch  1, batch     8 | loss: 88.2579737CurrentTrain: epoch  1, batch     9 | loss: 93.1361425CurrentTrain: epoch  1, batch    10 | loss: 89.3216919CurrentTrain: epoch  1, batch    11 | loss: 91.0333160CurrentTrain: epoch  1, batch    12 | loss: 87.7599552CurrentTrain: epoch  1, batch    13 | loss: 91.0893498CurrentTrain: epoch  1, batch    14 | loss: 108.7495609CurrentTrain: epoch  1, batch    15 | loss: 77.3596643CurrentTrain: epoch  1, batch    16 | loss: 67.1846659CurrentTrain: epoch  1, batch    17 | loss: 67.7741707CurrentTrain: epoch  1, batch    18 | loss: 103.5792581CurrentTrain: epoch  1, batch    19 | loss: 88.5656017CurrentTrain: epoch  1, batch    20 | loss: 93.1255334CurrentTrain: epoch  1, batch    21 | loss: 78.0452146CurrentTrain: epoch  1, batch    22 | loss: 79.1244532CurrentTrain: epoch  1, batch    23 | loss: 87.4608318CurrentTrain: epoch  1, batch    24 | loss: 108.3466235CurrentTrain: epoch  1, batch    25 | loss: 89.5242194CurrentTrain: epoch  1, batch    26 | loss: 76.9391928CurrentTrain: epoch  1, batch    27 | loss: 91.1581142CurrentTrain: epoch  1, batch    28 | loss: 104.5159818CurrentTrain: epoch  1, batch    29 | loss: 110.8991989CurrentTrain: epoch  1, batch    30 | loss: 90.8922319CurrentTrain: epoch  1, batch    31 | loss: 75.5866089CurrentTrain: epoch  1, batch    32 | loss: 110.3133143CurrentTrain: epoch  1, batch    33 | loss: 137.3199057CurrentTrain: epoch  1, batch    34 | loss: 87.6673156CurrentTrain: epoch  1, batch    35 | loss: 88.9573179CurrentTrain: epoch  1, batch    36 | loss: 75.2263521CurrentTrain: epoch  1, batch    37 | loss: 63.6735725CurrentTrain: epoch  1, batch    38 | loss: 89.6532970CurrentTrain: epoch  1, batch    39 | loss: 104.1385512CurrentTrain: epoch  1, batch    40 | loss: 85.0468394CurrentTrain: epoch  1, batch    41 | loss: 73.1658451CurrentTrain: epoch  1, batch    42 | loss: 90.0346085CurrentTrain: epoch  1, batch    43 | loss: 73.7691612CurrentTrain: epoch  1, batch    44 | loss: 84.4196734CurrentTrain: epoch  1, batch    45 | loss: 88.2759868CurrentTrain: epoch  1, batch    46 | loss: 106.7586449CurrentTrain: epoch  1, batch    47 | loss: 134.3907748CurrentTrain: epoch  1, batch    48 | loss: 85.3324429CurrentTrain: epoch  1, batch    49 | loss: 140.3457976CurrentTrain: epoch  1, batch    50 | loss: 109.0938285CurrentTrain: epoch  1, batch    51 | loss: 73.2209397CurrentTrain: epoch  1, batch    52 | loss: 133.4827962CurrentTrain: epoch  1, batch    53 | loss: 65.3601759CurrentTrain: epoch  1, batch    54 | loss: 90.4781850CurrentTrain: epoch  1, batch    55 | loss: 88.9157473CurrentTrain: epoch  1, batch    56 | loss: 184.5852994CurrentTrain: epoch  1, batch    57 | loss: 62.6031514CurrentTrain: epoch  1, batch    58 | loss: 177.2841531CurrentTrain: epoch  1, batch    59 | loss: 109.7088243CurrentTrain: epoch  1, batch    60 | loss: 71.7937932CurrentTrain: epoch  1, batch    61 | loss: 89.7578495CurrentTrain: epoch  1, batch    62 | loss: 87.9024631CurrentTrain: epoch  1, batch    63 | loss: 75.2455235CurrentTrain: epoch  1, batch    64 | loss: 103.4103943CurrentTrain: epoch  1, batch    65 | loss: 88.5701736CurrentTrain: epoch  1, batch    66 | loss: 88.2733042CurrentTrain: epoch  1, batch    67 | loss: 88.7371620CurrentTrain: epoch  1, batch    68 | loss: 108.3305334CurrentTrain: epoch  1, batch    69 | loss: 90.5101543CurrentTrain: epoch  1, batch    70 | loss: 89.6156767CurrentTrain: epoch  1, batch    71 | loss: 74.8900759CurrentTrain: epoch  1, batch    72 | loss: 122.9346983CurrentTrain: epoch  1, batch    73 | loss: 60.3596903CurrentTrain: epoch  1, batch    74 | loss: 104.2603956CurrentTrain: epoch  1, batch    75 | loss: 76.9568770CurrentTrain: epoch  1, batch    76 | loss: 141.1157139CurrentTrain: epoch  1, batch    77 | loss: 87.8587889CurrentTrain: epoch  1, batch    78 | loss: 89.2136171CurrentTrain: epoch  1, batch    79 | loss: 90.9924331CurrentTrain: epoch  1, batch    80 | loss: 93.1764938CurrentTrain: epoch  1, batch    81 | loss: 91.6721219CurrentTrain: epoch  1, batch    82 | loss: 106.5450030CurrentTrain: epoch  1, batch    83 | loss: 94.3455572CurrentTrain: epoch  1, batch    84 | loss: 106.2631973CurrentTrain: epoch  1, batch    85 | loss: 107.3573752CurrentTrain: epoch  1, batch    86 | loss: 83.7547383CurrentTrain: epoch  1, batch    87 | loss: 83.9550803CurrentTrain: epoch  1, batch    88 | loss: 66.3367450CurrentTrain: epoch  1, batch    89 | loss: 108.5535443CurrentTrain: epoch  1, batch    90 | loss: 72.8974111CurrentTrain: epoch  1, batch    91 | loss: 135.1205266CurrentTrain: epoch  1, batch    92 | loss: 75.2823194CurrentTrain: epoch  1, batch    93 | loss: 107.2874264CurrentTrain: epoch  1, batch    94 | loss: 88.7776117CurrentTrain: epoch  1, batch    95 | loss: 150.5995195CurrentTrain: epoch  2, batch     0 | loss: 75.6002202CurrentTrain: epoch  2, batch     1 | loss: 86.4270733CurrentTrain: epoch  2, batch     2 | loss: 84.4086726CurrentTrain: epoch  2, batch     3 | loss: 106.0669685CurrentTrain: epoch  2, batch     4 | loss: 91.6513057CurrentTrain: epoch  2, batch     5 | loss: 66.0918802CurrentTrain: epoch  2, batch     6 | loss: 64.4658674CurrentTrain: epoch  2, batch     7 | loss: 73.3875382CurrentTrain: epoch  2, batch     8 | loss: 105.1299195CurrentTrain: epoch  2, batch     9 | loss: 136.3740055CurrentTrain: epoch  2, batch    10 | loss: 74.5201436CurrentTrain: epoch  2, batch    11 | loss: 180.8450713CurrentTrain: epoch  2, batch    12 | loss: 106.5046206CurrentTrain: epoch  2, batch    13 | loss: 67.1085919CurrentTrain: epoch  2, batch    14 | loss: 131.6814029CurrentTrain: epoch  2, batch    15 | loss: 83.6892552CurrentTrain: epoch  2, batch    16 | loss: 85.4965213CurrentTrain: epoch  2, batch    17 | loss: 82.0579039CurrentTrain: epoch  2, batch    18 | loss: 72.7583258CurrentTrain: epoch  2, batch    19 | loss: 101.1631690CurrentTrain: epoch  2, batch    20 | loss: 74.1048055CurrentTrain: epoch  2, batch    21 | loss: 107.5516279CurrentTrain: epoch  2, batch    22 | loss: 63.8930895CurrentTrain: epoch  2, batch    23 | loss: 73.9656521CurrentTrain: epoch  2, batch    24 | loss: 72.2555772CurrentTrain: epoch  2, batch    25 | loss: 130.6122490CurrentTrain: epoch  2, batch    26 | loss: 109.9290249CurrentTrain: epoch  2, batch    27 | loss: 133.6384684CurrentTrain: epoch  2, batch    28 | loss: 88.5255263CurrentTrain: epoch  2, batch    29 | loss: 105.6275856CurrentTrain: epoch  2, batch    30 | loss: 89.9717266CurrentTrain: epoch  2, batch    31 | loss: 103.5389916CurrentTrain: epoch  2, batch    32 | loss: 85.0146487CurrentTrain: epoch  2, batch    33 | loss: 70.7292976CurrentTrain: epoch  2, batch    34 | loss: 89.9507781CurrentTrain: epoch  2, batch    35 | loss: 89.2587882CurrentTrain: epoch  2, batch    36 | loss: 68.9867477CurrentTrain: epoch  2, batch    37 | loss: 103.6015432CurrentTrain: epoch  2, batch    38 | loss: 84.4458181CurrentTrain: epoch  2, batch    39 | loss: 74.7098555CurrentTrain: epoch  2, batch    40 | loss: 88.7917831CurrentTrain: epoch  2, batch    41 | loss: 76.5516025CurrentTrain: epoch  2, batch    42 | loss: 103.4300197CurrentTrain: epoch  2, batch    43 | loss: 129.2966563CurrentTrain: epoch  2, batch    44 | loss: 72.9409945CurrentTrain: epoch  2, batch    45 | loss: 84.3440758CurrentTrain: epoch  2, batch    46 | loss: 63.9232600CurrentTrain: epoch  2, batch    47 | loss: 100.4866290CurrentTrain: epoch  2, batch    48 | loss: 86.8022186CurrentTrain: epoch  2, batch    49 | loss: 88.9080158CurrentTrain: epoch  2, batch    50 | loss: 87.7644737CurrentTrain: epoch  2, batch    51 | loss: 106.8265674CurrentTrain: epoch  2, batch    52 | loss: 74.8934150CurrentTrain: epoch  2, batch    53 | loss: 131.5045672CurrentTrain: epoch  2, batch    54 | loss: 61.2657332CurrentTrain: epoch  2, batch    55 | loss: 70.8528903CurrentTrain: epoch  2, batch    56 | loss: 101.5004127CurrentTrain: epoch  2, batch    57 | loss: 88.1226619CurrentTrain: epoch  2, batch    58 | loss: 107.4603708CurrentTrain: epoch  2, batch    59 | loss: 75.2883276CurrentTrain: epoch  2, batch    60 | loss: 133.0125782CurrentTrain: epoch  2, batch    61 | loss: 83.5783117CurrentTrain: epoch  2, batch    62 | loss: 83.6288962CurrentTrain: epoch  2, batch    63 | loss: 72.5159755CurrentTrain: epoch  2, batch    64 | loss: 72.8432489CurrentTrain: epoch  2, batch    65 | loss: 83.8484718CurrentTrain: epoch  2, batch    66 | loss: 74.0335151CurrentTrain: epoch  2, batch    67 | loss: 81.7500929CurrentTrain: epoch  2, batch    68 | loss: 87.4169446CurrentTrain: epoch  2, batch    69 | loss: 102.9914698CurrentTrain: epoch  2, batch    70 | loss: 103.9502312CurrentTrain: epoch  2, batch    71 | loss: 65.0845562CurrentTrain: epoch  2, batch    72 | loss: 72.7903469CurrentTrain: epoch  2, batch    73 | loss: 82.4391456CurrentTrain: epoch  2, batch    74 | loss: 85.7307098CurrentTrain: epoch  2, batch    75 | loss: 74.8428024CurrentTrain: epoch  2, batch    76 | loss: 87.8783766CurrentTrain: epoch  2, batch    77 | loss: 126.7219642CurrentTrain: epoch  2, batch    78 | loss: 100.7864866CurrentTrain: epoch  2, batch    79 | loss: 103.8923034CurrentTrain: epoch  2, batch    80 | loss: 62.7764685CurrentTrain: epoch  2, batch    81 | loss: 88.0685992CurrentTrain: epoch  2, batch    82 | loss: 66.5762356CurrentTrain: epoch  2, batch    83 | loss: 83.1694569CurrentTrain: epoch  2, batch    84 | loss: 75.1682689CurrentTrain: epoch  2, batch    85 | loss: 95.6293740CurrentTrain: epoch  2, batch    86 | loss: 102.9746119CurrentTrain: epoch  2, batch    87 | loss: 101.4436447CurrentTrain: epoch  2, batch    88 | loss: 134.3753051CurrentTrain: epoch  2, batch    89 | loss: 74.0232304CurrentTrain: epoch  2, batch    90 | loss: 87.9300094CurrentTrain: epoch  2, batch    91 | loss: 83.7599632CurrentTrain: epoch  2, batch    92 | loss: 84.8630488CurrentTrain: epoch  2, batch    93 | loss: 88.1890782CurrentTrain: epoch  2, batch    94 | loss: 72.0292026CurrentTrain: epoch  2, batch    95 | loss: 88.2402561CurrentTrain: epoch  3, batch     0 | loss: 83.7734570CurrentTrain: epoch  3, batch     1 | loss: 85.5478870CurrentTrain: epoch  3, batch     2 | loss: 99.6438739CurrentTrain: epoch  3, batch     3 | loss: 85.7419600CurrentTrain: epoch  3, batch     4 | loss: 107.5401046CurrentTrain: epoch  3, batch     5 | loss: 61.6921746CurrentTrain: epoch  3, batch     6 | loss: 70.8361481CurrentTrain: epoch  3, batch     7 | loss: 71.2949390CurrentTrain: epoch  3, batch     8 | loss: 125.9713260CurrentTrain: epoch  3, batch     9 | loss: 101.1825075CurrentTrain: epoch  3, batch    10 | loss: 107.0487840CurrentTrain: epoch  3, batch    11 | loss: 104.7738301CurrentTrain: epoch  3, batch    12 | loss: 72.2389164CurrentTrain: epoch  3, batch    13 | loss: 102.8036103CurrentTrain: epoch  3, batch    14 | loss: 102.0965323CurrentTrain: epoch  3, batch    15 | loss: 68.6572303CurrentTrain: epoch  3, batch    16 | loss: 79.1493091CurrentTrain: epoch  3, batch    17 | loss: 82.3036802CurrentTrain: epoch  3, batch    18 | loss: 100.6278872CurrentTrain: epoch  3, batch    19 | loss: 103.9275890CurrentTrain: epoch  3, batch    20 | loss: 75.5412187CurrentTrain: epoch  3, batch    21 | loss: 65.7844477CurrentTrain: epoch  3, batch    22 | loss: 66.2850961CurrentTrain: epoch  3, batch    23 | loss: 124.8397915CurrentTrain: epoch  3, batch    24 | loss: 87.1611993CurrentTrain: epoch  3, batch    25 | loss: 74.9590782CurrentTrain: epoch  3, batch    26 | loss: 102.8400391CurrentTrain: epoch  3, batch    27 | loss: 104.5266711CurrentTrain: epoch  3, batch    28 | loss: 58.9663585CurrentTrain: epoch  3, batch    29 | loss: 69.5793282CurrentTrain: epoch  3, batch    30 | loss: 87.1076571CurrentTrain: epoch  3, batch    31 | loss: 99.2230239CurrentTrain: epoch  3, batch    32 | loss: 101.8040953CurrentTrain: epoch  3, batch    33 | loss: 69.0757227CurrentTrain: epoch  3, batch    34 | loss: 84.5668086CurrentTrain: epoch  3, batch    35 | loss: 56.3918220CurrentTrain: epoch  3, batch    36 | loss: 69.2393951CurrentTrain: epoch  3, batch    37 | loss: 134.1082205CurrentTrain: epoch  3, batch    38 | loss: 82.4958293CurrentTrain: epoch  3, batch    39 | loss: 74.3828425CurrentTrain: epoch  3, batch    40 | loss: 88.1536487CurrentTrain: epoch  3, batch    41 | loss: 86.0401391CurrentTrain: epoch  3, batch    42 | loss: 62.6055138CurrentTrain: epoch  3, batch    43 | loss: 86.9619623CurrentTrain: epoch  3, batch    44 | loss: 97.4180620CurrentTrain: epoch  3, batch    45 | loss: 103.4681138CurrentTrain: epoch  3, batch    46 | loss: 100.6514105CurrentTrain: epoch  3, batch    47 | loss: 60.7541346CurrentTrain: epoch  3, batch    48 | loss: 73.7090844CurrentTrain: epoch  3, batch    49 | loss: 73.9381437CurrentTrain: epoch  3, batch    50 | loss: 68.9015314CurrentTrain: epoch  3, batch    51 | loss: 97.8979544CurrentTrain: epoch  3, batch    52 | loss: 70.0468705CurrentTrain: epoch  3, batch    53 | loss: 86.1051871CurrentTrain: epoch  3, batch    54 | loss: 81.8475135CurrentTrain: epoch  3, batch    55 | loss: 103.6633365CurrentTrain: epoch  3, batch    56 | loss: 84.0463189CurrentTrain: epoch  3, batch    57 | loss: 133.5210133CurrentTrain: epoch  3, batch    58 | loss: 130.6650713CurrentTrain: epoch  3, batch    59 | loss: 129.2481048CurrentTrain: epoch  3, batch    60 | loss: 83.3955032CurrentTrain: epoch  3, batch    61 | loss: 82.1688977CurrentTrain: epoch  3, batch    62 | loss: 103.6255276CurrentTrain: epoch  3, batch    63 | loss: 132.6125633CurrentTrain: epoch  3, batch    64 | loss: 61.7681523CurrentTrain: epoch  3, batch    65 | loss: 73.5824587CurrentTrain: epoch  3, batch    66 | loss: 127.7522644CurrentTrain: epoch  3, batch    67 | loss: 86.8456692CurrentTrain: epoch  3, batch    68 | loss: 58.3154202CurrentTrain: epoch  3, batch    69 | loss: 103.7437196CurrentTrain: epoch  3, batch    70 | loss: 88.0000684CurrentTrain: epoch  3, batch    71 | loss: 69.0467421CurrentTrain: epoch  3, batch    72 | loss: 71.2291270CurrentTrain: epoch  3, batch    73 | loss: 83.4359703CurrentTrain: epoch  3, batch    74 | loss: 88.7891773CurrentTrain: epoch  3, batch    75 | loss: 72.6783745CurrentTrain: epoch  3, batch    76 | loss: 73.1981057CurrentTrain: epoch  3, batch    77 | loss: 71.4060530CurrentTrain: epoch  3, batch    78 | loss: 70.2474584CurrentTrain: epoch  3, batch    79 | loss: 108.9391854CurrentTrain: epoch  3, batch    80 | loss: 87.9999914CurrentTrain: epoch  3, batch    81 | loss: 79.3183833CurrentTrain: epoch  3, batch    82 | loss: 105.6367385CurrentTrain: epoch  3, batch    83 | loss: 101.8081175CurrentTrain: epoch  3, batch    84 | loss: 103.4970577CurrentTrain: epoch  3, batch    85 | loss: 104.5589757CurrentTrain: epoch  3, batch    86 | loss: 100.3248184CurrentTrain: epoch  3, batch    87 | loss: 103.8859288CurrentTrain: epoch  3, batch    88 | loss: 98.7547959CurrentTrain: epoch  3, batch    89 | loss: 60.8675622CurrentTrain: epoch  3, batch    90 | loss: 135.0174833CurrentTrain: epoch  3, batch    91 | loss: 102.1536280CurrentTrain: epoch  3, batch    92 | loss: 87.1739163CurrentTrain: epoch  3, batch    93 | loss: 84.0386838CurrentTrain: epoch  3, batch    94 | loss: 88.5208323CurrentTrain: epoch  3, batch    95 | loss: 70.5109392CurrentTrain: epoch  4, batch     0 | loss: 82.7749231CurrentTrain: epoch  4, batch     1 | loss: 70.1957837CurrentTrain: epoch  4, batch     2 | loss: 69.8165858CurrentTrain: epoch  4, batch     3 | loss: 127.7851722CurrentTrain: epoch  4, batch     4 | loss: 61.6880010CurrentTrain: epoch  4, batch     5 | loss: 72.4551888CurrentTrain: epoch  4, batch     6 | loss: 82.6164467CurrentTrain: epoch  4, batch     7 | loss: 83.2804903CurrentTrain: epoch  4, batch     8 | loss: 125.3978234CurrentTrain: epoch  4, batch     9 | loss: 69.5440495CurrentTrain: epoch  4, batch    10 | loss: 171.8859635CurrentTrain: epoch  4, batch    11 | loss: 126.9015983CurrentTrain: epoch  4, batch    12 | loss: 99.8163902CurrentTrain: epoch  4, batch    13 | loss: 105.5205767CurrentTrain: epoch  4, batch    14 | loss: 84.1945941CurrentTrain: epoch  4, batch    15 | loss: 65.8884643CurrentTrain: epoch  4, batch    16 | loss: 100.3879929CurrentTrain: epoch  4, batch    17 | loss: 70.8429956CurrentTrain: epoch  4, batch    18 | loss: 83.9321644CurrentTrain: epoch  4, batch    19 | loss: 68.4317203CurrentTrain: epoch  4, batch    20 | loss: 85.6478181CurrentTrain: epoch  4, batch    21 | loss: 80.9530366CurrentTrain: epoch  4, batch    22 | loss: 70.6866232CurrentTrain: epoch  4, batch    23 | loss: 101.2755924CurrentTrain: epoch  4, batch    24 | loss: 82.8775007CurrentTrain: epoch  4, batch    25 | loss: 99.7213183CurrentTrain: epoch  4, batch    26 | loss: 104.5769693CurrentTrain: epoch  4, batch    27 | loss: 66.4851818CurrentTrain: epoch  4, batch    28 | loss: 79.9339928CurrentTrain: epoch  4, batch    29 | loss: 99.3355598CurrentTrain: epoch  4, batch    30 | loss: 83.8018419CurrentTrain: epoch  4, batch    31 | loss: 80.8932651CurrentTrain: epoch  4, batch    32 | loss: 83.2804182CurrentTrain: epoch  4, batch    33 | loss: 99.6315296CurrentTrain: epoch  4, batch    34 | loss: 70.2822526CurrentTrain: epoch  4, batch    35 | loss: 133.7980294CurrentTrain: epoch  4, batch    36 | loss: 125.5459285CurrentTrain: epoch  4, batch    37 | loss: 106.0663518CurrentTrain: epoch  4, batch    38 | loss: 83.8639558CurrentTrain: epoch  4, batch    39 | loss: 104.7259835CurrentTrain: epoch  4, batch    40 | loss: 71.9163292CurrentTrain: epoch  4, batch    41 | loss: 69.5104789CurrentTrain: epoch  4, batch    42 | loss: 71.8432451CurrentTrain: epoch  4, batch    43 | loss: 83.5193168CurrentTrain: epoch  4, batch    44 | loss: 85.4798169CurrentTrain: epoch  4, batch    45 | loss: 58.1276795CurrentTrain: epoch  4, batch    46 | loss: 126.5865246CurrentTrain: epoch  4, batch    47 | loss: 82.0124471CurrentTrain: epoch  4, batch    48 | loss: 99.5151027CurrentTrain: epoch  4, batch    49 | loss: 81.6971281CurrentTrain: epoch  4, batch    50 | loss: 85.9667431CurrentTrain: epoch  4, batch    51 | loss: 99.8681715CurrentTrain: epoch  4, batch    52 | loss: 70.4707763CurrentTrain: epoch  4, batch    53 | loss: 86.4086981CurrentTrain: epoch  4, batch    54 | loss: 56.4414090CurrentTrain: epoch  4, batch    55 | loss: 63.4249122CurrentTrain: epoch  4, batch    56 | loss: 128.5149482CurrentTrain: epoch  4, batch    57 | loss: 96.1981620CurrentTrain: epoch  4, batch    58 | loss: 58.5941392CurrentTrain: epoch  4, batch    59 | loss: 73.1316838CurrentTrain: epoch  4, batch    60 | loss: 102.5962141CurrentTrain: epoch  4, batch    61 | loss: 69.5758052CurrentTrain: epoch  4, batch    62 | loss: 84.3552528CurrentTrain: epoch  4, batch    63 | loss: 99.1798293CurrentTrain: epoch  4, batch    64 | loss: 102.3081360CurrentTrain: epoch  4, batch    65 | loss: 97.9079486CurrentTrain: epoch  4, batch    66 | loss: 81.5630997CurrentTrain: epoch  4, batch    67 | loss: 81.6564397CurrentTrain: epoch  4, batch    68 | loss: 86.1849682CurrentTrain: epoch  4, batch    69 | loss: 88.8960262CurrentTrain: epoch  4, batch    70 | loss: 80.1630319CurrentTrain: epoch  4, batch    71 | loss: 70.4455249CurrentTrain: epoch  4, batch    72 | loss: 85.4782872CurrentTrain: epoch  4, batch    73 | loss: 70.7436621CurrentTrain: epoch  4, batch    74 | loss: 84.6897547CurrentTrain: epoch  4, batch    75 | loss: 60.6920888CurrentTrain: epoch  4, batch    76 | loss: 80.9632702CurrentTrain: epoch  4, batch    77 | loss: 104.3017305CurrentTrain: epoch  4, batch    78 | loss: 69.2794873CurrentTrain: epoch  4, batch    79 | loss: 100.2018123CurrentTrain: epoch  4, batch    80 | loss: 84.6176232CurrentTrain: epoch  4, batch    81 | loss: 88.5576995CurrentTrain: epoch  4, batch    82 | loss: 104.1378817CurrentTrain: epoch  4, batch    83 | loss: 81.3392651CurrentTrain: epoch  4, batch    84 | loss: 80.2469565CurrentTrain: epoch  4, batch    85 | loss: 70.4255741CurrentTrain: epoch  4, batch    86 | loss: 166.5608447CurrentTrain: epoch  4, batch    87 | loss: 73.8591103CurrentTrain: epoch  4, batch    88 | loss: 132.7437253CurrentTrain: epoch  4, batch    89 | loss: 72.1052800CurrentTrain: epoch  4, batch    90 | loss: 78.8385917CurrentTrain: epoch  4, batch    91 | loss: 69.8619364CurrentTrain: epoch  4, batch    92 | loss: 80.7503840CurrentTrain: epoch  4, batch    93 | loss: 134.9300146CurrentTrain: epoch  4, batch    94 | loss: 69.8242475CurrentTrain: epoch  4, batch    95 | loss: 69.8783542CurrentTrain: epoch  5, batch     0 | loss: 101.9410966CurrentTrain: epoch  5, batch     1 | loss: 60.7952036CurrentTrain: epoch  5, batch     2 | loss: 67.2991244CurrentTrain: epoch  5, batch     3 | loss: 83.2085393CurrentTrain: epoch  5, batch     4 | loss: 69.5464649CurrentTrain: epoch  5, batch     5 | loss: 103.1689099CurrentTrain: epoch  5, batch     6 | loss: 82.2624662CurrentTrain: epoch  5, batch     7 | loss: 79.3967705CurrentTrain: epoch  5, batch     8 | loss: 58.7517800CurrentTrain: epoch  5, batch     9 | loss: 97.4388287CurrentTrain: epoch  5, batch    10 | loss: 82.7326226CurrentTrain: epoch  5, batch    11 | loss: 101.5331148CurrentTrain: epoch  5, batch    12 | loss: 99.0478031CurrentTrain: epoch  5, batch    13 | loss: 82.9052167CurrentTrain: epoch  5, batch    14 | loss: 68.7077933CurrentTrain: epoch  5, batch    15 | loss: 103.2928265CurrentTrain: epoch  5, batch    16 | loss: 83.9592742CurrentTrain: epoch  5, batch    17 | loss: 80.7469622CurrentTrain: epoch  5, batch    18 | loss: 166.2341743CurrentTrain: epoch  5, batch    19 | loss: 78.8408608CurrentTrain: epoch  5, batch    20 | loss: 84.5707724CurrentTrain: epoch  5, batch    21 | loss: 97.9864663CurrentTrain: epoch  5, batch    22 | loss: 82.5693728CurrentTrain: epoch  5, batch    23 | loss: 78.1249994CurrentTrain: epoch  5, batch    24 | loss: 59.5415310CurrentTrain: epoch  5, batch    25 | loss: 100.1808766CurrentTrain: epoch  5, batch    26 | loss: 88.2672439CurrentTrain: epoch  5, batch    27 | loss: 61.8374833CurrentTrain: epoch  5, batch    28 | loss: 65.2414889CurrentTrain: epoch  5, batch    29 | loss: 99.8673777CurrentTrain: epoch  5, batch    30 | loss: 80.1583891CurrentTrain: epoch  5, batch    31 | loss: 67.5573740CurrentTrain: epoch  5, batch    32 | loss: 95.5937397CurrentTrain: epoch  5, batch    33 | loss: 84.4642119CurrentTrain: epoch  5, batch    34 | loss: 68.5999420CurrentTrain: epoch  5, batch    35 | loss: 63.5135786CurrentTrain: epoch  5, batch    36 | loss: 67.0608438CurrentTrain: epoch  5, batch    37 | loss: 83.5748638CurrentTrain: epoch  5, batch    38 | loss: 63.4779981CurrentTrain: epoch  5, batch    39 | loss: 66.5985965CurrentTrain: epoch  5, batch    40 | loss: 67.8976805CurrentTrain: epoch  5, batch    41 | loss: 79.4758320CurrentTrain: epoch  5, batch    42 | loss: 102.6320854CurrentTrain: epoch  5, batch    43 | loss: 105.2760015CurrentTrain: epoch  5, batch    44 | loss: 122.2643209CurrentTrain: epoch  5, batch    45 | loss: 70.8807787CurrentTrain: epoch  5, batch    46 | loss: 99.0915059CurrentTrain: epoch  5, batch    47 | loss: 59.5787309CurrentTrain: epoch  5, batch    48 | loss: 86.6246087CurrentTrain: epoch  5, batch    49 | loss: 101.5368941CurrentTrain: epoch  5, batch    50 | loss: 79.8993159CurrentTrain: epoch  5, batch    51 | loss: 67.6177699CurrentTrain: epoch  5, batch    52 | loss: 100.5423037CurrentTrain: epoch  5, batch    53 | loss: 63.1767802CurrentTrain: epoch  5, batch    54 | loss: 128.1144028CurrentTrain: epoch  5, batch    55 | loss: 98.4682326CurrentTrain: epoch  5, batch    56 | loss: 70.3093486CurrentTrain: epoch  5, batch    57 | loss: 83.3320717CurrentTrain: epoch  5, batch    58 | loss: 100.1587621CurrentTrain: epoch  5, batch    59 | loss: 71.7282941CurrentTrain: epoch  5, batch    60 | loss: 84.6473024CurrentTrain: epoch  5, batch    61 | loss: 79.6778117CurrentTrain: epoch  5, batch    62 | loss: 70.2028878CurrentTrain: epoch  5, batch    63 | loss: 99.1478234CurrentTrain: epoch  5, batch    64 | loss: 81.1134968CurrentTrain: epoch  5, batch    65 | loss: 82.0254395CurrentTrain: epoch  5, batch    66 | loss: 65.8486575CurrentTrain: epoch  5, batch    67 | loss: 93.7464635CurrentTrain: epoch  5, batch    68 | loss: 99.9671015CurrentTrain: epoch  5, batch    69 | loss: 63.2617704CurrentTrain: epoch  5, batch    70 | loss: 100.4100782CurrentTrain: epoch  5, batch    71 | loss: 58.8213366CurrentTrain: epoch  5, batch    72 | loss: 84.3166419CurrentTrain: epoch  5, batch    73 | loss: 55.6595559CurrentTrain: epoch  5, batch    74 | loss: 81.4293379CurrentTrain: epoch  5, batch    75 | loss: 64.1576019CurrentTrain: epoch  5, batch    76 | loss: 124.1971180CurrentTrain: epoch  5, batch    77 | loss: 83.0467659CurrentTrain: epoch  5, batch    78 | loss: 103.2774630CurrentTrain: epoch  5, batch    79 | loss: 80.0740077CurrentTrain: epoch  5, batch    80 | loss: 100.1064760CurrentTrain: epoch  5, batch    81 | loss: 101.3452741CurrentTrain: epoch  5, batch    82 | loss: 85.2880358CurrentTrain: epoch  5, batch    83 | loss: 174.6385782CurrentTrain: epoch  5, batch    84 | loss: 59.5833306CurrentTrain: epoch  5, batch    85 | loss: 66.3482685CurrentTrain: epoch  5, batch    86 | loss: 178.3833368CurrentTrain: epoch  5, batch    87 | loss: 85.1296490CurrentTrain: epoch  5, batch    88 | loss: 121.8527466CurrentTrain: epoch  5, batch    89 | loss: 65.8267093CurrentTrain: epoch  5, batch    90 | loss: 71.4698542CurrentTrain: epoch  5, batch    91 | loss: 129.0878590CurrentTrain: epoch  5, batch    92 | loss: 68.3252729CurrentTrain: epoch  5, batch    93 | loss: 100.6854108CurrentTrain: epoch  5, batch    94 | loss: 97.0254977CurrentTrain: epoch  5, batch    95 | loss: 110.4164312CurrentTrain: epoch  6, batch     0 | loss: 70.9049537CurrentTrain: epoch  6, batch     1 | loss: 98.7242394CurrentTrain: epoch  6, batch     2 | loss: 77.8166384CurrentTrain: epoch  6, batch     3 | loss: 69.6359070CurrentTrain: epoch  6, batch     4 | loss: 81.3365588CurrentTrain: epoch  6, batch     5 | loss: 97.2266889CurrentTrain: epoch  6, batch     6 | loss: 99.3274866CurrentTrain: epoch  6, batch     7 | loss: 121.3805810CurrentTrain: epoch  6, batch     8 | loss: 66.1582591CurrentTrain: epoch  6, batch     9 | loss: 53.0566187CurrentTrain: epoch  6, batch    10 | loss: 93.1842995CurrentTrain: epoch  6, batch    11 | loss: 82.0052192CurrentTrain: epoch  6, batch    12 | loss: 95.5963709CurrentTrain: epoch  6, batch    13 | loss: 96.8697071CurrentTrain: epoch  6, batch    14 | loss: 58.1258150CurrentTrain: epoch  6, batch    15 | loss: 81.4507430CurrentTrain: epoch  6, batch    16 | loss: 56.3055335CurrentTrain: epoch  6, batch    17 | loss: 70.0887043CurrentTrain: epoch  6, batch    18 | loss: 96.4681053CurrentTrain: epoch  6, batch    19 | loss: 79.1236129CurrentTrain: epoch  6, batch    20 | loss: 57.9319613CurrentTrain: epoch  6, batch    21 | loss: 75.3591034CurrentTrain: epoch  6, batch    22 | loss: 80.5967557CurrentTrain: epoch  6, batch    23 | loss: 97.1536480CurrentTrain: epoch  6, batch    24 | loss: 99.7102841CurrentTrain: epoch  6, batch    25 | loss: 95.5832180CurrentTrain: epoch  6, batch    26 | loss: 78.6027202CurrentTrain: epoch  6, batch    27 | loss: 95.2517262CurrentTrain: epoch  6, batch    28 | loss: 81.1508475CurrentTrain: epoch  6, batch    29 | loss: 68.9855498CurrentTrain: epoch  6, batch    30 | loss: 98.6113597CurrentTrain: epoch  6, batch    31 | loss: 72.7570108CurrentTrain: epoch  6, batch    32 | loss: 56.9834648CurrentTrain: epoch  6, batch    33 | loss: 64.2730676CurrentTrain: epoch  6, batch    34 | loss: 97.9040994CurrentTrain: epoch  6, batch    35 | loss: 65.1140611CurrentTrain: epoch  6, batch    36 | loss: 82.8198786CurrentTrain: epoch  6, batch    37 | loss: 58.4801769CurrentTrain: epoch  6, batch    38 | loss: 98.1672770CurrentTrain: epoch  6, batch    39 | loss: 83.0565307CurrentTrain: epoch  6, batch    40 | loss: 81.8376377CurrentTrain: epoch  6, batch    41 | loss: 129.4743636CurrentTrain: epoch  6, batch    42 | loss: 123.4261287CurrentTrain: epoch  6, batch    43 | loss: 62.4623242CurrentTrain: epoch  6, batch    44 | loss: 83.9045306CurrentTrain: epoch  6, batch    45 | loss: 97.4876655CurrentTrain: epoch  6, batch    46 | loss: 80.3123914CurrentTrain: epoch  6, batch    47 | loss: 101.8402772CurrentTrain: epoch  6, batch    48 | loss: 85.0804571CurrentTrain: epoch  6, batch    49 | loss: 96.4682001CurrentTrain: epoch  6, batch    50 | loss: 123.3090512CurrentTrain: epoch  6, batch    51 | loss: 79.1755883CurrentTrain: epoch  6, batch    52 | loss: 103.2497302CurrentTrain: epoch  6, batch    53 | loss: 98.4132776CurrentTrain: epoch  6, batch    54 | loss: 99.3920758CurrentTrain: epoch  6, batch    55 | loss: 98.3198336CurrentTrain: epoch  6, batch    56 | loss: 101.9900638CurrentTrain: epoch  6, batch    57 | loss: 169.0964723CurrentTrain: epoch  6, batch    58 | loss: 80.5237539CurrentTrain: epoch  6, batch    59 | loss: 72.3817006CurrentTrain: epoch  6, batch    60 | loss: 124.5505450CurrentTrain: epoch  6, batch    61 | loss: 79.9967860CurrentTrain: epoch  6, batch    62 | loss: 98.6153306CurrentTrain: epoch  6, batch    63 | loss: 68.0021418CurrentTrain: epoch  6, batch    64 | loss: 78.9440276CurrentTrain: epoch  6, batch    65 | loss: 70.7397190CurrentTrain: epoch  6, batch    66 | loss: 67.1222218CurrentTrain: epoch  6, batch    67 | loss: 70.2817455CurrentTrain: epoch  6, batch    68 | loss: 80.9208920CurrentTrain: epoch  6, batch    69 | loss: 76.6440524CurrentTrain: epoch  6, batch    70 | loss: 69.0979892CurrentTrain: epoch  6, batch    71 | loss: 64.0564115CurrentTrain: epoch  6, batch    72 | loss: 108.9115398CurrentTrain: epoch  6, batch    73 | loss: 100.3582699CurrentTrain: epoch  6, batch    74 | loss: 59.4497739CurrentTrain: epoch  6, batch    75 | loss: 99.5936236CurrentTrain: epoch  6, batch    76 | loss: 79.3384479CurrentTrain: epoch  6, batch    77 | loss: 75.9024026CurrentTrain: epoch  6, batch    78 | loss: 77.9378361CurrentTrain: epoch  6, batch    79 | loss: 71.1702202CurrentTrain: epoch  6, batch    80 | loss: 81.1629675CurrentTrain: epoch  6, batch    81 | loss: 98.7475618CurrentTrain: epoch  6, batch    82 | loss: 99.8455835CurrentTrain: epoch  6, batch    83 | loss: 54.4520114CurrentTrain: epoch  6, batch    84 | loss: 69.0171423CurrentTrain: epoch  6, batch    85 | loss: 96.1014078CurrentTrain: epoch  6, batch    86 | loss: 65.1081057CurrentTrain: epoch  6, batch    87 | loss: 65.9064461CurrentTrain: epoch  6, batch    88 | loss: 81.3866232CurrentTrain: epoch  6, batch    89 | loss: 78.3113434CurrentTrain: epoch  6, batch    90 | loss: 97.3282770CurrentTrain: epoch  6, batch    91 | loss: 80.5254888CurrentTrain: epoch  6, batch    92 | loss: 66.6980436CurrentTrain: epoch  6, batch    93 | loss: 99.6616078CurrentTrain: epoch  6, batch    94 | loss: 97.9690297CurrentTrain: epoch  6, batch    95 | loss: 81.9940346CurrentTrain: epoch  7, batch     0 | loss: 77.5318701CurrentTrain: epoch  7, batch     1 | loss: 67.9915786CurrentTrain: epoch  7, batch     2 | loss: 123.8603747CurrentTrain: epoch  7, batch     3 | loss: 80.5004843CurrentTrain: epoch  7, batch     4 | loss: 80.9889519CurrentTrain: epoch  7, batch     5 | loss: 76.9990714CurrentTrain: epoch  7, batch     6 | loss: 77.7711947CurrentTrain: epoch  7, batch     7 | loss: 64.5681913CurrentTrain: epoch  7, batch     8 | loss: 68.1033041CurrentTrain: epoch  7, batch     9 | loss: 79.2508487CurrentTrain: epoch  7, batch    10 | loss: 124.0345561CurrentTrain: epoch  7, batch    11 | loss: 83.0861664CurrentTrain: epoch  7, batch    12 | loss: 96.4543063CurrentTrain: epoch  7, batch    13 | loss: 97.0593191CurrentTrain: epoch  7, batch    14 | loss: 56.9843378CurrentTrain: epoch  7, batch    15 | loss: 68.8602754CurrentTrain: epoch  7, batch    16 | loss: 123.3877594CurrentTrain: epoch  7, batch    17 | loss: 82.1769308CurrentTrain: epoch  7, batch    18 | loss: 95.7184697CurrentTrain: epoch  7, batch    19 | loss: 76.7133967CurrentTrain: epoch  7, batch    20 | loss: 95.8978408CurrentTrain: epoch  7, batch    21 | loss: 75.7271676CurrentTrain: epoch  7, batch    22 | loss: 79.0914390CurrentTrain: epoch  7, batch    23 | loss: 80.1385085CurrentTrain: epoch  7, batch    24 | loss: 80.1635728CurrentTrain: epoch  7, batch    25 | loss: 64.0619596CurrentTrain: epoch  7, batch    26 | loss: 77.0379038CurrentTrain: epoch  7, batch    27 | loss: 81.2968269CurrentTrain: epoch  7, batch    28 | loss: 78.2037005CurrentTrain: epoch  7, batch    29 | loss: 58.1573497CurrentTrain: epoch  7, batch    30 | loss: 79.4597303CurrentTrain: epoch  7, batch    31 | loss: 124.2716213CurrentTrain: epoch  7, batch    32 | loss: 79.5163749CurrentTrain: epoch  7, batch    33 | loss: 101.7453194CurrentTrain: epoch  7, batch    34 | loss: 92.0869508CurrentTrain: epoch  7, batch    35 | loss: 95.1399934CurrentTrain: epoch  7, batch    36 | loss: 65.0500917CurrentTrain: epoch  7, batch    37 | loss: 78.6059393CurrentTrain: epoch  7, batch    38 | loss: 95.5827508CurrentTrain: epoch  7, batch    39 | loss: 79.7709343CurrentTrain: epoch  7, batch    40 | loss: 121.2319860CurrentTrain: epoch  7, batch    41 | loss: 103.2376516CurrentTrain: epoch  7, batch    42 | loss: 101.3225870CurrentTrain: epoch  7, batch    43 | loss: 81.7308670CurrentTrain: epoch  7, batch    44 | loss: 67.8730690CurrentTrain: epoch  7, batch    45 | loss: 73.5903669CurrentTrain: epoch  7, batch    46 | loss: 56.3335801CurrentTrain: epoch  7, batch    47 | loss: 70.4547764CurrentTrain: epoch  7, batch    48 | loss: 68.0049334CurrentTrain: epoch  7, batch    49 | loss: 68.6621834CurrentTrain: epoch  7, batch    50 | loss: 78.1033040CurrentTrain: epoch  7, batch    51 | loss: 98.7367364CurrentTrain: epoch  7, batch    52 | loss: 66.1307768CurrentTrain: epoch  7, batch    53 | loss: 75.9564972CurrentTrain: epoch  7, batch    54 | loss: 98.2336618CurrentTrain: epoch  7, batch    55 | loss: 67.6112080CurrentTrain: epoch  7, batch    56 | loss: 65.9153338CurrentTrain: epoch  7, batch    57 | loss: 65.1923185CurrentTrain: epoch  7, batch    58 | loss: 54.8817979CurrentTrain: epoch  7, batch    59 | loss: 127.5768646CurrentTrain: epoch  7, batch    60 | loss: 63.0879561CurrentTrain: epoch  7, batch    61 | loss: 68.4644117CurrentTrain: epoch  7, batch    62 | loss: 79.9486113CurrentTrain: epoch  7, batch    63 | loss: 120.5889249CurrentTrain: epoch  7, batch    64 | loss: 125.3869270CurrentTrain: epoch  7, batch    65 | loss: 99.3985668CurrentTrain: epoch  7, batch    66 | loss: 99.8382258CurrentTrain: epoch  7, batch    67 | loss: 65.4921448CurrentTrain: epoch  7, batch    68 | loss: 66.7091857CurrentTrain: epoch  7, batch    69 | loss: 96.9729244CurrentTrain: epoch  7, batch    70 | loss: 99.1147044CurrentTrain: epoch  7, batch    71 | loss: 77.8316513CurrentTrain: epoch  7, batch    72 | loss: 57.9259658CurrentTrain: epoch  7, batch    73 | loss: 79.5969177CurrentTrain: epoch  7, batch    74 | loss: 129.6532304CurrentTrain: epoch  7, batch    75 | loss: 96.2427067CurrentTrain: epoch  7, batch    76 | loss: 96.0762010CurrentTrain: epoch  7, batch    77 | loss: 129.3297315CurrentTrain: epoch  7, batch    78 | loss: 66.3327570CurrentTrain: epoch  7, batch    79 | loss: 123.9744868CurrentTrain: epoch  7, batch    80 | loss: 82.1487750CurrentTrain: epoch  7, batch    81 | loss: 65.5032777CurrentTrain: epoch  7, batch    82 | loss: 94.6730674CurrentTrain: epoch  7, batch    83 | loss: 65.9685295CurrentTrain: epoch  7, batch    84 | loss: 79.2508988CurrentTrain: epoch  7, batch    85 | loss: 68.4561767CurrentTrain: epoch  7, batch    86 | loss: 69.2594963CurrentTrain: epoch  7, batch    87 | loss: 99.2245899CurrentTrain: epoch  7, batch    88 | loss: 99.3304655CurrentTrain: epoch  7, batch    89 | loss: 74.8246583CurrentTrain: epoch  7, batch    90 | loss: 95.9596674CurrentTrain: epoch  7, batch    91 | loss: 75.1974974CurrentTrain: epoch  7, batch    92 | loss: 64.5491692CurrentTrain: epoch  7, batch    93 | loss: 128.2566530CurrentTrain: epoch  7, batch    94 | loss: 68.2193312CurrentTrain: epoch  7, batch    95 | loss: 81.1166015CurrentTrain: epoch  8, batch     0 | loss: 51.7986113CurrentTrain: epoch  8, batch     1 | loss: 126.0247785CurrentTrain: epoch  8, batch     2 | loss: 79.9628121CurrentTrain: epoch  8, batch     3 | loss: 90.5825652CurrentTrain: epoch  8, batch     4 | loss: 78.6659676CurrentTrain: epoch  8, batch     5 | loss: 65.3237585CurrentTrain: epoch  8, batch     6 | loss: 98.5270551CurrentTrain: epoch  8, batch     7 | loss: 78.6286913CurrentTrain: epoch  8, batch     8 | loss: 76.1505864CurrentTrain: epoch  8, batch     9 | loss: 120.6097397CurrentTrain: epoch  8, batch    10 | loss: 95.5797316CurrentTrain: epoch  8, batch    11 | loss: 96.8818091CurrentTrain: epoch  8, batch    12 | loss: 53.6403483CurrentTrain: epoch  8, batch    13 | loss: 56.8367375CurrentTrain: epoch  8, batch    14 | loss: 59.0435251CurrentTrain: epoch  8, batch    15 | loss: 78.4727993CurrentTrain: epoch  8, batch    16 | loss: 78.3919002CurrentTrain: epoch  8, batch    17 | loss: 61.5152737CurrentTrain: epoch  8, batch    18 | loss: 58.8540477CurrentTrain: epoch  8, batch    19 | loss: 68.2894867CurrentTrain: epoch  8, batch    20 | loss: 64.2422436CurrentTrain: epoch  8, batch    21 | loss: 80.2144922CurrentTrain: epoch  8, batch    22 | loss: 76.7732584CurrentTrain: epoch  8, batch    23 | loss: 56.3444350CurrentTrain: epoch  8, batch    24 | loss: 93.9254848CurrentTrain: epoch  8, batch    25 | loss: 98.4518901CurrentTrain: epoch  8, batch    26 | loss: 79.7431179CurrentTrain: epoch  8, batch    27 | loss: 92.9599335CurrentTrain: epoch  8, batch    28 | loss: 96.8957879CurrentTrain: epoch  8, batch    29 | loss: 95.9549557CurrentTrain: epoch  8, batch    30 | loss: 56.6838767CurrentTrain: epoch  8, batch    31 | loss: 67.6030947CurrentTrain: epoch  8, batch    32 | loss: 62.2493506CurrentTrain: epoch  8, batch    33 | loss: 66.9253271CurrentTrain: epoch  8, batch    34 | loss: 66.8127537CurrentTrain: epoch  8, batch    35 | loss: 70.1778545CurrentTrain: epoch  8, batch    36 | loss: 66.5005512CurrentTrain: epoch  8, batch    37 | loss: 57.1889127CurrentTrain: epoch  8, batch    38 | loss: 66.0710364CurrentTrain: epoch  8, batch    39 | loss: 95.5657669CurrentTrain: epoch  8, batch    40 | loss: 81.7051868CurrentTrain: epoch  8, batch    41 | loss: 80.3971317CurrentTrain: epoch  8, batch    42 | loss: 58.9603383CurrentTrain: epoch  8, batch    43 | loss: 67.3960412CurrentTrain: epoch  8, batch    44 | loss: 76.9078264CurrentTrain: epoch  8, batch    45 | loss: 67.7974351CurrentTrain: epoch  8, batch    46 | loss: 63.1610118CurrentTrain: epoch  8, batch    47 | loss: 89.8878262CurrentTrain: epoch  8, batch    48 | loss: 64.3316226CurrentTrain: epoch  8, batch    49 | loss: 97.0250689CurrentTrain: epoch  8, batch    50 | loss: 77.9062234CurrentTrain: epoch  8, batch    51 | loss: 69.5406820CurrentTrain: epoch  8, batch    52 | loss: 68.7876539CurrentTrain: epoch  8, batch    53 | loss: 72.6595817CurrentTrain: epoch  8, batch    54 | loss: 95.2576705CurrentTrain: epoch  8, batch    55 | loss: 79.4273965CurrentTrain: epoch  8, batch    56 | loss: 118.4284593CurrentTrain: epoch  8, batch    57 | loss: 126.4895566CurrentTrain: epoch  8, batch    58 | loss: 75.0129975CurrentTrain: epoch  8, batch    59 | loss: 66.4719341CurrentTrain: epoch  8, batch    60 | loss: 99.5891841CurrentTrain: epoch  8, batch    61 | loss: 99.7940518CurrentTrain: epoch  8, batch    62 | loss: 80.4446765CurrentTrain: epoch  8, batch    63 | loss: 94.7618849CurrentTrain: epoch  8, batch    64 | loss: 96.0890966CurrentTrain: epoch  8, batch    65 | loss: 76.0745852CurrentTrain: epoch  8, batch    66 | loss: 69.2270872CurrentTrain: epoch  8, batch    67 | loss: 79.9480542CurrentTrain: epoch  8, batch    68 | loss: 66.6067166CurrentTrain: epoch  8, batch    69 | loss: 77.9545594CurrentTrain: epoch  8, batch    70 | loss: 93.7809180CurrentTrain: epoch  8, batch    71 | loss: 69.2874133CurrentTrain: epoch  8, batch    72 | loss: 66.7161391CurrentTrain: epoch  8, batch    73 | loss: 125.2950084CurrentTrain: epoch  8, batch    74 | loss: 124.0678630CurrentTrain: epoch  8, batch    75 | loss: 97.6930284CurrentTrain: epoch  8, batch    76 | loss: 97.5809951CurrentTrain: epoch  8, batch    77 | loss: 118.4812775CurrentTrain: epoch  8, batch    78 | loss: 95.8794337CurrentTrain: epoch  8, batch    79 | loss: 72.8275285CurrentTrain: epoch  8, batch    80 | loss: 61.9670800CurrentTrain: epoch  8, batch    81 | loss: 96.9913451CurrentTrain: epoch  8, batch    82 | loss: 123.2461572CurrentTrain: epoch  8, batch    83 | loss: 95.5397175CurrentTrain: epoch  8, batch    84 | loss: 81.7491129CurrentTrain: epoch  8, batch    85 | loss: 126.9104774CurrentTrain: epoch  8, batch    86 | loss: 81.1055934CurrentTrain: epoch  8, batch    87 | loss: 66.1113483CurrentTrain: epoch  8, batch    88 | loss: 126.3697010CurrentTrain: epoch  8, batch    89 | loss: 68.9210203CurrentTrain: epoch  8, batch    90 | loss: 78.0838470CurrentTrain: epoch  8, batch    91 | loss: 66.9963658CurrentTrain: epoch  8, batch    92 | loss: 76.4644278CurrentTrain: epoch  8, batch    93 | loss: 63.7883288CurrentTrain: epoch  8, batch    94 | loss: 77.4243908CurrentTrain: epoch  8, batch    95 | loss: 81.4585277CurrentTrain: epoch  9, batch     0 | loss: 78.7119548CurrentTrain: epoch  9, batch     1 | loss: 69.4171869CurrentTrain: epoch  9, batch     2 | loss: 63.9593667CurrentTrain: epoch  9, batch     3 | loss: 80.6910514CurrentTrain: epoch  9, batch     4 | loss: 80.6343326CurrentTrain: epoch  9, batch     5 | loss: 95.0607111CurrentTrain: epoch  9, batch     6 | loss: 74.9217862CurrentTrain: epoch  9, batch     7 | loss: 54.0622039CurrentTrain: epoch  9, batch     8 | loss: 75.1534006CurrentTrain: epoch  9, batch     9 | loss: 98.1886154CurrentTrain: epoch  9, batch    10 | loss: 67.0278638CurrentTrain: epoch  9, batch    11 | loss: 54.6383414CurrentTrain: epoch  9, batch    12 | loss: 79.0644511CurrentTrain: epoch  9, batch    13 | loss: 93.0169600CurrentTrain: epoch  9, batch    14 | loss: 62.2225316CurrentTrain: epoch  9, batch    15 | loss: 94.8744888CurrentTrain: epoch  9, batch    16 | loss: 78.9525565CurrentTrain: epoch  9, batch    17 | loss: 73.8402251CurrentTrain: epoch  9, batch    18 | loss: 79.9679676CurrentTrain: epoch  9, batch    19 | loss: 76.2952447CurrentTrain: epoch  9, batch    20 | loss: 92.4383404CurrentTrain: epoch  9, batch    21 | loss: 65.2800215CurrentTrain: epoch  9, batch    22 | loss: 71.4439291CurrentTrain: epoch  9, batch    23 | loss: 81.2453749CurrentTrain: epoch  9, batch    24 | loss: 64.5191092CurrentTrain: epoch  9, batch    25 | loss: 100.5370890CurrentTrain: epoch  9, batch    26 | loss: 65.1508794CurrentTrain: epoch  9, batch    27 | loss: 76.1040637CurrentTrain: epoch  9, batch    28 | loss: 76.5247143CurrentTrain: epoch  9, batch    29 | loss: 74.1099957CurrentTrain: epoch  9, batch    30 | loss: 75.6090435CurrentTrain: epoch  9, batch    31 | loss: 72.3800677CurrentTrain: epoch  9, batch    32 | loss: 67.3051108CurrentTrain: epoch  9, batch    33 | loss: 76.3838457CurrentTrain: epoch  9, batch    34 | loss: 75.1471951CurrentTrain: epoch  9, batch    35 | loss: 77.7705995CurrentTrain: epoch  9, batch    36 | loss: 98.6406445CurrentTrain: epoch  9, batch    37 | loss: 171.2301905CurrentTrain: epoch  9, batch    38 | loss: 93.7460255CurrentTrain: epoch  9, batch    39 | loss: 80.1980157CurrentTrain: epoch  9, batch    40 | loss: 63.0819288CurrentTrain: epoch  9, batch    41 | loss: 80.2428630CurrentTrain: epoch  9, batch    42 | loss: 65.1836011CurrentTrain: epoch  9, batch    43 | loss: 80.5063598CurrentTrain: epoch  9, batch    44 | loss: 64.1240295CurrentTrain: epoch  9, batch    45 | loss: 93.1944662CurrentTrain: epoch  9, batch    46 | loss: 79.8180122CurrentTrain: epoch  9, batch    47 | loss: 65.1106414CurrentTrain: epoch  9, batch    48 | loss: 78.7181231CurrentTrain: epoch  9, batch    49 | loss: 70.0358080CurrentTrain: epoch  9, batch    50 | loss: 64.4715130CurrentTrain: epoch  9, batch    51 | loss: 77.8942081CurrentTrain: epoch  9, batch    52 | loss: 66.2260219CurrentTrain: epoch  9, batch    53 | loss: 126.3817980CurrentTrain: epoch  9, batch    54 | loss: 64.6948925CurrentTrain: epoch  9, batch    55 | loss: 123.2839241CurrentTrain: epoch  9, batch    56 | loss: 98.4494264CurrentTrain: epoch  9, batch    57 | loss: 77.1567957CurrentTrain: epoch  9, batch    58 | loss: 95.7561152CurrentTrain: epoch  9, batch    59 | loss: 92.4754634CurrentTrain: epoch  9, batch    60 | loss: 78.6574621CurrentTrain: epoch  9, batch    61 | loss: 83.2562673CurrentTrain: epoch  9, batch    62 | loss: 66.0971409CurrentTrain: epoch  9, batch    63 | loss: 75.2992495CurrentTrain: epoch  9, batch    64 | loss: 127.5831563CurrentTrain: epoch  9, batch    65 | loss: 96.6186290CurrentTrain: epoch  9, batch    66 | loss: 81.6602333CurrentTrain: epoch  9, batch    67 | loss: 95.5161264CurrentTrain: epoch  9, batch    68 | loss: 123.9447461CurrentTrain: epoch  9, batch    69 | loss: 93.7366553CurrentTrain: epoch  9, batch    70 | loss: 75.0561818CurrentTrain: epoch  9, batch    71 | loss: 67.9669794CurrentTrain: epoch  9, batch    72 | loss: 56.1445310CurrentTrain: epoch  9, batch    73 | loss: 67.6097652CurrentTrain: epoch  9, batch    74 | loss: 74.1707530CurrentTrain: epoch  9, batch    75 | loss: 81.0239023CurrentTrain: epoch  9, batch    76 | loss: 65.5645604CurrentTrain: epoch  9, batch    77 | loss: 119.2934275CurrentTrain: epoch  9, batch    78 | loss: 91.9690471CurrentTrain: epoch  9, batch    79 | loss: 52.6198166CurrentTrain: epoch  9, batch    80 | loss: 57.0038966CurrentTrain: epoch  9, batch    81 | loss: 58.5909438CurrentTrain: epoch  9, batch    82 | loss: 67.1144114CurrentTrain: epoch  9, batch    83 | loss: 65.8690198CurrentTrain: epoch  9, batch    84 | loss: 83.6842497CurrentTrain: epoch  9, batch    85 | loss: 70.5443289CurrentTrain: epoch  9, batch    86 | loss: 55.9804715CurrentTrain: epoch  9, batch    87 | loss: 76.8587881CurrentTrain: epoch  9, batch    88 | loss: 92.7882205CurrentTrain: epoch  9, batch    89 | loss: 65.8332362CurrentTrain: epoch  9, batch    90 | loss: 76.7138132CurrentTrain: epoch  9, batch    91 | loss: 69.2749810CurrentTrain: epoch  9, batch    92 | loss: 125.7000886CurrentTrain: epoch  9, batch    93 | loss: 79.8264778CurrentTrain: epoch  9, batch    94 | loss: 74.7596289CurrentTrain: epoch  9, batch    95 | loss: 102.4098858

F1 score per class: {32: 0.6274509803921569, 6: 0.8019323671497585, 19: 0.3888888888888889, 24: 0.7570621468926554, 26: 0.9405940594059405, 29: 0.8669950738916257}
Micro-average F1 score: 0.7852283770651117
Weighted-average F1 score: 0.7859032039663137
F1 score per class: {32: 0.6008583690987125, 6: 0.7807017543859649, 19: 0.27450980392156865, 24: 0.7204301075268817, 26: 0.9346733668341709, 29: 0.8542713567839196}
Micro-average F1 score: 0.75
Weighted-average F1 score: 0.7404244430215342
F1 score per class: {32: 0.5726872246696035, 6: 0.8036529680365296, 19: 0.2916666666666667, 24: 0.7362637362637363, 26: 0.9346733668341709, 29: 0.85}
Micro-average F1 score: 0.7534883720930232
Weighted-average F1 score: 0.7451028194247684

F1 score per class: {32: 0.6274509803921569, 6: 0.8019323671497585, 19: 0.3888888888888889, 24: 0.7570621468926554, 26: 0.9405940594059405, 29: 0.8669950738916257}
Micro-average F1 score: 0.7852283770651117
Weighted-average F1 score: 0.7859032039663137
F1 score per class: {32: 0.6008583690987125, 6: 0.7807017543859649, 19: 0.27450980392156865, 24: 0.7204301075268817, 26: 0.9346733668341709, 29: 0.8542713567839196}
Micro-average F1 score: 0.75
Weighted-average F1 score: 0.7404244430215342
F1 score per class: {32: 0.5726872246696035, 6: 0.8036529680365296, 19: 0.2916666666666667, 24: 0.7362637362637363, 26: 0.9346733668341709, 29: 0.85}
Micro-average F1 score: 0.7534883720930232
Weighted-average F1 score: 0.7451028194247684

F1 score per class: {32: 0.44912280701754387, 6: 0.7579908675799086, 19: 0.23728813559322035, 24: 0.694300518134715, 26: 0.8558558558558559, 29: 0.6848249027237354}
Micro-average F1 score: 0.654251012145749
Weighted-average F1 score: 0.6408916575009012
F1 score per class: {32: 0.40816326530612246, 6: 0.7235772357723578, 19: 0.1728395061728395, 24: 0.6568627450980392, 26: 0.8493150684931506, 29: 0.6827309236947792}
Micro-average F1 score: 0.6125186289120715
Weighted-average F1 score: 0.5908057828306359
F1 score per class: {32: 0.3939393939393939, 6: 0.7586206896551724, 19: 0.1891891891891892, 24: 0.6733668341708543, 26: 0.8493150684931506, 29: 0.6772908366533864}
Micro-average F1 score: 0.6206896551724138
Weighted-average F1 score: 0.598908910169503

F1 score per class: {32: 0.44912280701754387, 6: 0.7579908675799086, 19: 0.23728813559322035, 24: 0.694300518134715, 26: 0.8558558558558559, 29: 0.6848249027237354}
Micro-average F1 score: 0.654251012145749
Weighted-average F1 score: 0.6408916575009012
F1 score per class: {32: 0.40816326530612246, 6: 0.7235772357723578, 19: 0.1728395061728395, 24: 0.6568627450980392, 26: 0.8493150684931506, 29: 0.6827309236947792}
Micro-average F1 score: 0.6125186289120715
Weighted-average F1 score: 0.5908057828306359
F1 score per class: {32: 0.3939393939393939, 6: 0.7586206896551724, 19: 0.1891891891891892, 24: 0.6733668341708543, 26: 0.8493150684931506, 29: 0.6772908366533864}
Micro-average F1 score: 0.6206896551724138
Weighted-average F1 score: 0.598908910169503
cur_acc_wo_na:  ['0.7852']
his_acc_wo_na:  ['0.7852']
cur_acc des_wo_na:  ['0.7500']
his_acc des_wo_na:  ['0.7500']
cur_acc rrf_wo_na:  ['0.7535']
his_acc rrf_wo_na:  ['0.7535']
cur_acc_w_na:  ['0.6543']
his_acc_w_na:  ['0.6543']
cur_acc des_w_na:  ['0.6125']
his_acc des_w_na:  ['0.6125']
cur_acc rrf_w_na:  ['0.6207']
his_acc rrf_w_na:  ['0.6207']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges'])
CurrentTrain: epoch  0, batch     0 | loss: 85.6101500CurrentTrain: epoch  0, batch     1 | loss: 148.8800891CurrentTrain: epoch  0, batch     2 | loss: 113.9015777CurrentTrain: epoch  0, batch     3 | loss: 90.7190494CurrentTrain: epoch  0, batch     4 | loss: 118.1673657CurrentTrain: epoch  1, batch     0 | loss: 90.9363990CurrentTrain: epoch  1, batch     1 | loss: 109.0649558CurrentTrain: epoch  1, batch     2 | loss: 77.3156523CurrentTrain: epoch  1, batch     3 | loss: 106.8312334CurrentTrain: epoch  1, batch     4 | loss: 59.2284692CurrentTrain: epoch  2, batch     0 | loss: 84.7536840CurrentTrain: epoch  2, batch     1 | loss: 127.7965135CurrentTrain: epoch  2, batch     2 | loss: 106.3439667CurrentTrain: epoch  2, batch     3 | loss: 89.6966445CurrentTrain: epoch  2, batch     4 | loss: 66.5904358CurrentTrain: epoch  3, batch     0 | loss: 106.4798129CurrentTrain: epoch  3, batch     1 | loss: 83.2322126CurrentTrain: epoch  3, batch     2 | loss: 103.8324148CurrentTrain: epoch  3, batch     3 | loss: 100.9203920CurrentTrain: epoch  3, batch     4 | loss: 85.2236313CurrentTrain: epoch  4, batch     0 | loss: 99.5618803CurrentTrain: epoch  4, batch     1 | loss: 71.0661282CurrentTrain: epoch  4, batch     2 | loss: 129.3913018CurrentTrain: epoch  4, batch     3 | loss: 69.4298403CurrentTrain: epoch  4, batch     4 | loss: 83.2197533CurrentTrain: epoch  5, batch     0 | loss: 68.6975892CurrentTrain: epoch  5, batch     1 | loss: 102.0556073CurrentTrain: epoch  5, batch     2 | loss: 101.2233029CurrentTrain: epoch  5, batch     3 | loss: 125.9000422CurrentTrain: epoch  5, batch     4 | loss: 58.7784004CurrentTrain: epoch  6, batch     0 | loss: 81.1343787CurrentTrain: epoch  6, batch     1 | loss: 68.7072696CurrentTrain: epoch  6, batch     2 | loss: 121.7905894CurrentTrain: epoch  6, batch     3 | loss: 81.1500536CurrentTrain: epoch  6, batch     4 | loss: 109.4472370CurrentTrain: epoch  7, batch     0 | loss: 100.2372928CurrentTrain: epoch  7, batch     1 | loss: 95.8085630CurrentTrain: epoch  7, batch     2 | loss: 125.6050890CurrentTrain: epoch  7, batch     3 | loss: 63.2869575CurrentTrain: epoch  7, batch     4 | loss: 75.0582192CurrentTrain: epoch  8, batch     0 | loss: 67.8017723CurrentTrain: epoch  8, batch     1 | loss: 97.0082218CurrentTrain: epoch  8, batch     2 | loss: 81.0385512CurrentTrain: epoch  8, batch     3 | loss: 80.1307904CurrentTrain: epoch  8, batch     4 | loss: 57.3692345CurrentTrain: epoch  9, batch     0 | loss: 80.9608785CurrentTrain: epoch  9, batch     1 | loss: 95.1167521CurrentTrain: epoch  9, batch     2 | loss: 75.1275955CurrentTrain: epoch  9, batch     3 | loss: 79.8754227CurrentTrain: epoch  9, batch     4 | loss: 59.4628235
MemoryTrain:  epoch  0, batch     0 | loss: 1.8814367MemoryTrain:  epoch  1, batch     0 | loss: 1.4534424MemoryTrain:  epoch  2, batch     0 | loss: 1.2177022MemoryTrain:  epoch  3, batch     0 | loss: 1.0431380MemoryTrain:  epoch  4, batch     0 | loss: 0.8631385MemoryTrain:  epoch  5, batch     0 | loss: 0.7450326MemoryTrain:  epoch  6, batch     0 | loss: 0.6204535MemoryTrain:  epoch  7, batch     0 | loss: 0.4720947MemoryTrain:  epoch  8, batch     0 | loss: 0.4871148MemoryTrain:  epoch  9, batch     0 | loss: 0.4059755

F1 score per class: {32: 0.92, 5: 0.0, 6: 0.14545454545454545, 10: 0.6122448979591837, 16: 0.35294117647058826, 17: 0.18604651162790697, 18: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.547085201793722
Weighted-average F1 score: 0.6413353352782476
F1 score per class: {32: 0.7755102040816326, 5: 0.0, 6: 0.5070422535211268, 10: 0.6440677966101694, 16: 0.23529411764705882, 17: 0.3673469387755102, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5510534846029174
Weighted-average F1 score: 0.5182206350378344
F1 score per class: {32: 0.8378378378378378, 5: 0.0, 6: 0.463768115942029, 10: 0.6785714285714286, 16: 0.23529411764705882, 17: 0.3888888888888889, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.5818181818181818
Weighted-average F1 score: 0.5583055686757592

F1 score per class: {32: 0.92, 5: 0.5925925925925926, 6: 0.1391304347826087, 10: 0.5882352941176471, 16: 0.13333333333333333, 17: 0.17777777777777778, 18: 0.8269230769230769, 19: 0.3888888888888889, 24: 0.6941176470588235, 26: 0.9035532994923858, 29: 0.8436018957345972}
Micro-average F1 score: 0.6890203813280736
Weighted-average F1 score: 0.7201970684185479
F1 score per class: {32: 0.7279693486590039, 5: 0.5650557620817844, 6: 0.45, 10: 0.6129032258064516, 16: 0.1, 17: 0.32432432432432434, 18: 0.7489711934156379, 19: 0.3448275862068966, 24: 0.7182320441988951, 26: 0.9154228855721394, 29: 0.8198198198198198}
Micro-average F1 score: 0.6581858407079646
Weighted-average F1 score: 0.6488746561137617
F1 score per class: {32: 0.8122270742358079, 5: 0.5692883895131086, 6: 0.41025641025641024, 10: 0.6440677966101694, 16: 0.0975609756097561, 17: 0.358974358974359, 18: 0.7811158798283262, 19: 0.42857142857142855, 24: 0.7303370786516854, 26: 0.91, 29: 0.8387096774193549}
Micro-average F1 score: 0.6858823529411765
Weighted-average F1 score: 0.6838963899938199

F1 score per class: {32: 0.8034934497816594, 5: 0.0, 6: 0.13559322033898305, 10: 0.43478260869565216, 16: 0.23076923076923078, 17: 0.14285714285714285, 18: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.4236111111111111
Weighted-average F1 score: 0.4322840578814294
F1 score per class: {32: 0.6148867313915858, 5: 0.0, 6: 0.4311377245508982, 10: 0.40860215053763443, 16: 0.16, 17: 0.24324324324324326, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.38857142857142857
Weighted-average F1 score: 0.35509096809619795
F1 score per class: {32: 0.6813186813186813, 5: 0.0, 6: 0.39263803680981596, 10: 0.4222222222222222, 16: 0.16666666666666666, 17: 0.24347826086956523, 18: 0.0, 19: 0.0, 24: 0.0, 26: 0.0, 29: 0.0}
Micro-average F1 score: 0.40816326530612246
Weighted-average F1 score: 0.37289166509509775

F1 score per class: {32: 0.7965367965367965, 5: 0.3673469387755102, 6: 0.12403100775193798, 10: 0.38961038961038963, 16: 0.08571428571428572, 17: 0.12903225806451613, 18: 0.7713004484304933, 19: 0.23333333333333334, 24: 0.6344086021505376, 26: 0.8018018018018018, 29: 0.6544117647058824}
Micro-average F1 score: 0.5446985446985447
Weighted-average F1 score: 0.5412233010412183
F1 score per class: {32: 0.5571847507331378, 5: 0.3415730337078652, 6: 0.3564356435643564, 10: 0.36538461538461536, 16: 0.06349206349206349, 17: 0.20930232558139536, 18: 0.6642335766423357, 19: 0.17857142857142858, 24: 0.6403940886699507, 26: 0.7666666666666667, 29: 0.6476868327402135}
Micro-average F1 score: 0.48830529339351664
Weighted-average F1 score: 0.4687103481033372
F1 score per class: {32: 0.643598615916955, 5: 0.3462414578587699, 6: 0.32323232323232326, 10: 0.38, 16: 0.06349206349206349, 17: 0.21705426356589147, 18: 0.708171206225681, 19: 0.20689655172413793, 24: 0.6565656565656566, 26: 0.774468085106383, 29: 0.6570397111913358}
Micro-average F1 score: 0.5132042253521126
Weighted-average F1 score: 0.494818484530371
cur_acc_wo_na:  ['0.7852', '0.5471']
his_acc_wo_na:  ['0.7852', '0.6890']
cur_acc des_wo_na:  ['0.7500', '0.5511']
his_acc des_wo_na:  ['0.7500', '0.6582']
cur_acc rrf_wo_na:  ['0.7535', '0.5818']
his_acc rrf_wo_na:  ['0.7535', '0.6859']
cur_acc_w_na:  ['0.6543', '0.4236']
his_acc_w_na:  ['0.6543', '0.5447']
cur_acc des_w_na:  ['0.6125', '0.3886']
his_acc des_w_na:  ['0.6125', '0.4883']
cur_acc rrf_w_na:  ['0.6207', '0.4082']
his_acc rrf_w_na:  ['0.6207', '0.5132']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by'])
CurrentTrain: epoch  0, batch     0 | loss: 98.4810445CurrentTrain: epoch  0, batch     1 | loss: 81.7961654CurrentTrain: epoch  0, batch     2 | loss: 80.1304291CurrentTrain: epoch  0, batch     3 | loss: 10.8239445CurrentTrain: epoch  1, batch     0 | loss: 76.6205313CurrentTrain: epoch  1, batch     1 | loss: 75.7832287CurrentTrain: epoch  1, batch     2 | loss: 71.5618441CurrentTrain: epoch  1, batch     3 | loss: 19.4599308CurrentTrain: epoch  2, batch     0 | loss: 66.7658124CurrentTrain: epoch  2, batch     1 | loss: 83.7848399CurrentTrain: epoch  2, batch     2 | loss: 100.4079366CurrentTrain: epoch  2, batch     3 | loss: 19.5341313CurrentTrain: epoch  3, batch     0 | loss: 67.5902904CurrentTrain: epoch  3, batch     1 | loss: 72.4815734CurrentTrain: epoch  3, batch     2 | loss: 81.3534878CurrentTrain: epoch  3, batch     3 | loss: 11.2478591CurrentTrain: epoch  4, batch     0 | loss: 65.9408079CurrentTrain: epoch  4, batch     1 | loss: 67.2504965CurrentTrain: epoch  4, batch     2 | loss: 68.7546639CurrentTrain: epoch  4, batch     3 | loss: 19.6487923CurrentTrain: epoch  5, batch     0 | loss: 64.2925771CurrentTrain: epoch  5, batch     1 | loss: 64.0394255CurrentTrain: epoch  5, batch     2 | loss: 69.4143296CurrentTrain: epoch  5, batch     3 | loss: 17.9564885CurrentTrain: epoch  6, batch     0 | loss: 65.6508318CurrentTrain: epoch  6, batch     1 | loss: 61.5353737CurrentTrain: epoch  6, batch     2 | loss: 65.6991264CurrentTrain: epoch  6, batch     3 | loss: 20.4242833CurrentTrain: epoch  7, batch     0 | loss: 58.6804717CurrentTrain: epoch  7, batch     1 | loss: 73.5960116CurrentTrain: epoch  7, batch     2 | loss: 96.1358880CurrentTrain: epoch  7, batch     3 | loss: 42.0126908CurrentTrain: epoch  8, batch     0 | loss: 63.4870233CurrentTrain: epoch  8, batch     1 | loss: 62.5812027CurrentTrain: epoch  8, batch     2 | loss: 62.6286998CurrentTrain: epoch  8, batch     3 | loss: 17.6569487CurrentTrain: epoch  9, batch     0 | loss: 75.5129808CurrentTrain: epoch  9, batch     1 | loss: 73.2468317CurrentTrain: epoch  9, batch     2 | loss: 72.7209954CurrentTrain: epoch  9, batch     3 | loss: 9.4781887
MemoryTrain:  epoch  0, batch     0 | loss: 1.3845608MemoryTrain:  epoch  1, batch     0 | loss: 1.0983944MemoryTrain:  epoch  2, batch     0 | loss: 0.9502287MemoryTrain:  epoch  3, batch     0 | loss: 0.7871478MemoryTrain:  epoch  4, batch     0 | loss: 0.5608675MemoryTrain:  epoch  5, batch     0 | loss: 0.5497014MemoryTrain:  epoch  6, batch     0 | loss: 0.4297535MemoryTrain:  epoch  7, batch     0 | loss: 0.3673425MemoryTrain:  epoch  8, batch     0 | loss: 0.2991579MemoryTrain:  epoch  9, batch     0 | loss: 0.2549673

F1 score per class: {32: 0.0, 6: 0.4444444444444444, 7: 0.8727272727272727, 40: 0.0, 9: 0.0, 16: 0.0, 19: 0.4444444444444444, 26: 0.3333333333333333, 27: 0.0, 31: 0.2236842105263158}
Micro-average F1 score: 0.3116883116883117
Weighted-average F1 score: 0.25480359755997306
F1 score per class: {32: 0.0, 6: 0.6, 7: 0.704225352112676, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 19: 0.0, 24: 0.2222222222222222, 26: 0.2222222222222222, 27: 0.0, 31: 0.3484848484848485}
Micro-average F1 score: 0.34615384615384615
Weighted-average F1 score: 0.30131231219579496
F1 score per class: {32: 0.0, 6: 0.6666666666666666, 7: 0.7619047619047619, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 19: 0.5, 26: 0.25, 27: 0.0, 31: 0.35294117647058826}
Micro-average F1 score: 0.38
Weighted-average F1 score: 0.3218125960061444

F1 score per class: {32: 0.8761904761904762, 5: 0.43312101910828027, 6: 0.035398230088495575, 7: 0.8727272727272727, 40: 0.17094017094017094, 10: 0.5357142857142857, 9: 0.0, 16: 0.1276595744680851, 17: 0.6349206349206349, 18: 0.2, 19: 0.7303370786516854, 24: 0.38095238095238093, 26: 0.8736842105263158, 27: 0.25, 29: 0.8383838383838383, 31: 0.09340659340659341}
Micro-average F1 score: 0.5160320641282565
Weighted-average F1 score: 0.4735068885622529
F1 score per class: {32: 0.6811594202898551, 5: 0.36942675159235666, 6: 0.04225352112676056, 7: 0.6666666666666666, 40: 0.3561643835616438, 10: 0.49230769230769234, 9: 0.0, 16: 0.3116883116883117, 17: 0.6287878787878788, 18: 0.3076923076923077, 19: 0.7333333333333333, 24: 0.2, 26: 0.8854166666666666, 27: 0.15384615384615385, 29: 0.84, 31: 0.20087336244541484}
Micro-average F1 score: 0.5326295585412668
Weighted-average F1 score: 0.5056286690847951
F1 score per class: {32: 0.7603305785123967, 5: 0.375, 6: 0.047619047619047616, 7: 0.7619047619047619, 40: 0.3181818181818182, 10: 0.5423728813559322, 9: 0.0, 16: 0.22641509433962265, 17: 0.640926640926641, 18: 0.2727272727272727, 19: 0.7374301675977654, 24: 0.4, 26: 0.8854166666666666, 27: 0.18181818181818182, 29: 0.84, 31: 0.19591836734693877}
Micro-average F1 score: 0.5490394337714863
Weighted-average F1 score: 0.5250281052152234

F1 score per class: {32: 0.0, 6: 0.4444444444444444, 7: 0.8, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 19: 0.42105263157894735, 26: 0.0, 27: 0.2857142857142857, 29: 0.0, 31: 0.192090395480226}
Micro-average F1 score: 0.2711864406779661
Weighted-average F1 score: 0.22446049209653135
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.5, 7: 0.5952380952380952, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 19: 0.0, 24: 0.2222222222222222, 26: 0.16666666666666666, 27: 0.0, 31: 0.3129251700680272}
Micro-average F1 score: 0.29508196721311475
Weighted-average F1 score: 0.2600106180038153
F1 score per class: {32: 0.0, 5: 0.0, 6: 0.6, 7: 0.6956521739130435, 40: 0.0, 9: 0.0, 10: 0.0, 16: 0.0, 19: 0.47619047619047616, 26: 0.0, 27: 0.18181818181818182, 29: 0.0, 31: 0.3096774193548387}
Micro-average F1 score: 0.3285302593659942
Weighted-average F1 score: 0.2802612292189669

F1 score per class: {32: 0.7076923076923077, 5: 0.3177570093457944, 6: 0.019138755980861243, 7: 0.7868852459016393, 40: 0.14925373134328357, 10: 0.3191489361702128, 9: 0.0, 16: 0.11764705882352941, 17: 0.5947955390334573, 18: 0.2, 19: 0.6632653061224489, 24: 0.3333333333333333, 26: 0.7757009345794392, 27: 0.16666666666666666, 29: 0.6831275720164609, 31: 0.0735930735930736}
Micro-average F1 score: 0.413820811570912
Weighted-average F1 score: 0.37513450024083567
F1 score per class: {32: 0.5026737967914439, 5: 0.2636363636363636, 6: 0.023622047244094488, 7: 0.5376344086021505, 40: 0.2810810810810811, 10: 0.29906542056074764, 9: 0.0, 16: 0.19834710743801653, 17: 0.5824561403508772, 18: 0.21052631578947367, 19: 0.6567164179104478, 24: 0.17391304347826086, 26: 0.7623318385650224, 27: 0.09523809523809523, 29: 0.6970954356846473, 31: 0.16606498194945848}
Micro-average F1 score: 0.41172106824925814
Weighted-average F1 score: 0.38448559504118507
F1 score per class: {32: 0.5592705167173252, 5: 0.2608695652173913, 6: 0.026785714285714284, 7: 0.676056338028169, 40: 0.2625, 10: 0.32989690721649484, 9: 0.0, 16: 0.16666666666666666, 17: 0.6014492753623188, 18: 0.24, 19: 0.66, 24: 0.3333333333333333, 26: 0.7727272727272727, 27: 0.1111111111111111, 29: 0.6942148760330579, 31: 0.1568627450980392}
Micro-average F1 score: 0.431980906921241
Weighted-average F1 score: 0.4057158530066742
cur_acc_wo_na:  ['0.7852', '0.5471', '0.3117']
his_acc_wo_na:  ['0.7852', '0.6890', '0.5160']
cur_acc des_wo_na:  ['0.7500', '0.5511', '0.3462']
his_acc des_wo_na:  ['0.7500', '0.6582', '0.5326']
cur_acc rrf_wo_na:  ['0.7535', '0.5818', '0.3800']
his_acc rrf_wo_na:  ['0.7535', '0.6859', '0.5490']
cur_acc_w_na:  ['0.6543', '0.4236', '0.2712']
his_acc_w_na:  ['0.6543', '0.5447', '0.4138']
cur_acc des_w_na:  ['0.6125', '0.3886', '0.2951']
his_acc des_w_na:  ['0.6125', '0.4883', '0.4117']
cur_acc rrf_w_na:  ['0.6207', '0.4082', '0.3285']
his_acc rrf_w_na:  ['0.6207', '0.5132', '0.4320']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion'])
CurrentTrain: epoch  0, batch     0 | loss: 101.8272577CurrentTrain: epoch  0, batch     1 | loss: 94.9584311CurrentTrain: epoch  0, batch     2 | loss: 119.3117084CurrentTrain: epoch  0, batch     3 | loss: 92.7020398CurrentTrain: epoch  1, batch     0 | loss: 77.3521515CurrentTrain: epoch  1, batch     1 | loss: 74.9323098CurrentTrain: epoch  1, batch     2 | loss: 94.3932005CurrentTrain: epoch  1, batch     3 | loss: 91.6719176CurrentTrain: epoch  2, batch     0 | loss: 87.1224242CurrentTrain: epoch  2, batch     1 | loss: 71.9461084CurrentTrain: epoch  2, batch     2 | loss: 106.8101516CurrentTrain: epoch  2, batch     3 | loss: 71.4097160CurrentTrain: epoch  3, batch     0 | loss: 99.6370906CurrentTrain: epoch  3, batch     1 | loss: 84.5533818CurrentTrain: epoch  3, batch     2 | loss: 84.8505386CurrentTrain: epoch  3, batch     3 | loss: 81.5377035CurrentTrain: epoch  4, batch     0 | loss: 98.8269811CurrentTrain: epoch  4, batch     1 | loss: 100.9761869CurrentTrain: epoch  4, batch     2 | loss: 85.6388997CurrentTrain: epoch  4, batch     3 | loss: 53.8203942CurrentTrain: epoch  5, batch     0 | loss: 98.1517949CurrentTrain: epoch  5, batch     1 | loss: 79.4154163CurrentTrain: epoch  5, batch     2 | loss: 103.2310751CurrentTrain: epoch  5, batch     3 | loss: 64.3673858CurrentTrain: epoch  6, batch     0 | loss: 80.6306211CurrentTrain: epoch  6, batch     1 | loss: 78.5946656CurrentTrain: epoch  6, batch     2 | loss: 82.8491217CurrentTrain: epoch  6, batch     3 | loss: 78.1305502CurrentTrain: epoch  7, batch     0 | loss: 78.1891893CurrentTrain: epoch  7, batch     1 | loss: 96.8083226CurrentTrain: epoch  7, batch     2 | loss: 80.1888076CurrentTrain: epoch  7, batch     3 | loss: 67.1456814CurrentTrain: epoch  8, batch     0 | loss: 82.1071559CurrentTrain: epoch  8, batch     1 | loss: 76.7804020CurrentTrain: epoch  8, batch     2 | loss: 76.7110230CurrentTrain: epoch  8, batch     3 | loss: 54.8525258CurrentTrain: epoch  9, batch     0 | loss: 75.3029568CurrentTrain: epoch  9, batch     1 | loss: 65.8987964CurrentTrain: epoch  9, batch     2 | loss: 97.4612447CurrentTrain: epoch  9, batch     3 | loss: 78.3680848
MemoryTrain:  epoch  0, batch     0 | loss: 1.0597277MemoryTrain:  epoch  1, batch     0 | loss: 0.8891294MemoryTrain:  epoch  2, batch     0 | loss: 0.7355415MemoryTrain:  epoch  3, batch     0 | loss: 0.5892482MemoryTrain:  epoch  4, batch     0 | loss: 0.5094119MemoryTrain:  epoch  5, batch     0 | loss: 0.3985947MemoryTrain:  epoch  6, batch     0 | loss: 0.3742480MemoryTrain:  epoch  7, batch     0 | loss: 0.3059291MemoryTrain:  epoch  8, batch     0 | loss: 0.2695019MemoryTrain:  epoch  9, batch     0 | loss: 0.2481442

F1 score per class: {0: 0.9041095890410958, 4: 0.9247311827956989, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.2, 18: 0.0, 19: 0.0, 21: 0.4, 23: 0.8275862068965517, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.6731517509727627
Weighted-average F1 score: 0.5706744054482028
F1 score per class: {0: 0.7692307692307693, 4: 0.9035532994923858, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.16666666666666666, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.4827586206896552, 23: 0.75, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.6010016694490818
Weighted-average F1 score: 0.5016735646820852
F1 score per class: {0: 0.8918918918918919, 4: 0.9157894736842105, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.17391304347826086, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.4444444444444444, 23: 0.7586206896551724, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.6294964028776978
Weighted-average F1 score: 0.5211861226863593

F1 score per class: {0: 0.6804123711340206, 4: 0.9247311827956989, 5: 0.8440366972477065, 6: 0.3188405797101449, 7: 0.05084745762711865, 9: 0.8, 10: 0.08695652173913043, 13: 0.03305785123966942, 16: 0.6440677966101694, 17: 0.0, 18: 0.2978723404255319, 19: 0.6638655462184874, 21: 0.26229508196721313, 23: 0.7346938775510204, 24: 0.1, 26: 0.6190476190476191, 27: 0.3783783783783784, 29: 0.8497409326424871, 31: 0.25, 32: 0.8113207547169812, 40: 0.20863309352517986}
Micro-average F1 score: 0.5365853658536586
Weighted-average F1 score: 0.5030694432484578
F1 score per class: {0: 0.4861111111111111, 4: 0.8944723618090452, 5: 0.5900621118012422, 6: 0.3699421965317919, 7: 0.046875, 9: 0.6410256410256411, 10: 0.3067484662576687, 13: 0.03076923076923077, 16: 0.4888888888888889, 17: 0.0, 18: 0.3950617283950617, 19: 0.5592105263157895, 21: 0.302158273381295, 23: 0.6470588235294118, 24: 0.23076923076923078, 26: 0.6844919786096256, 27: 0.35714285714285715, 29: 0.8571428571428571, 31: 0.08333333333333333, 32: 0.7699530516431925, 40: 0.17674418604651163}
Micro-average F1 score: 0.5030425963488844
Weighted-average F1 score: 0.46978800527034736
F1 score per class: {0: 0.6226415094339622, 4: 0.9109947643979057, 5: 0.7049808429118773, 6: 0.35802469135802467, 7: 0.05084745762711865, 9: 0.7936507936507936, 10: 0.14388489208633093, 13: 0.027777777777777776, 16: 0.6111111111111112, 17: 0.0, 18: 0.32142857142857145, 19: 0.625, 21: 0.2702702702702703, 23: 0.66, 24: 0.18181818181818182, 26: 0.6881720430107527, 27: 0.3225806451612903, 29: 0.8542713567839196, 31: 0.125, 32: 0.7663551401869159, 40: 0.1532258064516129}
Micro-average F1 score: 0.5136017410228509
Weighted-average F1 score: 0.47821095451953527

F1 score per class: {0: 0.88, 4: 0.882051282051282, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.07692307692307693, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3047619047619048, 23: 0.75, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.5088235294117647
Weighted-average F1 score: 0.39163616817964647
F1 score per class: {0: 0.7, 4: 0.839622641509434, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.07692307692307693, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3559322033898305, 23: 0.6346153846153846, 24: 0.0, 26: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4326923076923077
Weighted-average F1 score: 0.3413272860824075
F1 score per class: {0: 0.8461538461538461, 4: 0.8656716417910447, 5: 0.0, 6: 0.0, 7: 0.0, 9: 0.0, 10: 0.0, 13: 0.07142857142857142, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 21: 0.3225806451612903, 23: 0.6407766990291263, 24: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 40: 0.0}
Micro-average F1 score: 0.4623513870541612
Weighted-average F1 score: 0.3568929158318276

F1 score per class: {0: 0.528, 4: 0.8686868686868687, 5: 0.6456140350877193, 6: 0.23404255319148937, 7: 0.03, 9: 0.7272727272727273, 10: 0.072992700729927, 13: 0.01652892561983471, 16: 0.3619047619047619, 17: 0.0, 18: 0.2222222222222222, 19: 0.6171875, 21: 0.1787709497206704, 23: 0.6101694915254238, 24: 0.08695652173913043, 26: 0.5621621621621622, 27: 0.22950819672131148, 29: 0.7130434782608696, 31: 0.14285714285714285, 32: 0.6099290780141844, 40: 0.16666666666666666}
Micro-average F1 score: 0.4101022248947685
Weighted-average F1 score: 0.37465392386685986
F1 score per class: {0: 0.35714285714285715, 4: 0.8127853881278538, 5: 0.35984848484848486, 6: 0.24150943396226415, 7: 0.02727272727272727, 9: 0.5050505050505051, 10: 0.2145922746781116, 13: 0.0163265306122449, 16: 0.2732919254658385, 17: 0.0, 18: 0.22377622377622378, 19: 0.48295454545454547, 21: 0.1935483870967742, 23: 0.5076923076923077, 24: 0.15384615384615385, 26: 0.6124401913875598, 27: 0.25, 29: 0.6877470355731226, 31: 0.05128205128205128, 32: 0.5899280575539568, 40: 0.13818181818181818}
Micro-average F1 score: 0.3573487031700288
Weighted-average F1 score: 0.329182374404807
F1 score per class: {0: 0.45517241379310347, 4: 0.8405797101449275, 5: 0.44878048780487806, 6: 0.24892703862660945, 7: 0.02912621359223301, 9: 0.704225352112676, 10: 0.1092896174863388, 13: 0.014084507042253521, 16: 0.3384615384615385, 17: 0.0, 18: 0.17475728155339806, 19: 0.5483870967741935, 21: 0.17543859649122806, 23: 0.5116279069767442, 24: 0.14285714285714285, 26: 0.6183574879227053, 27: 0.2127659574468085, 29: 0.6995884773662552, 31: 0.07407407407407407, 32: 0.5942028985507246, 40: 0.12025316455696203}
Micro-average F1 score: 0.372044140830268
Weighted-average F1 score: 0.33982587879655496
cur_acc_wo_na:  ['0.7852', '0.5471', '0.3117', '0.6732']
his_acc_wo_na:  ['0.7852', '0.6890', '0.5160', '0.5366']
cur_acc des_wo_na:  ['0.7500', '0.5511', '0.3462', '0.6010']
his_acc des_wo_na:  ['0.7500', '0.6582', '0.5326', '0.5030']
cur_acc rrf_wo_na:  ['0.7535', '0.5818', '0.3800', '0.6295']
his_acc rrf_wo_na:  ['0.7535', '0.6859', '0.5490', '0.5136']
cur_acc_w_na:  ['0.6543', '0.4236', '0.2712', '0.5088']
his_acc_w_na:  ['0.6543', '0.5447', '0.4138', '0.4101']
cur_acc des_w_na:  ['0.6125', '0.3886', '0.2951', '0.4327']
his_acc des_w_na:  ['0.6125', '0.4883', '0.4117', '0.3573']
cur_acc rrf_w_na:  ['0.6207', '0.4082', '0.3285', '0.4624']
his_acc rrf_w_na:  ['0.6207', '0.5132', '0.4320', '0.3720']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse'])
CurrentTrain: epoch  0, batch     0 | loss: 97.3687464CurrentTrain: epoch  0, batch     1 | loss: 92.3743520CurrentTrain: epoch  0, batch     2 | loss: 96.2501692CurrentTrain: epoch  0, batch     3 | loss: 53.0575836CurrentTrain: epoch  1, batch     0 | loss: 75.6694638CurrentTrain: epoch  1, batch     1 | loss: 136.8005583CurrentTrain: epoch  1, batch     2 | loss: 72.7606539CurrentTrain: epoch  1, batch     3 | loss: 55.8829686CurrentTrain: epoch  2, batch     0 | loss: 128.9985940CurrentTrain: epoch  2, batch     1 | loss: 106.3069182CurrentTrain: epoch  2, batch     2 | loss: 70.0468754CurrentTrain: epoch  2, batch     3 | loss: 47.5172233CurrentTrain: epoch  3, batch     0 | loss: 79.6979333CurrentTrain: epoch  3, batch     1 | loss: 71.9842029CurrentTrain: epoch  3, batch     2 | loss: 83.1727441CurrentTrain: epoch  3, batch     3 | loss: 105.3589297CurrentTrain: epoch  4, batch     0 | loss: 79.8431936CurrentTrain: epoch  4, batch     1 | loss: 81.2448070CurrentTrain: epoch  4, batch     2 | loss: 77.8030596CurrentTrain: epoch  4, batch     3 | loss: 104.1968367CurrentTrain: epoch  5, batch     0 | loss: 82.9604079CurrentTrain: epoch  5, batch     1 | loss: 82.4176500CurrentTrain: epoch  5, batch     2 | loss: 78.8386548CurrentTrain: epoch  5, batch     3 | loss: 38.5155327CurrentTrain: epoch  6, batch     0 | loss: 80.2266489CurrentTrain: epoch  6, batch     1 | loss: 73.4387186CurrentTrain: epoch  6, batch     2 | loss: 82.9702595CurrentTrain: epoch  6, batch     3 | loss: 47.5214125CurrentTrain: epoch  7, batch     0 | loss: 96.3886981CurrentTrain: epoch  7, batch     1 | loss: 65.8263405CurrentTrain: epoch  7, batch     2 | loss: 62.9925522CurrentTrain: epoch  7, batch     3 | loss: 49.1373190CurrentTrain: epoch  8, batch     0 | loss: 119.2838988CurrentTrain: epoch  8, batch     1 | loss: 63.3269835CurrentTrain: epoch  8, batch     2 | loss: 94.7568922CurrentTrain: epoch  8, batch     3 | loss: 45.3586862CurrentTrain: epoch  9, batch     0 | loss: 95.8368808CurrentTrain: epoch  9, batch     1 | loss: 76.6777062CurrentTrain: epoch  9, batch     2 | loss: 62.4385009CurrentTrain: epoch  9, batch     3 | loss: 37.5340151
MemoryTrain:  epoch  0, batch     0 | loss: 0.8410401MemoryTrain:  epoch  1, batch     0 | loss: 0.6715462MemoryTrain:  epoch  2, batch     0 | loss: 0.5617416MemoryTrain:  epoch  3, batch     0 | loss: 0.4754322MemoryTrain:  epoch  4, batch     0 | loss: 0.4051517MemoryTrain:  epoch  5, batch     0 | loss: 0.3616451MemoryTrain:  epoch  6, batch     0 | loss: 0.2897785MemoryTrain:  epoch  7, batch     0 | loss: 0.2352238MemoryTrain:  epoch  8, batch     0 | loss: 0.2280543MemoryTrain:  epoch  9, batch     0 | loss: 0.2131256

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.5161290322580645, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 20: 0.8596491228070176, 21: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8571428571428571, 32: 0.0, 33: 0.4, 36: 0.6, 40: 0.0}
Micro-average F1 score: 0.5579399141630901
Weighted-average F1 score: 0.47441183604171716
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.6493506493506493, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.78, 21: 0.0, 23: 0.0, 26: 0.0, 29: 0.0, 30: 0.7317073170731707, 31: 0.0, 32: 0.0, 33: 0.3448275862068966, 36: 0.7086614173228346, 40: 0.0}
Micro-average F1 score: 0.5220338983050847
Weighted-average F1 score: 0.4145275804641343
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.6301369863013698, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.819047619047619, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.7894736842105263, 31: 0.0, 32: 0.0, 33: 0.30303030303030304, 36: 0.6890756302521008, 40: 0.0}
Micro-average F1 score: 0.5444646098003629
Weighted-average F1 score: 0.4403687742220176

F1 score per class: {0: 0.6181818181818182, 4: 0.9473684210526315, 5: 0.8157894736842105, 6: 0.26277372262773724, 7: 0.047244094488188976, 8: 0.38323353293413176, 9: 0.8928571428571429, 10: 0.03773584905660377, 13: 0.0975609756097561, 16: 0.6, 17: 0.0, 18: 0.27586206896551724, 19: 0.6507936507936508, 20: 0.6163522012578616, 21: 0.30952380952380953, 23: 0.6947368421052632, 24: 0.1, 26: 0.6344086021505376, 27: 0.3888888888888889, 29: 0.8241206030150754, 30: 0.8571428571428571, 31: 0.3333333333333333, 32: 0.7447698744769874, 33: 0.1509433962264151, 36: 0.5555555555555556, 40: 0.22026431718061673}
Micro-average F1 score: 0.5455153949129853
Weighted-average F1 score: 0.5399142336487902
F1 score per class: {0: 0.43312101910828027, 4: 0.8995215311004785, 5: 0.5681159420289855, 6: 0.4025157232704403, 7: 0.031746031746031744, 8: 0.3225806451612903, 9: 0.6097560975609756, 10: 0.07936507936507936, 13: 0.1, 16: 0.42857142857142855, 17: 0.0, 18: 0.23809523809523808, 19: 0.5714285714285714, 20: 0.6724137931034483, 21: 0.272108843537415, 23: 0.64, 24: 0.25806451612903225, 26: 0.6536585365853659, 27: 0.3076923076923077, 29: 0.8217821782178217, 30: 0.5084745762711864, 31: 0.08695652173913043, 32: 0.7368421052631579, 33: 0.08928571428571429, 36: 0.5172413793103449, 40: 0.19387755102040816}
Micro-average F1 score: 0.47554714941907594
Weighted-average F1 score: 0.4546259224277302
F1 score per class: {0: 0.5151515151515151, 4: 0.9641025641025641, 5: 0.7238805970149254, 6: 0.38461538461538464, 7: 0.031746031746031744, 8: 0.34980988593155893, 9: 0.8064516129032258, 10: 0.08403361344537816, 13: 0.08888888888888889, 16: 0.547945205479452, 17: 0.0, 18: 0.3010752688172043, 19: 0.6, 20: 0.6666666666666666, 21: 0.27450980392156865, 23: 0.6736842105263158, 24: 0.16666666666666666, 26: 0.6633165829145728, 27: 0.3448275862068966, 29: 0.82, 30: 0.6976744186046512, 31: 0.125, 32: 0.7272727272727273, 33: 0.08130081300813008, 36: 0.5290322580645161, 40: 0.17674418604651163}
Micro-average F1 score: 0.5061153174140943
Weighted-average F1 score: 0.48180386100053496

F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.4413793103448276, 10: 0.0, 13: 0.0, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.5975609756097561, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.8108108108108109, 31: 0.0, 32: 0.0, 33: 0.34782608695652173, 36: 0.5042016806722689, 40: 0.0}
Micro-average F1 score: 0.4012345679012346
Weighted-average F1 score: 0.33446326113392144
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.49504950495049505, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6190476190476191, 21: 0.0, 23: 0.0, 24: 0.0, 26: 0.0, 29: 0.0, 30: 0.6976744186046512, 31: 0.0, 32: 0.0, 33: 0.20833333333333334, 36: 0.5844155844155844, 40: 0.0}
Micro-average F1 score: 0.36065573770491804
Weighted-average F1 score: 0.2894919895326723
F1 score per class: {0: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.4842105263157895, 9: 0.0, 10: 0.0, 13: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.6370370370370371, 21: 0.0, 23: 0.0, 26: 0.0, 27: 0.0, 29: 0.0, 30: 0.75, 31: 0.0, 32: 0.0, 33: 0.2127659574468085, 36: 0.5578231292517006, 40: 0.0}
Micro-average F1 score: 0.38022813688212925
Weighted-average F1 score: 0.30998601309299983

F1 score per class: {0: 0.4657534246575342, 4: 0.8910891089108911, 5: 0.6220735785953178, 6: 0.2033898305084746, 7: 0.02586206896551724, 8: 0.27467811158798283, 9: 0.8333333333333334, 10: 0.037037037037037035, 13: 0.05714285714285714, 16: 0.3829787234042553, 17: 0.0, 18: 0.17391304347826086, 19: 0.6074074074074074, 20: 0.2978723404255319, 21: 0.21311475409836064, 23: 0.515625, 24: 0.09523809523809523, 26: 0.5700483091787439, 27: 0.23333333333333334, 29: 0.6721311475409836, 30: 0.8108108108108109, 31: 0.15384615384615385, 32: 0.5393939393939394, 33: 0.1111111111111111, 36: 0.40268456375838924, 40: 0.17793594306049823}
Micro-average F1 score: 0.4090338770388959
Weighted-average F1 score: 0.391839117176161
F1 score per class: {0: 0.3148148148148148, 4: 0.8430493273542601, 5: 0.3705103969754253, 6: 0.2831858407079646, 7: 0.017777777777777778, 8: 0.18691588785046728, 9: 0.5102040816326531, 10: 0.07462686567164178, 13: 0.06349206349206349, 16: 0.2441860465116279, 17: 0.0, 18: 0.15228426395939088, 19: 0.5185185185185185, 20: 0.43820224719101125, 21: 0.17699115044247787, 23: 0.47761194029850745, 24: 0.17391304347826086, 26: 0.5606694560669456, 27: 0.21621621621621623, 29: 0.6561264822134387, 30: 0.40540540540540543, 31: 0.047619047619047616, 32: 0.5714285714285714, 33: 0.05714285714285714, 36: 0.35714285714285715, 40: 0.15833333333333333}
Micro-average F1 score: 0.34207968901846453
Weighted-average F1 score: 0.3218344171388152
F1 score per class: {0: 0.37158469945355194, 4: 0.9038461538461539, 5: 0.4911392405063291, 6: 0.2764976958525346, 7: 0.017316017316017316, 8: 0.20909090909090908, 9: 0.7246376811594203, 10: 0.078125, 13: 0.05555555555555555, 16: 0.29850746268656714, 17: 0.0, 18: 0.17721518987341772, 19: 0.5472312703583062, 20: 0.4174757281553398, 21: 0.18340611353711792, 23: 0.49230769230769234, 24: 0.125, 26: 0.5789473684210527, 27: 0.23255813953488372, 29: 0.6586345381526104, 30: 0.6382978723404256, 31: 0.06896551724137931, 32: 0.56, 33: 0.05649717514124294, 36: 0.3565217391304348, 40: 0.14339622641509434}
Micro-average F1 score: 0.36837643069097076
Weighted-average F1 score: 0.3454439768928093
cur_acc_wo_na:  ['0.7852', '0.5471', '0.3117', '0.6732', '0.5579']
his_acc_wo_na:  ['0.7852', '0.6890', '0.5160', '0.5366', '0.5455']
cur_acc des_wo_na:  ['0.7500', '0.5511', '0.3462', '0.6010', '0.5220']
his_acc des_wo_na:  ['0.7500', '0.6582', '0.5326', '0.5030', '0.4755']
cur_acc rrf_wo_na:  ['0.7535', '0.5818', '0.3800', '0.6295', '0.5445']
his_acc rrf_wo_na:  ['0.7535', '0.6859', '0.5490', '0.5136', '0.5061']
cur_acc_w_na:  ['0.6543', '0.4236', '0.2712', '0.5088', '0.4012']
his_acc_w_na:  ['0.6543', '0.5447', '0.4138', '0.4101', '0.4090']
cur_acc des_w_na:  ['0.6125', '0.3886', '0.2951', '0.4327', '0.3607']
his_acc des_w_na:  ['0.6125', '0.4883', '0.4117', '0.3573', '0.3421']
cur_acc rrf_w_na:  ['0.6207', '0.4082', '0.3285', '0.4624', '0.3802']
his_acc rrf_w_na:  ['0.6207', '0.5132', '0.4320', '0.3720', '0.3684']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death'])
CurrentTrain: epoch  0, batch     0 | loss: 117.3904471CurrentTrain: epoch  0, batch     1 | loss: 101.7180413CurrentTrain: epoch  0, batch     2 | loss: 111.1651677CurrentTrain: epoch  0, batch     3 | loss: 90.3511036CurrentTrain: epoch  0, batch     4 | loss: 33.6134489CurrentTrain: epoch  1, batch     0 | loss: 95.4596767CurrentTrain: epoch  1, batch     1 | loss: 135.0083031CurrentTrain: epoch  1, batch     2 | loss: 87.0566250CurrentTrain: epoch  1, batch     3 | loss: 102.6757553CurrentTrain: epoch  1, batch     4 | loss: 19.8778967CurrentTrain: epoch  2, batch     0 | loss: 70.7850216CurrentTrain: epoch  2, batch     1 | loss: 87.5003297CurrentTrain: epoch  2, batch     2 | loss: 81.1453109CurrentTrain: epoch  2, batch     3 | loss: 174.2333195CurrentTrain: epoch  2, batch     4 | loss: 23.7472500CurrentTrain: epoch  3, batch     0 | loss: 98.0787618CurrentTrain: epoch  3, batch     1 | loss: 69.0926059CurrentTrain: epoch  3, batch     2 | loss: 83.2152819CurrentTrain: epoch  3, batch     3 | loss: 102.6524474CurrentTrain: epoch  3, batch     4 | loss: 23.0360395CurrentTrain: epoch  4, batch     0 | loss: 78.5246155CurrentTrain: epoch  4, batch     1 | loss: 98.7934799CurrentTrain: epoch  4, batch     2 | loss: 78.1593497CurrentTrain: epoch  4, batch     3 | loss: 81.5348874CurrentTrain: epoch  4, batch     4 | loss: 39.8603486CurrentTrain: epoch  5, batch     0 | loss: 98.3910531CurrentTrain: epoch  5, batch     1 | loss: 67.3126605CurrentTrain: epoch  5, batch     2 | loss: 95.9268290CurrentTrain: epoch  5, batch     3 | loss: 81.4329574CurrentTrain: epoch  5, batch     4 | loss: 25.5980851CurrentTrain: epoch  6, batch     0 | loss: 78.3941850CurrentTrain: epoch  6, batch     1 | loss: 65.0992252CurrentTrain: epoch  6, batch     2 | loss: 81.4929004CurrentTrain: epoch  6, batch     3 | loss: 70.0172592CurrentTrain: epoch  6, batch     4 | loss: 40.5873029CurrentTrain: epoch  7, batch     0 | loss: 90.7652681CurrentTrain: epoch  7, batch     1 | loss: 75.1019694CurrentTrain: epoch  7, batch     2 | loss: 125.0275684CurrentTrain: epoch  7, batch     3 | loss: 79.1129115CurrentTrain: epoch  7, batch     4 | loss: 15.8071152CurrentTrain: epoch  8, batch     0 | loss: 93.8299244CurrentTrain: epoch  8, batch     1 | loss: 76.7791199CurrentTrain: epoch  8, batch     2 | loss: 80.7079002CurrentTrain: epoch  8, batch     3 | loss: 94.3770969CurrentTrain: epoch  8, batch     4 | loss: 8.9882082CurrentTrain: epoch  9, batch     0 | loss: 94.2532917CurrentTrain: epoch  9, batch     1 | loss: 61.1971465CurrentTrain: epoch  9, batch     2 | loss: 120.4841366CurrentTrain: epoch  9, batch     3 | loss: 74.4421412CurrentTrain: epoch  9, batch     4 | loss: 23.4986496
MemoryTrain:  epoch  0, batch     0 | loss: 0.8372490MemoryTrain:  epoch  1, batch     0 | loss: 0.7786236MemoryTrain:  epoch  2, batch     0 | loss: 0.6559805MemoryTrain:  epoch  3, batch     0 | loss: 0.5249501MemoryTrain:  epoch  4, batch     0 | loss: 0.5063925MemoryTrain:  epoch  5, batch     0 | loss: 0.3929391MemoryTrain:  epoch  6, batch     0 | loss: 0.3557877MemoryTrain:  epoch  7, batch     0 | loss: 0.3184710MemoryTrain:  epoch  8, batch     0 | loss: 0.2667673MemoryTrain:  epoch  9, batch     0 | loss: 0.2383528

F1 score per class: {0: 0.0, 2: 0.7, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.4444444444444444, 12: 0.6363636363636364, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 28: 0.2777777777777778, 29: 0.0, 31: 0.0, 32: 0.0, 39: 0.2, 40: 0.0}
Micro-average F1 score: 0.4618937644341801
Weighted-average F1 score: 0.40154347210421976
F1 score per class: {0: 0.0, 2: 0.5, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.5732484076433121, 12: 0.6078431372549019, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 28: 0.3333333333333333, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 39: 0.38461538461538464, 40: 0.0}
Micro-average F1 score: 0.41186161449752884
Weighted-average F1 score: 0.3253055266768461
F1 score per class: {0: 0.0, 2: 0.56, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.575, 12: 0.6349206349206349, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 28: 0.3333333333333333, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 39: 0.43478260869565216, 40: 0.0}
Micro-average F1 score: 0.45255474452554745
Weighted-average F1 score: 0.3650890982356557

F1 score per class: {0: 0.6238532110091743, 2: 0.4375, 4: 0.8522727272727273, 5: 0.8516746411483254, 6: 0.26865671641791045, 7: 0.019230769230769232, 8: 0.3308270676691729, 9: 0.7936507936507936, 10: 0.03508771929824561, 11: 0.21660649819494585, 12: 0.35555555555555557, 13: 0.11764705882352941, 16: 0.49056603773584906, 17: 0.0, 18: 0.0, 19: 0.6332046332046332, 20: 0.6197183098591549, 21: 0.36363636363636365, 23: 0.7311827956989247, 24: 0.10526315789473684, 26: 0.6035502958579881, 27: 0.3888888888888889, 28: 0.09345794392523364, 29: 0.7979274611398963, 30: 0.8571428571428571, 31: 0.16666666666666666, 32: 0.7489361702127659, 33: 0.0, 36: 0.08695652173913043, 39: 0.11764705882352941, 40: 0.22448979591836735}
Micro-average F1 score: 0.47096032202415183
Weighted-average F1 score: 0.4675405671580986
F1 score per class: {0: 0.3756906077348066, 2: 0.2222222222222222, 4: 0.8316831683168316, 5: 0.5597667638483965, 6: 0.3877551020408163, 7: 0.03418803418803419, 8: 0.2974683544303797, 9: 0.5952380952380952, 10: 0.2097902097902098, 11: 0.3146853146853147, 12: 0.2546201232032854, 13: 0.1111111111111111, 16: 0.4634146341463415, 17: 0.0, 18: 0.15053763440860216, 19: 0.5283018867924528, 20: 0.6341463414634146, 21: 0.21164021164021163, 23: 0.6481481481481481, 24: 0.21428571428571427, 26: 0.6492146596858639, 27: 0.27586206896551724, 28: 0.13333333333333333, 29: 0.81, 30: 0.4155844155844156, 31: 0.08695652173913043, 32: 0.7206477732793523, 33: 0.1016949152542373, 36: 0.5321100917431193, 39: 0.16666666666666666, 40: 0.13471502590673576}
Micro-average F1 score: 0.41654167559460037
Weighted-average F1 score: 0.3916735533104944
F1 score per class: {0: 0.4594594594594595, 2: 0.2641509433962264, 4: 0.8817204301075269, 5: 0.7580645161290323, 6: 0.3878787878787879, 7: 0.03361344537815126, 8: 0.30115830115830117, 9: 0.6944444444444444, 10: 0.12121212121212122, 11: 0.2866043613707165, 12: 0.2777777777777778, 13: 0.08333333333333333, 16: 0.48484848484848486, 17: 0.0, 18: 0.11538461538461539, 19: 0.5675675675675675, 20: 0.6231884057971014, 21: 0.2608695652173913, 23: 0.6732673267326733, 24: 0.16666666666666666, 26: 0.6666666666666666, 27: 0.4, 28: 0.12121212121212122, 29: 0.8080808080808081, 30: 0.6666666666666666, 31: 0.1, 32: 0.701195219123506, 33: 0.10526315789473684, 36: 0.3058823529411765, 39: 0.21739130434782608, 40: 0.1187214611872146}
Micro-average F1 score: 0.43843416370106764
Weighted-average F1 score: 0.4158720307230448

F1 score per class: {0: 0.0, 2: 0.4375, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.3508771929824561, 12: 0.5137614678899083, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 26: 0.0, 28: 0.15384615384615385, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 39: 0.16, 40: 0.0}
Micro-average F1 score: 0.32362459546925565
Weighted-average F1 score: 0.27653732079645066
F1 score per class: {0: 0.0, 2: 0.34146341463414637, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.45, 12: 0.47509578544061304, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.17142857142857143, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.23809523809523808, 40: 0.0}
Micro-average F1 score: 0.2651113467656416
Weighted-average F1 score: 0.21424376908766501
F1 score per class: {0: 0.0, 2: 0.3684210526315789, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.4423076923076923, 12: 0.4897959183673469, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 23: 0.0, 26: 0.0, 28: 0.16901408450704225, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 36: 0.0, 39: 0.2564102564102564, 40: 0.0}
Micro-average F1 score: 0.29142185663924797
Weighted-average F1 score: 0.24025266693149405

F1 score per class: {0: 0.49635036496350365, 2: 0.24561403508771928, 4: 0.7936507936507936, 5: 0.7091633466135459, 6: 0.21052631578947367, 7: 0.01020408163265306, 8: 0.2634730538922156, 9: 0.7246376811594203, 10: 0.032520325203252036, 11: 0.14742014742014742, 12: 0.1728395061728395, 13: 0.07692307692307693, 16: 0.32098765432098764, 17: 0.0, 18: 0.0, 19: 0.5857142857142857, 20: 0.33587786259541985, 21: 0.23728813559322035, 23: 0.6181818181818182, 24: 0.1, 26: 0.5454545454545454, 27: 0.2222222222222222, 28: 0.05181347150259067, 29: 0.6724890829694323, 30: 0.7692307692307693, 31: 0.1, 32: 0.5317220543806647, 33: 0.0, 36: 0.0821917808219178, 39: 0.07547169811320754, 40: 0.1888412017167382}
Micro-average F1 score: 0.34068219633943425
Weighted-average F1 score: 0.3204356726241269
F1 score per class: {0: 0.2764227642276423, 2: 0.13861386138613863, 4: 0.7567567567567568, 5: 0.3562152133580705, 6: 0.24203821656050956, 7: 0.01834862385321101, 8: 0.1760299625468165, 9: 0.47619047619047616, 10: 0.14925373134328357, 11: 0.21377672209026127, 12: 0.13262032085561498, 13: 0.07407407407407407, 16: 0.2923076923076923, 17: 0.0, 18: 0.1, 19: 0.45776566757493187, 20: 0.3842364532019704, 21: 0.1444043321299639, 23: 0.46357615894039733, 24: 0.13636363636363635, 26: 0.5740740740740741, 27: 0.14814814814814814, 28: 0.07453416149068323, 29: 0.6532258064516129, 30: 0.3047619047619048, 31: 0.047619047619047616, 32: 0.52046783625731, 33: 0.0625, 36: 0.38666666666666666, 39: 0.07462686567164178, 40: 0.11403508771929824}
Micro-average F1 score: 0.27890961262553804
Weighted-average F1 score: 0.25909078119737955
F1 score per class: {0: 0.3333333333333333, 2: 0.15730337078651685, 4: 0.8241206030150754, 5: 0.5207756232686981, 6: 0.25196850393700787, 7: 0.018018018018018018, 8: 0.18309859154929578, 9: 0.6329113924050633, 10: 0.08695652173913043, 11: 0.19206680584551147, 12: 0.13793103448275862, 13: 0.06060606060606061, 16: 0.3076923076923077, 17: 0.0, 18: 0.07228915662650602, 19: 0.5029940119760479, 20: 0.35684647302904565, 21: 0.1651376146788991, 23: 0.4857142857142857, 24: 0.11764705882352941, 26: 0.5933014354066986, 27: 0.22857142857142856, 28: 0.06593406593406594, 29: 0.6530612244897959, 30: 0.5333333333333333, 31: 0.06896551724137931, 32: 0.5028571428571429, 33: 0.06557377049180328, 36: 0.24528301886792453, 39: 0.1, 40: 0.1015625}
Micro-average F1 score: 0.29639133921411387
Weighted-average F1 score: 0.2744353825397107
cur_acc_wo_na:  ['0.7852', '0.5471', '0.3117', '0.6732', '0.5579', '0.4619']
his_acc_wo_na:  ['0.7852', '0.6890', '0.5160', '0.5366', '0.5455', '0.4710']
cur_acc des_wo_na:  ['0.7500', '0.5511', '0.3462', '0.6010', '0.5220', '0.4119']
his_acc des_wo_na:  ['0.7500', '0.6582', '0.5326', '0.5030', '0.4755', '0.4165']
cur_acc rrf_wo_na:  ['0.7535', '0.5818', '0.3800', '0.6295', '0.5445', '0.4526']
his_acc rrf_wo_na:  ['0.7535', '0.6859', '0.5490', '0.5136', '0.5061', '0.4384']
cur_acc_w_na:  ['0.6543', '0.4236', '0.2712', '0.5088', '0.4012', '0.3236']
his_acc_w_na:  ['0.6543', '0.5447', '0.4138', '0.4101', '0.4090', '0.3407']
cur_acc des_w_na:  ['0.6125', '0.3886', '0.2951', '0.4327', '0.3607', '0.2651']
his_acc des_w_na:  ['0.6125', '0.4883', '0.4117', '0.3573', '0.3421', '0.2789']
cur_acc rrf_w_na:  ['0.6207', '0.4082', '0.3285', '0.4624', '0.3802', '0.2914']
his_acc rrf_w_na:  ['0.6207', '0.5132', '0.4320', '0.3720', '0.3684', '0.2964']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings'])
CurrentTrain: epoch  0, batch     0 | loss: 121.3303225CurrentTrain: epoch  0, batch     1 | loss: 100.9356917CurrentTrain: epoch  0, batch     2 | loss: 101.0140077CurrentTrain: epoch  0, batch     3 | loss: 93.9115595CurrentTrain: epoch  0, batch     4 | loss: 80.6470721CurrentTrain: epoch  1, batch     0 | loss: 74.6278805CurrentTrain: epoch  1, batch     1 | loss: 92.8085674CurrentTrain: epoch  1, batch     2 | loss: 90.0507397CurrentTrain: epoch  1, batch     3 | loss: 138.7832585CurrentTrain: epoch  1, batch     4 | loss: 48.8751420CurrentTrain: epoch  2, batch     0 | loss: 107.0764461CurrentTrain: epoch  2, batch     1 | loss: 105.3380957CurrentTrain: epoch  2, batch     2 | loss: 86.3268284CurrentTrain: epoch  2, batch     3 | loss: 70.8381238CurrentTrain: epoch  2, batch     4 | loss: 101.0836507CurrentTrain: epoch  3, batch     0 | loss: 100.8817864CurrentTrain: epoch  3, batch     1 | loss: 102.3372557CurrentTrain: epoch  3, batch     2 | loss: 83.2269835CurrentTrain: epoch  3, batch     3 | loss: 70.9350455CurrentTrain: epoch  3, batch     4 | loss: 56.1746930CurrentTrain: epoch  4, batch     0 | loss: 82.3192306CurrentTrain: epoch  4, batch     1 | loss: 78.8109847CurrentTrain: epoch  4, batch     2 | loss: 126.8627476CurrentTrain: epoch  4, batch     3 | loss: 83.1236738CurrentTrain: epoch  4, batch     4 | loss: 69.1995265CurrentTrain: epoch  5, batch     0 | loss: 100.8006916CurrentTrain: epoch  5, batch     1 | loss: 78.7288467CurrentTrain: epoch  5, batch     2 | loss: 99.8289687CurrentTrain: epoch  5, batch     3 | loss: 68.6384167CurrentTrain: epoch  5, batch     4 | loss: 56.0195978CurrentTrain: epoch  6, batch     0 | loss: 166.4192307CurrentTrain: epoch  6, batch     1 | loss: 67.9455487CurrentTrain: epoch  6, batch     2 | loss: 122.0159660CurrentTrain: epoch  6, batch     3 | loss: 66.5073547CurrentTrain: epoch  6, batch     4 | loss: 70.3947751CurrentTrain: epoch  7, batch     0 | loss: 97.6597476CurrentTrain: epoch  7, batch     1 | loss: 63.2102330CurrentTrain: epoch  7, batch     2 | loss: 96.1311616CurrentTrain: epoch  7, batch     3 | loss: 80.6495463CurrentTrain: epoch  7, batch     4 | loss: 70.8822892CurrentTrain: epoch  8, batch     0 | loss: 66.4649281CurrentTrain: epoch  8, batch     1 | loss: 76.7385333CurrentTrain: epoch  8, batch     2 | loss: 65.7301850CurrentTrain: epoch  8, batch     3 | loss: 163.4684059CurrentTrain: epoch  8, batch     4 | loss: 68.9416436CurrentTrain: epoch  9, batch     0 | loss: 75.9912102CurrentTrain: epoch  9, batch     1 | loss: 76.8917418CurrentTrain: epoch  9, batch     2 | loss: 121.3945882CurrentTrain: epoch  9, batch     3 | loss: 77.3640366CurrentTrain: epoch  9, batch     4 | loss: 66.6066152
MemoryTrain:  epoch  0, batch     0 | loss: 1.1910493MemoryTrain:  epoch  1, batch     0 | loss: 1.0119861MemoryTrain:  epoch  2, batch     0 | loss: 0.8593099MemoryTrain:  epoch  3, batch     0 | loss: 0.7066561MemoryTrain:  epoch  4, batch     0 | loss: 0.5395369MemoryTrain:  epoch  5, batch     0 | loss: 0.4791704MemoryTrain:  epoch  6, batch     0 | loss: 0.4414248MemoryTrain:  epoch  7, batch     0 | loss: 0.3693969MemoryTrain:  epoch  8, batch     0 | loss: 0.3665304MemoryTrain:  epoch  9, batch     0 | loss: 0.3088448

F1 score per class: {0: 0.0, 1: 0.20359281437125748, 3: 0.6815642458100558, 6: 0.0, 7: 0.0, 8: 0.0, 11: 0.0, 12: 0.0, 14: 0.0963855421686747, 18: 0.0, 19: 0.0, 22: 0.5431034482758621, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.7050359712230215, 40: 0.0}
Micro-average F1 score: 0.40585774058577406
Weighted-average F1 score: 0.3655634929069002
F1 score per class: {0: 0.0, 1: 0.2441860465116279, 3: 0.6437768240343348, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.10989010989010989, 16: 0.0, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5069444444444444, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.6923076923076923, 36: 0.0, 40: 0.0}
Micro-average F1 score: 0.3677581863979849
Weighted-average F1 score: 0.32886976420768704
F1 score per class: {0: 0.0, 1: 0.24277456647398843, 3: 0.6759259259259259, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 14: 0.12, 18: 0.0, 19: 0.0, 21: 0.0, 22: 0.5481481481481482, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.7022900763358778, 36: 0.0, 40: 0.0}
Micro-average F1 score: 0.39461883408071746
Weighted-average F1 score: 0.35364148204136076

F1 score per class: {0: 0.7317073170731707, 1: 0.1588785046728972, 2: 0.5384615384615384, 3: 0.46923076923076923, 4: 0.8117647058823529, 5: 0.8737864077669902, 6: 0.2222222222222222, 7: 0.019417475728155338, 8: 0.25225225225225223, 9: 0.8333333333333334, 10: 0.017543859649122806, 11: 0.06451612903225806, 12: 0.35714285714285715, 13: 0.13333333333333333, 14: 0.0784313725490196, 16: 0.5818181818181818, 17: 0.0, 18: 0.0, 19: 0.5448275862068965, 20: 0.624, 21: 0.0, 22: 0.4772727272727273, 23: 0.7529411764705882, 24: 0.0, 26: 0.6850828729281768, 27: 0.0, 28: 0.06060606060606061, 29: 0.79, 30: 0.8823529411764706, 31: 0.6666666666666666, 32: 0.6341463414634146, 33: 0.0, 34: 0.18702290076335878, 36: 0.1917808219178082, 39: 0.1111111111111111, 40: 0.25}
Micro-average F1 score: 0.40690559440559443
Weighted-average F1 score: 0.39292282662731987
F1 score per class: {0: 0.44, 1: 0.18502202643171806, 2: 0.2692307692307692, 3: 0.4065040650406504, 4: 0.8524590163934426, 5: 0.6204620462046204, 6: 0.4682926829268293, 7: 0.047058823529411764, 8: 0.31788079470198677, 9: 0.5952380952380952, 10: 0.15942028985507245, 11: 0.06896551724137931, 12: 0.29508196721311475, 13: 0.06451612903225806, 14: 0.07692307692307693, 16: 0.5217391304347826, 17: 0.0, 18: 0.0, 19: 0.4853801169590643, 20: 0.5656565656565656, 21: 0.04081632653061224, 22: 0.39037433155080214, 23: 0.6792452830188679, 24: 0.14285714285714285, 26: 0.6564102564102564, 27: 0.0, 28: 0.10526315789473684, 29: 0.7676767676767676, 30: 0.5245901639344263, 31: 0.09523809523809523, 32: 0.6183206106870229, 33: 0.06060606060606061, 34: 0.19522776572668113, 36: 0.5540540540540541, 39: 0.19230769230769232, 40: 0.23529411764705882}
Micro-average F1 score: 0.3884413309982487
Weighted-average F1 score: 0.373236065197256
F1 score per class: {0: 0.5892857142857143, 1: 0.18502202643171806, 2: 0.35, 3: 0.42441860465116277, 4: 0.8636363636363636, 5: 0.768595041322314, 6: 0.4186046511627907, 7: 0.04, 8: 0.3448275862068966, 9: 0.7142857142857143, 10: 0.15492957746478872, 11: 0.058823529411764705, 12: 0.3270440251572327, 13: 0.07142857142857142, 14: 0.08391608391608392, 16: 0.5454545454545454, 17: 0.0, 18: 0.0, 19: 0.50920245398773, 20: 0.5714285714285714, 21: 0.12, 22: 0.42528735632183906, 23: 0.6666666666666666, 24: 0.09523809523809523, 26: 0.6701570680628273, 27: 0.0, 28: 0.08108108108108109, 29: 0.7638190954773869, 30: 0.7692307692307693, 31: 0.1111111111111111, 32: 0.623574144486692, 33: 0.0, 34: 0.17523809523809525, 36: 0.27906976744186046, 39: 0.20408163265306123, 40: 0.22325581395348837}
Micro-average F1 score: 0.3916136278547361
Weighted-average F1 score: 0.3708299884731911

F1 score per class: {0: 0.0, 1: 0.1152542372881356, 2: 0.0, 3: 0.5062240663900415, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.09195402298850575, 18: 0.0, 19: 0.0, 20: 0.0, 22: 0.42424242424242425, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5833333333333334, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.27285513361462727
Weighted-average F1 score: 0.24208499993741936
F1 score per class: {0: 0.0, 1: 0.13725490196078433, 2: 0.0, 3: 0.4297994269340974, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.1, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.3882978723404255, 23: 0.0, 24: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5590062111801242, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.23714131023280996
Weighted-average F1 score: 0.21339235535650275
F1 score per class: {0: 0.0, 1: 0.13680781758957655, 2: 0.0, 3: 0.45625, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.10909090909090909, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.41456582633053224, 23: 0.0, 24: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.5575757575757576, 36: 0.0, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.25507246376811593
Weighted-average F1 score: 0.2298705284824185

F1 score per class: {0: 0.5309734513274337, 1: 0.08785529715762273, 2: 0.28, 3: 0.2863849765258216, 4: 0.7582417582417582, 5: 0.7086614173228346, 6: 0.18064516129032257, 7: 0.009302325581395349, 8: 0.2074074074074074, 9: 0.78125, 10: 0.014814814814814815, 11: 0.05154639175257732, 12: 0.16666666666666666, 13: 0.07142857142857142, 14: 0.07079646017699115, 16: 0.4050632911392405, 17: 0.0, 18: 0.0, 19: 0.49221183800623053, 20: 0.3263598326359833, 21: 0.0, 22: 0.33421750663129973, 23: 0.6464646464646465, 24: 0.0, 26: 0.6138613861386139, 27: 0.0, 28: 0.033112582781456956, 29: 0.6396761133603239, 30: 0.8333333333333334, 31: 0.2857142857142857, 32: 0.4642857142857143, 33: 0.0, 34: 0.11073446327683616, 36: 0.16666666666666666, 39: 0.06451612903225806, 40: 0.2066115702479339}
Micro-average F1 score: 0.2828927377696749
Weighted-average F1 score: 0.2617126984067399
F1 score per class: {0: 0.3188405797101449, 1: 0.10218978102189781, 2: 0.1590909090909091, 3: 0.23734177215189872, 4: 0.7918781725888325, 5: 0.4034334763948498, 6: 0.2831858407079646, 7: 0.024242424242424242, 8: 0.1835564053537285, 9: 0.49019607843137253, 10: 0.11956521739130435, 11: 0.057692307692307696, 12: 0.1393548387096774, 13: 0.041666666666666664, 14: 0.06622516556291391, 16: 0.35294117647058826, 17: 0.0, 18: 0.0, 19: 0.43116883116883115, 20: 0.33136094674556216, 21: 0.03636363636363636, 22: 0.2698706099815157, 23: 0.4931506849315068, 24: 0.12121212121212122, 26: 0.5739910313901345, 27: 0.0, 28: 0.06060606060606061, 29: 0.6055776892430279, 30: 0.41025641025641024, 31: 0.04878048780487805, 32: 0.4366576819407008, 33: 0.03361344537815126, 34: 0.12, 36: 0.36283185840707965, 39: 0.08928571428571429, 40: 0.18972332015810275}
Micro-average F1 score: 0.25724889816747853
Weighted-average F1 score: 0.24321813952651977
F1 score per class: {0: 0.4230769230769231, 1: 0.10144927536231885, 2: 0.19444444444444445, 3: 0.24212271973466004, 4: 0.8085106382978723, 5: 0.5406976744186046, 6: 0.2857142857142857, 7: 0.018691588785046728, 8: 0.25, 9: 0.6493506493506493, 10: 0.1111111111111111, 11: 0.04819277108433735, 12: 0.1492109038737446, 13: 0.0425531914893617, 14: 0.07317073170731707, 16: 0.3711340206185567, 17: 0.0, 18: 0.0, 19: 0.45604395604395603, 20: 0.32085561497326204, 21: 0.11320754716981132, 22: 0.2901960784313726, 23: 0.5037037037037037, 24: 0.08695652173913043, 26: 0.5953488372093023, 27: 0.0, 28: 0.04316546762589928, 29: 0.6031746031746031, 30: 0.6976744186046512, 31: 0.06666666666666667, 32: 0.43617021276595747, 33: 0.0, 34: 0.1073512252042007, 36: 0.21238938053097345, 39: 0.10752688172043011, 40: 0.17843866171003717}
Micro-average F1 score: 0.26340972047343236
Weighted-average F1 score: 0.24446752374653652
cur_acc_wo_na:  ['0.7852', '0.5471', '0.3117', '0.6732', '0.5579', '0.4619', '0.4059']
his_acc_wo_na:  ['0.7852', '0.6890', '0.5160', '0.5366', '0.5455', '0.4710', '0.4069']
cur_acc des_wo_na:  ['0.7500', '0.5511', '0.3462', '0.6010', '0.5220', '0.4119', '0.3678']
his_acc des_wo_na:  ['0.7500', '0.6582', '0.5326', '0.5030', '0.4755', '0.4165', '0.3884']
cur_acc rrf_wo_na:  ['0.7535', '0.5818', '0.3800', '0.6295', '0.5445', '0.4526', '0.3946']
his_acc rrf_wo_na:  ['0.7535', '0.6859', '0.5490', '0.5136', '0.5061', '0.4384', '0.3916']
cur_acc_w_na:  ['0.6543', '0.4236', '0.2712', '0.5088', '0.4012', '0.3236', '0.2729']
his_acc_w_na:  ['0.6543', '0.5447', '0.4138', '0.4101', '0.4090', '0.3407', '0.2829']
cur_acc des_w_na:  ['0.6125', '0.3886', '0.2951', '0.4327', '0.3607', '0.2651', '0.2371']
his_acc des_w_na:  ['0.6125', '0.4883', '0.4117', '0.3573', '0.3421', '0.2789', '0.2572']
cur_acc rrf_w_na:  ['0.6207', '0.4082', '0.3285', '0.4624', '0.3802', '0.2914', '0.2551']
his_acc rrf_w_na:  ['0.6207', '0.5132', '0.4320', '0.3720', '0.3684', '0.2964', '0.2634']
seen_des: dict_keys(['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters', 'unknown', 'NA', 'person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges', 'person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by', 'organization founded', 'person age', 'person city of birth', 'organization members', 'person religion', 'person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse', 'person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death', 'organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings', 'person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death'])
CurrentTrain: epoch  0, batch     0 | loss: 106.0027426CurrentTrain: epoch  0, batch     1 | loss: 117.7734690CurrentTrain: epoch  0, batch     2 | loss: 113.7893076CurrentTrain: epoch  0, batch     3 | loss: 55.0849678CurrentTrain: epoch  1, batch     0 | loss: 82.2726600CurrentTrain: epoch  1, batch     1 | loss: 109.6629386CurrentTrain: epoch  1, batch     2 | loss: 82.4374239CurrentTrain: epoch  1, batch     3 | loss: 75.3666366CurrentTrain: epoch  2, batch     0 | loss: 74.7307965CurrentTrain: epoch  2, batch     1 | loss: 87.8664599CurrentTrain: epoch  2, batch     2 | loss: 103.1141029CurrentTrain: epoch  2, batch     3 | loss: 55.1454116CurrentTrain: epoch  3, batch     0 | loss: 132.6183983CurrentTrain: epoch  3, batch     1 | loss: 67.0745021CurrentTrain: epoch  3, batch     2 | loss: 81.6588026CurrentTrain: epoch  3, batch     3 | loss: 65.7794809CurrentTrain: epoch  4, batch     0 | loss: 102.8185314CurrentTrain: epoch  4, batch     1 | loss: 125.7414866CurrentTrain: epoch  4, batch     2 | loss: 64.5115799CurrentTrain: epoch  4, batch     3 | loss: 46.4026070CurrentTrain: epoch  5, batch     0 | loss: 81.1180679CurrentTrain: epoch  5, batch     1 | loss: 73.6377678CurrentTrain: epoch  5, batch     2 | loss: 98.5065023CurrentTrain: epoch  5, batch     3 | loss: 65.9146879CurrentTrain: epoch  6, batch     0 | loss: 64.6407129CurrentTrain: epoch  6, batch     1 | loss: 68.5425555CurrentTrain: epoch  6, batch     2 | loss: 76.6327451CurrentTrain: epoch  6, batch     3 | loss: 68.1044092CurrentTrain: epoch  7, batch     0 | loss: 78.0959504CurrentTrain: epoch  7, batch     1 | loss: 97.0141984CurrentTrain: epoch  7, batch     2 | loss: 75.5376852CurrentTrain: epoch  7, batch     3 | loss: 44.2719949CurrentTrain: epoch  8, batch     0 | loss: 73.9706668CurrentTrain: epoch  8, batch     1 | loss: 77.2721538CurrentTrain: epoch  8, batch     2 | loss: 91.8392744CurrentTrain: epoch  8, batch     3 | loss: 81.2310049CurrentTrain: epoch  9, batch     0 | loss: 76.7914396CurrentTrain: epoch  9, batch     1 | loss: 75.6539440CurrentTrain: epoch  9, batch     2 | loss: 94.5700436CurrentTrain: epoch  9, batch     3 | loss: 49.1197521
MemoryTrain:  epoch  0, batch     0 | loss: 1.0177383MemoryTrain:  epoch  1, batch     0 | loss: 0.8765080MemoryTrain:  epoch  2, batch     0 | loss: 0.7591742MemoryTrain:  epoch  3, batch     0 | loss: 0.6168941MemoryTrain:  epoch  4, batch     0 | loss: 0.6266772MemoryTrain:  epoch  5, batch     0 | loss: 0.4746946MemoryTrain:  epoch  6, batch     0 | loss: 0.4377117MemoryTrain:  epoch  7, batch     0 | loss: 0.3558922MemoryTrain:  epoch  8, batch     0 | loss: 0.3155956MemoryTrain:  epoch  9, batch     0 | loss: 0.2986219

F1 score per class: {0: 0.0, 1: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.6666666666666666, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.3225806451612903, 28: 0.0, 32: 0.0, 34: 0.0, 35: 0.6506024096385542, 37: 0.6605504587155964, 38: 0.6086956521739131, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.35294117647058826
Weighted-average F1 score: 0.22301199726863385
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.631578947368421, 16: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5454545454545454, 26: 0.0, 28: 0.0, 30: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.8316831683168316, 36: 0.0, 37: 0.528, 38: 0.7241379310344828, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.423407917383821
Weighted-average F1 score: 0.30776018555146184
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.6, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.4931506849315068, 28: 0.0, 32: 0.0, 34: 0.0, 35: 0.7789473684210526, 36: 0.0, 37: 0.5426356589147286, 38: 0.6792452830188679, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.41454545454545455
Weighted-average F1 score: 0.30101100611654996

F1 score per class: {0: 0.6597938144329897, 1: 0.16597510373443983, 2: 0.5, 3: 0.16666666666666666, 4: 0.8255813953488372, 5: 0.7931034482758621, 6: 0.22727272727272727, 7: 0.02040816326530612, 8: 0.21621621621621623, 9: 0.746268656716418, 10: 0.017391304347826087, 11: 0.08633093525179857, 12: 0.30303030303030304, 13: 0.07142857142857142, 14: 0.07920792079207921, 15: 0.1791044776119403, 16: 0.5517241379310345, 17: 0.0, 18: 0.0, 19: 0.49079754601226994, 20: 0.3333333333333333, 21: 0.0, 22: 0.5, 23: 0.7333333333333333, 24: 0.1, 25: 0.3225806451612903, 26: 0.625, 27: 0.0, 28: 0.08130081300813008, 29: 0.79, 30: 0.8571428571428571, 31: 0.2222222222222222, 32: 0.5806451612903226, 33: 0.0, 34: 0.18227848101265823, 35: 0.2465753424657534, 36: 0.029850746268656716, 37: 0.1718377088305489, 38: 0.2916666666666667, 39: 0.12844036697247707, 40: 0.2376237623762376}
Micro-average F1 score: 0.3491050859471912
Weighted-average F1 score: 0.33717229784439035
F1 score per class: {0: 0.43037974683544306, 1: 0.16589861751152074, 2: 0.2222222222222222, 3: 0.35074626865671643, 4: 0.8020833333333334, 5: 0.5201072386058981, 6: 0.3891891891891892, 7: 0.06451612903225806, 8: 0.3876651982378855, 9: 0.5319148936170213, 10: 0.11594202898550725, 11: 0.14666666666666667, 12: 0.3076923076923077, 13: 0.08333333333333333, 14: 0.07692307692307693, 15: 0.34285714285714286, 16: 0.5074626865671642, 17: 0.0, 18: 0.0, 19: 0.4444444444444444, 20: 0.48214285714285715, 21: 0.04081632653061224, 22: 0.373134328358209, 23: 0.7222222222222222, 24: 0.07692307692307693, 25: 0.525, 26: 0.65625, 27: 0.0, 28: 0.09375, 29: 0.7614213197969543, 30: 0.5714285714285714, 31: 0.10526315789473684, 32: 0.6090225563909775, 33: 0.05970149253731343, 34: 0.21710526315789475, 35: 0.3387096774193548, 36: 0.38095238095238093, 37: 0.13692946058091288, 38: 0.23333333333333334, 39: 0.14705882352941177, 40: 0.2639593908629442}
Micro-average F1 score: 0.3565867331049517
Weighted-average F1 score: 0.340527629617012
F1 score per class: {0: 0.5396825396825397, 1: 0.16143497757847533, 2: 0.35, 3: 0.3381294964028777, 4: 0.8777777777777778, 5: 0.6956521739130435, 6: 0.31446540880503143, 7: 0.06060606060606061, 8: 0.36129032258064514, 9: 0.7142857142857143, 10: 0.031007751937984496, 11: 0.1282051282051282, 12: 0.2900763358778626, 13: 0.07407407407407407, 14: 0.08108108108108109, 15: 0.22641509433962265, 16: 0.5573770491803278, 17: 0.0, 18: 0.0, 19: 0.463768115942029, 20: 0.463768115942029, 21: 0.08163265306122448, 22: 0.4141689373297003, 23: 0.7422680412371134, 24: 0.0, 25: 0.48, 26: 0.6702127659574468, 27: 0.0, 28: 0.08108108108108109, 29: 0.7575757575757576, 30: 0.7804878048780488, 31: 0.1111111111111111, 32: 0.6036363636363636, 33: 0.0, 34: 0.20359281437125748, 35: 0.3135593220338983, 36: 0.16, 37: 0.13282732447817835, 38: 0.22085889570552147, 39: 0.11235955056179775, 40: 0.2549019607843137}
Micro-average F1 score: 0.35682747342600163
Weighted-average F1 score: 0.3392146130540831

F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 15: 0.3870967741935484, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.30303030303030304, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.574468085106383, 37: 0.5454545454545454, 38: 0.5283018867924528, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.23076923076923078
Weighted-average F1 score: 0.14802290678116
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.4444444444444444, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.5121951219512195, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7567567567567568, 36: 0.0, 37: 0.4520547945205479, 38: 0.5384615384615384, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2808219178082192
Weighted-average F1 score: 0.19870607980939498
F1 score per class: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.42857142857142855, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 25: 0.4675324675324675, 26: 0.0, 28: 0.0, 29: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.7115384615384616, 36: 0.0, 37: 0.445859872611465, 38: 0.5217391304347826, 39: 0.0, 40: 0.0}
Micro-average F1 score: 0.2766990291262136
Weighted-average F1 score: 0.19543153748840136

F1 score per class: {0: 0.47761194029850745, 1: 0.08968609865470852, 2: 0.27450980392156865, 3: 0.0989010989010989, 4: 0.7717391304347826, 5: 0.5897435897435898, 6: 0.17647058823529413, 7: 0.009708737864077669, 8: 0.1951219512195122, 9: 0.6944444444444444, 10: 0.014388489208633094, 11: 0.07453416149068323, 12: 0.15086206896551724, 13: 0.04081632653061224, 14: 0.06896551724137931, 15: 0.0821917808219178, 16: 0.34782608695652173, 17: 0.0, 18: 0.0, 19: 0.4244031830238727, 20: 0.15360983102918588, 21: 0.0, 22: 0.35051546391752575, 23: 0.6346153846153846, 24: 0.08695652173913043, 25: 0.29850746268656714, 26: 0.55, 27: 0.0, 28: 0.0423728813559322, 29: 0.61003861003861, 30: 0.8108108108108109, 31: 0.11764705882352941, 32: 0.41116751269035534, 33: 0.0, 34: 0.11162790697674418, 35: 0.17088607594936708, 36: 0.029850746268656716, 37: 0.09795918367346938, 38: 0.16279069767441862, 39: 0.06481481481481481, 40: 0.19047619047619047}
Micro-average F1 score: 0.2317101858386262
Weighted-average F1 score: 0.21384869990131988
F1 score per class: {0: 0.3119266055045872, 1: 0.08845208845208845, 2: 0.125, 3: 0.2052401746724891, 4: 0.7549019607843137, 5: 0.3409490333919156, 6: 0.23684210526315788, 7: 0.03389830508474576, 8: 0.24929178470254956, 9: 0.42016806722689076, 10: 0.08247422680412371, 11: 0.12290502793296089, 12: 0.14625228519195613, 13: 0.046511627906976744, 14: 0.06451612903225806, 15: 0.23529411764705882, 16: 0.3269230769230769, 17: 0.0, 18: 0.0, 19: 0.36446469248291574, 20: 0.2634146341463415, 21: 0.034482758620689655, 22: 0.25728987993138935, 23: 0.5342465753424658, 24: 0.06451612903225806, 25: 0.4666666666666667, 26: 0.5526315789473685, 27: 0.0, 28: 0.05454545454545454, 29: 0.5882352941176471, 30: 0.4155844155844156, 31: 0.04878048780487805, 32: 0.43315508021390375, 33: 0.03636363636363636, 34: 0.13894736842105262, 35: 0.22641509433962265, 36: 0.25806451612903225, 37: 0.0858257477243173, 38: 0.12962962962962962, 39: 0.07462686567164178, 40: 0.2}
Micro-average F1 score: 0.23681489141675285
Weighted-average F1 score: 0.22364742603356477
F1 score per class: {0: 0.38636363636363635, 1: 0.08591885441527446, 2: 0.1917808219178082, 3: 0.19183673469387755, 4: 0.8272251308900523, 5: 0.47880299251870323, 6: 0.2183406113537118, 7: 0.030456852791878174, 8: 0.2692307692307692, 9: 0.6410256410256411, 10: 0.022598870056497175, 11: 0.10416666666666667, 12: 0.1384335154826958, 13: 0.04, 14: 0.0670391061452514, 15: 0.14814814814814814, 16: 0.35051546391752575, 17: 0.0, 18: 0.0, 19: 0.387409200968523, 20: 0.25098039215686274, 21: 0.07142857142857142, 22: 0.28733459357277885, 23: 0.5625, 24: 0.0, 25: 0.4235294117647059, 26: 0.5727272727272728, 27: 0.0, 28: 0.046875, 29: 0.5882352941176471, 30: 0.6808510638297872, 31: 0.06896551724137931, 32: 0.43005181347150256, 33: 0.0, 34: 0.13076923076923078, 35: 0.21511627906976744, 36: 0.1411764705882353, 37: 0.08149010477299184, 38: 0.1180327868852459, 39: 0.05649717514124294, 40: 0.19330855018587362}
Micro-average F1 score: 0.23896615923776146
Weighted-average F1 score: 0.22255904666855994
cur_acc_wo_na:  ['0.7852', '0.5471', '0.3117', '0.6732', '0.5579', '0.4619', '0.4059', '0.3529']
his_acc_wo_na:  ['0.7852', '0.6890', '0.5160', '0.5366', '0.5455', '0.4710', '0.4069', '0.3491']
cur_acc des_wo_na:  ['0.7500', '0.5511', '0.3462', '0.6010', '0.5220', '0.4119', '0.3678', '0.4234']
his_acc des_wo_na:  ['0.7500', '0.6582', '0.5326', '0.5030', '0.4755', '0.4165', '0.3884', '0.3566']
cur_acc rrf_wo_na:  ['0.7535', '0.5818', '0.3800', '0.6295', '0.5445', '0.4526', '0.3946', '0.4145']
his_acc rrf_wo_na:  ['0.7535', '0.6859', '0.5490', '0.5136', '0.5061', '0.4384', '0.3916', '0.3568']
cur_acc_w_na:  ['0.6543', '0.4236', '0.2712', '0.5088', '0.4012', '0.3236', '0.2729', '0.2308']
his_acc_w_na:  ['0.6543', '0.5447', '0.4138', '0.4101', '0.4090', '0.3407', '0.2829', '0.2317']
cur_acc des_w_na:  ['0.6125', '0.3886', '0.2951', '0.4327', '0.3607', '0.2651', '0.2371', '0.2808']
his_acc des_w_na:  ['0.6125', '0.4883', '0.4117', '0.3573', '0.3421', '0.2789', '0.2572', '0.2368']
cur_acc rrf_w_na:  ['0.6207', '0.4082', '0.3285', '0.4624', '0.3802', '0.2914', '0.2551', '0.2767']
his_acc rrf_w_na:  ['0.6207', '0.5132', '0.4320', '0.3720', '0.3684', '0.2964', '0.2634', '0.2390']
----------END
his_acc mean_wo_na:  [0.7694 0.6442 0.5513 0.5157 0.4542 0.4336 0.3947 0.3674]
his_acc des mean_wo_na:  [0.7497 0.6328 0.5406 0.4869 0.4316 0.4082 0.3776 0.3571]
his_acc rrf mean_wo_na:  [0.7616 0.6445 0.5499 0.495  0.4398 0.4188 0.3843 0.3614]
his_acc mean_w_na:  [0.6415 0.5041 0.4259 0.3845 0.3266 0.3127 0.2778 0.259 ]
his_acc des mean_w_na:  [0.6088 0.4774 0.4004 0.3415 0.2972 0.2804 0.2551 0.2418]
his_acc rrf mean_w_na:  [0.6236 0.4914 0.4129 0.3523 0.3073 0.2901 0.2632 0.2474]
